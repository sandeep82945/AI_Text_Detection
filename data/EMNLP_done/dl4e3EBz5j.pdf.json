{
    "abstractText": "Several recent papers have published good solutions for language identification (LID) for about 300 high-resource and medium-resource languages. However, there is no LID available that (i) covers a wide range of low-resource languages, (ii) is rigorously evaluated and reliable and (iii) efficient and easy to use. Here, we publish GlotLID-M, an LID model that satisfies the desiderata of wide coverage, reliability and efficiency. It identifies 1665 languages, a large increase in coverage compared to prior work. In our experiments, GlotLID-M outperforms four baselines (CLD3, FT176, OpenLID and NLLB) when balancing F1 and false positive rate (FPR). We analyze the unique challenges that low-resource LID poses: incorrect corpus metadata, leakage from high-resource languages, difficulty separating closely related languages, handling of macrolanguage vs varieties and in general noisy data. We hope that integrating GlotLID-M into dataset creation pipelines will improve quality and enhance accessibility of NLP technology for low-resource languages and cultures. GlotLID-M model, code, and list of data sources are available: https: //github.com/cisnlp/GlotLID.",
    "authors": [
        {
            "affiliations": [],
            "name": "Amir Hossein Kargaran"
        },
        {
            "affiliations": [],
            "name": "Ayyoob Imani"
        },
        {
            "affiliations": [],
            "name": "Fran\u00e7ois Yvon"
        },
        {
            "affiliations": [],
            "name": "Hinrich Sch\u00fctze"
        }
    ],
    "id": "SP:1e0dce5ee440479a806ae0b6d71d18a46efa363c",
    "references": [
        {
            "authors": [
                "Julien Abadji",
                "Pedro Javier Ortiz Su\u00e1rez",
                "Laurent Romary",
                "Beno\u00eet Sagot."
            ],
            "title": "Ungoliant: An optimized pipeline for the generation of a very large-scale multilingual web corpus",
            "venue": "Proceedings of the Workshop on Challenges in the Management of Large",
            "year": 2021
        },
        {
            "authors": [
                "Kathrein Abu Kwaik",
                "Motaz Saad",
                "Stergios Chatzikyriakidis",
                "Simon Dobnik."
            ],
            "title": "Shami: A corpus of Levantine Arabic dialects",
            "venue": "Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018), Miyazaki,",
            "year": 2018
        },
        {
            "authors": [
                "Ife Adebara",
                "Muhammad Abdul-Mageed."
            ],
            "title": "Towards afrocentric NLP for African languages: Where we are and where we can go",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages",
            "year": 2022
        },
        {
            "authors": [
                "Ife Adebara",
                "AbdelRahim Elmadany",
                "Muhammad Abdul-Mageed",
                "Alcides Inciarte."
            ],
            "title": "AfroLID: A neural language identification tool for African languages",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2022
        },
        {
            "authors": [
                "hamed",
                "Fuad Mire Hassan",
                "Moges Ahmed Mehamed",
                "Evrard Ngabire",
                "Pontus Stenetorp"
            ],
            "title": "Masakhanews: News topic classification for african",
            "year": 2023
        },
        {
            "authors": [
                "Gustavo Aguilar",
                "Sudipta Kar",
                "Thamar Solorio."
            ],
            "title": "LinCE: A centralized benchmark for linguistic code-switching evaluation",
            "venue": "Proceedings of the Twelfth Language Resources and Evaluation Conference, pages 1803\u20131813, Marseille, France. European",
            "year": 2020
        },
        {
            "authors": [
                "Sina Ahmadi",
                "Milind Agarwal",
                "Antonios Anastasopoulos."
            ],
            "title": "PALI: A language identification benchmark for Perso-Arabic scripts",
            "venue": "Tenth Workshop on NLP for Similar Languages, Varieties and Dialects (VarDial 2023), pages 78\u201390, Dubrovnik,",
            "year": 2023
        },
        {
            "authors": [
                "Israa Alsarsour",
                "Esraa Mohamed",
                "Reem Suwaileh",
                "Tamer Elsayed."
            ],
            "title": "DART: A large dataset of dialectal Arabic tweets",
            "venue": "Proceedings of the Eleventh International Conference on Language Resources",
            "year": 2018
        },
        {
            "authors": [
                "Rosana Ardila",
                "Megan Branson",
                "Kelly Davis",
                "Michael Kohler",
                "Josh Meyer",
                "Michael Henretty",
                "Reuben Morais",
                "Lindsay Saunders",
                "Francis Tyers",
                "Gregor Weber."
            ],
            "title": "Common voice: A massivelymultilingual speech corpus",
            "venue": "Proceedings of the",
            "year": 2020
        },
        {
            "authors": [
                "Ankur Bapna",
                "Isaac Caswell",
                "Julia Kreutzer",
                "Orhan Firat",
                "Daan van Esch",
                "Aditya Siddhant",
                "Mengmeng Niu",
                "Pallavi Baljekar",
                "Xavier Garcia",
                "Wolfgang Macherey"
            ],
            "title": "Building machine translation systems for the next thousand languages",
            "year": 2022
        },
        {
            "authors": [
                "Monz",
                "Makoto Morishita",
                "Masaaki Nagata",
                "Toshiaki Nakazawa",
                "Santanu Pal",
                "Matt Post",
                "Marcos Zampieri."
            ],
            "title": "Findings of the 2020 conference on machine translation (WMT20)",
            "venue": "Proceedings of the Fifth Conference on Machine Translation, pages",
            "year": 2020
        },
        {
            "authors": [
                "Verena Blaschke",
                "Hinrich Schuetze",
                "Barbara Plank."
            ],
            "title": "A survey of corpora for Germanic lowresource languages and dialects",
            "venue": "Proceedings of the 24th Nordic Conference on Computational Linguistics (NoDaLiDa), pages 392\u2013414, T\u00f3rshavn,",
            "year": 2023
        },
        {
            "authors": [
                "Terra Blevins",
                "Luke Zettlemoyer."
            ],
            "title": "Language contamination helps explains the cross-lingual capabilities of English pretrained models",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 3563\u20133574, Abu",
            "year": 2022
        },
        {
            "authors": [
                "Ond\u0159ej Bojar",
                "Christian Buck",
                "Chris Callison-Burch",
                "Christian Federmann",
                "Barry Haddow",
                "Philipp Koehn",
                "Christof Monz",
                "Matt Post",
                "Radu Soricut",
                "Lucia Specia."
            ],
            "title": "Findings of the 2013 Workshop on Statistical Machine Translation",
            "venue": "Proceedings of",
            "year": 2013
        },
        {
            "authors": [
                "Ond\u0159ej Bojar",
                "Christian Buck",
                "Christian Federmann",
                "Barry Haddow",
                "Philipp Koehn",
                "Johannes Leveling",
                "Christof Monz",
                "Pavel Pecina",
                "Matt Post",
                "Herve SaintAmand",
                "Radu Soricut",
                "Lucia Specia",
                "Ale\u0161 Tamchyna"
            ],
            "title": "Findings of the 2014 workshop",
            "year": 2014
        },
        {
            "authors": [
                "Raphael Rubino",
                "Carolina Scarton",
                "Lucia Specia",
                "Marco Turchi",
                "Karin Verspoor",
                "Marcos Zampieri."
            ],
            "title": "Findings of the 2016 conference on machine translation",
            "venue": "Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Pa-",
            "year": 2016
        },
        {
            "authors": [
                "Ond\u0159ej Bojar",
                "Christian Federmann",
                "Mark Fishel",
                "Yvette Graham",
                "Barry Haddow",
                "Matthias Huck",
                "Philipp Koehn",
                "Christof Monz."
            ],
            "title": "Findings of the 2018 conference on machine translation (WMT18)",
            "venue": "Proceedings of the Third Conference on Machine",
            "year": 2018
        },
        {
            "authors": [
                "Jan A. Botha",
                "Emily Pitler",
                "Ji Ma",
                "Anton Bakalov",
                "Alex Salcianu",
                "David Weiss",
                "Ryan McDonald",
                "Slav Petrov"
            ],
            "title": "Natural language processing with small",
            "year": 2017
        },
        {
            "authors": [
                "Houda Bouamor",
                "Sabit Hassan",
                "Nizar Habash."
            ],
            "title": "The MADAR shared task on Arabic fine-grained dialect identification",
            "venue": "Proceedings of the Fourth Arabic Natural Language Processing Workshop, pages 199\u2013207, Florence, Italy. Association for Computa-",
            "year": 2019
        },
        {
            "authors": [
                "Ralf Brown"
            ],
            "title": "Language-aware string extractor",
            "year": 2014
        },
        {
            "authors": [
                "Ralf Brown."
            ],
            "title": "Non-linear mapping for improved identification of 1300+ languages",
            "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 627\u2013 632, Doha, Qatar. Association for Computational",
            "year": 2014
        },
        {
            "authors": [
                "Ralf D Brown."
            ],
            "title": "Finding and identifying text in 900+ languages",
            "venue": "Digital Investigation, 9:S34\u2013S43.",
            "year": 2012
        },
        {
            "authors": [
                "Laurie Burchell",
                "Alexandra Birch",
                "Nikolay Bogoychev",
                "Kenneth Heafield."
            ],
            "title": "An open dataset and model for language identification",
            "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),",
            "year": 2023
        },
        {
            "authors": [
                "Isaac Caswell",
                "Theresa Breiner",
                "Daan van Esch",
                "Ankur Bapna."
            ],
            "title": "Language ID in the wild: Unexpected challenges on the path to a thousand-language web text corpus",
            "venue": "Proceedings of the 28th International Conference on Computational Linguistics,",
            "year": 2020
        },
        {
            "authors": [
                "William B Cavnar",
                "John M Trenkle"
            ],
            "title": "N-grambased text categorization",
            "venue": "In Proceedings of SDAIR94, 3rd annual symposium on document analysis and information retrieval,",
            "year": 1994
        },
        {
            "authors": [
                "Alexis Conneau",
                "Kartikay Khandelwal",
                "Naman Goyal",
                "Vishrav Chaudhary",
                "Guillaume Wenzek",
                "Francisco Guzm\u00e1n",
                "Edouard Grave",
                "Myle Ott",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Unsupervised cross-lingual representation learning at scale",
            "venue": "Pro-",
            "year": 2020
        },
        {
            "authors": [
                "Morris H DeGroot",
                "Stephen E Fienberg."
            ],
            "title": "The comparison and evaluation of forecasters",
            "venue": "Journal of the Royal Statistical Society: Series D (The Statistician), 32(1-2):12\u201322.",
            "year": 1983
        },
        {
            "authors": [
                "Jonathan Dunn."
            ],
            "title": "Mapping languages: The corpus of global language use",
            "venue": "Language Resources and Evaluation, 54:999\u20131018.",
            "year": 2020
        },
        {
            "authors": [
                "Mahmoud El-Haj",
                "Paul Rayson",
                "Mariam Aboelezz."
            ],
            "title": "Arabic dialect identification in the context of bivalency and code-switching",
            "venue": "Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018), Miyazaki,",
            "year": 2018
        },
        {
            "authors": [
                "Dirk Goldhahn",
                "Thomas Eckart",
                "Uwe Quasthoff."
            ],
            "title": "Building large monolingual dictionaries at the Leipzig corpora collection: From 100 to 200 languages",
            "venue": "Proceedings of the Eighth International Conference on Language Resources and Evaluation",
            "year": 2012
        },
        {
            "authors": [
                "Santiago G\u00f3ngora",
                "Nicol\u00e1s Giossa",
                "Luis Chiruzzo"
            ],
            "title": "Can we use word embeddings for enhancing Guarani-Spanish machine translation",
            "venue": "In Proceedings of the Fifth Workshop on the Use of Computational Methods in the Study of Endangered Lan-",
            "year": 2022
        },
        {
            "authors": [
                "Koustava Goswami",
                "Rajdeep Sarkar",
                "Bharathi Raja Chakravarthi",
                "Theodorus Fransen",
                "John P. McCrae."
            ],
            "title": "Unsupervised deep language and dialect identification for short texts",
            "venue": "Proceedings of the 28th International Conference on Computational",
            "year": 2020
        },
        {
            "authors": [
                "Thamme Gowda",
                "Zhao Zhang",
                "Chris Mattmann",
                "Jonathan May."
            ],
            "title": "Many-to-English machine translation tools, data, and pretrained models",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th",
            "year": 2021
        },
        {
            "authors": [
                "Hendrik J Groenewald",
                "Liza du Plooy."
            ],
            "title": "Processing parallel text corpora for three south african language pairs in the autshumato project",
            "venue": "AfLaT 2010, page 27.",
            "year": 2010
        },
        {
            "authors": [
                "Ren\u00e9 Haas",
                "Leon Derczynski."
            ],
            "title": "Discriminating between similar Nordic languages",
            "venue": "Proceedings of the Eighth Workshop on NLP for Similar Languages, Varieties and Dialects, pages 67\u201375, Kiyv, Ukraine. Association for Computational Linguistics.",
            "year": 2021
        },
        {
            "authors": [
                "Tahmid Hasan",
                "Abhik Bhattacharjee",
                "Md. Saiful Islam",
                "Kazi Mubasshir",
                "Yuan-Fang Li",
                "Yong-Bin Kang",
                "M. Sohel Rahman",
                "Rifat Shahriyar."
            ],
            "title": "XLsum: Large-scale multilingual abstractive summarization for 44 languages",
            "venue": "Findings of the Association",
            "year": 2021
        },
        {
            "authors": [
                "Rudali Huidrom",
                "Yves Lepage",
                "Khogendra Khomdram."
            ],
            "title": "EM corpus: a comparable corpus for a less-resourced language pair Manipuri-English",
            "venue": "In",
            "year": 2021
        },
        {
            "authors": [
                "Ayyoob ImaniGooghari",
                "Peiqin Lin",
                "Amir Hossein Kargaran",
                "Silvia Severini",
                "Masoud Jalili Sabet",
                "Nora Kassner",
                "Chunlan Ma",
                "Helmut Schmid",
                "Andr\u00e9 Martins",
                "Fran\u00e7ois Yvon",
                "Hinrich Sch\u00fctze"
            ],
            "title": "Glot500: Scaling multilingual corpora and language",
            "year": 2023
        },
        {
            "authors": [
                "Abderrahmane Issam",
                "Khalil Mrini"
            ],
            "title": "Goud.ma: a news article dataset for summarization in moroccan darija",
            "venue": "In 3rd Workshop on African Natural Language Processing",
            "year": 2022
        },
        {
            "authors": [
                "Heidi Jauhiainen",
                "Tommi Jauhiainen",
                "Krister Linden."
            ],
            "title": "Wanca in korp: Text corpora for underresourced uralic languages",
            "venue": "Proceedings of the Research data and humanities (RDHUM) 2019 conference. University of Oulu.",
            "year": 2019
        },
        {
            "authors": [
                "Tommi Jauhiainen",
                "Heidi Jauhiainen",
                "Krister Lind\u00e9n."
            ],
            "title": "HeLI-OTS, off-the-shelf language identifier for text",
            "venue": "Proceedings of the Thirteenth Language Resources and Evaluation Conference, pages 3912\u20133922, Marseille, France. European Lan-",
            "year": 2022
        },
        {
            "authors": [
                "Tommi Jauhiainen",
                "Marco Lui",
                "Marcos Zampieri",
                "Timothy Baldwin",
                "Krister Lind\u00e9n."
            ],
            "title": "Automatic language identification in texts: A survey",
            "venue": "Journal of Artificial Intelligence Research, 65:675\u2013782.",
            "year": 2019
        },
        {
            "authors": [
                "Armand Joulin",
                "Edouard Grave",
                "Piotr Bojanowski",
                "Tomas Mikolov."
            ],
            "title": "Bag of tricks for efficient text classification",
            "venue": "Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Pa-",
            "year": 2017
        },
        {
            "authors": [
                "David Jurgens",
                "Yulia Tsvetkov",
                "Dan Jurafsky."
            ],
            "title": "Incorporating dialectal variability for socially equitable language identification",
            "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages",
            "year": 2017
        },
        {
            "authors": [
                "Amir Hossein Kargaran",
                "Fran\u00e7ois Yvon",
                "Hinrich Sch\u00fctze."
            ],
            "title": "Glotscript: A resource and tool for low resource writing system identification",
            "venue": "arXiv preprint arXiv:2309.13320.",
            "year": 2023
        },
        {
            "authors": [
                "Omid Kashefi."
            ],
            "title": "Mizan: A large persian-english parallel corpus",
            "venue": "arXiv preprint arXiv:1801.02107.",
            "year": 2018
        },
        {
            "authors": [
                "Tom Kocmi",
                "Ond\u0159ej Bojar."
            ],
            "title": "LanideNN: Multilingual language identification on character window",
            "venue": "Proceedings of the 15th Conference of the European Chapter of the Association for Computational",
            "year": 2017
        },
        {
            "authors": [
                "tisti",
                "Ahmed Baruwa",
                "Ankur Bapna",
                "Pallavi Baljekar",
                "Israel Abebe Azime",
                "Ayodele Awokoya",
                "Duygu Ataman",
                "Orevaoghene Ahia",
                "Oghenefego Ahia",
                "Sweta Agrawal",
                "Mofetoluwa Adeyemi"
            ],
            "title": "Quality at a glance: An audit of web-crawled multilingual",
            "year": 2022
        },
        {
            "authors": [
                "Sneha Kudugunta",
                "Isaac Caswell",
                "Biao Zhang",
                "Xavier Garcia",
                "Christopher A Choquette-Choo",
                "Katherine Lee",
                "Derrick Xin",
                "Aditya Kusupati",
                "Romi Stella",
                "Ankur Bapna"
            ],
            "title": "Madlad-400: A multilingual and document-level large audited",
            "year": 2023
        },
        {
            "authors": [
                "Anoop Kunchukuttan",
                "Pratik Mehta",
                "Pushpak Bhattacharyya."
            ],
            "title": "The IIT Bombay English-Hindi parallel corpus",
            "venue": "Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018), Miyazaki, Japan. European",
            "year": 2018
        },
        {
            "authors": [
                "Richard Lastrucci",
                "Jenalea Rajab",
                "Matimba Shingange",
                "Daniel Njini",
                "Vukosi Marivate."
            ],
            "title": "Preparing the vuk\u2019uzenzele and ZA-gov-multilingual South African multilingual corpora",
            "venue": "Proceedings of the Fourth workshop on Resources for African In-",
            "year": 2023
        },
        {
            "authors": [
                "Marco Lui",
                "Timothy Baldwin"
            ],
            "title": "langid.py: An off-the-shelf language identification tool",
            "venue": "In Proceedings of the ACL 2012 System Demonstrations,",
            "year": 2012
        },
        {
            "authors": [
                "Kang Kwong Luke",
                "May LY Wong."
            ],
            "title": "The hong kong cantonese corpus: design and uses",
            "venue": "Journal of Chinese Linguistics Monograph Series, 1(25):312\u2013 333.",
            "year": 2015
        },
        {
            "authors": [
                "Thomas Mayer",
                "Michael Cysouw."
            ],
            "title": "Creating a massively parallel Bible corpus",
            "venue": "Proceedings of the Ninth International Conference on Language",
            "year": 2014
        },
        {
            "authors": [
                "Michael McCandless."
            ],
            "title": "Accuracy and performance of google\u2019s compact language detector",
            "venue": "Blog post.",
            "year": 2010
        },
        {
            "authors": [
                "Salima Medhaffar",
                "Fethi Bougares",
                "Yannick Est\u00e8ve",
                "Lamia Hadrich-Belguith."
            ],
            "title": "Sentiment analysis of Tunisian dialects: Linguistic ressources and experiments",
            "venue": "Proceedings of the Third Arabic Natural Language Processing Workshop, pages 55\u2013",
            "year": 2017
        },
        {
            "authors": [
                "Karima Meftouh",
                "Salima Harrat",
                "Salma Jamoussi",
                "Mourad Abbas",
                "Kamel Smaili."
            ],
            "title": "Machine translation experiments on PADIC: A parallel Arabic DIalect corpus",
            "venue": "Proceedings of the 29th Pacific Asia Conference on Language, Information and Com-",
            "year": 2015
        },
        {
            "authors": [
                "Firat",
                "Sriram Chellappan."
            ],
            "title": "A large-scale study of machine translation in Turkic languages",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 5876\u20135890, Online and Punta Cana, Dominican Re-",
            "year": 2021
        },
        {
            "authors": [
                "Alexandru Niculescu-Mizil",
                "Rich Caruana."
            ],
            "title": "Predicting good probabilities with supervised learning",
            "venue": "Proceedings of the 22nd international conference on Machine learning, pages 625\u2013632.",
            "year": 2005
        },
        {
            "authors": [
                "Joakim Nivre",
                "Marie-Catherine de Marneffe",
                "Filip Ginter",
                "Jan Haji\u010d",
                "Christopher D. Manning",
                "Sampo Pyysalo",
                "Sebastian Schuster",
                "Francis Tyers",
                "Daniel Zeman."
            ],
            "title": "Universal Dependencies v2: An evergrowing multilingual treebank collection",
            "venue": "In",
            "year": 2020
        },
        {
            "authors": [
                "Gao",
                "Vedanuj Goswami",
                "Francisco Guzm\u00e1n",
                "Philipp Koehn",
                "Alexandre Mourachko",
                "Christophe Ropers",
                "Safiyyah Saleem",
                "Holger Schwenk",
                "Jeff Wang."
            ],
            "title": "No language left behind: Scaling human-centered machine translation",
            "venue": "arXiv preprint",
            "year": 2022
        },
        {
            "authors": [
                "goma",
                "Priscilla A. Amuok",
                "Ruqayya Nasir Iro",
                "Sonia Adhiambo"
            ],
            "title": "Afriqa: Cross-lingual openretrieval question answering for african",
            "year": 2023
        },
        {
            "authors": [
                "Atul Kr Ojha."
            ],
            "title": "English-bhojpuri smt system: Insights from the karaka model",
            "venue": "arXiv preprint arXiv:1905.02239.",
            "year": 2019
        },
        {
            "authors": [
                "Pedro Javier Ortiz Su\u00e1rez",
                "Beno\u00eet Sagot",
                "Laurent Romary."
            ],
            "title": "Asynchronous pipelines for processing huge corpora on medium to low resource infrastructures",
            "venue": "Proceedings of the Workshop on Challenges in the Management of Large Corpora (CMLC-",
            "year": 2019
        },
        {
            "authors": [
                "Mohammad Taher Pilevar",
                "Heshaam Faili",
                "Abdol Hamid Pilevar."
            ],
            "title": "Tep: Tehran englishpersian parallel corpus",
            "venue": "International Conference on Intelligent Text Processing and Computational Linguistics, pages 68\u201379. Springer.",
            "year": 2011
        },
        {
            "authors": [
                "Matt Post",
                "Chris Callison-Burch",
                "Miles Osborne."
            ],
            "title": "Constructing parallel corpora for six Indian languages via crowdsourcing",
            "venue": "Proceedings of the Seventh Workshop on Statistical Machine Translation, pages 401\u2013409, Montr\u00e9al, Canada. Association for",
            "year": 2012
        },
        {
            "authors": [
                "Ye Qi",
                "Devendra Sachan",
                "Matthieu Felix",
                "Sarguna Padmanabhan",
                "Graham Neubig"
            ],
            "title": "When and why are pre-trained word embeddings useful for neural machine translation",
            "venue": "In Proceedings of the 2018 Conference of the North American Chapter",
            "year": 2018
        },
        {
            "authors": [
                "Xingzhang Ren",
                "Baosong Yang",
                "Dayiheng Liu",
                "Haibo Zhang",
                "Xiaoyu Lv",
                "Liang Yao",
                "Jun Xie."
            ],
            "title": "Effective approaches to neural query language identification",
            "venue": "Computational Linguistics, 48(4):887\u2013906.",
            "year": 2022
        },
        {
            "authors": [
                "Alex Salcianu",
                "Andy Golding",
                "Anton Bakalov",
                "Chris Alberti",
                "Daniel Andor",
                "David Weiss",
                "Emily Pitler",
                "Greg Coppola",
                "Jason Riesa",
                "Kuzman Ganchev"
            ],
            "title": "Compact language detector v3",
            "year": 2018
        },
        {
            "authors": [
                "Nakatani Shuyo"
            ],
            "title": "Language detection library for java",
            "year": 2010
        },
        {
            "authors": [
                "Kathleen Siminyu",
                "Godson Kalipe",
                "Davor Orlic",
                "Jade Abbott",
                "Vukosi Marivate",
                "Sackey Freshia",
                "Prateek Sibal",
                "Bhanu Neupane",
                "David I Adelani",
                "Amelia Taylor"
            ],
            "title": "Ai4d\u2013african language program",
            "venue": "arXiv preprint arXiv:2104.02516",
            "year": 2021
        },
        {
            "authors": [
                "Martin Thoma."
            ],
            "title": "The wili benchmark dataset for written language identification",
            "venue": "arXiv preprint arXiv:1801.07779.",
            "year": 2018
        },
        {
            "authors": [
                "J\u00f6rg Tiedemann."
            ],
            "title": "Parallel data, tools and interfaces in OPUS",
            "venue": "Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC\u201912), pages 2214\u20132218, Istanbul, Turkey. European Language Resources Association",
            "year": 2012
        },
        {
            "authors": [
                "Daan van Esch",
                "Tamar Lucassen",
                "Sebastian Ruder",
                "Isaac Caswell",
                "Clara Rivera."
            ],
            "title": "Writing system and speaker metadata for 2,800+ language varieties",
            "venue": "Proceedings of the Thirteenth Language Resources and Evaluation Conference, pages 5035\u20135046, Mar-",
            "year": 2022
        },
        {
            "authors": [
                "Guillaume Wenzek",
                "Marie-Anne Lachaux",
                "Alexis Conneau",
                "Vishrav Chaudhary",
                "Francisco Guzm\u00e1n",
                "Armand Joulin",
                "Edouard Grave."
            ],
            "title": "CCNet: Extracting high quality monolingual datasets from web crawl data",
            "venue": "Proceedings of the Twelfth Lan-",
            "year": 2020
        },
        {
            "authors": [
                "Linting Xue",
                "Noah Constant",
                "Adam Roberts",
                "Mihir Kale",
                "Rami Al-Rfou",
                "Aditya Siddhant",
                "Aditya Barua",
                "Colin Raffel."
            ],
            "title": "mT5: A massively multilingual pre-trained text-to-text transformer",
            "venue": "Proceedings of the 2021 Conference of the North American Chap-",
            "year": 2021
        },
        {
            "authors": [
                "Lisa Yankovskaya",
                "Maali Tars",
                "Andre T\u00e4ttar",
                "Mark Fishel."
            ],
            "title": "Machine translation for low-resource Finno-Ugric languages",
            "venue": "Proceedings of the 24th Nordic Conference on Computational Linguistics (NoDaLiDa), pages 762\u2013771, T\u00f3rshavn, Faroe Is-",
            "year": 2023
        },
        {
            "authors": [
                "Jihad Zahir."
            ],
            "title": "Iadd: An integrated arabic dialect identification dataset",
            "venue": "Data in Brief, 40:107777.",
            "year": 2022
        },
        {
            "authors": [
                "Omar F. Zaidan",
                "Chris Callison-Burch."
            ],
            "title": "The Arabic online commentary dataset: an annotated dataset of informal Arabic with high dialectal content",
            "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Hu-",
            "year": 2011
        },
        {
            "authors": [
                "Biao Zhang",
                "Philip Williams",
                "Ivan Titov",
                "Rico Sennrich."
            ],
            "title": "Improving massively multilingual neural machine translation and zero-shot translation",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1628\u2013",
            "year": 2020
        },
        {
            "authors": [
                "Micha\u0142 Ziemski",
                "Marcin Junczys-Dowmunt",
                "Bruno Pouliquen"
            ],
            "title": "The United Nations parallel corpus v1.0",
            "venue": "In Proceedings of the Tenth International Conference on Language Resources and Evaluation",
            "year": 2016
        }
    ],
    "sections": [
        {
            "text": "1 Introduction\nThe NLP community should create technology that covers as many languages as possible, not only medium-resource and high-resource languages. This goal can only be achieved if corpora for low-resource languages are available. Web-mined datasets \u2013 including CC100 (Wenzek et al., 2020), mC4 (Xue et al., 2021) and OSCAR (Abadji et al., 2021; Ortiz Su\u00e1rez et al., 2019) \u2013 have made important contributions to low-resource NLP. In particular, they lay the ground for multilingual neural models like XLM-R (Conneau et al., 2020), mT5 (Xue et al., 2021) and Glot500 (ImaniGooghari et al., 2023). However, existing web-mined datasets have systematic quality issues (Kreutzer et al., 2022) and insufficient coverage of low-resource languages.\nLow-quality datasets cause poor performance for downstream applications. They can also give rise to a misleading perception of progress when coverage of a low-resource language is claimed based on noisy data. NLP for low-resource languages requires high-quality datasets and high-quality datasets require high-quality LID (language identification). For this reason, high-quality LID for low-resource languages is paramount. To address this need, in this paper we present GlotLID-M, a high-quality LID that covers 1665 languages. We use ISO 639-3 to individuate languages.\nWhen expanding the scope of LID from a few hundred to 1665 languages, the problem of granularity becomes severe. In real-world settings, LID needs to support both macrolanguages and their varieties; it also needs to be robust against out-ofmodel cousins (Caswell et al., 2020; Kreutzer et al., 2022). We pay particular attention to this issue.\nWhile low-resource is our main focus, Blevins and Zettlemoyer (2022) point out that low-quality LID also affects high-resource corpora through contamination, resulting in claims of successful crosslingual transfer that are due to unrecognized coverage of low-resource languages. We also address this issue, e.g., we improve English F1 on the \u201cUniversal Declaration of Human Rights\u201d corpus (UDHR) to .85 compared to .43 for OpenLID.\nContributions. (i) We curate GlotLID-C, a comprehensive dataset covering 1665 languages, most of them low-resource, from a diverse set of domains. (ii) We train GlotLID-M on GlotLID-C, an open-source LID covering these 1665 languages. (iii) In our experiments, GlotLID-M outperforms several baselines by more than 12% absolute F1 on UDHR, which we take as the best benchmark for our focus on low-resource languages. (iv) When balancing F1 and false positive rate (FPR), GlotLID-M also outperforms baselines on FLORES-200, which is dominated by high/medium-resource languages.\n2 Requirements for low-resource LID\nMain use case: Corpus creation. Corpus creation and cleaning is the main use case for our low-resource LID because we want to address the need for high-quality corpora for low-resource languages. Line-by-line LID filtering is an effective method for achieving high corpus quality. Reliable LID can eliminate various types of noise (see (Caswell et al., 2020; Kreutzer et al., 2022)) \u2013 including data from other languages and nonlinguistic data \u2013 that is frequent, especially in web-crawled content. By adjusting the confidence threshold, users will have control over the level of quality of the corpora they create.\nBroad coverage of languages, minimize out-ofmodel cousin errors. We strive for as broad a coverage as is possible given available datasets. This has two benefits. First, it reduces \u201cout-of-model cousin\u201d errors (Caswell et al., 2020; Kreutzer et al., 2022), i.e., it reduces the risk that a language not covered is misclassified as a closely related covered language. Second, having LIDs that discriminate many low-resource languages is a pre-requisite for developing NLP technologies for the largest possible number of languages. Yet many existing LIDs only cover a few hundred languages. In this study, we therefore focus on LIDs having a broad coverage, excluding CLD2 (McCandless, 2010), Equilid (Jurgens et al., 2017), Langdetect (Shuyo, 2010) and langid.py (Lui and Baldwin, 2012). These LIDs cover less than 100 languages or are outperformed by the models we compare with.\nOpen-source. LIDs should be open-source to encourage open collaboration and conform to best research practices. Some LIDs that meet our other requirements are not open-source, e.g., those published by Caswell et al. (2020), Bapna et al. (2022) and Kudugunta et al. (2023). CLD3 (Botha et al., 2017; Salcianu et al., 2018) is freely available, but its training code is not open-source.\nEase of use. LIDs should be easily deployable across platforms and programming environments without having to worry about dependencies, compatibility and lack of maintenance.\nBecause of this ease-of-use requirement, we do not consider whatlang (Brown, 2014b,a) nor idNet (Dunn, 2020), two broad-coverage LIDs that meet many other requirements, but are hard to use in many practical scenarios due to software issues and lack of maintenance.\nUncertainty assessment. In our use cases, we\nwould like to rely on uncertainty measures to distinguish cases where the highest-probability language is certain from those where it is not. This would allow us to choose a level of confidence for the resulting corpus. For example, we may want to retain only sentences identified with a high confidence (say, 70%). This is essential to produce high-quality low-resource corpora.\nBecause of this requirement, we do not consider Franc (Wormer, 2014) as a baseline. While it has many desirable properties, it generally does not provide well-calibrated probabilities. It usually returns several classes, giving 1.0 to the top class and values close to 1.0 to several others.\nEfficiency. LID is easy to run in parallel, but we still need an efficient solution to make it applicable to large corpora, not least for ecological reasons.\nLack of efficiency is the reason why we do not use AfroLID (Adebara et al., 2022) as a baseline, despite its excellent coverage of African languages.1 AfroLID is a transformer architecture and less efficient than its competitors.\nGranularity flexibility. When scaling LID from a few hundred languages to more than 1500, it is hardly practical to restrict the set of labels to a single level of the language hierarchy (e.g., using resources like https://iso639-3.sil.org). This is due to the complexity of defining and delimiting languages, including the coexistence of macrolanguages and their varieties. In many cases, we want to keep both the macrolanguage and the varieties in our label set because the varieties we have data for are important languages in their own right. But for other varieties, we do not have varietylabeled data, so the only way to include them is through the macrolanguage. For example, FLORES covers the macrolanguage aka (Akan) and its variety twi (Twi), but not its variety fat (Fanti). Keeping both aka and twi gives flexibility to LID users: they can either differentiate aka and twi or they can consolidate the two labels to the single label aka, depending on what makes more sense in their setting.\n3 Dataset curation\nWe now describe GlotLID-C, a corpus for LID training that covers 1665 languages.\nSource selection. We choose sources that we deem trustworthy (i.e., high chance of correct language label). To address the domain sensitivity of\n1It has no coverage of other low-resource languages.\nLID and broaden language coverage, we curate a diverse set of text domains.\nWe review sources referenced by ImaniGooghari et al. (2023); Burchell et al. (2023); Blaschke et al. (2023); Adebara et al. (2022); Adebara and AbdulMageed (2022). In each case, we consider the collection methodology, selecting sources whose language labels are trustworthy. We generally do not use web-crawled sources to avoid the associated problems (Kreutzer et al., 2022). Most selected sources are derived from Wikipedia, religious texts, collaborative translations, storybooks, and news sites. This gives us a coverage of 1832 languages, more than any other public LID. For a list of data sources, see \u00a7A.\nPreprocessing. We ensure that each sentence is written in the correct script, based on the writing system databases of Kargaran et al. (2023) and van Esch et al. (2022). We use the GlotScript (Kargaran et al., 2023) Python library to determine scripts.2\nWe also eliminate duplicate sentences. Statistics. Our final corpus, GlotLID-C, com-\nprises 289 million sentences (i.e., lines of data) totaling 40GB and spans 1832 languages (identified by their ISO 639-3 code). 1677 languages have more than 1000 sentences. Refer to \u00a7D for the total number of sentences per language.\nTrain/test split. We designate 85% of the data as GlotLID-C train. Let nl be the number of sentences from language l in the remaining 15%. Then we sample min(1000, nl) sentences from it. We refer to the resulting dataset as GlotLID-C test.\nContamination. To make sure our evaluation data (especially UDHR, refer to \u00a75.1) do not overlap with our sources, we compute contamination of UDHR in GlotLID-C train.\nWe count a UDHR test sentence as occurring in the training set if all of its word four-grams occur in one sentence of GlotLID-C. Most of these contaminations are due to two resources: Wikipedia and Tatoeba.3 GlotLID-C train shares 374 languages with UDHR.\nFor 292 languages, we find that none of the UDHR test sentences occurs in the training data. For 57 languages, less than 10% of UDHR test sentences occur in the training data. The remaining 25 languages with a contamination rate over 10% are all high/medium resource languages.\nIn our experiments, we decided against remov-\n2https://github.com/cisnlp/GlotScript 3https://tatoeba.org/en/downloads\ning any sentences from GlotLID-C, as there is little contamination of UDHR for low-resource languages. We follow here most prior work which has the problem of contamination of UDHR for high-resource languages. We will however remove from GlotLID-C train the sentences causing contamination as part of our next release.\n4 GlotLID-M\nWe select FastText (Joulin et al., 2017) as the architecture for GlotLID-M, because it satisfies all requirements outlined in \u00a72 as we will explain now.\nWe train our FastText model GlotLID-M on GlotLID-C train with 1832 languages. FastText can easily handle the large number of languages in the corpus. Because of this broad coverage, outof-model cousin errors are reduced. Although we restrict the number of classes to 1665 for some experiments (e.g., in Table 1), GlotLID-M\u2019s classification always uses all 1832 languages to mitigate out-of-model cousin errors. This satisfies the first requirement from \u00a72: GlotLID-M is a useful tool for corpora creation because it has a broad coverage of languages that can occur in raw data.\nFastText provides an open-source codebase for training, which supports customization and extension of GlotLID-M.\nFastText is easy to use: It offers a number of language bindings, making it compatible with multiple programming languages (including C++, Python, Java, Node.js, Rust, Ruby, R) and reducing dependency, incompatibility and other software issues.\nFastText meets the requirement of uncertainty assessment because it provides confidence scores that can serve as thresholds to effectively mitigate noise in the data. For the same reason, FastText also supports granularity flexibility: we can accumulate probabilities over language varieties to get a good estimate of the probability of the macrolanguage. To this end, we simply add to the macrolanguage probability the probabilities of its varieties. This way, the system can return appropriate estimates at various levels of granularity.\nAs a professionally designed and implemented linear classifier, FastText is efficient: it had the best throughput of the candidate solutions we tested and can process large corpora with high speed. As a linear model, FastText has the additional advantage of delivering explainable classification decisions. FastText is a multinomial logistic classifier. The input sentence is represented as an average of n-gram\nembeddings. This allows us to visualize how much each n-gram contributed to the final prediction. See NLLB Team et al. (2022), Fig. 8, for details.\nTaking all these requirements together (and its good LID performance demonstrated in \u00a76 and acceptable calibration in \u00a7F), GlotLID-M, based on FastText, is, in our opinion, an excellent tool for supporting our use case, the creation of highquality low-resource corpora.\n5 Experimental setup\nWe train GlotLID-M on GlotLID-C train using the hyperparameters in (NLLB Team et al., 2022; Burchell et al., 2023) and otherwise FastText defaults (see \u00a7B). Following Arivazhagan et al. (2019), NLLB Team et al. (2022) and Burchell et al. (2023), we perform up-sampling for low resource languages. Sentences from a language l representing pl of the dataset are sampled proportionally to p 1 T l where T is the temperature. Following NLLB Team et al. (2022) and Burchell et al. (2023), we set 1T = .3.\n5.1 Evaluation data\nWe evaluate GlotLID-M on GlotLID-C test, FLORES-200 (NLLB Team et al., 2022) and UDHR4 (Universal Declaration of Human Rights).\nWhile testing on data unseen in training is standard in NLP, the results have to be taken with a grain of salt because there is often a domain mismatch in real-world applications of LID (Caswell et al., 2020; Dunn, 2020). FLORES-200 and UDHR address this concern: they are not part of our training set (however, see discussion in \u00a73) and do not draw on our sources. Many other benchmarks share sources like Wikipedia with us (Thoma, 2018; Haas and Derczynski, 2021; Ahmadi et al., 2023). FLORES-200 and UDHR are also the benchmarks with the broadest available language coverage.\nFLORES-200 is a collection of 842 articles obtained from English-language Wikimedia projects. Each sentence in the articles was translated into 204 distinct language-script combinations, corresponding to 196 distinct languages, and human-verified. It provides 997 sentences for development, 1012 for dev-test and 992 for test. FLORES-200 test is not publicly available. Following prior work, we use dev-test as our FLORES test set.\n4http://www.unicode.org/udhr/d/\nThe level of granularity across language (sub)families varies in FLORES; e.g., it includes nine varieties of Arabic. On the other hand, some languages (e.g., est:Estonian) are only available as macrolanguage. In some cases, FLORES includes both a macrolanguage and varieties, e.g., aka (Akan) and its variety twi (Twi), and zho (Chinese) and its variety yue (Yue Chinese). Although some issues have been reported (see \u00a7C.1) with FLORES, we do not have the resources to investigate them, so we use it as is.\nUDHR consists of more than 500 translations of the \u201cUniversal Declaration of Human Rights\u201d. 419 translations available from the \u201cUDHR in Unicode\u201d project have a iso-639-3 code that is not \u201cund\u201d (undetermined). We discard short sentences (e.g., consisting of just an article number or the single English word \u2018missing\u2019) by discarding the 35% shortest sentences for each language.\nIn some cases (e.g., Zulu and Quechua), UDHR contains both a macrolanguage and one of its varieties. We have also seen some issues in UDHR (see \u00a7C.2), but we have not extensively investigated these potential problems.\n5.2 Baselines\nOur baselines are FT176,5 CLD3, NLLB (NLLB Team et al., 2022) and OpenLID (Burchell et al., 2023). The first two were used for filtering the resources OSCAR and mC4 (Kreutzer et al., 2022).\nCLD3. CLD3 uses an n-gram (1\u2264n\u22643) based neural network model. CLD3 sometimes deviates from established metadata conventions. For example, ISO-639-1 ku refers to kur (Kurdish), but in CLD3 ku refers to its variety kmr (Northern Kurdish). It refers to Hebrew as iw, but the ISO code for Hebrew has changed to he and heb.\nFT176. FT176 is a FastText model that uses Wikipedia (WP) codes as labels. The documentation of language metadata is sometimes unclear; e.g., FT176 refers to Alemannic German as als although ISO-639-3 als is Tosk Albanian. It refers to the Malay macrolanguage as ms, but unlike ISO639-3, this does not include ind (Indonesian).\nNLLB and OpenLID. NLLB and OpenLID are FastText models. Their language label sets are mostly taken from FLORES, so granularity and coverage are similar to FLORES.\nLanguage metadata matching. Matching the\n5https://fasttext.cc/docs/en/ language-identification.html\nDecision rule Given an LID classifier m, a base set B of languages and a threshold \u03b8, we assign label \u03d5(s,m,B, \u03b8) to sentence s as follows:\n\u03d5(s,m,B, \u03b8) = { undetermined if maxl\u2208B Pm(l|s) < \u03b8 argmaxl\u2208BPm(l|s) otherwise\nWe distinguish two scenarios: SET! and SET?. In scenario SET!, the set of languages covered by the evaluation benchmark is known. We restrict a model\u2019s predictions to those languages that occur in the benchmark. This means that B is a (proper or improper, see table captions for details) subset of the languages occurring in the benchmark. In scenario SET?, the set of languages covered by the evaluation benchmark is not known. We do not restrict a model m\u2019s predictions: the model considers the entire set of languages it was trained on. This means that B is the set of languages that m was trained on.\nGlotLID-M, \u03b8=.0 GlotLID-M, \u03b8=.5\nBenchmark |L| F1\u2191 FPR\u2193 F1\u2191 FPR\u2193\nGlotLID-C all 1832 .940 .0005 .938 .0003 GlotLID-C subset 1665 .977 .0003 .973 .0002 UDHR all 374 .750 .0015 .734 .0007 UDHR subset 342 .784 .0014 .770 .0006 FLORES-200 all 196 .917 .0042 .887 .0013 FLORES-200 subset 177 .957 .0029 .924 .0010\nprobability is) and \u03b8 = .5 (i.e., we only assign a language label if its probability exceeds .5). See Figure 1 for the definition of our decision rule.\nFocusing on the \u201csubset\u201d results for \u03b8 = .5, F1 is .973 on GlotLID-C and .924 on FLORES; and FPR is .0002 on GlotLID-C and .0010 on FLORES. This is a very good performance, in particular for the use case of low-resource corpus creation because low FPR means that the resulting corpora will be less contaminated. On UDHR, again for the \u201csubset\u201d results for \u03b8 = .5, F1 is .770 and FPR .0006. This is again an encouragingly low FPR, but F1 is quite a bit lower than for GlotLID-C and FLORES. The reason is that we have a domain shift (compared to GlotLID-C) and many more languages (compared to FLORES), resulting in lower F1. Although the UDHR results should be improved further, we will now show that they outperform the state of the art.\nTable 2 compares GlotLID-M with four baselines. We consider two evaluation settings (SET? and SET!) and three thresholds \u03b8. The top part of the table (SET?) corresponds to the case where the set of languages in the benchmark is not known, i.e., the LID makes predictions for all languages it was trained on. In contrast, in the SET! setting (bottom part), the set of languages in the benchmark is known, and each LID only makes predictions for those languages. SET? is a more realistic setting, as we usually do not know which languages occur in a corpus that needs to be cleaned.\nFor the SET? setting, GlotLID-M consistently outperforms CLD3 by a large margin. Taking into account that F1 and FPR should be balanced, we also take it to outperform FT176. Even though GlotLID-M\u2019s FPR is slightly higher in some cases, its F1 is better by a large margin, so that it is clearly the better performing system.\nOn UDHR, GlotLID-M also clearly outperforms OpenLID and NLLB for F1 and FPR by large margins. On FLORES, F1 is slightly worse and FPR slightly better compared with OpenLID and NLLB. We point out that this comparison is not entirely fair since OpenLID and NLLB were designed with FLORES in mind. More importantly, our use case is the creation of low-resource corpora for which UDHR is the more appropriate benchmark.\nComparing results for different thresholds, we observe that increasing \u03b8 lowers F1 (because recall is hurt) and lowers FPR (because precision is increased). This suggests that a higher thresh-\nFLORES-200 UDHR CLD3 FT176 OpenLID NLLB CLD3 FT176 OpenLID NLLB |L| = 96 |L| = 108 |L| = 195 |L| = 188 |L| = 100 |L| = 124 |L| = 159 |L| = 172\nLID Model \u03b8 F1\u2191 FPR\u2193 F1\u2191 FPR\u2193 F1\u2191 FPR\u2193 F1\u2191 FPR\u2193 F1\u2191 FPR\u2193 F1\u2191 FPR\u2193 F1\u2191 FPR\u2193 F1\u2191 FPR\u2193\nSE T\n? baselines .0 .753 .0098 .775 .0090 .923 .0051 .947 .0053 .544 .0099 .566 .0079 .645 .0056 .641 .0051 baselines \u03b81 .779 .0081 .816 .0033 .923 .0050 .948 .0051 .576 .0081 .644 .0025 .676 .0046 .677 .0040 baselines \u03b82 .799 .0060 .796 .0021 .923 .0044 .947 .0047 .618 .0060 .647 .0014 .718 .0034 .717 .0030 GlotLID-M .0 .978 .0051 .987 .0042 .916 .0043 .947 .0035 .868 .0033 .868 .0030 .848 .0020 .847 .0019 GlotLID-M .3 .980 .0042 .987 .0037 .898 .0020 .927 .0019 .881 .0028 .879 .0026 .846 .0015 .844 .0015 GlotLID-M .5 .980 .0031 .987 .0029 .886 .0014 .916 .0013 .903 .0023 .890 .0021 .847 .0012 .846 .0011\nSE T ! baselines .0 .952 .0104 .881 .0093 .923 .0051 .950 .0053 .922 .0101 .739 .0081 .881 .0063 .854 .0058 GlotLID-M .0 .983 .0104 .991 .0093 .922 .0051 .954 .0053 .952 .0100 .927 .0081 .926 .0064 .925 .0060\nFLORES-200 UDHR GlotLID-C language FP cl top FP source #FP % language FP cl top FP source #FP % language FP cl top FP source #FP %\nm os\nte rr\nor s arb:St Arabic 3787 .18 ars:Najdi Arabi 829 .22 cmn:Mandarin Ch 596 .38 chr:Cherokee 81 .14 spa:Spanish 1952 .34 pid:Piaroa 156 .08\narz:Egyptian Ar 1726 .32 apc:Levantine A 440 .25 qub:Huallaga Hu 247 .00 qvh:Huamal\u00edes-D 55 .22 eng:English 1168 .46 lir:Liberian En 254 .22 pes:Ir. Persian 1495 .40 prs:Dari 905 .61 fin:Finnish 224 .22 krl:Karelian 138 .62 rus:Russian 1057 .49 chu:Church Slav 661 .63 cmn:Mandarin Ch 1008 .00 yue:Yue Chinese 1008 .99 wuu:Wu Chinese 172 .24 hak:Hakka Chine 44 .26 bho:Bhojpuri 882 .50 bih:Bihari Lgs 854 .97 hin:Hindi 977 .51 awa:Awadhi 693 .71 rus:Russian 157 .28 niv:Gilyak 44 .28 lir:Liberian En 712 .47 din:Dinka 174 .24\nm os\ntn oi\nsy arb:St Arabic 3787 .18 ars:Najdi Arabi 829 .22 evn:Evenki 36 .23 oaa:Orok 19 .53 rus:Russian 1057 .49 chu:Church Slav 661 .63 arz:Egyptian Ar 1726 .32 apc:Levantine A 440 .25 quz:Cusco Quech 82 .40 qxu:Arequipa-La 61 .74 eng:English 1168 .46 lir:Liberian En 254 .22 prs:Dari 338 .24 pbt:S Pashto 310 .92 hrv:Croatian 84 .42 bos:Bosnian 39 .46 spa:Spanish 1952 .34 pid:Piaroa 156 .08 dyu:Dyula 255 .25 bam:Bambara 255 .99 tzm:C Atlas Tam 52 .02 zgh:St Moroccan 52 .99 crq:Iyo\u2019wujwa C 347 .47 crt:Iyojwa\u2019ja C 347 .99 apc:Levantine A 161 .42 ajp:S Levantine 70 .43 uzn:N Uzbek 72 .46 cbu:Candoshi-Sh 16 .22 crt:Iyojwa\u2019ja C 698 .48 crq:Iyo\u2019wujwa C 697 .99\nno po\nsi tiv es tet:Tetum 0 .00 sck:Sadri 0 .00 hsn:Xiang Chine 0 .00 chg:Chagatai 0 .00 abk:Abkhazian 0 .00 liv:Liv 0 .00 vep:Veps 0 .00 gbm:Garhwali 0 .00 niv:Gilyak 0 .00 tmw:Temuan 0 .00\nhi re\nso ur\nce arb:St Arabic 3787 .99 ars:Najdi Arabi 829 .22 cmn:Mandarin Ch 596 .99 chr:Cherokee 81 .14 cmn:Mandarin Ch 367 .99 wuu:Wu Chinese 208 .57 dzo:Dzongkha 10300 .09 bod:Tibetan 10300 .99 fin:Finnish 224 .99 krl:Karelian 138 .62 eng:English 1267 .99 lir:Liberian En 254 .20 hin:Hindi 977 .99 awa:Awadhi 693 .71 hin:Hindi 76 .99 mai:Maithili 24 .32 hin:Hindi 488 .99 bho:Bhojpuri 98 .20 rus:Russian 1 .99 bul:Bulgarian 1 .99 rus:Russian 256 .99 eng:English 100 .39 rus:Russian 1156 .99 chu:Church Slav 661 .57 spa:Spanish 10 .99 ast:Asturian 7 .70 spa:Spanish 62 .99 agr:Aguaruna 20 .32 spa:Spanish 1952 .99 pid:Piaroa 156 .08\ncorpora are noisy, an issue that we will have to address in future work.\nNo positives. Part 3 of Table 3 (\u201cno positives\u201d) gives five random examples from languages for which there was not a single positive classification. There were no such languages for FLORES.\nFor UDHR, we identified two reasons. (i) Performance on GlotLID-C is good, but poor on UDHR. Tetum is an example. The most likely cause is a domain shift or some other big train/test difference. (ii) The training set is too small (less than 30 sentences): hsn (Xiang Chinese), abk (Abkhazian), vep (Veps) and niv (Gilyak) are in this class.\nFor the five GlotLID-C random examples with no positives, the reason is also that the training sets were too small (less than 40 sentences): sck (Sadri), chg (Chagatai), liv (Liv), gbm (Garhwali) and tmw (Temuan). We should have set a higher threshold for minimum size of the training corpus. Note that the number of 1665 languages that we use throughout the paper already reflects this insight. Even though we train on 1832 languages, we claim reasonable performance for only 1665 (Table 1).\nTest set skewed in favor of high-resource. FLORES and UDHR test sets are balanced: highresource and low-resource languages have about the same size. Following this model, we constructed the test set of GlotLID-C in the same way. F1 is independent of this distribution, but FPR and cleanness (\u201ccl\u201d) are strongly dependent on it. The Spanish corpus generated by GlotLID-M on GlotLID-C test has a dismal cleanness of only .34. Is this a problem for GlotLID-M?\nWe believe the answer is no, as the corpora we run LID on will have a distribution skewed in favor of high-resource languages. To simulate this more realistic scenario, the last part of Table 3 (\u201chi resource\u201d) gives five selected languages for each benchmark where we have inflated the subsets for high-resource languages by a factor of 100. For example, instead of a single copy of the English part of FLORES, the test set now contains 100 copies.\nWe see in Table 3 that this results in clean corpora (cl=.99) for each of the fourteen highresource languages shown: Standard Arabic, Hindi, Russian, Spanish (FLORES); Mandarin, Finnish, Hindi, Russian, Spanish (UDHR); Mandarin, English, Hindi, Russian, Spanish (GlotLID-C). As an example, looking at Spanish for GlotLID-C (the first and last lines in the table), the number of false positives (1952) and the number of false positives\ncontributed by the low-resource language Piaroa (156) are the same. But since the size of Spanish is increased 100x, its cleanness improves from .34 for the unrealistic uniform distribution to .99 for the realistic skewed distribution. Thus, as we would expect, LID for high-resource languages is a relatively easy problem and this does not change much if we run a broad-coverage LID like GlotLID-M.\nConversely, LID numbers for low-resource languages can be negatively affected. The Dzongkha corpus generated from FLORES in the uniform setting has 103 false positives and a cleanness of .91 (not shown). In the skewed setting, making Tibetan a high-resource language causes 10,300 false positives from Tibetan to leak into Dzongkha, reducing its cleanness to an unacceptable .09.\nThis discussion suggests that the established evaluation methodology for LID is unsatisfactory. We recommend that future work considers both unifom and skewed test sets to better assess how LID is expected to perform in the real world.\nThis analysis demonstrates how much harder LID becomes when we represent as large and diverse sets of languages as we do. What we have shown is that there is a real danger of creating corpora that are badly contaminated. To address this, we need to develop methodologies and resources that better handle low-resource languages.\nBased on the analysis described in this section we created and open-sourced a much improved version of the UDHR test set for evaluation of LID.6\nAll UDHR results in this paper are based on the version of the UDHR test set descibed in \u00a75.1.\n8 Conclusion\nWe create GlotLID-C, an LID resource that covers 1832 languages, several times more than prior work. We introduce GlotLID-M, an open-source LID that covers 1665 languages with good results. The comparison of GlotLID-M against four LID baselines shows superior performance for the lowresource use case. In future research, we would like to improve quality of our training corpora and add more low-resource languages in to GlotLID. We hope GlotLID will be a valuable resource in creating higher-quality corpora for low-resource languages.\n6https://huggingface.co/datasets/ cis-lmu/udhr-lid\nLimitations\n(1) We publish list of GlotLID-C data sources as part of this work. There is no other LID benchmark available that covers as many languages as GlotLID-C does. GlotLID-C, FLORES and UDHR all have drawbacks as evaluation datasets for LID. An LID trained on GlotLID-C train and tested on GlotLID-C test will often find the same domain in the test set as in the training set. It is well known that this results in overly optimistic evaluation numbers. FLORES and UDHR consist of data that were not originallly produced in each language. Rather, they were translated from high-resource languages. The same is true to a lesser extent for GlotLID-C. Translated language is only an imperfect evaluation benchmark because it can differ greatly from natural language data, i.e., translationese is often not a good model of natural language data.\n(2) Many corpora for the lowest resource languages are derived from religious sources. It should be noted that many Bible translations do not reflect actual language use.\n(3) We do not conduct hyperparameter search and instead use the hyperparameters employed by previous studies. However, conducting such a search can make our findings more robust, considering the difference in the number of languages included in our study compared to the prior work.\n(4) Although we tried our best to select the most suitable LIDs as the baseline. We could not compare against all of the LID models. This includes CLD2 (McCandless, 2010), Equilid (Jurgens et al., 2017), Langdetect (Shuyo, 2010), langid.py (Lui and Baldwin, 2012), whatlang (Brown, 2014b,a), idNet (Dunn, 2020), Franc (Wormer, 2014), AfroLID (Adebara et al., 2022), HeLI-OTS (Jauhiainen et al., 2022), transliterate7, whatthelang8, whatlang-rs9, lingua10, Google/Bing Online, LanideNN (Kocmi and Bojar, 2017), Paasaa11, Q-LID (Ren et al., 2022), UDLDI (Goswami et al., 2020), PALI (Ahmadi et al., 2023), SS-LID (Caswell et al., 2020; Bapna et al., 2022; Kudugunta et al., 2023) and TextCat (Cavnar et al., 1994).\n7https://github.com/barseghyanartur/ transliterate\n8https://github.com/indix/whatthelang 9https://github.com/greyblake/\nwhatlang-rs 10https://github.com/pemistahl/lingua 11https://github.com/minibikini/paasaa\nEthics Statement\nWe here highlight key ethical considerations for GlotLID.\nData. The data used in our study comes from openly available (but not necessarily freely redistributable) datasets, including resources previously published by researchers, publishers, and translators. We ensured that the data collection process complied with licensing of each dataset.\nBias. We recognize potential biases towards higher resource languages. We conducted a comprehensive analysis of errors and evaluated their impact on our results.\nInclusivity. We acknowledge the challenges associated with low-resource languages and have taken steps to include a diverse range of languages in our study.\nEthical Use. We have demonstrated both positive and negative outcomes of applying GlotLID-M as an LID tool. We acknowledge that GlotLID-M has a high error rate for some low-resource languages. This means that there is a potential risk of excluding low-resource languages during the collection and processing of NLP corpora.\nTransparency. We provide detailed descriptions of our methodology, model architecture, and evaluation process. Additionally, we make our research artifacts, including model, code, and list of data sources openly available to foster collaboration and reproducibility.\n9 Acknowledgements\nWe would like to thank anonymous reviewers. This work was funded by the European Research Council (grant #740516).\nReferences Julien Abadji, Pedro Javier Ortiz Su\u00e1rez, Laurent Ro-\nmary, and Beno\u00eet Sagot. 2021. Ungoliant: An optimized pipeline for the generation of a very large-scale multilingual web corpus. Proceedings of the Workshop on Challenges in the Management of Large Corpora (CMLC-9) 2021. Limerick, 12 July 2021 (Online-Event), pages 1 \u2013 9, Mannheim. LeibnizInstitut f\u00fcr Deutsche Sprache.\nKathrein Abu Kwaik, Motaz Saad, Stergios Chatzikyriakidis, and Simon Dobnik. 2018. Shami: A corpus of Levantine Arabic dialects. In Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018), Miyazaki, Japan. European Language Resources Association (ELRA).\nIfe Adebara and Muhammad Abdul-Mageed. 2022. Towards afrocentric NLP for African languages: Where we are and where we can go. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3814\u20133841, Dublin, Ireland. Association for Computational Linguistics.\nIfe Adebara, AbdelRahim Elmadany, Muhammad Abdul-Mageed, and Alcides Inciarte. 2022. AfroLID: A neural language identification tool for African languages. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 1958\u20131981, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\nDavid Ifeoluwa Adelani, Marek Masiak, Israel Abebe Azime, Jesujoba Oluwadara Alabi, Atnafu Lambebo Tonja, Christine Mwase, Odunayo Ogundepo, Bonaventure F. P. Dossou, Akintunde Oladipo, Doreen Nixdorf, Chris Chinenye Emezue, Sana Sabah al azzawi, Blessing K. Sibanda, Davis David, Lolwethu Ndolela, Jonathan Mukiibi, Tunde Oluwaseyi Ajayi, Tatiana Moteu Ngoli, Brian Odhiambo, Abraham Toluwase Owodunni, Nnaemeka C. Obiefuna, Shamsuddeen Hassan Muhammad, Saheed Salahudeen Abdullahi, Mesay Gemeda Yigezu, Tajuddeen Gwadabe, Idris Abdulmumin, Mahlet Taye Bame, Oluwabusayo Olufunke Awoyomi, Iyanuoluwa Shode, Tolulope Anu Adelani, Habiba Abdulganiy Kailani, Abdul-Hakeem Omotayo, Adetola Adeeko, Afolabi Abeeb, Anuoluwapo Aremu, Olanrewaju Samuel, Clemencia Siro, Wangari Kimotho, Onyekachi Raphael Ogbu, Chinedu E. Mbonu, Chiamaka I. Chukwuneke, Samuel Fanijo, Jessica Ojo, Oyinkansola F. Awosan, Tadesse Kebede Guge, Sakayo Toadoum Sari, Pamela Nyatsine, Freedmore Sidume, Oreen Yousuf, Mardiyyah Oduwole, Ussen Kimanuka, Kanda Patrick Tshinu, Thina Diko, Siyanda Nxakama, Abdulmejid Tuni Johar, Sinodos Gebre, Muhidin Mohamed, Shafie Abdi Mohamed, Fuad Mire Hassan, Moges Ahmed Mehamed, Evrard Ngabire, , and Pontus Stenetorp. 2023. Masakhanews: News topic classification for african languages. ArXiv.\nGustavo Aguilar, Sudipta Kar, and Thamar Solorio. 2020. LinCE: A centralized benchmark for linguistic code-switching evaluation. In Proceedings of the Twelfth Language Resources and Evaluation Conference, pages 1803\u20131813, Marseille, France. European Language Resources Association.\nSina Ahmadi, Milind Agarwal, and Antonios Anastasopoulos. 2023. PALI: A language identification benchmark for Perso-Arabic scripts. In Tenth Workshop on NLP for Similar Languages, Varieties and Dialects (VarDial 2023), pages 78\u201390, Dubrovnik, Croatia. Association for Computational Linguistics.\nIsraa Alsarsour, Esraa Mohamed, Reem Suwaileh, and Tamer Elsayed. 2018. DART: A large dataset of dialectal Arabic tweets. In Proceedings of the Eleventh International Conference on Language Resources\nand Evaluation (LREC 2018), Miyazaki, Japan. European Language Resources Association (ELRA).\nRosana Ardila, Megan Branson, Kelly Davis, Michael Kohler, Josh Meyer, Michael Henretty, Reuben Morais, Lindsay Saunders, Francis Tyers, and Gregor Weber. 2020. Common voice: A massivelymultilingual speech corpus. In Proceedings of the Twelfth Language Resources and Evaluation Conference, pages 4218\u20134222, Marseille, France. European Language Resources Association.\nNaveen Arivazhagan, Ankur Bapna, Orhan Firat, Dmitry Lepikhin, Melvin Johnson, Maxim Krikun, Mia Xu Chen, Yuan Cao, George F. Foster, Colin Cherry, Wolfgang Macherey, Zhifeng Chen, and Yonghui Wu. 2019. Massively multilingual neural machine translation in the wild: Findings and challenges. CoRR, abs/1907.05019.\nAnkur Bapna, Isaac Caswell, Julia Kreutzer, Orhan Firat, Daan van Esch, Aditya Siddhant, Mengmeng Niu, Pallavi Baljekar, Xavier Garcia, Wolfgang Macherey, et al. 2022. Building machine translation systems for the next thousand languages. arXiv preprint arXiv:2205.03983.\nLo\u00efc Barrault, Magdalena Biesialska, Ondr\u030cej Bojar, Marta R. Costa-juss\u00e0, Christian Federmann, Yvette Graham, Roman Grundkiewicz, Barry Haddow, Matthias Huck, Eric Joanis, Tom Kocmi, Philipp Koehn, Chi-kiu Lo, Nikola Ljube\u0161ic\u0301, Christof Monz, Makoto Morishita, Masaaki Nagata, Toshiaki Nakazawa, Santanu Pal, Matt Post, and Marcos Zampieri. 2020. Findings of the 2020 conference on machine translation (WMT20). In Proceedings of the Fifth Conference on Machine Translation, pages 1\u201355, Online. Association for Computational Linguistics.\nLo\u00efc Barrault, Ondr\u030cej Bojar, Marta R. Costa-juss\u00e0, Christian Federmann, Mark Fishel, Yvette Graham, Barry Haddow, Matthias Huck, Philipp Koehn, Shervin Malmasi, Christof Monz, Mathias M\u00fcller, Santanu Pal, Matt Post, and Marcos Zampieri. 2019. Findings of the 2019 conference on machine translation (WMT19). In Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1), pages 1\u201361, Florence, Italy. Association for Computational Linguistics.\nVerena Blaschke, Hinrich Schuetze, and Barbara Plank. 2023. A survey of corpora for Germanic lowresource languages and dialects. In Proceedings of the 24th Nordic Conference on Computational Linguistics (NoDaLiDa), pages 392\u2013414, T\u00f3rshavn, Faroe Islands. University of Tartu Library.\nTerra Blevins and Luke Zettlemoyer. 2022. Language contamination helps explains the cross-lingual capabilities of English pretrained models. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 3563\u20133574, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\nOndr\u030cej Bojar, Christian Buck, Chris Callison-Burch, Christian Federmann, Barry Haddow, Philipp Koehn, Christof Monz, Matt Post, Radu Soricut, and Lucia Specia. 2013. Findings of the 2013 Workshop on Statistical Machine Translation. In Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 1\u201344, Sofia, Bulgaria. Association for Computational Linguistics.\nOndr\u030cej Bojar, Christian Buck, Christian Federmann, Barry Haddow, Philipp Koehn, Johannes Leveling, Christof Monz, Pavel Pecina, Matt Post, Herve SaintAmand, Radu Soricut, Lucia Specia, and Ale\u0161 Tamchyna. 2014. Findings of the 2014 workshop on statistical machine translation. In Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 12\u201358, Baltimore, Maryland, USA. Association for Computational Linguistics.\nOndr\u030cej Bojar, Rajen Chatterjee, Christian Federmann, Yvette Graham, Barry Haddow, Shujian Huang, Matthias Huck, Philipp Koehn, Qun Liu, Varvara Logacheva, Christof Monz, Matteo Negri, Matt Post, Raphael Rubino, Lucia Specia, and Marco Turchi. 2017. Findings of the 2017 conference on machine translation (WMT17). In Proceedings of the Second Conference on Machine Translation, pages 169\u2013214, Copenhagen, Denmark. Association for Computational Linguistics.\nOndr\u030cej Bojar, Rajen Chatterjee, Christian Federmann, Yvette Graham, Barry Haddow, Matthias Huck, Antonio Jimeno Yepes, Philipp Koehn, Varvara Logacheva, Christof Monz, Matteo Negri, Aur\u00e9lie N\u00e9v\u00e9ol, Mariana Neves, Martin Popel, Matt Post, Raphael Rubino, Carolina Scarton, Lucia Specia, Marco Turchi, Karin Verspoor, and Marcos Zampieri. 2016. Findings of the 2016 conference on machine translation. In Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers, pages 131\u2013198, Berlin, Germany. Association for Computational Linguistics.\nOndr\u030cej Bojar, Rajen Chatterjee, Christian Federmann, Barry Haddow, Matthias Huck, Chris Hokamp, Philipp Koehn, Varvara Logacheva, Christof Monz, Matteo Negri, Matt Post, Carolina Scarton, Lucia Specia, and Marco Turchi. 2015. Findings of the 2015 workshop on statistical machine translation. In Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 1\u201346, Lisbon, Portugal. Association for Computational Linguistics.\nOndr\u030cej Bojar, Christian Federmann, Mark Fishel, Yvette Graham, Barry Haddow, Matthias Huck, Philipp Koehn, and Christof Monz. 2018. Findings of the 2018 conference on machine translation (WMT18). In Proceedings of the Third Conference on Machine Translation: Shared Task Papers, pages 272\u2013303, Belgium, Brussels. Association for Computational Linguistics.\nJan A. Botha, Emily Pitler, Ji Ma, Anton Bakalov, Alex Salcianu, David Weiss, Ryan McDonald, and Slav Petrov. 2017. Natural language processing with small\nfeed-forward networks. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2879\u20132885, Copenhagen, Denmark. Association for Computational Linguistics.\nHouda Bouamor, Sabit Hassan, and Nizar Habash. 2019. The MADAR shared task on Arabic fine-grained dialect identification. In Proceedings of the Fourth Arabic Natural Language Processing Workshop, pages 199\u2013207, Florence, Italy. Association for Computational Linguistics.\nRalf Brown. 2014a. Language-aware string extractor.\nRalf Brown. 2014b. Non-linear mapping for improved identification of 1300+ languages. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 627\u2013 632, Doha, Qatar. Association for Computational Linguistics.\nRalf D Brown. 2012. Finding and identifying text in 900+ languages. Digital Investigation, 9:S34\u2013S43.\nLaurie Burchell, Alexandra Birch, Nikolay Bogoychev, and Kenneth Heafield. 2023. An open dataset and model for language identification. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 865\u2013879, Toronto, Canada. Association for Computational Linguistics.\nIsaac Caswell, Theresa Breiner, Daan van Esch, and Ankur Bapna. 2020. Language ID in the wild: Unexpected challenges on the path to a thousand-language web text corpus. In Proceedings of the 28th International Conference on Computational Linguistics, pages 6588\u20136608, Barcelona, Spain (Online). International Committee on Computational Linguistics.\nWilliam B Cavnar, John M Trenkle, et al. 1994. N-grambased text categorization. In Proceedings of SDAIR94, 3rd annual symposium on document analysis and information retrieval, volume 161175, page 14. Las Vegas, NV.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm\u00e1n, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. 2020. Unsupervised cross-lingual representation learning at scale. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8440\u2013 8451, Online. Association for Computational Linguistics.\nMorris H DeGroot and Stephen E Fienberg. 1983. The comparison and evaluation of forecasters. Journal of the Royal Statistical Society: Series D (The Statistician), 32(1-2):12\u201322.\nJonathan Dunn. 2020. Mapping languages: The corpus of global language use. Language Resources and Evaluation, 54:999\u20131018.\nMahmoud El-Haj, Paul Rayson, and Mariam Aboelezz. 2018. Arabic dialect identification in the context of bivalency and code-switching. In Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018), Miyazaki, Japan. European Language Resources Association (ELRA).\nDirk Goldhahn, Thomas Eckart, and Uwe Quasthoff. 2012. Building large monolingual dictionaries at the Leipzig corpora collection: From 100 to 200 languages. In Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC\u201912), pages 759\u2013765, Istanbul, Turkey. European Language Resources Association (ELRA).\nSantiago G\u00f3ngora, Nicol\u00e1s Giossa, and Luis Chiruzzo. 2022. Can we use word embeddings for enhancing Guarani-Spanish machine translation? In Proceedings of the Fifth Workshop on the Use of Computational Methods in the Study of Endangered Languages, pages 127\u2013132, Dublin, Ireland. Association for Computational Linguistics.\nKoustava Goswami, Rajdeep Sarkar, Bharathi Raja Chakravarthi, Theodorus Fransen, and John P. McCrae. 2020. Unsupervised deep language and dialect identification for short texts. In Proceedings of the 28th International Conference on Computational Linguistics, pages 1606\u20131617, Barcelona, Spain (Online). International Committee on Computational Linguistics.\nThamme Gowda, Zhao Zhang, Chris Mattmann, and Jonathan May. 2021. Many-to-English machine translation tools, data, and pretrained models. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations, pages 306\u2013316, Online. Association for Computational Linguistics.\nHendrik J Groenewald and Liza du Plooy. 2010. Processing parallel text corpora for three south african language pairs in the autshumato project. AfLaT 2010, page 27.\nRen\u00e9 Haas and Leon Derczynski. 2021. Discriminating between similar Nordic languages. In Proceedings of the Eighth Workshop on NLP for Similar Languages, Varieties and Dialects, pages 67\u201375, Kiyv, Ukraine. Association for Computational Linguistics.\nTahmid Hasan, Abhik Bhattacharjee, Md. Saiful Islam, Kazi Mubasshir, Yuan-Fang Li, Yong-Bin Kang, M. Sohel Rahman, and Rifat Shahriyar. 2021. XLsum: Large-scale multilingual abstractive summarization for 44 languages. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 4693\u20134703, Online. Association for Computational Linguistics.\nRudali Huidrom, Yves Lepage, and Khogendra Khomdram. 2021. EM corpus: a comparable corpus for a less-resourced language pair Manipuri-English. In\nProceedings of the 14th Workshop on Building and Using Comparable Corpora (BUCC 2021), pages 60\u201367, Online (Virtual Mode). INCOMA Ltd.\nAyyoob ImaniGooghari, Peiqin Lin, Amir Hossein Kargaran, Silvia Severini, Masoud Jalili Sabet, Nora Kassner, Chunlan Ma, Helmut Schmid, Andr\u00e9 Martins, Fran\u00e7ois Yvon, and Hinrich Sch\u00fctze. 2023. Glot500: Scaling multilingual corpora and language models to 500 languages. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1082\u2013 1117, Toronto, Canada. Association for Computational Linguistics.\nAbderrahmane Issam and Khalil Mrini. 2022. Goud.ma: a news article dataset for summarization in moroccan darija. In 3rd Workshop on African Natural Language Processing.\nHeidi Jauhiainen, Tommi Jauhiainen, and Krister Linden. 2019a. Wanca in korp: Text corpora for underresourced uralic languages. In Proceedings of the Research data and humanities (RDHUM) 2019 conference. University of Oulu.\nTommi Jauhiainen, Heidi Jauhiainen, and Krister Lind\u00e9n. 2022. HeLI-OTS, off-the-shelf language identifier for text. In Proceedings of the Thirteenth Language Resources and Evaluation Conference, pages 3912\u20133922, Marseille, France. European Language Resources Association.\nTommi Jauhiainen, Marco Lui, Marcos Zampieri, Timothy Baldwin, and Krister Lind\u00e9n. 2019b. Automatic language identification in texts: A survey. Journal of Artificial Intelligence Research, 65:675\u2013782.\nArmand Joulin, Edouard Grave, Piotr Bojanowski, and Tomas Mikolov. 2017. Bag of tricks for efficient text classification. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 427\u2013431, Valencia, Spain. Association for Computational Linguistics.\nDavid Jurgens, Yulia Tsvetkov, and Dan Jurafsky. 2017. Incorporating dialectal variability for socially equitable language identification. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 51\u201357, Vancouver, Canada. Association for Computational Linguistics.\nAmir Hossein Kargaran, Fran\u00e7ois Yvon, and Hinrich Sch\u00fctze. 2023. Glotscript: A resource and tool for low resource writing system identification. arXiv preprint arXiv:2309.13320.\nOmid Kashefi. 2018. Mizan: A large persian-english parallel corpus. arXiv preprint arXiv:1801.02107.\nTom Kocmi and Ondr\u030cej Bojar. 2017. LanideNN: Multilingual language identification on character window. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational\nLinguistics: Volume 1, Long Papers, pages 927\u2013936, Valencia, Spain. Association for Computational Linguistics.\nJulia Kreutzer, Isaac Caswell, Lisa Wang, Ahsan Wahab, Daan van Esch, Nasanbayar Ulzii-Orshikh, Allahsera Tapo, Nishant Subramani, Artem Sokolov, Claytone Sikasote, Monang Setyawan, Supheakmungkol Sarin, Sokhar Samb, Beno\u00eet Sagot, Clara Rivera, Annette Rios, Isabel Papadimitriou, Salomey Osei, Pedro Ortiz Suarez, Iroro Orife, Kelechi Ogueji, Andre Niyongabo Rubungo, Toan Q. Nguyen, Mathias M\u00fcller, Andr\u00e9 M\u00fcller, Shamsuddeen Hassan Muhammad, Nanda Muhammad, Ayanda Mnyakeni, Jamshidbek Mirzakhalov, Tapiwanashe Matangira, Colin Leong, Nze Lawson, Sneha Kudugunta, Yacine Jernite, Mathias Jenny, Orhan Firat, Bonaventure F. P. Dossou, Sakhile Dlamini, Nisansa de Silva, Sakine \u00c7abuk Ball\u0131, Stella Biderman, Alessia Battisti, Ahmed Baruwa, Ankur Bapna, Pallavi Baljekar, Israel Abebe Azime, Ayodele Awokoya, Duygu Ataman, Orevaoghene Ahia, Oghenefego Ahia, Sweta Agrawal, and Mofetoluwa Adeyemi. 2022. Quality at a glance: An audit of web-crawled multilingual datasets. Transactions of the Association for Computational Linguistics, 10:50\u201372.\nSneha Kudugunta, Isaac Caswell, Biao Zhang, Xavier Garcia, Christopher A Choquette-Choo, Katherine Lee, Derrick Xin, Aditya Kusupati, Romi Stella, Ankur Bapna, et al. 2023. Madlad-400: A multilingual and document-level large audited dataset. arXiv preprint arXiv:2309.04662.\nAnoop Kunchukuttan, Pratik Mehta, and Pushpak Bhattacharyya. 2018. The IIT Bombay English-Hindi parallel corpus. In Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018), Miyazaki, Japan. European Language Resources Association (ELRA).\nRichard Lastrucci, Jenalea Rajab, Matimba Shingange, Daniel Njini, and Vukosi Marivate. 2023. Preparing the vuk\u2019uzenzele and ZA-gov-multilingual South African multilingual corpora. In Proceedings of the Fourth workshop on Resources for African Indigenous Languages (RAIL 2023), pages 18\u201325, Dubrovnik, Croatia. Association for Computational Linguistics.\nMarco Lui and Timothy Baldwin. 2012. langid.py: An off-the-shelf language identification tool. In Proceedings of the ACL 2012 System Demonstrations, pages 25\u201330, Jeju Island, Korea. Association for Computational Linguistics.\nKang Kwong Luke and May LY Wong. 2015. The hong kong cantonese corpus: design and uses. Journal of Chinese Linguistics Monograph Series, 1(25):312\u2013 333.\nThomas Mayer and Michael Cysouw. 2014. Creating a massively parallel Bible corpus. In Proceedings of the Ninth International Conference on Language\nResources and Evaluation (LREC\u201914), pages 3158\u2013 3163, Reykjavik, Iceland. European Language Resources Association (ELRA).\nMichael McCandless. 2010. Accuracy and performance of google\u2019s compact language detector. Blog post.\nSalima Medhaffar, Fethi Bougares, Yannick Est\u00e8ve, and Lamia Hadrich-Belguith. 2017. Sentiment analysis of Tunisian dialects: Linguistic ressources and experiments. In Proceedings of the Third Arabic Natural Language Processing Workshop, pages 55\u2013 61, Valencia, Spain. Association for Computational Linguistics.\nKarima Meftouh, Salima Harrat, Salma Jamoussi, Mourad Abbas, and Kamel Smaili. 2015. Machine translation experiments on PADIC: A parallel Arabic DIalect corpus. In Proceedings of the 29th Pacific Asia Conference on Language, Information and Computation, pages 26\u201334, Shanghai, China.\nJamshidbek Mirzakhalov, Anoop Babu, Duygu Ataman, Sherzod Kariev, Francis Tyers, Otabek Abduraufov, Mammad Hajili, Sardana Ivanova, Abror Khaytbaev, Antonio Laverghetta Jr., Bekhzodbek Moydinboyev, Esra Onal, Shaxnoza Pulatova, Ahsan Wahab, Orhan Firat, and Sriram Chellappan. 2021. A large-scale study of machine translation in Turkic languages. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 5876\u20135890, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.\nAlexandru Niculescu-Mizil and Rich Caruana. 2005. Predicting good probabilities with supervised learning. In Proceedings of the 22nd international conference on Machine learning, pages 625\u2013632.\nJoakim Nivre, Marie-Catherine de Marneffe, Filip Ginter, Jan Hajic\u030c, Christopher D. Manning, Sampo Pyysalo, Sebastian Schuster, Francis Tyers, and Daniel Zeman. 2020. Universal Dependencies v2: An evergrowing multilingual treebank collection. In Proceedings of the Twelfth Language Resources and Evaluation Conference, pages 4034\u20134043, Marseille, France. European Language Resources Association.\nNLLB Team, Marta R. Costa-juss\u00e0, James Cross, Onur \u00c7elebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, Anna Sun, Skyler Wang, Guillaume Wenzek, Al Youngblood, Bapi Akula, Loic Barrault, Gabriel Mejia Gonzalez, Prangthip Hansanti, John Hoffman, Semarley Jarrett, Kaushik Ram Sadagopan, Dirk Rowe, Shannon Spruit, Chau Tran, Pierre Andrews, Necip Fazil Ayan, Shruti Bhosale, Sergey Edunov, Angela Fan, Cynthia Gao, Vedanuj Goswami, Francisco Guzm\u00e1n, Philipp Koehn, Alexandre Mourachko, Christophe Ropers, Safiyyah Saleem, Holger Schwenk, and Jeff Wang. 2022. No language left behind: Scaling human-centered machine translation. arXiv preprint arXiv:2207.04672.\nOdunayo Ogundepo, Tajuddeen R. Gwadabe, Clara E. Rivera, Jonathan H. Clark, Sebastian Ruder, David Ifeoluwa Adelani, Bonaventure F. P. Dossou, Abdou Aziz DIOP, Claytone Sikasote, Gilles Hacheme, Happy Buzaaba, Ignatius Ezeani, Rooweither Mabuya, Salomey Osei, Chris Emezue, Albert Njoroge Kahira, Shamsuddeen H. Muhammad, Akintunde Oladipo, Abraham Toluwase Owodunni, Atnafu Lambebo Tonja, Iyanuoluwa Shode, Akari Asai, Tunde Oluwaseyi Ajayi, Clemencia Siro, Steven Arthur, Mofetoluwa Adeyemi, Orevaoghene Ahia, Aremu Anuoluwapo, Oyinkansola Awosan, Chiamaka Chukwuneke, Bernard Opoku, Awokoya Ayodele, Verrah Otiende, Christine Mwase, Boyd Sinkala, Andre Niyongabo Rubungo, Daniel A. Ajisafe, Emeka Felix Onwuegbuzia, Habib Mbow, Emile Niyomutabazi, Eunice Mukonde, Falalu Ibrahim Lawan, Ibrahim Said Ahmad, Jesujoba O. Alabi, Martin Namukombo, Mbonu Chinedu, Mofya Phiri, Neo Putini, Ndumiso Mngoma, Priscilla A. Amuok, Ruqayya Nasir Iro, and Sonia Adhiambo. 2023. Afriqa: Cross-lingual openretrieval question answering for african languages.\nAtul Kr Ojha. 2019. English-bhojpuri smt system: Insights from the karaka model. arXiv preprint arXiv:1905.02239.\nPedro Javier Ortiz Su\u00e1rez, Beno\u00eet Sagot, and Laurent Romary. 2019. Asynchronous pipelines for processing huge corpora on medium to low resource infrastructures. Proceedings of the Workshop on Challenges in the Management of Large Corpora (CMLC7) 2019. Cardiff, 22nd July 2019, pages 9 \u2013 16, Mannheim. Leibniz-Institut f\u00fcr Deutsche Sprache.\nMohammad Taher Pilevar, Heshaam Faili, and Abdol Hamid Pilevar. 2011. Tep: Tehran englishpersian parallel corpus. In International Conference on Intelligent Text Processing and Computational Linguistics, pages 68\u201379. Springer.\nMatt Post, Chris Callison-Burch, and Miles Osborne. 2012. Constructing parallel corpora for six Indian languages via crowdsourcing. In Proceedings of the Seventh Workshop on Statistical Machine Translation, pages 401\u2013409, Montr\u00e9al, Canada. Association for Computational Linguistics.\nYe Qi, Devendra Sachan, Matthieu Felix, Sarguna Padmanabhan, and Graham Neubig. 2018. When and why are pre-trained word embeddings useful for neural machine translation? In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 529\u2013535, New Orleans, Louisiana. Association for Computational Linguistics.\nXingzhang Ren, Baosong Yang, Dayiheng Liu, Haibo Zhang, Xiaoyu Lv, Liang Yao, and Jun Xie. 2022. Effective approaches to neural query language identification. Computational Linguistics, 48(4):887\u2013906.\nRoberts Rozis and Raivis Skadin, \u0161. 2017. Tilde MODEL - multilingual open data for EU languages. In Proceedings of the 21st Nordic Conference on Computational Linguistics, pages 263\u2013265, Gothenburg, Sweden. Association for Computational Linguistics.\nAlex Salcianu, Andy Golding, Anton Bakalov, Chris Alberti, Daniel Andor, David Weiss, Emily Pitler, Greg Coppola, Jason Riesa, Kuzman Ganchev, et al. 2018. Compact language detector v3.\nNakatani Shuyo. 2010. Language detection library for java.\nKathleen Siminyu, Godson Kalipe, Davor Orlic, Jade Abbott, Vukosi Marivate, Sackey Freshia, Prateek Sibal, Bhanu Neupane, David I Adelani, Amelia Taylor, et al. 2021. Ai4d\u2013african language program. arXiv preprint arXiv:2104.02516.\nMartin Thoma. 2018. The wili benchmark dataset for written language identification. arXiv preprint arXiv:1801.07779.\nJ\u00f6rg Tiedemann. 2012. Parallel data, tools and interfaces in OPUS. In Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC\u201912), pages 2214\u20132218, Istanbul, Turkey. European Language Resources Association (ELRA).\nDaan van Esch, Tamar Lucassen, Sebastian Ruder, Isaac Caswell, and Clara Rivera. 2022. Writing system and speaker metadata for 2,800+ language varieties. In Proceedings of the Thirteenth Language Resources and Evaluation Conference, pages 5035\u20135046, Marseille, France. European Language Resources Association.\nGuillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzm\u00e1n, Armand Joulin, and Edouard Grave. 2020. CCNet: Extracting high quality monolingual datasets from web crawl data. In Proceedings of the Twelfth Language Resources and Evaluation Conference, pages 4003\u20134012, Marseille, France. European Language Resources Association.\nTitus Wormer. 2014. Franc library.\nLinting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. 2021. mT5: A massively multilingual pre-trained text-to-text transformer. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 483\u2013498, Online. Association for Computational Linguistics.\nLisa Yankovskaya, Maali Tars, Andre T\u00e4ttar, and Mark Fishel. 2023. Machine translation for low-resource Finno-Ugric languages. In Proceedings of the 24th Nordic Conference on Computational Linguistics (NoDaLiDa), pages 762\u2013771, T\u00f3rshavn, Faroe Islands. University of Tartu Library.\nJihad Zahir. 2022. Iadd: An integrated arabic dialect identification dataset. Data in Brief, 40:107777.\nOmar F. Zaidan and Chris Callison-Burch. 2011. The Arabic online commentary dataset: an annotated dataset of informal Arabic with high dialectal content. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 37\u201341, Portland, Oregon, USA. Association for Computational Linguistics.\nBiao Zhang, Philip Williams, Ivan Titov, and Rico Sennrich. 2020. Improving massively multilingual neural machine translation and zero-shot translation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1628\u2013 1639, Online. Association for Computational Linguistics.\nMicha\u0142 Ziemski, Marcin Junczys-Dowmunt, and Bruno Pouliquen. 2016. The United Nations parallel corpus v1.0. In Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC\u201916), pages 3530\u20133534, Portoro\u017e, Slovenia. European Language Resources Association (ELRA).\nA List of data sources\n\u2022 Wikipedia articles: Wikipedia dumps,12 WiLI-2018 (Thoma, 2018), Leipzig corporawikipedia split (Goldhahn et al., 2012)\n\u2022 News: BBC News (Hasan et al., 2021), Global Voices (Tiedemann, 2012), Leipzig corpora-news split (Goldhahn et al., 2012), SETIMES (Tiedemann, 2012)\n\u2022 Translation: NLLB Seed (NLLB Team et al., 2022)\n\u2022 Religious: PBC (Mayer and Cysouw, 2014), Jehovah\u2019s Witnesses,13 1000Langs14\n\u2022 Crowdsourcing: Tatoeba15 \u2022 Multiple domain: MT-560 (Gowda et al.,\n2021; Tiedemann, 2012; Burchell et al., 2023; Post et al., 2012; Ziemski et al., 2016; Rozis and Skadin, \u0161, 2017; Kunchukuttan et al., 2018; Qi et al., 2018; Zhang et al., 2020; Bojar et al., 2013, 2014, 2015, 2016, 2017, 2018; Barrault et al., 2019, 2020), LTI (Brown, 2012), Arabic (Zahir, 2022; Alsarsour et al., 2018; Abu Kwaik et al., 2018; Medhaffar et al., 2017; Meftouh et al., 2015; Zaidan and Callison-Burch, 2011; El-Haj et al., 2018; Bouamor et al., 2019), Persian (Pilevar et al., 2011; Kashefi, 2018), Turkic (Mirzakhalov et al., 2021), Bhojpuri (Ojha, 2019), Cantonese (Luke and Wong, 2015), Guaran\u00ed (G\u00f3ngora et al., 2022), Manipuri (Huidrom et al., 2021)\n\u2022 Government domain: Autshumato (Groenewald and du Plooy, 2010)\nWe also introduce additional data sources suited for LID, but they are not included in the training of the version of GlotLID-M discussed in the paper:\n\u2022 Crowdsourcing: CommonVoice v11 (Ardila et al., 2020)\n\u2022 Web: Wanca 2016 (Jauhiainen et al., 2019a) \u2022 News: GlotSparse16 which is a collec-\ntion of news websites in low-resource languages, MasakhaNEWS (Adelani et al., 2023), Goud.ma (Issam and Mrini, 2022), AI4D Siminyu et al. (2021), Radio Ramogi,17\n12https://dumps.wikimedia.org/ 13https://www.jw.org/ 14https://github.com/ehsanasgari/\n1000Langs 15https://tatoeba.org/en/downloads 16https://github.com/cisnlp/GlotSparse 17https://github.com/Pogayo/\nLuo-News-Dataset\nsmugri (Yankovskaya et al., 2023), finnougric (Yankovskaya et al., 2023)\n\u2022 Trasnlation: GlotStoryBook18 which is a collection of children storybooks in 174 languages from Global Storybooks19, AfriQA Ogundepo et al. (2023), smugriflores (Yankovskaya et al., 2023)\n\u2022 Multiple domain: Universal Dependencies v2.12 (Nivre et al., 2020), Abkhaz National Corpus20\n\u2022 Lyrics: lyricstranslate21 \u2022 Government domain: Vuk\u2019uzenzele (Las-\ntrucci et al., 2023)\nSpecifically, GlotSparse16 and GlotStoryBook18\nare two corpora that compiled as a side of this project to include more languages and domains for LID.\nB GlotLID-M hyperparameters\nWe provide the hyperparameters used to train the GlotLID-M in Table 4.\nC Evaluation data issues\nC.1 FLORES-200\nThere are some mistakes in the FLORES-200 dataset which have been raised by the community.\nFor example, in a GitHub issue,22 it is pointed out that yue_Hant and zho_Hant should actually be very easy to distinguish from each other, and the Cantonese (Yue Chinese, yue_Hant) data in FLORES-200 is completely wrong.\n18https://github.com/cisnlp/ GlotStoryBook\n19https://github.com/global-asp/ 20https://clarino.uib.no/abnc/page 21https://lyricstranslate.com/ 22https://github.com/facebookresearch/\nflores/issues/61\nIn another issue23, it is mentioned that the Central Atlas Tamazight (tzm) is actually in Standard Moroccan Tamazight (zgh), as confirmed by a native speaker of Central Atlas Tamazight.\nC.2 UDHR\nThere are some mistakes with UDHR. For example, both ckb and kmr files are the same. ckb is known for the Arabic script, although it can also be written in Latin. There are also some files that the writing system is not in popular use (based on Kargaran et al. (2023) metadata):\n\u2022 ckb_Latn (Arabic script is in use.) \u2022 azb_Latn (Arabic script is in use.) \u2022 khk_Mong (Cyrillic script is in use.) \u2022 vie_Hani (Latin script is in use.)\nD Performance of GlotLID-M per language\nThe list of languages used to train GlotLID-M, along with the corresponding amount of available data and detailed results for each language, can be found in Tables 5-29\nE Language metadata matching\nThe per-language comparison between GlotLID-M and the baselines (CLD3, FT176, OpenLID, and NLLB) for each benchmark in scenario is as follows:\nFLORES-200. (i) GlotLID-M vs CLD3: Tables 30-31 (ii) GlotLID-M vs FT176: Tables 32- 33 (iii) GlotLID-M vs OpenLID: Tables 34-35 (iv) GlotLID-M vs NLLB: Tables 36-37\nUDHR. (i) GlotLID-M vs CLD3: Tables 38-41 (ii) GlotLID-M vs FT176: Tables 42-45 (iii) GlotLID-M vs OpenLID: Tables 46-47 (iv) GlotLID-M vs NLLB: Tables 48-49\nThe underlined results in each table show the best result for each model, and the bold result indicates the overall best result.\nThe tables also contain the metadata matching rules we define. Column \u201cisocode639-3\u201d contains the ISO 639-3 code of each language. This corresponds to the class used by GlotLID-M (since all our classes are ISO 639-3 codes). The following columns contain the codes that we mapped the ISO\n23https://github.com/facebookresearch/ flores/issues/63\n639-3 codes to. For example, Table 30 indicates that we map ISO 639-3 code fas to pes/prs in FLORES. In other words, to evaluate our performance for the language fas in FLORES, we (only conceptually) create a new test set in which all sentences labeled as pes or prs in FLORES, are relabeled as fas.\nF Calibration\nAs stated in \u00a72, an LID model should provide a calibrated confidence measure in addition to its prediction. Reliability diagrams illustrate model calibration (DeGroot and Fienberg, 1983; Niculescu-Mizil and Caruana, 2005). These diagrams use expected sample accuracy as a function of confidence. If the model is perfectly calibrated, then the diagram plots the identity function.\nWe provide the reliability diagram for GlotLID-M on GlotLID-C test in Figure 2. For GlotLID-C test, the plot is nearly close to the identity function. However, for some of the low confidence scores, it\u2019s not calibrated. This mostly happens because we included so many languages in our models, and some of these languages are very similar to each other or have small training sizes.\nGlotLID-C test\nG Analysis of Baseline Results\nIn this section, we analyze baseline results summarized in Table 2. For a detailed breakdown of results for each language in scenario SET?, see \u00a7E.\nCLD3: The largest performance gap between SET? and SET! is for CLD3. This could be attributed to the different architecture of CLD3, which performs better when the set is known. Also, since CLD3 supports fewer languages compared\nto other models, it has an advantage in terms of supporting a higher percentage of high-resource languages within its base set.\nCLD3 achieves a lower F1 score than the other FastText-based models in the UDHR benchmark in scenario SET?. However, in scenario SET! for the UDHR, it outperforms all other FastText-based models on F1. Additionally, the good performance achieved in both benchmarks (.952 and .922) in scenario SET! illustrates the robustness of this LID.\nFT176. When comparing FT176 and GlotLID-M in FLORES-200, GlotLID-M achieves the highest F1 scores in FLORES. This may be attributed to the fact that all languages in FT176 are supported by Wikipedia, and GlotLID-M has strong support for these languages. On the other hand, FT176 has the worst overall performance among the FastText models.\nNLLB and OpenLID. In FLORES-200, OpenLID and NLLB have an advantage in scenario SET?, as this scenario closely aligns with SET!. Both models provide near-complete support for the languages available in FLORES-200. For the rest of the models and benchmarks, GlotLID-M displays a marked difference in performance and takes the lead. Among the baseline models in scenario SET?, OpenLID shows the best performance in both benchmarks. We will now investigate which languages OpenLID performs better or worse in compared to GlotLID-M.\nIn scenario SET?, when comparing OpenLID and GlotLID-M on FLORES-200, most of the time the per language scores are very close to each other (see Tables 34- 35). However, there are cases where OpenLID performs noticeably better, for example, with a .39 improvement for azb and .29 for awa. On the other hand, GlotLID-M performs better by .2 for zho, which can be attributed to the poorer performance of OpenLID in zho_Hant. Additionally, both models perform poor on languages such as Yue Chinese, which could be attributed to an issue with FLORES-200 (see \u00a7C.1). However, this situation is quite different for UDHR, as GlotLID-M supports more languages than OpenLID, GlotLID-M performs much better in handling languages that are outside the intersection of both models\u2019 base sets.\nwith confidence threshold \u03b8\nGlotLID-M CLD3 GlotLID-M \u03b8=.3 GlotLID-M \u03b8=.5 CLD3 \u03b8=.5 CLD3 \u03b8=.7\niso639-3 FLORES Code(s) CLD3 Code(s) F1\u2191 FPR\u2193 F1\u2191 FPR\u2193 F1\u2191 FPR\u2193 F1\u2191 FPR\u2193 F1\u2191 FPR\u2193 F1\u2191 FPR\u2193\nace ace - 0.95757 0.01099 0.95732 0.00984 0.95689 0.00788 afr afr af 1.0 0.0 0.86863 0.00308 1.0 0.0 1.0 0.0 0.88042 0.00275 0.91091 0.00188 aka aka/twi - 0.99876 0.00012 0.99901 0.0 0.99901 0.0 amh amh am 0.99951 0.00012 0.66579 0.01046 0.99951 0.00011 0.99951 0.0001 0.66579 0.01043 0.66579 0.0102 ara arz/ars/acm/ary/aeb/acq/apc/arb/ajp ar 0.93244 0.06061 0.88458 0.0137 0.92725 0.0542 0.92631 0.04535 0.89253 0.01176 0.90924 0.00751 asm asm - 1.0 0.0 1.0 0.0 1.0 0.0 ast ast - 0.99209 0.00099 0.99308 0.00066 0.99257 0.00049 awa awa - 0.38951 0.00012 0.38982 0.0 0.35313 0.0 ayr ayr - 0.99556 0.00086 0.96738 0.00011 0.93193 0.0 aze azb/azj az 0.71808 0.00049 0.45528 0.01446 0.71546 0.00033 0.69627 0.00019 0.48299 0.0118 0.53112 0.00768 bak bak - 1.0 0.0 1.0 0.0 1.0 0.0 bam bam - 0.52563 0.11122 0.52664 0.0991 0.53084 0.08467 ban ban - 0.97521 0.00012 0.97571 0.0 0.97467 0.0 bel bel be 1.0 0.0 0.98827 0.00024 1.0 0.0 1.0 0.0 0.98972 0.00021 0.99312 0.00013 bem bem - 0.9906 0.00099 0.99256 0.00044 0.99256 0.00039 ben ben bn 0.99852 0.00012 0.5005 0.02059 0.99852 0.00011 0.99852 0.0001 0.5005 0.02053 0.5005 0.02006 bho bho - 0.94329 0.00963 0.94374 0.00852 0.94846 0.00477 bod bod - 0.94589 0.00012 0.94589 0.00011 0.94589 0.0001 bos bos bs 0.58206 0.00679 0.53913 0.00518 0.57353 0.00608 0.49605 0.00331 0.55291 0.00431 0.00197 1e-05 bug bug - 0.99802 0.00012 0.99703 0.0 0.99404 0.0 bul bul bg-Latn/bg 0.99951 0.0 0.93506 0.00141 0.99951 0.0 0.99951 0.0 0.96413 0.00073 0.98051 0.00034 cat cat ca 1.0 0.0 0.54988 0.017 1.0 0.0 1.0 0.0 0.56949 0.01565 0.60701 0.01305 ceb ceb ceb 0.99503 0.0 0.65466 0.01081 0.99454 0.0 0.99404 0.0 0.67137 0.00999 0.68434 0.00908 ces ces cs 0.99951 0.00012 0.89569 0.0024 0.99951 0.00011 0.99901 0.0001 0.92435 0.00166 0.95071 0.00096 cjk cjk - 0.84493 0.00012 0.83429 0.00011 0.79834 0.0001 ckb ckb - 1.0 0.0 1.0 0.0 1.0 0.0 cos - co 0.00037 0.02364 0.00033 0.00019 0.02132 0.0177 crh crh - 0.98851 0.0 0.988 0.0 0.988 0.0 cym cym cy 0.99951 0.00012 0.84123 0.00396 0.99951 0.00011 1.0 0.0 0.89947 0.00232 0.93828 0.00133 dan dan da 0.99357 0.00074 0.93422 0.00118 0.99554 0.00022 0.99554 0.00019 0.95082 0.00079 0.96071 0.00046 deu deu de 0.99901 0.0 0.98011 0.0004 0.99901 0.0 0.99852 0.0 0.98826 0.00023 0.99016 0.00014 dik dik - 0.99653 0.00012 0.99653 0.0 0.99454 0.0 dyu dyu - 0.12435 0.03148 0.11878 0.0282 0.11186 0.02472 dzo dzo - 0.9496 0.01271 0.9491 0.01139 0.9491 0.01002 ell ell el-Latn/el 1.0 0.0 0.90811 0.00207 1.0 0.0 1.0 0.0 0.94382 0.0012 0.96691 0.00066 eng eng en 0.9878 0.00309 0.95943 0.00081 0.99215 0.00166 0.99556 0.00058 0.97473 0.00044 0.98132 0.00024 epo epo eo 0.99901 0.00025 0.91773 0.00178 0.99951 0.00011 1.0 0.0 0.95121 0.00098 0.97423 0.00043 est est et 0.99852 0.00037 0.92477 0.00166 0.99951 0.00011 0.99951 0.0001 0.95905 0.00084 0.9781 0.00038 eus eus eu 0.99951 0.0 0.86818 0.00317 0.99951 0.0 0.99951 0.0 0.93137 0.00153 0.96099 0.00081 ewe ewe - 1.0 0.0 1.0 0.0 1.0 0.0 fao fao - 0.99951 0.0 0.99951 0.0 0.99951 0.0 fas pes/prs fa 0.79937 0.12542 0.62121 0.02544 0.80032 0.11171 0.82343 0.08448 0.64803 0.02256 0.71721 0.01595 fij fij - 0.99901 0.0 0.99901 0.0 0.99901 0.0 fil - fil 0.01989 0.01719 0.01419 fin fin fi 0.99951 0.00012 0.92449 0.00169 1.0 0.0 1.0 0.0 0.95825 0.00089 0.97817 0.00041 fon fon - 0.99752 0.0 0.99752 0.0 0.99703 0.0 fra fra fr 0.99951 0.00012 0.82909 0.00428 0.99951 0.00011 0.99852 0.0001 0.85641 0.00345 0.89511 0.00233 fry - fy 0.00228 0.00168 0.00099 fur fur - 0.99951 0.00012 0.99951 0.00011 1.0 0.0 fuv fuv - 0.96738 0.00012 0.96099 0.0 0.94693 0.0 gaz gaz - 0.99312 0.0016 0.90232 0.00055 0.78878 0.00029 gla gla gd 0.99951 0.00012 0.82382 0.00446 1.0 0.0 1.0 0.0 0.86919 0.00312 0.91561 0.00185 gle gle ga 1.0 0.0 0.92696 0.00162 1.0 0.0 1.0 0.0 0.95234 0.00101 0.97961 0.00039 glg glg gl 0.99703 0.00025 0.76147 0.0063 0.99703 0.00022 0.99703 0.0001 0.79904 0.00501 0.84291 0.0035 grn grn - 1.0 0.0 1.0 0.0 1.0 0.0 guj guj gu 1.0 0.0 0.99703 0.0 1.0 0.0 1.0 0.0 0.99703 0.0 0.99703 0.0 hat hat ht 0.99852 0.00037 0.63645 0.01196 0.99852 0.00033 0.99901 0.00019 0.70798 0.00861 0.78066 0.0057 hau hau ha 0.95924 0.01062 0.44685 0.02592 0.98348 0.00376 0.99313 0.00136 0.53795 0.0179 0.64867 0.01101 haw - haw 0.01547 0.01097 0.00655 heb heb iw 1.0 0.0 0.99555 4e-05 1.0 0.0 1.0 0.0 0.99555 4e-05 0.99604 3e-05 hin hin hi-Latn/hi 0.67624 0.11961 0.21078 0.07843 0.67692 0.10685 0.69697 0.08564 0.22372 0.07248 0.25377 0.05996 hmn - hmn 0.00069 0.00029 0.00014 hne hne - 0.90296 0.00555 0.90343 0.00487 0.898 0.0035 hrv hrv hr 0.75157 0.0711 0.56313 0.0049 0.75573 0.06216 0.768 0.04389 0.56847 0.00456 0.00394 0.0 hun hun hu 1.0 0.0 0.71804 0.00822 1.0 0.0 0.99951 0.0 0.79889 0.00525 0.88105 0.00275 hye hye hy 1.0 0.0 0.99703 0.0 1.0 0.0 1.0 0.0 0.99703 0.0 0.99703 0.0 ibo ibo ig 0.99951 0.00012 0.66843 0.01036 1.0 0.0 1.0 0.0 0.75769 0.00665 0.85581 0.0034 ilo ilo - 0.99951 0.00012 0.99951 0.00011 0.99951 0.0001 isl isl is 0.99901 0.00025 0.62928 0.01231 0.99951 0.00011 0.99951 0.0001 0.64432 0.01148 0.6569 0.01061 ita ita it 0.99803 0.00037 0.53638 0.01807 0.99852 0.00022 0.99901 0.0001 0.56196 0.01622 0.61908 0.01244 jav jav jv 0.98442 0.00383 0.57216 0.0156 0.99166 0.00177 0.99213 0.00127 0.62126 0.01266 0.67272 0.00972 jpn jpn ja-Latn/ja 1.0 0.0 0.67627 0.00998 1.0 0.0 1.0 0.0 0.76672 0.00631 0.854 0.00345 kab kab - 0.86127 0.02605 0.87886 0.01858 0.90909 0.00954 kac kac - 1.0 0.0 1.0 0.0 1.0 0.0 kam kam - 0.91658 0.00012 0.91183 0.0 0.87368 0.0 kan kan kn 1.0 0.0 0.99153 0.0 1.0 0.0 1.0 0.0 0.99153 0.0 0.99153 0.0 kas kas - 0.97649 0.0 0.97597 0.0 0.96945 0.0 kat kat ka 1.0 0.0 0.99354 0.0 1.0 0.0 1.0 0.0 0.99354 0.0 0.99354 0.0 kaz kaz kk 0.99951 0.0 0.71835 0.00819 0.99951 0.0 0.99951 0.0 0.72531 0.00788 0.75968 0.00643 kbp kbp - 0.99901 0.00012 0.99901 0.00011 0.99901 0.0001 kea kea - 0.95238 0.0 0.9513 0.0 0.93586 0.0 khm khm km 0.99951 0.0 0.99404 0.0 0.99951 0.0 0.99951 0.0 0.99404 0.0 0.99404 0.0 kik kik - 0.96562 0.00876 0.96509 0.00774 0.96456 0.00672 kin kin - 0.91471 0.00074 0.91471 0.00066 0.91471 0.00058 kir kir ky 1.0 0.0 0.63286 0.01214 1.0 0.0 1.0 0.0 0.63803 0.01182 0.67267 0.00987 kmb kmb - 0.96321 0.00901 0.96923 0.00664 0.97713 0.0038 kmr kmr ku (Latn) 0.99901 0.0 0.51859 0.01944 0.99901 0.0 0.99901 0.0 0.60922 0.01339 0.72318 0.00781 knc knc - 0.86459 0.00272 0.86869 0.00055 0.86966 0.0001 kon kon - 0.99359 0.00111 0.99408 0.00088 0.99703 0.00019 kor kor ko 1.0 0.0 0.98657 7e-05 1.0 0.0 1.0 0.0 0.98805 4e-05 0.98853 2e-05 lao lao lo 1.0 0.0 0.97726 0.0 0.99901 0.0 0.99802 0.0 0.97726 0.0 0.97726 0.0 lat - la 0.00042 0.00028 0.00013\nTable 30: Comparison of GlotLID vs CLD3 on FLORES-200 benchmark (part 1)\nwith confidence threshold \u03b8\nGlotLID-M CLD3 GlotLID-M \u03b8=.3 GlotLID-M \u03b8=.5 CLD3 \u03b8=.5 CLD3 \u03b8=.7\niso639-3 FLORES Code(s) CLD3 Code(s) F1\u2191 FPR\u2193 F1\u2191 FPR\u2193 F1\u2191 FPR\u2193 F1\u2191 FPR\u2193 F1\u2191 FPR\u2193 F1\u2191 FPR\u2193\nlav lvs/ltg lv 0.99951 0.00012 0.89202 0.00348 0.99975 0.0 0.99951 0.0 0.91429 0.00225 0.92211 0.00122 lij lij - 0.99901 0.00012 0.99852 0.00011 0.99753 0.0001 lim lim - 0.99253 0.0 0.99253 0.0 0.99153 0.0 lin lin - 0.99901 0.00025 0.99852 0.00022 0.99901 0.0001 lit lit lt 0.99951 0.0 0.78767 0.00561 0.99951 0.0 0.99951 0.0 0.84719 0.00373 0.91057 0.00196 lmo lmo - 0.99554 0.00025 0.99554 0.00022 0.99504 0.0001 ltz ltz lb 0.99951 0.0 0.93872 0.00136 0.99901 0.0 0.99901 0.0 0.96011 0.00086 0.97821 0.00043 lua lua - 0.99653 0.00012 0.99553 0.0 0.99404 0.0 lug lug - 0.99603 0.0 0.99603 0.0 0.99454 0.0 luo luo - 1.0 0.0 1.0 0.0 0.99951 0.0 lus lus - 0.99653 0.00012 0.99703 0.0 0.99653 0.0 mag mag - 0.95459 0.00296 0.95507 0.00254 0.95408 0.00127 mai mai - 0.97366 0.00012 0.97366 0.00011 0.97102 0.0 mal mal ml 1.0 0.0 0.99653 0.0 1.0 0.0 1.0 0.0 0.99653 0.0 0.99653 0.0 mar mar mr 1.0 0.0 0.47801 0.02287 1.0 0.0 1.0 0.0 0.50855 0.02018 0.59325 0.01397 mkd mkd mk 1.0 0.0 0.99407 6e-05 1.0 0.0 1.0 0.0 0.99357 5e-05 0.99305 2e-05 mlg plt mg 0.99951 0.00012 0.89399 0.00249 0.99951 0.00011 0.99951 0.0001 0.94757 0.00116 0.97352 0.00055 mlt mlt mt 0.97731 0.0058 0.55065 0.01708 0.99216 0.00177 0.99803 0.00039 0.62752 0.01237 0.72662 0.00765 mni mni - 0.99901 0.00012 0.99901 0.00011 0.99901 0.0001 mon khk mn 1.0 0.0 0.99508 0.0001 1.0 0.0 0.99951 0.0 0.99704 6e-05 0.99901 2e-05 mos mos - 0.98138 0.0 0.97415 0.0 0.96418 0.0 mri mri mi 0.99901 0.00012 0.60242 0.01228 0.99951 0.0 0.99901 0.0 0.67426 0.00878 0.7575 0.00525 msa min/ind/bjn/zsm ms/id 0.97122 0.02271 0.64861 0.01076 0.97515 0.01482 0.97478 0.01051 0.64377 0.00722 0.54044 0.00419 mya mya my 1.0 0.0 0.67006 0.00983 1.0 0.0 1.0 0.0 0.67006 0.0098 0.67006 0.00958 nep npi ne 0.99852 0.00025 0.34321 0.03993 0.99852 0.00022 0.99951 0.0 0.36071 0.03687 0.41741 0.02834 nld nld nl 0.99803 0.00049 0.77945 0.00588 0.99901 0.00022 0.99901 0.0001 0.78535 0.00565 0.80854 0.00471 nor nno/nob no 0.99729 0.00111 0.95306 0.0011 0.99753 0.00088 0.99778 0.00068 0.96076 0.0007 0.95885 0.00038 nso nso - 0.99704 0.00062 0.99704 0.00055 0.99655 0.00049 nus nus - 0.99951 0.0 0.99951 0.0 0.99951 0.0 nya nya ny 0.99753 0.00049 0.37034 0.03552 0.99803 0.00033 0.99852 0.0001 0.41506 0.02935 0.50325 0.01996 oci oci - 0.99951 0.00012 1.0 0.0 0.99951 0.0 ory ory - 1.0 0.0 0.80519 0.0 0.66314 0.0 pag pag - 0.99852 0.0 0.99852 0.0 0.99852 0.0 pan pan pa 1.0 0.0 0.99553 0.0 1.0 0.0 1.0 0.0 0.99553 0.0 0.99553 0.0 pap pap - 0.99069 0.00222 0.99118 0.00188 0.99557 0.00078 pol pol pl 0.9907 0.00235 0.585 0.01483 0.99167 0.00188 0.99167 0.00165 0.61774 0.01289 0.64081 0.01137 por por pt 0.99655 0.00086 0.77611 0.00597 0.99704 0.00066 0.99704 0.00049 0.81641 0.0046 0.86528 0.00305 pus pbt ps 0.88805 0.00198 0.66315 0.01051 0.88853 0.00166 0.89142 0.00088 0.70238 0.00869 0.80064 0.00494 quy quy - 0.75676 0.0 0.625 0.0 0.57163 0.0 ron ron ro 0.99951 0.0 0.82339 0.00442 0.99951 0.0 0.99951 0.0 0.87289 0.00297 0.91857 0.00172 run run - 0.92541 0.01913 0.92584 0.01703 0.92627 0.01489 rus rus ru-Latn/ru 0.99901 0.00012 0.94255 0.00124 0.99901 0.00011 0.99901 0.0001 0.96459 0.00072 0.97521 0.00042 sag sag - 0.99901 0.0 0.99901 0.0 0.99901 0.0 san san - 0.99104 0.00012 0.99104 0.00011 0.99103 0.0 sat sat - 1.0 0.0 1.0 0.0 1.0 0.0 scn scn - 0.99802 0.00012 0.99802 0.00011 0.99852 0.0 shn shn - 1.0 0.0 1.0 0.0 1.0 0.0 sin sin si 1.0 0.0 0.99354 0.0 1.0 0.0 1.0 0.0 0.99354 0.0 0.99354 0.0 slk slk sk 0.99852 0.00012 0.75414 0.00668 0.99852 0.00011 0.99901 0.0 0.81083 0.00474 0.87511 0.00279 slv slv sl 0.99508 0.00123 0.97107 0.00057 0.99655 0.00077 0.99951 0.0001 0.97955 0.00037 0.98528 0.00022 smo smo sm 0.99603 0.0 0.90934 0.00198 0.99603 0.0 0.99603 0.0 0.944 0.00114 0.97187 0.00048 sna sna sn 0.99951 0.00012 0.61449 0.01309 0.99951 0.00011 1.0 0.0 0.66955 0.01024 0.75336 0.00662 snd snd sd 0.99606 0.00099 0.47115 0.02287 0.99606 0.00088 0.99704 0.00058 0.49206 0.02092 0.57735 0.01439 som som so 0.97028 0.00765 0.52313 0.01912 0.98973 0.00232 0.99803 0.00039 0.57094 0.01571 0.61892 0.01256 sot sot st 1.0 0.0 0.5043 0.02015 1.0 0.0 1.0 0.0 0.52382 0.01851 0.54545 0.01644 spa spa es 0.99508 0.00123 0.63533 0.01193 0.99508 0.00111 0.99508 0.00088 0.64905 0.0112 0.67561 0.00971 sqi als sq 0.99951 0.00012 0.79984 0.00523 1.0 0.0 1.0 0.0 0.88762 0.00263 0.94382 0.00117 srd srd - 0.99951 0.0 0.99901 0.0 0.99901 0.0 srp srp sp 0.99901 0.00025 0.99753 2e-05 0.99951 0.00011 1.0 0.0 0.99802 1e-05 0.99653 1e-05 ssw ssw - 0.99455 0.00037 0.99455 0.00033 0.99455 0.00029 sun sun su 0.9906 0.00099 0.45112 0.02448 0.99306 0.00033 0.99304 0.0001 0.4952 0.02034 0.55461 0.01544 swa swh sw 0.96611 0.00876 0.26502 0.05802 0.97542 0.00564 0.98635 0.00273 0.29567 0.04969 0.35717 0.03665 swe swe sv 0.99803 0.00049 0.96728 0.00063 0.99951 0.00011 1.0 0.0 0.97431 0.00048 0.97991 0.00029 szl szl - 0.99104 0.00012 0.99104 0.00011 0.99104 0.0001 tam tam ta 1.0 0.0 0.99303 0.0 1.0 0.0 1.0 0.0 0.99303 0.0 0.99303 0.0 taq taq - 0.80906 0.04234 0.83861 0.02168 0.84449 0.00876 tat tat - 1.0 0.0 1.0 0.0 1.0 0.0 tel tel te 1.0 0.0 0.97881 0.0 1.0 0.0 1.0 0.0 0.97881 0.0 0.97881 0.0 tgk tgk tg 1.0 0.0 0.99411 0.00012 1.0 0.0 1.0 0.0 0.99754 5e-05 0.99803 3e-05 tgl tgl - 0.99951 0.00012 0.99901 0.00011 0.99901 0.0001 tha tha th 1.0 0.0 0.98749 0.0 1.0 0.0 1.0 0.0 0.98749 0.0 0.98749 0.0 tir tir - 0.98851 0.0 0.98851 0.0 0.988 0.0 tpi tpi - 0.99951 0.0 0.99951 0.0 0.99901 0.0 tsn tsn - 0.99753 0.00012 0.99753 0.00011 0.99753 0.0001 tso tso - 0.99753 0.00049 0.99753 0.00044 0.99901 0.0001 tuk tuk - 0.99951 0.00012 0.99951 0.00011 0.99951 0.0001 tum tum - 0.99852 0.00025 0.99901 0.00011 0.99901 0.0 tur tur tr 0.99119 0.00222 0.54852 0.01718 0.99119 0.00199 0.99119 0.00175 0.56606 0.01595 0.59982 0.01354 tzm tzm - 0.94421 0.01173 0.94421 0.01051 0.94524 0.00837 uig uig - 1.0 0.0 1.0 0.0 1.0 0.0 ukr ukr uk 0.99951 0.00012 0.9936 0.0001 0.99951 0.00011 0.99951 0.0001 0.99409 9e-05 0.99309 8e-05 umb umb - 0.88585 0.00099 0.8781 0.00055 0.834 0.00029 urd urd ur 0.98346 0.00407 0.64759 0.0113 0.98346 0.00365 0.99021 0.00185 0.66866 0.01026 0.73611 0.00724 uzb uzn uz 0.96015 0.01037 0.63806 0.01187 0.98396 0.00365 0.99411 0.00117 0.70282 0.00882 0.77099 0.00604 vec vec - 0.99703 0.00012 0.99703 0.00011 0.99653 0.0 vie vie vi 0.99951 0.00012 0.27281 0.0559 1.0 0.0 1.0 0.0 0.30667 0.04728 0.37036 0.03475 war war - 0.99951 0.0 0.99951 0.0 0.99951 0.0 wol wol - 0.99852 0.0 0.99802 0.0 0.99802 0.0 xho xho xh 0.98918 0.00198 0.58763 0.0121 0.98918 0.00177 0.99113 0.00117 0.64438 0.00924 0.71054 0.00591 yid ydd yi 1.0 0.0 0.99304 1e-05 1.0 0.0 1.0 0.0 0.99304 1e-05 0.99254 1e-05 yor yor yo 0.99455 0.00037 0.16841 0.00856 0.99355 0.00033 0.99053 0.0001 0.1928 0.00562 0.21661 0.00333 zho yue/zho zh-Latn/zh 1.0 0.0 0.88912 0.00752 1.0 0.0 1.0 0.0 0.93055 0.00435 0.9619 0.00208 zul zul zu 0.96158 0.00185 0.41074 0.02908 0.96353 0.00122 0.96293 0.00088 0.4457 0.02503 0.50587 0.01868\nTable 31: Comparison of GlotLID vs CLD3 on FLORES-200 benchmark (part 2)\nwith confidence threshold \u03b8\nGlotLID-M OpenLID GlotLID-M \u03b8=.3 GlotLID-M \u03b8=.5 OpenLID \u03b8=.3 OpenLID \u03b8=.5\niso639-3 FLORES Code(s) OpenLID Code(s) F1\u2191 FPR\u2193 F1\u2191 FPR\u2193 F1\u2191 FPR\u2193 F1\u2191 FPR\u2193 F1\u2191 FPR\u2193 F1\u2191 FPR \u2193\nace ace ace 0.95503 0.00579 0.96012 0.00734 0.955 0.00426 0.95689 0.00299 0.96012 0.00734 0.96099 0.0067 acm acm acm 0.01562 0.00023 0.03279 0.00051 0.00784 0.00017 0.00393 0.00011 0.03282 0.00051 0.02713 0.00038 acq acq acq 0.00197 6e-05 0.00197 0.0 0.00197 4e-05 0.00197 0.0 0.00197 0.0 0.00197 0.0 aeb aeb aeb 0.28501 0.00199 0.33982 0.00624 0.20348 0.00089 0.15064 0.00026 0.34033 0.00624 0.32223 0.00444 afr afr afr 1.0 0.0 0.99951 0.0 1.0 0.0 1.0 0.0 0.99951 0.0 0.99951 0.0 ajp ajp ajp 0.10836 0.00102 0.19064 0.00206 0.09328 0.00042 0.05369 0.00011 0.18924 0.00206 0.14808 0.00163 aka aka/twi twi 0.99852 0.0 0.99852 0.0 0.99852 0.0 0.99852 0.0 0.99852 0.0 0.99802 0.0 als als als 0.99852 0.00011 0.99951 6e-05 0.67539 0.0 0.44785 0.0 1.0 6e-05 1.0 0.0 amh amh amh 0.99951 6e-05 0.99951 6e-05 0.99951 4e-05 0.99951 4e-05 0.99951 6e-05 0.99951 6e-05 apc apc apc 0.17857 0.00915 0.23324 0.01287 0.13223 0.00498 0.09179 0.00255 0.23229 0.01287 0.21749 0.01001 arb arb arb 0.25705 0.21515 0.24409 0.14709 0.12788 0.06529 0.09585 0.03457 0.24413 0.14709 0.24699 0.13319 ars ars ars 0.00894 0.00574 0.01843 0.0179 0.00377 0.00202 0.00387 0.00074 0.01846 0.0179 0.01784 0.01314 ary ary ary 0.56588 0.05164 0.48937 0.09958 0.71835 0.00578 0.69151 0.00218 0.48966 0.09958 0.50306 0.08957 arz arz arz 0.46309 0.09806 0.42345 0.14168 0.57422 0.03125 0.58652 0.01647 0.42387 0.14168 0.43993 0.12643 asm asm asm 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 ast ast ast 0.9916 0.00051 0.99011 0.00058 0.99308 0.00025 0.99257 0.00018 0.99011 0.00058 0.99009 0.00044 awa awa awa 0.38951 6e-05 0.67704 0.00051 0.38982 0.0 0.35313 0.0 0.67704 0.00051 0.64761 0.00031 ayr ayr ayr 0.99557 0.00045 0.99852 0.00019 0.96738 4e-05 0.93193 0.0 0.99852 0.00019 0.99951 6e-05 azb azb azb 0.36583 0.00011 0.75139 0.0 0.25065 0.0 0.13112 0.0 0.75139 0.0 0.74286 0.0 azj azj azj 0.99901 0.0 0.99901 6e-05 0.49963 0.0 0.27304 0.0 0.99901 6e-05 0.99901 6e-05 bak bak bak 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 bam bam bam 0.52563 0.05119 0.61044 0.06424 0.52664 0.03779 0.53084 0.03213 0.61123 0.06424 0.61308 0.0614 ban ban ban 0.97521 6e-05 0.97887 0.00019 0.97571 0.0 0.97467 0.0 0.97937 0.00019 0.97885 0.00013 bel bel bel 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 bem bem bem 0.9906 0.00045 0.97961 0.00251 0.99256 0.00017 0.99256 0.00015 0.98152 0.00251 0.99116 0.00094 ben ben ben 0.99852 6e-05 0.99253 0.0 0.99852 4e-05 0.99852 4e-05 0.99253 0.0 0.99253 0.0 bho bho bho 0.94329 0.00443 0.89206 0.01481 0.94374 0.00325 0.94846 0.00181 0.89206 0.01481 0.90785 0.01195 bjn bjn bjn 0.79496 0.05329 0.79638 0.06225 0.7956 0.03927 0.79547 0.03335 0.79638 0.06225 0.79741 0.05965 bod bod bod 0.94589 6e-05 0.80449 0.0 0.94589 4e-05 0.94589 4e-05 0.80449 0.0 0.80378 0.0 bos bos bos 0.58206 0.00312 0.69239 0.01229 0.57353 0.00232 0.49605 0.00126 0.69239 0.01229 0.69172 0.01183 bug bug bug 0.99802 6e-05 0.99654 0.00013 0.99703 0.0 0.99404 0.0 0.99653 0.00013 0.99703 0.0 bul bul bul 0.99951 0.0 1.0 0.0 0.99951 0.0 0.99951 0.0 1.0 0.0 0.99951 0.0 cat cat cat 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 0.99951 0.0 ceb ceb ceb 0.99503 0.0 0.99951 6e-05 0.99454 0.0 0.99404 0.0 0.99951 6e-05 1.0 0.0 ces ces ces 0.99951 6e-05 0.99753 0.00019 0.99951 4e-05 0.99901 4e-05 0.99704 0.00019 0.99704 0.00019 cjk cjk cjk 0.84493 6e-05 0.90232 0.00032 0.83429 4e-05 0.79834 4e-05 0.89554 0.00032 0.8716 0.00019 ckb ckb ckb 0.99901 0.00011 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 crh crh crh 0.98902 6e-05 0.99204 6e-05 0.988 0.0 0.988 0.0 0.99204 6e-05 0.99204 6e-05 cym cym cym 0.99951 6e-05 1.0 0.0 0.99951 4e-05 1.0 0.0 1.0 0.0 1.0 0.0 dan dan dan 0.9931 0.00051 0.98661 0.00064 0.99505 0.00017 0.99554 7e-05 0.98758 0.00064 0.98757 0.00038 deu deu deu 0.99901 0.0 1.0 0.0 0.99901 0.0 0.99852 0.0 1.0 0.0 1.0 0.0 dik dik dik 0.99653 6e-05 0.99951 0.0 0.99653 0.0 0.99454 0.0 0.99901 0.0 0.99802 0.0 dyu dyu dyu 0.12435 0.01449 0.04212 0.00367 0.11878 0.01075 0.11186 0.00938 0.04212 0.00367 0.04033 0.00357 dzo dzo dzo 0.9496 0.00585 0.85848 0.02131 0.9491 0.00434 0.9491 0.0038 0.85848 0.02131 0.85848 0.02072 ell ell ell 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 eng eng eng 0.98732 0.00148 0.99263 0.00084 0.99215 0.00063 0.99556 0.00022 0.99308 0.00084 0.99256 0.00025 epo epo epo 0.99852 0.00017 0.99901 0.00013 0.99951 4e-05 1.0 0.0 1.0 0.00013 0.99951 0.0 est est est 1.0 0.0 0.99753 0.00026 1.0 0.0 1.0 0.0 0.99901 0.00026 0.99802 0.0 eus eus eus 0.99951 0.0 0.99704 0.00032 0.99951 0.0 0.99951 0.0 0.99852 0.00032 0.99951 0.0 ewe ewe ewe 1.0 0.0 0.99803 0.00026 1.0 0.0 1.0 0.0 0.99803 0.00026 0.99803 0.00025 fao fao fao 0.99951 0.0 1.0 0.0 0.99951 0.0 0.99951 0.0 1.0 0.0 1.0 0.0 fij fij fij 0.99951 0.0 0.99852 6e-05 0.99901 0.0 0.99901 0.0 0.99852 6e-05 0.99852 6e-05 fin fin fin 0.99901 0.00011 0.99951 6e-05 1.0 0.0 1.0 0.0 1.0 6e-05 1.0 0.0 fon fon fon 0.99752 0.0 0.99802 0.0 0.99752 0.0 0.99703 0.0 0.99802 0.0 0.99802 0.0 fra fra fra 0.99951 6e-05 0.99503 0.0 0.99951 4e-05 0.99852 4e-05 0.99454 0.0 0.99253 0.0 fur fur fur 0.99951 6e-05 0.99852 0.00019 0.99951 4e-05 1.0 0.0 0.99901 0.00019 1.0 0.0 fuv fuv fuv 0.96843 6e-05 0.98649 6e-05 0.96099 0.0 0.94693 0.0 0.98699 6e-05 0.98189 0.0 gaz gaz gaz 0.99411 0.00068 0.99264 0.00097 0.90281 0.00017 0.78925 7e-05 0.99508 0.00097 0.99852 0.00019 gla gla gla 0.99951 6e-05 0.99704 0.00039 1.0 0.0 1.0 0.0 0.99754 0.00039 0.99901 0.00013 gle gle gle 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 glg glg glg 0.99703 0.00011 0.99704 0.00032 0.99703 8e-05 0.99703 4e-05 0.99704 0.00032 0.99704 0.00031 grn grn grn 1.0 0.0 0.99754 0.00032 1.0 0.0 1.0 0.0 0.99803 0.00032 0.99901 0.00013 guj guj guj 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 hat hat hat 0.99852 0.00017 0.99655 0.00045 0.99852 0.00013 0.99901 7e-05 0.99754 0.00045 0.99852 0.00019 hau hau hau 0.95427 0.00551 0.90196 0.01416 0.98348 0.00143 0.99313 0.00052 0.93143 0.01416 0.97495 0.00325 heb heb heb 0.99606 0.00045 0.99901 0.00013 1.0 0.0 1.0 0.0 0.99901 0.00013 0.99901 0.00013 hin hin hin 0.67444 0.05551 0.84774 0.02279 0.6767 0.04078 0.69697 0.0325 0.8481 0.02279 0.86814 0.01859 hne hne hne 0.90296 0.00256 0.93617 0.00406 0.90343 0.00186 0.898 0.00133 0.93617 0.00406 0.9314 0.00344 hrv hrv hrv 0.75157 0.03272 0.74355 0.02427 0.75573 0.0237 0.768 0.01666 0.74389 0.02427 0.74309 0.02347 hun hun hun 1.0 0.0 1.0 0.0 1.0 0.0 0.99951 0.0 1.0 0.0 0.99951 0.0 hye hye hye 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 ibo ibo ibo 0.99951 6e-05 0.99951 6e-05 1.0 0.0 1.0 0.0 0.99951 6e-05 0.99951 6e-05 ilo ilo ilo 0.99951 6e-05 0.99901 0.00013 0.99951 4e-05 0.99951 4e-05 0.99901 0.00013 0.99951 6e-05 ind ind ind 0.91929 0.00983 0.92788 0.00566 0.92775 0.00645 0.93204 0.00528 0.92788 0.00566 0.92728 0.00545 isl isl isl 0.99901 0.00011 1.0 0.0 0.99951 4e-05 0.99951 4e-05 1.0 0.0 1.0 0.0 ita ita ita 0.99803 0.00017 0.99404 0.0 0.99852 8e-05 0.99901 4e-05 0.99404 0.0 0.99203 0.0 jav jav jav 0.98346 0.00187 0.96103 0.00521 0.99166 0.00067 0.99213 0.00048 0.96978 0.00521 0.98394 0.002 jpn jpn jpn 1.0 0.0 0.99951 0.0 1.0 0.0 1.0 0.0 0.99951 0.0 0.99951 0.0 kab kab kab 0.85967 0.01221 0.83636 0.02549 0.87886 0.00709 0.90909 0.00362 0.8404 0.02549 0.84887 0.02247 kac kac kac 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 kam kam kam 0.92406 6e-05 0.90011 6e-05 0.91416 0.0 0.87368 0.0 0.89651 6e-05 0.87382 6e-05 kan kan kan 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 kas kas kas 0.97674 0.0 0.98497 0.00013 0.97597 0.0 0.96945 0.0 0.98497 0.00013 0.98445 6e-05 kat kat kat 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 kaz kaz kaz 0.99951 0.0 0.99951 0.0 0.99951 0.0 0.99951 0.0 0.99951 0.0 0.99951 0.0 kbp kbp kbp 0.99901 6e-05 1.0 0.0 0.99901 4e-05 0.99901 4e-05 1.0 0.0 1.0 0.0 kea kea kea 0.95238 0.0 0.96524 0.0 0.9513 0.0 0.93586 0.0 0.96258 0.0 0.95455 0.0 khk khk khk 1.0 0.0 1.0 0.0 0.36246 0.0 0.21674 0.0 1.0 0.0 1.0 0.0 khm khm khm 0.99951 0.0 0.99951 0.0 0.99951 0.0 0.99951 0.0 0.99951 0.0 0.99951 0.0 kik kik kik 0.96562 0.00403 0.96282 0.00489 0.96509 0.00295 0.96456 0.00255 0.96374 0.00489 0.96512 0.00444 kin kin kin 0.91471 0.00034 0.8872 0.0009 0.91471 0.00025 0.91471 0.00022 0.8872 0.0009 0.88768 0.00081 kir kir kir 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 kmb kmb kmb 0.96321 0.00415 0.93939 0.00695 0.96923 0.00253 0.97713 0.00144 0.94291 0.00695 0.95349 0.00426 kmr kmr kmr 0.99901 0.0 0.99852 0.00013 0.99901 0.0 0.99901 0.0 0.99852 0.00013 0.99852 6e-05 knc knc knc 0.8634 0.00153 0.86942 0.00013 0.86869 0.00021 0.86966 4e-05 0.86935 0.00013 0.86935 6e-05 kon kon kon 0.99802 0.0 0.99458 0.00058 0.99802 0.0 0.99802 0.0 0.99507 0.00058 0.99654 0.00013 kor kor kor 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0\nTable 34: Comparison of GlotLID vs OpenLID on FLORES-200 benchmark (part 1)\nwith confidence threshold \u03b8\nGlotLID-M OpenLID GlotLID-M \u03b8=.3 GlotLID-M \u03b8=.5 OpenLID \u03b8=.3 OpenLID \u03b8=.5\niso639-3 FLORES Code(s) OpenLID Code(s) F1\u2191 FPR\u2193 F1\u2191 FPR\u2193 F1\u2191 FPR\u2193 F1\u2191 FPR\u2193 F1\u2191 FPR\u2193 F1\u2191 FPR \u2193\nlao lao lao 1.0 0.0 1.0 0.0 0.99901 0.0 0.99802 0.0 1.0 0.0 1.0 0.0 lij lij lij 0.99901 6e-05 0.99803 0.00019 0.99852 4e-05 0.99753 4e-05 0.99803 0.00019 0.99852 0.00013 lim lim lim 0.99253 0.0 0.99654 0.00019 0.99253 0.0 0.99153 0.0 0.99703 0.00019 0.99703 0.00013 lin lin lin 0.99901 0.00011 0.99901 0.00013 0.99852 8e-05 0.99901 4e-05 0.99901 0.00013 0.99802 0.00013 lit lit lit 0.99951 0.0 0.99852 0.00013 0.99951 0.0 0.99951 0.0 0.99852 0.00013 0.99901 6e-05 lmo lmo lmo 0.99554 0.00011 0.99753 0.00026 0.99554 8e-05 0.99504 4e-05 0.99753 0.00026 0.99802 6e-05 ltg ltg ltg 0.99653 0.0 0.99852 0.0 0.99653 0.0 0.99503 0.0 0.99852 0.0 0.99802 0.0 ltz ltz ltz 0.99951 0.0 0.99951 0.0 0.99901 0.0 0.99901 0.0 0.99951 0.0 0.99951 0.0 lua lua lua 0.99653 6e-05 0.99604 6e-05 0.99553 0.0 0.99404 0.0 0.99554 6e-05 0.99553 0.0 lug lug lug 0.99653 0.0 0.99409 0.00058 0.99603 0.0 0.99454 0.0 0.99458 0.00058 0.99605 0.00031 luo luo luo 1.0 0.0 0.99852 0.00019 1.0 0.0 0.99951 0.0 0.99951 0.00019 0.99951 6e-05 lus lus lus 0.99653 6e-05 0.99852 0.0 0.99703 0.0 0.99653 0.0 0.99852 0.0 0.99802 0.0 lvs lvs lvs 0.99655 0.00034 0.99901 6e-05 0.92495 0.00021 0.80189 0.00015 0.99901 6e-05 0.99901 6e-05 mag mag mag 0.95459 0.00136 0.96204 0.00174 0.95507 0.00097 0.95408 0.00048 0.96204 0.00174 0.96393 0.00138 mai mai mai 0.97366 6e-05 0.98802 0.00013 0.97366 4e-05 0.97102 0.0 0.98802 0.00013 0.98701 0.00013 mal mal mal 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 mar mar mar 1.0 0.0 0.99901 0.00013 1.0 0.0 1.0 0.0 0.99901 0.00013 1.0 0.0 min min min 0.6616 0.00017 0.66183 0.00039 0.66182 8e-05 0.66116 4e-05 0.66183 0.00039 0.66205 0.00019 mkd mkd mkd 1.0 0.0 0.99951 6e-05 1.0 0.0 1.0 0.0 0.99951 6e-05 0.99951 6e-05 mlt mlt mlt 0.97401 0.00307 0.93143 0.00959 0.99216 0.00067 0.99803 0.00015 0.95788 0.00959 0.98684 0.00169 mni mni mni 0.99901 6e-05 0.99411 0.00077 0.99901 4e-05 0.99901 4e-05 0.99411 0.00077 0.99411 0.00075 mos mos mos 0.98138 0.0 0.9814 6e-05 0.97415 0.0 0.96418 0.0 0.97881 6e-05 0.96997 0.0 mri mri mri 0.99901 6e-05 0.99951 6e-05 0.99951 0.0 0.99901 0.0 1.0 6e-05 1.0 0.0 mya mya mya 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 nld nld nld 0.99803 0.00023 0.99704 0.00019 0.99901 8e-05 0.99901 4e-05 0.99704 0.00019 0.99704 0.00019 nno nno nno 0.98507 0.00045 0.98277 0.00135 0.98606 0.00025 0.986 7e-05 0.98277 0.00135 0.98374 0.00119 nob nob nob 0.98185 0.00148 0.97188 0.00193 0.95835 0.0011 0.89931 0.00078 0.97086 0.00193 0.96883 0.00188 npi npi npi 0.99104 0.00011 0.99803 0.00026 0.53362 8e-05 0.30628 0.0 0.99803 0.00026 0.99803 0.00019 nso nso nso 0.99704 0.00028 0.9868 0.00154 0.99704 0.00021 0.99655 0.00018 0.9868 0.00154 0.98776 0.00138 nus nus nus 0.99951 0.0 0.99951 0.0 0.99951 0.0 0.99951 0.0 0.99951 0.0 0.99951 0.0 nya nya nya 0.99753 0.00023 0.99606 0.00051 0.99803 0.00013 0.99852 4e-05 0.99704 0.00051 0.99754 0.00031 oci oci oci 0.99951 6e-05 0.9941 0.00071 1.0 0.0 0.99951 0.0 0.9941 0.00071 0.99557 0.0005 ory ory ory 1.0 0.0 1.0 0.0 0.80519 0.0 0.66314 0.0 1.0 0.0 1.0 0.0 pag pag pag 0.99852 0.0 0.99901 6e-05 0.99852 0.0 0.99852 0.0 0.99901 6e-05 0.99901 6e-05 pan pan pan 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 pap pap pap 0.99069 0.00102 0.97681 0.00303 0.99118 0.00072 0.99557 0.0003 0.9787 0.00303 0.9815 0.00213 pbt pbt pbt 0.81486 0.00051 0.99704 0.00032 0.76381 0.00025 0.68523 0.00011 0.99704 0.00032 0.99753 0.00025 pes pes pes 0.57435 0.08493 0.54791 0.06997 0.60647 0.04812 0.57502 0.02962 0.54829 0.06997 0.55288 0.06641 plt plt plt 0.99852 0.0 1.0 0.0 0.99802 0.0 0.99603 0.0 1.0 0.0 1.0 0.0 pol pol pol 0.9907 0.00108 0.99411 0.00077 0.99167 0.00072 0.99167 0.00063 0.99606 0.00077 0.99606 0.0005 por por por 0.99655 0.0004 0.99408 0.00051 0.99704 0.00025 0.99704 0.00018 0.99457 0.00051 0.99654 0.00019 prs prs prs 0.14688 0.0192 0.51273 0.01571 0.14348 0.01135 0.1171 0.00716 0.51273 0.01571 0.51395 0.01502 quy quy quy 0.75904 0.0 0.99951 6e-05 0.625 0.0 0.57163 0.0 0.99951 6e-05 1.0 0.0 ron ron ron 0.99951 0.0 0.99754 0.00032 0.99951 0.0 0.99951 0.0 0.99852 0.00032 0.99852 0.00019 run run run 0.92541 0.00881 0.9044 0.01268 0.92584 0.0065 0.92627 0.00565 0.9044 0.01268 0.90563 0.01214 rus rus rus 0.99901 6e-05 0.99901 6e-05 0.99901 4e-05 0.99901 4e-05 0.99901 6e-05 0.99901 0.0 sag sag sag 0.99901 0.0 0.99901 0.0 0.99901 0.0 0.99901 0.0 0.99901 0.0 0.99901 0.0 san san san 0.99104 6e-05 0.99002 0.0 0.99104 4e-05 0.99103 0.0 0.99002 0.0 0.98749 0.0 sat sat sat 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 scn scn scn 0.99802 6e-05 0.99507 0.00051 0.99802 4e-05 0.99852 0.0 0.99556 0.00051 0.99556 0.00038 shn shn shn 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 sin sin sin 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 slk slk slk 0.99852 6e-05 0.99654 0.00019 0.99852 4e-05 0.99901 0.0 0.99703 0.00019 0.99753 6e-05 slv slv slv 0.99459 0.00062 0.99606 0.00045 0.99655 0.0003 0.99951 4e-05 0.99753 0.00045 0.99951 0.0 smo smo smo 0.99603 0.0 0.99852 0.00013 0.99603 0.0 0.99603 0.0 0.99901 0.00013 0.99951 0.0 sna sna sna 0.99901 0.00011 0.99411 0.00077 0.99951 4e-05 1.0 0.0 0.99704 0.00077 0.99852 0.00019 snd snd snd 0.99362 0.00074 0.99901 0.0 0.99508 0.00042 0.99704 0.00022 0.99901 0.0 0.99901 0.0 som som som 0.96657 0.00398 0.97683 0.00309 0.98973 0.00089 0.99803 0.00015 0.98828 0.00309 0.99557 0.00056 sot sot sot 1.0 0.0 0.9567 0.0 1.0 0.0 1.0 0.0 0.9567 0.0 0.95401 0.0 spa spa spa 0.99508 0.00057 0.99211 0.00064 0.99508 0.00042 0.99508 0.00033 0.9921 0.00064 0.99259 0.0005 srd srd srd 0.99951 0.0 0.99606 0.00039 0.99901 0.0 0.99901 0.0 0.99704 0.00039 0.99704 0.00025 srp srp srp 0.99901 0.00011 0.99951 0.0 0.99951 4e-05 1.0 0.0 0.99951 0.0 0.99951 0.0 ssw ssw ssw 0.99654 0.00023 0.99106 0.00026 0.99456 0.00017 0.99455 0.00011 0.99205 0.00026 0.99254 6e-05 sun sun sun 0.99012 0.00057 0.99014 0.00077 0.99355 0.00013 0.99304 4e-05 0.99112 0.00077 0.99259 0.00044 swe swe swe 0.99754 0.00028 0.99852 0.00019 0.99951 4e-05 1.0 0.0 1.0 0.00019 1.0 0.0 swh swh swh 0.94869 0.00187 0.92378 0.01075 0.91684 0.00072 0.74599 0.00018 0.93015 0.01075 0.95247 0.00632 szl szl szl 0.99104 6e-05 0.99504 0.00013 0.99104 4e-05 0.99104 4e-05 0.99554 0.00013 0.99553 0.0 tam tam tam 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 taq taq taq 0.80642 0.02022 0.82365 0.01635 0.83861 0.00827 0.84449 0.00332 0.83486 0.01635 0.85028 0.00814 tat tat tat 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 tel tel tel 1.0 0.0 0.99901 0.0 1.0 0.0 1.0 0.0 0.99901 0.0 0.99901 0.0 tgk tgk tgk 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 tgl tgl tgl 0.99901 0.00011 1.0 0.0 0.99901 4e-05 0.99901 4e-05 1.0 0.0 0.99951 0.0 tha tha tha 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 tir tir tir 0.98851 0.0 0.99951 0.0 0.98851 0.0 0.988 0.0 0.99951 0.0 0.99951 0.0 tpi tpi tpi 0.99951 0.0 1.0 0.0 0.99951 0.0 0.99901 0.0 0.99951 0.0 0.99951 0.0 tsn tsn tsn 0.99753 6e-05 0.96932 0.00406 0.99753 4e-05 0.99753 4e-05 0.96932 0.00406 0.97305 0.00344 tso tso tso 0.99803 0.00023 0.99606 0.00045 0.99753 0.00017 0.99901 4e-05 0.99655 0.00045 0.99852 0.00013 tuk tuk tuk 0.99803 0.00023 0.99951 6e-05 0.99951 4e-05 0.99951 4e-05 1.0 6e-05 1.0 0.0 tum tum tum 0.99852 0.00011 0.99556 0.00045 0.99901 4e-05 0.99901 0.0 0.99655 0.00045 0.99802 0.00013 tur tur tur 0.9907 0.00108 0.99362 0.00084 0.9907 0.0008 0.99119 0.00066 0.99362 0.00084 0.99362 0.00081 tzm tzm tzm 0.94421 0.0054 0.95352 0.00515 0.94421 0.00401 0.94524 0.00318 0.95352 0.00515 0.95302 0.00501 uig uig uig 0.99901 0.00011 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 ukr ukr ukr 0.99951 6e-05 0.99951 6e-05 0.99951 4e-05 0.99951 4e-05 0.99951 6e-05 0.99951 6e-05 umb umb umb 0.88585 0.00045 0.97762 0.00103 0.8781 0.00021 0.834 0.00011 0.97906 0.00103 0.97433 0.00044 urd urd urd 0.98346 0.00187 0.98491 0.002 0.98346 0.00139 0.99021 0.0007 0.98491 0.002 0.98684 0.00169 uzn uzn uzn 0.96885 0.00364 0.92844 0.01004 0.87839 0.00089 0.76439 0.0003 0.94934 0.01004 0.97731 0.00294 vec vec vec 0.99703 6e-05 0.99605 0.00026 0.99703 4e-05 0.99653 0.0 0.99605 0.00026 0.99605 0.00025 vie vie vie 0.99951 6e-05 0.99951 6e-05 1.0 0.0 1.0 0.0 0.99951 6e-05 1.0 0.0 war war war 0.99951 0.0 1.0 0.0 0.99951 0.0 0.99951 0.0 1.0 0.0 0.99951 0.0 wol wol wol 0.99852 0.0 0.99704 0.00026 0.99802 0.0 0.99802 0.0 0.99753 0.00026 0.99852 0.0 xho xho xho 0.99118 0.00097 0.98581 0.00154 0.98968 0.00067 0.99113 0.00044 0.98725 0.00154 0.98968 0.001 ydd ydd ydd 0.99603 0.0 0.99901 0.0 0.99901 0.0 0.99901 0.0 yor yor yor 0.99406 0.00023 0.99901 0.00013 0.99355 0.00013 0.99053 4e-05 0.99951 0.00013 0.99951 0.0 yue yue yue 0.00394 0.0 0.00588 0.00032 0.00588 0.00032 0.00588 0.00031 zho zho zho 1.0 0.0 0.79873 0.06508 1.0 0.0 1.0 0.0 0.79873 0.06508 0.79881 0.06315 zsm zsm zsm 0.93506 0.00307 0.92745 0.00766 0.94641 0.00127 0.94972 0.00081 0.93589 0.00766 0.94665 0.00463 zul zul zul 0.96893 0.0 0.98277 0.00135 0.96893 0.0 0.96735 0.0 0.98325 0.00135 0.98519 0.001\nTable 35: Comparison of GlotLID vs OpenLID on FLORES-200 benchmark (part 2)\nwith confidence threshold \u03b8\nGlotLID-M NLLB GlotLID-M \u03b8=.3 GlotLID-M \u03b8=.5 NLLB \u03b8=.3 NLLB \u03b8=.5\niso639-3 FLORES Code(s) NLLB Code(s) F1\u2191 FPR\u2193 F1\u2191 FPR\u2193 F1\u2191 FPR\u2193 F1\u2191 FPR\u2193 F1\u2191 FPR\u2193 F1\u2191 FPR \u2193\nace ace ace 0.95503 0.00579 0.93532 0.01209 0.955 0.00426 0.95689 0.00299 0.93532 0.01209 0.9379 0.01076 afr afr afr 1.0 0.0 0.99852 0.00011 1.0 0.0 1.0 0.0 0.99901 0.00011 0.99951 0.0 aka aka aka 0.99852 0.0 0.82334 0.0058 0.99852 0.0 0.99852 0.0 0.82334 0.0058 0.82272 0.00554 als als als 0.99852 0.00011 0.99803 0.00022 0.67539 0.0 0.44785 0.0 0.99901 0.00022 0.99951 5e-05 amh amh amh 0.99951 6e-05 0.99901 0.00011 0.99951 4e-05 0.99951 4e-05 0.99951 0.00011 0.99951 5e-05 arb arb arb 0.25705 0.21515 0.31812 0.46765 0.12788 0.06529 0.09585 0.03457 0.31814 0.46765 0.31769 0.44516 asm asm asm 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 ast ast ast 0.9916 0.00051 0.99016 0.00076 0.99308 0.00025 0.99257 0.00018 0.99065 0.00076 0.9926 0.00047 awa awa awa 0.38951 6e-05 0.96113 0.00092 0.38982 0.0 0.35313 0.0 0.96113 0.00092 0.96304 0.00062 ayr ayr ayr 0.99557 0.00045 0.99802 5e-05 0.96738 4e-05 0.93193 0.0 0.99802 5e-05 0.99852 0.0 azb azb azb 0.36583 0.00011 0.8767 0.00119 0.25065 0.0 0.13112 0.0 0.87956 0.00119 0.88136 0.00057 azj azj azj 0.99901 0.0 0.99704 0.00033 0.49963 0.0 0.27304 0.0 0.99704 0.00033 0.99704 0.00031 bak bak bak 1.0 0.0 0.99901 5e-05 1.0 0.0 1.0 0.0 0.99901 5e-05 0.99901 5e-05 bam bam bam 0.52563 0.05119 0.61944 0.05293 0.52664 0.03779 0.53084 0.03213 0.61987 0.05293 0.62064 0.04998 ban ban ban 0.97521 6e-05 0.9712 0.00033 0.97571 0.0 0.97467 0.0 0.9712 0.00033 0.97117 0.00026 bel bel bel 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 bem bem bem 0.9906 0.00045 0.97394 0.00277 0.99256 0.00017 0.99256 0.00015 0.97677 0.00277 0.98922 0.00098 ben ben ben 0.99852 6e-05 0.99951 5e-05 0.99852 4e-05 0.99852 4e-05 0.99951 5e-05 0.99951 5e-05 bho bho bho 0.94329 0.00443 0.93354 0.00168 0.94374 0.00325 0.94846 0.00181 0.93354 0.00168 0.93416 0.00124 bjn bjn bjn 0.79496 0.05329 0.75225 0.06312 0.7956 0.03927 0.79547 0.03335 0.7523 0.06312 0.75747 0.05764 bod bod bod 0.94589 6e-05 0.96512 0.00385 0.94589 4e-05 0.94589 4e-05 0.9678 0.00385 0.96569 0.00222 bos bos bos 0.58206 0.00312 0.5954 0.0064 0.57353 0.00232 0.49605 0.00126 0.5954 0.0064 0.5949 0.00605 bug bug bug 0.99802 6e-05 0.97649 0.0006 0.99703 0.0 0.99404 0.0 0.97747 0.0006 0.97742 0.00036 bul bul bul 0.99951 0.0 0.99951 0.0 0.99951 0.0 0.99951 0.0 0.99951 0.0 0.99951 0.0 cat cat cat 1.0 0.0 0.98732 0.00141 1.0 0.0 1.0 0.0 0.98828 0.00141 0.99459 0.00057 ceb ceb ceb 0.99503 0.0 0.99951 0.0 0.99454 0.0 0.99404 0.0 0.99951 0.0 0.99951 0.0 ces ces ces 0.99951 6e-05 0.99901 0.00011 0.99951 4e-05 0.99901 4e-05 0.99951 0.00011 1.0 0.0 cjk cjk cjk 0.84493 6e-05 0.86875 0.00098 0.83429 4e-05 0.79834 4e-05 0.8611 0.00098 0.83995 0.00052 ckb ckb ckb 0.99901 0.00011 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 crh crh crh 0.98902 6e-05 0.98291 0.0 0.988 0.0 0.988 0.0 0.98291 0.0 0.98138 0.0 cym cym cym 0.99951 6e-05 0.99951 5e-05 0.99951 4e-05 1.0 0.0 1.0 5e-05 1.0 0.0 dan dan dan 0.9931 0.00051 0.99456 0.00022 0.99505 0.00017 0.99554 7e-05 0.99505 0.00022 0.99604 5e-05 deu deu deu 0.99901 0.0 0.9907 0.00103 0.99901 0.0 0.99852 0.0 1.0 0.00103 1.0 0.0 dik dik dik 0.99653 6e-05 0.99253 0.0 0.99653 0.0 0.99454 0.0 0.99203 0.0 0.99002 0.0 dyu dyu dyu 0.12435 0.01449 0.04797 0.00249 0.11878 0.01075 0.11186 0.00938 0.04797 0.00249 0.04621 0.00233 dzo dzo dzo 0.9496 0.00585 0.96791 5e-05 0.9491 0.00434 0.9491 0.0038 0.96685 5e-05 0.95405 5e-05 ell ell ell 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 eng eng eng 0.98732 0.00148 0.97825 0.00244 0.99215 0.00063 0.99556 0.00022 0.98925 0.00244 0.99362 0.00067 epo epo epo 0.99852 0.00017 0.99704 0.00033 0.99951 4e-05 1.0 0.0 0.99803 0.00033 0.99901 0.0001 est est est 1.0 0.0 0.99852 0.00016 1.0 0.0 1.0 0.0 0.99901 0.00016 0.99951 0.0 eus eus eus 0.99951 0.0 0.99852 0.00016 0.99951 0.0 0.99951 0.0 0.99901 0.00016 0.99951 5e-05 ewe ewe ewe 1.0 0.0 0.99704 0.00033 1.0 0.0 1.0 0.0 0.99704 0.00033 0.99704 0.00031 fao fao fao 0.99951 0.0 0.50517 0.0 0.99951 0.0 0.99951 0.0 0.50517 0.0 0.49852 0.0 fij fij fij 0.99951 0.0 1.0 0.0 0.99901 0.0 0.99901 0.0 1.0 0.0 0.99951 0.0 fin fin fin 0.99901 0.00011 0.99951 5e-05 1.0 0.0 1.0 0.0 1.0 5e-05 1.0 0.0 fon fon fon 0.99752 0.0 0.99703 0.0 0.99752 0.0 0.99703 0.0 0.99703 0.0 0.99703 0.0 fra fra fra 0.99951 6e-05 0.99606 0.00038 0.99951 4e-05 0.99852 4e-05 0.99852 0.00038 0.99901 5e-05 fur fur fur 0.99951 6e-05 0.99802 0.0 0.99951 4e-05 1.0 0.0 0.99802 0.0 0.99752 0.0 fuv fuv fuv 0.96843 6e-05 0.98102 0.00043 0.96099 0.0 0.94693 0.0 0.97842 0.00043 0.97578 0.00016 gaz gaz gaz 0.99411 0.00068 0.99951 5e-05 0.90281 0.00017 0.78925 7e-05 0.99951 5e-05 0.99951 5e-05 gla gla gla 0.99951 6e-05 0.99803 0.00016 1.0 0.0 1.0 0.0 0.99901 0.00016 0.99901 5e-05 gle gle gle 1.0 0.0 0.99803 0.00022 1.0 0.0 1.0 0.0 0.99951 0.00022 0.99951 5e-05 glg glg glg 0.99703 0.00011 0.9931 0.00054 0.99703 8e-05 0.99703 4e-05 0.99457 0.00054 0.99605 0.00021 grn grn grn 1.0 0.0 0.99654 0.00016 1.0 0.0 1.0 0.0 0.99703 0.00016 0.99703 0.0 guj guj guj 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 hat hat hat 0.99852 0.00017 0.99852 5e-05 0.99852 0.00013 0.99901 7e-05 0.99852 5e-05 0.99852 0.0 hau hau hau 0.95427 0.00551 0.99704 0.00027 0.98348 0.00143 0.99313 0.00052 0.99704 0.00027 0.99802 0.0001 heb heb heb 0.99606 0.00045 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 hin hin hin 0.67444 0.05551 0.87219 0.01594 0.6767 0.04078 0.69697 0.0325 0.87295 0.01594 0.88558 0.0134 hne hne hne 0.90296 0.00256 0.92997 0.00146 0.90343 0.00186 0.898 0.00133 0.92997 0.00146 0.92713 0.00135 hrv hrv hrv 0.75157 0.03272 0.73352 0.02901 0.75573 0.0237 0.768 0.01666 0.73382 0.02901 0.73361 0.02758 hun hun hun 1.0 0.0 0.99264 0.00081 1.0 0.0 0.99951 0.0 0.99557 0.00081 1.0 0.0 hye hye hye 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 ibo ibo ibo 0.99951 6e-05 0.99951 5e-05 1.0 0.0 1.0 0.0 1.0 5e-05 0.99951 0.0 ilo ilo ilo 0.99951 6e-05 0.99852 0.00016 0.99951 4e-05 0.99951 4e-05 0.99852 0.00016 0.99951 5e-05 ind ind ind 0.91929 0.00983 0.81942 0.02294 0.92775 0.00645 0.93204 0.00528 0.82348 0.02294 0.8455 0.018 isl isl isl 0.99901 0.00011 0.76205 0.03427 0.99951 4e-05 0.99951 4e-05 0.76205 0.03427 0.76522 0.03213 ita ita ita 0.99803 0.00017 0.97212 0.00309 0.99852 8e-05 0.99901 4e-05 0.97634 0.00309 0.98346 0.00171 jav jav jav 0.98346 0.00187 0.97674 0.00239 0.99166 0.00067 0.99213 0.00048 0.97769 0.00239 0.98244 0.0016 jpn jpn jpn 1.0 0.0 0.98268 0.00087 1.0 0.0 1.0 0.0 0.97702 0.00087 0.96175 0.00031 kab kab kab 0.85967 0.01221 0.85787 0.01811 0.87886 0.00709 0.90909 0.00362 0.8586 0.01811 0.86043 0.01692 kac kac kac 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 kam kam kam 0.92406 6e-05 0.75811 0.00011 0.91416 0.0 0.87368 0.0 0.74892 0.00011 0.70415 0.0001 kan kan kan 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 kas kas kas 0.97674 0.0 0.97753 5e-05 0.97597 0.0 0.96945 0.0 0.97752 5e-05 0.97545 0.0 kat kat kat 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 kaz kaz kaz 0.99951 0.0 0.99951 0.0 0.99951 0.0 0.99951 0.0 0.99951 0.0 0.99951 0.0 kbp kbp kbp 0.99901 6e-05 1.0 0.0 0.99901 4e-05 0.99901 4e-05 0.99951 0.0 0.99901 0.0 kea kea kea 0.95238 0.0 0.96099 0.0 0.9513 0.0 0.93586 0.0 0.95831 0.0 0.94693 0.0 khk khk khk 1.0 0.0 1.0 0.0 0.36246 0.0 0.21674 0.0 1.0 0.0 1.0 0.0 khm khm khm 0.99951 0.0 0.99901 0.0 0.99951 0.0 0.99951 0.0 0.99901 0.0 0.99901 0.0 kik kik kik 0.96562 0.00403 0.96357 0.00374 0.96509 0.00295 0.96456 0.00255 0.96257 0.00374 0.96158 0.00357 kin kin kin 0.91471 0.00034 0.97881 0.0013 0.91471 0.00025 0.91471 0.00022 0.97879 0.0013 0.97879 0.00119 kir kir kir 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 kmb kmb kmb 0.96321 0.00415 0.93613 0.00564 0.96923 0.00253 0.97713 0.00144 0.94106 0.00564 0.94588 0.00357 kmr kmr kmr 0.99901 0.0 0.99508 0.00054 0.99901 0.0 0.99901 0.0 0.99655 0.00054 0.99704 0.00026 knc knc knc 0.8634 0.00153 0.86855 0.00016 0.86869 0.00021 0.86966 4e-05 0.86784 0.00016 0.86745 5e-05 kon kon kon 0.99802 0.0 0.9936 0.00054 0.99802 0.0 0.99802 0.0 0.99507 0.00054 0.99605 0.00021 kor kor kor 1.0 0.0 0.99606 0.00043 1.0 0.0 1.0 0.0 0.99951 0.00043 1.0 0.0 lao lao lao 1.0 0.0 0.99951 0.0 0.99901 0.0 0.99802 0.0 0.99901 0.0 0.99852 0.0 lij lij lij 0.99901 6e-05 0.97738 0.00027 0.99852 4e-05 0.99753 4e-05 0.97735 0.00027 0.97423 0.00016 lim lim lim 0.99253 0.0 0.98701 0.00011 0.99253 0.0 0.99153 0.0 0.98701 0.00011 0.98701 0.0001 lin lin lin 0.99901 0.00011 0.99556 0.00033 0.99852 8e-05 0.99901 4e-05 0.99605 0.00033 0.99654 0.00016 lit lit lit 0.99951 0.0 0.99901 0.00011 0.99951 0.0 0.99951 0.0 0.99951 0.00011 0.99901 5e-05\nTable 36: Comparison of GlotLID vs NLLB on FLORES-200 benchmark (part 1)\nwith confidence threshold \u03b8\nGlotLID-M NLLB GlotLID-M \u03b8=.3 GlotLID-M \u03b8=.5 NLLB \u03b8=.3 NLLB \u03b8=.5\niso639-3 FLORES Code(s) NLLB Code(s) F1\u2191 FPR\u2193 F1\u2191 FPR\u2193 F1\u2191 FPR\u2193 F1\u2191 FPR\u2193 F1\u2191 FPR\u2193 F1\u2191 FPR \u2193\nlmo lmo lmo 0.99554 0.00011 0.96961 0.00119 0.99554 8e-05 0.99504 4e-05 0.97003 0.00119 0.96569 0.00067 ltg ltg ltg 0.99653 0.0 0.99203 0.0 0.99653 0.0 0.99503 0.0 0.99203 0.0 0.99153 0.0 ltz ltz ltz 0.99951 0.0 0.99951 0.0 0.99901 0.0 0.99901 0.0 0.99951 0.0 0.99951 0.0 lua lua lua 0.99653 6e-05 0.99358 0.00038 0.99553 0.0 0.99404 0.0 0.99357 0.00038 0.99554 0.0001 lug lug lug 0.99653 0.0 0.99214 0.00076 0.99603 0.0 0.99454 0.0 0.99311 0.00076 0.99458 0.00041 luo luo luo 1.0 0.0 0.99753 5e-05 1.0 0.0 0.99951 0.0 0.99753 5e-05 0.99703 0.0 lus lus lus 0.99653 6e-05 0.99454 5e-05 0.99703 0.0 0.99653 0.0 0.99404 5e-05 0.99303 0.0 lvs lvs lvs 0.99655 0.00034 0.99362 0.0007 0.92495 0.00021 0.80189 0.00015 0.99362 0.0007 0.99411 0.00062 mag mag mag 0.95459 0.00136 0.9311 0.00233 0.95507 0.00097 0.95408 0.00048 0.9311 0.00233 0.93218 0.00181 mai mai mai 0.97366 6e-05 0.98709 0.00043 0.97366 4e-05 0.97102 0.0 0.98709 0.00043 0.98706 0.00031 mal mal mal 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 mar mar mar 1.0 0.0 0.99508 0.00054 1.0 0.0 1.0 0.0 0.99655 0.00054 0.99901 0.0001 min min min 0.6616 0.00017 0.29545 5e-05 0.66182 8e-05 0.66116 4e-05 0.29558 5e-05 0.26341 0.0 mkd mkd mkd 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 mlt mlt mlt 0.97401 0.00307 0.99901 0.00011 0.99216 0.00067 0.99803 0.00015 0.99951 0.00011 0.99951 5e-05 mni mni mni 0.99901 6e-05 0.99951 0.0 0.99901 4e-05 0.99901 4e-05 0.99951 0.0 0.99951 0.0 mos mos mos 0.98138 0.0 0.9684 0.0 0.97415 0.0 0.96418 0.0 0.96629 0.0 0.95992 0.0 mri mri mri 0.99901 6e-05 0.99852 5e-05 0.99951 0.0 0.99901 0.0 0.99852 5e-05 0.99852 0.0 mya mya mya 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 nld nld nld 0.99803 0.00023 0.983 0.0019 0.99901 8e-05 0.99901 4e-05 0.98587 0.0019 0.98828 0.00124 nno nno nno 0.98507 0.00045 0.9697 0.00228 0.98606 0.00025 0.986 7e-05 0.9697 0.00228 0.97491 0.00155 nob nob nob 0.98185 0.00148 0.98289 0.00152 0.95835 0.0011 0.89931 0.00078 0.98385 0.00152 0.98481 0.00124 npi npi npi 0.99104 0.00011 0.99803 0.00022 0.53362 8e-05 0.30628 0.0 0.99803 0.00022 0.99852 0.00016 nso nso nso 0.99704 0.00028 0.98386 0.00146 0.99704 0.00021 0.99655 0.00018 0.98386 0.00146 0.98579 0.00119 nus nus nus 0.99951 0.0 0.99803 0.00016 0.99951 0.0 0.99951 0.0 0.99803 0.00016 0.99852 0.0001 nya nya nya 0.99753 0.00023 0.94604 0.00179 0.99803 0.00013 0.99852 4e-05 0.94636 0.00179 0.95175 0.00047 oci oci oci 0.99951 6e-05 0.98346 0.00179 1.0 0.0 0.99951 0.0 0.98634 0.00179 0.99118 0.00088 ory ory ory 1.0 0.0 1.0 0.0 0.80519 0.0 0.66314 0.0 1.0 0.0 1.0 0.0 pag pag pag 0.99852 0.0 0.99703 0.00011 0.99852 0.0 0.99852 0.0 0.99753 0.00011 0.99653 0.0 pan pan pan 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 pap pap pap 0.99069 0.00102 0.98394 0.00174 0.99118 0.00072 0.99557 0.0003 0.98538 0.00174 0.98681 0.00129 pbt pbt pbt 0.81486 0.00051 0.99654 0.00016 0.76381 0.00025 0.68523 0.00011 0.99654 0.00016 0.99555 0.00016 pes pes pes 0.57435 0.08493 0.68739 0.04815 0.60647 0.04812 0.57502 0.02962 0.68763 0.04815 0.6893 0.04553 plt plt plt 0.99852 0.0 1.0 0.0 0.99802 0.0 0.99603 0.0 1.0 0.0 1.0 0.0 pol pol pol 0.9907 0.00108 0.98396 0.00179 0.99167 0.00072 0.99167 0.00063 0.9878 0.00179 0.98925 0.00114 por por por 0.99655 0.0004 0.98538 0.00157 0.99704 0.00025 0.99704 0.00018 0.98924 0.00157 0.99312 0.00067 prs prs prs 0.14688 0.0192 0.49305 0.00098 0.14348 0.01135 0.1171 0.00716 0.49305 0.00098 0.49305 0.00093 quy quy quy 0.75904 0.0 1.0 0.0 0.625 0.0 0.57163 0.0 1.0 0.0 1.0 0.0 ron ron ron 0.99951 0.0 0.99852 0.00016 0.99951 0.0 0.99951 0.0 0.99852 0.00016 0.99852 0.00016 run run run 0.92541 0.00881 0.97824 0.00114 0.92584 0.0065 0.92627 0.00565 0.97824 0.00114 0.97919 0.00093 rus rus rus 0.99901 6e-05 0.99901 0.00011 0.99901 4e-05 0.99901 4e-05 0.99901 0.00011 0.99901 0.0001 sag sag sag 0.99901 0.0 0.99703 5e-05 0.99901 0.0 0.99901 0.0 0.99752 5e-05 0.99703 0.0 san san san 0.99104 6e-05 0.98853 0.00011 0.99104 4e-05 0.99103 0.0 0.98902 0.00011 0.98495 0.0 sat sat sat 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 scn scn scn 0.99802 6e-05 0.99361 0.0006 0.99802 4e-05 0.99852 0.0 0.99458 0.0006 0.99458 0.00041 shn shn shn 1.0 0.0 0.99852 0.0 1.0 0.0 1.0 0.0 0.99802 0.0 0.99503 0.0 sin sin sin 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 slk slk slk 0.99852 6e-05 0.99951 5e-05 0.99852 4e-05 0.99901 0.0 1.0 5e-05 1.0 0.0 slv slv slv 0.99459 0.00062 0.99852 0.00016 0.99655 0.0003 0.99951 4e-05 0.99951 0.00016 1.0 0.0 smo smo smo 0.99603 0.0 0.99852 0.00011 0.99603 0.0 0.99603 0.0 0.99951 0.00011 0.99951 0.0 sna sna sna 0.99901 0.00011 0.99411 0.00065 0.99951 4e-05 1.0 0.0 0.99508 0.00065 0.99655 0.00031 snd snd snd 0.99362 0.00074 0.99704 0.00033 0.99508 0.00042 0.99704 0.00022 0.99704 0.00033 0.99852 0.00016 som som som 0.96657 0.00398 1.0 0.0 0.98973 0.00089 0.99803 0.00015 1.0 0.0 1.0 0.0 sot sot sot 1.0 0.0 0.75523 0.0 1.0 0.0 1.0 0.0 0.75523 0.0 0.75062 0.0 spa spa spa 0.99508 0.00057 0.99215 0.00081 0.99508 0.00042 0.99508 0.00033 0.99361 0.00081 0.99655 0.00031 srd srd srd 0.99951 0.0 0.97726 0.0 0.99901 0.0 0.99901 0.0 0.97519 0.0 0.96735 0.0 srp srp srp 0.99901 0.00011 1.0 0.0 0.99951 4e-05 1.0 0.0 1.0 0.0 1.0 0.0 ssw ssw ssw 0.99654 0.00023 0.99155 0.00016 0.99456 0.00017 0.99455 0.00011 0.99155 0.00016 0.99204 5e-05 sun sun sun 0.99012 0.00057 0.95988 0.00277 0.99355 0.00013 0.99304 4e-05 0.96129 0.00277 0.96781 0.00155 swe swe swe 0.99754 0.00028 0.99901 5e-05 0.99951 4e-05 1.0 0.0 0.99951 5e-05 0.99951 0.0 swh swh swh 0.94869 0.00187 0.88153 0.01475 0.91684 0.00072 0.74599 0.00018 0.88811 0.01475 0.91328 0.00988 szl szl szl 0.99104 6e-05 0.98753 0.00016 0.99104 4e-05 0.99104 4e-05 0.98852 0.00016 0.98901 0.0 tam tam tam 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 taq taq taq 0.80642 0.02022 0.82223 0.0 0.83861 0.00827 0.84449 0.00332 0.82189 0.0 0.81776 0.0 tat tat tat 1.0 0.0 0.99951 0.0 1.0 0.0 1.0 0.0 0.99951 0.0 0.99951 0.0 tel tel tel 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 tgk tgk tgk 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 tgl tgl tgl 0.99901 0.00011 0.99704 0.00027 0.99901 4e-05 0.99901 4e-05 0.99803 0.00027 0.99852 0.0001 tha tha tha 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 tir tir tir 0.98851 0.0 0.99951 0.0 0.98851 0.0 0.988 0.0 0.99951 0.0 0.99951 0.0 tpi tpi tpi 0.99951 0.0 0.99802 0.0 0.99951 0.0 0.99901 0.0 0.99802 0.0 0.99752 0.0 tsn tsn tsn 0.99753 6e-05 0.84237 0.02039 0.99753 4e-05 0.99753 4e-05 0.84259 0.02039 0.84577 0.01888 tso tso tso 0.99803 0.00023 0.99069 0.00098 0.99753 0.00017 0.99901 4e-05 0.99214 0.00098 0.99606 0.00031 tuk tuk tuk 0.99803 0.00023 1.0 0.0 0.99951 4e-05 0.99951 4e-05 1.0 0.0 1.0 0.0 tum tum tum 0.99852 0.00011 0.98155 0.00201 0.99901 4e-05 0.99901 0.0 0.98251 0.00201 0.98972 0.00103 tur tur tur 0.9907 0.00108 0.98348 0.00184 0.9907 0.0008 0.99119 0.00066 0.98491 0.00184 0.98539 0.00155 twi twi twi 0.99951 0.0 0.8426 0.01231 0.99752 0.0 0.99503 0.0 0.84299 0.01231 0.84284 0.01164 tzm tzm tzm 0.94421 0.0054 0.88539 0.01421 0.94421 0.00401 0.94524 0.00318 0.8849 0.01421 0.8849 0.01356 uig uig uig 0.99901 0.00011 0.99951 5e-05 1.0 0.0 1.0 0.0 0.99951 5e-05 0.99951 5e-05 ukr ukr ukr 0.99951 6e-05 1.0 0.0 0.99951 4e-05 0.99951 4e-05 1.0 0.0 1.0 0.0 umb umb umb 0.88585 0.00045 0.96869 0.00228 0.8781 0.00021 0.834 0.00011 0.97247 0.00228 0.97617 0.00098 urd urd urd 0.98346 0.00187 0.97354 0.00298 0.98346 0.00139 0.99021 0.0007 0.97401 0.00298 0.97495 0.00269 uzn uzn uzn 0.96885 0.00364 0.99852 0.00016 0.87839 0.00089 0.76439 0.0003 0.99852 0.00016 0.99951 5e-05 vec vec vec 0.99703 6e-05 0.99159 0.00038 0.99703 4e-05 0.99653 0.0 0.99208 0.00038 0.99206 0.00016 vie vie vie 0.99951 6e-05 0.98925 0.00119 1.0 0.0 1.0 0.0 0.99951 0.00119 1.0 0.0 war war war 0.99951 0.0 0.99901 0.00011 0.99951 0.0 0.99951 0.0 0.99951 0.00011 1.0 0.0 wol wol wol 0.99852 0.0 0.99504 0.00011 0.99802 0.0 0.99802 0.0 0.99554 0.00011 0.99554 5e-05 xho xho xho 0.99118 0.00097 0.97793 0.00163 0.98968 0.00067 0.99113 0.00044 0.97937 0.00163 0.98566 0.00072 ydd ydd ydd 0.99603 0.0 1.0 0.0 1.0 0.0 1.0 0.0 yor yor yor 0.99406 0.00023 0.99556 0.00033 0.99355 0.00013 0.99053 4e-05 0.99654 0.00033 0.99752 0.0 yue yue yue 0.00394 0.0 0.4777 0.03395 0.47231 0.03395 0.46292 0.02752 zho zho zho 1.0 0.0 0.69715 0.02543 1.0 0.0 1.0 0.0 0.65912 0.02543 0.617 0.01821 zsm zsm zsm 0.93506 0.00307 0.93459 0.00342 0.94641 0.00127 0.94972 0.00081 0.93924 0.00342 0.94869 0.00171 zul zul zul 0.96893 0.0 0.96955 0.00293 0.96893 0.0 0.96735 0.0 0.97379 0.00293 0.97901 0.00176\nTable 37: Comparison of GlotLID vs NLLB on FLORES-200 benchmark (part 2)\nwith confidence threshold \u03b8\nGlotLID-M CLD3 GlotLID-M \u03b8=.3 GlotLID-M \u03b8=.5 CLD3 \u03b8=.5 CLD3 \u03b8=.7\niso639-3 UDHR Code(s) CLD3 Code(s) F1\u2191 FPR\u2193 F1\u2191 FPR\u2193 F1\u2191 FPR\u2193 F1\u2191 FPR\u2193 F1\u2191 FPR\u2193 F1\u2191 FPR\u2193\nwith confidence threshold \u03b8\nGlotLID-M CLD3 GlotLID-M \u03b8=.3 GlotLID-M \u03b8=.5 CLD3 \u03b8=.5 CLD3 \u03b8=.7\niso639-3 UDHR Code(s) CLD3 Code(s) F1\u2191 FPR\u2193 F1\u2191 FPR\u2193 F1\u2191 FPR\u2193 F1\u2191 FPR\u2193 F1\u2191 FPR\u2193 F1\u2191 FPR\u2193\nwith confidence threshold \u03b8\nGlotLID-M FT176 GlotLID-M \u03b8=.3 GlotLID-M \u03b8=.5 FT176 \u03b8=.3 FT176 \u03b8=.5\niso639-3 UDHR Code(s) FT176 Code(s) F1\u2191 FPR\u2193 F1\u2191 FPR\u2193 F1\u2191 FPR\u2193 F1\u2191 FPR\u2193 F1\u2191 FPR\u2193 F1\u2191 FPR\u2193\nwith confidence threshold \u03b8\nGlotLID-M FT176 GlotLID-M \u03b8=.3 GlotLID-M \u03b8=.5 FT176 \u03b8=.3 FT176 \u03b8=.5\niso639-3 UDHR Code(s) FT176 Code(s) F1\u2191 FPR\u2193 F1\u2191 FPR\u2193 F1\u2191 FPR\u2193 F1\u2191 FPR\u2193 F1\u2191 FPR\u2193 F1\u2191 FPR\u2193\nwith confidence threshold \u03b8\nGlotLID-M OpenLID GlotLID-M \u03b8=.3 GlotLID-M \u03b8=.5 OpenLID \u03b8=.3 OpenLID \u03b8=.5\niso639-3 UDHR Code(s) OpenLID Code(s) F1\u2191 FPR\u2193 F1\u2191 FPR\u2193 F1\u2191 FPR\u2193 F1\u2191 FPR\u2193 F1\u2191 FPR\u2193 F1\u2191 FPR\u2193\nace ace ace 0.90909 0.00126 0.71856 0.00247 0.91603 0.00107 0.91603 0.00103 0.83916 0.00247 0.88889 0.00075 afr afr afr 0.95238 0.00068 0.88235 0.00086 0.96774 0.00043 0.97561 0.00031 0.90909 0.00086 0.96 0.00027 als als als 0.86131 0.00205 0.32153 0.01331 0.7377 0.00182 0.56863 0.00134 0.38689 0.01331 0.472 0.00702 amh amh amh 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 arb arb arb 0.98333 0.00011 0.92437 0.00021 0.92035 0.00011 0.8381 0.0001 0.94017 0.00021 0.94828 5e-05 ast ast ast 0.97521 0.00011 0.66667 0.00317 0.97521 0.00011 0.98333 0.0 0.75472 0.00317 0.81633 0.00139 ayr ayr ayr 0.99174 0.00011 0.75472 0.00209 0.91071 0.00011 0.84615 0.0 0.81081 0.00209 0.89552 0.00075 azb azb azb azj azj azj 0.74306 0.00696 0.48163 0.00354 0.15789 0.00215 0.03008 0.00113 0.4856 0.00354 0.4876 0.00338 bam bam bam 0.49682 0.00662 0.19403 0.02275 0.5098 0.0058 0.55319 0.00433 0.20553 0.02275 0.26 0.01543 ban ban ban 0.97561 0.00011 0.79195 0.0015 0.98361 0.0 0.97521 0.0 0.83453 0.0015 0.87218 0.0007 bel bel bel 0.98333 0.0 0.34706 0.0118 0.98333 0.0 0.98333 0.0 0.35224 0.0118 0.39333 0.00964 bem bem bem 0.98333 0.00023 0.40411 0.00934 0.98333 0.00021 0.98333 0.00021 0.41844 0.00934 0.47581 0.00696 ben ben ben 1.0 0.0 0.99213 5e-05 1.0 0.0 1.0 0.0 1.0 5e-05 1.0 0.0 bho bho bho 0.78519 0.00034 0.78519 0.00016 0.78519 0.00032 0.77273 0.00021 0.79104 0.00016 0.78195 0.00011 bod bod bod 0.89091 0.00011 0.7451 0.00021 0.89091 0.00011 0.89091 0.0001 0.7451 0.00021 0.7451 0.00021 bos bos bos 0.18103 0.01038 0.18841 0.00698 0.18605 0.00805 0.14607 0.00464 0.19403 0.00698 0.20077 0.00605 bug bug bug 0.95312 0.00057 0.76129 0.00182 0.96063 0.00043 0.976 0.00021 0.80272 0.00182 0.86765 0.0008 bul bul bul 0.96 0.00057 0.9375 0.00043 0.96 0.00054 0.97561 0.00031 0.95238 0.00043 0.95238 0.00032 cat cat cat 0.93023 0.00103 0.91603 0.00059 0.95238 0.00064 0.96774 0.00041 0.92308 0.00059 0.96 0.00027 ceb ceb ceb 0.96721 0.00046 0.58416 0.00451 0.9916 0.00011 1.0 0.0 0.60825 0.00451 0.64481 0.00348 ces ces ces 0.98387 0.0 0.93846 0.00032 0.98387 0.0 0.98387 0.0 0.96063 0.00032 0.976 5e-05 cjk cjk cjk 0.92641 0.00034 0.61995 0.00724 0.92641 0.00032 0.92035 0.0001 0.63014 0.00724 0.66667 0.00563 ckb ckb ckb crh crh crh 0.97561 0.00034 0.82759 0.00134 0.98361 0.00021 0.98361 0.00021 0.86331 0.00134 0.88235 0.00086 cym cym cym 1.0 0.0 0.82667 0.0014 1.0 0.0 1.0 0.0 0.91852 0.0014 0.96124 0.00027 dan dan dan 0.85135 0.00251 0.84932 0.00113 0.91304 0.00129 0.98437 0.00021 0.88571 0.00113 0.96124 0.00021 deu deu deu 0.98745 0.00011 0.97119 0.00027 0.98745 0.00011 0.98745 0.0001 0.97521 0.00027 0.98333 0.00011 dyu dyu dyu 0.23188 0.00719 0.05594 0.00429 0.22059 0.00665 0.17323 0.00587 0.05714 0.00429 0.06452 0.00327 dzo dzo dzo 0.90769 0.00126 0.81159 0.00118 0.90769 0.00118 0.90769 0.00113 0.81159 0.00118 0.81159 0.00118 ell ell ell 0.97908 0.0 1.0 0.0 0.97908 0.0 0.97908 0.0 1.0 0.0 1.0 0.0 eng eng eng 0.85294 0.00205 0.43123 0.0081 0.87218 0.00161 0.8855 0.00134 0.46586 0.0081 0.50435 0.006 epo epo epo 0.96825 0.00046 0.63492 0.00365 0.976 0.00032 0.976 0.00031 0.69767 0.00365 0.77419 0.00182 est ekk est 0.9375 0.00091 0.33803 0.01261 1.0 0.0 1.0 0.0 0.41096 0.01261 0.55556 0.00514 eus eus eus 0.91729 0.00126 0.16901 0.0316 0.96825 0.00043 0.98387 0.00021 0.22901 0.0316 0.34091 0.01238 ewe ewe ewe 0.98361 0.00023 0.38462 0.0103 0.98361 0.00021 0.98361 0.00021 0.4 0.0103 0.43165 0.00847 fao fao fao 0.98305 0.0 0.94309 0.00027 0.98305 0.0 0.98305 0.0 0.96667 0.00027 0.98305 0.0 fij fij fij 1.0 0.0 0.9 0.00075 1.0 0.0 1.0 0.0 0.94737 0.00075 0.96923 0.00021 fin fin fin 0.36311 0.02522 0.20064 0.02694 0.38769 0.02136 0.41311 0.01844 0.23909 0.02694 0.29717 0.01597 fon fon fon 0.94118 0.00046 0.34627 0.0117 0.94915 0.00032 0.95726 0.00021 0.35692 0.0117 0.42804 0.00825 fra fra fra 0.95238 0.00068 0.95161 0.00027 0.95935 0.00043 0.9661 0.0001 0.96721 0.00027 0.98333 5e-05 fur fur fur 0.944 0.00068 0.45736 0.00746 0.95935 0.00043 0.96721 0.00031 0.46825 0.00746 0.5514 0.00509 fuv fuv fuv 0.77912 0.00217 0.52133 0.0096 0.80833 0.00107 0.81197 0.00062 0.57743 0.0096 0.69401 0.00396 gaz gaz gaz 0.83221 0.00285 0.248 0.02017 0.88235 0.0015 0.83761 0.00062 0.27991 0.02017 0.36364 0.01163 gla gla gla 0.94574 0.00023 0.57416 0.00445 0.95312 0.00011 0.96063 0.0 0.64516 0.00445 0.71429 0.00225 gle gle gle 0.95 0.00091 0.7037 0.00343 0.96815 0.00054 0.96815 0.00052 0.80423 0.00343 0.89412 0.00096 glg glg glg 0.98305 0.00011 0.88372 0.0007 0.98305 0.00011 0.98305 0.0001 0.89764 0.0007 0.91935 0.00043 grn gug grn 0.816 0.00023 0.23742 0.01964 0.82927 0.0 0.81967 0.0 0.26281 0.01964 0.30287 0.01355 guj guj guj 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 hat hat hat 0.90706 0.00285 0.41368 0.01835 0.9313 0.00193 0.95312 0.00124 0.45149 0.01835 0.51489 0.01216 hau hau hau 0.94488 0.0024 0.57508 0.01427 0.96 0.00161 0.97297 0.00103 0.6679 0.01427 0.78261 0.00536 heb heb heb 0.99145 0.00011 0.99145 5e-05 1.0 0.0 1.0 0.0 0.99145 5e-05 0.99145 5e-05 hin hin hin 0.62 0.00867 0.6359 0.00381 0.62312 0.00805 0.62944 0.00752 0.6359 0.00381 0.63918 0.00375 hrv hrv hrv 0.60302 0.00902 0.52252 0.00558 0.62176 0.00784 0.69364 0.00546 0.57426 0.00558 0.62032 0.0037 hun hun hun 0.82192 0.00297 0.56459 0.00483 0.84507 0.00236 0.89552 0.00144 0.71084 0.00483 0.81379 0.00139 hye hye hye 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 ibo ibo ibo 0.98718 0.00023 0.63115 0.00483 0.99355 0.00011 1.0 0.0 0.66379 0.00483 0.74038 0.00289 ilo ilo ilo 0.91339 0.00126 0.71166 0.00252 0.928 0.00097 0.97479 0.00031 0.8227 0.00252 0.92063 0.00054 ind ind ind 0.72483 0.00411 0.68531 0.00188 0.76056 0.00311 0.78261 0.00258 0.71014 0.00188 0.72593 0.00145 isl isl isl 0.9916 0.00011 0.9916 5e-05 0.9916 0.00011 0.9916 0.0001 0.9916 5e-05 0.9916 5e-05 ita ita ita 0.7947 0.00342 0.67416 0.00306 0.83916 0.00236 0.86957 0.00175 0.73171 0.00306 0.78431 0.00171 jav jav jav 0.97581 0.00046 0.41404 0.00553 0.97581 0.00043 0.97561 0.00031 0.46275 0.00553 0.50213 0.00284 jpn jpn jpn 0.72195 0.01301 0.64745 0.00842 0.7861 0.00848 0.79245 0.00783 0.71921 0.00842 0.77249 0.0045 kan kan kan 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 kat kat kat 1.0 0.0 0.99394 5e-05 1.0 0.0 1.0 0.0 1.0 5e-05 1.0 0.0 kaz kaz kaz 0.96721 0.00034 0.40989 0.00885 0.97521 0.00021 0.96667 0.00021 0.41281 0.00885 0.42336 0.00836 kbp kbp kbp 0.85714 0.00228 0.23438 0.02103 0.90909 0.00129 0.9375 0.00082 0.24048 0.02103 0.26786 0.01757 kea kea kea 0.72727 0.00114 0.07627 0.00896 0.72727 0.00107 0.69811 0.00093 0.08333 0.00896 0.07254 0.00675 khk khk khk 0.99225 0.0 0.5 0.00681 0.63158 0.0 0.33333 0.0 0.50593 0.00681 0.55172 0.00552 khm khm khm 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 kin kin kin 0.76336 0.0024 0.53061 0.00258 0.81967 0.00129 0.8547 0.00072 0.56522 0.00258 0.64463 0.00118 kir kir kir 0.94488 0.0008 0.35503 0.0117 0.95238 0.00064 0.96774 0.00041 0.35714 0.0117 0.39216 0.00997 kmb kmb kmb 0.99194 0.00011 0.82034 0.00268 0.99194 0.00011 0.99194 0.0001 0.86738 0.00268 0.92015 0.00096 kmr kmr kmr 0.66667 0.00673 0.61458 0.00397 0.66667 0.00633 0.66667 0.00608 0.63441 0.00397 0.64481 0.00348 knc knc knc 0.97059 0.00034 0.5641 0.00542 0.97778 0.00021 0.97778 0.00021 0.63768 0.00542 0.76301 0.00214 kon kng kon 0.40789 0.0 0.55172 0.00816 0.40789 0.0 0.39735 0.0 0.56522 0.00816 0.56742 0.00718 kor kor kor 0.94488 0.0008 0.83333 0.00129 0.96 0.00054 0.99174 0.0001 0.90909 0.00129 0.96774 0.00021 lao lao lao 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 lij lij lij 0.49785 0.01324 0.43446 0.00805 0.53953 0.01052 0.64804 0.00639 0.46032 0.00805 0.53211 0.00541\nTable 46: Comparison of GlotLID vs OpenLID on UDHR benchmark (part 1)\nwith confidence threshold \u03b8\nGlotLID-M OpenLID GlotLID-M \u03b8=.3 GlotLID-M \u03b8=.5 OpenLID \u03b8=.3 OpenLID \u03b8=.5\niso639-3 UDHR Code(s) OpenLID Code(s) F1\u2191 FPR\u2193 F1\u2191 FPR\u2193 F1\u2191 FPR\u2193 F1\u2191 FPR\u2193 F1\u2191 FPR\u2193 F1\u2191 FPR\u2193\nlin lin lin 0.99145 0.00023 0.93173 0.00091 1.0 0.0 1.0 0.0 0.95868 0.00091 0.98712 0.00011 lit lit lit 0.9375 0.00091 0.78146 0.00172 0.96774 0.00043 0.98361 0.00021 0.90769 0.00172 0.95161 0.00027 ltz ltz ltz 0.98305 0.0 0.87879 0.00075 0.98305 0.0 0.98305 0.0 0.87879 0.00075 0.90625 0.00054 lua lua lua 0.71186 0.00183 0.68263 0.00268 0.75 0.00107 0.78846 0.00031 0.72152 0.00268 0.7619 0.00166 lug lug lug 0.9589 0.00068 0.4 0.01127 0.97222 0.00043 0.98592 0.00021 0.4375 0.01127 0.49822 0.00755 lus lus lus 0.93548 0.00091 0.28087 0.01594 0.95868 0.00054 0.97479 0.00031 0.30208 0.01594 0.31868 0.01329 lvs lvs lvs 0.94118 0.00171 0.9375 0.00086 0.92623 0.00118 0.90213 0.00093 0.97959 0.00086 0.98361 0.00021 mag mag mag 0.75385 0.00046 0.76336 0.00021 0.76562 0.00021 0.76562 0.00021 0.76336 0.00021 0.75969 0.00016 mai mai mai 0.83099 0.0 0.81944 0.00011 0.83099 0.0 0.83099 0.0 0.81944 0.00011 0.81944 0.00011 mal mal mal 1.0 0.0 0.99595 5e-05 1.0 0.0 1.0 0.0 0.99595 5e-05 1.0 0.0 mar mar mar 0.99174 0.00011 0.98361 0.00011 0.99174 0.00011 0.99174 0.0001 0.99174 0.00011 0.99174 5e-05 min min min 0.88235 0.00171 0.63492 0.00365 0.91603 0.00107 0.92187 0.00082 0.69767 0.00365 0.74534 0.00214 mkd mkd mkd 0.99174 0.0 0.97561 0.00011 0.99174 0.0 0.99174 0.0 0.97561 0.00011 0.98361 5e-05 mlt mlt mlt 0.77419 0.00399 0.59 0.00435 0.78947 0.00343 0.82759 0.00258 0.68208 0.00435 0.82517 0.00129 mos mos mos 0.97015 0.00046 0.56769 0.00531 0.98485 0.00021 0.99237 0.0001 0.62201 0.00531 0.7027 0.00295 mri mri mri 0.8227 0.00023 0.26608 0.01663 0.82857 0.00011 0.82857 0.0001 0.27523 0.01663 0.2864 0.01489 mya mya mya 0.66292 0.00673 0.66667 0.00311 0.66292 0.00633 0.67045 0.00587 0.66667 0.00311 0.66667 0.00311 nld nld nld 0.70238 0.00571 0.57843 0.00461 0.71084 0.00515 0.71515 0.00484 0.68208 0.00461 0.77124 0.00188 nno nno nno 0.95868 0.00034 0.912 0.00043 0.96667 0.00021 0.9661 0.0001 0.92683 0.00043 0.94215 0.00021 nob nob nob 0.98462 0.00023 0.85906 0.00113 0.99225 0.00011 0.98438 0.0001 0.92754 0.00113 0.96241 0.00027 npi npi npi 0.98214 0.0 0.97345 5e-05 0.575 0.0 0.32353 0.0 0.97345 5e-05 0.97297 0.0 nso nso nso 0.86957 0.00205 0.80537 0.00156 0.87591 0.00182 0.88235 0.00165 0.82759 0.00156 0.83916 0.00123 nya nya nya 0.96414 0.00103 0.74462 0.00445 0.97581 0.00064 0.99588 0.0001 0.7707 0.00445 0.86121 0.00209 oci oci oci 0.41101 0.0 0.41187 0.00118 0.40516 0.0 0.38131 0.0 0.40773 0.00118 0.38838 0.00059 pan pan pan 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 pap pap pap 0.79195 0.00354 0.35224 0.01164 0.83099 0.00258 0.86765 0.00185 0.41404 0.01164 0.5514 0.00514 pes pes pes 0.65922 0.00696 0.62745 0.00247 0.63855 0.0058 0.54135 0.00391 0.63158 0.00247 0.63158 0.00241 plt plt plt 0.98182 0.0 0.92308 0.00038 0.98182 0.0 0.98182 0.0 0.95575 0.00038 0.97297 5e-05 pol pol pol 0.74074 0.00479 0.64516 0.00354 0.76923 0.00386 0.81633 0.00278 0.68966 0.00354 0.76433 0.00198 por por por 0.86331 0.00434 0.70588 0.00537 0.87273 0.00376 0.89219 0.00299 0.73171 0.00537 0.7717 0.0038 prs prs prs 0.0 0.00274 0.33333 0.00075 0.0 0.00258 0.0 0.00247 0.34483 0.00075 0.34483 0.00059 quy quy quy 0.70115 0.00593 0.10816 0.05398 0.82993 0.00268 0.88406 0.00165 0.11244 0.05398 0.11949 0.04817 ron ron ron 0.80992 0.00514 0.76078 0.00317 0.80833 0.00472 0.82906 0.00391 0.776 0.00317 0.79508 0.00257 run run run 0.88235 0.00183 0.44776 0.00794 0.88889 0.00161 0.90909 0.00124 0.50633 0.00794 0.6 0.00429 rus rus rus 0.43321 0.01792 0.37037 0.01095 0.47431 0.01427 0.51064 0.01185 0.37855 0.01095 0.41522 0.00905 sag sag sag 0.81553 0.0 0.65806 0.00231 0.80392 0.0 0.79208 0.0 0.74453 0.00231 0.81356 0.00048 san san san 0.66667 0.0 0.66376 5e-05 0.66667 0.0 0.66667 0.0 0.66376 5e-05 0.66376 5e-05 shn shn shn 0.99145 0.0 0.99145 0.0 0.99145 0.0 0.99145 0.0 0.99145 0.0 0.99145 0.0 sin sin sin 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 slk slk slk 0.87591 0.00194 0.68571 0.00295 0.9375 0.00086 0.94488 0.00072 0.75472 0.00295 0.90909 0.00064 slv slv slv 0.89552 0.0016 0.50847 0.00622 0.95238 0.00064 0.97561 0.00031 0.65934 0.00622 0.78947 0.00171 smo smo smo 1.0 0.0 0.84564 0.00123 1.0 0.0 1.0 0.0 0.85135 0.00123 0.875 0.00096 sna sna sna 0.93846 0.00091 0.41781 0.00912 0.96063 0.00054 0.98387 0.00021 0.54955 0.00912 0.7673 0.00198 som som som 0.75817 0.00422 0.44106 0.00789 0.80556 0.00301 0.89231 0.00144 0.48333 0.00789 0.59184 0.00429 sot sot sot 0.98333 0.00011 0.86667 0.00043 0.9916 0.0 0.9916 0.0 0.87395 0.00043 0.88889 0.00027 spa spa spa 0.72321 0.00685 0.69333 0.00343 0.75701 0.00537 0.78 0.00402 0.72558 0.00343 0.75362 0.00246 srd src srd 0.9916 0.0 0.63158 0.00376 0.9916 0.0 0.9916 0.0 0.66667 0.00376 0.76433 0.00198 srp srp srp 0.5124 0.00685 0.48133 0.00338 0.4958 0.00633 0.48945 0.00608 0.48333 0.00338 0.48536 0.00327 ssw ssw ssw 0.94891 0.00068 0.72626 0.00258 0.97015 0.00032 0.99237 0.0 0.76923 0.00258 0.89655 0.00075 sun sun sun 0.9697 0.00034 0.52282 0.00606 0.9771 0.00021 0.9771 0.00021 0.63317 0.00606 0.77301 0.00188 swe swe swe 0.86301 0.00228 0.89362 0.0008 0.93333 0.00097 1.0 0.0 0.96183 0.0008 0.98437 0.00011 swh swh swh 0.84956 0.00046 0.1868 0.02694 0.84685 0.00032 0.78 0.0 0.22351 0.02694 0.29517 0.01468 tam tam tam 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 tat tat tat 0.65556 0.00696 0.40972 0.00907 0.68208 0.0058 0.69822 0.00515 0.42143 0.00907 0.46457 0.00723 tel tel tel 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 tgk tgk tgk 0.67429 0.0065 0.44106 0.00783 0.92913 0.00097 0.95082 0.00052 0.45312 0.00783 0.53211 0.00541 tgl tgl tgl 0.9403 0.0008 0.58879 0.00467 0.95455 0.00054 0.97674 0.00021 0.61165 0.00467 0.6738 0.00321 tha tha tha 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 tir tir tir 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 tpi tpi tpi 0.98361 0.00011 0.47431 0.00708 0.98361 0.00011 0.98361 0.0001 0.51724 0.00708 0.56338 0.00493 tsn tsn tsn 0.98361 0.00023 0.85106 0.00113 0.99174 0.00011 0.99174 0.0001 0.90909 0.00113 0.92308 0.00054 tso tso tso 0.94158 0.0016 0.63205 0.00875 0.94158 0.0015 0.95139 0.00113 0.64965 0.00875 0.68293 0.00696 tuk tuk tuk 0.94821 0.00148 0.62564 0.0008 0.96748 0.00086 0.98347 0.00041 0.65591 0.0008 0.66667 0.00016 tur tur tur 0.4918 0.01415 0.34384 0.01229 0.50209 0.01277 0.51502 0.01164 0.35928 0.01229 0.37975 0.0105 twi twi twi 0.95349 0.00137 0.5234 0.01202 0.96094 0.00107 0.9685 0.00082 0.55034 0.01202 0.60891 0.00847 tzm tzm tzm 0.01754 0.00593 0.01653 0.00317 0.01754 0.00558 0.0177 0.00525 0.01653 0.00317 0.01653 0.00316 uig uig uig 0.90295 0.00114 0.66667 0.0 0.93333 0.0 0.89908 0.0 0.66667 0.0 0.66667 0.0 ukr ukr ukr 0.98361 0.00023 0.96 0.00027 0.98361 0.00021 0.99174 0.0001 0.96 0.00027 0.98361 0.00011 umb umb umb 0.87931 0.00114 0.85409 0.0022 0.87931 0.00107 0.87611 0.00072 0.86957 0.0022 0.90226 0.00139 urd urd urd 0.96522 0.0008 0.61838 0.0073 0.96522 0.00075 0.96522 0.00072 0.62011 0.0073 0.62535 0.00707 uzn uzn uzn 0.50407 0.0073 0.18072 0.02597 0.53881 0.00429 0.56716 0.00247 0.21277 0.02597 0.27523 0.01372 vec vec vec 0.92308 0.00114 0.76923 0.00193 0.95238 0.00064 0.96774 0.00041 0.7947 0.00193 0.89552 0.00075 vie vie vie 0.66304 0.00011 0.61616 0.0008 0.66304 0.00011 0.66304 0.0001 0.62887 0.0008 0.63874 0.00043 war war war 0.9916 0.0 0.90909 0.00064 0.9916 0.0 0.9916 0.0 0.9375 0.00064 0.94488 0.00038 wol wol wol 0.79747 0.00365 0.39117 0.0103 0.86897 0.00204 0.92537 0.00093 0.45756 0.0103 0.56881 0.00498 xho xho xho 0.93846 0.00091 0.62245 0.00397 0.96825 0.00043 0.976 0.00031 0.69714 0.00397 0.78205 0.00182 ydd ydd ydd 0.99187 0.0 0.99187 0.0 0.99187 0.0 0.99187 0.0 yor yor yor 0.85106 0.0024 0.33898 0.01256 0.88889 0.00161 0.93023 0.00093 0.41667 0.01256 0.57692 0.00471 zho cjy/hak/cmn/hsn/yue/gan/wuu/nan zho/yue 0.96345 0.00011 0.62049 0.04561 0.96408 0.0 0.96408 0.0 0.71366 0.04561 0.8037 0.01645 zul zul zul 0.98305 0.0 0.58883 0.00424 0.98305 0.0 0.98305 0.0 0.61053 0.00424 0.65169 0.00321\nTable 47: Comparison of GlotLID vs OpenLID on UDHR benchmark (part 2)\nwith confidence threshold \u03b8\nGlotLID-M NLLB GlotLID-M \u03b8=.3 GlotLID-M \u03b8=.5 NLLB \u03b8=.3 NLLB \u03b8=.5\niso639-3 UDHR Code(s) NLLB Code(s) F1\u2191 FPR\u2193 F1\u2191 FPR\u2193 F1\u2191 FPR\u2193 F1\u2191 FPR\u2193 F1\u2191 FPR\u2193 F1\u2191 FPR\u2193\nabk abk abk 0.98333 6e-05 0.9916 6e-05 0.9916 0.0 ace ace ace 0.90909 0.00126 0.53153 0.0057 0.91603 0.00107 0.91603 0.00103 0.57005 0.0057 0.62434 0.0038 ady ady ady 0.83495 0.0 0.63441 0.00375 0.83495 0.0 0.83495 0.0 0.65193 0.00375 0.67045 0.00314 afr afr afr 0.95238 0.00068 0.90769 0.00062 0.96774 0.00043 0.97561 0.00031 0.91473 0.00062 0.93651 0.00039 aka fat/twi aka/twi 1.0 0.0 0.60884 0.01264 1.0 0.0 1.0 0.0 0.65209 0.01264 0.72032 0.00743 als als als 0.86131 0.00205 0.43066 0.00867 0.7377 0.00182 0.56863 0.00134 0.48163 0.00867 0.58416 0.00457 alt alt alt 0.95495 0.00034 0.76259 0.00173 0.96364 0.00021 0.96364 0.00021 0.76259 0.00173 0.80303 0.00132 amh amh amh 1.0 0.0 0.88189 6e-05 1.0 0.0 1.0 0.0 0.7156 6e-05 0.6 0.0 arb arb arb 0.98333 0.00011 0.60914 0.00431 0.92035 0.00011 0.8381 0.0001 0.76433 0.00431 0.88235 0.00088 arn arn arn 0.93913 0.00011 0.75676 0.00179 0.94737 0.0 0.94737 0.0 0.86154 0.00179 0.9322 0.00017 ast ast ast 0.97521 0.00011 0.57561 0.00475 0.97521 0.00011 0.98333 0.0 0.76129 0.00475 0.86765 0.00088 ayr ayr ayr 0.99174 0.00011 0.69461 0.00274 0.91071 0.00011 0.84615 0.0 0.76821 0.00274 0.87218 0.00083 azb azb azb azj azj azj 0.74306 0.00696 0.42754 0.00542 0.15789 0.00215 0.03008 0.00113 0.45736 0.00542 0.4739 0.00385 bam bam bam 0.49682 0.00662 0.20961 0.01957 0.5098 0.0058 0.55319 0.00433 0.22857 0.01957 0.27666 0.01315 ban ban ban 0.97561 0.00011 0.7044 0.00229 0.98361 0.0 0.97521 0.0 0.77241 0.00229 0.82963 0.00094 bel bel bel 0.98333 0.0 0.80272 0.00151 0.98333 0.0 0.98333 0.0 0.81379 0.00151 0.88722 0.00072 bem bem bem 0.98333 0.00023 0.40714 0.00917 0.98333 0.00021 0.98333 0.00021 0.41455 0.00917 0.46914 0.00699 ben ben ben 1.0 0.0 0.992 0.0 1.0 0.0 1.0 0.0 0.992 0.0 0.992 0.0 bho bho bho 0.78519 0.00034 0.66116 0.00011 0.78519 0.00032 0.77273 0.00021 0.66116 0.00011 0.66116 0.00011 bis bis bis 1.0 0.0 0.71006 0.00274 1.0 0.0 1.0 0.0 0.71856 0.00274 0.75949 0.00209 bod bod bod 0.89091 0.00011 0.61224 0.00425 0.89091 0.00011 0.89091 0.0001 0.72727 0.00425 0.77922 0.00187 bos bos bos 0.18103 0.01039 0.14857 0.01141 0.18605 0.00805 0.14607 0.00464 0.16456 0.01141 0.18056 0.00781 bug bug bug 0.95312 0.00057 0.7439 0.00229 0.96063 0.00043 0.976 0.00021 0.82432 0.00229 0.91729 0.00055 bul bul bul 0.96 0.00057 0.95868 0.00017 0.96 0.00054 0.97561 0.00031 0.95868 0.00017 0.97479 6e-05 cat cat cat 0.93023 0.00103 0.49793 0.00677 0.95238 0.00064 0.96774 0.00041 0.63492 0.00677 0.81633 0.00149 ceb ceb ceb 0.96721 0.00046 0.57143 0.00481 0.9916 0.00011 1.0 0.0 0.59487 0.00481 0.61702 0.00391 ces ces ces 0.98387 0.0 0.79487 0.00173 0.98387 0.0 0.98387 0.0 0.89855 0.00173 0.94656 0.00033 chv chv chv 0.86154 0.0 0.8 0.00028 0.86154 0.0 0.86154 0.0 0.82353 0.00028 0.83582 0.00011 cjk cjk cjk 0.92641 0.00034 0.56296 0.00951 0.92641 0.00032 0.92035 0.0001 0.57431 0.00951 0.6137 0.00726 ckb ckb ckb crh crh crh 0.97561 0.00034 0.9375 0.00045 0.98361 0.00021 0.98361 0.00021 0.96 0.00045 1.0 0.0 cym cym cym 1.0 0.0 0.77215 0.00196 1.0 0.0 1.0 0.0 0.82432 0.00196 0.87143 0.00094 dan dan dan 0.85135 0.00251 0.87143 0.00089 0.91304 0.00129 0.98437 0.00021 0.96825 0.00089 0.96825 0.00011 deu deu deu 0.98745 0.00011 0.64 0.00755 0.98745 0.00011 0.98745 0.0001 0.73846 0.00755 0.79208 0.00347 dyu dyu dyu 0.23188 0.00719 0.04167 0.00721 0.22059 0.00666 0.17323 0.00587 0.04396 0.00721 0.02581 0.00517 dzo dzo dzo 0.90769 0.00126 0.68132 0.0 0.90769 0.00118 0.90769 0.00113 0.68132 0.0 0.58824 0.0 ell ell ell 0.97908 0.0 0.98333 0.0 0.97908 0.0 0.97908 0.0 0.97479 0.0 0.97479 0.0 eng eng eng 0.85294 0.00205 0.34188 0.01292 0.87218 0.00161 0.8855 0.00134 0.39867 0.01292 0.46693 0.00754 epo epo epo 0.96825 0.00046 0.31202 0.01504 0.976 0.00032 0.976 0.00031 0.36858 0.01504 0.44203 0.00847 est ekk est 0.9375 0.00091 0.27778 0.01745 1.0 0.0 1.0 0.0 0.3252 0.01745 0.43165 0.00869 eus eus eus 0.91729 0.00126 0.67052 0.00302 0.96825 0.00043 0.98387 0.00021 0.81119 0.00302 0.91339 0.00044 ewe ewe ewe 0.98361 0.00023 0.36137 0.01135 0.98361 0.00021 0.98361 0.00021 0.4 0.01135 0.44106 0.00798 fao fao fao 0.98305 0.0 0.64444 6e-05 0.98305 0.0 0.98305 0.0 0.64444 6e-05 0.65169 0.0 fij fij fij 1.0 0.0 0.96825 0.00011 1.0 0.0 1.0 0.0 0.96825 0.00011 0.976 6e-05 fin fin fin 0.36311 0.02522 0.2344 0.02259 0.38769 0.02136 0.41311 0.01844 0.27434 0.02259 0.33155 0.0137 fon fon fon 0.94118 0.00046 0.40702 0.00939 0.94915 0.00032 0.95726 0.00021 0.41135 0.00939 0.48333 0.00677 fra fra fra 0.95238 0.00068 0.58252 0.00481 0.95935 0.00043 0.9661 0.0001 0.6383 0.00481 0.68966 0.00297 fur fur fur 0.944 0.00068 0.78146 0.00179 0.95935 0.00043 0.96721 0.00031 0.80822 0.00179 0.92187 0.0005 fuv fuv fuv 0.77912 0.00217 0.4793 0.01208 0.80833 0.00107 0.81197 0.00062 0.53528 0.01208 0.66066 0.00495 gaz gaz gaz 0.83221 0.00285 0.41892 0.00962 0.88235 0.0015 0.83761 0.00062 0.43357 0.00962 0.49206 0.00704 gla gla gla 0.94574 0.00023 0.9375 0.00011 0.95312 0.00011 0.96063 0.0 0.9375 0.00011 0.95238 0.0 gle gle gle 0.95 0.00091 0.90909 0.00078 0.96815 0.00054 0.96815 0.00052 0.98684 0.00078 0.99338 0.0 glg glg glg 0.98305 0.00011 0.65169 0.00341 0.98305 0.00011 0.98305 0.0001 0.76821 0.00341 0.90625 0.00061 grn gug grn 0.816 0.00023 0.28409 0.01286 0.82927 0.0 0.81967 0.0 0.36765 0.01286 0.53191 0.00363 guj guj guj 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 hat hat hat 0.90706 0.00285 0.68768 0.00598 0.9313 0.00193 0.95312 0.00124 0.69971 0.00598 0.71856 0.00506 hau hau hau 0.94488 0.0024 0.83256 0.00397 0.96 0.00161 0.97297 0.00103 0.87745 0.00397 0.93229 0.00138 heb heb heb 0.99145 0.00011 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 hin hin hin 0.62 0.00867 0.6359 0.00397 0.62312 0.00805 0.62944 0.00752 0.64583 0.00397 0.64583 0.00374 hrv hrv hrv 0.60302 0.00902 0.42636 0.008 0.62176 0.00784 0.69364 0.00546 0.51643 0.008 0.58511 0.00402 hun hun hun 0.82192 0.00297 0.22814 0.0227 0.84507 0.00236 0.89552 0.00144 0.3183 0.0227 0.46332 0.00765 hye hye hye 1.0 0.0 0.97778 0.0 1.0 0.0 1.0 0.0 0.97778 0.0 0.97778 0.0 ibo ibo ibo 0.98718 0.00023 0.50877 0.0052 0.99355 0.00011 1.0 0.0 0.52968 0.0052 0.59184 0.00336 ilo ilo ilo 0.91339 0.00126 0.7651 0.0019 0.928 0.00097 0.97479 0.00031 0.83824 0.0019 0.93443 0.00039 ind ind ind 0.72483 0.00411 0.397 0.00867 0.76056 0.00311 0.78261 0.00258 0.53807 0.00867 0.69799 0.00209 isl isl isl 0.9916 0.00011 0.76129 0.00207 0.9916 0.00011 0.9916 0.0001 0.80272 0.00207 0.80272 0.0016 ita ita ita 0.7947 0.00342 0.44195 0.00822 0.83916 0.00236 0.86957 0.00175 0.52444 0.00822 0.6178 0.00391 jav jav jav 0.97581 0.00046 0.36646 0.00783 0.97581 0.00043 0.97561 0.00031 0.41696 0.00783 0.47012 0.0038 jpn jpn jpn 0.72195 0.01301 0.73 0.00593 0.7861 0.00848 0.79245 0.00783 0.82486 0.00593 0.85373 0.00242 kal kal kal 0.98305 0.0 0.76 0.00185 0.98305 0.0 0.98305 0.0 0.83824 0.00185 0.94215 0.00022 kan kan kan 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 kat kat kat 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 kaz kaz kaz 0.96721 0.00034 0.42182 0.00878 0.97521 0.00021 0.96667 0.00021 0.42182 0.00878 0.42491 0.00853 kbp kbp kbp 0.85714 0.00228 0.27685 0.01683 0.90909 0.00129 0.9375 0.00082 0.28019 0.01683 0.30851 0.0142 kea kea kea 0.72727 0.00114 0.08571 0.00789 0.72727 0.00107 0.69811 0.00093 0.09 0.00789 0.07527 0.00655 khk khk khk 0.99225 0.0 0.71591 0.00268 0.63158 0.0 0.33333 0.0 0.73256 0.00268 0.82895 0.00132 khm khm khm 1.0 0.0 0.96825 0.00022 1.0 0.0 1.0 0.0 1.0 0.00022 1.0 0.0 kin kin kin 0.76336 0.0024 0.82258 0.00073 0.81967 0.00129 0.8547 0.00072 0.82927 0.00073 0.84298 0.00055 kir kir kir 0.94488 0.0008 0.57416 0.00498 0.95238 0.00064 0.96774 0.00041 0.58537 0.00498 0.625 0.00396 kmb kmb kmb 0.99194 0.00011 0.82562 0.00229 0.99194 0.00011 0.99194 0.0001 0.85926 0.00229 0.928 0.00055 kmr kmr kmr 0.66667 0.00673 0.47773 0.00721 0.66667 0.00633 0.66667 0.00608 0.50862 0.00721 0.54378 0.00545 knc knc knc 0.97059 0.00034 0.50187 0.00744 0.97778 0.00021 0.97778 0.00021 0.57021 0.00744 0.71277 0.00297 kon kng kon 0.40789 0.0 0.45399 0.00733 0.40789 0.0 0.39735 0.0 0.4486 0.00733 0.43226 0.00671 kor kor kor 0.94488 0.0008 0.51064 0.00643 0.96 0.00054 0.99174 0.0001 0.64516 0.00643 0.67416 0.00319 lao lao lao 1.0 0.0 0.992 6e-05 1.0 0.0 1.0 0.0 1.0 6e-05 1.0 0.0\nTable 48: Comparison of GlotLID vs NLLB on UDHR benchmark (part 1)\nwith confidence threshold \u03b8\nGlotLID-M NLLB GlotLID-M \u03b8=.3 GlotLID-M \u03b8=.5 NLLB \u03b8=.3 NLLB \u03b8=.5\niso639-3 UDHR Code(s) NLLB Code(s) F1\u2191 FPR\u2193 F1\u2191 FPR\u2193 F1\u2191 FPR\u2193 F1\u2191 FPR\u2193 F1\u2191 FPR\u2193 F1\u2191 FPR\u2193\nlij lij lij 0.49785 0.01324 0.56701 0.00447 0.53953 0.01052 0.64804 0.00639 0.57592 0.00447 0.67485 0.0027 lin lin lin 0.99145 0.00023 0.94561 0.00056 1.0 0.0 1.0 0.0 0.96996 0.00056 0.98261 6e-05 lit lit lit 0.9375 0.00091 0.68208 0.00302 0.96774 0.00043 0.98361 0.00021 0.74684 0.00302 0.90076 0.00066 ltz ltz ltz 0.98305 0.0 0.60733 0.00408 0.98305 0.0 0.98305 0.0 0.63388 0.00408 0.67836 0.00292 lua lua lua 0.71186 0.00183 0.61017 0.00352 0.75 0.00107 0.78846 0.00031 0.63158 0.00352 0.69231 0.00231 lug lug lug 0.9589 0.00068 0.37398 0.01286 0.97222 0.00043 0.98592 0.00021 0.39205 0.01286 0.42724 0.01013 lus lus lus 0.93548 0.00091 0.33238 0.01303 0.95868 0.00054 0.97479 0.00031 0.33623 0.01303 0.34421 0.01216 lvs lvs lvs 0.94118 0.00171 0.95582 0.00056 0.92623 0.00118 0.90213 0.00093 0.97143 0.00056 0.98347 0.00017 mag mag mag 0.75385 0.00046 0.61635 0.00185 0.76562 0.00021 0.76562 0.00021 0.62821 0.00185 0.66216 0.00121 mai mai mai 0.83099 0.0 0.80537 0.00034 0.83099 0.0 0.83099 0.0 0.80537 0.00034 0.81081 0.00028 mal mal mal 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 0.975 0.0 mar mar mar 0.99174 0.00011 0.91603 0.00062 0.99174 0.00011 0.99174 0.0001 1.0 0.00062 1.0 0.0 min min min 0.88235 0.00171 0.22857 6e-05 0.91603 0.00107 0.92187 0.00082 0.23188 6e-05 0.20588 0.0 mkd mkd mkd 0.99174 0.0 0.99174 0.0 0.99174 0.0 0.99174 0.0 0.99174 0.0 0.99174 0.0 mlt mlt mlt 0.77419 0.00399 0.78146 0.00179 0.78947 0.00343 0.82759 0.00258 0.83099 0.00179 0.88722 0.00077 mos mos mos 0.97015 0.00046 0.75581 0.00235 0.98485 0.00021 0.99237 0.0001 0.78313 0.00235 0.81761 0.0016 mri mri mri 0.8227 0.00023 0.35913 0.01029 0.82857 0.00011 0.82857 0.0001 0.37061 0.01029 0.40138 0.00825 mya mya mya 0.66292 0.00673 0.67416 0.00324 0.66292 0.00633 0.67045 0.00587 0.68182 0.00324 0.7362 0.00237 nav nav nav 0.9916 0.00011 0.57282 0.00492 1.0 0.0 1.0 0.0 0.58706 0.00492 0.65556 0.00341 nld nld nld 0.70238 0.00571 0.59184 0.00442 0.71084 0.00515 0.71515 0.00484 0.65909 0.00442 0.6988 0.0027 nno nno nno 0.95868 0.00034 0.8855 0.00073 0.96667 0.00021 0.9661 0.0001 0.95082 0.00073 0.97479 6e-05 nob nob nob 0.98462 0.00023 0.96183 0.00022 0.99225 0.00011 0.98438 0.0001 0.98438 0.00022 0.98438 6e-05 npi npi npi 0.98214 0.0 0.88189 0.00078 0.575 0.0 0.32353 0.0 0.96552 0.00078 0.97391 0.00011 nso nso nso 0.86957 0.00205 0.86131 0.00101 0.87591 0.00182 0.88235 0.00165 0.86765 0.00101 0.91473 0.00055 nya nya nya 0.96414 0.00103 0.79333 0.00336 0.97581 0.00064 0.99588 0.0001 0.83509 0.00336 0.91892 0.00105 oci oci oci 0.41101 0.0 0.41688 0.0104 0.40516 0.0 0.38131 0.0 0.45698 0.0104 0.4357 0.00462 oss oss oss 0.50273 0.00696 0.71875 0.00034 0.50549 0.00644 0.50829 0.00608 0.74797 0.00034 0.7541 0.0 pan pan pan 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 pap pap pap 0.79195 0.00354 0.65537 0.00336 0.83099 0.00258 0.86765 0.00185 0.7205 0.00336 0.8227 0.00132 pcm pcm pcm 0.71739 0.0 0.84112 0.00017 0.71739 0.0 0.71739 0.0 0.84906 0.00017 0.84906 0.00011 pes pes pes 0.65922 0.00696 0.64407 0.00341 0.63855 0.0058 0.54135 0.00392 0.65143 0.00341 0.65143 0.00325 plt plt plt 0.98182 0.0 0.88525 0.00067 0.98182 0.0 0.98182 0.0 0.90756 0.00067 0.93913 0.00028 pol pol pol 0.74074 0.00479 0.41404 0.00928 0.76923 0.00386 0.81633 0.00278 0.4856 0.00928 0.60513 0.00418 por por por 0.86331 0.00434 0.73171 0.00492 0.87273 0.00376 0.89219 0.00299 0.81081 0.00492 0.84806 0.00237 prs prs prs 0.0 0.00274 0.08824 0.00022 0.0 0.00258 0.0 0.00247 0.08824 0.00022 0.08955 0.00017 quy quy quy 0.70115 0.00593 0.09147 0.06543 0.82993 0.00268 0.88406 0.00165 0.09992 0.06543 0.11018 0.05233 roh roh roh 0.99268 0.0 0.97515 0.00112 0.99145 0.0 0.98775 0.0 0.98095 0.00112 0.9892 0.00044 ron ron ron 0.80992 0.00514 0.81667 0.0024 0.80833 0.00472 0.82906 0.00392 0.83051 0.0024 0.85965 0.00171 run run run 0.88235 0.00183 0.71951 0.00252 0.88889 0.00161 0.90909 0.00124 0.75159 0.00252 0.81944 0.00138 rus rus rus 0.43321 0.01792 0.43321 0.00878 0.47431 0.01428 0.51064 0.01185 0.45455 0.00878 0.58824 0.00462 sag sag sag 0.81553 0.0 0.21176 0.00084 0.80392 0.0 0.79208 0.0 0.225 0.00084 0.23377 0.00039 san san san 0.66667 0.0 0.67521 0.00017 0.66667 0.0 0.66667 0.0 0.67249 0.00017 0.67249 0.0 shn shn shn 0.99145 0.0 1.0 0.0 0.99145 0.0 0.99145 0.0 1.0 0.0 1.0 0.0 sin sin sin 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 slk slk slk 0.87591 0.00194 0.85926 0.00095 0.9375 0.00086 0.94488 0.00072 0.89231 0.00095 0.93548 0.00033 slv slv slv 0.89552 0.0016 0.71006 0.00274 0.95238 0.00064 0.97561 0.00031 0.78947 0.00274 0.85714 0.0011 smo smo smo 1.0 0.0 0.68182 0.00296 1.0 0.0 1.0 0.0 0.70175 0.00296 0.76433 0.00187 sna sna sna 0.93846 0.00091 0.46964 0.00716 0.96063 0.00054 0.98387 0.00021 0.57711 0.00716 0.69461 0.00264 som som som 0.75817 0.00422 0.54286 0.00531 0.80556 0.00301 0.89231 0.00144 0.61957 0.00531 0.7451 0.00209 sot sot sot 0.98333 0.00011 0.69118 0.00162 0.9916 0.0 0.9916 0.0 0.72868 0.00162 0.78333 0.00072 spa spa spa 0.72321 0.00685 0.39709 0.01387 0.75701 0.00537 0.78 0.00402 0.43968 0.01387 0.51923 0.00814 srd src srd 0.9916 0.0 0.86567 0.00089 0.9916 0.0 0.9916 0.0 0.95082 0.00089 0.97479 6e-05 srp srp srp 0.5124 0.00685 0.48739 0.00336 0.4958 0.00633 0.48945 0.00608 0.48945 0.00336 0.49153 0.00319 ssw ssw ssw 0.94891 0.00068 0.80272 0.00123 0.97015 0.00032 0.99237 0.0 0.86131 0.00123 0.90769 0.00028 sun sun sun 0.9697 0.00034 0.75294 0.00229 0.9771 0.00021 0.9771 0.00021 0.84211 0.00229 0.9078 0.00066 swe swe swe 0.86301 0.00228 0.81579 0.00151 0.93333 0.00097 1.0 0.0 0.95385 0.00151 0.98413 6e-05 swh swh swh 0.84956 0.00046 0.24742 0.02036 0.84685 0.00032 0.78 0.0 0.29268 0.02036 0.35821 0.01178 tah tah tah 0.91892 0.00011 0.89922 0.00067 0.91892 0.00011 0.92727 0.0 0.92063 0.00067 0.98305 6e-05 tam tam tam 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 tat tat tat 0.65556 0.00696 0.46457 0.00755 0.68208 0.0058 0.69822 0.00515 0.46825 0.00755 0.48163 0.00693 tel tel tel 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 tgk tgk tgk 0.67429 0.00651 0.47581 0.00727 0.92913 0.00097 0.95082 0.00052 0.50213 0.00727 0.59296 0.00446 tgl tgl tgl 0.9403 0.0008 0.5368 0.00587 0.95455 0.00054 0.97674 0.00021 0.54148 0.00587 0.56364 0.00517 tha tha tha 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 tir tir tir 1.0 0.0 0.97561 0.00017 1.0 0.0 1.0 0.0 0.99174 0.00017 1.0 0.0 ton ton ton 1.0 0.0 1.0 0.0 1.0 0.0 tpi tpi tpi 0.98361 0.00011 0.91339 0.00045 0.98361 0.00011 0.98361 0.0001 0.94309 0.00045 0.95868 0.00011 tsn tsn tsn 0.98361 0.00023 0.79452 0.00157 0.99174 0.00011 0.99174 0.0001 0.85926 0.00157 0.89922 0.00061 tso tso tso 0.94158 0.0016 0.64073 0.00878 0.94158 0.0015 0.95139 0.00113 0.66986 0.00878 0.70886 0.00633 tuk tuk tuk 0.94821 0.00148 0.65574 0.00022 0.96748 0.00086 0.98347 0.00041 0.65574 0.00022 0.65574 0.00022 tur tur tur 0.4918 0.01415 0.32967 0.01365 0.50209 0.01277 0.51502 0.01164 0.37037 0.01365 0.39604 0.01007 tzm tzm tzm 0.01754 0.00593 0.01653 0.0033 0.01754 0.00558 0.0177 0.00525 0.01653 0.0033 0.01653 0.00325 uig uig uig 0.90295 0.00114 0.66667 0.0 0.93333 0.0 0.89908 0.0 0.66667 0.0 0.66667 0.0 ukr ukr ukr 0.98361 0.00023 0.91339 0.0005 0.98361 0.00021 0.99174 0.0001 0.94309 0.0005 0.97479 6e-05 umb umb umb 0.87931 0.00114 0.89313 0.0014 0.87931 0.00107 0.87611 0.00072 0.91051 0.0014 0.94737 0.00055 urd urd urd 0.96522 0.0008 0.61838 0.00761 0.96522 0.00075 0.96522 0.00072 0.63068 0.00761 0.63068 0.0071 uzn uzn uzn 0.50407 0.0073 0.28365 0.01325 0.53881 0.00429 0.56716 0.00247 0.33908 0.01325 0.41404 0.00583 vec vec vec 0.92308 0.00114 0.85714 0.00112 0.95238 0.00064 0.96774 0.00041 0.87591 0.00112 0.90909 0.00066 vie vie vie 0.66304 0.00011 0.45675 0.00565 0.66304 0.00011 0.66304 0.0001 0.56621 0.00565 0.61307 0.00088 war war war 0.9916 0.0 0.75159 0.00213 0.9916 0.0 0.9916 0.0 0.78146 0.00213 0.81379 0.00143 wol wol wol 0.79747 0.00365 0.23985 0.02192 0.86897 0.00204 0.92537 0.00093 0.2684 0.02192 0.33973 0.01321 xho xho xho 0.93846 0.00091 0.64835 0.00347 0.96825 0.00043 0.976 0.00031 0.68605 0.00347 0.74214 0.00215 ydd ydd ydd 0.99187 0.0 1.0 0.0 1.0 0.0 1.0 0.0 yor yor yor 0.85106 0.0024 0.44697 0.00811 0.88889 0.00161 0.93023 0.00093 0.50213 0.00811 0.63784 0.00363 zho cjy/hak/cmn/hsn/yue/gan/wuu/nan zho 0.96345 0.00011 0.54715 0.05559 0.96408 0.0 0.96408 0.0 0.64193 0.05559 0.56574 0.02179 zul zul zul 0.98305 0.0 0.59487 0.00431 0.98305 0.0 0.98305 0.0 0.61702 0.00431 0.64088 0.00347\nTable 49: Comparison of GlotLID vs NLLB on UDHR benchmark (part 2)"
        }
    ],
    "title": "GlotLID: Language Identification for Low-Resource Languages",
    "year": 2023
}