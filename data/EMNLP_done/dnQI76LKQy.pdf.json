{
    "abstractText": "Metaphor is a pervasive aspect of human communication, and its presence in multimodal forms has become more prominent with the progress of mass media. However, there is a scarcity of research focusing on multimodal metaphor resources in languages other than English, despite the evident variations in how different languages express multimodal metaphors. Furthermore, the existing work in natural language processing does not address the exploration of categorizing the source and target domains in metaphors. This omission is significant considering the extensive research conducted in the fields of cognitive linguistics, which emphasizes that a profound understanding of metaphor relies on recognizing the differences and similarities between domain categories. We, therefore, introduce MultiCMET, a multimodal Chinese metaphor dataset, consisting of 13,820 text-image pairs of advertisements with manual annotations of the occurrence of metaphors, source and target domain categories, and sentiments metaphors convey. Furthermore, we have developed a domain lexicon that encompasses categorizations of domains and corresponding examples. We propose the Cascading Domain Knowledge Integration (CDKI) benchmark, which utilizes domain-specific features from the domain lexicon to enhance the understanding of metaphors. Experimental results demonstrate the effectiveness of CDKI. The dataset and code are publicly available. 1",
    "authors": [
        {
            "affiliations": [],
            "name": "Dongyu Zhang"
        },
        {
            "affiliations": [],
            "name": "Jingwei Yu"
        },
        {
            "affiliations": [],
            "name": "Jinsen Yuan"
        },
        {
            "affiliations": [],
            "name": "Liang Yang"
        },
        {
            "affiliations": [],
            "name": "Hongfei Lin"
        }
    ],
    "id": "SP:918ab1ddd71b8e1439348a06f7ab9070cb68c94d",
    "references": [
        {
            "authors": [
                "Muhammad Abulaish",
                "Ashraf Kamal",
                "Mohammed J Zaki."
            ],
            "title": "A survey of figurative language and its computational detection in online social networks",
            "venue": "ACM Transactions on the Web (TWEB), 14(1):1\u201352.",
            "year": 2020
        },
        {
            "authors": [
                "John A. Barnden",
                "Mark G. Lee."
            ],
            "title": "An artificial intelligence approach to metaphor understanding",
            "venue": "Theoria et Historia Scientiarum, 6(1):399\u2013412.",
            "year": 2007
        },
        {
            "authors": [
                "Julia Birke",
                "Anoop Sarkar."
            ],
            "title": "A clustering approach for nearly unsupervised recognition of nonliteral language",
            "venue": "11th Conference of the European",
            "year": 2006
        },
        {
            "authors": [
                "Efrat Blaier",
                "Itzik Malkiel",
                "Lior Wolf."
            ],
            "title": "Caption enriched samples for improving hateful memes detection",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 9350\u20139358, Online and Punta Cana, Do-",
            "year": 2021
        },
        {
            "authors": [
                "Yitao Cai",
                "Huiyu Cai",
                "Xiaojun Wan."
            ],
            "title": "Multimodal sarcasm detection in Twitter with hierarchical fusion model",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2506\u20132515, Florence, Italy. Associa-",
            "year": 2019
        },
        {
            "authors": [
                "Minjin Choi",
                "Sunkyung Lee",
                "Eunseong Choi",
                "Heesoo Park",
                "Junhyuk Lee",
                "Dongwon Lee",
                "Jongwuk Lee."
            ],
            "title": "MelBERT: Metaphor detection via contextualized late interaction using metaphorical identification theories",
            "venue": "Proceedings of the 2021 Conference of",
            "year": 2021
        },
        {
            "authors": [
                "Francesca M.M. Citron",
                "Adele E. Goldberg."
            ],
            "title": "Metaphorical Sentences Are More Emotionally Engaging than Their Literal Counterparts",
            "venue": "Journal of Cognitive Neuroscience, 26(11):2585\u20132595.",
            "year": 2014
        },
        {
            "authors": [
                "Seana Coulson",
                "Cyma Van Petten."
            ],
            "title": "Conceptual integration and metaphor: An event-related potential study",
            "venue": "Memory & cognition, 30(6):958\u2013968.",
            "year": 2002
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "arXiv preprint arXiv:1810.04805.",
            "year": 2018
        },
        {
            "authors": [
                "Yuning Du",
                "Chenxia Li",
                "Ruoyu Guo",
                "Xiaoting Yin",
                "Weiwei Liu",
                "Jun Zhou",
                "Yifan Bai",
                "Zilin Yu",
                "Yehua Yang",
                "Qingqing Dang",
                "Hongya Wang."
            ],
            "title": "Ppocr: A practical ultra lightweight ocr system",
            "venue": "ArXiv, abs/2009.09941.",
            "year": 2020
        },
        {
            "authors": [
                "Joseph L Fleiss."
            ],
            "title": "Measuring nominal scale agreement among many raters",
            "venue": "Psychological bulletin, 76(5):378.",
            "year": 1971
        },
        {
            "authors": [
                "Charles Forceville."
            ],
            "title": "Visual and multimodal metaphor in advertising: Cultural perspectives",
            "venue": "Styles of Communication, 9(2).",
            "year": 2017
        },
        {
            "authors": [
                "Mengshi Ge",
                "Rui Mao",
                "Erik Cambria."
            ],
            "title": "Explainable metaphor identification inspired by conceptual metaphor theory",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, 36(10):10681\u2013 10689.",
            "year": 2022
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun."
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013 778.",
            "year": 2016
        },
        {
            "authors": [
                "Sepp Hochreiter",
                "J\u00fcrgen Schmidhuber."
            ],
            "title": "Long short-term memory",
            "venue": "Neural Computation, 9(8):1735\u2013 1780.",
            "year": 1997
        },
        {
            "authors": [
                "Gitit Kehat",
                "James Pustejovsky."
            ],
            "title": "Improving neural metaphor detection with visual datasets",
            "venue": "Proceedings of the Twelfth Language Resources and Evaluation Conference, pages 5928\u20135933, Marseille, France. European Language Resources Association.",
            "year": 2020
        },
        {
            "authors": [
                "Diederik P Kingma",
                "Jimmy Ba."
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "arXiv preprint arXiv:1412.6980.",
            "year": 2014
        },
        {
            "authors": [
                "Walter Kintsch."
            ],
            "title": "Metaphor comprehension: A computational theory",
            "venue": "Psychonomic bulletin & review, 7(2):257\u2013266.",
            "year": 2000
        },
        {
            "authors": [
                "Zolt\u00e1n K\u00f6vecses."
            ],
            "title": "Metaphor, language, and culture",
            "venue": "DELTA: Documenta\u00e7\u00e3o de Estudos em Ling\u00fc\u00edstica Te\u00f3rica e Aplicada, 26:739\u2013757.",
            "year": 2010
        },
        {
            "authors": [
                "Kevin Kuo",
                "Marine Carpuat."
            ],
            "title": "Evaluating a Bi-LSTM model for metaphor detection in TOEFL essays",
            "venue": "Proceedings of the Second Workshop on Figurative Language Processing, pages 192\u2013196, Online. Association for Computational Linguistics.",
            "year": 2020
        },
        {
            "authors": [
                "George Lakoff."
            ],
            "title": "Master metaphor list",
            "venue": "University of California.",
            "year": 1994
        },
        {
            "authors": [
                "George Lakoff",
                "Mark Johnson."
            ],
            "title": "The metaphorical structure of the human conceptual system",
            "venue": "Cognitive science, 4(2):195\u2013208.",
            "year": 1980
        },
        {
            "authors": [
                "Liunian Harold Li",
                "Mark Yatskar",
                "Da Yin",
                "Cho-Jui Hsieh",
                "Kai-Wei Chang."
            ],
            "title": "Visualbert: A simple and performant baseline for vision and language",
            "venue": "arXiv preprint arXiv:1908.03557.",
            "year": 2019
        },
        {
            "authors": [
                "Jerry Liu",
                "Nathan O\u2019Hara",
                "Alexander Rubin",
                "Rachel Draelos",
                "Cynthia Rudin"
            ],
            "title": "Metaphor detection using contextual word embeddings from transformers",
            "venue": "In Proceedings of the Second Workshop on Figurative Language Processing,",
            "year": 2020
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Roberta: A robustly optimized bert pretraining approach",
            "venue": "arXiv.",
            "year": 2019
        },
        {
            "authors": [
                "Birte L\u00f6nneker."
            ],
            "title": "Lexical databases as resources for linguistic creativity: Focus on metaphor",
            "venue": "Proceedings of the LREC 2004 workshop on language resources for linguistic creativity, pages 9\u201316.",
            "year": 2004
        },
        {
            "authors": [
                "Birte L\u00f6nneker-Rodman."
            ],
            "title": "The hamburg metaphor database project: issues in resource creation",
            "venue": "Language Resources and Evaluation, 42:293\u2013318.",
            "year": 2008
        },
        {
            "authors": [
                "Jiasen Lu",
                "Dhruv Batra",
                "Devi Parikh",
                "Stefan Lee."
            ],
            "title": "Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks",
            "venue": "Advances in neural information processing systems, 32.",
            "year": 2019
        },
        {
            "authors": [
                "Rui Mao",
                "Chenghua Lin",
                "Frank Guerin."
            ],
            "title": "Word embedding and WordNet based metaphor identification and interpretation",
            "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1222\u2013",
            "year": 2018
        },
        {
            "authors": [
                "Zachary J Mason."
            ],
            "title": "Cormet: a computational, corpus-based conventional metaphor extraction system",
            "venue": "Computational linguistics, 30(1):23\u201344.",
            "year": 2004
        },
        {
            "authors": [
                "George A Miller."
            ],
            "title": "Wordnet: a lexical database for english",
            "venue": "Communications of the ACM, 38(11):39\u201341.",
            "year": 1995
        },
        {
            "authors": [
                "Saif Mohammad",
                "Ekaterina Shutova",
                "Peter Turney."
            ],
            "title": "Metaphor as a medium for emotion: An empirical study",
            "venue": "Proceedings of the Fifth Joint Conference on Lexical and Computational Semantics, pages 23\u201333, Berlin, Germany. Association for",
            "year": 2016
        },
        {
            "authors": [
                "Adam Paszke",
                "Sam Gross",
                "Francisco Massa",
                "Adam Lerer",
                "James Bradbury",
                "Gregory Chanan",
                "Trevor Killeen",
                "Zeming Lin",
                "Natalia Gimelshein",
                "Luca Antiga"
            ],
            "title": "Pytorch: An imperative style, high-performance deep learning library",
            "year": 2019
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark"
            ],
            "title": "Learning transferable visual models from natural language supervision",
            "year": 2021
        },
        {
            "authors": [
                "Ronald Rivest."
            ],
            "title": "The md5 message-digest algorithm",
            "venue": "Technical report.",
            "year": 1992
        },
        {
            "authors": [
                "Ekaterina Shutova",
                "Douwe Kiela",
                "Jean Maillard."
            ],
            "title": "Black holes and white rabbits: Metaphor identification with visual features",
            "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Hu-",
            "year": 2016
        },
        {
            "authors": [
                "Ekaterina Shutova",
                "Lin Sun",
                "Elkin Dar\u00edo Guti\u00e9rrez",
                "Patricia Lichtenstein",
                "Srini Narayanan."
            ],
            "title": "Multilingual Metaphor Processing: Experiments with Semi-Supervised and Unsupervised Learning",
            "venue": "Computational Linguistics, 43(1):71\u2013123.",
            "year": 2017
        },
        {
            "authors": [
                "Ekaterina Shutova",
                "Lin Sun",
                "Anna Korhonen."
            ],
            "title": "Metaphor identification using verb and noun clustering",
            "venue": "Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1002\u20131010, Beijing, China. Coling 2010 Orga-",
            "year": 2010
        },
        {
            "authors": [
                "Ekaterina Shutova",
                "Simone Teufel."
            ],
            "title": "Metaphor corpus annotated for source-target domain mappings",
            "venue": "LREC, volume 2, pages 2\u20132. Citeseer.",
            "year": 2010
        },
        {
            "authors": [
                "Ekaterina Shutova",
                "Simone Teufel",
                "Anna Korhonen."
            ],
            "title": "Statistical Metaphor Processing",
            "venue": "Computational Linguistics, 39(2):301\u2013353.",
            "year": 2013
        },
        {
            "authors": [
                "Karen Simonyan",
                "Andrew Zisserman."
            ],
            "title": "Very deep convolutional networks for large-scale image recognition",
            "venue": "arXiv.",
            "year": 2014
        },
        {
            "authors": [
                "Gerard Steen",
                "Aletta G Dorst",
                "J Berenike Herrmann",
                "Anna Kaal",
                "Tina Krennmayr",
                "Trijntje Pasma."
            ],
            "title": "A method for linguistic metaphor identification",
            "venue": "Amsterdam: Benjamins.",
            "year": 2010
        },
        {
            "authors": [
                "Chang Su",
                "Shuman Huang",
                "Yijiang Chen."
            ],
            "title": "Automatic detection and interpretation of nominal metaphor based on the theory of meaning",
            "venue": "Neurocomputing, 219:300\u2013311.",
            "year": 2017
        },
        {
            "authors": [
                "Chang Su",
                "Kechun Wu",
                "Yijiang Chen."
            ],
            "title": "Enhanced metaphor detection via incorporation of external knowledge based on linguistic theories",
            "venue": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 1280\u20131287, On-",
            "year": 2021
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin."
            ],
            "title": "Attention is all you need",
            "venue": "Advances in neural information processing systems, 30.",
            "year": 2017
        },
        {
            "authors": [
                "Bo Xu",
                "Tingting Li",
                "Junzhe Zheng",
                "Mehdi Naseriparsa",
                "Zhehuan Zhao",
                "Hongfei Lin",
                "Feng Xia."
            ],
            "title": "Met-meme: A multimodal meme dataset rich in metaphors",
            "venue": "Proceedings of the 45th International ACM SIGIR Conference on Research and De-",
            "year": 2022
        },
        {
            "authors": [
                "Fan-Pei Gloria Yang",
                "Kailyn Bradley",
                "Madiha Huq",
                "DaiLin Wu",
                "Daniel C. Krawczyk."
            ],
            "title": "Contextual effects on conceptual blending in metaphors: An event-related potential study",
            "venue": "Journal of Neurolinguistics, 26(2):312\u2013326.",
            "year": 2013
        },
        {
            "authors": [
                "Lu Ying",
                "Xinhuan Shu",
                "Dazhen Deng",
                "Yuchen Yang",
                "Tan Tang",
                "Lingyun Yu",
                "Yingcai Wu."
            ],
            "title": "Metaglyph: Automatic generation of metaphoric glyph-based visualization",
            "venue": "IEEE Transactions on Visualization and Computer Graphics, 29(1):331\u2013341.",
            "year": 2023
        },
        {
            "authors": [
                "Dongyu Zhang",
                "Minghao Zhang",
                "Heting Zhang",
                "Liang Yang",
                "Hongfei Lin."
            ],
            "title": "Multimet: A multimodal dataset for metaphor understanding",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Metaphor plays an important role in human cognition and communication, and its ubiquity has been established by empirical studies showing the occurrence of metaphor about once in every three sentences in regular parlance (Steen et al., 2010; Shutova et al., 2010). With the development of\n\u2217*Corresponding author 1https://github.com/PedaloJSY/MultiCMET\nmodern media, when compared to monomodal metaphors, multimodal metaphors are significantly increasing due to their vivid, attractive, and persuasive effects. A multimodal metaphor is defined as a mapping that conceptualizes one target domain in terms of another source domain from different modes, such as text and image, text and sound, or image and sound (Forceville and UriosAparisi, 2009). Figure 1 presents a compelling metaphorical representation depicting lungs constructed from cigarettes, symbolizing the connection between two distinct entities, namely the lung and the cigarette. This metaphor invokes the perceptual notion that smoking is a leading cause of lung cancer. The source domain, represented by the \"cigarette\" image, intertwines with the target domain, represented both textually and visually through the depiction of the \"lung\". Understanding multimodal metaphor mainly focuses on identification of the underlying mapping of the two domains as well as the extraction of properties conveyed by metaphors(Kintsch, 2000; Su et al., 2017). It also requires the decoding of implicit messages and the identification of the semantic relationship between target and source domains (Coulson and Van Petten, 2002; Yang et al., 2013), so it involves cognitive efforts, which makes it significantly challenging for machines.\nIn recent years, there has been a surge of interest\nin multimodal metaphor research within the field of Natural Language Processing (NLP) (Shutova et al., 2016; Xu et al., 2022; Ying et al., 2023). However, the study of multimodal metaphors in the Chinese language is extremely limited. It is important to note that metaphorical linguistic and visual expressions may vary across languages and cultures, despite the existence of universal conceptual metaphors (K\u00f6vecses, 2010). For example, \"human is animal\" is a universal conceptual metaphor in which the attributes or behaviors of animals are used to describe human attributes or behaviors. However, various languages may pick up different metaphorical linguistic and visual expressions underlying the same or similar semantics to realize the metaphorical conceptualization. For example, to use a metaphor to describe the state of being drunk, English speakers choose newt or skunk as a source domain, such as \"as drunk as a skunk\", while Chinese speakers would choose mud, as in \"as drunk as mud\". Due to the variations in multimodal metaphors across different languages, there is an urgent need to construct multimodal metaphor datasets in languages other than English. This will enable a more comprehensive automatic understanding of metaphor.\nFurthermore, previous research on multimodal metaphor understanding has been limited in its depth of analysis, primarily focusing on metaphor detection tasks while neglecting tasks related to deep metaphor comprehension. These tasks include differentiating various categories of source domains and target domains, as they significantly contribute to the emergence of metaphors and facilitate a more profound understanding of their underlying meanings.Therefore, given the importance of domain classification knowledge, the establishment of a domain dictionary that includes domain categorization is crucial for the automatic comprehension of metaphors.\nTo bridge the aforementioned research gap, we introduce the Chinese Multimodal Metaphor Dataset (MultiCMET). This dataset comprises textimage pairs extracted from Chinese advertisements, encompassing textual slogans and their corresponding image counterparts. We annotate not only the occurrence of metaphor as in previous work, but also how metaphor arises in multimodality. Specifically, we provide explicit reasoning for multimodal metaphors by annotating both source and target domains. The quality control and agreement analyses\nfor multiple annotators are also presented. Furthermore, we construct a domain lexicon which contains hierarchical domain categories based on the upper/lower relation in WordNet\u2019s set of nouns (Miller, 1995) along with corresponding examples. We introduce three evaluation tasks to assess the capabilities of multimodal metaphor understanding: metaphor detection, sentiment analysis, and a novel task, namely domain classification, which focuses on classifying the source and target domains of metaphors\u2014an area that has not been previously explored. We propose the Cascading Domain Knowledge Integration (CDKI) benchmark, which leverages domain-specific features derived from the domain lexicon. During the evaluation phase, we employ multiple benchmarks with CDKI to assess the performance of metaphor understanding tasks, showcasing the effectiveness of CDKI. Our contributions are as follows:\n\u2022 Creation of a Chinese dataset, MultiCMET, consisting of 13,820 text-image pairs from advertisements.\n\u2022 The domain classification approach and the construction of the domain lexicon provide a deeper understanding of metaphor usage and facilitate further research.\n\u2022 We propose three tasks to evaluate multimodal metaphor understanding capabilities, including a new task: domain classification. The CDKI benchmark demonstrates the capabilities of the MultiCMET dataset and its usefulness in advancing multimodal metaphor understanding."
        },
        {
            "heading": "2 Related Work",
            "text": ""
        },
        {
            "heading": "2.1 Multimodal Metaphor Datasets",
            "text": "Research on multimodal metaphor datasets has just started, and few multimodal metaphor datasets have been created, although a number of textual metaphor datasets have been introduced for metaphor processing in NLP (Steen et al., 2010; Birke and Sarkar, 2006; Mohammad et al., 2016). Shutova et al. (2016) and Zhang et al. (2021) are among the very few to construct multimodal samples to explore multimodal metaphor processing. However, both types of dataset suffer from the same two deficiencies: (1) they only contain English samples, and (2) they focus on metaphor detection (metaphorical or literal), and they have not\nachieved a profound understanding of multimodal metaphor or how metaphor arises. This makes our dataset, which contains Chinese samples with annotations of mechanisms for metaphor development, different from existing datasets."
        },
        {
            "heading": "2.2 Metaphor Understanding",
            "text": "Certain tasks must be completed before decoding metaphorical messages automatically. We propose three tasks to evaluate multimodal metaphor understanding capabilities, namely metaphor detection, domain classification, and sentiment analysis. Previous research has focused on a variety of approaches to metaphor processing in textual data. Early studies of metaphor focused on the techniques of machine learning and hand-constructed knowledge (Mason, 2004). Others used distributional clustering (Shutova et al., 2013) or unsupervised approaches (Shutova et al., 2017; Mao et al., 2018). Recently, others explored deep learning models to illuminate metaphor (Kuo and Carpuat, 2020; Liu et al., 2020; Choi et al., 2021; Ge et al., 2022). Several recent researchers have become interested in multimodal metaphor detection, such as (Shutova et al., 2016; Kehat and Pustejovsky, 2020; Su et al., 2021), who have explored the fusion of textual and image modalities to detect metaphor. Their results demonstrated the positive effect of combining textual and image features for metaphor detection.\nHowever, they only extracted features from English samples without considering cultural and language variations. Moreover, apart from multimodal metaphor detection, the tasks in our work related to a deep understanding of metaphor and explicit reasoning of how multimodal metaphor arises, like target/source domain classification, which has rarely been studied before."
        },
        {
            "heading": "2.3 Domain Categories in Metaphors",
            "text": "The source domain refers to a familiar concept or domain that is used to understand or describe another concept or domain, known as the target domain (Lakoff and Johnson, 1980). Arriving at a full understanding of metaphorical messages requires identifying two distinct domains that contribute to the occurrence of metaphor. One notable and influential example is the Master Metaphor List (Lakoff, 1994), which includes a wide range of source and target domain categories. This list provides examples of how concepts from the source domain are metaphorically mapped onto the target domain.\nFor instance, the source domain includes categories such as physical object, living being, location, container, path, gateway, depth, motion, journey etc. The target domain includes categories such as life, death, time, past, progress, mind, ideas, problem, and others. The source and target domain categories from the Master Metaphor List have been widely adopted in NLP research (Barnden and Lee, 2007; L\u00f6nneker, 2004) and have been extended with novel categories in subsequent work (Shutova and Teufel, 2010).\nHowever, the domain categories of previous studies are often created based on intuition and existing metaphor examples, which means that they may not capture the full range of metaphorical expressions across different contexts and domains. Additionally, there is a lack of clear structuring principles in the mapping (L\u00f6nneker-Rodman, 2008), resulting in confusion regarding taxonomical levels (Shutova and Teufel, 2010).\nOur work in this paper, offers significant advantages compared to previous domain classification approaches. It allows for the categorization of domains in a broad context and provides clear structuring principles. Furthermore, it establishes clear taxonomical levels and distinct classes of domains."
        },
        {
            "heading": "3 MultiCMET Dataset",
            "text": ""
        },
        {
            "heading": "3.1 Data collection",
            "text": "MultiCMET aims to provide a Chinese annotated dataset to facilitate the research on automatic multimodal metaphor understanding. We chose advertisements with both text and image as our data source because they provide an important context for multi-modal metaphor research (Forceville, 2017). Whether they are commercial or public service ads, these two most common types of advertisements are particularly attractive as a means of communication and contain a large amount of metaphorical information and visual and language features.\nWe collected potential Chinese metaphorical ad samples by using Chinese keywords to search on Baidu and Bing, the two most popular search engines in China. In particular, the overall process of Chinese data selection was driven by researchers who are native Chinese speakers. Specifically, we compiled a list of keywords related to \"advertisement\" and \"metaphor\" and queried these keywords through search engines. The selected keywords encompassed everyday products such as \"mobile\",\n\"car\", and \"pork\"; public service announcement topics such as \"smoking\", \"bullying\", and \"driving\"; as well as widely studied relevant topics to metaphor in pure linguistics such as \"anger\", \"color\", and \"animal\". We also referred to a Master Metaphor List (Lakoff, 1994) to select target/source domains in conceptual metaphor, looking for keywords such as \"change\", \"emotion\", \"people\", \"beliefs\", etc. The detailed list of keywords can be found in Appendix A.1. Additionally, we collected potential metaphorical ad samples of Chinese metaphors from a large-scale commercial advertising dataset with images and internal texts from the IFlytek Advertising Image Classification Competition released in 2021.2 We obtained a total of 20,672 images, with 10,325 obtained through keyword searches and 10,347 obtained from the dataset.\nAfter obtaining the noisy images, we eliminated duplicate images by comparing their MD5 encoding(Rivest, 1992). We manually screened out images that were not advertisements, and those that were blurry or smaller than 350 x 350 pixels. In addition, we removed images without any internal text. To extract textual information from the remaining images, we employed the paddle OCR model (Du et al., 2020), which enabled us to process the text separately. However, due to potential inaccuracies in the OCR output, we manually corrected the extracted text to ensure accuracy. After data cleaning, we finally obtained 13,820 imagetext pairs."
        },
        {
            "heading": "3.2 Annotation model",
            "text": "We annotated the text-image advertising pairs, identifying the metaphors (metaphorical or literal); (if metaphorical) categories of target and source domain (13 categories: relation, communication, attribute, psychological feature, person, plant, animal, process, event, substance, natural object, artifact, or location); and sentiment category(the sentiment metaphors evoke, namely negative, neutral, or positive). The annotation model was MetaphorModel=(Occurrence, TargetCategory, SourceCategory, SentimentCategory). An example of the annotation can be seen in Figure 2."
        },
        {
            "heading": "3.3 Data annotation",
            "text": "Metaphorcial or literal We followed the approach of metaphor identification used in MultiMET (Zhang et al., 2021)to identify data. The\n2https://aistudio.baidu.com/aistudio/datasetdetail/102279\nmetaphor annotation was conducted at the relational level, which involved identifying metaphorical relations between expressions from the source and target domains. In this process, annotations of text modality were made based on language clues, while annotations of image modality were made based on visual features. Both methods determined the linguistic categories of text and images. Specifically, the annotators identified metaphorical text and image pairs by scrutinizing the incongruous units and explaining a non-reversible \"A is B\" identity relation, where two domains were expressed by multiple modalities.\nDomain categories and domain lexicon Metaphor involves using one concrete concept to comprehend and express another abstract concept, aiding reasoning and communication (Lakoff and Johnson, 1980). As a starting point for our hierarchical domain classifications, we use \"physical entity\" to refer to a concrete and tangible object or substance, and \"abstraction\" to refer to a concept or idea representing a general quality, attribute, or state. These two categories form the \"source level\" of our classification.\nTo further organize the domain categories, we refer to the hierarchical structure of WordNet (Miller, 1995) and divide the domains into three additional\nhierarchical levels: macro level, micro level, and entity level (as shown in Figure 3). At the macro level, the source level is subdivided into 13 categories. This includes 9 physical entity categories such as person, animal, artifact, event, location, and process, as well as 4 abstract categories including relation, communication, attribute, and psychological. The division at this level is based on the upper/lower relations in WordNet\u2019s set of nouns. Due to some overlap among the original 25 categories in WordNet, we made adjustments and reclassified the domains into these 13 classes. For example, in Chinese, the meanings of person and people are similar, so they are not strictly distinguished. Similarly, feelings, motivations, and cognition share common psychological features and are classified as subcategories of psychological features. Food, being part of substance, is classified as a subclass of substance. The micro level comprises category words obtained by further subdividing the macro level categories. For example, animals are divided into subcategories such as mammals, birds, reptiles, insects, and fish. The entity level represents specific examples within the micro level. These examples are representative of the categories. For instance, \"tiger\", \"mouse\", and \"bat\" are specific examples of mammals at the entity level.\nWe used the 13 categories from the macro level of domain classifications to annotate the source and target domain categories of metaphors. We didn\u2019t use the 46 categories from the micro level because they were more complex and detailed. The macro level categories offered a more manageable and meaningful level of abstraction for our annotation task. Furthermore, using the domain classifications described above, we construct a domain-specific lexicon that includes vocabulary corresponding to each domain category, along with exemplar words. The lexicon consists of 2,755 words, including 61 category words and 2,694 specific example words. Refer to Appendix A.5 for a detailed domain lexicon.\nSentiment categories The comprehension of metaphor primarily entails identifying the mapping\nof two domains and extracting properties conveyed by metaphors. One essential aspect of metaphor is sentiment, which has been found to have a stronger emotional impact than literal expressions in previous studies (Citron and Goldberg, 2014; Mohammad et al., 2016). Therefore, to investigate whether the sentiment impact of metaphors is more pronounced than that of literary expressions from multicultural and multimodal perspectives, we annotated sentiment in our dataset. The sentiment was categorized into negative, neutral, and positive."
        },
        {
            "heading": "3.4 Annotation process and quality control",
            "text": "We employed an expert-based approach to annotate data for three challenging metaphor understanding tasks, with annotations completed by five Chinese native speakers acting as annotators. The five annotators were divided into three small groups, with two groups consisting of two members each, and the third group consisting of only one member. In cases where the two-member groups could not reach a consensus, the single-member group participated in the final decision. If there were no disagreements between group members, the annotation task was considered complete. Otherwise, the single-member group would re-annotate the data. Finally, if there were disagreements in the annotations from all groups, everyone would discuss and decide on the annotation to ensure its accuracy and consistency. For samples with discrepant annotations, we held multiple in-group discussions and conducted checks and modifications to improve annotation consistency and accuracy.\nTo improve annotation quality, we took several effective measures. We established strict criteria and documentation for every annotation option, including detailed explanations, extensive examples, and notes. For the convenience of annotation, we provide an interface for the annotators as detailed in Appendix A.2. Additionally, prior to each annotation session, we held a training course to provide guidance. During the pre-annotation process, we adjusted the course and guidance documents to address any issues, ensuring that the annotation document was comprehensive and definitive before large-scale annotation.\nKappa score (Fleiss, 1971) was used to measure classification consistency, with scores of\u03ba=0.69, \u03ba=0.66,\u03ba=0.62, and\u03ba=0.77 achieved for identifying metaphors, identifying target domain categories, identifying source domain categories, and\nidentifying emotional categories, respectively, indicating the reliability of our annotations."
        },
        {
            "heading": "4 Dataset Analysis",
            "text": "Refer to Table 1 for an overview of the dataset statistics. We conducted an analysis of category distribution in both the source and target domains, as presented in Figure 4. It is evident that the categories \"artifact\", \"natural object\" and \"substance\" appear most frequently in both domains, indicating the preference for these tangible categories in expressing ideas and emotions in metaphorical advertising. Conversely, the categories \"communication\", \"relation\" and \"psychological\" have relatively lower frequencies, as they often involve more abstract and intricate concepts that require a deeper understanding of contextual and background knowledge. Furthermore, among the 13 categories examined, the source domain exhibits a more dispersed distribution, suggesting a diverse utilization and manifestation of source domains, while the target domain revolves around more centralized themes and situations compared to the source domain.\nFigure 5 demonstrates varying emotional distributions across different metaphors. In metaphorical data, positive and neutral emotions are evenly distributed, while negative emotions account for a smaller portion. This indicates that metaphorical expressions in advertising often correlate with positive or neutral attitudes. In non-metaphorical data, neutral emotions dominate the dataset, followed by positive emotions, with negative emotions being the least prevalent. This reflects the directness and clarity of non-metaphorical expressions, which are typically utilized to convey widely accepted information or depict everyday life scenarios. By comparing the statistical data of metaphorical and\nnon-metaphorical expressions, we can observe that the proportion of neutral data in metaphorical expressions is lower than that in non-metaphorical expressions, suggesting that metaphorical expressions tend to encompass a richer emotional range. Our findings are consistent with previous research suggesting that metaphors convey more emotions or feelings than simple prose (Citron and Goldberg, 2014)."
        },
        {
            "heading": "5 Methodology",
            "text": "Taking into account the significance of domain knowledge in metaphor detection, we propose a benchmark called Cascading Domain Knowledge Integration (CDKI) that aims to enhance the model\u2019s ability to detect metaphors by incorporating domain knowledge. The schematic diagram of CDKI is illustrated in Figure 6.\nIntroducing domain knowledge into models poses a challenge due to inherent differences in how it is presented in text and images. Textual domain knowledge is typically easier to extract, expressed explicitly through specific vocabulary and sentence structures. Conversely, domain knowledge in images tends to be more implicit. In the following section, we will elaborate on how CDKI addresses this disparity and achieves the extraction and integration of domain knowledge.\nRegarding the text modality, we employ a segmentation technique to preserve multiple inherent domain-specific vocabularies in the textual data by extracting nouns. These nouns are then connected using the [SEP] token to form Ktext, which serves as the domain knowledge for the text modality.\nRegarding the image modality, we have constructed a cascaded domain word set to introduce and enhance the dependency relationships of domain knowledge within the images. Specifically,\nthe cascaded domain word set for an image consists of three parts: the macro-level domain word set, the micro-level domain word set, and the entity-level domain word set. These word sets are composed of vocabulary corresponding to the respective levels in the domain lexicon. Taking the macro-level domain word set as an example, we will describe the construction method of the cascaded domain word set for images. By utilizing Clip (Radford et al., 2021), we obtain the probabilities of each vocabulary in the macro-level of the domain dictionary appearing in the image. We then retain the top n vocabularies with the highest probabilities as the macro-domain word set Setmacro for that particular image. The construction method for the microdomain word set, Setmicro, and the entity-domain word set, Setentity, follows the same approach. Each set consists of p and q vocabularies, respectively. Collectively, these three sets constitute the domain knowledge within the image,denoted as Kimage:\nKimage = Setmacro + Setmicro + Setentity (1)\nLastly, the domain knowledge from the image and text modalities are combined to generate the final domain knowledge, denoted as Kpair, for the image-text pairs:\nKpair = Kimage +Ktext (2)\nIn addition, we conducted multiple sets of experiments to investigate the impact of different vocabulary sizes on the model results while keeping the text attributes fixed. The details of these experiments can be found in Appendix ??. Ultimately, we determined that the optimal values for n, p, and q are 2, 3, and 5, respectively.\nTo incorporate the domain knowledge Kpair into the model, we employ BERT (Devlin et al., 2018) to convert the Kpair into vectors, which serve as the domain feature input for the image-text pairs. We utilized BERT and ResNet50 (He et al., 2016) to obtain textual and visual features respectively. Subsequently, we utilize cross-attention mechanism (Vaswani et al., 2017) to jointly model the domain features with the image and text features. This approach enhances the interaction of domain knowledge between different features. The resulting outputs are concatenated and fed into the softmax function to obtain the final result."
        },
        {
            "heading": "6 Experiments",
            "text": ""
        },
        {
            "heading": "6.1 Baselines",
            "text": "In this section, we introduce the baselines used in our experiments. Specifically, we adopted random, text-based, image-based, and multi-modal models as the baseline models and compared their performance with our proposed model. Due to the fact that metaphors, irony, and hate speech fall under the category of figurative language, there exists a certain degree of similarity between these three tasks(Abulaish et al., 2020). Thus, we selected these multi-modal methods as our control models.\nRandom: It denotes random predictions based on the data.\nResnet50 (He et al., 2016): It is a 50-layer convolutional neural network with residual connections. Image features are extracted from its last convolutional layer\u2019s output vector.\nVGG16 (Simonyan and Zisserman, 2014): It enhances network depth and the expression capability of non-linear features by stacking consecutive 3x3 convolutional layers and max pooling layers.\nBi-LSTM (Hochreiter and Schmidhuber, 1997): It\u2019s a bidirectional recurrent neural network that combines the outputs of forward and backward LSTM to capture global context in a sequence.\nBert (Devlin et al., 2018) and Roberta (Liu et al., 2019): We conducted the experiment using two widely-used Chinese transformer-based pre-trained language models, namely bert-basedchinese and roberta-base-chinese, as baselines.\nRes-BERT (Zhang et al., 2021): It uses\nResNet50 to extract image features and BERT to encode text. Then, the two sets of features are concatenated to form a fused feature vector, which is input into a fully connected layer for classification.\nVilBERT (Lu et al., 2019) and VisualBERT (Li et al., 2019): They are both multimodal language models based on the Transformer architecture. VilBERT utilizes a dual-stream attention mechanism to encode visual and textual information. VisualBERT employs a self-attention mechanism to encode images and text in a single stream manner.\nHFM (Cai et al., 2019): The multimodal fusion model combines image, attribute, and text features using Bi-LSTM and MLP to generate a unified representation for ironic prediction tasks.\nCES (Blaier et al., 2021): This method improves hate speech meme classifier accuracy by leveraging image-caption to enhance both multimodal and unimodal models."
        },
        {
            "heading": "6.2 Implementation",
            "text": "Our model was constructed using the PyTorch framework (Paszke et al., 2019), and we preprocessed our data utilizing the pandas and NumPy libraries. Our optimization algorithm of choice was Adam (Kingma and Ba, 2014), with cross-entropy serving as our loss function. After fine-tuning the baseline, we selected the best-performing model and hyperparameters to be tested on the reserved testing set. Table 3 presents a comprehensive overview of the key hyperparameters employed in our experimental study."
        },
        {
            "heading": "6.3 Results and discussion",
            "text": "We evaluate our model\u2019s performance by comparing it with baseline models using accuracy and F1\nscores, as shown in Table 2. Our model excels in metaphor detection, followed by metaphor sentiment detection, and performs weakest in target and source domain detection. The subpar performance in target and source domain detection can be attributed to two factors. Firstly, with 13 categories, there is an excessive number of category features that the model struggles to accurately capture. Secondly, an issue of data imbalance exists among different categories, resulting in inadequate generalization capabilities of the model for minority categories. Nonetheless, our model outperforms all baseline models, validating its effectiveness.\nFrom the table, it is evident that models solely relying on image features exhibited subpar performance. Among them, ResNet50 outperformed VGG16 due to its utilization of residual connections to mitigate the issue of gradient vanishing.\nText-based methods exhibited superior performance compared to image-based methods, as text can convey more contextual information and provide valuable context to the model. Both Bert and Roberta, having been fully trained on large-scale language corpora, outperformed Bi-LSTM.\nThe multimodal approach yielded the best results, as the fusion of image and text features enhanced the detection performance of the model.\nOur model achieved the highest performance in the multimodal setting, In comparison to the CES model that incorporates image captions as additional features, the inclusion of domain knowledge provides the model with richer and more valuable information. While HFM utilizes image attributes as additional features, our approach introduces domain attributes with hierarchical relationships, enabling the model to leverage higher-level concepts and semantic associations. This incorporation of hierarchical domain attributes effectively enhances the performance of the model.\nWe also performed a qualitative analysis of the wrongly classified samples. We examined approximately 50 misclassified instances and categorized the reasons for the errors, which are detailed in Appendix A.4."
        },
        {
            "heading": "7 Conclusion",
            "text": "In conclusion, this paper addresses the limitations in research on multimodal metaphors beyond the English language and the lack of exploration in categorizing source and target domains in metaphors. It introduces the MultiCMET dataset, a large-scale multimodal Chinese metaphor dataset consisting of text-image pairs from advertisements. The dataset includes manual annotations of metaphors, domain categories, and conveyed sentiments. Additionally, a domain lexicon is constructed, providing hierarchical domain classifications and corresponding examples. We propose the CDKI benchmark, which leverages domain-specific features for better metaphor understanding.\nThis paper significantly contributes to the field of multimodal metaphor understanding by providing a comprehensive dataset, novel methodologies, and benchmark results. It opens up avenues for future research in exploring metaphors across different languages, enhancing metaphor comprehension, and addressing the cognitive mechanisms involved in metaphor processing. The availability of the dataset and code further encourages the research community to expand and improve upon the findings presented in this work.\nLimitations\nOne limitation of the current version of MultiCMET is its exclusive coverage of the Chinese language. It is highly necessary to develop the MultiCMET dataset for other languages in order to facilitate comparative research on multimodal\nmetaphors between different language systems. This expansion would help uncover biases present in current metaphor models and provide a deeper understanding of the cognitive mechanisms underlying metaphor comprehension. Moreover, a dataset encompassing more diverse languages and cultures would greatly benefit the models\u2019 capacity to comprehend multimodal metaphors. We encourage researchers to embrace the challenging yet captivating task of expanding MultiCMET by incorporating data from additional languages in future endeavors.\nEthics Statement\nBased on the source of the data, our MultiCMET dataset can be divided into two parts. The first part consists of data obtained through querying search engines, specifically Baidu and Bing. For this portion, we have followed the data use agreements of these search engine querying platforms. We have made our annotations publicly available for this data, and we provide a detailed description of the query process.The second part of the dataset comprises data collected from publicly available datasets that allow data usage for non-commercial academic purposes. We strictly adhere to the terms and conditions of these publicly available datasets. We have released the unique IDs associated with this data, along with our annotations. By ensuring transparency and compliance with data use agreements, we aim to promote responsible and ethical data usage while enabling researchers to access and utilize the MultiCMM dataset effectively."
        },
        {
            "heading": "Acknowledgements",
            "text": "This work is supported by NSFC Programs (No.62076051)."
        },
        {
            "heading": "A Appendix",
            "text": "A.1 Keyword query The keywords used during the data collection process are presented in Table 4.\nA.2 Annotation Interface Figure 7 presents several examples of our annotated illustrations.\nA.3 Number of vocabulary terms In this section, we aim to explore the relationship between the number of subset vocabulary in image cascading and model performance. Firstly, we use a baseline model without domain knowledge, where the values of n, p, and q are all 0. The accuracy of this model is 0.7134. Next, we gradually increase the values of n, p, and q while keeping other features constant to analyze their impact on model performance and eliminate interference. The ranges of n, p, and q are [0-5], [0-7], and [0-10] respectively. We select representative data from these ranges and present them in Table 5.\nWhen n, p, and q are all set to 1, the model\u2019s accuracy in metaphor recognition is already higher than that of the baseline model without domain knowledge, reaching 0.7272. This indicates that"
        },
        {
            "heading": "1 1 1 0.7272",
            "text": ""
        },
        {
            "heading": "1 2 4 0.7396",
            "text": ""
        },
        {
            "heading": "1 3 5 0.7452",
            "text": ""
        },
        {
            "heading": "1 6 9 0.7426",
            "text": ""
        },
        {
            "heading": "1 7 10 0.7315",
            "text": ""
        },
        {
            "heading": "2 2 2 0.7445",
            "text": ""
        },
        {
            "heading": "2 2 4 0.7538",
            "text": ""
        },
        {
            "heading": "2 3 5 0.7632",
            "text": ""
        },
        {
            "heading": "2 6 9 0.7499",
            "text": ""
        },
        {
            "heading": "2 7 10 0.7433",
            "text": ""
        },
        {
            "heading": "3 3 3 0.7516",
            "text": ""
        },
        {
            "heading": "3 2 4 0.7501",
            "text": ""
        },
        {
            "heading": "3 3 5 0.7541",
            "text": ""
        },
        {
            "heading": "3 6 9 0.7432",
            "text": ""
        },
        {
            "heading": "3 7 10 0.7389",
            "text": "introducing domain knowledge is helpful in improving model accuracy. As the values of n, p, and q increase, the accuracy of the model in metaphor recognition also improves. When n = p = q = 3, the accuracy of the model is 0.7516, higher than the model\u2019s performance when n = p = q = 1. This suggests that increasing the number of vocabulary can improve the model\u2019s accuracy.\nTo analyze the relationship between subset vocabulary and model accuracy, we conducted experiments using controlled variables. Results showed that increasing values of p and q improved model accuracy, while keeping n constant. Micro-domain and entity-specific vocabulary helped capture detailed features and enhance recognition accuracy. However, higher values of n, p, and q were not always better. The parameters n = 2, p = 3, and q = 5 achieved optimal results, while n = 3, p = 6, and q = 9 did not perform as well despite having a larger vocabulary. Excessively high values of n, p, and q introduced noise, resulting in decreased accuracy.\nIn conclusion, incorporating domain knowledge can enhance the model\u2019s recognition accuracy, and expanding the subset vocabulary in image cascading can improve model performance. However, an excessive number of vocabulary may introduce\nnoise and lead to performance deterioration. This is further supported by the parameters n = 2, p = 7, q = 10, and n = 3, p = 7, q = 10. Hence, we select the parameters n = 2, p = 3, and q = 5 as the subset vocabulary size, which yielded the best experimental outcomes.\nA.4 Error analysis\nOne type of error is when the model misclassifies metaphoric text and images as non-metaphoric. This often occurs when the target or source domain in the image is heavily distorted or partially visible, limiting the model\u2019s ability to learn domain information. As a result, conflicts between the text and image may go undetected, leading to the incorrect identification of metaphors. For instance, in Figure 8 on the left, the obscured tiger tail prevents proper recognition of the conflicting elements with the euro symbol, resulting in the image being mistakenly classified as non-metaphoric.\nThe second classification error occurs when the model misclassifies non-literal text as metaphorical. This can happen due to changes in artistic style or an information overload, making it difficult for the model to learn domain-specific information and detect conflicts between domains. In the Figure 8 on the right, the text itself lacks metaphorical meaning, but the model mistakenly identifies it as metaphorical. Additionally, the model wrongly\ncategorizes the target domain as \"artifacts\" and the source domain as \"natural objects.\" Analysis suggests this may be because the image is handdrawn, leading the model to incorrectly interpret certain elements as natural objects and perceive a conflict between toothpaste and natural objects. As a result, the text-image pair is mistakenly classified as a metaphor.\nA.5 Domain Lexicon"
        }
    ],
    "title": "MultiCMET: A Novel Chinese Benchmark for Understanding Multimodal Metaphor",
    "year": 2023
}