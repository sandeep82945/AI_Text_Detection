{
    "abstractText": "Automatic Post-Editing (APE) systems are prone to over-correction of the Machine Translation (MT) outputs. While a Word-level Quality Estimation (QE) system can provide a way to curtail the over-correction, a significant performance gain has not been observed thus far by utilizing existing APE and QE combination strategies. This paper proposes joint training of a model over QE (sentenceand word-level) and APE tasks to improve the APE. Our proposed approach utilizes a multi-task learning (MTL) methodology, which shows significant improvement while treating the tasks as a \u2018bargaining game\u2019 during training. Moreover, we investigate various existing combination strategies and show that our approach achieves stateof-the-art performance for a \u2018distant\u2019 language pair, viz., English-Marathi. We observe an improvement of 1.09 TER and 1.37 BLEU points over a baseline QE-Unassisted APE system for English-Marathi while also observing 0.46 TER and 0.62 BLEU points improvement for English-German. Further, we discuss the results qualitatively and show how our approach helps reduce over-correction, thereby improving the APE performance. We also observe that the degree of integration between QE and APE directly correlates with the APE performance gain. We release our code publicly1.",
    "authors": [
        {
            "affiliations": [],
            "name": "Sourabh Deoghare"
        },
        {
            "affiliations": [],
            "name": "Diptesh Kanojia"
        },
        {
            "affiliations": [],
            "name": "Tharindu Ranasinghe"
        },
        {
            "affiliations": [],
            "name": "Fr\u00e9d\u00e9ric Blain"
        },
        {
            "affiliations": [],
            "name": "Pushpak Bhattacharyya"
        }
    ],
    "id": "SP:46e6f69c1051d04664d2f5d97f32c6fc3117c51c",
    "references": [
        {
            "authors": [
                "Morishita",
                "Masaaki Nagata",
                "Ajay Nagesh",
                "Toshiaki Nakazawa",
                "Matteo Negri",
                "Santanu Pal",
                "Allahsera Auguste Tapo",
                "Marco Turchi",
                "Valentin Vydrin",
                "Marcos Zampieri."
            ],
            "title": "Findings of the 2021 conference on machine translation (WMT21)",
            "venue": "Proceedings of",
            "year": 2021
        },
        {
            "authors": [
                "Akanksha Bansal",
                "Esha Banerjee",
                "Girish Nath Jha."
            ],
            "title": "Corpora creation for indian language technologies\u2013the ilci project",
            "venue": "the sixth Proceedings of Language Technology Conference (LTC \u201813).",
            "year": 2013
        },
        {
            "authors": [
                "Pushpak Bhattacharyya",
                "Rajen Chatterjee",
                "Markus Freitag",
                "Diptesh Kanojia",
                "Matteo Negri",
                "Marco Turchi."
            ],
            "title": "Findings of the WMT 2022 shared task on automatic post-editing",
            "venue": "Proceedings of the Seventh Conference on Machine Translation (WMT),",
            "year": 2022
        },
        {
            "authors": [
                "Rajen Chatterjee",
                "Markus Freitag",
                "Matteo Negri",
                "Marco Turchi."
            ],
            "title": "Findings of the WMT 2020 shared task on automatic post-editing",
            "venue": "Proceedings of the Fifth Conference on Machine Translation, pages 646\u2013659, Online. Association for Computa-",
            "year": 2020
        },
        {
            "authors": [
                "Rajen Chatterjee",
                "Matteo Negri",
                "Marco Turchi",
                "Fr\u00e9d\u00e9ric Blain",
                "Lucia Specia."
            ],
            "title": "Combining quality estimation and automatic post-editing to enhance machine translation output",
            "venue": "Proceedings of the 13th Conference of the Association for Machine Transla-",
            "year": 2018
        },
        {
            "authors": [
                "Alexis Conneau",
                "Kartikay Khandelwal",
                "Naman Goyal",
                "Vishrav Chaudhary",
                "Guillaume Wenzek",
                "Francisco Guzm\u00e1n",
                "Edouard Grave",
                "Myle Ott",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Unsupervised cross-lingual representation learning at scale",
            "venue": "Pro-",
            "year": 2020
        },
        {
            "authors": [
                "Sourabh Deoghare",
                "Pushpak Bhattacharyya."
            ],
            "title": "IIT Bombay\u2019s WMT22 automatic post-editing shared task submission",
            "venue": "Proceedings of the Seventh Conference on Machine Translation (WMT), pages 682\u2013 688, Abu Dhabi, United Arab Emirates (Hybrid).",
            "year": 2022
        },
        {
            "authors": [
                "Fangxiaoyu Feng",
                "Yinfei Yang",
                "Daniel Cer",
                "Naveen Arivazhagan",
                "Wei Wang."
            ],
            "title": "Language-agnostic BERT sentence embedding",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages",
            "year": 2022
        },
        {
            "authors": [
                "Yvette Graham."
            ],
            "title": "Improving evaluation of machine translation quality estimation",
            "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing",
            "year": 2015
        },
        {
            "authors": [
                "Chris Hokamp."
            ],
            "title": "Ensembling factored neural machine translation models for automatic post-editing and quality estimation",
            "venue": "Proceedings of the Second Conference on Machine Translation, pages 647\u2013654, Copenhagen, Denmark. Association for Computa-",
            "year": 2017
        },
        {
            "authors": [
                "Julia Ive",
                "Fr\u00e9d\u00e9ric Blain",
                "Lucia Specia."
            ],
            "title": "deepQuest: A framework for neural-based quality estimation",
            "venue": "Proceedings of the 27th International Conference on Computational Linguistics, pages 3146\u2013 3157, Santa Fe, New Mexico, USA. Association for",
            "year": 2018
        },
        {
            "authors": [
                "Marcin Junczys-Dowmunt",
                "Roman Grundkiewicz."
            ],
            "title": "Log-linear combinations of monolingual and bilingual neural machine translation models for automatic post-editing",
            "venue": "Proceedings of the First Conference on Machine Translation: Volume 2, Shared",
            "year": 2016
        },
        {
            "authors": [
                "Divyanshu Kakwani",
                "Anoop Kunchukuttan",
                "Satish Golla",
                "Gokul N.C",
                "Avik Bhattacharyya",
                "Mitesh M. Khapra",
                "Pratyush Kumar"
            ],
            "title": "IndicNLPSuite: Monolingual corpora, evaluation benchmarks and pre-trained multilingual language models for Indian",
            "year": 2020
        },
        {
            "authors": [
                "Fabio Kepler",
                "Jonay Tr\u00e9nous",
                "Marcos Treviso",
                "Miguel Vera",
                "Andr\u00e9 F.T. Martins."
            ],
            "title": "OpenKiwi: An open source framework for quality estimation",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: System",
            "year": 2019
        },
        {
            "authors": [
                "Nagata",
                "Toshiaki Nakazawa",
                "Michal Nov\u00e1k",
                "Martin Popel",
                "Maja Popovi\u0107."
            ],
            "title": "Findings of the 2022 conference on machine translation (WMT22)",
            "venue": "Proceedings of the Seventh Conference on Machine",
            "year": 2022
        },
        {
            "authors": [
                "Samuel L\u00e4ubli",
                "Mark Fishel",
                "Gary Massey",
                "Maureen Ehrensberger-Dow",
                "Martin Volk."
            ],
            "title": "Assessing post-editing efficiency in a realistic translation environment",
            "venue": "Proceedings of the 2nd Workshop on Post-editing Technology and Practice, Nice, France.",
            "year": 2013
        },
        {
            "authors": [
                "Dongjun Lee."
            ],
            "title": "Two-phase cross-lingual language model fine-tuning for machine translation quality estimation",
            "venue": "Proceedings of the Fifth Conference on Machine Translation, pages 1024\u20131028, Online. Association for Computational Linguistics.",
            "year": 2020
        },
        {
            "authors": [
                "Jihyung Lee",
                "WonKee Lee",
                "Jaehun Shin",
                "Baikjin Jung",
                "Young-Kil Kim",
                "Jong-Hyeok Lee."
            ],
            "title": "POSTECH-ETRI\u2019s submission to the WMT2020 APE shared task: Automatic post-editing with crosslingual language model",
            "venue": "Proceedings of the Fifth",
            "year": 2020
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Roberta: A robustly optimized bert pretraining approach",
            "venue": "ArXiv, abs/1907.11692.",
            "year": 2019
        },
        {
            "authors": [
                "Ant\u00f3nio V. Lopes",
                "M. Amin Farajian",
                "Gon\u00e7alo M. Correia",
                "Jonay Tr\u00e9nous",
                "Andr\u00e9 F.T. Martins."
            ],
            "title": "Unbabel\u2019s submission to the WMT2019 APE shared task: BERT-based encoder-decoder for automatic post-editing",
            "venue": "Proceedings of the Fourth Confer-",
            "year": 2019
        },
        {
            "authors": [
                "Andr\u00e9 F.T. Martins",
                "Marcin Junczys-Dowmunt",
                "Fabio N. Kepler",
                "Ram\u00f3n Astudillo",
                "Chris Hokamp",
                "Roman Grundkiewicz."
            ],
            "title": "Pushing the limits of translation quality estimation",
            "venue": "Transactions of the Association for Computational Linguistics, 5:205\u2013",
            "year": 2017
        },
        {
            "authors": [
                "Aviv Navon",
                "Aviv Shamsian",
                "Idan Achituve",
                "Haggai Maron",
                "Kenji Kawaguchi",
                "Gal Chechik",
                "Ethan Fetaya."
            ],
            "title": "Multi-task learning as a bargaining game",
            "venue": "International Conference on Machine Learning.",
            "year": 2022
        },
        {
            "authors": [
                "Matteo Negri",
                "Marco Turchi",
                "Rajen Chatterjee",
                "Nicola Bertoldi."
            ],
            "title": "ESCAPE: a large-scale synthetic corpus for automatic post-editing",
            "venue": "Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018),",
            "year": 2018
        },
        {
            "authors": [
                "Shinhyeok Oh",
                "Sion Jang",
                "Hu Xu",
                "Shounan An",
                "Insoo Oh."
            ],
            "title": "Netmarble AI center\u2019s WMT21 automatic post-editing shared task submission",
            "venue": "Proceedings of the Sixth Conference on Machine Translation, pages 307\u2013314, Online. Association for",
            "year": 2021
        },
        {
            "authors": [
                "Santanu Pal",
                "Sudip Kumar Naskar",
                "Josef van Genabith."
            ],
            "title": "Multi-engine and multi-alignment based automatic post-editing and its impact on translation productivity",
            "venue": "Proceedings of COLING 2016, the 26th International Conference on Computational Lin-",
            "year": 2016
        },
        {
            "authors": [
                "Kishore Papineni",
                "Salim Roukos",
                "Todd Ward",
                "WeiJing Zhu."
            ],
            "title": "Bleu: a method for automatic evaluation of machine translation",
            "venue": "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311\u2013318, Philadelphia,",
            "year": 2002
        },
        {
            "authors": [
                "Vivek Raghavan",
                "Anoop Kunchukuttan",
                "Pratyush Kumar",
                "Mitesh Shantadevi Khapra."
            ],
            "title": "Samanantar: The largest publicly available parallel corpora collection for 11 indic languages",
            "venue": "Transactions of the Association for Computational Linguistics, 10:145\u2013",
            "year": 2022
        },
        {
            "authors": [
                "Tharindu Ranasinghe",
                "Constantin Orasan",
                "Ruslan Mitkov."
            ],
            "title": "TransQuest at WMT2020: Sentencelevel direct assessment",
            "venue": "Proceedings of the Fifth Conference on Machine Translation, pages 1049\u2013 1055, Online. Association for Computational Lin-",
            "year": 2020
        },
        {
            "authors": [
                "Tharindu Ranasinghe",
                "Constantin Orasan",
                "Ruslan Mitkov."
            ],
            "title": "TransQuest: Translation quality estimation with cross-lingual transformers",
            "venue": "Proceedings of the 28th International Conference on Computational Linguistics, pages 5070\u20135081, Barcelona,",
            "year": 2020
        },
        {
            "authors": [
                "Tharindu Ranasinghe",
                "Constantin Orasan",
                "Ruslan Mitkov."
            ],
            "title": "An exploratory analysis of multilingual word-level quality estimation with cross-lingual transformers",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Lin-",
            "year": 2021
        },
        {
            "authors": [
                "Abhishek Sharma",
                "Prabhakar Gupta",
                "Anil Nelakanti."
            ],
            "title": "Adapting neural machine translation for automatic post-editing",
            "venue": "Proceedings of the Sixth Conference on Machine Translation, pages 315\u2013319, Online. Association for Computational Linguistics.",
            "year": 2021
        },
        {
            "authors": [
                "Matthew Snover",
                "Bonnie Dorr",
                "Rich Schwartz",
                "Linnea Micciulla",
                "John Makhoul."
            ],
            "title": "A study of translation edit rate with targeted human annotation",
            "venue": "Proceedings of the 7th Conference of the Association for Machine Translation in the Americas: Technical",
            "year": 2006
        },
        {
            "authors": [
                "Lucia Specia",
                "Fr\u00e9d\u00e9ric Blain",
                "Marina Fomicheva",
                "Erick Fonseca",
                "Vishrav Chaudhary",
                "Francisco Guzm\u00e1n",
                "Andr\u00e9 F.T. Martins."
            ],
            "title": "Findings of the WMT 2020 shared task on quality estimation",
            "venue": "Proceedings of the Fifth Conference on Machine Translation,",
            "year": 2020
        },
        {
            "authors": [
                "Lucia Specia",
                "Fr\u00e9d\u00e9ric Blain",
                "Marina Fomicheva",
                "Chrysoula Zerva",
                "Zhenhao Li",
                "Vishrav Chaudhary",
                "Andr\u00e9 F.T. Martins."
            ],
            "title": "Findings of the WMT 2021 shared task on quality estimation",
            "venue": "Proceedings of the Sixth Conference on Machine Translation,",
            "year": 2021
        },
        {
            "authors": [
                "Daimeng Wei",
                "Hengchao Shang",
                "Zhanglin Wu",
                "Zhengzhe Yu",
                "Liangyou Li",
                "Jiaxin Guo",
                "Minghan Wang",
                "Hao Yang",
                "Lizhi Lei",
                "Ying Qin",
                "Shiliang Sun."
            ],
            "title": "HW-TSC\u2019s participation in the WMT 2020 news translation shared task",
            "venue": "Proceedings of",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Significant progress in Machine Translation (MT) and adopting MT systems in the translation industry has led to motivated research on auxiliary tasks. One auxiliary task in the MT field is Automatic Post-editing (APE), which is aimed toward the automatic identification and correction of translation errors. The APE task is motivated by the need to improve MT output in black-box conditions, where translation models are not accessible for modification or retraining (Chatterjee et al., 2020), which\n1https://github.com/cfiltnlp/APE_MTL\ncan be costly in terms of time and effort. Quality Estimation (QE) is another auxiliary task that assesses the quality of MT output in the absence of reference translations (Specia et al., 2020). QE is performed at a word, sentence, and document level. A word-level QE system predicts an OK or BAD tag for each token on both source and target segments, while a sentence-level QE assigns a rating (0-100), referred to as a Direct Assessment (DA) score, to the translation denoting its overall quality.\nBoth QE and APE have been explored significantly as standalone tasks. Over the years, predicting MT quality has been formulated differently (e.g. ranking, scoring, qualitative metrics) and has been dealt with at diverse levels of granularity. Consistent advancements in both these tasks make QE and APE more appealing technologies. Despite the growing interest in the two tasks and their intuitive relatedness, previous research in both areas has mostly followed separate paths. Current QE integration strategies in the MT-APE pipeline have only been applied over statistical phrase-based MT system translations (Chatterjee et al., 2018).\nOur motivation for the current work is: the potential usefulness of leveraging the two technologies to develop a better APE system has been scarcely explored. Also, there is no systematic analysis of APE performance when different QE and APE combination strategies are used to postedit translations obtained from a Neural Machine Translation (NMT) system. We hypothesize that QE can help APE in the following manner: A sentencelevel QE output can provide an overall idea of how much editing is required to correct a translation, whereas a word-level QE output can help APE by identifying translation tokens that need editing.\nOur goal is to reduce over-correction in APE by using QE. Hence, our contributions are:\n1. Joint training over QE (sentence- and wordlevel) and APE tasks, which helps APE learn to identify and correct erroneous translation\nsegments, leading to significant (based on the significance test with p value of 0.05) performance gains for English-Marathi (1.09 TER) and English-German (0.46 TER) over the QEunassisted APE baseline. (Refer Table 4)\n2. A novel approach to multi-task learning (NashMTL) applied to QE and APE, which treats learning both tasks as a bargaining game. The improvements obtained over the baseline are given in the first point.\n3. A comprehensive study investigating known QE and APE combination strategies while showing that a tighter coupling of both tasks is increasingly beneficial in improving APE performance. For this study, we consider three existing QE and APE combination strategies (QE as APE Activator, QE as MT/APE Selector, QE as APE Guide) and the proposed method (Joint Training over QE and APE). (Refer Table 5 or Figure 4)"
        },
        {
            "heading": "2 Related Work",
            "text": "During the past decade, there has been tremendous progress in the field of QE and APE, primarily due to the shared tasks organized annually by the Conference on Machine Translation (WMT), since 2012 and 2015, respectively (Zerva et al., 2022; Bhattacharyya et al., 2022).\nL\u00e4ubli et al. (2013) and Pal et al. (2016) have shown that the APE systems have the potential to reduce human effort by correcting systematic and repetitive translation errors. Recent APE approaches utilize transfer learning by adapting pretrained language or translation models to perform post-editing (Lopes et al., 2019; Wei et al., 2020; Sharma et al., 2021). Also, the recent approaches use multilingual or cross-lingual models to get latent representations of the source and target sentences (Lee et al., 2020). Oh et al. (2021) have shown that gradually adapting pre-trained models to APE by using the Curriculum Training Strategy (CTS) improves performance. Deoghare and Bhattacharyya (2022) have demonstrated that augmenting the APE data by phrase-level APE triplets improves feature diversity and used the CTS to train the APE system on high-quality data.\nRecently, in the field of QE, neural-based systems such as deepQuest (Ive et al., 2018) and OpenKiwi (Kepler et al., 2019) have consistently outperformed other approaches in WMT Quality\nEstimation shared tasks (Kepler et al., 2019). These systems utilize an encoder-decoder Recurrent Neural Network (RNN) architecture, commonly called the \u2018predictor,\u2019 combined with a bidirectional RNN known as the \u2018estimator,\u2019 which generates quality estimates. However, one drawback of these architectures is that they require extensive predictor pre-training, relying on large parallel data and demanding computational resources (Ive et al., 2018). To address this limitation, TransQuest (Ranasinghe et al., 2020b) emerged as a solution, winning the WMT20 sentence-level QE (DA prediction) shared task Specia et al. (2020). TransQuest eliminates the need for a predictor by leveraging cross-lingual embeddings. The authors fine-tuned an XLMRoberta model for the sentence-level QE, demonstrating that a simple architecture can achieve stateof-the-art results. Subsequently, the TransQuest framework has been extended to the word-level QE task (Ranasinghe et al., 2021). Recently, Deoghare and Bhattacharyya (2022) showed that combining sentence-level and word-level QE systems can help alleviate the problem of inconsistent predictions.\nMartins et al. (2017) used APE outputs to improve QE systems. Hokamp (2017) used an ensemble of factored NMT models for word-level QE and APE tasks. Chatterjee et al. (2018) compared three combination approaches and showed the potential of QE systems in improving APE on output obtained from a phrase-based MT system. We use these three approaches for comparison in the current work. The winning submission of the WMT22 APE shared task shows the usefulness of a sentence-level QE system in deciding whether APE has improved a translation or not (Bhattacharyya et al., 2022)."
        },
        {
            "heading": "3 Standalone APE and QE Systems",
            "text": "This section discusses the approaches used to develop standalone APE and QE systems. We follow the state-of-the-art approaches for training the systems as it allows us to investigate whether the findings of Chatterjee et al. (2018) hold when neural APE and QE systems are used to post-edit and assess the quality of NMT-generated translations."
        },
        {
            "heading": "3.1 Automatic Post-Editing",
            "text": "The subsection describes a standalone neural APE system that we use as a baseline as well and refer to it as APE w/o QE.\nArchitecture We develop the APE system using transformer-based encoder-decoder architecture. We use two separate encoders for the EnglishMarathi APE system to encode a source sentence and its translation, as these languages do not share script or vocabulary. Outputs of both encoders are passed to two consecutive cross-attention layers in the decoder. An architecture shown in Figure 1 without the Sentence-QE and Word-QE heads represents the English-Marathi APE architecture. While for the English-German APE system, we use a single-encoder single-decoder architecture as there is a script and vocabulary overlap between these two languages. Therefore, a single encoder encodes the concatenation of source and translation generated by adding a \u2018<SEP>\u2019 tag between them, and the encoder output is passed to a single crossattention layer in the decoder. For both the pairs, the encoders are initialized using IndicBERT (Kakwani et al., 2020) weights.\nDataset We use datasets released through the WMT21 (Akhbardeh et al., 2021) and WMT22 (Bhattacharyya et al., 2022) EnglishGerman and English-Marathi APE shared tasks, respectively, to conduct the experiments. The\nAPE data consists of real (human-generated postedits) and synthetic (artificial post-edits) (JunczysDowmunt and Grundkiewicz, 2016). The EnglishMarathi APE dataset contains 18K real APE triplets from General, News, and Healthcare domains in the train set, and the synthetic APE data contains around 2.5M triplets from multiple domains. The train set for the English-German pair contains 7K real APE triplets from the general domain, and the synthetic APE data, eSCAPE (Negri et al., 2018), contains around 4M shuffled triplets from multiple domains. We use the corresponding development sets containing 1K APE triplets to evaluate the APE systems.\nWe also use the parallel corpora during the APE training phase. For the English-Marathi pair, we use Anuvaad2, Samanantar (Ramesh et al., 2022), and ILCI (Bansal et al., 2013) datasets containing around 6M sentence pairs. While for the EnglishGerman pair, we use the News-Commentary-v16 WMT22 MT task dataset (Kocmi et al., 2022) of around 10M sentence pairs.\nData Augmentation and Pre-processing We augment the synthetic APE data with automatically generated phrase-level APE triplets. First, we train source-to-translation and source-to-post-edit phrase-based statistical MT systems by employing Moses (Koehn et al., 2007). The phrase pairs from both MT systems are extracted in the next step. Then, we form the APE triplet by matching the source side of both phrase pairs. We control the quality of synthetic APE triplets (including the phrase-level APE triplets) by performing LaBSEbased filtering (Feng et al., 2022) and filter the low-quality triplets from the synthetic APE data. It is done by computing cosine similarity between the normalized embeddings of a source sentence and its corresponding post-edited translation. We retain the triplet if the cosine similarity is more than 0.91. We get around 50K and 60K phrase-level triplets for English-Marathi and English-German pairs, respectively.\nModel Training We follow the Curriculum Training Strategy (CTS) similar to Oh et al. (2021) for training our APE systems. It involves gradually adapting a model to more and more complex tasks. The steps of the CTS are described below.\nIn the first step, we train a single-encoder singledecoder model for performing source-to-target lan-\n2Anuvaad Parallel Corpus\nguage translation using the parallel corpus. In the next step, we add another encoder to the encoderdecoder model for English-Marathi APE while we use the same architecture for the English-German APE. We train the resulting model for the APE task using the synthetic APE data in the two phases for English-Marathi and in one phase for EnglishGerman. In the first phase, we train the model for the APE task using the out-of-domain APE triplets (i.e. any domains except the General, News, and Healthcare for English-Marathi). In the second phase, we train the model using the in-domain synthetic APE triplets. As the English-German APE data is of general (news or wiki) domain, we train the English-German APE model in a single phase using all synthetic data. Finally, we fine-tune the APE model using in-domain real APE data. Equation 1 shows the cross-entropy loss function used to train the APE model.\nLAPE = \u2212 |S|\u2211 w=1 |V |\u2211 e=1 yw,e log (y\u0302w,e) (1)\nWhere |S| and |V | denote the number of tokens in the sentence and the number of tokens in the vocabulary, respectively. The APE output is denoted by the y\u0302w,e, while yw,e represents the ground truth."
        },
        {
            "heading": "3.2 Quality Estimation",
            "text": "This section describes the standalone sentence(Sent-QE) and word-level (Word-QE) QE systems.\nArchitecture We use a transformer encoder to develop the QE models. To obtain representations of the input (concatenated source sentence and its translation), we employ XLM-R (Conneau et al., 2020). This model is trained on a massive multilingual dataset of 2.5TB, which includes 104 different languages, and the training is conducted using the masked language modeling (MLM) objective, similar to RoBERTa (Liu et al., 2019). The WMT20 sentence- and word-level QE shared task winning systems have utilized XLM-R-based QE models (Ranasinghe et al., 2020a; Lee, 2020). Therefore, we adopt a similar strategy for both QE tasks. We add a feedforward layer on the top of XLM-R to perform regression (sentence-level QE) and token-level classification (word-level QE).\nDataset For the QE tasks as well, we use datasets released through the WMT21 (Specia et al., 2021) and WMT22 (Zerva et al., 2022) QE shared tasks.\nThe WMT21 QE shared task data for EnglishGerman sentence- and word-level QE includes 7K and 1K instances in the train and development sets, respectively. For English-Marathi, the WMT22 dataset contains 26K and 1K instances in the train and development sets. For evaluating the QE systems, we use the corresponding development sets.\nEach sample in the word-level English-German QE data consists of a source sentence, its translation, and a sequence of tags for tokens and gaps. The WMT22 dataset does not contain tags for gaps between translation tokens for the English-Marathi pair. So, we used the QE-corpus-builder3 to obtain annotations for translations using their post-edits.\nTraining Approach We train XLM-R-based Sent-QE and Word-QE models for each language pair using the respective sentence- and word-level QE task datasets. During training, the weights of all layers of the model are updated.\nSentence-level Quality Estimation Head This task is modeled as a regression task. We use the hidden representation of the classification token (CLS) of the transformer model to predict normalized DA scores (Zerva et al., 2022) through the application of a linear transformation:\ny\u0302da = W T [CLS] \u00b7 h[CLS] + b[CLS] (2)\nwhere \u00b7 denotes matrix multiplication, W[CLS] \u2208 RD\u00d71, b[CLS] \u2208 R1\u00d71, and D is the dimension of input layer h (top-most layer of the transformer). Equation 3 shows the Mean Squared Error (MSE) loss used for this task.\nLsent = MSE ( yda, y\u0302da ) (3)\nWord-level Quality Estimation Head We treat this task as a token-level classification task. We predict the word-level labels (OK/BAD) by applying a linear transformation (also followed by the softmax) over every input token from the last hidden layer of the XLM-R model:\ny\u0302word = \u03c3(W T word \u00b7 ht + bword) (4)\nwhere t marks which token the model is to label within a T -length window/token sequence, Wword \u2208 RD\u00d72, and bword \u2208 R1\u00d72. The crossentropy loss utilized for training the model is depicted in Equation 5, which bears similarity to\n3https://github.com/deep-spin/ qe-corpus-builder\nthe architecture of MicroTransQuest as described in Ranasinghe et al. (2021).\nLword = \u2212 2\u2211\ni=1\n( yword \u2299 log(y\u0302word) ) [i] (5)\nRefer Appendix A for the details about the hyperparameters and the hardware used to conduct all the experiments."
        },
        {
            "heading": "4 Progressively Integrating QE with APE",
            "text": "This section discusses three existing strategies for combining QE with APE. Starting with lighter combination approaches, we move towards strongly coupled ones that help APE learn from the QE subtasks (sentence- and word-level QE)."
        },
        {
            "heading": "4.1 QE as APE Activator",
            "text": "In this strategy, we use a QE system to decide whether a translation requires post-editing. We pass the source sentence and its translation to a sentencelevel QE system to get a DA score prediction. If the DA score is below a decided threshold4, only then do we use the APE system to generate the post-edited version of the translation."
        },
        {
            "heading": "4.2 QE as MT/APE Selector",
            "text": "Contrary to the QE as APE Activator approach, we use a QE system to decide whether an APE output is an improved version of a translation or not. This is done using a sentence-level QE system to get DA score predictions for the original translation and corresponding APE output. We consider the APE output as the final output only if it receives a higher DA score than the original translation."
        },
        {
            "heading": "4.3 QE as APE Guide",
            "text": "In the earlier two approaches, QE information is not passed to APE, but a sentence-level QE system is utilized merely to decide whether to APE or consider the APE output. Through the current approach, we explore a tighter combination of QE and APE by passing the sentence-level and wordlevel information obtained from the QE systems as additional inputs to APE. We use sentence-level QE systems to predict either DA scores or TER (Snover et al., 2006) scores. If we pass just the predicted DA or TER score to APE, we prepend it. If both scores are to be passed, then we modify the input format:\n4decided empirically (Chatterjee et al., 2018).\n<DA_Score > <TER_Score > <Source_sentence > <Target_sentence >. Similarly, to pass the wordlevel QE information, we add a <BAD > token before every source and translation token for which a BAD tag is predicted."
        },
        {
            "heading": "5 Joint Training over QE and APE",
            "text": "This approach investigates the tightest coupling between the QE tasks (sentence- and word-level QE) and APE. We follow exactly the same steps in the CTS (explained in Section 3.1), except the last one, and train the model for the APE task. During the last (fine-tuning) stage of the CTS, we jointly train the model on the APE and QE tasks using the real APE training data and the QE training data. To do so, we add the task-specific heads (refer Section 3.2) on top of a shared representation layer that receives inputs from the final encoder layers (Refer Figure 1). The representation layer has 2x neurons for En-Mr than for En-De. For EnDe, its size is equal to the size of the final encoder layer. The following two multi-task learning (MTL) approaches are used to perform the experiments.\nLinear Scalarization (LS-MTL) We use a straightforward MTL approach, LS-MTL, to combine the task-specific losses.\nLLS\u2212MTL = Lsent + Lword + LAPE (6)\nAll loss functions are weighed equally and are added together to get the combined loss (LLS\u2212MTL) as shown in Equation 6.\nNash-MTL To further explore the use of sophisticated MTL methods for training the APE model, we choose the Nash-MTL approach proposed by Navon et al. (2022), as the authors have shown that their proposed approach outperforms several other MTL methods through experiments on two different sets of tasks.\nThe utilization of MTL to jointly train a single model has been recognized as a means to reduce computation costs. However, conflicts arising from gradients of different tasks often lead to inferior performance of the jointly trained model compared to individual single-task models. To address this challenge, a widely used technique involves combining per-task gradients into a unified update direction using a heuristic. In this approach, the tasks negotiate for a joint direction of parameter update.\nAlgorithm 1 Nash-MTL\nInput: \u03b80 - initial parameter vector, {li}Ki=1 - differentiable loss functions, \u03b7 - learning rate Output: \u03b8T for t = 1,..., T do\nCompute task gradients gti = \u2207\u03b8(t\u22121)li Set G(t) the matrix with columns g(t)i Solve for \u03b1 : (Gt)T (Gt)\u03b1 = 1/\u03b1 to obtain\n\u03b1(t)\nUpdate the parameters \u03b8(t) = \u03b8(t) \u2212 \u03b7G(t)\u03b1(t) end for return \u03b8T\nThe Nash-MTL approach considers a spherical region centered at the origin with a radius of \u03f5, for the MTL problem having parameters \u03b8. The update vectors are constrained within this sphere. The problem is formulated as a bargaining problem, where the center of the sphere represents the point of disagreement and the region serves as an agreement set. Each player\u2019s utility function is the dot product between the gradient vector gi of task i\u2019s loss li at \u03b8 and the update vector. Navon et al. (2022) showed that a Nash bargaining solution is a solution to (Gt)T (Gt)\u03b1 = 1/\u03b1. Algorithm 1 shows the process followed in the Nash-MTL method to update the parameters."
        },
        {
            "heading": "6 Results and Analysis",
            "text": "This section discusses an evaluation of the QE and APE combination strategies. The performance of the APE systems is reported on the WMT development sets in terms of TER (Snover et al., 2006) and BLEU (Papineni et al., 2002) scores. We perform a statistical significance test considering primary metric (TER) using William\u2019s significance test (Graham, 2015).\nFor comparison, we use two APE baselines as follows: (i) a Do-Nothing baseline that does not modify original translations, as obtained from the\nMT system. (ii) A standalone APE system that is unassisted by a QE system (APE w/o QE).\nExperiments for the first two combination strategies (QE as APE activator and QE as MT/APE selector) are performed using the standalone APE and QE systems. For the QE as APE Activator experiments, we experiment with different thresholds of the DA scores and report the best results. The QE as APE Guide experiments use the same standalone sentence-level and word-level QE systems, but we modify the APE inputs from the second step of the CTS onwards, for each experiment, and train different APE models following the same training approach described in Section 3.\nIn the MTL-based experiments, first, we use the LS-MTL approach to perform various experiments using a combination of tasks to see the improvements brought by a simple MTL approach. Further, we try to optimize the performance on the best combination of tasks through the Nash-MTL approach.\nQE as APE Activator We experimented with various threshold DA scores for each language pair and tried to find the optimal threshold value over a held-out subset taken from the train set. Thresholds of 0.32 and 0.35 DA scores give the best results\nfor English-Marathi and English-German language pairs, respectively.\nThe TER and BLEU scores obtained using this approach for both language pairs are reported in Table 1. We see a drop in performance compared to the APE w/o QE baseline system for both language pairs. For English-German, we observe that the performance is even poorer than the Do-Nothing baseline, which suggests the possibility that the APE can correct minor errors even in the translations that receive a very high DA score. A possible reason for not seeing improvements over the standalone APE system could be that the sentence-level QE information is too coarse for APE to figure out whether a translation requires editing.\nQE as MT/APE Selector Table 2 shows the helpfulness of this combination strategy. We see 0.38 and 0.07 TER point improvements over the APE w/o QE baseline system for English-Marathi and English-German pairs, respectively. The results highlight the problem of over-correction that the neural APE systems face. We conjecture that the sentence-level QE information may be too coarse to decide whether a translation needs post-editing. Still, it can be used to compare the translations.\nQE as APE Guide Table 3 reports the results of experiments when different combinations of the QE information are embedded into the APE inputs. It shows that passing sentence-level QE information to APE is not much effective. A possible reason could be the coarse nature of the sentence-level QE as discussed earlier. In particular, we see better performance when predicted DA scores are passed than the predicted HTER scores, which do not consider the semantic meaning.\nWe observe the best results when the source and translation tokens, tagged as BAD by the wordlevel QE, are annotated and passed to the APE for the English-Marathi pair. For the English-German pair, we get the best results when both the predicted DA scores and the word-level QE predicted information is passed as additional inputs to APE. With these combinations, we get the 0.58 and 0.07 TER point improvements for the English-Marathi and English-German pairs, respectively. It suggests that granular word-level QE information helps APE identify poorly translated segments, thereby helping it focus on their correction.\nJoint Training over QE and APE Through these experiments, we explore the most robust cou-\npling of QE and APE. Table 4 compiles MTL-based results. We jointly train a model using LS-MTL in each experiment on a different combination of the APE and QE tasks. We observe better improvements by training on the word-level QE task and APE than when combining the sentence-level QE (DA prediction) task and APE. Further addition of the sentence-level QE (HTER prediction) does not significantly improve the performance. Unlike the findings from the QE as APE Guide experimental results, we get better improvements when sentencelevel QE (DA prediction) and word-level QE tasks are used along with APE.\nAs jointly training a model using LS-MTL on the APE, Sent-QE (DA), and Word-QE tasks yields high improvements, we try to improve it using an advanced MTL method. Using the Nash-MTL method, we get further APE performance enhancements of 0.24 and 0.15 TER points for EnglishMarathi and English-German pairs, respectively. Similarly, gains over the APE w/o QE baseline system are 1.09 and 0.46 TER points.\nAdditional analysis, like the efficiency of these combination strategies, the number of translations improved or deteriorated by the jointly trained systems, and types of editing performed by the models, is presented in Appendix B and Appendix C.\nKey Observation Our quantitative analysis from the comparison between the improvements (Table 5, Figure 4) brought by each combination strategy over the APE w/o QE baseline shows that as the QE and APE coupling gets stronger, we see better enhancements in the APE performance.\nQualitative Analysis We compared the EnglishMarathi QE-assisted APE outputs (of the NashMTL-based method) with the corresponding APE w/o QE outputs. Our finding that QE helps APE mitigate the over-correction issue is supported by multiple examples from the sample of the data we analyzed. We show two examples in Figure 2.\nThe first example shows a fluent and highly adequate translation. Unlike the QE-unassisted APE system that modifies the translation and replaces three words, compromising adequacy, the QE-assisted APE leaves all translation words untouched, except the \u2018ghaaloon\u2019, which is tagged as BAD by the Word-QE, is changed to \u2018ghaatalelee\u2019 which improves the fluency. In the second example, the APE w/o QE system has compromised the fluency by dropping the pronoun \u2018he\u2019 in the trans-\nlation and by translating the English phrase \u2018make the rhythmic sound\u2019 literally. For this translation, the Sent-QE prediction suggests a need for some editing. The Word-QE correctly tags the word\n\u2018pasaralelyaa\u2019 as \u2018BAD.\u2019 It wrongly tags the word \u2018yantra\u2019 as \u2018OK\u2019 and the gap before the last word (suggesting missing words before it) as \u2018BAD.\u2019 We\nsee the QE-assisted APE corrects the translation of the BAD-tagged word but is unaffected by the wrongly tagged gap."
        },
        {
            "heading": "7 Conclusion and Future Work",
            "text": "In this paper, we have investigated four strategies to integrate QE systems into the MT-APE pipeline, intending to improve APE performance when translations to be post-edited are obtained from an NMT system. Experimental results highlight the complementary nature of APE and QE tasks, which can be best utilized through our proposed approach: joint training of a model that learns both tasks simultaneously. The quantitative results and our qualitative analysis suggest that the QE helps APE to address the over-correction problem.\nAmong the existing combination strategies, the QE as MT/APE Selector improves performance over QE-unassisted APE by discarding low-quality\nAPE outputs. APE Guide is a more beneficial approach as we observe even better improvements using the tighter combination strategy. It provides APE with explicit information about the overall translation quality and erroneous segments. Moreover, a comparison of these combination strategies shows that tighter coupling between QE and APE is increasingly beneficial for improving APE.\nWe plan to conduct a thorough qualitative analysis of QE-assisted APE systems to gain a deeper understanding of the contribution of QE. We also aim to strengthen the coupling between APE and QE to enhance APE performance further. Additionally, we intend to conduct a study to evaluate the potential of APE in improving QE systems."
        },
        {
            "heading": "8 Limitations",
            "text": "In the current work, we focus on using QE to improve APE, and we see that the joint training on the QE tasks and APE helps improve APE performance. This joint training may also benefit QE. However, we have not investigated it in the current work. We observe more minor performance improvements in the case of the English-German pair, having a strict Do-Nothing baseline, compared to the English-Marathi. It suggests that the law of diminishing returns affects the QE and APE combination strategies. Improvements obtained by the QE and APE coupling tend to get lower as the quality of original translations keeps improving. The current work has compared four QE and APE combination strategies using only two language pairs due to the unavailability of an adequate amount of good-quality APE and QE datasets for other language pairs. Current findings may be domainspecific, and future research could evaluate the proposed combination strategies on diverse domains to assess their generalizability and robustness."
        },
        {
            "heading": "9 Ethics Statement",
            "text": "Our APE and QE models are trained on publicly available datasets referenced in this paper. These datasets have been previously collected and annotated; no new data collection has been carried out as part of this work. Furthermore, these are standard benchmarks released in recent WMT shared tasks. No user information was present in the datasets, protecting the privacy and identity of users. We understand that every dataset is subject to intrinsic bias and that computational models will inevitably learn biased information from any dataset."
        },
        {
            "heading": "A Training Details",
            "text": "To maintain uniformity across all the experiments, we use an identical set of settings for all the QE and APE experiments. Our APE models are trained using a batch size of 32. We specified a maximum of 1000 epochs for training, implementing early stopping with patience of 5. The Adam optimizer was employed with a learning rate of 5 x 10\u22125,\nalong with \u03b21 set to 0.9, and \u03b22 set to 0.997. Additionally, we utilized 25,000 warmup steps. On the decoder side, beam search was applied with a beam size of 5. For the QE experiments, we use a batch size of 16. We start with a learning rate of 2e\u2212 5 and use 5% of training data for warm-up. We use early stopping and patience over 20 steps. In the QE experiments and all MTL-based experiments, we used WandB for hyperparameter search. All the experiments are performed using NVIDIA A100 GPUS. The APE model contains around 40M parameters and training the model using CTS takes about 48 hours. The QE model has about 125M parameters and training one QE model takes around\n2.25 hours. For pre-processing the English and German data, we used the NLTK library5, and the IndicNLP library6 is used for processing Marathi text. We used Pytorch7 for Model training and inference. For computing the TER scores, we use the official WMT APE and QE evaluation script8, and for computing the BLEU scores, we use the SacreBLEU9 library.\n5https://www.nltk.org/ 6https://github.com/anoopkunchukuttan/\nindic_nlp_library 7https://pytorch.org/ 8https://github.com/sheffieldnlp/ qe-eval-scripts 9https://github.com/mjpost/sacrebleu"
        },
        {
            "heading": "B Efficiency of APE Systems",
            "text": "Table 6 shows the inference time of the four APEQE coupling strategies for 1K Marathi translations on the same single GPU. The first two QE-APE combination strategies are notably faster than the latter two. The \u2018QE as APE Guide\u2019 strategy shows the highest latency, likely due to word-level tag generation from QE and then longer inputs passed to APE. Even though it is pipeline-free, the fourth strategy exhibits somewhat higher latency than the second strategy; the likely reason could be that it involves word-level tag generation. To summarize, strategy choice has a high impact on efficiency. Despite having slightly higher latency than the \u2018QE as APE Selector\u2019 strategy, the \u2018QE-APE MTL-based approach\u2019 remains the optimal choice due to its superior overall performance."
        },
        {
            "heading": "C Additional Quantitative Analysis",
            "text": "We analyzed the number of translations improved and deteriorated by the jointly trained APE models on the QE and APE tasks. The results are reported in Table 7. Also, the number of edit operations performed by these jointly trained models are compiled in Table 8"
        },
        {
            "heading": "D Example Post-Edits with English Transliteration",
            "text": "Figure 3 contains English transliterations of the Marathi sentences shown in Figure 2."
        }
    ],
    "title": "Quality Estimation-Assisted Automatic Post-Editing",
    "year": 2023
}