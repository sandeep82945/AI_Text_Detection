{
    "abstractText": "This paper studies the use of reinforcement learning for conditional text generation, which overcomes the limitation of the prevalent supervised maximum likelihood estimation approach. However, it still suffers from challenges including the large action space and the delayed reward, as the reward can be computed only after an entire sequence is generated. To address these challenges, we propose a method that provides partial rewards for intermediate actions taken on partial sequences. This enables the model to promptly prioritize actions that lead to the generation of more desirable sequences. Our method\u2019s key contribution lies in its focus on distinguishing relatively more desirable actions rather than striving to precisely estimate pointwise values for arbitrary partial sequences. Instead, our reward shaping method learns to discern the relative desirability between pairs of actions, or rank actions in a pairwise manner, only when necessary and feasible. This is materialized in an efficient way by leveraging the prefix tree constructed from the sampled sequences. Experimental results on paraphrase generation and constrained machine translation tasks showcase the effectiveness of our method.1",
    "authors": [
        {
            "affiliations": [],
            "name": "Youngwon Lee"
        },
        {
            "affiliations": [],
            "name": "Jinu Lee"
        },
        {
            "affiliations": [],
            "name": "Seung-won Hwang"
        }
    ],
    "id": "SP:59331dd9b7124ba705c28f6e4133cd8ac3f5c642",
    "references": [
        {
            "authors": [
                "Dzmitry Bahdanau",
                "Philemon Brakel",
                "Kelvin Xu",
                "Anirudh Goyal",
                "Ryan Lowe",
                "Joelle Pineau",
                "Aaron C. Courville",
                "Yoshua Bengio."
            ],
            "title": "An actor-critic algorithm for sequence prediction",
            "venue": "5th International Conference on Learning Representations,",
            "year": 2017
        },
        {
            "authors": [
                "Yue Cao",
                "Xiaojun Wan."
            ],
            "title": "DivGAN: Towards diverse paraphrase generation via diversified generative adversarial network",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 2411\u20132421, Online. Association for Computational",
            "year": 2020
        },
        {
            "authors": [
                "Hou Pong Chan",
                "Wang Chen",
                "Lu Wang",
                "Irwin King."
            ],
            "title": "Neural keyphrase generation via reinforcement learning with adaptive rewards",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2163\u20132174, Florence,",
            "year": 2019
        },
        {
            "authors": [
                "Guanhua Chen",
                "Yun Chen",
                "Victor O.K. Li."
            ],
            "title": "Lexically constrained neural machine translation with explicit alignment guidance",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, 35(14):12630\u201312638.",
            "year": 2021
        },
        {
            "authors": [
                "Ting Chen",
                "Simon Kornblith",
                "Mohammad Norouzi",
                "Geoffrey Hinton."
            ],
            "title": "A simple framework for contrastive learning of visual representations",
            "venue": "Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings",
            "year": 2020
        },
        {
            "authors": [
                "Angela Fan",
                "Mike Lewis",
                "Yann Dauphin."
            ],
            "title": "Hierarchical neural story generation",
            "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 889\u2013898, Melbourne, Australia. Association",
            "year": 2018
        },
        {
            "authors": [
                "Tianyu Gao",
                "Xingcheng Yao",
                "Danqi Chen."
            ],
            "title": "SimCSE: Simple contrastive learning of sentence embeddings",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6894\u20136910, Online and Punta Cana, Do-",
            "year": 2021
        },
        {
            "authors": [
                "Han Guo",
                "Bowen Tan",
                "Zhengzhong Liu",
                "Eric Xing",
                "Zhiting Hu."
            ],
            "title": "Efficient (soft) Q-learning for text generation with limited good data",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2022, pages 6969\u20136991, Abu Dhabi, United Arab",
            "year": 2022
        },
        {
            "authors": [
                "Ari Holtzman",
                "Jan Buys",
                "Li Du",
                "Maxwell Forbes",
                "Yejin Choi."
            ],
            "title": "The curious case of neural text degeneration",
            "venue": "8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net.",
            "year": 2020
        },
        {
            "authors": [
                "Tomoyuki Kajiwara."
            ],
            "title": "Negative lexically constrained decoding for paraphrase generation",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6047\u2013 6052, Florence, Italy. Association for Computational",
            "year": 2019
        },
        {
            "authors": [
                "Julia Kreutzer",
                "Artem Sokolov",
                "Stefan Riezler."
            ],
            "title": "Bandit structured prediction for neural sequence-tosequence learning",
            "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1503\u20131513,",
            "year": 2017
        },
        {
            "authors": [
                "Ashutosh Kumar",
                "Kabir Ahuja",
                "Raghuram Vadapalli",
                "Partha Talukdar."
            ],
            "title": "Syntax-guided controlled generation of paraphrases",
            "venue": "Transactions of the Association for Computational Linguistics, 8:329\u2013345.",
            "year": 2020
        },
        {
            "authors": [
                "Mike Lewis",
                "Yinhan Liu",
                "Naman Goyal",
                "Marjan Ghazvininejad",
                "Abdelrahman Mohamed",
                "Omer Levy",
                "Veselin Stoyanov",
                "Luke Zettlemoyer"
            ],
            "title": "BART: Denoising sequence-to-sequence pre-training for natural language",
            "year": 2020
        },
        {
            "authors": [
                "Jiwei Li",
                "Will Monroe",
                "Tianlin Shi",
                "S\u00e9bastien Jean",
                "Alan Ritter",
                "Dan Jurafsky."
            ],
            "title": "Adversarial learning for neural dialogue generation",
            "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2157\u20132169, Copen-",
            "year": 2017
        },
        {
            "authors": [
                "Tsung-Yi Lin",
                "Michael Maire",
                "Serge J. Belongie",
                "James Hays",
                "Pietro Perona",
                "Deva Ramanan",
                "Piotr Doll\u00e1r",
                "C. Lawrence Zitnick."
            ],
            "title": "Microsoft COCO: common objects in context",
            "venue": "Computer Vision ECCV 2014 - 13th European Conference, Zurich,",
            "year": 2014
        },
        {
            "authors": [
                "Yixin Liu",
                "Pengfei Liu",
                "Dragomir Radev",
                "Graham Neubig."
            ],
            "title": "BRIO: Bringing order to abstractive summarization",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2890\u20132903,",
            "year": 2022
        },
        {
            "authors": [
                "Clara Meister",
                "Ryan Cotterell",
                "Tim Vieira."
            ],
            "title": "If beam search is the answer, what was the question? In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2173\u20132185, Online",
            "venue": "Association for Computa-",
            "year": 2020
        },
        {
            "authors": [
                "Nathan Ng",
                "Kyra Yee",
                "Alexei Baevski",
                "Myle Ott",
                "Michael Auli",
                "Sergey Edunov."
            ],
            "title": "Facebook FAIR\u2019s WMT19 news translation task submission",
            "venue": "Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day",
            "year": 2019
        },
        {
            "authors": [
                "Animesh Nighojkar",
                "John Licato."
            ],
            "title": "Improving paraphrase detection with the adversarial paraphrasing task",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference",
            "year": 2021
        },
        {
            "authors": [
                "Tong Niu",
                "Semih Yavuz",
                "Yingbo Zhou",
                "Nitish Shirish Keskar",
                "Huan Wang",
                "Caiming Xiong."
            ],
            "title": "Unsupervised paraphrasing with pretrained language models",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2021
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J. Liu"
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text",
            "year": 2020
        },
        {
            "authors": [
                "Marc\u2019Aurelio Ranzato",
                "Sumit Chopra",
                "Michael Auli",
                "Wojciech Zaremba"
            ],
            "title": "Sequence level training with recurrent neural networks",
            "venue": "In 4th International Conference on Learning Representations,",
            "year": 2016
        },
        {
            "authors": [
                "Lingfeng Shen",
                "Lemao Liu",
                "Haiyun Jiang",
                "Shuming Shi."
            ],
            "title": "On the evaluation metrics for paraphrase generation",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 3178\u20133190, Abu Dhabi, United Arab Emi-",
            "year": 2022
        },
        {
            "authors": [
                "Shiqi Shen",
                "Yong Cheng",
                "Zhongjun He",
                "Wei He",
                "Hua Wu",
                "Maosong Sun",
                "Yang Liu."
            ],
            "title": "Minimum risk training for neural machine translation",
            "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long",
            "year": 2016
        },
        {
            "authors": [
                "Felix Stahlberg",
                "Bill Byrne"
            ],
            "title": "On NMT search errors and model errors: Cat got your tongue",
            "venue": "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Lan-",
            "year": 2019
        },
        {
            "authors": [
                "Shuo Wang",
                "Peng Li",
                "Zhixing Tan",
                "Zhaopeng Tu",
                "Maosong Sun",
                "Yang Liu."
            ],
            "title": "A templatebased method for constrained neural machine translation",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2022
        },
        {
            "authors": [
                "Ronald J. Williams."
            ],
            "title": "Simple statistical gradientfollowing algorithms for connectionist reinforcement learning",
            "venue": "Mach. Learn., 8(3\u20134):229\u2013256.",
            "year": 1992
        },
        {
            "authors": [
                "Yilin Yang",
                "Liang Huang",
                "Mingbo Ma."
            ],
            "title": "Breaking the beam search curse: A study of (re-)scoring methods and stopping criteria for neural machine translation",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Process-",
            "year": 2018
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Conditional text generation encompasses various tasks, with the objective of generating a sequence y based on a given source sequence x, in a way that maximizes the utility u. For example, in paraphrase generation, which we use as our running example, u is high when (1) y matches the meaning of the source sequence x, (2) y is fluent in terms of language modeling, and (3) it has high lexical dissimilarity from x (Kumar et al., 2020;\n\u2217Equal contribution. \u2020Corresponding author.\n1Our code is publicly available at: https://github.com/ jinulee-v/PairwisePartialReward\nCao and Wan, 2020; Nighojkar and Licato, 2021, inter alia).\nSupervised learning, particularly maximum likelihood estimation (MLE) training, has typically been the standard approach for conditional text generation. In this approach, the model is trained to maximize the likelihood of the reference given the source. During inference, it generates output sequences for which it assigns high probability.\nThis approach is known to suffer from two significant challenges: exposure bias and train-test objective discrepancy (Ranzato et al., 2016). To address both challenges, reinforcement learning (RL) has gained attention as an alternative (Shen et al., 2016; Kreutzer et al., 2017). RL formulates autoregressive text generation as a Markov decision process where a state corresponds to a generated prefix under construction, 2 an action represents the selection of the next token, guided by the policy network, which is essentially the language model. The agent\u2019s goal is to maximize the reward it receives, which is determined by the utility u of the output sequence.\nHowever, this utility can be computed only after the generation is finished. This sparse and delayed reward problem introduces challenges in learning the impact of each action taken by the model, known as the credit assignment problem in RL, as depicted in Figure 1 (Guo et al., 2022; Li\n2Or, defined to include the source, st = (x,y\u2264t).\net al., 2017). When a pointwise estimation of each partial candidate is unreliable, generation leads to undesirable sequences such as blank or repeated n-grams (Holtzman et al., 2020).\nWe hypothesize that such pointwise estimation of state values, or equivalently expecting models to predict a total order over all partially generated sequences, is an overly ambitious objective. In this sense, while text degeneration is often attributed to the failure of decoding, we show that the policy, the model itself, is more responsible to the failure.\nTo support our claim that language models indeed struggle with the pointwise estimation of the final outcome from partially generated sequences, we suggest comparing their performance with beam search using different beam sizes, as illustrated in Figure 2. Beam search can be seen as providing some tolerance for the model\u2019s mistakes by holding on to candidate sequences even when its prefix values are underestimated (Meister et al., 2020). Ideally, if the model\u2019s belief about the sequence\u2019s utility aligns positively with the actual utility of the fully generated sequence, increasing the beam size should only increase such a tolerance. We refer to this property as beam monotonicity, which is violated in a baseline model shown in Figure 2A.\nSimilarly, reducing the beam size should not significantly hurt the performance if the model accurately estimates values of prefixes in earlier stages, as higher-ranked candidates would still survive with a smaller beam size. We call this property beam resilience, which is violated in Figure 2B. A reliable policy should satisfy both properties, as shown in Figure 2C.\nOur distinction is proposing a reliable policy in\nFigure 2C, by adjusting an overly ambitious requirement of full ordering, to partially ordering a subset of possible pairs of partial sequences. Essentially, we aim to train the model to choose actions that will lead to better outcomes, aligning with our training objective. This approach also provides the model with partial rewards, offering direct feedback on individual actions long before the delayed reward becomes available.\nOur method, called PPR (Pairwise Partial Reward), implements this idea by utilizing a prefix tree constructed from the sampled sequences. PPR employs a pairwise ranking objective for the branching nodes in the prefix tree. Firstly, we assess the desirability of following each branch by considering the leaf nodes in the corresponding subtree and their associated observed utilities. Then, we compare pairs of branches and encourage the model to prefer the branch with a higher expected return, effectively establishing a partial order over partially generated sequences. For example, in Figure 3, PPR can encourage the model to prefer action (or the resulting partial sequence) c over a or b, defining ordering of a \u2192 c and b \u2192 c as shown in the figure.\nImportantly, PPR does not require the model to directly compare its value estimate of c with other partial sequences apart from a or b. Additionally, it can be seen as the model receiving partial reward for choosing the action c. In summary, this paper makes two contributions:\n\u2022 We introduce a novel method for reward shaping in conditional text generation through pairwise learning to rank.\n\u2022 We demonstrate the effectiveness of our approach, showcasing not only high-quality generation but also improved estimation of partial sequence values through beam monotonicity and resilience."
        },
        {
            "heading": "2 Background",
            "text": "As a background, we formulate the description of how autoregressive conditional text generation is formulated as a Markov decision process and the agent-environment interaction is defined to transform the task into an RL problem. As mentioned before, state st at time step t corresponds to (x,y\u2264t), and the possible action at at this moment is the act of choosing the next token, yt+1 to reach the next state st+1. Here the environment is fully observable and the state transition given the action is deterministic, that is, selecting an action (the next token) is equivalent to selecting the next state (prefix with length increased by 1). A conditional language model then can be regarded as directly parameterizing the policy, \u03c0\u03b8(at | st) = p\u03b8(yt+1 |x,y\u2264t).\nTypically, the interaction with the environment is simulated as the delayed reward\nRt = { u(y;x), if t = |y|, 0, otherwise,\n(1)\nwhere Rt := R(st\u22121, at\u22121, st) is the reward for making transition from st\u22121 to st by choosing the action at. Delayed reward hinders the model from figuring out which actions in the past should be attributed to the final outcome."
        },
        {
            "heading": "3 Method",
            "text": ""
        },
        {
            "heading": "3.1 Baselines",
            "text": "We first briefly introduce the baseline methods we consider and proceed to our proposed method."
        },
        {
            "heading": "3.1.1 Minimum risk training",
            "text": "Minimum risk training (MRT) (Shen et al., 2016) is one of the most representative RL practices for conditional text generation. It is a slightly altered variation of the REINFORCE algorithm (Williams, 1992), or Monte Carlo policy gradient. Initially adopted in NLP for machine translation task, MRT uses sequences sampled from the model to estimate the gradient of the expected cumulative reward to be obtained under the current policy. It minimizes the risk, or the negative expected reward\nL = \u2212 \u2211 y\u2208Y q(y)u(y;x), (2)\nwhere Y is the set of samples generated with beam search, and q(y) is the weight of y, which is computed as the normalized model-assigned likelihood\nof y with tempering. The weight term q is similar to the importance weight in importance sampling, but it differs in that here q values are normalized over the set Y."
        },
        {
            "heading": "3.1.2 BRIO",
            "text": "BRIO (Liu et al., 2022), one of the state-of-the-art models for abstractive summarization, also serves as the baseline for our study.\nContrastive learning has been proven effective at learning embeddings of images and sentences (Chen et al., 2020; Gao et al., 2021), and it has recently gained attention within conditional text generation. BRIO utilizes a contrastive objective between full sequences for rank learning. It assigns higher (length-normalized) probability f to sequences with high utility u using a pairwise contrastive loss\nLc = \u2211 ui>uj max ( 0, f(j)\u2212 f(i)+\u03bb(j\u2212 i) ) ,\n(3)\nwhere i and j are the ranks of two samples yi and yj , with i being the higher rank, that is, ui := u(yi;x) > u(yj ;x) =: uj , and f(i) := (1/T\u03b1) \u2211 t log p\u03b8(y i t |x,yi<t) is the length penalized log likelihood of the sequence yi with length penalty hyperparameter \u03b1. \u03bb is a hyperparameter for determining the margin which is proportional to the rank difference j \u2212 i. This contrastive loss is then used in conjunction with the MLE loss (the negative log-likelihood loss for the reference) to train the model:\nL = LMLE + \u03b3 \u00b7 Lc. (4)"
        },
        {
            "heading": "3.2 Motivation",
            "text": "Our primary goal is to have the ability to evaluate individual actions. However, MRT and BRIO can compare only fully generated sequences y1 and y2, making it challenging to estimate the values of individual actions or partial sequences y1t1 and y 2 t2 within those sequences. To address this limitation and enable the model to understand the consequences of individual actions, we have the following requirements:\n\u2022 R1: We need a way to measure the desirability of an action or a partial sequence based on sparse sequence-level utility u. This would allow us to estimate how desirable a partial sequence is, even with this limited information.\n\u2022 R2: We should be able to determine which actions or partial sequences participate in the model update. When we modify the BRIO objective to focus on partial sequences instead of full sequences, we should identify a subset of partial sequence pairs that contribute to rank learning, to avoid requiring the model to meet an overly ambitious goal of estimating a full order of partial sequences.\nBy fulfilling these requirements, we can overcome the limitations of existing methods and enable the model to learn the value of individual actions and enhance their estimation of desirability."
        },
        {
            "heading": "3.3 Proposed: PPR",
            "text": "We now formally present our proposed method, PPR, that addresses R1 and R2, posed in 3.2, elegantly at once. This is achieved by leveraging the prefix tree T constructed from the set of sample sequences Y generated from the model conditioned on the source sequence x.\nFor R1, PPR infers the value VT (y\u2264t) of a partially generated sequence y\u2264t using the prefix tree T as\nVT (y\u2264t) := max y\u2032\u2264t=y\u2264t\nu(y\u2032;x), (5)\nwhere the max operator ranges over y\u2032 \u2208 Y. In other words, VT (y\u2264t) is the highest utility observed among the sequences sharing the common prefix y\u2264t. It serves as the missing ground-truth quality measure for partially generated sequences, just as the utility metric u evaluates fully generated sequences.\nThe prefix tree structure allows to efficiently enumerate over all sequences with the prefix y\u2264t, as they correspond to the leaf nodes of the subtree rooted at the node y\u2264t. This way, VT (y\u2264t) can be efficiently determined for any given prefix y\u2264t.\nAs previously explained in Section 1, although with VT we can measure the desirableness of any prefix of any sequence in Y, it can be an unrealistic goal to learn to make pointwise estimates of VT , which requires the model to decide ordering for arbitrary pairs of partially generated sequences in essence. Rather, we adopt the pairwise ranking approach of considering two actions as a pair and encourage the model to find the action that results in a partial sequence with higher VT .\nRegarding R2 and how to construct the set of pairs to present to the model, the prefix tree also\nprovides a natural solution. We select the branching nodes in the prefix tree and compare the actions corresponding to each branch.\nThe branching nodes in the prefix tree T are well-suited for the task of selecting a feasible and necessary set of pairs to maximize the benefit of learning to rank. This is because:\n\u2022 We present the model with pairs of partial sequences that share all but the last token. This requirement allows the model to assess the impact of a one-step move, which is more feasible compared to distinguishing pairs of partial sequences with much longer chains of differing actions.\n\u2022 The branches in the prefix tree were kept during the beam search because they were assigned sufficient probability mass by the model. Therefore, the model should be capable of distinguishing these challenging pairs in order to perform well.\nWe now provide a detailed description of how the pairwise ranking objective is obtained.\nAs illustrated in Figure 4, we begin by constructing a prefix tree T using the outputs generated with\nthe beam search. 3 In this tree, each node v represents a prefix of at least one sample sequence in Y, and each edge corresponds to a token.\nNext, we determine the rank of each token wi following the same prefix v in accordance to max(VT (vi)), the maximum possible utility when the generation starting from vi = vwi is complete. For all branching node v in T with deg(v) > 1 children nodes, we rank the associated edges wi and next states vi by VT (vi). As a result, selecting w1 and transitioning from v to v1 leads to the best outcome, while choosing wdeg(v) is the worst choice possible.\nWe provide a higher reward for assigning a higher probability to wi as the next token compared to wj if i < j. The resulting pairwise ranking loss of PPR is in the form\nLc = \u2211\ndeg(v)\u22652 \u2211 (i,j)\u2208Pv max(0, p\u2032(vj)\u2212 p\u2032(vi)+\u03bb),\n(6) where p\u2032(vi) := p\u03b8(wi | v,x) is the probability of choosing the token wi conditioned on the source x and the prefix v, and Pv is the set of pairs of indices that will participate in the comparison. For nodes with more than two children, deg(v) > 2, there are several possibilities for selecting which pairs to contrast, Pv. We considered the following candidates,\n\u2022 Dominate: Pv = {(1, 2), (1, 3), \u00b7 \u00b7 \u00b7 , (1, N)},\n\u2022 Adjacent pairs: Pv = {(1, 2), (2, 3), \u00b7 \u00b7 \u00b7 , (N \u2212 1, N)},\n\u2022 All pairs: Pv = {(i, j) | 1 \u2264 i < j \u2264 N},\nwhere N = deg(v), and proceeded with Dominate which performed well in our preliminary experiments. In practice, the difference was almost indistinguishable as most of the branching nodes had two children.\nFinally, we add pseudo-reference loss L\u2032MLE, where the sequence with the highest utility in Y serves as another reference sequence to obtain the following training objective for PPR:\nL = LMLE + L\u2032MLE + \u03b3 \u00b7 Lc (7)\nL\u2032MLE encourages the model to prefer high-quality sequences it has generated on sequence-level, as elaborated in Section 5.1.\n3A prefix tree is generated on-fly per each input sequences, as described in Equation 7."
        },
        {
            "heading": "4 Experiments",
            "text": ""
        },
        {
            "heading": "4.1 Experiment settings",
            "text": ""
        },
        {
            "heading": "4.1.1 Task",
            "text": "As a main evaluation task, we focus on paraphrase generation to evaluate the effectiveness of the proposed PPR for conditional text generation, we experiment on the paraphrase generation task. A paraphrase y of the source x exhibits high utility u(y;x) if it (1) preserves the meaning x conveys, (2) is a fluent sentence, and (3) deviates lexically and structurally from x, discouraging the copying behaviors y = x as a low utility. Regarding generalization to other tasks, please see Subsection 5.4."
        },
        {
            "heading": "4.1.2 Architecture and training details",
            "text": "We utilize an encoder-decoder transformer architecture as the policy network. Specifically, we start from the pretrained checkpoints of BART (Lewis et al., 2020) and T5 (Raffel et al., 2020). We provide the detailed hyperparmeter settings and other relevant configuration in Appendix A."
        },
        {
            "heading": "4.1.3 Datasets",
            "text": "We use two popular benchmarks on paraphrase generation, namely QQP-Pos (Chen et al., 2017) and MSCOCO (Lin et al., 2014) for training and evaluation. QQP-Pos is a subset of Quora Question Pairs (QQP) paraphrase identification corpus, where only positive paraphrase pairs are selected. Throughout this paper, we denote QQP-Pos as QQP for simplicity. MSCOCO is a dataset for image captioning, and two captions that describe the same image are treated as a paraphrase pair. For both datasets, we use the standard data splits.\nRoughly, QQP consists of more semantically and lexically similar paraphrases than MSCOCO. The self-BLEU (BLEU score between two paraphrases) measured from the test split is 18.35 in QQP, while only 2.46 in MSCOCO."
        },
        {
            "heading": "4.1.4 Metrics",
            "text": "While numerous metrics have been proposed for evaluating paraphrase generation automatically (Cao and Wan, 2020; Shen et al., 2022; Nighojkar and Licato, 2021), we choose BERT-iBLEU (Niu et al., 2021), which is a reference-free metric capturing both semantic similarity (using BERTscore) and lexical dissimilarity (using iBLEU). It is reported to outperform most of the source-only and reference-only metrics in human judgement\ncorrelation with low computation overhead (Shen et al., 2022; Nighojkar and Licato, 2021).\nBERT-iBLEU score is calculated as a weighted harmonic mean of the BERT-score and the iBLEU score between the source and the output. Following the original paper, we set the weight hyperparameter \u03b2 = 4 to match the scale of the two metrics. Outputs that are nearly identical to the source sentence result in low iBLEU, and thus penalized in terms of BERT-iBLEU score.\nBERT-iBLEU = \u03b2 + 1\n\u03b2 BERT-score + 1 1\u2212BLEU\n(8)\nWe used BERT-iBLEU as the utility u(\u00b7), that is, both for evaluating the model performance and for rewarding the model during training."
        },
        {
            "heading": "4.2 Results",
            "text": "The result on two base architectures (BART, T5) and two paraphrase generation benchmarks (QQP, MSCOCO) is presented in Table 1. We have evaluated the models with different decoding strategies, namely (1) unbiased sampling and (2) beam search with varying beam sizes (1, 2, 4, 8 and 16) where beam size of 1 is precisely the greedy decoding. The full results, including scores for all beam sizes along with the standard deviation can be found in Appendix C; also, the score-beam size plot can be found in Figure 5.\nOur proposed PPR consistently outperforms all baselines in all (model, dataset) pairs. We also present the interpretation of these results through the lens of beam monotonicty and resilience introduced in Section 1.\nFirst, regarding the beam monotonicity, MLE and MRT failing to benefit from increasing beam size and thus violating it indicates a poor correlation or calibration between the likelihood assigned by these models and the target utility score. This suggests that the values of partial sequences implicitly maintained by these models do not align well with the final desired outcome.\nSecond, for beam resilience, while BRIO does show performance improvement with larger beam sizes, its significant performance decline with smaller beam sizes or unbiased sampling, failing to achieve resilience indicates a failure to consistently estimate the values of partial sequences. This suggests that BRIO\u2019s high-ranking candidates are not trustworthy in early decoding stages, as we further anayze in Section 5.2.\nOur distinction is that PPR exhibits a nearly flat curve in Figure 5, indicating that the values of partial sequences estimated by our model align well with the actual final rewards obtained by continuing generation from those sequences. Although PPR was not explicitly designed for fully stochastic policy scenarios, where unbiased sampling is used for decoding, it demonstrates significant performance improvement in such circumstances.\nNow we examine the failure of MLE in more detail. In addition to its BERT-iBLEU scores generally ranking the lowest, Table 2 highlights its another weakness: the exact copy rate. This metric represents the percentage of model outputs that are exactly the same as the input sequence. Despite the training data consisting of (source, reference) pairs that explicitly discourage exact copying as a\ndesirable paraphrase, MLE training fails to capture this aspect from the data.\nFrom an RL perspective, MLE training can be seen as behavioral cloning, where an expert demonstrates desirable sequences of actions, and the agent learns by simply imitating those actions. Our results indicate that, when the desired goal is complex and multifaceted, simply following good paths leads to simple copying behaviors, which is worsened in QQP, which contains a larger proportion of similar (quantitatively, high self-BLEU) paraphrases compared to MSCOCO."
        },
        {
            "heading": "5 Analysis",
            "text": "We provide in-depth analysis of the behavior of our model and baselines."
        },
        {
            "heading": "5.1 Ablation study",
            "text": "Table 3 presents the ablation study results, where we compare the different composition of training objective components listed in Equation 7.\nFirst, the importance of the contrastive objective Lc is clearly demonstrated, as models trained with Lc generally outperform ones trained without it. Furthermore, alongside the performance gain in sufficient beam sizes, Lc alone can achieve beam monotonicity and resilience, i.e. model\u2019s robustness with varying beam sizes during inference as illustrated in Table 7.\nAlso, the pseudo-reference loss plays a key role in promoting the top-ranked candidate based on sequence-level utility. More detailed results regarding the pseudo-reference loss are available in Table 8. Finally, training without the reference loss, or reference-free in other words, led to a small drop in score for the QQP task but a gain for MSCOCO. This suggests that the reference sequences in QQP were better aligned with the source sequences, providing more helpful guidance during training."
        },
        {
            "heading": "5.2 Step-wise analysis",
            "text": "We analyze the model\u2019s beam resilience in a detailed, step-wise fashion. First, we select the topranked sequence with a beam size of 16 (referred as top beam). For each generation step, we computed the reciprocal rank of the chosen token within the entire vocabulary and averaged it over all sequences with a length of 10 (including EOS), which was the most common length observed.\nFigure 6 illustrates an interesting finding regarding the behavior of baseline methods compared to PPR. During the early to middle steps, baselines often do not consider the prefix of the top beam sequence as desirable. However, as they progress, they change their decisions and favor the prefix. In contrast, PPR consistently achieves high token MRR scores across all time steps and in all cases. This implies that, even with greedily following the best actions, PPR is able to generate a sequence that closely resembles the one obtained through beam search.\nThis observation has two important implications. Firstly, it directly explains PPR\u2019s success in achieving beam resilience during greedy decoding. Secondly, it strongly supports the effectiveness of PPR\u2019s pairwise ranking loss for partial sequences in enabling the model to consistently and accurately estimate the values of partial sequences, eliminating the need for looking ahead possible continuations as in beam search, to obtain sequences with high utility."
        },
        {
            "heading": "5.3 Additional metrics",
            "text": "To prove robustness of the proposed PPR against different metrics, we propose a variant of BERTiBLEU, BiP (BERT-iBLEU-PPL), which includes an additional perplexity term for measuring fluency of the generated paraphrase. When an output y\u2019s perplexity is higher than the source sentence x in a scale of k, the score is proportionally penalized.\nBiP = BERT-iBLEU \u00b7min ( 1,\nk \u00b7 PPL(x) PPL(y)\n) (9)\nAutomatic evaluation results for models trained with BiP as utility are provided in Table 4, where PPR clearly demonstrates beam monotoniticy and resilience together with high performance. For this purpose we have also reported results obtained by evaluating with metrics other than BiP, including ParaScore (Shen et al., 2022), which was designed to better align with human judgment.\nAlso, we report human evaluation results for different models. We performed example-wise comparison between randomly chosen outputs of PPR and BRIO/MLE trained models (generated by beam search with beam size of 16), asking the annotators to decide which one is a better paraphrase of the given source sentence, or if they are comparable in terms of faithfulness, lexical divergence and\nfluency. As a result, PPR won 24%/58% of the case and lost 18%/12% of the case compared to BRIO and MLE, respectively. Qualitatively, our strength was found with respect to faithfulness out of the three main requirements. Examples of generated samples and more details on (automatic) evaluation results can be found in Appendix C and B."
        },
        {
            "heading": "5.4 Generalizing PPR to constrained MT",
            "text": "In addition to the paraphrase generation task mainly considered in this paper, we also conducted experiments on lexically constrained machine translation, where the goal is to output a hypothesis given a source sentence to translate plus lexical constraints, which are series of chunks of tokens that must appear on the generated sentence. This dual goal of generating plausible and constraintssatisfying translation has been assessed with two metrics, namely BLEU and copying success rate (CSR4) (Wang et al., 2022; Chen et al., 2021).\nTable 5 shows the results on constrained machine translation task on the WMT16 En-Ro benchmark. BRIO and PPR used BLEU\u00b7CSR, the product of sentence-level BLEU score and CSR as the util-\n4Sometimes also denoted as EM (exact match).\nity for training. It is shown that our proposed PPR effectively pursues the dual goal of generating plausible translation containing the required constraints. Further experimental details and complete results can be found in Appendix A and C."
        },
        {
            "heading": "6 Related work",
            "text": "This section describes previous work addressing the misbehavior often observed from conditional text generation. Null sequences, repeated n-grams and gibberish can be generated as the model assigns high probability to them under certain circumstances (Stahlberg and Byrne, 2019; Holtzman et al., 2020, inter alia). We categorize efforts based on whether they attribute these to the decoding method (Subsection 6.1) or to the model\u2019s policy (Subsection 6.2)."
        },
        {
            "heading": "6.1 Decoding methods",
            "text": "Some attribute the degeneration problem to the decoding method itself, as Meister et al. (2020) examined the inductive bias of beam search. Alternative decoding methods to mitigate ill-formed outputs, such as sampling-based approaches (Fan et al., 2018; Holtzman et al., 2020), regularized beam search (Yang et al., 2018; Meister et al., 2020), and constrained decoding (Kajiwara, 2019; Niu et al., 2021) have been propsoed. In paraphrase generation, constrained decoding has been explored to discourage direct copying of the input.\nThese methods can be seen as ad-hoc solutions without addressing the root cause, unreliable value estimation, as we argue with beam monotonicity and resilience in Section 4.2."
        },
        {
            "heading": "6.2 Reward shaping",
            "text": "On the other hand, reward shaping can be employed to directly address the value prediction problem, either by (1) providing additional reward on top of the original one to nudge the model towards accomplishing subgoals, or (2) distributing the reward over actions through potential-based method. An example of the former is Chan et al. (2019), where generating each keyphrase is rewarded by using recall as the utility in keyphrase generation.\nAs an example of the latter, in the potentialbased method proposed by Bahdanau et al. (2017) for training actor-critic model on neural machine translation task, a partial reward at each time step is defined as the difference between the BLEU scores of the prefix of length t and t \u2212 1 with respect to the entire reference sentence:\nRt = up(y\u2264t;x)\u2212 up(y<t;x), (10)\nwhere the potential, or the utility of a partial sequence up(y\u2264t;x) = u\u0303p(y\u2264t;x, y\u0302) is the BLEU score of the partial sequence y\u2264t with respect to the whole reference sequence y\u0302.\nHowever, it is hard to define such a task-specific potential over all states so that the resulting rewards would faithfully reflect the desirableness of each action, which explains a marginal gain from the above partial reward. In contrast, our design of PPR allows to provide partial rewards directly based on the values of the resulting states."
        },
        {
            "heading": "7 Conclusion",
            "text": "We proposed PPR (Pairwise Partial Reward), a novel approach to reward shaping through providing partial rewards in reinforcement learning for conditional text generation. Our method utilizes prefix tree constructed from the set of sample sequences to learn to rank between action pairs, supporting any sequence-level utility metrics u for optimization. By considering samples collectively, our approach enables the model to reflect on its past decisions and immediately determine which actions led to favorable outcomes. This allows PPR to effectively guide the model in estimating values and selecting desirable actions during generation. Experimental results demonstrate that our model outperforms baselines, especially with smaller beam sizes or unbiased sampling, indicating a significant improvements in assessing the values of partial sequences and enhancing language models as stochastic policies."
        },
        {
            "heading": "Acknowledgements",
            "text": "This work was supported by Institute of Information & Communications Technology Planning & Evaluation (IITP) grant funded by the Korean government (MSIT) (No. 2022-0-00077, AI Technology Development for Commonsense Extraction, Reasoning, and Inference from Heterogeneous Data).\nLimitations\nOne limitation of our work is that it incurs exploration (sampling) overhead during training, which is common among reinforcement learning approaches to text generation. With this, PPR may require more time to update the model parameters compared to MLE, but surpasses MLE with a smaller number of updates. The trade-off analysis between two factors needs further exploration.\nAnother limitation of this paper is that, though our work is readily applicable to any conditional generation tasks with any sequence-level evaluation metric, it still requires an extensive analysis of generalization to more diverse set of tasks with possibly longer target sequences and utility functions with even more complex designs."
        },
        {
            "heading": "A Experimental settings in detail",
            "text": "Here we provide the detailed experimental settings.\nA.1 Paraphrase Generation experiments We used pretrained T5-small (60M) and BARTbase (139M) checkpoints, publicly available at Huggingface Transformers 5 with each corresponding tokenizers (with approximately 30k and 50k vocab size).\nWe used Adam optimizer with (\u03b21, \u03b22) = (.9, .999) and \u03f5 = 1e\u2212 8. We also used the linear learning rate scheduler with initial learning rate of 5e-5, batch size of 16, dropout rate of 0.1. We trained models for 5 epochs for both QQP and MSCOCO datasets and then chose the best checkpoint with target metric score (BERT-iBLEU) on the development set.\nFor methods other than MLE, we used beam search for obtaining on-policy samples at train time. For MRT, we used the fixed beam size of 16. For BRIO and PPR, we experimented with both 16 and 32, and chose the one with better performance on the development set. As a result, for all model and dataset combinations, BRIO performed better with 16 while ours enjoyed the benefit of the larger beam size, 32.\nNow we describe method-specific hyperparameter settings for BRIO and PPR. For BRIO, we used the fixed length penalty of \u03b1 = 1.0 for normalizing the sequence likelihood. We have searched over .001, .005, .01, .05, .1 for determining the unit margin \u03bb, and \u03bb = .1 was the best for all configurations. For the loss scale hyperparameter we found that \u03b3 = 10 worked the best for most of the cases, over 1, 2, 3, 5, 10 while \u03b3 = 2 gave the best result for BART on MSCOCO.\nFor PPR, all the model-dataset combinations shared the same best performing hyperparameter settings, where we have determined the contrastive loss scale hyperparamter \u03b3 as 10 in the same way as for BRIO, and the margin as \u03bb = 0.5 among 0.1, 0.5. As described before, we have put the same weight on the negative log-likelihood loss from the ground-truth reference sequence and the pseudoreference sequence.\nA.2 Constrained MT experiments Following previous works, as a fully annotated set of training examples for constrained machine translation task is not widely available, we chose the\n5https://github.com/huggingface/transformers\napproach of extracting constraints from public MT benchmarks for unconstrained MT in which only paired sentences without constraints are available. In order to best align with the actual use case of constrained MT in practice, we leveraged a pretrained MarianMT model trained on En-Ro direction available at Huggingface Transformers6, and the corresponding tokenizer (with approximately 59k vocab size) to identify the words in the target sentences that the model is least confident with, based on the model\u2019s output log probabilities. Note that we used word-level constraints, rather than token-level ones, allowing a series of tokens to constitute a single constraint; the confidence for a word was calculated as the average confidence for tokens in that word. We selected 1-3 words as constraints, proportional to the sentence length. In order for the extracted constraints to be reasonable, we applied language filtering based on language detection tools which is a widely adopted practice in building MT systems based on publicly available data (Ng et al., 2019), removing noisy examples such as those containing German(de) sentences. Among the 610k training examples in WMT16 En-Ro dataset, we only used pairs of which (1) source sentence is classified as En and target sentence is classified as Ro, and (2) both source and target sentences are shorter than 32 tokens, which leaves slightly less than 100k samples. The constraints were appended to the source sentence, where the source and constraints were separated by <eos> token, to be fed as input to the model. Validation and test splits were processed in the same manner.\nWe initialized the models with the aforementioned pretrained MarianMT checkpoint, which exhibited nearly 40-50% of CSR on its own. The models were trained for 5 epochs with initial learning rate 2e-5, effective batch size 128 and beam size 16. For other method-specific hyperparameters we used the same values as in the main experiments."
        },
        {
            "heading": "B Examples of generated paraphrases",
            "text": "Table 6 includes generated paraphrase samples from models trained with different metrics and training objectives.\nModels trained with MLE often generate a direct copy of the source as shown in Table 2, or omit important details. On the other hand, PPR and BRIO trained with BERT-iBLEU clearly learns to\n6https://huggingface.co/Helsinki-NLP/ opus-mt-en-ro\nexploit the design of BERT-iBLEU, which allows copying up to 3-grams from the source to maintain high semantic similarity captured by BERTscore yet still avoiding being penalized by BLEU score.\nThe difference between models trained on BRIO and PPR is best shown in BiP examples. BRIO often leads to generating false information that is not faithful to the content of the source, as highlighted in italic. In contrast, PPR effectively retains, or at least does not flip or change the meaning of important keywords present in the source sentence through directly contrasting token-level actions given the prefix, leading to better choices in critical steps. This showcases our method\u2019s effectiveness in token-level credit assignment in autoregressive conditional text generation."
        },
        {
            "heading": "C Complete results",
            "text": "Table 9 shows the results for paraphrasing in more detail including the standard deviation of each reported figure. Substantially improved performance with oracle reranker shows that MLE and MRT failing to achieve beam monotonicity is largely due to that their failing to align probability and target utility.\nIn addition, Table 10 displays the complete result\nfor models trained in BiP. Strong performance gain is achieved with PPR compared to its counterparts, especially with unbiased sampling and beam search decoding with small beam sizes.\nFinally, Table 11 shows the full results on lexically constrained machine translation task, where PPR outperforms others not only in the given metric for training, BRIO\u00b7CSR, but also in its individual components (BRIO and CSR). While models trained using MLE or BRIO objectives tend to frequently violate beam monotonicity, PPR can generally take advantage of the increased beam size."
        }
    ],
    "title": "Learning to Rank Generation with Pairwise Partial Rewards",
    "year": 2023
}