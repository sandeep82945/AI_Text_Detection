{
    "abstractText": "Behavioral testing offers a crucial means of diagnosing linguistic errors and assessing capabilities of NLP models. However, applying behavioral testing to machine translation (MT) systems is challenging as it generally requires human efforts to craft references for evaluating the translation quality of such systems on newly generated test cases. Existing works in behavioral testing of MT systems circumvent this by evaluating translation quality without references, but this restricts diagnosis to specific types of errors, such as incorrect translation of single numeric or currency words. In order to diagnose general errors, this paper proposes a new Bilingual Translation Pair Generation based Behavior Testing (BTPGBT) framework for conducting behavioral testing of MT systems. The core idea of BTPGBT is to employ a novel bilingual translation pair generation (BTPG) approach that automates the construction of high-quality test cases and their pseudoreferences. Experimental results on various MT systems demonstrate that BTPGBT could provide comprehensive and accurate behavioral testing results for general error diagnosis, which further leads to several insightful findings. Our code and data are available at https: //github.com/wujunjie1998/BTPGBT.",
    "authors": [
        {
            "affiliations": [],
            "name": "Junjie Wu"
        },
        {
            "affiliations": [],
            "name": "Lemao Liu"
        },
        {
            "affiliations": [],
            "name": "Dit-Yan Yeung"
        }
    ],
    "id": "SP:afa183e2c5786af8b0b5bd00144cb0cdca798c1d",
    "references": [
        {
            "authors": [
                "Boris Beizer."
            ],
            "title": "Black-box testing: techniques for functional testing of software and systems",
            "venue": "John Wiley & Sons, Inc.",
            "year": 1995
        },
        {
            "authors": [
                "Chi Chen",
                "Maosong Sun",
                "Yang Liu."
            ],
            "title": "Maskalign: Self-supervised neural word alignment",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language",
            "year": 2021
        },
        {
            "authors": [
                "Guanhua Chen",
                "Yun Chen",
                "Victor O.K. Li."
            ],
            "title": "Lexically constrained neural machine translation with explicit alignment guidance",
            "venue": "Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications",
            "year": 2021
        },
        {
            "authors": [
                "Chris Donahue",
                "Mina Lee",
                "Percy Liang."
            ],
            "title": "Enabling language models to fill in the blanks",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2492\u2013 2501, Online. Association for Computational Lin-",
            "year": 2020
        },
        {
            "authors": [
                "Denis Emelin",
                "Ivan Titov",
                "Rico Sennrich."
            ],
            "title": "Detecting word sense disambiguation biases in machine translation for model-agnostic adversarial attacks",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
            "year": 2020
        },
        {
            "authors": [
                "Nuno M Guerreiro",
                "Duarte Alves",
                "Jonas Waldendorf",
                "Barry Haddow",
                "Alexandra Birch",
                "Pierre Colombo",
                "Andr\u00e9 FT Martins."
            ],
            "title": "Hallucinations in large multilingual translation models",
            "venue": "ArXiv preprint, abs/2303.16104.",
            "year": 2023
        },
        {
            "authors": [
                "Shashij Gupta",
                "Pinjia He",
                "Clara Meister",
                "Zhendong Su."
            ],
            "title": "Machine translation testing via pathological invariance",
            "venue": "Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software",
            "year": 2020
        },
        {
            "authors": [
                "Pinjia He",
                "Clara Meister",
                "Zhendong Su."
            ],
            "title": "Structure-invariant testing for machine translation",
            "venue": "Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering, pages 961\u2013 973.",
            "year": 2020
        },
        {
            "authors": [
                "Pinjia He",
                "Clara Meister",
                "Zhendong Su."
            ],
            "title": "Testing machine translation via referential transparency",
            "venue": "2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE), pages 410\u2013422. IEEE.",
            "year": 2021
        },
        {
            "authors": [
                "Amr Hendy",
                "Mohamed Abdelrehim",
                "Amr Sharaf",
                "Vikas Raunak",
                "Mohamed Gabr",
                "Hitokazu Matsushita",
                "Young Jin Kim",
                "Mohamed Afify",
                "Hany Hassan Awadalla"
            ],
            "title": "How good are gpt models at machine translation? a comprehensive evaluation",
            "year": 2023
        },
        {
            "authors": [
                "Guoping Huang",
                "Lemao Liu",
                "Xing Wang",
                "Longyue Wang",
                "Huayang Li",
                "Zhaopeng Tu",
                "Chengyan Huang",
                "Shuming Shi."
            ],
            "title": "Transmart: A practical interactive machine translation system",
            "venue": "arXiv preprint arXiv:2105.13072.",
            "year": 2021
        },
        {
            "authors": [
                "Pin Ji",
                "Yang Feng",
                "Jia Liu",
                "Zhihong Zhao",
                "Baowen Xu."
            ],
            "title": "Automated testing for machine translation via constituency invariance",
            "venue": "2021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE), pages 468\u2013479. IEEE.",
            "year": 2021
        },
        {
            "authors": [
                "Wenxiang Jiao",
                "Wenxuan Wang",
                "Jen-tse Huang",
                "Xing Wang",
                "Zhaopeng Tu."
            ],
            "title": "Is chatgpt a good translator? a preliminary study",
            "venue": "ArXiv preprint, abs/2301.08745.",
            "year": 2023
        },
        {
            "authors": [
                "Kevin Knight."
            ],
            "title": "Statistical machine translation",
            "venue": "Proceedings of the Fourth Conference of the Association for Machine Translation in the Americas: Tutorial Descriptions, Cuernavaca, Mexico. Springer.",
            "year": 2000
        },
        {
            "authors": [
                "Klaus Krippendorff"
            ],
            "title": "Computing krippendorff\u2019s alpha-reliability",
            "year": 2011
        },
        {
            "authors": [
                "Siyu Lai",
                "Zhen Yang",
                "Fandong Meng",
                "Xue Zhang",
                "Yufeng Chen",
                "Jinan Xu",
                "Jie Zhou."
            ],
            "title": "Generating authentic adversarial examples beyond meaningpreserving with doubly round-trip translation",
            "venue": "Proceedings of the 2022 Conference of the North",
            "year": 2022
        },
        {
            "authors": [
                "Mike Lewis",
                "Yinhan Liu",
                "Naman Goyal",
                "Marjan Ghazvininejad",
                "Abdelrahman Mohamed",
                "Omer Levy",
                "Veselin Stoyanov",
                "Luke Zettlemoyer"
            ],
            "title": "BART: Denoising sequence-to-sequence pre-training for natural language",
            "year": 2020
        },
        {
            "authors": [
                "Keqin Peng",
                "Liang Ding",
                "Qihuang Zhong",
                "Li Shen",
                "Xuebo Liu",
                "Min Zhang",
                "Yuanxin Ouyang",
                "Dacheng Tao."
            ],
            "title": "Towards making the most of chatgpt for machine translation",
            "venue": "ArXiv preprint, abs/2303.13780.",
            "year": 2023
        },
        {
            "authors": [
                "Chengwei Qin",
                "Aston Zhang",
                "Zhuosheng Zhang",
                "Jiaao Chen",
                "Michihiro Yasunaga",
                "Diyi Yang"
            ],
            "title": "Is chatgpt a general-purpose natural language processing task solver? ArXiv preprint, abs/2302.06476",
            "year": 2023
        },
        {
            "authors": [
                "Vikas Raunak",
                "Matt Post",
                "Arul Menezes."
            ],
            "title": "SALTED: A framework for SAlient long-tail translation error detection",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2022, pages 5163\u20135179, Abu Dhabi, United Arab Emirates. As-",
            "year": 2022
        },
        {
            "authors": [
                "Ricardo Rei",
                "Jos\u00e9 G.C. de Souza",
                "Duarte Alves",
                "Chrysoula Zerva",
                "Ana C Farinha",
                "Taisiya Glushkova",
                "Alon Lavie",
                "Luisa Coheur",
                "Andr\u00e9 F.T. Martins."
            ],
            "title": "COMET-22: Unbabel-IST 2022 submission for the metrics shared task",
            "venue": "Proceedings of the",
            "year": 2022
        },
        {
            "authors": [
                "Marco Tulio Ribeiro",
                "Tongshuang Wu",
                "Carlos Guestrin",
                "Sameer Singh."
            ],
            "title": "Beyond accuracy: Behavioral testing of NLP models with CheckList",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4902\u2013",
            "year": 2020
        },
        {
            "authors": [
                "Paul R\u00f6ttger",
                "Bertie Vidgen",
                "Dong Nguyen",
                "Zeerak Waseem",
                "Helen Margetts",
                "Janet Pierrehumbert."
            ],
            "title": "HateCheck: Functional tests for hate speech detection models",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational",
            "year": 2021
        },
        {
            "authors": [
                "Danielle Saunders",
                "Bill Byrne."
            ],
            "title": "Reducing gender bias in neural machine translation as a domain adaptation problem",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7724\u20137736, Online. Association",
            "year": 2020
        },
        {
            "authors": [
                "Jie Hao"
            ],
            "title": "OPPO\u2019s machine translation systems for WMT20",
            "venue": "In Proceedings of the Fifth Conference on Machine Translation,",
            "year": 2020
        },
        {
            "authors": [
                "Zeyu Sun",
                "Jie M Zhang",
                "Mark Harman",
                "Mike Papadakis",
                "Lu Zhang."
            ],
            "title": "Automatic testing and improvement of machine translation",
            "venue": "Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering, pages 974\u2013985.",
            "year": 2020
        },
        {
            "authors": [
                "Betty Van Aken",
                "Sebastian Herrmann",
                "Alexander L\u00f6ser."
            ],
            "title": "What do you see in this patient? behavioral testing of clinical NLP models",
            "venue": "Proceedings of the 4th Clinical Natural Language Processing Workshop, pages 63\u201373, Seattle, WA. Association for",
            "year": 2022
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N. Gomez",
                "Lukasz Kaiser",
                "Illia Polosukhin."
            ],
            "title": "Attention is all you need",
            "venue": "Advances in Neural Information Processing Systems 30: Annual Conference on Neural",
            "year": 2017
        },
        {
            "authors": [
                "Jun Wang",
                "Benjamin Rubinstein",
                "Trevor Cohn."
            ],
            "title": "Measuring and mitigating name biases in neural machine translation",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages",
            "year": 2022
        },
        {
            "authors": [
                "Jun Wang",
                "Chang Xu",
                "Francisco Guzm\u00e1n",
                "Ahmed El-Kishky",
                "Benjamin Rubinstein",
                "Trevor Cohn."
            ],
            "title": "As easy as 1, 2, 3: Behavioural testing of NMT systems for numerical translation",
            "venue": "Findings of the Association for Computational Linguis-",
            "year": 2021
        },
        {
            "authors": [
                "Shuo Wang",
                "Peng Li",
                "Zhixing Tan",
                "Zhaopeng Tu",
                "Maosong Sun",
                "Yang Liu."
            ],
            "title": "A templatebased method for constrained neural machine translation",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2022
        },
        {
            "authors": [
                "Yanling Xiao",
                "Lemao Liu",
                "Guoping Huang",
                "Qu Cui",
                "Shujian Huang",
                "Shuming Shi",
                "Jiajun Chen."
            ],
            "title": "BiTIIMT: A bilingual text-infilling method for interactive machine translation",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Compu-",
            "year": 2022
        },
        {
            "authors": [
                "Xinze Zhang",
                "Junzhe Zhang",
                "Zhenhua Chen",
                "Kun He."
            ],
            "title": "Crafting adversarial examples for neural machine translation",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Confer-",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "In recent years, machine translation (MT) systems have achieved significant advancements in translation performance. Yet current MT systems are still brittle and could generate erroneous translations that lead to severe commercial losses 1, which makes error diagnosis of MT systems a crucial task to be solved. Behavioral testing, which has been widely applied in software engineering research to test complex systems, is a desired direction to tackle this issue. Generally, behavioral\n1https://www.androidpolice.com/ google-translation-mistake-io/\ntesting probes various system capabilities for error diagnosis by inspecting input-output behaviors on test cases with targeted edits, irrespective of knowledge about the system\u2019s internal structure (Beizer, 1995). Therefore, it is suitable for providing finegrained evaluations of different capabilities and diagnosing errors for MT systems.\nThere are systematic studies of behavioral testing on classification tasks such as sentiment analysis (Ribeiro et al., 2020), hate speech detection (R\u00f6ttger et al., 2021) and clinical outcome prediction (Van Aken et al., 2022). However, implementing behavioral testing in MT tasks poses significant challenges. The main reason is that it is usually difficult to automatically obtain the ref-\nerence translation for a test case modified from a source sentence, making it difficult to automatically judge the quality of the translation of the test case. To avoid this challenge, existing work attempts to judge the translation quality of such test case without its reference translation, yet they can only diagnose translation errors related to specific types of edits that target certain capabilities. For example, He et al. (2020); Sun et al. (2020); Gupta et al. (2020); Ji et al. (2021) only diagnose translation errors on test cases with the editing of a single noun or adjective word, and He et al. (2021); Wang et al. (2021); Raunak et al. (2022) can only diagnose incorrect translation of noun phrases, quantities or currency units that is related to the edits on them.\nTo address these challenges, this paper presents Bilingual Translation Pair Generation based Behavior Testing (BTPGBT), a novel framework for behavioral testing in MT systems. The core idea of our framework is the novel Bilingual Translation Pair Generation (BTPG) approach that can automatically generate high-quality test cases and their pseudo-references from standard MT test sets, 2 which allows the diagnosis of general translation errors in test cases targeting various MT system capabilities. As shown in Figure 1, BTPG takes a source sentence and its reference as an input translation pair, then masks specific aligned segments (\u201cMeta-universe\u201d and \u201c\u5143\u5b87\u5b99\u201d in Figure 1) in both sentences targeting the capability that needs to be tested (NER in Figure 1). Only masking aligned segments enables BTPG to make the best use of the structure information of unmasked positions in the original source and reference during generation, which enhances its generation quality. BTPG then generates a new bilingual translation pair as the test case and its pseudo-reference by using ChatGPT 3, a large language model proposed by OpenAI that has shown impressive performances on various NLP tasks, to infill the masked segments in both sentences at once. In this way, the test case and its pseudo-reference could have similar quality to human constructed ones, as illustrated in \u00a74.2.\nWith the high-quality test cases and their pseudoreferences crafted by BTPG, we can conduct behav-\n2Our framework indeed involves a test set with references, but this is not a very strong requirement since there are many off-the-shelf test datasets and in particular this is a standard setting in behavioral testing. For instance, the pioneered research about behavioral testing (Ribeiro et al., 2020) and many follow-up works (R\u00f6ttger et al., 2021; Van Aken et al., 2022) all use a test dataset with ground-truth labels.\n3https://chat.openai.com/\nioral testing to diagnose general translation errors targeting different capabilities. Extensive experiments on six MT systems using BTPGBT demonstrate that our method excels in evaluating multiple MT system capabilities with high error detection precision, outperforming prior works. Furthermore, our in-depth analysis of the testing results uncovers several insightful findings. The main contributions of this paper are summarized below:\n1. We design a new framework that can automatically conduct general behavioral testing to diagnose MT systems. Specifically, we propose a novel bilingual translation pair generation method to generate high-quality test cases and corresponding pseudo-references for behavioral testing.\n2. Through extensive experiments on six MT systems, we demonstrate our proposed method\u2019s effectiveness, leading to several insightful findings."
        },
        {
            "heading": "2 Revisiting Behavioral Testing in MT",
            "text": ""
        },
        {
            "heading": "2.1 Principled Definition",
            "text": "Behavioral testing (Beizer, 1995), which studies the capabilities of a system through examining its input-output behaviors on targeted test cases without knowing its internal information, has been demonstrated to be effective in NLP classification tasks (Ribeiro et al., 2020; R\u00f6ttger et al., 2021; Van Aken et al., 2022). Following these works, the principled definition of behavioral testing in MT can be extended as: given an input sentence x and its reference r. Let x\u2032 be a test case edited from x for testing a specific capability of an MT system M (e.g., x\u2032 in Figure 1 targeting the NER capability), and r\u2032 be the reference translation of x\u2032. We say M passes the behavioral testing on x\u2032 if it translates both x and x\u2032 in high-quality according to their references r and r\u2032. The idea behind this definition is straightforward: if M has the specific capability to handle the targeted edits in x\u2032, it should translate x\u2032 in the same quality as x.\nSpecifically, we first calculate the absolute difference of the quality between y and y\u2032, i.e., M\u2019s translations of x and x\u2032, as follows:\nDiff(y,y\u2032) = |Qual(y)\u2212 Qual(y\u2032)| (1)\nHere we can apply various quality measuring strategies as Qual() to evaluate y and y\u2032, while it would\nbe more authentic if we use r and r\u2032 for evaluation. M passes the behavioral test on the test case x\u2032 if it meets the following criteria:{\nQual(y) \u2265 \u03b1 Diff(y,y\u2032) \u2264 \u03b2\n(2)\nwhere \u03b1 and \u03b2 are thresholds ranging in [0, 1]. In practice, \u03b1 should be high and \u03b2 should be low to ensure the testing effectiveness. Interpreting the above criteria is intuitive: if Qual(y) is higher than \u03b1, we can conclude that y is a high-quality translation of x. On this basis, if Diff(y,y\u2032) is lower than \u03b2, we can conclude that y\u2032 keeps a similar and high translation quality with y and thus M passes the behavioral testing. Otherwise, we say M does not pass the test and y\u2032 is a diagnosed erroneous translation. As an example, the DeepL translator in Figure 1 fails on the behavioral testing on x\u2032 since the erroneous translation y\u2032 breaks Eq. 2, which is caused by the incorrect translation of \u201cMetaverse\u201d in y\u2032. We use Pass Rate, i.e., the percentage of test cases that an MT system passes, to quantify the behavioral testing results, where a higher score means less translation errors."
        },
        {
            "heading": "2.2 Related Work: Challenges and Existing Solutions",
            "text": "However, it is challenging to craft a high-quality reference for each test case since it requires much human efforts. Hence, previous works propose various approaches without the references of test cases to diagnose translation errors on testing results.\nExisting Solutions for MT Behavior Testing. (Wang et al., 2021; He et al., 2021; Raunak et al., 2022) construct a test case x\u2032 by modifying special components like numbers, physical units and noun phrases in x, since the reference translations of these components can be obtained ahead from external dictionaries. They define M passes the behavioral testing on x\u2032 if the reference translations of such components appear in y\u2032. Although these behavioral testing methods hold a high precision in diagnosing translation errors, the errors found by them are limited to such specific components. In other words, the recall of translation errors identified by these approaches is low. On the other hand, (He et al., 2020; Sun et al., 2020; Gupta et al., 2020; Ji et al., 2021) first edit a single noun or adjective in x to create a series of similar sentences as test cases, then denote M as passing\nthe behavioral testing if its translations of these sentences have similar syntactic structures, based on an assumption that the translations of similar sentences should be analogous. The reason why they only modify a single noun or adjective is to avoid largely shifting the structure of x, yet still limits the types of capability they can test as well as translation errors they can find. Further, even small modifications in x could dramatically change its semantic meaning, making the assumption of these methods not stable and thus largely biases their corresponding evaluation results.\nOther Error Diagnosing Methods. To provide fine-grained evaluation of MT systems, various evaluation methods except behavioral testing have been proposed, such as evaluating robustness to adversarial perturbations (Zhang et al., 2021; Lai et al., 2022), ambiguous words (Emelin et al., 2020) and diverse biases (Saunders and Byrne, 2020; Wang et al., 2022a). However, they all focus on a specific type of phenomenon in MT, thus lack the generality in diagnosing translation errors."
        },
        {
            "heading": "3 BTPGBT Framework",
            "text": "To solve issues in previous works, we propose a novel BTPGBT framework to conduct behavioral testing for general translation error diagnosis. Given a source sentence x and its reference r, the core idea of BTPGBT is to automatically edit one or more segments in x and r to construct various test cases x\u2032 and their pseudo-references r\u2032 targeting specific capabilities of MT systems. Then we can diagnose these systems following the steps described in \u00a72. In the following sections, we first describe how we implement the above core idea, then illustrate the capabilities BTPGBT tests as well as how to generate test cases targeting these capabilities for behavioral testing."
        },
        {
            "heading": "3.1 BTPG Method",
            "text": "However, implementing the core idea of BTPGBT is challenging due to two reasons. First, an appropriate strategy to decide which segment in x and r can be modified is needed, since we do not want the modification to largely shift the meaning of nonedited parts in x and r and hampers the generation quality. Second, how to edit the selected segments to craft fluent x\u2032 and r\u2032 is a non-trivial problem, while similar works like text infilling (Zhu et al., 2019; Donahue et al., 2020; Xiao et al., 2022) cannot be directly applied since we need to edit both x\nand r at once. To tackle these two issues and construct qualified x\u2032 and r\u2032 effectively, we present the bilingual translation pair generation (BTPG) approach and detail it in the following.\nDetermining Modification Positions. The first step of BTPG is to determine the segments that will be edited in the source sentence x, where segments here consist of words and phrases. Inspired by Knight (2000), we require that only a consecutive segment in x that is solely aligned with another consecutive segment in r can be edited, which avoids largely shifting the structures and meanings of non-edited segments. Take Figure 1 as an example. The segment \u201cIn order to tell\u201d in x cannot be edited since it is aligned with a non-consecutive segment \u201c\u4e3a\u4e86...\u8bb2\u51fa\u201d in y. Specifically, we apply the Mask-Align tool proposed by Chen et al. (2021a) to obtain a word-to-word alignment between x and r for extracting two types of segments in x that can be edited:\n\u2022 a word in x that only aligns with a word or a consecutive phrase in r.\n\u2022 a consecutive phrase in x that only aligns with a word or a consecutive phrase in r.\nwhere phrases in x and r are identified by Stanza 4. If a longer segment overlaps with another shorter segment, we will only keep the longer segment since it is usually more complete. Finally, we select one or several segments from the extracted segments for further editing.\nGenerating Bilingual Translation Pairs. Next, we illustrate how to craft a new bilingual translation pair (x\u2032, r\u2032). Specifically, we first mask the selected segments in x and its aligned segments in r to construct a masked translation pair (x\u0302, r\u0302), then construct a new bilingual translation pair (x\u2032, r\u2032) via filling in the masked positions in (x\u0302, r\u0302). However, existing text infilling approaches are conducted solely on source sentences (Zhu et al., 2019; Donahue et al., 2020) or target sentences (Chen et al., 2021b; Wang et al., 2022b; Xiao et al., 2022), and thus could not be adapted to our task since we need to fill in x\u0302 and r\u0302 at once.\nRecently, the large language model ChatGPT designed by OpenAI has shown impressive performances on various NLP tasks including text generation and machine translation (Qin et al., 2023;\n4https://stanfordnlp.github.io/stanza/\nJiao et al., 2023; Hendy et al., 2023; Peng et al., 2023), inspiring us to apply ChatGPT to generate x\u2032 and r\u2032. However, ChatGPT is still struggling with generating unbiased translations without hallucinations (Hendy et al., 2023; Guerreiro et al., 2023) when handling machine translation related tasks. To tackle this issue and make the best use of the power of ChatGPT, we propose to incorporate the masked translation pair (x\u0302, r\u0302) that contain semantic information of (x, r) to build prompts for constructing a new bilingual translation pair. Given a masked pair (x\u0302, r\u0302), we instruct ChatGPT through its API to first fill in the masked position in x\u0302, then fill in the masked positions in r\u0302 based on the filled x\u0302. We then guide ChatGPT to paraphrase the filled sentences to ensure fluency. After all, we obtained a new bilingual translation pair (x\u2032, r\u2032) where x\u2032 is the test case and r\u2032 is its pseudo-reference. Through repeating the above steps, we can craft various test cases and their pseudo-references from (x, r)."
        },
        {
            "heading": "3.2 Constructing Test Cases Targeting Various Capabilities",
            "text": "With BTPG, we can make edits on any segments in x that is aligned with another segment in r to build test cases and their pseudo-references for behavioral testing. This generality enables BTPGBT to diagnose translation errors towards various capabilities of MT systems. In the following, we describe the nine types of MT capabilities we diagnose with BTPGBT as well as how to use BTPG to craft targeted test cases and their pseudo-references.\nFirst, we consider a reliable MT system should appropriately understand the meanings and functions of segments that include words with different parts of speech. Therefore, we construct test cases with edits targeting seven types of parts of speech (POS). Next, it is crucial for MT systems to understand the tenses of different verbs in a sentence for correctly translating this sentence, thus we create test cases that edit the tense of a verb in a sentence to diagnose this capability (Tense). Finally, named entities that usually convey important information of a sentence should be translated properly. To this end, we build test cases with named entitiesbased edits to investigate MT systems\u2019 capability to properly translate named entities (NER). Except for modifying segments to other segments in the same category, we are also interested in MT systems\u2019 capability to handle uncertain modifications\nin the source sentence. Therefore, we craft test cases that do not limit the type and number of edits to examine this specific capability (General).\nDetailed descriptions of each capability and the corresponding test cases generation instructions are shown in Table 1. For each capability, the selected segments that will be masked in (x\u0302, r\u0302) should also meet the condition shown in Table 1. Then we can craft test cases x\u2032 and their pseudo-reference r\u2032 with BTPG for behavioral testing."
        },
        {
            "heading": "3.3 Judging Behavioral Testing Results",
            "text": "With test cases and their pseudo-references in hand, we judge an MT system M\u2019s performance on a test case x\u2032 following the principled definition in \u00a72.1 using its pseudo-reference r\u2032. To further enhance the reliability of Eq.2 when judging behavioral testing results, for each crafted test case x\u2032, we keep it only if Diff(r, r\u2032) \u2264 \u03b2, using the reference-free version of the widely used metric COMET (wmt20COMET-qe-da) (Rei et al., 2022) as the quality measurement Qual(). This filtering step avoids the situation that Diff(y,y\u2032) \u2264 \u03b2 is caused by the difference between their corresponding references."
        },
        {
            "heading": "4 Experiments",
            "text": "We conduct experiments on the English-Chinese translation task. First, we implement both auto-\nmatic and human evaluations to measure the quality of the BTPG method. Next, we apply BTPGBT to diagnose six MT systems to show the effectiveness of our proposed behavioral testing framework, and perform in-depth analysis on the evaluation results to obtain some insightful findings."
        },
        {
            "heading": "4.1 Settings",
            "text": "We conduct our experiments using data extracted from the test sets of the WMT21/22 En-Zh/Zh-En news translation task 5 to build a dataset named WMT for our experiments, which are not included in the training data of our evaluated research MT systems. As for detailed settings of the BTPGBT framework, we apply the reference-based COMET22 metric (wmt22-COMET-da) as the quality measurement Qual(). \u03b1 and \u03b2 in Eq. 2 are set to 0.8 and 0.05, respectively 6."
        },
        {
            "heading": "4.2 Quality of BTPG",
            "text": "Since we aim at using BTPG to craft high-quality bilingual translation pairs as behavioral testing cases and their pseudo-references, we are interested in its generation quality. To this end, we randomly sample 200 source sentences and their\n5https://www.statmt.org/wmt21/, https: //www.statmt.org/wmt22/\n6We provide details on how we set the value of the two hyperparameters in Appendix B\nreferences from WMT and use BTPG to create 200 bilingual translation pairs as test cases and their pseudo-references for evaluation 7."
        },
        {
            "heading": "4.2.1 Evaluation Methods",
            "text": "We conduct human evaluations to evaluate the test cases and their pseudo-references generated by BTPG from three perspectives: source sentence fluency, target sentence fluency and translation adequacy. For translation adequacy, we also apply an automatic metric as an evaluation supplement.\nSource Sentence Fluency (SSF). It measures whether the test case x\u2032 is semantically meaningful and grammatically correct. For human evaluation, we ask human annotators to score the fluency of x\u2032 based on a three-point rating scale, in which 1/2/3 represents unsatisfied/fair/satisfied, respectively.\nTarget Sentence Fluency (TSF). Similar to SSF, it measures whether the pseudo-reference r\u2032 of x\u2032 is semantically meaningful and grammatically correct. Likewise, we adopt the same three-point scale to score each r\u2032 during human evaluation.\nTranslation Adequacy (TA). It captures whether the crafted pseudo-reference r\u2032 of x\u2032 has translated all the information conveyed by x\u2032. For human evaluation, we ask annotators to compare x\u2032 and r\u2032 word-by-word and give a score for r\u2032 following the same scoring scale as SSF. As for automatic evaluation, since we do not have a ground truth for r\u2032, we apply the aforementioned wmt20-COMETqe-da metric to measure translation adequacy 8, where a higher score indicates a more adequate translation.\nFor each human evaluation task, we invite three annotators who are proficient in both English and Chinese to rate the given text and they achieve a Krippendorff\u2019s alpha score (Krippendorff, 2011) of 0.82/0.80/0.80 on SSF/TSF/TA, respectively, indicating a high inter-agreement. See Appendix C for detailed instructions of our human evaluations."
        },
        {
            "heading": "4.2.2 Baselines",
            "text": "To illustrate the effectiveness of BTPG, we compare it with two baselines:\n7To obtain more general evaluation results, we choose to target the General capability when generating test cases and skip the filtering step in \u00a73.3.\n8We do not use the latest reference-free wmt22COMETkiwi-da metric since it was unavailable at the time of our experiments (May 2023).\n\u2022 BART+BiTIMT. This approach is a combination of two text infilling models that generate x\u2032 and r\u2032 by first infilling the masked source x\u0302, then infilling the masked reference r\u0302. For source side infilling, we apply the widely used BART model (Lewis et al., 2020). As for the target side, we adopt the BiTIMT model (Xiao et al., 2022) to infill r\u0302 on top of x\u2032 filled by BART to craft r\u2032. \u2022 Ref. We are also interested in the quality of x\u2032 and r\u2032 crafted by BTPG compared to the source sentence x and its reference r. Specifically, we use the 200 original translation pairs sampled from WMT as x\u2032 and r\u2032 for comparison."
        },
        {
            "heading": "4.2.3 Results",
            "text": "Table 2 lists the quality evaluation results. We observe that Ref and BTPG largely outperform BART+BiTIMT across the board, demonstrating the limitation of prior text infilling approaches that merely consider filling the masked positions without ensuring the fluency of the completed sentence. This characteristic also strictly lowers the translation performances of BART+BiTIMT, proven by its low TA and COMET scores. Conversely, x\u2032 and r\u2032 crafted by BTPG obtain comparable fluency scores with Ref, which ensures the quality of test cases and their pseudo-references constructed by BTPG. Surprisingly, BTPG achieves slightly higher TA and COMET scores than Ref, demonstrating that the pseudo-references r\u2032 can act as the reference translation of crafted test cases x\u2032. This conclusion is crucial since it supports our idea to conduct MT behavioral testing following the principled definition in \u00a72 using the references of generated test cases."
        },
        {
            "heading": "4.3 Testing MT Systems with BTPGBT",
            "text": "Subsequently, we apply BTPGBT to examine various capabilities outlined in Table 1. For each capability except General (which is tested in Table 5), we randomly sample 1000 source sentences and their references from WMT and filter those that do not contain segments required by the tested ca-\npability (e.g., noun/noun phrase when testing the \u201cNoun\u201d capability). Then we use BTPGBT to construct test cases targeting these capabilities to test different MT systems and obtain the corresponding Pass Rates. For each capability in Table 1, we use a slightly different prompt for ChatGPT based on its instructions and list the prompt template targeting the POS capability in Table 3 as an example. The prompts targeting other capabilities are shown in Appendix \u00a7A. Noting that we only generate one test case for a given source sentence in this experiment, yet crafting more test cases can be simply done by repeating the generation process.\nMT Systems. In this work, we evaluate various commercial translation systems, including Google Translate (Google) 9, Microsoft Azure Translate (MS-Translator) 10, DeepL Translate (DeepL) 11 and Tencent Transmart (Tencent) (Huang et al., 2021) 12. We also evaluate a Transformerbased (Vaswani et al., 2017) NMT model as an evaluation supplement(Transformer) 13.\nAs mentioned in \u00a73.1, ChatGPT has recently shown strong ability in machine translation tasks. Therefore, we also evaluate ChatGPT using BTPGBT. The prompt for instructing ChatGPT as a translator is shown in Table 3 (Direct Translation).\nResults. Table 4 lists the behavioral testing results on different MT systems. We observe that all the other systems outperform Transformer on\n9https://translate.google.com/ 10https://learn.microsoft.com/en-us/azure/\ncognitive-services/translator/ 11https://www.deepl.com/translator 12https://transmart.qq.com/zh-CN/index 13See Appendix D for details of this model.\nall the capabilities, indicating the need for more advanced strategies in the design of robust MT systems. On the other hand, although ChatGPT largely outperforms Transformer, it still makes more translation errors compared to the top commercial MT system, particularly on test cases targeting Noun and Adv. This result illustrates that there still exists room for ChatGPT to become a stable translator. To our surprise, the commercial MT systems fail on 6%-17% of test cases targeting different capabilities, even when they are under regular testing and improvement. This finding further illustrates the importance of performing fine-grained error diagnosis with BTPGBT. Note that DeepL outperforms other commercial systems on all capabilities except Others, showing its strong ability to handle different types of edits in the input sentence.\nLarge-scale Behavioral Testing. Since BTPGBT can generalize large numbers of test cases and their pseudo-references efficiently, we also conduct behavioral testing on MT systems at scale to obtain unbiased error diagnosis. Concretely, we craft 20540 test cases targeting the General capability mentioned in Table 1 to test MT systems. As shown in Table 5, we notice that the performances of MT systems are similar to Table 4, where DeepL still outperforms other systems and all the systems outperform Transformer. These results further demonstrate that behavioral testing results in Table 4 are unbiased and could provide reliable error diagnosis of MT systems."
        },
        {
            "heading": "4.4 Effectiveness of BTPGBT",
            "text": "To further demonstrate the effectiveness of BTPGBT in diagnosing translation errors, we in-"
        },
        {
            "heading": "MT System Pass Rate",
            "text": "vestigate its testing results from two perspectives: (1) the proportion of actual erroneous translations within the erroneous translations diagnosed by BTPGBT (Precision). (2) how many actual erroneous translations can BTPGBT identify (Recall). To this end, we first randomly select 100 distinct source sentences in WMT and pick their test cases crafted in the previous large-scale behavioral testing, then evaluate Google\u2019s translations on these test cases with BTPGBT. To calculate Precision and Recall scores, we manually annotate all the actual erroneous translations produced by Google on these test cases. Besides BTPGBT, we also experiment with the method designed by He et al. (2020) which identifies translation errors by comparing syntactic structures between original translations and translations of test cases (Dep), using its recommended configurations. As shown in Table 6, BTPGBT achieves much higher precision and recall scores compared to Dep, demonstrating\nthat using pseudo-references to evaluate the quality of test case translations is beneficial for detecting more true translation errors."
        },
        {
            "heading": "4.5 Error Tendency",
            "text": "Notably, all MT systems in Table 4 tend to generate more translation errors towards the capabilities Verb and Adv, while achieving relatively high pass rates on Others. In this section, we study this phenomenon from the linguistic perspective. For the Others capability, we investigate the corresponding test cases and find that 98.60% of the test cases are related to numeric words substitution. Since numeric words usually do not affect the structures of sentences, it is not hard for MT systems to correctly translate different numeric words without affecting the translation of sentences, thus leading to high pass rates on test cases targeting the Others capability.\nConversely, a verb often forms collocations with different prepositions depending on the context. This factor makes it challenging for MT systems to correctly interpret the functions and meanings\nof verbs across various sentences, thus leading to errors when translating test cases with verb-based edits. An illustrative example is shown in Figure 2. Switching the verb \u201cteetering\u201d in x to \u201cbalancing\u201d should only change the sentence translation at the modified position. However, this edit makes the strongest MT system DeepL forget to translate \u201con the edge of favour\u201d in y\u2032, probably because DeepL pays much attention to the collocation \u201cbalance with\u201d in x\u2032. The translation difficulty of verbs may also affect MT systems\u2019 capability to translate adverbs and lead to the low pass rates on test cases targeting the Adv capability, since adverbs are often used to modify verbs in a sentence. In conclusion, through splitting evaluations of translation behaviors, researchers can systematically diagnose translation errors in MT systems, thereby facilitating more effective improvements."
        },
        {
            "heading": "4.6 Appearing Position of Errors",
            "text": "As demonstrated in Table 4, BTPGBT enables the identification of numerous translation errors in MT systems. Given that these errors arise from test cases with segment-level edits, there are two potential reasons for such translation errors:\n\u2022 The segment-level edit directly induces the translation error, resulting in incorrect translation at the edited position, as exemplified in Figure 2.\n\u2022 The segment-level edit indirectly induces the translation error. In this case, even though the edited position might be correctly translated, it affects the correct translation of other unedited parts of the test case and thus leads to translation errors.\nTo dive deeper into this phenomenon, we investigate whether the translation errors diagnosed by BTPGBT appear at the edited position, shedding light on why MT systems fail on such test cases. We annotate all the erroneous translations outputted by Google, DeepL and ChatGPT on three capabilities in Table 4, and calculate the percentage of\nerroneous translations whose translation errors appear at the modified position. Results are shown in Table 7. We observe that few translation errors identified by BTPGBT appear at the modified position, indicating that existing MT systems are proficient at translating individual segments in a sentence, even when these segments are replaced. However, they lack the ability to comprehend changes in a sentence\u2019s structure caused by segment modifications, which consequently leads to translation errors in non-modified positions."
        },
        {
            "heading": "5 Conclusion",
            "text": "In this paper, we introduce a novel behavioral testing framework BTPGBT to diagnose general translation errors for MT systems. The key idea of BTPGBT is to auto-generate test cases and their pseudo-references, which facilitates the diagnosis of translation errors targeting various capabilities of MT systems. To this end, we design the BTPG approach to craft test cases and pseudo-references, whose effectiveness has been proven by extensive experiments. Experimental results on six different MT systems further demonstrate that BTPGBT can provide general and faithful behavioral testing results and lead to some insightful findings.\nLimitations\nIn our proposed behavioral testing framework BTPGBT, we incorporate a ChatGPT-equipped module to automatically craft test cases and their pseudo-references for general error diagnosis. However, ChatGPT does not keep the same translation and text generation quality on all languages, which may hinder us applying BTPGBT on low-resource translation tasks. This characteristic requires us to introduce additional methods under these scenarios. We will study this direction in our future work.\nEthical Considerations\nSince the processes of crafting pseudo-references is totally automatic, some toxic and harmful test cases might be generated when querying the ChatGPT model. It is also possible that some toxic and harmful translations are outputted by commercial MT systems when they make translation errors on the test cases. These potential issues require the actual users to perform comprehensive data postprocessing before conducting behavioral testing\nusing BTPGBT, and make sure that the found translation errors will not be used under non-debugging situations."
        },
        {
            "heading": "Acknowledgement",
            "text": "This research has been made possible by funding support from the Research Grants Council of Hong Kong under the General Research Fund project 16204720 and the Research Impact Fund project R6003-21."
        },
        {
            "heading": "A Additional Prompts for ChatGPT",
            "text": "In this section, we list the prompt templates used for querying ChatGPT to craft test cases targeting the Tense and NER capability in Table 8 for reference as a supplement to Table 3."
        },
        {
            "heading": "B Details on Setting Hyperparameter Values",
            "text": "In \u00a74.1, we mentioned that we set the values of the two hyperparameters \u03b1 and \u03b2 as 0.8 and 0.05, respectively. In this section, we provide more details about how these two values are selected.\nUnlike behavioral testing on NLP classification tasks where errors can be directly derived from the prediction outputs, we cannot directly identify testing errors from MT outputs. Therefore, we propose \u03b1 and \u03b2 to help identify translation errors from the MT outputs automatically. As shown in Equation 2:\n\u2022 \u03b1 controls a user\u2019s degree of acceptance of MT systems\u2019 outputs. If Qual(y) \u2265 \u03b1, we say that the user regards y as a good translation and accepts it for further testing. In the paper, \u03b1 is set to 0.8 since we use COMET as the quality evaluation metric and we regard y as a good translation if Qual(y) \u2265 0.8.\n\u2022 \u03b2 controls a user\u2019s tolerance on translation errors and if Diff(y,y\u2032) \u2264 \u03b2, we say that the quality of y\u2032 (the MT system\u2019s output on the modified test case) is similar to y and the MT system passes the behavioral test. In this paper, \u03b2 is set to 0.05 since we believe that a COMET difference lower than 0.05 is a reasonable shift caused by the modifications in the test cases. If Diff(y,y\u2032) \u2264 0.05, we regard y and y\u2032 have similar translation quality and the MT system passes the behavioral test.\nMoreover, we would like to illustrate that the value setting of the two hyperparameters will not affect the conclusions drawn from the evaluation results. To demonstrate this point, we perform an experiment by 1): varying the value of \u03b1 while fixing \u03b2; 2): varying the value of \u03b2 while fixing \u03b1, and obtaining the corresponding pass rates for the\nsix MT systems on test cases targeting the Noun, Verb and NER capabilities. Specifically, we set 1): \u03b1 as 0.5/0.6/0.7/0.8 while fixing \u03b2 = 0.05; 2): \u03b2 as 0.02/0.05/0.08/0.11 while fixing \u03b1 = 0.8, and list the results in Table 9, Table 10 and Table 11, respectively.\nFrom the three tables, we observe that the values of the two hyperparameters will not affect the conclusions obtained from Table 4:\n\u2022 Transformer obtains the worst results.\n\u2022 In most cases, ChatGPT makes more translation errors compared to the top commercial MT system.\n\u2022 DeepL outperforms other commercial MT systems.\n\u2022 MT systems make more errors on test cases targeting the Verb capability.\nIn conclusion, we point out the following points regarding the hyperparameter value setting:\n\u2022 \u03b1 is set to 0.8 since we use COMET as the quality evaluation metric and we regard y as a good translation if Qual(y) \u2265 0.8. \u03b2 is set to 0.05 since we believe that a COMET difference lower than 0.05 is a reasonable shift caused by the modifications in the test cases.\n\u2022 The values of these two hyperparameters will not affect the conclusions obtained from the experiments."
        },
        {
            "heading": "C Detailed Human Evaluation Instructions",
            "text": "The detailed instructions of our human evaluation tasks are shown in Table 12."
        },
        {
            "heading": "D Details of the Transformer-based MT System",
            "text": "In this section, we provide details of the Transformer-based MT system introduced in \u00a74.3, which contains six 512-dimensional encoder and decoder layers, respectively. The word-embedding size is set to 512. Two separate BPE models are applied to generate two vocabularies for English (\u223c48K tokens) and Chinese (\u223c59K tokens). We train Transformer using the WMT20 En-Zh news translation corpus that includes 31M sentence pairs (Bojar et al., 2017) for 300K steps with 6K warmup steps using the cosine learning rate\nscheduling strategy. During training, we use the Adam optimizer (\u03b21 = 0.9, \u03b22 = 0.98) with a learning rate of 1e-4. The dropout rate is set to 0.3 and the label smoothing rate is set to 0.2. The maximum number of tokens in a batch is set to 65536. Transformer achieves an average BLEU score of 38.66 on the WMT20 En-Zh test set 14.\n14For reference, the rank 1st system (Shi et al., 2020) on this set achieves an average BLEU score of 43.20."
        }
    ],
    "title": "Towards General Error Diagnosis via Behavioral Testing in Machine Translation",
    "year": 2023
}