{
    "abstractText": "Abstract grammatical knowledge\u2014of parts of speech and grammatical patterns\u2014is key to the capacity for linguistic generalization in humans. But how abstract is grammatical knowledge in large language models? In the human literature, compelling evidence for grammatical abstraction comes from structural priming. A sentence that shares the same grammatical structure as a preceding sentence is processed and produced more readily. Because confounds exist when using stimuli in a single language, evidence of abstraction is even more compelling from crosslingual structural priming, where use of a syntactic structure in one language primes an analogous structure in another language. We measure crosslingual structural priming in large language models, comparing model behavior to human experimental results from eight crosslingual experiments covering six languages, and four monolingual structural priming experiments in three non-English languages. We find evidence for abstract monolingual and crosslingual grammatical representations in the models that function similarly to those found in humans. These results demonstrate that grammatical representations in multilingual language models are not only similar across languages, but they can causally influence text produced in different languages.grammatical knowledge\u2014of parts of speech and grammatical patterns\u2014is key to the capacity for linguistic generalization in humans. But how abstract is grammatical knowledge in large language models? In the human literature, compelling evidence for grammatical abstraction comes from structural priming. A sentence that shares the same grammatical structure as a preceding sentence is processed and produced more readily. Because confounds exist when using stimuli in a single language, evidence of abstraction is even more compelling from crosslingual structural priming, where use of a syntactic structure in one language primes an analogous structure in another language. We measure crosslingual structural priming in large language models, comparing model behavior to human experimental results from eight crosslingual experiments covering six languages, and four monolingual structural priming experiments in three non-English languages. We find evidence for abstract monolingual and crosslingual grammatical representations in the models that function similarly to those found in humans. These results demonstrate that grammatical representations in multilingual language models are not only similar across languages, but they can causally influence text produced in different languages.",
    "authors": [
        {
            "affiliations": [],
            "name": "James A. Michaelov"
        },
        {
            "affiliations": [],
            "name": "Catherine Arnett"
        },
        {
            "affiliations": [],
            "name": "Tyler A. Chang"
        },
        {
            "affiliations": [],
            "name": "Benjamin K. Bergen"
        }
    ],
    "id": "SP:369ebe9983a282e15f9b4137258947b6d9d6bae8",
    "references": [
        {
            "authors": [
                "Danbi Ahn",
                "Victor S Ferreira."
            ],
            "title": "Shared vs separate structural representations: Evidence from cumulative cross-language structural priming",
            "venue": "Quarterly Journal of Experimental Psychology.",
            "year": 2023
        },
        {
            "authors": [
                "Kabir Ahuja",
                "Sunayana Sitaram",
                "Sandipan Dandapat",
                "Monojit Choudhury."
            ],
            "title": "On the Calibration of Massively Multilingual Language Models",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 4310\u2013",
            "year": 2022
        },
        {
            "authors": [
                "Rami Al-Rfou"
            ],
            "title": "Gcld3: CLD3 is a neural network model for language identification",
            "year": 2020
        },
        {
            "authors": [
                "Jordi Armengol-Estap\u00e9",
                "Casimiro Pio Carrino",
                "Carlos Rodriguez-Penagos",
                "Ona de Gibert Bonet",
                "Carme Armentano-Oller",
                "Aitor Gonzalez-Agirre",
                "Maite Melero",
                "Marta Villegas"
            ],
            "title": "Are Multilingual Models the Best Choice for Moderately Under",
            "year": 2021
        },
        {
            "authors": [
                "Jordi Armengol-Estap\u00e9",
                "Ona de Gibert Bonet",
                "Maite Melero."
            ],
            "title": "On the Multilingual Capabilities of Very Large-Scale English Language Models",
            "venue": "Proceedings of the Thirteenth Language Resources and Evaluation Conference, pages 3056\u20133068, Marseille,",
            "year": 2022
        },
        {
            "authors": [
                "Catherine Arnett",
                "Tyler A. Chang",
                "James A. Michaelov",
                "Benjamin K. Bergen"
            ],
            "title": "Crosslingual Structural Priming and the Pre-Training Dynamics of Bilingual Language Models",
            "year": 2023
        },
        {
            "authors": [
                "Mikel Artetxe",
                "Sebastian Ruder",
                "Dani Yogatama."
            ],
            "title": "On the Cross-lingual Transferability of Monolingual Representations",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4623\u20134637, Online. Association",
            "year": 2020
        },
        {
            "authors": [
                "Yoav Benjamini",
                "Yosef Hochberg."
            ],
            "title": "Controlling the False Discovery Rate: A Practical and Powerful Approach to Multiple Testing",
            "venue": "Journal of the Royal Statistical Society. Series B (Methodological), 57(1):289\u2013300.",
            "year": 1995
        },
        {
            "authors": [
                "Sarah Bernolet",
                "Robert J. Hartsuiker",
                "Martin J. Pickering."
            ],
            "title": "From language-specific to shared syntactic representations: The influence of second language proficiency on syntactic sharing in bilinguals",
            "venue": "Cognition, 127(3):287\u2013306.",
            "year": 2013
        },
        {
            "authors": [
                "Mireille Besson",
                "Marta Kutas",
                "Cyma Van Petten."
            ],
            "title": "An Event-Related Potential (ERP) Analysis of Semantic Congruity and Repetition Effects in Sentences",
            "venue": "Journal of Cognitive Neuroscience, 4(2):132\u2013 149.",
            "year": 1992
        },
        {
            "authors": [
                "Terra Blevins",
                "Luke Zettlemoyer."
            ],
            "title": "Language Contamination Helps Explains the Cross-lingual Capabilities of English Pretrained Models",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 3563\u20133574,",
            "year": 2022
        },
        {
            "authors": [
                "J. Kathryn Bock."
            ],
            "title": "Syntactic persistence in language production",
            "venue": "Cognitive Psychology, 18(3):355\u2013 387.",
            "year": 1986
        },
        {
            "authors": [
                "Holly P. Branigan",
                "Martin J. Pickering."
            ],
            "title": "An experimental approach to linguistic representation",
            "venue": "Behavioral and Brain Sciences, 40:e282.",
            "year": 2017
        },
        {
            "authors": [
                "Joan Bybee."
            ],
            "title": "Language, Usage and Cognition",
            "venue": "Cambridge University Press, Cambridge.",
            "year": 2010
        },
        {
            "authors": [
                "Zhenguang G. Cai",
                "Martin J. Pickering",
                "Holly P. Branigan."
            ],
            "title": "Mapping concepts to syntax: Evidence from structural priming in Mandarin Chinese",
            "venue": "Journal of Memory and Language, 66(4):833\u2013849.",
            "year": 2012
        },
        {
            "authors": [
                "Yuan Chai",
                "Yaobo Liang",
                "Nan Duan."
            ],
            "title": "CrossLingual Ability of Multilingual Masked Language Models: A Study of Language Structure",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Pa-",
            "year": 2022
        },
        {
            "authors": [
                "Tyler Chang",
                "Zhuowen Tu",
                "Benjamin Bergen."
            ],
            "title": "The geometry of multilingual language model representations",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 119\u2013136, Abu Dhabi, United Arab Emirates.",
            "year": 2022
        },
        {
            "authors": [
                "Baoguo Chen",
                "Yuefang Jia",
                "Zhu Wang",
                "Susan Dunlap",
                "Jeong-Ah Shin"
            ],
            "title": "Is word-order similarity necessary for cross-linguistic structural priming",
            "venue": "Second Language Research,",
            "year": 2013
        },
        {
            "authors": [
                "Ethan A Chi",
                "John Hewitt",
                "Christopher D Manning."
            ],
            "title": "Finding universal grammatical relations in multilingual bert",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5564\u20135577, Online. Association for",
            "year": 2020
        },
        {
            "authors": [
                "Sunjoo Choi",
                "Myung-Kwan Park."
            ],
            "title": "Syntactic priming in the L2 neural language model",
            "venue": "The Journal of Linguistic Science, 103:81\u2013104.",
            "year": 2022
        },
        {
            "authors": [
                "Noam Chomsky."
            ],
            "title": "Aspects of the Theory of Syntax",
            "venue": "MIT Press, Cambridge, MA, USA.",
            "year": 1965
        },
        {
            "authors": [
                "Alexis Conneau",
                "Kartikay Khandelwal",
                "Naman Goyal",
                "Vishrav Chaudhary",
                "Guillaume Wenzek",
                "Francisco Guzm\u00e1n",
                "Edouard Grave",
                "Myle Ott",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Unsupervised Cross-lingual Representation Learning at Scale",
            "venue": "In",
            "year": 2020
        },
        {
            "authors": [
                "Alexis Conneau",
                "Shijie Wu",
                "Haoran Li",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Emerging Cross-lingual Structure in Pretrained Language Models",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages",
            "year": 2020
        },
        {
            "authors": [
                "Pablo Contreras Kallens",
                "Ross Deans KristensenMcLachlan",
                "Morten H. Christiansen."
            ],
            "title": "Large Language Models Demonstrate the Potential of Statistical Learning in Language",
            "venue": "Cognitive Science, 47(3):e13256.",
            "year": 2023
        },
        {
            "authors": [
                "Gao",
                "Vedanuj Goswami",
                "Francisco Guzm\u00e1n",
                "Philipp Koehn",
                "Alexandre Mourachko",
                "Christophe Ropers",
                "Safiyyah Saleem",
                "Holger Schwenk",
                "Jeff Wang"
            ],
            "title": "No Language Left Behind: Scaling HumanCentered Machine Translation",
            "year": 2022
        },
        {
            "authors": [
                "Andrea Gregor de Varda",
                "Marco Marelli."
            ],
            "title": "Datadriven Cross-lingual Syntax: An Agreement Study with Massively Multilingual Models",
            "venue": "Computational Linguistics, pages 1\u201339.",
            "year": 2023
        },
        {
            "authors": [
                "Gary S. Dell",
                "Victor S. Ferreira."
            ],
            "title": "Thirty years of structural priming: An introduction to the special issue",
            "venue": "Journal of Memory and Language, 91:1\u20134.",
            "year": 2016
        },
        {
            "authors": [
                "Juuso Eronen",
                "Michal Ptaszynski",
                "Fumito Masui."
            ],
            "title": "Zero-shot cross-lingual transfer language selection using linguistic similarity",
            "venue": "Information Processing & Management, 60(3):103250.",
            "year": 2023
        },
        {
            "authors": [
                "Victor S. Ferreira",
                "Kathryn Bock."
            ],
            "title": "The functions of structural priming",
            "venue": "Language and Cognitive Processes, 21(7-8):1011\u20131029.",
            "year": 2006
        },
        {
            "authors": [
                "Zuzanna Fleischer",
                "Martin J. Pickering",
                "Janet F. McLean."
            ],
            "title": "Shared information structure: Evidence from cross-linguistic priming",
            "venue": "Bilingualism: Language and Cognition, 15(3):568\u2013579.",
            "year": 2012
        },
        {
            "authors": [
                "Stefan Frank."
            ],
            "title": "Cross-language structural priming in recurrent neural network language models",
            "venue": "Proceedings of the Annual Meeting of the Cognitive Science Society, 43(43).",
            "year": 2021
        },
        {
            "authors": [
                "Daniela Gerz",
                "Ivan Vuli\u0107",
                "Edoardo Maria Ponti",
                "Roi Reichart",
                "Anna Korhonen"
            ],
            "title": "On the Relation between Linguistic Typology and (Limitations",
            "year": 2018
        },
        {
            "authors": [
                "Adele Goldberg."
            ],
            "title": "Constructions at Work: The Nature of Generalization in Language",
            "venue": "Oxford University Press, Oxford, New York.",
            "year": 2006
        },
        {
            "authors": [
                "Adam Goodkind",
                "Klinton Bicknell."
            ],
            "title": "Predictive power of word surprisal for reading times is a linear function of language model quality",
            "venue": "Proceedings of the 8th Workshop on Cognitive Modeling and Computational Linguistics (CMCL 2018), pages 10\u201318,",
            "year": 2018
        },
        {
            "authors": [
                "Naman Goyal",
                "Jingfei Du",
                "Myle Ott",
                "Giri Anantharaman",
                "Alexis Conneau."
            ],
            "title": "Larger-Scale Transformers for Multilingual Masked Language Modeling",
            "venue": "Proceedings of the 6th Workshop on Representation Learning for NLP (RepL4NLP-2021), pages",
            "year": 2021
        },
        {
            "authors": [
                "Raffaele Guarasci",
                "Stefano Silvestri",
                "Giuseppe De Pietro",
                "Hamido Fujita",
                "Massimo Esposito."
            ],
            "title": "BERT syntactic transfer: A computational experiment on Italian, French and English languages",
            "venue": "Computer Speech & Language, 71:101261.",
            "year": 2022
        },
        {
            "authors": [
                "Robert J. Hartsuiker",
                "Martin J. Pickering",
                "Eline Veltkamp."
            ],
            "title": "Is Syntax Separate or Shared Between Languages?: Cross-Linguistic Syntactic Priming in Spanish-English Bilinguals",
            "venue": "Psychological Science, 15(6):409\u2013414.",
            "year": 2004
        },
        {
            "authors": [
                "Petra Hendriks."
            ],
            "title": "Asymmetries between Language Production and Comprehension, volume 42 of Studies in Theoretical Psycholinguistics",
            "venue": "Springer Netherlands, Dordrecht.",
            "year": 2014
        },
        {
            "authors": [
                "John Hewitt",
                "Christopher D. Manning."
            ],
            "title": "A Structural Probe for Finding Syntax in Word Representations",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech-",
            "year": 2019
        },
        {
            "authors": [
                "Maren Heydel",
                "Wayne S. Murray."
            ],
            "title": "Conceptual Effects in Sentence Priming: A Cross-Linguistic Perspective",
            "venue": "Marica De Vincenzi and Vincenzo Lombardo, editors, Cross-Linguistic Perspectives on Language Processing, Studies in Theoretical Psy-",
            "year": 2000
        },
        {
            "authors": [
                "Simonyan",
                "Erich Elsen",
                "Oriol Vinyals",
                "Jack William Rae",
                "Laurent Sifre."
            ],
            "title": "An empirical analysis of compute-optimal large language model training",
            "venue": "Advances in Neural Information Processing Systems.",
            "year": 2022
        },
        {
            "authors": [
                "Nora Hollenstein",
                "Federico Pirovano",
                "Ce Zhang",
                "Lena J\u00e4ger",
                "Lisa Beinborn."
            ],
            "title": "Multilingual language models predict human reading behavior",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computa-",
            "year": 2021
        },
        {
            "authors": [
                "Ari Holtzman",
                "Jan Buys",
                "Li Du",
                "Maxwell Forbes",
                "Yejin Choi."
            ],
            "title": "The Curious Case of Neural Text Degeneration",
            "venue": "International Conference on Learning Representations.",
            "year": 2020
        },
        {
            "authors": [
                "Yufen Hsieh."
            ],
            "title": "Structural priming during sentence comprehension in Chinese\u2013English bilinguals",
            "venue": "Applied Psycholinguistics, 38(3):657\u2013678.",
            "year": 2017
        },
        {
            "authors": [
                "Alexander Jones",
                "William Yang Wang",
                "Kyle Mahowald."
            ],
            "title": "A Massively Multilingual Analysis of Cross-linguality in Shared Embedding Space",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 5833\u2013",
            "year": 2021
        },
        {
            "authors": [
                "Pratik Joshi",
                "Sebastin Santy",
                "Amar Budhiraja",
                "Kalika Bali",
                "Monojit Choudhury."
            ],
            "title": "The State and Fate of Linguistic Diversity and Inclusion in the NLP World",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages",
            "year": 2020
        },
        {
            "authors": [
                "Armand Joulin",
                "Edouard Grave",
                "Piotr Bojanowski",
                "Tomas Mikolov."
            ],
            "title": "Bag of Tricks for Efficient Text Classification",
            "venue": "Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Pa-",
            "year": 2017
        },
        {
            "authors": [
                "Karthikeyan K",
                "Zihan Wang",
                "Stephen Mayhew",
                "Dan Roth."
            ],
            "title": "Cross-Lingual Ability of Multilingual BERT: An Empirical Study",
            "venue": "International Conference on Learning Representations.",
            "year": 2020
        },
        {
            "authors": [
                "Jared Kaplan",
                "Sam McCandlish",
                "Tom Henighan",
                "Tom B. Brown",
                "Benjamin Chess",
                "Rewon Child",
                "Scott Gray",
                "Alec Radford",
                "Jeffrey Wu",
                "Dario Amodei"
            ],
            "title": "Scaling Laws for Neural Language Models",
            "year": 2020
        },
        {
            "authors": [
                "Stav Klein",
                "Reut Tsarfaty"
            ],
            "title": "Getting the ##life out of living: How Adequate Are Word-Pieces for Modelling Complex Morphology",
            "venue": "In Proceedings of the 17th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology,",
            "year": 2020
        },
        {
            "authors": [
                "Sotiria Kotzochampou",
                "Vasiliki Chondrogianni."
            ],
            "title": "How similar are shared syntactic representations? Evidence from priming of passives in Greek\u2013English bilinguals",
            "venue": "Bilingualism: Language and Cognition, 25(5):726\u2013738.",
            "year": 2022
        },
        {
            "authors": [
                "Taku Kudo",
                "John Richardson."
            ],
            "title": "SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System",
            "year": 2018
        },
        {
            "authors": [
                "Tatsuki Kuribayashi",
                "Yohei Oseki",
                "Takumi Ito",
                "Ryo Yoshida",
                "Masayuki Asahara",
                "Kentaro Inui."
            ],
            "title": "Lower Perplexity is Not Always Human-Like",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th",
            "year": 2021
        },
        {
            "authors": [
                "Bai Li",
                "Zining Zhu",
                "Guillaume Thomas",
                "Frank Rudzicz",
                "Yang Xu."
            ],
            "title": "Neural reality of argument structure constructions",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 7410\u20137423,",
            "year": 2022
        },
        {
            "authors": [
                "moyer",
                "Zornitsa Kozareva",
                "Mona Diab",
                "Veselin Stoyanov",
                "Xian Li"
            ],
            "title": "Few-shot Learning with Multilingual Generative Language Models",
            "venue": "In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2022
        },
        {
            "authors": [
                "Helga Loebell",
                "Kathryn Bock."
            ],
            "title": "Structural priming across languages",
            "venue": "Linguistics, 41(5):791\u2013 824.",
            "year": 2003
        },
        {
            "authors": [
                "Kyle Mahowald",
                "Ariel James",
                "Richard Futrell",
                "Edward Gibson."
            ],
            "title": "A meta-analysis of syntactic priming in language production",
            "venue": "Journal of Memory and Language, 91:5\u201327.",
            "year": 2016
        },
        {
            "authors": [
                "Clara D. Martin",
                "Francesca M. Branzi",
                "Moshe Bar."
            ],
            "title": "Prediction is Production: The missing link between language production and comprehension",
            "venue": "Scientific Reports, 8(1):1079.",
            "year": 2018
        },
        {
            "authors": [
                "Antje S. Meyer",
                "Falk Huettig",
                "Willem J.M. Levelt"
            ],
            "title": "Same, different, or closely related: What is the relationship between language production and comprehension",
            "venue": "Journal of Memory and Language,",
            "year": 2016
        },
        {
            "authors": [
                "James A. Michaelov",
                "Seana Coulson",
                "Benjamin K. Bergen."
            ],
            "title": "So Cloze yet so Far: N400 Amplitude is Better Predicted by Distributional Information than Human Predictability Judgements",
            "venue": "IEEE Transactions on Cognitive and Developmental Systems.",
            "year": 2022
        },
        {
            "authors": [
                "Penny F. Mitchell",
                "Sally Andrews",
                "Philip B. Ward."
            ],
            "title": "An event-related potential study of semantic congruity and repetition in a sentence-reading task: Effects of context change",
            "venue": "Psychophysiology, 30(5):496\u2013509.",
            "year": 1993
        },
        {
            "authors": [
                "banie",
                "Zaid Alyafeai",
                "Albert Webson",
                "Edward Raff",
                "Colin Raffel"
            ],
            "title": "Crosslingual Generalization through Multitask Finetuning",
            "venue": "In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics",
            "year": 2023
        },
        {
            "authors": [
                "Merel Muylle",
                "Sarah Bernolet",
                "Robert J. Hartsuiker."
            ],
            "title": "The Role of Case Marking and Word Order in Cross-Linguistic Structural Priming in Late L2 Acquisition",
            "venue": "Language Learning, 70(S2):194\u2013220.",
            "year": 2020
        },
        {
            "authors": [
                "Antoine Nzeyimana",
                "Andre Niyongabo Rubungo."
            ],
            "title": "KinyaBERT: A Morphology-aware Kinyarwanda Language Model",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages",
            "year": 2022
        },
        {
            "authors": [
                "Kelechi Ogueji",
                "Yuxin Zhu",
                "Jimmy Lin."
            ],
            "title": "Small Data? No Problem! Exploring the Viability of Pretrained Multilingual Language Models for Low-resourced Languages",
            "venue": "Proceedings of the 1st Workshop on Multilingual Representation Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Byung-Doh Oh",
                "William Schuler"
            ],
            "title": "Why Does Surprisal From Larger Transformer-Based Language Models Provide a Poorer Fit to Human Reading Times? Transactions of the Association for Computational Linguistics, 11:336\u2013350",
            "year": 2023
        },
        {
            "authors": [
                "Akintunde Oladipo",
                "Odunayo Ogundepo",
                "Kelechi Ogueji",
                "Jimmy Lin."
            ],
            "title": "An Exploration of Vocabulary Size and Transfer Effects in Multilingual Language Models for African Languages",
            "venue": "3rd Workshop on African Natural Language Processing.",
            "year": 2022
        },
        {
            "authors": [
                "Chanjun Park",
                "Sugyeong Eo",
                "Hyeonseok Moon",
                "Heuiseok Lim."
            ],
            "title": "Should we find another model?: Improving Neural Machine Translation Performance with ONE-Piece Tokenization Method without Model Modification",
            "venue": "Proceedings of the 2021 Conference",
            "year": 2021
        },
        {
            "authors": [
                "Martin J. Pickering",
                "Victor S. Ferreira."
            ],
            "title": "Structural priming: A critical review",
            "venue": "Psychological Bulletin, 134:427\u2013459.",
            "year": 2008
        },
        {
            "authors": [
                "Martin J. Pickering",
                "Simon Garrod"
            ],
            "title": "Do people use language production to make predictions during comprehension",
            "venue": "Trends in Cognitive Sciences,",
            "year": 2007
        },
        {
            "authors": [
                "Martin J. Pickering",
                "Simon Garrod."
            ],
            "title": "An integrated theory of language production and comprehension",
            "venue": "Behavioral and Brain Sciences, 36(4):329\u2013347.",
            "year": 2013
        },
        {
            "authors": [
                "Grusha Prasad",
                "Marten van Schijndel",
                "Tal Linzen."
            ],
            "title": "Using Priming to Uncover the Organization of Syntactic Representations in Neural Language Models",
            "venue": "Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL),",
            "year": 2019
        },
        {
            "authors": [
                "Jeff Stanway",
                "Lorrayne Bennett",
                "Demis Hassabis",
                "Koray Kavukcuoglu",
                "Geoffrey Irving"
            ],
            "title": "Scaling Language Models: Methods, Analysis & Insights from Training Gopher",
            "year": 2022
        },
        {
            "authors": [
                "David Reitter",
                "Frank Keller",
                "Johanna D. Moore."
            ],
            "title": "A Computational Cognitive Model of Syntactic Priming",
            "venue": "Cognitive Science, 35(4):587\u2013637.",
            "year": 2011
        },
        {
            "authors": [
                "Joost Rommers",
                "Kara D. Federmeier."
            ],
            "title": "Predictability\u2019s aftermath: Downstream consequences of word predictability as revealed by repetition effects",
            "venue": "Cortex, 101:16\u201330.",
            "year": 2018
        },
        {
            "authors": [
                "Michael D. Rugg."
            ],
            "title": "The Effects of Semantic Priming and Word Repetition on Event-Related Potentials",
            "venue": "Psychophysiology, 22(6):642\u2013647.",
            "year": 1985
        },
        {
            "authors": [
                "Michael D. Rugg."
            ],
            "title": "Event-related brain potentials dissociate repetition effects of high-and lowfrequency words",
            "venue": "Memory & Cognition, 18(4):367\u2013 379.",
            "year": 1990
        },
        {
            "authors": [
                "Sofie Schoonbaert",
                "Robert J. Hartsuiker",
                "Martin J. Pickering."
            ],
            "title": "The representation of lexical and syntactic information in bilinguals: Evidence from syntactic priming",
            "venue": "Journal of Memory and Language, 56(2):153\u2013171.",
            "year": 2007
        },
        {
            "authors": [
                "Abigail See",
                "Aneesh Pappu",
                "Rohun Saxena",
                "Akhila Yerukola",
                "Christopher D. Manning"
            ],
            "title": "Do Massively Pretrained Language Models Make Better Storytellers",
            "venue": "In Proceedings of the 23rd Conference on Computational Natural Language Learning",
            "year": 2019
        },
        {
            "authors": [
                "Amit Seker",
                "Elron Bandel",
                "Dan Bareket",
                "Idan Brusilovsky",
                "Refael Greenfeld",
                "Reut Tsarfaty."
            ],
            "title": "AlephBERT: Language Model Pre-training and Evaluation from Sub-Word to Sentence Level",
            "venue": "Proceedings of the 60th Annual Meeting of the",
            "year": 2022
        },
        {
            "authors": [
                "Jeong-Ah Shin."
            ],
            "title": "Structural priming and L2 proficiency effects on bilingual syntactic processing in production",
            "venue": "Korean Journal of English Language and Linguistics, 10(3):499\u2013518.",
            "year": 2010
        },
        {
            "authors": [
                "Jeong-Ah Shin",
                "Kiel Christianson."
            ],
            "title": "Syntactic processing in Korean\u2013English bilingual production: Evidence from cross-linguistic structural priming",
            "venue": "Cognition, 112(1):175\u2013180.",
            "year": 2009
        },
        {
            "authors": [
                "Anna Siewierska."
            ],
            "title": "Syntactic weight vs information structure and word order variation in Polish",
            "venue": "Journal of Linguistics, 29(2):233\u2013265.",
            "year": 1993
        },
        {
            "authors": [
                "Arabella Sinclair",
                "Jaap Jumelet",
                "Willem Zuidema",
                "Raquel Fern\u00e1ndez."
            ],
            "title": "Structural Persistence in Language Models: Priming as a Window into Abstract Language Representations",
            "venue": "Transactions of the Association for Computational Linguistics, 10:1031\u2013",
            "year": 2022
        },
        {
            "authors": [
                "Paul Soulos",
                "Sudha Rao",
                "Caitlin Smith",
                "Eric Rosen",
                "Asli Celikyilmaz",
                "R. Thomas McCoy",
                "Yichen Jiang",
                "Coleman Haley",
                "Roland Fernandez",
                "Hamid Palangi",
                "Jianfeng Gao",
                "Paul Smolensky"
            ],
            "title": "Structural Biases for Improving Transformers on Translation",
            "year": 2021
        },
        {
            "authors": [
                "Michael Tomasello."
            ],
            "title": "Constructing a Language: A Usage-Based Theory of Language Acquisition",
            "venue": "Harvard University Press.",
            "year": 2003
        },
        {
            "authors": [
                "Dimitra Irini Tzanidaki."
            ],
            "title": "Greek word order: towards a new approach",
            "venue": "UCL Working Paper in Linguistics, 7:247\u2013277.",
            "year": 1995
        },
        {
            "authors": [
                "Roger P.G. van Gompel",
                "Manabu Arai."
            ],
            "title": "Structural priming in bilinguals",
            "venue": "Bilingualism: Language and Cognition, 21(3):448\u2013455.",
            "year": 2018
        },
        {
            "authors": [
                "Cyma Van Petten",
                "Marta Kutas",
                "Robert Kluender",
                "Mark Mitchiner",
                "Heather McIsaac."
            ],
            "title": "Fractionating the Word Repetition Effect with Event-Related Potentials",
            "venue": "Journal of Cognitive Neuroscience, 3(2):131\u2013 150.",
            "year": 1991
        },
        {
            "authors": [
                "Elena Voita",
                "Ivan Titov."
            ],
            "title": "InformationTheoretic Probing with Minimum Description Length",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 183\u2013196, Online. Association for",
            "year": 2020
        },
        {
            "authors": [
                "Xiangpeng Wei",
                "Haoran Wei",
                "Huan Lin",
                "Tianhao Li",
                "Pei Zhang",
                "Xingzhang Ren",
                "Mei Li",
                "Yu Wan",
                "Zhiwei Cao",
                "Binbin Xie",
                "Tianxiang Hu",
                "Shangjie Li",
                "Binyuan Hui",
                "Bowen Yu",
                "Dayiheng Liu",
                "Baosong Yang",
                "Fei Huang",
                "Jun Xie"
            ],
            "title": "PolyLM: An Open Source",
            "year": 2023
        },
        {
            "authors": [
                "Guillaume Wenzek",
                "Marie-Anne Lachaux",
                "Alexis Conneau",
                "Vishrav Chaudhary",
                "Francisco Guzm\u00e1n",
                "Armand Joulin",
                "Edouard Grave."
            ],
            "title": "CCNet: Extracting High Quality Monolingual Datasets from Web Crawl Data",
            "venue": "Proceedings of the Twelfth Lan-",
            "year": 2020
        },
        {
            "authors": [
                "Ethan Wilcox",
                "Pranali Vani",
                "Roger Levy."
            ],
            "title": "A targeted assessment of incremental processing in neural language models and humans",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International",
            "year": 2021
        },
        {
            "authors": [
                "Genta Winata",
                "Shijie Wu",
                "Mayank Kulkarni",
                "Thamar Solorio",
                "Daniel Preotiuc-Pietro."
            ],
            "title": "Crosslingual Few-Shot Learning on Unseen Languages",
            "venue": "Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Lin-",
            "year": 2022
        },
        {
            "authors": [
                "Joe Davison",
                "Sam Shleifer",
                "Patrick von Platen",
                "Clara Ma",
                "Yacine Jernite",
                "Julien Plu",
                "Canwen Xu",
                "Teven Le Scao",
                "Sylvain Gugger",
                "Mariama Drame",
                "Quentin Lhoest",
                "Alexander Rush."
            ],
            "title": "Transformers: State-of-the-Art Natural Language Processing",
            "venue": "In",
            "year": 2020
        },
        {
            "authors": [
                "Zhengxuan Wu",
                "Isabel Papadimitriou",
                "Alex Tamkin"
            ],
            "title": "Oolong: Investigating What Makes Crosslingual Transfer Hard with Controlled Studies",
            "year": 2022
        },
        {
            "authors": [
                "Jin Xu",
                "Xiaojiang Liu",
                "Jianhao Yan",
                "Deng Cai",
                "Huayang Li",
                "Jian Li."
            ],
            "title": "Learning to Break the Loop: Analyzing and Mitigating Repetitions for Neural Text Generation",
            "venue": "Advances in Neural Information Processing Systems, 35:3082\u20133095.",
            "year": 2022
        },
        {
            "authors": [
                "Linting Xue",
                "Noah Constant",
                "Adam Roberts",
                "Mihir Kale",
                "Rami Al-Rfou",
                "Aditya Siddhant",
                "Aditya Barua",
                "Colin Raffel."
            ],
            "title": "mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer",
            "venue": "Proceedings of the 2021 Conference of the North American",
            "year": 2021
        },
        {
            "authors": [
                "Jayden Ziegler",
                "Rodrigo Morato",
                "Jesse Snedeker."
            ],
            "title": "Priming semantic structure in Brazilian Portuguese",
            "venue": "Journal of Cultural Cognitive Science, 3(1):25\u201337.",
            "year": 2019
        },
        {
            "authors": [
                "Xue"
            ],
            "title": "2021) as provided in the gcld3 python package (Al-Rfou, 2020). We use a stricter threshold of 0.9 (as recommended for high-resource languages; Costa-juss\u00e0 et al., 2022) for the former and use the default threshold",
            "year": 2022
        },
        {
            "authors": [
                "Lin"
            ],
            "title": "2022) by 500B, the total number of tokens. We first provide two estimates of contamination for Dutch and Polish in Table 1: the amount of contamination",
            "year": 2022
        },
        {
            "authors": [
                "Cai"
            ],
            "title": "Experiment 1/2 Mandarin\u2192Mandarin_Target",
            "venue": "Cai et al",
            "year": 2012
        },
        {
            "authors": [
                "Cai"
            ],
            "title": "Experiment 1/2 Mandarin\u2192Mandarin_Target",
            "venue": "Cai et al",
            "year": 2012
        },
        {
            "authors": [
                "Cai"
            ],
            "title": "Experiment 1/2 Mandarin\u2192Mandarin_Target",
            "venue": "Cai et al",
            "year": 2012
        },
        {
            "authors": [
                "Cai"
            ],
            "title": "Experiment 1/2 Mandarin\u2192Mandarin_Target",
            "venue": "Cai et al",
            "year": 2012
        },
        {
            "authors": [
                "Cai"
            ],
            "title": "Experiment 1/2 Mandarin\u2192Mandarin_Target",
            "venue": "Cai et al",
            "year": 2012
        },
        {
            "authors": [
                "Cai"
            ],
            "title": "Experiment 1/2 Mandarin\u2192Mandarin_Target",
            "venue": "Cai et al",
            "year": 2012
        },
        {
            "authors": [
                "Cai"
            ],
            "title": "Experiment 1/2 Mandarin\u2192Mandarin_Target",
            "venue": "Cai et al",
            "year": 2012
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "What do language models learn about the structure of the languages they are trained on? Under both more traditional generative (Chomsky, 1965) and cognitively-inspired usage-based theories of language (Tomasello, 2003; Goldberg, 2006; Bybee, 2010), the key to generalizable natural language comprehension and production is the acquisition of grammatical structures that are sufficiently abstract to account for the full range of possible sentences in a language. In fact, both theoretical and experimental accounts of language suggest that grammatical\n\u2217Equal contribution.\nrepresentations are abstract enough to be shared across languages in both humans (Heydel and Murray, 2000; Hartsuiker et al., 2004; Schoonbaert et al., 2007) and language models (Conneau et al., 2020b,a; Jones et al., 2021).\nThe strongest evidence for grammatical abstraction in humans comes from structural priming, a widely used and robust experimental paradigm. Structural priming is based on the hypothesis that grammatical structures may be activated during language processing. Priming then increases the likelihood of production or increased ease of processing of future sentences sharing the same grammatical structures (Bock, 1986; Ferreira and Bock, 2006; Pickering and Ferreira, 2008; Dell and Ferreira, 2016; Mahowald et al., 2016; Branigan and Pickering, 2017). For example, Bock (1986) finds that people are more likely to produce an active sentence (e.g. one of the fans punched the referee) than a passive sentence (e.g. the referee was punched by one of the fans) after another active sentence. This has been argued (Bock, 1986; Heydel and Murray, 2000; Pickering and Ferreira, 2008; Reitter et al., 2011; Mahowald et al., 2016; Branigan and Pickering, 2017) to demonstrate common abstractions generalized across all sentences with the same structure, regardless of content.\nResearchers have found evidence that structural priming for sentences with the same structure occurs even when the two sentences are in different languages (Loebell and Bock, 2003; Hartsuiker et al., 2004; Schoonbaert et al., 2007; Shin and Christianson, 2009; Bernolet et al., 2013; van Gompel and Arai, 2018; Kotzochampou and Chondrogianni, 2022). This crosslingual structural priming takes abstraction one step further. First, it avoids any possible confounding effects of lexical repetition and lexical priming of individual words\u2014 within a given language, sentences with the same structure often share function words (for discussion, see Sinclair et al., 2022). More fundamentally,\ncrosslingual structural priming represents an extra degree of grammatical abstraction not just within a language, but across languages.\nWe apply this same logic to language models in the present study. While several previous studies have explored structural priming in language models (Prasad et al., 2019; Sinclair et al., 2022; Frank, 2021; Li et al., 2022; Choi and Park, 2022), to the best of our knowledge, this is the first to look at crosslingual structural priming in Transformer language models. We replicate eight human psycholinguistic studies, investigating structural priming in English, Dutch (Schoonbaert et al., 2007; Bernolet et al., 2013), Spanish (Hartsuiker et al., 2004), German (Loebell and Bock, 2003), Greek (Kotzochampou and Chondrogianni, 2022), Polish (Fleischer et al., 2012), and Mandarin (Cai et al., 2012). We find priming effects in the majority of the crosslingual studies and all of the monolingual studies, which we argue supports the claim that multilingual models have shared grammatical representations across languages that play a functional role in language generation."
        },
        {
            "heading": "2 Background",
            "text": "Structural priming effects have been observed in humans both within a given language (Bock, 1986; Ferreira and Bock, 2006; Pickering and Ferreira, 2008; Dell and Ferreira, 2016; Mahowald et al., 2016; Branigan and Pickering, 2017) and crosslingually (Loebell and Bock, 2003; Hartsuiker et al., 2004; Schoonbaert et al., 2007; Shin and Christianson, 2009; Bernolet et al., 2013; van Gompel and Arai, 2018; Kotzochampou and Chondrogianni, 2022). In language models, previous work has demonstrated structural priming effects in English (Prasad et al., 2019; Sinclair et al., 2022; Choi and Park, 2022), and initial results have found priming effects between English and Dutch in LSTM language models (Frank, 2021). As these studies argue, the structural priming approach avoids several possible assumptions and confounds found in previous work investigating abstraction in grammatical learning. For example, differences in language model probabilities for individual grammatical vs. ungrammatical sentences may not imply that the models have formed abstract grammatical representations that generalize across sentences (Sinclair et al., 2022); other approaches involving probing (e.g. Hewitt and Manning, 2019; Chi et al., 2020) often do not test whether the internal model states\nare causally involved in the text predicted or generated by the model (Voita and Titov, 2020; Sinclair et al., 2022). The structural priming paradigm allows researchers to evaluate whether grammatical representations generalize across sentences in language models, and whether these representations causally influence model-generated text. Furthermore, structural priming is agnostic to the specific language model architecture and does not rely on direct access to internal model states.\nHowever, the structural priming paradigm has not been applied to modern multilingual language models. Previous work has demonstrated that multilingual language models encode grammatical features in shared subspaces across languages (Chi et al., 2020; Chang et al., 2022; de Varda and Marelli, 2023), largely relying on probing methods that do not establish causal effects on model predictions. Crosslingual structural priming would provide evidence that the abstract grammatical representations shared across languages in the models have causal effects on model-generated text. It would also afford a comparison between grammatical representations in multilingual language models and human bilinguals. These shared grammatical representations may help explain crosslingual transfer abilities in multilingual models, where tasks learned in one language can be transferred to another (Artetxe et al., 2020; Conneau et al., 2020a,b; K et al., 2020; Goyal et al., 2021; Ogueji et al., 2021; Armengol-Estap\u00e9 et al., 2021, 2022; Blevins and Zettlemoyer, 2022; Chai et al., 2022; Muennighoff et al., 2023; Wu et al., 2022; Guarasci et al., 2022; Eronen et al., 2023).\nThus, this study presents what is to our knowledge the first experiment testing for crosslingual structural priming in Transformer language models. The findings broadly replicate human structural priming results: higher probabilities for sentences that share grammatical structure with prime sentences both within and across languages."
        },
        {
            "heading": "3 Method",
            "text": "We test multilingual language models for structural priming using the stimuli from eight crosslingual and four monolingual priming studies in humans. Individual studies are described in \u00a74."
        },
        {
            "heading": "3.1 Materials",
            "text": "All replicated studies have open access stimuli with prime sentences for different constructions (\u00a73.3).\nWhere target sentences are not provided (because participant responses were manually coded by the experimenters), we reconstruct target sentences and verify them with native speakers."
        },
        {
            "heading": "3.2 Language Models",
            "text": "We test structural priming in XGLM 4.5B (Lin et al., 2022), a multilingual autoregressive Transformer trained on data from all languages we study in this paper, namely, English, Dutch, Spanish, German, Greek, Polish, and Mandarin. To the best of our knowledge, this is the only available pretrained (and not fine-tuned) autoregressive language model trained on all the aforementioned languages. To avoid drawing any conclusions based on the idiosyncrasies of a single language model, we also test a number of other multilingual language models trained on most of these languages, namely the other XGLM models, i.e., 564M, 1.7B, 2.9B, and 7.5B, which are trained on all the languages except for Dutch and Polish; and PolyLM 1.7B and 13B (Wei et al., 2023), which are trained on all the languages except for Greek."
        },
        {
            "heading": "3.3 Grammatical Alternations Tested",
            "text": "We focus on structural priming for the three alternations primarily used in existing human studies.\nDative Alternation (DO/PO) Some languages permit multiple orders of the direct and indirect objects in sentences. In PO (prepositional object) constructions, e.g., the chef gives a hat to the swimmer (Schoonbaert et al., 2007), the direct object a hat immediately follows the verb and the indirect object is introduced with the prepositional phrase to the swimmer. In DO (double object) constructions, e.g., the chef gives the swimmer a hat, the indirect object the swimmer appears before the direct object a hat and neither is introduced by a preposition. Researchers compare the proportion of DO or PO sentences produced by experimental participants following a DO or PO prime.\nActive/Passive In active sentences the syntactic subject is the agent of the action, while in passive sentences the syntactic subject is the patient or theme of the action. E.g., the taxi chases the truck is active, and the truck is chased by the taxi is passive (Hartsuiker et al., 2004). Researchers compare the proportion of active or passive sentences produced by experimental participants following an active or passive prime.\nOf-/S-Genitive Of - and S-Genitives represent two different ways of expressing possessive meaning. In an of -genitive, the possessed thing is followed by a preposition such as of and then the possessor, e.g., the scarf of the boy is yellow. In s-genitives in the languages we analyze (English and Dutch), the possessor is followed by a word or an attached morpheme such as \u2019s which is then followed by the possessed thing, e.g., the boy\u2019s scarf is yellow (Bernolet et al., 2013). Researchers compare the proportion of of -genitive or s-genitive sentences produced by experimental participants following an of -genitive or s-genitive prime."
        },
        {
            "heading": "3.4 Testing Structural Priming in Models",
            "text": "In human studies, researchers test for structural priming by comparing the proportion of sentences (targets) of given types produced following primes of different types. Analogously, for each experimental item, we prompt the language model with the prime sentence and compute the normalized probabilities of each of the two target sentences. We illustrate our approach to computing these normalized probabilities below.\nFirst, consider the example dative alternation stimulus sentences from Schoonbaert et al. (2007):\n(1) (a) DO prime: The cowboy shows the pirate an apple.\n(b) PO prime: The cowboy shows an apple to the pirate.\n(c) DO target: The chef gives the swimmer a hat.\n(d) PO target: The chef gives a hat to the swimmer.\nWe can use language models to calculate the probability of each target following each prime by taking the product of the conditional probabilities of all tokens in the target sentence given the prime sentence and all preceding tokens in the target sentence. In practice, these probabilities are very small, but for illustrative purposes, we can imagine these have the probabilities in (2).\n(2) (a) P(PO Target | DO Prime) = 0.03 (b) P(DO Target | DO Prime) = 0.02 (c) P(PO Target | PO Prime) = 0.04 (d) P(DO Target | PO Prime) = 0.01\nWe then normalize these probabilities by calculating the conditional probability of each target sentence given that the model response is one of the two target sentences, as shown in (3).\n(3) (a) PN (PO | DO) = 0.03/(0.03+0.02) = 0.60 (b) PN (DO | DO) = 0.02/(0.03+0.02) = 0.40 (c) PN (PO | PO) = 0.04/(0.04+0.01) = 0.80 (d) PN (DO | PO) = 0.01/(0.04+0.01) = 0.20\nBecause the normalized probabilities of the two targets following a given prime sum to one, we only consider the probabilities for one target type in our analyses (comparing over the two different prime types). For example, to test for a priming effect, we could either compare the difference between PN (PO | PO) and PN (PO | DO) or the difference between PN (DO | PO) and PN (DO | DO). We follow the original human studies in the choice of which target construction to plot and test.\nWe run statistical analyses, testing whether effects are significant for each language model on each set of stimuli. To do this, we construct a linear mixed-effects model predicting the target sentence probability (e.g. probability of a PO sentence) for each item. We include a random intercept for experimental item, and we test whether prime type (e.g. DO vs. PO) significantly predicts target structure probability. All reported p-values are corrected for multiple comparisons by controlling for false discovery rate (Benjamini and Hochberg, 1995). All stimuli, data, code, and statistical analyses are provided at https://osf.io/2vjw6/."
        },
        {
            "heading": "4 Results",
            "text": "In reporting whether the structural priming effects from human experiments replicate in XGLM language models, we primarily consider the direction of each effect in the language models (e.g. whether PO constructions are more likely after PO vs. DO primes) rather than effect sizes or raw probabilities. The mean of the relative probabilities assigned by language models to the different constructions in each condition may not be directly comparable to human probabilities of production. Humans are sensitive to contextual cues that may not be available to language models; notably, in these tasks, humans are presented with pictures corresponding to events in the structural priming paradigm. Furthermore, construction probabilities in language models may be biased by the frequency of related constructions in any of the many languages on which the models are trained. Thus, we focus only on whether the language models replicate the direction of the principal effect in each human study."
        },
        {
            "heading": "4.1 Crosslingual Structural Priming",
            "text": "We test whether eight human crosslingual structural priming studies replicate in language models. These studies cover structural priming between English and Dutch (Schoonbaert et al., 2007; Bernolet et al., 2013), Spanish (Hartsuiker et al., 2004), German (Loebell and Bock, 2003), Greek (Kotzochampou and Chondrogianni, 2022), and Polish (Fleischer et al., 2012). For each experiment, we show the original human probabilities and the normalized probabilities calculated using each language model, as well as whether there is a significant priming effect (Figure 1). The full statistical results are reported in Appendix B."
        },
        {
            "heading": "4.1.1 Schoonbaert et al. (2007):",
            "text": "Dutch\u2192English\nSchoonbaert et al. (2007) prime 32 Dutch-English bilinguals with 192 Dutch sentences with either prepositional (PO) or dative object (DO) constructions. Schoonbaert et al. (2007) find that experimental participants produce more PO sentences when primed with a PO sentence than when primed with a DO sentence (see Figure 1A). We see the same pattern with nearly all the language models (Figure 1A). With the exception of XGLM 1.7B, where the effect is only marginally significant after correction for multiple comparisons, all language models predict English PO targets to be significantly more likely when they follow Dutch PO primes than when they follow Dutch DO primes."
        },
        {
            "heading": "4.1.2 Schoonbaert et al. (2007):",
            "text": "English\u2192Dutch\nSchoonbaert et al. (2007) also observe DO/PO structural priming from English to Dutch (32 participants; 192 primes). As seen in Figure 1B, all language models show a significant priming effect."
        },
        {
            "heading": "4.1.3 Bernolet et al. (2013): Dutch\u2192English",
            "text": "Bernolet et al. (2013) conduct a Dutch\u2192English structural priming experiment with 24 DutchEnglish bilinguals on 192 prime sentences, and they find that the production of s-genitives is significantly more likely after an s-genitive prime than after an of -genitive prime. We also observe this in all of the language models, as seen in Figure 1C."
        },
        {
            "heading": "4.1.4 Hartsuiker et al. (2004):",
            "text": "Spanish\u2192English\nHartsuiker et al. (2004) investigate Spanish\u2192English structural priming with\n24 Spanish-English bilinguals on 128 prime sentences, finding a significantly higher proportion of passive responses after passive primes than active primes. As shown in Figure 1D, this effect is replicated by XGLM 564M, 2.9B, and 7.5B as well as PolyLM 13B, with XGLM 4.5B showing a marginal effect (p = 0.0565)."
        },
        {
            "heading": "4.1.5 Loebell and Bock (2003):",
            "text": "German\u2192English\nLoebell and Bock (2003) find a small but significant priming effect of dative alternation (DO/PO) from German to English with 48 German-English bilinguals on 32 prime sentences. As can be seen in Figure 1E, while all language models show a\nnumerical effect in the correct direction, the effect is only significant for XGLM 7.5B."
        },
        {
            "heading": "4.1.6 Loebell and Bock (2003):",
            "text": "English\u2192German\nLoebell and Bock (2003) also test 48 GermanEnglish bilinguals for a dative alternation (DO/PO) priming effect from English primes to German targets (32 prime sentences), finding a small but significant priming effect. As we show in Figure 1F, the models are relatively varied in direction of numerical difference. However, only XGLM 2.9B and PolyLM 13B display a significant effect, and in both cases the effect is in the same direction as that found with human participants."
        },
        {
            "heading": "4.1.7 Kotzochampou and Chondrogianni (2022): Greek\u2192English",
            "text": "Kotzochampou and Chondrogianni (2022) find active/passive priming from Greek to English in 25 Greek-English bilinguals. Participants are more likely to produce passive responses after passive primes (48 prime sentences). As shown in Figure 1G), all XGLMs display this effect, while the PolyLMs, which are not trained on Greek, do not."
        },
        {
            "heading": "4.1.8 Fleischer et al. (2012): Polish\u2192English",
            "text": "Similarly, Fleischer et al. (2012) find active/passive priming from Polish to English in 24 PolishEnglish bilinguals on 64 prime sentences. As we see in Figure 1H, while all models show a numerical difference in the correct direction, the effect is only significant for XGLM 564M, 2.9B, and 7.5B, and for PolyLM 1.7B."
        },
        {
            "heading": "4.2 Monolingual Structural Priming",
            "text": "In the previous section, we found crosslingual priming effects in language models for the majority of crosslingual priming studies in humans. However, six of the eight studies have English target sentences. Our results up to this point primarily show an effect of structural priming on English targets. While both previous work (Sinclair et al., 2022) and our results in \u00a74.1 may indeed demonstrate the effects of abstract grammatical representations on generated text in English, we should not assume that such effects can reliably be observed for other languages. Thus, we test whether multilingual language models exhibit within-language structural priming effects comparable to those found in human studies for Dutch (Schoonbaert et al., 2007), Greek (Kotzochampou and Chondrogianni, 2022), and two studies in Mandarin (Cai et al., 2012)."
        },
        {
            "heading": "4.2.1 Schoonbaert et al. (2007):",
            "text": "Dutch\u2192Dutch\nUsing Dutch prime and target sentences (192 primes), Schoonbaert et al. (2007) find that DutchEnglish bilinguals (N=32) produce PO sentences at a higher rate when primed by a PO sentence compared to a DO sentence. As we see in Figure 2A, all language models display this effect.\n4.2.2 Kotzochampou and Chondrogianni (2022): Greek\u2192Greek\nIn their Greek\u2192Greek priming experiment, Kotzochampou and Chondrogianni (2022) find an active/passive priming effect in native Greek speakers\n(N=25) using 48 primes. As shown in Figure 2B, this effect is replicated by all language models."
        },
        {
            "heading": "4.2.3 Cai et al. (2012): Mandarin\u2192Mandarin",
            "text": "Using two separate sets of stimuli, Cai et al. (2012) find within-language DO/PO priming effects in native Mandarin speakers (N=28, N=24).1 As seen in Figure 2C and 2D, all language models show significant effects for both sets of stimuli (48 prime sentences in their Experiments 1 and 2, and 68 prime sentences in their Experiment 3)."
        },
        {
            "heading": "4.3 Further Tests of Structural Priming",
            "text": "We have now observed within-language structural priming in multilingual language models for languages other than English. In \u00a74.1, we found robust English\u2192Dutch structural priming (Schoonbaert et al., 2007) but only limited priming effects for targets in German. Although there are no human results for the non-English targets in the other studies in \u00a74.1, we can still evaluate crosslingual structural priming with non-English targets in the language models by switching the prime and target sentences in the stimuli. Specifically, we test structural priming from English to Dutch (Bernolet et al., 2013), Spanish (Hartsuiker et al., 2004), Polish (Fleischer et al., 2012), and Greek (Kotzochampou and Chondrogianni, 2022).\nAll models show a significant effect on the reversed Bernolet et al. (2013) stimuli (Figure 3A; English\u2192Dutch), and all models but PolyLM 1.7B show the same for the reversed Hartsuiker et al. (2004) stimuli (Figure 3B; English\u2192Spanish). The other results are less clear-cut. While XGLM 564M, 2.9B, and 4.5B and the PolyLMs show a numerical effect in the correct direction for the reversed Fleischer et al. (2012) stimuli (English\u2192Polish; Figure 3C), only PolyLM 1.7B shows a significant effect. For the reversed Kotzochampou and Chondrogianni (2022) stimuli (English\u2192Greek; Figure 3D), all the XGLMs and PolyLM 13B show a numerical tendency in the correct direction, but only XGLM 564M and 4.5B show a significant effect.\n1The original study tests the effect of variants of DO/PO primes (topicalized DO/PO and Ba-DO; see Cai et al., 2012). To unify our analyses across studies, we only look at structural priming following the canonical DO and PO primes used in both Experiments 1 and 2 of the original study, as well as those used in Experiment 3."
        },
        {
            "heading": "5 Discussion",
            "text": "We find structural priming effects in at least one language model on each set of stimuli (correcting for multiple comparisons). Moreover, we observe a significant effect in all models with the monolingual stimuli, and in the majority of the models for 8 of the 12 crosslingual stimuli. In line with previous work (Hewitt and Manning, 2019; Chi et al., 2020), this supports the claim that language models learn generalized, abstract, and multilingual representations of grammatical structure. Our results further suggest that these shared grammatical representations are causally linked to model output."
        },
        {
            "heading": "5.1 Differences between models",
            "text": "In some ways, we see expected patterns across models. For example, for the XGLMs trained on 30 languages (XGLM 564M, 1.7B, 2.9B, and 7.5B), the larger models tend to display larger effect sizes than the smaller models, in line with the idea that model performance can scale with number of parameters (Brown et al., 2020; Kaplan et al., 2020; Rae et al., 2022; Hoffmann et al., 2022; Touvron et al., 2023). Additionally, the PolyLMs, which are not trained on Greek, do not show crosslingual structural priming for Greek (neither Greek\u2192English nor English\u2192Greek).\nOn the other hand, one surprising finding is that despite not being trained on Greek, the PolyLMs are able to successfully model monolingual structural priming in Greek. The most likely explanation for this is what Sinclair et al. (2022) refer to as \u2018lexical overlap\u2019\u2014the overlap of function words between primes and targets substantially boosts structural priming effects. In the\nsame way that humans find it easier to process words that have recently been mentioned (Rugg, 1985, 1990; Van Petten et al., 1991; Besson et al., 1992; Mitchell et al., 1993; Rommers and Federmeier, 2018), language models may predict that previously-mentioned words are more likely to occur again (a familiar phenomenon in the case of repeated text loops; see Holtzman et al., 2020; See et al., 2019; Xu et al., 2022) even if they are not trained on the words explicitly. This would explain the results for the Kotzochampou and Chondrogianni (2022) stimuli, as the Greek passive stimuli always include the word \u03b1\u03c0o\u0301.\nSuch an explanation could also account for the performance of XGLM 564M, 1.7B, 2.9B, and 7.5B on the Dutch and Polish stimuli. Despite not being intentionally trained on Dutch or Polish, we see robust crosslingual Dutch\u2192English and English\u2192Dutch structural priming, as well as Polish\u2192English structural priming, in three of these models. However, as discussed previously, crosslingual structural priming avoids the possible confound of lexical overlap. For these results, therefore, a more likely explanation is language contamination. In contemporaneous work, we find that training on fewer than 1M tokens in a second language is sufficient for structural priming effects to emerge (Arnett et al., 2023); our estimates of the amount of language contamination in XGLM 564M, 1.7B, 2.9B, and 7.5B range from 1.77M tokens of Dutch and 1.46M tokens of Polish at the most conservative to 152.5M and 33.4M tokens respectively at the most lenient (see Appendix A).\nThe smaller amount of Polish contamination, as well as the fact that Polish is less closely related to English, may explain the less consistent\nPolish\u2192English structural priming effects and the virtually non-existent English\u2192Polish effects in these models, but as will be discussed in \u00a75.2, there may be other reasons for this latter pattern."
        },
        {
            "heading": "5.2 Null Effects and Asymmetries",
            "text": "More theoretically interesting is the question of why some language models fail to display crosslingual structural priming on some sets of stimuli, even when trained on both languages. For example, in the Loebell and Bock (2003) replications, only XGLM 7.5B shows a significant effect of German\u2192English structural priming, and only XGLM 2.9B and PolyLM 13B show a significant effect of English\u2192German structural priming. This may be due to the grammatical structures used in the stimuli (DO/PO). While the original study does find crosslingual structural priming effects, the effect sizes are small; the authors suggest that this may partly be because \u201cthe prepositional form is used more restrictively in German\u201d (Loebell and Bock, 2003, p. 807).\nWe also see an asymmetry in the crosslingual structural priming effects between some languages. While the effects in the Dutch\u2192English (Bernolet et al., 2013) and Spanish\u2192English (Hartsuiker et al., 2004) studies mostly remain when the direction of the languages is reversed, this is not the case for the Polish\u2192English (Fleischer et al., 2012) and Greek\u2192English (Kotzochampou and Chondrogianni, 2022) results. This may be due to the smaller quantity of training data for Polish and Greek compared to Spanish in XGLM. While XGLM is only trained on slightly more Dutch than Polish, Dutch is also more similar to English in terms of its lexicon and morphosyntax, so it may benefit from more effective crosslingual transfer (Conneau et al., 2020b; Gerz et al., 2018; Guarasci et al., 2022; Winata et al., 2022; Ahuja et al., 2022; Oladipo et al., 2022; Eronen et al., 2023).\nIf it is indeed the case that structural priming effects in language models are weaker when the target language is less trained on, this would contrast with human studies, where crosslingual structural priming appears most reliable when the prime is in participants\u2019 native or primary language (L1) and the target is in their second language (L2). The reverse case often results in smaller effect sizes (Schoonbaert et al., 2007) or effects that are not significant at all (Shin, 2010). Under this account, language models\u2019 dependence on target language train-\ning and humans\u2019 dependence on prime language experience for structural priming would suggest that there are key differences between the models and humans in how grammatical representations function in comprehension and production.\nAn alternative reason for the absence of crosslingual structural priming effects for the English\u2192Polish and English\u2192Greek stimuli is a combination of model features and features of the languages themselves. For example, structural priming effects at the syntactic level may overall be stronger for English targets. English is a language with relatively fixed word order, and thus, competence in English may require a more explicit representation of word order than other languages. In contrast to English, Polish and Greek are morphologically rich languages, where important information is conveyed through morphology (e.g. word inflections), and word orders are less fixed (Tzanidaki, 1995; Siewierska, 1993). Thus, structural priming effects with Polish and Greek targets would manifest as differences in target sentence morphology. However, contemporary language models such as XGLM have a limited ability to deal with morphology. Most state-of-the-art models use WordPiece (Wu et al., 2016) or SentencePiece (Kudo and Richardson, 2018) tokenizers, but other approaches may be necessary for morphologically rich languages (Klein and Tsarfaty, 2020; Park et al., 2021; Soulos et al., 2021; Nzeyimana and Niyongabo Rubungo, 2022; Seker et al., 2022).\nThus, while humans are able to exhibit crosslingual structural priming effects between languages when the equivalent structures do not share the same word orders (Muylle et al., 2020; Ziegler et al., 2019; Hsieh, 2017; Chen et al., 2013), this may not hold for contemporary language models. Specifically, given the aforementioned limitations of contemporary language models, it would be unsurprising that structural priming effects are weaker for morphologically-rich target languages with relatively free word order such as Polish and Greek."
        },
        {
            "heading": "5.3 Implications for Multilingual Models",
            "text": "The results reported here seem to bode well for the crosslingual capacities of multilingual language models. They indicate shared representations of grammatical structure across languages (in line with Chi et al., 2020; Chang et al., 2022; de Varda and Marelli, 2023), and they show that these representations have a causal role in language generation.\nThe results also demonstrate that crosslinguistic transfer can take place at the level of grammatical structures, not just specific phrases, concepts, and individual examples. Crosslinguistic generalizations can extend at least to grammatical abstractions, and thus learning a grammatical structure in one language may aid in the acquisition of its homologue in a second language.\nHow do language models acquire these abstractions? As Contreras Kallens et al. (2023) point out, language models learn grammatical knowledge through exposure. To the degree that similar outcomes for models and humans indicate shared mechanisms, this serves to reinforce claims of usage-based (i.e. functional) accounts of language acquisition (Tomasello, 2003; Goldberg, 2006; Bybee, 2010), which argue that statistical, bottom-up learning may be sufficient to account for abstract grammatical knowledge. Specifically, the results of our study demonstrate the in-principle viability of learning the kinds of linguistic structures that are sensitive to structural priming using the statistics of language alone. Indeed, under certain accounts of language (e.g. Branigan and Pickering, 2017), it is precisely the kinds of grammatical structures that can be primed that are the abstract linguistic representations that we learn when we acquire language. Our results are thus in line with Contreras Kallens et al.\u2019s (2023) argument that it may be possible to use language models as tests for necessity in theories of grammar learning. Taking this further, future work might use different kinds of language models to test what types of priors or biases, if any, are required for any learner to acquire abstract linguistic knowledge.\nIn practical terms, the structural priming paradigm is an innovative way to probe whether a language model has formed an abstract representation of a given structure (Sinclair et al., 2022), both within and across languages. By testing whether a structure primes a homologous structure in another language, we can assess whether the model\u2019s representation for that structure is abstract enough to generalize beyond individual sentences and has a functional role in text generation. As language models are increasingly used in text generation scenarios (Lin et al., 2022) rather than fine-tuning representations (Conneau et al., 2020a), understanding the effects of such representations on text generation is increasingly important. Previous work has compared language models to human studies of\nlanguage comprehension (e.g. Oh and Schuler, 2023; Michaelov et al., 2022; Wilcox et al., 2021; Hollenstein et al., 2021; Kuribayashi et al., 2021; Goodkind and Bicknell, 2018), and while the degree to which the the mechanisms involved in comprehension and production differ in humans is a matter of current debate (Pickering and Garrod, 2007, 2013; Hendriks, 2014; Meyer et al., 2016; Martin et al., 2018), our results show that human studies of language production can also be reproduced in language models used for text generation."
        },
        {
            "heading": "6 Conclusion",
            "text": "Using structural priming, we measure changes in probability for target sentences that do or do not share structure with a prime sentence. Analogously to humans, models predict that a similar target structure is generally more likely than a different one, whether within or across languages. We observe several exceptions, which may reveal features of the languages in question, limitations of the models themselves, or interactions between the two. Based on our results, we argue that multilingual autoregressive Transformer language models display evidence of abstract grammatical knowledge both within and across languages. Our results provide evidence that these shared representations are not only latent in multilingual models\u2019 representation spaces, but also causally impact their outputs.\nLimitations\nTo ensure that the stimuli used for the language models indeed elicit structural priming effects in people, we only use stimuli made available by the authors of previously-published studies on structural priming in humans. Thus, our study analyzes only a subset of possible grammatical alternations and languages. All of our crosslingual structural priming stimuli involve English as one of the languages, and all other languages included are, with the exception of Mandarin, Indo-European languages spoken in Europe. All are also moderately or highly-resourced in the NLP literature (Joshi et al., 2020). Thus, our study is not able to account for the full diversity of human language.\nAdditionally, while psycholinguistic studies often take crosslingual structural priming to indicate shared representations, there are alternate interpretations. Most notably, because structurally similar sentences are more likely to occur in succession than chance, it is possible that increased proba-\nbility for same-structure target sentences reflects likely co-occurrence of distinct, associated representations, rather than a single, common, abstract representation (Ahn and Ferreira, 2023). While this is a much more viable explanation for monolingual than crosslingual priming, the presence of even limited code-switching in training data could in principle lead to similar effects across languages.\nEthics Statement\nOur work complies with the ACL Ethics Policy, and we believe that testing how well language models handle languages other than English is an important avenue of research to reduce the potential harms of language model applications. We did not train any models for this study; instead, we used the pre-trained XGLM (Lin et al., 2022) and PolyLM (Wei et al., 2023) families of models made available through the transformers Python package (Wolf et al., 2020). All analyses were run on an NVIDIA RTX A6000 GPU, running for a total of 4 hours."
        },
        {
            "heading": "Acknowledgements",
            "text": "We would like to thank Sarah Bernolet, Kathryn Bock, Holly P. Branigan, Zhenguang G. Cai, Vasiliki Chondrogianni, Zuzanna Fleischer, Robert J. Hartsuiker, Sotiria Kotzochampou, Helga Loebell, Janet F. McLean, Martin J. Pickering, Sofie Schoonbaert, and Eline Veltkamp for making their experimental stimuli available; and Nikitas Angeletos Chrysaitis, Pamela D. Rivi\u00e8re Ruiz, Stephan Kaufhold, Quirine van Engen, Alexandra Taylor, Robert Slawinski, Felix J. Binder, Johanna Meyer, Tiffany Wu, Fiona Tang, Emily Xu, and Jason Tran for their assistance in preparing them for use in the present study. Models were evaluated using hardware provided by the NVIDIA Corporation as part of an NVIDIA Academic Hardware Grant. Tyler Chang is partially supported by the UCSD HDSI graduate fellowship."
        },
        {
            "heading": "A Language Contamination in Multilingual Language Models",
            "text": "In this section, we estimate language contamination in CC-100-XL, the dataset used to train the XGLM models. While the dataset itself is not made available by Lin et al. (2022), the procedure used for language identification is similar to CC-100 (Conneau et al., 2020a; Wenzek et al., 2020).\nWhile there are some differences in the approaches used for filtering languages to ensure highquality data, both corpora are based on CommonCrawl snapshots and are divided into languages\nusing the fastText language identification model (Joulin et al., 2017). Both CC-100 and CC-100XL also involve a further language identification step. For CC-100, an unnamed internal tool is also used for language identification; for CC-100-XL, an additional step of language identification takes place where text language is also identified at the paragraph level.\nTo test for Dutch and Polish contamination, we sample roughly 100M tokens (based on the XGLM 7.5B tokenizer) of all languages in the replicated CC-100 dataset2 that XLGM 564M, 1.7B, 2.9B, and 7.5B are trained on. We only consider languages that have 100M or more tokens in CC-100 and that either use the Latin alphabet (Spanish, French, Italian, Portuguese, Finnish, Indonesian, Turkish, Vietnamese, Catalan, Estonian, Swahili, Basque), are Slavic (Russian, Bulgarian), or both (English, German). Specifically, we sample from each of these languages until we have enough documents that the number of tokens in each language is at least 100M. Thus, our sample of CC-100 includes roughly 1.6B tokens.\nTo replicate the additional filtering of CC-100XL, we split all documents by paragraph and run language identification on them using the latest version of the fastText language identification model released as part of the \u201cNo Language Left Behind\u201d project (Costa-juss\u00e0 et al., 2022). We set the identification threshold to 0.5, which the authors find to be effective for lower-resource languages (which some of our sampled languages are among). We note that this is a newer and likely more accurate version of the language identification model than that used to create CC-100-XL, and thus it is even less likely to include data from languages other than those intended. We only analyze the data from paragraphs identified to be the same language as the document label.\nTo identify Dutch and Polish in these paragraphs, 2https://data.statmt.org/cc-100/\nwe divide paragraphs into sentences by splitting at each period character, and we run each sentence through both the aforementioned latest version of the fastText language identification model (Costajuss\u00e0 et al., 2022; Joulin et al., 2017) and the cld3 language identifier (Xue et al., 2021) as provided in the gcld3 python package (Al-Rfou, 2020). We use a stricter threshold of 0.9 (as recommended for high-resource languages; Costa-juss\u00e0 et al., 2022) for the former and use the default threshold of 0.7 for the latter.3\nTo estimate the total amount of contamination in each of these languages, we calculate the proportion of each language sample that includes Dutch or Polish. We then multiply this by the number of tokens in each language, which we estimate by multiplying the proportions given in Figure 1 of Lin et al. (2022) by 500B, the total number of tokens. We first provide two estimates of contamination for Dutch and Polish in Table 1: the amount of contamination as identified by the fastText language identification model, and the amount identified by cld3. We also provide a third, more conservative estimate, that only includes the tokens that both language identification models identify as either Dutch or Polish. We note that because we only look at data from 16 of the 30 training languages, these numbers are likely to substantially underestimate the amount of language contamination in the XGLM pre-training data."
        },
        {
            "heading": "B Statistical Tests",
            "text": "We provide the full results of the statistical tests for XGLM 4.5B (Table 2), the PolyLMs (Table 3), and the remaining XGLMs (Table 4).\n3See https://github.com/google/cld3/blob/ master/src/nnet_language_identifier.h and https://github.com/google/cld3/blob/master/src/ nnet_language_identifier.cc."
        }
    ],
    "title": "Structural Priming Demonstrates Abstract Grammatical Representations in Multilingual Language Models",
    "year": 2023
}