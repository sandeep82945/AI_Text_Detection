{
    "abstractText": "In recent years, contrastive learning (CL) has been extensively utilized to recover sentence and document-level encoding capability from pre-trained language models. In this work, we question the length generalizability of CLbased models, i.e., their vulnerability towards length-induced semantic shift. We verify not only that length vulnerability is a significant yet overlooked research gap, but we can devise unsupervised CL methods solely depending on the semantic signal provided by document length. We first derive the theoretical foundations underlying length attacks, showing that elongating a document would intensify the high intra-document similarity that is already brought by CL. Moreover, we found that isotropy promised by CL is highly dependent on the length range of text exposed in training. Inspired by these findings, we introduce a simple yet universal document representation learning framework, LA(SER): length-agnostic self-reference for semantically robust sentence representation learning, achieving state-of-theart unsupervised performance on the standard information retrieval benchmark. Our code is publicly available.",
    "authors": [
        {
            "affiliations": [],
            "name": "Chenghao Xiao"
        },
        {
            "affiliations": [],
            "name": "Yizhi Li"
        },
        {
            "affiliations": [],
            "name": "G Thomas Hudson"
        },
        {
            "affiliations": [],
            "name": "Chenghua Lin"
        },
        {
            "affiliations": [],
            "name": "Noura Al Moubayed"
        }
    ],
    "id": "SP:0b3724bfd9a4c034e11ea8998c60e1a158bfdb0c",
    "references": [
        {
            "authors": [
                "Kaori Abe",
                "Sho Yokoi",
                "Tomoyuki Kajiwara",
                "Kentaro Inui"
            ],
            "title": "Why is sentence similarity benchmark not predictive of application-oriented task performance",
            "venue": "In Proceedings of the 3rd Workshop on Evaluation and Comparison of NLP Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Daniel Cer",
                "Mona Diab",
                "Eneko Agirre",
                "Inigo LopezGazpio",
                "Lucia Specia."
            ],
            "title": "Semeval-2017 task 1: Semantic textual similarity-multilingual and cross-lingual focused evaluation",
            "venue": "arXiv preprint arXiv:1708.00055.",
            "year": 2017
        },
        {
            "authors": [
                "Ting Chen",
                "Simon Kornblith",
                "Mohammad Norouzi",
                "Geoffrey Hinton."
            ],
            "title": "A simple framework for contrastive learning of visual representations",
            "venue": "International conference on machine learning, pages 1597\u20131607. PMLR.",
            "year": 2020
        },
        {
            "authors": [
                "Yung-Sung Chuang",
                "Rumen Dangovski",
                "Hongyin Luo",
                "Yang Zhang",
                "Shiyu Chang",
                "Marin Solja\u010di\u0107",
                "ShangWen Li",
                "Wen-Tau Yih",
                "Yoon Kim",
                "James Glass."
            ],
            "title": "Diffcse: Difference-based contrastive learning for sentence embeddings",
            "venue": "Annual Conference of",
            "year": 2022
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
            "year": 2019
        },
        {
            "authors": [
                "Kawin Ethayarajh."
            ],
            "title": "How contextual are contextualized word representations? Comparing the geometry of BERT, ELMo, and GPT-2 embeddings",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the",
            "year": 2019
        },
        {
            "authors": [
                "Jun Gao",
                "Di He",
                "Xu Tan",
                "Tao Qin",
                "Liwei Wang",
                "Tieyan Liu."
            ],
            "title": "Representation degeneration problem in training natural language generation models",
            "venue": "International Conference on Learning Representations.",
            "year": 2019
        },
        {
            "authors": [
                "Tianyu Gao",
                "Xingcheng Yao",
                "Danqi Chen."
            ],
            "title": "Simcse: Simple contrastive learning of sentence embeddings",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6894\u20136910.",
            "year": 2021
        },
        {
            "authors": [
                "Kaiming He",
                "Haoqi Fan",
                "Yuxin Wu",
                "Saining Xie",
                "Ross Girshick."
            ],
            "title": "Momentum contrast for unsupervised visual representation learning",
            "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 9729\u20139738.",
            "year": 2020
        },
        {
            "authors": [
                "Bohan Li",
                "Hao Zhou",
                "Junxian He",
                "Mingxuan Wang",
                "Yiming Yang",
                "Lei Li."
            ],
            "title": "On the sentence embeddings from pre-trained language models",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
            "year": 2020
        },
        {
            "authors": [
                "Tri Nguyen",
                "Mir Rosenberg",
                "Xia Song",
                "Jianfeng Gao",
                "Saurabh Tiwary",
                "Rangan Majumder",
                "Li Deng."
            ],
            "title": "Ms marco: A human generated machine reading comprehension dataset",
            "venue": "choice, 2640:660.",
            "year": 2016
        },
        {
            "authors": [
                "Aaron van den Oord",
                "Yazhe Li",
                "Oriol Vinyals."
            ],
            "title": "Representation learning with contrastive predictive coding",
            "venue": "arXiv preprint arXiv:1807.03748.",
            "year": 2018
        },
        {
            "authors": [
                "Nils Reimers",
                "Philip Beyer",
                "Iryna Gurevych."
            ],
            "title": "Task-oriented intrinsic evaluation of semantic textual similarity",
            "venue": "Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 87\u201396.",
            "year": 2016
        },
        {
            "authors": [
                "Nils Reimers",
                "Iryna Gurevych."
            ],
            "title": "Sentence-bert: Sentence embeddings using siamese bert-networks",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language",
            "year": 2019
        },
        {
            "authors": [
                "Kaitao Song",
                "Xu Tan",
                "Tao Qin",
                "Jianfeng Lu",
                "Tie-Yan Liu."
            ],
            "title": "Mpnet: masked and permuted pre-training for language understanding",
            "venue": "Proceedings of the 34th International Conference on Neural Information Processing Systems, pages 16857\u201316867.",
            "year": 2020
        },
        {
            "authors": [
                "Karen Sparck Jones."
            ],
            "title": "A statistical interpretation of term specificity and its application in retrieval",
            "venue": "Journal of documentation, 28(1):11\u201321.",
            "year": 1972
        },
        {
            "authors": [
                "Jianlin Su",
                "Jiarun Cao",
                "Weijie Liu",
                "Yangyiwen Ou."
            ],
            "title": "Whitening sentence representations for better semantics and faster retrieval",
            "venue": "arXiv preprint arXiv:2103.15316.",
            "year": 2021
        },
        {
            "authors": [
                "Nandan Thakur",
                "Nils Reimers",
                "Andreas R\u00fcckl\u00e9",
                "Abhishek Srivastava",
                "Iryna Gurevych."
            ],
            "title": "Beir: A heterogeneous benchmark for zero-shot evaluation of information retrieval models",
            "venue": "Thirty-fifth Conference on Neural Information Processing Systems",
            "year": 2021
        },
        {
            "authors": [
                "Kexin Wang",
                "Nils Reimers",
                "Iryna Gurevych."
            ],
            "title": "Tsdae: Using transformer-based sequential denoising auto-encoderfor unsupervised sentence embedding learning",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2021, pages 671\u2013688.",
            "year": 2021
        },
        {
            "authors": [
                "Tongzhou Wang",
                "Phillip Isola."
            ],
            "title": "Understanding contrastive representation learning through alignment and uniformity on the hypersphere",
            "venue": "International Conference on Machine Learning, pages 9929\u20139939. PMLR.",
            "year": 2020
        },
        {
            "authors": [
                "Wenhui Wang",
                "Furu Wei",
                "Li Dong",
                "Hangbo Bao",
                "Nan Yang",
                "Ming Zhou."
            ],
            "title": "Minilm: Deep selfattention distillation for task-agnostic compression of pre-trained transformers",
            "venue": "Advances in Neural Information Processing Systems, 33:5776\u20135788.",
            "year": 2020
        },
        {
            "authors": [
                "Xing Wu",
                "Chaochen Gao",
                "Zijia Lin",
                "Jizhong Han",
                "Zhongyuan Wang",
                "Songlin Hu."
            ],
            "title": "InfoCSE: Information-aggregated contrastive learning of sentence embeddings",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2022, pages",
            "year": 2022
        },
        {
            "authors": [
                "Xing Wu",
                "Chaochen Gao",
                "Liangjun Zang",
                "Jizhong Han",
                "Zhongyuan Wang",
                "Songlin Hu."
            ],
            "title": "ESimCSE: Enhanced sample building method for contrastive learning of unsupervised sentence embedding",
            "venue": "Proceedings of the 29th International Conference",
            "year": 2022
        },
        {
            "authors": [
                "Chenghao Xiao",
                "Yang Long",
                "Noura Al Moubayed."
            ],
            "title": "On isotropy, contextualization and learning dynamics of contrastive-based sentence representation learning",
            "venue": "Findings of the Association for Computational Linguistics: ACL 2023, pages 12266\u2013",
            "year": 2023
        },
        {
            "authors": [
                "Chenghao Xiao",
                "Zihuiwen Ye",
                "G Thomas Hudson",
                "Zhongtian Sun",
                "Phil Blunsom",
                "Noura Al Moubayed"
            ],
            "title": "Can text encoders be deceived by length attack",
            "year": 2023
        },
        {
            "authors": [
                "Yue Yu",
                "Chenyan Xiong",
                "Si Sun",
                "Chao Zhang",
                "Arnold Overwijk"
            ],
            "title": "Coco-dr: Combating distribution shifts in zero-shot dense retrieval with contrastive and distributionally robust learning",
            "year": 2022
        },
        {
            "authors": [
                "Mert Yuksekgonul",
                "Federico Bianchi",
                "Pratyusha Kalluri",
                "Dan Jurafsky",
                "James Zou"
            ],
            "title": "When and why vision-language models behave like bags-of-words, and what to do about it",
            "venue": "In International Conference on Learning Representations",
            "year": 2023
        },
        {
            "authors": [
                "Wang"
            ],
            "title": "2021), and effectiveness of document-level representation encoders should be evaluated beyond this task. The inferior predictability of STS-B on downstream task performances have been attributed to length ranges (Abe",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "In recent years, contrastive learning (CL) has become the go-to method to train representation encoder models (Chen et al., 2020; He et al., 2020; Gao et al., 2021; Su et al., 2022). In the field of natural language processing (NLP), the effectiveness of the proposed unsupervised CL methods is typically evaluated on two suites of tasks, namely, semantic textual similarity (STS) (Cer et al., 2017) and information retrieval (IR) (e.g., Thakur et al. (2021)). Surprisingly, a large number of works only validate the usefulness of the learned representations on STS tasks, indicating a strong but widely-adopted assumption that methods optimal for STS could also provide natural transferability to retrieval tasks.\nDue to the document length misalignment of these two types of tasks, the potential gap in models\u2019 capability to produce meaningful representation at different length ranges has been rarely explored (Xiao et al., 2023b). Studies of document length appear to have been stranded in the era where methods are strongly term frequencybased, because of the explicit reflection of document length to sparse embeddings, with little attention given on dense encoders. Length preference for dense retrieval models is observed by Thakur et al. (2021), who show that models trained with dot-product and cosine similarity exhibit different length preferences. However, this phenomenon has not been attributed to the distributional misalignment of length between training and inference domains/tasks, and it remains unknown what abilities of the model are enhanced and diminished when trained with a certain length range.\nIn this work, we provide an extensive analysis of length generalizability of standard contrastive learning methods. Our findings show that, with default contrastive learning, models\u2019 capability to encode document-level semantics largely comes from their coverage of length range in the training.\nWe first depict through derivation the theoretical underpinnings of the models\u2019 vulnerability towards length attacks. Through attacking the documents by the simple copy-and-concatenating elongation operation, we show that the vulnerability comes from the further intensified high intra-document similarity that is already pronounced after contrastive learning. This hinders a stable attention towards the semantic tokens in inference time. Further, we show that, the uniformity/isotropy promised by contrastive learning is heavily lengthdependent. That is, models\u2019 encoded embeddings are only isotropic on the length range seen in the training, but remain anisotropic otherwise, hindering the same strong expressiveness of the embeddings in the unseen length range.\nIn the quest to bridge these unideal properties, we propose a simple yet universal framework, LA(SER)3: Length-Agnostic SElf-Reference for SEmantically Robust SEntence Representation learning. By providing the simple signal that \"the elongated version of myself 1) should still mean myself, and thus 2) should not become more or less similar to my pairs\", this framework could not only act as an unsupervised contrastive learning method itself by conducting self-referencing, but could also be combined with any contrastive learning-based text encoding training methods in a plug-and-play fashion, providing strong robustness to length attacks and enhanced encoding ability.\nWe show that, our method not only improves contrastive text encoders\u2019 robustness to length attack without sacrificing their representational power, but also provides them with external semantic signals, leading to state-of-the-art unsupervised performance on the standard information retrieval benchmark."
        },
        {
            "heading": "2 Length-based Vulnerability of Contrastive Text Encoders",
            "text": "Length preference of text encoders has been observed in the context of information retrieval (Thakur et al., 2021), showing that contrastive learning-based text encoders trained with dotproduct or cosine similarity display opposite length\npreferences. Xiao et al. (2023b) further devised \"adversarial length attacks\" to text encoders, demonstrating that this vulnerability can easily fool text encoders, making them perceive a higher similarity between a text pair by only copying one of them n times and concatenating to itself.\nIn this section, we first formalize the problem of length attack, and then analyze the most important pattern (misaligned intra-document similarity) that gives rise to this vulnerability, and take an attention mechanism perspective to derive for the first time the reason why contrastive learning-based text encoders can be attacked.\nProblem Formulation: Simple Length Attack Given a sentence S with n tokens {x1, x2, ..., xn}, we artificially construct its elongated version by copying it m times, and concatenating it to itself. For instance, if m = 2, this would give us S\u0303 = {x1, ..., xn, x1, ..., xn}. Loosely speaking, we expect the elongation to be a \"semanticspreserved\" operation, as repeating a sentence m times does not change the semantics of a sentence in most cases. For instance, in the context of information retrieval, repeating a document d by m time should not make it more similar to a query q. In fact, using pure statistical representation such as tf-idf (Sparck Jones, 1972), the original sentence and the elongated version yield exact same representations:\nS\u0303 \u225c f(S,m) (1)\ntf-idf(S) = tf-idf(S\u0303) (2)\nwhere f(\u00b7) denotes the elongation operator, and m is a random integer.\nTherefore, no matter according to the semanticspreserved assumption discussed previously, or reference from statistics-based methods (Sparck Jones, 1972), one would hypothesize Transformer-based models to behave the same. Formally, we expect, given a Transformer-based text encoder g(\u00b7) to map a document into a document embedding, we could also (ideally) get:\ng(S) = g(S\u0303) (3)\nObservation 1: Transformer-based text encoders perceive different semantics in original texts and elongation-attacked texts. The central problem is: given a Transformer-based text encoder g(\u00b7), it is found empirically that:\ng(S) \u0338= g(S\u0303). (4)\nWe verify this phenomenon with Proof of Concept Experiment 1 (Figures 1, 2), showing that Transformer-based encoders perceive different semantics before and after elongation attacks.\nProof of Concept Experiment 1 To validate Observation 1, we fine-tune a vanilla MiniLM (Wang et al., 2020) with the standard infoNCE loss (Oord et al., 2018) with in-batch negatives, on the Quora duplicate question pair dataset (QQP). Notably, the dataset is composed of questions, and thus its length coverage is limited (average token length = 13.9, with 98.5% under 30 tokens).\nWith the fine-tuned model, we first construct two extreme cases: one with a false positive pair (\"what is NLP?\" v.s., \"what is computer vision?\"), one with a positive pair (\"what is natural language processing?\" v.s., \"what is computational linguistics?\"). We compute cosine similarity between mean-pooled embeddings of the original pairs, and between the embeddings attained after conducting an elongation attack with m = 100 (Eq. 1).\nWe found surprisingly that, while \"what is NLP?\" and \"what is computer vision?\" have 0.06 cosine similarity, their attacked versions achieve 0.42 cosine similarity - successfully attacked (cf. Figure 1). And the same between \"what is natural language processing?\" and \"what is computational linguistics?\" goes from 0.50 to 0.63 - similarity pattern augmented.\nOn a larger scale, we then construct an inference set with all the document pairs from Semantic Textual Similarity benchmark (STS-b) (Cer et al., 2017). We conduct an elongation attack on all sentences with m = 100 (Eq. 1). The distributions of document pair cosine similarity are plotted in Figure 2. For the fine-tuned MiniLM (Figure 2, left), it is clearly shown that, the model perceives in general a higher cosine similarity between documents\nafter elongation attacks, greatly increasing the perceived similarity, even for pairs that are not positive pairs. This phenomenon indicates a built-in vulnerability in contrastive text encoders, hindering their robustness for document encoding. For reference, we also plot out the same set of results on the vanilla MiniLM (Figure 2, right), demonstrating an opposite behavior, which will be further discussed in Proof of Concept Experiment 2.\nObservation 2: Intra-document token interactions experience a pattern shift after elongation attacks. Taking an intra-document similarity perspective (Ethayarajh, 2019), we can observe that, tokens in the elongated version of same text, do not interact with one another as they did in the original text (see Proof of Concept Experiment 2). Formally, given tokens in S providing an intra-document similarity of sim, and tokens in the elongated version S\u0303 providing \u02dcsim, we will show that sim \u0338= \u02dcsim. This pattern severely presents in models that have been finetuned with a contrastive loss, while is not pronounced in their corresponding vanilla models (PoC Experiment 2, Figure 3).\nA significant increase on intra-document similarity of contrastive learning-based models is observed by Xiao et al. (2023a), opposite to their vanilla pre-trained checkpoints (Ethayarajh, 2019). It is further observed that, after contrastive learning, semantic tokens (such as topical words) become dominant in deciding the embedding of a sentence, while embeddings of functional tokens (such as stop-words) follow wherever these semantic tokens travel in the embedding space. This was formalized as the \"entourage effect\" (Xiao et al., 2023a). Taking into account this conclusion, we further derive from the perspective of attention mechanism, the reason why conducting elongation attacks would further intensify the observed high intra-document similarity.\nThe attention that any token xi in the sentence S gives to the dominant tokens can be expressed as:\nAttention( xi i\u2208S\n\u2192 xdominant) = eqik\nT dominant/ \u221a dk\u2211\nn eqikTn /\n\u221a dk ,\n(5)\nwhere qi is the query vector produced by xi, kTdominant is the transpose of the key vector produced by xdominant, and kTn is the transpose of the key vector produced by every token xn. We omit the V matrix in the attention formula for simplicity.\nAfter elongating the sentence m times with the copy-and-concat operation, the attention distribution across tokens shifts, taking into consideration that the default prefix [cls] token is not elongated. Therefore, in inference time, [cls] tokens share less attention than in the original sentence.\nTo simplify the following derivations, we further impose the assumption that positional embeddings contribute little to representations, which loosely hold empirically in the context of contrastive learning (Yuksekgonul et al., 2023). In Section 6, we conduct an extra group of experiment to present the validity of this imposed assumption by showing the positional invariance of models after CL.\nWith this in mind, after elongation, the same token in different positions would get the same attention, because they have the same token embedding without positional embeddings added. Therefore:\nAtte\u0303ntion( xi i\u2208S\u0303 \u2192 xdominant)\n= meqik\nT dominant/ \u221a dk\nm \u2211 n eqikTn / \u221a dk \u2212 (m\u2212 1)eqik T [cls] / \u221a dk\n= eqik\nT dominant/ \u221a dk\u2211\nn eqikTn /\n\u221a dk \u2212 m\u22121m e qikT[cls]/ \u221a dk\n(6)\n> Attention( xi i\u2208S \u2192 xdominant)\nBased on Eq. 6, we can see that attentions towards dominant tokens would increase after document elongation attack. However, we can also derive that the same applies to non-dominant tokens:\nAtte\u0303ntion( xi i\u2208S\u0303 \u2192 xnon-dominant)\n> Attention( xi i\u2208S \u2192 xnon-dominant)\nIn fact, every unique token except [cls] would experience an attention gain. Therefore, we have to prove that, the attention gain Gd of dominant tokens (denoted as xd) outweighs the attention gain Gr of non-dominant (regular, denoted as xr) tokens. To this end, we define:\nGd\n\u225c Atte\u0303ntion( xi i\u2208S\u0303 \u2192 xd)\u2212 Attention( xi i\u2208S \u2192 xd)\n(7)\nGr\n\u225c Atte\u0303ntion( xi i\u2208S\u0303 \u2192 xr)\u2212 Attention( xi i\u2208S \u2192 xr)\n(8)\nLet eqik T dominant/ \u221a dk be ld, eqik T non-dominant/ \u221a dk be lr,\neqik T n / \u221a dk be ln, and e qik T [cls] / \u221a dk be a lc, we get:\nGd\n\u225c Atte\u0303ntion( xi i\u2208S\u0303 \u2192 xd)\u2212 Attention( xi i\u2208S \u2192 xd) = ld\u2211\nn ln \u2212 m\u22121m lc \u2212 ld\u2211 n ln = ld m\u22121 m lc\u2211 n ln( \u2211 n ln \u2212 m\u22121m lc)\n(9)\nSimilarly, we get:\nGr = lr m\u22121 m lc\u2211\nn ln( \u2211 n ln \u2212 m\u22121m lc)\n(10)\nAlso note that ld > lr: that\u2019s why they are called \"dominating tokens\" in the first place (Xiao et al., 2023a). Therefore, we prove that Gd > Gr.\nAs a result, with elongation operation, every token is going to assign even more attention to the embeddings of the dominating tokens. And this effect propagates throughout layers, intensifying the high intra-document similarity (\"entourage effect\") found in (Xiao et al., 2023a).\nProof of Concept Experiment 2 With the derivations, we conduct PoC Experiment 2, aiming to demonstrate that intra-document similarity experiences a pattern shift after elongation attack, intensifying the \"entourage effect\", for contrastive fine-tuned models.\nTaking the same fine-tuned MiniLM checkpoint from PoC Experiment 1, we compute the intradocument similarity of all the model outputs on STS-b. For each document, we first compute its document embedding by mean-pooling, then compute the average cosine similarity between each token embedding and the document embedding.1 The results are shown in Figure 3. After elongation attacks, we can see an increase in the already high\n1Notably, we further adjust these scores by the model\u2019s anisotropy estimation (average pair-wise similarity of random sampled tokens), because of the representation degeneration problem (Gao et al., 2019; Ethayarajh, 2019).\nintra-document similarity, meaning that all other tokens converge even further towards the tokens that dominate the document-level semantics.\nWhen using the vanilla MiniLM checkpoint, the intra-document similarity pattern is again reversed. This opposite pattern is well-aligned with the findings of Ethayarajh (2019) and Xiao et al. (2023a): Because in vanilla language models, the intradocument similarity generally becomes lower in the last few layers, while after contrastive learning, models show a drastic increase of intra-document similarity in the last few layers. Also, our derivations conclude that: if the intra-document similarity shows an accumulated increase in the last few layers, this increase will be intensified after elongation; and less affected otherwise.\nComplementing the intensified intra-document similarity, we also display an isotropy misalignment before and after elongation attacks in Figure 4. With the well-known representation degeneration or anisotropy problems in vanilla pre-trained models (Figure 4, right, green, Gao et al. (2019); Ethayarajh (2019)), it has been previously shown that after contrastive learning, a model\u2019s encoded embeddings will be promised with a more isotropic geometry (Figure 4, left, green, Wang and Isola (2020); Gao et al. (2021); Xiao et al. (2023a)). However, in this work, we question this general conclusion by showing that the promised isotropy is strongly length-dependent. After elongation, the\nembeddings produced by the fine-tuned checkpoint start becoming anisotropic (Figure 4, left, pink). This indicates that, if a model has only been trained on short documents with contrastive loss, only the short length range is promised with isotropy.\nOn the other hand, elongation attacks seem to be able to help vanilla pre-trained models to escape from anisotropy, interestingly (Figure 4, right, pink). However, the latter is not the key focus of this work.\n3 Method: LA(SER)3\nAfter examining the two fundamental reasons underlying the built-in vulnerability brought by standard contrastive learning, the formulation of our method emerges as an intuitive outcome. Naturally, we explore the possibility of using only length as the semantic signal to conduct contrastive sentence representation learning, and propose LA(SER)3: Length-Agnostic Self-Reference for Semantically Robust Sentence Representation Learning. LA(SER)3 builds upon the semanticspreserved assumption that \"the elongated version of myself 1) should still mean myself, and thus 2) should not become more or less similar to my pairs\". LA(SER)3 leverages elongation augmentation during the unsupervised constrastive learning to improve 1) the robustness of in-document interaction pattern in inference time; 2) the isotropy of larger length range. We propose two versions of reference methods, for different format availability of sentences in target training sets.\nSelf-reference In LA(SER)3self-ref setting, we take a sentence from the input as an anchor for each training input, and construct its positive pair by elongating the sentence to be m times longer.\nIntra-reference LA(SER)3intra-ref conducts intrareference within the document. The two components of a positive pair are constructed from different spans of the same document. Since we are only to validate effectiveness of LA(SER)3intra-ref, we implement this in the simple mutually-excluded span setting. In other words, the LA(SER)3intra-ref variant takes a sentence (either the first or a random sentence) from the text as an anchor, uses the rest of the text in the input as its positive pair, and elongates the anchor sentence m times as the augmented anchor.\nFor both versions, we use the standard infoNCE loss (Oord et al., 2018) with in-batch negatives as the contrastive loss."
        },
        {
            "heading": "4 Experiments",
            "text": "Training datasets We conduct our experiments on two training dataset settings: 1) trainingwiki uses 1M sentences sampled from Wikipedia, in line with previous works on contrastive sentence representation learning (Gao et al., 2021; Wu et al., 2022a,b); 2) trainingmsmarco uses MSMARCO\n(Nguyen et al., 2016), which is equivalent to indomain-only setting of the BEIR information retrieval benchmark (Thakur et al., 2021).\nEvaluation datasets The trained models are mainly evaluated on the BEIR benchmark (Thakur et al., 2021), which comprises 18 datasets on 9 tasks (fact checking, duplicate question retrieval, argument retrieval, news retrieval, questionanswering, tweet retrieval, bio-medical retrieval and entity retrieval). We evaluate on the 14 public zero-shot datasets from BEIR (BEIR-14). And we use STS-b (Cer et al., 2017) only as the auxiliary experiment.\nThe reasons why we do not follow the de facto practice, which mainly focuses on cherry-picking the best training setting that provides optimal performance on STS-b are as follows: Firstly, performances on STS-b do not display strong correlations with downstream tasks (Reimers et al., 2016). In fact, document-level encoders that provide strong representational abilities do not necessarily provide strong performance on STS-b (Wang et al., 2021). Furthermore, recent works have already attributed the inferior predictive power of STS-b performance on downstream task performances to its narrow length range coverage (Abe et al., 2022). Therefore, we believe a strong sentence and documentlevel representation encoder should be evaluated\nbeyond semantic textual similarity tasks. However, for completeness, we also provide the results of STS-b in Appendix A.\nBaselines We compare our methods in two settings, corresponding to the two versions of LA(SER)3: 1) Self-Reference. Since we assume using the input itself as its positive pair in this setting, it is natural to compare LA(SER)3self-ref to the strong baseline SimCSE (Gao et al., 2021). In the trainingwiki setting, we further compare with ESimCSE, DiffCSE, and InfoCSE (Wu et al., 2022b; Chuang et al., 2022; Wu et al., 2022a). Notably, these four baselines all have public available checkpoints trained on trainingwiki.\n2) Intra-Reference. The baseline method in this case is: taking a sentence (random or first) from a document as anchor, then use the remaining content of the document as its positive pair. Notably, this baseline is similar to the unsupervised pretraining part of COCO-DR (Yu et al., 2022), except COCODR only takes two sentences from a same document, instead of one sentence and the remaining part. Compared to the baseline, LA(SER)3intra-ref further elongates the anchor sentence. In the result table, we refer to baseline of this settings as COCO-DRPT-unsup.\nImplementation Details We evaluate the effectiveness of our method with BERT (Devlin\net al., 2019) and MiniLM2 (Wang et al., 2020). To compare to previous works, we first train a LA(SER)3self-ref on trainingwiki with BERT-base (uncased). We then conduct most of our in-depth experiments with vanilla MiniLM-L6 due to its low computational cost and established state-of-the-art potential after contrastive fine-tuning.3\nAll experiments are run with 1 epoch, a learning rate of 3e-5, a temperature \u03c4 of 0.05, a max sequence length of 256, and a batch size of 64 unless stated otherwise. All experiments are run on Nvidia A100 80G GPUs.\nNotably, previous works on contrastive sentence representation learning (Gao et al., 2021; Wu et al., 2022b; Chuang et al., 2022; Wu et al., 2022a) and even some information retrieval works such as (Yu et al., 2022) mostly use a max sequence length of 32 to 128. In order to study the effect of length, we set the max sequence length to 256, at the cost of constrained batch sizes and a bit of computational overhead. More detailed analyses on max sequence length are in ablation analysis (\u00a75).\nFor the selection of the anchor sentence, we take the first sentence of each document in the main experiment (we will discuss taking a random sen-\n2We use a 6-layer version by taking every second layer. https://huggingface.co/nreimers/MiniLM-L6-H384-uncased\n3For instance, sentence-transformers/all-MiniLM-L6-v2 is a SOTA sentence encoder fine-tuned with MiniLM-L6.\ntence instead of the first sentence in the ablation analysis in \u00a75.1). For LA(SER)3self-ref, we elongate the anchor sentence to serve as its positive pair; for LA(SER)3intra-ref, we take the rest of the document as its positive pair, but then elongate the anchor sentence as the augmented anchor. For the selection of the elongation hyperparameter m, we sample a random number for every input depending on its length and the max length of 256. For instance, if a sentence has 10 tokens excluding [cls], we sample a random integer from [1,25], making sure it is not exceeding maximum length; while for a 50-token sentence, we sample from [1, 5]. We will discuss the effect of elongating to twice longer, instead of random-times longer in ablation \u00a75.2.\nResults The main results are in Tables 1 and 2. Table 1 shows that our method leads to state-ofthe-art average results compared to previous public available methods and checkpoints, when training on the same trainingwiki with BERT.\nOur method has the exact same setting (training a vanilla BERT on the same trainingwiki) with the rest of the baselines except InfoCSE, which further benefits from the training of an auxiliary network. Note that with a batch size of 64, our method already outperforms all the baselines to a large margin except InfoCSE. Since we train with a max sequence length of 256 (all baselines are either 32 or 64), we find that training with a larger batch size (128) further stabilizes our training, achieving state-of-the-art results. Moreover, we achieve stateof-the-art with only a BERTbase.\nIn general, we find that our performance gain is more pronounced when the length range of the dataset is large. On BERT-base experiments, large nDCG@10 performance gain is seen on NFCorpus (doc. avg. length 232.26, SimCSE: 0.1048 -> LA(SER)3: 0.1919), Scifact (doc. avg. length 213.63, SimCSE: 0.2492 -> LA(SER)3: 0.4317), Arguana (doc. avg. length 166.80, SimCSE: 0.2796 -> LA(SER)3: 0.4227). On the other hand, our performance gain is limited when documents are shorter, such DBPedia (avg. length 49.68) and Quora (avg. length 11.44).\nTable 2 further analyzes the effect of datasets and LA(SER)3 variants with MiniLM-L6, showing a consistent improvement when used as a plug-andplay module to previous SOTA methods.\nWe also found that, even though MiniLM-L6 shows great representational power if after supervised contrastive learning with high-quality doc-\nument pairs (see popular Sentence Transformers checkpoint all-MiniLM-L6-v2), its performance largely falls short under unsupervised training settings, which we speculate to be due to that the linguistic knowledge has been more unstable after every second layer of the model is taken (from 12 layers in MiniLM-L12 to 6 layers). Under such setting, LA(SER)3intra-ref largely outperforms LA(SER)3self-ref, by providing signals of more lexical differences in document pairs."
        },
        {
            "heading": "5 Ablation Analysis",
            "text": "In this section, we ablate two important configurations of LA(SER)3. Firstly, the usage of LA(SER)3 involves deciding which sentence in the document to use as the anchor (\u00a7 5.1). Secondly, how do we maximize the utility of self-referential elongation? Is it more important for the model to know \"me * m = me\", or is it more important to cover a wider length range (\u00a7 5.2)?"
        },
        {
            "heading": "5.1 Selecting the Anchor: first or random?",
            "text": "If a document consists of more than one sentence, LA(SER)3 requires deciding which sentence in the document to use as the anchor. We ablate this with both LA(SER)3self-ref and LA(SER)3intra-ref on trainingmsmarco, because trainingwiki consists of mostly one-sentence inputs and thus is not able to do intra-ref or random sentence.\nThe results are in Table 3. In general, we observe that taking a random sentence as anchor brings certain noise. This is most corroborated by the performance drop of LA(SER)3self-ref + random sentence, compared to its SimCSE baseline. However, LA(SER)3intra-sim + random sentence seems to be able to act robustly against this noise.\nWe hypothesize that as LA(SER)3 provides augmented semantic signals to contrastive learning, it would be hurt by overly noisy in-batch inputs. By contrast, LA(SER)3intra-sim behaves robustly to this noise because the rest of the document apart from the anchor could serve as a stabilizer to the noise."
        },
        {
            "heading": "5.2 Importance of Self-referential Elongation",
            "text": "With the validated performance gain produced by the framework, we decompose the inner-workings by looking at the most important component, elongation. A natural question is: is the performance gain only brought by coverage of larger trained length range? Or does it mostly rely on the semantic signal that, \"my-longer-self\" still means myself?\nElongation Mode Max Seq. Length Zero-shot Average\nTable 4 shows that, elongating to random-times longer outperforms elongating to a fixed two-times longer. We hypothesize that, a fixed augmentation introduces certain overfitting, preventing the models to extrapolate the semantic signal that \"elongated me = me\". On the other hand, as long as they learn to extrapolate this signal (by * random times), increasing max sequence length provides decreasing marginal benefits."
        },
        {
            "heading": "6 Auxiliary Property Analysis",
            "text": ""
        },
        {
            "heading": "6.1 Positional Invariance",
            "text": "Recalling in Observation 2 and PoC experiment 2, we focused on analyzing the effect of elongation attack on intra-sentence similarity, which is already high after CL (Xiao et al., 2023a). Therefore, we have imposed the absence of positional embeddings with the aim to simplify the derivation in proving that, with elongation, dominant tokens receive higher attention gains than regular tokens. Here, we present the validity of this assumption by showing models\u2019 greatly reduced sensitivity towards positions after contrastive learning.\nWe analyze the positional (in)sensitivity of 4 models (MiniLM (Wang et al., 2020) and mpnet (Song et al., 2020) respectively before and\nafter contrastive learning on Sentence Embedding Training Data4). Models after contrastive learning are Sentence Transformers (Reimers and Gurevych, 2019) models all-mpnet-base-v2 and all-MiniLM-L12-v2.\nWe take the sentence pairs from STS-b test set as the inference set, and compute each model\u2019s perceived cosine similarity on the sentence pairs (distribution 1). We then randomly shuffle the word orders of all sentence 1s in the sentence pairs, and compute each model\u2019s perceived cosine similarity with sentence 2s again (distribution 2).\nThe divergence of the two distributions for each model can serve as a proxy indicator of the model\u2019s sensitivity towards word order, and thus towards positional shift. The lower the divergence, the more insensitive that a model is about positions.\nWe find that the Jenson Shannon divergence yielded by MiniLM has gone from 0.766 (vanilla) to 0.258 (after contrastive learning). And the same for mpnet goes from 0.819 (vanilla) to 0.302 (after contrastive learning). This finding shows that contrastive learning has largely removed the contribution of positions towards document embeddings, even in the most extreme case (with random shuffled word orders). This has made contrastivelylearned models acting more like bag-of-words models, aligning with what was previously found in vision-language models (Yuksekgonul et al., 2023).\nMoreover, MiniLM uses absolute positional embeddings while mpnet further applies relative positional embeddings. We believe that the positional insensitivity pattern holds for both models can partly make the pattern and LA(SER)3\u2019s utility more universal, especially when document encoders are trained with backbone models that have different positional encoding methods."
        },
        {
            "heading": "7 Conclusion",
            "text": "In this work, we questioned the length generalizability of contrastive learning-based text encoders. We observed that, despite their seemingly strong representational power, this ability is strongly vulnerable to length-induced semantic shifts. we formalized length attack, demystified it, and defended against it with LA(SER)3. We found that, teaching the models \"my longer-self = myself\" provides a standalone semantic signal for more robust and powerful unsupervised representation learning.\n4https://huggingface.co/datasets/ sentence-transformers/embedding-training-data\nLimitations\nWe position that the focus of our work lies more in analyzing theoretical properties and innerworkings, and thus mostly focus on unsupervised contrastive learning settings due to compute constraints. However, we believe that with a better unsupervised checkpoint, further supervised finetuning will yield better results with robust patterns. We leave this line of exploration for future work. Further, we only focus on bi-encoder settings. In information retrieval, there are other methods involving using cross-encoders to conduct re-ranking, and sparse retrieval methods. Though we envision our method can be used as a plug-and-play module to many of these methods, it is hard to exhaust testing with every method. We thus experiment the plug-and-play setting with a few representative methods. We hope that future works could evaluate the effectiveness of our method combining with other lines of baseline methods such as cross-encoder re-ranking methods."
        },
        {
            "heading": "A Results of STS-b",
            "text": "In this section, we present the results of STS-b test set (Table 5). As discussed in the main sections, we position that STS-b is not correlated with downstream semantic tasks performance (Reimers et al., 2016; Wang et al., 2021), and effectiveness of document-level representation encoders should be evaluated beyond this task. The inferior predictability of STS-B on downstream task performances have been attributed to length ranges (Abe et al., 2022). We hypothesize that, training with a large max sequence length increases the uncertainty of elongation hyperparameter m of LA(SER)3, resulting in a diverse length range, and less corresponding concrete examples at each length.\nWe show that, while out-performing SimCSE by a large margin on other downstream semantic tasks (Main Section, Table 1), our long sequence length poses a certain level of instability in converging, showing a small performance drop on shorter sentences (STS-b). The converging instability is further confirmed by training an extra LA(SER)3 with [cls]-pooling, as [cls]-pooling is faster in converging - as it involves only optimizing one token. Notably, SimCSE also uses [cls]-pooling. Therefore, we roughly stay on-par with SimCSE on encoding shorter documents, while out-performing it by a large margin on other downstream tasks."
        }
    ],
    "title": "Length is a Curse and a Blessing for Document-level Semantics",
    "year": 2023
}