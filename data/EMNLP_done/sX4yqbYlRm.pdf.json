{
    "abstractText": "The vital role of analogical reasoning in human cognition allows us to grasp novel concepts by linking them with familiar ones through shared relational structures. Despite the attention previous research has given to word analogies, this work suggests that Large Language Models (LLMs) often overlook the structures that underpin these analogies, raising questions about the efficacy of word analogies as a measure of analogical reasoning skills akin to human cognition. In response to this, our paper introduces a task of analogical structure abduction, grounded in cognitive psychology, designed to abduce structures that form an analogy between two systems. In support of this task, we establish a benchmark called SCAR, containing 400 scientific analogies from 13 distinct fields, tailored for evaluating analogical reasoning with structure abduction. The empirical evidence underlines the continued challenges faced by LLMs, including ChatGPT and GPT-4, in mastering this task, signifying the need for future exploration to enhance their abilities.1",
    "authors": [
        {
            "affiliations": [],
            "name": "Siyu Yuan"
        },
        {
            "affiliations": [],
            "name": "Jiangjie Chen"
        },
        {
            "affiliations": [],
            "name": "Xuyang Ge"
        },
        {
            "affiliations": [],
            "name": "Yanghua Xiao"
        },
        {
            "affiliations": [],
            "name": "Deqing Yang"
        }
    ],
    "id": "SP:069da1180184bb0816cbb5c8c38cae30c7cf89c2",
    "references": [
        {
            "authors": [
                "Paul Bartha"
            ],
            "title": "Analogy and analogical reasoning",
            "year": 2013
        },
        {
            "authors": [
                "Bhavya Bhavya",
                "Jinjun Xiong",
                "ChengXiang Zhai."
            ],
            "title": "Analogy generation by prompting large language models: A case study of instructgpt",
            "venue": "Proceedings of the 15th International Conference on Natural Language Generation, pages 298\u2013312, Water-",
            "year": 2022
        },
        {
            "authors": [
                "Adrian Boteanu",
                "Sonia Chernova."
            ],
            "title": "Solving and explaining analogy questions using semantic networks",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, 29(1).",
            "year": 2015
        },
        {
            "authors": [
                "Mario Bunge."
            ],
            "title": "Analogy between systems",
            "venue": "International Journal Of General System, 7(4):221\u2013223.",
            "year": 1981
        },
        {
            "authors": [
                "Jiangjie Chen",
                "Rui Xu",
                "Ziquan Fu",
                "Wei Shi",
                "Zhongqiao Li",
                "Xinbo Zhang",
                "Changzhi Sun",
                "Lei Li",
                "Yanghua Xiao",
                "Hao Zhou."
            ],
            "title": "E-KAR: A benchmark for rationalizing natural language analogical reasoning",
            "venue": "Findings of the Association for Computa-",
            "year": 2022
        },
        {
            "authors": [
                "Stoica",
                "Eric P. Xing"
            ],
            "title": "Vicuna: An opensource chatbot impressing gpt-4 with 90%* chatgpt quality",
            "year": 2023
        },
        {
            "authors": [
                "Tamara Czinczoll",
                "Helen Yannakoudakis",
                "Pushkar Mishra",
                "Ekaterina Shutova."
            ],
            "title": "Scientific and creative analogies in pretrained language models",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2022, pages 2094\u20132100, Abu",
            "year": 2022
        },
        {
            "authors": [
                "Zijian Ding",
                "Arvind Srinivasan",
                "Stephen MacNeil",
                "Joel Chan."
            ],
            "title": "Fluid transformers and creative analogies: Exploring large language models\u2019 capacity for augmenting cross-domain analogical creativity",
            "venue": "arXiv preprint arXiv:2302.12832.",
            "year": 2023
        },
        {
            "authors": [
                "Joseph L Fleiss",
                "Bruce Levin",
                "Myunghee Cho Paik"
            ],
            "title": "The measurement of interrater agreement. Statistical methods for rates and proportions, 2(212236):22\u201323",
            "year": 1981
        },
        {
            "authors": [
                "Louis Fournier",
                "Emmanuel Dupoux",
                "Ewan Dunbar."
            ],
            "title": "Analogies minus analogy test: measuring regularities in word embeddings",
            "venue": "Proceedings of the 24th Conference on Computational Natural Language Learning, pages 365\u2013375, Online. Association",
            "year": 2020
        },
        {
            "authors": [
                "Tianyu Gao",
                "Xingcheng Yao",
                "Danqi Chen."
            ],
            "title": "SimCSE: Simple contrastive learning of sentence embeddings",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6894\u20136910, Online and Punta Cana, Do-",
            "year": 2021
        },
        {
            "authors": [
                "Dedre Gentner."
            ],
            "title": "Structure-mapping: A theoretical framework for analogy",
            "venue": "Cognitive science, 7(2):155\u2013 170.",
            "year": 1983
        },
        {
            "authors": [
                "Dedre Gentner",
                "Francisco Maravilla."
            ],
            "title": "Analogical reasoning",
            "venue": "The Routledge International Handbook of Thinking and Reasoning, pages 186\u2013 203. Routledge.",
            "year": 2017
        },
        {
            "authors": [
                "Dedre Gentner",
                "Arthur B Markman."
            ],
            "title": "Structure mapping in analogy and similarity",
            "venue": "American psychologist, 52(1):45.",
            "year": 1997
        },
        {
            "authors": [
                "Anna Gladkova",
                "Aleksandr Drozd",
                "Satoshi Matsuoka."
            ],
            "title": "Analogy-based detection of morphological and semantic relations with word embeddings: what works and what doesn\u2019t",
            "venue": "Proceedings of the NAACL Student Research Workshop, pages 8\u201315, San",
            "year": 2016
        },
        {
            "authors": [
                "Douglas R Hofstadter."
            ],
            "title": "Analogy as the core of cognition",
            "venue": "The analogical mind: Perspectives from cognitive science, pages 499\u2013538.",
            "year": 2001
        },
        {
            "authors": [
                "Douglas R Hofstadter",
                "Emmanuel Sander."
            ],
            "title": "Surfaces and essences: Analogy as the fuel and fire of thinking",
            "venue": "Basic books.",
            "year": 2013
        },
        {
            "authors": [
                "Xiaoyang Hu",
                "Shane Storks",
                "Richard Lewis",
                "Joyce Chai."
            ],
            "title": "In-context analogical reasoning with pre-trained language models",
            "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages",
            "year": 2023
        },
        {
            "authors": [
                "Takeshi Kojima",
                "Shixiang Shane Gu",
                "Machel Reid",
                "Yutaka Matsuo",
                "Yusuke Iwasawa."
            ],
            "title": "Large language models are zero-shot reasoners",
            "venue": "ICML 2022 Workshop on Knowledge Retrieval and Language Models.",
            "year": 2022
        },
        {
            "authors": [
                "Harold W Kuhn."
            ],
            "title": "The hungarian method for the assignment problem",
            "venue": "Naval research logistics quarterly, 2(1-2):83\u201397.",
            "year": 1955
        },
        {
            "authors": [
                "Peng-Hsuan Li",
                "Tsan-Yu Yang",
                "Wei-Yun Ma."
            ],
            "title": "CA-EHN: Commonsense analogy from E-HowNet",
            "venue": "Proceedings of the Twelfth Language Resources and Evaluation Conference, pages 2984\u20132990, Marseille, France. European Language Resources Asso-",
            "year": 2020
        },
        {
            "authors": [
                "Shen Li",
                "Zhe Zhao",
                "Renfen Hu",
                "Wensi Li",
                "Tao Liu",
                "Xiaoyong Du."
            ],
            "title": "Analogical reasoning on Chinese morphological and semantic relations",
            "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2:",
            "year": 2018
        },
        {
            "authors": [
                "Xingxuan Li",
                "Yutong Li",
                "Linlin Liu",
                "Lidong Bing",
                "Shafiq Joty."
            ],
            "title": "Is gpt-3 a psychopath? evaluating large language models from a psychological perspective",
            "venue": "arXiv preprint arXiv:2212.10529.",
            "year": 2022
        },
        {
            "authors": [
                "Tomas Mikolov",
                "Kai Chen",
                "Greg Corrado",
                "Jeffrey Dean."
            ],
            "title": "Efficient estimation of word representations in vector space",
            "venue": "arXiv preprint arXiv:1301.3781.",
            "year": 2013
        },
        {
            "authors": [
                "Tomas Mikolov",
                "Wen-tau Yih",
                "Geoffrey Zweig."
            ],
            "title": "Linguistic regularities in continuous space word representations",
            "venue": "Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human",
            "year": 2013
        },
        {
            "authors": [
                "Melanie Mitchell."
            ],
            "title": "Abstraction and analogymaking in artificial intelligence",
            "venue": "Annals of the New York Academy of Sciences, 1505(1):79\u2013101.",
            "year": 2021
        },
        {
            "authors": [
                "Paul Christiano",
                "Jan Leike",
                "Ryan Lowe"
            ],
            "title": "Training language models to follow instructions with",
            "year": 2022
        },
        {
            "authors": [
                "Peter S Park",
                "Philipp Schoenegger",
                "Chongyang Zhu."
            ],
            "title": "Artificial intelligence in psychology research",
            "venue": "arXiv preprint arXiv:2302.07267.",
            "year": 2023
        },
        {
            "authors": [
                "Jeffrey Pennington",
                "Richard Socher",
                "Christopher Manning."
            ],
            "title": "GloVe: Global vectors for word representation",
            "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1532\u20131543, Doha, Qatar.",
            "year": 2014
        },
        {
            "authors": [
                "Haocong Rao",
                "Cyril Leung",
                "Chunyan Miao."
            ],
            "title": "Can chatgpt assess human personalities? a general evaluation framework",
            "venue": "arXiv preprint arXiv:2303.01248.",
            "year": 2023
        },
        {
            "authors": [
                "Nils Reimers",
                "Iryna Gurevych."
            ],
            "title": "SentenceBERT: Sentence embeddings using Siamese BERTnetworks",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natu-",
            "year": 2019
        },
        {
            "authors": [
                "Oren Sultan",
                "Dafna Shahaf."
            ],
            "title": "Life is a circus and we are the clowns: Automatically finding analogies between situations and processes",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 3547\u20133562,",
            "year": 2022
        },
        {
            "authors": [
                "Rohan Taori",
                "Ishaan Gulrajani",
                "Tianyi Zhang",
                "Yann Dubois",
                "Xuechen Li",
                "Carlos Guestrin",
                "Percy Liang",
                "Tatsunori B. Hashimoto."
            ],
            "title": "Stanford alpaca: An instruction-following llama model",
            "venue": "https:// github.com/tatsu-lab/stanford_alpaca.",
            "year": 2023
        },
        {
            "authors": [
                "Hugo Touvron",
                "Thibaut Lavril",
                "Gautier Izacard",
                "Xavier Martinet",
                "Marie-Anne Lachaux",
                "Timoth\u00e9e Lacroix",
                "Baptiste Rozi\u00e8re",
                "Naman Goyal",
                "Eric Hambro",
                "Faisal Azhar"
            ],
            "title": "Llama: Open and efficient foundation language models",
            "year": 2023
        },
        {
            "authors": [
                "Peter D Turney."
            ],
            "title": "The latent relation mapping engine: Algorithm and experiments",
            "venue": "Journal of Artificial Intelligence Research, 33:615\u2013655.",
            "year": 2008
        },
        {
            "authors": [
                "Peter D Turney",
                "Michael L Littman",
                "Jeffrey Bigham",
                "Victor Shnayder."
            ],
            "title": "Combining independent modules in lexical multiple-choice problems",
            "venue": "Recent Advances in Natural Language Processing III: Selected Papers from RANLP, 2003:101\u2013110.",
            "year": 2003
        },
        {
            "authors": [
                "Asahi Ushio",
                "Luis Espinosa Anke",
                "Steven Schockaert",
                "Jose Camacho-Collados"
            ],
            "title": "BERT is to NLP what AlexNet is to CV: Can pre-trained language models identify analogies",
            "venue": "In Proceedings of the 59th Annual Meeting of the Association for Compu-",
            "year": 2021
        },
        {
            "authors": [
                "Denny Vrande\u010di\u0107",
                "Markus Kr\u00f6tzsch."
            ],
            "title": "Wikidata: a free collaborative knowledgebase",
            "venue": "Communications of the ACM, 57(10):78\u201385.",
            "year": 2014
        },
        {
            "authors": [
                "Taylor Webb",
                "Keith J Holyoak",
                "Hongjing Lu."
            ],
            "title": "Emergent analogical reasoning in large language models",
            "venue": "arXiv preprint arXiv:2212.09196.",
            "year": 2022
        },
        {
            "authors": [
                "Jason Wei",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Maarten Bosma",
                "brian ichter",
                "Fei Xia",
                "Ed H. Chi",
                "Quoc V Le",
                "Denny Zhou"
            ],
            "title": "2022. Chain of thought prompting elicits reasoning in large language models",
            "venue": "In Advances in Neural Information Processing Systems",
            "year": 2022
        },
        {
            "authors": [
                "Siyu Yuan",
                "Jiangjie Chen",
                "Changzhi Sun",
                "Jiaqing Liang",
                "Yanghua Xiao",
                "Deqing Yang."
            ],
            "title": "Analogykb: Unlocking analogical reasoning of language models with a million-scale knowledge base",
            "venue": "arXiv preprint arXiv:2305.05994.",
            "year": 2023
        },
        {
            "authors": [
                "Ningyu Zhang",
                "Lei Li",
                "Xiang Chen",
                "Xiaozhuan Liang",
                "Shumin Deng",
                "Huajun Chen."
            ],
            "title": "Multimodal analogical reasoning over knowledge graphs",
            "venue": "The Eleventh International Conference on Learning Representations.",
            "year": 2023
        },
        {
            "authors": [
                "Zheng Zhang",
                "Scott Schwartz",
                "Lukas Wagner",
                "Webb Miller."
            ],
            "title": "A greedy algorithm for aligning dna sequences",
            "venue": "Journal of Computational biology, 7(12):203\u2013214.",
            "year": 2000
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Analogical reasoning is one of the foundations of human cognition, which helps humans understand complex and unfamiliar concepts by relating them to familiar ones (Gentner, 1983; Hofstadter, 2001; Hofstadter and Sander, 2013). In cognitive psychology, theories like the Structure Mapping Theory (SMT) have been proposed to explain the underlying mechanisms behind analogical reasoning (Gentner and Markman, 1997). According to SMT, individuals gain new knowledge by establishing mapping relations between familiar systems to unfamiliar systems (system analogy) (Bunge, 1981). As an example in Figure 1, an engineer can learn the eye cross-section by taking the analogy\n\u2217Corresponding authors. 1Resources of this paper can be found at https://github.\ncom/siyuyuan/scar.\nof the camera structure since both of them exhibit common relational structures.\nIn this paper, we aim to evaluate the analogical reasoning ability of language models (LMs) aligning with humans. In this regard, previous work on analogical reasoning mainly focuses on word analogy (e.g., \u201cking is to man as queen is to woman\u201d), which does not evaluate if LMs reason about the analogy between two systems in a manner akin to humans (Turney et al., 2003; Mikolov et al., 2013b; Boteanu and Chernova, 2015; Gladkova et al., 2016; Chen et al., 2022). There has been a paradigm shift in the study of analogies, moving from examining word analogies between phrases to exploring analogies between processes (Turney, 2008; Sultan and Shahaf, 2022), e.g., the process of hurricanes can be analogous to the process of volcano eruptions. However, these researches remain limited to the situations within the same domains, leaving cross-domain exploration uncharted, and lack benchmarks. Large language models (LLMs, e.g., Brown et al., 2020; Ouyang et al., 2022; OpenAI, 2022, 2023), despite their great abilities in\nmany tasks including analogy generation (Bhavya et al., 2022; Webb et al., 2022; Yuan et al., 2023), the evaluation is limited to simple word analogies. Little investigation has been done on system analogies to align with human cognition.\nIn this paper, we begin by evaluating and analyzing the analogical reasoning ability of LLMs on the word analogy test. Although LLMs, such as GPT-4 (OpenAI, 2023), exhibit exceptional performance in word analogy recognition, they often fail at abducing the correct structures when solving word analogies. To improve the evaluation and benchmarking of analogical reasoning for better alignment with human cognitive processes, we draw inspiration from SMT and propose an analogical structure abduction task. This task aims to construct the mappings between concepts in two systems based on the relational structure abduction to establish a system analogy.\nFor this purpose, we introduce a benchmark of SCientific Analogical Reasoning with structure abduction, i.e., SCAR, consisting of 400 system analogies across 13 domains with 1600 concept mappings, enriched with background knowledge from Wikipedia and GPT-4 generated explanations. Our experiments reveal that LLMs struggle in this task, but can be improved by incorporating background knowledge and explanations in a chain-ofthought (CoT) (Wei et al., 2022) manner.\nOur contributions are summarized as follows:\n\u2022 We demonstrate that word analogies do not adequately reflect the analogical reasoning ability of LMs to align with human cognition;\n\u2022 We propose the analogical structure abduction task to evaluate LLMs from a cognitive perspective to align with humans;\n\u2022 We develop a benchmark of scientific analogical reasoning with structure abduction, i.e., SCAR, and introduce a CoT prompting method to enhance model performance on this task."
        },
        {
            "heading": "2 Analogical reasoning for LLMs",
            "text": ""
        },
        {
            "heading": "2.1 A Cognitive Perspective for Analogical Reasoning",
            "text": "We first introduce the cognitive foundations of analogical reasoning through the lens of Structure Mapping Theory (SMT), a psychological framework proposed by Gentner and Markman (1997)."
        },
        {
            "heading": "I: Word Analogy Test",
            "text": ""
        },
        {
            "heading": "II: Relational Structure Identification (RSI)",
            "text": "SMT suggests that analogy is achieved by identifying common relational structures between two systems (i.e., system analogy) (Bunge, 1981; Gentner, 1983). Key components of SMT include\n1. Representation: Structured systems with concepts and relations; 2. Mapping: Comparisons between two representations for commonalities, resulting in structure abduction between two systems; 3. Evaluation: Analogies are evaluated based on the abduced structures between the two representations.\nAn example of SMT is illustrated in Figure 1. The two systems, i.e., camera and eye, can be represented into five concepts. Based on the relational structure, aperture should be mapped to pupil since both are channels for light to enter. Adhering to the one-to-one mapping, this process focuses on structure abduction to foster comprehension across both domains (Bartha, 2013).\n2.2 How are LLMs on word analogy test?\nPrevious work adopts word analogy tests (i.e., A is to B as C is to D) to evaluate the analogical reasoning ability of LMs. As illustrated in Table 1 (I), this task can be framed as a multiple-choice question-answering (QA) challenge. We adhere to this paradigm and test the performance of LLMs, e.g., InstructGPT series (Ouyang et al., 2022), ChatGPT (OpenAI, 2022) and GPT-4 (OpenAI, 2023), on this test. We also adopt InstructGPT embeddings (text-ada-embedding-002) and follow the method proposed by Ushio et al. (2021), which converts word analogies into embeddings and select\nthe answer candidate with the marginal likelihood biased perplexity.\nWe use six benchmarks and design instructions for LLMs to complete the tests. SAT (Turney et al., 2003), UNIT2 (Boteanu and Chernova, 2015) and UNIT4 (Boteanu and Chernova, 2015) come from educational resources. Google (Mikolov et al., 2013b) and BATS (Gladkova et al., 2016) are derived for word embedding evaluation with semantic and morphological relations. E-KAR (Chen et al., 2022) is from China\u2019s Civil Service Examinations with more complex relations and structures.2\nThe results in Table 2 show that 1) InstructGPT embeddings perform poorly on the word analogy test; 2) GPT-4 achieves human-level performance and providing examples improves model performance; 3) Despite GPT-4 exceeding human performance on most word analogy benchmarks, it lags considerably behind humans on E-KAR.\nHowever, analogical reasoning relies on identifying shared relational structures between two systems, but the word analogy test does not explicitly evaluate structural abduction. Thus, the word analogy test may not reflect model performance in analogical reasoning aligned with humans.\n2.3 Is the word analogy test aligned with humans?\nTo confirm our hypothesis, we explore the discerning relations between word analogies for LLMs. We analyze word analogies in E-KAR to explore whether LLMs can establish structural relations.\nAs shown in Table 1 (II), we define a relational structure identification (RSI) test where\n2Details on the benchmarks and prompt templates are provided in Appendix B.\nLLMs select the relation constituting the analogy from four options. We construct a benchmark using 700 E-KAR test data, where annotators identify the correct relation of the word analogy. For distractors, annotators write the relation opposite to the golden relation (relational opposite distractor) and the semantic opposite relation (semantic opposite distractor). Besides, we convert golden and Wikidata relations (Vrandec\u030cic\u0301 and Kr\u00f6tzsch, 2014) into InstructGPT embeddings (text-ada-embedding-002) and calculate cosine similarity. Then, annotators select an incorrect relation with the closest semantics as a distractor (similar distractor). Two annotators annotate each data at first, and then we hire a third annotator to select a better one as the distractor. For human performance, we test two undergraduates on the RSI test with their results averaged.\nWe evaluate LLMs on the RSI task and calculate the Accuracy and Overlap. Overlap is a metric that represents the ratio of data samples that are correctly identified in both tasks to the total number of samples correctly identified in at least one of the tasks. A higher overlap suggests LMs tend to understand word analogies based on structure.\nTable 2 shows superior model performance in the RSI test. However, the low overlap reveals LLMs doing well in the RSI test may not necessarily succeed in the word analogy test. According to SMT, analogical reasoning is based on identifying common relational structures between two systems. Such discrepancy indicates that the word analogy test is not aligned with humans, and we need a new way of evaluating and benchmarking analogical reasoning to align with humans."
        },
        {
            "heading": "3 SCAR: Scientific Analogical Reasoning with Structure Abduction",
            "text": ""
        },
        {
            "heading": "3.1 Schema for Analogies in SCAR",
            "text": "In this paper, we aim to explore the analogical reasoning ability of LLMs to align with humans. Inspired by SMT, we focus on the structure abduction of system analogy (Bunge, 1981). As shown in Figure 1, two systems are analogous based on their common relational structure. To construct the system analogy, concepts in System A (e.g., Camera) can be mapped into corresponding concepts in System B (e.g., Eye), forming multiple one-to-one concept mappings (e.g., Aperture maps to Pupil). This process facilitates analogical reasoning and enables a deeper understanding of both systems."
        },
        {
            "heading": "3.2 Data Collection",
            "text": "System Analogy Selection Given that analogical reasoning is usually used in scientific problemsolving, we construct a benchmark of SCientific Analogical Reasoning with structure abduction, i.e., SCAR. We recruit a team of five undergraduate students with different academic backgrounds to serve as annotators for this benchmark.\nThe annotators are provided with guidelines of SMT to learn about identifying potential analogies based on the relational structures of systems. To assist our annotators, we furnish them with scientific analogies sourced through online research.3 These resources contain various scientific analogies with detailed information and thus can prompt annotators to create system analogies with concept mappings. Overall, annotators manually curate 400 system analogies and define mappings between concepts based on their domain-specific expertise. We also ask the annotators to mark the domains in each system analogy. Then, we remove duplicates to collate the benchmark and conduct a review process to verify the correctness and plausibility of the analogies in the benchmark.\nBackground Knowledge Retrieval We incorporate background knowledge into each system to facilitate the understanding of LMs and streamline the mapping process. To achieve this, we first extract the encyclopedia abstracts from Wikipedia 4 for each system. Considering that abstracts may not include all relevant concepts and could be too lengthy, we use ChatGPT (OpenAI, 2022)\n3The online resources are shown in Appendix C.1. 4https://www.wikipedia.org/"
        },
        {
            "heading": "Statistic Number",
            "text": "to rewrite each abstract as the background. We prompt ChatGPT with human-written instructions to ensure that each revised background is limited to 500 words and encompasses all concepts in each system.5\nExplanation Generation As shown in Figure 1, Film maps Retina since both capture light and translate it into recognizable information. To rationalize analogical reasoning, we design prompts for GPT4 to generate explanations for each concept mapping.6 To ensure the quality of explanations, we employ two annotators to evaluate the accuracy of each explanation in SCAR. The annotation results indicate that 69.35% of the concept mapping explanations are accurate, with Fleiss\u2019s \u03ba = 0.93 (Fleiss et al., 1981). Then we ask the annotators who created the dataset to revise the wrong explanations (495 in total) with their expertise, thereby guaranteeing the quality of the explanations.\nBilinguality: English and Chinese To broaden the scope of this work, we also develop a Chinese version of SCAR through translation. We employ three native Chinese-speaking annotators to refine the machine translation provided by Google. Finally, we have a bilingual SCAR benchmark.7"
        },
        {
            "heading": "3.3 SCAR Analysis",
            "text": "Table 3 shows the main statistics of SCAR. SCAR contains a total of 400 system analogies, with 632 systems and 1,614 concept mappings, indicating a\n5The instruction template to revise backgrounds is shown in Appendix C.3.\n6The instruction template to generate explanations is shown in Appendix C.4.\n7The data examples of SCAR are provided in Appendix C.5.\nrich and complex analogy structure that can potentially challenge LLMs. The benchmark spans 13 domains for evaluating the generalizability of models across various domains. In addition, the benchmark provides backgrounds and explanations for concept mappings, serving as valuable resources to rationalize reasoning.\nComparison with Previous Benchmarks We compare SCAR to existing analogy resources by transforming it into word analogies. As in Fugure 1, we can obtain a word analogy, i.e., Film is to Retina as Aperture is to Pupil. Overall, as shown in Table 3, there are 3,159 word analogies in SCAR, which exhibits a larger number of word analogies than previous benchmarks.8\nDomain Analysis We consider the domains of the two systems in each system analogy as a pair, e.g., (Engineering, Biology), and calculate the frequency of each pair in SCAR to derive the domain transfer distribution of SCAR, as illustrated in Figure 2. The Figure highlights interdisciplinary aspects, with prominent cross-field relationships between Biology, Engineering, and Physics, emphasizing their inherent inter-connection. SCAR shows the prevalence of within-field analogies and asserts the significance of promoting interdisciplinary connections to foster collaborative advancements in knowledge acquisition."
        },
        {
            "heading": "3.4 Probing Task Formulation",
            "text": "Our task draws inspiration from SMT, which suggests that analogical reasoning involves drawing correspondences between two systems, founded on their shared relational structure. To this end, we define the analogical structure abduction task to\n8The detailed comparison is shown in Appendix C.6\nexplore the analogical reasoning ability of LLMs. Given two systems:\nSA = {tA1 , tA2 , ...., tAn }, SB = {tB1 , tB2 , ...., tBn },\nthis task involves establish mappings between concepts {tAi }i and {tBj }j for two systems to form an analogy between SA and SB . The task requires the understanding of the relational structures between concepts in both systems and creating a one-to-one mapping between them. Table 4 shows an instruction for LLMs to generate mappings. Our evaluation assesses the accuracy of concept mappings and system analogy. A system analogy is deemed correct if all concept mappings between the two systems are accurate."
        },
        {
            "heading": "4 Evaluation",
            "text": ""
        },
        {
            "heading": "4.1 Evaluation Settings",
            "text": "To minimize the impact of instruction design on LLMs, we create 10 different instruction templates for LLMs in this task and select the best one to evaluate model performance. Furthermore, we also explore the ability of models to use background knowledge and the CoT prompting (Kojima et al., 2022; Wei et al., 2022) with explanations in this task. The templates are shown in Table 11.\nMethod Concept Acc. System Acc. Avg\nAcc.En Zh En Zh"
        },
        {
            "heading": "4.2 Model Choices",
            "text": "We choose Alpaca (Taori et al., 2023) (7B), Vicuna (Chiang et al., 2023) (7B), InstructGPT series (Ouyang et al., 2022) (i.e., InstructGPTcurie001 (6.7B), InstructGPT002 (\u2265175B) and InstructGPT003 (\u2265175B)), ChatGPT (OpenAI, 2022) and GPT-4 (OpenAI, 2023). Alpaca and Vicuna are fine-tuned from a 7B LLaMA model (Touvron et al., 2023) with instructions or user-shared conversations. InstructGPT series are variants of GPT-3 (Brown et al., 2020) fine-tuned on instructions using reinforcement learning with human feedback (RLHF). ChatGPT is built on InstructGPT and trained on dialogue data using RLHF. GPT-4 is the most advanced LLM so far."
        },
        {
            "heading": "4.3 Overall Performance",
            "text": "We first compare LLMs with 0-shot, 1-shot, and background knowledge (w/ Backg.). Due to the limited input length, we excluded background knowledge from Alpaca, Vicuna, and InstructGPTcurie001 . For human performance, we test two graduate students, one in liberal arts and the\nother in science, with their results averaged. Result in Table 5 shows that: 1) GPT-4 achieves the best performance across both languages, and adding an example can enhance model performance. However, its ability still lags behind that of humans; 2) Smaller models perform poorly, but training on dialogue data can improve model performance; 3) The performance of the InstructGPT series and ChatGPT in Chinese is improved significantly by adding background knowledge, highlighting the model\u2019s struggle with domain-specific terminology in Chinese to affect their performance."
        },
        {
            "heading": "4.4 Analysis",
            "text": "Will step-by-step reasoning help solve SCAR? We dig into the effectiveness of the CoT prompting technique for LLMs in structure abduction. We adopt one example with the explanations of concept mappings to induce the LLMs to generate intermediate steps in analogical reasoning (w/ Expl.). One instruction template is shown in Table 11 (II). We also add \u201cLet\u2019s think step by step\u201d before each answer (w/ Step), which improves zero-shot reasoning for LLMs (Kojima et al., 2022). We conduct experiments on ten different templates to mitigate\nAr t\nBi ol\nog y\nCh em\nica l\nCo m\npu te r Ec on om y En gi ne er in g Ge og ra ph y Hi st or y Lit er at ur e M at he m at ics Ph ilo so ph y Ph ys ics Sp or t\nArt Biology Chemical Computer Economy Engineering Geography\nHistory Literature Mathematics Philosophy\nPhysics Sport\n100.0 42.9 70.0 87.5 50.0 0.0 50.0 50.0 33.3 0.0 50.0 87.5 50.0 42.9 66.7 100.0 75.0 100.0 81.8 83.3 80.0 37.5 73.9 100.0 70.0 100.0 100.0 0.0 50.0 50.0 0.0 100.0 50.0 50.0 50.0 100.0 87.5 75.0 100.0 50.0 42.9 80.0 0.0 100.0 83.3 100.0 0.0 50.0 100.0 0.0 50.0 100.0 33.3 0.0 100.0 60.0 0.0\n0.0 81.8 50.0 42.9 50.0 50.0 33.3 28.6 100.0 50.0 83.3 50.0 80.0 33.3 50.0 0.0 50.0 50.0 55.6 50.0 80.0 0.0 0.0 0.0 50.0 0.0 81.5 33.3 100.0 33.3 100.0 33.3 50.0 33.3 0.0 50.0 100.0 50.0 50.0 50.0 37.5 50.0 83.3 100.0 28.6 50.0 33.3 50.0 64.3 40.0 66.7 87.5 73.9 50.0 100.0 60.0 100.0 55.6 50.0 40.0 71.4 0.0 50.0 100.0100.0 0.0 0.0 100.0 66.7 0.0 100.0 0 20 40\n60\n80\n100\nFigure 4: The heatmap depicts the system accuracy of GPT-4 across different domains.\nhuman design bias for LLMs. Results in Figure 3 (a-c) show that: 1) CoT prompting enhances GPT4 performance in structure abduction but harms ChatGPT and InstructGPT003 performance due to flawed reasoning; 2) CoT prompting with explanations outperforms the \u201cLet\u2019s think step by step\u201d approach, highlighting the importance of explanations in CoT prompting.\nDoes instruction design affect the model performance? To answer this question, we calculate the average system accuracy of LLMs across all templates. Results in Figure 3(d) show that LLMs are sensitive to the instruction design. However, providing an example, additional backgrounds, or utilizing CoT prompting can enhance the robustness of LLMs to the instruction design.\nDo models behave differently for analogies across various domains? The heatmap in Figure 4 shows the system accuracy of GPT-4 across different domains.9 We find that: 1) The performance varies considerably for system analogies of different domains, indicating limited sensitivity of LLMs to domain boundaries; 2) In certain domains, e.g., literature, intra-domain system analogies have low accuracy, indicating LLMs may have some difficulties in these fields; 3) System analogies between similar domains (i.e., biology and chemistry) show high accuracy, demonstrating the potential for knowledge transfer.\n9We select the best results of GPT-4 on the English version of SCAR to draw the heatmap.\nHow do the embedding-mapping algorithms of concepts affect system analogies? We explore creating system analogies based on the embeddings of each concept in the systems. To achieve this, we implement three distinct mapping algorithms, leveraging the cosine similarity score of embeddings to facilitate the process: 1) Max-Similarity Algorithm: This algorithm maps each concept in System A to the concept from System B that exhibits the highest cosine similarity score, implying the same concept from System B can map to multiple concepts from System A; 2) Greedy Algorithm (Zhang et al., 2000): This algorithm iteratively maps the concepts with the highest cosine similarity. In each iteration, the concepts with the highest similarity are mapped and excluded from further consideration, generating one-to-one mappings without overall optimality; 3) Kuhn\u2013Munkres Algorithm (Kuhn, 1955): This combinatorial optimization algorithm generates one-to-one mappings, providing a globally optimized solution.10\nResults in Figure 5 reveal the insufficiency of the max-similarity algorithm in creating viable system analogies in SCAR. It underscores the inference that human-like analogical reasoning does not rely solely on surface-level embedding similarities. However, both the Greedy and Kuhn\u2013Munkres algorithms display enhanced performance, suggesting that the lackluster results of the LLMs on the structure abduction task might be attributed to their weak mapping capabilities. This observation indicates that human-like reasoning could employ embeddings alongside mapping algorithms as complementary tools to deduce system analogies.\n10Please refer to Appendix E for an in-depth explanation of the employed embedding similarity methods."
        },
        {
            "heading": "4.5 Open Analogical Structure Abduction",
            "text": "In the above experiments, we evaluate LLMs\u2019 ability of structure abduction in a close setting, where the concepts of each system are given (as in Table 4). However, a more intriguing question arises: Can LLMs perform analogical reasoning for systems with an open set of concepts, where concepts are not explicitly given? In this case, models must first identify concepts from contexts and then generate concept mappings to form a system analogy. This open analogical structure abduction problem more closely simulates the process of humans discovering and acquiring knowledge.\nWe provide LLMs with the background description texts of systems to simulate an open setting. LLMs are expected to retrieve concepts that can be used to create mappings from backgrounds and then establish concept mappings to form system analogies.11 For evaluation, we automatically calculate the recall of concept mappings based on SCAR to measure the correctness of newly generated mappings with annotated mappings (as in the close setting). Since some reasonable mappings may not be included in SCAR, we need to manually evaluate the precision. We randomly sample 50 data from SCAR and let LLMs generate concept mappings in the open setting. Two annotators assess the precision of the generated concept mappings with Fleiss\u2019s \u03ba = 0.86. Then F1 score can be calculated based on precision and recall.\nThe results in Figure 6 show that LLMs can establish some concept mappings even when concepts are not directly given. Despite the relatively low recall, higher precision indicates LLMs\u2019 ability to form new concept mappings, which can be utilized to further improve SCAR.\n11One instruction template is shown in the Appendix D."
        },
        {
            "heading": "5 Related Work",
            "text": "Analogical reasoning has been an area of interest in the AI community, primarily focusing on word analogies (Mitchell, 2021). Early researches in word analogy focus on evaluating the quality of word embeddings, which examines linear relations between words (Turney et al., 2003; Mikolov et al., 2013b; Gladkova et al., 2016) and can be effectively addressed through vector arithmetic for neural word embeddings such as Word2Vec (Mikolov et al., 2013a) and GloVe (Pennington et al., 2014).\nIn recent years, some studies explore analogy understanding of LMs on various benchmarks (Fournier et al., 2020; Ushio et al., 2021) and fine-tuned LMs using constructed knowledge bases of analogies to improve performance (Li et al., 2018, 2020; Yuan et al., 2023). New word analogy benchmarks with more complex relational structure (Chen et al., 2022; Czinczoll et al., 2022) and with multimodal elements (Zhang et al., 2023) (Zhang et al., 2023) are built to evaluate the performance of multilingual and multimodal models. However, a gap exists between word analogy formats and the nature of analogical reasoning in human cognition (Hofstadter, 2001; Bartha, 2013; Gentner and Maravilla, 2017), limiting the word analogy task to reflect the LLMs\u2019 analogical reasoning ability aligning with humans.\nThere has been a paradigm shift toward exploring analogies between situations (Turney, 2008; Sultan and Shahaf, 2022). These works are inspired by the SMT (Gentner, 1983), which aims to establish mappings between concepts in two domains based on a common relational structure. Nonetheless, Turney (2008) focuses on simple commonsense relations. Sultan and Shahaf (2022) argue that two processes with similar questioning formats can be analogies, which does not address complex structures and yield unsatisfactory performance on discovering analogies between different domains. Furthermore, some studies also explore the analogy generation of LLMs (Bhavya et al., 2022; Yuan et al., 2023; Ding et al., 2023). However, they mostly evaluate word analogies or simple analogies between two sentences, leaving complex structures in analogical reasoning unstudied. Webb et al. (2022) and Hu et al. (2023) examine the abstract language-based analogy task and evaluate the analogical reasoning ability of LLMs on this task. Compared to their task, our task requires the intensive involvement of commonsense, encyclopedic,\nand cultural (e.g., idiom and historical) knowledge. Recent researchers study AI alignment to guide AI toward achieving human preferences and ethical principles (Li et al., 2022; Rao et al., 2023; Park et al., 2023). We explore the analogical reasoning ability of LLMs with complex structure abduction, which is more aligned with human cognition."
        },
        {
            "heading": "6 Conclusion",
            "text": "In this paper, we explore the analogical reasoning ability of LLMs. We highlight word analogies neglect structures and thus can not evaluate LLMs in alignment with human cognition. To better evaluate LLMs aligning with humans, we propose an analogical structure abduction task with a new benchmark, SCAR. Experiments show that LLMs struggle with this task, but incorporating background knowledge and CoT prompting can improve their performance. We hope the SCAR can be a valuable resource to advance the research on analogical reasoning."
        },
        {
            "heading": "Limitations",
            "text": "We only instruct LLMs to establish concept mappings between systems in the analogical structure abduction task, leaving the discovery of novel analogies unexplored. Such a limitation highlights the potential for future work to adopt structure abduction to uncover analogies and learn about new knowledge.\nAnother limitation of this work is that our evaluation of the analogical structure abduction task relies on concept mappings. Although this criterion aligns with humans, it remains a challenge for the model. Future studies can consider designing more appropriate evaluation tasks. Additionally, although we mitigated the impact of templates on model results by designing ten templates and choosing the best results for evaluation, we believe there remains room for improvement in instruction design to fully harness the capability of LLMs."
        },
        {
            "heading": "Ethics Statement",
            "text": "All authors of this work abide by the provided Code of Ethics. Annotators recruited by our institution annotate the system analogies in SCAR. The annotation quality is ensured through a double-check strategy outlined in Section 3. We ensure that the privacy rights of all annotators are respected in the annotation process. As described in our paper, all annotators are compensated above the local min-\nimum wage and consent to using the SCAR for research purposes."
        },
        {
            "heading": "Acknowledgement",
            "text": "We thank the anonymous reviewers for their valuable comments, and Yikai Zhang and Shuang Li from Fudan University for their useful suggestions for the manuscript. This work is supported by the Chinese NSF Major Research Plan (No.92270121), Shanghai Science and Technology Innovation Action Plan (No.21511100401) and the Science and Technology Commission of Shanghai Municipality Grant (No. 22511105902)."
        },
        {
            "heading": "A Author Contributions",
            "text": "Siyu Yuan Lead the project, develop the original method and original code, lead the curation of the dataset, contribute to the original experiments, and contribute to the original manuscript.\nJiangjie Chen Conceptualization of the original idea, supervision of the research activity planning and execution, contribution to the original manuscript and figures, and acquisition of financial support for the project.\nXuyang Ge Contribute to the experiments and data curation.\nYanghua Xiao Provide financial support for the project and revise the manuscript.\nDeqing Yang Provide financial support for the project, revise the manuscript, and oversee the research activity execution."
        },
        {
            "heading": "B Word Analogy Test",
            "text": ""
        },
        {
            "heading": "B.1 Benchmark",
            "text": "We compare LLMs with human performance in 6 different analogy benchmarks.\n\u2022 E-KAR (Chen et al., 2022): A knowledgeintensive analogical reasoning benchmark from China\u2019s Civil Service Examinations (CSE), including linguistic, commonsense, encyclopedic, and cultural knowledge.\n\u2022 BATS (Gladkova et al., 2016): This benchmark features over 1,000 analogies in four categories: lexicographic, encyclopedic, derivational, and inflectional morphology.\n\u2022 UNIT2 (Boteanu and Chernova, 2015): A benchmark using word analogy problems from an educational resource.\n\u2022 UNIT4 (Boteanu and Chernova, 2015): Similar to U2, this benchmark comes from an educational resource but is more challenging.\n\u2022 Google (Mikolov et al., 2013b): A benchmark for intrinsic evaluation of word embeddings, containing semantic and morphological relations.\n\u2022 SAT (Turney et al., 2003): A benchmark derived from a US college admission test with 374 word analogy problems."
        },
        {
            "heading": "B.2 Prompt Templates for Large Language Models",
            "text": "As shown in Table 9, we design the instruction for LLMs to generate the answers."
        },
        {
            "heading": "C Details of SCAR",
            "text": ""
        },
        {
            "heading": "C.1 Resource of SCAR",
            "text": "To facilitate a more efficient and effective annotation process, we furnish annotators with scientific analogies sourced through online research:\n\u2022 https://homework. study.com/explanation/ what-is-analogy-in-science.html\n\u2022 www.csun.edu/science/ref/analogy/ analogy.htm\n\u2022 https://science-education-research. com/teaching-science/ constructivist-pedagogy/ making-the-unfamiliar-familiar/ science-analogies/\n\u2022 www.engagefastlearning.com"
        },
        {
            "heading": "Data # Analogy Lang. Backg. Expl.",
            "text": "For example, \u201cWhen Rutherford (following Nagoka) conceived of the atom as a miniature solar system \u2013 electrons circling the nucleus as planets circle the sun\u201d. These resources can prompt annotators to create system analogies with concept mappings."
        },
        {
            "heading": "C.2 Crowd-sourcing Details",
            "text": "We have recruited a team of five undergraduates with diverse academic backgrounds in Computer Science, History, Physics, Biology, and Chemistry. Among them, the student majoring in Computer Science has a minor in Economics, while the student majoring in History has minors in Creative Writing and Philosophy. We pay each annotator $8/h, exceeding the local minimum wage $5/h."
        },
        {
            "heading": "C.3 Backgroud Rewriting Template",
            "text": "As shown in Table 10 (I), we design the instruction for ChatGPT to ensure that each revised background is limited to 500 words and encompasses all concepts relevant to each system."
        },
        {
            "heading": "C.4 Explanation Generation Template",
            "text": "Table 10 (II) shows the human-written instruction for GPT-4 to generate the explanation for each mapping in SCAR."
        },
        {
            "heading": "C.5 Data Examples of SCAR",
            "text": "Table 7 presents some examples of SCAR for a better understanding."
        },
        {
            "heading": "C.6 Comparison to Previous Benchmarks",
            "text": "We compare SCAR with the resources related to the problem of analogy. As the existing benchmarks are based on word analogies, we transform SCAR for appropriate comparisons. We combine"
        },
        {
            "heading": "1 Limit Modification",
            "text": "the two concept mappings within the same system analogy to form a word analogy. For instance, in Fugure 1, we can obtain a word analogy, i.e., film is to retina as aperture is to pupil. As reported in Table 6, our method exhibits a larger number of word analogies with bilingual language."
        },
        {
            "heading": "D Details about Analogical Structure Abduction Task",
            "text": "We show the instructions combining backgrounds and the CoT prompting with explanations in Table 11 (I) and (II). One human-written instruction for the open analogical structure abduction task is shown in Table 8."
        },
        {
            "heading": "E Embedding Similarity Method",
            "text": "We convert concepts in two systems into different embeddings with the following strategies and calculate cosine similarity between concepts:\n1. Max-similarity algorithm, which establishes a mapping with the concept from System B that\nexhibits the highest cosine similarity score. This method performs mapping with replacement, meaning that a concept from System B can be mapped to multiple concepts from System A;\n2. Greedy algorithm (Zhang et al., 2000), which iteratively maps concepts exhibiting the highest cosine similarity in each step. In each round of iterations, we first calculate all cosine similarity scores between concepts in the two systems. Then, we map the concepts with the highest cosine similarity scores, and the concepts that have been mapped will not be considered in the next round of iterations. This strategy generates one-to-one mappings while not considering the overall optimality.\n3. Kuhn-Munkres algorithm (Kuhn, 1955), a combinatorial optimization technique for solving one-to-one mapping problems. Given a matrix C, with each C[i, j] representing the cost of matching vertex i (a \u201cworker\u201d) to vertex j (a \u201cjob\u201d), the objective is to find a minimal cost assignment of workers to jobs. Let X be a boolean matrix, with C[i, j] = 1 if and only if row i is assigned to column j. The optimal assignment cost is given by:\nmin \u2211 i \u2211 j Ci,jXi,j , (1)\nwhere X is square, each row corresponds to exactly one column, and each column corresponds to exactly one row.\nI: Background Revision /* Task prompt */ Given a description of a system with a list of concepts related to the system, please generate a short introduction of the system according to the description and concepts within 500 words. /* Data */ System: Biosphere Description: The biosphere, also known as the ecosphere, is the worldwide sum of all ecosystems. It can also... Concepts: biology, biodiversity, ecosystem Introduction: The biosphere refers to the sum of all ecosystems on Earth, including the interactions between living organisms and their environment..."
        }
    ],
    "title": "Beneath Surface Similarity: Large Language Models Make Reasonable Scientific Analogies after Structure Abduction",
    "year": 2023
}