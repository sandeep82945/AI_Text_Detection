{
    "abstractText": "The decoding algorithm is critical for openended text generation, transforming latent representations into coherent and meaningful outputs. This paper investigates the selfreinforcement effect in text generation and the effectiveness of a repetition penalty to mitigate it. However, determining the optimal repetition penalty value is challenging. To tackle this, we propose a forgetting mechanism that disregards distant tokens, reducing the burden of penalty selection. In addition, we introduce a length penalty to address overly short sentences caused by excessive penalties. Our penalty decoding approach incorporating three strategies helps resolve issues with sampling methods deviating from factual information. Experimental results demonstrate the efficacy of our approach in generating high-quality sentences resembling human output.1",
    "authors": [
        {
            "affiliations": [],
            "name": "Wenhong Zhu"
        },
        {
            "affiliations": [],
            "name": "Hongkun Hao"
        },
        {
            "affiliations": [],
            "name": "Rui Wang"
        },
        {
            "affiliations": [],
            "name": "Shanghai Jiao Tong"
        }
    ],
    "id": "SP:40ed14fe6045cf89e49da60f110e2f3fc59870eb",
    "references": [
        {
            "authors": [
                "Sourya Basu",
                "Govardana Sachitanandam Ramachandran",
                "Nitish Shirish Keskar",
                "Lav R. Varshney."
            ],
            "title": "Mirostat: a neural text decoding algorithm that directly controls perplexity",
            "venue": "9th International Conference on Learning Representations, ICLR 2021, Vir-",
            "year": 2021
        },
        {
            "authors": [
                "Angela Fan",
                "Mike Lewis",
                "Yann Dauphin."
            ],
            "title": "Hierarchical neural story generation",
            "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 889\u2013898, Melbourne, Australia. Association",
            "year": 2018
        },
        {
            "authors": [
                "John Hewitt",
                "Christopher Manning",
                "Percy Liang."
            ],
            "title": "Truncation sampling as language model desmoothing",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2022, pages 3414\u2013 3427, Abu Dhabi, United Arab Emirates. Association",
            "year": 2022
        },
        {
            "authors": [
                "Ari Holtzman",
                "Jan Buys",
                "Li Du",
                "Maxwell Forbes",
                "Yejin Choi."
            ],
            "title": "The curious case of neural text degeneration",
            "venue": "8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net.",
            "year": 2020
        },
        {
            "authors": [
                "Nitish Shirish Keskar",
                "Bryan McCann",
                "Lav R Varshney",
                "Caiming Xiong",
                "Richard Socher."
            ],
            "title": "Ctrl: A conditional transformer language model for controllable generation",
            "venue": "arXiv preprint arXiv:1909.05858.",
            "year": 2019
        },
        {
            "authors": [
                "Tian Lan",
                "Yixuan Su",
                "Shuhang Liu",
                "Heyan Huang",
                "Xian-Ling Mao."
            ],
            "title": "Momentum decoding: Openended text generation as graph exploration",
            "venue": "arXiv preprint arXiv:2212.02175.",
            "year": 2022
        },
        {
            "authors": [
                "Margaret Li",
                "Stephen Roller",
                "Ilia Kulikov",
                "Sean Welleck",
                "Y-Lan Boureau",
                "Kyunghyun Cho",
                "Jason Weston."
            ],
            "title": "Don\u2019t say that! making inconsistent dialogue unlikely with unlikelihood training",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for",
            "year": 2020
        },
        {
            "authors": [
                "Yen-Ting Lin",
                "Yun-Nung Chen."
            ],
            "title": "Llm-eval: Unified multi-dimensional automatic evaluation for open-domain conversations with large language models",
            "venue": "arXiv preprint arXiv:2305.13711.",
            "year": 2023
        },
        {
            "authors": [
                "Clara Meister",
                "Tiago Pimentel",
                "Gian Wiher",
                "Ryan Cotterell."
            ],
            "title": "Locally typical sampling",
            "venue": "Trans. Assoc. Comput. Linguistics, 11:102\u2013121.",
            "year": 2023
        },
        {
            "authors": [
                "Nasrin Mostafazadeh",
                "Nathanael Chambers",
                "Xiaodong He",
                "Devi Parikh",
                "Dhruv Batra",
                "Lucy Vanderwende",
                "Pushmeet Kohli",
                "James Allen"
            ],
            "title": "A corpus",
            "year": 2016
        },
        {
            "authors": [
                "OpenAI."
            ],
            "title": "Gpt-4 technical report",
            "venue": "arXiv preprint arXiv:2303.08774.",
            "year": 2023
        },
        {
            "authors": [
                "Krishna Pillutla",
                "Swabha Swayamdipta",
                "Rowan Zellers",
                "John Thickstun",
                "Sean Welleck",
                "Yejin Choi",
                "Zaid Harchaoui."
            ],
            "title": "Mauve: Measuring the gap between neural text and human text using divergence frontiers",
            "venue": "Advances in Neural Information Pro-",
            "year": 2021
        },
        {
            "authors": [
                "Alec Radford",
                "Jeffrey Wu",
                "Rewon Child",
                "David Luan",
                "Dario Amodei",
                "Ilya Sutskever"
            ],
            "title": "Language models are unsupervised multitask learners",
            "venue": "OpenAI blog,",
            "year": 2019
        },
        {
            "authors": [
                "Yixuan Su",
                "Nigel Collier."
            ],
            "title": "Contrastive search is what you need for neural text generation",
            "venue": "axXiv preprint arXiv:2210.14140.",
            "year": 2023
        },
        {
            "authors": [
                "Yixuan Su",
                "Tian Lan",
                "Yan Wang",
                "Dani Yogatama",
                "Lingpeng Kong",
                "Nigel Collier."
            ],
            "title": "A contrastive framework for neural text generation",
            "venue": "Advances in Neural Information Processing Systems, volume 35, pages 21548\u201321561. Curran Associates, Inc.",
            "year": 2022
        },
        {
            "authors": [
                "Aroyo",
                "Ravi Rajakumar",
                "Alena Butryna",
                "Matthew Lamm",
                "Viktoriya Kuzmina",
                "Joe Fenton",
                "Aaron Cohen",
                "Rachel Bernstein",
                "Ray Kurzweil",
                "Blaise AgueraArcas",
                "Claire Cui",
                "Marian Croak",
                "Ed Chi",
                "Quoc Le"
            ],
            "title": "Lamda: Language models for dialog",
            "year": 2022
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N. Gomez",
                "Lukasz Kaiser",
                "Illia Polosukhin."
            ],
            "title": "Attention is all you need",
            "venue": "Advances in Neural Information Processing Systems 30: Annual Conference on Neural",
            "year": 2017
        },
        {
            "authors": [
                "Yizhong Wang",
                "Yeganeh Kordi",
                "Swaroop Mishra",
                "Alisa Liu",
                "Noah A Smith",
                "Daniel Khashabi",
                "Hannaneh Hajishirzi."
            ],
            "title": "Self-instruct: Aligning language model with self generated instructions",
            "venue": "arXiv preprint arXiv:2212.10560.",
            "year": 2022
        },
        {
            "authors": [
                "Teven Le Scao",
                "Sylvain Gugger",
                "Mariama Drame",
                "Quentin Lhoest",
                "Alexander M. Rush."
            ],
            "title": "Huggingface\u2019s transformers: State-of-the-art natural language processing",
            "venue": "arXiv preprint arXiv:1910.03771.",
            "year": 2020
        },
        {
            "authors": [
                "Jin Xu",
                "Xiaojiang Liu",
                "Jianhao Yan",
                "Deng Cai",
                "Huayang Li",
                "Jian Li."
            ],
            "title": "Learning to break the loop: Analyzing and mitigating repetitions for neural text generation",
            "venue": "Advances in Neural Information Processing Systems, volume 35, pages 3082\u20133095. Cur-",
            "year": 2022
        },
        {
            "authors": [
                "Susan Zhang",
                "Stephen Roller",
                "Naman Goyal",
                "Mikel Artetxe",
                "Moya Chen",
                "Shuohui Chen",
                "Christopher Dewan",
                "Mona Diab",
                "Xian Li",
                "Xi Victoria Lin"
            ],
            "title": "Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Open-ended text generation tasks involve generating coherent and fluent output with limited input information (Holtzman et al., 2020). These tasks encompass various applications such as chit-chat dialog (Thoppilan et al., 2022), story generation (Mostafazadeh et al., 2016), and similar domains. Transformer-based models (Vaswani et al., 2017), which predict the probability of the next token during text decoding, have been widely adopted for such tasks. Among them, the GPT series, utilizing a decoder-only auto-regressive model, has demonstrated remarkable performance (Radford et al., 2019). The choice of decoding strategy plays a crucial role in determining the quality of text generation, not only in open-ended text generation but also in other natural language generation tasks.\nDecoding strategies can be categorized into two types. One is deterministic methods, and another\n\u2217Rui Wang is corresponding author. 1The source code and data will be shown at https://\ngithub.com/zwhong714/penalty_decoding\nis stochastic methods, also named truncation sampling (Meister et al., 2023). (1) Deterministic decoding strategies such as greedy search and beam search, which take the highest probability, would cause dull and repetitive text (Li et al., 2020). Contrastive search (Su et al., 2022), a recently proposed deterministic method, aims to enhance diversity while maintaining coherence in the generated text. However, it requires model retraining using contrastive training objectives and has a high time complexity of decoding. (2) Stochastic decoding strategies such as top-k (Fan et al., 2018) and top-p (Holtzman et al., 2020) sampling introduce randomness to increase the diversity of generated text. Several methods have been proposed to improve the truncation space based on these two methods, such as typical decoding (Meister et al., 2023), \u03b7sampling (Hewitt et al., 2022), Mirostat (Basu et al., 2021) and other methods. However, it is worth noting that existing stochastic decoding strategies may exacerbate the issue of model hallucinations (OpenAI, 2023; Lan et al., 2022).\nIn this paper, we begin by reviewing and ana-\nlyzing the self-reinforcement effect proposed by Xu et al. (2022), which refers to the tendency of maximization-based decoding algorithms to assign higher probabilities to tokens that have already been generated, leading to repetitive text. Inspired by the work of Wang et al. (2022), who observed that the vanilla GPT3 model (Brown et al., 2020) often produces irrelevant and repetitive text, we hypothesize that this phenomenon may also exist in large language models. A critical insight of our work is to consider the distribution of a neural LM as an enhanced version. For instance, when presented with a prefix such as \"Barack Obama was born in Honolulu, Hawaii. He was born in,\" the model (e.g., GPT2-XL) tends to repeat the previous context. This phenomenon might occur due to the probability distribution of the words \"Hawaii\" and \"Honolulu\" having a notable reinforcing effect within the probability space, as illustrated in the left part of Figure 1. To tackle this problem, we have introduced the repetition penalty proposed by Keskar et al. (2019). This penalty effectively mitigates the self-reinforcement effect and reshapes the token distribution, as demonstrated in the right section of Figure 1, resulting in a higher generation quality. However, tuning the repetition penalty hyperparameter can be challenging. Therefore, we propose the forgetting mechanism and length penalty as additional strategies to ensure generation quality. Through extensive experiments, we demonstrate the efficacy of our approach in generating sentences that closely resemble human-produced text."
        },
        {
            "heading": "2 Self-Reinforcement Effect",
            "text": "The self-reinforcement effect refers to the probability of repetition increasing with the number of historical repetitions (Holtzman et al., 2020; Xu et al., 2022). This effect can be intuitively reflected in the red probability shown in Figure 1, which may cause the model to get stuck in a repetition loop since the selection of tokens is forced to be limited to these enhanced tokens.\nTo quantify this phenomenon, we design the following metrics at various levels. Different from the metrics proposed by Xu et al. (2022), which analyze the self-reinforcement of repeated tokens by different numbers of repeated prefixes, we dynamically do analyzation in the model\u2019s natural decoding process to reasonably evaluate the selfreinforcement phenomenon arising from the natural decoding state of the model. Given a language\nmodel P\u03b8, we have: N-gram Level. If 1n \u2211n\u22121\ni=0 P\u03b8(xu+i|x<u+i) is greater than 1n \u2211n\u22121 i=0 P\u03b8(xv+i|x<v+i), where xu, ..., xu+n\u22121 is the next recurring n-gram of xv, ..., xv+n\u22121, u and v refer to the current decoding subscript position, then we say n-gram xu, ..., xu+n\u22121 is a reinforced version. We define the ratio of self-reinforcement at n-gram level SRn as:\nSRn = 1\nLg \u2212 n+ 1 #( n\u22121\u2211 i=0 P\u03b8(xu+i | x<u+i)\n> n\u22121\u2211 i=0 P\u03b8(xv+i | x<v+i)),\n(1)\nwhere #(\u00b7) is the number of n-gram reinforced, Lg is the length of the generated sentence.\nNucleus Level. The nucleus size of top-k highest probabilities in the probability distribution for the tth token is defined as NS(t) =\u2211\nx\u2208Vtopk P\u03b8(xt|x<t), where Vtopk is top-k highest probability token space according to P\u03b8. If 1 t \u2211t u=1 NS(u) > 1 t\u22121 \u2211t\u22121 u=1 NS(u), we say the model becomes much confident. Then\nSRtopk = 1\nLg Lg\u2211 t=1 I( 1 t t\u2211 u=1 NS(u)\n> 1\nt\u2212 1 t\u22121\u2211 u=1 NS(u)),\n(2)\nwhere I is an indicator function; this metric measures how confident the model becomes.\nResults and Analyses. We compare the selfreinforcement effect of three common decoding methods on a widely used dataset using the GPT2XL model (see \u00a74.1 for details). Table 1 illustrates this effect at different n-gram levels during text generation on the test set. It is observed that as the value of n increases, the rate of reinforcement decreases. However, the self-reinforcement effect still remains significant in the case of greedy decoding for n-grams. This finding highlights the notable self-reinforcement result of greedy decoding, and the stochastic decoding algorithms can effectively mitigate this effect. Top-p decoding demonstrates superior suppression, indicating a more pronounced mitigation as the sampling space expands.\nFigure 2 presents the self-reinforcement effect at the nucleus level on the test set, along with the probability of the Vtopk space. As depicted in Figure 2(a), when employing greedy decoding the model tends to concentrate the probability more on the Vtopk space compared to the stochastic methods. Furthermore, Figure 2(b) demonstrates that the probability distribution heavily favors the Vtopk space after decoding, with the greedy search approach yielding probabilities as high as 90%. As is well known, as the generation proceeds, decoding to some extent concentrates on a smaller set of directions and greedy decoding would accelerate this process.\nThe above analysis surprisingly finds that adding perturbation to these reinforced spaces can effectively suppress this effect. However, stochastic methods depending on randomness would lead to serious hallucination (OpenAI, 2023; Lan et al., 2022). We guess other techniques exist to modify the maximization-based decoding, such as adding repetition penalties, to achieve the same result."
        },
        {
            "heading": "3 Method",
            "text": "In this section, we will present our penalty decoding approach, which comprises three techniques de-\nsigned to enhance the performance of greedy decoding by efficiently mitigating the self-reinforcement effect to generate high-quality text."
        },
        {
            "heading": "3.1 Repetition Penalty",
            "text": "The repetition penalty, as introduced by Keskar et al. (2019) (Keskar et al., 2019), aligns closely with our insight by penalizing tokens that could potentially exacerbate self-reinforcement. These penalties will be directly applied to the reinforced tokens. Following the softmax function, this reinforced portion can be redistributed to other tokens, as depicted in the green part of Figure 1, thereby ensuring the feasibility of alternative token sampling.\nP\u03b8(v|x) = exp (vi/\u03b1 \u00b7 (I(i \u2208 x))\u2211 j exp (vj/\u03b1 \u00b7 (I(j \u2208 x)) , (3)\nwhere vi and vj represent specific tokens within the vocabulary, x represents the generated text, and \u03b1 is a hyper-parameter greater than one."
        },
        {
            "heading": "3.2 Forgetting Mechanism",
            "text": "As illustrated in Equation 3, the penalties persist throughout the generation process and may result in significant semantic deviations. Thus, we propose the forgetting mechanism to limit repetition counting to a window w around the current decoded location. This approach preserves text coherence by ensuring the decoding process aligns closely with contextual cues. The main implementation is as follows:\nP\u03b8(v|x) = exp (vi/\u03b1 \u00b7 (I(i \u2208 x[\u2212w :]))\u2211 j exp (vj/\u03b1 \u00b7 (I(j \u2208 x[\u2212w :])) , (4)"
        },
        {
            "heading": "3.3 Length Penalty",
            "text": "Choosing an appropriate repetition penalty can be challenging, as discussed in Basu et al. (2021). If the repetition penalty is too small, it may not effectively alleviate self-reinforcement, while a large one can lead to short sentences as the <eos> 2 token is sampled early. The length penalty is applied to mitigate this problem and a straightforward implementation is a linear penalty imposed on the logits of <eos> token P\u03b8(<eos>) as follows.\n2<eos>: end-of-sentence identifier\nP\u03b8(<eos>) = \u03b1 \u00b7 P\u03b8(<eos>)(Lt \u2212 Lx), (5)\nwhere Lt is the preset target length, typically the same as the maximum length allowed, and Lx denotes the current length of the decoded text."
        },
        {
            "heading": "3.4 Penalty Decoding",
            "text": "We introduce the penalty decoding algorithm outlined in Algorithm 1 by incorporating the strategies above.\nAlgorithm 1: Penalty decoding Input: Language Model P\u03b8; prefix x;\nrepetition penalty \u03b1; window size w, targeted length Lt, the vocabulary of the language model V .\nwhile x[-1] \u0338= <eos> do Calculate the next token logits: P\u03b8(v|x) Let w = min(w, len(x)) Let P\u03b8(<eos>) = \u03b1 \u00b7 P\u03b8(<eos>) \u00b7 (Lt \u2212 len(x))\nCalculate Softmax function:\nP\u03b8(v|x) = exp (xi/\u03b1 \u00b7 (I(i \u2208 x[\u2212w :]))\u2211 j exp (xj/\u03b1 \u00b7 (I(j \u2208 x[\u2212w :]))\nGet the most probable token v\u0302: v\u0302 = argmaxv\u2208V P\u03b8(v|x)\nUpdate the prefix x = [x : v\u0302] end Output: The generated text x."
        },
        {
            "heading": "4 Experiments",
            "text": "In this section, we examine the effectiveness of penalty decoding compared to other decoding methods for open-ended text generation tasks. Detailed setup for experiments can be found in Appendix A.1, and comprehensive ablation studies are shown in Appendix B."
        },
        {
            "heading": "4.1 Model and Dataset",
            "text": "In all of our experiments, we utilize the GPT2-XL (Radford et al., 2019) that is available in the Huggingface library (Wolf et al., 2020). The dataset is derived from WebText (Radford et al., 2019), specifically the held-out validation or test set of GPT-2."
        },
        {
            "heading": "4.2 Automatic Evaluation",
            "text": "We evaluate the quality of our generated texts using various automatic metrics, including diversity, MAUVE (Pillutla et al., 2021), coherence (Zhang et al., 2022), greedy ratio (Lan et al., 2022), and gen-length. Please refer to Appendix A.2 for more information about these metrics. Additionally, we measure the self-reinforcement effect after applying our penalty decoding.\nResults and Analysis. The main results are presented in Table 2. Observations reveal that all stochastic decoding algorithms, except Mirostat, exhibit high diversity and MAUVE scores. This may be attributed to the incorporation of randomness, which diminishes the model\u2019s confidence and effectively mitigates the self-reinforcement effect. Furthermore, stochastic decoding is conducive to generating longer text.\nBoth greedy decoding and beam search are associated with reduced diversity and lower MAUVE scores. Greedy decoding, in particular, often generates longer texts, potentially due to the model becoming trapped in a repetition loop.\nOur penalty-based decoding effectively strikes a balance among these automation metrics. In contrast to near-greedy decoding, it can generate longer sentences. Moreover, it achieves comparable levels of diversity and MAUVE scores, similar to contrastive decoding and other stochastic decoding methods. According to the greedy ratio, approximately 44.01% of tokens have been subjected to penalties in the generation, effectively mitigating the self-reinforcement effect at both the n-gram and nucleus levels, as demonstrated in Table 1 and Figure 2.\nFrom Figure 2(b), we surprisingly find that our penalty decoding falls between greedy decoding and the stochastic decoding algorithms within the Vtopk space. This demonstrates our approach\u2019s capacity to mitigate model overconfidence and prevent hallucination problems caused by decoding divergence. In other words, as the generation process unfolds, penalty decoding still tends to focus on a narrower set of directions.\nThe coherence score demonstrates a strong correlation with the greedy ratio, where a higher greedy ratio often leads to enhanced coherence. Comparing the near-greedy approach that only utilizes repetition penalty, we observe penalty decoding can generate longer texts with higher MAUVE scores while maintaining the same level of coherence."
        },
        {
            "heading": "4.3 LLM for Evaluation",
            "text": "Conventional evaluation methods typically require human annotations and rely on ground-truth responses, which can be resource-intensive and timeconsuming (Lin and Chen, 2023). We employ the large language model text-davinci-003 (Brown et al., 2020) to overcome these limitations as an evaluator surrogate. Details of the evaluation and prompt design can be found in the appendix C.\nResults. The comparison results are displayed in Table 3, indicating that penalty decoding surpasses top-p, typical, and \u03b7-sampling decoding methods, and it offers decoding performance on par with the contrastive search method."
        },
        {
            "heading": "4.4 Human Evaluation",
            "text": "Auto evaluation metrics are not always entirely reliable. For instance, the MAUVE metric has demonstrated sensitivity to the length of generated content, as discussed by Su and Collier (2023). Therefore, we have conducted human evaluations to address this concern. Comprehensive details regarding the evaluation process can be found in Appendix D.\nResults. The comparative results are displayed in Table 4. Human evaluations indicate that penalty decoding outperforms all other decoding methods."
        },
        {
            "heading": "4.5 Case Study",
            "text": "We present two examples to demonstrate the superior performance of penalty decoding compared\nto other decoding methods. The first tests whether the decoding method can generate factual information, while the second tests whether the decoding method can produce coherent text.\nTable 7 demonstrates that our penalty-based decoding yields more factually accurate information about DeepMind company and produces sentences that closely resemble human-written text. While Contrastive decoding does manage to generate some factual content, the resulting text appears somewhat lacking in diversity. In contrast, the quality of stochastic sampling is unsatisfactory. In Table 8, the provided prefix offers sufficient contextual information, and our penalty decoding method successfully generates coherent text that maintains logical flow and coherence."
        },
        {
            "heading": "5 Conclusion",
            "text": "This paper delves into the self-reinforcement phenomenon in text generation and introduces penalty decoding as a solution. Through the integration of a repetition penalty, a forgetting mechanism, and a length penalty, the token distribution is adjusted to enhance diversity and diminish model-induced hallucinations, consequently elevating the quality and coherence of the generated text. Our penalty decoding can also be combined with sampling-based methods like top-k and top-p sampling. The findings and evaluations in this study aim to encourage further research in advancing the generation capabilities of neural language models."
        },
        {
            "heading": "Acknowledgements",
            "text": "The authors are with the MT-Lab, Department of Computer Science and Engineering, School of Electronic Information and Electrical Engineering, and also with the MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University, Shanghai 200204, China. This paper is supported by the General Program of National Natural Science Foundation of China (62176153), Shanghai Pujiang Program (21PJ1406800), Shanghai Municipal Science and Technology Major Project (2021SHZDZX0102), the Alibaba-AIR Program (22088682), and the Tencent AI Lab Fund RBFR2023012.\nLimitations\nWe acknowledge that our study has certain limitations. Firstly, we focus solely on an English openended text generation task and do not investigate the application of the proposed penalty decoding algorithm in other generation tasks. Additionally, we limit our experiments to relatively small models such as GPT2-XL, which consists of 1.7B parameters. The existence of the self-reinforcement effect in larger language models remains a conjecture and requires further experimental verification. Furthermore, the sample size we used for human evaluation is only 50, so the variance may be a bit large. We also do not explore the sensitivity of different models to repetition penalty or investigate the effects of alternative length punishment mechanisms, such as exponential punishment. Moreover, we do not conduct specific experiments to examine the performance of different decoding methods in generating factual information. This is an area that warrants further investigation. These limitations provide opportunities for future research.\nEthics Statement\nThis paper will not pose any ethical problems. First, text generation is a standard task in natural language processing. Second, the datasets used in this paper have already been used in previous articles."
        },
        {
            "heading": "A Experimental Details",
            "text": "A.1 Expeimental Setups\nWe conducted all experiments using the GPT2-XL model. In the main experiment, we generated texts conditioned on the initial paragraph, limited to 32 tokens, from documents in the held-out set of WebText. The text generation process was terminated upon encountering an end-of-document token or reaching a maximum length of 128 new tokens. The test set consists of 1000 samples selected from Webtext, while the validation set comprises 500 samples extracted from the remaining data sets. The experimental configurations generally follow the work of Su and Collier (2023).\nA.2 Automatic Evaluation\nDiversity. This metric considers the repetition of generated text at different n-gram levels and can be calculated as follows: diversity = \u220f4 n=2(1.0\u2212 rep-n 100 ).\nMAUVE. The score (Pillutla et al., 2021) is a metric that quantifies the similarity in token distribution between generated text and human-written text.\nGen-Length. This metric is utilized to compute the average length of the generated text.\nCoherence. This metric (Su and Collier, 2023) employs the OPT-2.7B language model (Zhang et al., 2022) to assess the coherence between the generated text and a given prefix. The metric is defined as follows:\n1\n|x\u0302| |x\u0302|\u2211 i=1 log pM (x\u0302i | [x : x\u0302<i]) , (6)\nwhere [:] is the concatenation operation.\nGreedy Ratio. This metric quantifies the proportion of times the language model selects the token with the highest probability during text generation (Lan et al., 2022).\nA.3 Hyperparameters\nSome decoding methods rely on specific hyperparameters that significantly impact the quality of the generated sentences. To determine the optimal hyperparameters, we utilize the MAUVE metric and search for a predefined set, as outlined in Table 5. The best-performing hyperparameters on the validation set, indicated in bold font, are then selected, and their corresponding performance on the test set is reported in Table 2."
        },
        {
            "heading": "B Ablation Studies",
            "text": "We employ the validation dataset for conducting ablation studies.\nB.1 The impact of repetition penalty\nIn the absence of the repetition penalty, the decoding performance is equivalent to that of greedy decoding. The diversity measure is only 20.09%, indicating limited variation in the generated text, while the MAUVE score is only 27.03%, indicating a lower resemblance to human-written text. As depicted in Figure 3(a), higher repetition penalties result in increased diversity but lower MAUVE\nscores. Lower repetition penalties lead to decreased diversity and lower MAUVE scores.\nB.2 The impact of forgetting mechanism\nA window size of 0 corresponds to greedy decoding, while a window size larger than the length of the generated text indicates near-greedy decoding. From Figure 3(b), we can observe that using a small window size results in lower text quality. Typically, a larger window value is essential to ensure the generation\u2019s quality, signifying that the self-reinforcement effect tends to persist throughout the building process. Nevertheless, it\u2019s worth noting that bigger window values are not always advantageous. In some cases, they can lead to a reduction in MAUVE, potentially due to the accumulation of penalties causing the text to deviate from its intended semantics, subsequently degrading text quality.\nB.3 The impact of length penalty\nAs shown in Table 6, the introduction of a length penalty has a discernible impact on the MAUVE metric for the generated text. It\u2019s crucial to recognize that MAUVE is influenced by text length and may not always guarantee the holistic quality of the generated content. However, it remains a pertinent observation that the accumulation of penalties can lead to the model generating overly short text."
        },
        {
            "heading": "C Prompt Design",
            "text": "The design of prompts is illustrated in Figure 4. The Language Model (LLM) receives input consisting of a prefix, a reference completion, completion A generated by one decoding algorithm, and completion B generated by another decoding algorithm. The LLM then assesses and determines whether completion A is better than completion B based on criteria such as consistency, fluency, and informativeness. The evaluation process involves 200 samples, and any invalid outputs are excluded from the results."
        },
        {
            "heading": "D Human Evaluation",
            "text": "We randomly selected 50 samples from the test set and applied various decoding algorithms to generate sentences from them. For evaluation, we engaged two English language experts. The evaluation method involves comparing sentences generated by penalty-based decoding with those produced by alternative decoding algorithms. Prior to\nthe comparison, the experts are kept unaware of the specific decoding algorithm used for sentence generation. They should assess and select superior sentences based on three criteria: consistency, fluency, and informativeness."
        }
    ],
    "title": "Penalty Decoding: Well Suppress the Self-Reinforcement Effect in Open-Ended Text Generation",
    "year": 2023
}