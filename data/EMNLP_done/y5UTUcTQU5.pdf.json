{
    "abstractText": "Aspect Sentiment Triplet Extraction (ASTE) is one of the compound tasks of fine-grained aspect-based sentiment analysis (ABSA), aiming at extracting the triplets of aspect terms, corresponding opinion terms and the associated sentiment orientation. Recent efforts in exploiting span-level semantic interaction have shown superior performance on ASTE task. However, span-based approaches could suffer from excessive noise due to the large number of spans that have to be considered. To ease this burden, we propose a dual-channel span generation method to coherently constrain the search space of span candidates. Specifically, we leverage the syntactic relations among aspect/opinion terms and their part-of-speech characteristics to generate useful span candidates, which empirically reduces span enumeration by nearly a half. Besides, the interaction between syntactic and part-of-speech views brings relevant linguistic information to learned span representations. Extensive experiments on two public datasets demonstrate both the effectiveness of our design and the superiority on ASTE task 1.",
    "authors": [
        {
            "affiliations": [],
            "name": "Pan Li"
        },
        {
            "affiliations": [],
            "name": "Ping Li"
        },
        {
            "affiliations": [],
            "name": "Kai Zhang"
        }
    ],
    "id": "SP:09c14faa5d977b6cc75f93c3b13a9b849b3ca00f",
    "references": [
        {
            "authors": [
                "Xuefeng Bai",
                "Pengbo Liu",
                "Yue Zhang."
            ],
            "title": "Investigating typed syntactic dependencies for targeted sentiment classification using graph attention neural network",
            "venue": "IEEE ACM Trans. Audio Speech Lang. Process., 29:503\u2013514.",
            "year": 2021
        },
        {
            "authors": [
                "Gianni Brauwers",
                "Flavius Frasincar."
            ],
            "title": "A survey on aspect-based sentiment classification",
            "venue": "volume 55, pages 65:1\u201365:37.",
            "year": 2023
        },
        {
            "authors": [
                "Hongjie Cai",
                "Yaofeng Tu",
                "Xiangsheng Zhou",
                "Jianfei Yu",
                "Rui Xia."
            ],
            "title": "Aspect-category based sentiment analysis with hierarchical graph convolutional network",
            "venue": "COLING, pages 833\u2013843. International Committee on Computational Linguistics.",
            "year": 2020
        },
        {
            "authors": [
                "Mohna Chakraborty",
                "Adithya Kulkarni",
                "Qi Li."
            ],
            "title": "Open-domain aspect-opinion co-mining with doublelayer span extraction",
            "venue": "KDD, pages 66\u201375. ACM.",
            "year": 2022
        },
        {
            "authors": [
                "Chenhua Chen",
                "Zhiyang Teng",
                "Zhongqing Wang",
                "Yue Zhang."
            ],
            "title": "Discrete opinion tree induction for aspect-based sentiment analysis",
            "venue": "ACL (1), pages 2051\u20132064. Association for Computational Linguistics.",
            "year": 2022
        },
        {
            "authors": [
                "Hao Chen",
                "Zepeng Zhai",
                "Fangxiang Feng",
                "Ruifan Li",
                "Xiaojie Wang."
            ],
            "title": "Enhanced multi-channel graph convolutional network for aspect sentiment triplet extraction",
            "venue": "ACL, pages 2974\u20132985, Dublin, Ireland. Association for Computational Linguistics.",
            "year": 2022
        },
        {
            "authors": [
                "Hao Chen",
                "Zepeng Zhai",
                "Fangxiang Feng",
                "Ruifan Li",
                "Xiaojie Wang."
            ],
            "title": "Enhanced multi-channel graph convolutional network for aspect sentiment triplet extraction",
            "venue": "ACL (1), pages 2974\u20132985. Association for Computational Linguistics.",
            "year": 2022
        },
        {
            "authors": [
                "Shaowei Chen",
                "Yu Wang",
                "Jie Liu",
                "Yuelin Wang."
            ],
            "title": "Bidirectional machine reading comprehension for aspect sentiment triplet extraction",
            "venue": "AAAI, pages 12666\u201312674. AAAI Press.",
            "year": 2021
        },
        {
            "authors": [
                "Yuqi Chen",
                "Keming Chen",
                "Xian Sun",
                "Zequn Zhang."
            ],
            "title": "A span-level bidirectional network for aspect sentiment triplet extraction",
            "venue": "EMNLP, pages 4300\u2013 4309. Association for Computational Linguistics.",
            "year": 2022
        },
        {
            "authors": [
                "Zhexue Chen",
                "Hong Huang",
                "Bang Liu",
                "Xuanhua Shi",
                "Hai Jin."
            ],
            "title": "Semantic and syntactic enhanced aspect sentiment triplet extraction",
            "venue": "ACL/IJCNLP (Findings), volume ACL/IJCNLP 2021 of Findings of ACL, pages 1474\u20131483. Association for Computa-",
            "year": 2021
        },
        {
            "authors": [
                "Zhuang Chen",
                "Tieyun Qian."
            ],
            "title": "Enhancing aspect term extraction with soft prototypes",
            "venue": "EMNLP, pages 2107\u20132117. Association for Computational Linguistics.",
            "year": 2020
        },
        {
            "authors": [
                "Kyunghyun Cho",
                "Bart van Merrienboer",
                "\u00c7aglar G\u00fcl\u00e7ehre",
                "Dzmitry Bahdanau",
                "Fethi Bougares",
                "Holger Schwenk",
                "Yoshua Bengio."
            ],
            "title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation",
            "venue": "EMNLP, pages",
            "year": 2014
        },
        {
            "authors": [
                "Hongliang Dai",
                "Yangqiu Song."
            ],
            "title": "Neural aspect and opinion term extraction with mined rules as weak supervision",
            "venue": "ACL (1), pages 5268\u20135277. Association for Computational Linguistics.",
            "year": 2019
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: pre-training of deep bidirectional transformers for language understanding",
            "venue": "NAACL-HLT (1), pages 4171\u20134186. Association for Computational Linguistics.",
            "year": 2019
        },
        {
            "authors": [
                "Chunning Du",
                "Haifeng Sun",
                "Jingyu Wang",
                "Qi Qi",
                "Jianxin Liao",
                "Tong Xu",
                "Ming Liu."
            ],
            "title": "Capsule network with interactive attention for aspect-level sentiment classification",
            "venue": "EMNLP, pages 5488\u2013 5497. Association for Computational Linguistics.",
            "year": 2019
        },
        {
            "authors": [
                "Lei Gao",
                "Yulong Wang",
                "Tongcun Liu",
                "Jingyu Wang",
                "Lei Zhang",
                "Jianxin Liao."
            ],
            "title": "Question-driven span labeling model for aspect-opinion pair extraction",
            "venue": "AAAI, pages 12875\u201312883. AAAI Press.",
            "year": 2021
        },
        {
            "authors": [
                "Ruidan He",
                "Wee Sun Lee",
                "Hwee Tou Ng",
                "Daniel Dahlmeier."
            ],
            "title": "An interactive multi-task learning network for end-to-end aspect-based sentiment analysis",
            "venue": "ACL (1), pages 504\u2013515. Association for Computational Linguistics.",
            "year": 2019
        },
        {
            "authors": [
                "Mengting Hu",
                "Shiwan Zhao",
                "Li Zhang",
                "Keke Cai",
                "Zhong Su",
                "Renhong Cheng",
                "Xiaowei Shen."
            ],
            "title": "CAN: constrained attention networks for multi-aspect sentiment analysis",
            "venue": "EMNLP, pages 4600\u20134609. Association for Computational Linguistics.",
            "year": 2019
        },
        {
            "authors": [
                "Hao Li",
                "Wei Lu."
            ],
            "title": "Learning latent sentiment scopes for entity-level sentiment analysis",
            "venue": "AAAI, pages 3482\u20133489. AAAI Press.",
            "year": 2017
        },
        {
            "authors": [
                "Kun Li",
                "Chengbo Chen",
                "Xiaojun Quan",
                "Qing Ling",
                "Yan Song."
            ],
            "title": "Conditional augmentation for aspect term extraction via masked sequence-tosequence generation",
            "venue": "ACL, pages 7056\u20137066. Association for Computational Linguistics.",
            "year": 2020
        },
        {
            "authors": [
                "Qimai Li",
                "Zhichao Han",
                "Xiao-Ming Wu."
            ],
            "title": "Deeper insights into graph convolutional networks for semi-supervised learning",
            "venue": "Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, pages 3538\u20133545.",
            "year": 2018
        },
        {
            "authors": [
                "Ruifan Li",
                "Hao Chen",
                "Fangxiang Feng",
                "Zhanyu Ma",
                "Xiaojie Wang",
                "Eduard H. Hovy."
            ],
            "title": "Dual graph convolutional networks for aspect-based sentiment analysis",
            "venue": "ACL, pages 6319\u20136329. Association for Computational Linguistics.",
            "year": 2021
        },
        {
            "authors": [
                "Xin Li",
                "Lidong Bing",
                "Piji Li",
                "Wai Lam."
            ],
            "title": "A unified model for opinion target extraction and target sentiment prediction",
            "venue": "AAAI, pages 6714\u20136721. AAAI Press.",
            "year": 2019
        },
        {
            "authors": [
                "Jian Liu",
                "Zhiyang Teng",
                "Leyang Cui",
                "Hanmeng Liu",
                "Yue Zhang."
            ],
            "title": "Solving aspect category sentiment analysis as a text generation task",
            "venue": "EMNLP (1), pages 4406\u20134416. Association for Computational Linguistics.",
            "year": 2021
        },
        {
            "authors": [
                "Shu Liu",
                "Kaiwen Li",
                "Zuhe Li."
            ],
            "title": "A robustly optimized BMRC for aspect sentiment triplet extraction",
            "venue": "NAACL, pages 272\u2013278. Association for Computational Linguistics.",
            "year": 2022
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter."
            ],
            "title": "Fixing weight decay regularization in adam",
            "venue": "CoRR, abs/1711.05101.",
            "year": 2017
        },
        {
            "authors": [
                "Dehong Ma",
                "Sujian Li",
                "Fangzhao Wu",
                "Xing Xie",
                "Houfeng Wang."
            ],
            "title": "Exploring sequence-tosequence learning in aspect term extraction",
            "venue": "ACL, pages 3538\u20133547. Association for Computational Linguistics.",
            "year": 2019
        },
        {
            "authors": [
                "Yue Mao",
                "Yi Shen",
                "Chao Yu",
                "Longjun Cai."
            ],
            "title": "A joint training dual-mrc framework for aspect based sentiment analysis",
            "venue": "AAAI, pages 13543\u201313551. AAAI Press.",
            "year": 2021
        },
        {
            "authors": [
                "Haiyun Peng",
                "Lu Xu",
                "Lidong Bing",
                "Fei Huang",
                "Wei Lu",
                "Luo Si."
            ],
            "title": "Knowing what, how and why: A near complete solution for aspect-based sentiment analysis",
            "venue": "AAAI, pages 8600\u20138607. AAAI Press.",
            "year": 2020
        },
        {
            "authors": [
                "Jeffrey Pennington",
                "Richard Socher",
                "Christopher D. Manning."
            ],
            "title": "Glove: Global vectors for word representation",
            "venue": "EMNLP, pages 1532\u20131543. ACL.",
            "year": 2014
        },
        {
            "authors": [
                "Loukachevitch",
                "Evgeniy V. Kotelnikov",
                "N\u00faria Bel",
                "Salud Mar\u00eda Jim\u00e9nez Zafra",
                "G\u00fclsen Eryigit."
            ],
            "title": "Semeval-2016 task 5: Aspect based sentiment analysis",
            "venue": "SemEval@NAACL-HLT, pages 19\u201330. The Association for Computer Linguistics.",
            "year": 2016
        },
        {
            "authors": [
                "Maria Pontiki",
                "Dimitris Galanis",
                "Haris Papageorgiou",
                "Suresh Manandhar",
                "Ion Androutsopoulos."
            ],
            "title": "Semeval-2015 task 12: Aspect based sentiment analysis",
            "venue": "SemEval@NAACL-HLT, pages 486\u2013495. The Association for Computer Linguistics.",
            "year": 2015
        },
        {
            "authors": [
                "Maria Pontiki",
                "Dimitris Galanis",
                "John Pavlopoulos",
                "Harris Papageorgiou",
                "Ion Androutsopoulos",
                "Suresh Manandhar."
            ],
            "title": "Semeval-2014 task 4: Aspect based sentiment analysis",
            "venue": "SemEval@COLING, pages 27\u201335. The Association for Computer Linguis-",
            "year": 2014
        },
        {
            "authors": [
                "Peng Qi",
                "Yuhao Zhang",
                "Yuhui Zhang",
                "Jason Bolton",
                "Christopher D. Manning."
            ],
            "title": "Stanza: A python natural language processing toolkit for many human languages",
            "venue": "ACL (demo), pages 101\u2013108. Association for Computational Linguistics.",
            "year": 2020
        },
        {
            "authors": [
                "Martin Schmitt",
                "Simon Steinheber",
                "Konrad Schreiber",
                "Benjamin Roth."
            ],
            "title": "Joint aspect and polarity classification for aspect-based sentiment analysis with end-to-end neural networks",
            "venue": "EMNLP, pages 1109\u20131114. Association for Computational Linguis-",
            "year": 2018
        },
        {
            "authors": [
                "Kim Schouten",
                "Flavius Frasincar."
            ],
            "title": "Survey on aspect-level sentiment analysis",
            "venue": "volume 28, pages 813\u2013830.",
            "year": 2016
        },
        {
            "authors": [
                "Duyu Tang",
                "Bing Qin",
                "Ting Liu."
            ],
            "title": "Aspect level sentiment classification with deep memory network",
            "venue": "EMNLP, pages 214\u2013224. The Association for Computational Linguistics.",
            "year": 2016
        },
        {
            "authors": [
                "Hao Tang",
                "Donghong Ji",
                "Chenliang Li",
                "Qiji Zhou."
            ],
            "title": "Dependency graph enhanced dual-transformer structure for aspect-based sentiment classification",
            "venue": "ACL, pages 6578\u20136588. Association for Computational Linguistics.",
            "year": 2020
        },
        {
            "authors": [
                "Maria Mihaela Trusca",
                "Flavius Frasincar."
            ],
            "title": "Survey on aspect detection for aspect-based sentiment analysis",
            "venue": "volume 56, pages 3797\u20133846.",
            "year": 2023
        },
        {
            "authors": [
                "Hai Wan",
                "Yufei Yang",
                "Jianfeng Du",
                "Yanan Liu",
                "Kunxun Qi",
                "Jeff Z. Pan."
            ],
            "title": "Target-aspect-sentiment joint detection for aspect-based sentiment analysis",
            "venue": "AAAI, pages 9122\u20139129. AAAI Press.",
            "year": 2020
        },
        {
            "authors": [
                "Wenya Wang",
                "Sinno Jialin Pan",
                "Daniel Dahlmeier",
                "Xiaokui Xiao."
            ],
            "title": "Coupled multi-layer attentions for co-extraction of aspect and opinion terms",
            "venue": "AAAI, pages 3316\u20133322. AAAI Press.",
            "year": 2017
        },
        {
            "authors": [
                "Yequan Wang",
                "Minlie Huang",
                "Xiaoyan Zhu",
                "Li Zhao."
            ],
            "title": "Attention-based LSTM for aspectlevel sentiment classification",
            "venue": "EMNLP, pages 606\u2013 615. The Association for Computational Linguistics.",
            "year": 2016
        },
        {
            "authors": [
                "Zhen Wu",
                "Chengcan Ying",
                "Fei Zhao",
                "Zhifang Fan",
                "Xinyu Dai",
                "Rui Xia."
            ],
            "title": "Grid tagging scheme for aspect-oriented fine-grained opinion extraction",
            "venue": "Findings-EMNLP, pages 2576\u20132585, Online. Association for Computational Linguistics.",
            "year": 2020
        },
        {
            "authors": [
                "Zhen Wu",
                "Fei Zhao",
                "Xin-Yu Dai",
                "Shujian Huang",
                "Jiajun Chen."
            ],
            "title": "Latent opinions transfer network for target-oriented opinion words extraction",
            "venue": "AAAI, pages 9298\u20139305. AAAI Press.",
            "year": 2020
        },
        {
            "authors": [
                "Lu Xu",
                "Yew Ken Chia",
                "Lidong Bing."
            ],
            "title": "Learning span-level interactions for aspect sentiment triplet extraction",
            "venue": "ACL, pages 4755\u20134766. Association for Computational Linguistics.",
            "year": 2021
        },
        {
            "authors": [
                "Lu Xu",
                "Hao Li",
                "Wei Lu",
                "Lidong Bing."
            ],
            "title": "Position-aware tagging for aspect sentiment triplet extraction",
            "venue": "EMNLP, pages 2339\u20132349. Association for Computational Linguistics.",
            "year": 2020
        },
        {
            "authors": [
                "Wei Xue",
                "Tao Li."
            ],
            "title": "Aspect based sentiment analysis with gated convolutional networks",
            "venue": "ACL, pages 2514\u20132523. Association for Computational Linguistics.",
            "year": 2018
        },
        {
            "authors": [
                "Hang Yan",
                "Junqi Dai",
                "Tuo Ji",
                "Xipeng Qiu",
                "Zheng Zhang."
            ],
            "title": "A unified generative framework for aspect-based sentiment analysis",
            "venue": "ACL/IJCNLP (1), pages 2416\u20132429. Association for Computational Linguistics.",
            "year": 2021
        },
        {
            "authors": [
                "Bishan Yang",
                "Claire Cardie."
            ],
            "title": "Extracting opinion expressions with semi-markov conditional random fields",
            "venue": "EMNLP, pages 1335\u20131345. ACL.",
            "year": 2012
        },
        {
            "authors": [
                "Bishan Yang",
                "Claire Cardie."
            ],
            "title": "Joint inference for fine-grained opinion extraction",
            "venue": "ACL, pages 1640\u20131649. The Association for Computer Linguistics.",
            "year": 2013
        },
        {
            "authors": [
                "Yichun Yin",
                "Furu Wei",
                "Li Dong",
                "Kaimeng Xu",
                "Ming Zhang",
                "Ming Zhou."
            ],
            "title": "Unsupervised word and dependency path embeddings for aspect term extraction",
            "venue": "IJCAI, pages 2979\u20132985. IJCAI/AAAI Press.",
            "year": 2016
        },
        {
            "authors": [
                "Zepeng Zhai",
                "Hao Chen",
                "Fangxiang Feng",
                "Ruifan Li",
                "Xiaojie Wang."
            ],
            "title": "COM-MRC: A contextmasked machine reading comprehension framework for aspect sentiment triplet extraction",
            "venue": "EMNLP, pages 3230\u20133241. Association for Computational",
            "year": 2022
        },
        {
            "authors": [
                "Chen Zhang",
                "Qiuchi Li",
                "Dawei Song",
                "Benyou Wang."
            ],
            "title": "A multi-task learning framework for opinion triplet extraction",
            "venue": "EMNLP (Findings), volume EMNLP 2020 of Findings of ACL, pages 819\u2013828. Association for Computational Linguistics.",
            "year": 2020
        },
        {
            "authors": [
                "He Zhao",
                "Longtao Huang",
                "Rong Zhang",
                "Quan Lu",
                "Hui Xue."
            ],
            "title": "Spanmlt: A span-based multi-task learning framework for pair-wise aspect and opinion terms extraction",
            "venue": "ACL, pages 3239\u20133248. Association for Computational Linguistics.",
            "year": 2020
        },
        {
            "authors": [
                "Yichun Zhao",
                "Kui Meng",
                "Gongshen Liu",
                "Jintao Du",
                "Huijia Zhu."
            ],
            "title": "A multi-task dual-tree network for aspect sentiment triplet extraction",
            "venue": "COLING, pages 7065\u20137074. International Committee on Computational Linguistics.",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Aspect Sentiment Triplet Extraction (ASTE) is a compound task in fine-grained Aspect-Based Sentiment Analysis (ABSA) (Pontiki et al., 2014). It is composed of three fundamental subtasks: Aspect Term Extraction (ATE) (Yin et al., 2016; Ma et al., 2019; Chen and Qian, 2020; Li et al., 2020), Opinion Term Extraction (OTE) (Yang and Cardie, 2012, 2013; Wan et al., 2020) and Aspect Sentiment Classification (ASC) (Wang et al., 2016; Tang et al., 2016; Xue and Li, 2018; Tang et al., 2020; Li et al., 2021). In particular, ASTE aims to extract the sentiment triplet of aspect terms, corresponding opinion terms and their associated sentiment polarity in a given sentence. For example, in\n\u2217Corresponding author: dping.li@gmail.com 1We release our code at https://github.com/\nbert-ply/Dual_Span\nthe sentence \"My vegetable risotto was burnt, and infused totally in a burnt flavor\", there are two sentiment triplets, namely, (\u201cvegetable risotto\u201d, \u201cburnt\u201d, Negative\u201d) and (\u201cflavor\u201d, \u201cburnt\u201d, Negative\u201d), where \u201cvegetable risotto\u201d and \u201cflavor\u201d are aspect terms, \u201cburnt\u201d is the opinion term corresponding to the aspect of interest, and \u201cNegative\u201d is the sentiment polarity of these two triplets.\nWhen the idea of ASTE was first proposed, a two-stage pipeline method (Peng et al., 2020) was developed for this task. However, staged processing scheme often lead to error propagation between subtasks. More than that, opinion terms are generally associated with the aspect target, staged pipeline method breaks this interaction. To address those issue, some end-to-end approaches (Wu et al., 2020a; Xu et al., 2021; Chen et al., 2021b, 2022b) are devised, which attempt to simultaneously extract aspect-opinion pairs and perform sentiment classification by introducing novel tagging schemes. In particular, most of existing end-toend models (Wu et al., 2020a; Chen et al., 2021b, 2022b) build the interaction between aspect and its corresponding opinion at token-level, i.e., word-toword interactions. Despite of its efficacy, it is hard to guarantee the consistency of predicted sentiment polarity between multiple word-to-word pairs when many aspects/opinions are expressed using multiple words. On account of this, recent work (Xu et al., 2021; Chen et al., 2022d) adopt span-level interactions in the sentiment triplet structure. Compared with the token-level pairing, span-level in-\nteraction is proved to bring significant gains to the model.\nHowever, one prominent problem with spanbased methods is that they usually enumerate all spans in a sentence, which will bring about high computational cost and many noises. Specifically, the number of enumerated spans for a sentence of length n is O(n2), while the number of possible interactions between all opinion and aspect candidate spans is O(n4) at the later span-pairing stage, implying a lot of invalid aspect/opinion spans and span pairs. Moreover, most of the existing spanbased methods model direct interactions between two spans. The high-order interactions are generally overlooked.\nTo address those issues, we explore the linguistic phenomena in the spans. Our observations are two-fold: First, multiple words composed of the span of an aspect/opinion target are generally syntactically dependent, and multiple dependency relations can transmit higher-order interactions between spans. For example, in Figure 1, the aspect term \u201cvegetable risotto\u201d has an intra-span syntactic dependency \u201ccompound\u201d and an inter-span dependency \u201cnsubj\u201d with \u201cburnt\u201d. On the other hand, the span \u201cflavor\u201d is indirectly related to the first \u201cburnt\u201d (i.e., the one associated with \u201cvegetable risotto\u201d) within 2 hops in the syntactic tree. This indirect relation may suggest the relevance of the sentiment polarity of the two span pairs, namely, (\u201cvegetable risotto\u201d, \u201cburnt\u201d) and (\u201cflavor\u201d, \u201cburnt\u201d). In effect, the ground truth of sentiment polarity of these two aspect-opinion pairs are the same, as shown in Figure 1.\nSecond, we also observe that there are some frequent patterns in aspect and opinion spans in terms of part-of-speech. For instance, in many cases aspect terms are noun or noun phrase which we refer to as NN and (NN \u2212NN), respectively. Moreover, it is fairly common that opinion terms are adjective (denoted by JJ). As shown in Figure 1, the aspect \u201cvegetable risotto\u201d has the part-ofspeech structure (NN \u2212NN), and opinion term \"burnt\u201d is JJ . Therefore, it is possible to extract the aspect/opinion spans according to the lexical characteristics of the words so as to avoid enumerating all word combination.\nMotivated by the two observations, we propose a dual-channel span generation approach for aspectlevel sentiment triplet extraction, which we term as Dual-Span. Dual-Span utilizes two relational\ngraph attention networks (RGAT) to separately learn high-order syntactic dependency between words/spans and linguistic features in constructed part-of-speech relations among words. Then a gating mechanism is adopted to fuse the syntactic and lexical information of span candidates, which helps to enhance the feature representation of spans. On the other hand, instead of enumerating all possible spans, the span candidates are extracted from two channels, i.e., the syntactic dependency relations and part-of-speech based relations, thus largely reducing the noisy information in favor of valid span pairing.\nOur main contributions are as follows:\n\u2022 We devise a dual-channel span generation method for aspect sentiment triplet extraction, which produces a span candidate set much smaller than the greedily enumerated one by leveraging the syntactic dependency and partof-speech correlation among tokens/spans in a dual-channel manner.\n\u2022 We construct the intra-span and inter-span relations based on the part-of-speech correlation of spans/words, on top of which the high-order linguistic interactions is able to be captured by relational graph neural networks.\n\u2022 We combine the syntactic information learned from dependency tree with the part-of-speech information learned from constructed lexical relations to enrich span representation. We conduct extensive experiments on benchmark datasets to evaluate the efficacy and efficiency of the proposed method. The experimental results show that our model Dual-Span outperforms all state-of-the-art methods on the ASTE task."
        },
        {
            "heading": "2 Related Work",
            "text": "Aspect-based sentiment analysis (ABSA) (Pontiki et al., 2014; Schouten and Frasincar, 2016; Xue and Li, 2018; Chen et al., 2022a; Trusca and Frasincar, 2023) is fine-grained sentiment analysis. The early work of ABSA was to identify its three sentiment elements (i.e., aspect, opinion, sentiment polarity) as basic tasks: ATE (e.g., (Yin et al., 2016; Ma et al., 2019; Chen and Qian, 2020; Li et al., 2020), OTE (Yang and Cardie, 2012, 2013; Wan et al., 2020)) and ASC (e.g., (Wang et al., 2016; Tang et al., 2016; Xue and Li, 2018; Du et al., 2019;\nLi et al., 2021; Brauwers and Frasincar, 2023)). Subsequently, some studies began to consider multiple sentiment element composite tasks in order to better understand fine-grained sentiment analysis: aspect term polarity co-extraction (APCE) (Li and Lu, 2017; He et al., 2019; Li et al., 2019), AspectOpinion Pair Extraction (AOPE) (Zhao et al., 2020; Wu et al., 2020a; Gao et al., 2021; Chakraborty et al., 2022) and Aspect Category Sentiment Analysis (ACSA) (Schmitt et al., 2018; Hu et al., 2019; Cai et al., 2020; Liu et al., 2021).\nSome recent works started to consider the integrity among the three sentiment elements and thus proposed the ASTE task. A diversity of techniques were proposed for it: two-stage pipeline (Peng et al., 2020), multi-task unified framework (Li et al., 2019; Zhang et al., 2020; Yan et al., 2021), multi-round machine reading comprehension method (Mao et al., 2021; Chen et al., 2021a; Liu et al., 2022) and end-to-end method (Wu et al., 2020a; Xu et al., 2020; Chen et al., 2021b, 2022c; Xu et al., 2021; Chen et al., 2022d). The span-level based approaches adopt end-to-end implementation. For instance, SpanASTE (Xu et al., 2021) enumerates aspect and viewpoint spans and directly exploits their interaction to solve ASTE tasks, while SBN (Chen et al., 2022d) proposed a span-level bidirectional network that enumerates all possible spans as input, and completes the ASTE task by designing two decoders and adopting inference strategies. Despite that, it still remains an open challenge to improve the search efficiency and feature representation for\nthe span of sentiment triplets."
        },
        {
            "heading": "3 Proposed Framework",
            "text": "In this section, the overall architecture of our proposed model Dual-Span is shown in Figure 2, which consists of four main components: sentence encoding, feature enhancing module, dual-channel span generation and triplet module."
        },
        {
            "heading": "3.1 Task Definition",
            "text": "For a sentence X = {w1, w2, . . . , wn} of length n, the ASTE task is to extract the set of aspect sentiment triplets T = {(a, o, s)m}|T |m=1 from the given sentence X , where a, o and s \u2208 {POS,NEU,NEG} represent the aspect term, opinion term and sentiment polarity, respectively. |T | is the number of sentiment triplets contained sentence X ."
        },
        {
            "heading": "3.2 Sentence Encoding",
            "text": "To obtain contextual representations for each word, we explore two sentence encoding methods, namely, BiLSTM and BERT.\nBiLSTM We first use the GloVe (Pennington et al., 2014) embedding to get the embedding matrix E \u2208 R|V |\u2217dw of the corpus, where |V | represents the vocabulary size, and ds represents the embedding dimension. For the embedding tokens Ex = {e1, e2, . . . , en} in the sentence, we use BiLSTM to get its hidden representation H = {h1, h2, . . . , hn}, where h \u2208 R2dn is obtained by splicing the hidden state \u2192 h \u2208 Rdn generated by\nforward LSTM and the hidden state \u2190 h \u2208 Rdn generated by backward LSTM:\nh = [ \u2192 h ; \u2190 h ] (1)\nBERT An alternative approach is to utilize BERT (Devlin et al., 2019) as the sentence encoder to generate contextualized word representations. Given a sentence X = {w1, w2, . . . , wn} with n words, the hidden representation sequence H = {h1, h2, . . . , hn} is the output of the encoding layer of BERT at the last transformer block."
        },
        {
            "heading": "3.3 Feature Enhancing Module",
            "text": "As aforementioned, spans (or intra-span words) involve syntactical dependency and part-of-speech correlation, therefore incorporating those information into feature representations can be beneficial for span pairing and sentiment prediction. To capture the high order dependency relations, here we devise a graph neural network based method to encode the syntactic dependency and part-of-speech relations of intra- and inter-spans in high orders. In particular, we construct the part-of-speech relational graph (corresponding to a multi-relation matrix as shown in Figure 3 (b)). Then we apply two relational graph attention networks to learn the high order interactions between words on syntactic dependency tree of the sentence in question and constructed part-of-speech graph, respectively."
        },
        {
            "heading": "3.3.1 Part-of-speech Graph Construction",
            "text": "The goal of part-of-speech graph construction is to characterize the word formation patterns of aspect and opinion terms so as to better identify the possible spans. Specifically, we adopt the following three rules to construct the part-of-speech graph GPos = (V,RPos) of a given sentence X . First, following previous work (Chakraborty et al., 2022), assuming that aspect terms are usually nouns and opinion terms are usually adjectives, we can define part-of-speech relations based on part-of-speech tags NN or JJ . In particular, we consider the relations between words in a given window that contains words tagged with NN or JJ . Therefore, a relational edge RPosi,j of G\nPos is defined for two words i and j as the combination of part-of-speech tags of the two words, whose representation vector is rpi,j \u2208 Rdp , where dp is the dimension of part-of-speech combination embedding. Besides, we consider the special syntactic relation nsubj, since opinion terms are usually directly used to\nmodify aspect terms, leading to better extraction of aspect-opinion pairs. Finally, for each word\u2019s partof-speech, we add a self-loop relational edge to itself, as the diagonal elements shown in Figure 3.\nOn the other hand, the syntactic dependency graph GSyn = (V,RSyn) is constructed according to the dependency parsing tree, where edges are represented by syntactic relation types. Moreover, we define the self-dependency for each word. So for a given sentence of length n, the syntactic relation between words wi and wj is denoted as R Syn i,j , whose corresponding vectorization representation is denoted as the vector rsi,j \u2208 Rds , where ds is the dimension of syntactic relation embeddings."
        },
        {
            "heading": "3.3.2 High-order Feature Learning with Relational Graph Attention Network",
            "text": "Next, we use relational graph attention networks (RGAT) to capture the multiple types of linguistic features and high-order interaction between spans/words on syntactic dependency graph and part-of-speech graph, respectively. Moreover, we use two graph attentional network based modules, namely, SynGAT and PosGAT to learn syntactic dependency graphs and part-of-speech graphs, respectively, which will distinguish between various syntactic relationships and part-of-speech relationships when calculating the attention weight between nodes. In particular, following previous work (Bai et al., 2021), we denote two specific relations on each edge by rsi,j and r p i,j , respectively. Specifically, for the i\u2212 th node, the update process is as follows:\nhsyni (l) = \u2225 Z z=1\u03c3  \u2211 j\u2208N (i) \u03b1\u0302lzi,j ( W lzs1h syn j (l \u2212 1) +W l s2r syn i,j ) (2) hposi (l) = \u2225 Z z=1\u03c3  \u2211 j\u2208N (i) \u03b2\u0302lzi,j ( W lzp1h pos j (l \u2212 1) +W l p2r pos i,j ) (3)\nwhere W ls2 \u2208 R d z \u00d7d and W lp2 \u2208 R d z \u00d7d are parameter matrices. z denotes the number of attention heads, and \u03c3 is the sigmoid activation. Ni is the set of immediate neighbors of node i. \u03b1\u0302(lz)i,j , \u03b2\u0302 (lz) i,j are the normalized attention coefficients for the z-th head at the l-th layer.\nTo fuse syntactic dependency and part-of-speech relation features, we introduce a gating mechanism (Cho et al., 2014) to merge the two views as follows:\ng = \u03c3 (Wg [h syn : hpos] + bg) (4)\nh = g \u25e6 hsyn + (1\u2212 g) \u25e6 hpos (5)\nwhere \u25e6 is element-wise product operation. [hsyn : hpos] is the concatenation of hsyn and hpos, and Wg and bg are model parameters. This way, g is learned to optimize the feature fusion."
        },
        {
            "heading": "3.4 Dual-Channel Span Generation",
            "text": "In this section, we propose a dual-channel span generation module, which consists of two parts: dual-channel span generation and span classification."
        },
        {
            "heading": "3.4.1 Dual-Channel Span Generation",
            "text": "Syntactic Span Generation Given a sentence X whose syntactic dependency graph is GSyn = (V,RSyn), if there is a dependency edge eij between words wi and wj , then all words positioned between them are considered to be a span ssyni,j . In particular, self-dependent edges represents spans of length Ls = 1. We define the representation of ssyni,j as follows\nssyni,j = [hi : hj : fwidth(i, j)], if ei,j = 1 (6)\nwhere fwidth(i, j) denotes trainable embedding of span length (i.e., j \u2212 i+ 1 ). ei,j = 1 suggests that there is an edge between wi and wj .\nPart-of-speech Span Generation For a given sentence X = {w1, w2, . . . , wn}, if the part-ofspeech tag of word wo is NN or JJ , the words in a predefined window will be exhaustively enumerated and then the enumeration is further combined with central word wo to form spans. The part-ofspeech induced span sposk,l can be represented as:\nsposk,l =[hk : hl : fwidth(k, l)],\nif poso = NN or JJ , and o \u2208 [k, l] (7)\nwhere fwidth(k, l) refers to the trainable embedding of span length.\nFinally, we merge the two types of span candidates: S = ssyni,j \u222a s pos k,l .\nCompared to exhaustive enumeration on the whole sentence in previous span-based approaches, whose time complexity of enumerated spans is O(n2), for a sentence of length n. However, in our syntactic span generation, the parsing tree containing 2n edge dependencies (Qi et al., 2020) (including self-dependent edges), so the number of generated spans is O(2n). On the other hand, the statistics shows that in the benchmark datasets, there are about 2.5 part-of-speech NN and JJ in each sentence on average. Therefore, in the part-of-speech span generation procedure, the number of span candidates is O(2.5Swindow(Swindow \u2212 1)) \u2264 n, where Swindow is the window size to restrict span length and generally set to be a small value (e.g., Swindow = 3 in our experiments). That is, the time complexity of our method to generate the span is O(n), which significantly reduce the span candidate size."
        },
        {
            "heading": "3.4.2 Span Classification",
            "text": "After obtaining the span candidates S, we further narrow down the pool of possible spans by leveraging two auxiliary tasks, namely, ATE and OTE tasks. Specifically, all span candidates in S will be classified into one of the three categories:{Aspect, Opinion, Invalid} by a span classifier. Next, nz spans are singled out with higher prediction scores \u03a6aspect or \u03a6opinion, where z is the threshold hyper-parameter and \u03a6aspect and \u03a6opinion are obtained by\n\u03a6aspect(si,j) = softmax (FFNNt=aspect (si,j)) (8)\n\u03a6opinion(si,j) = softmax (FFNNt=opinion (si,j)) (9)\nwhere FFNN denotes a feed-forward neural network with non-linear activation."
        },
        {
            "heading": "3.5 Triplet Module",
            "text": "Based on the shrinked candidate pool of aspect and opinion terms, the aspect candidate saa,b \u2208 Sa and opinion candidate soc,d \u2208 So are paired and represented as\ngsaa,b,s o c,d = [ saa,b : s o c,d : r s ab,cd : fdistance(a, b, c, d) ] . (10) where fdistance(a, b, c, d) denotes trainable embeddings of span length. rsab,cd is a trainable embedding vector which is the average pooling of the dependency vectors between words ab\nand cd. Additionally, since opinions are more likely to modify the aspects that match them, we consider the dependency relationship rsab,cd \u2208 RSyn between them. Then, sentiment classification is performed for the obtained span pairs, where the sentiment types are defined as r \u2208 R = {Positive, Negative, Neutral, Invalid} Formally, the triplet prediction is written as\nP ( r | saa,b, soc,d ) = softmax ( FFNNr ( gsaa,b,s o c,d )) (11)"
        },
        {
            "heading": "3.6 Training objective",
            "text": "The loss function for training is defined as the sum of the negative log-likelihoods from the span-pair classification in the span-classification and triplets modules:\nL =\u2212 \u2211\nsi,j\u2208S logP\n( t\u0302i,j | si,j ) \u2212\n\u2211 sta,b\u2208Sa,s o c,d\u2208So logP ( r\u0302 | saa,b, soc,d ) (12)\nwhere t\u0302i,j and r\u0302i,j are the ground truth labels for span si,j and span-pair (saa,b, s o c,d), respectively. S, Sa and So are the final span representation, pruned aspect and opinion candidate pools in dual-channel span generation, respectively."
        },
        {
            "heading": "4 Experiments",
            "text": ""
        },
        {
            "heading": "4.1 Datasets",
            "text": "To verify the effectiveness of our proposed model, we conduct experiments on four public datasets, i.e., Lap14, Res14, Res 15 and Res16, which come from the sentiment evaluation benchmarks SemEval 2014 (Pontiki et al., 2014), SemEval 2015 (Pontiki et al., 2015) and SemEval 2016 (Pontiki et al., 2016), respectively. Moreover, the four datasets have two versions: ASTE-Data-v1(D1 for short) (Peng et al., 2020) and ASTE-Data-v2(D2 for short) (Xu et al., 2020). Statistics for public datasets are shown in the appendix A.1."
        },
        {
            "heading": "4.2 Experimental Setting",
            "text": "We initialize word embedding with two different encoders: BiLSTM-based and BERT-based encoders. The hidden dimension of BiLSTM-based encoder is set to 300 with dropout rate 0.5. To alleviate overfitting, the input embedding dropout rate is 0.7. For the proposed model, we use the AdamW optimizer (Loshchilov and Hutter, 2017) with a\nlearning rate of 1e-3 in the training. In the implementation of BERT-based encoding, the model parameters are optimized using Adamw with a maximum learning rate of 5e-5 and weight decay of 1e-2. We run the model for 20 training epochs. For other parameter groups, the same parameter settings are used for both embedding initialization schemes. The maximum span length Ls is fixed to 8, the span pruning threshold z is set to 0.5, and the part-of-speech window Swindow is 3. We choose the best model parameters based on the F1 score on the validation set and report the average of the results for 5 different random seeds."
        },
        {
            "heading": "4.3 Baselines",
            "text": "We compare our model to the following state-ofthe-art methods:\n\u2022 Pipeline: including CMLA+ (Wang et al., 2017), RINANTE+ (Dai and Song, 2019), Li-unified-R (Li et al., 2019), Peng-twostage (Peng et al., 2020) and IMN+IOG (Wu et al., 2020b).\n\u2022 End-to-end: OTE-MTL (Zhang et al., 2020), JET (Xu et al., 2020), GTS-CNN, GTSBiLSTM, GTS-BERT (Wu et al., 2020a), S3E2 (Chen et al., 2021b), BART-ABSA (Yan et al., 2021), MTDTN (Zhao et al., 2022), EMC-GCN (Chen et al., 2022c). These approaches are end-to-end models that include a unified grid tagging scheme and a positionaware tagging scheme.\n\u2022 MRC: Dual-MRC (Mao et al., 2021), BMRC (Chen et al., 2021a), COMMRC (Zhai et al., 2022). All these method are based on the framework of machine reading comprehension.\n\u2022 Span-based: Span-ASTE (Xu et al., 2021), SBN (Chen et al., 2022d). Span-based models consider all possible spans in a sentence and match aspect terms with opinion terms in an end-to-end manner."
        },
        {
            "heading": "4.4 Main Results",
            "text": "We conduct experiments on the two versions of four benchmark datasets, i.e., D1 and D2, whose results are shown in Table 1 and Table 2, respectively. As can be seen from the two tables, under the comprehensive performance indicator F1, the proposed Dual-span consistently outperforms\nall baselines both for BiLTSM encoder and BERT encoder. Moreover, our model achieves the superior performance in precision and/or recall in most cases. On the other side, the experimental results suggest that non-pipeline methods (i.e., End-to-end, MRC-based, Span-based) are better than pipeline methods, which should be attributed to the fact that the pipeline methods do not consider the correlation between sentiment elements, thus leading to error propagation between stages. It is noteworthy that among tagging based end-to-end methods, some methods that employ syntactic structure of the sentence such as S3E2, MTDTN and EMCGCN generally outperform the methods that only learn tagging information (e.g., OTE-MTL, GTS and JET), suggesting that the syntactic features of sentences are meaningful for triplet representation. In particular, our end-to-end Dual-Span model outperforms all end-to-end based methods and spanbased methods Span-ASTE, SBN, which can be attributed to the fact that our method not only uti-\nlizes the syntactic relationship and other linguistic features of sentences for span representation learning, but reduce the noise for span generation and pairing, which can facilitate valid span pairing. Specifically, the F1 score of Dua-Span on datasets D1 and D2 outperforms over other state-of-the-art models by about 2% on average."
        },
        {
            "heading": "4.5 Model Analysis",
            "text": ""
        },
        {
            "heading": "4.5.1 Ablation Study",
            "text": "To further explore the effectiveness of different modules in Dual-Span, we conduct ablation experiments on the D2 dataset. Table 3 shows the experimental results in terms of F1 scores in D2. W/o SynGAT and w/o PosGAT denote the removal of syntactic graph attention network (SynGAT) and part-of-speech graph attention network (PosGAT), respectively, while W/o Dual-RGAT denotes the removal of both SynGAT and PosGAT. We also compare our approach with unitary graph attention networks Dual-GAT that performs attention convolution on syntactic dependency graphs and part-of-speech graphs respectively without distinguishing edge types. By comparing w/o SynGAT, w/o PosGAT, w/o Dual-RGAT and Dual-Span, we observe that both the dependency relationship and part-of-speech features of the sentence are informative to the representation of spans. In particular, the syntactic structural feature and part-of-speech information can be complementary. This is manifested by the outperformance of Dual-RGAT over Transformer and Dual-GAT.When removing the syntactic span generation module (corresponding to w/o SynSpan) or part-of-speech span generation module (corresponding to w/o PosSpan), the performance is also degraded. This observation illustrates that span candidate size can be effectively reduced. Overall, each module of our Dual-span contributes to the overall performance of the ASTE task."
        },
        {
            "heading": "4.5.2 Effectiveness of Dual-Span in Span Generation",
            "text": "We use two subtasks, namely, ATE and OTE of ABSA, to explore the effectiveness of dual-channel span Generation strategy. We evaluate our model\non the D2 dataset with F1 metric and the results are shown in the table 4. On ATE task, Dual-Span is consistently superior to Span-ASTE and GTS, indicating that syntactic and part-of-speech correlation based candidate reduction and representation are effective for aspect term identification. However, on the OTE task, our model is slightly inferior to Span-ASTE on most of the benchmark datasets, which is caused by lower P values. We expect the reason behind lies in the part-of-speech based span generation. In effect, we only consider the spans involving words that are tagged with JJor NN . However, opinion terms can be tagged with V BN , which we do not include. We leave the expansion of more valid part-of-speech spans in the future work.\nIn order to verify that our proposed dual-channel span generation strategy can noticeably reduce the computational cost of span enumeration, we test the time consumption of Dual-span and Span-ASTE on span enumeration under the same runtime environment. From the results shown in Table 5, we can see that the proposed dual-channel span generation strategy cuts time cost in half."
        },
        {
            "heading": "5 Conclusion",
            "text": "In this work, we present a Dual-Span model for improving the performance on ASTE task. Based on the observations of syntactic relations and partof-speech features among spans, we design a dualchannel span generation method to refine the span candidate set so as to mitigate the negative impacts of invalid spans. Moreover, we employ relational graph neural networks to capture the high-order interactions between possible spans from both views: syntactic dependency relation and part-of-speech relation. Our experimental results demonstrate that the proposed method brings meaningful gains to ASTE as well as ATE task, compared to all baselines. We also note that for OTE task, our method is generally inferior to the vanilla span-based method that enumerate all possible spans. The reason may lie in the limited part-of-speech relations, which will be considered in the future work."
        },
        {
            "heading": "Acknowledgements",
            "text": "This work is supported by National Nature Science Foundation (NO.62276099) and SWPU Innovation Foundation (NO.642)."
        },
        {
            "heading": "A Appendix",
            "text": "A.1 Dataset statistics\nWe counted the data of the two versions separately 2 3. As shown in the Table 6, in addition to the number of sentences and triples in the dataset, the number of words and multi-word spans is also counted. In addition, we perform part-of-speech statistics on the D1 and D2 data sets. The results are shown in Table 7. The number of parts of speech with JJ or NN accounts for a relatively high proportion of the overall A&O, and the distribution of the rest of the parts of speech is scattered and unrepresentative. As there is a trade-off between prediction accuracy and time consumption, we only consider spans involving words tagged with JJ or NN in Section 4.5.2.\nA.2 Hyperparameter analysis\nFigure 4 shows the sensitivity analysis of the hyperparameters Swindow, Ls, on the D2 dataset. From the figure, we can observe that the effect is the best when the part-of-speech window Swindow is 3. In fact, when the part-of-speech window is set to Swindow = 3, it can basically cover all aspect terms and opinion terms whose parts-of-speech are NN and JJ . When Swindow exceeds 3, more noise and complexity may be introduced. When the hyperparameter Ls=8, the performance is the best.\nA.3 Impact of SynGAT and PosGAT Layers\nTo explore the impact of the number of layers of SynGAT and PosGAT in Dual-Span, we evaluate the number of layers of SynGAT and PosGAT on the D2 dataset, where multiple layers indicate that node information can be propagated to higher-order neighbors. As shown in Figure 6, our model achieves the best performance when both SynGAT and PosGAT are two layers. Specifically, on the syntactic dependency tree of a sentence, 2-hops are helpful for the interaction between aspect and opinion terms, while on the part-of-speech graph, 2-hop relations involving NN or JJ are conducive for capturing valid spans. Note that, the performance declines as the Dual-RGAT goes deeper, which may be due to the oversmoothing (Li et al., 2018) of graph neural networks.\n2https://github.com/xuuuluuu/ SemEval-Triplet-data\n3https://github.com/xuuuluuu/ SemEval-Triplet-data/tree/master/ ASTE-Data-V2-EMNLP2020\nA.4 Visualization of Linguistic Correlations\nTo explore how the syntactic dependency and partof-speech correlation between words contribute to valid span generation, we visualize the attention scores of the syntactic relations and part-of-speech adjacency matrix in two RGAT modules, where rows are queries and columns are keys. As shown in the figure 6, the sampled text \u201cGood creative rolls !\u201d (i.e., the fourth example in the section A.5) contains the triplet: (\u201crolls\u201d, \u201cgood creative\u201d, \u201cpositive\u201d). Since our model employs syntactic dependency relation to learn representations, and exploits the part-of-speech information around indicative words (e.g., the words with \u201cNN\" tag) as well, the inter-span and intra-span relations can be success-\nfully captured by graph attention networks. So the aspect term \u201crolls\u201d pays attention to opinion terms \u201cgood\" and \u201c creative\" (the last row of the left plot in Figure 6), while the two words with \u201cJJ\" tag, i.e., \u201cgood\" and \u201c creative\" shows more strong correlation than with \u201crolls\" in part-of-speech graph (corresponding to the right plot of Figure 6), demonstrating that they are more likely to fall into the same category. As a result, our model gives the correct triplet, in contrast to Span-ASTE whose prediction is (\u201ccreative rolls\u201d, \u201cgood\u201d, \u201cpositive\u201d).\nA.5 Case Study\nWe use several examples from the test set of dataset D2 to analyze and validate our method, as shown in the Table 8. For the first example, our method Dual-Span may perform better in predicting sentiment consistency than Span-ASTE. On the 2nd,\n5th, and 6th examples, it can be shown that our method makes full use of syntactic and semantic information to improve the accuracy of effective span capture. It can be shown in the fourth example that our method reduces the span boundary error by using part-of-speech structural features.\nA.6 Error analysis In order to explore the reason behind the slight inferiority of our model on the OTE task, compared to Span-ASTE on most benchmark datasets, we conduct error case study analysis on the datasets of the D2 version. As shown in Table 9, since we do not use the tag \u201cV BN\u201d in constructing partof-speech graph, our method fails to extract the opinion words with the part of speech \"V BN\" in the four examples, e.g. \u201corganized (V BN )\u201d and \u201cupgraded(V BN )\u201d. Additionally, to identify the limitations of our work and potential areas for improvement in the future, we perform error sample analysis on the D2 dataset. As shown in Table 10, for opinions whose part of speech is not JJ , our method is more likely to give wrong prediction results. Moreover, there are some non-aspect or opinion words whose part-of-speech are NN or JJ , which also mislead the model to make wrong span identification."
        }
    ],
    "title": "Dual-Channel Span for Aspect Sentiment Triplet Extraction",
    "year": 2023
}