{
    "abstractText": "Recently, post-processing networks (PPNs), which modify the outputs of arbitrary modules including non-differentiable ones in taskoriented dialogue systems, have been proposed. PPNs have successfully improved the dialogue performance by post-processing natural language understanding (NLU), dialogue state tracking (DST), and dialogue policy (Policy) modules with a classification-based approach. However, they cannot be applied to natural language generation (NLG) modules because the post-processing of utterances output by NLG modules requires a generative approach. In this study, we propose a new postprocessing component for NLG, generative post-processing networks (GenPPNs). For optimizing GenPPNs via reinforcement learning, the reward function incorporates dialogue act contribution, a new measure to evaluate the contribution of GenPPN-generated utterances with regard to task completion in dialogue. Through simulation and human evaluation experiments based on the MultiWOZ dataset, we confirmed that GenPPNs improve the task completion performance of task-oriented dialogue systems1.",
    "authors": [
        {
            "affiliations": [],
            "name": "Atsumoto Ohashi"
        },
        {
            "affiliations": [],
            "name": "Ryuichiro Higashinaka"
        }
    ],
    "id": "SP:03b4053c2f9f6fae15846a76a37cf0ffa6a3277a",
    "references": [
        {
            "authors": [
                "Gabor Angeli",
                "Percy Liang",
                "Dan Klein."
            ],
            "title": "A simple domain-independent probabilistic approach to generation",
            "venue": "Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 502\u2013512.",
            "year": 2010
        },
        {
            "authors": [
                "Anusha Balakrishnan",
                "Jinfeng Rao",
                "Kartikeya Upasani",
                "Michael White",
                "Rajen Subba."
            ],
            "title": "Constrained decoding for neural NLG from compositional representations in task-oriented dialogue",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for",
            "year": 2019
        },
        {
            "authors": [
                "Pawe\u0142 Budzianowski",
                "Tsung-Hsien Wen",
                "Bo-Hsiang Tseng",
                "I\u00f1igo Casanueva",
                "Stefan Ultes",
                "Osman Ramadan",
                "Milica Ga\u0161i\u0107."
            ],
            "title": "MultiWOZ - A Large-Scale Multi-Domain Wizard-of-Oz Dataset for Task-Oriented Dialogue Modelling",
            "venue": "Proceedings",
            "year": 2018
        },
        {
            "authors": [
                "Qian Chen",
                "Zhu Zhuo",
                "Wen Wang."
            ],
            "title": "BERT for Joint Intent Classification and Slot Filling",
            "venue": "arXiv preprint arXiv:1902.10909.",
            "year": 2019
        },
        {
            "authors": [
                "Zhi Chen",
                "Lu Chen",
                "Xiang Zhou",
                "Kai Yu."
            ],
            "title": "Deep reinforcement learning for on-line dialogue state tracking",
            "venue": "Proceedings of Man-Machine Speech Communication, pages 278\u2013292.",
            "year": 2023
        },
        {
            "authors": [
                "Hyung Won Chung",
                "Le Hou",
                "Shayne Longpre",
                "Barret Zoph",
                "Yi Tay",
                "William Fedus",
                "Eric Li",
                "Xuezhi Wang",
                "Mostafa Dehghani",
                "Siddhartha Brahma"
            ],
            "title": "Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416",
            "year": 2022
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association",
            "year": 2019
        },
        {
            "authors": [
                "Ao Guo",
                "Atsumoto Ohashi",
                "Yuya Chiba",
                "Yuiko Tsunomori",
                "Ryu Hirai",
                "Ryuichiro Higashinaka."
            ],
            "title": "Personality-aware natural language generation for task-oriented dialogue using reinforcement learning",
            "venue": "Proceedings of the 32nd IEEE Interna-",
            "year": 2023
        },
        {
            "authors": [
                "Edward J Hu",
                "yelong shen",
                "Phillip Wallis",
                "Zeyuan AllenZhu",
                "Yuanzhi Li",
                "Shean Wang",
                "Lu Wang",
                "Weizhu Chen"
            ],
            "title": "LoRA: Low-rank adaptation of large language models",
            "venue": "In Proceedings of International Conference on Learning Representations",
            "year": 2022
        },
        {
            "authors": [
                "Vojt\u011bch Hude\u010dek",
                "Ondrej Dusek."
            ],
            "title": "Are large language models all you need for task-oriented dialogue? In Proceedings of the 24th Meeting of the Special Interest Group on Discourse and Dialogue, pages 216\u2013228, Prague, Czechia",
            "venue": "Association for",
            "year": 2023
        },
        {
            "authors": [
                "Diederik P Kingma",
                "Jimmy Ba."
            ],
            "title": "Adam: A Method for Stochastic Optimization",
            "venue": "arXiv preprint arXiv:1412.6980.",
            "year": 2014
        },
        {
            "authors": [
                "Hwaran Lee",
                "Seokhwan Jo",
                "Hyungjun Kim",
                "Sangkeun Jung",
                "Tae-Yoon Kim."
            ],
            "title": "SUMBT+LaRL: Effective Multi-Domain End-to-End Neural TaskOriented Dialog System",
            "venue": "IEEE Access, pages 116133\u2013116146.",
            "year": 2021
        },
        {
            "authors": [
                "Xiujun Li",
                "Yun-Nung Chen",
                "Lihong Li",
                "Jianfeng Gao",
                "Asli Celikyilmaz."
            ],
            "title": "End-to-End TaskCompletion Neural Dialogue Systems",
            "venue": "Proceedings of the Eighth International Joint Conference on Natural Language Processing, pages 733\u2013743.",
            "year": 2017
        },
        {
            "authors": [
                "Yangming Li",
                "Kaisheng Yao",
                "Libo Qin",
                "Wanxiang Che",
                "Xiaolong Li",
                "Ting Liu."
            ],
            "title": "Slot-consistent nlg for task-oriented dialogue systems with iterative rectification network",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational",
            "year": 2020
        },
        {
            "authors": [
                "Zichuan Lin",
                "Jing Huang",
                "Bowen Zhou",
                "Xiaodong He",
                "Tengyu Ma."
            ],
            "title": "Joint System-Wise Optimization for Pipeline Goal-Oriented Dialog System",
            "venue": "arXiv preprint arXiv:2106.04835.",
            "year": 2021
        },
        {
            "authors": [
                "Fran\u00e7ois Mairesse",
                "Steve Young."
            ],
            "title": "Stochastic Language Generation in Dialogue using Factored Language Models",
            "venue": "Computational Linguistics, pages 763\u2013799.",
            "year": 2014
        },
        {
            "authors": [
                "Shikib Mehri",
                "Tejas Srinivasan",
                "Maxine Eskenazi."
            ],
            "title": "Structured Fusion Networks for Dialog",
            "venue": "Proceedings of the 20th Annual SIGdial Meeting on Discourse and Dialogue, pages 165\u2013177.",
            "year": 2019
        },
        {
            "authors": [
                "Volodymyr Mnih",
                "Koray Kavukcuoglu",
                "David Silver",
                "Alex Graves",
                "Ioannis Antonoglou",
                "Daan Wierstra",
                "Martin Riedmiller."
            ],
            "title": "Playing Atari with Deep Reinforcement Learning",
            "venue": "arXiv preprint arXiv:1312.5602.",
            "year": 2013
        },
        {
            "authors": [
                "Alice H Oh",
                "Alexander I Rudnicky."
            ],
            "title": "Stochastic natural language generation for spoken dialog systems",
            "venue": "Computer Speech & Language, pages 387\u2013 407.",
            "year": 2002
        },
        {
            "authors": [
                "Atsumoto Ohashi",
                "Ryuichiro Higashinaka."
            ],
            "title": "Adaptive natural language generation for taskoriented dialogue via reinforcement learning",
            "venue": "Proceedings of the 29th International Conference on Computational Linguistics, pages 242\u2013252.",
            "year": 2022
        },
        {
            "authors": [
                "Atsumoto Ohashi",
                "Ryuichiro Higashinaka."
            ],
            "title": "Post-processing networks: Method for optimizing pipeline task-oriented dialogue systems using reinforcement learning",
            "venue": "Proceedings of the 23rd Annual Meeting of the Special Interest Group on Dis-",
            "year": 2022
        },
        {
            "authors": [
                "Wenbo Pan",
                "Qiguang Chen",
                "Xiao Xu",
                "Wanxiang Che",
                "Libo Qin."
            ],
            "title": "A preliminary evaluation of chatgpt for zero-shot dialogue understanding",
            "venue": "arXiv preprint arXiv:2304.04256.",
            "year": 2023
        },
        {
            "authors": [
                "Baolin Peng",
                "Chenguang Zhu",
                "Chunyuan Li",
                "Xiujun Li",
                "Jinchao Li",
                "Michael Zeng",
                "Jianfeng Gao."
            ],
            "title": "Few-shot Natural Language Generation for Task-Oriented Dialog",
            "venue": "Findings of the 2021 Conference on Empirical Methods in Natural Language",
            "year": 2020
        },
        {
            "authors": [
                "Alec Radford",
                "Jeffrey Wu",
                "Rewon Child",
                "David Luan",
                "Dario Amodei",
                "Ilya Sutskever"
            ],
            "title": "Language models are unsupervised multitask learners",
            "venue": "OpenAI blog,",
            "year": 2019
        },
        {
            "authors": [
                "Abhinav Rastogi",
                "Xiaoxue Zang",
                "Srinivas Sunkara",
                "Raghav Gupta",
                "Pranav Khaitan."
            ],
            "title": "Towards Scalable Multi-Domain Conversational Agents: The Schema-Guided Dialogue Dataset",
            "venue": "Proceedings of the 34th AAAI Conference on Artificial Intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "Jost Schatzmann",
                "Blaise Thomson",
                "Karl Weilhammer",
                "Hui Ye",
                "Steve Young."
            ],
            "title": "Agenda-Based User Simulation for Bootstrapping a POMDP Dialogue System",
            "venue": "Proceedings of Human Language Technologies 2007: The Conference of the North Amer-",
            "year": 2007
        },
        {
            "authors": [
                "Jianfeng Gao",
                "Minlie Huang"
            ],
            "title": "Is Your Goal",
            "year": 2020
        },
        {
            "authors": [
                "Bill Byrne"
            ],
            "title": "Transferable Dialogue Systems",
            "year": 2021
        },
        {
            "authors": [
                "Kaiser",
                "Illia Polosukhin"
            ],
            "title": "Attention is All",
            "year": 2017
        },
        {
            "authors": [
                "Marilyn A. Walker",
                "Owen C. Rambow",
                "Monica Rogati."
            ],
            "title": "Training a sentence planner for spoken dialogue using boosting",
            "venue": "Computer Speech & Language, 16(3):409\u2013433.",
            "year": 2002
        },
        {
            "authors": [
                "Jason Wei",
                "Maarten Bosma",
                "Vincent Zhao",
                "Kelvin Guu",
                "Adams Wei Yu",
                "Brian Lester",
                "Nan Du",
                "Andrew M. Dai",
                "Quoc V Le."
            ],
            "title": "Finetuned language models are zero-shot learners",
            "venue": "Proceedings of International Conference on Learning Representations.",
            "year": 2022
        },
        {
            "authors": [
                "Tsung-Hsien Wen",
                "Milica Ga\u0161i\u0107",
                "Nikola Mrk\u0161i\u0107",
                "Lina M. Rojas-Barahona",
                "Pei-Hao Su",
                "David Vandyke",
                "Steve Young."
            ],
            "title": "Multi-domain Neural Network Language Generation for Spoken Dialogue Systems",
            "venue": "Proceedings of the 2016 Conference of the North",
            "year": 2016
        },
        {
            "authors": [
                "Tsung-Hsien Wen",
                "Milica Ga\u0161i\u0107",
                "Nikola Mrk\u0161i\u0107",
                "PeiHao Su",
                "David Vandyke",
                "Steve Young."
            ],
            "title": "Semantically Conditioned LSTM-based Natural Language Generation for Spoken Dialogue Systems",
            "venue": "Proceedings of the 2015 Conference on Empirical",
            "year": 2015
        },
        {
            "authors": [
                "Steve Young",
                "Milica Ga\u0161i\u0107",
                "Blaise Thomson",
                "Jason D. Williams."
            ],
            "title": "Pomdp-based statistical spoken dialog systems: A review",
            "venue": "Proceedings of the IEEE, pages 1160\u20131179.",
            "year": 2013
        },
        {
            "authors": [
                "Zheng Zhang",
                "Ryuichi Takanobu",
                "Qi Zhu",
                "MinLie Huang",
                "XiaoYan Zhu."
            ],
            "title": "Recent advances and challenges in task-oriented dialog systems",
            "venue": "Science China Technological Sciences, pages 1\u201317.",
            "year": 2020
        },
        {
            "authors": [
                "Tiancheng Zhao",
                "Maxine Eskenazi."
            ],
            "title": "Towards End-to-End Learning for Dialog State Tracking and Management using Deep Reinforcement Learning",
            "venue": "Proceedings of the 17th Annual Meeting of the Special Interest Group on Discourse and Dialogue,",
            "year": 2016
        },
        {
            "authors": [
                "Qi Zhu",
                "Zheng Zhang",
                "Yan Fang",
                "Xiang Li",
                "Ryuichi Takanobu",
                "Jinchao Li",
                "Baolin Peng",
                "Jianfeng Gao",
                "Xiaoyan Zhu",
                "Minlie Huang."
            ],
            "title": "ConvLab-2: An Open-Source Toolkit for Building, Evaluating, and Diagnosing Dialogue Systems",
            "venue": "Proceedings",
            "year": 2020
        },
        {
            "authors": [
                "Daniel M Ziegler",
                "Nisan Stiennon",
                "Jeffrey Wu",
                "Tom B Brown",
                "Alec Radford",
                "Dario Amodei",
                "Paul Christiano",
                "Geoffrey Irving."
            ],
            "title": "Fine-tuning language models from human preferences",
            "venue": "arXiv preprint arXiv:1909.08593.",
            "year": 2019
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "A typical task-oriented dialogue system has a pipelined structure consisting of four modules (Young et al., 2013; Zhang et al., 2020): natural language understanding (NLU), dialogue state tracking (DST), dialogue policy (Policy), and natural language generation (NLG). Many studies have used reinforcement learning (RL) to improve the task completion performance of an entire pipelined dialogue system by fine-tuning modules directly (Lee et al., 2021; Lin et al., 2021; Chen et al., 2023).\nRecently, Ohashi and Higashinaka (2022b) proposed a novel method using Post-Processing Networks (PPNs) that can optimize pipelined dialogue\n1Our code is publicly available at https://github.com/ nu-dialogue/GenPPN\nsystems that consist of arbitrary modules including non-differentiable ones (e.g., rule-based and Web API-based). Their method uses RL to optimize a neural-based component called a PPN, which modifies the output of each module. PPNs have been applied to three modules, namely NLU, DST, and Policy, and have been shown to improve the task completion performance of various pipelined systems. In PPNs, the post-processing of module outputs is treated as a binary classification task, i.e., adding or removing slots in the output of each module, and this classification task is modeled with a multi-layer perceptron (MLP). This means that the current PPNs could not be applied to NLGs, where the unit of output is not slots but a sequence of tokens, i.e., natural language.\nTo overcome this limitation, we propose a Pretrained Language Model (PLM)-based Generative Post-processing Network (GenPPN) that can postprocess the output of NLGs in a manner similar to conventional PPNs (Figure 1). To optimize GenPPN via commonly used RL frameworks for PLMs (Ziegler et al., 2019; Stiennon et al., 2020), a reward for each utterance at each turn (an utterancelevel reward) is required. In a task-oriented dialogue, however, a reward indicating success or failure is obtained only at the end of a multi-turn interaction (a dialogue-level reward). Therefore, we introduce a dialogue act (DA) contribution for distributing the dialogue-level reward to the utterance-\nlevel reward. Here, a DA is the meaning representation of the information that NLG converts into an utterance, and DA contribution is a measure of how much the DA of each utterance contributes to the final task completion of the dialogue.\nExperiments on the MultiWOZ dataset (Budzianowski et al., 2018) confirm that GenPPNs can improve the task performance of the entire dialogue system, regardless of the architecture of the NLG module. Furthermore, an ablation study reveals that the introduction of DA contribution is effective for learning GenPPNs to improve task completion. The contributions of this study are threefold:\n\u2022 We propose the generative post-processing network (GenPPN) to modify the output utterances of the NLG module, which was impossible with conventional PPNs.\n\u2022 We introduce DA contribution to optimize the GenPPN by evaluating the impact of each utterance on dialogue task completion.\n\u2022 Simulation experiments on the MultiWOZ dataset confirm that the proposed GenPPN improves the task completion performance of the entire dialogue system, regardless of the architecture of the NLG module. We also validated that a GenPPN optimized using simulation is effective in human evaluation experiments."
        },
        {
            "heading": "2 Related Work",
            "text": ""
        },
        {
            "heading": "2.1 Optimization of Pipelined Task-oriented Dialogue Systems",
            "text": "Methods have been proposed to optimize the task completion performance of an entire pipelined system using RL. Zhao and Eskenazi (2016) and Li et al. (2017) optimized a Policy network implemented in MLP using the Deep Q-Network algorithm (Mnih et al., 2013) to achieve robustness against errors that occur in real dialogues. Mehri et al. (2019) proposed a method to add additional parameters to NLU, Policy, and NLG and optimize them using RL. By expressing a dialogue state output by a DST as a probability distribution, Lee et al. (2021) made the entire system differentiable and jointly optimized it via RL. A method of fine-tuning an NLU while optimizing a Policy in dialogue simulations was proposed (Lin et al., 2021). Tseng et al. (2021) proposed a domain-adaptive learning\nframework that simultaneously optimizes the Policy module of the dialogue system and the user simulator. An RL framework for online DST optimization was also proposed to improve dialogue management performance (Chen et al., 2023).\nInstead of training each module, Ohashi and Higashinaka (2022b) proposed a generalized method that optimizes PPNs, classification-based models that modify the outputs of NLU, DST, and Policy, to enhance the task completion performance. In this paper, we propose a new generative-based PPN for post-processing NLG modules, which has not been supported by conventional PPNs."
        },
        {
            "heading": "2.2 Natural Language Generation Module for Task-oriented Dialogues",
            "text": "Conventional NLGs for task-oriented dialogues used template-based or rule-based methods (Walker et al., 2002; Stent et al., 2004). Later, datadriven methods using machine learning were proposed (Oh and Rudnicky, 2002; Angeli et al., 2010; Mairesse and Young, 2014) that do not require the cost of template and rule creation.\nIn recent years, many generative models based on deep learning have been proposed (Wen et al., 2016; Tran and Nguyen, 2017; Su et al., 2018). Wen et al. (2015) proposed an SC-LSTM that controls utterance generation using DA feature vectors and reading gates. SC-GPT (Peng et al., 2020) is the best NLG model of MultiWOZ. It achieves high performance by fine-tuning GPT-2 (Radford et al., 2019) on many task-oriented dialogue datasets such as MultiWOZ (Budzianowski et al., 2018) and the Schema-Guided Dialogue Dataset (Rastogi et al., 2020).\nFor task-oriented dialogue, it is crucial not only to generate natural utterances via maximum likelihood estimation (MLE) but also to accurately reflect the input DA\u2019s content. To achieve this, Balakrishnan et al. (2019) introduced a conditional decoding approach utilizing a tree-shaped semantic representation, enhancing the slot content in generated utterances. Furthermore, Li et al. (2020) offered a method to lower the slot error rate in utterances using an iterative RL framework for slot consistency. Ohashi and Higashinaka (2022a) presented a fine-tuning method for utterance generation that uses a user\u2019s NLU model so that the NLU can understand the DA accurately.\nIn all of the above studies, the optimization was performed at the utterance level using a fixed cor-\npus of utterances. In this study, we aim to optimize post-processing not only on the utterance level but also on the dialogue level to improve the task completion performance for multi-turn dialogues."
        },
        {
            "heading": "3 Method",
            "text": ""
        },
        {
            "heading": "3.1 Overview",
            "text": "Figure 2 shows a schematic diagram of GenPPN and how it is optimized. At the beginning of each interaction, the user is given a user goal consisting of a list of constraints to be informed to the system and information to be obtained from the system through the dialogue. During the dialogue, from the user\u2019s utterance at a certain turn t, the NLU and DST modules of the system first estimate the current belief state of the user. Then, on the basis of the belief state, the Policy module determines the next actions to be taken by the system as DAs at. The a takes a structure containing one or more triples of intent I , slot s, and value v:\na = {(Ii, si, vi) | i = 1, ..., |a|} (1)\nThe NLG module maps at to the system\u2019s utterance ut. Here, GenPPNM post-processes, i.e., rewrites ut. A prompt xt = Prompt(ht, at, ut) is created from the previous dialogue context ht, at, and ut, andM generates a new utterance u\u2032t: u\u2032t \u223cM(xt) (see Section 3.2 for details on GenPPN and the prompt format). The user then receives u\u2032t, estimates the system DAs a\u0302t, and the user\u2019s utterance is output for the next turn t+1 on the basis of the a\u0302t and current status of the user goal. This exchange is repeated until the user goal is achieved or until the maximum number of turns is reached.\nLet T be the number of turns at the end of a dialogue. In addition, the dialogue result e \u2208 {0, 1} of whether the user goal was achieved is determined,\ni.e., 0 means that the task failed, and 1 means that the task succeeded. The objective is to obtain the optimal parameters \u03b8\u2217 for the GenPPNM\u03b8 such that the expected value of e is maximized:\n\u03b8\u2217 = arg max \u03b8 E U \u2032\u223cM\u03b8(X) [e(U \u2032)] (2)\nwhere U \u2032 = {u\u20321, u\u20322, ..., u\u2032T } are the system utterances sampled by M from prompts X = {x1, x2, ..., xT } at each turn.\nSince the component to be trained is not the NLG itself but the GenPPN, our optimization is independent of the architecture and differentiability of NLG modules. In the following subsections, we describe the GenPPN, reward design for optimizing Eq. (2), and RL algorithm used."
        },
        {
            "heading": "3.2 Generative Post-processing with PLM",
            "text": "We assume that the modelM of GenPPN is a language model based on the Transformer architecture (Vaswani et al., 2017). At turn t, the input prompt xt is created from the previous dialogue context ht, DA at, and system utterances ut by using a hand-crafted prompt (specific prompts are shown in Section A.2 in the appendix).\nThrough RL,M learns to generate a rewritten system utterance u\u2032 such that the dialogue result e is maximized. However, even in the early stages of learning, if the system cannot produce at least reasonable system utterances, the dialogue always breaks down, and the training does not progress properly. For this reason, we adopt an instructiontuned PLM (Wei et al., 2022; Chung et al., 2022; Taori et al., 2023), which has shown high performance on many NLP benchmarks including taskoriented dialogue modeling in few- and zero-shot settings (Pan et al., 2023; Hudec\u030cek and Dusek,\n2023). Since the general PLM has a massive number of parameters, we use Low-Rank Adaptation (LoRA) (Hu et al., 2022), which has good parameter efficiency. That is, a small number of parameters \u03b8 is added to each self-attention module in the PLM, and only \u03b8 is optimized during training."
        },
        {
            "heading": "3.3 Reward with DA Contribution",
            "text": "Eq. (2) implies that only e or a dialogue-level reward at the end of a multi-turn dialogue should be used to evaluate the generated utterances U \u2032 for all T turns. However, since this reward is too sparse and it is not feasible to optimize M, we approximate Eq. (2) by using the utterance-level reward r, which evaluates how much of an effect each utterance has on the final task completion:\n\u03b8\u2217 = arg max \u03b8 E U \u2032\u223cM\u03b8(X)  T\u2211 u\u2032t\u2208U \u2032 r(u\u2032t)  (3) The role of NLG is to accurately convey DAs to the user, i.e., to generate utterances such that at = a\u0302t, but this kind of evaluation based on the consistency of at and a\u0302t does not take into account e, so it cannot be used for r directly.\nWith this in mind, we design r by combining e with the consistency evaluation for at and a\u0302t. Specifically, we statistically measure the contribution of correctly conveying (I, s, v) \u2208 at to the user in terms of the impact on e on the basis of the dialogues we have sampled. Then, we give a contribution-weighted reward to the utterance u\u2032t that succeeds in conveying (I, s, v).\nTo realize this, first, at the end of each dialogue, triples {(at, a\u0302t, e)|t \u2208 T} consisting of at and a\u0302t at each turn together with e are added to the DA history D. Here, e is determined retrospectively on the basis of the outcome at the end of the dialogue; e of all turns in a successful dialogue is 1, {(at, a\u0302t, 1)|t \u2208 T}, and e of all turns in a failed dialogue is 0, {(at, a\u0302t, 0)|t \u2208 T}. Here, all (a, a\u0302) pairs in D sampled so far are split into S and F depending on whether the task was successful:\nS = {(a, a\u0302) | (a, a\u0302, e) \u2208 D, e = 1} F = {(a, a\u0302) | (a, a\u0302, e) \u2208 D, e = 0}\nThen, for each (I, s, v) \u2208 at, the contribution c(I,s) of (I, s) is calculated as follows:\nc(I,s) = nRec,S(I,s) + n Unr,F (I,s)\nnRec,S(I,s) + n Rec,F (I,s) + n Unr,S (I,s) + n Unr,F (I,s)\n(4)\nwhere nRec,S(I,s) is the number of times that (I, s) has been correctly recognized by the user (subscript \u201cRec\u201d for \u201crecognized\u201d) in a successful dialogue S:\nnRec,S(I,s) = \u2211\n(a,a\u0302)\u2208S \u2211 (I\u2032,s\u2032,v\u2032)\u2208a\u2229a\u0302 [(I, s) = (I \u2032, s\u2032)]\nSimilarly, nUnr,S(I,s) indicates the number of times in S that (I, s) was not recognized by the user (subscript \u201cUnr\u201d for \u201cunrecognized\u201d), nRec,F(I,s) indicates the number of times in F that (I, s) was recognized by the user, and nUnr,F(I,s) indicates the number of times in F that (I, s) was not recognized by the user.\nSince c(I,s) is the co-occurrence probability of (I, s) being recognized (or unrecognized) and the task being a success (or a failure), this quantifies how accurately conveying (I, s) leads to task success. Also, c(I,s) is a value that is updated as the number of dialogue samples increases during training. That is, c(I,s) can be computed adaptively to the current performance of the GenPPN at each learning step. Note that v is not taken into account for each count because the possible values of v are so large (e.g., instances of phone number, address, etc.) that counting them separately would yield unreliable statistics.\nFinally, the reward r(u\u2032t) for the utterance generated by GenPPN at turn t is calculated using this DA contribution c(I,s) as follows:\nr(u\u2032t) = Aggregate(w(at, a\u0302t)) (5)\nw(at, a\u0302t) = {\u03c4 \u00b7 c(I,s) | (I, s, v) \u2208 at \u2229 a\u0302t} \u222a {\u2212\u03c4 \u00b7 c(I,s) | (I, s, v) \u2208 at \u2229 \u00af\u0302at} (6)\nHere, the DAs correctly recognized by the user (at\u2229 a\u0302t) are given a default score \u03c4 , and conversely, the DAs not recognized by the user (a \u2229 \u00af\u0302at) are given a negative score \u2212\u03c4 , and w(at, a\u0302t) is the set of these scores weighted by each DA contribution c(I,s). The constant \u03c4 is a hyperparameter, and \u201cAggregate\u201d is a function for aggregating w(at, a\u0302t) into a scalar value for the reward. We design the following two types of Aggregate functions and empirically determine which one is better:\nmean Output the mean of w(at, a\u0302t)\nabsmax Output the value with the highest absolute value among w(at, a\u0302t). This is to emphasize the weighted score of (I, s, v) with the highest contribution.\nAt the beginning of learning, the number of DAs recorded in D is small, so it is expected that an appropriate c(I,s) cannot be calculated. Therefore, prior to learning, (a, a\u0302) is sampled and recorded in D by conducting multiple dialogues between the dialogue system and the user simulator without using GenPPN."
        },
        {
            "heading": "3.4 Optimization via RL",
            "text": "Following Stiennon et al. (2020), the RL algorithm used in this method is Proximal Policy Optimization (PPO) (Schulman et al., 2017). As a value network, a linear layer that outputs a scalar value randomly initialized with the parameter \u03d5 is added, and the overall trainable parameters are set to \u03c8 = [\u03b8;\u03d5]. Also, following (Ziegler et al., 2019), to prevent the probability distribution ofM\u03c8 from deviating too far from that of the originalM due to parameter updates and thus losing its naturalness, a Kullback-Leibler (KL) divergence penalty is added to r(u\u2032t) as the final reward Rt for utterance u \u2032 t at turn t:\nRt = r(u \u2032 t)\u2212 \u03b2 log M\u03c8(u\u2032t|xt) M(u\u2032t|xt)\n(7)\nThe clipped surrogate objective L(\u03c8) (Schulman et al., 2017) is used to optimize \u03c8 with the advantage estimated from the reward and value network. Since we need the user\u2019s subjective understanding results a\u0302, this study uses a user simulator to optimize the GenPPN. The learning algorithm is summarized in Algorithm 1 in the appendix."
        },
        {
            "heading": "4 Experiments",
            "text": "In our experiments, we first evaluated the effectiveness of GenPPNs using a user simulator. Then, using the optimized GenPPN, we conducted a dialogue evaluation experiment using human subjects."
        },
        {
            "heading": "4.1 Dataset and Platform",
            "text": "We evaluated the effectiveness of our GenPPN using a dialogue system and a user simulator implemented on the basis of the MultiWOZ dataset (Budzianowski et al., 2018). MultiWOZ is a task-oriented dialogue dataset between a clerk and a customer at a travel information center, collected in Wizard-of-OZ style. It contains a variety of tasks across a total of seven domains (attraction, hotel, hospital, restaurant, taxi, train, and police).\nWe used ConvLab-2 (Zhu et al., 2020), a platform for evaluating task-oriented dialogue systems\nthat provide various modules for dialogue systems, a user simulator, and an evaluation tool. The following describes the dialogue system, user simulator, and user goals used in this experiment.\nDialogue System To make it easier to assess changes in the performance of utterance generation, the other modules (that is, NLU, DST, and Policy) should have stable performance. Therefore, we used the best-performing BERT (Devlin et al., 2019)-based NLU (Chen et al., 2019), rule-based DST, and rule-based Policy available in ConvLab-2. BERT-based NLU is a model that uses representations embedded by BERT to classify user intentions in user utterances and extract slots by sequence labeling. Rule-based DST and Policy are modules implemented using hand-crafted rules. Three models were selected as NLG modules (see Section 4.2 for the NLG models used), and GenPPN was applied to each of them to verify its generality.\nUser Simulator For the user simulator, we used a combination of BERT-based NLU, agenda-based Policy (Schatzmann et al., 2007), and templatebased NLG. The agenda-based Policy models a user\u2019s behavior in MultiWOZ by using a stack-like agenda created using hand-crafted rules.\nUser Goal A user goal for each dialogue is randomly generated; the domains are randomly selected from one to three domains (out of all seven domains). The slots are also randomly selected on the basis of the slots\u2019 frequency in MultiWOZ."
        },
        {
            "heading": "4.2 NLG Baselines",
            "text": "We applied GenPPN to each of the three NLGs available in ConvLab-2 with different architectures in order to demonstrate that it works for a variety of NLGs. In addition, one NLG optimized with only utterance-level rewards, without considering task success, was also evaluated for comparison.\nTemplate NLG An NLG model that uses the template utterances representing each DA. Because each utterance is carefully designed by hand, this model has significantly higher performance than other NLG baselines (Takanobu et al., 2020).\nSC-LSTM (Wen et al., 2015) An LSTM-based model with a reading gate mechanism. This model takes binary feature vectors representing DAs as context and decodes utterances.\nSC-GPT (Peng et al., 2020) A GPT-2 based model that generates utterances from DA text sequences. This has been trained on seven task-oriented dialogue corpora including MultiWOZ and the Schema-Guided Dialogue Dataset (Rastogi et al., 2020), and it is a SOTA on the MultiWOZ NLG benchmark.\nGPT-2 + RL (Ohashi and Higashinaka, 2022a) A GPT-2-based NLG model optimized with an utterance-level reward. This model was trained to maximize the accuracy of a and a\u0302 (as measured by F1) using the DA-system utterance pairs in the MultiWOZ corpus in an offline fashion.\nNote that, since the purpose of our experiment is to verify whether GenPPN can enhance the performance of NLG irrespective of its architecture or base performance, the verification of methods that fine-tune NLG models themselves is outside the scope of this study."
        },
        {
            "heading": "4.3 Evaluation Metrics",
            "text": "In the evaluation of the dialogue system, we used common metrics for task-oriented dialogues: Turn, Inform F1/Precision/Recall, Book Rate, Task Success. Turn indicates the number of turns required for each dialogue; the smaller it is, the more efficiently the dialogue can be conducted. Inform F1/Precision/Recall indicate whether the system responded appropriately to the user\u2019s requests. Book Rate indicates whether the entity booked by the system correctly matched that of the user\u2019s goal.\nTask Success is evaluated by calculating whether Inform Recall and Match Rate both became one within the maximum number of turns. To evaluate the utterance-level performance of NLG, we additionally used DA F1, which measures the match rate of a and a\u0302 at each turn by F1 (Ohashi and Higashinaka, 2022a; Guo et al., 2023)."
        },
        {
            "heading": "4.4 Training GenPPN",
            "text": "Stanford Alpaca 7B (Taori et al., 2023) was used as the instruction-tuned PLM for the GenPPN. This model is a fine-tuned version of LLaMA-7B (Touvron et al., 2023) with a 52K instruction dataset. The input prompts include the dialogue history, the previous DA, the system utterances generated by NLG from that DA, and the instructions for rewriting that system utterance (specific prompts are shown in Section A.2 in the appendix). Throughout the experiment, we first sampled 1K dialogues to initialize the DA history and then trained 200 iterations. See Section A.3 for further training details.\nFor the evaluation, the GenPPN at the highestreward iteration was used, and dialogue simulations were performed using 1,024 user goals prepared specifically for the test. We report the average scores in this paper."
        },
        {
            "heading": "4.5 Main Results",
            "text": "Table 1 shows the evaluation results2. For Template and SC-LSTM, GenPPN improved all evalua-\n2We used the latest version of ConvLab-2 as of June 2023; we could not reproduce the scores reported on the leaderboard for some baseline systems. The main reason is probably due to the difference in the random seed and BERT NLU weights.\ntion metrics. The absmax aggregation function improved the final performance more than the mean. For SC-LSTM, Task Success was nearly 19 points better than the original SC-LSTM, especially when using absmax. These results indicate that a better strategy for evaluating each utterance is to prioritize learning to convey the DA with the highest contribution rather than to take the average. Considering that the improvement in Template NLG was only about 1.7%, the lower the performance of the original NLG, the more room there is for improvement by GenPPN.\nFor SC-GPT, Task Success, Book Rate, and Turn steadily improved. However, Inform Precision slightly decreased, unlike other cases where GenPPN was applied to other NLG modules. This suggests that GenPPN learned that improving Book Rate is more important for improving Task Success, even at the expense of Inform Precision.\nGPT-2 + RL has a higher DA F1 than any of the GenPPNs. This is reasonable, given that GPT-2 + RL is optimized using DA F1 as a reward. GenPPN was able to further improve GPT-2 + RL\u2019s Task Success while the utterance-level metric DA F1 slightly decreased from the original score. This suggests that not only utterance-level rewards but dialoguelevel rewards must be introduced to improve the task completion performance of the entire system.\nNote that the performance of SC-LSTM + GenPPN and SC-GPT + GenPPN is lower than that of Template NLG baseline. However, the primary goal of our study is not to obtain an NLG model with SOTA performance but rather to enhance the performance of NLG irrespective of its architecture or its base performance. In our experiments, the dialogue performance of NLG models like SCLSTM and SC-GPT, in addition to Template NLG, were improved using GenPPN, showing that our primary goal has been achieved."
        },
        {
            "heading": "4.6 Ablation Study",
            "text": "We analyzed the impact of each factor in the GenPPN optimization on the final performance. For the NLG model in this ablation study, we used SC-LSTM, which showed the greatest improvement in performance over the other NLGs in Table 1. Table 2 shows the results.\nAdaptive DA Contribution \u201cw/o adaptive c(I,s)\u201d is a reward design that does not update the DA history, and in all steps, uses the DA contribution calculated only from the 1,000 dialogues sampled prior to training. This resulted in a 10 points decrease in Task Success. Therefore, we found that it is important to constantly update the DA contributions as the GenPPN changes.\nDialogue-level Reward \u201cw/o e\u201d does not use DA contribution, and only an utterance-level reward based on the accuracy (F1) between a and a\u0302 is used. That is, the reward design does not take into account the dialogue-level reward e related to task completion. As a result, Task Success decreased significantly, while DA F1 did not decrease much. Therefore, it was shown that the DA contribution is a useful factor in improving the task success rate.\nUtterance-level Reward \u201cw/o r(u\u2032)\u201d does not use the utterance-level reward r(u\u2032) but only the last dialogue evaluation result as a reward. That is, all utterances were given the dialogue result e \u2208 {0, 1} equally. Although Book Rate improved, Inform F1 did not, indicating that it is difficult to optimize GenPPN with only a sparse e reward, as described in Section 3.3.\nDialogue Context \u201cw/o context\u201d indicates that the input of GenPPN does not include the dialogue history. The slight decrease in Task Success, Inform F1, and Book Rate indicates that GenPPN should make use of the previous history to improve\ndialogue performance. Meanwhile, the improvement in DA F1 suggests that dialogue history is not necessary if we only want to optimize at the utterance level.\nSystem Response \u201cw/o response\u201d does not rewrite the system utterance, but GenPPN generates it directly from the dialogue history and DA. It can be seen that the performance of all the methods significantly degraded. The reason for this is that the untrained GenPPN, i.e., Alpaca-7B, has never seen the mapping from DA to system utterance in MultiWOZ and thus has difficulty generating system utterances in a zero-shot setting. Therefore, it is considered more reasonable to learn to rewrite utterances rather than to generate them from scratch."
        },
        {
            "heading": "4.7 Human Evaluation",
            "text": "We tested whether GenPPN optimized by dialogue simulation is also effective for humans. In this experiment, we also used the system using SC-LSTM, which showed the best performance improvement over the other NLGs in Table 1. Over 50 crowd workers were recruited via Amazon Mechanical Turk, and each worker interacted once with one of the systems using SC-LSTM or SC-LSTM + GenPPN for up to 20 turns after being instructed about their user goal. Each worker had 20 turns to judge whether or not the task was successful; after 20 turns, the task was forced to fail. After the dialogue was completed, each worker was asked to rate the system\u2019s ability to understand the language (Und.), the accuracy of the system\u2019s responses (App.), and overall satisfaction with the dialogue (Sat.) on a 5-point Likert scale. See Section A.5 in the appendix for more details on the human evaluation settings.\nTable 3 shows the results of the evaluations of each system. GenPPN significantly improved in terms of both Task Success and Turn, which are\nmeasures of task completion. However, the user\u2019s subjective ratings of understanding, appropriateness, and dialogue satisfaction showed no improvement, which is reasonable given that GenPPN is optimized for task completion only. This also means that GenPPN did not generate unnatural utterances to improve Task Success and Turn, which is considered a positive result. We would like to evaluate the effectiveness of GenPPN for other NLG baselines than SC-LSTM by humans in the future."
        },
        {
            "heading": "5 Case Study",
            "text": "Table 4 compares the behavior of two systems using SC-LSTM and using SC-LSTM + GenPPN in a dialogue. As a context, the user requested the address and phone number of the restaurant, but the Policy module of the dialogue system mistakenly decided to answer with the address and phone number of an attraction as well. Here, SC-LSTM was not able to generate sentences because there was no example in its training data, in which DAs in different domains occur at the same time. As a result, SC-LSTM failed to convey any information to the user. In contrast, GenPPN reflected the address and phone number of the restaurant in the post-processed utterance, and as a result, the information was correctly conveyed as requested by the user, and the task was successful.\nIt is worth noting that the utterance generated by post-processing does not contain any information about the attraction. This is probably because GenPPN judged from the dialogue context that the information about the attraction was not what the user requested, and that conveying this piece of information would interfere with the task completion. These results indicate that GenPPN can correct errors propagated from preceding modules beyond the realm of NLG so that the entire system\u2019s performance improves."
        },
        {
            "heading": "6 Summary and Future Work",
            "text": "Post-Processing Networks (PPNs), which modify the outputs of arbitrary modules including nondifferentiable ones in task-oriented dialogue systems, have been applied to NLU, DST, and Policy modules. In this paper, we proposed Generative Post-processing Networks (GenPPNs) for NLG modules. We optimized GenPPN toward the task completion performance of the entire system by using DA contribution-based rewards. Experiments on the MultiWOZ dataset and dialogue simulations confirmed that our GenPPN could improve the task success rate of various dialogue systems, regardless of the NLG architecture. It was also confirmed that a GenPPN optimized with a dialogue simulation was effective in a human evaluation experiment.\nIn the future, we would like to integrate our GenPPN with the existing PPNs of NLU, DST, and Policy to realize the post-processing of all modules and further improve the systems performance. Since GenPPN is a generative model, we would like to extend it not only to text but also to the post-processing of speech recognition and speech synthesis modules in spoken dialogue systems.\nLimitations\nOur work has several limitations, which we aim to address in our future work.\nFirst, this study implemented, trained, and evaluated the GenPPN using only the MultiWOZ dataset. Therefore, the applicability of the GenPPN to taskoriented dialogues other than MultiWOZ has not been verified. In particular, the DA-dependent reward design in this study needs to be improved for application to dialogues such as open-domain chats, where ontologies such as intent, slot, and value are not defined.\nSecond, this study used a user simulator for training the GenPPN. When applied to a new domain, the cost of implementing a user simulator is likely to be high. Therefore, a user simulator-free learning method is needed in this respect. In addition, a GenPPN optimized by a user simulator learns usersimulator-specific utterances, which may not be the best utterances for humans. To learn the optimal GenPPN for humans, it is essential to design a reward model that takes humans into account, using a human-in-the-loop approach.\nThird, this study required running a PLM with a relatively large parameter size. During inference, in addition to the system\u2019s response generation, the\nGenPPN is required to rewrite utterances, which is expected to increase the operation cost and generation time. In addition, more computational resources are required during learning. Therefore, it is necessary to study ways to reduce learning and inference costs.\nEthics Statement\nWe used the publicly available MultiWOZ dataset in full compliance with its terms of use. Our use of the LLaMA language model was also entirely consistent with the prescribed usage guidelines. No private or confidential data was used at any stage of the research. For our human evaluation experiment, we underwent and passed an ethical review from our institution and strictly adhered to its rules and guidelines. The anonymity, privacy, and rights of the subjects were preserved throughout. We acknowledge that there are potential ethical concerns associated with the use of large pre-trained language models, such as the risk of generating harmful or discriminatory statements."
        },
        {
            "heading": "Acknowledgments",
            "text": "This work was supported by JSPS KAKENHI Grant Number 19H05692. We used the computational resources of the supercomputer \u201cFlow\u201d at the Information Technology Center, Nagoya University."
        },
        {
            "heading": "A Appendix",
            "text": "A.1 GenPPN Optimization Algorithm\nAlgorithm 1 Optimization of GenPPN via PPO Require: Dialogue system A, User simulator U Require: GenPPNM\n1: Initialize DA history D by sampling several dialogues using A and U 2: PrepareM\u03c8old with randomly initialized LoRA parameters \u03b8 and value network parameters \u03d5 3: for each training iteration do 4: while #turns does not reach batch size do 5: Sample a dialogue and (at, a\u0302t) for each turn t using A,M\u03c8old , and U 6: Obtain final evaluation result e 7: Add (at, a\u0302t, e) of each t to D 8: end while 9: Calculate reward Rt using Eq. (7) 10: Compute advantage estimates 11: Optimize L(\u03c8) with a certain epoch and mini-batch size 12: Update \u03c8old \u2190 \u03c8 13: end for\nA.2 Prompt for GenPPN At a given turn t, the input prompts to GenPPN include the DA at output by the system\u2019s Policy module and the original system utterance ut generated by the NLG from at. As the dialogue context, we also included user utterances at t\u22121, DAs at\u22121, system utterances u\u2032t\u22121 generated by GenPPN, and user utterances at t. The format of the instructional text in the prompts was adapted from the phrases in the Alpaca dataset (Taori et al., 2023). Examples of prompts are shown in Figure 3. In our experiments, due to the maximum input length of the model, the dialogue history was limited to the past one turn.\nA.3 Details of GenPPN Training As the model for GenPPN, we used Stanford Alpaca-7B trained weights for the prompts, which are publicly available on HuggingFace Hub3. The maximum number of input tokens for a prompt was set to 512, and the maximum number of generated tokens was set to 128. When generating utterances, sampling was performed with the beam size set to 1, temperature to 1.0, and top-p to 1.0. The \u03c4 used in the reward calculation (Eq. (6)) was set to 1.\n3https://huggingface.co/tatsu-lab/ alpaca-7b-wdiff\nTable 5 shows the hyperparameters of LoRA and PPO during training. Throughout the experiment, we first sampled 1,000 dialogues to initialize the DA history and then trained 200 iterations, with a batch size of 512 turns per iteration (corresponding to about 50 dialogues) and a fixed learning rate of 1e-5 with Adam Optimizer (Kingma and Ba, 2014). We used 16 \u00d7 V100 32-GB computational resources. Training of 200 iterations took about seven hours to complete.\nFigure 4 shows the training curves when GenPPN is applied to Template NLG, SC-LSTM, SC-GPT, and GPT-2 + RL respectively. The figure shows that both the reward and Task Success are low at the start of training (i.e., before RL is applied) and improve as RL progresses, which means that fine-tuning LLMs with RL is important for our GenPPN. We have yet to investigate the effects of instruction tuning. We plan to validate this by comparing the performance of instruction-tuned models other than Alpaca (e.g., Flan-T5 (Chung et al., 2022)) and non-instruction-tuned models (e.g., LLaMA (Touvron et al., 2023)).\nA.4 Additional Ablations\nIn addition to the study on SC-LSTM in Section 4.6, we conducted the same ablation study on two higher-performing NLGs: Template NLG and GPT2 + RL. Table 6 shows the results. In Template NLG, the improvement with GenPPN was the greatest when no context was used. Nevertheless, for both Template NLG and GPT-2 w/ RL, there was a trend showing that adaptive DA contribution was important for performance improvement.\nA.5 Details of Human Evaluation\nIn collecting participants for the human evaluation via Amazon Mechanical Turk (AMT), we recruited workers who met the following four conditions: (1) had residence in an English-speaking county, (2) had at least 100 completed Human-Intelligence Tasks (HIT) on AMT, (3) had an acceptance rate of 95% or higher on the HIT on AMT, and (4) answered all five common sense questions correctly.\nIn the experimental procedure, each worker was first instructed about a certain user goal. The user goal was randomly generated by a program as in the dialogue simulation. Considering the high comprehension ability of humans, the number of domains included in the user goal was set to 3, which is a higher difficulty level than in the simulation. After reading the user goal, the workers interacted with either the system using SC-LSTM or the system using SC-LSTM + GenPPN. The maximum number of turns in the dialogue was 20, and each worker judged whether the task was completed within the\nmaximum number of turns. After 20 turns, the task was forced to fail. After the dialogue, the workers answered the three-question questionnaire described in Section 4.7.\nConsidering that each HIT should take about 10 minutes to complete, the reward was set at $2, and the time limit was 30 minutes. The number of subjects was not equal because the system was randomly selected each time, resulting in 54 evaluators for SC-LSTM and 50 evaluators for SC-LSTM + GenPPN."
        }
    ],
    "title": "Enhancing Task-oriented Dialogue Systems with Generative Post-processing Networks",
    "year": 2023
}