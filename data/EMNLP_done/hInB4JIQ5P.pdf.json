{
    "abstractText": "We introduce COEDIT, a state-of-the-art text editing system for writing assistance. COEDIT takes instructions from the user specifying the attributes of the desired text, such as \"Make the sentence simpler\" or \"Write it in a more neutral style,\" and outputs the edited text. We present a large language model fine-tuned on a diverse collection of task-specific instructions for text editing (a total of 82K instructions). Our model (1) achieves state-of-the-art performance on various text editing benchmarks, (2) is competitive with publicly available largestsized LLMs trained on instructions while being \u223c60x smaller, (3) is capable of generalizing to unseen edit instructions, and (4) exhibits abilities to generalize to composite instructions containing different combinations of edit actions. Through extensive qualitative and quantitative analysis, we show that writers prefer the edits suggested by COEDIT, relative to other stateof-the-art text editing models.",
    "authors": [
        {
            "affiliations": [],
            "name": "Vipul Raheja"
        },
        {
            "affiliations": [],
            "name": "Dhruv Kumar"
        },
        {
            "affiliations": [],
            "name": "Ryan Koo"
        },
        {
            "affiliations": [],
            "name": "Dongyeop Kang"
        }
    ],
    "id": "SP:7e52d36951bbd525b271c0e242c28ba93f39127f",
    "references": [
        {
            "authors": [
                "Fernando Alva-Manchego",
                "Louis Martin",
                "Antoine Bordes",
                "Carolina Scarton",
                "Beno\u00eet Sagot",
                "Lucia Specia."
            ],
            "title": "ASSET: A dataset for tuning and evaluation of sentence simplification models with multiple rewriting transformations",
            "venue": "Proceedings of the 58th",
            "year": 2020
        },
        {
            "authors": [
                "Almubarak",
                "Xiangru Tang",
                "Dragomir Radev",
                "Mike Tian-jian Jiang",
                "Alexander Rush."
            ],
            "title": "PromptSource: An integrated development environment and repository for natural language prompts",
            "venue": "Proceedings of the 60th Annual Meeting of the Association",
            "year": 2022
        },
        {
            "authors": [
                "Christopher Bryant",
                "Mariano Felice",
                "\u00d8istein E. Andersen",
                "Ted Briscoe."
            ],
            "title": "The BEA-2019 shared task on grammatical error correction",
            "venue": "Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 52\u201375,",
            "year": 2019
        },
        {
            "authors": [
                "Daniel Cer",
                "Mona Diab",
                "Eneko Agirre",
                "I\u00f1igo LopezGazpio",
                "Lucia Specia."
            ],
            "title": "SemEval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation",
            "venue": "Proceedings of the 11th International Workshop on Semantic",
            "year": 2017
        },
        {
            "authors": [
                "Jishnu Ray Chowdhury",
                "Yong Zhuang",
                "Shuyi Wang."
            ],
            "title": "Novelty controlled paraphrase generation with retrieval augmented conditional prompt tuning",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, 36(10):10535\u201310544.",
            "year": 2022
        },
        {
            "authors": [
                "cob Devlin",
                "Adam Roberts",
                "Denny Zhou",
                "Quoc V. Le",
                "Jason Wei"
            ],
            "title": "Scaling instruction-finetuned language models",
            "year": 2022
        },
        {
            "authors": [
                "Allan Collins",
                "Dedre Gentner."
            ],
            "title": "A framework for a cognitive theory of writing",
            "venue": "Cognitive processes in writing, pages 51\u201372. Erlbaum.",
            "year": 1980
        },
        {
            "authors": [
                "Daniel Dahlmeier",
                "Hwee Tou Ng",
                "Siew Mei Wu."
            ],
            "title": "Building a large annotated corpus of learner English: The NUS corpus of learner English",
            "venue": "Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages",
            "year": 2013
        },
        {
            "authors": [
                "William B. Dolan",
                "Chris Brockett."
            ],
            "title": "Automatically constructing a corpus of sentential paraphrases",
            "venue": "Proceedings of the Third International Workshop on Paraphrasing (IWP2005).",
            "year": 2005
        },
        {
            "authors": [
                "Wanyu Du",
                "Zae Myung Kim",
                "Vipul Raheja",
                "Dhruv Kumar",
                "Dongyeop Kang."
            ],
            "title": "Read, revise, repeat: A system demonstration for human-in-the-loop iterative text revision",
            "venue": "Proceedings of the First Workshop on Intelligent and Interactive Writing As-",
            "year": 2022
        },
        {
            "authors": [
                "Wanyu Du",
                "Vipul Raheja",
                "Dhruv Kumar",
                "Zae Myung Kim",
                "Melissa Lopez",
                "Dongyeop Kang."
            ],
            "title": "Understanding iterative revision from human-written text",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Vol-",
            "year": 2022
        },
        {
            "authors": [
                "Yupei Du",
                "Qi Zheng",
                "Yuanbin Wu",
                "Man Lan",
                "Yan Yang",
                "Meirong Ma."
            ],
            "title": "Understanding gender bias in knowledge base embeddings",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),",
            "year": 2022
        },
        {
            "authors": [
                "Jane Dwivedi-Yu",
                "Timo Schick",
                "Zhengbao Jiang",
                "Maria Lomeli",
                "Patrick Lewis",
                "Gautier Izacard",
                "Edouard Grave",
                "Sebastian Riedel",
                "Fabio Petroni."
            ],
            "title": "Editeval: An instruction-based benchmark for text improvements",
            "venue": "arXiv.",
            "year": 2022
        },
        {
            "authors": [
                "Felix Faltings",
                "Michel Galley",
                "Gerold Hintz",
                "Chris Brockett",
                "Chris Quirk",
                "Jianfeng Gao",
                "Bill Dolan."
            ],
            "title": "Text editing by command",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics:",
            "year": 2021
        },
        {
            "authors": [
                "Tao Fang",
                "Shu Yang",
                "Kaixin Lan",
                "Derek F. Wong",
                "Jinpeng Hu",
                "Lidia S. Chao",
                "Yue Zhang"
            ],
            "title": "Is chatgpt a highly fluent grammatical error correction system? a comprehensive evaluation",
            "year": 2023
        },
        {
            "authors": [
                "Katja Filippova",
                "Yasemin Altun."
            ],
            "title": "Overcoming the lack of parallel data in sentence compression",
            "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1481\u20131491, Seattle, Washington, USA. Association",
            "year": 2013
        },
        {
            "authors": [
                "Linda Flower."
            ],
            "title": "The dynamics of composing: Making plans and juggling constraints",
            "venue": "Cognitive processes in writing, pages 31\u201350.",
            "year": 1980
        },
        {
            "authors": [
                "Mor Geva",
                "Eric Malmi",
                "Idan Szpektor",
                "Jonathan Berant."
            ],
            "title": "DiscoFuse: A large-scale dataset for discourse-based sentence fusion",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics:",
            "year": 2019
        },
        {
            "authors": [
                "J. Edward Hu",
                "Abhinav Singh",
                "Nils Holzenberger",
                "Matt Post",
                "Benjamin Van Durme."
            ],
            "title": "Large-scale, diverse, paraphrastic bitexts via sampling and clustering",
            "venue": "Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL),",
            "year": 2019
        },
        {
            "authors": [
                "Srinivasan Iyer",
                "Xi Victoria Lin",
                "Ramakanth Pasunuru",
                "Todor Mihaylov",
                "D\u00e1niel Simig",
                "Ping Yu",
                "Kurt Shuster",
                "Tianlu Wang",
                "Qing Liu",
                "Punit Singh Koura"
            ],
            "title": "Opt-iml: Scaling language model instruction meta learning through the lens of generalization",
            "year": 2022
        },
        {
            "authors": [
                "Chao Jiang",
                "Mounica Maddela",
                "Wuwei Lan",
                "Yang Zhong",
                "Wei Xu."
            ],
            "title": "Neural CRF model for sentence alignment in text simplification",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7943\u20137960, On-",
            "year": 2020
        },
        {
            "authors": [
                "David Kauchak."
            ],
            "title": "Improving text simplification language modeling using unsimplified text data",
            "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1537\u20131546, Sofia, Bulgaria. As-",
            "year": 2013
        },
        {
            "authors": [
                "Zae Myung Kim",
                "Wanyu Du",
                "Vipul Raheja",
                "Dhruv Kumar",
                "Dongyeop Kang."
            ],
            "title": "Improving iterative text revision by learning where to edit from other revision tasks",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2022
        },
        {
            "authors": [
                "Brian Lester",
                "Rami Al-Rfou",
                "Noah Constant."
            ],
            "title": "The power of scale for parameter-efficient prompt tuning",
            "venue": "Proceedings of the 2021 Conference on",
            "year": 2021
        },
        {
            "authors": [
                "Haokun Liu",
                "Derek Tam",
                "Muqeeth Mohammed",
                "Jay Mohta",
                "Tenghao Huang",
                "Mohit Bansal",
                "Colin Raffel."
            ],
            "title": "Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning",
            "venue": "Advances in Neural Information Processing Systems.",
            "year": 2022
        },
        {
            "authors": [
                "Shayne Longpre",
                "Le Hou",
                "Tu Vu",
                "Albert Webson",
                "Hyung Won Chung",
                "Yi Tay",
                "Denny Zhou",
                "Quoc V Le",
                "Barret Zoph",
                "Jason Wei"
            ],
            "title": "The flan collection: Designing data and methods for effective instruction tuning",
            "venue": "arXiv preprint arXiv:2301.13688",
            "year": 2023
        },
        {
            "authors": [
                "Yao Lu",
                "Max Bartolo",
                "Alastair Moore",
                "Sebastian Riedel",
                "Pontus Stenetorp."
            ],
            "title": "Fantastically ordered prompts and where to find them: Overcoming fewshot prompt order sensitivity",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Compu-",
            "year": 2022
        },
        {
            "authors": [
                "Aman Madaan",
                "Yiming Yang."
            ],
            "title": "Neural language modeling for contextualized temporal graph generation",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language",
            "year": 2021
        },
        {
            "authors": [
                "Jonathan Mallinson",
                "Jakub Adamek",
                "Eric Malmi",
                "Aliaksei Severyn."
            ],
            "title": "EdiT5: Semi-autoregressive text editing with t5 warm-start",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2022, pages 2126\u20132138, Abu Dhabi, United Arab",
            "year": 2022
        },
        {
            "authors": [
                "Swaroop Mishra",
                "Daniel Khashabi",
                "Chitta Baral",
                "Hannaneh Hajishirzi."
            ],
            "title": "Cross-task generalization via natural language crowdsourcing instructions",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume",
            "year": 2022
        },
        {
            "authors": [
                "Courtney Napoles",
                "Keisuke Sakaguchi",
                "Matt Post",
                "Joel Tetreault."
            ],
            "title": "Ground truth for grammatical error correction metrics",
            "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Confer-",
            "year": 2015
        },
        {
            "authors": [
                "Courtney Napoles",
                "Keisuke Sakaguchi",
                "Joel Tetreault."
            ],
            "title": "JFLEG: A fluency corpus and benchmark for grammatical error correction",
            "venue": "Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics:",
            "year": 2017
        },
        {
            "authors": [
                "Paul Christiano",
                "Jan Leike",
                "Ryan Lowe."
            ],
            "title": "Training language models to follow instructions with human feedback",
            "venue": "Advances in Neural Information Processing Systems.",
            "year": 2022
        },
        {
            "authors": [
                "Paul F Christiano",
                "Jan Leike",
                "Ryan Lowe."
            ],
            "title": "Training language models to follow instructions with human feedback",
            "venue": "Advances in Neural Information Processing Systems, volume 35, pages 27730\u201327744. Curran Associates, Inc.",
            "year": 2022
        },
        {
            "authors": [
                "Reid Pryzant",
                "Richard Diehl Martinez",
                "Nathan Dass",
                "Sadao Kurohashi",
                "Dan Jurafsky",
                "Diyi Yang."
            ],
            "title": "Automatically neutralizing subjective bias in text",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, 34(01):480\u2013489.",
            "year": 2020
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J. Liu."
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "Journal of Machine Learning Research,",
            "year": 2020
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J. Liu."
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "J. Mach. Learn. Res., 21(1).",
            "year": 2020
        },
        {
            "authors": [
                "Sudha Rao",
                "Joel Tetreault."
            ],
            "title": "Dear sir or madam, may I introduce the GYAFC dataset: Corpus, benchmarks and metrics for formality style transfer",
            "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computa-",
            "year": 2018
        },
        {
            "authors": [
                "Jeff Rasley",
                "Samyam Rajbhandari",
                "Olatunji Ruwase",
                "Yuxiong He."
            ],
            "title": "Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters",
            "venue": "Proceedings of the 26th ACM SIGKDD International Conference on",
            "year": 2020
        },
        {
            "authors": [
                "Machel Reid",
                "Graham Neubig."
            ],
            "title": "Learning to model editing processes",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2022, pages 3822\u20133832, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.",
            "year": 2022
        },
        {
            "authors": [
                "Emily Reif",
                "Daphne Ippolito",
                "Ann Yuan",
                "Andy Coenen",
                "Chris Callison-Burch",
                "Jason Wei."
            ],
            "title": "A recipe for arbitrary text style transfer with large language models",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics",
            "year": 2022
        },
        {
            "authors": [
                "Nils Reimers",
                "Iryna Gurevych."
            ],
            "title": "SentenceBERT: Sentence embeddings using Siamese BERTnetworks",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natu-",
            "year": 2019
        },
        {
            "authors": [
                "drea Santilli",
                "Thibault Fevry",
                "Jason Alan Fries",
                "Ryan Teehan",
                "Teven Le Scao",
                "Stella Biderman",
                "Leo Gao",
                "Thomas Wolf",
                "Alexander M Rush"
            ],
            "title": "Multitask prompted training enables zero-shot task generalization",
            "venue": "In International Conference on Learning",
            "year": 2022
        },
        {
            "authors": [
                "Timo Schick",
                "Jane A. Yu",
                "Zhengbao Jiang",
                "Fabio Petroni",
                "Patrick Lewis",
                "Gautier Izacard",
                "Qingfei You",
                "Christoforos Nalmpantis",
                "Edouard Grave",
                "Sebastian Riedel."
            ],
            "title": "PEER: A collaborative language model",
            "venue": "International Conference on Learning",
            "year": 2023
        },
        {
            "authors": [
                "Sanja \u0160tajner",
                "Kim Cheng Sheang",
                "Horacio Saggion."
            ],
            "title": "Sentence simplification capabilities of transferbased models",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, 36(11):12172\u201312180.",
            "year": 2022
        },
        {
            "authors": [
                "Toshikazu Tajiri",
                "Mamoru Komachi",
                "Yuji Matsumoto."
            ],
            "title": "Tense and aspect error correction for ESL learners using global context",
            "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),",
            "year": 2012
        },
        {
            "authors": [
                "Rohan Taori",
                "Ishaan Gulrajani",
                "Tianyi Zhang",
                "Yann Dubois",
                "Xuechen Li",
                "Carlos Guestrin",
                "Percy Liang",
                "Tatsunori B. Hashimoto."
            ],
            "title": "Stanford alpaca: An instruction-following llama model",
            "venue": "https:// github.com/tatsu-lab/stanford_alpaca.",
            "year": 2023
        },
        {
            "authors": [
                "Marie M. Vaughan",
                "David D. McDonald."
            ],
            "title": "A model of revision in natural language generation",
            "venue": "24th Annual Meeting of the Association for Computational Linguistics, pages 90\u201396, New York, New York, USA. Association for Computational Linguis-",
            "year": 1986
        },
        {
            "authors": [
                "Shailaja Keyur Sampat",
                "Siddhartha Mishra",
                "Sujan Reddy A",
                "Sumanta Patro",
                "Tanay Dixit",
                "Xudong Shen."
            ],
            "title": "Super-NaturalInstructions: Generalization via declarative instructions on 1600+ NLP tasks",
            "venue": "Proceedings of the 2022 Conference on Empiri-",
            "year": 2022
        },
        {
            "authors": [
                "Jason Wei",
                "Maarten Bosma",
                "Vincent Zhao",
                "Kelvin Guu",
                "Adams Wei Yu",
                "Brian Lester",
                "Nan Du",
                "Andrew M. Dai",
                "Quoc V Le."
            ],
            "title": "Finetuned language models are zero-shot learners",
            "venue": "International Conference on Learning Representations.",
            "year": 2022
        },
        {
            "authors": [
                "Kristian Woodsend",
                "Mirella Lapata."
            ],
            "title": "Learning to simplify sentences with quasi-synchronous grammar and integer programming",
            "venue": "Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 409\u2013420, Edinburgh,",
            "year": 2011
        },
        {
            "authors": [
                "Haoran Wu",
                "Wenxuan Wang",
                "Yuxuan Wan",
                "Wenxiang Jiao",
                "Michael Lyu"
            ],
            "title": "Chatgpt or grammarly? evaluating chatgpt on grammatical error correction benchmark",
            "year": 2023
        },
        {
            "authors": [
                "Wei Xu",
                "Chris Callison-Burch",
                "Courtney Napoles."
            ],
            "title": "Problems in current text simplification research: New data can help",
            "venue": "Transactions of the Association for Computational Linguistics, 3:283\u2013297.",
            "year": 2015
        },
        {
            "authors": [
                "Wei Xu",
                "Courtney Napoles",
                "Ellie Pavlick",
                "Quanze Chen",
                "Chris Callison-Burch."
            ],
            "title": "Optimizing statistical machine translation for text simplification",
            "venue": "Transactions of the Association for Computational Linguistics, 4:401\u2013415.",
            "year": 2016
        },
        {
            "authors": [
                "Xingxing Zhang",
                "Mirella Lapata."
            ],
            "title": "Sentence simplification with deep reinforcement learning",
            "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 584\u2013 594, Copenhagen, Denmark. Association for Compu-",
            "year": 2017
        },
        {
            "authors": [
                "Zhemin Zhu",
                "Delphine Bernhard",
                "Iryna Gurevych."
            ],
            "title": "A monolingual tree-based translation model for sentence simplification",
            "venue": "Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1353\u20131361, Beijing,",
            "year": 2010
        },
        {
            "authors": [
                "BLEU (Zhu"
            ],
            "title": "2018) to measure the diversity of the paraphrases relative to the given source and ref",
            "year": 2018
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Large language models (LLMs) have made remarkable progress toward generating coherent text in a wide variety of tasks and domains to support writing assistance (Du et al., 2022a; Mallinson et al., 2022; Schick et al., 2023), such as grammatical error correction (Wu et al., 2023), text simplification (\u0160tajner et al., 2022), paraphrasing (Chowdhury et al., 2022), and style transfer (Reif et al., 2022). One of the emergent abilities of LLMs is the capability to generalize to unseen tasks by following new or composed instructions. Instruction-tuning, where LLMs are fine-tuned on a collection of tasks phrased as instructions, makes the models more adept at interpreting and following instructions, reducing the need for few-shot exemplars (Sanh et al., 2022; Ouyang et al., 2022b; Wei et al., 2022; Chung et al., 2022b).\n1Code, data, and models available at https://github. com/vipulraheja/coedit\nText editing is a complex task because human writers cannot simultaneously grasp multiple demands and constraints of the task and tend to iterate and revise their work multiple times (Flower, 1980; Collins and Gentner, 1980; Vaughan and McDonald, 1986). This poses a significant challenge for intelligent writing assistants.\nIn this work, we aim to improve the capabilities of instruction-tuned models for text editing by leveraging instruction-tuning from diverse tasks of text editing benchmarks. While multiple previous works have attempted to develop general-purpose text editing models using LLMs, they are either not trained with instruction-tuning (Du et al., 2022c; Kim et al., 2022), trained on much smaller models or not trained on task-specific datasets (Mallinson et al., 2022; Schick et al., 2023), or are not publicly available (Schick et al., 2023), which limits their effectiveness, performance, or usability.\nWe introduce COEDIT, a text editing system designed to provide writing assistance with a natural language interface. A user can employ COEDIT by providing natural language instructions such as \"Paraphrase the sentence\" or \"Fix the grammar\". Our experiments demonstrate that fine-tuning in-\nstructions for specific tasks is more effective than multi-task learning and general-purpose instruction tuning. We conjecture that task-specific instructions increase the density of the instruction space, reinforcing the complementary effects of multiple tasks and facilitating their generalization to composite and new text editing tasks, as shown in Fig. 2.\nTo build COEDIT, we fine-tune a pre-trained sequence-to-sequence model on a parallel corpus of instruction-based 82K input-output pairs. The inputs and outputs are sourced from publicly available corpora for different text editing tasks, and the instructions are constructed based on rules that introduce lexical and semantic variations.\nOur main contributions are as follows: \u2022 We achieve state-of-the-art performance on mul-\ntiple text editing tasks: grammatical error correction, text simplification, sentence fusion, iterative text editing, and three stylistic editing tasks (formality style transfer, neutralization, and paraphrasing). \u2022 We find that even our smallest instruction-tuned model outperforms other supervised text editing models, instruction-tuned models, and generalpurpose LLMs with nearly 60x greater parameters, on both manual and automatic evaluations. \u2022 COEDIT generalizes well to new, adjacent tasks not seen while fine-tuning, as well as composite instructions with multiple task specifications. \u2022 Our data and models will be publicly available."
        },
        {
            "heading": "2 Related Work",
            "text": "Large Language Models for Text Editing In general, our work is related to many prior works that leverage LLMs; for instance, finetuning T5 (Raffel et al., 2020a) on pairs of original and edited text (Faltings et al., 2021; Reid and Neubig, 2022; Mallinson et al., 2022; Du et al., 2022a,b; Kim et al., 2022). However, these aforementioned works are either not based on instruction tuning, use different modeling techniques such as tag-based se-\nquence labeling, or are not general enough to work on multiple text editing tasks. Moreover, several LLMs are trained to solve specific tasks only, such as grammar errors (Mallinson et al., 2022; Fang et al., 2023), text simplification (\u0160tajner et al., 2022), paraphrase generation (Chowdhury et al., 2022), or style transfer (Reif et al., 2022), which limits their generalizability.\nInstruction Tuning for Writing Assistance Explicitly teaching models how to follow natural language instructions is closely related to recent work for fine-tuning models using large datasets of human-written instructions (Wei et al., 2022; Mishra et al., 2022; Sanh et al., 2022; Ouyang et al., 2022a; Wang et al., 2022; Iyer et al., 2022; Bach et al., 2022; Longpre et al., 2023). Recently, advanced data augmentation and instruction tuning, starting with the Flan models (Chung et al., 2022b), have shown that strong results stem both from the larger and more diverse set of tasks. Additionally, enriching task diversity and balancing task sources (Sanh et al., 2022) are shown to be critical to performance, suggesting instruction-tuned models offer a more computationally-efficient starting checkpoint for downstream applications, corroborating Liu et al. (2022) and Aribandi et al. (2022).\nOn instruction tuning for writing assistance, our work is closely related to PEER (Schick et al., 2023), who fine-tuned T5-based LLMs by following user-provided text-editing plans to perform the said edits. There are a few significant differences in our approach compared to PEER. While PEER attempts to either create or leverage a user-provided plan, realize the edits conditioned on the plan, and try to explain the plan, we focus only on the plan and edit parts of the pipeline. Even when it comes to handling editing plans in the form of natural language instructions, our work focuses on edits that do not add new information. Therefore, we compare our models only against PEER-Edit models.\nFinally, no prior works, to the best of our knowledge, have investigated the ability of instructiontuned LLMs for text editing to generalize to composite instructions."
        },
        {
            "heading": "3 COEDIT",
            "text": ""
        },
        {
            "heading": "3.1 Training Dataset",
            "text": "Our dataset creation is based on the ITERATER+ dataset proposed by Kim et al. (2022) who combined datasets from various text editing tasks (See Table 1). Their work, in turn, is based on Du et al.\n(2022b), who categorized each edit into MEANINGCHANGED or NON-MEANING-CHANGED. Edits that belong to the latter group are further assigned to FLUENCY, COHERENCE, CLARITY, or STYLE. The aforementioned taxonomy of edit intents from ITERATER reflects writers\u2019 general intention behind their revision, providing more in-depth information than just superficial edit operations, such as ADD and DELETE.\nSimilar to Kim et al. (2022), our work focuses on non-MEANING-CHANGED edits. We consider those edits to be ones that do not add new information or perform fact updates. Since the STYLE edits are quite subjective in nature, we allow for the possibility of meaning change so as to fulfill the needs of making stylistic edits, but we constrain the editing tasks to ensure the edited texts are semantically similar to the sources, but not to the extent of adding new information or fact updates. With this in mind, we expand the STYLE edit intention category from ITERATER+ to include three new sub-intentions: Paraphrasing, Formality Style Transfer (or Formalization), and Neutralization.\nThe aforementioned ITERATER dataset taxonomy lends itself conveniently to be articulated as natural language instructions and allows us to naturally formulate them into instructional prompts (See Table 1). We rewrite each edit intention as a set of natural language instruction prompts to create the COEDIT dataset. To allow models to adapt to linguistic variations of the instructions, we also include paraphrases of the instruction templates, e.g., instead of \u201cWrite\" we also use \u201cGenerate\u201d or \"Rewrite,\" or instead of \u201cParaphrase the text\u201d we use \u201cRewrite the text with different wording,\" and so on. For each task, we develop a variety of such diverse instructional prompts and ran-\ndomly sample an instruction from the aforementioned group of task-specific instruction candidates to be pre-pended to the source in order to form an <instruction: source, target> data pair. We provide the full list of our instructional prompts in \u00a7C. In total, our training dataset consists of around 82K <instruction: source, target> pairs. We keep the original train-validation-test splits consistent as the original datasets but diversify the train and validation splits with the paraphrasing augmentations. The details of datasets and instructions used to train our models are described in \u00a7A."
        },
        {
            "heading": "3.2 Text Editing Models",
            "text": "We fine-tune different versions of pre-trained FLANT5 (Chung et al., 2022a) models on the COEDIT dataset. Specifically, we use FLANT5-L (770M parameters), FLANT5-XL (3B parameters), FLANT5-XXL (11B parameters) models, which are henceforth referred to as COEDIT-L, COEDIT-XL, and COEDIT-XXL respectively. The training details are summarized in \u00a7D."
        },
        {
            "heading": "4 Experimental Setup",
            "text": "We conduct experiments to determine if a standard instruction-tuned language model fine-tuned using task-specific data can improve text editing performance and if it can further generalize into a general-purpose text editing model capable of following human-written instructions and handling a wider array of editing tasks, such as unseen and composite instructions. Specifically, we aim to answer the following research questions: \u2022 RQ1: Can COEDIT follow text editing instruc-\ntions and perform high-quality edits across a wide variety of tasks?\n\u2022 RQ2: Is COEDIT generalizable to perform high-quality edits for new types of text editing instructions? \u2022 RQ3: Does COEDIT make the writing process more efficient and effective for human writers? We answer these questions via quantitative analyses of model outputs (Section 5) and via qualitative analyses and human evaluations of model outputs (Section 6). Further, we investigate RQ2 along two dimensions: (1) generalization to composite instructions containing combinations of multiple different kinds of edits and (2) out-of-domain generalization to instructions with new task requirements on previously unseen data."
        },
        {
            "heading": "4.1 Models",
            "text": "No-Edits Baseline We first evaluate a no-edits baseline, where the output is simply a copy of the source input without the instruction. This strategy performs reasonably well on tasks where the target output largely overlaps with the input (e.g., GEC).\nSupervised Text Editing Models We also evaluate existing LLMs for text editing that are not finetuned with instruction-specific data. Specifically, to understand the effect of task-specific fine-tuning, we evaluate against T52 (Raffel et al., 2020b) models as primary alternatives of our FLAN-T5 models. We also compare our models against ITERATER (Du et al., 2022b) and DELITERATER (Kim et al., 2022), which have shown strong performance on a variety of text editing tasks.3\nInstruction-tuned LLMs A major group of our comparisons is against instruction-tuned LLMs: \u2022 Our main comparison is against PEER (Schick\net al., 2023), which is primarily based on the LM Adapted variant of T5. As the focus of our work is on improving revision quality (Section 2), we compare against PEER-EDIT (both 3B and 11B versions). \u2022 T0, T0++ (Sanh et al., 2022) and Tk-Instruct (Wang et al., 2022), which are all initialized from the LM Adapted variant of T5, and finetuned using PromptSource (Bach et al., 2022), and Super-NaturalInstructions (Wang et al., 2022) datasets, respectively.\n2The original T5 model cannot continue text well due to its infilling pre-training objective. Hence, similar to Schick et al. (2023), we evaluate its LM Adapted versions (Lester et al., 2021), which are trained with a language modeling objective.\n3We are unable to make full comparisons against EdiT5 (Mallinson et al., 2022) and PEER (Schick et al., 2023) as the models are not publicly available.\n\u2022 Alpaca (Taori et al., 2023) is an instructiontuned version of the LLaMA-7B model (Touvron et al., 2023) trained on 52K instructionfollowing demonstrations generated by GPT3. \u2022 We also compare InstructGPT (Ouyang et al., 2022a), a variant of GPT3 fine-tuned via reinforcement learning on a large dataset of instructions and human-written outputs.4 \u2022 GPT3.5 (henceforth referred to as ChatGPT), is an improved version of InstructGPT optimized for chat. We utilize OpenAI\u2019s API for all inference tasks.5 \u2022 GPT3 also offers a text Editing API6 (we refer to as GPT3-Edit), which is usable for editing tasks rather than completion, making it directly comparable to the tasks we train COEDIT on.\nLarge-Pretrained Decoder-only Models We compare against LLMs with no instruction tuning in two settings \u2013 zero-shot and few-shot (details in Section 5.1): \u2022 The 175B GPT3 (Brown et al., 2020) model\nthat is not instruction-tuned demonstrates strong general-purpose text revision capabilities. \u2022 LLaMA (Touvron et al., 2023) is Meta AI\u2019s general-purpose language model trained only on publicly available data. We utilize the 7B model due to computing constraints.\nOutputs of all models were generated using greedy decoding unless specified otherwise."
        },
        {
            "heading": "4.2 Test Datasets",
            "text": "To assess the editing capabilities of COEDIT, we perform evaluations on standard test sets sourced from a variety of text editing task benchmarks, most notably, EDITEVAL (Dwivedi-Yu et al., 2022). Owing to the overlap of our work with PEER, we keep our evaluation datasets and evaluation metrics as close to theirs as possible for consistency: We used JFLEG (Napoles et al., 2017) for grammatical error collection, TurkCorpus (Xu et al., 2016) and ASSET (Alva-Manchego et al., 2020) for text simplification, Coherence split of ITERATER (Du et al., 2022b) and the DISCOFUSE dataset (Geva et al., 2019) for coherence, and ITERATER (Du et al., 2022b) for iterative text revision. For Stylerelated edits, we used GYAFC (Rao and Tetreault, 2018) for formality style, WNC (Pryzant et al., 2020) for neutralization, and MRPC (Dolan and\n4We use text-davinci-003 5We use gpt-3.5-turbo 6We use text-davinci-edit-001\nBrockett, 2005), STS (Cer et al., 2017), and QQP for paraphrasing. Detailed descriptions of each dataset and its evaluation metrics are in \u00a7B."
        },
        {
            "heading": "5 Quantitative Results",
            "text": ""
        },
        {
            "heading": "5.1 Text Editing Performance",
            "text": "Table 2 helps us answer RQ1 by comparing the performance of COEDIT to other models across various text editing tasks. We first present results from the more well-known evaluation sets here and present additional results (i.e., sub-tasks and additional datasets) in Table 11.\nWe segregate the models into seven groups. The first group (a) consists of the copy baseline and T5-LARGE baseline fine-tuned with prefix-tuning (each data point is prefixed with task-specific tags rather than instructions), while the second group (b) consists of instruction-fine-tuned T5-based models on non-text-editing tasks. We find that COEDIT substantially outperforms these models across all tasks.\nThe next two groups (c, d) show different LLMs varying from 7B to 176B parameters in size, evaluated in a zero-shot setting. Those in group (c) are decoder-only models, while those in group (d) are instruction-tuned. We find that COEDIT outperforms all LLMs comparable to its model size (e.g., Alpaca and LLaMA) across all tasks, as well as on most tasks compared to models several times larger, such as ChatGPT and InstructGPT. This indicates that current general-purpose and instruction-tuned models are underfitted, and it is beneficial to densify the task/instruction space rather than to scale model size.\nAlthough models such as Alpaca and T5-based models (Tk-instruct, T0, T0++) have previously shown strong capabilities for zero-shot tasks, they show weaker performance compared to COEDIT. We also see that the decoder-only models (e.g., GPT3 and LLaMA) often repeat the input for more complex tasks, such as ones under the Style intent group. This can be attributed to difficulty understanding the prompted task, resulting in the models either repeating the input sentence or generating a continuation unrelated to the task.\n7Since PEER had several scores missing, and due to the high scores of paraphrasing transfer, for fairness, it was left out of the Overall score calculations. For results with multiple metrics, the best-performing method is calculated by taking the average. For the MRPC average, we subtract the SelfBLEU score from 100 since lower is better.\nNext, in the fifth group (e), we evaluate the LLMs under a few-shot setting. As mentioned in Section 4.1, we conduct these experiments in a 4-shot evaluation setting, where example inputs were constructed by randomly sampling four inputs for each task from the COEDIT dataset such that all examples chosen would fit in the input window for all models as seen in (Brown et al., 2020). The input sentence and its corresponding revised reference were pre-pended to the instructional prompt. We conduct few-shot evaluations for decoder-only LLMs (GPT3) and three instruction-tuned LLMs (InstructGPT, ChatGPT, and Alpaca). Outputs of all models were generated using greedy decoding unless specified otherwise.\nWe observe that giving specific examples improves performance in all models for all tasks except MRPC for GPT3. This may be because GPT3 still exhibits some similar behavior in repeating its generations continuously, resulting in a low BLEU score but low semantic similarity as well. We don\u2019t present any experiments for GPT3-Edit under the few-shot setting, as scores tended to stay the same across all tasks \u2013 implying that GPT3-Edit may not have as good in-context learning capabilities. Overall, we find that even our smallest 770M parameter model is competitive against LLMs evaluated in a few-shot setting in most tasks.\nIn the final group (f), we compare our models against task-specific text editing models such as ITERATER, DELITERATER, and PEER. ITERATER and DELITERATER perform comparatively worse than the scores reported in the original paper as we present different and more difficult inputs, only pre-pending instructions to the inputs while ITERATER and DELITERATER were trained with task-specific tags. Furthermore, they were trained using BART and Pegasus, respectively, both of which have a summarization pre-training objective, and were not trained to follow instructions. On average, COEDIT beats PEER across all reported evaluations except the ITERATER benchmark. This can primarily be attributed to the difference in taskspecific fine-tuning since PEER uses Wikipedia as the source of instructional edit data."
        },
        {
            "heading": "5.2 Ablation Studies",
            "text": "Table 3 shows the performance of various baselines, which we discuss in detail in this section.\nInstruction Tuning. To understand the effectiveness of instruction-tuning, we fine-tune the 3B pa-\nrameter T5 model (T5-XL) and compare it with COEDIT-XL, its FLANT5 counterpart on the same training and validation sets. The only change is that the instructional prompts for the training datasets are replaced by task-specific prefixes. Specifically, the 82k <instruction: source, target> pairs in the training dataset used to train the COEDIT models were modified to <task: source, target>\n8. We observe (Table 3(a)) that the instruction-tuned COEDIT models consistently outperform prefix-tuned T5 models, showing the ef-\n8 task was one of gec, simplify, clarify,\ncoherence, formalize, neutralize and paraphrase\nfectiveness of instruction-tuning over prefix-tuning.\nTask-Specific Training. A core contribution of this work is to push the performance of small- (<1B parameters) to medium-sized (1-10B parameters) LLMs for common text editing tasks. This drives the need for fine-tuning on task-specific datasets. The impact of this task-specific data augmentation for text editing tasks has already been shown in Kim et al. (2022). For this work, we compare our task-specific fine-tuned models against their FLANT5 un-tuned counterparts referred to as FLANT5-XL (Table 3(b)). We see a substantial gap\nbetween the two for all datasets and model sizes, thus, confirming prior findings.\nQuality of Instructions. While we developed with a limited set of task-specific instructional prompts, there has been widespread work on the prompt sensitivity of LLMs, especially with growing model capacity (Lu et al., 2022). To assess the robustness of COEDIT models on instructional prompts, we train another baseline COEDIT-XL model with randomized task-specific instructions (henceforth referred to as COEDIT-XL-R). Specifically, the entire training dataset was randomized, where an instruction from one task was replaced randomly by an instruction from another task. Table 3(c) shows the results for this experiment. We observe that while COEDIT-XL-R achieves scores that are higher than the non-task-specific tuned FLANT5-XL (especially on edit-based metrics such as SARI), it significantly falls behind COEDIT-XL on those, as well as on the style accuracy metrics such as formality transfer accuracy and paraphrasing semantic similarity. This indicates that while the instructional structure of the inputs and task-specific training makes the model learn how to make edits (which drives up the SARI scores), however, the accuracy of those edits suffers since they are trained with the wrong instructions most of the time. Overall, the improvements highlight the positive impact of task-specific training, and the gaps in performance highlight the negative impact of lack of proper instruction tuning."
        },
        {
            "heading": "6 Qualitative Results",
            "text": "We now address RQ2 and RQ3 (Section 4). We show that COEDIT shows generalization abilities to adjacent tasks not seen during fine-tuning and can generalize to composite instructions containing a combination of tasks. Further, our human evaluation studies show that expert human evaluators find the text generated by COEDIT to be of higher quality than a much larger instruction-tuned LLM."
        },
        {
            "heading": "6.1 Text Editing Quality",
            "text": "Since text editing is often subjective, and automatic metrics are not always accurate in measuring if an instruction is satisfied, we conduct human evaluations for our model outputs by linguistic experts on 50 test inputs to ensure they meet the instructional constraints. Given the automatic evaluation results in Section 5, we compare our 3B-parameter COEDIT-XL model against the largest comparable 175B instruction-tuned LLM for text editing GPT3-EDIT. Specifically, we conducted a pairwise comparison: each annotator was shown an instructional input and outputs from both models (they were not aware which output was generated by which model). They were then asked to evaluate the fluency, accuracy, and meaning preservation of the edited texts and choose the higher-quality output (\"neither\" and \"tie\" are also valid options). We collect three annotations for each question and use the majority vote as the final judgment.\nTable 4 shows the results of the evaluation. The annotators prefer our COEDIT model for 64% of the inputs, whereas, for 10% of the inputs, GPT3EDIT\u2019s output is preferred. In 4% cases, both models produce equally good outputs, whereas, for 22% of the inputs, both models generate unacceptable outputs. Table 12 provides a side-by-side comparison of the outputs generated by the two models."
        },
        {
            "heading": "6.2 Generalizability to Adjacent Tasks",
            "text": "We analyze the generalization capabilities of our models by evaluating them on a few related tasks that do not exist in the fine-tuning data. Specifically, we chose two standard NLP tasks \u2013 sentence compression (SC) (Filippova and Altun, 2013) and politeness transfer (PT) (Madaan and Yang, 2021). It is noteworthy that while our models were not fine-tuned on these exact tasks, we chose them so that the models could still comprehend them based\non other tasks they were fine-tuned on. We define them as being adjacent tasks, which still exist within the scope of existing tasks but have not been seen during fine-tuning (blue lines in Fig. 2).\nSimilar to the previous experiment, in addition to GPT3-EDIT, we compare COEDIT-XL against the similarly-sized prefix-tuned (T5-XL) model and the non-task-specific trained FLANT5-XL model (same models as the ones used in Table 3 (a) and (b)). For evaluation, we curated a set of new instructional prompts geared towards both the new tasks (details in Appendix C). We evaluated the models on the respective test datasets from Filippova and Altun (2013) and Madaan and Yang (2021).\nTable 5 shows the results of COEDIT-XL against various models on the sentence compression and politeness transfer tasks. For SC, we report the SARI metric for rewrite quality and compression ratio (CR) for task-specific quality. For PT, we report Self-BLEU (Zhu et al., 2018) for the rewrite quality9 and Transfer Accuracy (TA) for the taskspecific quality. We observe that COEDIT consistently outperforms other models on both tasks, which indicates its generalization abilities on these new and unseen adjacent tasks. It is noteworthy that GPT3-EDIT performs quite well out-of-thebox on PT, but not so much on the SC task."
        },
        {
            "heading": "6.3 Generalizability to Composite Instructions",
            "text": "Finally, we also explore the capability of our model to understand composite natural language instruc-\n9We report Self-BLEU based on the original PT paper since there are no references provided in the dataset.\ntions. Composite instructions are made up of a combination of tasks. For example, for the composite instruction, \"Make the text simpler, paraphrase it, and make it formal\", the model needs to simultaneously perform simplification, paraphrasing and formalization of the input sentence.\nSince there is no publicly available dataset for composite instructions, we create the COEDIT-COMPOSITE dataset by expanding the COEDIT dataset to a total of 90k pairs. In addition to the single-task instructions, we use seven new combinations of instructions as part of our training set, with each composite instruction having either two or three tasks. Specifically, these are GEC-Paraphrasing, GECSimplification, GEC-Paraphrasing-Simplification, Formality-Paraphrasing, Formality-Simplification, Formality-Paraphrasing-Simplification, and Paraphrasing-Simplification (more details in \u00a7A). We then fine-tune the FLANT5-XL model on COEDIT-COMPOSITE (referred as COEDITXL-C). The training details are summarized in \u00a7D.\nWe evaluate COEDIT-XL-C on both single and composite instructions. For the single instructions, we use the same evaluation setup as in Table 2 and find that the overall performance of COEDITXL-C is on par with that of COEDIT-XL (Table 6). This shows that training the model additionally on composite prompts has no negative impact on single-task performance.\nFor composite instructions, we conduct human evaluations since there is no standard test dataset available. We use three new task combinations in addition to the seven seen during training to evaluate the model\u2019s generalizability. These are Coherence-Paraphrase, Coherence-Simplify, and Coherence-Simplify-Paraphrase. Specifically, we conduct two sets of pairwise annotations (similar setup as the one in Section 6.1) comparing COEDIT-XL-C with GPT3-EDIT and COEDITXL (shown in Table 7) on 30 composite instructions. For a fair comparison against COEDIT-XL, we pre-\npare a chaining pipeline10 by decomposing composite instructions into a sequence of multiple single instructions and executing them one-by-one. In 38% of cases, experts show a preference for COEDITXL-C, compared to 34% for GPT3-EDIT. In 3% cases, both models are preferred equally, whereas, for 25% of the cases, none of them are preferred. The experts prefer COEDIT-XL-C for 34% of the cases versus 21% for the chaining baseline. Both outputs are preferred equally in 14% cases, whereas, for 31% of the cases, both models generate unacceptable predictions. Table 13 provides a side-by-side comparison of outputs generated by these models."
        },
        {
            "heading": "7 Conclusions",
            "text": "We present COEDIT \u2013 an open-sourced dataset and set of instruction-tuned large language models that can act as a writing assistant by following natural language instructions to perform various textual edits by removing, updating, or adding words, phrases, and sentences. COEDIT achieves state-ofthe-art performance on multiple text editing benchmarks, spanning syntactic, semantic, and stylistic edit requirements. Through extensive experiments, we have shown that COEDIT is capable of further generalizing to unseen, adjacent, and composite instructions to perform edits along multiple dimensions in a single turn. In our human evaluations, we observe that COEDIT can assist writers with various aspects of the text revision process at scale by following natural language instructions.\nLimitations\nAlthough COEDIT achieves state-of-the-art performance on multiple text editing benchmarks, we acknowledge some limitations to our approach and evaluation methods. Our task-specific fine-tuning (like most other works) mainly focuses on sentencelevel editing tasks, and its effectiveness on much longer sequences of texts that are more appropriate to real-world editing settings remains to be seen. Additionally, our system mainly focuses on nonmeaning-changing text edits, thus, which could potentially limit the utility of our model to more real-world scenarios where fact-based editing or corrections are needed. Another limitation of our\n10Chaining increases the inference time, and the ordering of the tasks in the sequence is also likely to result in different outputs. We leave the optimal ordering of the tasks in prompt chaining for future work.\nwork involves prompt sensitivity. While we construct our inputs by randomly choosing from a pool of verbalizers for every task, we acknowledge that different prompts may induce better or worse edits, and as we evaluate each input with a random verbalizer, a fully controlled comparison for each available prompt across all models is not done. Furthermore, the prompting format was kept uniform across all evaluated models, whereas some models may perform better with a different prompting format. We plan to address this in future work. Finally, computing resource requirements could pose some difficulty in replicating the results (which we try to address by sharing our models publicly).\nEthics Statement\nSince our work mainly focuses on non-meaningchanging text edits, we are able to avoid many issues involving generating harmful text. Although, there is still a possibility of small meaning changes for stylistic tasks, we try to reduce the chance of hallucinations by constraining the generation to strictly edit tasks in order to reduce the chance of adding any new information, or perpetuating biases."
        },
        {
            "heading": "Acknowledgements",
            "text": "We sincerely thank Alice Kaiser-Schatzlein, Robyn Perry, Maya Barzilai, and Claudia Leacock for providing their invaluable linguistic expertise and insightful feedback with the evaluations. We also thank Max Gubin, Leonardo Neves, and Vivek Kulkarni for their helpful suggestions."
        },
        {
            "heading": "A Training Dataset Description",
            "text": "In this section, we discuss the details of the datasets used to create our training datasets and also expand on the dataset creation pipeline. For both COEDIT and COEDIT-COMPOSITE, we use the following datasets:\nFluency: We use three prominent corpora for GEC: the NUS Corpus of Learner English (NUCLE) (Dahlmeier et al., 2013), the W&I-LOCNESS (Bryant et al., 2019), and the NAIST LANG-8 Corpus of Learner English (Tajiri et al., 2012), which is one of the largest and most widely used datasets for GEC.\nClarity: We split Clarity into two sub-tasks, one focused on Text Simplification, and the other category focused on the set of edits outside of Simplification. In total, we use five corpora for Clarity tasks: Four of them - the NEWSELA corpus (Xu et al., 2015), WIKILARGE (Zhu et al., 2010; Woodsend and Lapata, 2011; Kauchak, 2013), WIKIAUTO (Jiang et al., 2020) and a subset from PARABANKV2 corpus (Hu et al., 2019) focus on text simplification, and the last one comes from the Clarity split of ITERATER (Du et al., 2022b).\nCoherence: We use the DISCOFUSE dataset (Geva et al., 2019), as it involves linking two given sentences as coherently as possible using edit operations such as inserting discourse connectives.\nStyle: Owing to the subjective nature of STYLE edits based on different sub-intentions (eg. conveying writers\u2019 writing preferences, including emotions, tone, and voice, etc.). We use the following datasets for making different stylistic edits to reflect those distinctions:\n\u2022 Formality: We use Grammarly\u2019s Yahoo Answers Formality Corpus (GYAFC) (Rao and Tetreault, 2018) which is a parallel corpus of informal and formal sentence pairs from two different domains.\n\u2022 Neutralization: We use WNC (Pryzant et al., 2020), a dataset from the Subjective Bias Neutralization task, where the objective is to remove or mitigate biased words to make sentences more neutral;\n\u2022 Paraphrasing: For paraphrase generation, we used the PARABANKV2 corpus (Hu et al., 2019), since it is a large-scale corpus that contains multiple diverse sentential paraphrases.\nOnce the raw datasets were collected, we randomly sampled them to the quantities mentioned in Table 1 based on a few heuristics such as old word retention, complexity ratios, dependency tree depth ratio, and character length ratio. The sampled pairs were then modified by prefixing the source texts with task-specific verbalizers (Appendix C) to convert a <source, target> pair to a <instruction: source, target> pair. All our models were then fine-tuned on the verbalized dataset.\nComposite instructions: Table 8 shows the composition of the COEDIT-COMPOSITE dataset, in addition to the details about datasets and prompts. We use seven such composite instructions during model training. For the first three composite prompts (GEC-Paraphrasing, GEC-Simplification, GEC-Paraphrasing-Simplification), we use GEC datasets to extract datapoints that show simplification and paraphrasing edits in addition to GEC. For the next three prompts (FormalityParaphrasing, Formality-Simplification, FormalityParaphrasing-Simplification), we use the formality dataset (GYAFC) to extract pairs which exhibit paraphrasing and simplification edits in addition to formality. Lastly, for the last prompt (ParaphrasingSimplification), we use the ParabankV2 paraphrasing dataset to extract data points which show a simplification of the source text in addition to paraphrasing.\nTo select the appropriate source-target pairs for a composite instruction, we use similar heuristics as with single-task instructions, i.e. old word retention, complexity ratios, dependency tree depth ratio, and character length ratio. For example, a source-target pair from a GEC dataset can be used for the composite instruction involving GEC, paraphrasing and simplification if the target and source sentence has a high edit distance and low complexity ratio, character length and word retention scores. The exact details can be found in the code.\nFinally, for building the prompts for the composite instructions, we randomly sample from the\ntask-specific verbalizers and concatenate them. The ordering of the single tasks in a composite instruction is also chosen randomly to ensure better generalization."
        },
        {
            "heading": "B Testing Dataset Description",
            "text": "Specifically, we consider the following datasets:\nGrammatical Error Correction We use the JFLEG (Napoles et al., 2017) corpus of English sentences that represents a range of language proficiency levels and comprehensive fluency edits. For evaluation, we use the GLEU (Napoles et al., 2015) score as the primary metric and also report results using the SARI (Xu et al., 2016) metric.\nText Simplification We use the TurkCorpus (Xu et al., 2016) and ASSET (Alva-Manchego et al., 2020) datasets, which were both created from WikiLarge data (Zhang and Lapata, 2017), where each complex sentence consists of multiple crowdsourced reference simplifications. We report results using the SARI metric.\nCoherence We use the Coherence split of ITERATER (Du et al., 2022b), and the DISCOFUSE dataset (Geva et al., 2019), as it involves linking two given sentences as coherently as possible using edit operations such as inserting discourse connectives. We report results using the SARI metric.\nIterative Text Editing We use ITERATER (Du et al., 2022b), an iterative text revision dataset spanning five edit intentions (Section 3) across three different domains (ArXiv, News, Wikipedia). We evaluate our models using the SARI metric. We report the performance on individual intentions \u2013 Fluency, Clarity, and Coherence, and also aggregated scores on the full dataset, which includes Style edits.\nThe rest of the section describes the evaluation setups for Style-related edits:\nFormality Style Transfer We use Grammarly\u2019s Yahoo Answers Formality Corpus (GYAFC) (Rao and Tetreault, 2018), a parallel corpus of informal and formal sentence pairs from two different domains. Similar to prior works, we evaluate the quality of rewriting using SARI, and the accuracy of style transfer using a formality classification model11.\n11 https://huggingface.co/s-nlp/xlmr_formality_\nclassifier\nNeutralization We use WNC (Pryzant et al., 2020), a dataset from the Subjective Bias Neutralization task. Based on prior works, we use ExactMatch (EM) for evaluations, which is the percentage of examples for which the edited text exactly matches the reference(s).\nParaphrasing We use the widely-used Microsoft Research Paraphrase Corpus (MRPC) (Dolan and Brockett, 2005), the STS benchmark from SemEval-2017 (STS) (Cer et al., 2017), and the Quora Question Pairs12 (QQP) datasets. We evaluate paraphrasing on two criteria and metrics: SelfBLEU (Zhu et al., 2018) to measure the diversity of the paraphrases relative to the given source and ref-\n12 https://quoradata.quora.com/\nFirst-Quora-Dataset-Release-Question-Pairs\nerence texts, and Semantic Similarity13 to measure meaning preservation."
        },
        {
            "heading": "C Task Verbalizers",
            "text": "We manually curated a variety of task-specific verbalizers to construct the instructional inputs. Table 9 shows the full list of the verbalizers used for training and evaluations. Table 10 shows the verbalizers used for the experiments conducted in Section 6.2."
        },
        {
            "heading": "D Training Details",
            "text": "We used the Adam optimizer with a learning rate of 1e \u2212 4. Each model is trained for 5 epochs\n13We use the paraphrase-mpnet-base-v2 model from SentenceTransformers (Reimers and Gurevych, 2019)\nwith early stopping. All models were fine-tuned on A100 GPUs using Deepspeed (Rasley et al., 2020). Maximum sequence lengths for both the source and the target were set to 256 tokens (via filtering). The best-performing checkpoints were chosen based the validation loss."
        },
        {
            "heading": "E Model Performance",
            "text": "Table 11 compares the performance of COEDIT with the other models on the remaining test datasets. We observe similar trends as the ones observed in Table 2, where COEDIT outperforms most models we compare against.\nF Data Examples"
        }
    ],
    "title": "COEDIT: Text Editing by Task-Specific Instruction Tuning",
    "year": 2023
}