{
    "abstractText": "Large Language Models (LLMs) have significantly advanced the field of Natural Language Processing (NLP), but their lack of interpretability has been a major concern. Current methods for interpreting LLMs are post hoc, applied after inference time, and have limitations such as their focus on low-level features and lack of explainability at higherlevel text units. In this work, we introduce proto-lm, a prototypical network-based whitebox framework that allows LLMs to learn immediately interpretable embeddings during the fine-tuning stage while maintaining competitive performance. Our method\u2019s applicability and interpretability are demonstrated through experiments on a wide range of NLP tasks, and our results indicate a new possibility of creating interpretable models without sacrificing performance. This novel approach to interpretability in LLMs can pave the way for more interpretable models without the need to sacrifice performance. We release our code at https://github.com/yx131/proto-lm.",
    "authors": [
        {
            "affiliations": [],
            "name": "Sean Xie"
        },
        {
            "affiliations": [],
            "name": "Soroush Vosoughi"
        },
        {
            "affiliations": [],
            "name": "Saeed Hassanpour"
        }
    ],
    "id": "SP:b7d54179026b94978e26cdca64b609f6dd6943a6",
    "references": [
        {
            "authors": [
                "Simon Baker",
                "Ilona Silins",
                "Yufan Guo",
                "Imran Ali",
                "Johan H\u00f6gberg",
                "Ulla Stenius",
                "Anna Korhonen."
            ],
            "title": "Automatic semantic classification of scientific literature according to the hallmarks of cancer",
            "venue": "Bioinformatics, 32(3):432\u2013440.",
            "year": 2016
        },
        {
            "authors": [
                "Aaron Chan",
                "Maziar Sanjabi",
                "Lambert Mathias",
                "Liang Tan",
                "Shaoliang Nie",
                "Xiaochang Peng",
                "Xiang Ren",
                "Hamed Firooz."
            ],
            "title": "Unirex: A unified learning framework for language model rationale extraction",
            "venue": "International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Chaofan Chen",
                "Oscar Li",
                "Daniel Tao",
                "Alina Barnett",
                "Cynthia Rudin",
                "Jonathan K Su."
            ],
            "title": "This looks like that: deep learning for interpretable image recognition",
            "venue": "Advances in neural information processing systems, 32.",
            "year": 2019
        },
        {
            "authors": [
                "Sanjoy Dasgupta",
                "Nave Frost",
                "Michal Moshkovitz."
            ],
            "title": "Framework for evaluating faithfulness of local explanations",
            "venue": "International Conference on Machine Learning, pages 4794\u20134815. PMLR.",
            "year": 2022
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "arXiv preprint arXiv:1810.04805.",
            "year": 2018
        },
        {
            "authors": [
                "Jay DeYoung",
                "Sarthak Jain",
                "Nazneen Fatema Rajani",
                "Eric Lehman",
                "Caiming Xiong",
                "Richard Socher",
                "Byron C Wallace."
            ],
            "title": "Eraser: A benchmark to evaluate rationalized nlp models",
            "venue": "arXiv preprint arXiv:1911.03429.",
            "year": 2019
        },
        {
            "authors": [
                "Finale Doshi-Velez",
                "Been Kim."
            ],
            "title": "Towards a rigorous science of interpretable machine learning",
            "venue": "arXiv preprint arXiv:1702.08608.",
            "year": 2017
        },
        {
            "authors": [
                "Patrick Fernandes",
                "Marcos Treviso",
                "Danish Pruthi",
                "Andr\u00e9 Martins",
                "Graham Neubig."
            ],
            "title": "Learning to scaffold: Optimizing model explanations for teaching",
            "venue": "Advances in Neural Information Processing Systems, 35:36108\u201336122.",
            "year": 2022
        },
        {
            "authors": [
                "Felix Friedrich",
                "Patrick Schramowski",
                "Christopher Tauchmann",
                "Kristian Kersting."
            ],
            "title": "Interactively providing explanations for transformer language models",
            "venue": "arXiv preprint arXiv:2110.02058.",
            "year": 2021
        },
        {
            "authors": [
                "Tianyu Gao",
                "Xu Han",
                "Zhiyuan Liu",
                "Maosong Sun."
            ],
            "title": "Hybrid attention-based prototypical networks for noisy few-shot relation classification",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 6407\u20136414.",
            "year": 2019
        },
        {
            "authors": [
                "Tianyu Gao",
                "Xingcheng Yao",
                "Danqi Chen."
            ],
            "title": "Simcse: Simple contrastive learning of sentence embeddings",
            "venue": "arXiv preprint arXiv:2104.08821.",
            "year": 2021
        },
        {
            "authors": [
                "Diego Garcia-Olano",
                "Yasumasa Onoe",
                "Joydeep Ghosh",
                "Byron C Wallace."
            ],
            "title": "Intermediate entitybased sparse interpretable representation learning",
            "venue": "arXiv preprint arXiv:2212.01641.",
            "year": 2022
        },
        {
            "authors": [
                "Yu Gu",
                "Robert Tinn",
                "Hao Cheng",
                "Michael Lucas",
                "Naoto Usuyama",
                "Xiaodong Liu",
                "Tristan Naumann",
                "Jianfeng Gao",
                "Hoifung Poon."
            ],
            "title": "Domain-specific language model pretraining for biomedical natural language processing",
            "venue": "ACM Transactions on Computing",
            "year": 2021
        },
        {
            "authors": [
                "Peter Hase",
                "Chaofan Chen",
                "Oscar Li",
                "Cynthia Rudin."
            ],
            "title": "Interpretable image recognition with hierarchical prototypes",
            "venue": "Proceedings of the AAAI Conference on Human Computation and Crowdsourcing, volume 7, pages 32\u201340.",
            "year": 2019
        },
        {
            "authors": [
                "Alon Jacovi",
                "Yoav Goldberg"
            ],
            "title": "Towards faithfully interpretable nlp systems: How should we define and evaluate faithfulness? arXiv preprint arXiv:2004.03685",
            "year": 2020
        },
        {
            "authors": [
                "Alon Jacovi",
                "Yoav Goldberg."
            ],
            "title": "Aligning faithful interpretations with their social attribution",
            "venue": "Transactions of the Association for Computational Linguistics, 9:294\u2013310.",
            "year": 2021
        },
        {
            "authors": [
                "Jos\u00e9 Jim\u00e9nez-Luna",
                "Francesca Grisoni",
                "Gisbert Schneider."
            ],
            "title": "Drug discovery with explainable artificial intelligence",
            "venue": "Nature Machine Intelligence, 2(10):573\u2013584.",
            "year": 2020
        },
        {
            "authors": [
                "Diederik P Kingma",
                "Jimmy Ba."
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "arXiv preprint arXiv:1412.6980.",
            "year": 2014
        },
        {
            "authors": [
                "Mike Lewis",
                "Yinhan Liu",
                "Naman Goyal",
                "Marjan Ghazvininejad",
                "Abdelrahman Mohamed",
                "Omer Levy",
                "Ves Stoyanov",
                "Luke Zettlemoyer"
            ],
            "title": "Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension",
            "year": 2019
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Roberta: A robustly optimized bert pretraining approach",
            "venue": "arXiv preprint arXiv:1907.11692.",
            "year": 2019
        },
        {
            "authors": [
                "Scott M Lundberg",
                "Su-In Lee."
            ],
            "title": "A unified approach to interpreting model predictions",
            "venue": "Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc.",
            "year": 2017
        },
        {
            "authors": [
                "Andrew Maas",
                "Raymond E Daly",
                "Peter T Pham",
                "Dan Huang",
                "Andrew Y Ng",
                "Christopher Potts."
            ],
            "title": "Learning word vectors for sentiment analysis",
            "venue": "Proceedings of the 49th annual meeting of the association for computational linguistics: Human language",
            "year": 2011
        },
        {
            "authors": [
                "W James Murdoch",
                "Peter J Liu",
                "Bin Yu."
            ],
            "title": "Beyond word importance: Contextual decomposition to extract interactions from lstms",
            "venue": "arXiv preprint arXiv:1801.05453.",
            "year": 2018
        },
        {
            "authors": [
                "Danish Pruthi",
                "Rachit Bansal",
                "Bhuwan Dhingra",
                "Livio Baldini Soares",
                "Michael Collins",
                "Zachary C Lipton",
                "Graham Neubig",
                "William W Cohen"
            ],
            "title": "Evaluating explanations: How much do explanations from the teacher aid students",
            "year": 2022
        },
        {
            "authors": [
                "Nils Reimers",
                "Iryna Gurevych."
            ],
            "title": "Sentence-bert: Sentence embeddings using siamese bert-networks",
            "venue": "arXiv preprint arXiv:1908.10084.",
            "year": 2019
        },
        {
            "authors": [
                "Marco Tulio Ribeiro",
                "Sameer Singh",
                "Carlos Guestrin."
            ],
            "title": " why should i trust you?\" explaining the predictions of any classifier",
            "venue": "Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining, pages 1135\u2013",
            "year": 2016
        },
        {
            "authors": [
                "Cynthia Rudin."
            ],
            "title": "Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead",
            "venue": "Nature Machine Intelligence, 1(5):206\u2013215.",
            "year": 2019
        },
        {
            "authors": [
                "Avanti Shrikumar",
                "Peyton Greenside",
                "Anshul Kundaje."
            ],
            "title": "Learning important features through propagating activation differences",
            "venue": "CoRR, abs/1704.02685.",
            "year": 2017
        },
        {
            "authors": [
                "Avanti Shrikumar",
                "Peyton Greenside",
                "Anna Shcherbina",
                "Anshul Kundaje."
            ],
            "title": "Not just a black box: Learning important features through propagating activation differences",
            "venue": "arXiv preprint arXiv:1605.01713.",
            "year": 2016
        },
        {
            "authors": [
                "Jake Snell",
                "Kevin Swersky",
                "Richard Zemel."
            ],
            "title": "Prototypical networks for few-shot learning",
            "venue": "Advances in neural information processing systems, 30.",
            "year": 2017
        },
        {
            "authors": [
                "Richard Socher",
                "Alex Perelygin",
                "Jean Wu",
                "Jason Chuang",
                "Christopher D Manning",
                "Andrew Y Ng",
                "Christopher Potts."
            ],
            "title": "Recursive deep models for semantic compositionality over a sentiment treebank",
            "venue": "Proceedings of the 2013 conference on empiri-",
            "year": 2013
        },
        {
            "authors": [
                "Gizem So\u011fanc\u0131o\u011flu",
                "Hakime \u00d6zt\u00fcrk",
                "Arzucan \u00d6zg\u00fcr."
            ],
            "title": "Biosses: a semantic sentence similarity estimation system for the biomedical domain",
            "venue": "Bioinformatics, 33(14):i49\u2013i58.",
            "year": 2017
        },
        {
            "authors": [
                "Jost Tobias Springenberg",
                "Alexey Dosovitskiy",
                "Thomas Brox",
                "Martin Riedmiller."
            ],
            "title": "Striving for simplicity: The all convolutional net",
            "venue": "arXiv preprint arXiv:1412.6806.",
            "year": 2014
        },
        {
            "authors": [
                "Shengli Sun",
                "Qingfeng Sun",
                "Kevin Zhou",
                "Tengchao Lv."
            ],
            "title": "Hierarchical attention prototypical networks for few-shot text classification",
            "venue": "Proceedings of the 2019 conference on empirical methods in natural language processing and the 9th interna-",
            "year": 2019
        },
        {
            "authors": [
                "Zijun Sun",
                "Chun Fan",
                "Qinghong Han",
                "Xiaofei Sun",
                "Yuxian Meng",
                "Fei Wu",
                "Jiwei Li."
            ],
            "title": "Selfexplaining structures improve NLP models",
            "venue": "CoRR, abs/2012.01786.",
            "year": 2020
        },
        {
            "authors": [
                "Mukund Sundararajan",
                "Ankur Taly",
                "Qiqi Yan."
            ],
            "title": "Axiomatic attribution for deep networks",
            "venue": "CoRR, abs/1703.01365.",
            "year": 2017
        },
        {
            "authors": [
                "Betty Van Aken",
                "Jens-Michalis Papaioannou",
                "Marcel G Naik",
                "Georgios Eleftheriadis",
                "Wolfgang Nejdl",
                "Felix A Gers",
                "Alexander L\u00f6ser"
            ],
            "title": "2022. This patient looks like that patient: Prototypical networks for interpretable diagnosis prediction from clinical text",
            "year": 2022
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141 ukasz Kaiser",
                "Illia Polosukhin."
            ],
            "title": "Attention is all you need",
            "venue": "Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc.",
            "year": 2017
        },
        {
            "authors": [
                "Alex Wang",
                "Amanpreet Singh",
                "Julian Michael",
                "Felix Hill",
                "Omer Levy",
                "Samuel R Bowman."
            ],
            "title": "Glue: A multi-task benchmark and analysis platform for natural language understanding",
            "venue": "arXiv preprint arXiv:1804.07461.",
            "year": 2018
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "In recent years, Large Language Models (LLMs) have significantly improved results on a wide range of Natural Language Processing (NLP) tasks. However, despite their state-of-the-art performance, LLMs, such as BERT (Devlin et al., 2018), RoBERTa (Liu et al., 2019) and BART (Lewis et al., 2019), are not easily interpretable. Interpretability is a crucial aspect of language models, especially LLMs, as it enables trust and adoption in various domains. To address this issue, there is a growing interest in improving model interpretability for LLMs and neural models in general.\nCurrent interpretation methods have several limitations, such as requiring a surrogate model to be built (Ribeiro et al., 2016; Lundberg and Lee, 2017) or being applied post-hoc to each instance, separate\n\u2217Co-corresponding Authors.\nfrom the original decision-making process of the model (Shrikumar et al., 2016, 2017; Springenberg et al., 2014). These limitations add extra computational complexity to the explanation process and can result in approximate explanations that are unfaithful1 to the original model\u2019s decision-making process (Sun et al., 2020; DeYoung et al., 2019). Finally, current interpretability methods focus on attributing importance to different words in the input and do not explain the model\u2019s decision at the sentence or sample level. (Sundararajan et al., 2017; Vaswani et al., 2017; Murdoch et al., 2018).\nTo address these challenges, we propose a novel framework that utilizes a prototypical network to learn an interpretable prototypical embedding layer on top of a fine-tuned LLM, which can be trained end-to-end for a downstream task. Our framework utilizes trainable parameters, called prototypes, to both perform the downstream task by capturing important features from the training dataset and provide explanations for the model\u2019s decisions via projection onto the most influential training examples. Additionally, we utilize a token-level attention layer before the prototypical layer to select relevant parts within each input text, allowing our model to attribute importance to individual words like existing interpretability methods. Our framework, named proto-lm, achieves competitive results on a wide range of NLP tasks and offers interpretability while addressing the previously mentioned limitations of current interpretability methods.\nFig. 1 shows our proposed proto-lm framework applied to a multi-classification example from the SST5 dataset (Socher et al., 2013). As the figure illustrates, the model\u2019s decision-making process is simple, transparent, and accurate as it classifies the input as \u201cVery Positive\u201d based on its highest\n1Faithfulness is defined as how accurately the explanation reflects the decision-making process of the model (Jacovi and Goldberg, 2020).\nsimilarity to prototypes from the \u201cVery Positive\u201d class and the input\u2019s distance (lack of similarity) to \u201cNegative\u201d and \u201cVery Negative\u201d prototypes. A key advantage of our framework is its inherent interpretability. Our prototypical network does not require an additional surrogate model for explanation, and we can observe the exact contribution of each prototype to the final output, resulting in faithful explanations. Additionally, our framework offers interpretability at both the token and sample level, allowing for the identification of important words in the input text as well as the attribution of importance to impactful samples in the training data. In this work, we make the following contributions:\n\u2022 We introduce a novel framework, proto-lm based on prototypical networks that provides inherent interpretability to LLMs. \u2022 We demonstrate proto-lm\u2019s applicability on three LLMs and show its competitive performance on a wide range of NLP tasks. \u2022 We conduct ablation studies to demonstrate important characteristics of proto-lm under different hyperparameters. \u2022 We evaluate the interpretability of proto-lm under several desiderata and show that the explanations provided by proto-lm are of high quality."
        },
        {
            "heading": "2 proto-lm",
            "text": "The architecture of our proposed framework, proto-lm, is shown in Figure 2. We utilize a pretrained LLM as the underlying language model to encode the input and build a prototypical layer on top of it to provide interpretable prototypes. The goal of the learned prototypes is to capture features representative enough of each class so that they can be fine-tuned for the downstream classification task. In the classification process, similarities between the encoded input and the learned prototypes are fed through a fully-connected layer to produce logits. We formalize this below."
        },
        {
            "heading": "2.1 Token-level Attention",
            "text": "Let xi represent a single input text and yi its associated label. We denote f(xi) as the encoding of xi by an underlying LLM, f . We pass f(xi) through a token-level attention layer that learns to emphasize different parts of the text, allowing us to identify important words within each sample. We first calculate \u03c5t, the output of a fully-connected layer with bias (\u03c8 ), applied to ht, which the embedding of each token t in f(xi), followed by a tanh activation function (\u03c3). We then compute \u03bdt, the dot product of \u03c5t, and a token-level weight vector W\u03bd .\n\u03c5t = \u03c3(\u03c8(ht)) \u03bdt = \u03c5t \u00b7W\u03bd (1)\nWe calculate the attention score \u03b1t for each token t using the softmax function. The attended encoding of f(xi), Si, is then obtained by taking the weighted sum of the token embeddings, where the weight for each token is determined by its attention score.\n\u03b1t = exp(\u03bdt)\u2211\nt\u2208f(xi)\nexp(\u03bdt) Si = \u2211 t\u2208f(xi) \u03b1tht (2)"
        },
        {
            "heading": "2.2 Prototypical Layer",
            "text": "In the prototypical layer P , we create N prototype vectors and assign n prototypes to each class c \u2208 C, where C represents the set of all classes in the dataset. To ensure an equal number of prototype vectors are allocated to capture representative features for each class, we set n = N|C| , where |C| denotes the total number of classes in the dataset. The input Si is then fed into P , where we calculate the similarity between Si and every prototype vector p \u2208 P by inverting the L2-distance. We then concatenate all N similarities to obtain the vector Mi, which serves as the embedding of input\nxi in the prototypical space. Each dimension of Mi can be interpreted as the similarity between Mi and a prototype. Subsequently, Mi is fed through a fully-connected layer Wh of dimension N \u00d7 |C| to obtain logits for each class. Let Wc denote the set of weights in Wh that connect similarities in Mi to the logit of class c. Our model\u2019s output probabilities are computed as follows:\nPr(y\u0302 = c | xi) = exp(Miwc)\u2211\nc\u2208C exp(Miwc)\n(3)"
        },
        {
            "heading": "2.3 Training Objective",
            "text": "To create more representative prototypes and shape the clusters of prototypes in the prototypical embedding space, we introduce a cohesion loss term Lcoh and a separation loss term Lsep into our loss function in addition to the cross entropy loss Lce. Let K be an integer hyperparameter, pj denote a singular prototype, and Pyi represent all prototypes in P that belong to class yi, our loss terms are defined as follows:\nLcoh = 1\nK \u00b7 \u2211 \u2200j:pj\u2208Pyi max K \u2225Si \u2212 pj\u222522 (4)\nLsep = \u2212 1\nK \u00b7 \u2211 \u2200j:pj \u0338\u2208Pyi min K \u2225Si \u2212 pj\u222522 (5)\nFor every Si in the input batch, Lcoh penalizes the average distance between Si and the K most distant prototypes of its class (yi) while Lsep penalizes the average distance between Si and the K most similar prototypes that do not belong to yi. Intuitively, for every Si, Lcoh \u201cpulls\u201d K prototypes of the correct class close while Lsep \u201cpushes\u201d K prototypes of the incorrect class away. We then add cross-entropy loss Lce and weight each loss term with a hyperparameter \u03bb such that \u03bb0+\u03bb1+\u03bb2 = 1 to obtain the following loss function:\nLtotal = \u03bb0 \u00b7 Lce + \u03bb1 \u00b7 Lcoh + \u03bb2 \u00b7 Lsep (6)"
        },
        {
            "heading": "2.4 Prototype Projection",
            "text": "To understand each prototype vector pj in natural language, we project each prototype onto the nearest token-level attended encoding (Sj) of a sample text (xj) in the training data that belongs to the same class as pj and assign the token-attended text of that prototype. Let D be the training\ndataset. We formally denote the projection as in eq.7: \u2200(xj , yj) \u2208 D such that yj = c:\nText of pj \u2190 argmin Sj \u2225Sj \u2212 pj\u222522 (7)\nIn other words, for each prototype pj , we find the nearest Sj in the training dataset that belongs to the same class as pj and assign the corresponding token-attended text to pj (Details in App. D & E)."
        },
        {
            "heading": "3 Performance Experiments",
            "text": "As we do not want interpretability to come at the cost of performance, we first conduct experiments to assess the modeling capability of proto-lm. We implement proto-lm using BERT (base-uncased and large-uncased), RoBERTa (base and large) and BART-large as the base LLM encoders and train the token-level attention module, prototypical layer and classification head as described in \u00a72. We evaluate the predictive accuracy of proto-lm on 5 tasks (SST-2, QNLI, MNLI, WNLI, RTE) from the GLUE dataset (Wang et al., 2018), IMDb sentiment analysis (Maas et al., 2011) and SST-5 (Socher et al., 2013). For baselines, we use the same fine-tuned versions of the LLMs with a classification head. We tune all hyperparameters using the respective validation data in each dataset. We present the mean performance as well as the standard deviation over 5 runs under their respective optimal configurations of hyperparameters (cf. App.A) and compare them to their baselines in Table 1. Across our experiments 2, we observe that proto-lm either closely matches or exceeds the performance of its baseline LLM, proving that there is not a trade-off between proto-lm\u2019s interpretability and performance."
        },
        {
            "heading": "4 Prototype Interpretability",
            "text": ""
        },
        {
            "heading": "4.1 Interpretable prototypical space and decision-making",
            "text": "Compared to black-box models such as BERT/RoBERTa/BART, which have uninterpretable embedding dimensions, proto-lm offers the added benefit of interpretability in conjunction with competitive performance. We show an example of a 2-dimensional prototypical space in Fig. 3, where proto-lm provides insight into the model\u2019s decision-making process by allowing\n2We conduct additional performance experiments with proto-lm in App.B\nus to visualize examples in prototypical space, where each dimension represents the normalized similarity \u2208 [0, 1] between an example and one prototype.\nIn Fig. 3, we select one positive and one negative prototype from the prototypical layer of the model to create a 2-dimensional space in which we place examples #1-#4. The vertical axis represents similarity to the negative prototype, and the horizontal axis represents similarity to the positive prototype. From this, we can see that example #1 is much more similar to the negative prototype than to the positive prototype, and example #3 is much more similar to the positive prototype than to the negative prototype, and both are correctly classified as a result.\nFrom the prototypical space, we can see clearly that the model\u2019s decision to misclassify example #2 as positive is due to elements in the example that make it similar to the positive prototype. Similarly, the prototypical space of proto-lm reveals that example #4 was misclassified because, while it contains elements that are similar to both the positive and negative prototypes, it is closer to the negative prototype. The interpretable prototypical space of proto-lm provides an explanation for why difficult cases such as examples #2 and #4 are misclassified. It should be noted that while similar analyses and explanations can be obtained through post-hoc techniques such as generating sentence embeddings (Reimers and Gurevych, 2019; Gao et al., 2021), proto-lm has this capability built-in."
        },
        {
            "heading": "4.2 Prototype quality and performance",
            "text": "We investigate the effect of different weightings of the loss terms in our loss function. We train proto-lm, holding all other hyperparameters constant, under 11 different values of \u03bb0 evenly distributed on [0, 1], and report our results in Figure 4. For these experiments, we place equal weight on Lcoh and Lsep such that \u03bb1 = \u03bb2 = 1\u2212\u03bb02 (additional details in App. C).\nBecause prototypes not only help interpret the model but also capture the most representative features from the data, placing an excessive emphasis on Lce actually achieves the adverse effect. As shown in Figure 4, increasing \u03bb0 comes at the cost of learning representative prototypes (which Lcoh and Lsep do) and this is reflected in decreasing classification accuracy on the downstream task. We\nobserve that the optimal accuracy is achieved when \u03bb0 = 0.3. As we increase \u03bb0 from 0.3, we see not only a decline in accuracy but also a decline in the quality of the prototypes associated with the input. On the other, placing sole emphasis on Lcoh and Lsep without any emphasis on Lce (as in the case of \u03bb0 = 0) leads to prototypes that do not help with the downstream task."
        },
        {
            "heading": "4.3 Size of prototypical space",
            "text": "As noted in \u00a74.2, prototypes serve to capture features from data that are useful for making predictions in addition to interpretability. As a result, the number of prototypes (N ) in the prototypical layer directly affects the expressiveness of our model. We conduct experiments varying the number of prototypes N using RoBERTa-Large as the base and show our results in Fig. 5. We observe a general increase in accuracy up until N = 1000, after which\naccuracy plateaus for some tasks while increasing only slightly for others. We reason that increasing N can only improve the model\u2019s expressiveness until the point when N reaches the embedding dimension of the underlying LLM, after which more prototypes in the prototypical space no longer aid in capturing more useful features from the training data. In Fig. 5\u2019s case, as RoBERTa-large\u2019s output dimension is 1024, we see the increasing trend of accuracy stop at around 1000 prototypes."
        },
        {
            "heading": "4.4 Prototype uniqueness and performance",
            "text": "The interpretability of proto-lm stems from the retrieval of the most informative training examples. If all prototypes are predominantly associated with a limited number of training examples, this reduces their utility from an interpretability perspective. Hence, it is beneficial to encourage prototype segregation, that is, a broader projection onto the dataset and a more diverse representation of different samples. Besides normalizing prototype distances, which has been shown to influence prototype segregation (Das et al., 2022), proto-lm introduces an additional hyperparameter, K. This parameter controls the number of prototypes that each training example associates\nand disassociates with during training. As changing K also influences the decision-making process of the model by altering the number of samples the models compare for each input, we examine the impact of K on both prototype segregation and model performance. In our experiment, we employ proto-lm with RoBERTa-large as our base on SST2 and IMDb. We set N , the total number of prototypes, to be 1000, and n = 1000/2, the number of prototypes per class, to be 500. We train models under seven different K values, keeping all other hyperparameters constant. We report the accuracy of the models and the percentage of unique prototypes in P under each different K in Fig.6.\nA prototype is considered unique if no other prototype in P projects onto the same sample in the dataset. We observe that the best prototype segregation (highest number of unique prototypes) occurs when K = 1, and the number of unique prototypes significantly drops as we increase K. Intuitively, if more prototypes are drawn close to each sample during training (eq.4), it becomes challenging to create unique prototypes. It is important to note that eq. 5 is less related to the uniqueness of prototypes as prototypes are projected onto samples from the same class. We also witness a slight rise in model accuracy as we increase K. We conjecture that while unique prototypes contribute to interpretability, the model doesn\u2019t necessarily require prototypes to be unique to make accurate decisions. Thus, we observe a minor trade-off between interpretability and model performance."
        },
        {
            "heading": "5 Evaluating the Interpretability of",
            "text": "proto-lm\nThrough the usage of its prototypical space, proto-lm is a white-box, inherently interpretable model. But just how well do the explanations provided by proto-lm satisfy common desiderata in interpretability? We conduct experiments to try to answer that question in this section. Specifically, we evaluate the inherent interpretability provided by proto-lm via measures of faithfulness (DeYoung et al., 2019; Jacovi and Goldberg, 2020) and simulatability (Pruthi et al., 2022; Fernandes et al., 2022)."
        },
        {
            "heading": "5.1 Faithfulness experiments",
            "text": "Recently, the concept of faithfulness, which measures the extent to which an explanation accurately reflects the true decision-making process of a model, has garnered attention as a criterion for evaluating explainability methods (Jacovi and Goldberg, 2020, 2021; Chan et al., 2022; Dasgupta et al., 2022). For textual inputs, faithfulness is concretely evaluated using the metrics of comprehensiveness (Comp) and sufficiency (Suff), as defined by DeYoung et al. (2019). Specifically, Comp and Suff quantify the reduction in the model\u2019s confidence for its output when salient features are removed and retained, respectively.\nWe extend the application of Comp and Suff to prototypical networks to assess the faithfulness of proto-lm. For an input xi, we initially identify the top k% of most similar prototypes and the bottom k% of least similar prototypes. Let pki denote the k% prototypes identified, we compute Comp as the\npercentage difference in model confidence when pki are removed (by setting p k i \u2019s weights in Wh are set to 0). Specifically, let y\u0302i be the prediction of a modelM on xi, let pry\u0302i be the output logit of M for y\u0302i, and let pry\u0302i(P \\pki ) be the output logit when pki prototypes are removed from the prototypical layer, we calculate Comp as follows:\nComp = pry\u0302i \u2212 pry\u0302i(P \\ pki )\npry\u0302i (8)\nWe analogously calculate Suff as the percentage difference in model confidence when the k% of prototypes are retained:\nSuff = pry\u0302i \u2212 pry\u0302i(pki )\npry\u0302i (9)\nWe compute Comp and Suff k \u2208 1, 5, 10, 20, 50 and our present mean results across SST2, QNLI, MNLI and SST5 in Fig 7. Our formulation of prototype Comp and Suff are inspired by DeYoung et al. (2019). We note here that under this formulation, a lower sufficiency is better i.e., a smaller amount of reduction in model confidence when only salient features are retained is better. As we remove more top k% prototypes, we observe a general increase in Comp. We also note a general decrease in Suff (a lower Suff is preferable) as we retain more top k% prototypes. Moreover, when the bottom k% of prototypes are removed, there are relatively small changes in Comp and large changes in Suff when only the bottom k% of prototypes are retained. These trends underscore the influence of the learned prototypes on the model\u2019s decision-making process and their faithfulness."
        },
        {
            "heading": "5.2 Simulatability experiments",
            "text": "Simulatability refers to the capacity of a human to mimic or replicate the decision-making process of a machine learning model (Doshi-Velez and Kim, 2017; Pruthi et al., 2022). It is a desirable attribute as it inherently aligns with the goal of transparently communicating the model\u2019s underlying behavior to human users (Fernandes et al., 2022). We assess the simulatability of proto-lm by providing human annotators with various explanations of model outputs on SST2/QNLI and recording the percentage of model outputs that the annotators can replicate. We provide explanations under the following four settings:\n\u2022 No Explanations (NE): Annotators are presented with only the sample data, without any explanations, and asked to make a decision.\n\u2022 Random Explanations (Random): Each sample in the dataset is presented along with prototypes from proto-lm chosen randomly as explanations. This setting serves as our baseline. \u2022 Prototypical Explanations (PE): Each sample in the dataset is presented along with the top 5 prototypes most similar to the sample when proto-lm made its decision. \u2022 PE + Output: In addition to the prototypical explanations, the model decision for each sample is also presented.\nWe employ workers from Amazon Mechanical Turk to crowdsource our evaluations and presented 3 workers with 50 examples each from the SST2 and QNLI datasets, along with explanations for the model\u2019s decisions in the settings mentioned in \u00a75.2. We use the best performing models for SST2 and QNLI (those presented in Table 1), since previous studies found that the utility of PE\u2019s are reliant on model accuracy (Das et al., 2022). Additionally, we assess the accuracy of the human annotators in relation to the ground truth labels. We present results for both in Fig. 8. The results show that the case-based reasoning explanations offered by PEs are more effective in assisting annotators in simulating model predictions than the random baseline. We also notice a minor drop in accuracy when we provide PE + output compared to just PE. We\nattribute this to the fact that the models are not entirely accurate themselves, and in instances where the model is inaccurate, presenting the output leads annotators to replicate inaccurate decisions.\nFurthermore, we compare proto-lm\u2019s simulatability against 3 other well-known interpretability methods: LIME, Integrated Gradients, and Guided Backprop. We employed three workers for each example and reported the mean percentage of examples where the workers correctly replicated the model\u2019s decision. For an example of the a simulatability questionnaire with PE, see Fig.12. For an example of an accuracy questionnaire with Random explanations, see Fig.13. For an example of the questionnaire for LIME/IG/GB explanations, see Fig.14. For results of proto-lm against LIME, IG and GB, please see Table 2. Our addditional results further indicate that proto-lm allow the workers to replicate the model\u2019s decisions better than all the other interpretability methods on both tasks."
        },
        {
            "heading": "6 Related Works",
            "text": "Prototypical networks (Snell et al., 2017) have shown strong performance in few-shot image and text classification tasks (Sun et al., 2019; Gao et al., 2019). However, these approaches do not actively learn prototypes and instead rely on summarizing\nsalient parts of the training data. Their focus is on learning one representative prototype per class while their performance is dependent on the size of the support set in few-shot learning scenarios. Due to these limitations, there have been relatively few works that utilize prototypical networks to provide interpretability for LLM\u2019s in NLP (Garcia-Olano et al., 2022; Das et al., 2022; Van Aken et al., 2022). Chen et al. (2019) and Hase et al. (2019) use prototypical parts networks with multiple learned prototypes per class but only apply their methods to image classification tasks.\nOur work is most closely related to Das et al. (2022) and Van Aken et al. (2022) in terms of the approach taken. However, our work differs in that the architecture in (Das et al., 2022) only utilizes a single negative prototype for binary classification, while proto-lm enables multi-class classification by using multiple prototypes for each class. Additionally, we have extended the work of (Das et al., 2022) by implementing token-level attention to identify not only influential samples but also influential sections of text within each sample. Moreover, different from the single-prototype-as-asummary approach in Van Aken et al. (2022), by learning multiple prototypes per class, proto-lm creates a prototypical space (\u00a74.1), where, unlike the embedding space of LLM\u2019s, each dimension is meaningful, specifically the distance to a learned prototype, and can be used to explain a decision. Our work is also similar to Friedrich et al. (2021) in terms of our loss function design. However, Friedrich et al. (2021) \u2019s loss function aims to maximize the similarity of the closest prototypes of the same class. Conversely, our approach strives to minimize the distance of the furthest prototypes of the same class. This results in Friedrich et al. (2021)\u2019s approach tending to draw a single prototype closer to a specific example, potentially lim-\niting prototype diversity and representation power. Friedrich et al. (2021) counteracts this potential limitation by introducing an additional diversity loss term. proto-lm, in contrast, ensures prototype diversity by leveraging the K hyperparameter, which we delve into in section \u00a74.4."
        },
        {
            "heading": "7 Conclusion",
            "text": "We introduce proto-lm, a white-box framework designed to offer inherent interpretability for Language Model Learning (LLMs) through interpretable prototypes. These prototypes not only explain model decisions but also serve as feature extractors for downstream tasks. Our experimental results indicate that proto-lm delivers competitive performance across a range of Natural Language Processing (NLP) tasks and exhibits robustness under various hyperparameter settings. The interpretability of proto-lm is evaluated, and our findings show that proto-lm delivers faithful explanations that can effectively assist humans in understanding and predicting model decisions."
        },
        {
            "heading": "8 Limitations",
            "text": "While proto-lm offers inherent interpretability by creating a connection between input text and pertinent parts of training data through the use of prototypes, it remains dependent on an underlying language model to convert text into a semantic space. Consequently, the interpretability of proto-lm is constrained by the interpretability of the foundational language model.\nMoreover, interpreting the significance of a learned prototype in the context of Natural Language Processing (NLP) tasks is still an open research area. Computer Vision (CV) methods used for visualizing prototypes in image-based tasks, such as upsampling a prototypical tensor, are not transferrable to language embeddings. Instead, researchers depend on projecting prototypes onto nearby training examples to decode prototype tensors into comprehensible natural language."
        },
        {
            "heading": "9 Ethics Statement",
            "text": "We present proto-lm, a framework that enhances the interpretability of Language Model Learning (LLMs) by providing explanations for their decisions using examples from the training dataset. We anticipate that the broad applicability of proto-lm across various NLP tasks will promote transparency and trust in the use of LLMs, thereby en-\ncouraging their wider adoption. As observed by authors like Rudin (2019) and Jim\u00e9nez-Luna et al. (2020), models with higher levels of interpretability inspire more confidence and enjoy greater utilization. We aim to contribute to advancing the field of interpretability research in NLP through our work.\nFor assessing the simulatability of our method, we employed Amazon Mechanical Turk (MTurk) for our human evaluation. To ensure English proficiency among workers, we restricted their location to the United States. Furthermore, only workers with a HIT approval rate of at least 99% were permitted to undertake our tasks. We provided a compensation of $0.20 per task, which roughly translates to about $24 per hour, significantly exceeding the US federal minimum wage. To uphold anonymity, we refrained from collecting any personal information from the annotators."
        },
        {
            "heading": "10 Acknowledgements",
            "text": "This research was supported in part by grants from the US National Library of Medicine (R01LM012837 & R01LM013833) and the US National Cancer Institute (R01CA249758). In addition, we would like to extend our gratitude to Joseph DiPalma, Yiren Jian, Naofumi Tomita, Weicheng Ma, Alex DeJournett, Wayner Barrios Quiroga. Peiying Hua, Weiyi Wu, Ting Shao, Guang Yang, and Chris Cortese for their valuable feedback on the manuscript and support during the research process."
        },
        {
            "heading": "A Training Details",
            "text": "In our experiments, we utilized two NVIDIA Titan RTX and two GeForce RTX 3090 GPUs to run our experiments. We conducted an extensive search for the best hyperparameters through experimentation. Our models were trained for a maximum of 40 epochs. We initialize weights (in the classification layer) connecting each prototype\u2019s similarity to the logit of their respective weights to 1 and other weights to -.5. This technique (Chen et al., 2019) allows for better association of prototypes and faster convergence. In terms of initializing the parameters within the prototypes, we initialized them randomly from [0,1). We used a learning rate and batch size to be 3e-6 and 128, respectively. We employed Adam (Kingma and Ba, 2014) as our optimizer with \u03b2\u2019s of (0.9, 0.999). Additionally, we limited the input size (number of tokens) to 512 during tokenization. We also investigated the optimal number of N and tested proto-nlp under N \u2208 {100, 200, 400, 800, 1000, 1200, 1400}. We discuss thsi in \u00a74.3. Similarly, we tested K \u2208 {1, 2, 5, 10, 100, 250, 500} and reported the results in \u00a74.4. Furthermore, we evaluated the effect of \u03bb0, \u03bb1, and \u03bb2 on our framework. The results of one of these tasks (IMDb) are presented in Figure 4 in the paper. The optimal \u03bb0 for the remaining tasks are reported in Table.3.\nWe utilized pretrained transformer models from Hugging Face, including:\n\u2022 BERT-base-uncased: https://huggingface.co/bert-base-uncased\n\u2022 BERT-large-uncased: https://huggingface.co/bert-large-uncased\n\u2022 RoBERTa-base: https://huggingface.co/roberta-base\n\u2022 RoBERTa-large: https://huggingface.co/roberta-large\n\u2022 BART-large: https://facebook/bart-large"
        },
        {
            "heading": "B Additional experiments and interpretability examples",
            "text": "We perform additional experiments with proto-lm on two biomedical datasets: The Hallmarks of Cancer Corpus (HoC) (Baker et al., 2016) and Sentence Similarity Estimation System for the Biomedical Domain (BIOSSES) (Sog\u0306anc\u0131og\u0306lu et al., 2017). HoC is a text classification dataset comprising of 1852 abstracts from PubMed publications that have been annotated by medical experts based on a taxonomy. BIOSSES consists of 100 pairs of PubMed sentences, with each pair having been evaluated by five expert annotators. The sentences are assigned a similarity score ranging from 0 (indicating no relation) to 4 (indicating equivalent meanings).\nWe build proto-lm with two pretrained LLM\u2019s for the biomedical domain: PubMedBERT (Gu et al., 2021) and BioGPT (Gu et al., 2021). We report our results on the biomedical datasets in Table 4. For the HoC results, we use N = 1000, \u03bb0 = 0.3, \u03bb1 = 0.35, \u03bb2 = 0.35, and K = 1. For the BIOSSES results, we use N = 25, \u03bb0 = 0.6, \u03bb1 = 0.25, \u03bb2 = 0.25, and K = 1. Also, as BIOSSES is a regression task, we set C to 1, forgo the softmax layer in the classification head, and replace Lce with Lmse. Similar to our results in Table. 1, we observe competitive performances from proto-lm, once again showing that proto-lm offers interpretability, but not at the cost of performance. To demonstrate proto-lm\u2019s interpretability, we additionally show 3 examples from HoC and the most/least similar prototypes found for those examples in Fig. 11."
        },
        {
            "heading": "C Effect of unequal \u03bb1 and \u03bb2",
            "text": "We conduct experiments on proto-lm, varying only \u03bb1 and \u03bb2 (the weights of coh and sep losses, respectively) while keep other hyperparameters the same. We show the results for these instances of proto-lm on SST5 in Figure. 9. We observe that\nwhile differing \u03bb1 and \u03bb2 values lead to convergence, placing equal emphasis on \u03bb1 and \u03bb2 leads to convergence at a lower loss. Intuitively, relying on either only pulling the correct prototypes together (cohesion) or only relying on pushing the incorrect prototypes apart (separation) is sufficient to create a meaningful prototypical space that allows for adequate performance on the task. Our experimental results show that placing equal emphasis leads to better performance."
        },
        {
            "heading": "D Projecting prototypes",
            "text": "In order to help human users understand prototypes in natural language, we identify the closest training data sample for each prototype and project the prototype onto that sample. The projection of prototypes onto the nearest sample is a well-studied and established technique (Das et al., 2022; Van Aken et al., 2022; Chen et al., 2019; Hase et al., 2019). We compare the quality of our projections against the projections obtained via the training procedure of Proto-tex (Das et al., 2022) and the loss function used in Protoypical Network (Chen et al., 2019) by measuring the average normalized distance between each prototype and their projected sample\u2019s embedding. We show the results for two datasets in Fig. 10. We observe that proto-lm is able to train prototypes that are much closer in distance to their projected samples\u2019 embeddings than the prototypes obtained via Protoypical Network loss and within a margin of error to that of Proto-tex."
        },
        {
            "heading": "E Prototype faithfulness",
            "text": "In addition to experiments in \u00a75.1 and \u00a75.2, we provide a theoretical justification for the inherent interpretability in proto-lm in this section. Denote dji as the distance between two prototypes and/or\ntraining samples i and j. Let \u03a0j = \u03c0 j 1:|D| be the probability distribution for prototype pj over the dataset D, where \u03c0ji = \u03b7j\ndji and \u03b7j is a constant for\nnormalization. Let a and b represent two training samples, their relative probabilities (for being projected onto by pj) would be \u03c0 j a\n\u03c0jb =\n\u03b7j/d j a \u03b7j/d j b = djb dja .\nIn addition, \u2211\nk\u2208D \u03c0 j k = 1 = \u2211 k\u2208D \u03b7j/d j k \u2192\n\u03b7j = 1\u2211\nk\u2208D 1/d j k\n, which means that each prototype\nis a soft-clustering over examples in D. Moreover, since dja/d j b = 1\n\u03c0ja/\u03c0 j b\n, if a a is n times further away\nfrom pj than b, then b is n times more probable in the probability distribution \u03a0j . Similarly, during inference time, for example k, if a prototype i is n times away from k than prototype j, then dik/d j k = n \u2192 \u03c0 j k/\u03c0 i k = n, i.e. j will be n times more probable to be the prediction than i for k.\nF Interpretability examples and sample Mturk questionnaires"
        }
    ],
    "title": "Proto-lm: A Prototypical Network-Based Framework for Built-in Interpretability in Large Language Models",
    "year": 2023
}