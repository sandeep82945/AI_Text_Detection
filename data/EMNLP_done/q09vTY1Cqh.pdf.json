{
    "abstractText": "The task of repository-level code completion is to continue writing the unfinished code based on a broader context of the repository. While for automated code completion tools, it is difficult to utilize the useful information scattered in different files. We propose RepoCoder, a simple, generic, and effective framework to address the challenge. It streamlines the repositorylevel code completion process by incorporating a similarity-based retriever and a pre-trained code language model in an iterative retrievalgeneration pipeline. RepoCoder makes effective utilization of repository-level information for code completion and has the ability to generate code at various levels of granularity. Moreover, we propose a new benchmark RepoEval, which consists of the latest and highquality real-world repositories covering line, API invocation, and function body completion scenarios. Experimental results indicate that RepoCoder significantly improves the In-File completion baseline by over 10% in all settings and consistently outperforms the vanilla retrieval-augmented code completion approach. Furthermore, we validate the effectiveness of RepoCoder through comprehensive analysis, providing valuable insights for future research. Our source code and benchmark are publicly available: https://github.com/microsoft/ CodeT/tree/main/RepoCoder",
    "authors": [
        {
            "affiliations": [],
            "name": "Fengji Zhang"
        },
        {
            "affiliations": [],
            "name": "Bei Chen"
        },
        {
            "affiliations": [],
            "name": "Yue Zhang"
        },
        {
            "affiliations": [],
            "name": "Jacky Keung"
        },
        {
            "affiliations": [],
            "name": "Jin Liu"
        },
        {
            "affiliations": [],
            "name": "Daoguang Zan"
        },
        {
            "affiliations": [],
            "name": "Yi Mao"
        },
        {
            "affiliations": [],
            "name": "Jian-Guang Lou"
        },
        {
            "affiliations": [],
            "name": "Weizhu Chen"
        }
    ],
    "id": "SP:ebdaf4b097f6c82a727f43c13e24de3410279e54",
    "references": [
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners. Advances in neural information processing",
            "year": 2020
        },
        {
            "authors": [
                "Bei Chen",
                "Fengji Zhang",
                "Anh Nguyen",
                "Daoguang Zan",
                "Zeqi Lin",
                "Jian-Guang Lou",
                "Weizhu Chen."
            ],
            "title": "Codet: Code generation with generated tests",
            "venue": "arXiv preprint arXiv:2207.10397.",
            "year": 2022
        },
        {
            "authors": [
                "Mark Chen",
                "Jerry Tworek",
                "Heewoo Jun",
                "Qiming Yuan",
                "Henrique Ponde de Oliveira Pinto",
                "Jared Kaplan",
                "Harri Edwards",
                "Yuri Burda",
                "Nicholas Joseph",
                "Greg Brockman"
            ],
            "title": "Evaluating large language models trained on code",
            "year": 2021
        },
        {
            "authors": [
                "Aakanksha Chowdhery",
                "Sharan Narang",
                "Jacob Devlin",
                "Maarten Bosma",
                "Gaurav Mishra",
                "Adam Roberts",
                "Paul Barham",
                "Hyung Won Chung",
                "Charles Sutton",
                "Sebastian Gehrmann"
            ],
            "title": "Palm: Scaling language modeling with pathways",
            "year": 2022
        },
        {
            "authors": [
                "Colin B Clement",
                "Shuai Lu",
                "Xiaoyu Liu",
                "Michele Tufano",
                "Dawn Drain",
                "Nan Duan",
                "Neel Sundaresan",
                "Alexey Svyatkovskiy."
            ],
            "title": "Long-range modeling of source code files with ewash: Extended window access by syntax hierarchy",
            "venue": "arXiv preprint",
            "year": 2021
        },
        {
            "authors": [
                "Yangruibo Ding",
                "Zijian Wang",
                "Wasi Uddin Ahmad",
                "Murali Krishna Ramanathan",
                "Ramesh Nallapati",
                "Parminder Bhatia",
                "Dan Roth",
                "Bing Xiang."
            ],
            "title": "Cocomic: Code completion by jointly modeling in-file and cross-file context",
            "venue": "arXiv preprint",
            "year": 2022
        },
        {
            "authors": [
                "Daya Guo",
                "Shuai Lu",
                "Nan Duan",
                "Yanlin Wang",
                "Ming Zhou",
                "Jian Yin."
            ],
            "title": "Unixcoder: Unified crossmodal pre-training for code representation",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long",
            "year": 2022
        },
        {
            "authors": [
                "Kelvin Guu",
                "Kenton Lee",
                "Zora Tung",
                "Panupong Pasupat",
                "Mingwei Chang."
            ],
            "title": "Retrieval augmented language model pre-training",
            "venue": "International conference on machine learning, pages 3929\u20133938. PMLR.",
            "year": 2020
        },
        {
            "authors": [
                "Vincent J Hellendoorn",
                "Premkumar Devanbu"
            ],
            "title": "Are deep neural networks the best choice for modeling source code",
            "venue": "In Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering,",
            "year": 2017
        },
        {
            "authors": [
                "Vincent J Hellendoorn",
                "Sebastian Proksch",
                "Harald C Gall",
                "Alberto Bacchelli."
            ],
            "title": "When code completion fails: A case study on real-world completions",
            "venue": "2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE), pages 960\u2013970. IEEE.",
            "year": 2019
        },
        {
            "authors": [
                "Gautier Izacard",
                "Patrick Lewis",
                "Maria Lomeli",
                "Lucas Hosseini",
                "Fabio Petroni",
                "Timo Schick",
                "Jane Dwivedi-Yu",
                "Armand Joulin",
                "Sebastian Riedel",
                "Edouard Grave."
            ],
            "title": "Few-shot learning with retrieval augmented language models",
            "venue": "arXiv preprint",
            "year": 2022
        },
        {
            "authors": [
                "Paul Jaccard."
            ],
            "title": "The distribution of the flora in the alpine zone",
            "venue": "1. New phytologist, 11(2):37\u201350.",
            "year": 1912
        },
        {
            "authors": [
                "Vladimir I Levenshtein"
            ],
            "title": "Binary codes capable of correcting deletions, insertions, and reversals",
            "venue": "Soviet physics doklady, volume 10, pages 707\u2013710. Soviet Union.",
            "year": 1966
        },
        {
            "authors": [
                "Yoav Levine",
                "Itay Dalmedigos",
                "Ori Ram",
                "Yoel Zeldes",
                "Daniel Jannai",
                "Dor Muhlgay",
                "Yoni Osin",
                "Opher Lieber",
                "Barak Lenz",
                "Shai Shalev-Shwartz"
            ],
            "title": "Standing on the shoulders of giant frozen language models. arXiv preprint arXiv:2204.10019",
            "year": 2022
        },
        {
            "authors": [
                "Patrick Lewis",
                "Ethan Perez",
                "Aleksandra Piktus",
                "Fabio Petroni",
                "Vladimir Karpukhin",
                "Naman Goyal",
                "Heinrich K\u00fcttler",
                "Mike Lewis",
                "Wen-tau Yih",
                "Tim Rockt\u00e4schel"
            ],
            "title": "Retrieval-augmented generation for knowledge-intensive nlp tasks",
            "year": 2020
        },
        {
            "authors": [
                "Dong Li",
                "Yelong Shen",
                "Ruoming Jin",
                "Yi Mao",
                "Kuan Wang",
                "Weizhu Chen."
            ],
            "title": "Generationaugmented query expansion for code retrieval",
            "venue": "arXiv preprint arXiv:2212.10692.",
            "year": 2022
        },
        {
            "authors": [
                "Raymond Li",
                "Loubna Ben Allal",
                "Yangtian Zi",
                "Niklas Muennighoff",
                "Denis Kocetkov",
                "Chenghao Mou",
                "Marc Marone",
                "Christopher Akiki",
                "Jia Li",
                "Jenny Chim"
            ],
            "title": "Starcoder: may the source be with you! arXiv preprint arXiv:2305.06161",
            "year": 2023
        },
        {
            "authors": [
                "Fang Liu",
                "Ge Li",
                "Zhiyi Fu",
                "Shuai Lu",
                "Yiyang Hao",
                "Zhi Jin."
            ],
            "title": "Learning to recommend method names with global context",
            "venue": "Proceedings of the 44th International Conference on Software Engineering, pages 1294\u20131306.",
            "year": 2022
        },
        {
            "authors": [
                "Shuai Lu",
                "Nan Duan",
                "Hojae Han",
                "Daya Guo",
                "Seungwon Hwang",
                "Alexey Svyatkovskiy."
            ],
            "title": "Reacc: A retrieval-augmented code completion framework",
            "venue": "arXiv preprint arXiv:2203.07722.",
            "year": 2022
        },
        {
            "authors": [
                "Shuai Lu",
                "Daya Guo",
                "Shuo Ren",
                "Junjie Huang",
                "Alexey Svyatkovskiy",
                "Ambrosio Blanco",
                "Colin Clement",
                "Dawn Drain",
                "Daxin Jiang",
                "Duyu Tang"
            ],
            "title": "Codexglue: A machine learning benchmark dataset for code understanding and generation",
            "year": 2021
        },
        {
            "authors": [
                "Ziyang Luo",
                "Can Xu",
                "Pu Zhao",
                "Qingfeng Sun",
                "Xiubo Geng",
                "Wenxiang Hu",
                "Chongyang Tao",
                "Jing Ma",
                "Qingwei Lin",
                "Daxin Jiang."
            ],
            "title": "Wizardcoder: Empowering code large language models with evolinstruct",
            "venue": "arXiv preprint arXiv:2306.08568.",
            "year": 2023
        },
        {
            "authors": [
                "Erik Nijkamp",
                "Bo Pang",
                "Hiroaki Hayashi",
                "Lifu Tu",
                "Huan Wang",
                "Yingbo Zhou",
                "Silvio Savarese",
                "Caiming Xiong."
            ],
            "title": "Codegen: An open large language model for code with multi-turn program synthesis",
            "venue": "arXiv preprint arXiv:2203.13474.",
            "year": 2022
        },
        {
            "authors": [
                "Ori Ram",
                "Yoav Levine",
                "Itay Dalmedigos",
                "Dor Muhlgay",
                "Amnon Shashua",
                "Kevin Leyton-Brown",
                "Yoav Shoham."
            ],
            "title": "In-context retrieval-augmented language models",
            "venue": "arXiv preprint arXiv:2302.00083.",
            "year": 2023
        },
        {
            "authors": [
                "Veselin Raychev",
                "Martin Vechev",
                "Eran Yahav."
            ],
            "title": "Code completion with statistical language models",
            "venue": "Proceedings of the 35th ACM SIGPLAN conference on programming language design and implementation, pages 419\u2013428.",
            "year": 2014
        },
        {
            "authors": [
                "Md Rizwan Parvez",
                "Wasi Uddin Ahmad",
                "Saikat Chakraborty",
                "Baishakhi Ray",
                "Kai-Wei Chang."
            ],
            "title": "Retrieval augmented code generation and summarization",
            "venue": "arXiv e-prints, pages arXiv\u20132108.",
            "year": 2021
        },
        {
            "authors": [
                "Weijia Shi",
                "Sewon Min",
                "Michihiro Yasunaga",
                "Minjoon Seo",
                "Rich James",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Wen-tau Yih."
            ],
            "title": "Replug: Retrievalaugmented black-box language models",
            "venue": "arXiv preprint arXiv:2301.12652.",
            "year": 2023
        },
        {
            "authors": [
                "Disha Shrivastava",
                "Hugo Larochelle",
                "Daniel Tarlow."
            ],
            "title": "Repository-level prompt generation for large language models of code",
            "venue": "arXiv preprint arXiv:2206.12839.",
            "year": 2022
        },
        {
            "authors": [
                "Alexey Svyatkovskiy",
                "Shao Kun Deng",
                "Shengyu Fu",
                "Neel Sundaresan."
            ],
            "title": "Intellicode compose: Code generation using transformer",
            "venue": "Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foun-",
            "year": 2020
        },
        {
            "authors": [
                "Alexey Svyatkovskiy",
                "Sebastian Lee",
                "Anna Hadjitofi",
                "Maik Riechert",
                "Juliana Vicente Franco",
                "Miltiadis Allamanis."
            ],
            "title": "Fast and memory-efficient neural code completion",
            "venue": "2021 IEEE/ACM 18th International Conference on Mining Software Repositories",
            "year": 2021
        },
        {
            "authors": [
                "Alexey Svyatkovskiy",
                "Ying Zhao",
                "Shengyu Fu",
                "Neel Sundaresan."
            ],
            "title": "Pythia: Ai-assisted code completion system",
            "venue": "Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining, pages 2727\u20132735.",
            "year": 2019
        },
        {
            "authors": [
                "Romal Thoppilan",
                "Daniel De Freitas",
                "Jamie Hall",
                "Noam Shazeer",
                "Apoorv Kulshreshtha",
                "Heng-Tze Cheng",
                "Alicia Jin",
                "Taylor Bos",
                "Leslie Baker",
                "Yu Du"
            ],
            "title": "Lamda: Language models for dialog applications",
            "venue": "arXiv preprint arXiv:2201.08239",
            "year": 2022
        },
        {
            "authors": [
                "Zhaopeng Tu",
                "Zhendong Su",
                "Premkumar Devanbu."
            ],
            "title": "On the localness of software",
            "venue": "Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering, pages 269\u2013 280.",
            "year": 2014
        },
        {
            "authors": [
                "Liang Wang",
                "Nan Yang",
                "Furu Wei."
            ],
            "title": "Query2doc: Query expansion with large language models",
            "venue": "arXiv preprint arXiv:2303.07678.",
            "year": 2023
        },
        {
            "authors": [
                "Daoguang Zan",
                "Bei Chen",
                "Zeqi Lin",
                "Bei Guan",
                "Yongji Wang",
                "Jian-Guang Lou."
            ],
            "title": "When language model meets private library",
            "venue": "arXiv preprint arXiv:2210.17236.",
            "year": 2022
        },
        {
            "authors": [
                "Yury Zemlyanskiy",
                "Michiel de Jong",
                "Joshua Ainslie",
                "Panupong Pasupat",
                "Peter Shaw",
                "Linlu Qiu",
                "Sumit Sanghai",
                "Fei Sha."
            ],
            "title": "Generate-and-retrieve: Use your predictions to improve retrieval for semantic parsing",
            "venue": "Proceedings of the 29th Inter-",
            "year": 2022
        },
        {
            "authors": [
                "Yizhe Zhang",
                "Siqi Sun",
                "Xiang Gao",
                "Yuwei Fang",
                "Chris Brockett",
                "Michel Galley",
                "Jianfeng Gao",
                "Bill Dolan."
            ],
            "title": "Retgen: A joint framework for retrieval and grounded text generation modeling",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2022
        },
        {
            "authors": [
                "Shuyan Zhou",
                "Uri Alon",
                "Frank F Xu",
                "Zhengbao JIang",
                "Graham Neubig."
            ],
            "title": "Doccoder: Generating code by retrieving and reading docs",
            "venue": "arXiv preprint arXiv:2207.05987.",
            "year": 2022
        },
        {
            "authors": [
                "Weiqin Zou",
                "Jifeng Xuan",
                "Xiaoyuan Xie",
                "Zhenyu Chen",
                "Baowen Xu."
            ],
            "title": "How does code style inconsistency affect pull request integration? an exploratory study on 117 github projects",
            "venue": "Empirical Software Engineering, 24:3871\u20133903.",
            "year": 2019
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "In real-world software production, it is crucial for developers to be aware of other files within the repository during programming. This challenge gives rise to the task of repository-level code completion, where automated tools are expected to utilize the broader context of a repository rather than relying solely on in-file information to complete unfinished code. Code files within a repository often exhibit interrelated dependencies, including shared utilities, configurations, and cross-API invocations resulting from modularization (Tu et al.,\n2014). Additionally, each repository typically follows customized naming conventions and coding styles (Zou et al., 2019), which contribute to enhanced readability and maintainability. However, developing effective repository-level code completion tools remains an open problem. Although approaches relying on static code analysis and heuristic rules (Raychev et al., 2014; Svyatkovskiy et al., 2019, 2021) can reliably parse specific repository context, they have limitations in the completion scenario, limiting capability for varying-length completions anywhere in a file. Meanwhile, studies (Hellendoorn and Devanbu, 2017; Svyatkovskiy et al., 2020; Ding et al., 2022) tuning language models on labeled data excel in their respective evaluation scenarios but face challenges generalizing to unseen repositories without retraining.\nIn this paper, we propose an approach to leverage off-the-shelf retrievers in order to locate valuable information within a repository and enhance the context for language models. We introduce a novel framework called RepoCoder that aims to improve code retrieval and completion performance. As depicted in Figure 1, we enhance the conventional In-File code completion method by incorporating the Retrieval-Augmented Generation\n(RAG) technique, which allows us to search for relevant code snippets from the repository to assist in generating the code completion. Additionally, we introduce RepoCoder, which employs an iterative pipeline that utilizes the generated code completion to enhance the retrieval process, thus bridging the gap between the retrieval context and the intended completion target. Figure 2 provides an example that illustrates the rationale behind our design. We demonstrate that relying solely on the unfinished code is insufficient to retrieve useful information from the repository. In the example, the model improvises a statement calling the COLMAP API in the first iteration. The predicted parameters are reasonable yet incorrect. This is because the incomplete code preceding the code completion does not serve as an adequate retrieval query for the intended completion target. However, by performing a subsequent retrieval from the repository using the model\u2019s generated completion, we can successfully retrieve the target API signature and complete the code effectively.\nFurthermore, we introduce the RepoEval benchmark designed for evaluating the repository-level code completion task, which is constructed using the latest high-quality repositories sourced from GitHub1. By introducing RepoEval, we address the lack of established benchmarks in the repositorylevel scenario. Notably, RepoEval is the first benchmark that encompasses three levels of code completion granularity: line, API invocation, and function body. We also leverage unit tests present in the repository to enhance the accuracy of evaluation, which overcomes the limitations of similaritybased metrics. To rigorously validate the effectiveness of RepoCoder, we conduct extensive experiments using different language models of varying sizes, including GPT-3.5-Turbo2 and CODEGEN (Nijkamp et al., 2022). Experimental results demonstrate that RepoCoder achieves significant improvements over In-File completion performance, surpassing the baseline by over 10% across different experimental settings. Moreover, our iterative framework consistently enhances the performance of vanilla retrieval-augmented generation. We also provide a comprehensive analysis of the effectiveness and limitations of RepoCoder, offering insights for future research. Our contributions can be summarized as follows:\n1https://github.com 2https://platform.openai.com/docs/models/\ngpt-3-5\n\u2022 We propose RepoCoder, a novel iterative retrieval-generation framework for the repository-level code completion task.\n\u2022 We introduce the RepoEval benchmark, enabling the evaluation of repository-level code completion with varying levels of granularity and improved evaluation accuracy through the utilization of unit tests.\n\u2022 Through rigorous experimentation, we demonstrate that RepoCoder significantly outperforms the In-File code completion paradigm and enhances the performance of vanilla retrieval-augmented generation."
        },
        {
            "heading": "2 Methodology",
            "text": ""
        },
        {
            "heading": "2.1 Overall Framework",
            "text": "The task of code completion using a language model M can be generally described as Y\u0302 = M(X), where Y\u0302 represents the predicted tokens and X corresponds to the in-file unfinished code. By introducing an additional code retrieval model R, we can transform the code completion pipeline into a Retrieval-Augmented Generation (RAG) approach. Initially, we establish a retrieval database by partitioning the code files from the repository into a collection of code snippets Crepo = {c1, c2, \u00b7 \u00b7 \u00b7 }. Subsequently, we utilize the retrieval model R to extract the most relevant code snippets from Crepo by employing the unfinished code X\nas the retrieval query. This process yields a set of retrieved code snippets Cret = R(Crepo, X). Following this, we leverage the language model M to perform code completion, resulting in the prediction Y\u0302 = M(Cret, X). Consequently, we are able to incorporate the contextual information from the repository level during the code completion task.\nHowever, using the unfinished code X as the sole retrieval query introduces a gap between the retrieval context and the intended completion target, as exemplified in Figure 2. To address this limitation, we propose RepoCoder, an iterative retrieval-generation pipeline designed to further enhance the performance of the vanilla RAG method. Specifically, for the i-th retrieval-generation (i > 1) iteration, RepoCoder utilizes the previous model prediction Y\u0302 i\u22121 to construct a new query for the retrieval process. This leads to the generation of another set of relevant code snippets Ciret = R(Crepo, X, Y\u0302 i\u22121). Subsequently, a new prompt is constructed using Ciret, resulting in the generation of a new prediction Y\u0302 i = M(Ciret, X). The newly generated code completion can serve as either the output of RepoCoder or be utilized for the subsequent retrieval-generation iteration.\nImportantly, it is worth noting that the parameters of M and R remain unchanged throughout the entire process. Moreover, there is no requirement for static code analysis tools or heuristic rules to construct the retrieval database. In the following subsections, we provide a detailed explanation of the code retrieval process (Section 2.2) and the code generation process (Section 2.3)."
        },
        {
            "heading": "2.2 Code Retrieval",
            "text": "The retriever utilized within the RepoCoder framework can be any model capable of searching for relevant documents given a specific query. To construct the retrieval database, a sliding window approach is employed. The sliding window traverses the files in the repository and extracts contiguous lines of code that fit within the window size, denoted as Sw. The sliding window moves a fixed number of lines at each iteration, which is referred to as the sliding size, denoted as Ss.\nDuring the initial retrieval process, when no model prediction is available, the query is formulated using the last Sw lines of the unfinished code X . Consequently, the most similar code snippets are retrieved using the retrieval model, resulting in C1ret = R(Crepo, X). However, a gap exists\nbetween the retrieval context, based on X , and the intended completion target, which is to continue writing X . A possible solution is to adjust C1ret by shifting each code snippet down by a few lines to include the subsequent code. Although this shifting approach has shown effectiveness in previous work (Lu et al., 2022), indiscriminately shifting all retrieved code snippets without considering their content may not always be appropriate.\nTo address this issue, RepoCoder augments the retrieval query during the i-th iteration (i > 1) with the previously generated code Y\u0302 i\u22121. Despite the lack of customized information for new repositories, pre-trained code language models have demonstrated impressive general-domain understanding and generation capabilities. The generated code Y\u0302 i\u22121 can provide valuable supplementary information for the retrieval process, even though its correctness may not be guaranteed. Therefore, for the i-th iteration of retrieval (i > 1), the query is constructed by concatenating the last (Sw\u2212Ss) lines of X with the first Ss lines of Y\u0302 i\u22121. This approach yields the grounded retrieval results Ciret = R(Crepo, X, Y\u0302 i\u22121)."
        },
        {
            "heading": "2.3 Code Generation",
            "text": "The generator employed within the RepoCoder framework can be any pre-trained language model capable of predicting subsequent tokens given a specific prompt. As mentioned earlier, it is crucial to incorporate both the context from the repository Crepo and the context within the target file for effective code completion. This enables the model to"
        },
        {
            "heading": "ID Name License Created F. N.",
            "text": "leverage grounding information and enhances its generalization ability to unseen repositories.\nIn the RepoCoder framework, we retrieve the most relevant code examples, denoted as Cret, from the repository and concatenate them with the unfinished code X . To ensure readability and comprehension, we create a prompt template that seamlessly integrates X and Cret, as illustrated in Figure 3. The retrieved code snippets are arranged in ascending order based on their similarity scores to the query. Each code snippet is accompanied by its original file path, and the maximum number of code snippets included in the prompt, denoted as K, depends on the available prompt length. Ultimately, the prompt contains as much relevant information as possible to facilitate code completion."
        },
        {
            "heading": "3 Benchmark Construction",
            "text": "To facilitate the evaluation of code completion tools in the repository-level scenario, we propose a novel RepoEval benchmark. This benchmark is carefully constructed using the latest high-quality repositories sourced from GitHub and encompasses three levels of code completion granularity: line, API invocation, and function body. To assess the correctness of completed functions, we utilize unit tests present in the repository instead of relying solely on similarity-based metrics. Each sample in the RepoEval benchmark is annotated with the corre-\nsponding source repository, file path, line numbers, and ground truth completion. For analysis and unit test execution, complete copies of the repositories are archived as of January 2023.\nTo construct RepoEval, we first meticulously curate a collection of Python repositories from GitHub that satisfy the following criteria: opensource license, created after January 1, 20223, nonfork original repositories, over 100 stars, over 80% of files written in Python, and explicit unit tests. Furthermore, to mitigate potential biases, we employ a random selection process for the repositories and create three distinct datasets for line completion, API invocation completion, and function body completion. Additional details regarding the selected repositories can be found in Table 1.\nLine completion: In adherence to the conventions of code completion benchmarks (Lu et al., 2021, 2022), we implement the line completion scenario. First, according to the above-mentioned criteria, we select 8 repositories that vary in size and cover different domains. Then we randomly select 200 lines to complete from each repository, ensuring the lines are non-repetitive, not code comments, and each line contains at least 5 tokens. Eventually, a total of 1600 test samples are generated for the line completion dataset.\nAPI Invocation Completion: We also choose to test the API completion scenario, especially inrepository defined APIs. It is a harder problem than the completion of built-in or third-party APIs due to the lack of customized training data (Hellendoorn et al., 2019). We utilize the same group of repositories in the line dataset and parse the target repositories to locate invocations of in-repository APIs. From these candidates, we then randomly select 200 non-repetitive API invocations from each repository, resulting in a total of 1600 test samples for the API invocation completion dataset.\nFunction Body Completion: Alongside the line and API completion evaluations, we also assess the ability to complete function bodies, which requires executing unit tests present in the repository. However, running tests can be time-consuming and computationally expensive. To address this, we randomly select a separate set of smaller-scale repositories that are easy to deploy. Within these repositories, we locate functions covered by unit tests\n3The training data of GPT-3.5-Turbo and CODEGEN is up to 2021. We use data from 2022 to prevent data leakage.\nand select function bodies containing 3 to 30 lines of code to complete. This yields a total of 373 test samples for the function body completion dataset."
        },
        {
            "heading": "4 Experimental Setup",
            "text": ""
        },
        {
            "heading": "4.1 Methods for Comparison",
            "text": "In-File Completion: Previous studies (Chen et al., 2021; Nijkamp et al., 2022; Chen et al., 2022) have demonstrated the effectiveness of utilizing large pre-trained language models for code generation in a zero-shot completion manner, conditioned on the provided context. Furthermore, it has been established that incorporating in-file context is beneficial for code completion scenarios (Clement et al., 2021). Hence, as a baseline, we implement an In-File completion method by populating the prompt with the unfinished code and directly utilizing the pre-trained code generation model to predict the code completion.\nOracle Method: A key contribution of RepoCode is the integration of model predictions for retrieval, bridging the gap between retrieval and the intended completion target. To showcase the effectiveness of this approach, we devise an oracle retrieval-augmented generation method for comparison purposes. This method performs a single retrieval process to obtain relevant code snippets, denoted as Cgtret, by utilizing the last Sw \u2212 Ss lines of X and the first Ss lines of the ground truth code, Y . Subsequently, the completion code, denoted as Y\u0302 , is generated through M(Cgtret, X). This allows us to achieve the upper bound of performance for RepoCoder, conditioned on the retrieval model R and the generation model M."
        },
        {
            "heading": "4.2 Implementation Details",
            "text": "Retrieval Model: For our main experiments, we employ a sparse bag-of-words model as the retrieval model, which has demonstrated effectiveness in retrieving similar code snippets (Lu et al., 2022). This model transforms the query and candidate code snippets into sets of tokens and calculates their similarity using the Jaccard index (Jaccard, 1912), computed as Jaccard(Sq, Sc) =\n|Sq\u2229Sc| |Sq\u222aSc| ,\nwhere Sq and Sc represent the tokens of the query and candidate code snippets, respectively. We also experiment with a dense retriever based on UniXcoder (Guo et al., 2022), detailed in Appendix B.\nGeneration Model: We evaluate RepoCoder using four pre-trained language models with vary-\ning code generation capabilities. The first model, GPT-3.5-Turbo, is a state-of-the-art commercial code generation model with billions of trainable parameters and has been pre-trained on an extensive code corpus. Access to GPT-3.5-Turbo is obtained through the API provided by OpenAI. The second model, CODEGEN, is an open-source code generation model that has multiple published versions with varying model sizes and training data. In our experiments, we utilize three versions of CODEGEN model with 6B, 2B, and 350M parameters.\nHyper-parameters: We found that RepoCoder\u2019s performance was not highly sensitive to changes in hyper-parameters. Therefore, for our experiments on RepoEval, we assign hyper-parameters based on our experience. Specifically, the maximum number of tokens for the combined input prompt and output prediction is set to 4, 096 for GPT-3.5-Turbo and 2, 048 for CODEGEN. The length of retrieved code snippets is set to half the prompt length. For line and API completion, the maximum number of tokens in the generated completion (Y\u0302 ), the line length of the sliding window (Sw), and the sliding size (Ss) are set to 100, 20, and 10 respectively. For function body completion, these values are adjusted to 500, 50, and 10. The maximum number of retrieved snippets (K) is set to 10. The same hyperparameters were used for the single-iteration RAG, iterative RepoCoder, and Oracle baselines, ensuring a fair comparison between methods. Notably, given that these parameters are intricately linked to the programming language and contextual scenarios, practitioners should make adjustments to ensure optimal real-world performance."
        },
        {
            "heading": "4.3 Evaluation Metrics",
            "text": "Similarity-based Evaluation: Following established practices in code completion research (Lu et al., 2021, 2022), we evaluate our line and API completion datasets using two metrics: Exact Match (EM) and Edit Similarity (ES). The EM score is a binary metric that takes the value of 1 if the predicted code exactly matches the ground truth code, and 0 otherwise. The ES metric provides a more fine-grained evaluation and is calculated as ES = 1 \u2212 Lev(Y\u0302 ,Y )\nmax(|Y\u0302 |,|Y |) , where Lev represents the\nLevenshtein distance (Levenshtein et al., 1966).\nExecution-based Evaluation: For the function body completion dataset, we utilize unit tests present in the repository to evaluate functional\nMetric Oracle In-File RepoCoder Iterations\n1 2 3 4\nGPT-3.5-Turbo EM 57.75 40.56 55.31 56.81 57.00 56.63 ES 75.43 65.06 74.38 75.11 75.30 75.10\nCODEGEN-MONO-6B EM 48.81 34.56 45.81 47.06 47.75 47.44 ES 71.02 60.67 69.21 70.10 70.73 70.19\nCODEGEN-MONO-2B EM 47.31 33.63 44.56 46.94 46.69 47.13 ES 69.80 58.99 67.68 68.82 68.62 68.92\nCODEGEN-MONO-350M EM 45.19 29.56 41.88 43.06 43.94 43.06 ES 67.20 55.39 65.05 65.66 65.97 65.62\n(a) Line Completion.\nMetric Oracle In-File RepoCoder Iterations\n1 2 3 4\nGPT-3.5-Turbo EM 50.13 34.06 47.69 49.19 49.44 49.56 ES 74.50 63.22 73.63 74.43 74.59 74.48\nCODEGEN-MONO-6B EM 40.25 26.19 36.69 38.88 39.13 39.31 ES 67.94 56.45 64.20 65.52 65.53 65.90\nCODEGEN-MONO-2B EM 39.44 25.44 35.44 37.56 38.44 38.25 ES 66.78 56.88 63.47 64.15 64.53 64.60\nCODEGEN-MONO-350M EM 34.88 22.19 31.75 33.88 33.75 33.81 ES 63.06 52.24 59.82 61.03 60.96 61.06\n(b) API Invocation Completion.\nTable 2: Performance comparison on the line and API invocation completion datasets. Results present the average performance of each method evaluated using Exact Match (EM) and Edit Similarity (ES) scores. Numbers are shown in percentage (%), with the best performance highlighted in bold.\ncorrectness. This approach is more reliable than similarity-based metrics in assessing the behavior of the completed functions. While collecting unit tests can be time-consuming, we focus on a realistic scenario and utilize the unit tests available in GitHub repositories to validate the generated code. We execute the completed code and report the Pass Rate (PR), where PR is 1 if the code passes all the corresponding test cases, and 0 otherwise."
        },
        {
            "heading": "5 Experimental Results",
            "text": ""
        },
        {
            "heading": "5.1 Line and API Completion Datasets",
            "text": "We compare the performance of RepoCoder with the In-File completion method and the Oracle method on the line and API invocation completion datasets using four pre-trained language models and different retrieval-generation iterations. From the results listed in Table 2a and 2b, we find that RepoCoder consistently improves the In-File completion performance on both datasets across all model sizes. The absolute improvements in the Exact Match (EM) and Edit Similarity (ES) scores exceed 10% and 8%, respectively. RepoCoder also shows competitive results compared to the Oracle method. With two or more iterations, RepoCoder consistently outperforms the vanilla Retrieval-Augmented Generation (RAG) approach for all language models. Additionally, the CODEGEN model with 350M parameters is comparable to the GPT-3.5-Turbo model with In-File completion when integrated with RepoCoder. We also test RepoCoder using a dense retriever powered\nby UniXcoder (Guo et al., 2022) (detailed in Appendix B) and find that the simple sparse retriever achieves equivalent performance, highlighting the robustness of RepoCoder across different code retrieval and generation models."
        },
        {
            "heading": "5.2 Function Completion Dataset",
            "text": "We proceed to assess the performance of RepoCoder on the function body completion dataset. To tackle the greater difficulty of function body completion, we employ the GPT-3.5-Turbo model due to its superior code understanding and generation capabilities, as well as its larger prompt length suitable for longer function code snippets. The evaluation results, presented in Table 3, showcase similar trends to our findings on the line and API invo-"
        },
        {
            "heading": "CODEGEN-MONO-6B",
            "text": ""
        },
        {
            "heading": "CODEGEN-MONO-350M",
            "text": "cation completion datasets. Across most repositories, RepoCoder exhibits significant improvement over the In-File completion method and competitive performance compared to the Oracle method. Moreover, with additional retrieval-generation iterations, RepoCoder consistently outperforms the vanilla Retrieval-Augmented Generation (RAG) approach. These results reaffirm the effectiveness of our approach."
        },
        {
            "heading": "6 Analysis",
            "text": "In this section, we conduct further analyses on the retrieved code snippets to gain a deeper understanding of RepoCoder and provide valuable insights for future research."
        },
        {
            "heading": "6.1 Quality of Retrieved Code",
            "text": "We observe a significant impact of the retrieved code\u2019s quality on code completion performance. And the most helpful code snippets typically contain code statements similar to the target completion or demonstrate example usages of the target API invocation. Then, to validate the correlation between retrieval quality and completion performance, we design an analysis experiment using the API invocation completion dataset. In this experiment, we leverage a static code analysis tool to locate code snippets in other files that include invocations of the ground truth API. Subsequently, we rank these code snippets based on their similarity to the unfinished code and select the most similar\nones to include in the completion prompt. We refer to this method as GT-Code and compare its performance against the In-File and RepoCoder methods. Additionally, we show the recall performance of RepoCoder by counting the number of retrieved code snippets containing invocation examples of the ground truth API.\nSince not every API in the test dataset has invocation examples in other files, and we also exclude the invocation examples existing in the input prompt for the model, we finally extract from the API invocation dataset 1046 and 1083 eligible test samples respectively for the GPT-3.5-Turbo and CODEGEN models to conduct the experiment. From the obtained results in Table 4, we observe that the GT-Code method, which utilizes ground truth API invocation examples, generally achieves the best performance among all methods. Furthermore, RepoCoder with two iterations exhibits higher recall for ground truth API invocations compared to a single-iteration, which likely contributes to its superior code completion performance. Notably, as the language model grows more powerful, the recall value using RepoCoder Iter-2 also increases, indicating the model predictions indeed assist the retrieval process and emphasizing the effectiveness of RepoCoder."
        },
        {
            "heading": "6.2 Locations of Retrieved Code",
            "text": "The retrieval of code snippets provides valuable contextual information from other files to enhance the context for language models. We conduct a separate experiment to study the various locations from which effective retrieval occurs. Specifically, we select test samples that are successfully pre-\ndicted by the Oracle/RepoCoder method but not by In-File completion using GPT-3.5-Turbo. This yields a number of eligible test samples and retrieved code snippets for the line and API invocation completion datasets. To determine the original source of these code snippets, we adopt a classification scheme inspired by Shrivastava et al. (2022), consisting of five distinct file locations: 1. Imported: code from a file imported by the target file. 2. Current File: code from the excluded content of the target file. 3. Current Directory: code from a file in the same directory as the target file. 4. Similar Import: code from a file sharing at least one same API import with the target file. 5. Similar Name: code from a file with a file name sharing at least one token with the target file (assuming snake-case style file names).\nThe results are as outlined in Table 5. Our findings indicate a similar distribution of retrieved code snippets between the Oracle method and RepoCoder. The majority of code snippets fall within our defined categories, and a significant portion of code snippets originates from files with \u201cSimilar Import\u201d, \u201cSimilar Name\u201d, or \u201cCurrent Directory\u201d locations, underscoring the importance of contextual information in code completion tasks. Furthermore, we conduct an ablation study, wherein we restrict the retrieval process to only the aforementioned file locations. The results reveal a degradation in performance, highlighting the efficacy and simplicity of RepoCoder in the retrieval process."
        },
        {
            "heading": "7 Related Work",
            "text": "Repository Context in Code Completion: Incorporating repository-level context into code completion tools has been a long-standing challenge. Traditional code completion techniques typically involve analyzing code to identify potential suggestions, followed by re-ranking them (Raychev et al., 2014; Svyatkovskiy et al., 2019, 2021). While this approach offers efficient performance, it lacks the flexibility to generate code at arbitrary granularity. Another line of research treats code completion as a language modeling task, where the next tokens are generated based on the given context. Several methods have been proposed to incorporate repository context into the training of language models, including n-grams (Tu et al., 2014), LSTMs (Hellendoorn and Devanbu, 2017), and Transformers (Svyatkovskiy et al., 2020; Liu et al., 2022; Ding et al., 2022). However, the process of collecting\nlabeled data and fine-tuning models for different applications remains resource-intensive. In recent years, there has been significant attention on Large Language Models (LLMs) for code completion. A study by Shrivastava et al. (2022) also explores the scenario of repository-level code completion. Despite its innovative approach, the study relies on inflexible heuristics and classifier training for prompt construction. This highlights the ongoing challenges in effectively leveraging LLMs for code completion and the need for further research.\nJoint Modeling Retrieval and Generation: Despite the impressive capabilities of LLMs (Brown et al., 2020; Thoppilan et al., 2022; Chowdhery et al., 2022), their offline training paradigm often limits access to customized and up-to-date information. Recent studies have started exploring the joint modeling of retrieval and generation in knowledgeintensive tasks, such as question answering (Guu et al., 2020; Lewis et al., 2020; Izacard et al., 2022) and dialogue generation (Zhang et al., 2022). This approach has also been extended to code generation by incorporating retrieved documents or code examples into the generation process (Rizwan Parvez et al., 2021; Zhou et al., 2022; Lu et al., 2022; Zan et al., 2022). As language models have become increasingly sophisticated, there is a growing trend towards in-context joint retrieval and generation, treating the LLM as a fixed black box (Levine et al., 2022; Ram et al., 2023; Shi et al., 2023). Moreover, some studies have investigated utilizing the model\u2019s predictions as supplementary context to inform the retrieval process (Mao et al., 2020; Li et al., 2022; Wang et al., 2023; Zemlyanskiy et al., 2022). In this work, we demonstrate that adopting an iterative paradigm that combines code retrieval and generation can serve as an effective method for repository-level code completion."
        },
        {
            "heading": "8 Conclusion and Future Work",
            "text": "In conclusion, we introduce RepoCoder, a straightforward and effective framework for the repositorylevel code completion task. By leveraging a retriever and a language model, RepoCoder effectively utilizes repository-level information. Through an iterative process of retrieval and generation, RepoCoder bridges the gap between retrieval context and the target code, resulting in improved code completion performance. Our extensive experiments conducted on the RepoEval benchmark demonstrate that RepoCoder consistently and\nsignificantly enhances In-File completion performance, surpassing the vanilla Retrieval-Augmented Generation (RAG) approach. Furthermore, our analysis provides valuable insights into the rationale and limitations of RepoCoder. With its simplicity, versatility, and effectiveness, RepoCoder has the potential to become an indispensable tool in real-world software development, and we aim to further enhance its usability and robustness."
        },
        {
            "heading": "Limitations",
            "text": "Limited Effectiveness for Repositories with Low Code Duplication: Despite we have demonstrated the effectiveness of RepoCoder through extensive experiments and analysis, RepoCoder may not bring significant performance improvements when a repository has few instances of code duplication. In such scenarios, the code retrieval process struggles to find sufficient relevant information from the repository to facilitate code completion. This issue is further highlighted in the study presented in Appendix C."
        },
        {
            "heading": "Difficulty in Identifying the Optimal Number",
            "text": "of Iterations: While RepoCoder with two iterations outperforms the RAG method, determining the optimal number of iterations remains a challenge. Subsequent iterations of RepoCoder may exhibit unstable performance compared to previous iterations. Appendix D provides a demonstration of this issue. To mitigate this, we have explored different approaches to automatically terminate the iteration process when necessary. However, finding an optimal stopping criterion without significantly impacting RepoCoder\u2019s performance remains an ongoing challenge. Further research is required to develop techniques that can identify the iteration at which RepoCoder achieves the best performance."
        },
        {
            "heading": "Time Efficiency for Real-Time Deployment:",
            "text": "While RepoCoder demonstrates promising gains in code completion accuracy through iterative retrieval and generation, concerns may arise due to the latency of additional retrieval-generation steps. For real-time deployment scenarios with strict latency requirements, we can further improve RepoCoder through model optimizations such as quantization, distillation, and hardware acceleration to expedite inference. Techniques like caching frequent code snippets and pre-processing repositories can also boost speed. The model iterations can be dynamically adapted based on latency goals\nand contextual needs to balance accuracy and efficiency. Nevertheless, improving time efficiency is another important topic that is out of the scope of our current paper.\nLimited Exploration of Different Experimental Settings: First, while we have validated the effectiveness of RepoCoder, we have not yet explored the potential improvements that can be achieved through different prompt templates. We believe that more careful prompt engineering could enhance the performance of our approach even further. Second, our focus in this study has primarily been on exploring similarity-based retrieval models. The reason for this limited scope is rooted in the complexity of code retrieval, which involves numerous intricate details that are not directly relevant to the RepoCoder framework. Considering alternative retrieval models or expanding the exploration to other code retrieval techniques could provide further insights and comparative evaluations. Third, we have observed significant advancements in code generation models, such as GPT4 (OpenAI, 2023), StarCoder (Li et al., 2023), and WizardCoder (Luo et al., 2023). While our experiments demonstrate the efficacy of RepoCoder across different language models (GPT-3.5-Turbo and CODEGEN), it would be valuable to investigate how our approach performs with these advanced code generation models. Incorporating them into our experimental setup would provide a broader evaluation of RepoCoder across a wider range of language models. Fourth, our experiments primarily use the In-File and Oracle methods as baselines. This decision stems from the fact that repositorylevel code completion using language models is a relatively new task, lacking well-established and reproducible baselines. To provide further insights, we include comparisons to other commercial code completion products. Nonetheless, it is impractical to systematically benchmark against complex, confidential commercial products. We instead conduct a study in Appendix E showcasing the repositorylevel completion ability of RepoCoder and another three major commercial products, where we can illustrate their qualitative differences. In summary, future work should aim to explore different prompt designs, consider alternative retrieval or generation models, and incorporate additional baselines."
        },
        {
            "heading": "A Repository Details",
            "text": "As mentioned in Section 3, we meticulously selected repositories for our RepoEval benchmark based on criteria such as open-source license, creation date, code quantity, and quality. Detailed information about these repositories is provided in Table 7."
        },
        {
            "heading": "B Using the Dense Retriever",
            "text": "In our main experiments (as described in Section 4.2), we utilize a sparse retrieval model for RepoCoder due to its acceptable performance and computational efficiency. However, RepoCoder is a versatile framework that can be applied with other code retrieval models as well. To further validate the effectiveness of RepoCoder, we conduct additional experiments using a dense code retriever.\nSpecifically, we employ UniXcoder (Guo et al., 2022), a state-of-the-art code embedding model, to"
        },
        {
            "heading": "CODEGEN-MONO-6B",
            "text": ""
        },
        {
            "heading": "CODEGEN-MONO-2B",
            "text": ""
        },
        {
            "heading": "CODEGEN-MONO-350M",
            "text": ""
        },
        {
            "heading": "CODEGEN-MONO-6B",
            "text": ""
        },
        {
            "heading": "CODEGEN-MONO-2B",
            "text": ""
        },
        {
            "heading": "CODEGEN-MONO-350M",
            "text": "transform code snippets into hidden vectors. We then calculate the similarity between code snippets using cosine similarity. The experimental results on the line and API invocation completion datasets using the dense retriever are presented in Table 6a and Table 6b. Notably, the performance of RepoCoder using the dense retriever is comparable to that using the sparse retriever. Furthermore, the findings remain consistent across both retrievers, highlighting the robustness and generalizability of RepoCoder."
        },
        {
            "heading": "C Code Duplication in Repositories",
            "text": "We explore the relationship between the performance of RepoCoder and the code duplication ratio\nof the repositories. Intuitively, since RepoCoder utilizes similarity-based retrieval to find code exemplars, one might expect a positive correlation between its performance and the code duplication ratio. To assess this relationship, we calculate the code duplication ratio of the repositories by determining the ratio of duplicated code lines to the total code lines.\nFigure 4 presents the results, demonstrating the correlation between RepoCoder\u2019s performance, as measured by the Exact Match (EM) metric, and the code duplication ratio on the line and API completion datasets using GPT-3.5-Turbo. Notably, the repository \u201cdiffusers\u201d exhibits the highest duplication ratio, which corresponds to a significant performance improvement for RepoCoder on both datasets. Conversely, \u201crl\u201d and \u201cvizier\u201d have low duplication ratios, resulting in comparatively lower performance for RepoCoder. However, the correlation between RepoCoder\u2019s performance and the code duplication ratio is not absolute. For example, \u201cFedScope\u201d and \u201cevaluate\u201d have similar duplication ratios but show different performance gains for RepoCoder."
        },
        {
            "heading": "D Failed Cases between Iterations",
            "text": "In Section 5, the evaluation results demonstrate that increasing the number of RepoCoder iterations does not necessarily guarantee performance improvements. To further investigate this issue, we analyze the changes in the number of correct code completions achieved by different methods on the API invocation completion dataset. The prediction is considered correct when the EM score is 1. Table 8 presents the results, showing the counts\nof correct code completions for each iteration of RepoCoder. We observe that each iteration of RepoCoder both passes cases that the previous iteration failed and fails cases that the previous iteration has passed.\nUpon manually examining the failed cases, we have the following observations: Firstly, a majority of failures are caused by misleading retrieved code, which leads to incorrect predictions. For instance, the same API may have different sets of parameters across different files, and the retrieved API usage example can be misleading in such cases. Secondly, the model\u2019s predictions are not always suitable for retrieval. This is because the query is constructed using a fixed length of the predicted code, which may include noisy code beyond the initial lines of\nhelpful code completion. Furthermore, our investigation reveals that many cases in the line and API datasets are actually correct despite being evaluated as incorrect by the EM score. This highlights the importance of considering the actual functionality of the code, rather than solely relying on exact matching, and suggests incorporating unit tests to assess code correctness."
        },
        {
            "heading": "E Case Study of Commercial Products",
            "text": "We conduct a study to showcase the repositorylevel code completion ability of RepoCoder and another three major commercial code completion tools: Github Copilot 4, Tabnine 5, and Amazon CodeWhisperer 6. The experiment was conducted using the Visual Studio Code IDE, with each product providing completions as a plugin. These products are based on large language models pre-trained on code data and can perform line-level and blocklevel completion, similar to our study scenario. We selected a simple API invocation example from the RepoEval dataset. The task was to complete the function body for initializing a StableDiffusionKDiffusionPipeline, where the prefix in-file context provided little information. As shown in Figure 5, none of the commercial products generated the correct completion. The implementation details of these commercial products are confidential, it is\n4https://github.com/features/copilot 5https://www.tabnine.com 6https://aws.amazon.com/codewhisperer\ndifficult to perform systematic comparison. However, in our case, RepoCoder successfully predicted the correct completion by retrieving a relevant code snippet from the repository context. This demonstrates the need for state-of-the-art tools to effectively leverage repository-level context."
        }
    ],
    "title": "RepoCoder: Repository-Level Code Completion Through Iterative Retrieval and Generation",
    "year": 2023
}