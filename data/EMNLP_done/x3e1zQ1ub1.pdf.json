{
    "abstractText": "Large language models (LLMs) can use incontext demonstrations to improve performance on zero-shot tasks. However, selecting the best in-context examples is challenging because model performance can vary widely depending on the selected examples. We present a cross-entropy difference (CED) method for selecting in-context demonstrations. Our method is based on the observation that the effectiveness of in-context demonstrations negatively correlates with the perplexity of the test example by a language model that was finetuned on that demonstration. We utilize parameter efficient finetuning to train small models on training data that are used for computing the cross-entropy difference between a test example and every candidate in-context demonstration. This metric is used to rank and select incontext demonstrations independently for each test input. We evaluate our method on a mixdomain dataset that combines 8 benchmarks, representing 4 text generation tasks, showing that CED for in-context demonstration selection can improve performance for a variety of LLMs over baseline selection methods.1",
    "authors": [
        {
            "affiliations": [],
            "name": "Dan Iter"
        },
        {
            "affiliations": [],
            "name": "Reid Pryzant"
        },
        {
            "affiliations": [],
            "name": "Ruochen Xu"
        },
        {
            "affiliations": [],
            "name": "Shuohang Wang"
        },
        {
            "affiliations": [],
            "name": "Yang Liu"
        },
        {
            "affiliations": [],
            "name": "Yichong Xu"
        },
        {
            "affiliations": [],
            "name": "Chenguang Zhu"
        }
    ],
    "id": "SP:656053cec7d3bf770180ca8bd1558d9df5894285",
    "references": [
        {
            "authors": [
                "Monica Agrawal",
                "Stefan Hegselmann",
                "Hunter Lang",
                "Yoon Kim",
                "David Sontag."
            ],
            "title": "Large language models are few-shot clinical information extractors",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages",
            "year": 2022
        },
        {
            "authors": [
                "Amittai Axelrod",
                "Xiaodong He",
                "Jianfeng Gao."
            ],
            "title": "Domain adaptation via pseudo in-domain data selection",
            "venue": "Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2011
        },
        {
            "authors": [
                "Zhou",
                "Percy Liang"
            ],
            "title": "On the opportunities and risks of foundation models",
            "year": 2022
        },
        {
            "authors": [
                "Christopher Clark",
                "Kenton Lee",
                "Ming-Wei Chang",
                "Tom Kwiatkowski",
                "Michael Collins",
                "Kristina Toutanova."
            ],
            "title": "BoolQ: Exploring the surprising difficulty of natural yes/no questions",
            "venue": "Proceedings of the 2019 Conference of the North American Chap-",
            "year": 2019
        },
        {
            "authors": [
                "Damai Dai",
                "Yutao Sun",
                "Li Dong",
                "Yaru Hao",
                "Zhifang Sui",
                "Furu Wei"
            ],
            "title": "Why can gpt learn in-context? language models secretly perform gradient descent as meta-optimizers",
            "year": 2022
        },
        {
            "authors": [
                "Hila Gonen",
                "Srini Iyer",
                "Terra Blevins",
                "Noah A Smith",
                "Luke Zettlemoyer"
            ],
            "title": "Demystifying prompts",
            "year": 2022
        },
        {
            "authors": [
                "David Grangier."
            ],
            "title": "Distribution matched contrastive data selection",
            "venue": "Google internal report.",
            "year": 2019
        },
        {
            "authors": [
                "David Grangier",
                "Dan Iter."
            ],
            "title": "The trade-offs of domain adaptation for neural language models",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3802\u20133813, Dublin, Ireland.",
            "year": 2022
        },
        {
            "authors": [
                "Edward J Hu",
                "yelong shen",
                "Phillip Wallis",
                "Zeyuan AllenZhu",
                "Yuanzhi Li",
                "Shean Wang",
                "Lu Wang",
                "Weizhu Chen"
            ],
            "title": "LoRA: Low-rank adaptation of large language models",
            "venue": "In International Conference on Learning Representations",
            "year": 2022
        },
        {
            "authors": [
                "Dan Iter",
                "David Grangier"
            ],
            "title": "On the complementarity of data selection and fine tuning for domain adaptation",
            "year": 2021
        },
        {
            "authors": [
                "Daniel Khashabi",
                "Tushar Khot",
                "Ashish Sabharwal."
            ],
            "title": "More bang for your buck: Natural perturbation for robust question answering",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 163\u2013170,",
            "year": 2020
        },
        {
            "authors": [
                "Daniel Khashabi",
                "Sewon Min",
                "Tushar Khot",
                "Ashish Sabharwal",
                "Oyvind Tafjord",
                "Peter Clark",
                "Hannaneh Hajishirzi."
            ],
            "title": "UNIFIEDQA: Crossing format boundaries with a single QA system",
            "venue": "Findings of the Association for Computational Linguistics:",
            "year": 2020
        },
        {
            "authors": [
                "Tom\u00e1\u0161 Ko\u010disk\u00fd",
                "Jonathan Schwarz",
                "Phil Blunsom",
                "Chris Dyer",
                "Karl Moritz Hermann",
                "G\u00e1bor Melis",
                "Edward Grefenstette."
            ],
            "title": "The NarrativeQA reading comprehension challenge",
            "venue": "Transactions of the Association for Computational Linguistics, 6:317\u2013328.",
            "year": 2018
        },
        {
            "authors": [
                "Uszkoreit",
                "Quoc Le",
                "Slav Petrov."
            ],
            "title": "Natural questions: A benchmark for question answering research",
            "venue": "Transactions of the Association for Computational Linguistics, 7:452\u2013466.",
            "year": 2019
        },
        {
            "authors": [
                "Guokun Lai",
                "Qizhe Xie",
                "Hanxiao Liu",
                "Yiming Yang",
                "Eduard Hovy."
            ],
            "title": "RACE: Large-scale ReAding comprehension dataset from examinations",
            "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 785\u2013",
            "year": 2017
        },
        {
            "authors": [
                "Brian Lester",
                "Rami Al-Rfou",
                "Noah Constant."
            ],
            "title": "The power of scale for parameter-efficient prompt tuning",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2021
        },
        {
            "authors": [
                "Xiang Lisa Li",
                "Percy Liang."
            ],
            "title": "Prefix-tuning: Optimizing continuous prompts for generation",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language",
            "year": 2021
        },
        {
            "authors": [
                "Haokun Liu",
                "Derek Tam",
                "Muqeeth Mohammed",
                "Jay Mohta",
                "Tenghao Huang",
                "Mohit Bansal",
                "Colin Raffel."
            ],
            "title": "Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning",
            "venue": "Advances in Neural Information Processing Systems.",
            "year": 2022
        },
        {
            "authors": [
                "Jiachang Liu",
                "Dinghan Shen",
                "Yizhe Zhang",
                "Bill Dolan",
                "Lawrence Carin",
                "Weizhu Chen"
            ],
            "title": "What makes good in-context examples for GPT-3",
            "venue": "In Proceedings of Deep Learning Inside Out (DeeLIO",
            "year": 2022
        },
        {
            "authors": [
                "Yao Lu",
                "Max Bartolo",
                "Alastair Moore",
                "Sebastian Riedel",
                "Pontus Stenetorp."
            ],
            "title": "Fantastically ordered prompts and where to find them: Overcoming fewshot prompt order sensitivity",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Compu-",
            "year": 2022
        },
        {
            "authors": [
                "Todor Mihaylov",
                "Peter Clark",
                "Tushar Khot",
                "Ashish Sabharwal."
            ],
            "title": "Can a suit of armor conduct electricity? a new dataset for open book question answering",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2018
        },
        {
            "authors": [
                "Sewon Min",
                "Xinxi Lyu",
                "Ari Holtzman",
                "Mikel Artetxe",
                "Mike Lewis",
                "Hannaneh Hajishirzi",
                "Luke Zettlemoyer"
            ],
            "title": "Rethinking the role of demonstrations: What makes in-context learning work",
            "venue": "In Proceedings of the 2022 Conference on Empirical Methods",
            "year": 2022
        },
        {
            "authors": [
                "Robert C. Moore",
                "William Lewis."
            ],
            "title": "Intelligent selection of language model training data",
            "venue": "Proceedings of the ACL 2010 Conference Short Papers,",
            "year": 2010
        },
        {
            "authors": [
                "Paul Christiano",
                "Jan Leike",
                "Ryan Lowe"
            ],
            "title": "Training language models to follow instructions with human feedback",
            "year": 2022
        },
        {
            "authors": [
                "Pranav Rajpurkar",
                "Robin Jia",
                "Percy Liang."
            ],
            "title": "Know what you don\u2019t know: Unanswerable questions for SQuAD",
            "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 784\u2013789,",
            "year": 2018
        },
        {
            "authors": [
                "Ohad Rubin",
                "Jonathan Herzig",
                "Jonathan Berant."
            ],
            "title": "Learning to retrieve prompts for in-context learning",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech-",
            "year": 2022
        },
        {
            "authors": [
                "Erich Schubert."
            ],
            "title": "Automatic indexing for similarity search in ELKI",
            "venue": "Similarity Search and Applications - 15th International Conference, SISAP 2022, Bologna, Italy, October 5-7, 2022, Proceedings, volume 13590 of Lecture Notes in Computer Science,",
            "year": 2022
        },
        {
            "authors": [
                "Simeng Sun",
                "Yang Liu",
                "Dan Iter",
                "Chenguang Zhu",
                "Mohit Iyyer"
            ],
            "title": "How does in-context learning help prompt tuning",
            "year": 2023
        },
        {
            "authors": [
                "Wei Wang",
                "Ye Tian",
                "Jiquan Ngiam",
                "Yinfei Yang",
                "Isaac Caswell",
                "Zarana Parekh."
            ],
            "title": "Learning a multidomain curriculum for neural machine translation",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7711\u2013",
            "year": 2020
        },
        {
            "authors": [
                "Wei Wang",
                "Taro Watanabe",
                "Macduff Hughes",
                "Tetsuji Nakagawa",
                "Ciprian Chelba."
            ],
            "title": "Denoising neural machine translation training with trusted data and online data selection",
            "venue": "Proceedings of the Third Conference on Machine Translation: Research Pa-",
            "year": 2018
        },
        {
            "authors": [
                "Zhenyu Wu",
                "Yaoxiang Wang",
                "Jiacheng Ye",
                "Jiangtao Feng",
                "Jingjing Xu",
                "Yu Qiao",
                "Zhiyong Wu."
            ],
            "title": "Openicl: An open-source framework for in-context learning",
            "venue": "arXiv preprint arXiv:2303.02913.",
            "year": 2023
        },
        {
            "authors": [
                "Zhuosheng Zhang",
                "Junjie Yang",
                "Hai Zhao"
            ],
            "title": "Retrospective reader for machine reading comprehension",
            "year": 2020
        },
        {
            "authors": [
                "Tony Zhao",
                "Eric Wallace",
                "Shi Feng",
                "Dan Klein",
                "Sameer Singh."
            ],
            "title": "Calibrate before use: Improving few-shot performance of language models",
            "venue": "International Conference on Machine Learning.",
            "year": 2021
        },
        {
            "authors": [
                "Zihao Zhao",
                "Eric Wallace",
                "Shi Feng",
                "Dan Klein",
                "Sameer Singh."
            ],
            "title": "Calibrate before use: Improving few-shot performance of language models",
            "venue": "International Conference on Machine Learning, pages 12697\u201312706. PMLR.",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Large language models (LLMs) have been widely successful across many NLP tasks (Bommasani et al., 2022; OpenAI, 2023; Bubeck et al., 2023). The primary method for LLMs to adapt to new tasks has been using in-context learning, where a few examples and labels are provided as input to the model (Agrawal et al., 2022). This simple approach has shown large improvements over zero shot settings, and even outperformed finetuning methods when the training dataset is small. However, the model\u2019s performance can be greatly influenced by which in-context demonstrations (ICDs) are selected into the prompt (Lu et al., 2022; Zhao et al., 2021a; Min et al., 2022).\n1Code is available at https://github.com/microsoft/LMOps\nSelecting the best in-context demonstrations can be challenging. The variance in performance of similar demonstrations can be large, and the selected examples can introduce unfavorable prior biases on the output label space (Zhao et al., 2021b). The naive approach is to randomly sample demonstrations from the same source dataset. Previous methods for selecting ICDs include simple methods such as selecting nearest neighbors by embedding distance (Liu et al., 2022b) and retrieval-based methods that require training a retriever model (Rubin et al., 2022). This work presents a new method of selecting demonstrations that can be applied to any sized training data, requires training small PEFT models only and outperforms the nearest neighbor baseline on GPT-Davinci-003 (Ouyang et al., 2022).\nWe propose a cross entropy difference (CED) method for ICD selection. CED has been used to select in-domain data from large mixed-domain datasets for domain adaptation (Axelrod et al., 2011; Moore and Lewis, 2010; Wang et al., 2018). We borrow this idea to conduct ICD selection.\nSpecifically, we utilize parameter efficient finetuning to train small models on training data that are used for computing the CED between a test example and every candidate in-context demonstration. The CED scores are used to rank and select in-context demonstrations. We present a theoretical explanation for the effectiveness of CED. CED approximates the gradient alignment between training and test examples. Our analysis builds on previous findings that demonstrations operate as \u201cmeta-gradients\u201d and shows that demonstrations with gradients similar to those of test inputs lead to improved performance in downstream tasks (Dai et al., 2022).\nWe evaluate our proposed CED-ICD selection method on a mixed-domain dataset composed of 8 datasets on 4 tasks: binary classification, multiple choice and extractive question answering and ab-\nstractive question answering. We show that downstream model performance using CED-ICD outperforms nearest neighbor baselines and transfers across models allowing training small models for selection but evaluating test examples on much larger models including GPT-3.5.\nThe contributions of this work are the following: 1) We present a method for selecting in-context demonstrations based on cross entropy difference. 2) We provide theoretical guidance for why selecting demonstrations based on their gradient alignment with test example is an effective heuristic. 3) We evaluate our method on a mixed-domain benchmark composed of 8 datasets from 4 tasks. 4) We evaluate our method on different sizes of GPT3 models, showing that this method transfers to larger models leading to performance improvements over baseline selection methods."
        },
        {
            "heading": "2 Related Work",
            "text": "Our work combines ideas from three bodies of research: In-context learning (ICL), data selection for domain adaptation and parameter efficient finetuning (PEFT).\nWhile in-context learning (Agrawal et al., 2022) has shown very strong results for few-shot settings, recent work has shown that LLMs are very sensitive to the selected examples leading to large variance in performance (Zhao et al., 2021b), sensitivity to the order of examples (Lu et al., 2022) and even lack of sensitivity to the actual labels (Min et al., 2022). Other work has attempted to mitigate these challenges by selecting in-context demonstrations by the nearest neighbor examples in embedding space (Liu et al., 2022b), or training a retrieval mechanism (Rubin et al., 2022). We build on this line of work by proposing a novel selection method that combines the observations from Min et al. (2022) that domain similarity is a key characteristic of good in-context demonstrations, and observations from Gonen et al. (2022) that using perplexity could be a good heuristic for prompt selection.\nPrevious work on domain adaptation has focused on finding in-domain examples from a large outof-domain dataset to train a model that achieves a better generalization on a target distribution (Moore and Lewis, 2010; Axelrod et al., 2011; Grangier and Iter, 2022). Data selection is intended to maximize the distributional similarity between a training dataset and test dataset. However, cross entropy dif-\nference has not been used previously at the example granularity to rank the \u201cin-domainness\u201d of training data in reference to just a single target example. We propose a natural extension of this framework for selecting demonstrations that are \u201cin-domain\u201d for a test input, which we demonstrate is an effective metric for selecting demonstrations for in-context learning.\nParameter efficient finetuning (PEFT) proposes a class of methods for augmenting model parameters with a small number of additional parameters that can be trained and stored efficiently (Lester et al., 2021; Li and Liang, 2021; Liu et al., 2022a; Hu et al., 2022). However, PEFT is usually used independently from in-context learning. Liu et al. (2022a) report that combining in-context learning and PEFT has not been effective. Sun et al. (2023) does report some settings where PEFT and ICL can be combined, but only under specific task conditions. We report similar findings, that in-context demonstrations do not improve PEFT models when selected randomly, however, we do see improvements in PEFT performance when combined with CED for selecting in-context demonstrations during both training and inference time. We also utilize the ability of a PEFT model, T-Few (Liu et al., 2022a), to train on very few examples to be able to effectively compute CED scores without overfitting to the target domain."
        },
        {
            "heading": "3 Methodology",
            "text": "We propose a method for selecting in-context demonstrations (ICDs) by finding the training data that would minimize the perplexity of a test example, if a language model were finetuned on that training example. This approach stems from previous findings that in-context examples may act as a type of meta-gradient on the frozen LLM (Dai et al., 2022) and the assumption that models perform better on in-domain test data. As we show in the following sections, our method of using cross entropy difference finds the demonstrations that appear most likely to be from the same domain as the test example."
        },
        {
            "heading": "3.1 Cross-Entropy Difference for In-Context Demonstration Selection",
            "text": "Generally, large language models are trained to minimize the negative log likelihood of a token given some context C by training on samples from a dataset D. The parameters of the model are rep-\nresented by \u03b8.\nL(\u03b8;D, C) = \u2212 1 |D| \u2211 y\u2208D logP (y|\u03b8, C) (1)\nIn-context learning is the setting where the model weights \u03b8 are frozen and the only way to minimize the loss is to select an optimal context. In this work we also constrain the context to be examples from the training data. Note that the more general case is often referred to as prompt learning or prompt engineering where the context can include any natural language including instructions or descriptions in addition to examples.\nWe define a class of selection methods W, were each function in the set outputs a subset of the training dataset and the size of the subset is at most k, where k is the number of shots to include in the context. The selection may condition on the input so it is not restricted to selecting a single in-context demonstration for all test examples. The optimal selection method W\u2217 is defined as the selection method that minimizes the loss on the test domain.\nW\u2217 = arg min W\u2208W \u2212 1 |D| \u2211 (x,y)\u2208D logP (y|\u03b8,W(x))\n(2) Given a training set Dtrain = {(x1, y1), (x2, y2), ..., (xn, yn)} with n examples, where xi is the i-th input and yi is the corresponding label, the goal of few-shot learning is to learn a function f : X \u2192 Y that can accurately predict the label y for a new input x, given only a small number of training examples. For simplicity of analysis, we focus on the case were only 1 demonstration is selected. This is especially useful for scenarios where each example is long, such as background documents. We leave multiple-demonstration selection to future investigation.\nCross-entropy difference (CED) is the difference between the cross-entropy losses from two models; a generic or base domain model (e.g. a pretrained language model) and a target domain model (e.g. a language model finetuned on a target distribution).\nCED = logP (y|x; \u03b8targeti)\u2212 logP (y|x; \u03b8base) (3) CED, in various forms, has been used extensively for data selection for domain adaptation (Axelrod et al., 2011; Moore and Lewis, 2010; Iter and\nGrangier, 2021; Mindermann et al., 2022). Since we are selecting the argmax domain model per test example, as seen in Equation 2, in practice we can compare the cross-entropy of the target domain model directly as the base model loss is fixed per test example. Note that this differs from the standard data selection setting where there is one target domain and many data points which are scored.\nFor each xi in Dtrain, a separate target model is trained on the language modeling objective, producing n models, M1, ...,Mn. Given a test example xT , we apply each Mi to compute xT \u2019s perplexity L(Mi(xT )). We then select the training sample associated with the language model giving the lowest perplexity as the in-context demonstration for xT :\nICD = xargmini L(Mi(xT )) (4)\nUnlike the domain adaptation setting, rather than scoring all the training data using a single in-domain model, each training example is treated as its own domain. Each test example can be scored for \"in-domain-ness\" across all training examples.\nTo train each model on a single example, we use the (IA)3 PEFT method with a T-Few 3B parameter model (Liu et al., 2022a). The effectiveness of PEFT to train a model on a small dataset without catastrophic forgetting allows us to train a model on a single example. The model is trained for multiple epochs and a small development set is used to test for overfitting and early stopping. Also, since a small fraction of parameters are updated, storing each model only requires 2MB on disk. Liu et al. (2022a) also showed that training a T-Few model was equivalent in FLOPs to approximately 20 forward passes of a 175B parameter GPT model. Since our finetuned models only require an average of 20 steps to train, the training cost in FLOPs per model is less than one forward pass of a 175B parameter GPT model."
        },
        {
            "heading": "3.2 Clustering for Large Training Sets",
            "text": "Training many small models on each training example and storing the respective weights may become prohibitively expensive, even when using small models trained with PEFT. We present a simple method to apply cross-entropy difference for incontext demonstration selection on larger training sets by clustering the training data. The approach from Section 3.1 can be scaled to larger training datasets by training each model Mi on a set of examples that can be obtained by clustering the\ntraining data. Our experiments found that trivial clusters based on document embeddings did not perform well. Instead, we present a method for clustering which initializes k finetuned models on k random examples and uses cross-entropy difference to cluster the training data.\nWe sample a number of examples that is smaller than the size of the training datasets. For each example, a small model is trained with PEFT as in Section 3.1. All examples in the training dataset are scored with each model, producing a score for each example-model pair. The lower the loss on a model, the \u201ccloser\u201d that point is to the model cluster. We use these scores to create equal sized clusters using only the initialization step for same-size kmeans variation described by Schubert (2022). Mi models are retrained on each cluster, producing a new set of k models. For selecting in-context demonstrations, we can either randomly sample from the cluster or use the original seed of the cluster (i.e. the centroid by CED)."
        },
        {
            "heading": "3.3 ICDs as Meta-Gradients",
            "text": "Dai et al. (2022) describe in-context demonstrations as \u201cimplicit finetuning\u201d, where a component of the attention mechanism can be interpreted as a \u201cmeta-gradient\u201d. This formulation suggests that training directly on the in-context demonstration would have a similar effect to in-context learning with an LLM. Under this interpretation, the best selection strategy would be to choose examples that, if the model were to be trained on these examples, would result in the lowest loss on the test example. This strategy of decreasing the loss of a test example to measure domain similarity has been shown to correlate with performance on the downstream tasks in a domain adaptation setting (Grangier and Iter, 2022; Axelrod et al., 2011; Moore and Lewis, 2010). We apply the principle to the problem of in-context demonstration.\nDai et al. (2022) define an approximation to the standard attention head Attn(V,K, q) as linear attention that removes the softmax and scaling factor. V , K, and q are the value, key and query respectively and correspond to attention weight matrices, WV and WK . We omit the full derivation from Dai et al. (2022) but include the expanded form of the linear attention in line 2 of Equation 5. q is the attention query vector, q = WQx, and the input is [X;X \u2032] where X \u2032 is the in-context demonstration that is concatenated to the input X . Dai et al.\n(2022) rewrites the linear attention head weight matrix as a reparameterization of the zero shot attention head WZSL where the delta applied to the weights depends on the original wieghts and the in-context demonstration.\nAttn(V,K, q)\n\u2248 WV X(WKX)T q +WV X \u2032(WKX \u2032)T q = (WZSL +\u2206WICL)q (5)\n\u2206WICL can be seen as an update that is applied to the zero shot weights of the attention mechanism WZSL. Here we see that including in-context demonstrations is akin to finetuning the LLM on the selected demonstrations.\nIf the loss of the large language model can only be modified by selecting in-context examples and these examples act as a meta-gradient on the language model, the optimal selection would be the training example with a gradient most similar to test example. Computing similarities between gradients would be computationally expensive given the size of large language models and gradients of a test example can not be computed because the model can not access the labels of test instances. Cross entropy difference (Axelrod et al., 2011), used in data selection for domain adaptation, has been show to be effective at selecting in-domain examples using the perplexity of the input features without the label.\nGrangier (2019); Wang et al. (2020) describe cross entropy difference as an approximation of the dot product between the gradient of the target domain and the gradient of a single training example.\nlogP (y|x; \u03b8target)\u2212 logP (y|x; \u03b8base) \u2248 \u03bbg(x, y; \u03b8base)T g(Dtarget, \u03b8base) 2 (6)\nHere the cross entropy difference is approximating the gradient alignment between a single training example and a target domain. CED is simply defined as the difference between the log probabilities of a text span y evaluated on two models, a base model and a target specific model. The base model represents the background distribution for which we can use any pretrained language model. The target model represents the distribution of a\n2This holds when the base model and finetuned model are close, which is the case in finetuning. For ICDs and finetuning on a single example, this is even more limited than general finetuning.\ntarget domain. Unlike cross entropy difference, incontext learning is input specific rather than dataset specific. To adapt the CED method to in-context demonstration selection, we need a model that is finetuned on a single example. In Section 3.1 we describe how we are able to finetune such a model with parameter efficient finetuning (PEFT) without overfitting to the single example and limiting the space requirements to store independent parameters per training example.\nEquations 2 and 5 say that we want to find the examples that would minimize the loss if used as finetuning data. Equation 6 states that examples that have a gradient most similar to the actual test data can be approximated by finding the examples that most increase the likelihood of a test example. This provides the motivation for using CED to select ICDs. In the next section we describe in depth how to train models for each single-trainingexample domain and score the training data for selecting in-context demonstrations."
        },
        {
            "heading": "4 Experiments",
            "text": "We evaluate CED-ICD selection on both small models and the transfer of the selection method to larger models, including GPT-Davinci-003. We evaluate the selection method in a mixed-domain setting where random demonstrations are not trivially indomain. We do not provide task or dataset labels as input to the selection model. We show in Section 5.2, both CED and the nearest neighbors baseline do not exclusively select in-domain demonstrations in the mixed domain setting. In fact, we show in Section 5.2 that out-of-domain examples may also be strong in-context demonstrations. In practical settings, a single LLM may be used for multiple tasks and there may not be labels for the task type, especially with the more common chat interfaces. We find this mixed-domain setting to better reflect these realistic challenges."
        },
        {
            "heading": "4.1 Datasets and Models",
            "text": "To evaluate the ICD-CED selection method, we measure the performance of several data selection methods on 8 datasets from 4 tasks; binary classification(BoolQ, Clark et al. (2019), NPBoolQ, Khashabi et al. (2020a)), extractive question answering (Squad2, Rajpurkar et al. (2018), NewsQA (Zhang et al., 2020)), abstractive question answering (NarrativeQA, Koc\u030cisk\u00fd et al. (2018), NaturalQA, Kwiatkowski et al. (2019)) and multiple\nchoice (RACE, Lai et al. (2017), OpenBookQA Mihaylov et al. (2018)). All tasks are cast as a text generation task, in accordance to the evaluation used in UnifiedQA (Khashabi et al., 2020b). Binary classification and multiple choice are measured by accuracy, extractive QA is measured by F1 score based on the generated tokens and abstractive QA is measured by RougeL.\nWe combine these 8 datasets to create a larger mixed-domain dataset. We sample 32 examples from each dataset to create a medium-sized fewshot dataset with total of 256 training examples. We evaluate each dataset independently but with in-context demonstrations that can be selected from any dataset. We select one in-context demonstration as many examples have long \u201cbackground\u201d documents, where the input exceeds input length limits and need to be truncated.\nWe evaluate 2 settings, (1) small model performance combining PEFT with in-context learning and (2) in-context learning on LLMs. Our smaller model is T-Few 3B model (Liu et al., 2022a). Previous results don\u2019t report ICL performance because the authors did not find improvements from including ICL examples, however, as we show in our empirical results, T-Few can benefit from in-context learning if high quality demonstrations are selected. Further improvements are realized by finetuning T-Few with selected ICDs instead of random. For LLMs, we evaluate 3 sizes of GPT-3 (Babbage, Curie and Davinci (davinci-003)) (Ouyang et al., 2022).\nWe evaluate the following model settings, with the name corresponding to the rows in Table 1. Small Model Experiments T-Few PEFT is the standard parameter efficient finetuning setting where the model is finetuned on all the training data and inference does not include in-context demonstrations. T-Few PEFT + Random ICL includes randomly selected in-context demonstrations at inference. T-Few PEFT + NN uses OpenICL (Wu et al., 2023) to retrieve the most similar examples as measured by nearest neighbors in euclidean distance between embedding vectors. T-Few PEFT + CED is our proposed model which selects in-context demonstrations using CED scores. T-Few PEFT + CED (training) is our proposed model but includes using in-context selection during both training and inference.\nT-Few PEFT + Loss Oracle evaluates all available ICDs for each test example and reports the highest possible score when selected based on cross-entropy loss. T-Few PEFT + Oracle ICL is similar to the above but the best ICD is selected based on the respective evaluation metric for each task as a true upper bound. Large Language Model Experiments [GPT Model] Rand randomly selects in-context examples similar to T-Few PEFT + ICL. [GPT Model] NN uses OpenICL (Wu et al., 2023) to retrieve the most similar example from the test set as the in-context example. [GPT Model] CED is our proposed model which selects in-context demonstrations using CED scores.\nIn-context demonstrations that do not fit entirely into the T-Few context window are truncated in the \u201cbackground\u201d section of the input exclusively, to keep the question, answer choices and answer intact. A simple prompt is used for GPT requests that labels the sections as \u201cbackground\u201d, \u201cquestion\u201d, \u201canswer\u201d and \u201cexample\u201d. We found that performance dramatically improved for binary classification by including an instruction to answer with a yes or no answer.\nWe also report the performance of CED-ICD with a larger set of candidate in-context demonstrations in Table 2, \"+ Clusters\" row. We sample 256 examples per task (total 2,048) and cluster them as described in Section 3.2. The total number of models used to select in-context demonstrations is 256, the same as the other rows in the table. The ICD for each cluster is selected as the cluster centroid (i.e. the seed example)."
        },
        {
            "heading": "4.2 Results",
            "text": "Our results show that selecting in-context demonstrations using cross-entropy difference (CED) both outperforms baselines on a small trainable model and transfers to larger models, even improving results on GPT3-Davinci003. Table 1 reports the results of different selection methods on the TFew 3 billion parameter model. Parameter efficient finetuning (PEFT) is a strong baseline that finetunes T-Few on the full set of training data, which is a total of 256 training examples. PEFT acheives the best results on T-Few for both BoolQ and NaturalQA datasets. Liu et al. (2022a) report that \u201c[i]n preliminary experiments, we found that T0 was not able to perform few-shot ICL \u2013 performance actually decreased as we increased the number of in-context examples\u201d, which seems to be the case using random in-context demonstrations. However,\nwhen incorporating stronger ICD selection methods, we show that performance does improve on NQ-BoolQ, NarrativeQA, Squad2, NewsQA and RACE. We found that T-Few does not perform well with in-context demonstrations if they are not included in the finetuning phase. When finetuning with in-context demonstrations, we evaluated both random ICD selection and CED selection. We found that on some datasets, we get further improvements by using CED selection during training as well as inference, which we expected as the training data will have more examples where the in-context demonstration is helpful for label prediction.\nWe report oracle in-context demonstration selection, the performance of an ideal example selector given the training data. In this setting, we evaluate every training example as an in-context demonstration and report the metric as the average of the best scores per example. Evaluating generated text requires a verbalizer to map the text to the labels for some tasks. Due to this mapping and metrics that do not directly correlate with loss, such as Rouge and token matching F1, we report both oracle scores based on selecting in-context demonstrations by the lowest loss and by the highest task metric performance. The latter is the true oracle performance but the former suggests that there may be some limitations to the extent that a cross-entropy based model may approximate the downstream performance on a task.\nOracle results show that there is still a large gap between our method and the oracle, showing that there may be many opportunities to improve smaller model performance with better selection methods. However, Figure 1 shows that very few examples yield substantial improvements over the average in-domain performance, meaning that while in-context demonstrations may come from a small set, statistical outliers may compound to make a significant improvement but a predictive method for determining which examples are outliers may not exist. Similar figures for all datasets are in the appendix.\nUltimately, in-context demonstrations are best used on large language models that can not be finetuned. Although our proposed selection method is based on the perplexities measured by smaller and finetuned models, we show that our selection method transfers to large models, including GPTDavinci-003. These results are reported in Table 2.\nOur proposed method for selection outperforms the baseline of using nearest neighbor retrieval on macro average across 8 datasets and on each of the 3 GPT model sizes evaluated. Clustering a larger set of candidate in-context demonstrations has similar performance overall. We use the same number of CED models (256) but train them on clusters of 2,048 candidate examples, showing we can maintain similar performance with more training data without substantially increasing compute. To estimate variance in this method, we sampled 256 training/in-context examples 5 times. We execute the end-to-end pipeline of training target models, selecting in-context demonstrations and running evaluation. From the evaluation sets, we bootstrap 50,000 sets to estimate the standard deviation. We find standard deviation on the text-davinci003 model to be 0.011 and on text-curie-001 to be 0.007."
        },
        {
            "heading": "5 Analysis",
            "text": "We analyze the quality of cross entropy difference selection by computing the rank of selected demonstrations compared to an oracle. We also explore the presence of strong in-context demonstrations selected from out-of-domain data, compared to indomain data."
        },
        {
            "heading": "5.1 Ranking Selected ICDs",
            "text": "Oracle experiments provide a full ranking of all training data as in-context demonstrations. Table 4 shows the average rank of the top 1 selected in-context demonstration per dataset and average, comparing between CED selection and nearest neighbor selection. CED is better at selecting incontext demonstrations as a measured by the oracle ranking, 0 is the highest rank and 255 is lowest. The average rank of a CED selected demonstration is 16 out of 256.\nTable 5 shows one example of in-context demonstrations selected by nearest neighbor and crossentropy difference. While the nearest neighbor demonstration has significant lexical overlap with the test example, the demonstration is taken from a\ndifferent task. Conversely, the cross-entropy difference selected-demonstration is sampled from the same task and displays more similarity to the test example in the type of question."
        },
        {
            "heading": "5.2 In-Domain vs Out-of-Domain ICDs",
            "text": "Table 3 reports the percentage of selected incontext demonstrations that are from the same domain or same task as the inference dataset. The task refers to boolean classification, multiple choice, abstractive question answering and extractive question answering. Although the nearest neighbors approach attempts to directly find the most similar examples, cross entropy difference tends to select more in-domain demonstrations. In-domain demonstrations are a subset of in-task demonstrations so in-task selection percentage is always larger than in-task but CED has a larger proportion of in-domain selections indicating that CED is better at distinguishing the domain format even when there are other datasets that have a similar format or task structure.\nTable 3 reports the percentage of oracle best incontext demonstrations that appear in each subset of in-domain, in-task and out-of-domain demonstrations. Different in-context demonstrations can result in the same output and different outputs may score the same on the final metric so the oracle best in-context demonstration may appear both in-domain and out-of-domain. Interestingly, this table shows that an oracle selection method that only has access to out-of-domain demonstrations can still achieve the best performance from this model on 98% of examples, showing that out-of-\ndomain selection of in-context demonstrations can be highly effective. This suggests that for datasets that have very few or no demonstrations, out-ofdomain demonstrations may still improve model performance."
        },
        {
            "heading": "6 Conclusion",
            "text": "This work shows that cross entropy difference can be used as a heuristic for selecting in-context demonstrations for in-context learning. We motivate our approach by linking previous observations that in-context demonstrations operate as metagradients on a frozen large language model to the data selection literature which has shown that CED is a method that selects examples that have a gradient most similar to the in-domain examples. We empirically show that we can use smaller models to compute CED scores and that this selection method effectively transfers to large models, such as the 175 billion parameter GPT3, and is able to improve performance over baseline selection methods."
        },
        {
            "heading": "7 Limitations",
            "text": "The main limitation of this work is the added complexity of training multiple small models and the tradeoff between this extra computational and space cost and the improvement over baselines that may require less computation. While PEFT training tends to be stable, it does require searching for an appropriate learning rate, although in this work we found 3e-2 to be appropriate for all datasets. As we report, clustering and using fewer models does degrade performance so a valuable direction for future work would be to better scale to larger training datasets. Also, as mentioned earlier, we\nfocus on the one shot in-context learning setting as many of the datasets that we evaluate contain \u201cbackground\u201d as part of the input such that including one in-context demonstration often exceeds the maximum input length for both GPT and T-Few. This requires careful truncation that may effect performance. However, recent GPT-4 releases have included extended input lengths, up to 32,000 tokens, which allow for many more in-context examples and overall better performance on tasks.\nAlthough we evaluate difference sizes of LLMs, we only evaluate using GPT models. With recent releases of open-source LLMs such as LLaMa (Touvron et al., 2023), it would be of value to compare the effectiveness of different selection methods across different flavors of LLMs. Furthermore, while there are significant limitations for finetuning or accessing LLM weights of proprietary GPT models, open-source models open the opportunity to include a finetuning phase for the LLM or using activations from the LLM as a signal for the selection method."
        }
    ],
    "title": "In-Context Demonstration Selection with Cross Entropy Difference",
    "year": 2023
}