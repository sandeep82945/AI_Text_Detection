{
    "abstractText": "Recent work has shown the promise of learning with human feedback paradigms to produce human-determined high-quality text. Existing works use human feedback to train large language models (LLMs) in general domain abstractive summarization and have obtained summary quality exceeding traditional likelihood training. In this paper, we focus on a less explored form of human feedback \u2013 Human Edits. We propose Sequence Alignment (un)Likelihood Training (SALT), a novel technique to use both the human-edited and modelgenerated data together in the training loop. In addition, we demonstrate simulating Human Edits with ground truth summaries coming from existing training data \u2013 Imitation edits, along with the model-generated summaries obtained after the training, to reduce the need for expensive human-edit data. In our experiments, we extend human feedback exploration from general domain summarization to medical domain summarization. Our results1 demonstrate the effectiveness of SALT in improving the summary quality with Human and Imitation Edits. Through additional experiments, we show that SALT outperforms the conventional RLHF method (designed for human preferences) \u2013 DPO, when applied to human-edit data. We hope the evidence in our paper prompts researchers to explore, collect and better use different human feedback approaches scalably.",
    "authors": [
        {
            "affiliations": [],
            "name": "Zonghai Yao"
        },
        {
            "affiliations": [],
            "name": "Benjamin J Schloss"
        },
        {
            "affiliations": [],
            "name": "Sai P. Selvaraj"
        }
    ],
    "id": "SP:70d2c4ad21f370e2904c5d8f65b1e35e76a2c582",
    "references": [
        {
            "authors": [
                "Asma Ben Abacha",
                "Wen-wai Yim",
                "Yadan Fan",
                "Thomas Lin."
            ],
            "title": "An empirical study of clinical note generation from doctor-patient encounters",
            "venue": "12This part of the data and code will appear on our GitHub",
            "year": 2023
        },
        {
            "authors": [
                "Asma Ben Abacha",
                "Wen-wai Yim",
                "George Michalopoulos",
                "Thomas Lin."
            ],
            "title": "An investigation of evaluation metrics for automated medical note generation",
            "venue": "arXiv preprint arXiv:2305.17364.",
            "year": 2023
        },
        {
            "authors": [
                "Griffin Adams",
                "Han-Chin Shing",
                "Qing Sun",
                "Christopher Winestock",
                "Kathleen McKeown",
                "No\u00e9mie Elhadad."
            ],
            "title": "Learning to revise references for faithful summarization",
            "venue": "arXiv preprint arXiv:2204.10290.",
            "year": 2022
        },
        {
            "authors": [
                "Griffin Adams",
                "Jason Zucker",
                "No\u00e9mie Elhadad."
            ],
            "title": "A meta-evaluation of faithfulness metrics for long-form hospital-course summarization",
            "venue": "arXiv preprint arXiv:2303.03948.",
            "year": 2023
        },
        {
            "authors": [
                "Afra Feyza Aky\u00fcrek",
                "Ekin Aky\u00fcrek",
                "Aman Madaan",
                "Ashwin Kalyan",
                "Peter Clark",
                "Derry Wijaya",
                "Niket Tandon."
            ],
            "title": "Rl4f: Generating natural language feedback with reinforcement learning for repairing model outputs",
            "venue": "arXiv preprint arXiv:2305.08844.",
            "year": 2023
        },
        {
            "authors": [
                "Asma Ben Abacha",
                "Wen-wai Yim",
                "Griffin Adams",
                "Neal Snider",
                "Meliha Yetisgen."
            ],
            "title": "Overview of the mediqa-chat 2023 shared tasks on the summarization and generation of doctor-patient conversations",
            "venue": "ACL-ClinicalNLP 2023.",
            "year": 2023
        },
        {
            "authors": [
                "Florian B\u00f6hm",
                "Yang Gao",
                "Christian M Meyer",
                "Ori Shapira",
                "Ido Dagan",
                "Iryna Gurevych."
            ],
            "title": "Better rewards yield better summaries: Learning to summarise without references",
            "venue": "arXiv preprint arXiv:1909.01214.",
            "year": 2019
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners. Advances in neural information processing",
            "year": 2020
        },
        {
            "authors": [
                "Pengshan Cai",
                "Zonghai Yao",
                "Fei Liu",
                "Dakuo Wang",
                "Meghan Reilly",
                "Huixue Zhou",
                "Lingxi Li",
                "Yi Cao",
                "Alok Kapoor",
                "Adarsha Bajracharya"
            ],
            "title": "Paniniqa: Enhancing patient education through interactive question answering",
            "year": 2023
        },
        {
            "authors": [
                "Shuyang Cao",
                "Lu Wang."
            ],
            "title": "Cliff: Contrastive learning for improving faithfulness and factuality in abstractive summarization",
            "venue": "arXiv preprint arXiv:2109.09209.",
            "year": 2021
        },
        {
            "authors": [
                "Haw-Shiuan Chang",
                "Zonghai Yao",
                "Alolika Gon",
                "Hong Yu",
                "Andrew McCallum."
            ],
            "title": "Revisiting the architectures like pointer networks to efficiently improve the next word distribution, summarization factuality, and beyond",
            "venue": "arXiv preprint",
            "year": 2023
        },
        {
            "authors": [
                "Wei",
                "Kathy Meier-Hellstern",
                "Douglas Eck",
                "Jeff Dean",
                "Slav Petrov",
                "Noah Fiedel."
            ],
            "title": "PaLM: Scaling language modeling with pathways",
            "venue": "arXiv preprint arXiv:2204.02311.",
            "year": 2022
        },
        {
            "authors": [
                "Ashwin Devaraj",
                "Iain J Marshall",
                "Byron C Wallace",
                "Junyi Jessy Li."
            ],
            "title": "Paragraph-level simplification of medical texts",
            "venue": "arXiv preprint arXiv:2104.05767.",
            "year": 2021
        },
        {
            "authors": [
                "Hanze Dong",
                "Wei Xiong",
                "Deepanshu Goyal",
                "Rui Pan",
                "Shizhe Diao",
                "Jipeng Zhang",
                "Kashun Shum",
                "Tong Zhang."
            ],
            "title": "Raft: Reward ranked finetuning for generative foundation model alignment",
            "venue": "arXiv preprint arXiv:2304.06767.",
            "year": 2023
        },
        {
            "authors": [
                "Jinlan Fu",
                "See-Kiong Ng",
                "Zhengbao Jiang",
                "Pengfei Liu."
            ],
            "title": "Gptscore: Evaluate as you desire",
            "venue": "arXiv preprint arXiv:2302.04166.",
            "year": 2023
        },
        {
            "authors": [
                "Mingqi Gao",
                "Jie Ruan",
                "Renliang Sun",
                "Xunjian Yin",
                "Shiping Yang",
                "Xiaojun Wan."
            ],
            "title": "Human-like summarization evaluation with chatgpt",
            "venue": "arXiv preprint arXiv:2304.02554.",
            "year": 2023
        },
        {
            "authors": [
                "James Kirkpatrick",
                "Razvan Pascanu",
                "Neil Rabinowitz",
                "Joel Veness",
                "Guillaume Desjardins",
                "Andrei A Rusu",
                "Kieran Milan",
                "John Quan",
                "Tiago Ramalho",
                "Agnieszka Grabska-Barwinska"
            ],
            "title": "Overcoming catastrophic forgetting in neural networks",
            "year": 2017
        },
        {
            "authors": [
                "Kundan Krishna",
                "Sopan Khosla",
                "Jeffrey P Bigham",
                "Zachary C Lipton."
            ],
            "title": "Generating soap notes from doctor-patient conversations using modular summarization techniques",
            "venue": "arXiv preprint arXiv:2005.01795.",
            "year": 2020
        },
        {
            "authors": [
                "Sunjae Kwon",
                "Zonghai Yao",
                "Harmon S Jordan",
                "David A Levy",
                "Brian Corner",
                "Hong Yu."
            ],
            "title": "Medjex: A medical jargon extraction model with wiki\u2019s hyperlink span and contextualized masked language model score",
            "venue": "arXiv preprint arXiv:2210.05875.",
            "year": 2022
        },
        {
            "authors": [
                "Chin-Yew Lin."
            ],
            "title": "ROUGE: A package for automatic evaluation of summaries",
            "venue": "Text summarization branches out, pages 74\u201381.",
            "year": 2004
        },
        {
            "authors": [
                "Shayne Longpre",
                "Le Hou",
                "Tu Vu",
                "Albert Webson",
                "Hyung Won Chung",
                "Yi Tay",
                "Denny Zhou",
                "Quoc V. Le",
                "Barret Zoph",
                "Jason Wei",
                "Adam Roberts"
            ],
            "title": "The flan collection: Designing data and methods for effective instruction tuning",
            "year": 2023
        },
        {
            "authors": [
                "Ling Luo",
                "Po-Ting Lai",
                "Chih-Hsuan Wei",
                "Cecilia N Arighi",
                "Zhiyong Lu."
            ],
            "title": "Biored: a rich biomedical relation extraction dataset",
            "venue": "Briefings in Bioinformatics, 23(5):bbac282.",
            "year": 2022
        },
        {
            "authors": [
                "Francesco Moramarco",
                "Alex Papadopoulos Korfiatis",
                "Mark Perera",
                "Damir Juric",
                "Jack Flann",
                "Ehud Reiter",
                "Anya Belz",
                "Aleksandar Savkov."
            ],
            "title": "Human evaluation and correlation with automatic metrics in consultation note generation",
            "venue": "arXiv preprint",
            "year": 2022
        },
        {
            "authors": [
                "Shashi Narayan",
                "Shay B. Cohen",
                "Mirella Lapata."
            ],
            "title": "Don\u2019t give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization",
            "venue": "ArXiv, abs/1808.08745.",
            "year": 2018
        },
        {
            "authors": [
                "Saul B Needleman",
                "Christian D Wunsch."
            ],
            "title": "A general method applicable to the search for similarities in the amino acid sequence of two proteins",
            "venue": "Journal of molecular biology, 48(3):443\u2013453.",
            "year": 1970
        },
        {
            "authors": [
                "OpenAI."
            ],
            "title": "Gpt-4 technical report",
            "venue": "arXiv preprint arXiv:2303.08774.",
            "year": 2023
        },
        {
            "authors": [
                "Paul Christiano",
                "Jan Leike",
                "Ryan Lowe."
            ],
            "title": "Training language models to follow instructions with human feedback",
            "venue": "arXiv preprint arXiv:2203.02155.",
            "year": 2022
        },
        {
            "authors": [
                "Fabio Petroni",
                "Tim Rockt\u00e4schel",
                "Patrick Lewis",
                "Anton Bakhtin",
                "Yuxiang Wu",
                "Alexander H Miller",
                "Sebastian Riedel"
            ],
            "title": "Language models as knowledge bases? arXiv preprint arXiv:1909.01066",
            "year": 2019
        },
        {
            "authors": [
                "Dragomir R Radev",
                "Hong Qi",
                "Harris Wu",
                "Weiguo Fan."
            ],
            "title": "Evaluating web-based question answering systems",
            "venue": "LREC. Citeseer.",
            "year": 2002
        },
        {
            "authors": [
                "Rafael Rafailov",
                "Archit Sharma",
                "Eric Mitchell",
                "Stefano Ermon",
                "Christopher D Manning",
                "Chelsea Finn."
            ],
            "title": "Direct preference optimization: Your language model is secretly a reward model",
            "venue": "arXiv preprint arXiv:2305.18290.",
            "year": 2023
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J Liu"
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "J. Mach. Learn. Res.,",
            "year": 2020
        },
        {
            "authors": [
                "Sanjana Ramprasad",
                "Elisa Ferracane",
                "Sai P Selvaraj"
            ],
            "title": "Generating more faithful and consistent soap notes using attribute-specific parameters",
            "year": 2023
        },
        {
            "authors": [
                "Sylvestre-Alvise Rebuffi",
                "Alexander Kolesnikov",
                "Georg Sperl",
                "Christoph H Lampert."
            ],
            "title": "icarl: Incremental classifier and representation learning",
            "venue": "Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, pages 2001\u20132010.",
            "year": 2017
        },
        {
            "authors": [
                "Santilli",
                "Thibault F\u00e9vry",
                "Jason Alan Fries",
                "Ryan Teehan",
                "Stella Biderman",
                "Leo Gao",
                "Tali Bers",
                "Thomas Wolf",
                "Alexander M. Rush."
            ],
            "title": "Multitask prompted training enables zero-shot task generalization",
            "venue": "CoRR, abs/2110.08207.",
            "year": 2021
        },
        {
            "authors": [
                "Benjamin Schloss",
                "Sandeep Konam."
            ],
            "title": "Towards an automated soap note: classifying utterances from medical conversations",
            "venue": "Machine Learning for Healthcare Conference, pages 610\u2013631. PMLR.",
            "year": 2020
        },
        {
            "authors": [
                "Abigail See",
                "Peter J. Liu",
                "Christopher D. Manning."
            ],
            "title": "Get to the point: Summarization with pointergenerator networks",
            "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1073\u2013",
            "year": 2017
        },
        {
            "authors": [
                "Nisan Stiennon",
                "Long Ouyang",
                "Jeffrey Wu",
                "Daniel Ziegler",
                "Ryan Lowe",
                "Chelsea Voss",
                "Alec Radford",
                "Dario Amodei",
                "Paul F Christiano."
            ],
            "title": "Learning to summarize with human feedback",
            "venue": "Advances in Neural Information Processing Systems, 33:3008\u2013",
            "year": 2020
        },
        {
            "authors": [
                "Yixuan Su",
                "Tian Lan",
                "Yan Wang",
                "Dani Yogatama",
                "Lingpeng Kong",
                "Nigel Collier."
            ],
            "title": "A contrastive framework for neural text generation",
            "venue": "arXiv preprint arXiv:2202.06417.",
            "year": 2022
        },
        {
            "authors": [
                "Mujeen Sung",
                "Jinhyuk Lee",
                "Sean Yi",
                "Minji Jeon",
                "Sungdong Kim",
                "Jaewoo Kang"
            ],
            "title": "Can language models be biomedical knowledge bases? arXiv preprint arXiv:2109.07154",
            "year": 2021
        },
        {
            "authors": [
                "Junda Wang",
                "Zonghai Yao",
                "Avijit Mitra",
                "Samuel Osebe",
                "Zhichao Yang",
                "Hong Yu"
            ],
            "title": "UMASS BioNLP at MEDIQA-chat 2023: Can LLMs generate high-quality synthetic note-oriented doctor-patient conversations",
            "year": 2023
        },
        {
            "authors": [
                "Sean Welleck",
                "Ilia Kulikov",
                "Stephen Roller",
                "Emily Dinan",
                "Kyunghyun Cho",
                "Jason Weston."
            ],
            "title": "Neural text generation with unlikelihood training",
            "venue": "arXiv preprint arXiv:1908.04319.",
            "year": 2019
        },
        {
            "authors": [
                "Zonghai Yao",
                "Yi Cao",
                "Zhichao Yang",
                "Vijeta Deshpande",
                "Hong Yu."
            ],
            "title": "Extracting biomedical factual knowledge using pretrained language model and electronic health record context",
            "venue": "arXiv preprint arXiv:2209.07859.",
            "year": 2022
        },
        {
            "authors": [
                "Zonghai Yao",
                "Yi Cao",
                "Zhichao Yang",
                "Hong Yu."
            ],
            "title": "Context variance evaluation of pretrained language models for prompt-based biomedical knowledge probing",
            "venue": "arXiv preprint arXiv:2211.10265.",
            "year": 2022
        },
        {
            "authors": [
                "Wen-wai Yim",
                "Yujuan Fu",
                "Asma Ben Abacha",
                "Neal Snider",
                "Thomas Lin",
                "Meliha Yetisgen."
            ],
            "title": "Acibench: a novel ambient clinical intelligence dataset for benchmarking automatic visit note generation",
            "venue": "Submitted to Nature Scientific Data.",
            "year": 2023
        },
        {
            "authors": [
                "Zheng Yuan",
                "Hongyi Yuan",
                "Chuanqi Tan",
                "Wei Wang",
                "Songfang Huang",
                "Fei Huang."
            ],
            "title": "Rrhf: Rank responses to align language models with human feedback without tears",
            "venue": "arXiv preprint arXiv:2304.05302.",
            "year": 2023
        },
        {
            "authors": [
                "Yao Zhao",
                "Rishabh Joshi",
                "Tianqi Liu",
                "Misha Khalman",
                "Mohammad Saleh",
                "Peter J Liu."
            ],
            "title": "Slic-hf: Sequence likelihood calibration with human feedback",
            "venue": "arXiv preprint arXiv:2305.10425.",
            "year": 2023
        },
        {
            "authors": [
                "Daniel M Ziegler",
                "Nisan Stiennon",
                "Jeffrey Wu",
                "Tom B Brown",
                "Alec Radford",
                "Dario Amodei",
                "Paul Christiano",
                "Geoffrey Irving."
            ],
            "title": "Fine-tuning language models from human preferences",
            "venue": "arXiv preprint arXiv:1909.08593.",
            "year": 2019
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Large-scale language model pretraining has become increasingly prevalent to achieve high performance on various natural language processing (NLP) tasks (Brown et al., 2020; Sanh et al., 2021; Chowdhery et al., 2022; Longpre et al., 2023; OpenAI, 2023; Cai et al., 2023). When applying these\n1Code and the public dataset (Appendix A.2) is at https: //github.com/seasonyao/LearnFromHumanEdit\n\u2020 Work was done during internship at Abridge AI Inc\nmodels to a specific task, they are usually finetuned to maximize the likelihood of human-written text. While this strategy has led to markedly improved performance in many metrics, models still cannot consistently produce human-determined high-quality output. The NLP community has pointed out some key drawbacks of traditional finetuning. First, important errors (e.g. hallucinations) and unimportant errors (e.g. minor grammar errors) equally contribute to the final loss. Second, the model weighs the loss equally on all labeled data of different types, qualities, and difficulties. Third, distribution shifts in new data degrade performance (catastrophic forgetting)(Kirkpatrick et al., 2017).\nSome works tackle these problems with human feedback (HF). Specifically, they fine-tune language models with HF using reward learning (Stiennon et al., 2020; Ziegler et al., 2019). With a large amount of HF data, these works demonstrate that large-scale LMs, such as GPT-3 (Brown et al., 2020), have a text generation quality exceeding traditional likelihood training. However, the acquisition cost of large-scale HF is high, and whether smaller LMs can also benefit is not fully studied. In\naddition, because LLMs are often provided in the form of third-party APIs and are too large for many companies\u2019 and labs\u2019 infrastructure to host, smaller models (e.g., T5 family (Raffel et al., 2020)) still play important roles in many domains (e.g., medical), where privacy issues and pragmatic economics dominate decision-making strategies.\nOur goal in this paper is to explore methods to train language models to improve the summary quality with HF inexpensively. HF for summarization can come in different forms. One is to obtain human scores for the summaries. Previous work (Stiennon et al., 2020; Ziegler et al., 2019) focuses on training a reward function through HF data and using such rewards as training objectives by comparing different summaries\u2019 scores. More recently, this is used by generative AI works (e.g., ChatGPT and GPT4 (Ouyang et al., 2022; OpenAI, 2023)), and they call the method RLHF. Another HF is obtaining edits to make the summary correct. The second approach is a natural way to collect feedback from users in workflows where users may be working off of an AI-generated summary in their workflow. For example, the summaries SE , in Table 1 are the results of clinicians/scribes modifying our AI-generated EHR summaries SAI . In addition, the second approach might be more data efficient in improving the summarization models than the first, as it conveys more granular information than a score for the entire summary. Human Edits from the second approach can also be converted to scores with simple rules like the percentage of edits, although this has not been studied extensively. Hence, from an ML data point of view, the second approach has certain unique advantages. Furthermore, large-scale expert feedback is hard to get using annotation ways in RLHF, considering the expert/user\u2019s time, cost, and willingness. But, Human Edits, which can be obtained from the users using the AI summaries for their work, may become a more reasonable alternative in various professional-knowledge-intensive domains.\nWe explore how to use Human Edits to improve summary quality. In addition to general domain summarization, we also focus on a medical domain summarization task in automatic clinical note generation from doctor-patient conversations, which is understudied due to privacy and data inaccessibility problems. Table 1 provides an example of a Clinician Conversation from our dataset (CC). We present our work from two experiments on a novel\ntechnique, Sequence Alignment (un)Likelihood Training (SALT), which uses Human Edits and unlikelihood objectives together with the standard likelihood training paradigm to improve the summary quality. Unlikelihood training was proposed to reduce the probability of unlikely tokens predicted by models (Welleck et al., 2019).\nIn our first experiment, we use the Human Edits from physicians editing AI-generated clinical summaries from medical conversations to improve the summarization models. In our second, we explore how we can get similar benefits with pre-existing ground-truth human summaries that are not written as edits to the AI-generated summaries, which we call Imitation Edits. We refer to AI-generated summary SAI , human-edit summary SE , and imitationedit summary SI . We show how the unlikelihood objective can be generalized to improve the summary quality together with (SAI , SE) and (SAI , SI ) pairs. In addition, our results show that SALT stably improves summary quality for T5 (small and large) summarization models with Human and Imitation Edits. Further experiments show how SALT can address the catastrophic forgetting problem arising from the distribution difference between SAI and SE with the help of RSALT, which is an improved version of the Replay-based methods in Continual Learning (Rebuffi et al., 2017).\nFinally, to compare SALT and RLHF, we experiment with SALT and Direct Preference Optimization (DPO) (Rafailov et al., 2023) on human edit data and demonstrate the superiority of SALT on this type of human feedback.\nTo conserve space constraints, we have relegated specific contents to the appendix. In Appendix A.1 and A.2, we provide definitions of the SOAP Structure and implementation details. In Appendix A.3, we focus on the utilization of Imitation Edits and SALT for training on publicly available datasets, accompanied by the experimental results. Lastly, in Appendix A.4, we have more discussion about the relation between SALT and various other RLHFs. In summary, our contributions are as follows:\n\u2022 To our knowledge, we are the first to extend current HF trends in summarization research to the automatic clinical note-generation task.\n\u2022 Different from the form of HF used in previous work, we explore Human Edits to improve summary quality in this paper.\n\u2022 We show how to construct Imitation Edits to reduce the need for expensive HF data.\n\u2022 We show SALT extends unlikelihood training into a general framework using sequence alignment and further combines SALT and Replay-based methods (Rebuffi et al., 2017) into RSALT for tackling catastrophic forgetting.\n\u2022 Finally, we show that SALT achieves better performance than DPO on human-edit feedback."
        },
        {
            "heading": "2 Related Work",
            "text": "Most directly related to our work is research on automatic clinical note generation from doctorpatient conversations (Schloss and Konam, 2020; Ramprasad et al., 2023; Krishna et al., 2020; Abacha et al., 2023a; Ben Abacha et al., 2023; Yim et al., 2023; Wang et al., 2023), and the difference is that those works focus on training a summarization model with pre-labeled data, while we focus on using HF further to improve the summary quality of the trained models.\nPrevious work used HF to train summarization models with reinforcement learning (RL) (Bo\u0308hm et al., 2019; Ziegler et al., 2019; Stiennon et al., 2020) and used GPT-2 and GPT-3 to optimize HF across various summarization tasks. These RLbased methods focus on training a reward function through HF data and use such rewards as training objectives by comparing different summaries (RLHF). Recently, some RLHF variants collect or use rewards more flexibly and stably (Akyu\u0308rek et al., 2023; Dong et al., 2023; Zhao et al., 2023; Yuan et al., 2023). We introduce unlikelihood training as an additional learning objective in supervised learning. Our technique aims to decrease the probability of unlikely sequences, defined as those which appear in the SAI but not in SE , and increase the probability of verified sequences, which are in SAI and reinforced by SE , as well as novel sequences which do not appear in SAI but do appear in SE .\nUnlikelihood training (Welleck et al., 2019) involves adding unlikelihood loss to lower the probability of negative candidates. Previous work has explored many scenarios with various negative candidates for unlikelihood training, including: style transfer (Devaraj et al., 2021), repetition, copying, and contradictions (Li et al., 2019), factuality (Cao and Wang, 2021), text degeneration (Su et al., 2022), and clinical summarization (Adams et al., 2022). In this work, we align the SE with SAI to identify negative candidates and train different tokens with unlikelihood and likelihood loss. We also show that our experiments on Human Edits\ncan be extended to Imitation Edits to reduce the need for HF data which can be expensive to get."
        },
        {
            "heading": "3 Dataset",
            "text": ""
        },
        {
            "heading": "3.1 Clinician Conversations (CC) Dataset",
            "text": "This dataset is a collection of 63000 consented doctor-patient de-identification conversations with human transcripts with an average duration of 9 minutes. We segmented the dataset to create training, validation, and test sets of 52,000, 5,000, and 6,000 files each while controlling important characteristics of the distribution in each split. The transcripts of the conversations were annotated according to the traditional SOAP format 2. A SOAP note can contain numerous observations that are grounded to shorter excerpts from the transcript via timestamps that relate back to the original audio. There are several sections and subsections in the SOAP structure, each of which needs specific information and is written in a different format. Table 2 shows the average length span of different subsections is large."
        },
        {
            "heading": "3.2 CCUser Dataset",
            "text": "In order to generate SOAP notes from doctorpatient conversations, our pipeline follows (Ramprasad et al., 2023; Krishna et al., 2020). We first record the clinical conversation, then transcribe it either using humans or using Google\u2019s medical-conversations Automatic Speech Recognition (ASR) service. Then, using our proprietary models, we classify utterances into SOAP sections. Finally, using our section-conditioned summarization model trained on the CC dataset, we generate summaries for each of the utterance clusters belonging to each section.\n2SOAP structure details can be found in the Appendix A.1.\nWe use our pipeline to extract SOAP summaries for our clinician users who record their conversations with their patients via a mobile app. The generated summaries were edited by scribes and doctors using our dashboard for their documentation tasks. The dashboard is built for doctors and scribes to check and fix AI-generated summaries in their regular workflow quickly. Hence, we didn\u2019t enforce any training/instructions that might make the data more useful for research, and the users were free to use the dashboard as they saw fit.\nThe distribution of the CCUser dataset differs from the CC dataset in the following ways. First, CC uses human-written transcripts as training inputs, while CCUser uses our pipeline\u2019s inputs from ASR transcripts rather than human-tagged utterances. Second, the average length of a conversation was 20 min for CCUser compared to 9 min for CC dataset, which could mean more complex conversations. The dataset has 215 ASR transcripts with AI-generated notes (along with the Human Edits) from 10 physicians. We randomly select 70 notes from 7 physicians as a training dataset, 10 for each physician, and divide the remaining 145 notes into evaluation and test sets. Finally, our dataset is split as a train:eval:test = 1279:1457:1458 \u2013 (utterance cluster, edited summary, AI summary) triplet."
        },
        {
            "heading": "4 Methods",
            "text": "Given a tokenized utterance cluster as input U = [x1, x2, x3, ...xlenU ], the CC summarization model M generates a summary SAI = [y\u20321, y \u2032 2, y \u2032 3, ...y \u2032 lenSAI\n] for it. The user edits this summary from SAI to SE , where SE = [y1, y2, y3, ...ylenSE ]. We aim to update parameters in M based on both SAI and SE . Let, lenU , lenSAI , and lenSE be the number of tokens in U , SAI , and SE respectively."
        },
        {
            "heading": "4.1 Sequence Alignment (un)Likelihood",
            "text": "Training (SALT) using SAI and SE\nWhen a user edits a summary from SAI to SE , they can modify or delete a span of tokens, insert a new span of tokens, or not change anything to a span of tokens. We want to use these Human Edits to improve our summarization models and produce outputs that are closer to the user\u2019s modified summary than before. We do this using both SAI and SE in the training. We train the model to:\n(i) Lower the probability of producing words that the user deleted or modified in SAI .\n(ii) Reinforce the probability of producing words that the user didn\u2019t change in SAI and are retained in SE . (iii) Increase the probability of producing words that the new user added in SE .\nThe loss functions to train the summarization model with SAI and SE :\nLSAI = \u2211\nx\u2208SAI\n[1AI\u2212C (t) wAI\u2212C Lp(x, t) +\n1AI\u2212NC (t) wAI\u2212NC Lr(x, t) ]\n(1)\nLSE = \u2211\nx\u2208SE\n[1E\u2212C (t) wE\u2212C Lp(x, t) +\n1E\u2212NC (t) wE\u2212NC Lr(x, t) ]\n(2)\nLp(x, t) = \u2212 log (1\u2212 p\u03b8(xt|x<t, U)) (3) Lr(x, t) = \u2212 log p\u03b8(xt|x<t, U) (4)\nWhere: 1. U is the utterance cluster used as input 2. C and NC mean \u201cchanged\u201d and \u201cnot changed\u201d\ntokens when we align SAI and SE sequences. 3. 1AI\u2212C (t) and 1AI\u2212NC (t) are the indicator\nfunction to signify if the token xt in SAI is changed or not-changed by the user. Similarly, 1E\u2212C (t) and 1E\u2212NC (t) corresponds to SE .\n4. wx are the loss weights, for example, wAI\u2212C is the weight to penalize tokens that are in SAI but not in SE . 5. Lr(x, t) and Lp(x, t) are the likelihood and unlikelihood loss functions\nThe losses LSAI and LSE used in the (SAI , SE) pair are used to train the summarization model. The indicator functions used in the above equations can be found by tracking the user changes as they edit the summary or by aligning SE to SAI using a sequence alignment algorithm. We use sequence alignment (the Needleman-Wunsch Algorithm (Needleman and Wunsch, 1970)) in this work because our dashboard doesn\u2019t log the users\u2019 keystrokes. Assume we have a pair from SAI and the corresponding SE , \u201cpatient takes one aspirin daily\u201d and \u201cpatient doesn\u2019t want to take aspirin\u201d. We can align these two sentences as below: patient \u2212 \u2212 \u2212 takes one aspirin daily patient doesn \u2032 t want to take \u2212 aspirin \u2212\nC I I I S D C D\nWhere \u201cC\u201d is \u201cCorrespondence\u201d (matching), \u201cI\u201d is \u201cInserted\u201d, \u201cD\u201d is \u201cDeleted\u201d, and \u201cS\u201d is \u201cSubstituted\u201d. Note that we do it on the token level in the implementation. For SAI word list [\u201cpatient\u201d, \u201ctakes\u201d, \u201cone\u201d, \u201caspirin\u201d, \u201cdaily\u201d], the corresponding indicator function in Equation 1 are:\n1AI\u2212C (t) = [0, 1, 1, 0, 1]\n1AI\u2212NC (t) = [1, 0, 0, 1, 0]\nFor SE word list [\u201cpatient\u201d, \u201cdoesn\u2019t\u201d, \u201cwant\u201d, \u201cto\u201d, \u201ctake\u201d, \u201caspirin\u201d], the corresponding indicator function in Equation 2 are:\n1E\u2212C (t) = [0, 1, 1, 1, 1, 0]\n1E\u2212NC (t) = [1, 0, 0, 0, 0, 1]"
        },
        {
            "heading": "4.2 Imitation Edits",
            "text": "SE is a special kind of ground truth summary from the user. SE is obtained by the user using U and SAI \u2013 SE = Fn(U, SAI). An interesting question is whether we can approximate the edited summary SI (Imitation Edits), and use it to improve the models in the absence of actual Human Edits with SALT. In our work, we use the pre-existing ground-truth summaries as SI even though they were not explicitly written as edits to SAI . Leveraging such data has several advantages. First, SE is not easy to obtain, approximating SE with SI can increase the amount of data available for unlikelihood training. And we will be able to use SALT even without human-edit data or any new annotations. Second, under the premise of ensuring that the Imitation Edits are of high quality, combining Human Edits and Imitation Edits can further improve the model\u2019s performance since both of them bring effective data points for training. Third, Imitation Edits can be used to solve the forgetting problem when we do SALT training with SAI and SE , we show this in the next section.\nTo imitate Human Edits, we assume the original ground truth summary is generated from SAI and its utterance cluster U (even though the ground truth notes were written independently). Similar to the above setting with SAI and SE , we use the alignment algorithm to align SAI and SI . Then we calculate LSI .\nLSI = \u2211 x\u2208SI [1I\u2212C (t) wI\u2212CLp(x, t) +\n1I\u2212NC (t) wI\u2212NCLr(x, t)]\n(5)\nwhere 1I\u2212C (t) and 1I\u2212NC (t) signify if the token xt in SI is changed or not-changed compared to SAI , and wx are the loss weights."
        },
        {
            "heading": "4.3 Replay-based SALT (RSALT) for Catastrophic Forgetting Problem",
            "text": "We continue training the model M that has converged in the original summarization dataset (e.g., CC) on the Human Edits dataset (e.g., CCUser) to\nimprove the summary quality, subjecting the model to the catastrophic forgetting problem because of the distribution differences between them. We use the traditional Replay-based methods, (Rebuffi et al., 2017), which sample a part of the data from the seen dataset (e.g., CC) and add it to the unseen data (e.g., CCUser), to address the catastrophic forgetting problem. Here, the likelihood loss is calculated for both sampled seen data SI(seen) and human-edit data SE(unseen) with the loss function L = MLESI(seen) + MLESE(unseen) , where we use Maximum Likelihood Estimation for the loss.\nFollowing Section 4.1, we can use both SAI(unseen) and SE(unseen) to do SALT training. Following Section 4.2, for the sampled previously seen data, we can also get (SAI(seen), SI(seen)) pairs and do SALT training. According to Equations 1, 2, 5, the loss function with RSALT is\nLSALT = LSAI(unseen) + LSE(unseen) (6)\nLRSALT = LSAI(seen) + LSI(seen) (7)\nL = LSALT + LRSALT (8)"
        },
        {
            "heading": "5 Metrics",
            "text": "ROUGE and UMLS-F1 Models are evaluated with full-length F1-scores of ROUGE (Lin, 2004). We use QuickUMLS3 to extract medical concepts from both model-generated and ground truth summaries and then calculate F1-scores for these two lists of concepts, which is named UMLSF1 (Adams et al., 2023; Ramprasad et al., 2023).\nGPT4 & Human preference Recent work shows a higher correlation between human and GPT4 evaluation than traditional metrics (Moramarco et al., 2022; Gao et al., 2023; Fu et al., 2023), so we also use GPT4 preference as measurements to evaluate summary quality. Specifically, we instruct GPT4 to give preference ranking on different AI-generated summaries based on the conversation snippet and reference summary 4. Similarly, we asked 2 medical students5 to rate summaries from CC based on the same information, for privacy reasons, we did not evaluate CCUser with humans. We discuss the Mean Reciprocal Rank (MRR) (Radev et al., 2002) of different models in Section 6.4. Generally, a higher MRR value implies that evaluators have more preference over an approach.\n3https://github.com/Georgetown-IR-Lab/QuickUMLS 4Prompts can be found in Appendix. 5Both with hospital internship experience\nSAGE ROUGE and UMLS-F1 measure the degree of \u201clikelihood,\u201d i.e., they evaluate whether or not the model can generate something closer to some references. However, we don\u2019t just want to know how much \u201ccloser to SE\u201d is newly generated summary, but also how \u201cfar away from the bad part of SAI\u201d \u2013 spans that are changed by the Human Edits. To address this problem, we design an evaluation method to measure how likely machines are to make the same mistakes as before and how likely they are to generate summaries more like the target users (as identified during the editing process). We call this System output Against the Generated and Edited sentence (SAGE). Given the evaluation data (U , SAI , SE), where SAI is generated by the model trained by the original summarization dataset (e.g., CC) and SE is edited by human based on (U , SAI ), we can get the new summary Snew generated by the new model trained by Human Edits dataset (e.g., CCUser). Using (Snew, SAI , SE), we can define three groups of words after removing stop words and punctuation in Snew:\n1. Gw1(AI\u2212E) = {w|w \u2208 SAI \u2227 w /\u2208 SE} 2. Gw2(E\u2212AI) = {w|w /\u2208 SAI \u2227 w \u2208 SE} 3. Gw3(AI\u2229E) = {w|w \u2208 SE \u2227 w \u2208 SAI} By training on HF, we aim to have Snew closer to SE while avoiding the mistakes found in SAI . So SAGE counts how many words in Snew are in Gw1(AI\u2212E), Gw2(E\u2212AI), and Gw3(AI\u2229E). We call this word level SAGE (SAGEw). Similarly, we can define Gc1(AI\u2212E), Gc2(E\u2212AI), Gc3(AI\u2229E) and make Concept-level SAGE (SAGEc) based on UMLS concept overlap in Snew, SAI , and SE .\nWe have two assumptions regarding SAGE: 1. users can accept machines making some mis-\ntakes, but they can\u2019t tolerate machines making the same mistake, again and again.\n2. users will be more satisfied if the model, over time, learns to generate outputs more similar to the user\u2019s edited summaries\nAccording to Assumption 1 and 2, a model trained on HF should be able to generate less content belonging to G1 (Gw1 and Gc1), and more content belonging to G2 (Gw2 and Gc2). The model should also be able generate G3 (Gw3 and Gc3) since G3 represents human-verified information."
        },
        {
            "heading": "6 Experiments",
            "text": "We use the following symbols: 1. M refers to models that are trained and al-\nready converged on the CC dataset. All methods below are initialized from M and continue training on SE , SI , and SAI . 2. SALTl: the baseline, which is only based on likelihood training on SE or SI 3. SALTld (or SALTli): likelihood training on SE or SI , but with decreased (or increased) weights for 1E\u2212C or 1I\u2212C tokens 4. SALTu: only unlikelihood training on SAI 5. SALTl+u: both likelihood (on SE or SI ) and\nunlikelihood (on SAI ) 6. SALTx: all the above SALT variations 7. SALTx+RSALTl is the traditional replay-\nbased method. When continuing to train M with different SALT variations on new data, this method will sample a part of the data from the dataset that M has already seen and use them for training with likelihood loss.\n8. SALTx+RSALTl+u: following Section 4.3, RSALT treats sampled data from the replaybased method as imitation-edit data and uses both likelihood and unlikelihood training."
        },
        {
            "heading": "6.1 SALT in human-edit dataset",
            "text": ""
        },
        {
            "heading": "6.1.1 Analyzing the behavior of SALT",
            "text": "In Table 3, the evaluation on the CCUsereval shows compared to the regular likelihood training\n6There are no scores for M on human-edit evaluation data (CCUser) here. Because human-edit data is directly modified from M \u2019s SAI , so it is unfair to calculate the scores of its SAI and human-edit data and compare with other methods.\n(SALTl), changing loss weights for 1E\u2212C tokens in likelihood training (SALTld or SALTli) can bring changes to their performance. Predictably we see in Table 4, that SALTli produces higher Gw2 than SALTld , and the trends in other columns are not as pronounced since SAI isn\u2019t considered. Similarly, SALTu produces lower Gw1 than the others. However, SALTl+u achieves significantly higher performance on both CC and CCUser. We further show how we can manipulate a model\u2019s behaviors using different SALT through SAGE in Table 4.\nFirst, SALTl only uses SE , and all tokens in SE contribute to the loss equally. SALT can increase or decrease the emphasis of the model on 1E\u2212C through different weights on the loss function. Increasing the loss weight of 1E\u2212C will make the model generate more words/concepts belonging to 1E\u2212C (Gw2 and Gc2), which follows our SAGE Assumption 2. While reducing the loss weight of 1E\u2212C will make the model generate fewer words and concepts belonging to 1E\u2212C (Gw2 and Gc2), at the same time it can also reduce the generation of words/concepts belonging to 1AI\u2212C (Gw1 and Gc1), which satisfies our SAGE Assumption 1. So SALTld and SALTli make the model better for users according to the SAGE metric.\nSecond, unlike the three above SALT variations, SALTu only uses SAI but it knows which tokens in SAI belong to 1AI\u2212C and 1AI\u2212NC respectively. So SALTu significantly reduces the words and concepts belonging to 1AI\u2212C . However, because the data of 1E\u2212NC has not been seen, SALTu rarely generates related words and concepts.\nFinally, SALTl+u has more granular information\u2013 that tokens belonging to 1AI\u2212C , 1AI\u2212NC , 1E\u2212C , and 1E\u2212NC in SAI (SE) through their corresponding loss weights. Therefore, SALTl+u can learn the more suitable distribution, which decreases the generation of words and concepts belonging to 1AI\u2212C while increasing the generation of words and concepts belonging to 1AI\u2212NC , 1E\u2212C and 1E\u2212NC ."
        },
        {
            "heading": "6.1.2 Reducing the forgetting problem",
            "text": "In Table 3, we see a dip in evaluation metrics for SALTl in the old evaluation dataset CCeval when we train the model trained on the CCUser \u2013 catastrophic forgetting. The reason could be the distribution difference between CCUser and CC dataset described in Section 3.2. Both SALTu and SALTl+u have different degrees of improvement in ROUGE-1 and UMLS-F1 on CCeval data. This result shows that SALT training also alleviates the forgetting problem to a certain extent.\nOne widely used and effective technique to reduce catastrophic forgetting is the replay-based method, which mixes in the seen data the model was trained on (e.g., CC). In this work, we set the ratio of CCUser and CC data to 2:1. That is, assuming that there are n CCUser data, we will sample 0.5 \u2217 n CC data to train together 7. Table 3 shows that SALTx+RSALTl is effective in helping the model reduce the catastrophic forgetting problem. Adding the sampled seen data improves the model\u2019s performance in both the new \u2013 CCUser and the original \u2013 CC data. However, we still see a reduction in the performance of SALTx+RSALTl in the CC dataset compared with M , which shows that the traditional replay-based method cannot completely solve this problem. In Section 6.3, we show how we address the problem further with SALT, imitation-edit data, and RSALT."
        },
        {
            "heading": "6.2 SALT in imitation-edit dataset",
            "text": "SALT uses the relationship between SE and SAI to get better performance than using just SE and likelihood training. In this section, we show that we can directly improve the summarization model M using a similar relationship between SI (the ground truth data) and SAI without new human-edit data or additional annotation, i.e., by assuming that the SI is the output of human-edit data on SAI . Simulating Human Edits this way lets us 1) demonstrate the effectiveness of SALT on a public dataset that does not have the human-edit component in them,8 and 2) reduce the amount of Human Edits needed as it is hard to get.\nAlthough both come from humans, SE and SI are fundamentally different in their relationship with SAI . The former is modified from SAI while humans generate the latter from scratch. There-\n7Adjusting this ratio will bring some improvements in certain SALTx, but we found that 2:1 has relatively good performance in most SALTx, so we use this ratio uniformly.\n8Due to the space limit, we put results in the Appendix.\nfore, SE is directly dependent on SAI , but SI is not. Consequently, even though SE and SI are dependent on the same data as input, the differences between SAI and SI are likely to be larger than between SAI and SE . We can see this difference in the average percentage of changed tokens \u2013 1E\u2212C and 1I\u2212C is 1, the former (6.17%) is much lower than the latter (45.59%). Hence, after we do sequence alignment between SI and SAI , we perform a two-step post-processing operation 9 to ensure the training stability, which helps us to reduce the percentage of changed tokens from 45.59% to 19.07% with an acceptable amount of data lost (21.38%)."
        },
        {
            "heading": "6.2.1 Imitation Edits using seen data",
            "text": "We use the training data from CC to experiment with the effects of SALT and Imitation Edits on seen data. First, for the CC dataset, the results in Table 5 show that continuing to use likelihood loss on the training dataset to train the already convergent M does not improve the performance and leads to overfitting. However, when we use SI as imitation-edit data and do SALT training on it with SAI , we can see an improvement. Second, we see similar results for the CNN dataset. Even though there is no performance degradation arising from overfitting for SALTl, doing SALT training with SI and SAI can improve the performance more than using just the likelihood training. These results show that we can get additional improvement on the model by continuing to train it with SALT on\n9The details are in Appendix A.3.1.\nthe seen dataset even if the model is already converged (on the seen/original training data). Third, different from previous human-edit results, SALTu of CC is better than SALTl+u. We think this is because M has started to overfit on CC data, so continuing to add likelihood to the original training data reduces the scores."
        },
        {
            "heading": "6.2.2 Imitation Edits using unseen data",
            "text": "We use a part of the test dataset (not used in the evaluation) from CC to experiment with the effects of SALT and Imitation Edits on unseen data. In Table 6, we take M (trained on CC-train) and train it with a part of CC-test as the imitation-edit data with SALT. We take the remaining test data of the CC-test to evaluate the model performance in new imitation-edit data and then use CC-eval to evaluate the model performance in the original data. In imitation-edit evaluation results (CCtest\u2212r) of Table 6, SALTl+u has better performance than the baseline method SALTl, which is consistent with our results using human-edit data in Table 3. In the original data evaluation results (CCeval) of Table 6, although there was no forgetting problem arising from distribution shift, SALTl+u still has a higher score than the baseline model SALTl."
        },
        {
            "heading": "6.3 Solving forgetting problem with RSALT",
            "text": "Through previous analysis, we see that SALT helps M to continue training on human-edit data or imitation-edit data. In Section 6.1.2 and 6.2.2, we observed that the traditional replay-based method cannot completely solve the catastrophic forgetting problem, so the performance of SALTx+RSALTl on Table 3 and 6 is still lower than M \u2019s performance if there are distribution differences.\nWe report the results of SALTx+RSALTl+u in Table 3 and 6. We find that SALTx+RSALTl+u does not have the forgetting problem when continuing to train with human-edit data. We attribute this result to the data augmentation that RSALT brings to the traditional replay-based method. RSALT not just reuses the seen data to prevent the model from forgetting the learned distribution but also uses the output generated by the model itself with SALT to expand the effective training data points further."
        },
        {
            "heading": "6.4 Preference Evaluation",
            "text": "In CC dataset, GPT4 (on 500 data points) ranks SALTl+u+RSALTl+u higher than other variations (SALTl and SALTl+u) and M . To verify the GPT ranking, we performed human evaluation on a\nsmaller set (25 data points). Human ranking agrees with the GPT4 ranking. In CCUser, GPT4 (on 500 data points) ranks SALTl+u higher than other variations, which is expected as SALTl+u+RSALTl+u is also trained on the replay dataset. Because of privacy reasons, we did not do the human evaluation on CCUser. In Appendix Table 12, we show the prompt used with GPT4 for ranking the summaries. We show all the MRR scores for different models in our work in Figure 1."
        },
        {
            "heading": "7 Discussion: SALT vs RLHF",
            "text": "First, we argue that Human Edits is a more natural way to collect feedback from users as they fix AI-generated text for their workflow to improve generation. Collecting other forms of feedback that are not directly tied to the user\u2019s workflow will not scale as much, this is especially true in domains requiring expert domain knowledge and with nuanced user goals. Considering the cost, time, and availability of the experts, it is important to collect HF from the expert\u2019s daily workflow.\nSecond, we experiment with Direct Preference Optimization (DPO) (Rafailov et al., 2023) to compare the difference between RLHF and SALT while using a human edit feedback dataset. The training setup of DPO and SALT are similar, they are trained directly on the human preference dataset without training explicit reward models. We use SAI as the rejected summary and SE as the chosen summary and calculate the DPO loss \u2013 LDPO, between them to train the model.\nlRatio\u03b8 = log \u03c0\u03b8(SE |U)\u2212 log \u03c0\u03b8(SAI |U) (9)\nlRatioref = log \u03c0ref (SE |U)\u2212 log \u03c0ref (SAI |U) (10)\nLDPO = \u2212log \u03c3(\u03b2 \u2217 (lRatio\u03b8 \u2212 lRatioref )) (11)\nwhere \u03b8 and ref are the current and original model parameters. Table 7 shows the performance of DPO\nfor \u03b2 = {0.1, 0.5} on GPT-210 (117M parameters), with Rouge, Meteor, and Reward Accuracy (Reward Acc) on the CCUser test dataset. Reward Accuracy used in DPO11 is the ratio of data points for which chosen reward > rejected reward.\nchosen reward = \u03b2 \u2217 (\u03c0\u03b8(SE |U) \u2212 \u03c0ref (SE |U)) (12) rejected reward = \u03b2 \u2217 ((\u03c0\u03b8(SAI |U) \u2212 \u03c0ref (SAI |U)) (13)\nWe find that DPO is better than SALTl which is just equivalent to likelihood training on SE . This is expected since DPO also uses SAI . However, DPO gets lower performance than SALTl+u. When we change hyper-parameter \u03b2 to get higher Reward Accuracy, others (ROUGE, and Meteor) degrade, and vice versa. We think this is because, DPO penalizes the entire rejected summary, which is not suitable for human edit feedback, because most words in SAI and SE are the same. DPO does not explicitly consider such cases, and hence, it might be difficult for DPO to learn an implicit reward through SAI and SE without using the fine-grained relationship between their tokens. It is interesting to see that Reward Accuracy is higher for SALT than DPO, even though the SALT loss function does not explicitly maximize chosen and rejected log probability like DPO.\nIt should be noted that DPO was developed for using comparisons and not human edit feedback. For human edits feedback, a straightforward way to improve DPO could be to modify the loss function to use only the \u201cnegative tokens\u201d in the rejected summary, which aligns with our SALT ideas."
        },
        {
            "heading": "8 Conclusion",
            "text": "In this work, we explore improving language models with Human Edits feedback, which can be collected scalably than others. Specifically, we propose the SALT training objective based on sequence alignment and unlikelihood training and show how to design Imitation Edits to reduce the need for expensive HF. We further show on human edits data, SALT performs better than a straightforward RLHF (DPO) approach.\n10We used GPT because, at the time of this paper, DPO is only implemented on decoder-only models in Hugging Face\n11https://huggingface.co/docs/trl/main/en/dpo trainer"
        },
        {
            "heading": "9 Limitations",
            "text": "In our experiments, we find that our method improves relatively smaller language models like T5. Due to the limitation of computational resources, we are not able to try our methods on larger language models. So we don\u2019t understand which HF (human feedback or human edit data) is better on LLMs. But like what we discussed in Section 1, Human-Edits have many unique advantages from an ML data point of view. Given that it\u2019s a natural way to collect feedback from users as they fix our AI-generated summaries for their workflow, many products in the industry can more easily use this HF approach and our SALT method to improve their text generation quality without too much extra effort. In addition, other HF methods should be explored more in various domains and models of various sizes so as to help the NLP community find the most suitable HF method in various scenarios.\nAnother point that has not been explored in this paper is LLM-in-the-loop. With the emergence of GPT3.5 and ChatGPT, LLM has shown a level close to or beyond human beings in many domains. In this paper, we did not use LLMs to conduct experiments similar to Human Edits (that is, treat the LLM as a human to modify SAI to get SE(LLM)). Ideally, this would provide better Imitation-Edits to reduce HF costs. In addition to time and resource constraints, as we discussed in Section 1, data privacy issues make it hard for many practitioners in the industry to input their data into these third-party APIs or service websites for related experiments. LLM-in-the-loop is undoubtedly a worthwhile next step in the future, and we will study how to deal with related data privacy issues. This will also be a problem to be solved for many other tasks in medical and other privacy-oriented domains.\nThe current implementation of our methods also has some room for improvement. Our code currently only tries one global sequence alignment algorithm, the Needleman-Wunsch Algorithm. In fact, there are many alternatives that can help the model improve in different aspects. For example, how to improve factuality during LM\u2019s summaries is one key topic for both NLP and BioNLP community (Tang et al., 2022; Abacha et al., 2023b; Chang et al., 2023). Some previous work exploring language models and knowledge has shown that insufficient knowledge may lead to factual errors (Petroni et al., 2019; Sung et al., 2021; Yao et al., 2022a,b). So we can limit the scope of se-\nquence alignment to the medical entities (Luo et al., 2022) or jargon (Kwon et al., 2022) to help the model focus more on important tokens during the training process to reduce hallucination further."
        },
        {
            "heading": "10 Ethics Statement",
            "text": "The methods related to unlikelihood training are very dependent on the quality of negative candidates. In this paper, we propose a very general framework to provide negative candidates, that is, to calculate the sequence alignment between SAI and Human-Edits or Imitation-Edits. There will be some potential problems in actual deployment: First of all, for Human-Edits, we don\u2019t know whether the user is modifying because of some kind of error in SAI or because of the user\u2019s personal preference. These two behaviors need to be distinguished in future research or actual deployment because the former data is more suitable for improving the problems of the model itself (such as some factual errors), and the latter data is more suitable for user-personalized training data. Secondly, whether for Human-Edits or Imitation-Edits, when a large number of complex Edits appear, the sequence alignment algorithm we currently use may not be able to get the correct negative candidates, resulting in rewards or penalties for wrong tokens. In the experiments in this paper, we use some filters to control the quality of the training data provided for unlikelihood training, but the reality will be very complicated. In addition to using similar filters in this paper, another solution is to directly track the users\u2019 changes as they edit the summary on the product, and the subsequent training steps will not change. But this will add a lot of extra overhead to the product engineering."
        },
        {
            "heading": "Acknowledgements",
            "text": "We thank the Abridge AI for CC and CCUser data, as well as the professionals who performed the human evaluations. In addition, we also thank UMass BioNLP Lab for producing and providing us with a large batch of publicly available GPT Edits data for related work 12."
        },
        {
            "heading": "A Appendix",
            "text": "A.1 SOAP Structure The SOAP (Subjective, Objective, Assessment, and Plan) structure is commonly used by providers (Podder et al., 2021).\n\u2217 The Chief Complaint section is a brief description of a patient\u2019s conditions and the reasons for the visit.\n\u2217 The Subjective section is a detailed report of the patient\u2019s current conditions, such as source, onset, and duration of symptoms, mainly based on the patient\u2019s self-report. This section usually includes a history of present illness and symptoms, current medications, and allergies.\n\u2217 The Objective section documents the results of physical exam findings, laboratory data, vital signs, and descriptions of imaging results.\n\u2217 The Assessment section typically contains medical diagnoses and reasons that lead to medical diagnoses. The assessment is typically based on the content of the chief complaint and the subjective and objective sections.\n\u2217 The Plan section addresses treatment plans based on the assessment.\nA.2 Implementation Details Due to data privacy issues, we cannot disclose our CC and CCUser datasets. But for the reproduction of our methods, in the Appendix, we also use two general domain summarization datasets, CNN/Daily Mail (CNN) (See et al., 2017) and Extreme Summarization (XSum) (Narayan et al., 2018) to test the imitation-edit experiments.\nThe summarization model used in this paper is based on the publicly available T5-small model13 and T5-large14. Note that the experimental results of our t5-large-based model are not real human edit feedback for the summaries it generates, because of some deployment and privacy issues, we can only collect the CCUser data (Human Edits) for t5samll-based-model-generated summaries via our mobile app. Therefore, we put t5-large-related results only in the appendix. All the results in Section 6.1 are for our t5-small-based model. But overall, the patterns and findings are consistent on both t5-small and t5-large.\n13https://huggingface.co/t5-small 14https://huggingface.co/t5-large\nIn this paper, we used \u2018-1\u2019 for wAI\u2212C 15 and 1 for wAI\u2212NC in Equations 1, 2, 5. We trained MCC on the annotated Clinician Conversations (CC) dataset for 10000 steps and MCNN on CNN data with 100000 steps (batch size of 8). We then initialized the CCUser models SALTx with M and trained them on 70 human-edit notes for 1000 steps (batch size of 8)16.\nIn Section 6.2.1 and A.3.2, we ran M on both CC and CNN datasets\u2019 training data (See et al., 2017) to get the AI generated summaries SAI , and we use the ground truth data as Imitation Edits on the seen data SI . We then initialized SALTx from MCC and MCNN separately and trained on corresponding Imitation Edits with 1000 steps. We used CC-eval and CNN-eval to evaluate the models\u2019 performance.\nIn Section 6.2.2 and A.3.3, we sampled 3000 CC test data summaries (11812 data in total), 3000 CNN test data (11490 data in total), and 3000 XSum test data (11334 data in total) as Imitation Edits on the unseen data since we don\u2019t have the unseen training data in these datasets. Similarly, we initialized SALTx from M and trained on Imi-\n15For wAI\u2212C , we used -1.2 for SALTli and -0.5 for SALTld , and all other SALTx and RSALTx use -1.\n16We did all the experiments with 1 NVIDIA Tesla P100 GPU - 16 GB memory, with Adam optimizer \u2013 betas=(0.9,0.999), epsilon=1e-08, learning rate=5e-05.\ntation Edits with 1000 steps. We took the remaining test data of CC-test, CNN-test, and XSumtest (Narayan et al., 2018) respectively as Imitation Edits evaluation data, and then used CC-eval and CNN-eval to evaluate the performance of the model in the original data.\nIn all our evaluations, we used a beam size of 4, no-repeat-ngram-size=2, and minimum length and maximum length of sentences were set as (10, 100). We used five different random seeds to sample training data for all our experiments, and the scores reported in the tables are the average of these random seeds.\nA.3 Imitation Edits Experiments\nA.3.1 Imitation Edits smoothing function\nAlthough both come from humans, SE and SI are fundamentally different in their relationship with SAI . The former is modified from SAI while humans generate the latter from scratch. Therefore, SE is directly dependent on SAI , but SI is not. Consequently, even though SE and SI are dependent on the same data as input, the differences between SAI and SI are likely to be larger than between SAI and SE . We can see this difference in the average percentage of changed tokens \u2013 1E\u2212C and 1I\u2212C is 1, the former (6.17%) is much lower than the latter (45.59%). Hence, after we do se-\nquence alignment between SI and SAI , we perform a two-step post-processing operation to ensure the training stability, First, we only penalize consecutive tokens (> 1) in SAI that are not aligned with SI , for eg., the 1AI\u2212NC (t) = [1, 0, 1, 1, 0, 0, 1], becomes [1, 1, 1, 1, 0, 0, 1], and the corresponding change is made to 1AI\u2212C (t). This smoothing is to reduce the impact of less important negative tokens, for e.g., punctuation and word plural, as they are more frequently present in such single negative tokens. On the contrary, consecutive negative tokens are more likely to represent important errors (e.g., hallucination and missing information). Second, we discard data with more than 60% of the tokens being 0 in the indicator function 1AI\u2212NC , which helps us to reduce the percentage of changed tokens from 45.59% to 19.07% with an acceptable amount of data lost (21.38%).\nA.3.2 Imitation Edits using seen data\nWe use the training data from two datasets, CC and CNN, to experiment with the effects of SALT and Imitation Edits on seen data. First, for the CC dataset, the results in Table 5 show that continu-\ning to use likelihood loss on the training dataset to train the already convergent M does not improve the performance and leads to overfitting. However, when we use SI as imitation-edit data and do SALT training on it with SAI , we can see an improvement. Second, we see similar results for the CNN dataset. Even though there is no performance degradation arising from overfitting for SALTl, doing SALT training with SI and SAI can improve the performance more than using just the likelihood training. These results show that we can get additional improvement on the model by continuing to train it with SALT on the seen dataset even if the model is already converged (on the seen/original training data). Third, different from previous human-edit results, SALTu of CC is better than SALTl+u. We think this is because M has started to overfit on CC data, so continuing to add likelihood to the original training data reduces the scores.\nA.3.3 Imitation Edits using unseen data We use a part of the test dataset (not used in evaluation) from CC, CNN and XSum to experiment with the effects of SALT and Imitation Edits on unseen data. In Table 6, we show three experimental results. In the first experiment, we take M (trained on CC-train) and train it with a part of the CC-test as the imitation-edit data with SALT. In the second experiment, we take M (trained on CNN-train) and train it with a part of the CNN-test as imitation-edit data. In the third experiment, we take M (trained on CNN-train) and train it with a part of the XSum-test as imitation-edit data. In the three experiments, we took the remaining test data of the CC-test, CNN-test, and XSum-test, respectively, to evaluate the model performance in new imitation-edit data and then used CC-eval and CNN-eval to evaluate the model performance in\nthe original data. In imitation-edit evaluation results (CCtest\u2212r, CNNtest\u2212r, XSumtest\u2212r) of Table 6, SALTl+u has better performance than the baseline method SALTl in all three experiments, which is consistent with our results using human-edit data in Table 3. In the original data evaluation results (CCeval, CNNeval) of Table 6, although there was no forgetting problem in the first two experiments, SALTl+u still has a higher score than the baseline model SALTl. In the third experiment, we successfully imitated the forgetting problem similar to CC and CCUser by using the distribution difference between CNN and XSum. Similar to the results in Table 3, SALTl+u can alleviate the forgetting problem to a certain extent while improving the performance on the new dataset.\nA.4 More Discussion Why does SALT work? First, SALT makes good use of the SAI data. From the perspective of data augmentation, SE provides a new ground truth summary from the user, and the users also verify the remaining tokens in SAI . SALT helps the model to use all the tokens in both SE and SAI , which greatly improves the utilization of humanedit data. Second, SALT gives the model more objectives. Using SAI in SALT makes the model not just \u201cbe close to the correct distribution\u201d as in the likelihood training, but also \u201cbe far away from a negative distribution\u201d. Thus, we can teach the model to avoid making the same mistakes again, which has a special meaning for the user (Assumption 1).\nHuman Edits and Imitation Edits Even though SALT can be used with human-edit data or imitation-edit data to improve the summarization models, our experiments are not enough to conclude that Imitation Edits can completely replace\nHuman Edits. Using Imitation Edits is essentially a kind of data augmentation method during training. But, when we have edits to our model\u2019s original output from our real users, we have the unique opportunity to improve model output according to their individual expectations. SALT can model such information during the training and help the model have more appropriate behaviors to serve the users better in a more data-efficient way.\nSALT and RLHF We discuss SALT and DPO in the Section 7. Regarding the relationship between SALT and other RLHFs, we have some preliminary discussions here, and they need follow-up work to demonstrate. It seems that SALT keeps most of the advantages and disadvantages of DPO against\nPPO. Often, no reinforcement learning means more stable and easy training (and hyper-tuning). Our human eval also shows that SALT can make models more aligned with human preference without explicit reward models, which is the same with DPO. Also, it\u2019s questionable whether a good explicit reward model can be learned from SAI and SE since it\u2019s not as easy as positive or negative movie reviews to distinguish. For limitation, How does the SALT model generalize out of distribution, compared with PPO with an explicit reward function? For example, standard RLHF methods can leverage additional unlabeled prompts by labeling LM generations with the learned reward model. Can training with self-labeling from the SALT similarly make effective use of unlabeled prompts? Other papers like RAFT and RRHF use explicit reward models to filter high-score data points for SFT. Whether we can train a good reward model as a good filter is also a big question here. Another difference is that we will make full use of all data points (SAI + SE) during the training, but they will only use high-quality ones (SE) and discard the rest (SAI ). So theoretically, we use data more efficiently and model more information from SAI ."
        }
    ],
    "title": "Improving Summarization with Human Edits",
    "year": 2023
}