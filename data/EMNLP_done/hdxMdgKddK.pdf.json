{
    "abstractText": "The use of visually-rich documents (VRDs) in various fields has created a demand for Document AI models that can read and comprehend documents like humans, which requires the overcoming of technical, linguistic, and cognitive barriers. Unfortunately, the lack of appropriate datasets has significantly hindered advancements in the field. To address this issue, we introduce DOCTRACK, a VRD dataset really aligned with human eye-movement information using eye-tracking technology. This dataset can be used to investigate the challenges mentioned above. Additionally, we explore the impact of human reading order on document understanding tasks and examine what would happen if a machine reads in the same order as a human. Our results suggest that although Document AI models have made significant progress, they still have a long way to go before they can read VRDs as accurately, continuously, and flexibly as humans do. These findings have potential implications for future research and development of Document AI models. The data is available at https://github.com/hint-lab/doctrack.",
    "authors": [
        {
            "affiliations": [],
            "name": "Hao Wang"
        },
        {
            "affiliations": [],
            "name": "Qingxuan Wang"
        },
        {
            "affiliations": [],
            "name": "Yue Li"
        },
        {
            "affiliations": [],
            "name": "Changqing Wang"
        },
        {
            "affiliations": [],
            "name": "Chenhui Chu"
        },
        {
            "affiliations": [],
            "name": "Rui Wang"
        }
    ],
    "id": "SP:4c4f4600b1d351902bc5332264c124bffc992069",
    "references": [
        {
            "authors": [
                "Maria Barrett",
                "Joachim Bingel",
                "Nora Hollenstein",
                "Marek Rei",
                "Anders S\u00f8gaard."
            ],
            "title": "Sequence classification with human attention",
            "venue": "Proceedings of the 22nd Conference on Computational Natural Language Learning, CoNLL 2018, Brussels, Bel-",
            "year": 2018
        },
        {
            "authors": [
                "Maria Barrett",
                "Joachim Bingel",
                "Frank Keller",
                "Anders S\u00f8gaard."
            ],
            "title": "Weakly supervised part-ofspeech tagging using eye-tracking data",
            "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016, August 7-",
            "year": 2016
        },
        {
            "authors": [
                "Nilavra Bhattacharya",
                "Somnath Rakshit",
                "Jacek Gwizdka",
                "Paul Kogut."
            ],
            "title": "Relevance prediction from eye-movements using semi-interpretable convolutional neural networks",
            "venue": "CHIIR \u201920: Conference on Human Information Interaction and Re-",
            "year": 2020
        },
        {
            "authors": [
                "Michelangelo Ceci",
                "Margherita Berardi",
                "Donato Malerba."
            ],
            "title": "Relational data mining and ILP for document image understanding",
            "venue": "Appl. Artif. Intell., 21(4&5):317\u2013342.",
            "year": 2007
        },
        {
            "authors": [
                "Michael Collins",
                "Philipp Koehn",
                "Ivona Ku\u010derov\u00e1."
            ],
            "title": "Clause restructuring for statistical machine translation",
            "venue": "Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL\u201905), pages 531\u2013540, Ann Arbor, Michi-",
            "year": 2005
        },
        {
            "authors": [
                "Xiao Ding",
                "Bowen Chen",
                "Li Du",
                "Bing Qin",
                "Ting Liu."
            ],
            "title": "CogBert: Cognition-guided pre-trained language models",
            "venue": "Proceedings of the 29th International Conference on Computational Linguistics, COLING 2022, Gyeongju, Republic of Korea, Oc-",
            "year": 2022
        },
        {
            "authors": [
                "JM Henderson",
                "F Ferreira."
            ],
            "title": "Eye movement control during reading: fixation measures reflect foveal but not parafoveal processing difficulty",
            "venue": "Canadian journal of experimental psychology = Revue canadienne de psychologie experimentale,",
            "year": 1993
        },
        {
            "authors": [
                "Nora Hollenstein",
                "Ce Zhang."
            ],
            "title": "Entity recognition at first sight: Improving NER with eye movement information",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Lan-",
            "year": 2019
        },
        {
            "authors": [
                "Yupan Huang",
                "Tengchao Lv",
                "Lei Cui",
                "Yutong Lu",
                "Furu Wei."
            ],
            "title": "Layoutlmv3: Pre-training for document AI with unified text and image masking",
            "venue": "MM \u201922: The 30th ACM International Conference on Multimedia, Lisboa, Portugal, October 10 - 14,",
            "year": 2022
        },
        {
            "authors": [
                "Guillaume Jaume",
                "Hazim Kemal Ekenel",
                "JeanPhilippe Thiran."
            ],
            "title": "FUNSD: A dataset for form understanding in noisy scanned documents",
            "venue": "2nd International Workshop on Open Services and Tools for Document Analysis, OST@ICDAR 2019, Sydney,",
            "year": 2019
        },
        {
            "authors": [
                "Varun Khurana",
                "Yaman Kumar",
                "Nora Hollenstein",
                "Rajesh Kumar",
                "Balaji Krishnamurthy."
            ],
            "title": "Synthesizing human gaze feedback for improved NLP performance",
            "venue": "Proceedings of the 17th Conference of the European Chapter of the Association for",
            "year": 2023
        },
        {
            "authors": [
                "Sung-Dong Kim."
            ],
            "title": "Syntactic category prediction for improving translation quality in english-korean machine translation",
            "venue": "Proceedings of the 23rd Pacific Asia Conference on Language, Information and Computation, PACLIC 23, Hong Kong, China, De-",
            "year": 2009
        },
        {
            "authors": [
                "Chen-Yu Lee",
                "Chun-Liang Li",
                "Chu Wang",
                "Renshen Wang",
                "Yasuhisa Fujii",
                "Siyang Qin",
                "Ashok Popat",
                "Tomas Pfister."
            ],
            "title": "ROPE: Reading order equivariant positional encoding for graph-based document information extraction",
            "venue": "Proceedings of the 59th",
            "year": 2021
        },
        {
            "authors": [
                "Chenxia Li",
                "Ruoyu Guo",
                "Jun Zhou",
                "Mengtao An",
                "Yuning Du",
                "Lingfeng Zhu",
                "Yi Liu",
                "Xiaoguang Hu",
                "Dianhai Yu."
            ],
            "title": "Pp-structurev2: A stronger document analysis system",
            "venue": "CoRR, abs/2210.05391.",
            "year": 2022
        },
        {
            "authors": [
                "Yulin Li",
                "Yuxi Qian",
                "Yuechen Yu",
                "Xiameng Qin",
                "Chengquan Zhang",
                "Yan Liu",
                "Kun Yao",
                "Junyu Han",
                "Jingtuo Liu",
                "Errui Ding."
            ],
            "title": "Structext: Structured text understanding with multi-modal transformers",
            "venue": "MM \u201921: ACM Multimedia Conference,",
            "year": 2021
        },
        {
            "authors": [
                "Xiaojing Liu",
                "Feiyu Gao",
                "Qiong Zhang",
                "Huasha Zhao."
            ],
            "title": "Graph convolution for multimodal information extraction from visually rich documents",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computa-",
            "year": 2019
        },
        {
            "authors": [
                "Minesh Mathew",
                "Viraj Bagal",
                "Rub\u00e8n Tito",
                "Dimosthenis Karatzas",
                "Ernest Valveny",
                "C.V. Jawahar."
            ],
            "title": "Infographicvqa",
            "venue": "IEEE/CVF Winter Conference on Applications of Computer Vision, WACV 2022, Waikoloa, HI, USA, January 3-8, 2022, pages 2582\u2013",
            "year": 2022
        },
        {
            "authors": [
                "Abhijit Mishra",
                "Diptesh Kanojia",
                "Seema Nagar",
                "Kuntal Dey",
                "Pushpak Bhattacharyya."
            ],
            "title": "Leveraging cognitive features for sentiment analysis",
            "venue": "CoRR, abs/1701.05581.",
            "year": 2017
        },
        {
            "authors": [
                "Tetsuji Nakagawa."
            ],
            "title": "Efficient top-down BTG parsing for machine translation preordering",
            "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Pro-",
            "year": 2015
        },
        {
            "authors": [
                "Graham Neubig",
                "Taro Watanabe",
                "Shinsuke Mori."
            ],
            "title": "Inducing a discriminative parser to optimize machine translation reordering",
            "venue": "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational",
            "year": 2012
        },
        {
            "authors": [
                "K Rayner."
            ],
            "title": "Eye movements in reading and information processing: 20 years of research",
            "venue": "Psychological bulletin, 124(3):372422.",
            "year": 1998
        },
        {
            "authors": [
                "Yuqi Ren",
                "Deyi Xiong."
            ],
            "title": "Cogalign: Learning to align textual neural representations to cognitive language processing signals",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International",
            "year": 2021
        },
        {
            "authors": [
                "Ekta Sood",
                "Simon Tannert",
                "Philipp M\u00fcller",
                "Andreas Bulling."
            ],
            "title": "Improving natural language processing tasks with human gaze-guided neural attention",
            "venue": "Advances in Neural Information Processing Systems 33: Annual Conference on Neural In-",
            "year": 2020
        },
        {
            "authors": [
                "Ece Takmaz",
                "Sandro Pezzelle",
                "Lisa Beinborn",
                "Raquel Fern\u00e1ndez."
            ],
            "title": "Generating image descriptions via sequential cross-modal alignment guided by human gaze",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language",
            "year": 2020
        },
        {
            "authors": [
                "Jiapeng Wang",
                "Chongyu Liu",
                "Lianwen Jin",
                "Guozhi Tang",
                "Jiaxin Zhang",
                "Shuaitao Zhang",
                "Qianying Wang",
                "Yaqiang Wu",
                "Mingxiang Cai."
            ],
            "title": "Towards robust visual information extraction in real world: New dataset and novel solution",
            "venue": "Pro-",
            "year": 2021
        },
        {
            "authors": [
                "Zilong Wang",
                "Yiheng Xu",
                "Lei Cui",
                "Jingbo Shang",
                "Furu Wei."
            ],
            "title": "LayoutReader: Pre-training of text and layout for reading order detection",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 4735\u2013",
            "year": 2021
        },
        {
            "authors": [
                "Fei Xia",
                "Michael McCord."
            ],
            "title": "Improving a statistical MT system with automatically learned rewrite patterns",
            "venue": "COLING 2004: Proceedings of the 20th International Conference on Computational Linguistics, pages 508\u2013514, Geneva, Switzerland. COL-",
            "year": 2004
        },
        {
            "authors": [
                "Yang Xu",
                "Yiheng Xu",
                "Tengchao Lv",
                "Lei Cui",
                "Furu Wei",
                "Guoxin Wang",
                "Yijuan Lu",
                "Dinei A.F. Flor\u00eancio",
                "Cha Zhang",
                "Wanxiang Che",
                "Min Zhang",
                "Lidong Zhou."
            ],
            "title": "Layoutlmv2: Multi-modal pre-training for visually-rich document understanding",
            "venue": "Pro-",
            "year": 2021
        },
        {
            "authors": [
                "Yiheng Xu",
                "Minghao Li",
                "Lei Cui",
                "Shaohan Huang",
                "Furu Wei",
                "Ming Zhou."
            ],
            "title": "Layoutlm: Pretraining of text and layout for document image understanding",
            "venue": "KDD \u201920: The 26th ACM SIGKDD Conference on Knowledge Discovery and Data Min-",
            "year": 2020
        },
        {
            "authors": [
                "Victoria Yaneva",
                "Constantin Orasan",
                "Richard Evans",
                "Omid Rohanian."
            ],
            "title": "Combining multiple corpora for readability assessment for people with cognitive disabilities",
            "venue": "Proceedings of the 12th Work-",
            "year": 2017
        },
        {
            "authors": [
                "Wenwen Yu",
                "Ning Lu",
                "Xianbiao Qi",
                "Ping Gong",
                "Rong Xiao."
            ],
            "title": "PICK: Processing key information extraction from documents using improved graph learning-convolutional networks",
            "venue": "2020 25th International Conference on Pattern Recogni-",
            "year": 2021
        },
        {
            "authors": [
                "Yuechen Yu",
                "Yulin Li",
                "Chengquan Zhang",
                "Xiaoqiang Zhang",
                "Zengyuan Guo",
                "Xiameng Qin",
                "Kun Yao",
                "Junyu Han",
                "Errui Ding",
                "Jingdong Wang"
            ],
            "title": "2023a. Structextv2: Masked visual-textual prediction for document image pre-training",
            "year": 2023
        },
        {
            "authors": [
                "Yuechen Yu",
                "Yulin Li",
                "Chengquan Zhang",
                "Xiaoqiang Zhang",
                "Zengyuan Guo",
                "Xiameng Qin",
                "Kun Yao",
                "Junyu Han",
                "Errui Ding",
                "Jingdong Wang."
            ],
            "title": "Structextv2: Masked visual-textual prediction for document image pre-training",
            "venue": "The Eleventh Inter-",
            "year": 2023
        },
        {
            "authors": [
                "Junwei Zhang",
                "Hao Wang",
                "Xiangfeng Luo."
            ],
            "title": "Dual-VIE: Dual-level graph attention network for visual information extraction",
            "venue": "The 19th Pacific-Rim International Conference on Artificial Intelligence, pages 422\u2013434.",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "With the continuous development of information technology, our access to information is becoming increasingly diverse. Among the various formats, the proportion of visual information in daily documents such as tables, graphs, diagrams, etc., is on the rise (Ceci et al., 2007; Jaume et al., 2019). Therefore, effectively utilizing such visual information has become a hot research topic in the NLP research community and introduces a new challenge of understanding Visually-Rich Documents (VRDs) (Liu et al., 2019; Yu et al., 2021), which are documents that contain substantial visual components.\n\u2217Corresponding author\nIdentifying and understanding VRDs is a timeconsuming and laborious task due to their diversity and complexity (Wang et al., 2021a). Textual information alone is insufficient for extracting key information from diverse document types, necessitating a multimodal approach that considers the consistency and correlation of multiple modalities, including text, visual, and layout, through joint modeling. Examples of modern document AI models for VRD understanding include the LayoutLM series models (Xu et al., 2020, 2021; Huang et al., 2022) and StructText series models (Li et al., 2021; Yu et al., 2023a).\nWhile these models can obtain fine-grained multimodal document representations and achieve promising results in downstream VRD (VRD) understanding tasks, they lack the ability to generate a serialized input order from a given document\nthat fits into the Transformer architecture. As a result, they typically utilize simple rules, such as leftto-right or top-to-bottom, or directly use the input order generated by the OCR tool in the previous step (Lee et al., 2021) to serialize inputs. However, these input orders are quite different from the reading order that humans are used to (See Figure 1 for an example). The different input orders can significantly affect the performance of document AI models on the downstream document understanding tasks, which is often overlooked.\nTo address this issue, Wang et al. (2021b) construct the ReadingBank dataset containing the local priority order using the order of the XML source code of Word documents. However, it remains questionable whether this reading order is consistent with the actual human reading order and whether it is actually beneficial for machine comprehension tasks. Therefore, state-of-the-art Document AI models lack a deep understanding of spatial relationships and document structure, resulting in limited performance when dealing with VRDs, e.g., forms and infographics.\nTo this end, we propose DOCTRACK, a benchmark dataset containing various types of real-world documents aligned with human eyemovement information. Specifically, we propose a preordering pipeline to integrate human reading order within modern document AI models. The integration process occurs during inputting document contents after OCR parsing but before feeding them into the downstream task. We also explore different approaches to generate human-like reading orders, including default OCR tools, Zpattern, simple rules, and AI models that utilize multimodal information. Our objective is to evaluate the impact of different reading orders on downstream document comprehension tasks and identify the most effective reading order for existing multimodal document AI models. This provides insight into the similarities and differences between human and machine reading patterns.\nIn summary, this paper makes three contributions:\n1. We construct a benchmark dataset, namely DOCTRACK, aligned with human eye movement information. To our knowledge, DOCTRACK is the first human-annotated benchmark dataset for the purpose of research on VRD reading order generation.\n2. We investigate different human-like reading\norder generation methods, which refer to techniques for generating machine reading orders that mimic human reading patterns in VRDs. We explore various techniques for generating these reading orders and propose a practical preordering pipeline that leverages these generated reading orders to improve document understanding tasks.\n3. We conduct both intrinsic and extrinsic evaluations to analyze the performance of the human-like reading order generation model and measure its impact on downstream tasks. The observations suggest that human reading order may not be suitable for reading VRDs."
        },
        {
            "heading": "2 Human Reading Order",
            "text": "The human reading order plays a significant role in the comprehension of VRDs. Human reading order refers to the direction and sequential order in which people scan documents as they read the texts. Generally, people read left-to-right or top-tobottom to comprehend the text and obtain information. This order is also related to the way the text is written in most languages and scripts, including English and Modern Chinese. However, other reading order patterns exist. For example, traditional Chinese and Japanese are usually written vertically from right to left, while Arabic is written horizontally from right to left (Rayner, 1998). As a result, people naturally scan their eyes in this order while reading (Henderson and Ferreira, 1993).\nAlthough the reading order of text generally follows a relatively fixed pattern, VRDs usually present information in a mixture of modalities (e.g., text, images, graphics, etc.) with a complex two-dimensional layout and semantic structure. Readers typically choose the appropriate reading order by considering the spatial structure of text content, image information, and the typographic position of the document in combination. This natural temporal processing can help readers better understand the content and intent of the document and establish connections between different modalities. For example, when presented with a diagram, readers typically first visually analyze it as a whole to get a general idea of what it describes. From there, they usually focus on the most obvious data or labels first, and then gradually scan the rest of the data and labels to gain a more complete understanding of the information presented."
        },
        {
            "heading": "3 Eye Tracking in NLP",
            "text": ""
        },
        {
            "heading": "3.1 Basic Notions",
            "text": "Eye tracking is an important technique for studying eye movements during human reading. Human reading is a complex cognitive activity, and eye movement trajectories can visualize the reading process and are important for understanding how humans acquire and comprehend knowledge. Intuitively, different visual tasks result in varying scan (i.e., eye movement) patterns. Studying these patterns can help us understand the mechanisms of human cognitive processing. Numerous research in neuroscience has established a strong association between eye-tracking data and language comprehension activity in human brains (Henderson and Ferreira, 1993). The eye movement trajectory is typically described as an irregular curve, mainly composed of two alternating eye movement actions: saccade and fixation. A saccade is when our eyes rapidly jump from one word to the next in the text, representing the shift of attention. Fixation refers to the situation we sometimes focus on a word and remains stationary during the visual task for a period of time. In addition, the scan pattern and speed of the eye movement trajectory can be affected by various factors, such as font size, line spacing, contrast, etc. of the text."
        },
        {
            "heading": "3.2 Applications",
            "text": "In cognitive-motivated natural language processing (NLP), several studies have investigated the impact of eye-tracking data on NLP tasks, for example, designing machine learning models for NLP tasks such as part-of-speech tagging (Barrett et al., 2016), term extraction (Yaneva et al., 2017), and syntactic parsing (Kim, 2009). Later, researchers combine eye-tracking data with word embeddings in neural models to improve NLP tasks, including sentiment analysis (Mishra et al., 2017) and named entity recognition (NER) (Hollenstein and Zhang, 2019) or revising neural attention (Barrett et al., 2018; Sood et al., 2020; Takmaz et al., 2020). Recent studies (Bhattacharya et al., 2020; Ren and Xiong, 2021; Ding et al., 2022; Khurana et al., 2023) attempt to align text features and cognitive signals to identify their differences and commonalities."
        },
        {
            "heading": "4 Dataset",
            "text": ""
        },
        {
            "heading": "4.1 Data Collection",
            "text": "Our work aims to evaluate how reading order impacts the comprehension of VRDs. To achieve this, we randomly select documents mainly from three available datasets: FUNSD (Jaume et al., 2019), SeaBill (Zhang et al., 2022), and Infographic (Mathew et al., 2022). These datasets are widely used and provide diverse examples of complex structured and graphic documents for our analysis. We reuse all document images in\nthe FUNSD dataset (Jaume et al., 2019). Given that these documents are less structured than the other datasets and the eye movement tracks primarily to conform to the normal-Z reading order pattern, we rename this sub-dataset as \u201cWEAK.\u201d We have selected a number of structured tabular documents from the SEABILL dataset that contains detailed information related to the shipment of goods. This information includes the name of the consignor and consignee, the type and quantity of the shipment, etc. The subset comprises 160 training samples and 50 testing samples, totaling 13,454 semantic entities, denoted as \u201cSTRUCTURED.\u201d We also select 100 training samples and 30 test samples from the Infographic dataset (Mathew et al., 2022). This subset contains a large number of graphic components, it is more diverse and complicated than the other two subsets, called \u201cINFOGRAPH.\u201d Table 1 shows details of the dataset and statistics."
        },
        {
            "heading": "4.2 Analysis of Eye Movement Patterns",
            "text": "We observe mainly four types of reading patterns among these documents. Figure 2 illustrates human reading behaviors when reading the documents from the DOCTRACK dataset. Normal-Z. The normal-Z order, also called\n\u201czigzag pattern,\u201d refers to the typical eye movement pattern that occurs when people read pure text or weakly structured documents. This zigzag pattern is an efficient way for the human visual system to process and understand text. Specifically, our eyes move from left to right along a line of text and then jump back to the beginning of the next line during reading, creating a zigzag scan trajectory of eye movements. Figure 2(a) shows the diagram of eye movement sequences during human reading, which often resembles the shape of the letter \u201cZ\u201d. Local priority. Local-priority eye-movement patterns are a common cognitive strategy that humans use when dealing with the hierarchical layout structure of forms or tables. Usually, as shown in Figure 2(b), by prioritizing local information, we focus on the content inside a tabular cell first and then shift our attention to other elements around the cell while looking up in the tabular or a form document. This facilitates quick reading and retrieval of informative text to obtain the needed information. Cross-modal interaction. When people read infographics that contain pies, charts, or other types of data graphs, they typically show interactive radial eye movements. During reading, the eyes follow a radial path between the graph and the associated text. For example, when people read a pie chart, they usually focus first on the pie portion of the chart and then scan along its perimeter to find the corresponding text. This back-and-forth movement results in a cross-modal interaction pattern. The unique shape and layout structure of pie charts contributes to this pattern because the graphical portion often contains the most important information that people attend to first. In contrast, the text label section provides detailed information that people need to scan step by step in order to understand. Therefore, eye tracking presents a radial pattern in these types of infographics. Visual instruction. When reading flowcharts, an eye movement pattern characterized by backtracking often develops, as readers must understand and compare different texts in the boxes. Flowcharts commonly contain a great deal of information and detail presented in a hierarchical, logical manner. Readers typically focus first on the chart\u2019s overall structure and theme, then progress gradually to each detail. When encountering text that requires\ncontextual referencing, readers may re-read earlier sections to aid in better understanding the information\u2019s meaning. This self-correcting, backtracking eye movement pattern reflects the information processing of the reader and can help with better comprehension and memory retention of the information. Figure 3 shows the statistics of the four types of reading patterns found in DOCTRACK."
        },
        {
            "heading": "4.3 Eye-tracking Experiments",
            "text": "To conduct eye-tracking experiments, we randomly divide the dataset into five parts and recruit five participants.1 All participants are instructed to read the data on an HP 24-inch 1080P display. We record the participants\u2019 eye trajectories while they are reading using Tobii TX300 and Tobii Studio. These devices can record the visual movement trajectory of each subject and convert it into digital data. Data extracted from the eye tracker includes fixation points, fixation time, frequency, eye saccade distance, and pupil size. These measurements can help researchers gain insights into participants\u2019 internal cognitive processes.\nThe high sampling frequency of an eye tracker may result in an unsmooth trajectory with recorded gaze points. In addition, gaze points during eye movements may deviate or miss due to peripheral vision (see Appendix A.1). To improve the accuracy of the eye tracker when recording the reading trajectory, we do as follows:\n1. In the case of missing gaze points (case 1), if the gaze point hits the periphery of the known OCR bounding box within a certain Euclidean distance, we use the ordinal number of the peripheral gaze point as the reading sequence index of the current bounding box.\n2. In the case of missing gaze points (case 2), we use the ordinal number from the surround-\n1All participants are graduate or undergraduate students.\ning adjacent reading sequence or the ordinal number between two gaze points.\n3. We delete gaze points that are repeatedly returned to the eye multiple times and keep only the ordinal numbers of the first eye moment.\nBy adopting the above corrections, we ensure the accuracy of the recorded reading trajectory."
        },
        {
            "heading": "4.4 Annotation Agreement",
            "text": "In our final experimental setup, we assign two out of the five participants to label the same subset of data. Subsequently, for each document file, we compare the labeling results from these two participants. We then conduct a voting process among the five participants to select the most appropriate labelingdocument-levelthat aligns with everyone\u2019s expectations. This chosen labeling is ultimately considered as the final data."
        },
        {
            "heading": "5 Reading Order Integration",
            "text": ""
        },
        {
            "heading": "5.1 Rule-based Heuristic Methods",
            "text": "Multi-modal information extraction methods rely on accurate sequence detection of documents. However, inconsistencies in OCR engines can lead to variances in reading order. To address this issue, Li et al. (2022) introduce position offset threshold to standardize reading order and deal with OCR\u2019s instabilities. The text boxes are sorted from top to bottom, and if the distance between two boxes in the Y direction is smaller than the threshold, then their order is determined based on their X direction order. Besides, Gu et al. (2022) sort the bounding box according to the two coordinates of the Y -axis, and then performs the X-axis search and the right-down search with Y +X combined consideration. By changing the order of token input into the model, both approaches achieve good results. The proposed rule-based sorting approaches conform to the basic cognition of humans when reading, resulting in accurate information extraction. Therefore, leveraging the reading order generated by rule-based sorting approaches can significantly improve the accuracy of multimodal information extraction."
        },
        {
            "heading": "5.2 Modal-based Preordering Methods",
            "text": ""
        },
        {
            "heading": "5.2.1 Proposed Pipeline",
            "text": "To follow human reading behaviors, we use a process called \u201cpreordering\u201d (reordering-as-\npreprocessing), a term borrowed from the domain of statistical machine translation (Xia and McCord, 2004; Collins et al., 2005; Neubig et al., 2012; Nakagawa, 2015). This process involved reorganizing the inputs in the order they would be read by a human. By this, we make it easier for the reader to understand the procedure of aligning the multimodal sequential input features with human reading order and thus enabling us to evaluate the impact of reading order on the VRD understanding tasks."
        },
        {
            "heading": "5.2.2 Atomic Comparison Models",
            "text": "We obtain the basic features of different modalities of documents through different encoders and imitate human reading order according to these features. Therefore, we propose four different models for reading order generation. Each model uses information from single or multiple modalities simultaneously: Box, Text, Text+Box, and Text+Box+Image.\nEach model takes into account different factors that influence how humans prioritize and read elements in VRDs, including the position of the element, the text within the element, and the visual region associated with it. By utilizing these models, we can more accurately evaluate the impact of reading order on human comprehension of such documents.\np = f(bi : bj) =\n{ 0, if r[bi] < r[bj ]\n1, if r[bi] > r[bj ] (1)\nwhere bi and bj denote the i-th and j-th bounding boxes, respectively, r[bi] refers to the index ID of the bounding box bi in the predicted reading sequence, and r[bi] < r[bj ] indicates that bi appears before bj in the reading order sequence. Box. In this model, we use only two-dimensional\npositions to learn the order of each bounding box. Specifically, at first, the given bounding box coordinates (xup, yup, xdown, ydown) (representing the top-left and bottom-right coordinates) generated by the OCR tool are used to compute the centroid coordinates (xi, yi) of each bounding box. We combine all the bounding box centroid coordinates in pairs, then feed the centroid coordinates of the two combined bounding boxes directly into a multi-layer Transformer network. This enables us to predict the spatial relationship between the two bounding boxes, i.e., which one should come before or after in the human reading order.\nbi : bj = Transformer(xi, yi, xj , yj) (2)\nText. We use BERT to encode the texts, and since each bounding box contains one or more tokens, we take the latent representation at the first token position as the embedding for a bounding box.\nbi : bj = BERT(ti, tj) (3)\nText+Box. To jointly encode the text and 2Dpositions inputs, we use LayoutLM. Similar to the operation in the Text section, we take the first latent representation as the input to the classifier. The final model can be formulated as follows:\nbi : bj = LayoutLM(ti, tj ;xi, yi, xj , yj) (4)\nText+Box+Image. We use LayoutLMv2 to joint encode the text, image and 2D-position within the bounding boxes as follows:\nbi : bj = LayoutLMv2(ti, tj ;xi, yi, xj , yj ; Ii, Ij) (5)\nwhere we consider the ROIs of document images, denoted as Ii and Ij . We take the first latent encoding to represent each bounding box.\nAlgorithm 1 Model-based Sequenece Preordering Input: the serialized input sequence [b1, . . . , bl] that contains multimodal features ; Output: the sorted input sequence r;\nr\u2190 [b1, . . . , bl]; for i\u2190 0 to l \u2212 2 do\nfor j \u2190 0 to l \u2212 i\u2212 2 do p \u2190 f(rj : rj+1); \u25b7 Calling the atomic comparison model to determine the precedence order.\nif p < 0.5 then swap rj with rj+1; \u25b7 Consistent with Bubble sort.\nreturn r; \u25b7 New sorted sequence."
        },
        {
            "heading": "5.2.3 Sequence Preordering Algorithm",
            "text": "We test four atomic comparison models within the model-based sequence preordering algorithm as a before-and-after judgment. The preordering algorithm takes as input the atomic comparison models outputs (0/1 sequence) to construct an adjacent matrix. Thus, the preordering algorithm is a variant of the commonly used Bubble Sort algorithm, which outputs the sorted input sequence by rearranging the bounding box positions in the sequence. Refer to Algorithm 1 for more details."
        },
        {
            "heading": "6 Experiments and Analysis",
            "text": "To gain a deeper understanding of the differences between human reading order and machine processing order, we conduct both intrinsic and extrinsic evaluations using the DOCTRACK dataset. By comparing different modal fusions, the intrinsic evaluation allows us to assess the quality of the reading order generation model, which in turn helps us to understand the extent to which information from each modality influences the reading order built by humans. In order to determine ex-\nactly what reading order is needed for a machine document intelligence model, i.e., what input order enhances machine document comprehension, we make use of extrinsic evaluation to assess the quality of temporal order on human-like reading order generation."
        },
        {
            "heading": "6.1 Intrinsic Evaluation",
            "text": "Specifically, we compare machine-generated input orders with ground truths, i.e., human reading orders, to evaluate the model\u2019s performance in the intermediate task of reading order generation. We measure the correlation between the machine-generated input order and the reading order of human experts by calculating Spearman\u2019s rank and Kendall\u2019s tau scores to assess the accuracy of the machine-generated reading order and the interaction between different modalities and different types of documents.\nTable 2 shows the sequence correlation evaluation results of four different models on three datasets. Among them, the average result of the model based on Text+Box+Image is the best, and the result based on the bounding box is the worst, which also verifies multimodal features have an important impact on document ranking. The results in the above table show that humans read VRDs with weak table structure, and humans still use visual features such as font background to help model the temporal sequence, and visual features are less important in the case of table structure, and basic text and position are enough. However, infographics with more images and visual features require more powerful models that can capture text, location, and visual features. For more details see Appendix A.3."
        },
        {
            "heading": "6.2 Extrinsic Evaluation",
            "text": "Semantic entity recognition. The purpose of this study is to analyze the impact of reading order on the VRD understanding using the Semantic Entity Recognition (SER) task for the WEAK and STRUCTURED subset of documents. Table 3 shows the results. We find that the impact of reading order on the SER task varies depending on the document AI model used. When using BERT, the simple Z-order works best, and the effect of each order is better than the effect of the original order. In the multimodal LayoutLMv2 and LayoutLMv3 models, multimodal ordering works best, slightly better than Z-order. These results suggest that human reading order and machine ordering order have a strong influence on the SER task, and different models have different degrees of sensitivity to these orders.\nDQA. Document Question Answering (DQA) is a challenging task in VRD understanding that requires machines to understand both the visual and textual content of a document image and to answer questions about it. Table 3 lists the Average Normalized Levenshtein Similarity (ANLS) scores on the INFORGRAPH subset of text-only baseline BERT, layout-aware multimodal baselines LayoutLMv2 and LayoutLMv3. We observe that LayoutLMv2 and LayoutLMv3 models outperform text-only baselines (BERT) by a large margin. While integrating human reading order enhances the state-of-the-art document AI model in downstream tasks, it does not always outperform the human-like reading order generated using a rule-based approach. This suggests that true human reading order may not be necessary to enhance existing machine document AI models.\nThere are several possible reasons for this. First, most of the datasets used by existing document AI models are sorted by simple rules and therefore are better suited for the orders generated by using simple rules such as Z-pattern. Additionally, individual human reading orders may be very noisy unless a large human eye-movement dataset is constructed by collecting a significant amount of human eye-movement data."
        },
        {
            "heading": "7 Conclusion",
            "text": "We investigate the impact of human reading order on Document AI models for VRD understanding tasks. We propose different methods to generate human-like reading orders, along with a practical preordering pipeline that can leverage the generated reading orders. Our observations suggest that true human reading order may not always be suitable for reading VRDs. The dataset we construct can help in designing better document AI models and human reading robots in the future.\nLimitations\nIn this work, we focus on the impact of human eyetracking order and machine reading ordering for VRD understanding. Due to the complexity of eye movement characteristics, when the participants were doing eye movement experiments, they were required to ignore the eye movements information, such as the fixation time of each fixation point, back gaze, and the number of fixations. Therefore, our next step will be to explore the impact of more eye movement gaze information on the independent understanding of VRDs. In addition, due to the high annotation cost, the annotation has not been done by multiple annotators. Therefore, the inner-agreement rate is not available for the current dataset."
        },
        {
            "heading": "Acknowledgements",
            "text": "This work was supported by National Natural Science Foundation of China (Young Program: 62306173, General Program: 62176153), JSPS KAKENHI Program (JP23H03454), Shanghai Sailing Program (21YF1413900), Shanghai Pujiang Program (21PJ1406800), Shanghai Municipal Science and Technology Major Project (2021SHZDZX0102), the Alibaba-AIR Program (22088682), and the Tencent AI Lab Fund (RBFR2023012).\nEthics Statement\nThis study has received institutional ethics approval and complies with the Declaration of Helsinki and subsequent revisions. Informed consent was obtained from all participants before the study began, and they were informed that they had the right to withdraw from the study at any time. Personal identities are removed from the data to ensure anonymity. The participants have also approved eye-tracking data release for research purposes."
        },
        {
            "heading": "A Appendix",
            "text": "A.1 Visualization of Human Reading Order Figure 6 shows the missing gaze points during human reading and after smoothing.\nA.2 Human Reading Patterns Figure 7 shows four patterns of human reading orders.\nA.3 Missing Gaze Points Visualization Figure 8 shows the sequence of model generation and the sequence of human eye gaze."
        }
    ],
    "title": "DocTrack: A Visually-Rich Document Dataset Really Aligned with Human Eye Movement for Machine Reading",
    "year": 2023
}