{
    "abstractText": "We study how multilingual sentence representations capture European countries and occupations and how this differs across European languages. We prompt the models with templated sentences that we machine-translate into 12 European languages and analyze the most prominent dimensions in the embeddings. Our analysis reveals that the most prominent feature in the embedding is the geopolitical distinction between Eastern and Western Europe and the country\u2019s economic strength in terms of GDP. When prompted specifically for job prestige, the embedding space clearly distinguishes high and low-prestige jobs. The occupational dimension is uncorrelated with the most dominant country dimensions in three out of four studied models. The exception is a small distilled model that exhibits a connection between occupational prestige and country of origin, which is a potential source of nationality-based discrimination. Our findings are consistent across languages.",
    "authors": [
        {
            "affiliations": [],
            "name": "Jind\u0159ich Libovick\u00fd"
        }
    ],
    "id": "SP:94992a1dac30fb62c94664c99c9e8525eba2d9f4",
    "references": [
        {
            "authors": [
                "Jaimeen Ahn",
                "Hwaran Lee",
                "Jinhwa Kim",
                "Alice Oh."
            ],
            "title": "Why knowledge distillation amplifies gender bias and how to mitigate from the perspective of DistilBERT",
            "venue": "Proceedings of the 4th Workshop on Gender Bias in Natural Language Processing (GeBNLP),",
            "year": 2022
        },
        {
            "authors": [
                "Arnav Arora",
                "Lucie-Aim\u00e9e Kaffee",
                "Isabelle Augenstein."
            ],
            "title": "Probing Pre-trained Language Models for Cross-cultural Differences in Values",
            "venue": "CoRR, abs/2203.13722.",
            "year": 2022
        },
        {
            "authors": [
                "G\u00e1bor Bella",
                "Khuyagbaatar Batsuren",
                "Fausto Giunchiglia."
            ],
            "title": "A Database and Visualization of the Similarity of Contemporary Lexicons",
            "venue": "Text, Speech, and Dialogue - 24th International Conference, TSD 2021, Olomouc, Czech Republic, Septem-",
            "year": 2021
        },
        {
            "authors": [
                "Su Lin Blodgett",
                "Solon Barocas",
                "Hal Daum\u00e9 III",
                "Hanna Wallach."
            ],
            "title": "Language (technology) is power: A critical survey of \u201cbias\u201d in NLP",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5454\u2013",
            "year": 2020
        },
        {
            "authors": [
                "Alexis Conneau",
                "Kartikay Khandelwal",
                "Naman Goyal",
                "Vishrav Chaudhary",
                "Guillaume Wenzek",
                "Francisco Guzm\u00e1n",
                "Edouard Grave",
                "Myle Ott",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Unsupervised cross-lingual representation learning at scale",
            "venue": "Pro-",
            "year": 2020
        },
        {
            "authors": [
                "Pieter Delobelle",
                "Ewoenam Tokpo",
                "Toon Calders",
                "Bettina Berendt."
            ],
            "title": "Measuring fairness with biased rulers: A comparative study on bias metrics for pre-trained language models",
            "venue": "Proceedings of the 2022 Conference of the North American Chap-",
            "year": 2022
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
            "year": 2019
        },
        {
            "authors": [
                "Fangxiaoyu Feng",
                "Yinfei Yang",
                "Daniel Cer",
                "Naveen Arivazhagan",
                "Wei Wang."
            ],
            "title": "Language-agnostic BERT sentence embedding",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages",
            "year": 2022
        },
        {
            "authors": [
                "Katharina H\u00e4mmerl",
                "Bj\u00f6rn Deiseroth",
                "Patrick Schramowski",
                "Jindrich Libovick\u00fd",
                "Constantin A. Rothkopf",
                "Alexander Fraser",
                "Kristian Kersting."
            ],
            "title": "Speaking Multiple Languages Affects the Moral Bias of Language Models",
            "venue": "Findings of",
            "year": 2023
        },
        {
            "authors": [
                "Kevin Heffernan",
                "Onur \u00c7elebi",
                "Holger Schwenk."
            ],
            "title": "Bitext mining using distilled sentence representations for low-resource languages",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2022, pages 2101\u20132112, Abu Dhabi, United",
            "year": 2022
        },
        {
            "authors": [
                "Moin Nadeem",
                "Anna Bethke",
                "Siva Reddy."
            ],
            "title": "StereoSet: Measuring stereotypical bias in pretrained language models",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference",
            "year": 2021
        },
        {
            "authors": [
                "Nikita Nangia",
                "Clara Vania",
                "Rasika Bhalerao",
                "Samuel R. Bowman."
            ],
            "title": "CrowS-pairs: A challenge dataset for measuring social biases in masked language models",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language",
            "year": 2020
        },
        {
            "authors": [
                "Isabel Papadimitriou",
                "Kezia Lopez",
                "Dan Jurafsky."
            ],
            "title": "Multilingual BERT has an accent: Evaluating English influences on fluency in multilingual models",
            "venue": "Findings of the Association for Computational Linguistics: EACL 2023, pages 1194\u20131200, Dubrovnik,",
            "year": 2023
        },
        {
            "authors": [
                "Nils Reimers",
                "Iryna Gurevych."
            ],
            "title": "SentenceBERT: Sentence embeddings using Siamese BERTnetworks",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natu-",
            "year": 2019
        },
        {
            "authors": [
                "Nils Reimers",
                "Iryna Gurevych."
            ],
            "title": "Making monolingual sentence embeddings multilingual using knowledge distillation",
            "venue": "Proceedings of the",
            "year": 2020
        },
        {
            "authors": [
                "Victor Sanh",
                "Lysandre Debut",
                "Julien Chaumond",
                "Thomas Wolf."
            ],
            "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter",
            "venue": "CoRR, abs/1910.01108.",
            "year": 2019
        },
        {
            "authors": [
                "Patrick Schramowski",
                "Cigdem Turan",
                "Nico Andersen",
                "Constantin A. Rothkopf",
                "Kristian Kersting."
            ],
            "title": "Large pre-trained language models contain humanlike biases of what is right and wrong to do",
            "venue": "Nat. Mach. Intell., 4(3):258\u2013268.",
            "year": 2022
        },
        {
            "authors": [
                "Tom W. Smith",
                "Jaesok Son."
            ],
            "title": "Measuring Occupational Prestige on the 2012 General Social Survey, GSS Methodological Report No",
            "venue": "122. Technical report, NORC at the University of Chicago, Chicago, IL, USA.",
            "year": 2014
        },
        {
            "authors": [
                "Kaitao Song",
                "Xu Tan",
                "Tao Qin",
                "Jianfeng Lu",
                "TieYan Liu."
            ],
            "title": "MPNet: Masked and Permuted Pretraining for Language Understanding",
            "venue": "Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing",
            "year": 2020
        },
        {
            "authors": [
                "Jannis Vamvas",
                "Rico Sennrich."
            ],
            "title": "Contrastive conditioning for assessing disambiguation in MT: A case study of distilled bias",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 10246\u201310265, Online",
            "year": 2021
        },
        {
            "authors": [
                "Pranav Narayanan Venkit",
                "Sanjana Gautam",
                "Ruchi Panchanadikar",
                "Ting-Hao K. Huang",
                "Shomir Wilson."
            ],
            "title": "Nationality Bias in Text Generation",
            "venue": "Proceedings of the 17th Conference of the European Chapter of the Association for Computational Lin-",
            "year": 2023
        },
        {
            "authors": [
                "Yinfei Yang",
                "Daniel Cer",
                "Amin Ahmad",
                "Mandy Guo",
                "Jax Law",
                "Noah Constant",
                "Gustavo Hernandez Abrego",
                "Steve Yuan",
                "Chris Tar",
                "Yun-hsuan Sung",
                "Brian Strope",
                "Ray Kurzweil."
            ],
            "title": "Multilingual universal sentence encoder for semantic retrieval",
            "venue": "Proceedings",
            "year": 2020
        },
        {
            "authors": [
                "Da Yin",
                "Hritik Bansal",
                "Masoud Monajatipoor",
                "Liunian Harold Li",
                "Kai-Wei Chang."
            ],
            "title": "GeoMLAMA: Geo-diverse commonsense probing on multilingual pre-trained language models",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Nat-",
            "year": 2022
        }
    ],
    "sections": [
        {
            "text": "We study how multilingual sentence representations capture European countries and occupations and how this differs across European languages. We prompt the models with templated sentences that we machine-translate into 12 European languages and analyze the most prominent dimensions in the embeddings. Our analysis reveals that the most prominent feature in the embedding is the geopolitical distinction between Eastern and Western Europe and the country\u2019s economic strength in terms of GDP. When prompted specifically for job prestige, the embedding space clearly distinguishes high and low-prestige jobs. The occupational dimension is uncorrelated with the most dominant country dimensions in three out of four studied models. The exception is a small distilled model that exhibits a connection between occupational prestige and country of origin, which is a potential source of nationality-based discrimination. Our findings are consistent across languages."
        },
        {
            "heading": "1 Introduction",
            "text": "Language models and pre-trained representations in Natural Language Processing (NLP) are known to manifest biases against groups of people, including negative stereotypes connected to ethnicity or gender (Nangia et al., 2020; Nadeem et al., 2021). It has been extensively studied with monolingual models. Multilingual models, often used for model transfer between languages, introduce another potential issue: stereotypes of speakers of some languages can be imposed in other languages covered by the model.\nIn this case study, we try to determine the most prominent biases connected to European countries in multilingual sentence representation models. We adopt an unsupervised methodology (\u00a7 2) based on hand-crafted prompt templates and principle component analysis (PCA), originally developed to\nextract moral sentiments from sentence representation (Schramowski et al., 2022).\nOur exploration encompasses four sentence representation models across 13 languages (\u00a7 3). We find only minor differences between languages in the models. The results (\u00a7 4) show that the strongest dimension in all models correlates with the political and economic distinction between Western and Eastern Europe and the Gross Domestic Product (GDP). Prompting specifically for country prestige leads to similar results. When prompted for occupations, the models can distinguish between low and high-prestige jobs. In most cases, the extracted job-prestige dimension only loosely correlates with the country-prestige dimension. This result suggests that the models do not connect individual social prestige with the country of origin. The exception is a small model distilled from Multilingual Universal Sentence Encoder (Yang et al., 2020) that seems to mix these two and thus confirms previous work claiming that distilled models are more prone to biases (Ahn et al., 2022).\nThe source code for the experiments is available on GitHub.1"
        },
        {
            "heading": "2 Methodology",
            "text": "We analyze sentence representation models (\u00a7 2.1) using a generalization of the Moral Direction framework (\u00a7 2.2). We represent concepts (countries, jobs) using sets of templated sentences (\u00a7 2.3), for which we compute the sentence embeddings. Then, we compute the principal components of the embeddings and analyze what factors statistically explain the main principal component (\u00a7 2.4)."
        },
        {
            "heading": "2.1 Sentence Embeddings Models",
            "text": "Sentence-embedding models are trained to produce a single vector capturing the semantics of an en-\n1https://github.com/jlibovicky/ europe-in-sentence-embeddings\ntire sentence. Contextual embeddings trained via the masked-language-modeling objective (Devlin et al., 2019) capture subwords well in context; however, they fail to provide a sentence representation directly comparable across sentences. SentenceBERT (Reimers and Gurevych, 2019) approaches this problem by fine-tuning existing contextual embeddings using Siamese Networks on sentence classification tasks. As a result, sentences with similar meanings receive similar vector representation.\nThe issue of sentence representation also applies to multilingual contextual embeddings such as XLM-R (Conneau et al., 2020). In a multilingual setup, the additional requirement is that similar sentences receive similar representation regardless of the language. This is typically achieved using parallel data for knowledge distillation (Reimers and Gurevych, 2020; Heffernan et al., 2022) or more directly in a dual encoder setup (Feng et al., 2022)."
        },
        {
            "heading": "2.2 Embedding Analysis Method",
            "text": "We base our methodology on an unsupervised method for extracting semantic dimensions from sentence embeddings, originally introduced in the context of moral institutions (Schramowski et al., 2022). The original study analyzed the moral sentiment of English verb phrases using SentenceBERT.\nThe method consists of three steps:\n1. Generate templated sentences associating verbs with morality (e.g., \u201cYou should smile.\u201d, \u201cIt is good to smile.\u201d) and average them for each verb. I.e., there is one average sentence embedding per verb.\n2. The sentences are processed with SentenceBERT, and the representations are averaged for each phrase.\n3. Apply PCA over the representations.\nThe results show that the most significant dimension roughly corresponds to the moral sentiment of the phrases. They use multiple templates so that linguistic differences and potential verb connotations average out. Using templated sentences also eliminates linguistic diversity in the data. Because of that, the main principle component does capture linguistic differences but the most prominent semantic nuances across the verbs when used in the specific context of moral intuitions.\nWe extend this method to a more exploratory setup. We use a similar set of template sentences,\nputting countries and occupations in the context of prestige. We average their embeddings and get the main principle component using PCA. Then, using three different template sets, we analyse what the main principle component best correlates with."
        },
        {
            "heading": "2.3 Templating Sentences",
            "text": "Similar to H\u00e4mmerl et al. (2023), who extended the Moral Dimension framework to multilingual models, we use templates in English and machinetranslate the sentences into other languages after filling in the templates. We use three template sets. The sets consist of synonymous sentences with the following meaning:\n1. They come from [COUNTRY].\n2. Being from [COUNTRY] is considered prestigious.\n3. Working as [JOB] is considered prestigious.\nSee Appendix A for the complete template list. In the first set of templated sentences, we search for the general trend in how countries are represented. In the second set of sentences, we specifically prompt the model for country prestige to compare how general country representation correlates with assumed prestige. In the third case, we fit the PCA with templates containing job titles, i.e., the most prominent dimension captures job prestige according to the models. We apply the same projection to template representations related to country prestige from Set 2 (country prestige).\nCountries. We include all European countries of the size of at least Luxembourg and use their short names (e.g., Germany instead of the Federal Republic of Germany), which totals 40 countries. The list of countries is in Appendix A.3.\nLow- and high-prestige jobs. We base our list of low- and high-prestige jobs on a sociological study conducted in 2012 in the USA (Smith and Son, 2014). We manually selected 30 jobs for each category to avoid repetitions and to exclude USspecific positions. By using this survey, we also bring in the assumption that the European countries have approximately similar cultural distance from the USA. The complete list of job titles used is in Appendix A.2."
        },
        {
            "heading": "2.4 Evaluation",
            "text": "Interpreting the dominant dimension. For the analysis, we assign the countries with abstractive\nlabels based on geographical (location, mountains, seas), political (international organization membership, common history), and linguistic features (see Table 5 in the Appendix for a detailed list). The labels are not part of the templates.\nWe compute the Pearson correlation of the onehot indicator vector of the country labels with the extracted dominant dimension to provide an interpretation of the dimension (some also called point-biserial correlation). Finally, because creating a fixed unambiguous list of Western and Eastern countries is difficult and most criteria are ambiguous, we manually annotate if the most positively and negatively correlated labels correspond to the economic and political distinction between Eastern and Western Europe.\nIn addition, we compute the country dimension\u2019s correlation with the respective countries\u2019 GDP based on purchasing power parity in 2019, according to the World Bank.2\nCross-lingual comparison. We measure how the extracted dimensions correlate across languages. To explain where differences across languages come from, we compute how the differences correlate with the geographical distance of the countries where the languages are spoken, the GDP of the countries, and the lexical similarity of the languages (Bella et al., 2021).3"
        },
        {
            "heading": "3 Experimental Setup",
            "text": ""
        },
        {
            "heading": "3.1 Evaluated Sentence Embeddings",
            "text": "We experimented with diverse sentence embedding models, which were trained using different methods. We experimented with models available in the SentenceBERT repository and an additional model. The overview of the models is in Table 1.\nMultilingual MPNet was created by multilingual distillation from the monolingual English MPNet\n2https://ourworldindata.org/grapher/ gdp-per-capita-worldbank.\n3http://ukc.disi.unitn.it/index.php/ lexsim\nBase model (Song et al., 2020) finetuned for sentence representation using paraphrasing (Reimers and Gurevych, 2019). In the distillation stage, XLM-R Base (Conneau et al., 2020) was finetuned to produce similar sentence representations using parallel data (Reimers and Gurevych, 2020).\nDistilled mUSE is a distilled version of Multilingual Universal Sentence Encoder (Yang et al., 2020) that was distilled into Distill mBERT (Sanh et al., 2019). This model was both trained and distilled multilingually.\nLaBSE (Feng et al., 2022) was trained on a combination of monolingual data and parallel data with a max-margin objective for better parallel sentence mining combined with masked language modeling.\nXLM-R-XNLI is trained without parallel data using machine-translated NLI datasets (H\u00e4mmerl et al., 2023). The model is based on XLM-R Base but was finetuned using Arabic, Chinese, Czech, English, and German data following the SentenceBERT recipe (Reimers and Gurevych, 2019)."
        },
        {
            "heading": "3.2 Translating Templates",
            "text": "To evaluate the multilingual representations in more languages, we machine translate the templated text into 12 European languages: Bulgarian, Czech, German, Greek, Spanish, Finnish, French, Hungarian, Italian, Portuguese, Romanian, and Russian (and keep the English original). We selected languages for which high-quality machine translation systems are available on the Huggingface Hub. The models are listed in Appendix B."
        },
        {
            "heading": "4 Results",
            "text": "Aggregated results. The results aggregated over language are presented in Table 2. The detailed results per language are in the Appendix in Tables 6 and 7.\nWhen prompting the models for countries, the most prominent dimensions almost always separate the countries according to the political east-west axis, consistently across languages. This is further stressed by the high correlation of the country dimension with the country\u2019s GDP, which is particularly strong in multilingual MPNet and Distilled mUSE. When we prompt the models specifically for country prestige, the correlation with the country\u2019s GDP slightly increases.\nWhen we prompt the models for job prestige, they can distinguish high- and low-prestige jobs\nMul. Par. MPNet\ncsdeelenesfifrhuitptroru bgcsdeel enesfi fr huit pt ro\nDist. mUSE\ncsdeelenesfifrhuitptroru bgcsdeel enesfi fr huit pt ro\nLaBSE\ncsdeelenesfifrhuitptroru bgcsdeel enesfi fr huit pt ro\nXLM-R-NLI\ncsdeelenesfifrhuitptroru bgcsdeel enesfi fr huit pt ro\n0.2\n0.4\n0.6\n0.8\n1.0\nFigure 1: Cross-language correlation of the job-prestige dimension. Languages are coded using ISO 639-1 codes.\nwell (accuracy 85\u201393%). When we apply the same projection to prompts about countries, in most cases, the ordering of the countries is random. Therefore, we conclude that the models do not correlate job prestige and country of origin.\nThe only exception is Distilled mUSE, where the job-prestige dimension applied to countries still correlates with the country\u2019s GDP and the east-west axis. This is consistent with previous work showing that distilled student models exhibit more biases than model trained on authentic data (Vamvas and Sennrich, 2021; Ahn et al., 2022).\nDifferences between languages. Further, we evaluate how languages differ from each other.\nIn all models, the first PCA dimension from the job-prestige prompts separates low- and highprestige jobs almost perfectly. Nevertheless, multilingual MPNet and distilled mUSE show a relatively low correlation of the job dimension across languages (see Figure 1).\nFinally, we try to explain the correlation between languages by connecting them to countries where the languages are spoken. We measure how the correlation between languages correlates with the geographical distances of (the biggest) countries speaking the language, the difference in their GDP, and the lexical similarity of the languages. The results are presented in Table 3.\nFor all models except XLM-R-NLI, the lexical similarity of the languages is the strongest predictor. XLM-R-NLI, with low differences between languages, better correlates with geographical distances."
        },
        {
            "heading": "5 Related Work",
            "text": "Societal biases of various types in neural NLP models are widely studied, especially focusing on gender and ethnicity. The results of the efforts were already summarized in comprehensive overviews (Blodgett et al., 2020; Delobelle et al., 2022).\nNationality bias has also been studied. Venkit et al. (2023) show that GPT-2 associates countries of the global south with negative-sentiment adjectives. However, only a few studies focus on biases in how multilingual models treat different lan-\nguages. Papadimitriou et al. (2023) showed that in Spanish and Greek, mBERT prefers syntactic structures prevalent in English. Arora et al. (2022) and H\u00e4mmerl et al. (2023) studied differences in moral biases in multilingual language models, concluding there are some differences but no systematic trends. Yin et al. (2022) created a dataset focused on culturally dependent factual knowledge (e.g., the color of the wedding dress) and concluded it is not the case that Western culture propagates across languages."
        },
        {
            "heading": "6 Conclusions",
            "text": "We showed that all sentence representation models carry a bias that the most prominent feature of European countries is their economic power and affiliation to former geopolitical Western and Eastern blocks. In the models we studied, this presumed country prestige does not correlate with how the models represent the occupation status of people. The exception is Distilled mUSE, where the two correlate, which might lead to discrimination based on nationality.\nLimitations & Ethical Considerations\nThe validity for different cultures. The \u201cground truth\u201d for job prestige was taken from studies conducted in the USA. They might not be representative of other countries included in this case study. Given that all countries considered in this case study are a part of the so-called Global North, we can assume a certain degree of cultural similarity, which makes our results valid. However, our methodology is not guaranteed to generalize beyond the Western world.\nUnintended use. Some methods we use in the study might create a false impression that we have developed a scorer of job or country prestige. This is not the case. The correlations that we show in the Results section (\u00a7 4) do not guarantee the reliability of the scoring beyond the intended use in the study, which is an assessment of multilingual sentence representation models. Drawing any conclusions about occupation groups, nations, or individual people using the methods used in this study might have harmful consequences."
        },
        {
            "heading": "Acknowledgments",
            "text": "Many thanks to Tom\u00e1\u0161 Musil and Rudolf Rosa for discussing the methodology in this paper and to\nOndr\u030cej Du\u0161ek and Dominik Mach\u00e1c\u030cek for their comments on the paper draft.\nThis research was supported by the Charles University project PRIMUS/23/SCI/023."
        },
        {
            "heading": "A Templated texts",
            "text": "A.1 Sentence Templates Country of origin They are from [COUNTRY]. They come from [COUNTRY]. Their homeland in [COUNTRY]. They are natives of [COUNTRY]. They originate from [COUNTRY]. They were born in [COUNTRY]. Their country of birth is [COUNTRY]. They hail from [COUNTRY]. They grew up in [COUNTRY]. They are citizens of [COUNTRY]. [COUNTRY] is their country of origin. [COUNTRY] is where they are from. They were raised in [COUNTRY].\nA.1.1 Country prestige Being from [COUNTRY] is considered prestigious. Being from [COUNTRY] has high prestige. Being from [COUNTRY] is seen as a symbol of success. Being from [COUNTRY] is highly regarded. People from [COUNTRY] are looked upon with admiration. People from [COUNTRY] are highly esteemed. It is prestigious to be from [COUNTRY].\nJob prestige Being [JOB] is considered prestigious. Being [JOB] has high prestige. Working as [JOB] is seen as a symbol of success. Working as [JOB] is highly regarded. People who work as [JOB] are looked upon with admiration.\nPeople who work as [JOB] are highly esteemed. It is prestigious to work as [JOB].\nA.2 Low- and High-prestige jobs Low-profile jobs. a hotel chambermaid, a doorto-door salesman, a leaflet distributor, a janitor, a used car salesman, a bartender, a telephone operator, a carwash attendant, a cattle killer in a slaughtering plant, a dishwasher, a stockroom attendant, a box-folding-machine operator, a crushing-machine operator, a taxicab driver, a bicycle messenger, a salesperson in a hardware store, a street sweeper, a cashier in a supermarket, a pump operator, a railroad ticket agent, a desk clerk in a hotel, a cable TV installer, a sewing machine operator, a waiter in a restaurant, an assembly line worker, a shoeshiner, a ditch digger, an unskilled worker in a factory, a tire retreader, a dry cleaner\nHigh-profile jobs. a surgeon, a university professor, an architect, a lawyer, a priest, a banker, a school principal, an airline pilot, an economist, a network administrator, an air traffic controller, an author, a nuclear plant operator, a computer scientist, a psychologist, a pharmacist, a colonel in the army, a mayor of a city, a university president, a dentist, a fire department lieutenant, a high school teacher, a policeman, a software developer, an actor, a fashion model, a journalist, a musician in a symphony orchestra, a psychiatrist, a chemical engineer\nA.3 Countries We consider the following 40 countries (ordered by their ISO 3166-1 codes): Austria, Bosnia and Herzegovina, Belgium, Bulgaria, Belarus, Switzerland, the Czech Republic, Cyprus, Den-\nmark, Germany, Greece, Spain, Estonia, Finland, France, Hungary, Croatia, Ireland, Iceland, Italy, Latvia, Lithuania, Luxembourg, Netherlands, Moldova, Montenegro, North Macedonia, Malta, Norway, Portugal, Poland, Romania, Russia, Slovakia, Slovenia, Albania, Serbia, Sweden, Turkey, Ukraine, Great Britain, Kosovo.\nThe qualitative group labels we assign to the countries we use in the further analysis are in Table 5. The values reflect the world as in the training data (estimated pre-2021) for the models and, therefore, do not reflect recent events (i.e., Croatia is not listed among countries paying with Euro, and Finland is considered neutral)."
        },
        {
            "heading": "B Machine Translation Models",
            "text": "The machine translation models we used are listed in Table 4 While keeping default values for all decoding parameters."
        },
        {
            "heading": "C Detailed per-language results",
            "text": "The detailed per-language results are presented in Tables 6 and 7."
        }
    ],
    "title": "Is a Prestigious Job the same as a Prestigious Country? A Case Study on Multilingual Sentence Embeddings and European Countries",
    "year": 2023
}