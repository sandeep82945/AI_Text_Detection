{
    "abstractText": "Data augmentation techniques are widely used in low-resource automatic morphological inflection to overcome data sparsity. However, the full implications of these techniques remain poorly understood. In this study, we aim to shed light on the theoretical aspects of the prominent data augmentation strategy STEMCORRUPT (Silfverberg et al., 2017; Anastasopoulos and Neubig, 2019), a method that generates synthetic examples by randomly substituting stem characters in gold standard training examples. To begin, we conduct an information-theoretic analysis, arguing that STEMCORRUPT improves compositional generalization by eliminating spurious correlations between morphemes, specifically between the stem and the affixes. Our theoretical analysis further leads us to study the sampleefficiency with which STEMCORRUPT reduces these spurious correlations. Through evaluation across seven typologically distinct languages, we demonstrate that selecting a subset of datapoints with both high diversity and high predictive uncertainty significantly enhances the data-efficiency of STEMCORRUPT. However, we also explore the impact of typological features on the choice of the data selection strategy and find that languages incorporating a high degree of allomorphy and phonological alternations derive less benefit from synthetic examples with high uncertainty. We attribute this effect to phonotactic violations induced by STEMCORRUPT, emphasizing the need for further research to ensure optimal performance across the entire spectrum of natural language morphology.1",
    "authors": [
        {
            "affiliations": [],
            "name": "Farhan Samir"
        },
        {
            "affiliations": [],
            "name": "Miikka Silfverberg"
        }
    ],
    "id": "SP:91397ed281a1bf62f610e4d575cf6beddda41df7",
    "references": [
        {
            "authors": [
                "Ekin Aky\u00fcrek",
                "Afra Feyza Aky\u00fcrek",
                "Jacob Andreas."
            ],
            "title": "Learning to recombine and resample data for compositional generalization",
            "venue": "International Conference on Learning Representations.",
            "year": 2021
        },
        {
            "authors": [
                "Antonios Anastasopoulos",
                "Graham Neubig."
            ],
            "title": "Pushing the limits of low-resource morphological inflection",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natu-",
            "year": 2019
        },
        {
            "authors": [
                "Jacob Andreas."
            ],
            "title": "Good-Enough Compositional Data Augmentation",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7556\u20137566, Online. Association for Computational Linguistics.",
            "year": 2020
        },
        {
            "authors": [
                "Marco Baroni."
            ],
            "title": "Linguistic generalization and compositionality in modern artificial neural networks",
            "venue": "Philosophical Transactions of the Royal Society B, 375(1791):20190307. Publisher: The Royal Society.",
            "year": 2020
        },
        {
            "authors": [
                "faty",
                "Ekaterina Vylomova"
            ],
            "title": "UniMorph 4.0: Universal Morphology",
            "venue": "In Proceedings of the Thirteenth Language Resources and Evaluation",
            "year": 2022
        },
        {
            "authors": [
                "Balthasar Bickel",
                "Johanna Nichols"
            ],
            "title": "Exponence of selected inflectional formatives (v2020.3)",
            "venue": "The World Atlas of Language Structures Online. Zenodo",
            "year": 2013
        },
        {
            "authors": [
                "Ben Bogin",
                "Shivanshu Gupta",
                "Jonathan Berant."
            ],
            "title": "Unobserved local structures make compositional generalization hard",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 2731\u20132747, Abu",
            "year": 2022
        },
        {
            "authors": [
                "Jiaao Chen",
                "Derek Tam",
                "Colin Raffel",
                "Mohit Bansal",
                "Diyi Yang."
            ],
            "title": "An empirical survey of data augmentation for limited data learning in NLP",
            "venue": "Transactions of the Association for Computational Linguistics, 11:191\u2013211.",
            "year": 2023
        },
        {
            "authors": [
                "Tri Dao",
                "Albert Gu",
                "Alexander Ratner",
                "Virginia Smith",
                "Chris De Sa",
                "Christopher R\u00e9."
            ],
            "title": "A kernel theory of modern data augmentation",
            "venue": "Proceedings of the 36th International Conference on Machine Learning, ICML 2019, volume 97 of Proceedings",
            "year": 2019
        },
        {
            "authors": [
                "Bradley Efron",
                "Robert J Tibshirani."
            ],
            "title": "An introduction to the bootstrap",
            "venue": "CRC press.",
            "year": 1994
        },
        {
            "authors": [
                "Bryan Eikema",
                "Wilker Aziz."
            ],
            "title": "Is MAP decoding all you need? the inadequacy of the mode in neural machine translation",
            "venue": "Proceedings of the 28th International Conference on Computational Linguistics, pages 4506\u20134520, Barcelona, Spain (Online). Inter-",
            "year": 2020
        },
        {
            "authors": [
                "Steven Y. Feng",
                "Varun Gangal",
                "Jason Wei",
                "Sarath Chandar",
                "Soroush Vosoughi",
                "Teruko Mitamura",
                "Eduard Hovy."
            ],
            "title": "A survey of data augmentation approaches for NLP",
            "venue": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021,",
            "year": 2021
        },
        {
            "authors": [
                "Matt Gardner",
                "William Merrill",
                "Jesse Dodge",
                "Matthew Peters",
                "Alexis Ross",
                "Sameer Singh",
                "Noah A. Smith."
            ],
            "title": "Competency problems: On finding and removing artifacts in language data",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Nat-",
            "year": 2021
        },
        {
            "authors": [
                "Omer Goldman",
                "David Guriel",
                "Reut Tsarfaty."
            ],
            "title": "Un)solving Morphological Inflection: Lemma Overlap Artificially Inflates Models\u2019 Performance",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2:",
            "year": 2022
        },
        {
            "authors": [
                "Demi Guo",
                "Yoon Kim",
                "Alexander Rush."
            ],
            "title": "Sequence-level mixed sample data augmentation",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 5547\u20135552, Online. Association for Computa-",
            "year": 2020
        },
        {
            "authors": [
                "Shivanshu Gupta",
                "Sameer Singh",
                "Matt Gardner."
            ],
            "title": "Structurally diverse sampling for sampleefficient training and comprehensive evaluation",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2022, pages 4966\u20134979, Abu",
            "year": 2022
        },
        {
            "authors": [
                "Martin Haspelmath",
                "Matthew S Dryer",
                "David Gil",
                "Bernard Comrie."
            ],
            "title": "The world atlas of language structures",
            "venue": "OUP Oxford.",
            "year": 2005
        },
        {
            "authors": [
                "Martin Haspelmath",
                "Andrea Sims."
            ],
            "title": "Understanding Morphology",
            "venue": "Routledge.",
            "year": 2013
        },
        {
            "authors": [
                "Badr Jaidi",
                "Utkarsh Saboo",
                "Xihan Wu",
                "Garrett Nicolai",
                "Miikka Silfverberg."
            ],
            "title": "Impact of sequence length and copying on clause-level inflection",
            "venue": "Proceedings of the The 2nd Workshop on Multi-lingual Representation Learning (MRL), pages",
            "year": 2022
        },
        {
            "authors": [
                "Robin Jia",
                "Percy Liang."
            ],
            "title": "Data recombination for neural semantic parsing",
            "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 12\u201322, Berlin, Germany. Association for Computa-",
            "year": 2016
        },
        {
            "authors": [
                "Bari\u015f Kabak."
            ],
            "title": "Turkish vowel harmony",
            "venue": "The Blackwell companion to phonology, pages 1\u201324.",
            "year": 2011
        },
        {
            "authors": [
                "Najoung Kim",
                "Tal Linzen."
            ],
            "title": "COGS: A Compositional Generalization Challenge Based on Semantic Interpretation",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 9087\u20139105, Online. As-",
            "year": 2020
        },
        {
            "authors": [
                "Jordan Kodner",
                "Sarah Payne",
                "Salam Khalifa",
                "Zoey Liu."
            ],
            "title": "Morphological inflection: A reality check",
            "venue": "Annual Meeting of the Association for Computational Linguistics.",
            "year": 2023
        },
        {
            "authors": [
                "Angelika Kratzer",
                "Irene Heim."
            ],
            "title": "Semantics in generative grammar, volume 1185",
            "venue": "Blackwell Oxford.",
            "year": 1998
        },
        {
            "authors": [
                "Brenden M. Lake",
                "Marco Baroni."
            ],
            "title": "Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks",
            "venue": "Proceedings of the 35th International Conference on Machine Learning, ICML 2018, volume 80 of",
            "year": 2018
        },
        {
            "authors": [
                "William Lane",
                "Steven Bird."
            ],
            "title": "Bootstrapping techniques for polysynthetic morphological analysis",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6652\u20136661, Online. Association for Computational",
            "year": 2020
        },
        {
            "authors": [
                "Paul M.A. Lewis"
            ],
            "title": "Ethnologue : languages of the world",
            "year": 2009
        },
        {
            "authors": [
                "Ling Liu",
                "Mans Hulden."
            ],
            "title": "Can a Transformer Pass the Wug Test? Tuning Copying Bias in Neural Morphological Inflection Models",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Pa-",
            "year": 2022
        },
        {
            "authors": [
                "Katerina Margatina",
                "Giorgos Vernikos",
                "Lo\u00efc Barrault",
                "Nikolaos Aletras."
            ],
            "title": "Active Learning by Acquiring Contrastive Examples",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 650\u2013663, Online and",
            "year": 2021
        },
        {
            "authors": [
                "Sarah Moeller",
                "Ling Liu",
                "Changbing Yang",
                "Katharina Kann",
                "Mans Hulden."
            ],
            "title": "IGT2P: From interlinear glossed texts to paradigms",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages",
            "year": 2020
        },
        {
            "authors": [
                "Rafael M\u00fcller",
                "Simon Kornblith",
                "Geoffrey E Hinton."
            ],
            "title": "When does label smoothing help? volume 32",
            "venue": "Advances in Neural Information Processing Systems.",
            "year": 2019
        },
        {
            "authors": [
                "Saliha Muradoglu",
                "Mans Hulden."
            ],
            "title": "Eeny, meeny, miny, moe",
            "venue": "How to choose data for morphological inflection. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 7294\u20137303, Abu Dhabi, United",
            "year": 2022
        },
        {
            "authors": [
                "Inbar Oren",
                "Jonathan Herzig",
                "Jonathan Berant."
            ],
            "title": "Finding needles in a haystack: Sampling Structurallydiverse Training Sets from Synthetic Data for Compositional Generalization",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Lan-",
            "year": 2021
        },
        {
            "authors": [
                "Auli."
            ],
            "title": "fairseq: A Fast, Extensible Toolkit for Sequence Modeling",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations), pages 48\u201353, Minneapolis, Minnesota. As-",
            "year": 2019
        },
        {
            "authors": [
                "Barbara Partee"
            ],
            "title": "Compositionality",
            "venue": "Varieties of formal semantics, 3:281\u2013311.",
            "year": 1984
        },
        {
            "authors": [
                "Neil Rathi",
                "Michael Hahn",
                "Richard Futrell."
            ],
            "title": "An information-theoretic characterization of morphological fusion",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 10115\u201310120, Online and Punta Cana,",
            "year": 2021
        },
        {
            "authors": [
                "Farhan Samir",
                "Miikka Silfverberg."
            ],
            "title": "One wug, two wug+s transformer inflection models hallucinate affixes",
            "venue": "Proceedings of the Fifth Workshop on the Use of Computational Methods in the Study of Endangered Languages, pages 31\u201340, Dublin, Ireland.",
            "year": 2022
        },
        {
            "authors": [
                "Miikka Silfverberg",
                "Adam Wiemerslage",
                "Ling Liu",
                "Lingshuang Jack Mao."
            ],
            "title": "Data augmentation for morphological reinflection",
            "venue": "Proceedings of the CoNLL SIGMORPHON 2017 Shared Task: Universal Morphological Reinflection, pages 90\u201399, Van-",
            "year": 2017
        },
        {
            "authors": [
                "Swabha Swayamdipta",
                "Roy Schwartz",
                "Nicholas Lourie",
                "Yizhong Wang",
                "Hannaneh Hajishirzi",
                "Noah A. Smith",
                "Yejin Choi."
            ],
            "title": "Dataset Cartography: Mapping and Diagnosing Datasets with Training Dynamics",
            "venue": "Proceedings of the 2020 Conference on Em-",
            "year": 2020
        },
        {
            "authors": [
                "Alex Tamkin",
                "Dat Nguyen",
                "Salil Deshpande",
                "Jesse Mu",
                "Noah Goodman."
            ],
            "title": "Active learning helps pretrained models learn the intended task",
            "venue": "Advances in Neural Information Processing Systems.",
            "year": 2022
        },
        {
            "authors": [
                "Joy A Thomas",
                "Thomas M Cover."
            ],
            "title": "Elements of information theory",
            "venue": "John Wiley & Sons.",
            "year": 2006
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141 ukasz Kaiser",
                "Illia Polosukhin."
            ],
            "title": "Attention is all you need",
            "venue": "volume 30. Advances in Neural Information Processing Systems.",
            "year": 2017
        },
        {
            "authors": [
                "Shijie Wu",
                "Ryan Cotterell",
                "Mans Hulden."
            ],
            "title": "Applying the Transformer to Character-level Transduction",
            "venue": "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 1901\u20131907,",
            "year": 2021
        },
        {
            "authors": [
                "Zhengxuan Wu",
                "Christopher D. Manning",
                "Christopher Potts"
            ],
            "title": "Recogs: How incidental details of a logical form overshadow an evaluation of semantic interpretation",
            "year": 2023
        },
        {
            "authors": [
                "Michelle Yuan",
                "Hsuan-Tien Lin",
                "Jordan BoydGraber."
            ],
            "title": "Cold-start active learning through selfsupervised language modeling",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 7935\u20137948,",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Compositional mechanisms are widely believed to be the basis for human language production and comprehension (Baroni, 2020). These mechanisms\n1Our code is available at https://github.com/smfsamir/ understanding-augmentation-morphology.\ninvolve the combination of simpler parts to form complex concepts, where valid combinations are licensed by a recursive grammar (Kratzer and Heim, 1998; Partee et al., 1984). However, domain general neural architectures often fail to generalize to new, unseen data in a compositional manner, revealing a failure in inferring the data-generating grammar (Kim and Linzen, 2020; Lake and Baroni, 2018; Wu et al., 2023). This failure hinders these models from closely approximating the productivity and systematicity of human language.\nConsider the task of automatic morphological inflection, where models must learn the underlying rules of a language\u2019s morphoysyntax to produce the inflectional variants for any lexeme from a large lexicon. The task is challenging: the models must efficiently induce the rules with only a small human-annotated dataset. Indeed, a recent analysis by Goldman et al. (2022) demonstrates that even state-of-the-art, task-specific automatic inflection models fall short of a compositional solution: they perform well in random train-test splits, but struggle in compositional ones where they must inflect lexemes that were unseen at training time.\nNevertheless, there is reason for optimism. Several works have shown that automatic inflection models come much closer to a compositional solution when the human-annotated dataset is complimented by a synthetic data-augmentation procedure (Liu and Hulden, 2022; Silfverberg et al., 2017; Anastasopoulos and Neubig, 2019; Lane and Bird, 2020; Samir and Silfverberg, 2022), where morphological affixes are identified and attached to synthetic lexemes distinct from those in the training dataset (Fig. 2). However, little is understood about this prominent data augmentation method and the extent to which it can improve compositional generalization in neural word inflection. In this work, we seek to reveal the implicit assumptions about morpheme distributions made by this rule-based augmentation scheme, and analyze the\neffect of these assumptions on learning of crosslinguistically prevalent morphological patterns.\nTo this end, our work presents the first theoretical explanation for the effectiveness of compositional data augmentation in morphological inflection. Through an information-theoretic analysis (Section 3), we show that this method eliminates \u201cspurious correlations\u201d (Gardner et al., 2021) between the word\u2019s constituent morphemes, specifically between the stem (e.g., walk) and the inflectional affix (e.g., -ed). By removing these correlations, the training data distribution becomes aligned with concatenative morphology, where a word can be broken down into two independent substructures: the stem (identifying the lexeme) and the inflectional affixes (specifying grammatical function). This finding sheds light on why the method is widely attested to improve compositional generalization (Liu and Hulden, 2022), as concatenative morphological distributions are cross-linguistically prevalent (Haspelmath and Sims, 2013).\nWe go on to show, however, that the augmentation method tends towards removing all correlations between stems and affixes, whether spurious or not. Unfortunately, this crude representation of concatenative morphology, while reasonable in broad strokes, is violated in virtually all languages to varying degrees by long-distance phonological phenomena like vowel harmony and reduplication. Thus, our analysis demonstrates that while the method induces a useful approximation to concatenative morphology, there is still ample room for improvement in better handling of allomorphy and phonological alternations.\nBuilding on our theoretical analysis, we in-\n[Illustration from Anastasopoulos and Neubig (2019)]\nFigure 2: STEMCORRUPT: a data augmentation method, where the stem \u2013 aligned subsequences of length 3 or greater in the input and output \u2013 is mutated by substitution with random characters from the alphabet.\nvestigate whether it is possible to improve the sample-efficiency with which the data augmentation method induces probabilistic independence between stems and affixes. Specifically, we investigate whether we can use a small subset of the synthetic data to add to our training dataset. We find that selecting a subset that incorporates both high predictive uncertainty and high diversity (see Fig. 1) is significantly more efficient in removing correlations between stems and affixes, providing an improvement in sample-efficiency for languages where the morphological system is largely concatenative. At the same time, in accordance with our theoretical analysis, this selection strategy impairs performance for languages where phonological alternations are common.\nOur work contributes to a comprehensive understanding of a prominent data augmentation method from both a theoretical (Section 3) and practical standpoint (Section 4). Through our systematic analysis, we aim to inspire further research in the analysis and evaluation of existing compositional data augmentation methods (reviewed in Section 6), as well as the development of novel augmentation methods that can better capture cross-linguistic diversity in morphological patterns."
        },
        {
            "heading": "2 Preliminaries",
            "text": "In automatic morphological inflection, we assume access to a gold-standard dataset Dtrain with triples of \u27e8X,Y,T\u27e9, where X is the character sequence of the lemma, T is a morphosyntactic description (MSD), and Y is the character sequence of the inflected form.2 The goal is then to learn the distribution P (Y|X,T) over inflected forms conditioned\n2For example, <dog, dogs, N+PL>.\non a lemma and MSD . Generating a synthetic training dataset with STEMCORRUPT. For many languages, Dtrain is too small for models to learn and make systematic morphological generalizations. Previous work has found that generating a complementary synthetic dataset DSyntrain using a data augmentation technique can substantially improve generalization (Anastasopoulos and Neubig, 2019; Silfverberg et al., 2017, among others).\nThe technique, henceforth called STEMCORRUPT, works as follows: We identify the aligned subsequences (of length 3 or greater) between a lemma X and an inflected form Y, which we denote the stem.3 We then substitute some of the characters in the stem with random ones from the language\u2019s alphabet; Fig. 2. The STEMCORRUPT procedure has a hyperparameter \u03b8. It sets the probability that a character in the stem will be substituted by a random one; a higher value of \u03b8 (approaching 1) indicates a greater number of substitutions in the stem.4\nHow does STEMCORRUPT improve compositional generalization? Despite the widespread adoption of this technique for automatic morphological inflection and analysis, this fundamental question has heretofore remained unanswered. In the next section, we argue that STEMCORRUPT improves compositional generalization by removing correlations between inflectional affixes and the stems to which they are attached. By enforcing independence between these two substructures, STEMCORRUPT facilitates productive reuse of the inflectional affixes with other lexemes. That is, our theoretical argument shows that the effectiveness of the method arises from factoring the probability distribution to approximate concatenative morphology, a cross-linguistically prevalent strategy for word formation where words can be \u201cneatly segmented into roots and affixes\u201d (Haspelmath and Sims, 2013). We formalize and prove this in the following section."
        },
        {
            "heading": "3 STEMCORRUPT induces compositional structure",
            "text": "In this section, we analyze the ramifications of training on the synthetic training dataset generated by STEMCORRUPT (DSyntrain) and the humanannotated training dataset (Dtrain). Our analysis\n3The stem can thus be discontinuous; see Fig. 2. 4We use the implementation here, which sets \u03b8 = 0.5.\nfocuses on asymptotic behaviour, specifically, how the conditional distribution P (Y|X,T) evolves as we add more augmented data (|DSyntrain| \u2192 \u221e). Theorem 1. For all \u27e8X,Y,T\u27e9 datapoints in Dtrain, assume that X and Y share a stem Ystem that is non-empty, and let Yaffix be the remaining characters of Y. Let Xstem and Xaffix be analogously defined. Further, let DSyntrain be generated with STEMCORRUPT using \u03b8 = 1.5 Next, consider the data-generating probability distribution P (Y|X,T) over DSyntrain \u222a Dtrain. Then, as |DSyntrain| \u2192 \u221e, we have that P (Y|X,T) \u2261 P (Ystem,Yaffix|X,T) = P (Yaffix|Xaffix,T)P (Ystem|Xstem). Remark 1 (Concatenative compositionality). The augmentation method thus makes the model more effective at capturing concatenative morphological patterns, as the conditional probability distribution becomes factorized into a root generation component (P (Ystem|\u00b7)) and an affix generation component (P (Yaffix|\u00b7)). Crucially, this removes the potential for any spurious correlation between these two substructures.6\nRemark 2 (Stem-affix dependencies). While concatenation is a cross-linguistically prevalent strategy for inflection (Haspelmath and Sims, 2013), stems and affixes are rarely entirely independent. Therefore, enforcing complete independence between these structures is an overly strong constraint that STEMCORRUPT places on the training data. The constraint is consistently violated in Turkish, for example, where front-back vowel harmony constraints dictate that the vowels in the suffix share the same front/back feature as the initial vowel in the stem. This leads to forms like \u201cdalar\u0131n\u201d and pre-\n5That is, we substitute all characters in the stem. 6Additionally, this factorization is likely to reducing overfitting: The model effectively learns to minimize the negative log-likelihood of this simplified and factorized data distribution P (Ystem|\u00b7)P (Yaffix|\u00b7), rather than the complex joint distribution P (Ystem,Yaffix|\u00b7).\nvents forms like \u201cdalerin\u201d, \u201cdaler\u0131n\u201d, or \u201cdalarin\u201d (Kabak, 2011). In Section 5, we show that STEMCORRUPT regularly generates examples violating vowel harmony, and that this can undermine its effectiveness. Nevertheless, the empirical success of STEMCORRUPT demonstrates that the benefits of its concatenative bias outweigh its limitations. Remark 3 (Comparison to previous accounts). Our analysis provides a simple yet the most accurate characterization of STEMCORRUPT. Previous works have called STEMCORRUPT a beneficial \u201ccopying bias \u201d (Liu and Hulden, 2022; Jaidi et al., 2022) or a strategy for mitigating overgeneration of common character n-grams (Anastasopoulos and Neubig, 2019). However, our analysis demonstrates that neither of these characterizations are entirely accurate. First, the denotation of a \u201ccopying bias\u201d is only suggestive of the second factor in our statement P (Ystem|Xstem), and does not address the impact on affix generation. In contrast, our analysis shows that both stem and affix generation are affected. Furthermore, alleviating overfitting to common character sequences is also misleading, as it would suggest that STEMCORRUPT serves the same purpose as standard regularization techniques like label smoothing (M\u00fcller et al., 2019).7"
        },
        {
            "heading": "3.1 Proving the theorem",
            "text": "The proof of the theorem is straightforward with the following proposition (proved in Appendix B).\nProposition 1. As |DSyntrain| \u2192 \u221e, the mutual information between certain pairs of random variables declines:\n(i) I(Ystem;T) \u2192 0 (ii) I(Ystem;Xaffix) \u2192 0\n(iii) I(Yaffix;Ystem) \u2192 0 (iv) I(Yaffix;Xstem) \u2192 0\nProof of Theorem 1. By the definition of Y = YstemYaffix, we have that P (Y|X,T) \u2261 P (Ystem,Yaffix|X,T). Then, by the chain rule of probability, we have P (Yaffix|Ystem,X,T)P (Ystem|X,T). We first deconstruct the second factor. By Proposition 1 (i), we have that the second factor P (Ystem|X,T) = P (Ystem|X), since the\n7Our preliminary experiments did not support this position; compositional generalization performance was not sensitive to label smoothing, yet was significantly improved by STEMCORRUPT.\nstem is invariant with respect to the inflectional features. Then, by Proposition 1 (ii), we have that P (Ystem|X) = P (Ystem|Xstem,Xaffix) = P (Ystem|Xstem), since the stem is invariant with respect to the inflectional affix of the lemma.\nNext, we tackle the factor P (Yaffix|Ystem,X,T). By parts (iii) and (iv) of Proposition 1, we have that this can be simplified to P (Yaffix|Xaffix,T). Taken together, we have that P (Y|X,T) can be decomposed into P (Yaffix|Xaffix,T)P (Ystem|Xstem).\nInvestigating STEMCORRUPT\u2019s sample efficiency. So far, we have studied the behaviour of STEMCORRUPT through an asymptotic argument, demonstrating that in the infinite limit of |DSyntrain|, STEMCORRUPT enforces complete independence between stems and affixes. In doing so, it likely removes a number of spurious correlations between stems and affixes, thus providing a theoretical explanation for its attested benefit in improving compositional generalization. However, our theoretical analysis, while informative of the overall effect of STEMCORRUPT, says little about the sample efficiency of the method in practice.\nIndeed, recent studies in semantic parsing have demonstrated that sample efficiency can be greatly increased by strategically sampling data to overcome spurious correlations that hinder compositional generalization in non-IID data splits (Oren et al., 2021; Bogin et al., 2022; Gupta et al., 2022). In the following section, we examine whether strategic data selection can yield similar benefits in the context of typologically diverse morphological inflection."
        },
        {
            "heading": "4 Extracting sample-efficient training sets",
            "text": "from STEMCORRUPT\nProblem setup. We use the following problem setup from Oren et al. (2021) to investigate whether the sample-efficiency of STEMCORRUPT can be improved. Recall that we have a dataset Dtrain with gold triples of \u27e8X,Y,T\u27e9. Further, we have a synthesized dataset DSyntrain where |DSyntrain| \u226b |Dtrain|. Our goal is now to select D\u0302Syntrain \u2282 D Syn train so that training the model on Dtrain \u222a D\u0302Syntrain maximizes performance on a heldout compositional testing split."
        },
        {
            "heading": "4.1 Model and training",
            "text": "We start by training an inflection model M on the gold-standard training data, denoted as Dtrain. Fol-\nlowing Wu et al. (2021); Liu and Hulden (2022), we employ Transformer (Vaswani et al., 2017) for M. We use the fairseq package (Ott et al., 2019) for training our models and list our hyperparameter settings in Appendix A. We conduct all of our experiments with |Dtrain| = 100 gold-standard examples, adhering to the the low-resource setting for SIGMORPHON 2018 shared task for each language. We next describe the construction of D\u0302Syntrain."
        },
        {
            "heading": "4.2 Subset sampling strategies",
            "text": "Here, we introduce a series of strategies for sampling from DSyntrain oriented for improving compositional generalization. Broadly, we focus on selecting subsets that reflect either high structural diversity, high predictive uncertainty, or both, as these properties have been tied to improvements in compositional generalization in prior work on semantic parsing (e.g., Bogin et al., 2022), an NLP research area where compositionality is well studied. RANDOM. Our baseline sampling method is to construct D\u0302Syntrain by sampling from the large synthetic training data DSyntrain uniformly.\nUNIFORM MORPHOLOGICAL TEMPLATE (UMT). With this method, we seek to improve the structural diversity in our subsampled in synthetic training dataset D\u0302Syntrain. Training on diverse subset is crucial, as the SIGMORPHON 2018 shared task dataset is imbalanced in frequency of different morphosyntactic descriptions (MSDs).8 These imbalances can pose challenges to the model in generalizing to rarer MSDs. To incorporate greater structural diversity, we employ the templatic sampling process proposed by Oren et al. (2021). Specifically, we modify the distribution over MSDs to be closer to uniform in D\u0302Syntrain.\nFormally, we sample without replacement from the following distribution: q\u03b1(X,Y,T) = p(T) \u03b1/ \u2211 T p(T) \u03b1 where p(T) is the proportion of times that T appears in DSyntrain. We consider two cases: \u03b1 = 0 corresponds to sampling MSDs from a uniform distribution (UMT), while \u03b1 = 1 corresponds to sampling tags according to the empirical distribution over MSDs (EMT).\n8For example, the low-resource Georgian training dataset contains 9 instances of of nouns inflected for PL;ERG, but only one instance of a noun inflected for SG;ERG.\nHIGHLOSS. Next, we employ a selection strategy that selects datapoints that have high predictive uncertainty to the initial model M. Spurious correlations between substructures (like Ystem and T; Section 3) will exist in any dataset of bounded size (Gupta et al., 2022; Gardner et al., 2021), and we conjecture that selecting high uncertainty datapoints will efficiently mitigate these correlations.\nWe quantify the uncertainty of a synthetic datapoint in DSyntrain by computing the negative loglikelihood (averaged over all tokens in the the target Y) for each synthetic datapoint in DSyntrain. Next, we select the synthetic datapoints with the highest uncertainty and add them to D\u0302Syntrain.\nTo thoroughly demonstrate that incorporating predictive uncertainty is important for yielding training examples that counteract the spurious dependencies in the ground-truth training dataset, we benchmark it against another subset selection strategy LOWLOSS. With this method, we instead select synthetic datapoints that the model finds easy, i.e., those with the lowest uncertainty scores. We hypothesize this strategy will yield less performant synthetic training datasets, as it is biased towards selecting datapoints that corroborate rather than counteract the spurious correlations learned by M.\nUMT/EMT+ LOSS. Finally, we test a hybrid approach containing both high structural diversity and predictive uncertainty by combining UMT/EMT and HIGHLOSS. First, we sample an MSD T (according to the MSD distribution defined by UMT/EMT) and then select the most uncertain synthetic datapoint for that T."
        },
        {
            "heading": "5 Experiments and Results",
            "text": "Data. We use data from the UniMorph project (Batsuren et al., 2022), considering typological diversity when selecting languages to include. We aim for an evaluation similar in scope to Muradoglu and Hulden (2022). That is, broadly, we attempt to include types of languages that exhibit variation in inflectional characteristics such as inflectional synthesis of the verb, exponence, and morphological paradigm size (Haspelmath et al., 2005). Our selected languages can be seen in Fig. 4. We provide further information on the languages in Appendix C.\nObtaining a large synthetic dataset. In order to\nGenerating a compositional generalization test set. For generating test sets, we adopt the lemma-split approach of Goldman et al. (2022). Specifically, we use all available data from SIGMORPHON2018 for the target language, excluding any lexemes from the low setting since those were used to train the initial model M (Section 4.1). The remaining lexemes and their associated paradigms comprise our compositional generalization test set; see Fig. 3.\nPopulating D\u0302Syntrain. We evaluate the performance of all methods listed in Section 4 in selecting D\u0302Syntrain. We evaluated the performance of using D\u0302Syntrain of sizes ranging from 128 to 2048 examples, increasing the size by powers of 2."
        },
        {
            "heading": "5.1 Results",
            "text": "We demonstrate the results of each strategy for all languages, considering each combination of language, subset selection strategy, and |DSyntrain|, thus obtaining 35 sets of results. For each setting, we report the performance achieved over 6 different random initializations, along with their respective standard deviations. For brevity, we show the results for |D\u0302Syntrain| \u2208 {128, 512, 2048}; we include the expanded set of results (including {256, 1024}) in Appendix D. STEMCORRUPT improves compositional gener-\nalization. At a high level, we find that data augmentation brings substantial benefits for compositional generalization compared to models trained solely on the gold-standard training data Dtrain. Without STEMCORRUPT, the initial model M for every language achieves only single-digit accuracy, while their augmented counterparts perform significantly better. For instance, the best models for Georgian and Spanish achieve over 50% accuracy. These findings agree with those of Liu and Hulden (2022) who found that unaugmented Transformer models fail to generalize inflection patterns.\nWe also find that performance tends to increase as we add more synthetic data; the best models for every language are on the higher end of the |D\u0302Syntrain| sizes. This finding agrees with our theoretical results that the dependence between the stem (Ystem) and that of the inflectional affix (Yaffix) is weakened as we add more samples from STEMCORRUPT (Section 3; Proposition 1).\nEffective subsets have high diversity and predictive uncertainty. Our analysis reveals statistically significant differences between the subset selection strategies, highlighting the effectiveness of the hybrid approaches (UMT/EMT+LOSS) that consider both diversity and predictive uncertainty. Among the strategies tested, the UMT+LOSS method outperformed all others in approximately one-third of the 35 settings examined, as indicated in Figure 5 (left). The improvements achieved by the UMT+LOSS method over a random baseline were statistically significant (p < 0.05) according to a bootstrap percentile test (Efron and Tibshirani, 1994), as shown in Figure 5 (right). Moreover, our results also show that the EMT+LOSS strategy closely followed the UMT+LOSS approach,\nachieving the highest performance in a quarter of the cases. In contrast, the same strategies without the uncertainty component were much less effective. For instance, UMT never achieved the best performance in any combination of the languages and |D\u0302Syntrain| sizes, highlighting that selecting a diverse subset without factoring in predictive uncertainty is suboptimal.\nFurthermore, selecting datapoints based solely on high predictive uncertainty without considering diversity (HIGHLOSS) is an ineffective strategy, having the second lowest proportion of wins (Fig. 5, right). Empirically, we find this may be attributed to the HIGHLOSS strategy fixating on a particular MSD, as shown in Fig. 6, rather than exploring the full distribution of MSDs. The figure displays the frequency of the most commonly sampled morphosyntactic description for each of UMT+LOSS, RANDOM, and HIGHLOSS strategies. Across all languages, the HIGHLOSS method samples the most frequent tag much more often than the RANDOM and UMT+LOSS methods.9 \u201cEasy\u201d synthetic datapoints have low sample efficiency. As hypothesized, the datapoints with low uncertainty hurt performance. We attribute this to the LowLoss strategy selecting datapoints with a smaller number of substitutions to the stem. In Fig. 7, we show that the number of edits made to the stem \u2013 as measured by Levenshtein distance between the corrupted target sequence and the uncorrupted version \u2013 is strongly correlated with the uncertainty assigned to the synthetic datapoint across all languages. Moreover, the correlation between the number of edits and uncertainty\n9The reason that uncertainty estimates are higher for a given MSD is not entirely clear. In our investigation, we found a small correlation (\u03c1 = 0.15) between the morphosyntactic description frequency and uncertainty. However, there are likely other factors beyond frequency that contribute to higher uncertainty; for example, morphological fusion (Bickel and Nichols, 2013; Rathi et al., 2021).\nis higher than the correlation with other plausible factors driving uncertainty, namely stem length and target length.10 Overall, the lagging sample efficiency of LOWLOSS corroborates our theory; STEMCORRUPT is effective because it generates datapoints where the stem has no correspondence with the affix. LOWLOSS counteracts its effectiveness, as it is biased towards datapoints where spurious dependencies between the stem and affix are maintained.11 Selecting by high predictive uncertainty worsens performance when there are stem-affix dependencies. We found that the UMT+LOSS strategy improves performance for 5 out of 7 languages compared to the RANDOM baseline. The improvement ranges from 4.8 (Georgian) to small declines of \u22121.9 (Turkish) and \u22120.9 (Finnish). The declines for Finnish and Turkish are partly due to a mismatch between the generated synthetic examples and the languages\u2019 morphophonology. STEMCORRUPT will generate synthetic examples that violate vowel harmony constraints between the stem and the affix. For instance, it may replace a front vowel in the stem with a back one. As a result, UMT+LOSS will select such harmony-violating examples more often, since they have greater uncertainty to the initial model M (Section 4.1), resulting in the augmented model tending to violate the harmony restrictions more often. Indeed, for Turkish, the average uncertainty for synthetic examples violating vowel harmony (0.46) is significantly higher than those that adhere to vowel harmony\n10Target length is known to contribute to predictive uncertainty (Eikema and Aziz, 2020; Kim and Linzen, 2020), and stem length is a confounding factor of the levenshtein distance calculation.\n11The ineffectiveness of LOWLOSS also corroborates that STEMCORRUPT does not simply induce a \u201ccopying bias\u201d (Remark 3), since then we would expect all of the subset selection methods to perform similarly.\nWe provided empirical evidence that the gains are tempered by STEMCORRUPT\u2019s tendencies to violate stem-affix constraints in its synthetic training examples, such as vowel harmony constraints in Turkish. Thus, further work is needed to adapt or supplant STEMCORRUPT for languages where such long-range dependencies are commonplace. In doing so, the data selection strategies are likely to fetch greater gains in sample efficiency."
        },
        {
            "heading": "6 Related work",
            "text": "Compositional data augmentation methods. In general, such methods synthesize new data points by splicing or swapping small parts from existing training data, leveraging the fact that certain constituents can be interchanged while preserving overall meaning or syntactic structure. A common approach is to swap spans between pairs of datapoints when their surrounding contexts are identical (Andreas, 2020; Guo et al., 2020; Jia and Liang, 2016, inter alia). Recently, Aky\u00fcrek et al. (2021) extended this approach, eschewing rule-based splic-\ning in favour of neural network-based recombination. Chen et al. (2023) review more compositional data augmentation techniques, situating them within the broader landscape of limited-data learning techniques for NLP.\nExtracting high-value subsets in NLP training data. Oren et al. (2021); Bogin et al. (2022); Gupta et al. (2022) propose methods for extracting diverse sets of abstract templates to improve compositional generalization in semantic parsing. Muradoglu and Hulden (2022) train a baseline model on a small amount of data and use entropy estimates to select new data points for annotation, reducing annotation costs for morphological inflection. Swayamdipta et al. (2020) identify effective data subsets for training high-quality models for question answering, finding that small subsets of ambiguous examples perform better than randomly selected ones. Our work is also highly related to active-learning in NLP (Tamkin et al., 2022; Yuan et al., 2020; Margatina et al., 2021, inter alia); however we focus on selecting synthetic rather than unlabeled datapoints, and our experiments are geared towards compositional generalization rather than IID performance.\nCompositional data splits in morphological inflection. Assessing the generalization capacity of morphological inflections has proven a challenging and multifaceted problem. Relying on standard \u201cIID\u201d (Oren et al., 2021; Liu and Hulden, 2022) splits obfuscated (at least) two different manners in which inflection models fail to generalize compositionally.\nFirst, Goldman et al. (2022) uncovered that generalizing to novel lexemes was challenging for even state of the art inflection models. Experiments by Liu and Hulden (2022) however showed that the STEMCORRUPT method could significantly improve generalization to novel lexemes. Our work builds on theirs by contributing to understanding the relationship between STEMCORRUPT and lexical compositional generalization. Specifically, we studied the structure of the probability distribution that StemCorrupt promotes (Section 3), and the conditions under which it succeeds (Remark 1) and fails (Remark 2).\nSecond, Kodner et al. (2023) showed that inflection models also fail to generalize compositionally to novel feature combinations, even with agglutinative languages that have typically have a strong oneto-one alignment between morphological features and affixes. Discovering strategies to facilitate com-\npositional generalization in terms of novel feature combinations remains an open-area of research."
        },
        {
            "heading": "7 Conclusion",
            "text": "This paper presents a novel theoretical explanation for the effectiveness of STEMCORRUPT, a widelyused data augmentation method, in enhancing compositional generalization in automatic morphological inflection. By applying information-theoretic constructs, we prove that the augmented examples work to improve compositionality by eliminating dependencies between substructures in words \u2013 stems and affixes. Building off of our theoretical analysis, we present the first exploration of whether the sample efficiency of reducing these spurious dependencies can be improved. Our results show that improved sample efficiency is achievable by selecting subsets of synthetic data reflecting high structural diversity and predictive uncertainty, but there is room for improvement \u2013 both in strategic sampling strategies and more cross-linguistically effective data augmentation strategies that can represent long distance phonological alternations.\nOverall, NLP data augmentation strategies are poorly understood (Dao et al., 2019; Feng et al., 2021) and our work contributes to filling in this gap. Through our theoretical and empirical analyses, we provide insights that can inform future research on the effectiveness of data augmentation methods in improving compositional generalization."
        },
        {
            "heading": "8 Limitations",
            "text": "Theoretical analysis. We make some simplifying assumptions to facilitate our analysis in Section 3. First, we assume that the stem between a gold-standard lemma and inflected form X and Y is discoverable. This is not always the case; for example, with suppletive inflected forms, the relationship between the source lemma and the target form is not systematic. Second, we assume that all characters in the stem are randomly substituted, corresponding to setting the \u03b8 = 1 for STEMCORRUPT. This does not correspond to how we deploy STEMCORRUPT; the implementation provided by Anastasopoulos and Neubig (2019) sets \u03b8 = 0.5 and we use this value for our empirical analysis Section 5. We believe the analysis under \u03b8 = 1 provides a valuable and accurate characterization of STEMCORRUPT nonetheless and can be readily extended to accommodate the 0 < \u03b8 < 1 case in future work.\nEmpirical analysis. In our empirical analysis, we acknowledge two limitations that can be addressed in future research. First, we conduct our experiments using data collected for the SIGMORPHON 2018 shared task, which may not contain a naturalistic distribution of morphosyntactic descriptions since it was from an online database (Wiktionary). In future work, we aim to replicate our work in a more natural setting such as applications for endangered language documentation (Muradoglu and Hulden, 2022; Moeller et al., 2020), where the morphosyntactic description distribution is likely to be more imbalanced. Second, we perform our analyses in an extremely data-constrained setting where only 100 gold-standard examples are available. In higher resourced settings, data augmentation with STEMCORRUPT may provide a much more limited improvement to compositional generalization; indeed the compositional generalization study of morphological inflection systems by Goldman et al. (2022) demonstrates that the disparity between IID generalization and compositional generalization largely dissipates when the model is trained on more gold-standard data."
        },
        {
            "heading": "9 Acknowledgements",
            "text": "We thank the EMNLP reviewers, the area chair, Vered Shwartz, Kat Vylomova, and Adam Wiemerslage for helpful discussion and feedback on the manuscript. We also thank Ronak D. Mehta for insightful discussion on properties of mutual information. This work was supported by an NSERC PGS-D scholarship to the first author."
        },
        {
            "heading": "A Transformer training details",
            "text": "Batch size 16 Label smoothing 0.2 Warmup updates 4000 Total updates 6000 Dropout 0.3 Encoder layers 4 Decoder layers 4 Attention dropout 0.1 Adam (\u03b21, \u03b22) (0.9, 0.999)"
        },
        {
            "heading": "B Proof of Proposition 1",
            "text": "We recall proposition 1:\nProposition 1. As |DSyntrain| \u2192 \u221e, the mutual information between certain pairs of random variables declines:\n(i) I(Ystem;T) \u2192 0 (ii) I(Ystem;Xaffix) \u2192 0\n(iii) I(Yaffix;Ystem) \u2192 0 (iv) I(Yaffix;Xstem) \u2192 0\nOur proof hinges on the fact that mutual information I(X;Y ) is convex in the conditional distribution P (Y |X) when the marginal distribution P (X) is held constant, due to Thomas and Cover (2006).\nTheorem 2 (Thomas & Cover). Let (X,Y ) \u223c p(x, y) = p(x)p(y|x). The mutual information I(X;Y ) is a concave function of p(x) for fixed p(y|x) and a convex function of p(y|x) for fixed p(x).\nThis theorem is useful for our argument since the data augmentation algorithm results in the marginal distribution over some random variables being affected (namely Ystem,Xstem) and other marginals staying fixed (T,Xaffix,Yaffix). This enables us to invoke the latter half of the theorem (\u201cconvex function of p(y|x) for fixed p(x)\u201d) and thus obtain an upper bound on the mutual information between the pairs of variables stated in proposition 1. We will argue that this upper bound will decline to 0 as we take |DSyntrain| \u2192 \u221e, and thus the mutual information must also decline to 0.\nProof. Let IG := I(T;Ystem) be the mutual information between the random variables T and Ystem in the human annotated dataset Dtrain, where T is generated from some distribution P (T) and Ystem be generated from PG(Ystem|T).\nLet IA := I(T;Ystem) be the mutual information between the random variables T and Ystem in the synthetic dataset DSyntrain, where T is generated from P (T) (as before) and Ystem is generated from PA(Ystem|T). The data augmentation algorithm generates the stem characters by uniformly sampling characters the a language\u2019s alphabet. Crucially, this means the mutual information IA = 0, since the value of Ystem is independent of the value of T.\nThen, let I := I(T;Ystem) be the mutual information between the random variables T and Ystem over Dtrain \u222aDSyntrain, where (T,Ystem) \u223c (p(T), \u03bbPG(Ystem|T) + (1\u2212 \u03bb)PA(Ystem|T )) and \u03bb := |Dtrain|/|Dtrain \u222a DSyntrain|. By the convexity of mutual information (Theorem 2), we have that I \u2264 \u03bbIG + (1\u2212 \u03bb)IA.\nAs we take |DSyntrain| \u2192 \u221e, we have that \u03bbIG + (1\u2212 \u03bb)IA \u2192 0 \u00b7 IG + 1 \u00b7 IA = 0. Thus, I is lower bounded by zero (since mutual information is nonnegative) and upper bounded by 0 as DSyntrain \u2192 0 (by the above argument). Thus, we have that I \u2192 0, as desired. This proves (1).\nThe same argument can be applied to prove (ii), (iii), and (iv). For (ii), we let Xaffix take the place of T and repeat the argument above. For (iii), we let Yaffix take the place of T. For (iv), we let Yaffix take the place of T and let Xstem take the place of Ystem."
        },
        {
            "heading": "C Language information",
            "text": "In Table 2, we list the languages assessed in our experiments on assessing the sample efficiency of STEMCORRUPT with their language families and estimated number of speakers (Lewis, 2009)."
        },
        {
            "heading": "D Expanded results for assessing STEMCORRUPT\u2019s sample efficiency",
            "text": "Here we present the expanded set of results for Section 5; see Fig. 8. The results are the same as those in Fig. 4, except they also include |D\u0302Syntrain| \u2208\n{256, 1024} in addition to {128, 512, 2048}."
        }
    ],
    "title": "Understanding Compositional Data Augmentation in Typologically Diverse Morphological Inflection",
    "year": 2023
}