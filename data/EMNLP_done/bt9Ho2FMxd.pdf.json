{
    "abstractText": "Research on automatic hate speech (HS) detection has mainly focused on identifying explicit forms of hateful expressions on user-generated content. Recently, a few works have started to investigate methods to address more implicit and subtle abusive content. However, despite these efforts, automated systems still struggle to correctly recognize implicit and more veiled forms of HS. As these systems heavily rely on proper textual representations for classification, it is crucial to investigate the differences in embedding implicit and explicit messages. Our contribution to address this challenging task is fourfold. First, we present a comparative analysis of transformer-based models, evaluating their performance across five datasets containing implicit HS messages. Second, we examine the embedding representations of implicit messages across different targets, gaining insight into how veiled cases are encoded. Third, we compare and link explicit and implicit hateful messages across these datasets through their targets, enforcing the relation between explicitness and implicitness and obtaining more meaningful embedding representations. Lastly, we show how these newer representation maintains high performance on HS labels, while improving classification in borderline cases.",
    "authors": [
        {
            "affiliations": [],
            "name": "Nicolas Ocampo"
        },
        {
            "affiliations": [],
            "name": "Serena Villata"
        }
    ],
    "id": "SP:a44bfacf97af1207243924565aa5f9fe79cd5afc",
    "references": [
        {
            "authors": [
                "Valerio Basile",
                "Cristina Bosco",
                "Elisabetta Fersini",
                "Debora Nozza",
                "Viviana Patti",
                "Francisco Manuel Rangel Pardo",
                "Paolo Rosso",
                "Manuela Sanguinetti"
            ],
            "title": "SemEval-2019 task 5: Multilingual detection of hate speech against immigrants and women",
            "year": 2019
        },
        {
            "authors": [
                "Elisa Bassignana",
                "Valerio Basile",
                "Viviana Patti."
            ],
            "title": "Hurtlex: A Multilingual Lexicon of Words to Hurt",
            "venue": "Elena Cabrio, Alessandro Mazzei, and Fabio Tamburini, editors, Proceedings of the Fifth Italian Conference on Computational Linguistics CLiC-it",
            "year": 2018
        },
        {
            "authors": [
                "Tom Bourgeade",
                "Patricia Chiril",
                "Farah Benamara",
                "V\u00e9ronique Moriceau."
            ],
            "title": "What did you learn to hate? a topic-oriented analysis of generalization in hate speech detection",
            "venue": "Proceedings of the 17th Conference of the European Chapter of the Asso-",
            "year": 2023
        },
        {
            "authors": [
                "Tommaso Caselli",
                "Valerio Basile",
                "Jelena Mitrovi\u0107",
                "Michael Granitzer."
            ],
            "title": "HateBERT: Retraining BERT for abusive language detection in English",
            "venue": "Proceedings of the 5th Workshop on Online Abuse and Harms (WOAH 2021), pages 17\u201325, Online. As-",
            "year": 2021
        },
        {
            "authors": [
                "Thomas Davidson",
                "Dana Warmsley",
                "Michael Macy",
                "Ingmar Weber."
            ],
            "title": "Automated Hate Speech Detection and the Problem of Offensive Language",
            "venue": "Proceedings of the International AAAI Conference on Web and Social Media, 11(1):512\u2013515. Number: 1.",
            "year": 2017
        },
        {
            "authors": [
                "Ona de Gibert",
                "Naiara Perez",
                "Aitor Garc\u00eda-Pablos",
                "Montse Cuadros."
            ],
            "title": "Hate speech dataset from a white supremacy forum",
            "venue": "Proceedings of the 2nd Workshop on Abusive Language Online (ALW2), pages 11\u201320, Brussels, Belgium. Association for",
            "year": 2018
        },
        {
            "authors": [
                "Mai ElSherief",
                "Caleb Ziems",
                "David Muchlinski",
                "Vaishnavi Anupindi",
                "Jordyn Seybolt",
                "Munmun De Choudhury",
                "Diyi Yang."
            ],
            "title": "Latent hatred: A benchmark for understanding implicit hate speech",
            "venue": "Proceedings of the 2021 Conference on Empirical Meth-",
            "year": 2021
        },
        {
            "authors": [
                "Nicolas Kourtellis"
            ],
            "title": "Large Scale Crowdsourcing and Characterization of Twitter Abusive Behavior",
            "venue": "Proceedings of the International AAAI Conference on Web and Social Media,",
            "year": 2018
        },
        {
            "authors": [
                "Bj\u00f6rn Gamb\u00e4ck",
                "Utpal Kumar Sikdar."
            ],
            "title": "Using convolutional neural networks to classify hate-speech",
            "venue": "Proceedings of the First Workshop on Abusive Language Online, pages 85\u201390, Vancouver, BC, Canada. Association for Computational Linguistics.",
            "year": 2017
        },
        {
            "authors": [
                "Beliz Gunel",
                "Jingfei Du",
                "Alexis Conneau",
                "Ves Stoyanov."
            ],
            "title": "Supervised contrastive learning for pre-trained language model fine-tuning",
            "venue": "CoRR, abs/2011.01403.",
            "year": 2020
        },
        {
            "authors": [
                "Xiaochuang Han",
                "Yulia Tsvetkov."
            ],
            "title": "Fortifying toxic speech detectors against veiled toxicity",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 7732\u20137739, Online. Association for Computa-",
            "year": 2020
        },
        {
            "authors": [
                "Thomas Hartvigsen",
                "Saadia Gabriel",
                "Hamid Palangi",
                "Maarten Sap",
                "Dipankar Ray",
                "Ece Kamar."
            ],
            "title": "ToxiGen: A large-scale machine-generated dataset for adversarial and implicit hate speech detection",
            "venue": "Proceedings of the 60th Annual Meeting of the",
            "year": 2022
        },
        {
            "authors": [
                "David Jurgens",
                "Libby Hemphill",
                "Eshwar Chandrasekharan."
            ],
            "title": "A just and comprehensive strategy for using NLP to address online abuse",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3658\u2013",
            "year": 2019
        },
        {
            "authors": [
                "Youngwook Kim",
                "Shinwoo Park",
                "Yo-Sub Han."
            ],
            "title": "Generalizable implicit hate speech detection using contrastive learning",
            "venue": "Proceedings of the 29th International Conference on Computational Linguistics, pages 6667\u20136679, Gyeongju, Republic of Korea. In-",
            "year": 2022
        },
        {
            "authors": [
                "Rohan Kshirsagar",
                "Tyrus Cukuvac",
                "Kathy McKeown",
                "Susan McGregor."
            ],
            "title": "Predictive embeddings for hate speech detection on Twitter",
            "venue": "Proceedings of the 2nd Workshop on Abusive Language Online (ALW2), pages 26\u201332, Brussels, Belgium. Associa-",
            "year": 2018
        },
        {
            "authors": [
                "Ju-Hyoung Lee",
                "Jun-U Park",
                "Jeong-Won Cha",
                "YoSub Han."
            ],
            "title": "Detecting context abusiveness using hierarchical deep learning",
            "venue": "Proceedings of the Second Workshop on Natural Language Processing for Internet Freedom: Censorship, Disinformation,",
            "year": 2019
        },
        {
            "authors": [
                "Isar Nejadgholi",
                "Kathleen Fraser",
                "Svetlana Kiritchenko."
            ],
            "title": "Improving generalizability in implicitly abusive language detection with concept activation vectors",
            "venue": "Proceedings of the 60th Annual",
            "year": 2022
        },
        {
            "authors": [
                "Nicolas Ocampo",
                "Ekaterina Sviridova",
                "Elena Cabrio",
                "Serena Villata."
            ],
            "title": "An in-depth analysis of implicit and subtle hate speech messages",
            "venue": "Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics,",
            "year": 2023
        },
        {
            "authors": [
                "Ji Ho Park",
                "Pascale Fung."
            ],
            "title": "One-step and twostep classification for abusive language detection on Twitter",
            "venue": "Proceedings of the First Workshop on Abusive Language Online, pages 41\u201345, Vancouver, BC, Canada. Association for Computational Linguis-",
            "year": 2017
        },
        {
            "authors": [
                "John Pavlopoulos",
                "Prodromos Malakasiotis",
                "Juli Bakagianni",
                "Ion Androutsopoulos."
            ],
            "title": "Improved abusive comment moderation with user embeddings",
            "venue": "Proceedings of the 2017 EMNLP Workshop: Natural Language Processing meets Journalism, pages",
            "year": 2017
        },
        {
            "authors": [
                "Fabio Poletto",
                "Valerio Basile",
                "Manuela Sanguinetti",
                "Cristina Bosco",
                "Viviana Patti."
            ],
            "title": "Resources and benchmark corpora for hate speech detection: a systematic review",
            "venue": "Language Resources and Evaluation, 55(2):477\u2013523.",
            "year": 2021
        },
        {
            "authors": [
                "Nils Rethmeier",
                "Isabelle Augenstein."
            ],
            "title": "A primer on contrastive pretraining in language processing: Methods, lessons learned and perspectives",
            "venue": "CoRR, abs/2102.12982.",
            "year": 2021
        },
        {
            "authors": [
                "Paul R\u00f6ttger",
                "Bertie Vidgen",
                "Dong Nguyen",
                "Zeerak Waseem",
                "Helen Margetts",
                "Janet Pierrehumbert."
            ],
            "title": "HateCheck: Functional tests for hate speech detection models",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational",
            "year": 2021
        },
        {
            "authors": [
                "Sumegh Roychowdhury",
                "Vikram Gupta."
            ],
            "title": "Dataefficient methods for improving hate speech detection",
            "venue": "Findings of the Association for Computational Linguistics: EACL 2023, pages 125\u2013132, Dubrovnik, Croatia. Association for Computational",
            "year": 2023
        },
        {
            "authors": [
                "Maarten Sap",
                "Saadia Gabriel",
                "Lianhui Qin",
                "Dan Jurafsky",
                "Noah A. Smith",
                "Yejin Choi."
            ],
            "title": "Social bias frames: Reasoning about social and power implications of language",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational",
            "year": 2020
        },
        {
            "authors": [
                "Josiah Poon"
            ],
            "title": "Detect all abuse! toward univer",
            "year": 2020
        },
        {
            "authors": [
                "Ingmar Weber"
            ],
            "title": "Understanding abuse: A",
            "year": 2017
        },
        {
            "authors": [
                "Eder"
            ],
            "title": "Implicitly abusive language \u2013 what does",
            "year": 2021
        },
        {
            "authors": [
                "Clayton Greenberg"
            ],
            "title": "Inducing a lexicon",
            "year": 2018
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "The proliferation of hate speech (HS) on social media platforms has become a pressing concern in online social communities. While significant progress has been made in the development of HS detection methods, current SOTA models focus on detecting explicit HS, leaving implicit hate cases undetected (ElSherief et al., 2021; Ocampo et al., 2023). This issue is aggravated by the sheer volume of implicit hate speech content being spread across various online platforms, necessitating automated approaches to detect them effectively.\nImplicit HS detection poses unique challenges compared to its explicit counterpart: it contains\ncoded, ambiguous or indirect language that does not immediately denote hate, but still disparages a person or a group based on protected characteristics such as race, gender, cultural identity, or religion (e.g., \u201cI think it is a bit late to think to look after the safety and the future of white people in South-Africa\" - the White Supremacy Forum Dataset (de Gibert et al., 2018)). The performance of current HS systems heavily relies on how coded language is represented and how well classifiers can capture the underlying semantic meaning of messages through embeddings. Hence, obtaining better text representation becomes crucial in effectively identifying implicit HS messages.\nIn this direction, the goal of our work is to bridge the gap between explicit and implicit messages, aiming to enhance the embedding representations of SOTA models. Our contribution is fourfold: i) We analyze the embedding representations of five benchmark datasets with veiled hateful content, examining the levels of explicitness and implicitness, through cross-evaluation using state-of-theart transformer models. ii) We examine the embedding representations of implicit messages across different target groups. Through this analysis, we gain insights into how implicit HS messages are encoded based on their target groups. iii) We propose a novel approach to link explicit and implicit HS messages in the representation space. iv) We illustrate that the newer representation space preserves strong efficacy for HS labels, while also refining classification in borderline instances. Using contrastive learning techniques (Gunel et al., 2020; Rethmeier and Augenstein, 2021; Kim et al., 2022; Tian et al., 2020), we aim to push explicit and implicit messages effectively enforcing the uncovered relation between these two notions and thereby obtaining more meaningful representations than those obtained through fine-tuning learning methods.1\n1The accompanying software can be found at: https:// github.com/benjaminocampo/bridging_ie_hs_embs.\nNOTE: This paper contains examples of language which may be offensive to some readers. They do not represent the views of the authors."
        },
        {
            "heading": "2 Related Work",
            "text": "HS detection has been extensively studied by the research community providing multiple resources, such as lexicons (Wiegand et al., 2018; Bassignana et al., 2018), datasets (Zampieri et al., 2019; Basile et al., 2019; Davidson et al., 2017; Founta et al., 2018), and supervised methods (Park and Fung, 2017; Gamb\u00e4ck and Sikdar, 2017; Wang et al., 2020; Lee et al., 2019) (for a survey, see (Poletto et al., 2021)). These studies provide a solid starting point to examine the problem of abusive language, especially in social media messages. Lately, there has been growing interest in the detection of implicit HS, which provides additional challenges. Datasets specifically designed for implicit HS (Ocampo et al., 2023; Hartvigsen et al., 2022; ElSherief et al., 2021; Vidgen et al., 2021; Sap et al., 2020), more solid veiled detectors (Han and Tsvetkov, 2020), guided augmentation strategies (Nejadgholi et al., 2022; Roychowdhury and Gupta, 2023), and theoretical analysis (Jurgens et al., 2019; Waseem et al., 2017; Wiegand et al., 2021) have been recently proposed to advance in this direction.\nHowever, little attention has been dedicated to effectively represent implicit messages through embeddings on these benchmarks. Embeddings play a crucial role in the performance of classifiers (Pavlopoulos et al., 2017; Kshirsagar et al., 2018; Ocampo et al., 2023), yet their application to capture the implicit nature of HS has been underinvestigated. In this direction, (Kim et al., 2022) tackles cross-dataset underperforming issues on HS classifiers and proposes a contrastive learning method that encodes a hateful post and its corresponding implication close in representation space, closely depending on the annotated implications and without contra positioning explicitness with implicitness. (Bourgeade et al., 2023) captures topic-generic and topic-specific knowledge when trained on different data to improve generalization."
        },
        {
            "heading": "3 Implicit and Explicit HS Embeddings",
            "text": ""
        },
        {
            "heading": "3.1 Research Questions",
            "text": "We will focus on the behavior of SOTA models in cross-evaluation settings, specifically on datasets containing implicit hate. The study explores the models\u2019 behavior on different HS classes, including\nboth explicit and implicit hate. In particular, we target the following research questions (RQ): RQ1: How do the models\u2019 embeddings capture the HS classes? Are explicit and implicit hateful messages encoded differently across different datasets? What is the extent of this variation? RQ2: Does grouping the test sets by target result in similar encoding patterns for explicit HS and distinct encoding patterns for implicit HS in the embeddings? RQ2 builds upon the analysis conducted in RQ1, but with a focus on target groups. RQ3: Can we link and bring explicit and implicit embedding representations closer together within the learned embedding space through their target groups? RQ4: How do these newer embedding representations capture HS classes in comparison with RQ1?"
        },
        {
            "heading": "3.2 Datasets",
            "text": "We carried out our analysis on the following standard datasets, containing implicit HS messages: Implicit Subtle Hate (ISHate) (Ocampo et al., 2023), Social Bias Inference (SBIC) (Sap et al., 2020), Implicit Hate Corpus (IHC) (ElSherief et al., 2021), Dynahate (DYNA) (Vidgen et al., 2021), and Toxigen (TOX) (Hartvigsen et al., 2022). We ensured that the definitions of HS were consistent across the datasets. Specifically, for the SBIC dataset, messages are considered as HS if labeled as offensive and target a specific group. As for the explicitimplicit HS labeling across all datasets, the provided implicit labels are used for IHC and ISHate datasets. For SBIC, DYNA, and TOX, we computed the percentage of HS implicit messages as the ones where none of the words of the Google profanity words resource was present2. The datasets were divided into train, dev, and test sets. Existing dataset splits were retained, while datasets without predefined splits were divided using a stratified splitting method with a 60% train, 20% dev, and 20% test ratio. Table 4 in Appendix shows the percentage of implicit/explicit instances per dataset."
        },
        {
            "heading": "3.3 Experimental Settings",
            "text": "Concerning our research questions (Section 3.1), to answer to RQ1 we performed fine-tuning on two state-of-the-art models commonly used for HS detection: BERT and HateBERT (Caselli et al., 2021). Both models were fine-tuned on each dataset using\n2List of swear words banned by Google: https:// github.com/RobertJGabriel/Google-profanity-words\na two-label classification approach, distinguishing between non-HS and HS messages. To ensure robustness and account for randomness in the training process, we repeated the fine-tuning procedure five times, each time employing a different random seed. This allowed us to evaluate the performance of the models consistency. To assess the performance of the models, we cross-evaluate the benchmarks calculating the average F1-score across all fine-tuning runs. Additionally, we calculated the standard deviation to quantify the variability in performance observed across the different runs. Finally, we calculated the embeddings of these models using TSNE highlighting how explicit and implicit messages were encoded. We used the base versions size of these models with batch size of 32, weight decay of 0.01, 4 epochs, and a learning rate of 2e-5. As for TSNE, we use perplexity of 30, and 1000 maximum iterations for convergence.\nFor RQ2, we grouped the embeddings per target in the plots. To ensure consistency across datasets, we standardized the target names, addressing label variations, e.g., we resolved differences like \"asian\" and \"asian people\" by using a unified label. Moreover, when a message targeted multiple offensive groups (e.g. Asians and Migrants), we selected the label corresponding to the predominant target (among MUSLIMS, WOMEN, JEWS, LGBTQ+, BLACK PEOPLE, WHITE PEOPLE, IMMIGRANTS, ASIAN, and DISEASE).\nFor RQ3, we aim to validate the potential linkage between explicit and implicit messages through their target groups. To achieve this, we employ contrastive learning techniques on the pre-trained and fine-tuned models. Contrastive learning involves defining pairs of positive and negative samples and training the model using a modified loss function. In our experimental settings, we designate pairs of implicit and explicit messages with the same target as positive samples. For each implicit ones, a randomly selected explicit message with the same target is paired. In cases where they are unavailable or when the implicit instance lacks a target label, we randomly assign any explicit message. Negative samples consist of pairs of HS and NonHS instances. For every Non-HS instance, one HS instance is randomly selected. Using contrastive learning facilitates the training process by pushing positive pairs closer together while pushing negative pairs further apart within the embedding space.\nThe contrastive loss is defined as follows: loss_cont = mean ( (1\u2212 l) \u00b7 s2\n+ l \u00b7 (max(0,m\u2212 s))2 ) (1)\nWhere l represents the label pair (1 for positive pairs, 0 for negative pairs), s is the cosine similarity between paired messages, and m is the margin hyper-parameter. For classification, the crossentropy loss is:\nloss_clf = \u2212 N\u22121\u2211 i=0 (gilog(pi)\n+ (1\u2212 gi)log(1\u2212 pi))\n(2)\nWhere g is the gold label (labels of the dataset on which the model is fine-tuned) and p is the prediction. The final loss is:\ntotal_loss = loss_cont + loss_clf (3)\nBy combining them, we optimize both the model\u2019s understanding of embeddings and classification.\nFor RQ4, we fine-tuned both BERT and HateBERT using our enhanced embeddings (same settings of our initial RQs). Additionally, to gain more targeted diagnostic insights, models\u2019 accuracy was evaluated on three categories defined on the SBIC dataset (Non-HS, Explicit HS, and Implicit HS), and the HateCheck dataset (R\u00f6ttger et al., 2021), a suite of functional tests for HS detection models."
        },
        {
            "heading": "3.4 Obtained Results and Discussion",
            "text": "Regarding RQ1, Table 1a shows that training and evaluating HateBERT (BERT results can be found in the Appendix) on the same dataset yields\nbetter results overall, as could be expected. However, even in cross-evaluation scenarios, reasonable performances are observed. Notably, among the most generalizable models, HateBERT trained on DYNA exhibits better generalization. We therefore selected HateBERT trained on DYNA as the best configuration and we plot the embeddings for the test sets of all the datasets, applying the TSNE algorithm. Figure 1 shows how explicit HS and non-HS messages are encoded with clear separation, resulting in a noticeable distance between them.3 On the other hand, implicit HS instances tend to be intertwined with both non-HS and explicit HS messages. This pattern holds true across all 5 datasets.\nAs for the results for RQ2, Figure 2 shows ex3Due to space constraints, we show only the plots of the TSNE embeddings of the SBIC test set using HateBERT finetuned on DYNA. The plots showing the embeddings for all the other test sets can be found in the Appendix.\nplicit and implicit text representations per target group highlighting how, in general, embeddings of explicit and implicit messages tend to be linked by their target groups in representation spaces. Finally, as for RQ3, Figure 3 demonstrates that the embedding representations of explicit and implicit instances starts to overlap across all datasets when using HateBERT trained on DYNA. Additionally, Figure 4 highlights that by leveraging the targets of HS using contrastive learning, explicit and implicit messages exhibit a similar representation.\nAs for RQ4, Table 1b illustrates that the novel representation enhances the F1-score for certain datasets, such as SBIC, TOX, and ISHate. Conversely, for other datasets like IHC and DYNA, the performance remains comparable to that of the non-contrastive approach. Table 2 shows higher capability of the contrastive\nHateBERT in accurately classifying challenging Non-HS messages across all five datasets. A significant reduction in false positives is also observed in HateCheck categories such as quoted announcements (counter_quote_nh), direct references (counter_ref_nh), positive identifiers (ident_pos_nh), negated hateful remarks (negate_neg_nh), nonhateful profanity (profanity_nh), reclaimed slurs (slur_reclaimed_nh), homonym slurs (slur_homonym_nh), as well as targeted abuse directed at individuals (target_indiv_nh), objects (target_obj_nh), and non-protected groups (target_group_nh). Additionally, Table 3 indicates that both Explicit and Implicit categories exhibit similarly high accuracy levels,\nhighlighting their nearly indistinguishable impact on the model\u2019s aggregate performance. Also, the importance of the Non-HS category is underscored, varying with different training datasets, yet remaining a critical component.\nHence, our experiments emphasize the importance of studying implicit representations, as classical training strategies cannot encode them properly (RQ1). We showed that implicit and explicit messages share a connection conveying similar messages to the same target (RQ2) and how contrastive learning effectively forces that property by bridging explicit and implicit instances through their targets (RQ3), thereby obtaining more meaningful representations that the ones obtained through finetuning. Finally, we reduced biases in non-hateful implicit cases often misclassified due to trigger words or nuanced content. Our enhanced method maintains high performance on HS labels while improving classification in borderline cases, proving its robustness and precision (RQ4)."
        },
        {
            "heading": "4 Conclusions",
            "text": "Our contribution in this study is fourfold: i) We studied how models\u2019 embeddings capture HS w.r.t. explicitness and implicitness, ii) We showed how explicit and implicit HS messages result in similar encodings if grouped by their protected target, iii) We analyzed a contrastive learning method to force this property when representing implicit text. We prove our research hypothesis on 5 HS benchmarks, moving a step forward in bridging the gap between explicitness and implicitness, and iv) We show how the newer representation space maintains high performance on HS labels while improving classification in borderline cases. In future work, we\u2019ll refine contrastive learning, delving into contextual pairing based on other semantic dependencies between explicit and implicit cues, aiming to sharpen nuanced hate speech detection.\nLimitations\nIn this study, we are aware of some key issues, one of which pertains to the selection of positive and negative samples in contrastive learning. The effectiveness of the algorithm heavily relies on the careful selection of these pairs. While our investigation demonstrates that explicit and implicit messages exhibit a relationship through their target groups across five distinct datasets, it is important to acknowledge that this assumption may not always hold true. Additionally, ensuring a clear separation between non-hateful and HS instances can be challenging due to the heterogeneity of each category.\nMoreover, the efficacy of our approach is contingent upon the availability and alignment of target information across the datasets. While target information is commonly provided in benchmark datasets, different datasets may address various protected characteristics. Our approach assumes that there is some degree of overlap in terms of target groups among the selected datasets.\nFurthermore, the selection of pairs when linking explicit and implicit messages can vary in terms of the number of combinations. However, it is important to note that as the number of pairs increases, the training requirements tend to grow significantly, resulting in slower training processes. This trade-off between the number of pairs and training efficiency should be carefully considered when implementing the approach.\nEthics Statement\nThis paper uses a collection of HS examples extracted from linguistic resources commonly employed for HS detection, ensuring their independence from the authors\u2019 personal opinions. The datasets used in this study have been meticulously handled to address privacy concerns associated with user data. While we acknowledge the potential for misuse, we firmly believe that developing robust HS classifiers is essential in combating the proliferation of harmful content. In this regard, our work represents a significant contribution towards this objective and encourages further exploration and investigation within the scientific community."
        },
        {
            "heading": "Acknowledgements",
            "text": "This work has been supported by the French government, through the 3IA C\u00f4te d\u2019Azur Investments in the Future project managed by the National Re-\nsearch Agency (ANR) with the reference number ANR- 19-P3IA-0002."
        },
        {
            "heading": "A Datasets Statistics and Details",
            "text": "Table 4 displays statistics related to the datasets used in our study, including IHC, SBIC, DYNA, TOX, and ISHate. The table presents the percentage of implicit and explicit instances per dataset, along with their distribution across set partitions, and target groups distribution."
        },
        {
            "heading": "B Evaluation results with BERT",
            "text": "In this section we show the evaluation of the BERT and Contrastive BERT models for RQ1 and RQ4 specified in sections 3.1 and 3.3.\nTable 5a demonstrates BERT\u2019s efficacy in crossevaluation contexts, mirroring the results seen with HateBERT. Among the models, SBIC stands out, displaying superior generalization capabilities. Conversely, Table 5b illustrates that while models like SBIC, IHC, and TOX reap advantages from contrastive learning, others experience a slight dip in performance, though maintaining an overall high-quality output.\nMoving on to Table 6, it\u2019s evident that a segment of the enhancement is attributed to the precise categorization of challenging Non-HS messages prevalent across all five datasets. This precision underscores a more conservative and meticulous approach in classifying a message as Hateful.\nFinally, Table 7 highlights BERT\u2019s consistent performance, boasting high accuracy in handling Non-HS instances for each dataset. This is achieved without compromising the emphasis on discerning between Explicit and Implicit labels, thereby ensuring that the model maintains a balanced focus on varied content nuances."
        },
        {
            "heading": "C TSNE embeddings for RQ1, RQ2, and RQ3 in all datasets",
            "text": "This section presents the TSNE results for each research questions RQ1, RQ2, and RQ3, illustrated in Figures 5, 6, 7, and 8. These visualizations are generated from the embeddings captured by HateBERT, specifically trained on the DYNA dataset and evaluated on all datasets."
        }
    ],
    "title": "Unmasking the Hidden Meaning: Bridging Implicit and Explicit Hate Speech Embedding Representations",
    "year": 2023
}