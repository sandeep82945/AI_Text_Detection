{
    "abstractText": "In the field of Large Language Models (LLMs), researchers are increasingly exploring their effectiveness across a wide range of tasks. However, a critical area that requires further investigation is the interpretability of these models, particularly the ability to generate rational explanations for their decisions. Most existing explanation datasets are limited to the English language and the general domain, which leads to a scarcity of linguistic diversity and a lack of resources in specialized domains, such as medical. To mitigate this, we propose ExplainCPE, a challenging medical dataset consisting of over 7K problems from Chinese Pharmacist Examination, specifically tailored to assess the model-generated explanations. From the overall results, only GPT-4 passes the pharmacist examination with a 75.7% accuracy, while other models like ChatGPT fail. Further detailed analysis of LLM-generated explanations reveals the limitations of LLMs in understanding medical text and executing computational reasoning. With the increasing importance of AI safety and trustworthiness, ExplainCPE takes a step towards improving and evaluating the interpretability of LLMs in the medical domain. The dataset is available at https: //github.com/HITsz-TMG/ExplainCPE.",
    "authors": [
        {
            "affiliations": [],
            "name": "Dongfang Li"
        },
        {
            "affiliations": [],
            "name": "Jindi Yu"
        },
        {
            "affiliations": [],
            "name": "Baotian Hu"
        },
        {
            "affiliations": [],
            "name": "Zhenran Xu"
        },
        {
            "affiliations": [],
            "name": "Min Zhang"
        }
    ],
    "id": "SP:7a6f23333c953f9136bbce1826932743394ace95",
    "references": [
        {
            "authors": [
                "Shourya Aggarwal",
                "Divyanshu Mandowara",
                "Vishwajeet Agrawal",
                "Dinesh Khandelwal",
                "Parag Singla",
                "Dinesh Garg."
            ],
            "title": "Explanations for commonsenseqa: New dataset and models",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational",
            "year": 2021
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners",
            "venue": "Advances in Neural Information Processing",
            "year": 2020
        },
        {
            "authors": [
                "Oana-Maria Camburu",
                "Tim Rockt\u00e4schel",
                "Thomas Lukasiewicz",
                "Phil Blunsom."
            ],
            "title": "e-snli: Natural language inference with natural language explanations",
            "venue": "Neural Information Processing Systems.",
            "year": 2018
        },
        {
            "authors": [
                "Zhengxiao Du",
                "Yujie Qian",
                "Xiao Liu",
                "Ming Ding",
                "Jiezhong Qiu",
                "Zhilin Yang",
                "Jie Tang."
            ],
            "title": "Glm: General language model pretraining with autoregressive blank infilling",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational",
            "year": 2022
        },
        {
            "authors": [
                "Dan Hendrycks",
                "Collin Burns",
                "Steven Basart",
                "Andy Zou",
                "Mantas Mazeika",
                "Dawn Song",
                "Jacob Steinhardt."
            ],
            "title": "Measuring massive multitask language understanding",
            "venue": "9th International Conference on Learning Representations, ICLR 2021, Virtual Event,",
            "year": 2021
        },
        {
            "authors": [
                "Yuzhen Huang",
                "Yuzhuo Bai",
                "Zhihao Zhu",
                "Junlei Zhang",
                "Jinghan Zhang",
                "Tangjun Su",
                "Junteng Liu",
                "Chuancheng Lv",
                "Yikai Zhang",
                "Jiayi Lei",
                "Yao Fu",
                "Maosong Sun",
                "Junxian He"
            ],
            "title": "C-eval: A multi-level multi-discipline chinese evaluation suite",
            "year": 2023
        },
        {
            "authors": [
                "Yunjie Ji",
                "Yong Deng",
                "Yan Gong",
                "Yiping Peng",
                "Qiang Niu",
                "Baochang Ma",
                "Xiangang Li."
            ],
            "title": "Belle: Be everyone\u2019s large language model engine",
            "venue": "https: //github.com/LianjiaTech/BELLE.",
            "year": 2023
        },
        {
            "authors": [
                "Yunjie Ji",
                "Yong Deng",
                "Yan Gong",
                "Yiping Peng",
                "Qiang Niu",
                "Baochang Ma",
                "Xiangang Li"
            ],
            "title": "Exploring the impact of instruction data scaling on large",
            "year": 2023
        },
        {
            "authors": [
                "Qiao Jin",
                "Bhuwan Dhingra",
                "Zhengping Liu",
                "William W. Cohen",
                "Xinghua Lu."
            ],
            "title": "Pubmedqa: A dataset for biomedical research question answering",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and",
            "year": 2019
        },
        {
            "authors": [
                "Jenny Kunz",
                "Martin Jirenius",
                "Oskar Holmstr\u00f6m",
                "Marco Kuhlmann."
            ],
            "title": "Human ratings do not reflect downstream utility: A study of free-text explanations for model predictions",
            "venue": "Proceedings of the Fifth BlackboxNLP Workshop on Analyzing",
            "year": 2022
        },
        {
            "authors": [
                "Dongfang Li",
                "Baotian Hu",
                "Qingcai Chen",
                "Weihua Peng",
                "Anqi Wang."
            ],
            "title": "Towards medical machine reading comprehension with structural knowledge and plain text",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language",
            "year": 2020
        },
        {
            "authors": [
                "Jianquan Li",
                "Xidong Wang",
                "Xiangbo Wu",
                "Zhiyi Zhang",
                "Xiaolong Xu",
                "Jie Fu",
                "Prayag Tiwari",
                "Xiang Wan",
                "Benyou Wang."
            ],
            "title": "Huatuo-26m, a large-scale chinese medical QA dataset",
            "venue": "CoRR, abs/2305.01526.",
            "year": 2023
        },
        {
            "authors": [
                "Julius Monsen",
                "Evelina Rennes."
            ],
            "title": "Perceived text quality and readability in extractive and abstractive summaries",
            "venue": "Proceedings of the Thirteenth Language Resources and Evaluation Conference, LREC 2022, Marseille, France, 20-25 June 2022,",
            "year": 2022
        },
        {
            "authors": [
                "OpenAI."
            ],
            "title": "GPT-4 technical report",
            "venue": "CoRR, abs/2303.08774.",
            "year": 2023
        },
        {
            "authors": [
                "Long Ouyang",
                "Jeffrey Wu",
                "Xu Jiang",
                "Diogo Almeida",
                "Carroll Wainwright",
                "Pamela Mishkin",
                "Chong Zhang",
                "Sandhini Agarwal",
                "Katarina Slama",
                "Alex Gray"
            ],
            "title": "Training language models to follow instructions with human feedback",
            "year": 2022
        },
        {
            "authors": [
                "Anusri Pampari",
                "Preethi Raghavan",
                "Jennifer J. Liang",
                "Jian Peng."
            ],
            "title": "emrqa: A large corpus for question answering on electronic medical records",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels,",
            "year": 2018
        },
        {
            "authors": [
                "Alec Radford",
                "Jeffrey Wu",
                "Rewon Child",
                "David Luan",
                "Dario Amodei",
                "Ilya Sutskever"
            ],
            "title": "Language models are unsupervised multitask learners",
            "venue": "OpenAI blog,",
            "year": 2019
        },
        {
            "authors": [
                "Nazneen Fatema Rajani",
                "Bryan McCann",
                "Caiming Xiong",
                "Richard Socher."
            ],
            "title": "Explain yourself! leveraging language models for commonsense reasoning",
            "venue": "Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL",
            "year": 2019
        },
        {
            "authors": [
                "Cynthia Rudin."
            ],
            "title": "Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead",
            "venue": "Nat. Mach. Intell., 1(5):206\u2013215.",
            "year": 2019
        },
        {
            "authors": [
                "Swarnadeep Saha",
                "Peter Hase",
                "Nazneen Rajani",
                "Mohit Bansal."
            ],
            "title": "Are hard examples also harder to explain? A study with human and model-generated explanations",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language",
            "year": 2022
        },
        {
            "authors": [
                "las Baskiotis",
                "Patrick Gallinari",
                "Thierry Arti\u00e8res",
                "Axel-Cyrille Ngonga Ngomo",
                "Norman Heino",
                "\u00c9ric Gaussier",
                "Liliana Barrio-Alvers",
                "Michael Schroeder",
                "Ion Androutsopoulos",
                "Georgios Paliouras"
            ],
            "title": "An overview of the BIOASQ large-scale biomedical",
            "year": 2015
        },
        {
            "authors": [
                "George Tsatsaronis",
                "Michael Schroeder",
                "Georgios Paliouras",
                "Yannis Almirantis",
                "Ion Androutsopoulos",
                "\u00c9ric Gaussier",
                "Patrick Gallinari",
                "Thierry Arti\u00e8res",
                "Michael R. Alvers",
                "Matthias Zschunke",
                "AxelCyrille Ngonga Ngomo"
            ],
            "title": "Bioasq: A challenge",
            "year": 2012
        },
        {
            "authors": [
                "Miles Turpin",
                "Julian Michael",
                "Ethan Perez",
                "Samuel R. Bowman."
            ],
            "title": "Language models don\u2019t always say what they think: Unfaithful explanations in chain-of-thought prompting",
            "venue": "CoRR, abs/2305.04388.",
            "year": 2023
        },
        {
            "authors": [
                "Haochun Wang",
                "Chi Liu",
                "Nuwa Xi",
                "Zewen Qiang",
                "Sendong Zhao",
                "Bing Qin",
                "Ting Liu"
            ],
            "title": "2023a. Huatuo: Tuning llama model with chinese medical knowledge",
            "year": 2023
        },
        {
            "authors": [
                "Haochun Wang",
                "Chi Liu",
                "Sendong Zhao",
                "Bing Qin",
                "Ting Liu."
            ],
            "title": "Chatglm-med: \u57fa\u4e8e\u4e2d\u6587\u533b\u5b66 \u77e5\u8bc6\u7684chatglm\u6a21\u578b\u5fae\u8c03",
            "venue": "https://github.com/ SCIR-HI/Med-ChatGLM.",
            "year": 2023
        },
        {
            "authors": [
                "Jason Wei",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Maarten Bosma",
                "brian ichter",
                "Fei Xia",
                "Ed Chi",
                "Quoc V Le",
                "Denny Zhou"
            ],
            "title": "2022b. Chain-of-thought prompting elicits reasoning in large language models",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Sarah Wiegreffe",
                "Jack Hessel",
                "Swabha Swayamdipta",
                "Mark O. Riedl",
                "Yejin Choi."
            ],
            "title": "Reframing human-ai collaboration for generating free-text explanations",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for",
            "year": 2022
        },
        {
            "authors": [
                "Sarah Wiegreffe",
                "Ana Marasovic."
            ],
            "title": "Teach me to explain: A review of datasets for explainable natural language processing",
            "venue": "Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Bench-",
            "year": 2021
        },
        {
            "authors": [
                "Xi Ye",
                "Greg Durrett."
            ],
            "title": "The unreliability of explanations in few-shot prompting for textual reasoning",
            "venue": "Advances in Neural Information Processing Systems, volume 35, pages 30378\u201330392. Curran Associates, Inc.",
            "year": 2022
        },
        {
            "authors": [
                "Xiang Yue",
                "Bernal Jimenez Gutierrez",
                "Huan Sun."
            ],
            "title": "Clinical reading comprehension: A thorough analysis of the emrqa dataset",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10,",
            "year": 2020
        },
        {
            "authors": [
                "Xiao Zhang",
                "Ji Wu",
                "Zhiyang He",
                "Xien Liu",
                "Ying Su."
            ],
            "title": "Medical exam question answering with large-scale reading comprehension",
            "venue": "Proceedings of the Thirty-Second AAAI Conference on Artificial",
            "year": 2018
        },
        {
            "authors": [
                "Xiaotian Zhang",
                "Chunyang Li",
                "Yi Zong",
                "Zhengyu Ying",
                "Liang He",
                "Xipeng Qiu."
            ],
            "title": "Evaluating the performance of large language models on GAOKAO benchmark",
            "venue": "CoRR, abs/2305.12474.",
            "year": 2023
        },
        {
            "authors": [
                "Xuanwei Zhang",
                "Liang Xu"
            ],
            "title": "Promptclue: A zero-shot learning model that supports full chinese tasks",
            "year": 2022
        },
        {
            "authors": [
                "Wanjun Zhong",
                "Ruixiang Cui",
                "Yiduo Guo",
                "Yaobo Liang",
                "Shuai Lu",
                "Yanlin Wang",
                "Amin Saied",
                "Weizhu Chen",
                "Nan Duan."
            ],
            "title": "Agieval: A human-centric benchmark for evaluating foundation models",
            "venue": "CoRR, abs/2304.06364.",
            "year": 2023
        },
        {
            "authors": [
                "Wang"
            ],
            "title": "2023b) constructed a Chinese medical instruction data set through the medical knowledge graph and GPT3.5 API, and based on this, fine-tuned the instructions of ChatGLM-6B",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Advancements in the field of Large Language Models (LLMs) (Radford et al., 2019; Brown et al., 2020; Ouyang et al., 2022), exemplified by models such as GPT-4 (OpenAI, 2023), have opened up new possibilities and challenges across a myriad of natural language processing (NLP) tasks (Wei et al., 2022a). These models have shown remarkable success in understanding and generating human-like text, promoting research that spans a wide array of applications (Bubeck et al., 2023).\n*Both authors contributed equally to this work. \u2020Corresponding author.\nA critical aspect that remains unexplored is the interpretability of these models, specifically the ability to provide accurate and faithful rationale for their decisions (Wei et al., 2022b; Turpin et al., 2023). The degree to which these models can explain their reasoning is of crucial significance, especially in high-stakes domains such as healthcare, where the clarity of generated responses directly impacts decision-making and outcomes (Rudin, 2019).\nCurrent benchmarks for LLMs mostly focus on the exam performance, such as MMLU (Hendrycks et al., 2021) and AGIEval (Zhong et al., 2023). These datasets do not allow for a detailed assessment of LLMs\u2019 justifications of their decisions, because of the unavailability of high-quality and professional explanations. Moreover, accurately measuring the explainability of these LLMs is a difficult task due to the lack of comprehensive and standardized datasets that come from unbiased and trustworthy sources (Li et al., 2023). Existing benchmarks predominantly are from online forums and consumer feedback and only consist of English-language general knowledge questions (Wiegreffe and Marasovic, 2021), which results in insufficient thematic and linguistic diversity. Overall, the lack of appropriate evaluation datasets has prevented a full understanding of LLMs\u2019 strengths and weaknesses in the field of interpretability.\nTo address this gap, we introduce ExplainCPE, a challenging medical benchmark dataset in Chinese, encompassing over 7K instances. This dataset, specifically tailored to evaluate the capacity of model explainability, diversifies the linguistic scope of interpretability research and allows for a rigorous assessment of model performance in a specialized, high-stakes domain. An example from our dataset is presented in Table 1. The in-depth analysis of LLMs performance on ExplainCPE brings to light several critical observations. First,\nwe find substantial limitations in understanding of these LLMs over medical text and their ability to execute computational reasoning effectively. For example, only GPT-4 passed Chinese Pharmacist Examination with 75.7% accuracy, while other models like ChatGPT failed. Through the case analysis of GPT-4 and ChatGPT, we found that the explanations generated by LLMs still have flaws such as contradictory, insufficient analysis, confused logic, and how to improve its interpretability is the part that LLMs should pay attention to in the future. Furthermore, we report heterogeneous preferences for in-context learning among different LLMs, suggesting varying strategies for explanation generation. For example, models with little chatting ability such as BELLE (Ji et al., 2023b,a) are more sensitive to the number of few-shot examples than with ChatGPT with strong chatting ability. To the best of our knowledge, we are the first to propose a free-text explanation benchmark in Chinese medical examination and further explore the interpretability of LLMs in the medical field. We provide a baseline for future research on explanation generation research, and this dataset can also be used to improve the interpretability of these large language models. As the broader issues of AI safety and trustworthiness gain attraction, our work represents a pioneering step towards enhancing the medical interpretability of LLMs, underscoring the urgent need to develop AI that\nis not only intelligent, but also transparent, robust, unbiased and reliable.\nOur main contributions can be summarized as follows:\n\u2022 We introduce ExplainCPE, a challenging benchmark for generating free-text explanations in Chinese medical QA, which provides a baseline for future research on explanation generated by LLMs, and can be used to study how to improve the ability of the model to generate explanation.\n\u2022 We analyze the basic attributes of the dataset, such as the average length of questions, options, and explanations. Additionally, we examine the high-level categories of questions, which can assist researchers in understanding the distribution of categories in ExplainCPE and the interpretability performance of the models.\n\u2022 We conduct experiments on the ExplainCPE dataset to demonstrate its effectiveness and feasibility. Our findings reveal that different LLMs exhibit varying preferences for incontext learning. We analyze error cases and identify some limitations of current LLMs, which can serve as directions for future development."
        },
        {
            "heading": "2 Related Work",
            "text": ""
        },
        {
            "heading": "2.1 Medical Question Answering",
            "text": "In the medical domain, addressing questions can be particularly challenging due to their specialized and complex nature. Consequently, community efforts have been directed towards advancing biomedical question-answering systems, such as BioASQ (Tsatsaronis et al., 2012, 2015). Another system, SeaReader (Zhang et al., 2018), was proposed to answer clinical medical questions by leveraging documents extracted from medical publications. In a study by Yue et al. (2020), the authors performed a comprehensive analysis of the emrQA (Pampari et al., 2018) dataset to evaluate the capacity of QA systems to utilize clinical domain knowledge and generalize to novel questions. Furthermore, Jin et al. (2019) introduced PubMedQA, a system that generates questions based on article titles and can be answered using their respective abstracts. Li et al. (2020) developed a largescale medical multiple-choice question dataset and proposed a novel reading comprehension model, KMQA, capable of incorporating both structural medical knowledge and plain text."
        },
        {
            "heading": "2.2 Free-text Explanation",
            "text": "Since deep learning became the dominant paradiagm in NLP research, how to interpret the predictions of neural models has become an essential part of model transparency. In explainable NLP, various forms of explanations exist, including extractive rationales, semi-structured, structured explanations, and free-text explanations. Saha et al. (2022) examine the impact of sample hardness on the capacity of both LLMs and humans to elucidate data labels. Camburu et al. (2018) augment\nthe SNLI dataset by introducing e-SNLI, which encompasses an additional layer of human-annotated natural language explanations for entailment relations. Rajani et al. (2019) gather human explanations for commonsense reasoning in the form of natural language sequences and highlighted annotations within a novel dataset known as Common Sense Explanations (CoS-E). Aggarwal et al. (2021) develop a first-of-its-kind dataset named ECQA, comprising human-annotated positive and negative properties, as well as free-flow explanations for 11,000 question-answer pairs derived from the CQA dataset. Ye and Durrett (2022) assess the performance of four LLMs across three textual reasoning datasets utilizing prompts containing explanations in multiple styles. Their findings indicate that human-evaluated high-quality explanations are more likely to coincide with accurate predictions."
        },
        {
            "heading": "2.3 LLMs Benchmarks",
            "text": "New NLP benchmarks are urgently needed to align with the rapid development of LLMs. MMLU(Hendrycks et al., 2021) is a collection of English-language materials that encompasses knowledge from 57 different disciplines including elementary mathematics, US history, computer science, law, and more. To attain high accuracy on this test, models must possess extensive world knowledge and problem solving ability. Another significant contribution to this field is the C-EVAL(Huang et al., 2023), which represents the first comprehensive effort to evaluate foundational models\u2019 knowledge and reasoning capabilities within a Chinese context. C-EVAL consists of multiple-choice questions designed to assess performance across four difficulty levels:\nmiddle school, high school, college, and professional. These questions cover 52 diverse disciplines, spanning from humanities to science and engineering, thereby providing a holistic evaluation of the model\u2019s capabilities. Zhang et al. (2023) introduces the GAOKAO-Benchmark (GAOKAOBench), an intuitive benchmark that employs questions from the Chinese Gaokao examination as test samples for evaluating LLMs. Most benchmarks focus on evaluating the performance of LLMs in answering or answering questions, but few focus on the ability of LLMs to explain the answers given."
        },
        {
            "heading": "3 ExplainCPE Dataset",
            "text": ""
        },
        {
            "heading": "3.1 Dataset Collection",
            "text": "The National Licensed Pharmacist Examination in China, collaboratively administered by the Ministry of Personnel and the State Food and Drug Administration, serves as the basis for our question set.\nIn order to evaluate the performance and generalizability of our models, we have compiled a test set using examples from the previous two years (2020-2021) of the official examination. Each official question\u2019s explanation is sourced from official examination solution. Additionally, we have collected over 7,000 instances from various sources, including the internet and exercise books. The instance in ExplainCPE dataset is multiple choice question with five options.\nIn addition to the official questions, we also collaborated with three doctoral students from Peking Union Medical College (all of whom have undergone standardized residency training). They manually reviewed 320 samples from the collected data to evaluate the completeness and accuracy of the label and explanations. The evaluation resulted in the 99.4%/99.0% accuracy rate, with 318/317 out of the 320 samples being deemed correct.\nFollowing the removal of duplicate and incomplete questions (e.g., those lacking answers or options), we randomly divided the remaining instances into training and development sets based on a predetermined ratio. To further enhance the quality of our dataset, we inspected instances with an edit distance of less than 0.1 and manually removed questions containing different words that conveyed the same meaning.\nTrain Dev Test #Questions 6867 500 189\nAvg. words of Q 28.31 28.44 37.79 Avg. words of O 8.12 8.55 9.76 Avg. words of E 120.52 116.32 171.94 Max words of Q 338 259 164 Max words of O 146 95 57 Max words of E 1011 604 685\nOptions per problem 5"
        },
        {
            "heading": "3.3 Data Analysis",
            "text": ""
        },
        {
            "heading": "3.2 Data Statistic",
            "text": "within the dataset is presented in Figure 2. A more detailed proportional distribution of each category within the dataset is presented in Appendix B."
        },
        {
            "heading": "4 Experiments",
            "text": ""
        },
        {
            "heading": "4.1 Prompting",
            "text": "Prompting has a significant impact on the output of generative language models, so we standardized the structure of our prompts. In order to better analyze the performance and interpretability of language models, we designed prompts to request the model to provide an answer option along with an explanation in the test set. An example of the template and a fully instantiated prompt can be found in Appendix A. Two types of prompt templates were utilized: with and without instructions. The purpose of this design was to explore the influence of instructions on different models. In the zero-shot setting, the few_shot_example slot was left blank. Additionally, it should be noted that prompts without instructions are the same as prompts with instructions in the zero-shot setting.\nTo investigate the impact of in-context on model performance, we designed prompts with different numbers of few-shot examples, including zeroshot, one-shot, four-shot, and eight-shot prompts. For one-shot prompts, we randomly selected a single instance from the training set. For four-shot and eight-shot prompts, we manually selected instances with varying question types to ensure model predictions were balanced. It should be noted that the\nfew-shot examples were the same for all models in each respective prompt type."
        },
        {
            "heading": "4.2 Model Comparison",
            "text": "To compare the performance of different models, we evaluated several LLMs on our test dataset. We recognize that LLMs can be classified as chat or non-chat models, depending on their ability to engage in human-like conversation. Chat models, which are pre-trained with vast amounts of data and fine-tuned through reinforcement learning from human feedback (RLHF), include GPT-4 (OpenAI, 2023), ChatGPT (OpenAI, 2022), ChatGLM6B (Du et al., 2022; Zeng et al., 2023), and ChatYuan (ClueAI, 2023). Non-chat models, on the other hand, are typically pre-trained on unsupervised plain text and fine-tuned on code or instructional data but do not have sufficient RLHF to enable human-like conversation. Examples of nonchat models include GPT-3 (Ouyang et al., 2022), BELLE (Ji et al., 2023b,a), and GPT-3 (Brown et al., 2020). Consequently, non-chat models are more inclined to predict the next word or complete a given task rather than engage in conversation. In this section, we provide a brief introduction to the LLMs used in our experiments.\n\u2022 ChatGPT (OpenAI, 2022) is a large language model with hundreds of billions of parameters, specifically designed for human-like conversation across a wide range of topics. ChatGPT\u2019s text understanding ability is derived from language model pre-training, its reasoning ability is derived from code pre-training, its logical reasoning ability is derived from supervised instruction training, and its dialogue ability is derived from RLHF.\n\u2022 GPT-4 (OpenAI, 2023) represents the latest milestone in OpenAI\u2019s deep learning scaling efforts, and is a large multimodal model that exhibits human-level performance on various professional and academic benchmarks. GPT4 outperforms ChatGPT on most tasks.\n\u2022 GPT-3 (Ouyang et al., 2022) is a series of models. In this paper, we simply call textdavinci-003 with GPT-3. Text-davinci-003 is capable of performing any language task with better quality, longer output, and more consistent instruction-following than GPT-3.\n\u2022 ChatGLM-6B (Du et al., 2022; Zeng et al.,\n2023) is an open-source dialogue language model that supports both Chinese and English bilinguals. Utilizing technology similar to ChatGPT, it is optimized for Chinese question-answering and dialogue. After about 1T identifiers of Chinese and English bilingual training, supplemented by supervision, fine-tuning, feedback self-help, human feedback reinforcement learning, and other technologies, ChatGLM-6B with 6.2 billion parameters can generate answers that closely align with human preferences.\n\u2022 The BELLE-7B-2M (Ji et al., 2023b,a) model is based on Bloomz-7b1-mt and trained on 2M pieces of Chinese data, combined with 50,000 pieces of English data open-sourced by Stanford-Alpaca. It has demonstrated good Chinese instruction understanding and response generation capabilities.\n\u2022 ChatYuan (ClueAI, 2023) is further trained based on PromptCLUE-large (Zhang and Xu, 2022), combined with hundreds of millions of functional dialogues and multiple rounds of dialogue data. ChatYuan is capable of answering questions in fields such as law, and can be used for question-answering, dialogue in context, creative writing.\nModel Acc(%) Rouge-1 Rouge-2 Rouge-L GPT-4 75.7 0.384 0.140 0.247 ChatGPT 54.5 0.341 0.114 0.216 GPT-3 40.2 - - - ChatGLM-6B 29.1 0.315 0.099 0.184 BELLE-7B-2M 33.3 - - - ChatYuan 27.0 - - -"
        },
        {
            "heading": "5 Results",
            "text": "ments for some models, such as GPT-3. Moreover, some models did not respond with answers and explanations even when we requested them, which is why some models lack a Rouge score."
        },
        {
            "heading": "5.1 Automatic Metrics",
            "text": "The performance of each model in each setting can be found in Appendix C. Table 3 presents the best performance of each model on the test set, regardless of whether the prompt of each model is consistent. Not surprisingly, GPT-4 is the bestperforming model, achieving 75.7% accuracy with the most suitable set of 1-shot without instruction. Therefore, GPT-4 has demonstrated the ability to pass the National Licensed Pharmacist Examination in China and has outperformed more than 80% of the people who take the examination. In contrast, ChatGPT achieved an accuracy rate of 54.5% on the test set, which would not be sufficient to pass the examination. GPT-3, ChatGLM-6B, BELLE-7B-2M, and ChatYuan achieved accuracies of 40.2%, 29.1%, 33.3%, and 27.0% on the test set, respectively. Models with fewer parameters generally perform worse than models with more parameters. This can be attributed to smaller parameter sizes, which means that models may not have the capacity to remember enough knowledge or understand the meaning of context.\nRegarding interpretable automatic evaluation indicators, GPT-4 achieved the best results in explanation generation with a Rouge-L score of 0.247, followed by ChatGPT, ChatGLM-6B, and BELLE7B-2M. ChatGLM-6B yielded unexpected results in metrics, despite its relatively small parameter size, with high accuracy and Rouge scores.\nWe plotted line charts of model performance as a function of the number of few-shots. The line chart is divided into two, the chart on the left for chat models and the chart on the right for non-chat models. From the figure, we identified three key findings.\nFirstly, it is evident from Figure 3 that regard-\nless of the size of the model parameters or whether instructions are given, going from zero-shot to oneshot often results in a significant performance improvement, which is better than any subsequent increase of few-shot examples.\nSecondly, when comparing chat models and nonchat models, GPT-3 is a model with a large number of parameters but weak dialogue ability, while GPT-4 and ChatGPT are models with strong dialogue ability. Regardless of whether instructions are provided, the performance of GPT-3 increases with an increase in few-shot examples, but GPT-4 and ChatGPT tend to achieve their maximum performance in one-shot setting. This suggests that for a model with a large number of parameters and strong dialogue ability, one-shot setting is a good choice. Conversely, for models with weak dialogue ability, their performance is somewhat proportional to the number of few-shot examples.\nThirdly, when comparing the two figures, the models in the left picture have strong dialogue ability. Therefore, in the case of the same number of few-shot examples, providing instructions is better than not providing instructions. However, in the right picture, the models have weak dialogue ability. Therefore, in the case of the same number of few-shot examples, not providing instructions is better."
        },
        {
            "heading": "5.2 Human Evaluation",
            "text": "From the perspective of interpretability, there are certain limitations in using the rouge evaluation metric to evaluate the interpretability of the model. So we also used human evaluation to assess the qualitative properties of the generated explanations. We follow Monsen and Rennes (2022), Wiegreffe et al. (2022) and Kunz et al. (2022) asking annotators to rate from 1 to 5 according to the following questions for each e.\n\u2022 Is e a well-formed sentence?\n\u2022 Does e support the label?\n\u2022 Is the content of e factually correct?\n\u2022 Does e provide a valid reasoning path for the label?\n\u2022 Does e add new information, rather than recombining information from the input?\nDue to the low accuracy of some models and the poor quality of the generated explanations, we only manually evaluated a subset of the models, and the results are presented in Table 4. As expected, GPT-4 remains the best performer. It is noteworthy that the performance of GPT-4 and ChatGPT in terms of well-formed and support is the same. This indicates that both GPT-4 and ChatGPT can comprehend the question\u2019s requirements, provide the label, and generate a complete and coherent explanation that supports the label. However, GPT-4 outperforms ChatGPT in terms of the correctness of the explanation, effectiveness of the explanation process, and novelty. ChatGLM lags behind ChatGPT and GPT-4 on all five indicators. And We also ask GPT-4 to evaluate responses, the relative scores of different metrics are consistent with human evaluation results."
        },
        {
            "heading": "5.3 Error Analyses",
            "text": "In this subsection, we present an analysis of the performance of the models from an overall perspective and specific examples. Table 5 displays the performance of the models on different types of questions. Notably, GPT-4 and ChatGPT perform better on negative questions, while other models perform better on positive questions. Moreover, GPT-4 demonstrates improvement in logical reasoning, whereas other models do not. While GPT4 improves in scenario analysis questions, other models exhibit a decline. Conversely, GPT-4 declines in general knowledge questions while other models improve. GPT-4 correctly solves two mathematical calculation questions, whereas ChatGPT\nfails on all such questions. These findings suggest that GPT-4 has stronger logical reasoning, scenario analysis, and mathematical calculation abilities than other models. The superior performance of GPT-4 and ChatGPT on negative questions indicates their better understanding of text and ability to answer questions.\nWe analyze specific error cases of ChatGPT and GPT-4 to identify the limitations of current LLMs. Appendix D outlines the reasons for explanation errors. Although the results of LLMs are impressive, they are not yet perfect. In example 1, GPT-4 provides the correct answer but the wrong explanation, which is difficult to detect. Thus, models should pay close attention to such errors before widespread use. In Example 2, although the model has a certain calculation capability, the reliability of its calculation is still not guaranteed. In Example 3, neither GPT-4 nor ChatGPT fully comprehends the detailed requirements of the question, leading to errors. Therefore, LLMs still have scope for improvement in text comprehension and generating explanations."
        },
        {
            "heading": "6 Conclusion",
            "text": "In this work, we propose ExplainCPE, a challenging medical dataset for natural language explanation evaluation. Our study on ExplainCPE dataset demonstrates the potential of LLMs in medical question answering with explanations. Our analysis of model performance on different types of questions reveals the strengths and limitations of different LLMs in terms of in-context learning. The error cases point out the need for further improvement in LLMs in explanation generation and text comprehension. Further work can use our dataset to improve and evaluate the model interpretability.\nLimitations\nDue to the lack of interpretable benchmarks in the medical professional field, we present ExplainCPE in this paper. While there are many explainable methods, we only contribute to the Explanation Generation. Moreover, most of the current interpretable methods are aimed at classification tasks. For LLMs which are used to generate response, new interpretable methods are necessary. We explore the ability of LLMs in medical diagnosis and interpretability. While model performance can be well assessed by accuracy, automatic assessment of interpretability is still lacking.\nHowever, our analysis of ExplainCPE dataset is just a preliminary exploration, and there is still much room for further research and development. For example, future work can focus on improving the quality and diversity of the explanations in the dataset, expanding the coverage of medical knowledge, and exploring new evaluation metrics for interpretability. In addition, more advanced LLMs can be developed to further improve the performance of medical question answering with explanations by utilizing the data in the training set. We believe that the ExplainCPE dataset can serve as a valuable resource for the research community to advance the field of medical question answering and LLMs.\nEthical Statement\nThis paper is concerned about proposing a dataset on explanations of medical question answers. The data in this dataset are all from Chinese Pharmacist Examination related exercises. Moreover, the cases in the exercises are all fictitious cases, and there is no personal privacy, discrimination or attack content. Judging from its impact, this data set can be used to improve the interpretation ability in human medical diagnosis, reduce misdiagnosis, and contribute to human intelligent medical care."
        },
        {
            "heading": "Acknowledgments",
            "text": "We thank Yuxiang Wu for valuable feedback and support on running experiments. We thank Xinshuo Hu for helpful discussions and suggestions. This work is jointly supported by grants: Natural Science Foundation of China (No. 62006061, 82171475), Strategic Emerging Industry Development Special Funds of Shenzhen (No.JCYJ20200109113403826)."
        },
        {
            "heading": "A Prompting Template",
            "text": "There are two types of prompt templates, prompting with instruction and prompting without instruction. And you can check the template in Table 6. You can also see the template instantiation in Table 7."
        },
        {
            "heading": "B Distribution of Categories",
            "text": "In Figure 4, we show the proportion distribution of each type in the dataset in more detail."
        },
        {
            "heading": "C Performance Comparison",
            "text": "Perhaps due to the training data or model size, these models do not respond well to a given multiple-choice question. We have already evaluated two popular medical LLMs\u2014ChatGLM-Med and Huatuo-Llama-Med-Chinese\u2014and found that they struggled with our multi-choice questions.\nWang et al. (2023b) constructed a Chinese medical instruction data set through the medical knowledge graph and GPT3.5 API, and based on this, fine-tuned the instructions of ChatGLM-6B to improve the question-answering effect of ChatGLM in the medical field called Med-ChatGLM (Wang et al., 2023b). Based on the same data, we also trained a medical version of the LLaMA model called Huatuo (Wang et al., 2023a).\nIn Table 8, we show the results of each model under all settings, including the number of fewshots and with or without instructions."
        },
        {
            "heading": "D Error Example",
            "text": "In this section, we mainly show some classic errors made by ChatGPT and GPT-4. We analyzed these errors in order to study the current shortcomings of LLMs.\nExample 1 Question:The patient, female, 65 years old, received the second cycle of chemotherapy for breast cancer. Which antiemetic drugs are prohibited? Option A:Metoclopramide.\u2713 Option B:Ondansetron. Option C:Dexamethasone. Option D:Diphenhydramine. Option E:Aprepitant. Gold Explanation: This question examines antiemetic drugs. Optional antiemetic drugs are mainly dopamine receptor blockers (such\nas metoclopramide), 5-HT3 receptor blockers (such as ondansetron, tropisetron, and palonosetron, etc.) , corticosteroids (such as dexamethasone), anticholinergics and antihistamines (such as diphenhydramine), and NK-1 receptor blockers (such as aprepitant), etc. Special emphasis should be placed on the prohibition of metoclopramide as an antiemetic in patients with breast cancer. Therefore, choose A for this question. ChatGPT Response: Answer: B. Ondansetron. Analysis: Ondansetron is a 5-HT3 receptor antagonist, mainly used to prevent nausea and vomiting caused by chemotherapy for malignant tumors. However, ondansetron is contraindicated in patients with hepatic insufficiency or pregnancy, because these patients have reduced ability to metabolize and clear drugs, which may easily cause drug accumulation and cause side effects. For the specific situation of the patient, it should be selected according to the clinical judgment of the doctor. But in this case, ondansetron is contraindicated, so the correct answer is B. GPT-4 Response: Answer: A. Metoclopramide. Analysis: This question examines the contraindications of antiemetic drugs. Metoclopramide is a dopamine D2 receptor antagonist, and its anti-emetic effect is achieved by blocking central and peripheral 5-HT3 receptors and dopamine D2 receptors. Elderly patients over 65 years of age are contraindicated for metoclopramide, because the drug may increase central nervous system adverse reactions in the elderly population, such as drowsiness, ataxia, and akathisia. Other antiemetics, such as ondansetron, dexamethasone, diphenhydramine, and aprepitant, are safe in older patients. Therefore, the correct answer is A. Error Analyse: GPT-4 is right. ChatGPT is wrong. Although GPT-4 makes a right prediction, its explanation is flawed because the reason given by GPT-4 is old age but the real reason is cancer. Example 2 Question: 100 mg of a certain drug is injected intravenously, blood samples are collected immediately, and the drug concentration is measured to be 5\u03bcg/ml, what is the apparent volume of distribution? Option A: 5L.\nOption B: 2L. Option C: 20L.\u2713 Option D: 50L. Option E: 29ml. Gold Explanation: This question examines the calculation of the apparent volume of distribution. The formula for calculating the apparent volume of distribution is \"V=X/C\", where V is the apparent volume of distribution, X is the amount of drug in the body, and C is the blood drug concentration. V=X/C=100mg/5\u03bcg/ml=20L. Others are interference options, so choose C for this question. ChatGPT Response: Answer: D.50L. Analysis: Apparent volume of distribution (Vd) is a parameter representing the distribution range of a drug, and its calculation formula is: Vd\n= total drug dose/drug concentration. In this question, the total dose of the drug is 100 mg, and the drug concentration is 5\u03bcg/ml (note that the unit should be the same). The total dose of the drug is converted to \u03bcg to be 100,000\u03bcg, which is substituted into the calculation formula: Vd = 100,000\u03bcg/5\u03bcg/ml = 20,000ml = 20L. Because the distribution of drugs not only occurs in the blood, but may also be distributed in cells, tissues and organs, etc., the apparent distribution volume of common drugs is relatively large, generally ranging from tens of liters to hundreds of liters (ie: 0.03L/kg to 0.8L/kg). The answer to this question is option D, and 50L is within the apparent volume of distribution of common drugs. GPT-4 Response: Answer: C.20L. Explanation: The apparent volume of distribution (Vd)\nis a theoretical value used to describe the range of drug distribution in the body. The formula for calculating Vd is: Vd = drug dose / initial drug concentration. In this question, the drug dose is 100 mg, and the measured drug concentration is 5\u03bcg/ml. First convert the dose to micrograms: 100mg = 100,000\u03bcg. Then apply the Vd formula: Vd = 100,000\u03bcg / 5\u03bcg/ml = 20,000ml, which is 20L. Therefore, the apparent volume of distribution of the drug is 20L, and the correct answer is C. Error Analyse: GPT-4 is right. ChatGPT is wrong. ChatGPT gets an answer in the middle process, but it do not take that answer. Example 3 Question:What are the medicines used to prevent relapse and transmission and to prevent malaria? Option A:Quinine. Option B:Hydroxychloroquine. Option C:Primaquine.\u2713 Option D:Amodiaquine. Option E:Artemisinin. Gold Explanation: This question examines the action characteristics of primaquine. Primaquine can kill Plasmodium vivax, Malaria malaria, and Plasmodium falciparum. It can also kill the gametocytes of various Plasmodium parasites, and is clinically used as the drug of choice for controlling recurrence and preventing transmission. Quinine, chloroquine, and artemisinin are used as drugs to control the symptoms of malaria. So choose C for this question. ChatGPT Response: Answer: E. Artemisinin. Analysis: Artemisinin is currently an effective drug for treating malaria, especially for falciparum malaria and refractory malaria. In addition, artemisinin also has certain antitumor and antiviral effects. Quinine is another drug used to treat malaria, but it is mainly used to treat the symptoms and complications of malaria, not to treat malaria itself directly. Hydroxychloroquine, primaquine, and amodiaquine are also drugs used to treat malaria, but drug resistance has emerged and their use is limited. Therefore, artemisinin is considered to be the drug of choice for the treatment of malaria. GPT-4 Response: Answer: B. Hydroxychloroquine. Analysis: This question examines malaria prevention drugs. Hydroxychloro-\nquine is an antimalarial drug used for the prevention and treatment of malaria, especially against infection by susceptible strains of Plasmodium. It can not only kill the malaria parasite, but also reduce the transmission probability of the malaria parasite, so it is used to prevent the recurrence and transmission of malaria. Other options Quinine, primaquine, amodiaquine, and artemisinin are mainly used in the treatment of malaria. So the correct answer is B. Error Analyse: Both GPT-4 and ChatGPT are wrong. The question asks for a drug to prevent malaria, but ChatGPT gives a drug to treat malaria. GPT-4 says that Hydroxychloroquine can prevent malaria, but Hydroxychloroquine is actually used to control malaria, not to prevent malaria. Example 4 Question: Which medicines require water restriction while taking? Option A:cough syrup.\u2713 Option B:Diprophylline. Option C:ursodeoxycholic acid. Option D:ritonavir. Option E:Alendronate Sodium. Gold Explanation: This question examines drugs that should restrict drinking water. Cough medicines such as cough syrup, licorice mixture, etc. These viscous medicines will adhere to the inflamed throat and play a role. After use, drink less water, especially hot water, to avoid washing the local medicines. Taking theophylline or theophylline controlledrelease tablets, aminophylline, cholophylline, dihydroxyprophylline, etc., because it can increase renal blood flow, has a diuretic effect, increases urine output and easily causes dehydration, dry mouth, Polyuria or heart palpitations; at the same time, asthmatics are often accompanied by low blood volume. Therefore, it is advisable to pay attention to the right amount of fluids and drink plenty of boiled water. Phenylpropanol, hydroxymethylcoumarol, dehydrocholic acid and ursodeoxycholic acid in choleretic drugs can cause excessive secretion of bile and diarrhea. Therefore, drink plenty of water when taking it to avoid dehydration caused by excessive diarrhea. Among protease inhibitors, ritonavir, indinavir, nelfinavir, amprenavir, lopinavir, etc., most of them can\ncause urethral stones or kidney stones. Therefore, sufficient hydration should be ensured during the treatment period. In order to avoid the occurrence of stones, it is advisable to increase the daily water intake, and the daily drinking water should be more than 2000ml. Bisphosphonates are irritating to the esophagus and must be taken with more than 200ml of water; among them, sodium alendronate, sodium patradronate, and disodium clodronate can cause water when they are used to treat hypercalcemia. , Electrolyte disorder, so attention should be paid to replenishing fluids so that the daily urine output can reach more than 2000ml. Therefore, choose A for this question. ChatGPT Response: Answer: B. Diprophylline. Analysis: Diprophylline is metabolized in the body to produce uric acid, and uric acid is easy to cause stones, so you need to limit the amount of water you drink when taking it. Drugs such as cough syrup, ursodeoxycholic acid, ritonavir, and alendronate do not require water restriction. GPT-4 Response: Answer: E. Alendronate Sodium. Analysis: This question examines the contraindications and precautions of the drug. Alendronate Sodium (Alendronate Sodium) is a drug used to treat fractures, osteoporosis and other symptoms. It improves bone density by inhibiting bone resorption. Drinking water should be limited when taking alendronate sodium, because it needs to be taken on an empty stomach, and you must stand or sit upright for at least 30 minutes immediately, and avoid lying down after taking the medicine to prevent side effects such as esophagitis, stricture and ulcer. Other options, such as cough syrup, dyphylline, ursodeoxycholic acid, and ritonavir, do not require fluid restriction. Therefore, the correct answer is E. alendronate. Error Analyse: Both GPT-4 and ChatGPT are wrong. The reason for the error is the lack of mechanism by which cough syrup works. Example 5 Question:A 28-year-old female patient was given isotretinoin capsules because of acne. Which statement about drug education for this patient is false? Option A:Contraceptive measures should be taken during treatment and within 3 months after drug withdrawal.\nOption B:If skin and mucous membrane dryness occurs during treatment, the dose should be reduced or the drug should be discontinued.\u2713 Option C:Adverse reactions are common, but most of them can be recovered after stopping the drug. Option D:The first 2 months of treatment may lead to abnormal liver enzymes, check liver function if necessary. Option E:Option E: Some patients experienced transient exacerbation of rash after 2-4 weeks of medication. Gold Explanation: This question examines the precautions for oral retinoic acid drugs. (1) Adverse reactions of isotretinoin are common, but most of them can be recovered after stopping the drug, and serious adverse reactions are rare or even rare. (2) The most common adverse reaction is dry skin and mucous membranes, and it is recommended to be used with skin barrier repair agents. (3) Rare adverse reactions include musculoskeletal pain, elevated blood lipids, abnormal liver enzymes, and dry eyes, which usually occur in the first 2 months of treatment, and should be used with caution in patients with obesity, dyslipidemia, and liver disease. (4) Isotretinoin has a clear teratogenic effect. Female patients of childbearing age and their spouses should strictly avoid pregnancy 1 month before treatment, during treatment and within 3 months after treatment. (5) It should be used with caution in patients who already have obvious depressive symptoms or suffer from depression. (6) Some patients will experience short-term exacerbation of skin rash after 2 to 4 weeks of use, which is usually transient, and those with severe reactions need to reduce the dose or even stop the drug. Therefore, choose B for this question. ChatGPT Response: Answer: A. Contraceptive measures should be taken during treatment and within 3 months after drug withdrawal. Analysis: Isotretinoin capsule is a medicine used to treat acne, and its main ingredient is retinoic acid. Contraceptive measures should be taken during drug treatment and within 3 months after stopping the drug, because the drug can affect fetal development. Know the contraindications before treatment, such as pregnant women, lactating women, and those\nwith severe liver and kidney impairment. If dry skin and mucous membranes are found during treatment, the dose should be reduced or the drug should be stopped to avoid aggravating the condition. The first 2 months of treatment may lead to abnormal liver enzymes, so the patient\u2019s liver function should be checked before treatment, and if necessary, liver function should be checked regularly during the medication. Some patients experience transient exacerbation of the rash after 2 to 4 weeks of medication. This is a normal phenomenon, and there is no need to stop the medication, and it will be relieved within a few days. Therefore, the correct answer to this question is A. GPT-4 Response: Answer: B. If skin and mucous membrane dryness occurs during treatment, the dose should be reduced or the drug should be discontinued. Analysis: Isotretinoin capsules may cause some adverse reactions during the treatment of acne. Contraceptive measures should be taken during treatment and within 3 months after drug withdrawal, because isotretinoin has a strong teratogenic effect (choice A is correct). Dry skin or dry mucous membranes may occur during treatment, but dose reduction or drug withdrawal is not required; usually it can be relieved by using mild skin care products, lip balm, artificial tears, etc. (option B is wrong). Adverse reactions are common, but most of them can be recovered after stopping the drug (choice C is correct). The first 2 months of treatment may lead to abnormal liver enzymes, check liver function if necessary (option D is correct). Some patients experienced transient exacerbation of rash during 2 to 4 weeks of medication (choice E is correct). Therefore, the wrong statement is option B. Error Analyse: GPT-4 is right. ChatGPT is wrong. The explanation given by ChatGPT is correct and conforms to the gold explanation, but the label it gives is wrong. Example 6 Question:The patient, male, 62 years old, developed bradykinesia 4 years ago and was diagnosed with Parkinson\u2019s disease. Symptoms are currently progressively worsening with mild cognitive impairment. Which drug should be used? Option A:pramipexole.\nOption B:Selegiline. Option C:Adamantadine. Option D:Entacapone. Option E:Benserazide-levodopa.\u2713 Gold Explanation: This question examines the drug treatment of Parkinson\u2019s. 1. Earlyonset patients without mental decline: (1) Pramipexole, selegiline or compound levodopa + COMT inhibitor (entecapone) should be the first choice if the treatment guidelines in the United States and Europe are followed; (2) If the patient cannot afford high-priced drugs due to economic reasons, amantadine is the first choice; (3) If due to the needs of special work, the patient strives to significantly improve motor symptoms or suffers from cognitive decline, the first choice is the first choice Compound levodopa or compound levodopa + COMT inhibitor (entecapone); (4) obvious tremor but poor efficacy of other drugs \u2192 trihexyphenidyl. 2. Late onset or accompanied by mental decline (1) Compound levodopa is generally the first choice for treatment. (2) Try not to use anticholinergic drugs, especially for elderly male patients. Therefore, choose E for this question. ChatGPT Response: Answer: B. Selegiline Analysis: The main manifestation of Parkinson\u2019s disease is movement disorder, because Parkinson\u2019s disease is caused by the loss of dopaminergic neurons, so dopamine receptor agonists are one of the treatments for Parkinson\u2019s disease drugs. Pramipexole is also an effective treatment for Parkinson\u2019s disease in young people, but it is less suitable for this patient because of its more side effects in older patients. Amantadine works similarly to dopamine in treating Parkinson\u2019s disease, but it often causes side effects in elderly patients, so it is not suitable for this patient. Entacapone is a combination drug for the treatment of Parkinson\u2019s disease, which consists of levodopa and entapazide, but this drug is prone to side effects such as lowering of blood pressure, and is not suitable for elderly patients with mild cognitive impairment. Therefore, it is a better choice to choose B. selegiline, which is a selective dopamine D2 and D3 receptor agonist, which will not cause aggravation of dyskinesia symptoms such as muscle stiffness and tremor, and can improve cognitive cognitive barriers. GPT-4 Response: Answer: D. Entacapone\nAnalysis: Parkinson\u2019s disease treatment drugs include dopamine precursors (levodopa and carbidopa), dopamine receptor agonists (selegiline, pramipexole), COMT inhibitors Entacapone, MAO-B inhibitors (Utahsi, Ruizuosi, etc.), anticholinergic amantadine. This patient is 62 years old, with a medical history of 4 years. The treatment effect of dopamine precursor combined with dopamine receptor agonist is more accurate. Considering the patient has mild cognitive impairment, the anticholinergic drug amantadine should be avoided. It is planned to use entacapone as the COMT inhibitor, which is beneficial to prolong the effect of dopamine prodrugs and improve the symptoms of patients. Therefore, Entacapone was chosen. The correct answer is D. Error Analyse: Both are wrong. The model is more inclined to prescribe medicine to the disease, and it is difficult to make a diagnosis based on the specific situation of the patient."
        }
    ],
    "title": "ExplainCPE: A Free-text Explanation Benchmark of Chinese Pharmacist Examination",
    "year": 2023
}