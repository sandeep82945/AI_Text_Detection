{
    "abstractText": "Due to the remarkable language understanding and generation abilities of large language models (LLMs), their use in educational applications has been explored. However, little work has been done on investigating the pedagogical ability of LLMs in helping students to learn mathematics. In this position paper, we discuss the challenges associated with employing LLMs to enhance students\u2019 mathematical problem-solving skills by providing adaptive feedback. Apart from generating the wrong reasoning processes, LLMs can misinterpret the meaning of the question, and also exhibit difficulty in understanding the given questions\u2019 rationales when attempting to correct students\u2019 answers. Three research questions are formulated.",
    "authors": [
        {
            "affiliations": [],
            "name": "An-Zi Yen"
        },
        {
            "affiliations": [],
            "name": "Wei-Ling Hsu"
        },
        {
            "affiliations": [],
            "name": "Yang Ming Chiao"
        }
    ],
    "id": "SP:834348552fac9df77ce05a51b7eec5fd69e016fd",
    "references": [
        {
            "authors": [
                "Nakano"
            ],
            "title": "Training verifiers to solve math",
            "year": 2021
        },
        {
            "authors": [
                "Leila Khalatbari",
                "Maria Ryskina",
                "Rita Frieske",
                "Ryan Cotterell",
                "Zhijing Jin."
            ],
            "title": "State-of-the-art generalisation research in NLP: a taxonomy and review",
            "venue": "CoRR.",
            "year": 2022
        },
        {
            "authors": [
                "Gautier Izacard",
                "Patrick Lewis",
                "Maria Lomeli",
                "Lucas Hosseini",
                "Fabio Petroni",
                "Timo Schick",
                "Jane Dwivedi-Yu",
                "Armand Joulin",
                "Sebastian Riedel",
                "Edouard Grave."
            ],
            "title": "Few-shot learning with retrieval augmented language models",
            "venue": "arXiv preprint",
            "year": 2022
        },
        {
            "authors": [
                "Adi Jafar",
                "Ramli Dollah",
                "Nordin Sakke",
                "Mohammad Tahir Mapa",
                "Ang Kean Hua",
                "Oliver Valentine Eboy",
                "Eko Prayitno Joko",
                "Diana Hassan",
                "Chong Vun Hung"
            ],
            "title": "Assessing the challenges of e-learning in Malaysia during the pandemic",
            "year": 2022
        },
        {
            "authors": [
                "Zhanming Jie",
                "Jierui Li",
                "Wei Lu."
            ],
            "title": "Learning to reason deductively: Math word problem solving as complex relation extraction",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages",
            "year": 2022
        },
        {
            "authors": [
                "Poquet",
                "Michael Sailer",
                "Albrecht Schmidt",
                "Tina Seidel",
                "Matthias Stadler",
                "Jochen Weller",
                "Jochen Kuhn",
                "Gjergji Kasneci."
            ],
            "title": "ChatGPT for good? On opportunities and challenges of large language models for education",
            "venue": "Learning and Individual Differ-",
            "year": 2023
        },
        {
            "authors": [
                "Md Tahmid Rahman Laskar",
                "M Saiful Bari",
                "Mizanur Rahman",
                "Md Amran Hossen Bhuiyan",
                "Shafiq Joty",
                "Jimmy Xiangji Huang."
            ],
            "title": "A systematic study and comprehensive evaluation of chatgpt on benchmark datasets",
            "venue": "arXiv preprint arXiv:2305.18486.",
            "year": 2023
        },
        {
            "authors": [
                "Hunter Lightman",
                "Vineet Kosaraju",
                "Yura Burda",
                "Harri Edwards",
                "Bowen Baker",
                "Teddy Lee",
                "Jan Leike",
                "John Schulman",
                "Ilya Sutskever",
                "Karl Cobbe."
            ],
            "title": "Let\u2019s verify step by step",
            "venue": "arXiv preprint arXiv:2305.20050.",
            "year": 2023
        },
        {
            "authors": [
                "Pengfei Liu",
                "Weizhe Yuan",
                "Jinlan Fu",
                "Zhengbao Jiang",
                "Hiroaki Hayashi",
                "Graham Neubig."
            ],
            "title": "Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing",
            "venue": "ACM Computing Surveys, 55(9):1\u201335.",
            "year": 2023
        },
        {
            "authors": [
                "Ruibo Liu",
                "Jason Wei",
                "Shixiang Shane Gu",
                "Te-Yen Wu",
                "Soroush Vosoughi",
                "Claire Cui",
                "Denny Zhou",
                "Andrew M Dai"
            ],
            "title": "Mind\u2019s Eye: Grounded language",
            "year": 2022
        },
        {
            "authors": [
                "Gr\u00e9goire Mialon",
                "Roberto Dess\u00ec",
                "Maria Lomeli",
                "Christoforos Nalmpantis",
                "Ram Pasunuru",
                "Roberta Raileanu",
                "Baptiste Rozi\u00e8re",
                "Timo Schick",
                "Jane Dwivedi-Yu",
                "Asli Celikyilmaz"
            ],
            "title": "Augmented language models: A survey",
            "venue": "arXiv preprint arXiv:2302.07842",
            "year": 2023
        },
        {
            "authors": [
                "Steven Moore",
                "Huy A Nguyen",
                "Norman Bier",
                "Tanvi Domadia",
                "John Stamper."
            ],
            "title": "Assessing the quality of student-generated short answer questions using GPT-3",
            "venue": "Educating for a New Future: Making Sense of Technology-Enhanced Learning Adop-",
            "year": 2022
        },
        {
            "authors": [
                "Paul Christiano",
                "Jan Leike",
                "Ryan Lowe."
            ],
            "title": "Training language models to follow instructions with human feedback",
            "venue": "Advances in Neural Information Processing Systems, 35:27730\u201327744.",
            "year": 2022
        },
        {
            "authors": [
                "Arkil Patel",
                "Satwik Bhattamishra",
                "Navin Goyal"
            ],
            "title": "Are NLP models really able to solve simple math word problems",
            "venue": "In Proceedings of the 2021 Conference of the North American Chapter of the Association",
            "year": 2021
        },
        {
            "authors": [
                "P Jonathon Phillips",
                "Carina A Hahn",
                "Peter C Fontana",
                "David A Broniatowski",
                "Mark A Przybocki."
            ],
            "title": "Four principles of explainable artificial intelligence",
            "venue": "Gaithersburg, Maryland, page 18.",
            "year": 2020
        },
        {
            "authors": [
                "Michael Sailer",
                "Elisabeth Bauer",
                "Riikka Hofmann",
                "Jan Kiesewetter",
                "Julia Glas",
                "Iryna Gurevych",
                "Frank Fischer."
            ],
            "title": "Adaptive feedback from artificial neural networks facilitates pre-service teachers\u2019 diagnostic reasoning in simulation-based learning",
            "venue": "Learning",
            "year": 2023
        },
        {
            "authors": [
                "Timo Schick",
                "Jane Dwivedi-Yu",
                "Roberto Dess\u00ec",
                "Roberta Raileanu",
                "Maria Lomeli",
                "Luke Zettlemoyer",
                "Nicola Cancedda",
                "Thomas Scialom."
            ],
            "title": "Toolformer: Language models can teach themselves to use tools",
            "venue": "arXiv preprint arXiv:2302.04761.",
            "year": 2023
        },
        {
            "authors": [
                "Jianhao Shen",
                "Yichun Yin",
                "Lin Li",
                "Lifeng Shang",
                "Xin Jiang",
                "Ming Zhang",
                "Qun Liu."
            ],
            "title": "Generate & rank: A multi-task framework for math word problems",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2021, pages 2269\u20132279.",
            "year": 2021
        },
        {
            "authors": [
                "Valerie J Shute."
            ],
            "title": "Focus on formative feedback",
            "venue": "Review of Educational Research, 78(1):153\u2013189.",
            "year": 2008
        },
        {
            "authors": [
                "Ana\u00efs Tack",
                "Chris Piech."
            ],
            "title": "The AI Teacher Test: Measuring the pedagogical ability of Blender and GPT-3 in educational dialogues",
            "venue": "Proceedings of the 15th International Conference on Educational Data Mining, page 522.",
            "year": 2022
        },
        {
            "authors": [
                "Hugo Touvron",
                "Thibaut Lavril",
                "Gautier Izacard",
                "Xavier Martinet",
                "Marie-Anne Lachaux",
                "Timoth\u00e9e Lacroix",
                "Baptiste Rozi\u00e8re",
                "Naman Goyal",
                "Eric Hambro",
                "Faisal Azhar"
            ],
            "title": "Llama: Open and efficient foundation language models",
            "year": 2023
        },
        {
            "authors": [
                "Yan Wang",
                "Xiaojiang Liu",
                "Shuming Shi."
            ],
            "title": "Deep neural solver for math word problems",
            "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 845\u2013854.",
            "year": 2017
        },
        {
            "authors": [
                "Jason Wei",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Maarten Bosma",
                "brian ichter",
                "Fei Xia",
                "Ed H. Chi",
                "Quoc V Le",
                "Denny Zhou"
            ],
            "title": "2022. Chain of thought prompting elicits reasoning in large language models",
            "venue": "In Advances in Neural Information Processing Systems",
            "year": 2022
        },
        {
            "authors": [
                "Cheng-Kuang Wu",
                "Wei-Lin Chen",
                "Hsin-Hsi Chen"
            ],
            "title": "Large language models perform diagnostic reasoning",
            "year": 2023
        },
        {
            "authors": [
                "Weijiang Yu",
                "Yingpeng Wen",
                "Fudan Zheng",
                "Nong Xiao."
            ],
            "title": "Improving math word problems with pre-trained knowledge and hierarchical reasoning",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "After the pandemic, e-learning has become part of mainstream education (Alqahtani and Rajkhan, 2020; Jafar et al., 2022). However, for students, online learning is not without its problems (Abdur Rehman et al., 2021). Apart from the difficulty in maintaining focus in online classes, the lack of real-time communication and timely feedback are also serious problems. In particular, in online assessments, students may fail to understand their mistakes, even after reviewing the provided answers; such failure to immediately clarify points of confusion yields poor learning outcomes. Synchronous communication between teachers and students in an e-learning setting is necessary, but teachers find that promptly responding to students\u2019 questions is a significant challenge.\nAs large language models (LLMs) offer a wide range of applications, several studies (Tack and Piech, 2022; Kasneci et al., 2023; Zhang et al., 2023) address the use of LLMs in education. In this work, we seek to investigate the integration\n\u2217Corresponding author.\nof LLMs into education. Since each subject has its own issues that must be addressed, we explore the scenario of LLM use in mathematics education and the challenges thereof. Many studies address math word problem solving (Shen et al., 2021; Yu et al., 2021; Jie et al., 2022), but the human utility of the mathematics reasoning processes and the natural language explanations generated by models within educational settings is rarely discussed. Given the extraordinary capability of LLMs to generate free-text rationalization, we investigate their mathematical problem-solving competence and assess whether the generated step-by-step explanations constitute a useful educational resource. In particular, we analyze the ability of LLMs to provide adaptive feedback by identifying and explaining errors within a student\u2019s math problem-solving process.\nWe address these issues using the MathQA dataset (Amini et al., 2019), which consists of GRE-level questions that require advanced mathematical knowledge. The questions cover arithmetic, algebra, geometry, and data analysis. In Appendix A we explain in detail our reasoning behind the dataset selection. For all experiments, we use GPT-3.5 (Ouyang et al., 2022). We raise and discuss a total of three research questions. In this pilot study, we conduct both quantitative and qualitative evaluations to answer the research questions. The contributions of this work are threefold.\n1. We explore the application of LLMs for instructing students in solving math word problems. 2. Drawing on the results of LLMs to solve math word problems, we comprehensively identify the existing problems of current models. 3. We discuss the pedagogical suitability of the generated equations and the corresponding free-text explanations in the context of mathematics education."
        },
        {
            "heading": "2 Utility of LLMs in Mathematics Learning",
            "text": "To explore the utility of LLMs in mathematics learning, we raise the first research question (RQ1): How can LLMs be utilized to assist students in learning mathematical problem-solving skills? Since exams are a common way for teachers to evaluate student learning progress, in this study, we focus on leveraging LLMs to equip students with the knowledge skills needed to solve math word problems.\nIn terms of question types, in addition to true/false and multiple-choice questions, shortanswer questions are also included in math exams. Students answer the questions using problemsolving processes. Automated assessment (Moore et al., 2022) of the student\u2019s problem-solving process remains to be investigated. In education, feedback is also crucial (Shute, 2008). Simply scoring the student answer is often insufficient, as scores do not reflect the reasons for incorrect answers. To learn from their mistakes, students need the corresponding explanation. Figure 1 illustrates the scenario of an LLM applied in mathematics education. After a student responds to a question, the model determines whether the student\u2019s answer is correct or not. If the answer is correct, the system informs the student of the test objective for that question, enhancing their understanding of the examined skills. If the answer is incorrect, the system provides adaptive feedback (Bernius et al., 2022; Sailer et al., 2023) by indicating the location of the error and offering an explanation, assisting the student to clarify any misunderstanding.\nGiven that there are multiple methods for solving a math word problem, a model must be able to understand and correct a student\u2019s thought process. If the overall strategy is sound apart from minor concept errors or obstacles, the model should guide students to the next step following their chosen ap-\nproach. Thus the ability of LLMs to solve math word problems and to understand and explain equations is critical. The following sections address these two key points."
        },
        {
            "heading": "3 LLM Ability to Solve Math Problems",
            "text": "Many textbook math exercises provide the calculation process but lack a natural language explanation. Accompanying each step with an explanation would greatly enhance the student\u2019s understanding of each equation. This leads to the second research question (RQ2): What is the problem-solving capability of LLMs in mathematical problems and their ability to explain the computational process? Automatic Evaluation: The dataset used in this work is a modified version of MathQA in which unsolvable questions were removed by Jie et al. (2022). We utilized OpenAI\u2019s API, in particular the \u201cgpt-3.5-turbo-0301\u201d model. The temperature was set to 0. Table 1 shows the accuracy of three commonly used prompting methods: zeroshot prompting (Zero-shot), few-shot prompting (Few-shot), and chain-of-thought (CoT) prompting. Three prompting templates are shown in Appendix B. \u201cQuestions\u201d denotes the number of questions of each type in the test set. We also show the results for six MathQA question types. In this experiment, we compared only the model-generated answers with the provided answers, without verifying the correctness of the reasoning process. As shown in Table 1, the CoT performance was poorer than expected, possibly due to the higher mathematical skill demands of MathQA compared to previously-used datasets. Zero-shot prompting does not significantly outperform CoT (p < 0.8). Exploring suitable prompting methods is left as future work. Based on the results, we observe that the calculation abilities of the GPT-3.5 model remain far from satisfactory. It frequently fails at simple arithmetic operations or counting. Human Evaluation of LLM Results: It is known that LLMs might produce the correct answer even\nwith incorrect reasoning, or give an incorrect answer despite correct reasoning (Laskar et al., 2023; Lightman et al., 2023). However, detailed analyses and statistical evaluations of model errors have been less extensively studied. To further analyze whether LLMs are able to reason through complex mathematical problems, we invited an expert who majored in Mathematics to evaluate the answers generated by the GPT-3.5 model. A total of 120 questions\u201420 from each of the six question types\u2014 were selected by the expert. Each selected question involved a process of reasoning and calculation of more than four steps. The error types are shown in Table 2. The most common error made by the model was \u201cmisconception in problem-solving\u201d: the model understood the question but used incorrect formulas or methods to solve it. \u201cMisinterpretation of the question\u201d, in turn, is a different error: the model does not understand the question and generates an unrelated result. We also find that the GPT-3.5 model is good at providing comprehensive explanations for each equation without omitting parts of the problem-solving process. However, it exhibits inconsistent calculations, often making simple arithmetic errors. Furthermore, its grasp of set theory and three-dimensional spatial reasoning is limited. Examples of some error types are provided in Appendix C. Research Issues of Augmented Language Models: Using external tools for calculation may be one way to address LLM drawbacks. Mialon et al. (2023) refer to language models that utilize external tools (Gao et al., 2022; Liu et al., 2022), retrieve relevant information (Izacard et al., 2022), or use specific reasoning strategies (Wei et al., 2022) as augmented language models (ALMs). We argue that for a model to solve more complex tasks, it should comprehend the tasks and know when, how,\nand why to request augmentation. Otherwise, the improvements yielded by the augmented information would remain limited in various real-world applications. For instance, for mathematical calculations, Schick et al. (2023) propose having the LLM use a calculator API to solve arithmetic tasks. However, the propensity of LLMs to misinterpret question meanings and substitute incorrect numbers remains a prominent challenge in advanced mathematical reasoning. From our observation, the GPT-3.5 model behaves much like a student focused on formula memorization in that it struggles to adapt to variations in the question, particularly in probability questions. Hence, enhancing the mathematical reasoning capabilities of LLMs is a critical research direction."
        },
        {
            "heading": "4 Pedagogical Ability of LLMs to Rectify Students\u2019 Answers",
            "text": "The most important thing when helping students to learn mathematical problem-solving is providing immediate adaptive feedback. In this section, we measure the pedagogical ability of LLMs in mathematics. Pedagogical ability refers to the ability to understand and help the student (Tack and Piech, 2022). This leads to the third research question (RQ3): Are LLMs able to identify errors in students\u2019 answers and provide corresponding explanations? Teacher\u2013Student Framework for Quantitative Evaluation: Due to the difficulty in obtaining real-world student answers, we simulate a scenario in which students answer questions and teachers correct the students\u2019 responses. Based on the experimental results from Table 1, we use the responses from zero-shot prompting as the student answers. These answers are then input into the GPT-3.5 model, which acts as a teacher, correcting the student\u2019s answers according to the question. The GPT-3.5 model is tasked with identifying whether the student\u2019s answer is correct and explaining why. Specifically, given an input question q, prompt Ps, and model M, we obtain the initial problem-solving result ys = M(q;Ps). Next, we input ys to M, and ask M to act as a teacher with prompt Pt to correct ys based on q. Finally, we obtain the feedback yt = M(q, ys, r;Pt), where r is the rationale of q provided in MathQA. If M struggles to understand its responses (with detailed processes and natural language explanations), then its potential to assist teachers in corrections be-\ncomes questionable. The Pt template is shown in Appendix B. We refer to this framework as the \u201cteacher\u2013student framework\u201d.\nTable 3 presents the results of the teacher\u2013 student framework. Accuracy is adopted as the evaluation metric: accuracy measures whether M correctly identifies the correctness of ys. As yt could be a lengthy paragraph, we simply use keywords such as \u201cis correct\u201d, \u201cis incorrect\u201d, \u201cnot correct\u201d, \u201calmost correct\u201d or \u201cnot correct\u201d to identify M. We compare the results with and without the question rationales. Interestingly, if the corresponding rationales are not given as input, the accuracy of the GPT-3.5 model acting as a teacher to correct students\u2019 answers (53.96%) is lower than that when directly answering questions (66.54%). However, if the corresponding rationales are given as input, accuracy is only 73.71%. Thus the GPT-3.5 model has difficulty understanding the equations given in the rationales. Human Evaluation of Correcting ModelGenerated Answers: To understand why the GPT3.5 model exhibits these characteristics when correcting answers, our expert also analyzed the results of the teacher\u2013student framework based on the selected 120 questions. Examples are given in Appendix D. Table 4 presents the correction results yt for ys by human evaluation. \u201cw/o r\u201d and \u201cw/ r\u201d denote Pt without and with rationales, respectively. Comparing the results of \u201cw/o r\u201d and \u201cw/ r\u201d, we find that providing rationales seriously confuses M, such that it determines ys is wrong in most cases. Furthermore, M simply rephrases the content of ys (67.65%). Note that the results in Table 3\nshow that Pt with r is better than that without r. However, the results in Table 4 are different, because the questions selected by the expert are more challenging. If ys is incorrect, M has a 53.85% chance of erroneously claiming that ys is correct when r is not provided. When r is given, M correctly identifies that ys is incorrect. Nonetheless, M has only a 10.29% chance to accurately identify ys as correct with r. In addition, according to our statistic, M has only a 3.85% and 1.92% chance to accurately correct the calculation results and the problem-solving processes, respectively. This is primarily because it has difficulty exploiting r and because it usually misunderstands the ys equations, even to the point of inaccurately correcting those that are already correct. Furthermore, it often forgets the equations and calculations in ys. To verify whether LLMs can understand and correct human problem-solving processes, we also invited five college students to answer three questions. The results are presented in Appendix E. Utility of LLMs in Complex Reasoning Tasks for Education: The failures of the GPT-3.5 model are comprehensively analyzed by Borji (2023). In this work, we find that LLMs tend to be confused by human answers, especially when tasks demand advanced knowledge or reasoning skills, even if relevant and correct information is provided. Hence, an alternative framework is needed under which to leverage LLMs\u2019 language understanding ability in complex reasoning tasks. Other crucial research issues are how to make the models aware of what they do not know and how to produce truthful and interpretable results (Phillips et al., 2020). Besides, this work primarily focuses on the capabilities and challenges of directly using LLMs for correcting students\u2019 answers. Developing a cognitive model for reasoning processes and their potential roles in rectifying student mistakes is crucial."
        },
        {
            "heading": "5 Conclusion",
            "text": "In this work we propose a research agenda for leveraging LLMs in mathematics learning. First, we explore the use of LLMs to assist students in learning math word problem-solving skills. Then, we analyze the mathematical reasoning ability of LLMs. Finally, we investigate the pedagogical ability of LLMs in terms of rectifying model-generated or human answers and offering adaptive feedback. We conduct experiments with the GPT-3.5 model, and conclude that there remains room for improve-\nment in the LLM\u2019s performance in solving complex mathematical problems. In addition, although it generates comprehensive explanations, the LLM is limited in accurately identifying model\u2019s and human\u2019s errors due to its poor ability to interpret mathematical equations. In the future, we plan to develop an advanced method by which to improve the utility of LLMs in mathematics education.\nLimitations\nConsidering that LLMs are widely accessible to the general public and many educational institutions are examining the challenges and benefits of their use by teachers and students, this paper primarily focuses on the application of LLMs in educational settings. However, our experiments employed only the GPT-3.5 model and did not explore other LLMs such as GPT-4 (OpenAI, 2023) and LLaMA (Touvron et al., 2023). Furthermore, while our current work investigates the utility of LLMs in enhancing students\u2019 mathematical problem-solving skills, there are many other applications in education. For instance, LLMs could be utilized to help teachers to generate teaching content or questions. Additionally, the potential issues of violating teaching ethics involved in introducing LLMs into educational applications are significant topics that have not been included in this study currently. We also conducted a human evaluation on 120 questions, which may be insufficient. Although we invited the expert to select questions that cover as many types of questions as possible, there may remain worthwhile examples that were not selected for analysis. Moreover, the mistakes made by humans and those made by LLMs may differ. However, at the current stage, we invited only five college students to answer three questions: a larger-scale experiment would be more helpful.\nEthics Statement\nIn the context of educational applications, we utilize students\u2019 personal data and answer records for our experiments, which raises privacy concerns. For this study, we invited college students to answer mathematical questions to investigate issues that might be found in real-world applications. These participants fully understood how their data would be used, and we ensured that their personal information would not be leaked."
        },
        {
            "heading": "Acknowledgements",
            "text": "This research was partially supported by National Science and Technology Council, Taiwan, under grant NSTC 111-2222-E-A49-010-MY2."
        },
        {
            "heading": "A Dataset Selection",
            "text": "Datasets commonly used to evaluate a model\u2019s ability to solve math word problems include MathQA (Amini et al., 2019), SVAMP (Patel et al., 2021), Math23K (Wang et al., 2017), and GSM8K (Cobbe et al., 2021). GSM8K consists of grade-school math word problems, and SVAMP primarily covers up to grade 4. Math23K is designed for Chinese elementary school students, and focuses on evaluating the ability of students to solve one-unknown-variable linear math word problems. By contrast, MathQA is composed of GRE-level math problems, which serve as a benchmark for university research institute admissions in the United States. Therefore, we use MathQA in our experiments to examine the utility of LLMs in higher education applications.\nB Input Formats\nLLMs are known for their success in zero-shot or few-shot learning (Brown et al., 2020). We can solve various tasks (Liu et al., 2023) through prompt engineering by providing a few examples or instructions about the task descriptions. For solving math word problem, Wei et al. (2022) propose chain-of-thought prompting (CoT) by providing step-by-step reasoning examples to cause the model to decompose the given question and generate reasoning steps. They demonstrate that CoT achieves promising results in solving math word problems. Moreover, Wu et al. (2023) demonstrate the CoT\u2019s effectiveness by incorporating it into the medical diagnostic framework. Hence, we use zero-shot, few-shot, and CoT prompting in our experiments. Tables 5, 6, and 7 contain templates of zero-shot, few-shot, and CoT prompting, respectively. The template of the proposed teacher\u2013 student framework is shown in Table 8."
        },
        {
            "heading": "C Examples Error Types when Solving Math Word Problems",
            "text": "Among the 120 selected questions, the model correctly answers 68 questions and incorrectly answers 52 questions. Eighteen of the incorrect responses are due to unclear question definitions, 24 stem from errors in the model\u2019s problem-solving process, and 10 are caused by calculation errors. In this section, we will present errors made by the GPT-3.5 model to show its limitations in mathematics reasoning.\nAs shown in Table 2, the most common mistake is misconception in problem-solving. Moreover, as mentioned in Section 3, one weakness of the GPT3.5 model is in reasoning about three-dimensional spatial problems, as illustrated in Figure 3. The model directly divides the length, width, and height by three to calculate how many cubes can be placed in a box, without considering the fact that cubes cannot be accommodated if the height is not evenly divisible. The question in the red box should be 3\u00d7 3\u00d7 2, not 4\u00d7 3\u00d7 2.\nMisinterpretation of the question is another critical error. Figure 4 is an example of the GPT-3.5 model misinterpreting the meaning of the question. Based on the result responded by the model, once we obtain the value of \u201cx\u201d, the greater number is \u201c2x\u201d. However, the model incorrectly interprets the question as asking which number is greater after adding five."
        },
        {
            "heading": "D Example Results from Teacher\u2013Student Framework",
            "text": "According to our observations, when acting as a teacher, the GPT-3.5 model achieves high accuracy in correcting simple equations when the rationale contains a single equation. However, for more complex problems involving two or more equations, the quality of the rationale significantly impacts the\nInput: You are a student who is good at mathematics. Apply the appropriate mathematical operations to solve the math problem. Please choose an answer from the options. Q: a student gets 55 % in one subject , 65 % in the other . to get an overall of 55 % how much should get in third subject . Options: a ) 75 % , b ) 25 % , c ) 45 % , d ) 55 % , e ) 65 % A: Let\u2019s think step by step. Output: To get an overall of 55%, the average of the three subjects should be 55%. Let\u2019s assume the third subject has a weightage of 100%. So, the total weightage of the three subjects is 100% + 100% + 100% = 300%. Now, we know that the student got 55% in the first subject and 65% in the second subject. So, the total marks obtained in the first two subjects are: 55% + 65% = 120% To get an overall of 55%, the total marks obtained in all three subjects should be: 55% x 3 = 165% Now, we can find out how much the student should get in the third subject: Total marks in all three subjects = Marks obtained in the first two subjects + Marks obtained in the third subject 165% = 120% + Marks obtained in the third subject Marks obtained in the third subject = 165% - 120% = 45% Therefore, the answer is option c) 45%.\nTable 7: CoT prompting\nmodel\u2019s identification results. Figure 2 shows the confusion matrix of answer correction. We define the difficulty of the questions based on the number of equations in the given rationales. We categorize the question difficulty into three groups: those with fewer than five equations, those with 5 to 10 equations, and those with more than 11 equations. \u201cCorrect\u201d and \u201cwrong\u201d in the true labels indicate that ys is correct and incorrect, respectively. \u201cCorrect\u201d and \u201cwrong\u201d in the predicted labels label indicate that the identification of yt is correct and incorrect, respectively. Thus \u201ccorrect\u201d in the predicted labels indicates that the model identifies ys to be correct when ys is indeed correct. As shown in Figure 2, the model\u2019s accuracy in correcting answers decreases as the problem becomes more difficult, especially when no rationale is provided to the model. Clear and explicit rationales aligned with the problem-solving process in ys cause the teacher model to understand correctly and thus accurately identify whether the ys is correct.\nAdditionally, when ys is correct, the GPT-3.5 model frequently identifies ys as incorrect but its explanation is merely a paraphrase of ys. The reason may be that the GPT-3.5 model does not merely determine correctness based on the answer\u2019s value. Although it has stringent requirements for this process, its comprehension of equations is less than ideal, resulting in misinterpretations.\nTo measure the ability of LLMs to correct students\u2019 answers, we utilize the problem-solving results returned by the GPT-3.5 model as the students\u2019 answers and then ask the model to correct the input answers. Figure 5 presents an example in which the reasoning in the answer is not finished: it stops at calculating the total volume of the drinks. The model, which acts as a teacher, accurately points out the error and provides the complete solution. However, the GPT-3.5 model may also mistakenly rectify correct answers as incorrect. Taking Figure 6 as an example, the result in the given answer correctly solves for the number of sheep, yet the model identifies it as incorrect, further producing an erroneous reasoning process."
        },
        {
            "heading": "E Human Evaluation on Correcting Human\u2019s Answers",
            "text": "Since the problem-solving approaches of LLMs may differ from those of humans, to verify whether LLMs can understand and correct human problemsolving processes, we also invited five college students from different departments (Mathematics, Statistics, and Japanese) to answer three questions selected by the expert. Their answers were input into the GPT-3.5 model for correction. Subsequently, the expert analyzed the correction results. Based on the collected answers, we find that LLM is more likely to be misled by human answers and even ignore reasoning processes written by humans. Whether the answer is correct or not, LLM tends to identify the answer as incorrect. As shown in Figure 7, the model misidentifies the answer of the human as \u201c$8000\u201d. This may be because humans sometimes omit mathematical conversions in their answers, or because the problem-solving strategy may differ from that of the LLM. This can confuse the LLM. For instance, in Figure 8, the model fails to comprehend the equations without natural language explanations, erroneously identifies the human\u2019s problem-solving approach as incorrect, and proceeds to solve it using its own method. Even when provided with a rationale, the model usually fails to make an accurate identification, often leading to erroneous outcomes."
        },
        {
            "heading": "F Type of Generalization",
            "text": "Table 9 shows the type of generalization we are investigating, based on the taxonomy defined by Hupkes et al. (2022)."
        }
    ],
    "title": "Three Questions Concerning the Use of Large Language Models to Facilitate Mathematics Learning",
    "year": 2023
}