{
    "abstractText": "Recent Language Models (LMs) have shown impressive capabilities in generating texts with the knowledge internalized in parameters. Yet, LMs often generate the factually incorrect responses to the given queries, since their knowledge may be inaccurate, incomplete, and outdated. To address this problem, previous works propose to augment LMs with the knowledge retrieved from an external knowledge source. However, such approaches often show suboptimal text generation performance due to two reasons: 1) the model may fail to retrieve the knowledge relevant to the given query, or 2) the model may not faithfully reflect the retrieved knowledge in the generated text. To overcome these, we propose to verify the output and the knowledge of the knowledge-augmented LMs with a separate verifier, which is a small LM that is trained to detect those two types of errors through instruction-finetuning. Then, when the verifier recognizes an error, we can rectify it by either retrieving new knowledge or generating new text. Further, we use an ensemble of the outputs from different instructions with a single verifier to enhance the reliability of the verification processes. We validate the effectiveness of the proposed verification steps on multiple question answering benchmarks, whose results show that the proposed verifier effectively identifies retrieval and generation errors, allowing LMs to provide more factually correct outputs. Our code is available at https://github.com/JinheonBaek/KALMV.",
    "authors": [
        {
            "affiliations": [],
            "name": "Jinheon Baek"
        },
        {
            "affiliations": [],
            "name": "Soyeong Jeong"
        },
        {
            "affiliations": [],
            "name": "Minki Kang"
        },
        {
            "affiliations": [],
            "name": "Jong C. Park"
        },
        {
            "affiliations": [],
            "name": "Sung Ju Hwang"
        }
    ],
    "id": "SP:ec9908701dafc72547f7071b47ef4e15d51b53d9",
    "references": [
        {
            "authors": [
                "aoyu Feng",
                "Vlad Fienber",
                "Markus Freitag",
                "Xavier Garcia",
                "Sebastian Gehrmann",
                "Lucas Gonzalez"
            ],
            "title": "Palm 2 technical report",
            "venue": "arXiv preprint arXiv:2305.10403",
            "year": 2023
        },
        {
            "authors": [
                "Jinheon Baek",
                "Alham Fikri Aji",
                "Amir Saffari."
            ],
            "title": "Knowledge-augmented language model prompting for zero-shot knowledge graph question answering",
            "venue": "arXiv preprint arXiv:2306.04136.",
            "year": 2023
        },
        {
            "authors": [
                "Yejin Bang",
                "Samuel Cahyawijaya",
                "Nayeon Lee",
                "Wenliang Dai",
                "Dan Su",
                "Bryan Wilie",
                "Holy Lovenia",
                "Ziwei Ji",
                "Tiezheng Yu",
                "Willy Chung",
                "Quyet V. Do",
                "Yan Xu",
                "Pascale Fung"
            ],
            "title": "A multitask, multilingual, multimodal evaluation of chatgpt on",
            "year": 2023
        },
        {
            "authors": [
                "Jonathan Berant",
                "Andrew Chou",
                "Roy Frostig",
                "Percy Liang."
            ],
            "title": "Semantic parsing on freebase from question-answer pairs",
            "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, EMNLP 2013, 18-21 October",
            "year": 2013
        },
        {
            "authors": [
                "Saffron Huang",
                "Loren Maggiore",
                "Chris Jones",
                "Albin Cassirer",
                "Andy Brock",
                "Michela Paganini",
                "Geoffrey Irving",
                "Oriol Vinyals",
                "Simon Osindero",
                "Karen Simonyan",
                "Jack W. Rae",
                "Erich Elsen",
                "Laurent Sifre"
            ],
            "title": "Improving language models by retrieving",
            "year": 2022
        },
        {
            "authors": [
                "Wei",
                "Kathy Meier-Hellstern",
                "Douglas Eck",
                "Jeff Dean",
                "Slav Petrov",
                "Noah Fiedel."
            ],
            "title": "Palm: Scaling language modeling with pathways",
            "venue": "arXiv preprint arXiv:2204.02311.",
            "year": 2022
        },
        {
            "authors": [
                "Narang",
                "Gaurav Mishra",
                "Adams Yu",
                "Vincent Y. Zhao",
                "Yanping Huang",
                "Andrew M. Dai",
                "Hongkun Yu",
                "Slav Petrov",
                "Ed H. Chi",
                "Jeff Dean",
                "Jacob Devlin",
                "Adam Roberts",
                "Denny Zhou",
                "Quoc V. Le",
                "Jason Wei"
            ],
            "title": "Scaling instruction-finetuned language models",
            "year": 2022
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: pre-training of deep bidirectional transformers for language understanding",
            "venue": "NAACL. Association for Computational Linguistics.",
            "year": 2019
        },
        {
            "authors": [
                "Angela Fan",
                "Mike Lewis",
                "Yann N. Dauphin."
            ],
            "title": "Hierarchical neural story generation",
            "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018, Melbourne, Australia, July 15-20, 2018, Volume 1: Long Papers,",
            "year": 2018
        },
        {
            "authors": [
                "Jiazhan Feng",
                "Chongyang Tao",
                "Xiubo Geng",
                "Tao Shen",
                "Can Xu",
                "Guodong Long",
                "Dongyan Zhao",
                "Daxin Jiang."
            ],
            "title": "Knowledge refinement via interaction between search engines and large language models",
            "venue": "arXiv preprint arXiv:2305.07402.",
            "year": 2023
        },
        {
            "authors": [
                "Luyu Gao",
                "Zhuyun Dai",
                "Panupong Pasupat",
                "Anthony Chen",
                "Arun Tejasvi Chaganty",
                "Yicheng Fan",
                "Vincent Zhao",
                "N. Lao",
                "Hongrae Lee",
                "Da-Cheng Juan",
                "Kelvin Guu"
            ],
            "title": "Rarr: Researching and revising what language models say, using language models",
            "year": 2023
        },
        {
            "authors": [
                "Zhibin Gou",
                "Zhihong Shao",
                "Yeyun Gong",
                "Yelong Shen",
                "Yujiu Yang",
                "Nan Duan",
                "Weizhu Chen."
            ],
            "title": "CRITIC: large language models can self-correct with tool-interactive critiquing",
            "venue": "arXiv preprint arXiv:2305.11738.",
            "year": 2023
        },
        {
            "authors": [
                "Kelvin Guu",
                "Kenton Lee",
                "Zora Tung",
                "Panupong Pasupat",
                "Ming-Wei Chang."
            ],
            "title": "Retrieval augmented language model pre-training",
            "venue": "Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume",
            "year": 2020
        },
        {
            "authors": [
                "Gautier Izacard",
                "Patrick S.H. Lewis",
                "Maria Lomeli",
                "Lucas Hosseini",
                "Fabio Petroni",
                "Timo Schick",
                "Jane Dwivedi-Yu",
                "Armand Joulin",
                "Sebastian Riedel",
                "Edouard Grave."
            ],
            "title": "Few-shot learning with retrieval augmented language models",
            "venue": "CoRR,",
            "year": 2022
        },
        {
            "authors": [
                "Zhengbao Jiang",
                "Frank F. Xu",
                "Luyu Gao",
                "Zhiqing Sun",
                "Qian Liu",
                "Jane Dwivedi-Yu",
                "Yiming Yang",
                "Jamie Callan",
                "Graham Neubig."
            ],
            "title": "Active retrieval augmented generation",
            "venue": "arXiv preprint arXiv:2305.06983.",
            "year": 2023
        },
        {
            "authors": [
                "Minki Kang",
                "Jinheon Baek",
                "Sung Ju Hwang."
            ],
            "title": "KALA: knowledge-augmented language model adaptation",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech-",
            "year": 2022
        },
        {
            "authors": [
                "Vladimir Karpukhin",
                "Barlas Oguz",
                "Sewon Min",
                "Patrick S.H. Lewis",
                "Ledell Wu",
                "Sergey Edunov",
                "Danqi Chen",
                "Wen-tau Yih."
            ],
            "title": "Dense passage retrieval for open-domain question answering",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natu-",
            "year": 2020
        },
        {
            "authors": [
                "Uszkoreit",
                "Quoc Le",
                "Slav Petrov."
            ],
            "title": "Natural questions: A benchmark for question answering research",
            "venue": "Transactions of the Association for Computational Linguistics, 7:452\u2013466.",
            "year": 2019
        },
        {
            "authors": [
                "Angeliki Lazaridou",
                "Elena Gribovskaya",
                "Wojciech Stokowiec",
                "Nikolai Grigorev"
            ],
            "title": "Internetaugmented language models through few-shot",
            "year": 2022
        },
        {
            "authors": [
                "Kenton Lee",
                "Ming-Wei Chang",
                "Kristina Toutanova."
            ],
            "title": "Latent retrieval for weakly supervised open domain question answering",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6086\u20136096, Florence, Italy.",
            "year": 2019
        },
        {
            "authors": [
                "Patrick S.H. Lewis",
                "Ethan Perez",
                "Aleksandra Piktus",
                "Fabio Petroni",
                "Vladimir Karpukhin",
                "Naman Goyal",
                "Heinrich K\u00fcttler",
                "Mike Lewis",
                "Wen-tau Yih",
                "Tim Rockt\u00e4schel",
                "Sebastian Riedel",
                "Douwe Kiela"
            ],
            "title": "Retrieval-augmented generation",
            "year": 2020
        },
        {
            "authors": [
                "Daliang Li",
                "Ankit Singh Rawat",
                "Manzil Zaheer",
                "Xin Wang",
                "Michal Lukasik",
                "Andreas Veit",
                "Felix X. Yu",
                "Sanjiv Kumar."
            ],
            "title": "Large language models with controllable working memory",
            "venue": "arXiv preprint arXiv:2211.05110.",
            "year": 2022
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Roberta: A robustly optimized BERT pretraining approach",
            "venue": "arXiv preprint arXiv:1907.11692.",
            "year": 2019
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter."
            ],
            "title": "Decoupled weight decay regularization",
            "venue": "7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "Yao Lu",
                "Max Bartolo",
                "Alastair Moore",
                "Sebastian Riedel",
                "Pontus Stenetorp."
            ],
            "title": "Fantastically ordered prompts and where to find them: Overcoming fewshot prompt order sensitivity",
            "venue": "ACL. Association for Computational Linguistics.",
            "year": 2022
        },
        {
            "authors": [
                "Clark."
            ],
            "title": "Self-refine: Iterative refinement with self-feedback",
            "venue": "arXiv preprint arXiv:2303.17651.",
            "year": 2023
        },
        {
            "authors": [
                "Alex Mallen",
                "Akari Asai",
                "Victor Zhong",
                "Rajarshi Das",
                "Hannaneh Hajishirzi",
                "Daniel Khashabi."
            ],
            "title": "When not to trust language models: Investigating effectiveness and limitations of parametric and nonparametric memories",
            "venue": "ACL 2023. Association for",
            "year": 2023
        },
        {
            "authors": [
                "OpenAI."
            ],
            "title": "Introducing chatgpt",
            "venue": "https://openai. com/blog/chatgpt.",
            "year": 2022
        },
        {
            "authors": [
                "OpenAI."
            ],
            "title": "GPT-4 technical report",
            "venue": "arXiv preprint arXiv:2303.08774.",
            "year": 2023
        },
        {
            "authors": [
                "Baolin Peng",
                "Michel Galley",
                "Pengcheng He",
                "Hao Cheng",
                "Yujia Xie",
                "Yu Hu",
                "Qiuyuan Huang",
                "Lars Liden",
                "Zhou Yu",
                "Weizhu Chen",
                "Jianfeng Gao"
            ],
            "title": "Check your facts and try again: Improving large language models with external knowledge and automated feed",
            "year": 2023
        },
        {
            "authors": [
                "Fabio Petroni",
                "Tim Rockt\u00e4schel",
                "Sebastian Riedel",
                "Patrick S.H. Lewis",
                "Anton Bakhtin",
                "Yuxiang Wu",
                "Alexander H. Miller"
            ],
            "title": "Language models as knowledge bases",
            "venue": "In Proceedings of the 2019 Conference on Empirical Methods in Natu-",
            "year": 2019
        },
        {
            "authors": [
                "Yujia Qin",
                "Yankai Lin",
                "Ryuichi Takanobu",
                "Zhiyuan Liu",
                "Peng Li",
                "Heng Ji",
                "Minlie Huang",
                "Maosong Sun",
                "Jie Zhou."
            ],
            "title": "ERICA: improving entity and relation understanding for pre-trained language models via contrastive learning",
            "venue": "Proceedings of the 59th",
            "year": 2021
        },
        {
            "authors": [
                "Alec Radford",
                "Karthik Narasimhan",
                "Tim Salimans",
                "Ilya Sutskever"
            ],
            "title": "Improving language understanding by generative pre-training",
            "year": 2018
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J. Liu."
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "J. Mach. Learn. Res.",
            "year": 2020
        },
        {
            "authors": [
                "Adam Roberts",
                "Colin Raffel",
                "Noam Shazeer"
            ],
            "title": "How much knowledge can you pack into the parameters of a language model",
            "year": 2020
        },
        {
            "authors": [
                "Stephen E. Robertson",
                "Steve Walker",
                "Susan Jones",
                "Micheline Hancock-Beaulieu",
                "Mike Gatford."
            ],
            "title": "Okapi at TREC-3",
            "venue": "Proceedings of The Third Text REtrieval Conference, TREC 1994, Gaithersburg, Maryland, USA, November 2-4, 1994, volume",
            "year": 1994
        },
        {
            "authors": [
                "Anna Rohrbach",
                "Lisa Anne Hendricks",
                "Kaylee Burns",
                "Trevor Darrell",
                "Kate Saenko."
            ],
            "title": "Object hallucination in image captioning",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31",
            "year": 2018
        },
        {
            "authors": [
                "Amir Saffari",
                "Armin Oliya",
                "Priyanka Sen",
                "Tom Ayoola."
            ],
            "title": "End-to-end entity resolution and question answering using differentiable knowledge graphs",
            "venue": "Proceedings of the 2021 Conference on",
            "year": 2021
        },
        {
            "authors": [
                "drea Santilli",
                "Thibault F\u00e9vry",
                "Jason Alan Fries",
                "Ryan Teehan",
                "Teven Le Scao",
                "Stella Biderman",
                "Leo Gao",
                "Thomas Wolf",
                "Alexander M. Rush"
            ],
            "title": "Multitask prompted training enables zero-shot task generalization",
            "year": 2022
        },
        {
            "authors": [
                "Priyanka Sen",
                "Alham Fikri Aji",
                "Amir Saffari."
            ],
            "title": "Mintaka: A complex, natural, and multilingual dataset for end-to-end question answering",
            "venue": "COLING. International Committee on Computational Linguistics.",
            "year": 2022
        },
        {
            "authors": [
                "Weijia Shi",
                "Sewon Min",
                "Michihiro Yasunaga",
                "Minjoon Seo",
                "Rich James",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Wen-tau Yih."
            ],
            "title": "REPLUG: retrievalaugmented black-box language models",
            "venue": "arXiv preprint arXiv:2301.12652.",
            "year": 2023
        },
        {
            "authors": [
                "Kurt Shuster",
                "Spencer Poff",
                "Moya Chen",
                "Douwe Kiela",
                "Jason Weston."
            ],
            "title": "Retrieval augmentation reduces hallucination in conversation",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2021, Virtual Event / Punta Cana, Domini-",
            "year": 2021
        },
        {
            "authors": [
                "Kaitao Song",
                "Xu Tan",
                "Tao Qin",
                "Jianfeng Lu",
                "TieYan Liu."
            ],
            "title": "Mpnet: Masked and permuted pretraining for language understanding",
            "venue": "NeurIPS.",
            "year": 2020
        },
        {
            "authors": [
                "Harsh Trivedi",
                "Niranjan Balasubramanian",
                "Tushar Khot",
                "Ashish Sabharwal."
            ],
            "title": "Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions",
            "venue": "arXiv preprint arXiv:2212.10509.",
            "year": 2022
        },
        {
            "authors": [
                "Jason Wei",
                "Maarten Bosma",
                "Vincent Y. Zhao",
                "Kelvin Guu",
                "Adams Wei Yu",
                "Brian Lester",
                "Nan Du",
                "Andrew M. Dai",
                "Quoc V. Le."
            ],
            "title": "Finetuned language models are zero-shot learners",
            "venue": "The Tenth",
            "year": 2022
        },
        {
            "authors": [
                "Shicheng Xu",
                "Liang Pang",
                "Huawei Shen",
                "Xueqi Cheng",
                "Tat-seng Chua."
            ],
            "title": "Search-in-the-chain: Towards the accurate, credible and traceable content generation for complex knowledge-intensive tasks",
            "venue": "arXiv preprint arXiv:2304.14732.",
            "year": 2023
        },
        {
            "authors": [
                "Ikuya Yamada",
                "Akari Asai",
                "Hiroyuki Shindo",
                "Hideaki Takeda",
                "Yuji Matsumoto."
            ],
            "title": "LUKE: deep contextualized entity representations with entity-aware self-attention",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language",
            "year": 2020
        },
        {
            "authors": [
                "Zhilin Yang",
                "Peng Qi",
                "Saizheng Zhang",
                "Yoshua Bengio",
                "William Cohen",
                "Ruslan Salakhutdinov",
                "Christopher D. Manning."
            ],
            "title": "HotpotQA: A dataset for diverse, explainable multi-hop question answering",
            "venue": "Proceedings of the 2018 Conference on Empiri-",
            "year": 2018
        },
        {
            "authors": [
                "Wen-tau Yih",
                "Matthew Richardson",
                "Christopher Meek",
                "Ming-Wei Chang",
                "Jina Suh."
            ],
            "title": "The value of semantic parse labeling for knowledge base question answering",
            "venue": "ACL. The Association for Computer Linguistics.",
            "year": 2016
        },
        {
            "authors": [
                "Zhengyan Zhang",
                "Xu Han",
                "Zhiyuan Liu",
                "Xin Jiang",
                "Maosong Sun",
                "Qun Liu."
            ],
            "title": "ERNIE: enhanced language representation with informative entities",
            "venue": "Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Flo-",
            "year": 2019
        },
        {
            "authors": [
                "Zihao Zhao",
                "Eric Wallace",
                "Shi Feng",
                "Dan Klein",
                "Sameer Singh."
            ],
            "title": "Calibrate before use: Improving few-shot performance of language models",
            "venue": "ICML, Proceedings of Machine Learning Research. PMLR.",
            "year": 2021
        },
        {
            "authors": [
                "Shen Zheng",
                "Jie Huang",
                "Kevin Chen-Chuan Chang"
            ],
            "title": "Why does chatgpt fall short in providing truthful answers? arXiv preprint arXiv:2304.10513",
            "year": 2023
        },
        {
            "authors": [
                "Chunting Zhou",
                "Junxian He",
                "Xuezhe Ma",
                "Taylor BergKirkpatrick",
                "Graham Neubig."
            ],
            "title": "Prompt consistency for zero-shot task generalization",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2022, Abu Dhabi, United Arab Emirates, De-",
            "year": 2022
        },
        {
            "authors": [
                "Peng"
            ],
            "title": "2023), and we now describe it in more detail. Note that the main focus of this baseline is to verify whether the generated answers from large LMs are grounded in the retrieved knowledge, and they propose two strategies",
            "year": 2023
        },
        {
            "authors": [
                "Bob Russell"
            ],
            "title": "Originally recorded by Kelly Gordon in 1969, the song became a worldwide",
            "year": 1969
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Recent Language Models (LMs) (Brown et al., 2020; Chowdhery et al., 2022; Chung et al., 2022), which have a large number of parameters and are further instruction-finetuned on massive datasets, have achieved remarkable successes on various language tasks. For example, they are able to perform closed-book zero-shot question answering, which aims to provide an answer to a user\u2019s query without\nupdating the LM parameters while using only the knowledge internalized in their parameters. However, while the generated answers from LMs look plausible and sound, they are often factually incorrect, which is a problem widely known as hallucination (Rohrbach et al., 2018; Bang et al., 2023; Zheng et al., 2023). Hallucination is a critical problem when deploying LMs, since it poses a risk of spreading misinformation, potentially misleading users who rely on the information.\nTo mitigate hallucination of LMs, recent works have proposed to augment LMs with the knowledge retrieved from external knowledge sources (e.g., Wikipedia and Wikidata) (Lazaridou et al., 2022; Mallen et al., 2023; Baek et al., 2023). Moreover, some other works have proposed to check the factuality of generated texts and refine them by using the knowledge in LMs themselves or from the external knowledge sources (Madaan et al., 2023; Gao et al., 2023; Jiang et al., 2023; Gou et al., 2023; Xu et al., 2023; Feng et al., 2023). However, while the aforementioned knowledge-augmentation strategies are effective in reducing hallucinations, we find that there still exists a couple of challenges: 1) the retrieved knowledge may not be relevant to the given question from the user, and 2) the generated answer may not be grounded in the retrieved knowledge, as illustrated in Figure 1 and shown in Figure 2.\nIn this work, we aim to overcome these suboptimalities of knowledge-augmented LMs. In other words, our goal is to verify whether the retrieved knowledge used for augmenting LMs is related to generating the answers for the given questions and whether the generated answers include the relevant parts of the retrieved knowledge. To this end, we propose to train a small, tailorable LM that is able to verify the aforementioned two failure cases of knowledge-augmented LMs in retrieval and generation steps. More specifically, we first automatically construct the training labels by categorizing the failure of knowledge-augmented LMs into two\ncases: retrieval error and generation error, based on the triplet of the input question, retrieved knowledge, and generated answer. Then, we instructionfinetune the LM with pairs of a certain verification instruction and its associated label, during verifier training. At the inference step, we validate the generated texts through our verifier, to filter out potentially incorrect generations due to retrieval or generation failures, to prevent the generation of texts with inaccurate information. Note that there exists a concurrent work (Peng et al., 2023) that proposes to check whether the generated answers from LMs are grounded in the knowledge provided to LMs, by using API calls to proprietary LLMs or a heuristic measure (F1). However, this work clearly differs from our method, since we further verify the relevance of the retrieved knowledge in addition to the answer groundedness, through instruction-finetuning of LMs.\nIn addition, we further propose refining the output from knowledge-augmented LMs if our verifier identifies the error in either the knowledge retrieval or the knowledge reflection. Specifically, we repeat the answer generation process until the model retrieves the knowledge relevant to the given question and incorporates the correctly retrieved knowledge into the generated answer, based on the verifier outcome. Also, since detecting errors of knowledge-augmented LMs with a single instruction given to the verifier might be inaccurate, we further construct an ensemble over multiple outputs from different instructions with a sin-\ngle verifier. Notably, one extra advantage of our verifier is that it is a plug-and-play module that works with any public or proprietary LMs, since we only require input-output pairs of LMs for verification without any architectural changes. We refer to our proposed method as Knowledge-Augmented Language Model Verification (KALMV).\nWe experimentally validate the effectiveness of our KALMV on two different Question Answering (QA) tasks, namely open-domain QA and knowledge graph QA. The experimental results show that our KALMV can effectively verify the failure cases of knowledge-augmented LMs in knowledge retrieval and answer generation steps, contributing to significant reduction of the hallucination. Also, further analyses demonstrate the effectiveness of our error-rectifying and ensemble strategies.\nOur findings and contributions are threefolds: \u2022 We point out the underexplored challenges\nof knowledge-augmented LMs, which are retrieval of irrelevant knowledge and unfaithful knowledge grounding.\n\u2022 We introduce a novel verifier that identifies whether the retrieved knowledge is relevant to the question and reflected in the answer, and further present useful strategies for rectifying incorrect answers as well as improving the effectiveness of the verifier via ensembling.\n\u2022 We validate our KALMV on open-domain and knowledge graph question answering tasks, demonstrating its effectiveness in verifying the errors of knowledge-augmented LMs."
        },
        {
            "heading": "2 Background and Related Work",
            "text": "Language Models Pre-trained Language Models (LMs) (Devlin et al., 2019; Liu et al., 2019; Radford et al., 2018; Raffel et al., 2020), which are trained on a large corpus with self-supervised learning, show impressive performances across diverse natural language tasks and are used as the base architecture. Recently, large language models (Brown et al., 2020; Chowdhery et al., 2022; Touvron et al., 2023) having billions of parameters are able to respond to a user\u2019s query without any model training on the target task. On the other hand, finetuning LMs on a massive collection of natural language datasets phrased as instructions (Wei et al., 2022; Chung et al., 2022; Sanh et al., 2022), which is known as instruction finetuning, also enables the LMs to attain reasonable zero-shot learning abilities without focused training on the target task. However, while large and instruction-finetuned LMs show performance improvement on factual tasks (e.g., question answering), they are still suboptimal since they cannot memorize all the world knowledge and may contain distorted facts. To overcome this challenge, recent studies propose augmenting LMs with external knowledge, which we discuss below.\nKnowledge-Augmented LMs Early works aim to incorporate knowledge from external knowledge sources (e.g., Wikipedia) into LMs, in order to enhance their performances on tasks that require factual knowledge, such as question answering. While such previous knowledge-augmented LMs (Zhang et al., 2019; Guu et al., 2020; Yamada et al., 2020; Qin et al., 2021; Borgeaud et al., 2022) show performance improvements on knowledge-intensive tasks, in order to integrate the external knowledge, they utilize the specific pre-training but also require changing the model architecture, which are not easily generalizable across different LMs and tasks. Similarly, while some recent works (Lewis et al., 2020; Kang et al., 2022; Li et al., 2022; Izacard et al., 2022) propose augmenting LMs with external knowledge during finetuning, they also require specific training on each target task and dataset, and often require architecture modifications. However, training the task- and data-specific LMs with model updates are computationally prohibitive as the size of LMs increases exponentially. Also, previous approaches involving architecture changes are not applicable to black-box LMs (e.g., ChatGPT), which are accessible only through API. Considering these challenges, recent methods (Lazaridou et al., 2022;\nTrivedi et al., 2022; Baek et al., 2023; Shi et al., 2023; Peng et al., 2023) use the large or instructionfinetuned LMs to incorporate the external knowledge, which allows us to design only the input text to LMs without requiring additional training thanks to their strong generalization capabilities. Following this trend, we focus on knowledge-augmented instruction-finetuned LMs, while exploring their two underrepresented challenges: incorrect knowledge retrieval and unfaithful knowledge reflection.\nKnowledge-Augmented Fact Checking Similar to the motivation of the aforementioned knowledgeaugmented LMs, recent works (Mallen et al., 2023; Gao et al., 2023; Peng et al., 2023; Jiang et al., 2023; Xu et al., 2023) propose to check the factuality of the answers generated by LMs using the external knowledge. Typically, these approaches generate the answer in response to the user\u2019s query with LMs, and then identify whether the generated answer aligns with the retrieved knowledge. However, there are significant differences between our work and the existing methods. First of all, they assume that the retrieved knowledge is pertinent, which is yet unrelated and unhelpful sometimes, making the model generate incorrect predictions. In contrast, our proposed verifier can recognize the relevance of the retrieved knowledge before incorporating it into the LMs. Second, previous works suppose that the retrieved knowledge used for factchecking is accurately reflected in the generated answer; however, LMs often ignore the given knowledge and hallucinate the answer, whereas we can detect and rectify such the grounding error. Lastly, unlike most fact-checking methods that always provide the answer with its refinement, our method can further decline to provide answers unless they are validated as correct. These differences highlight the novel contributions of our verification approach, compared against previous fact-checking methods."
        },
        {
            "heading": "3 Method",
            "text": "We now formally describe knowledge-augmented LMs, and present our method, Knowledge Augmented Language Model Verification (KALMV)."
        },
        {
            "heading": "3.1 Knowledge-Augmented Language Models",
            "text": "We begin with the explanation of language models.\nLanguage Models In our problem setup, the goal of Language Models (LMs) is to generate a factually correct answer in response to an input query from a user, which is formally defined as follows:\ny\u0302 = LM(x), where x and y\u0302 are the input and output pair, each of which consists of a sequence of tokens, and LM is the language model. We assume that LMs are already trained on massive instructionfinetuning datasets, which are capable of performing diverse tasks (e.g., question answering) (Wei et al., 2022; Chung et al., 2022), and also not further trainable since we sometimes cannot update the parameters of LMs due to their huge sizes or inaccessibility (OpenAI, 2023; Anil et al., 2023).\nNote that, while previous works (Petroni et al., 2019; Roberts et al., 2020) show that LMs are capable of memorizing the knowledge seen during training, such naive LMs encounter several challenges when dealing with factual questions. In particular, LMs cannot memorize all the factual knowledge due to their limited number of parameters. Also, some knowledge is changed and updated over time; however, LMs remain static unless they are further trained while training them is also very expensive.\nKnowledge-Augmented LMs In order to tackle the aforementioned challenges of naive LMs, some works (Lazaridou et al., 2022; Mallen et al., 2023; Baek et al., 2023) propose to augment LMs with the knowledge retrieved from the external knowledge base, called knowledge-augmented LMs. Formally, let K be the external knowledge base, which could be an encyclopedia (Wikipedia) consisting of millions of documents or a knowledge graph (Wikidata) consisting of billions of facts. Then, we first retrieve the pertinent knowledge k from the knowledge base K based on its relevance score to the input query x, by using the retriever model denoted as follows: k = Retriever(x,K) where k \u2208 K. After that, the retrieved knowledge k is incorporated into the input of the LM along with the input query, as follows: y\u0302 = LM(x,k). This knowledge augmentation strategy brings impressive performance improvements on factual language tasks by reducing the hallucination issue of LMs.\nHowever, despite the enormous successes of the aforementioned knowledge-augmented LMs, there exist remaining issues that have largely underexplored. First, the knowledge retrieved to augment LMs might be irrelevant to answer the given question, since the retrieval is not always accurate in real-world scenarios. Second, even if the retrieved knowledge is useful, LMs sometimes reflect the irrelevant part of the retrieved knowledge, or might completely ignore the knowledge and generate the answer based on their incorrect knowledge. In par-\nticular, as shown in Figure 2, there are significant occurrences of retrieval and grounding errors."
        },
        {
            "heading": "3.2 KALMV: Learning to Verify Knowledge-Augmented Language Models",
            "text": "To overcome the challenges of existing knowledgeaugmented LMs, we propose a novel verification method that identifies not only the relevance of the retrieved knowledge to the input question but also the reflection of the knowledge in the generated answer, which we refer to as Knowledge-Augmented Language Model Verification (KALMV).\nVerification of Retrieved Knowledge Given the triplet of the input query, the retrieved knowledge, and the generated answer (x,k, y\u0302), we aim to verify whether the retrieved knowledge k is relevant to the input query x. Since recent LMs (Wei et al., 2022; Chung et al., 2022) can contextualize multiple sentences and understand their underlying relationships, we use such a small and instruction-finetuned LM to identify the relatedness between the input query and the knowledge. To be specific, we prompt the verifier LM to determine the relevance based on the verification instruction i as well as the input, knowledge, and generated answer triplet (x,k, y\u0302), formalized as follows: ok = Verifierk(i,x,k, y\u0302), where Verifierk denotes the LM for retrieved knowledge verification, and ok denotes its output. Note that we formulate the verification task as a multiple-choice questionanswering task, i.e., the verifier should produce either \"A\" for incorrect retrieval or \"B\" for correct.\nVerification of Generated Answer Our next objective is to identify whether the generated answer from LM is grounded in the retrieved knowledge. To achieve this, similar to the retrieved knowledge verification process explained in the above paragraph, we use the separate, small-size, instructionfinetuned LM for answer verification. Formally, given the input query, retrieved knowledge, and generated answer triplet (x,k, y\u0302), as well as the instruction i describing the task of generated answer verification, the verifier LM produces the output token, namely \"A\" or \"B\" where \"A\" represents that the retrieved knowledge is not reflected in the generated answer and \"B\" represents the vice versa, formalized as follows: oy = Verifiery(i,x,k, y\u0302).\nThus far, we propose to detect the errors of knowledge-augmented LMs in knowledge retrieval and answer generation by using distinct LM-based verifiers. However, it is inefficient to perform two\nindividual verification processes, since both verification formulations are identical. Also, the knowledge retrieval and answer generation processes are sequential, which means that verifying the generated answer is unnecessary if the retrieved knowledge is irrelevant. Therefore, we further combine two verification procedures into one by changing the task instruction accordingly with the single verification LM (Verifier). Specifically, Verifier produces one among the following three options: A. the retrieved knowledge is not helpful to answer the question; B. the generated answer is not grounded in the retrieved knowledge; C. all the other cases.\nInstruction-Finetuning for Verifier While recent instruction-finetuned LMs might be capable of performing the proposed verification task, it may be more beneficial to tailor the LM to the verification task through additional instruction-finetuning. To perform this, we require the following inputoutput pairs: {(x,k,y), o}, where the input consists of the given question, retrieved knowledge, and true answer, and the output is the verification label which we automatically generate. In particular, we first examine whether the retrieved knowledge includes the correct answer, y \u2286 k, as annotated in the training data, and then label it as a retrieval error when the knowledge does not include the correct answer. Similarly, if the retrieval is correct yet the generated answer y\u0302 from LM(x,k) does not have overlapping tokens with the retrieved knowledge k, we label it as the generation error. Finally, for all cases where the generated answer is correct, we label it as correct1. Then, by using the inputs phrased as instructions and their corresponding labels, we instruction-finetune the proposed Verifier.\nEnsemble Verification To identify retrieval and generation errors in knowledge-augmented LMs, we forward the instruction along with the query, knowledge, and generated answer to the verifier. However, it might be inaccurate to determine the errors only with a single instruction, since recent LMs are sensitive even to minor changes in the input prompt (Zhao et al., 2021; Lu et al., 2022; Zhou et al., 2022) and also our small-size verifier LM might not fully understand the given input context. Therefore, we design various instructions, forward them to our single verifier, and ensemble the multiple outputs from the verifier with average.\n1There might be more sophisticated techniques to automatically assign verifier labels, which we leave as future work."
        },
        {
            "heading": "3.3 Strategies for Rectifying Errors of Knowledge-Augmented Language Models",
            "text": "Our verification method provides a distinct advantage in contrast to existing knowledge-augmented LMs and knowledge-augmented fact-checking approaches. That is, existing approaches always provide the answers to users even if they are not reliable; however, our method can withhold the answers if errors are detected by the proposed verifier, which can enhance the reliability and trustworthiness of LM-based systems. However, instead of simply refraining from responding to user queries, it is more worthwhile to rectify errors in the knowledge retrieval and answer generation stages. Thus, we further propose simple yet effective strategies, iteratively correcting errors detected by our verifier.\nRectifying Errors in Knowledge Retrieval The retrieved knowledge from the external knowledge base might be irrelevant to answer the question due to the retrieval error, which may mislead LMs to generate an incorrect answer. To overcome this issue, we retrieve the new knowledge iteratively until our verifier confirms that the retrieved knowledge is related to answering the question, for a certain number of times (e.g., ten times). Specifically, the knowledge with the highest relevance score to the question is retrieved, while excluding any knowledge that has been used in the previous iterations.\nRectifying Errors in Answer Generation Even though the retrieved knowledge is pertinent to the given question, LMs sometimes ignore the knowledge augmented to them and then generate the answer based on their inaccurate knowledge. To tackle this issue, similar to what we previously did on knowledge retrieval, we iteratively generate the answer until the answer is confirmed by the verifier, for the specific number of times. Note that, in order to generate the answer differently across different trials, we leverage the top-k sampling (Fan et al., 2018) that enables stochastic generation processes."
        },
        {
            "heading": "4 Experimental Setups",
            "text": "In this section, we describe the datasets, models, evaluation metrics, and implementation details. We provide the additional details in Appendix A."
        },
        {
            "heading": "4.1 Tasks and Datasets",
            "text": "We evaluate our Knowledge-Augmented Language Model Verification (KALMV) on factual OpenDomain Question Answering (ODQA) and Knowledge Graph Question Answering (KGQA) tasks.\nOpen-Domain Question Answering The goal of open-domain question answering (ODQA) task is to generate answers in response to factual questions usually with the relevant knowledge retrieved from the external knowledge source. As the knowledge source, we use Wikipedia which is an open encyclopedia consisting of millions of documents. For datasets, we use Natural Questions2 (Lee et al., 2019) that is modified from Kwiatkowski et al. (2019) for ODQA and HotpotQA3 (Yang et al., 2018), both of which are designed with Wikipedia.\nKnowledge Graph Question Answering In addition to ODQA, we evaluate our KALMV method on knowledge graph question answering (KGQA), whose goal is to answer the questions that are answerable by the facts over knowledge graphs. For datasets, we use WebQSP (Yih et al., 2016) that is modified from Berant et al. (2013) to filter out unanswerable questions, and Mintaka (Sen et al., 2022). Further, for the knowledge source, we use Wikidata which includes billions of facts that are represented as the triplet: (subject, relation, object), and we follow the standard preprocessing setup for KGQA (Saffari et al., 2021; Baek et al., 2023)."
        },
        {
            "heading": "4.2 Baselines and Our Model",
            "text": "We compare our KALMV against relevant baselines that augment LMs with external knowledge and have strategies to reduce hallucinations. Note that models including verification can refrain from providing answers if the verifier identifies errors.\nNaive Language Models This baseline uses only the LMs without incorporating external knowledge.\nKnowledge-Augmented LMs This baseline augments LMs with the knowledge retrieved from the external knowledge base (Wikipedia or Wikidata).\nAdaptive Retrieval This baseline (Mallen et al., 2023) adaptively augments the LMs by retrieving the knowledge only when the external knowledge is necessary. In particular, if the entity that appeared in the question is less frequent, they retrieve the knowledge and provide it to the LMs. This model, namely Adaptive Retrieval with Entity, is applicable to questions that have pre-annotated entities (i.e., KGQA); therefore, we also include its variant, namely Adaptive Retrieval with Confidence, that augments LMs with retrieval only when the answer generation probability of naive LMs is low.\n2https://huggingface.co/datasets/nq_open 3https://huggingface.co/datasets/hotpot_qa\nLLM-Augmenter This baseline (Peng et al., 2023) first augments LMs with knowledge retrieval, and then verifies whether the retrieved knowledge is reflected in the generated answer with Knowledge F1 (Shuster et al., 2021) that measures overlapping terms between the knowledge and the answer. Yet, unlike our KALMV, it cannot identify retrieval errors but also uses a heuristic metric for verification. In addition to the aforementioned LLMAugmenter w/ Knowledge F1, we also include the LLM-Augmenter w/ Confidence that verifies the answer based on its generation probability.\nKALMV This is our Knowledge-Augmented Language Model Verification (KALMV) method, which not only verifies both the retrieval and generation errors with the instruction-finetuned tailored verifier, but also iteratively rectifies errors."
        },
        {
            "heading": "4.3 Evaluation Metrics",
            "text": "Following the standard evaluation protocol of generative QA (Mallen et al., 2023; Baek et al., 2023), we use F1 which measures the number of overlapping words between the generated answer and the labeled answer with precision/recall, EM which measures whether the generated answer is exactly the same as the labeled answer, and accuracy which measures whether the generated answer includes the labeled answer. For KGQA, following Baek et al. (2023), we further consider a set of alternative names of the labeled answers available in Wikidata."
        },
        {
            "heading": "4.4 Implementation Details",
            "text": "We use the same retriever across different models for fair comparisons. In particular, for ODQA, we use BM25 (Robertson et al., 1994) that considers the term-based matching, following Mallen et al. (2023). Also, for KGQA, we use MPNet (Song et al., 2020) that is based on the dense retrieval, following Baek et al. (2023). For the input prompt to LMs for all baselines and our model, we follow the existing works (Mallen et al., 2023; Baek et al., 2023) which use the simple prompt, such as \"Context: {Context}. Question: {Question}. Answer: \". Regarding the LMs to generate answers, we use FLAN (Chung et al., 2022) with three different sizes: Base, Large, and XL having 250M, 780M, and 3B parameters, respectively. In our KALMV, we use the FLAN Base as the verification LM, and we instruction-finetune it with the batch size of 8 and the learning rate of 5e-5 with AdamW (Loshchilov and Hutter, 2019) as the optimizer. In addition, we set the maximum number\nof error-rectifying steps in the range of {1, 2, 3}, and filter out answers that are determined to have errors by our verifier after the maximum step. Further, for the ensemble, we use 5 different outputs, which have the probabilities of three choices (Section 3.2), from 5 different instructions, and average probabilities to select one option for verification."
        },
        {
            "heading": "5 Experimental Results and Analyses",
            "text": "Main Results We conduct experiments on two question answering tasks: open-domain QA with Wikipedia and knowledge graph QA with Wikidata. As shown in Table 1, our proposed KALMV significantly improves the performance of knowledgeaugmented LMs on all datasets across different LM sizes by effectively verifying errors in the knowl-\nRatio Ret. Gro. Cor. 0\n20\n40\n60\n80\n100\nVe ri\nfic at\nio n\nAc cu\nra cy\n33%\n14%\n53%\nWebQSP\nRatio Ret. Gro. Cor.\n76%\n11%\n13% Mintaka\nRatio Ret. Gro. Cor.\n76%\n8%\n15% Natural Questions\nRatio Ret. Gro. Cor.\n65%\n10%\n25%\nHotpotQA Retrieval Verification Groundness Verification Correctness Verification\nFigure 2: Ratios of verification types and verification accuracies on them, on each dataset with the FLAN Base as LMs.\nedge retrieval and answer generation steps. In addition, for knowledge graph QA, we also validate our KALMV on the setting where LMs are augmented with the documents from Wikipedia in Table 2, on which it also outperforms baselines substantially. Note that LLM-Augmenter, which verifies whether the generated answers are grounded in the retrieved knowledge, shows decent performance compared to other baselines. However, KALMV outperforms it by large margins, which suggests the importance of verifying the retrieval error and training the separate LM compared to using the heuristic measure to verify only the groundedness in answer generation.\nAnalyses on Verification To understand how the proposed verifier works, we analyze it in multiple aspects. In the first bar of each subplot in Figure 2, we report the percentages of the knowledge retrieval error, the knowledge grounding error, and the correct generation, and we can see that the most common errors come from the incorrect knowledge\nretrieval, which signifies the importance of verifying the retrieved knowledge. Also, on the remaining three bars in Figure 2, we report the verifier accuracy on each class category and then observe that our KALMV is able to detect errors in a balanced way across different verification categories.\nWe also report the performance of our verifier with regards to F1, recall, and precision scores in Figure 3, while varying the number of rectifying steps. In particular, precision denotes the proportion of the correct verification out of all verification predicted as correct; meanwhile, recall evaluates the proportion of the correctly predicted verification out of all actual correct verification. As shown in Figure 3, recall and F1 scores reach their almost highest points around two to three rectifying steps, while precision scores decrease slightly. These results suggest that, by increasing the number of rectifying steps, the coverage of our KALMV in delivering correct answers (i.e., recall) increases much, albeit with a slight compromise in the proportion of correct answers delivered (i.e., precision).\nPlease note that we also provide the case study on the three verification categories in Table 7.\nAblation & Sensitive Analyses To see how much our ensemble strategy contributes to the performance gain, and also how sensitive the components in KALMV are across different models, we perform ablation and sensitive analyses on ensemble, retrieval, verification, and generation parts. First, as shown in the first row of Table 3, ensemble, which forwards multiple verification instructions to the verifier and averages their results, improves the performance of both the verification and answer generation steps, demonstrating its efficacy.\nFor sensitive analyses, we first change the knowledge retriever for open-domain QA from the sparse (BM25) to the dense (DPR) retriever (Karpukhin et al., 2020). As shown in Table 3, while the dense retriever further brings performance improvement against the sparse retriever on most metrics, our KALMV consistently detects errors of knowledgeaugmented LMs with high performance regardless\nof retrievers. Also, for sensitive analyses on verification and generation, we further include ChatGPT (OpenAI, 2022) as a reference model to understand the proprietary model\u2019s performance. Regarding verification, we observe that our FLAN-based instruction-finetuned verifier is superior to the ChatGPT (Peng et al., 2023), which suggests that customizing the available LM to our target verification task with further training is more worthwhile than using the general-purpose large LMs. Moreover, for generation LMs that make answers to the given questions, large LMs obviously outperform the performance of relatively small LMs, since large LMs might be more skilled and knowledgeable in answering questions. Note that our KALMV can accurately identify the errors even when coupled with ChatGPT as well as the other instruction-finetuned T0 (Sanh et al., 2022), confirming its versatility.\nAnalyses on Generalization to Unseen Data It is worthwhile noting that our KALMV can be directly applicable to other datasets without any further training on them. To show this, we first train the verifier of KALMV on the source data (e.g., Natural Questions) and then evaluate KALMV on the target data (e.g., HotpotQA), with FLAN Base used as the LM for generation and verification. As shown in Table 4, we observe that our KALMV has the capacity to generalize to other data without much performance degradation. Furthermore, for the Natural Questions dataset, the verifier trained\non the HotpotQA might be stronger than the verifier trained on the same Natural Questions, from the observation of the KALMV\u2019s performances on Natural Questions from models trained on each of HopotQA and Natrual Questions datasets, which further signifies its generalization ability."
        },
        {
            "heading": "6 Conclusion",
            "text": "In this work, we proposed Knowledge-Augmented Language Model Verification (KALMV), which identifies not only the relevance of the retrieved knowledge to the input query but also the faithfulness of the reflection of knowledge in the generated answers, in order to prevent incorrect answer generations with knowledge-augmented LMs. To this end, we developed a verifier that can detect errors in both the knowledge retrieval and answer generation stages by instruction-finetuning LMs. Further, during inference, we proposed to rectify errors by re-retrieving knowledge and re-generating answers if our KALMV detects errors, and also perform an ensemble over multiple verification outputs from different instructions, to improve the efficacy of the verifier. We validated KALMV on two question answering tasks and showed its effectiveness in significantly reducing hallucinations. We believe that KALMV will bring substantial practical impact in improving the reliability of LM-based systems, especially since it is a plug-and-play module.\nLimitations\nIn this section, we faithfully discuss the current limitations and potential avenues for future research.\nFirst, we propose to instruction-finetune the verifier LM to customize it to the proposed verification task that aims to detect errors in knowledge retrieval and answer generation steps. Then, through our experimental results and analyses, we show that our proposed verifier trained by the automatically generated input-output pairs (See Section 3.2) is effective in identifying errors. However, the automatic label-generation processes that we suggest are indeed simple and they may introduce the potential to incorrectly generate the verification label in some particular scenarios (e.g., multi-step reasoning with multiple sources of knowledge). Therefore, someone may improve the labels required for instruction-finetuning verifiers by annotating them manually with humans or designing more sophisticated strategies, which we leave as future work.\nSecond, our work initiates a new problem setup\nof detecting errors of knowledge-augmented LMs in two different perspectives: knowledge retrieval and answer generation. However, each component and strategy of the proposed KALMV method is a bit separated. Specifically, the retriever and verifier are not jointly trained, while the signal from training the verifier may help improve the retriever\u2019s performance. Also, regarding the error rectifying steps, while we can iteratively correct failures on knowledge-augmented LMs, the previous and current rectifying steps are handled separately. However, the current step may get benefits from the results of the previous steps. We leave developing and building more ideas on improving components of our proposed KALMV method as future work.\nEthics Statement\nHallucination, which is a phenomenon where the language models generate responses that are plausible and sound yet factually incorrect, is a critical problem especially when deploying LMs in production since it can induce the spreading of misinformation. In this work, the proposed knowledgeaugmented language model verification (KALMV) method contributes to significantly reducing hallucinations of LMs, by verifying their retrieved knowledge and generated answers, and further rectifying them if errors are detected. However, there may be some cases where our verifier misclassifies the failure cases of knowledge-augmented LMs as correct, potentially leading to severe negative consequences, especially in mission-critical domains and systems. Therefore, it is important for us to put more effort into making LMs more reliable and trustworthy with advanced verification methods."
        },
        {
            "heading": "Acknowledgements",
            "text": "This work was supported by the Institute of Information & communications Technology Planning & Evaluation (IITP) grant funded by the Korea government (MSIT) (No. 2019-0-00075, Artificial Intelligence Graduate School Program (KAIST) and No. RS-2022-00187238, Development of Large Korean Language Model Technology for Efficient Pre-training), the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIT) (No. RS-2023-00256259), and the Engineering Research Center Program through the National Research Foundation of Korea (NRF) funded by the Korea Government (MSIT) (NRF2018R1A5A1059921)."
        },
        {
            "heading": "A Additional Experimental Setups",
            "text": "Here we provide additional experimental setups, including the instruction that we use for verification.\nInstruction Prompt In Table 6, we provide a set of 5 different instructions that we use for verification ensemble as well as instruction-finetuning verifiers (Please refer to Section 3.2 for details).\nLLM-Augmenter Details In our experiments in Section 5, we include this LLM-Augmenter model as our major baseline (Peng et al., 2023), and we now describe it in more detail. Note that the main focus of this baseline is to verify whether the generated answers from large LMs are grounded in the retrieved knowledge, and they propose two strategies to identify the groundedness. Specifically, the first strategy is the one that measures the Knowledge F1 score between the retrieved knowledge and the generated answer, which we already used for comparisons against our KALMV in our main experiments. On the other hand, the second strategy is to ask proprietary LMs (e.g., ChatGPT) to verify the groundedness of the generated answer in the retrieved knowledge. However, for the second one, it is infeasible for us to run every experiment with private Large LMs, and also it is clearly unfair to compare the public LMs against the proprietary LMs since their training data and capacity may be largely different. Nevertheless, we show that our KALMV is superior to LLM-Augmenter with ChatGPT on verification and answer generation in Table 3. Moreover, LLM-Augmenter with ChatGPT is known to have similar performances to the one that we compare (i.e, LLM-Augmenter w/ Knowledge F1) according to Peng et al. (2023), which may further support the fact that our KALMV is more effective on verification compared to the LLM-Augmenter based on ChatGPT since our KALMV significantly outperforms LLM-Augmenter w/ Knowledge F1."
        },
        {
            "heading": "B Additional Experimental Results",
            "text": "B.1 Verification Cost As it is worthwhile to investigate the increment of computational costs incurred by answer verification of our KALMV compared to the one without verification, we measure the relative increment in costs that our verifier additionally brings compared to the whole costs of running base knowledge-augmented LMs, and report it in Table 5. In particular, following the main experiment settings, we use the FLAN Base (250M) as the verification LM and use three different sizes of FLAN: Base (250M), Large (780M), and XL (3B), as the generation LM. Also, we set the cost of knowledge retrieval and answer generation (e.g., cost of running the entire knowledge-augmented LMs) as 100, and then report the relative increment from using the proposed verification. As shown in Table 5, our KALMV yields only the marginal increment, since not only do we use the smaller LM (Base) compared against larger LMs (Large and XL) for verification, but also the proposed verification LM generates only one token (e.g., A, B, or C) unlike the generation LM that decodes multiple tokens. For example, verifying answers with KALMV is 34 times faster than generating answers with Flan XL on the WebQSP data, which suggests that ours is highly efficient.\nYet, each rectifying step of our KALMV method incurs a cost that is approximately equivalent to the cost of running entire knowledge-augmented LMs with verification. To be specific, let\u2019s assume that, through the KALMV framework, the error in the generated answer is identified, the rectifying step is subsequently performed, and the new answer is verified as correct. Then, it takes twice as slow as the model without rectification. Yet, fortunately, since not every generated answer is verified as incorrect, the number of samples that require rectifying steps is far less than the number of all samples (e.g., only 38% of samples require rectification on WebQSP).\nB.2 Case Study In Table 7, we provide examples of our KALMV framework on three verification categories: incorrect knowledge retrieval, incorrect answer generation, and correct answer generation, on knowledgeaugmented LMs. As shown in Table 7, KALMV can detect the errors of knowledge-augmented LMs by contextualizing and understanding the relationships between the input question, retrieved knowledge, and generated answer effectively.\nTable 7 \u2013 Continued from the previous page Types Examples Correct Answer Question: when was the first hunger games book published?\nKnowledge: The Hunger Games was first published in hardcover on September 14, 2008, by Scholastic, featuring a cover designed by Tim O\u2019Brien. It has since been released in paperback and also as an audiobook and ebook. After an initial print of 200,000, the book had sold 800,000 copies by February 2010. Since its release, The Hunger Games has been translated into 26 languages, and publishing rights have been sold in 38\u00c2 territories. The novel is the first in The Hunger Games trilogy, followed by Catching Fire (2009) and Mockingjay (2010). A film adaptation, directed by Gary Ross and co-written and co-produced by Collins herself, was released in 2012. Correct answers: [\u2019September 14, 2008\u2019, \u20192008\u2019] Generated answer: September 14, 2008"
        }
    ],
    "title": "Knowledge-Augmented Language Model Verification",
    "year": 2023
}