{
    "abstractText": "We investigate the task of out-of-domain (OOD) text classification with the aim of extending a classification model, trained on multiple source domains, to an unseen target domain. Recent studies have shown that learning invariant representations can enhance the performance of OOD generalization. However, the inherent disparity in data distribution across different domains poses challenges for achieving effective invariance learning. This study addresses this issue by employing memory augmentations. Specifically, we augment the original feature space using key-value memory and employ a meta-learning-based approach to enhance the quality of the invariant representations. Experimental results on sentiment analysis and natural language inference tasks show the effectiveness of memory-based method for invariance learning, leading to state-of-the-art performance on six datasets.",
    "authors": [
        {
            "affiliations": [],
            "name": "Chen Jia"
        },
        {
            "affiliations": [],
            "name": "Yue Zhang"
        }
    ],
    "id": "SP:763c650d280f1e00a9d41ac4fe4e57968b4a6594",
    "references": [
        {
            "authors": [
                "Isabela Albuquerque",
                "Jo\u00e3o Monteiro",
                "Mohammad Darvishi",
                "Tiago H Falk",
                "Ioannis Mitliagkas."
            ],
            "title": "Generalizing to unseen domains via distribution matching",
            "venue": "arXiv preprint arXiv:1911.00804.",
            "year": 2019
        },
        {
            "authors": [
                "Martin Arjovsky",
                "L\u00e9on Bottou",
                "Ishaan Gulrajani",
                "David Lopez-Paz."
            ],
            "title": "Invariant risk minimization",
            "venue": "arXiv preprint arXiv:1907.02893.",
            "year": 2019
        },
        {
            "authors": [
                "Nabiha Asghar",
                "Lili Mou",
                "Kira A Selby",
                "Kevin D Pantasdo",
                "Pascal Poupart",
                "Xin Jiang."
            ],
            "title": "Progressive memory banks for incremental domain adaptation",
            "venue": "arXiv preprint arXiv:1811.00239.",
            "year": 2018
        },
        {
            "authors": [
                "Yogesh Balaji",
                "Swami Sankaranarayanan",
                "Rama Chellappa."
            ],
            "title": "Metareg: Towards domain generalization using meta-regularization",
            "venue": "Advances in neural information processing systems, 31.",
            "year": 2018
        },
        {
            "authors": [
                "Eyal Ben-David",
                "Nadav Oved",
                "Roi Reichart."
            ],
            "title": "Pada: Example-based prompt learning for on-the-fly adaptation to unseen domains",
            "venue": "Transactions of the Association for Computational Linguistics, 10:414\u2013 433.",
            "year": 2022
        },
        {
            "authors": [
                "Shai Ben-David",
                "John Blitzer",
                "Koby Crammer",
                "Alex Kulesza",
                "Fernando Pereira",
                "Jennifer Wortman Vaughan."
            ],
            "title": "A theory of learning from different domains",
            "venue": "Machine learning, 79:151\u2013175.",
            "year": 2010
        },
        {
            "authors": [
                "Shai Ben-David",
                "John Blitzer",
                "Koby Crammer",
                "Fernando Pereira."
            ],
            "title": "Analysis of representations for domain adaptation",
            "venue": "Advances in neural information processing systems, 19.",
            "year": 2006
        },
        {
            "authors": [
                "Gilles Blanchard",
                "Gyemin Lee",
                "Clayton Scott."
            ],
            "title": "Generalizing from several related classification tasks to a new unlabeled sample",
            "venue": "Advances in Neural Information Processing Systems, 24:2178\u20132186.",
            "year": 2011
        },
        {
            "authors": [
                "John Blitzer",
                "Mark Dredze",
                "Fernando Pereira."
            ],
            "title": "Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification",
            "venue": "Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 440\u2013447.",
            "year": 2007
        },
        {
            "authors": [
                "Samuel R Bowman",
                "Gabor Angeli",
                "Christopher Potts",
                "Christopher D Manning."
            ],
            "title": "A large annotated corpus for learning natural language inference",
            "venue": "arXiv preprint arXiv:1508.05326.",
            "year": 2015
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners. Advances in neural information processing",
            "year": 2020
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Com-",
            "year": 2019
        },
        {
            "authors": [
                "Ning Ding",
                "Shengding Hu",
                "Weilin Zhao",
                "Yulin Chen",
                "Zhiyuan Liu",
                "Haitao Zheng",
                "Maosong Sun."
            ],
            "title": "Openprompt: An open-source framework for promptlearning",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics:",
            "year": 2022
        },
        {
            "authors": [
                "Yaroslav Ganin",
                "Evgeniya Ustinova",
                "Hana Ajakan",
                "Pascal Germain",
                "Hugo Larochelle",
                "Fran\u00e7ois Laviolette",
                "Mario Marchand",
                "Victor Lempitsky."
            ],
            "title": "Domain-adversarial training of neural networks",
            "venue": "The journal of machine learning research, 17(1):2096\u2013",
            "year": 2016
        },
        {
            "authors": [
                "Ishaan Gulrajani",
                "David Lopez-Paz."
            ],
            "title": "In search of lost domain generalization",
            "venue": "arXiv preprint arXiv:2007.01434.",
            "year": 2020
        },
        {
            "authors": [
                "Han Guo",
                "Ramakanth Pasunuru",
                "Mohit Bansal."
            ],
            "title": "Multi-source domain adaptation for text classification via distancenet-bandits",
            "venue": "Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 7830\u20137838.",
            "year": 2020
        },
        {
            "authors": [
                "Chen Jia",
                "Yue Zhang."
            ],
            "title": "Meta-learning the invariant representation for domain generalization",
            "venue": "Machine Learning, pages 1\u201321.",
            "year": 2022
        },
        {
            "authors": [
                "Chen Jia",
                "Yue Zhang."
            ],
            "title": "Prompt-based distribution alignment for domain generalization in text classification",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 10147\u201310157.",
            "year": 2022
        },
        {
            "authors": [
                "Urvashi Khandelwal",
                "Angela Fan",
                "Dan Jurafsky",
                "Luke Zettlemoyer",
                "Mike Lewis."
            ],
            "title": "Nearest neighbor machine translation",
            "venue": "arXiv preprint arXiv:2010.00710.",
            "year": 2020
        },
        {
            "authors": [
                "Urvashi Khandelwal",
                "Omer Levy",
                "Dan Jurafsky",
                "Luke Zettlemoyer",
                "Mike Lewis."
            ],
            "title": "Generalization through memorization: Nearest neighbor language models",
            "venue": "arXiv preprint arXiv:1911.00172.",
            "year": 2019
        },
        {
            "authors": [
                "Guillaume Lample",
                "Alexandre Sablayrolles",
                "Marc\u2019Aurelio Ranzato",
                "Ludovic Denoyer",
                "Herv\u00e9 J\u00e9gou"
            ],
            "title": "Large memory layers with product keys",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Da Li",
                "Yongxin Yang",
                "Yi-Zhe Song",
                "Timothy Hospedales."
            ],
            "title": "Learning to generalize: Metalearning for domain generalization",
            "venue": "Proceedings of the AAAI conference on artificial intelligence, volume 32.",
            "year": 2018
        },
        {
            "authors": [
                "Da Li",
                "Yongxin Yang",
                "Yi-Zhe Song",
                "Timothy M Hospedales."
            ],
            "title": "Deeper, broader and artier domain generalization",
            "venue": "Proceedings of the IEEE international conference on computer vision, pages 5542\u20135550.",
            "year": 2017
        },
        {
            "authors": [
                "Pan Li",
                "Da Li",
                "Wei Li",
                "Shaogang Gong",
                "Yanwei Fu",
                "Timothy M Hospedales."
            ],
            "title": "A simple feature augmentation for domain generalization",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 8886\u20138895.",
            "year": 2021
        },
        {
            "authors": [
                "Ya Li",
                "Xinmei Tian",
                "Mingming Gong",
                "Yajing Liu",
                "Tongliang Liu",
                "Kun Zhang",
                "Dacheng Tao."
            ],
            "title": "Deep domain generalization via conditional invariant adversarial networks",
            "venue": "Proceedings of the European conference on computer vision (ECCV), pages",
            "year": 2018
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Roberta: A robustly optimized bert pretraining approach",
            "venue": "arXiv preprint arXiv:1907.11692.",
            "year": 2019
        },
        {
            "authors": [
                "Marco Marelli",
                "Stefano Menini",
                "Marco Baroni",
                "Luisa Bentivogli",
                "Raffaella Bernardi",
                "Roberto Zamparelli."
            ],
            "title": "A sick cure for the evaluation of compositional distributional semantic models",
            "venue": "Proceedings of the Ninth International Conference on Language",
            "year": 2014
        },
        {
            "authors": [
                "Alexander Miller",
                "Adam Fisch",
                "Jesse Dodge",
                "AmirHossein Karimi",
                "Antoine Bordes",
                "Jason Weston."
            ],
            "title": "Key-value memory networks for directly reading documents",
            "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language",
            "year": 2016
        },
        {
            "authors": [
                "Krikamol Muandet",
                "David Balduzzi",
                "Bernhard Sch\u00f6lkopf."
            ],
            "title": "Domain generalization via invariant feature representation",
            "venue": "International Conference on Machine Learning, pages 10\u201318.",
            "year": 2013
        },
        {
            "authors": [
                "Matthew E. Peters",
                "Mark Neumann",
                "Mohit Iyyer",
                "Matt Gardner",
                "Christopher Clark",
                "Kenton Lee",
                "Luke Zettlemoyer."
            ],
            "title": "Deep contextualized word representations",
            "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for",
            "year": 2018
        },
        {
            "authors": [
                "Fengchun Qiao",
                "Long Zhao",
                "Xi Peng."
            ],
            "title": "Learning to learn single domain generalization",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12556\u201312565.",
            "year": 2020
        },
        {
            "authors": [
                "Adam Santoro",
                "Sergey Bartunov",
                "Matthew Botvinick",
                "Daan Wierstra",
                "Timothy Lillicrap."
            ],
            "title": "Metalearning with memory-augmented neural networks",
            "venue": "International conference on machine learning, pages 1842\u20131850. PMLR.",
            "year": 2016
        },
        {
            "authors": [
                "Richard Socher",
                "Alex Perelygin",
                "Jean Wu",
                "Jason Chuang",
                "Christopher D Manning",
                "Andrew Y Ng",
                "Christopher Potts."
            ],
            "title": "Recursive deep models for semantic compositionality over a sentiment treebank",
            "venue": "Proceedings of the 2013 conference on empiri-",
            "year": 2013
        },
        {
            "authors": [
                "Sainbayar Sukhbaatar",
                "Jason Weston",
                "Rob Fergus"
            ],
            "title": "End-to-end memory networks",
            "venue": "Advances in neural information processing systems,",
            "year": 2015
        },
        {
            "authors": [
                "Baochen Sun",
                "Kate Saenko."
            ],
            "title": "Deep coral: Correlation alignment for deep domain adaptation",
            "venue": "European conference on computer vision, pages 443\u2013 450. Springer.",
            "year": 2016
        },
        {
            "authors": [
                "Qingyu Tan",
                "Ruidan He",
                "Lidong Bing",
                "Hwee Tou Ng."
            ],
            "title": "Domain generalization for text classification with memory-based supervised contrastive learning",
            "venue": "Proceedings of the 29th International Conference on Computational Linguistics, pages 6916\u2013",
            "year": 2022
        },
        {
            "authors": [
                "Qingyu Tan",
                "Ruidan He",
                "Lidong Bing",
                "Hwee Tou Ng."
            ],
            "title": "Domain generalization for text classification with memory-based supervised contrastive learning",
            "venue": "Proceedings of the 29th International Conference on Computational Linguistics, pages 6916\u2013",
            "year": 2022
        },
        {
            "authors": [
                "Tan Thongtan",
                "Tanasanee Phienthrakul."
            ],
            "title": "Sentiment classification using document embeddings trained with cosine similarity",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop,",
            "year": 2019
        },
        {
            "authors": [
                "Paul Upchurch",
                "Jacob Gardner",
                "Geoff Pleiss",
                "Robert Pless",
                "Noah Snavely",
                "Kavita Bala",
                "Kilian Weinberger."
            ],
            "title": "Deep feature interpolation for image content changes",
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2017
        },
        {
            "authors": [
                "Laurens Van der Maaten",
                "Geoffrey Hinton."
            ],
            "title": "Visualizing data using t-sne",
            "venue": "Journal of machine learning research, 9(11).",
            "year": 2008
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin."
            ],
            "title": "Attention is all you need",
            "venue": "Advances in Neural Information Processing Systems, pages 5998\u20136008.",
            "year": 2017
        },
        {
            "authors": [
                "Vikas Verma",
                "Alex Lamb",
                "Christopher Beckham",
                "Amir Najafi",
                "Ioannis Mitliagkas",
                "David Lopez-Paz",
                "Yoshua Bengio."
            ],
            "title": "Manifold mixup: Better representations by interpolating hidden states",
            "venue": "International conference on machine learning, pages",
            "year": 2019
        },
        {
            "authors": [
                "Bailin Wang",
                "Mirella Lapata",
                "Ivan Titov."
            ],
            "title": "Meta-learning for domain generalization in semantic parsing",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language",
            "year": 2021
        },
        {
            "authors": [
                "Hongru Wang",
                "Zezhong Wang",
                "Gabriel Pui Cheong Fung",
                "Kam-Fai Wong."
            ],
            "title": "Mcml: A novel memory-based contrastive meta-learning method for few shot slot tagging",
            "venue": "arXiv preprint arXiv:2108.11635.",
            "year": 2021
        },
        {
            "authors": [
                "Yuyang Zhao",
                "Zhun Zhong",
                "Fengxiang Yang",
                "Zhiming Luo",
                "Yaojin Lin",
                "Shaozi Li",
                "Nicu Sebe."
            ],
            "title": "Learning to generalize unseen domains via memory-based multi-source meta-learning for person re-identification",
            "venue": "Proceedings of the IEEE/CVF",
            "year": 2021
        },
        {
            "authors": [
                "Xin Zheng",
                "Zhirui Zhang",
                "Shujian Huang",
                "Boxing Chen",
                "Jun Xie",
                "Weihua Luo",
                "Jiajun Chen."
            ],
            "title": "Nonparametric unsupervised domain adaptation for neural machine translation",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2021,",
            "year": 2021
        },
        {
            "authors": [
                "Zexuan Zhong",
                "Tao Lei",
                "Danqi Chen."
            ],
            "title": "Training language models with memory augmentation",
            "venue": "arXiv preprint arXiv:2205.12674.",
            "year": 2022
        },
        {
            "authors": [
                "Ding"
            ],
            "title": "2022), for two text classification tasks. The entire model was trained for up to 20 epochs, with a mini-batch size of 32 sentences applied across all datasets. Optimization was performed using AdamW with an initial learning rate set",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Text classification has made remarkable progress in recent years, thanks to the advancements in deep neural networks such as Transformer (Vaswani et al., 2017) and pretrained language models (PLMs) (Peters et al., 2018; Devlin et al., 2019; Brown et al., 2020). However, these learning systems heavily rely on the assumption that the training and test sets come from the same domain. When there is a significant discrepancy between the test domain (also known as the target domain) and the training domains (also known as source domains), the performance of traditional learning systems suffers significant declines (Blanchard et al., 2011; Muandet et al., 2013). Domain generalization (DG) aims to address this out-of-domain (OOD) problem, which is a practical and challenging issue, particularly when the labeled and unlabeled information of the target domains is unknown during the training phase.\n\u2217Corresponding author.\nIn this paper, we focus on a multi-source domain generalization setting where there are multiple source domains available for training. In recent years, domain-invariant representation learning has shown high effectiveness in multi-source DG (BenDavid et al., 2010; Ganin et al., 2016). Most existing approaches use the same model parameters across domains to construct a domain-shared feature space for domain-invariant representation learning (Li et al., 2018b; Albuquerque et al., 2019; Guo et al., 2020; Jia and Zhang, 2022b). However, the intrinsic distribution discrepancy across domains poses challenges for distribution matching in order to learn a domain-invariant feature space.\nInspired by recent work (Khandelwal et al., 2019, 2020; Zheng et al., 2021), which demonstrates that memory vectors can serve as rich feature augmentations for neural models, we propose to adopt memory augmentations to improve domain-invariant representation learning. As shown in Figure 1, the traditional parameter sharing mechanism produces distinct feature distributions between source domains (left) and target domains (right) due to the intrinsic domain discrepancy. To address this, we\nuse memory augmentations to alleviate the discrepancy of feature distributions between source and target domains and improve the invariant feature distribution, constructing a domain-invariant feature subspace (middle).\nTo achieve this goal, we use a key-value memory network (Miller et al., 2016) to improve the Transformer model (Vaswani et al., 2017) by feature augmentation. Specifically, we employ a metalearning strategy to learn memory augmentations for achieving the invariant representation distribution across domains. In each training episode, source domains are randomly split into the metatarget and meta-source domains to simulate domain shifts. Consequently, we propose a bi-level optimization objective to learn memory augmentations for domain invariance. The inner-loop objective is to minimize the meta-source risk w.r.t. the Transformer parameters, while the outer-loop objective is to minimize the domain discrepancy between the meta-source and meta-target samples w.r.t. the keyvalue memory, based on the optimized Transformer parameters from the inner-loop. As a result, after the meta-test phase, the memory augmentations improve the domain invariance between the source domain and unseen target domains.\nWe evaluate our method on sentiment analysis and natural language inference (NLI) tasks. The results show that the learned memory by bilevel optimization provides better augmentations to the feature representation compared with the traditional learning strategy. Our method achieves state-of-the-art results on six datasets, outperforming a range of strong baselines. To the best of our knowledge, we are the first to leverage a memory network for improving domain-invariant representation learning. The code will be released at https://github.com/jiachenwestlake/MIL."
        },
        {
            "heading": "2 Related Work",
            "text": "Domain generalization. In this work, we specifically focus on multi-source domain generalization (DG) (Blanchard et al., 2011; Muandet et al., 2013), which offers broader application opportunities compared to the single-source scenario (Qiao et al., 2020). With the advancements in deep neural networks (Li et al., 2017), DG has achieved promising results. Existing methods primarily aim to learn domain-invariant representations across source domains to enhance out-of-distribution (OOD) robustness, which have proven effective in objective\nrecognition tasks (Sun and Saenko, 2016; Li et al., 2018b; Arjovsky et al., 2019). However, these methods face challenges when there is a significant discrepancy between the source and target domains. The invariant classification model across source domains cannot easily adapt to unseen target domains. To address this issue, some studies in objective recognition (Li et al., 2018a; Balaji et al., 2018; Jia and Zhang, 2022a) and semantic parsing (Wang et al., 2021a) employ meta-learningbased approaches with episodic training strategies to improve model adaptation to domain shifts. In contrast to these works, we aim to learn explicit memory augmentations for domain transfer. In contrast, the meta-learner in our method aims to learn a domain-invariant feature space by learning a memory augmentation. The novelty of our work is reflected in the design of meta-learning objectives. The advantage of our design lies in the ability to leverage an additional memory network to learn more robust feature representations across domains.\nRecently, there has been an increasing interest in DG for text classification. Ben-David et al. (2022) learn an example-based prompt for each instance for classification. In contrast, we focus on learning a more general memory augmentation that can address domain shifts comprehensively. Jia and Zhang (2022b) utilize a distribution alignment method to enhance domain invariance for DG. Tan et al. (2022a) adopt a memory-enhanced supervised contrastive learning method for DG. In comparison, we propose the use of key-value memory to explicitly augment feature representations and improve domain invariance in the feature space.\nMemory-based model adaptation. The augmentation of neural networks with previous memory has proven effective for model adaptation in the testing phase (Santoro et al., 2016). Prior works improve network predictions using memory banks, which serve as a continuous cache for storing longrange contextual information (Khandelwal et al., 2019, 2020; Zhong et al., 2022). Memory banks have also shown utility for task adaptation (Santoro et al., 2016; Wang et al., 2021b). However, there are limited studies on memory-based crossdomain transfer. Existing works (Asghar et al., 2018; Zheng et al., 2021) rely on target-domain unlabeled data for domain transfer. However, these methods cannot be directly applied to DG since both labeled and unlabeled target information is\nunknown during training. In contrast, we leverage memory to optimize the transferability from source domains to target domains through a meta-learning strategy.\nTo the best of our knowledge, only one existing memory-based work for DG refers to (Tan et al., 2022b), which leverages the memories of sourcedomain samples to augment contrasting features for computing supervised contrastive loss. Our work differs significantly from (Tan et al., 2022b). Firstly, our memory network is trainable, whereas they employ static source-domain banks that are not optimized during training. Secondly, we explicitly utilize memory as feature augmentation to enhance invariant representation learning, whereas they employ memory as contrasting features for computing the contrastive loss.\nFeature augmentation. Previous studies have shown that model generalization can be improved by augmenting features through the mixing of feature vectors (Verma et al., 2019). In computer vision, prior works learn interpolation for semantic changes (Upchurch et al., 2017) or perturbs latent features with random noises using mix-up techniques (Zhou et al.; Li et al., 2021; Zhao et al., 2021). In contrast, we focus on learning memory augmentations to enhance domain invariance in the feature space."
        },
        {
            "heading": "3 Method",
            "text": "As illustrated in Figure 2, the proposed model comprises (a) a vanilla Transformer enhanced by (b) a key-value memory. Furthermore, (c) the output layer is responsible for text classification, while (d) the domain discriminators handle domain classification tasks.\nThe memory serves to enhance the feature representation and mitigate domain-specific feature distributions. To accomplish this, we employ a key-value memory bank that learns the appropriate feature augmentations (Section 3.1). To address domain shifts through memory augmentations, we introduce an episodic training strategy (Section 3.2). The training objective of the key-value memory can be formulated as bi-level optimization (Section 3.3). Lastly, we present the overarching meta-training and meta-test algorithms (Section 3.4)."
        },
        {
            "heading": "3.1 Key-Value Memory-Augmented Network",
            "text": "We consider a key-value memory layer as a function m : Rd \u2192 Rd, which can be trained end-\nto-end by gradient backpropagation (Sukhbaatar et al., 2015). Following previous work (Miller et al., 2016; Lample et al., 2019), the overall structure of our memory layer consists of a query network and a value lookup table.\nKey-value memory. Given a hidden state of one position from the previous layer h \u2208 Rd, the query network acts as a function q : h 7\u2192 q(h) \u2208 Rdq , mapping from a d-dimensional hidden vector into a latent query space with the dimensionality dq. In this paper, q(\u00b7) is a linear mapping or a multilayer perceptron to reduce the dimensionality of hidden space to a lower-dimensional query space for distance computation w.r.t. the keys.\nGiven a query q(h) and a set of keys K = {k1, . . . ,k|K|} that consists of |K| dq-dimensional vectors, we first compute the dot-product similarity between the query and each key {\u03b1k} |K| k=1. For each k \u2208 {1, . . . , |K|},\n\u03b1k = exp(q(h)\u22a4kk)\u2211|K| j=1 exp(q(h) \u22a4kj) (1)\nGiven a set of memory values V = {v1, . . . ,v|K|} that consists of |K| dm-dimensional vectors, the function of the key-value memory can be represented as a weighted sum of memory values:\nm(h) = |K|\u2211 k=1 \u03b1kvk (2)\nMemory-augmented network. We use the aggregated memory by key-value memory sublayer as feature augmentations for the original Transformer model to improve domain transfer. Particularly, we perform the feature augmentation through residual connection. Let g : x 7\u2192 g(x) \u2208 Rd denote the Transformer model that mapping from an input text to a feature vector, we represent the memoryaugmented network gm : x 7\u2192 gm(x) \u2208 Rd as follows\ngm(x) = (1\u2212 \u03bb)g(x) + \u03bb \u00b7 (m \u25e6 g(x)), (3)\nwhere \u03bb represents the coefficient that balances the original features and augmented memory."
        },
        {
            "heading": "3.2 Episodic Training Procedure",
            "text": "Following Li et al. (2018a); Balaji et al. (2018), we leverage an episodic training procedure to simulate\nAlgorithm 1 Episodic training process. Input: Source domains S = {S1, S2, . . . , Sn} Input parameters:. Output: Optimized memory net m\u2217\n1: while not converge or not reach stopping conditions do 2: Randomly select a meta-target domain Dte \u2208 S 3: The meta-source domains are Dtr = S \u2212Dte 4: for t \u2208 {1, . . . , T} do 5: Sample mini-batch Dtr \u2282 Dtr and Dte \u2282 Dte 6: Optimize Transformer parameters \u03b8g on Dtr 7: Optimize the key-value memory parameters \u03b8m using the optimized Transformer parameters \u03b8\u2217g 8: end for 9: end while\nthe domain shifts. Each episode can be viewed as a meta-task to learn how to learn a better key-value memory for tackling the domain shifts between source domains and the unseen target domains. In particular, the meta-task in this paper is specified as learning memory augmentations to improve the invariance of feature representations across domains.\nA brief view of the episodic training process is shown in Algorithm 1. Given a set of sourcedomain training samples S = {S1, S2, . . . , Sn}, in each training episode, we first randomly select a meta-target domain and the rest serve as the metasource domains (lines 2-3). Then, in each training iteration t \u2208 [T ], we first optimize the Transformer model g : X \u2192 Rd parameterized by \u03b8g and task output layer h : Rd \u2192 Y parameterized by \u03b8h over the mini-batch of meta-source samples Dtr (line 6). Then, we optimize the parameters of key-\nvalue memory network \u03b8m on the mini-batch of meta-target sample Dte and meta-source samples Dtr using the optimized Transformer parameters \u03b8\u2217g (line 7)."
        },
        {
            "heading": "3.3 Memory-Based Invariance Learning",
            "text": "Based on the episodic training process, we now describe in detail the optimization objectives w.r.t. the training samples and parameters.\nDomain-invariant representation learning objective. Given n training domains, we need n domain discriminators to differ each domain from the other domains. To simplify the presentation, we use an unified function symbol fd to denote the domain discriminator between the meta-test (meta-target) data Dte and the meta-training (meta-source) data Dtr. The domain classification objective can be represented as the binary cross-entropy loss:\nLd = \u2211\nx\u2208Dtr\u222aDte\n\u2113(CE)(fd \u25e6 gm(x), I[x\u2208Dte]) (4)\nThe domain-invariant representaion learning solves the following minimax optimization objective w.r.t. the domain discriminator fd and keyvalue memory network m:\nmax \u03b8m min \u03b8fd Ld(\u03b8g,\u03b8m,\u03b8fd ;Dte,Dtr) (5)\nTheoretically, following Ben-David et al. (2010), the above minimax training objective aims to minimize the H-divergence for obtaining the invariant representation between the meta-training (metasource) and meta-test (meta-target) domains.\nAlgorithm 2 Meta-training procedure. Input: Source domains S = {S1, S2, . . . , Sn} Parameters (randomly initialized): \u03b8g , \u03b8h, \u03b8fd , \u03b8m Output: Optimized memory net m\u2217\n1: while not converge or not reach stopping conditions do 2: Randomly select a meta-target domain Dte = Sd \u2208 S 3: The meta-source domains Dtr = S \u2212Dte 4: for t \u2208 {1, . . . , T} do 5: Sample mini-batch Dtr \u2282 Dtr and Dte \u2282 Dte 6: Lt \u2190 Lt(\u03b8g,\u03b8m,\u03b8h;Dtr) task obj. 7: \u03b8h \u2190 \u03b8h \u2212 \u03b7\u2207\u03b8hLt 8: \u03b8\u2032g \u2190 \u03b8g \u2212 \u03b7\u2207\u03b8gLt 9: Ld \u2190 Ld(\u03b8\u2032g,\u03b8m,\u03b8fd ;Dtr, Dte) inv. obj. 10: \u03b8fd \u2190 \u03b8fd \u2212 \u03b7\u2207\u03b8fdLd min Ld w.r.t. fd 11: \u03b8m \u2190 \u03b8m + \u03b3\u03b7\u2207\u03b8mLd max Ld w.r.t. m 12: end for 13: end while 14: \u03b8\u2217m \u2190 \u03b8m\nTask objective. Let Dtr denote the meta-training data in each training episode, and h : Rd \u2192 Y denote the task classifier. We represent the task objective as the empirical risk on the meta-training data with the cross-entropy loss \u2113(CE)(\u00b7, \u00b7):\nLt = \u2211\n(x,y)\u2208Dtr\n\u2113(CE) ( h \u25e6 gm(x), y ) (6)\nBi-level optimization objective. Given the metatraining data Dtr and meta-test data Dte, we consider the following bi-level optimization objective for learning an optimized classification model h\u2217 \u25e6 g\u2217m:\n\u03b8\u2217m = argmax \u03b8m min \u03b8fd\nLd ( \u03b8\u2217g,\u03b8m,\u03b8fd ;Dte,Dtr ) ;\n\u03b8\u2217g,\u03b8 \u2217 h = argmin\n\u03b8g ,\u03b8h Lt(\u03b8g,\u03b8m,\u03b8h;Dtr)\ufe38 \ufe37\ufe37 \ufe38 inner-loop objective ,\n(7)\nwhere the inner-loop optimization objective is the empirical task risk on the meta-training samples and the outer-loop optimization objective is the domain-invariant representation learning objective between the meta-target sample and meta-source samples."
        },
        {
            "heading": "3.4 Meta-Optimization Algorithm",
            "text": "We now design the full gradient-based algorithm to optimize the bi-level optimization objective in Eq. (7).\nGradient update. In the gradient-based optimization algorithm, the inner-loop optimization has L gradient updating steps and the outer-loop optimization has T gradient updating steps. Each\nAlgorithm 3 Meta-test procedure. Input: Source domains S = {S1, S2, . . . , SN} Parameters (by meta-training): \u03b8\u2217m Parameters (randomly initialized): \u03b8g,\u03b8h Output: Optimized model h\u2217\u25e6g\u2217m 1: while not converge or not reach stopping conditions do 2: Randomly select a training domain Dtr \u2208 S 3: for t \u2208 {1, . . . , T} do 4: Sample mini-batch Dtr \u2282 Dtr 5: Lt \u2190 Lt(\u03b8g,\u03b8\u2217m,\u03b8h;Dtr) task obj. 6: \u03b8h \u2190 \u03b8h \u2212 \u03b7\u2207\u03b8hLt min Lt w.r.t. h 7: \u03b8g \u2190 \u03b8g \u2212 \u03b7\u2207\u03b8gLt min Lt w.r.t. g 8: end for 9: end while 10: \u03b8\u2217g \u2190 \u03b8g , \u03b8\u2217h \u2190 \u03b8h\ngradient updating step in the inner-loop optimization is represented as:\nInner-loop opt. : for the lth \u2208 [L] step,\n\u03b8(l)g = \u03b8 (l\u22121) g \u2212 \u03b7\u2207\u03b8gLt(\u03b8(l\u22121)g ,\u03b8m,\u03b8 (l\u22121) h ;Dtr); \u03b8 (l) h = \u03b8 (l\u22121) h \u2212 \u03b7\u2207\u03b8hLt(\u03b8 (l\u22121) g ,\u03b8m,\u03b8 (l\u22121) h ;Dtr)\n(8)\nEach gradient updating step in the outer-loop optimization is represented as:\nOuter-loop opt. : for the tth \u2208 [T ] step,\n\u03b8(t)m = \u03b8 (t\u22121) m + \u03b3\u03b7\u2207\u03b8mLd(\u03b8 (L) g ,\u03b8 (t\u22121) m ,\u03b8 (t\u22121) fd ;Dte, Dtr); \u03b8 (t) fd = \u03b8 (t\u22121) fd \u2212 \u03b7\u2207\u03b8fdLd(\u03b8 (L) g ,\u03b8 (t\u22121) m ,\u03b8 (t\u22121) fd ;Dte, Dtr),\n(9)\nwhere \u03b7 represents the gradient updating rate and \u03b3 represents the coefficient of gradient updating for the key-value memory network.\nThe full learning algorithm is a consequence of a meta-training procedure and a meta-test procedure, as shown in Algorithm 2 and Algorithm 3, respectively.\nMeta-training. For each training episode, lines 4-12 in Algorithm 2 present T iterations of parameter updating for the Transformer and key-value memory network. In particular, lines 6-8 present the inner-loop optimization by gradient updates on the parameters of Transformer and the task classifier. Then, lines 9-11 present the outer-loop optimization by gradient updates on the parameters of key-value memory network \u03b8m based on the updated Transformer parameters \u03b8\u2032g. As a result, the meta-training procedure preduces the optimized parameters of key-value memory network \u03b8\u2217m (line 14).\nMeta-test. Based on the learned parameters of key-value memory network \u03b8\u2217m by the metatraining procedure, the meta-test procedure optimizes parameters of Transformer \u03b8g and the task classifier \u03b8h using all the source training data. In each iteration of lines 3-8 in Algorithm 3, the source training data are used to update the parameters of Transformer and task classifier by stochastic gradient descent (SGD). As a result, the meta-test procedure produces the optimized Transformer \u03b8\u2217g and task classifier \u03b8\u2217h (line 10).\nAfter the meta-training and meta-test procedures, the optimized model h\u2217 \u25e6 g\u2217m can be used to make classification on the unseen target domain."
        },
        {
            "heading": "4 Experiments",
            "text": "We evaluate the proposed method on sentiment analysis and natural language inference (NLI) tasks."
        },
        {
            "heading": "4.1 Experimental Setup",
            "text": "Datasets. For the sentiment analysis task, we use Amazon Reviews (Blitzer et al., 2007) for leave-one-domain-out evaluation. This dataset comprises two classes (positive and negative) and four domains: book (B), DVD (D), electronics (E) and kitchen (K). Additionally, we include IMDB (Thongtan and Phienthrakul, 2019) and SST2 (Socher et al., 2013) as test datasets for crossdataset evaluation. For the NLI task, we employ a scaled-down version of MNLI (Ben-David et al., 2022)1 for leave-one-domain-out evaluation. This dataset consists of three classes (entailment, neutral, contradiction) and five domains: fiction (F), government (G), slate (S), telephone (T) and travel (T\u2019). Moreover, we use SNLI (Bowman et al., 2015) and SICK (Marelli et al., 2014) as test datasets for cross-dataset evaluation. Appendix A presents the statistics of the used datasets.\nEvaluation. The evaluation methods include leave-one-domain-out evaluation (Gulrajani and Lopez-Paz, 2020) and cross-dataset evaluation (Jia and Zhang, 2022b). Specifically, we employ standard leave-one-domain-out evaluation on Amazon Reviews and MNLI, and cross-dataset evaluation on IMDB and SST-2 for sentiment analysis, as well as SNLI and SICK for NLI.\nArchitecture and hyperparameters. In all our experiments, we fine-tune RoBERTaBASE (Liu\n1https://github.com/eyalbd2/PADA.\net al., 2019). We introduce a key-value memory sublayer after the 12th layer of RoBERTaBASE. Further details regarding the model architecture and hyperparameters can be found in Appendix B."
        },
        {
            "heading": "4.2 Main Results",
            "text": "The results for sentiment analysis and NLI using RoBERTaBASE are presented in Table 1 and Table 2, respectively. Additionally, we include the results of another pre-trained language model (PLM), BERTBASE, in Appendix C.1 to demonstrate the robustness of our approach.\nBefore investigating the performance of our method, we first analyze the challenges of OOD setting on the used text classification datasets by making comparisons to the in-domain setting. Compared with the in-domain results (oracle), directly testing on OOD data (baseline) shows a significant drop in performance. This indicates the difficulty of the used datasets for OOD evaluation.\nThe last four rows in Table 1 and Table 2 provide comparisons with four baselines. The notation \u201c+ memory\u201d indicates that the baseline model was augmented with key-value memory, similar to our approach, but without the bi-level optimization for invariance learning. \u201cinvariance learning (w/o memory)\u201d refers to a method similar to the works by Li et al. (2018b); Albuquerque et al. (2019), which directly optimize domain invariance in the feature space without memory augmentations. The results indicate that \"+ memory\" does not significantly improve over the baseline, suggesting that simply integrating memory layers into the baseline model is insufficient for learning transferable information to address domain shifts. Although domain-invariant representation learning has been shown to be effective for out-of-distribution (OOD) objective recognition (Li et al., 2018b), \u201cinvariance learning (w/o memory)\u201d only exhibits marginal improvements in our experiments. This suggests that traditional invariance learning methods face challenges in addressing OOD text classification. In comparison to these baselines, our method learns memory augmentations to improve domain invariance in the feature space and demonstrates significant enhancements in both sentiment analysis and NLI.\nWe compare our method with several state-ofthe-art DG methods for text classification, most of which aim to achieve domain invariance across source domains. DEEP CORAL (Sun and Saenko,\n2016) learns domain-invariant feature representations by optimizing second-order statistics over feature states. IRM (Arjovsky et al., 2019) further considers the intrinsic relationship between feature representation and labeling prediction to tackle domain shifts. PDA (Jia and Zhang, 2022b) simultaneously learns domain invariance for both feature representation and predicted probability. M-SCL (Tan et al., 2022a) employs a supervised contrast learning method with memory augmentations to increase the contrasting examples.\nTo ensure fair comparison, we reproduce M-SCL on sentiment analysis using RoBERTaBASE, while the results of the other methods are taken from the literature that uses RoBERTaBASE. For leave-onedomain-out evaluation, our method outperforms all the compared methods by 0.7% F1 and 1.3% F1 on the Amazon Reviews and MNLI datasets, respectively. In terms of cross-dataset evaluation, our method achieves over 1.0% F1 improvement on two sentiment analysis datasets and approximately 3.0% F1 improvement on two NLI datasets compared to the other methods. These results demonstrate the superiority of employing meta-learning\nto acquire transferable memory for domain generalization."
        },
        {
            "heading": "4.3 Analysis",
            "text": "Effects of Additional Parameters. Our method utilizes an additional key-value memory layer and includes approximately 4.8M more parameters compared to the RoBARTaBASE baseline model. To ensure a fair comparison in terms of parameter size, we consider three additional baselines: (i) \u201c+ memory uses the same key-value memory as our method but does not employ our invariance learning technique; (ii) \u201c+ FFN\u201d adds a feed-forward network (FFN) to the RoBARTaBASE\nmodel; and (iii) \u201c+ self-attn + FFN\u201d incorporates both a self-attention layer and an FFN on top of the RoBARTaBASE model. Although these three baselines have a similar number of parameters as our method, they do not yield significant improvements in performance. This observation indicates that merely increasing the parameter size with additional layers does not enhance out-ofdistribution (OOD) text classification, thus demonstrating the effectiveness of our memory-based invariance learning method.\nVisualization. We adopt t-SNE (Van der Maaten and Hinton, 2008) to visualize the feature representations, as shown in Figure 3. From Figure 3 (a), we can observe that the Transformer features of the target domain exhibit a distinctly different distribution compared to those of the source domains. However, with the aid of memory augmentations, Figure 3 (c) shows a smaller distance between the features of the target domain and those of the source domains. Interestingly, the memory distribution in Figure 3 (b) reveals a strong domain specificity across different domains. These findings demonstrate that our method is capable of effectively learning memory augmentations for different domains, thereby achieving domain invariance in the feature space.\nInvariant representation learning. We adopt the A-distance (Ben-David et al., 2006) to measure the distance of feature distributions between the target domain and the source domains using three sentiment analysis datasets. As depicted in Figure 4, incorporating the key-value memory over\nthe RoBARTaBASE model without employing the invariance learning strategy barely improves the Adistance. In contrast, the traditional invariant representation learning approach proves effectiveness in reducing the target-source domain A-distance. Furthermore, our method further optimizes the Adistance to a much greater extent, which suggests that the memory learned by our method contributes to the invariance of feature representations.\nEffects of memory learning. As demonstrated in Figure 5, the development results for both the Amazon Reviews and MNLI datasets show a significant increase as the memory size increases from 128 to 1,024. This observation indicates that a larger memory bank size encompasses richer features, allowing for accurate memory augmentations in generating domain-invariant representations. However, the\nmagnitude of performance improvement tends to diminish as the memory size continues to increase, especially when the memory size exceeds 1,024. In our experiments, we choose a memory size of 1,024 to strike a balance between performance and model size. Additionally, we also analyze the effects of memory optimization in Appendix C.3."
        },
        {
            "heading": "5 Conclusion",
            "text": "We have conducted an investigation into a memorybased approach for domain generalization (DG). Our study involves the integration of a key-value memory network into the Transformer model, and the proposal of a meta-learning algorithm that incorporates an episodic training strategy to effectively learn transferable memory for addressing domain shifts. The results obtained from experiments conducted on sentiment analysis and natural language inference tasks demonstrate the significant enhancement in transferability of the sourcedomain model through the usage of the memory unit. Additionally, our approach achieves state-ofthe-art performance on six different datasets.\nLimitations\nOur method only applies the BASE-level pretrained language models, such as RoBERTaBASE and BERTBASE. The recently developed largescale pretrained language models, such as RoBERTaLARGE and GPT (Brown et al., 2020) have shown strong performances on classification and generatioin tasks. Due to resource limitations, we leave such large-model results in future work.\nEthics Statement\nWe agree with the statements of the ACL Code of Ethics and declare that this submission follows the submission policies of ACL."
        },
        {
            "heading": "Acknowledgments",
            "text": "We thank the anonymous reviewers for their helpful comments and suggestions. We gratefully acknowledge funding from the National Natural Science Foundation of China (NSFC No. 61976180) and the Zhejiang Province Key Project 2022SDXHDX0003."
        },
        {
            "heading": "A Statistical Details of the Used Datasets",
            "text": "Dataset Domain Train (src) Dev (src) Test (tgt)\nSentiment Analysis\nAmazon\nbook (B) 1.6K 0.4K 0.4K DVD (D) 1.6K 0.4K 0.4K electronics (E) 1.6K 0.4K 0.4K kitchen (K) 1.6K 0.4K 0.4K\nIMDB movie - - 25K\nSST-2 movie - - 1.8K\nNLI\nFor the sentiment analysis task, we use Amazon Reviews (Blitzer et al., 2007), which comprises two classes (positive and negative) and four domains: book (B), DVD (D), electronics (E) and kitchen (K). Additionally, we include IMDB (Thongtan and Phienthrakul, 2019) and SST-2 (Socher et al., 2013) as test datasets for cross-dataset evaluation. For the NLI task, we employ a scaled-down version of MNLI (Ben-David et al., 2022), which consists of three classes (entailment, neutral, contradiction) and five domains: fiction (F), government (G), slate (S), telephone (T) and travel (T\u2019). Moreover, we use SNLI (Bowman et al., 2015) and SICK (Marelli et al., 2014) as test datasets for cross-dataset evaluation. Table 4 presents the statistics of the used datasets."
        },
        {
            "heading": "B Details on Architecture and Hyperparameters",
            "text": "We utilize RoBERTaBASE (Liu et al., 2019) as the primary Pretrained Language Models (PLMs) in our study, following the OpenPrompt framework (Ding et al., 2022), for two text classification tasks. The entire model was trained for up to 20 epochs, with a mini-batch size of 32 sentences applied across all datasets. Optimization was performed using AdamW with an initial learning rate set to 1e\u22125, a weight decay rate of 0.01, and warm-up steps of 500. We incorporated a key-value memory layer after the 12-th layer of RoBERTaBASE. This memory layer was added exclusively to the position used for classification, such as [MASK]\nduring prompting or [CLS] during traditional finetuning. To ensure balanced features, we selected a coefficient \u03b3 of 0.5 for our experiments. For each key-value memory network, the hidden size of keys was set to 256, and the default number of values was 1024. Following the methodology of Lample et al. (2019), we employed a multi-head query-key attention mechanism with four heads. The total parameter count of the memory layers was approximately 4.8M, significantly smaller compared to the 108M total parameters of RoBERTaBASE."
        },
        {
            "heading": "C Additional Results",
            "text": "C.1 Results based on BERTBASE The results obtained using BERTBASE are consistent with the main findings presented in Table 1 and Table 2, as illustrated in Table 5 and Table 6. The baseline models, namely \u201c+ memory\u201d and \u201cinvariance learning (w/o memory)\u201d, either show minimal or no significant improvement compared to the baseline model. In contrast, our method demonstrates superior performance in both sentiment analysis and NLI tasks, surpassing these baseline models. This indicates the robustness of our approach across different pre-trained language models (PLMs).\nC.2 Invariant Representation Learning for NLI\nWe adopt the A-distance (Ben-David et al., 2006) to measure the distance of feature distributions between the target domain and the source domains on three NLI datasets. As depicted in Figure 6, incorporating the key-value memory over the RoBARTaBASE model without employing the invariance learning strategy barely improves the\nA-distance. In contrast, the traditional invariant representation learning approach proves effective in reducing the target-source domain A-distance. Furthermore, our method further optimizes the Adistance to a much greater extent, which suggests that the memory learned by our method contributes to the invariance of feature representations.\nC.3 Effects of Memory Optimization\nFigure 7 presents the results obtained from the Amazon Reviews dev set and the MNLI dev set, as\nthe learning rate for memory values ranges from 0 to 1e\u22123. When the learning rate is set to 0, the key-value memory network remains untrained, thus failing to produce appropriate memory augmentations. As a consequence, the results are noticeably lower than those of the baseline model without memory augmentations. As the learning rate gradually increases, the results improve with minor fluctuations, ultimately reaching a plateau when the learning rate reaches a sufficiently high value. This indicates that optimizing the key-value memory network facilitates the performance of OOD text classification."
        }
    ],
    "title": "Memory-Based Invariance Learning for Out-of-Domain Text Classification",
    "year": 2023
}