{
    "abstractText": "Previous studies have shown that large language models (LLMs) like GPTs store massive factual knowledge in their parameters. However, the stored knowledge could be false or outdated. Traditional knowledge editing methods refine LLMs via fine-tuning on texts containing specific knowledge. However, with the increasing scales of LLMs, these gradient-based approaches bring large computation costs. The trend of model-as-a-service also makes it impossible to modify knowledge in black-box LLMs. Inspired by in-context learning (ICL), a new paradigm based on demonstration contexts without parameter updating, we explore whether ICL can edit factual knowledge. To answer this question, we give a comprehensive empirical study of ICL strategies. Experiments show that in-context knowledge editing (IKE), without any gradient and parameter updating, achieves a competitive success rate compared to gradient-based methods on GPT-J (6B) but with much fewer side effects, including less over-editing on similar but unrelated facts and less knowledge forgetting on previously stored knowledge. We also apply the method to larger LMs with tens or hundreds of parameters like OPT-175B, which shows the scalability of our method. The code is available at https://github.com/pkunlp-icler/IKE.",
    "authors": [
        {
            "affiliations": [],
            "name": "Ce Zheng"
        },
        {
            "affiliations": [],
            "name": "Lei Li"
        },
        {
            "affiliations": [],
            "name": "Qingxiu Dong"
        },
        {
            "affiliations": [],
            "name": "Yuxuan Fan"
        },
        {
            "affiliations": [],
            "name": "Zhiyong Wu"
        },
        {
            "affiliations": [],
            "name": "Jingjing Xu"
        },
        {
            "affiliations": [],
            "name": "Baobao Chang"
        }
    ],
    "id": "SP:7b26c54d410594968756220f62e59586d3127cde",
    "references": [
        {
            "authors": [
                "Samuel Weinbach"
            ],
            "title": "Gpt-neox-20b: An open-source autoregressive language model. CoRR, abs/2204.06745",
            "year": 2022
        },
        {
            "authors": [
                "Boxi Cao",
                "Hongyu Lin",
                "Xianpei Han",
                "Le Sun",
                "Lingyong Yan",
                "Meng Liao",
                "Tong Xue",
                "Jin Xu."
            ],
            "title": "Knowledgeable or educated guess? revisiting language models as knowledge bases",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for",
            "year": 2021
        },
        {
            "authors": [
                "Nicola De Cao",
                "Wilker Aziz",
                "Ivan Titov."
            ],
            "title": "Editing factual knowledge in language models",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Repub-",
            "year": 2021
        },
        {
            "authors": [
                "Mark Chen",
                "Jerry Tworek",
                "Heewoo Jun",
                "Qiming Yuan",
                "Henrique Ponde de Oliveira Pinto",
                "Jared Kaplan",
                "Harrison Edwards",
                "Yuri Burda",
                "Nicholas Joseph",
                "Greg Brockman"
            ],
            "title": "Evaluating large language models trained on code",
            "year": 2021
        },
        {
            "authors": [
                "Wei-Lin Chiang",
                "Zhuohan Li",
                "Zi Lin",
                "Ying Sheng",
                "Zhanghao Wu",
                "Hao Zhang",
                "Lianmin Zheng",
                "Siyuan Zhuang",
                "Yonghao Zhuang",
                "Joseph E. Gonzalez",
                "Ion Stoica",
                "Eric P. Xing"
            ],
            "title": "Vicuna: An opensource chatbot impressing gpt-4 with 90%* chatgpt",
            "year": 2023
        },
        {
            "authors": [
                "Damai Dai",
                "Li Dong",
                "Yaru Hao",
                "Zhifang Sui",
                "Baobao Chang",
                "Furu Wei."
            ],
            "title": "Knowledge neurons in pretrained transformers",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL",
            "year": 2022
        },
        {
            "authors": [
                "Bhuwan Dhingra",
                "Jeremy R. Cole",
                "Julian Martin Eisenschlos",
                "Daniel Gillick",
                "Jacob Eisenstein",
                "William W. Cohen."
            ],
            "title": "Time-aware language models as temporal knowledge bases",
            "venue": "Trans. Assoc. Comput. Linguistics, 10:257\u2013273.",
            "year": 2022
        },
        {
            "authors": [
                "Qingxiu Dong",
                "Damai Dai",
                "Yifan Song",
                "Jingjing Xu",
                "Zhifang Sui",
                "Lei Li."
            ],
            "title": "Calibrating factual knowledge in pretrained language models",
            "venue": "CoRR, abs/2210.03329.",
            "year": 2022
        },
        {
            "authors": [
                "Qingxiu Dong",
                "Lei Li",
                "Damai Dai",
                "Ce Zheng",
                "Zhiyong Wu",
                "Baobao Chang",
                "Xu Sun",
                "Jingjing Xu",
                "Lei Li",
                "Zhifang Sui."
            ],
            "title": "A survey for in-context learning",
            "venue": "CoRR, abs/2301.00234.",
            "year": 2023
        },
        {
            "authors": [
                "Yanai Elazar",
                "Nora Kassner",
                "Shauli Ravfogel",
                "Abhilasha Ravichander",
                "Eduard Hovy",
                "Hinrich Sch\u00fctze",
                "Yoav Goldberg."
            ],
            "title": "Measuring and improving consistency in pretrained language models",
            "venue": "Transactions of the Association for Computational Linguis-",
            "year": 2021
        },
        {
            "authors": [
                "Leo Gao",
                "Stella Biderman",
                "Sid Black",
                "Laurence Golding",
                "Travis Hoppe",
                "Charles Foster",
                "Jason Phang",
                "Horace He",
                "Anish Thite",
                "Noa Nabeshima",
                "Shawn Presser",
                "Connor Leahy"
            ],
            "title": "The pile: An 800gb dataset of diverse text for language modeling",
            "year": 2021
        },
        {
            "authors": [
                "Samuel Gehman",
                "Suchin Gururangan",
                "Maarten Sap",
                "Yejin Choi",
                "Noah A. Smith."
            ],
            "title": "Realtoxicityprompts: Evaluating neural toxic degeneration in language models",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020, Online",
            "year": 2020
        },
        {
            "authors": [
                "Omer Levy",
                "Minjoon Seo",
                "Eunsol Choi",
                "Luke Zettlemoyer."
            ],
            "title": "Zero-shot relation extraction via reading comprehension",
            "venue": "Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017), pages 333\u2013342, Vancouver,",
            "year": 2017
        },
        {
            "authors": [
                "Jiachang Liu",
                "Dinghan Shen",
                "Yizhe Zhang",
                "Bill Dolan",
                "Lawrence Carin",
                "Weizhu Chen"
            ],
            "title": "What makes good in-context examples for GPT-3",
            "venue": "In Proceedings of Deep Learning Inside Out (DeeLIO",
            "year": 2022
        },
        {
            "authors": [
                "Yao Lu",
                "Max Bartolo",
                "Alastair Moore",
                "Sebastian Riedel",
                "Pontus Stenetorp."
            ],
            "title": "Fantastically ordered prompts and where to find them: Overcoming fewshot prompt order sensitivity",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Compu-",
            "year": 2022
        },
        {
            "authors": [
                "Kevin Meng",
                "David Bau",
                "Alex Andonian",
                "Yonatan Belinkov."
            ],
            "title": "Locating and editing factual associations in GPT",
            "venue": "Advances in Neural Information Processing Systems, 35.",
            "year": 2022
        },
        {
            "authors": [
                "Kevin Meng",
                "Arnab Sen Sharma",
                "Alex Andonian",
                "Yonatan Belinkov",
                "David Bau."
            ],
            "title": "Mass-editing memory in a transformer",
            "venue": "CoRR, abs/2210.07229.",
            "year": 2022
        },
        {
            "authors": [
                "Eric Mitchell",
                "Charles Lin",
                "Antoine Bosselut",
                "Chelsea Finn",
                "Christopher D. Manning."
            ],
            "title": "Fast model editing at scale",
            "venue": "The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net.",
            "year": 2022
        },
        {
            "authors": [
                "Eric Mitchell",
                "Charles Lin",
                "Antoine Bosselut",
                "Christopher D. Manning",
                "Chelsea Finn."
            ],
            "title": "Memorybased model editing at scale",
            "venue": "International Conference on Machine Learning, ICML 2022, 17-23",
            "year": 2022
        },
        {
            "authors": [
                "Reiichiro Nakano",
                "Jacob Hilton",
                "Suchir Balaji",
                "Jeff Wu",
                "Long Ouyang",
                "Christina Kim",
                "Christopher Hesse",
                "Shantanu Jain",
                "Vineet Kosaraju",
                "William Saunders"
            ],
            "title": "Webgpt: Browser-assisted questionanswering with human feedback",
            "year": 2021
        },
        {
            "authors": [
                "TB OpenAI."
            ],
            "title": "Chatgpt: Optimizing language models for dialogue",
            "venue": "OpenAI.",
            "year": 2022
        },
        {
            "authors": [
                "Long Ouyang",
                "Jeffrey Wu",
                "Xu Jiang",
                "Diogo Almeida",
                "Carroll Wainwright",
                "Pamela Mishkin",
                "Chong Zhang",
                "Sandhini Agarwal",
                "Katarina Slama",
                "Alex Ray"
            ],
            "title": "Training language models to follow instructions with human",
            "year": 2022
        },
        {
            "authors": [
                "Adam Paszke",
                "Sam Gross",
                "Soumith Chintala",
                "Gregory Chanan",
                "Edward Yang",
                "Zachary DeVito",
                "Zeming Lin",
                "Alban Desmaison",
                "Luca Antiga",
                "Adam Lerer"
            ],
            "title": "Automatic differentiation in pytorch",
            "year": 2017
        },
        {
            "authors": [
                "Alec Radford",
                "Jeffrey Wu",
                "Rewon Child",
                "David Luan",
                "Dario Amodei",
                "Ilya Sutskever"
            ],
            "title": "Language models are unsupervised multitask learners",
            "venue": "OpenAI blog,",
            "year": 2019
        },
        {
            "authors": [
                "Nils Reimers",
                "Iryna Gurevych."
            ],
            "title": "Sentence-bert: Sentence embeddings using siamese bert-networks",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics.",
            "year": 2019
        },
        {
            "authors": [
                "Ohad Rubin",
                "Jonathan Herzig",
                "Jonathan Berant."
            ],
            "title": "Learning to retrieve prompts for in-context learning",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech-",
            "year": 2022
        },
        {
            "authors": [
                "Emily Sheng",
                "Kai-Wei Chang",
                "Premkumar Natarajan",
                "Nanyun Peng."
            ],
            "title": "The woman worked as a babysitter: On biases in language generation",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the",
            "year": 2019
        },
        {
            "authors": [
                "Chenglei Si",
                "Zhe Gan",
                "Zhengyuan Yang",
                "Shuohang Wang",
                "Jianfeng Wang",
                "Jordan L. Boyd-Graber",
                "Lijuan Wang."
            ],
            "title": "Prompting GPT-3 to be reliable",
            "venue": "CoRR, abs/2210.09150.",
            "year": 2022
        },
        {
            "authors": [
                "Tianxiang Sun",
                "Yunfan Shao",
                "Hong Qian",
                "Xuanjing Huang",
                "Xipeng Qiu."
            ],
            "title": "Black-box tuning for language-model-as-a-service",
            "venue": "arXiv preprint arXiv:2201.03514.",
            "year": 2022
        },
        {
            "authors": [
                "Azhar"
            ],
            "title": "2023a. Llama: Open and effi",
            "year": 2023
        },
        {
            "authors": [
                "Bhosale"
            ],
            "title": "2023b. Llama 2: Open founda",
            "year": 2023
        },
        {
            "authors": [
                "Ningyu Zhang"
            ],
            "title": "Editing large language",
            "year": 2023
        },
        {
            "authors": [
                "Sameer Singh"
            ],
            "title": "Calibrate before use: Improv",
            "year": 2021
        },
        {
            "authors": [
                "Gurevych"
            ],
            "title": "Pytorch is licensed under the modified BSD license. Huggingface and Sentence transformers are under Apache License 2.0. IKE with 32 examples are run in a 40 GB NVIDIA A40 GPU for about 3 GPU hours",
            "year": 2019
        },
        {
            "authors": [
                "Liu"
            ],
            "title": "2022) to choose k-NN examples from the training corpus. The demonstrations are encoded by all-MiniLM-L6-v2. For LMs with maximum context length",
            "year": 2048
        },
        {
            "authors": [
                "Mitchell"
            ],
            "title": "2022b) find that gradient-based knowledge editing methods encounter difficulties when attempting to update multiple knowledge facts simultaneously. When the number of factual edits increases, IKE also faces",
            "year": 2022
        },
        {
            "authors": [
                "Mitchell"
            ],
            "title": "2022b) proposes a memory-based retrieval-augmented method to handle multiple factual edits. For a given prompt, a scope classifier can retrieve the relevant knowledge fact from an external memory storing multiple factual edits",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Pre-trained Language models (LMs) have set a new paradigm for NLP research and sweep across all existing NLP benchmarks. Due to their promising results, researchers have empowered LMs with new skills that meet real-world needs, such as using web browsers (Nakano et al., 2021), coding (Chen et al., 2021), playing strategic game (FAIR et al., 2022), and conversational talents (OpenAI, 2022, 2023). However, the wide application of LMs also raises growing concerns regarding its pitfall of generating content that is fake (Elazar et al., 2021; Cao\n\u2217Corresponding author\net al., 2021a), out-dated (Dhingra et al., 2022), biased (Sheng et al., 2019; Zhao et al., 2021), and offensive (Gehman et al., 2020). To mitigate this pitfall, knowledge editing (Fig. 1) aiming to modify the knowledge learned of LMs has attracted increasing attention (Mitchell et al., 2022a; Meng et al., 2022a). The goal of knowledge editing is two-fold: generalization and specificity. The former requires generalizing to various prompts describing the same knowledge and the latter requires no interference with other unrelated knowledge.\nPrevious knowledge editing methods mainly adopt gradient-based methods to modify specific model parameters for a desired model behavior (Mitchell et al., 2022a; Meng et al., 2022a), e.g., updating the president after the election. However, the identification of the target knowledge neurons usually requires gradient estimation with heavy computation overhead (Dai et al., 2022). In addition, the updated parameters inherently lead to side effects beyond the desired editions, such as forgetting previously-learned facts or over-editing on unrelated facts. Previous studies have shown that when a large-scale LM (LLM) is deployed as a black-box service (Sun et al., 2022), a minor modification to its parameters could dramatically influence its behavior for end users. Therefore, traditional methods still suffer from editing LLMs\nsince these limitations impede the scalability and generalizability.\nRecently, in-context learning (ICL) (Brown et al., 2020) has emerged as a new paradigm for instructing LLMs to perform complex tasks. In ICL, the task description and demonstration examples are represented in natural language to form a context, and the prediction of LMs conditioned on the context is transformed into answers according to predefined rules (Brown et al., 2020). In this way, large LMs adapt to various downstream tasks without any modifications to parameters, making it a natural fit for knowledge editing on large LMs. First, it reduces the computation overhead by avoiding modifications to parameters, as well as eliminates the risk of side effects introduced by parameter updates. Most importantly, ICL provides an interpretable way for humans to calibrate LM behaviors. Despite these advantages, whether ICL is applicable to knowledge editing still remains unclear.\nIn this paper, we investigate the potential of ICL to perform knowledge editing for LLMs. We focus on two goals: (1) ensuring generalization, so that large LMs can generalize to various text surfaces for a piece of updated knowledge, and (2) ensuring specificity, by making accurate modifications to the target knowledge fact while preserving other irrelevant facts. To achieve these goals simultaneously, we design demonstration formatting and organization strategies to construct suitable incontext learning demonstrations for guiding knowledge editing on LLMs. We define three types of demonstration formatting templates including (i) copy, which aims to inject new facts into LMs; (ii) update, which improves the generalization of injected knowledge fact; and (iii) retain, which guides LMs to preserve unrelated knowledge facts. Additionally, to fully harness the potential of ICL for knowledge editing, we retrieve relevant knowledge facts from the training corpus as demonstration inputs.\nExperimental results on knowledge editing benchmarks with GPT-J (6B) show that the proposed in-context learning knowledge editing (IKE), achieves overall comparable knowledge editing performance with strong baselines. For example, IKE outperforms MEND (Mitchell et al., 2022a) by an absolute 10% editing success rate and obtains 30 points gain regarding the specificity over ROME (Meng et al., 2022a). As there are no parameter modifications, IKE is applicable to LLMs\nsuch as OPT-175B and exhibits better memorization ability, i.e., after editing, nearly 50% knowledge facts retain relatively high probability. Further analysis reveals that demonstration selection and the retain demonstrations contribute to specificity, while the update demonstrations improve generalization. Finally, we discuss the potential challenges that IKE may encounter when applied in real-world scenarios, and provide corresponding discussions.\nIn summary, the contributions of this study are four-fold:\n\u2022 To the best of our knowledge, this work represents the first systematic exploration of the potential for ICL to edit knowledge in LMs.\n\u2022 We give comprehensive empirical studies on ICL strategies and analyze how these strategies affect the final performance.\n\u2022 By designing proper demonstration formatting and organization strategies, IKE achieves comparable success rates with less computation overhead and side effects.\n\u2022 We investigate the feasibility of applying IKE to real-world scenarios and discuss potential challenges."
        },
        {
            "heading": "2 Related Work",
            "text": "Knowledge Editing Methods Recent studies on knowledge editing are mostly hype-network-based or attribution-based (Yao et al., 2023). The hypenetwork-based methods train a hyper-network to get gradient changes for certain edits. For example, Cao et al. (2021b) used a hyper-network to predict parameter shift at test time, which alters a fact while retaining unrelated facts. MEND (Mitchell et al., 2022a) learned to transform the original finetuning gradient into a low-rank decomposition of the gradient. Mitchell et al. (2022b) used an edit memory retriever and a counterfactual model to generate without updating the parameters of the base model. Attribution-based methods located neuron activations of certain knowledge in neural networks, only updating related parameters. Dai et al. (2022) evaluated the contribution of different neurons to specific knowledge using gradientbased attributions, and updated or erased facts by replacing columns in Multilayer Perceptron(MLP) weight matrices with scaled embedding vectors. Meng et al. (2022a) located single layer that expresses factual knowledge, and edited such factual\nknowledge by writing new key-value pair in MLP module.\nKnowledge Editing Benchmarks Several knowledge editing benchmarks are commonly used to evaluate the efficacy and specificity of editing approaches. For BERT-style models, factchecking dataset FEVER (Thorne et al., 2018) and question-answer dataset zsRE (Levy et al., 2017) are usally adopted. In FEVER, each x is a claim and each y indicates the validity of corresponding claim. In zsRE, each x is a question about a fact and each y is the answer, and xloc questions fact irrelevant to x. For GPT-style models, Mitchell et al. (2022a) introduced Wikitext editing dataset that requests the model to complete passage with edited continuation while the distribution of each token is unrelated passage xloc should remain unchanged. In our experiment, we use a more challenging QA dataset called COUNTERFACT (Meng et al., 2022a). In COUNTERFACT, the edited answer y to question x can sometimes be counterfactual to real world, and unrelated out-of-scope sample xloc is much more difficult than that in zsRE, which makes it harder for the model to predict desired answer. Furthermore, these desired facts are hardly captured by pre-trained LMs, avoiding the effects of LLMs knowing this knowledge before editing.\nIn-context Learning In-Context Learning (ICL) is a training-free paradigm that learns from demonstrations concatenated in the input context. Given related examples and a query, the model learns from analogy to make predictions (Brown et al., 2020; Liu et al., 2022). Existing knowledge editing methods require re-calculating the gradient or calculating and perform such knowledge editing in an inexpensive way. Si et al. (2022) is the first to explore whether in-context learning can update knowledge in LLMs, and show that incorporating all kinds of demonstration increase the success rate of knowledge editing. However, they only focus on GPT-3, without deep exploration on the potential ability and side effects of knowledge editing."
        },
        {
            "heading": "3 Task Formulation",
            "text": "The goal of knowledge editing is to inject a new fact (x\u2217, y\u2217) into a LM M by maximizing the probability PM(y\u2217|x\u2217). The x\u2217 is the prompt to probe the factual knowledge in M (e.g., The president of the US is), and y\u2217 will be the editing target Joe Biden. Knowledge editing also requires\ngeneralization and specificity:\n\u2022 Generalization: For the prompt x in the edit scope Dx\u2217 (i.e., prompts related to the new fact), the prediction of x \u2208 Dx\u2217 should be also updated to y\u2217. For example, the prediction of Q: Who is the president of the US? A: will be updated to Joe Biden.\n\u2022 Specificity: For the prompt x out of the edit scope, x /\u2208 Dx\u2217 , the prediction of x should be its original prediction yo. For example, the prediction of The president of Russia is should be retained."
        },
        {
            "heading": "4 Method: IKE",
            "text": ""
        },
        {
            "heading": "4.1 In-Context Learning",
            "text": "In-Context Learning (ICL) is proposed by Brown et al. (2020) for few-shot learning. For a large language model M, ICL aims to predict y\u0302 \u2208 Y for an input x without any parameter updating based on k demonstrations C = {(x1, y1), . . . , (xk, yk)}. The language model M predicts the probability of y \u2208 Y given x: PM(y | x,C). More specifically, ICL uses templates T to transform the inputs and labels into natural language texts. Take sentiment analysis as an example, an in-context demonstration with input xi and label yi will be transformed to Sentence: xi. Sentiment: yi, then the language model M will predict y \u2208 Y given T (x1, y1), . . . , T (xk, yk), T (x, )."
        },
        {
            "heading": "4.2 In-Context Knowledge Editing",
            "text": "When we inject a target fact f = (x\u2217, y\u2217) into LMs, we will construct k demonstrations C =\n{c1, . . . , ck}. The goal of knowledge editing is to maximize P(y\u2217 | x, f, C) when prompt x is in the editing scope of target prompt x\u2217, x \u2208 Dx\u2217 (the Generalization goal) and minimize the distance between P (y | x, f, C) and P (y | x) when x /\u2208 Dx\u2217 (the Specificity goal). LMs should determine whether the probing prompt x is in the editing scope of x\u2217, namely Dx\u2217 . To achieve these goals with ICL, proper demonstration inputs are crucial. We further decompose the demonstration construction for knowledge editing with f as the target into two sub-problems:\n(i) how to design the format of each demonstration; and (ii) how to select and rank in-context demonstrations (Dong et al., 2023)."
        },
        {
            "heading": "4.2.1 Demonstration Formatting",
            "text": "Each demonstration ci contains a new fact fi = (x\u2217i , y \u2217 i ), a probing prompt xi and its prediction yi. In-context demonstrations should teach LMs to copy, update and retain the predictions for different prompts:\n\u2022 copy: To inject new facts into LMs, the first step is to teach them to copy the prediction of the target prompt in new facts. In copy demonstrations, xi = x\u2217i and yi = y \u2217 i .\n\u2022 update: Knowledge editing is not simply teaching LMs to repeat the new fact. For the generalization of knowledge editing, the prediction of prompts in the editing scope should also be updated. In update demonstrations, xi \u2208 Dx\u2217i and yi = y \u2217 i .\n\u2022 retain: For the specificity of knowledge editing, LMs should keep their original prediction in out-of-scope prompts. In retain demonstrations, xi /\u2208 Dx\u2217i and yi should be its original answer yoi .\nThe template T of IKE transforms f , x and y into natural language: T (f, x, y) = New Fact: f . Prompt: x y. Details are listed in \u00a7A."
        },
        {
            "heading": "4.2.2 Demonstration Organization",
            "text": "When we edit a knowledge fact f in LMs, we construct k demonstrations C = {c1, . . . , ck} from the training corpus. Which demonstrations are good demonstrations for in-context editing? We follow Liu et al. (2022) to use an unsupervised retriever to choose k nearest neighbors. More specifically, we use a pretrained sentence encoder E to encode\nthe prompt x\u2217 of new fact f together with its original answer yo and targeted prediction y\u2217. The records in the training corpus will be encoded in the same way and k-NN facts are retrieved based on the cosine similarity. The ranking of in-context demonstrations also depends on the cosine similarity: cos(c0, f) < cos(c1, f) < . . . < cos(ck, f), where c1, . . . , ck are placed in the context from left to right."
        },
        {
            "heading": "4.3 Discussion: Gradient-based methods and gradient-free methods",
            "text": "Previous parameter updating methods will adjust the parameters \u03b8 of LMs M. They calculate \u2206\u03b8 based on the gradients \u2207\u03b8 \u2212 logPM(y\u2217|x\u2217) to update the base model M\u03b8 to a edited one M\u2032\u03b8+\u2206\u03b8. The editing method will then be evaluated by PM\u2032(y | x). Instead, in-context learning modifies the knowledge fact in M by constructing demonstrations C for the new fact f = (x\u2217, y\u2217), then the editing method will be evaluated by PM(y | x, f, C). Comparing PM(y | x, f, C) with PM\u2032(y | x), it can be found that: (i) ICL requires no gradient estimation for the target fact and keeps the original LM M untouched after knowledge editing. This greatly reduces the computation overhead thus making the editing applicable for LMs with trillion-level parameters, as well as eliminating the side effects of the modified parameters. (ii) The demonstration C is represented in the natural text which is more interpretable than the salient parameter update \u2206\u03b8. It provides a humanunderstandable interface for calibrating the model behavior. We highlight the characteristics of these two methods in Table 1."
        },
        {
            "heading": "5 Experiment",
            "text": "In this section, we perform experiments to answer the following research question:\n\u2022 Compared to gradient-based methods, what\u2019s the performance of IKE?\n\u2022 How do the demonstration designing strategies influence the performance of IKE?\n\u2022 How does the scale of LMs affect the performance of IKE, can IKE scale up to large language models with tens or hundreds of billions of parameters?\n\u2022 What are the side effects of knowledge editing and does IKE cause more or fewer side effects than other parameter updating methods?\nWe first introduce the experimental settings including the compared baseline methods, evaluation benchmark, and LMs across different scales for knowledge editing (\u00a75.1). We then analyze the main knowledge editing results in \u00a75.2 and the impacting factors of in-context learning knowledge editing (\u00a75.3)."
        },
        {
            "heading": "5.1 Experimental Setting",
            "text": "We aim to evaluate the performance of in-context knowledge editing compared to parameter updating approaches. We also conduct experiments on different sizes of LMs to explore the scaling-up ability of in-context knowledge editing."
        },
        {
            "heading": "5.1.1 Baselines",
            "text": "Following previous knowledge-editing methods, we also choose GPT-J (6B) as our main evaluation backbone. The compared baselines include:\nFT Fine-tuning the base model on text describing the edit fact, without training a new model editor by applying Adam with early stopping.\nMEND MEND (Mitchell et al., 2022a) transforms the fine-tuning gradient of an updated fact by decomposing the weight matrix into rank-1 form with the pretrained hyper-network.\nROME ROME (Meng et al., 2022a) learns to locate factual retrievals of a specific set of MLP modules and update knowledge by directly writing in new key-value pairs in the MLP module.\nPROMPT To explore how in-context demonstrations influence the performance of IKE. We directly use the new fact as context to probe the LMs by P(y|x, f) where f = (x\u2217, y\u2217).\nThe implementation details are in \u00a7A"
        },
        {
            "heading": "5.1.2 Evaluation Setup",
            "text": "Models To explore how the scale of LMs will influence the effectiveness of in-context knowledge editing, we evaluate in-context knowledge editing\non five GPT-like auto-regressive transformer language models whose scales range from 1.5B to 175B parameters:\n\u2022 GPT-2 XL (1.5B) (Radford et al., 2019), the 1.5 billion parameter version of GPT-2.\n\u2022 GPT-NEO (2.7B) (Gao et al., 2021), the 2.7 billion parameter version of a GPT-2 like causal language model released by EleutherAI. It is trained on the Pile dataset specifically designed for LLM training.\n\u2022 GPT-J (6B) (Wang and Komatsuzaki, 2021), an auto-regressive text generation model trained on the Pile with 6 billion parameters.\n\u2022 GPT-NEOX (20B) (Black et al., 2022), a 20 billion parameter auto-regressive language model trained on the Pile.\n\u2022 OPT (175B) (Zhang et al., 2022), open pretrained transformers with 175 billion parameters created by MetaAI.\nBenchmark We mainly evaluate baselines on COUNTERFACT (Meng et al., 2022a), a challenging benchmark suitable for GPT-like causal language models with difficult editing targets and hard-to-distinguish editing scopes. It contains 21, 919 records of diverse relations and entities. The goal of each record is to change the knowledge triplet (s\u2217, r\u2217, oc) to (s\u2217, r\u2217, o\u2217) where s\u2217 and r\u2217 are described by the target prompt x\u2217. The record also contains paraphrase prompts PP as inscope prompts and neighborhood prompts PN , i.e., knowledge triplets (s\u2032, r\u2217, oc) that share the same object with target triplets as out-of-scope prompts. We follow Meng et al. (2022a) to use first 2000 records as the test set and the remaining records are divided into training set. The details of COUNTERFACT are listed in \u00a7B.\nMetrics The performance of knowledge editing is measured from three aspects (Efficacy, Generalization, and Specificity).\n\u2022 Efficacy measures the post-editing accuracy for target prompts by Efficacy Score (ES, E[I[P(o\u2217) > P(oc)]]) and Efficacy Magnitude (EM, E[P(o\u2217)\u2212 P(oc)]).\n\u2022 Generalization measures post-editing accuracy on paraphrase prompts by Paraphrase\nScore (PS) and Paraphrase Magnitude (PM). The definition of PS and PM is similar to ES and EM.\n\u2022 Specificity measures the accuracy of neighborhood prompts by Neighborhood Score (NS, E[I[P(oc) > P(o\u2217)]]) and Neighborhood Magnitude (NM, E[P(oc) \u2212 P(o\u2217)]), as the neighborhood prompts (s\u2032, r\u2217, oc) share the same original object with the target prompt and these facts are not supposed to be edited.\nWe also follow Meng et al. (2022a) to report the harmonic mean of ES, PS, NS as Score (S)"
        },
        {
            "heading": "5.2 Main Results",
            "text": "The top rows of Table 2 show the knowledge editing results of different methods. Our findings are: (i) All methods perform well in terms of efficacy, as indicated by their close ES scores. However, there are significant differences in terms of generalization and specificity. For instance, FT achieves high ES (99.9) and PS (96.4) scores but performs poorly in terms of specificity. This highlights the challenge of balancing generalization and specificity in knowledge editing. (ii) Among the baseline methods, ROME performs the best overall regarding all three metrics, but comes with high computational overheads. Due to this limitation, it is not applicable to larger LMs such as OPT175B that are in more urgent need of knowledge editing. (iii) The proposed method IKE excels in specificity but also performs well in efficacy and generalization. For example, IKE achieves a comparable overall score with ROME on GPTJ (89.6 v.s. 91.5), while requiring no parameter\nmodifications on LMs. This computation benefit makes it possible to perform knowledge editing on large LMs such as OPT-175B, where IKE achieves clear improvements over PROMPT by 36.0 points. These results demonstrate the effectiveness, efficiency and scalability of IKE in knowledge editing."
        },
        {
            "heading": "5.3 Analysis",
            "text": "In this part, we discuss the effects of different demonstration strategies, the scalability of IKE for models across scales and side effects introduced by knowledge editing."
        },
        {
            "heading": "5.3.1 Ablation on Demonstration",
            "text": "Demonstration Numbers The number of demonstrations is one of the influencing factors for the ICL performance (Brown et al., 2020). We investigate how the number of demonstrations influences the IKE performance in the second\nblock in Table 3. Without any demonstrations, PROMPT exhibits over-generalization for its low NS (37.9), indicating it simply learns to copy the prediction. Given a few demonstrations (4 or 8), IKE performs worse than PROMPT in Efficacy and Generalization as it begins to distinguish whether a prompt is in the editing scope. With the increased number of demonstrations, IKE gradually learns to balance generalization and specificity, achieving a better trade-off.\nDemonstration Organization Previous studies (Liu et al., 2022; Rubin et al., 2022; Lu et al., 2022) suggest that demonstration organization including Demonstration Selection and Demonstration Ordering (Dong et al., 2023) is also crucial for ICL. Our proposal follows a simple unsupervised method Liu et al. (2022), to retrieve and order demonstrations from the training corpus based on the cosine similarities between the input prompt and demonstrations. In our two ablation studies in the third block of Table 3, we find that removing the selection procedure (i.e., Random Selection) leads to a clear drop in the NS score from 77.0 to 45.0, indicating the importance of proper prompt selection. However, random ordering brings negligible performance difference. We speculate that this is because the selected prompts are highly related to the target fact and the attention mechanism in Transformer-based LMs can handle long-range dependencies well. We leave further improvements as future work.\nDemonstration Formatting We further examine the impact of demonstration types including copy, update and retain. As shown in the fourth block in Table 3, removing copy demonstrations causes slight performance degradation, as LMs can easily copy the content in the demonstration even without a copy demonstration. Instead, update demonstrations perform an important role in teaching LMs\nto modify their knowledge, as indicated by a much poorer generalization score after removing upate demonstrations. Besides, The removal of retain demonstrations leads to a dramatic drop in the specificity, as measured by the NM score, which decreases from 35.2 to -47.6. This indicates that retain demonstrations are crucial in helping LMs identify out-of-scope facts and maintain their original predictions on those prompts."
        },
        {
            "heading": "5.3.2 IKE Benefits from Model Scaling",
            "text": "We further evaluate IKE on COUNTERFACT for five GPT-like causal language models across different scales. As previous experiments have shown that all methods exhibit high knowledge editing efficacy, we focus on the generalization and specificity for large LMs, as these metrics are defined to measure the side effects that could cause great influences on end users. As demonstrated in Table 4, we find that the performance of IKE is positively correlated with the scale of the LM and the largest OPT-175B achieves the strongest generalization and specificity results. This is inspiring as the performance IKE could be enhanced with the increased scale of LMs, making it pluggable for future stronger LM backbones."
        },
        {
            "heading": "5.3.3 Resilience to Over-Editing",
            "text": "Over-editing is a common side effect of knowledge editing, which denotes the influences on outof-scope facts when editing a targeted fact. Although COUNTERFACT already includes out-ofscope prompts consisting of (s\u2032, r\u2217, oc) sharing the same relation r and original object oc with the editing target: (s\u2217, r\u2217, oc) \u2192 (s\u2217, r\u2217, o\u2217), we perform a more comprehensive evaluation on overediting by adopting the contrastive knowledge assessment (CKA) proposed by Dong et al. (2022). Specifically, for a triplet (s, r, o), CKA replaces r with other similar but unrelated relations r\u2032 and compares PM(o|s, r) and PM(o|s, r\u2032) to assess whether M knows the fact (s, r, o). Inspired by this, we regard (s\u2217, r\u2032, o\u2217) as similar but unrelated prompts and consider the change in P(o\u2217|s\u2217, r\u2032) and find that P(o\u2217|s\u2217, r\u2032) will also increase after injecting (s\u2217, r\u2217, o\u2217). To further explore over-editing in different methods, we consider the CKA score, P(o\u2217|s\u2217, r\u2217)/Er\u2032\u2208RP(o\u2217|s\u2217, r\u2032).\nThe results of CKA evaluation are listed in Table 5. If the CKA score is less than predefined threshold \u03b1, the perplexity of the correct fact is close to the perplexity of contrastive fake facts,\nwhich turns out to be an editing failure. Although all baselines perform well in terms of editing efficacy, they tend to be over-generalization under a stricter contrastive assessment. ROME gets the lowest average CKA score and highest false rate, which shows its poor ability to identify out-ofscope prompts sharing the same subject with target prompts. IKE has less influence on over-editing."
        },
        {
            "heading": "5.3.4 Maintenance for Original Knowledge",
            "text": "We conclude that previous factual knowledge stored in LMs will be erased or forgotten in knowledge editing. We consider the change of P(oc|s\u2217, r) before and after editing in Table 6. The results demonstrate that all editing methods will cause the drop of P(oc|s\u2217, r\u2217). ROME forgets almost all original facts. If we want to correct the prediction of LMs, erasing the original factual knowledge is necessary. However, if we want to update the prediction of language models like updating the prediction of The president of US is from Donald Trump to Joe Biden (timeaware relations), the old knowledge In 2017, the president of US was Donald Trump should not be forgotten.\nTo evaluate the forgetting of such time-aware knowledge in editing, we construct a small benchmark based on TEMPLAMA (Dhingra et al., 2022) to further show that IKE can cause less knowledge forgetting than other baselines in \u00a7D."
        },
        {
            "heading": "6 Discussions",
            "text": "In previous experiments, we follow the setup of previous studies Meng et al. (2022a) and mainly evaluate methods to edit individual facts for a fair comparison. Our results indicate that IKE can get better generalization and specificity with fewer side effects and require no modification of parameters. Nevertheless, in order to investigate the feasibility\nMethod Prob. Drop (\u2193) Forgetting Rate (\u2193)\nof applying IKE to real-world scenarios, several important questions remain under-explored: (1) Can IKE be extended to accommodate a larger number of editing facts? Considering the limited input length of language models, it may not be feasible to include tremendous editing facts within the context. (2) Can IKE be adapted to handle different formats and domains of facts and prompts? In IKE, the domain and format of facts and prompts are kept consistent. However, in real-world settings, facts and prompts come in diverse forms.\nMitchell et al. (2022b) propose a retrieval-based method for editing multiple knowledge facts. Similarly, IKE with an external memory to store factual edits can retrieve the proper factual edit to construct context for a given prompt, thus avoid prepending all factual edits in context forever. To validate the generalization of IKE on different forms of facts or prompts, we replaced facts with neutral data from Wikipedia, or replaced prompts with generation prompts that prompt the LM to generate text related to the new object. Detailed discussion can be found in \u00a7E."
        },
        {
            "heading": "7 Conclusion",
            "text": "In this work, we examine the potential of in-context learning for knowledge editing on large-scale language models. Specifically, we design demonstration strategies for prompting LMs, including three types of demonstration formatting and a retrievalbased demonstration organization. We show that the proposed method, IKE, achieves competitive knowledge editing efficacy without requiring any parameter modifications, as well as maintains decent generalization and specificity performance. Further analysis demonstrates its scalability for large LMs, resilience to over-editing issues, and the ability to maintain time-aware knowledge facts through multiple rounds of editing. Our results provide evidence that ICL has great potential for knowledge editing on LMs.\nLimitations\nThe limitations of our work are summarized as follow: The limitations of our work primarily include the following aspects:\n1. We only discuss modifications on factual knowledge in language models and do not address the editing of other types of knowledge like commonsense knowledge.\n2. Introducing ICL in our method brings additional computational costs due to the longer context used.\n3. Despite discussing some of the side effects of knowledge editing, the mechanism of ICL is still unclear, and there may be potential risks associated with modifying the knowledge in the model.\n4. Although we provide some preliminary discussions on applying IKE in practical scenarios, there is still a considerable way to go before real-world applications can be realized."
        },
        {
            "heading": "Acknowledgements",
            "text": "This paper is supported by the National Key R&D Program of China under Grand No.2018AAA0102003, and the National Science Foundation of China under Grant No.61936012."
        },
        {
            "heading": "B Details of COUNTERFACT Dataset",
            "text": "Table 8 illustrates an example from COUNTERFACT. This entry requests that \"the mother tongue\n1https://github.com/kmeng01/rome\nof Danielle Darrieux should be changed from English to French\". Each entry has several paraphrase prompts and several neighborhood prompts. Paraphrase prompts are semantically equivalent to the original prompt, neighborhood prompts are those that share the same relation and object with the original prompt but have different subjects. The raw COUNTERFACT dataset also includes attribute prompts and generation prompts, but they are not adopted in our work. We use the first 2,000 records as test split for evaluation and other records are training split."
        },
        {
            "heading": "C Model Details",
            "text": "In Table 4, we discussed the performance of IKE on five different decoder-only GPT-like models at various scales. All models use 32 samples except for GPT-2 XL, which is limited by its maximum input length. The existing results indicate that on these models that have emerging ICL capabilities, IKE benefits from the increase in model size.\nWith the popularity of instruction tuning (Wei et al., 2021; Ouyang et al., 2022), LLMs have shown better alignment ability in following instructions. We also apply IKE for the recently widely discussed model, LLaMA 7B (Touvron et al., 2023a), a model from the era of instruction tuning and other instruction tuned models Vicuna 7B (Chiang et al., 2023) and LLaMA-2 Chat 7B (Touvron et al., 2023b) in Table 9. Their parameter scale is similar to the GPT-J model primarily discussed in the paper. The results show that IKE achieves similar performance in terms of ES/PS/NS on GPT-J and LLaMA, and achieves better NS in Vicuna and LLaMA-2. This suggests that IKE can also be used in instruction-tuned models. We will supplement this experimental result in the paper and also consider more instruction-tuned models of similar scale. Additionally, with the popularity of instruction tuning, we believe it is worth exploring the direction of designing better knowledge editing instructions and demonstrations for instruction tuned LMs in the future."
        },
        {
            "heading": "D Time-aware Knowledge Editing",
            "text": "Table 10 illustrates an example from TEMPLAMA 2. This entry shows that for (s, r, o) where subject s is Tom Brady and relation r is plays_for (P54), the object o is New England Patriots in 2019 and\n2https://github.com/google-research/language/ tree/master/language/templama\nTampa Bay Buccaneers in 2020. TEMPLAMA includes time-aware relations such as member of sports team, where the object of the relationship could be changed in different times. We collect three relations in TEMPLAMA: member of sports team, position held, employer including 2067 facts (t, s, r, o). We inject different facts: (t1, s, r, ot1), . . . , (tn, s, r, otn) for same subject and relation sequentially. By sampling knowledge facts (t, s, r, ot) and the object ot is changing for different time t and injecting facts in chronological order, we evaluate whether the editing history could be maintained by LMs.\nTake the president of US as example, we inject (2010, Obama), (2017, Trump) and (2021, Biden) sequentially. We probe the oldest fact: In 2010, the president of US was to test if the LM can still memorize the oldest fact after multiple edits of the same fact by the memorization ratio, Pt=tn(ot1 |s, r, t1)/Pt=t1(ot1 |s, r, t1). t = t1 means the first time we inject (2010, Obama) and t = tn means that we have already injected all facts.\nTable 11 shows that ROME forgets facts that have already been injected in LMs with an extremely low memorization ratio, indicating that the parameter updating of these time-aware facts may conflict in the same FFN module and cause the forgetting. Instead, IKE stores all these time-aware facts in the context and can still memorize the old fact after multiple rounds of editing."
        },
        {
            "heading": "E Detailed Discussions",
            "text": "E.1 Scale up to more factual edits\nMitchell et al. (2022b); Meng et al. (2022b) find that gradient-based knowledge editing methods encounter difficulties when attempting to update multiple knowledge facts simultaneously. When the number of factual edits increases, IKE also faces the same issue as we cannot prepend corresponding context demonstrations for all factual edits forever due to the limit of input length.\nMitchell et al. (2022b) proposes a memory-based retrieval-augmented method to handle multiple factual edits. For a given prompt, a scope classifier can retrieve the relevant knowledge fact from an external memory storing multiple factual edits. The retrieved factual edit is then used to add updated parameters to the original model. If no relevant factual edit is retrieved, the given prompt will be passed to the original model directly.\nSimilarly, IKE and retrieval augmentation can also be a good combination. An external memory is used to store multiple factual edits. For a given\nprompt, IKE can retrieve relevant knowledge facts and construct the demonstrations in context. Otherwise, we directly use original LM to generate the answer. With external memory and retrieval augmentation, We only need to retain in the context the fact that are relevant to the current prompt, along with their corresponding demonstrations.\nE.2 Generalization on facts and prompts\nIn IKE, the domain and format of facts and prompts are consistent. However, in reality, facts and prompts come in various formats and domains. Can IKE generalize between in-consistent facts and prompts?\nIn our main experiments, we assess the probability P(o\u2217|x, f, C). However, in real-world scenarios, prompts may have different formats than the facts. We also want the LM to generate text related to the new object o\u2217 instead of simply generating the object o\u2217 itself for these prompts. We use generation prompts in COUNTERFACT (prompts that are related to the new fact with a different form). Some generation examples are listed in Fig. 3. We can find that IKE can generalize to prompts with different forms and generation outputs are not simply new objects but texts related to the new objects.\nWe replaced facts with longer and more complicated neutral data retrieved from Wikipedia in 100 cases. By replacing the entities in the facts that are related to the original object oc with the new object o\u2217, we obtain new facts.\nWith the retrieved neutral data, IKE gets 75 PS on target prompts and 73 NS on neighborhood prompts, while PROMPT (retrieval-augmentation only, no examples) gets 65 and 64. The results indicate that despite the increased difficulty of updating\nfacts from longer and more complex neutral texts, IKE still exhibits higher levels of generalization and specificity compared to PROMPT."
        }
    ],
    "title": "Can We Edit Factual Knowledge by In-Context Learning?",
    "year": 2023
}