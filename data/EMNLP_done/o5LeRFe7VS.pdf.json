{
    "abstractText": "Factual probing is a method that uses prompts to test if a language model \u201cknows\u201d certain world knowledge facts. A problem in factual probing is that small changes to the prompt can lead to large changes in model output. Previous work aimed to alleviate this problem by optimizing prompts via text mining or fine-tuning. However, such approaches are relation-specific and do not generalize to unseen relation types. Here, we propose to use test-time augmentation (TTA) as a relation-agnostic method for reducing sensitivity to prompt variations by automatically augmenting and ensembling prompts at test time. Experiments show improved model calibration, i.e., with TTA, model confidence better reflects prediction accuracy. Improvements in prediction accuracy are observed for some models, but for other models, TTA leads to degradation. Error analysis identifies the difficulty of producing high-quality prompt variations as the main challenge for TTA. github.com/gokamoda/TTA4FactualProbing",
    "authors": [
        {
            "affiliations": [],
            "name": "Go Kamoda"
        },
        {
            "affiliations": [],
            "name": "Benjamin Heinzerling"
        },
        {
            "affiliations": [],
            "name": "Keisuke Sakaguchi"
        },
        {
            "affiliations": [],
            "name": "Kentaro Inui"
        }
    ],
    "id": "SP:98b0858da4fd0a4199b4fb8a346d698ec7033964",
    "references": [
        {
            "authors": [
                "Boxi Cao",
                "Hongyu Lin",
                "Xianpei Han",
                "Fangchao Liu",
                "Le Sun."
            ],
            "title": "Can prompt probe pretrained language models? understanding the invisible risks from a causal view",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational",
            "year": 2022
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association",
            "year": 2019
        },
        {
            "authors": [
                "Benjamin Heinzerling",
                "Kentaro Inui."
            ],
            "title": "Language models as knowledge bases: On entity representations, storage capacity, and paraphrased queries",
            "venue": "Proceedings of the 16th Conference of the European Chapter of the Association for Computational",
            "year": 2021
        },
        {
            "authors": [
                "Leila Khalatbari",
                "Maria Ryskina",
                "Rita Frieske",
                "Ryan Cotterell",
                "Zhijing Jin."
            ],
            "title": "State-of-the-art generalisation research in NLP: a taxonomy and review",
            "venue": "CoRR.",
            "year": 2022
        },
        {
            "authors": [
                "Zhengbao Jiang",
                "Frank F. Xu",
                "Jun Araki",
                "Graham Neubig"
            ],
            "title": "How can we know what language models know? Transactions of the Association for Computational Linguistics",
            "year": 2020
        },
        {
            "authors": [
                "Alex Krizhevsky",
                "Ilya Sutskever",
                "Geoffrey E. Hinton."
            ],
            "title": "Imagenet classification with deep convolutional neural networks",
            "venue": "Advances in Neural Information Processing Systems 25. Curran Associates, Inc.",
            "year": 2012
        },
        {
            "authors": [
                "Kazuhisa Matsunaga",
                "Akira Hamada",
                "Akane Minagawa",
                "Hiroshi Koga."
            ],
            "title": "Image classification of melanoma, nevus and seborrheic keratosis by deep neural network ensemble",
            "venue": "arXiv.",
            "year": 2017
        },
        {
            "authors": [
                "Jeffrey Pennington",
                "Richard Socher",
                "Christopher Manning."
            ],
            "title": "GloVe: Global vectors for word representation",
            "venue": "Proceedings of the 2014 Conference",
            "year": 2014
        },
        {
            "authors": [
                "F\u00e1bio Perez",
                "Cristina Vasconcelos",
                "Sandra Avila",
                "Eduardo Valle."
            ],
            "title": "Data augmentation for skin lesion analysis",
            "venue": "Lecture Notes in Computer Science. Springer International Publishing.",
            "year": 2018
        },
        {
            "authors": [
                "Fabio Petroni",
                "Tim Rockt\u00e4schel",
                "Sebastian Riedel",
                "Patrick Lewis",
                "Anton Bakhtin",
                "Yuxiang Wu",
                "Alexander Miller"
            ],
            "title": "Language models as knowledge bases",
            "venue": "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Process-",
            "year": 2019
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J. Liu."
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "Journal of Machine Learning Research.",
            "year": 2020
        },
        {
            "authors": [
                "Nils Rethmeier",
                "Isabelle Augenstein."
            ],
            "title": "A primer on contrastive pretraining in language processing: Methods, lessons learned, and perspectives",
            "venue": "ACM Comput. Surv.",
            "year": 2023
        },
        {
            "authors": [
                "Adam Roberts",
                "Colin Raffel",
                "Noam Shazeer"
            ],
            "title": "How much knowledge can you pack into the parameters of a language model",
            "venue": "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing",
            "year": 2020
        },
        {
            "authors": [
                "Chakaveh Saedi",
                "Ant\u00f3nio Branco",
                "Jo\u00e3o Ant\u00f3nio Rodrigues",
                "Jo\u00e3o Silva."
            ],
            "title": "WordNet embeddings",
            "venue": "Proceedings of the Third Workshop on Representation Learning for NLP.",
            "year": 2018
        },
        {
            "authors": [
                "drea Santilli",
                "Thibault Fevry",
                "Jason Alan Fries",
                "Ryan Teehan",
                "Teven Le Scao",
                "Stella Biderman",
                "Leo Gao",
                "Thomas Wolf",
                "Alexander M Rush"
            ],
            "title": "Multitask prompted training enables zero-shot task generalization",
            "venue": "In International Conference on Learning",
            "year": 2022
        },
        {
            "authors": [
                "Taylor Shin",
                "Yasaman Razeghi",
                "Robert L. Logan IV",
                "Eric Wallace",
                "Sameer Singh."
            ],
            "title": "AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in",
            "year": 2020
        },
        {
            "authors": [
                "J\u00f6rg Tiedemann",
                "Santhosh Thottingal."
            ],
            "title": "OPUSMT \u2013 building open translation services for the world",
            "venue": "Proceedings of the 22nd Annual Conference of the European Association for Machine Translation.",
            "year": 2020
        },
        {
            "authors": [
                "Guotai Wang",
                "Wenqi Li",
                "Michael Aertsen",
                "Jan Deprest",
                "S\u00e9bastien Ourselin",
                "Tom Vercauteren."
            ],
            "title": "Aleatoric uncertainty estimation with test-time augmentation for medical image segmentation with convolutional neural networks",
            "venue": "Neurocomputing.",
            "year": 2019
        },
        {
            "authors": [
                "Xuezhi Wang",
                "Jason Wei",
                "Dale Schuurmans",
                "Quoc Le",
                "Ed Chi",
                "Sharan Narang",
                "Aakanksha Chowdhery",
                "Denny Zhou"
            ],
            "title": "Self-consistency improves chain of thought reasoning in language models",
            "year": 2023
        },
        {
            "authors": [
                "Jason Wei",
                "Maarten Bosma",
                "Vincent Y. Zhao",
                "Kelvin Guu",
                "Adams Wei Yu",
                "Brian Lester",
                "Nan Du",
                "Andrew M. Dai",
                "Quoc V. Le"
            ],
            "title": "Finetuned language models are zero-shot learners",
            "year": 2022
        },
        {
            "authors": [
                "Zexuan Zhong",
                "Dan Friedman",
                "Danqi Chen."
            ],
            "title": "Factual probing is [MASK]: Learning vs",
            "venue": "learning to recall. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language",
            "year": 2021
        },
        {
            "authors": [
                "E GenBench"
            ],
            "title": "Evaluation Card To situate our work in the broader context of efforts to understand and improve the generalization of machine learning models for natural language processing, Table 4 provides a GenBench evaluation card (Hupkes et al., 2022)",
            "year": 2022
        }
    ],
    "sections": [
        {
            "text": "github.com/gokamoda/TTA4FactualProbing"
        },
        {
            "heading": "1 Introduction",
            "text": "Pre-trained language models (LMs) such as BERT (Devlin et al., 2019) and T5 (Raffel et al., 2020) implicitly encode world knowledge from the training corpus in their parameters (Roberts et al., 2020). Encoded knowledge can be retrieved from an LM via a suitable prompt (Petroni et al., 2019). For example, a prompt such as \u201cWhere did Albert Einstein die?\u201d is designed to retrieve the fact that \u201cAlbert Einstein died in Princeton.\u201d However, this capability is not robust since small changes to the prompt can lead to drastic output changes (Heinzerling and Inui, 2021; Cao et al., 2022). If the model fails to answer correctly, it is thus difficult to distinguish if it did not learn the corresponding fact during pre-training or if it actually did but did not produce the correct answer with the given prompt.\nPrior work has aimed at finding better prompts for factual probing1, typically by employing supervised learning to find an optimal input token sequence for a given relation (Shin et al., 2020; Jiang et al., 2020; Zhong et al., 2021). Since these approaches require supervision for each relation, they do not generalize to unseen relation types, which reduces practical applicability.\nHere, we propose to use test time augmentation (TTA) as an unsupervised, relation-agnostic approach for improving prompt-based factual probing. TTA originates in computer vision, where given an input image at test time, the idea is to 1) apply augmentations such as flipping, cropping, or contrast changes, 2) have the model make predictions for all augmented versions of the image, and then 3) aggregate these predictions to obtain the final prediction. TTA has been found to increase robust-\n1While \u201cprobing\u201d more commonly refers to model analysis via light-weight classifiers, we follow prior work in using \u201cfactual probing\u201d to denote model analysis via knowledgeeliciting prompts.\nness and to improve model calibration by reducing overconfidence in wrong predictions (Krizhevsky et al., 2012; Wang et al., 2019; Perez et al., 2018; Matsunaga et al., 2017). The benefits are also desirable in factual probing, where LMs should exhibit robustness to paraphrases and should generate wellcalibrated predictions. However, TTA has found little use in NLP so far. While it is relatively easy to create feature-preserving augmentations of images (e.g., a flipped image of a cat is still an image of a cat), meaning-preserving augmentation of text is a challenging problem since even small edits, such as replacing a single token, can completely change the meaning of a sentence (Rethmeier and Augenstein, 2023). In this paper, we empirically demonstrate the benefits and challenges of TTA for factual probing.\nAs a similar approach in the context of chainof-thought reasoning, Wang et al. (2023) prepare a single prompt as model input and aggregate multiple outputs (\u201creasoning paths\u201d) to improve model performance. TTA differs from this method in that it automatically prepares multiple inputs, i.e., prompts.\nTo apply TTA to factual probing, we add a prompt augmenter and a prediction aggregator to the prediction process (Figure 1). First, the input prompt is automatically augmented by the augmenter. The augmented prompts are then individually fed to an LM. The aggregator collects the outputs for each prompt and determines the final prediction. Our evaluation of this setup consists of two parts: We 1) evaluated the overall prediction accuracy and investigated the impact of the number of augmented prompts on the accuracy, and 2) inspected the change in the confidence of model predictions.\nResults showed that the greater the number of augmented prompts, the better the performance when implementing TTA. TTA was also effective in reducing the number of overconfident and incorrect outputs. However, in terms of overall accuracy, TTA was only effective on smaller models."
        },
        {
            "heading": "2 Setup",
            "text": "Dataset We constructed a dataset of 12,500 relational facts from Wikidata. Each fact is composed of a subject, a relation, and an object. We filtered out facts with multiple possible objects and collected 500 unique facts for each of the 25 relations2.\n2The relations we selected are shown in the appendix.\nFor each relation, we manually created an English prompt template, e.g., \u201cWhere did {subject} die?\u201d\nAugmenter To yield paraphrases with variety, the augmenter uses three types of prompt augmentations. The first type is synonym replacement, which replaces words in the input prompt with a synonym. For instance, this type of augmentation replaced the word \u201cburied\u201d with \u201cinhumed\u201d. Synonyms are selected via word embedding similarity using GLoVe (Pennington et al., 2014) or via WordNet (Saedi et al., 2018). The second augmentation type is back-translation, with French, Russian, German, Spanish, and Japanese as target languages. The third augmentation type is stopword filtering, which removes stopwords from the prompts.\nFrom a single original prompt, the augmenter produces one prompt via stopword filtering and four prompts for each of the seven other augmentation methods, resulting in a total of 30 prompts.\nModel We ran experiments on the following pre-trained language models: T5 for Closed Book Question Answering (Small, Large, 3B, 11B) (Roberts et al., 2020), FLAN-T5 (Small, XL) (Wei et al., 2022), and T0_3B (Sanh et al., 2022).\nModels decode with beam-search where the beam size is fixed to 10 and return generated sequences with scores. Scores are in the order of log-likelihood (negative), and the exponentiated scores are in the order of probability.\nAggregator We aggregate generations by taking the sum of generation probabilities. The model output with generation probabilities (PLM) for each of the K augmented prompts (pi) will be fed into the aggregator to choose one final prediction. The aggregator recalculates the generation score (s) by taking the sum of the generation probabilities of identical generations (Eq.1). The final prediction of an object y for the fact with subject x and relation r is the one with the highest score (Eq.2).\ns(y\u2032|x, r) = K\u2211 i=1 PLM(y \u2032|pi) (1)\ny = argmax(s(\u00b7|x, r))y\u2032 (2)\nEvaluation Metric We quantify the effect of TTA as the relative change in exact match accuracy compared to not using TTA:\nrelative effect = (#correct w/ TTA) + 1\n(#correct w/o TTA) + 1 (3)\nTo prevent division by zero, one is added to the numerator and the denominator. The metric judges correct only if the output is a perfect match. However, we observed cases where FLAN models are indifferent to capitalizations, e.g., the model generated \u201cafrica\u201d instead of \u201cAfrica. For this reason, we exceptionally adopt case-insensitive match accuracy for the FLAN models."
        },
        {
            "heading": "3 Results",
            "text": ""
        },
        {
            "heading": "3.1 Positive Effects",
            "text": "Accuracy The top half of Table 1 shows an example of TTA increasing the accuracy on the T5-11B model. The model gave an incorrect answer with the original prompt but produced the correct answer with TTA.\nHere, we assess the impact of the number of prompts (K) on TTA performance. As described in \u00a72, the augmenter produces 29 paraphrases from an original prompt. For each subset of K prompts containing the original prompt and K\u22121(1 \u2264 K \u2264 30) randomly sampled paraphrases, we measure the change in model accuracy in terms of relative effect (Eq.3).\nWe ran five iterations of random sampling and report the result in Figure 2. The sudden drop from K = 13 to K = 2 shows that models are highly sensitive to prompt variations. The relative effect appears to stabilize in the range of 20 \u2264 K \u2264 30 prompts, suggesting that aggregation of multiple prompts via TTA mitigates model sensitivity to variation of individual prompts. A comparison of\n3K = 1 setting indicates the baseline setting without TTA.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0 T5-Small w/ TTA w/out TTA\nT5-Large w/ TTA w/out TTA\n0.0 0.2 0.4 0.6 0.8 1.0 0.0\n0.2\n0.4\n0.6\n0.8\n1.0 T5-3B w/ TTA w/out TTA\n0.0 0.2 0.4 0.6 0.8 1.0\nT5-11B w/ TTA w/out TTAAc cu ra\ncy\nNormalized Confidence\nFigure 3: Comparison of model calibration with TTA and without TTA.TTA improves the model calibration of all four models in the comparison. Dotted lines represent ideal calibration, i.e., model confidence is equal to prediction accuracy.\nthe result between K = 1 and K = 30 shows that TTA leads to better performance for three models, namely T5-Small, T5-3B, and T0_3B.\nCalibration In addition to increased accuracy, a second potential benefit of TTA is to improve model calibration by reducing overconfidence in incorrect answers. In this section, we investigate the effect of TTA on model calibration.\nIn our setup, the aggregator re-ranks generations by calculating the sum of the generation probability of identical generations for each fact instance. Because the original generation scores cannot be used as confidence after aggregation, we define the confidence of the aggregator as the ratio of the score to the final output and the sum of calculated scores for all candidate generations from all prompts for each fact instance.\nconfidence = scorefinal output\u2211\ncandidates score (4)\nTo enable a comparison of the relationship between the defined confidence and accuracy with and without TTA, we normalize scores to a maximum of 1. We then calculate the accuracy of model prediction with confidence between 0.1i and 0.1(i+ 1) for each 0 \u2264 i < 10 (i \u2208 N).\nFigure 3 shows the relation between the calculated confidence and accuracy before and after ap-\n\u201c South America \u201d, the model gives a wrong answer under the original prompt. Applying TTA yields the correct answer (top half). The bottom half of the table shows an example of TTA degrading performance. The correct answer is \u201c Heidelberg \u201d, but TTA yields \u201c Erlangen, Germany \u201d. The Type column specifies the augmentation type, with \u201cGLoVe\u201d and \u201cWordNet\u201d referring to synonym replacement and the \u201cbt-\u201d prefix indicating back-translation. Further examples are given in Appendix Table 3.\nplying TTA. Without TTA, models tend to exhibit overconfidence by generating high-confidence answers with relatively low accuracy. This effect is especially pronounced in larger LMs, i.e., T5-3b and T5-11B. In contrast, after aggregation of multiple answers with TTA, confidence much better reflects accuracy since the accuracy of high-confidence predictions improved. These results show that TTA can improve model calibration."
        },
        {
            "heading": "3.2 Negative Effects",
            "text": "Accuracy Accuracy declines when the original prompt elicits the correct answer, but the TTA results in an incorrect answer. The bottom half of Table 1 shows an example of this. The 30 prompts yielded 18 unique model answers, among which seven prompts yielded the wrong answer \u201cErlangen, Germany\u201d, while only four prompted the correct answer \u201cHeidelberg\u201d (Table 1 shows only a subset of these answers). Overall, TTA led to performance degradation with the T5-Large, T5-11B, and the FLAN-T5 models (Figure 2).\nError Analysis Manual inspection suggests that the negative effects of TTA are mainly due to the low quality of the augmented prompts. Ideally, paraphrases should be grammatical, meaningpreserving, and exhibit variety in vocabulary choice and sentence structure. However, many augmented\nprompts, such as those shown in the bottom half of Table 1, do not meet these criteria. For example, not all augmented prompts preserve the meaning of the original prompt.\nTo better understand the impact of the quality of automatically augmented prompts, we conducted additional evaluations. The first is to remove extremely poor paraphrases. Inspection of TTA errors revealed that one of the relations, namely \u201cfollows\u201d, was particularly error-prone in the augmentation step, likely due to the large variety of different entity and event types4 for which this relation is used in Wikidata. We thus analyze the impact of TTA after removing all instances of the \u201cfollows\u201d relation from the dataset. The second evaluation aims to improve the quality of paraphrases by using a large language model, namely GPT-3 text-davinci003 (Brown et al., 2020), assuming that this larger LM produces better paraphrases than the simple augmentation methods used in our original setup. Figure 4 shows how GPT-3-based augmentation changed the effect of TTA on the T5 models. With the exception of T5-11B, augmentations produced by GPT-3 show a positive effect for all models.\n4Arguments of the \u201cfollows\u201d relation include: numbers (2 follows 1), months (February follows January) and events (the 2024 Paris Olympics follows the 2020 Tokyo Olympics)."
        },
        {
            "heading": "4 Conclusions",
            "text": "We applied the idea of test-time augmentation (TTA) to language models, motivated by the observation that models are not robust to prompt variations. Specifically, we examined the effect of TTA on model accuracy and calibration in a factual probing setting.\nOut of the seven models we investigated, TTA had a positive effect on the accuracy of smaller models, namely T5-Small, T5-3B, T0_3B. When controlling for the quality of augmentations, TTA also improved the accuracy of one more model (T5-Large). On other models, TTA had a negative effect in terms of accuracy. In terms of model calibration, we observed a positive effect since TTA reduced the number of high-confidence incorrect model answers.\nThe main remaining question is why the effect of TTA is inconsistent. We hypothesized that the inconsistent effect of TTA is due to the poor quality of automatically augmented prompts, and our analysis showed that the high quality of paraphrases is one of the important conditions for TTA to be effective. A related question we left to future work is how to enable TTA for relations that are difficult to paraphrase automatically, such as the \u201cfollows\u201d relation (See Section 3.2).\nWhile the ultimate goal is arguably to make language models robust against paraphrases without extensions like TTA, the results presented in this work show the potential benefit of TTA, especially for smaller LMs.\nLimitations\nThe TTA setup we present in this paper can be applied to short-answer generation tasks such as factual probing or classification tasks. A different setup would be necessary to apply TTA to tasks involving more complex model outputs, such as summarization or long-form question answering, since the aggregation step in our setup is not suited for such outputs.\nIn factual probing, TTA showed improvement in overall accuracy on three out of seven models. This means that whether or not TTA would improve the accuracy is unsure beforehand in practical uses. Deeper analyses of what improves/degrades the performance are needed to judge whether to use TTA or not.\nEthics\nOur experiments involved pre-trained language models and Wikidata, both of which are characterized by various forms of social biases. Consequently, our experiments and conclusions inherit and possibly amplify these biases."
        },
        {
            "heading": "Acknowledgements",
            "text": "This work was supported by JST CREST Grant Number JPMJCR20D2, Japan, and JSPS KAKENHI Grant Numbers JP21K17814, JP21K21343, and JP22H00524. We would like to thank the members of TohokuNLP for their frequent participation in discussions during the course of this research."
        },
        {
            "heading": "A Dataset",
            "text": "We collected 500 relational facts for each of the 25 relations selected from WikiData. Table 2 shows the corresponding Wikidata property ID and English property names."
        },
        {
            "heading": "B Augmentation Methods",
            "text": "Synonym Replacement We use the Python library \u201cTextAttack\u201d to replace words with their synonyms. Specifically, for synonym replacement using WordNet, we use the WordNetAugmenter, and for synonym replacement using GloVe embeddings, we use the WordSwapEmbedding class. Back-translation We first translate the original prompt to eight candidates in the target language. Each candidate is then translated back into eight candidates in the source language, resulting in a total of 64 back-translated prompt candidates. We adopt the round-trip probability as the score of the back-translated prompt candidates and select four candidates using the aggregation method mentioned in Section 2. For translations, we used OPUS-MT models (Tiedemann and Thottingal, 2020) The OPUS-MT models occupy roughly the same memory size as the T5-Small model. Stopwords-filtering This method removes stopwords and diacritics from the original prompt using the Python library \u201cTexthero\u201d."
        },
        {
            "heading": "C Count-based Aggregation of Model Answers",
            "text": "Counting the number of appearances in the generations is one method of aggregation. We did not use count-based aggregation in our main experiments because the possibility of having multiple generations with the same counts is high. The phenomenon is predicted to occur more often when we increase the number of sequences the model outputs. In addition, this method cannot take confidence into account as all generations by beam-search are equally weighted.\nThe result of using count-based aggregation is shown in figure 5. TTA degrades model performance with count-based aggregation."
        },
        {
            "heading": "D Additional output examples",
            "text": "Table 3 gives a complete example of all augmented prompts and corresponding model outputs. (A subset of these is also shown in Table 1 in the main part of the paper).\nto the question \u201cWhere is Hans-Georg Gadamer buried?\u201d is \u201c Heidelberg \u201d, but the aggregator returned\n\u201c Erlangen, Germany \u201d."
        },
        {
            "heading": "E GenBench Evaluation Card",
            "text": "To situate our work in the broader context of efforts to understand and improve the generalization of machine learning models for natural language processing, Table 4 provides a GenBench evaluation card (Hupkes et al., 2022)."
        }
    ],
    "title": "Test-time Augmentation for Factual Probing",
    "year": 2023
}