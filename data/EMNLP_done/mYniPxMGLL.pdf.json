{
    "abstractText": "Large language models (e.g., GPT-4) are uniquely capable of producing highly rated text simplification, yet current human evaluation methods fail to provide a clear understanding of systems\u2019 specific strengths and weaknesses. To address this limitation, we introduce SALSA, an edit-based human annotation framework that enables holistic and fine-grained text simplification evaluation. We develop twenty one linguistically grounded edit types, covering the full spectrum of success and failure across dimensions of conceptual, syntactic and lexical simplicity. Using SALSA, we collect 19K edit annotations on 840 simplifications, revealing discrepancies in the distribution of simplification strategies performed by fine-tuned models, prompted LLMs and humans, and find GPT-3.5 performs more quality edits than humans, but still exhibits frequent errors. Using our finegrained annotations, we develop LENS-SALSA, a reference-free automatic simplification metric, trained to predict sentenceand word-level quality simultaneously. Additionally, we introduce word-level quality estimation for simplification and report promising baseline results. Our data, new metric, and annotation toolkit are available at https://salsa-eval.com.",
    "authors": [
        {
            "affiliations": [],
            "name": "David Heineman"
        },
        {
            "affiliations": [],
            "name": "Yao Dou"
        },
        {
            "affiliations": [],
            "name": "Mounica Maddela"
        },
        {
            "affiliations": [],
            "name": "Wei Xu"
        }
    ],
    "id": "SP:715893477865007c8c860ff4e2fec8cca57339e7",
    "references": [
        {
            "authors": [
                "Qian Yang"
            ],
            "title": "Med-easi: Finely annotated dataset",
            "year": 2023
        },
        {
            "authors": [
                "Eleftheria Briakou",
                "Sweta Agrawal",
                "Ke Zhang",
                "Joel Tetreault",
                "Marine Carpuat."
            ],
            "title": "A review of human evaluation for style transfer",
            "venue": "Proceedings of the 1st Workshop on Natural Language Generation, Evaluation, and Metrics (GEM 2021), pages 58\u201367,",
            "year": 2021
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners. Advances in neural information processing",
            "year": 2020
        },
        {
            "authors": [
                "Dominique Brunato",
                "Felice Dell\u2019Orletta",
                "Giulia Venturi"
            ],
            "title": "Linguistically-based comparison of different approaches to building corpora for text simplification: A case study on Italian",
            "venue": "Frontiers in Psychology,",
            "year": 2022
        },
        {
            "authors": [
                "Meng Cao",
                "Yue Dong",
                "Jackie Cheung."
            ],
            "title": "Hallucinated but factual! inspecting the factuality of hallucinations in abstractive summarization",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long",
            "year": 2022
        },
        {
            "authors": [
                "R\u00e9mi Cardon",
                "Adrien Bibal",
                "Rodrigo Wilkens",
                "David Alfter",
                "Magali Norr\u00e9",
                "Adeline M\u00fcller",
                "Watrin Patrick",
                "Thomas Fran\u00e7ois."
            ],
            "title": "Linguistic corpus annotation for automatic text simplification evaluation",
            "venue": "Proceedings of the 2022 Conference on Empiri-",
            "year": 2022
        },
        {
            "authors": [
                "Mert Cemri",
                "Tolga \u00c7ukur",
                "Aykut Ko\u00e7."
            ],
            "title": "Unsupervised simplification of legal texts",
            "venue": "arXiv preprint arXiv:2209.00557.",
            "year": 2022
        },
        {
            "authors": [
                "R. Chandrasekar",
                "Christine Doran",
                "B. Srinivas."
            ],
            "title": "Motivations and methods for text simplification",
            "venue": "COLING 1996 Volume 2: The 16th International Conference on Computational Linguistics.",
            "year": 1996
        },
        {
            "authors": [
                "Jiangjie Chen",
                "Rui Xu",
                "Wenxuan Zeng",
                "Changzhi Sun",
                "Lei Li",
                "Yanghua Xiao."
            ],
            "title": "Converge to the truth: Factual error correction via iterative constrained editing",
            "venue": "Proceedings of the Thirty-Seventh AAAI Conference on Artificial Intelligence, AAAI\u201923.",
            "year": 2023
        },
        {
            "authors": [
                "Wei-Lin Chiang",
                "Zhuohan Li",
                "Zi Lin",
                "Ying Sheng",
                "Zhanghao Wu",
                "Hao Zhang",
                "Lianmin Zheng",
                "Siyuan Zhuang",
                "Yonghao Zhuang",
                "Joseph E. Gonzalez",
                "Ion Stoica",
                "Eric P. Xing"
            ],
            "title": "Vicuna: An opensource chatbot impressing gpt-4 with 90%* chatgpt",
            "year": 2023
        },
        {
            "authors": [
                "Leshem Choshen",
                "Omri Abend."
            ],
            "title": "Inherent biases in reference-based evaluation for grammatical error correction",
            "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 632\u2013642,",
            "year": 2018
        },
        {
            "authors": [
                "Oscar M Cumbicus-Pineda",
                "Itziar Gonzalez-Dios",
                "Aitor Soroa."
            ],
            "title": "Linguistic capabilities for a checklist-based evaluation in automatic text simplification",
            "venue": "CTTS@ SEPLN.",
            "year": 2021
        },
        {
            "authors": [
                "Oscar M. Cumbicus-Pineda",
                "Itziar Gonzalez-Dios",
                "Aitor Soroa."
            ],
            "title": "A syntax-aware edit-based system for text simplification",
            "venue": "Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2021), pages 324\u2013",
            "year": 2021
        },
        {
            "authors": [
                "Ashwin Devaraj",
                "William Sheffield",
                "Byron Wallace",
                "Junyi Jessy Li."
            ],
            "title": "Evaluating factuality in text simplification",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 7331\u20137345,",
            "year": 2022
        },
        {
            "authors": [
                "Yue Dong",
                "Zichao Li",
                "Mehdi Rezagholizadeh",
                "Jackie Chi Kit Cheung."
            ],
            "title": "EditNTS: An neural programmer-interpreter model for sentence simplification through explicit editing",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computa-",
            "year": 2019
        },
        {
            "authors": [
                "Yao Dou",
                "Maxwell Forbes",
                "Rik Koncel-Kedziorski",
                "Noah A. Smith",
                "Yejin Choi."
            ],
            "title": "Is GPT-3 text indistinguishable from human text? scarecrow: A framework for scrutinizing machine text",
            "venue": "Proceedings of the 60th Annual Meeting of the Association",
            "year": 2022
        },
        {
            "authors": [
                "Mark Dras."
            ],
            "title": "Tree adjoining grammar and the reluctant paraphrasing of text",
            "venue": "Macquarie University Sydney.",
            "year": 1999
        },
        {
            "authors": [
                "Wanyu Du",
                "Vipul Raheja",
                "Dhruv Kumar",
                "Zae Myung Kim",
                "Melissa Lopez",
                "Dongyeop Kang."
            ],
            "title": "Understanding iterative revision from human-written text",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Vol-",
            "year": 2022
        },
        {
            "authors": [
                "Markus Freitag",
                "George Foster",
                "David Grangier",
                "Viresh Ratnakar",
                "Qijun Tan",
                "Wolfgang Macherey."
            ],
            "title": "Experts, errors, and context: A large-scale study of human evaluation for machine translation",
            "venue": "Transactions of the Association for Computational Linguis-",
            "year": 2021
        },
        {
            "authors": [
                "Aparna Garimella",
                "Abhilasha Sancheti",
                "Vinay Aggarwal",
                "Ananya Ganesh",
                "Niyati Chhaya",
                "Nandakishore Kambhatla."
            ],
            "title": "Text simplification for legal domain: Insights and challenges",
            "venue": "Proceedings of the Natural Legal Language Processing Workshop",
            "year": 2022
        },
        {
            "authors": [
                "Sian Gooding."
            ],
            "title": "On the ethical considerations of text simplification",
            "venue": "Ninth Workshop on Speech and Language Processing for Assistive Technologies (SLPAT-2022), pages 50\u201357, Dublin, Ireland. Association for Computational Linguistics.",
            "year": 2022
        },
        {
            "authors": [
                "Sian Gooding",
                "Manuel Tragut."
            ],
            "title": "One size does not fit all: The case for personalised word complexity models",
            "venue": "Findings of the Association for Computational Linguistics: NAACL 2022, pages 353\u2013365, Seattle, United States. Association for Computational",
            "year": 2022
        },
        {
            "authors": [
                "Tanya Goyal",
                "Junyi Jessy Li",
                "Greg Durrett."
            ],
            "title": "News summarization and evaluation in the era of gpt-3",
            "venue": "arXiv preprint arXiv:2209.12356.",
            "year": 2022
        },
        {
            "authors": [
                "Arnav Gudibande",
                "Eric Wallace",
                "Charles Burton Snell",
                "Xinyang Geng",
                "Hao Liu",
                "P. Abbeel",
                "Sergey Levine",
                "Dawn Song."
            ],
            "title": "The false promise of imitating proprietary llms",
            "venue": "ArXiv, abs/2305.15717.",
            "year": 2023
        },
        {
            "authors": [
                "Tatsunori B. Hashimoto",
                "Hugh Zhang",
                "Percy Liang."
            ],
            "title": "Unifying human and statistical evaluation for natural language generation",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Hu-",
            "year": 2019
        },
        {
            "authors": [
                "David Heineman",
                "Yao Dou",
                "Wei Xu."
            ],
            "title": "Thresh: A unified, customizable and deployable platform for fine-grained text evaluation",
            "venue": "arXiv preprint arXiv:2308.06953.",
            "year": 2023
        },
        {
            "authors": [
                "T Jaeger",
                "Roger Levy."
            ],
            "title": "Speakers optimize information density through syntactic reduction",
            "venue": "Advances in neural information processing systems, 19.",
            "year": 2006
        },
        {
            "authors": [
                "Ziwei Ji",
                "Nayeon Lee",
                "Rita Frieske",
                "Tiezheng Yu",
                "Dan Su",
                "Yan Xu",
                "Etsuko Ishii",
                "Ye Jin Bang",
                "Andrea Madotto",
                "Pascale Fung."
            ],
            "title": "Survey of hallucination in natural language generation",
            "venue": "ACM Comput. Surv., 55(12).",
            "year": 2023
        },
        {
            "authors": [
                "Chao Jiang",
                "Mounica Maddela",
                "Wuwei Lan",
                "Yang Zhong",
                "Wei Xu."
            ],
            "title": "Neural CRF model for sentence alignment in text simplification",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7943\u20137960, On-",
            "year": 2020
        },
        {
            "authors": [
                "Xiaotong Jiang",
                "Zhongqing Wang",
                "Guodong Zhou."
            ],
            "title": "Semantic simplification for sentiment classification",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 11022\u201311032, Abu Dhabi, United Arab",
            "year": 2022
        },
        {
            "authors": [
                "Sebastian Joseph",
                "Kathryn Kazanas",
                "Keziah Reina",
                "Vishnesh J Ramanathan",
                "Wei Xu",
                "Byron C Wallace",
                "Junyi Jessy Li."
            ],
            "title": "Multilingual simplification of medical texts",
            "venue": "arXiv preprint arXiv:2305.12532.",
            "year": 2023
        },
        {
            "authors": [
                "Klaus Krippendorff."
            ],
            "title": "Content analysis: An introduction to its methodology",
            "venue": "Sage publications.",
            "year": 2018
        },
        {
            "authors": [
                "Reno Kriz",
                "Marianna Apidianaki",
                "Chris CallisonBurch."
            ],
            "title": "Simple-QE: Better automatic quality estimation for text simplification",
            "venue": "arXiv preprint arXiv:2012.12382.",
            "year": 2020
        },
        {
            "authors": [
                "Dhruv Kumar",
                "Lili Mou",
                "Lukasz Golab",
                "Olga Vechtomova."
            ],
            "title": "Iterative edit-based unsupervised sentence simplification",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7918\u20137928, Online. Association",
            "year": 2020
        },
        {
            "authors": [
                "Philippe Laban",
                "Tobias Schnabel",
                "Paul Bennett",
                "Marti A. Hearst."
            ],
            "title": "Keep it simple: Unsupervised simplification of multi-paragraph text",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International",
            "year": 2021
        },
        {
            "authors": [
                "Wuwei Lan",
                "Chao Jiang",
                "Wei Xu."
            ],
            "title": "Neural semi-Markov CRF for monolingual word alignment",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Lan-",
            "year": 2021
        },
        {
            "authors": [
                "Arle Lommel",
                "Hans Uszkoreit",
                "Aljoscha Burchardt."
            ],
            "title": "Multidimensional quality metrics (MQM): A framework for declaring and describing translation quality metrics",
            "venue": "Revista Tradum\u00e0tica: tecnologies de la traducci\u00f3, 12:455\u2013463.",
            "year": 2014
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter."
            ],
            "title": "Decoupled weight decay regularization",
            "venue": "International Conference on Learning Representations.",
            "year": 2019
        },
        {
            "authors": [
                "Mounica Maddela",
                "Fernando Alva-Manchego",
                "Wei Xu."
            ],
            "title": "Controllable text simplification with explicit paraphrasing",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Lan-",
            "year": 2021
        },
        {
            "authors": [
                "Mounica Maddela",
                "Yao Dou",
                "David Heineman",
                "Wei Xu."
            ],
            "title": "LENS: A learnable evaluation metric for text simplification",
            "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 16383\u2013",
            "year": 2023
        },
        {
            "authors": [
                "Louis Martin",
                "\u00c9ric de la Clergerie",
                "Beno\u00eet Sagot",
                "Antoine Bordes."
            ],
            "title": "Controllable sentence simplification",
            "venue": "Proceedings of the Twelfth Language Resources and Evaluation Conference, pages 4689\u2013 4698, Marseille, France. European Language Re-",
            "year": 2020
        },
        {
            "authors": [
                "Louis Martin",
                "Angela Fan",
                "\u00c9ric de la Clergerie",
                "Antoine Bordes",
                "Beno\u00eet Sagot."
            ],
            "title": "MUSS: Multilingual unsupervised sentence simplification by mining paraphrases",
            "venue": "Proceedings of the Thirteenth Language Resources and Evaluation Conference, pages",
            "year": 2022
        },
        {
            "authors": [
                "Joshua Maynez",
                "Shashi Narayan",
                "Bernd Bohnet",
                "Ryan McDonald."
            ],
            "title": "On faithfulness and factuality in abstractive summarization",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1906\u20131919, On-",
            "year": 2020
        },
        {
            "authors": [
                "Clara Meister",
                "Ryan Cotterell",
                "Tim Vieira."
            ],
            "title": "If beam search is the answer, what was the question? In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2173\u20132185, Online",
            "venue": "Association for Computa-",
            "year": 2020
        },
        {
            "authors": [
                "Masaaki Nagata",
                "Katsuki Chousa",
                "Masaaki Nishino."
            ],
            "title": "A supervised word alignment method based on cross-language span prediction using multilingual BERT",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing",
            "year": 2020
        },
        {
            "authors": [
                "OpenAI."
            ],
            "title": "ChatGPT: Optimizing language models for dialogue",
            "venue": "https://openai.com/blog/ chatgpt.",
            "year": 2022
        },
        {
            "authors": [
                "OpenAI."
            ],
            "title": "GPT-4 technical report",
            "venue": "arXiv preprint arXiv:2303.08774.",
            "year": 2023
        },
        {
            "authors": [
                "Long Ouyang",
                "Jeff Wu",
                "Xu Jiang",
                "Diogo Almeida",
                "Carroll L Wainwright",
                "Pamela Mishkin",
                "Chong Zhang",
                "Sandhini Agarwal",
                "Katarina Slama",
                "Alex Ray"
            ],
            "title": "Training language models to follow instructions with human feedback",
            "year": 2022
        },
        {
            "authors": [
                "Gustavo H. Paetzold",
                "Lucia Specia."
            ],
            "title": "Text simplification as tree transduction",
            "venue": "Proceedings of the 9th Brazilian Symposium in Information and Human Language Technology.",
            "year": 2013
        },
        {
            "authors": [
                "Artidoro Pagnoni",
                "Vidhisha Balachandran",
                "Yulia Tsvetkov."
            ],
            "title": "Understanding factuality in abstractive summarization with FRANK: A benchmark for factuality metrics",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Asso-",
            "year": 2021
        },
        {
            "authors": [
                "Kishore Papineni",
                "Salim Roukos",
                "Todd Ward",
                "WeiJing Zhu."
            ],
            "title": "Bleu: a method for automatic evaluation of machine translation",
            "venue": "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311\u2013318, Philadelphia,",
            "year": 2002
        },
        {
            "authors": [
                "Fabio Petroni",
                "Tim Rockt\u00e4schel",
                "Sebastian Riedel",
                "Patrick Lewis",
                "Anton Bakhtin",
                "Yuxiang Wu",
                "Alexander Miller"
            ],
            "title": "Language models as knowledge bases",
            "venue": "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Pro-",
            "year": 2019
        },
        {
            "authors": [
                "Jipeng Qiang",
                "Yun Li",
                "Yi Zhu",
                "Yunhao Yuan",
                "Xindong Wu."
            ],
            "title": "Lexical simplification with pretrained encoders",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 8649\u20138656.",
            "year": 2020
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J Liu"
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "J. Mach. Learn. Res.,",
            "year": 2020
        },
        {
            "authors": [
                "Pranav Rajpurkar",
                "Robin Jia",
                "Percy Liang."
            ],
            "title": "Know what you don\u2019t know: Unanswerable questions for SQuAD",
            "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 784\u2013789,",
            "year": 2018
        },
        {
            "authors": [
                "Ricardo Rei",
                "Craig Stewart",
                "Ana C Farinha",
                "Alon Lavie."
            ],
            "title": "COMET: A neural framework for MT evaluation",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2685\u20132702, Online. Association",
            "year": 2020
        },
        {
            "authors": [
                "Ricardo Rei",
                "Marcos Treviso",
                "Nuno M. Guerreiro",
                "Chrysoula Zerva",
                "Ana C Farinha",
                "Christine Maroti",
                "Jos\u00e9 G.C. de Souza",
                "Taisiya Glushkova",
                "Duarte Alves",
                "Luisa Coheur",
                "Alon Lavie",
                "Andr\u00e9 F.T. Martins"
            ],
            "title": "CometKiwi: IST-unbabel 2022",
            "year": 2022
        },
        {
            "authors": [
                "Michael Ryan",
                "Tarek Naous",
                "Wei Xu."
            ],
            "title": "Revisiting non-English text simplification: A unified multilingual benchmark",
            "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 4898\u2013",
            "year": 2023
        },
        {
            "authors": [
                "Boaz Shmueli",
                "Jan Fell",
                "Soumya Ray",
                "Lun-Wei Ku."
            ],
            "title": "Beyond fair pay: Ethical implications of NLP crowdsourcing",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human",
            "year": 2021
        },
        {
            "authors": [
                "Advaith Siddharthan."
            ],
            "title": "Syntactic simplification and text cohesion",
            "venue": "Research on Language and Computation, 4(1):77\u2013109.",
            "year": 2006
        },
        {
            "authors": [
                "Advaith Siddharthan."
            ],
            "title": "A survey of research on text simplification",
            "venue": "ITL-International Journal of Applied Linguistics, 165(2):259\u2013298.",
            "year": 2014
        },
        {
            "authors": [
                "Sanja \u0160tajner."
            ],
            "title": "New data-driven approaches to text simplification",
            "venue": "Ph.D. thesis, University of Wolverhampton.",
            "year": 2016
        },
        {
            "authors": [
                "Sanja Stajner."
            ],
            "title": "Automatic text simplification for social good: Progress and challenges",
            "venue": "Findings of the Association for Computational Linguistics: ACLIJCNLP 2021, pages 2637\u20132652, Online. Association for Computational Linguistics.",
            "year": 2021
        },
        {
            "authors": [
                "Regina Stodden",
                "Laura Kallmeyer."
            ],
            "title": "TSANNO: An annotation tool to build, annotate and evaluate text simplification corpora",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics: System Demonstra-",
            "year": 2022
        },
        {
            "authors": [
                "Elior Sulem",
                "Omri Abend",
                "Ari Rappoport."
            ],
            "title": "BLEU is not suitable for the evaluation of text simplification",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 738\u2013744, Brussels, Belgium. Association for",
            "year": 2018
        },
        {
            "authors": [
                "Elior Sulem",
                "Omri Abend",
                "Ari Rappoport."
            ],
            "title": "Semantic structural evaluation for text simplification",
            "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
            "year": 2018
        },
        {
            "authors": [
                "Elior Sulem",
                "Omri Abend",
                "Ari Rappoport."
            ],
            "title": "Simple and effective text simplification using semantic and neural methods",
            "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 162\u2013173,",
            "year": 2018
        },
        {
            "authors": [
                "Renliang Sun",
                "Hanqi Jin",
                "Xiaojun Wan."
            ],
            "title": "Document-level text simplification: Dataset, criteria and baseline",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7997\u20138013, Online and Punta Cana,",
            "year": 2021
        },
        {
            "authors": [
                "Renliang Sun",
                "Zhe Lin",
                "Xiaojun Wan."
            ],
            "title": "On the helpfulness of document context to sentence simplification",
            "venue": "Proceedings of the 28th International Conference on Computational Linguistics, pages 1411\u2013 1423, Barcelona, Spain (Online). International Com-",
            "year": 2020
        },
        {
            "authors": [
                "Rohan Taori",
                "Ishaan Gulrajani",
                "Tianyi Zhang",
                "Yann Dubois",
                "Xuechen Li",
                "Carlos Guestrin",
                "Percy Liang",
                "Tatsunori B. Hashimoto."
            ],
            "title": "Stanford Alpaca: An instruction-following LLaMA model",
            "venue": "https:// github.com/tatsu-lab/stanford_alpaca.",
            "year": 2023
        },
        {
            "authors": [
                "Hugo Touvron",
                "Thibaut Lavril",
                "Gautier Izacard",
                "Xavier Martinet",
                "Marie-Anne Lachaux",
                "Timoth\u00e9e Lacroix",
                "Baptiste Rozi\u00e8re",
                "Naman Goyal",
                "Eric Hambro",
                "Faisal Azhar"
            ],
            "title": "LLaMA: Open and efficient foundation language models",
            "year": 2023
        },
        {
            "authors": [
                "Jan Trienes",
                "J\u00f6rg Schl\u00f6tterer",
                "Hans-Ulrich Schildhaus",
                "Christin Seifert."
            ],
            "title": "Patient-friendly clinical notes: Towards a new text simplification dataset",
            "venue": "Proceedings of the Workshop on Text Simplification, Accessibility, and Readability (TSAR-2022), pages",
            "year": 2022
        },
        {
            "authors": [
                "Yu Wan",
                "Dayiheng Liu",
                "Baosong Yang",
                "Haibo Zhang",
                "Boxing Chen",
                "Derek Wong",
                "Lidia Chao."
            ],
            "title": "UniTE: Unified translation evaluation",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Pa-",
            "year": 2022
        },
        {
            "authors": [
                "Yizhong Wang",
                "Yeganeh Kordi",
                "Swaroop Mishra",
                "Alisa Liu",
                "Noah A. Smith",
                "Daniel Khashabi",
                "Hannaneh Hajishirzi."
            ],
            "title": "Self-instruct: Aligning language models with self-generated instructions",
            "venue": "Proceedings of the 61st Annual Meeting of the Association for",
            "year": 2023
        },
        {
            "authors": [
                "Zeqiu Wu",
                "Yushi Hu",
                "Weijia Shi",
                "Nouha Dziri",
                "Alane Suhr",
                "Prithviraj Ammanabrolu",
                "Noah A Smith",
                "Mari Ostendorf",
                "Hannaneh Hajishirzi."
            ],
            "title": "Fine-grained human feedback gives better rewards for language model training",
            "venue": "arXiv preprint",
            "year": 2023
        },
        {
            "authors": [
                "Wei Xu",
                "Courtney Napoles",
                "Ellie Pavlick",
                "Quanze Chen",
                "Chris Callison-Burch."
            ],
            "title": "Optimizing statistical machine translation for text simplification",
            "venue": "Transactions of the Association for Computational Linguistics, 4:401\u2013415.",
            "year": 2016
        },
        {
            "authors": [
                "Wei Xu",
                "Alan Ritter",
                "Bill Dolan",
                "Ralph Grishman",
                "Colin Cherry."
            ],
            "title": "Paraphrasing for style",
            "venue": "Proceedings of COLING 2012, pages 2899\u20132914, Mumbai, India. The COLING 2012 Organizing Committee.",
            "year": 2012
        },
        {
            "authors": [
                "Daichi Yamaguchi",
                "Rei Miyata",
                "Sayuka Shimada",
                "Satoshi Sato."
            ],
            "title": "Gauging the gap between human and machine text simplification through analytical evaluation of simplification strategies and errors",
            "venue": "Findings of the Association for Computational Lin-",
            "year": 2023
        },
        {
            "authors": [
                "Tianyi Zhang",
                "Varsha Kishore",
                "Felix Wu",
                "Kilian Q. Weinberger",
                "Yoav Artzi."
            ],
            "title": "BERTScore: Evaluating text generation with BERT",
            "venue": "International Conference on Learning Representations.",
            "year": 2020
        },
        {
            "authors": [
                "Xingxing Zhang",
                "Mirella Lapata."
            ],
            "title": "Sentence simplification with deep reinforcement learning",
            "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 584\u2013 594, Copenhagen, Denmark. Association for Compu-",
            "year": 2017
        },
        {
            "authors": [
                "Meister"
            ],
            "title": "2020) exemplifies trivial changes should not be ignored as they may modify the information density and verbosity of a sentence. An example is famously shown by Jaeger and Levy",
            "year": 2006
        },
        {
            "authors": [
                "Basu et al",
                "Joseph"
            ],
            "title": "2023) or use case (Trienes et al., 2022). B Structural Edit Examples Examples of each structural edit sub-type are listed in Table 5. We find training annotators to label",
            "year": 2022
        },
        {
            "authors": [
                "Cardon"
            ],
            "title": "2022) reports that the number of modifications to a sentence does not correlate with input size. In Figure 13, we observe the same relationship on ASSET, however \u2013 because ASSET only represents simplifications of simpler sentences",
            "year": 2022
        },
        {
            "authors": [
                "FRANK (Pagnoni"
            ],
            "title": "2021), we asked annotators to note edits that could not be annotated, and we observe less than 0.5% of edits",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Text simplification aims to improve a text\u2019s readability or content accessibility while preserving its fundamental meaning (Stajner, 2021; Chandrasekar et al., 1996). Traditional human evaluation for text simplification often relies on individual, shallow sentence-level ratings (Sulem et al., 2018c; AlvaManchego et al., 2021), easily affected by the annotator\u2019s preference or bias. Maddela et al. (2023) recently proposes a more reliable and consistent human evaluation method by ranking and rating multiple simplifications altogether. However, as text simplification involves performing a series of transformations, or edits, such as paraphrasing, removing irrelevant details, or splitting a long sen-\ntence into multiple shorter ones (Xu et al., 2012), sentence-level scoring remains difficult to interpret since it is not reflective of detailed information about the types of edits being performed.\nFine-grained human evaluation through span selection has been explored for machine translation (Lommel et al., 2014) and open-ended text generation (Dou et al., 2022). Yet, these evaluation methods are error-driven \u2013 i.e., focusing solely on evaluating failure \u2013 which punishes creative and diverse generations with minor errors in favor of generic ones. Additionally, machine translation and open-ended generation tasks usually retain none of the input words, while text simplification must balance the editing and preservation of words in the original input (Xu et al., 2016). We thus evaluate simplification quality as the aggregation of edit successes and failures, as depicted in Figure 1.\nWe introduce SALSA \u2013 Success and FAiluredriven Linguistic Simplification Annotation \u2013 an\nedit-level human evaluation framework capturing a broad range of simplification transformations. SALSA is built on a comprehensive typology (\u00a72) containing 21 quality and error edit types. Using SALSA, we develop an interactive interface and collect 19K edit annotations of 840 simplifications written by eleven state-of-the-art language models and two humans. With these annotations, we conduct a large-scale analysis of model and automatic metric performance, and further introduce the automatic word-level quality estimation task for text simplification. Our main findings are as follows:\n\u2022 Few-shot GPT-3.5 far surpasses existing models, particularly in making syntax and content edits. However, its simplifications are not aligned to the types of operations performed by human. (\u00a74)\n\u2022 Some fine-tuned models such as the MUSS (Martin et al., 2022) produce more diverse edits than GPT-3.5, yet suffer from incredibly high errors, while others (T5, Raffel et al., 2020) learn to minimize loss by making very few changes. (\u00a74)\n\u2022 Open-source instruction fine-tuned models such as Alpaca (Taori et al., 2023) and Vicuna (Chiang et al., 2023) perform a similar number of edits as GPT-3.5, but at a cost of more conceptual errors due to the inherent limits of model imitation. (\u00a74)\n\u2022 Fine-tuned on SALSA annotations, our referencefree metric, LENS-SALSA, captures the subtleties of specific simplification approaches beyond existing automatic evaluation metrics. (\u00a75)\n\u2022 Leveraging our data, we present the automatic word-level quality estimation task for text simplification and establish several baseline approaches for future modeling efforts. (\u00a76)\nOur results demonstrate that SALSA provides an interpretable and exhaustive evaluation of text simplification."
        },
        {
            "heading": "2 SALSA Framework",
            "text": "We introduce SALSA, an edit-based human evaluation framework for text simplification. SALSA is defined by a typology of 21 linguistically-grounded edit types with the aim of capturing both successes and failures (i.e., quality changes and errors, see Figure 1). The annotation methodology of SALSA is structured as a decision tree and implemented via an easy-to-use interface, illustrated in Figure 2. Our interface is designed with Thresh (Heineman et al., 2023), and we release our configuration to encourage adaptation to other text rewriting tasks\n(Du et al., 2022) or collecting fine-grained human feedback (Wu et al., 2023)1. In the following, we describe each step of the annotation process."
        },
        {
            "heading": "2.1 Edit Selection",
            "text": "Annotation begins with edit selection, where annotators identify the edits performed by the simplification and select the corresponding spans for each edit. We define six types of edit operations: single-operation insertion, deletion, substitution, word-/clause-reorder, and multi-operation sentence split and structure changes. An insertion or deletion edit exclusively modifies content, while a substitution either modifies or paraphrases content. Reorder, split, or structure edits perform a contextfree syntax transformation. As split and structure edits are multi-operation (i.e., require a combination of single operations), they are defined by a set of underlying single-operation constituent edits. For example, this structure change from passive to active voice made by zero-shot GPT-3.5 involves multiple constituent edits:\nEXAMPLE Zero-shot GPT-3.5 On 14 November, an interview with journalist Piers Morgan was published, where Ronaldo said ... On 14 November, Piers Morgan interviewed Ronaldo, who expressed ...\n1https://thresh.tools/salsa"
        },
        {
            "heading": "2.2 Categorizing by Information Change",
            "text": "Each selected edit is then labeled with its impact on the underlying sentence information: less, same, more or different information. Given the type of operation and change to information, we subsequently organize each edit into three linguistic families as defined by Siddharthan (2014):\nLexical edits perform simple changes in \u201cwording\u201d. This includes paraphrasing (i.e., substitution that keeps the same information) and inconsequential trivial changes (e.g., inserting \u2018the\u2019).\nSyntax edits capture transformations to the distribution of information, rather than substance. A split converts a candidate sentence to two sentences, a re-order edit re-arranges clauses or wording within a clause, and a structural edit modifies the voice, tense or clausal structure. Examples of structural edit sub-types are in Appendix B.\nConceptual edits modify underlying ideas conveyed by the text. A conceptual edit requires elaboration to add clarifying information or generalization to delete unnecessary/complicated ideas."
        },
        {
            "heading": "2.3 Edit Type Classification",
            "text": "After being categorized into lexical, syntax, or conceptual edit families, we further classify each edit operation into 21 fine-grained success (quality), failure (error), or trivial edit types as listed in Figure 3. Successful edits simplify through diverse approaches, from paraphrasing complex spans, generalization of unnecessary information, or elaboration to add clarity and background context. E.g.,\nEXAMPLE (elaboration) Vicuna 7B ... can be fitted to an exponentially decaying curve.\n... can be represented by a curve that gets smaller and smaller over time.\nOften small edits, particularly to syntactic structure, can improve clarity, such as this addition of a clear subject-verb structure through the inclusion of the relative pronoun \u2018who\u2019:\nEXAMPLE (structure change) GPT-4 Paltrow in turn claims he was the one crashing rather than the other way around. Paltrow says he was the one who crashed, not her.\nOr this conversion of the participial phrase to a relative clause to help explain significance:\nEXAMPLE (structure change) ChatGPT ... for the first time since 2006, ending their 17-year playoff drought ... ... for the first time in 2006, which means they have ended their 17-year playoff drought.\nSentence splitting or reordering information may clarify a sequence of events:\nEXAMPLE (component reorder) ChatGPT Poland announces the closure of a major border crossing with Belarus \"until further notice\" amid heightened tensions between the two countries. Poland has closed a big border crossing with Belarus due to increased tensions between the two countries. The closure will remain in effect until further notice.\nFailure edits include any ablation from minor readability issues to hallucinations or deletions to sentence meaning. In the following example, the coreference error captures the deleted reference between the \u2018ICJ\u2019 and \u2018US\u2019 acronyms to their original definitions, useful contextual information:\nEXAMPLE (coreference error) ChatGPT The International Court of Justice (ICJ) rules that the United States violated its ...\nThe ICJ said that the US broke its ...\nAnd often multiple edits overlap, such as this information rewrite which successfully adds clarity via reordering, but botches the author\u2019s sarcasm:\nEXAMPLE (information rewrite) Alpaca 7B ... justifies a runtime nearing 3 hours (with a postcredits scene, no less), and it already opened to over $100 million worldwide. .. takes up almost 3 hours of the movie. The movie opened to over $100 million worldwide. A post-credits scene completes the story.\nWe also separately ask annotators to identify if the edit contains a grammar error. Appendix A provides an exhaustive description and examples for each edit type."
        },
        {
            "heading": "2.4 Rating Edit Efficacy / Severity",
            "text": "As each edit has a varying degree of impact on overall simplification quality, we finally ask annotators to rate the efficacy of quality edits or severity of error edits. We define three levels: 1 \u2013 minor, 2 \u2013 somewhat, and 3 \u2013 major. Examples of each severity level are included in Appendix A.3."
        },
        {
            "heading": "3 Data Collection",
            "text": "We describe our use of SALSA to collect 19K edit annotations covering 11.6K spans on 840 modelgenerated and human-written simplifications."
        },
        {
            "heading": "3.1 Simplification Data",
            "text": "Data collection is performed on an extended version of SIMPEVAL2022 (Maddela et al., 2023), including a train set covering state-of-the-art simplification systems and held-out test set of recent LLMs. We include a full description of each system in Appendix C.1. SALSA Train. We first extend the 360 simplifications from SIMPEVAL2022 to 700 simplifications based on 100 complex sentences from Wikipedia articles dated between Oct 2022 and Dec 2022. The complex sentences are unseen during the training of the LLMs and were selected to be intentionally difficult (avg. length of 37.3 words) to enable an evaluation of the models\u2019 full capabilities in performing diverse simplification edits. Simplifications are generated by five models including fine-tuned T5-3B and T5-11B (Raffel et al., 2020), MUSS (Martin et al., 2022), a controllable BARTlarge model trained with unsupervised, mined paraphrases, zero- and few-shot GPT-3.5 (Ouyang et al., 2022), and two human-written references. For modeling experiments in \u00a75 and \u00a76, we divide the initial 700 simplifications by the complex sentence with a 70/30% train/dev split. SALSA Test. We further gather 20 more complex sentences from Wikipedia articles published in Mar 2023 and generate 140 simplifications using recent LLMs including GPT-3.5, ChatGPT, GPT-4, Alpaca-7B (Touvron et al., 2023) and Vicuna-7B (Chiang et al., 2023), along with T5-3B and T511B fine-tuned with control tokens."
        },
        {
            "heading": "3.2 Annotation",
            "text": "As crowd-sourced annotators have shown to have inconsistent quality (Shmueli et al., 2021), we hire 6 undergraduate students from a US university. Annotators were trained with an in-depth tutorial con-"
        },
        {
            "heading": "Edit Sub-type Kripp. \u03b1 3 Agree% 2 Agree%",
            "text": "sisting of broad explanations of simplification concepts, over 100 examples covering each of the 21 SALSA edit types and interactive exercises, completed two rounds of onboarding annotations and were provided continuous feedback by the authors. To concretely measure agreement for each stage of the SALSA framework, we collect annotations in three stages: (1) we have three annotators select edits, (2) a fourth annotator adjudicates the edits into a single selection and (3) the initial three annotators classify and rate the adjudicated edits. Figure 2 illustrates our annotation interface, with further screenshots of our tutorial included in Appendix G."
        },
        {
            "heading": "3.3 Inter-Annotator Agreement",
            "text": "We calculate edit selection agreement (i.e. agreement prior to adjudication) by each token, with Table 1 reporting agreement per edit, further broken down by their type of information change. We observe edit agreement is highly dependent on the edit type and type of information change being performed. High agreements are seen for deletion (\u03b1=0.75), paraphrase (substitution with the same information, \u03b1=0.53), and sentence splits (\u03b1=0.66). Substitution that introduces more information, however, exhibits lower agreement (\u03b1=0.15), due to the subjectivity among annotators on determining whether new tokens contain \u2018novel\u2019 information, as was often mixed up with insertion. Reordering (\u03b1=0.12) and structure edits (\u03b1=0.25) also report lower agreements. We fully explore the phenomenon of annotator disagreement in Appendix C.2, and find overlapping syntactic and content edits often have multiple correct interpretations, leading to an inherent disagreement. Additionally, we find our % rates for annotator agreement are similar to fine-grained evaluation frameworks in other text generation tasks (Dou et al., 2022)."
        },
        {
            "heading": "4 Key Analysis",
            "text": "We use SALSA to evaluate state-of-the-art simplification by collecting annotations on our extended version of the SIMPEVAL corpus (Maddela et al., 2023), which includes fine-tuned, LLM- and human-written simplifications. Our resulting data collection includes 19K edit annotations across 840 simplifications.\nWe present our primary results in Figures 4, 5, and 6. Figures 4 and 5 illustrate the frequency of quality and error edit types. As edits vary in length, we calculate edit coverage: the length of each edit in proportion to the total length of the simplification and report the average edit coverage for different efficacy and severity ratings in 6, showing a view of edit ratings adjusted for length. Additionally, we include Figure 7, which compares simplifications generated by recent instruction fine-tuned language models. The following are our key findings: Models primarily write good edits, but still trail humans (Fig. 4, 5). We observe that 16% of modelgenerated edits are errors, with the best-performing model, few-shot GPT-3.5, producing errors in only 9% of edits. We find this still trails human simplifications, which have an error rate of 6%. MUSS and GPT-3.5 have a median count of 1 error per simplification and 63% of their simplifications contain at least one error, showing these errors are not concentrated in a few \u2018bad\u2019 simplifications but instead\noften occur among many good edits.\nLanguage models elaborate, while humans generalize (Fig. 4). When simplifying content, all models (excluding T5) tend to elaborate at a higher ratio than humans, for example, GPT-3.5 attempts to insert content 17% more often. As LLMs have shown to encode world knowledge in their parameters (Petroni et al., 2019; Brown et al., 2020), GPT3.5 elaboration is far more effective than MUSS, for example:\nEXAMPLE Few-shot GPT-3.5 After defeating PSD candidate Viorica Da\u0306ncila\u0306 by a landslide in 2019, his second term.. In 2019, Klaus Iohannis defeated PSD candidate Viorica Da\u0306ncila\u0306 by a large margin. His second term..\nGPT-3.5 writes quality edits at a higher frequency than humans, but human edits are longer and more effective (Fig. 4, 6). Both zeroshot and few-shot GPT-3.5 produce a larger number of edits, but human edits are more substantial, as demonstrated by the higher edit coverage across all efficacy levels, particularly for syntax and lexical edits. Human simplification typically deletes, paraphrases, or reorders entire clauses, while GPT-3.5 often edits single modifiers or words.\nFine-tuned T5-3B and T5-11B generate conservative simplifications (Fig. 4, 5, 6). Compared to all other systems, both T5 models make minimal changes in terms of frequency and edit coverage,"
        },
        {
            "heading": "Zero-shot",
            "text": "while still exhibiting high rates of error. This is likely due to their training data, Wiki-Auto (Jiang et al., 2020), containing shorter sentences, usually requiring simpler simplification techniques, making it difficult for models to generalize on longer and more complex sentences. Later in Appendix D, we show using control tokens (Martin et al., 2020) during training, as done by MUSS, can improve diversity but at the expense of increasing deletion and hallucination errors. Split edits are straightforward, Structure edits are far more complex (Fig. 4, 5). Surprisingly, sentence splitting is shown to be the easiest edit for all models to accomplish, with a similar number made by MUSS, GPT-3.5, and humans, with even the conservative T5 models making a comparable number of split edits. However, structure change and re-ordering edits are rarely seen in fine-tuned models. We speculate this may be attributed to (i) these types of edits are infrequent in the training data and (ii) GPT-3.5 has a unique ability to perform complicated syntax rewriting, echo with the findings in abstractive summarization (Goyal et al., 2022). Despite GPT-3.5\u2019s improvement, the structure error rate demonstrates it has not yet reached human-level ability. Additionally, we observe zeroshot GPT-3.5 produces structure errors (see below example) at a 19% rate higher than few-shot.\nEXAMPLE Zero-shot GPT-3.5 The sentence included a fine of $400... You will receive a fine of $400...\nWe find human simplifications are more conservative with re-ordering than models, yet attempts to simplify with re-ordering often appear arbitrary:\nEXAMPLE Human written On 3 November 2022, the British Secretary... On November 3rd, 2022, the British Secretary...\nHumans appear to produce bad deletion errors, but these are often subjective (Fig. 5). Bad dele-\ntion constitutes 35% of error edits made by humans, compared to 8% by few-shot GPT-3.5. The anomaly of the bad deletion errors reveals an inherent subjectivity in assessing deletion:\nEXAMPLE Human written Unlike the first film adaptation, in which director Samuel Fuller removed... Unlike the first film adaptation, Samuel Fuller removed...\nIn this example, some annotators marked the edit as a bad deletion while others consider it appropriate. As the sentence discusses a book adaptation into a film, the description of \u2018Samuel Fuller\u2019 is helpful depending on the reader, which underscores the need for adaptive levels of simplification to accommodate each reader\u2019s needs.\nParaphrasing is a crucial, but tricky mechanism (Fig. 4, 5). MUSS, GPT-3.5, and humans all paraphrase in at least 75% of sentences. Despite low performance in conceptual and syntactic simplification, MUSS paraphrases at a human-like rate likely due to its training on over one million paraphrase sentence pairs mined from web crawl data. Although zero-/few-shot GPT-3.5 paraphrases at a higher rate than humans, these edits are often are unnecessary. For instance:\nEXAMPLE Few-shot GPT-3.5 The club said on social media that customers subdued the gunman... The club reported on social media that customers were able...\nOpen-source LLMs are approaching GPT-3.5 simplifications, or are they (Fig. 7)? Given recent attention to ChatGPT (OpenAI, 2022), GPT-4 (OpenAI, 2023), and the emergence of instruction fine-tuning smaller language models on outputs from proprietary LLMs, we perform a supplementary evaluation on these systems. The open-source Alpaca (Taori et al., 2023) and Vicuna (Chiang et al., 2023) appear to perform a similar number of\nquality and error edits to GPT-3.5. However, these systems tend to write far more bad elaboration errors such as factual errors or contradictions:\nEXAMPLE Alpaca 7B ... a controversial \"angel tax\" provision seeking to capture some of the income entering the country from foreign investors funding India\u2019s start-ups. ... a controversial \"angel tax\" provision, which is aimed at stopping foreign investors from funneling money into India\u2019s startups.\nThis behavior suggests open-source instruction finetuned models mimic the style of their larger counterparts, but not their knowledge, a phenomenon observed by Gudibande et al. (2023). GPT-4 exhibits the best performance by making fewer content errors while producing a high number of quality edits, but still exhibits errors particularly when paraphrasing individual spans without considering the broader sentence meaning:\nEXAMPLE GPT-4 Grocery inflation in the United Kingdom reaches a record high of 17.1% ... The cost of groceries in the United Kingdom has increased to a record 17.1% ...\nWhile GPT-4 successfully paraphrases inflation by relating to cost, it fails to recognize the sentence is discussing inflation rate, rather than exact prices.\nWe include further analysis, discussion, and dataset statistics in Appendix D."
        },
        {
            "heading": "5 Evaluating Metric Edit Sensitivity",
            "text": "While automatic metrics are traditionally evaluated using correlation with sentence-level, Likert scale human ratings on dimensions of adequacy, fluency and simplicity, this fails to understand the ability of automatic metrics to capture the subtleties of lexical, syntactic, and conceptual simplification. With our SALSA annotations, we study how well current automatic metrics capture these distinct simplification approaches. Additionally, we introduce LENS-SALSA, a reference-free metric fine-tuned on SALSA annotations.\nExisting Automatic Metrics. We consider five automatic metrics: BLEU (Papineni et al., 2002), SARI (Xu et al., 2016), the most widely-used text simplification metric, BERTSCORE (Zhang et al., 2020), COMET-MQM, a machine translation metric (Rei et al., 2020) trained on MQM ratings (Freitag et al., 2021), and LENS (Maddela et al., 2023), a recently proposed text simplification metric finetuned on SIMPEVAL that contains rank-based human ratings of simplifications from 24 systems.\nLENS-SALSA. The automatic simplification metrics mentioned above require human-written references, which may not be available in every evaluation setting. To this end, we introduce LENSSALSA, a reference-free simplification metric enabled by edit-level information. Based on the COMETKIWI machine translation metric design (Rei et al., 2022), we first pre-train LENS-SALSA on the sentence-level human ratings from SIMPEVAL using UniTE (Wan et al., 2022), a multi-task learning method. Specifically, the metric is trained on the same score but from three input formats: Simp:Ref, Simp:Complex, and Simp:Complex:Ref, where \u201c:\u201d denotes concatenation. Then, we finetune LENS-SALSA on SALSA annotations using a dual-objective to predict both the sentence-level score (calculated by LENS) and a word-level quality score w\u0302i \u2208 [\u22123, 3], corresponding to the efficacy or severity rating (\u00a72.4) of each word wi in the complex and simplified sentences. We use RoBERTa-large as the base model for LENSSALSA, and 490, 210, and 140 sentence pairs for train, validation, and test, respectively. Implementation details are provided in Appendix F.2.\nResults. As fine-grained MQM annotations in machine translation are considered a gold-standard in metric evaluation (Freitag et al., 2021), we adapt their method (detailed in \u00a7A.4) to collapse edit-\nlevel ratings to a single score, and calculate subscores by only considering certain edit types. Table 2 reports the Pearson correlation between metric scores and human sub-scores across each SALSA dimension. LENS-SALSA achieves the highest correlation in nearly all edit approaches, showing its capability to capture all forms of simplification. Overall, only LENS and LENS-SALSA obtain substantial correlation with the overall human SALSA scores (0.33 and 0.45 respectively), while other metrics have spurious and even negative correlations with human judgments. Interestingly, COMET-MQM, intended for machine translation, performs better than BLEU and BERTScore, which further underlines the value of span-based ratings for trained metrics. Despite strong performance, we find LENS mainly evaluates lexical and syntactic edits, rather than conceptual ones, which may be attributed to its training data consisting of shorter, paraphrasebased simplifications. Lastly, all metrics have substantially higher correlation with quality than error edits. We posit this is primarily due to the sparsity and wide range of errors exhibited in the generations of current high-performing systems."
        },
        {
            "heading": "6 Word-Level Quality Estimation",
            "text": "Word-level quality estimation (QE) is the task of predicting the quality of each token in a generation, and has substantial downstream application to evaluating and refining text simplification. Despite word-level QE being a well understood task in machine translation (Basu et al., 2018; Zerva et al., 2022), it has not yet been studied for text simplification due to a lack of appropriately annotated data. In this section, we use SALSA annotations to demonstrate baseline approaches and highlight potential for future work. Task. We define word-level simplification QE as classifying each token in the complex and simplified sentences as quality, error, or ok. To adapt SALSA for the QE task, we label each token by the average efficacy/severity rating of its associated edit: < 0 as error, =0 as ok, and > 0 as quality. Words that are not part of any edits default to the ok label. We deconstruct split and structure edits into their constituent edits, only label the simplified spans for substitution edits, and exclude reorder edits due to their low frequency. The final label counts for our train, validation, test splits are: 6.8K/1.8K/27K, 2.7K/627/11K, and 1.7K/484/6.9K for quality/error/ok respectively.\nMethods. We propose two approaches: End-toend, where a single model labels each token directly; and Two-stage, where a word aligner first identifies edits, then the model labels each token using the identified edit information. For end-to-end, we implement the following two methods:\nTagging (Tag) is a native sequence tagging model with a classification head.\nTagging with Multi-task Loss (Tag-ML) is similar to the tagging method except trained with a multi-task loss function: L = Ltag + Lec. Lec is an additional objective that classifies each token into none, deletion, substitution, or insertion.\nFor two-stage methods, we first apply a QAbased word aligner (Nagata et al., 2020) to the sentence pair and use a set of rules to convert word alignments to edits: consecutive non-aligned words in the original sentence are labeled as a deletion edit; consecutive non-aligned words in the simplified sentence are labeled as an insertion edit; and aligned words or spans that differ are labeled as a substitution edit. Here are three two-stage methods:\nTagging with Edit Information (Tag-EI) is a sequence tagging model with a classification head that takes the concatenation of the hidden states of both edit type and token as the input. The hidden states of the edit type are obtained via a linear layer.\nEdit Classification with Separate Classifiers (EcSep) contains one classifier for each of the three edit operations. Each classifier is an encoder model with a feedforward neural network (FNN). The inputs to these FNNs are the hidden states of the [CLS] token and the max-pooled tokens from the edit spans (i.e., for substitution edit, one from the original span, and one from the simplified span).\nEdit Classification with One Classifier (Ec-One) is one classifier with three FNNs mentioned above. The difference is the encoder is trained collectively.\nAll methods (including the word aligner) use RoBERTa-large. Further implementation details and results are included in Appendix F.\nResults. Table 3 shows the test set performance for each label. Among the end-to-end methods, training with multi-task loss results in improvement on all three label F1 scores, achieving the second-best average F1 score overall. We find edit classification approaches detect error tokens more accurately than tagging approaches. Within edit classification methods, using one classifier outperforms multiple ones due to the benefit of joint encoder training. Overall, the edit classification with one classifier method performs the best with a gain of over 11 points on error F1 and a 4-point increase in average F1, compared to the base tagging model."
        },
        {
            "heading": "7 Related Work",
            "text": "Model Evaluation. Simplification work broadly agrees some typology of simplification operations exists (Siddharthan, 2014), starting with early rulebased systems which explicitly defined specific syntax operations (Dras, 1999). Past work has experimented with designing models to control the extent of each operation by using a pipeline to perform simplification operations independently (Maddela et al., 2021; Raffel et al., 2020), predicting edit operations (Dong et al., 2019) or augmenting finetuned models with learned control tokens (Martin et al., 2020, 2022). However, evaluation only considers a sentence in its entirety rather than rating individual operations, either by automatic metrics (Kriz et al., 2020), shown to be an inadequate representation of quality (Alva-Manchego et al., 2021; Sulem et al., 2018a), or by surface-level Likert ratings, typically asking crowd-sourced annotators to rate on scales of fluency, adequacy, and simplicity. These scores are difficult to interpret and capture no detail into the type of simplification being written (Briakou et al., 2021; Hashimoto et al., 2019). Additionally, despite current systems\u2019 often producing simplification errors (Choshen and Abend, 2018), annotating error has primarily been performed through inspection, and has not been incorporated into human or automatic evaluation (Gooding, 2022). Linguistic Inspection. Manual inspection attempts to understand the behavior of simplification models or datasets, characterized by detailed typologies and often conducted by authors or domain experts. Cardon et al. (2022) performs detailed inspection of the ASSET simplification test corpus (Alva-Manchego et al., 2020a) to study the behavior of automatic metrics and Cumbicus-Pineda\net al. (2021a) propose a framework for evaluating success and failure by answering a series of checklist items, with sentences given a capability score based on the number of requirements fulfilled. Yamaguchi et al. (2023) annotates simplifications of earlier models such as DRESS (Zhang and Lapata, 2017) and SUC (Sun et al., 2020) using a taxonomy of 62 error categories, but do not analyze the SOTA, MUSS, or LLMs. Stodden and Kallmeyer (2022) proposes an interactive linguistic inspection interface, but this interface is not designed for human evaluation of model outputs and does not provide ratings for measuring performance.\nFine-grained Human Evaluation. Human evaluation performed on a span-level has been previously proposed for a variety of NLP tasks. In translation, the Multidimensional Quality Metrics (MQM) (Lommel et al., 2014), categorizes error into accuracy and fluency sub-types and is later extended by Freitag et al. (2021) to weight errors by severity and combine into a single quality score. Dou et al. (2022) proposes SCARECROW to capture errors appearing in open-ended text generation. However, as these span-based evaluation schemes exclusively annotate error, they encourage generic outputs and punish interesting or diverse generations. For summarization, the FRANK typology (Pagnoni et al., 2021) aggregates errors into broader categories to benchmark metrics that measure factuality. Inspired by FRANK, Devaraj et al. (2022) introduces a framework to evaluate factuality for text simplification."
        },
        {
            "heading": "8 Conclusion",
            "text": "In this work, we introduce SALSA, a novel editbased evaluation framework incorporating error and quality evaluation, and dimensions of lexical, syntax and conceptual simplification and demonstrate SALSA benefits in granularity, accuracy, and consistency. We employ SALSA to collect a 19K edit annotation dataset and analyze the strengths and limitations of fine-tuned models, prompted LLMs, and human simplifications. Finally, we use SALSA annotations to develop a reference-free automatic metric for text simplification and demonstrate strong baselines for word-level quality estimation, showing promising avenues for the development of fine-grained human evaluation."
        },
        {
            "heading": "Limitations",
            "text": "Our annotation only represents a single use case of text simplification and we encourage an extension of SALSA to domain-specific simplification, such as medical (Joseph et al., 2023), legal (Garimella et al., 2022), or multi-lingual text (Ryan et al., 2023), and annotations by groups of specific downstream users (Stajner, 2021). The LENSSALSA reference-free metric is trained exclusively on Wikipedia simplification, and we do not consider its cross-domain generalization or its ability to capture the simplification need to specific target communities. Additionally, while we demonstrate promising results on sentence-level evaluation, simplification is often a document-level task (Laban et al., 2021; Sun et al., 2021). Incorporating higherlevel operations such as sentence fusion, paragraph compression, and reordering would require an extension to SALSA and presents unique analytical challenges. Finally, detailed human evaluation inherently requires greater resources to produce a high granularity of annotations. While we show this process can be streamlined with a robust annotator training, SALSA requires a similar amount of resources as widely used fine-grained evaluation in other tasks such as MQM (Lommel et al., 2014) or FRANK (Pagnoni et al., 2021)."
        },
        {
            "heading": "Ethics Statement",
            "text": "Our annotations were performed using the SIMPEVAL2022 corpus, originally collected from publicly available Wikipedia articles (Maddela et al., 2023) and we further extend the dataset with complex sentences collecting using the same methodology from publicly available Wikipedia articles. As discussed in \u00a73.2, we perform data collection with in-house annotators from a US university. Annotators were all native English speakers and paid $15-$18/hour. We took care to manually review all data prior to annotation as to exclude any triggering or sensitive material from our annotation data. Annotators were informed that any data they felt uncomfortable with was not required to annotate. Our interface was built using the open-source Vue.js2 library, and training of our added T5-11B system was implemented using the open-source Hugging Face Transformers3 library.\n2https://vuejs.org/ 3https://huggingface.co/"
        },
        {
            "heading": "Acknowledgements",
            "text": "We thank Tarek Naous, Nghia T. Le, Fan Bai, and Yang Chen for their helpful feedback on this work. We also thank Marcus Ma, Rachel Choi, Vishnesh J. Ramanathan, Elizabeth Liu, Govind Ramesh, Ayush Panda, Anton Lavrouk, Vinayak Athavale, and Kelly Smith for their help with human annotation. This research is supported in part by the NSF awards IIS-2144493 and IIS-2112633, ODNI and IARPA via the HIATUS program (contract 2022- 22072200004). The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of NSF, ODNI, IARPA, or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copyright annotation therein."
        },
        {
            "heading": "A Defining the SALSA Framework",
            "text": "We provide detail into the SALSA framework, including qualitative examples which helped guide design decisions when building the typology. Table 4 illustrates each final edit type, as organized by Figure 3. During development, we adjusted our scheme based on preliminary annotations with the final goal of SALSA\u2019s ability to evenly represent all modes of simplification and the full space of errors."
        },
        {
            "heading": "A.1 Quality Evaluation",
            "text": "We organize quality edits by their approach to simplification, as real-world application and models\u2019 capability to simplify falls into tiers of conceptual, syntactic and lexical simplification (Stajner, 2021). An ideal simplification system demonstrates a balance of these \u2018tiers\u2019 and incorporates different techniques depending on the original text, context and users (Gooding and Tragut, 2022). Automatic simplification research initially focused on lexical paraphrasing (Siddharthan, 2014), but has since evolved to emphasize the importance of syntactic and conceptual editing (Alva-Manchego et al., 2020b)."
        },
        {
            "heading": "A.1.1 Conceptual Simplification",
            "text": "These edits modify the underlying sentence information or ideas, a prerequisite for simplifying complex domains. We consider \u2018conceptual simplification\u2019 to be interchangeable with \u2018semantic simplification\u2019 as used in some literature (Sulem et al., 2018b; Jiang et al., 2022). Elaboration. An addition of meaningful, relevant and correct information (Siddharthan, 2006), such as clarifying vague terminology, providing background information on an entity or subject, or explicating general world knowledge unknown to the audience. Elaboration has been shown as a rare, but helpful mechanism in text generation (Cao et al., 2022) and we observe its careful use in human simplifications. Generalization. A deletion of unnecessary, irrelevant or complicated concepts. Although we ask annotators to rate the quality of elaboration by how it improves the readability of a sentence, we ask annotators to rate the quality of a generalization by the relevancy of the deleted information to the main idea of the sentence. As \u2018relevancy\u2019 is inherently subjective to the user, domain and annotator, determining the threshold for \u2018necessary information\u2019 is crucial to standardize (Devaraj et al., 2022).\nDeleting information will, by nature, contain some amount of information and SALSA instead focuses on ensuring the deleted information is not important sentence, context or users. Consider two candidate deletions:\nEXAMPLE Like so many hyped books before it, The Midnight Library excited me and gave me pause. Like so many hyped books before it, The Midnight Library excited me and gave me pause.\nAlthough the deletion of Midnight is shorter, it changed the subject of the sentence, and it is rated higher than the second deletion, which is not central to the main idea. Generalization using paraphrase is more often preferred than deleting full clauses.\nWe observe successful conceptual edits are often performed on the clause level. For example, adjunct removal via deletion:\nEXAMPLE Born into slavery in 1856, Booker T. Washington became an influential African American leader. Booker T. Washington became an influential African American leader.\nOr information insertion through an appositive or relative clause, although the prior is typically more common for the SIMPEVAL domain as it implies objective information:\nEXAMPLE \u00c9ric Gauthier is also a novella author... \u00c9ric Gauthier, famous for his soloist dancing career, is also a novella author..."
        },
        {
            "heading": "A.1.2 Syntactic Simplification",
            "text": "Syntax is a crucial mechanism for fluent, highly modified simplification (\u0160tajner, 2016). Given recent attention in automatic simplification to syntaxaware datasets and systems (Cumbicus-Pineda et al., 2021b; Kumar et al., 2020; Alva-Manchego et al., 2020a; Scarton et al., 2017), SALSA standardizes the first explicit evaluation accounting for these operations. Information Reorder. We classify two levels of reorder, word-level reorder, which reorganizes modifiers within a phrase, and component-level reorder which moves clauses or content across a sentence (Siddharthan, 2006). A component-level re-order typically may be accompanied by a broader structure change or both re-order types may overlap, as in:\nEXAMPLE The emergence of huge radio conglomerates is a direct consequence of the \u201996 Act.\nThe \u201996 Act had a direct consequence of the emergence of huge radio conglomerates.\nWhen faced with two equivalent phrases (e.g. \u2018A and B\u2019 \u2192 \u2018B and A\u2019), SALSA classifies the reordered span as the phrase more significant to the main idea of the sentence. In practice, we found this to be a helpful guideline, although annotators often simply selected the phrase appearing first in the candidate sentence.\nStructural Change. As this syntax modification necessarily includes some discourse preserving edits (Gooding, 2022), they are defined w.r.t. some combination of constituent edits (i.e. insertion,\ndeletion, substitution, reorder). Further discussion of structure changes in \u00a7B, with examples of structural change sub-types used for manual inspection in Table 5.\nSentence Split. A sub-type of a structural edit. We automatically identify split changes prior to annotation, but annotators must first select constituent spans and then associate those spans with the corresponding sentence split. We find the importance of this edit is highly domain-dependent (Figure 13)."
        },
        {
            "heading": "A.1.3 Lexical Simplification",
            "text": "Paraphrase. Swapping complex spans with equivalent, simpler alternatives, is the most primitive, yet important, approach to simplification (Qiang et al., 2020) (also referred to as a hypernym, e.g. \u0160tajner, 2016). These are exclusively defined by substitutions marked as same information and positive impact. Trivial Change. Captures any minor modifications to wording, either through a synonym replacement, or inconsequential change in wording (e.g. the, a). Trivial changes are identified as trivial insertion, trivial deletion or trivial substitution. These edits differ from a content or syntax modification in that they adds no new or major modification to the presentation of information. However, Meister et al. (2020) exemplifies trivial changes should not be ignored as they may modify the information density and verbosity of a sentence. An example is famously shown by Jaeger and Levy (2006):\nEXAMPLE How big is the family you cook for? How big is the family that you cook for?\nThe relativizer \u2018that\u2019 creates no syntactic or conceptual simplicity, but adds clarity as to the identify of the subject. Trivial changes have previously been described with finer granularity, including subcategories like abbreviation, filler words, compound segmentation, anaphora (Stodden and Kallmeyer, 2022) or even changes in number/date formatting (Cardon et al., 2022) but we exclude these groups due to their sparsity and our focus on evaluating performance."
        },
        {
            "heading": "A.2 Error Evaluation",
            "text": "We describe the SALSA error typology, with examples of each type in Table 4. Although despite their sparsity, errors have a far greater impact on fluency and adequacy than individual quality edits (Chen et al., 2023). We refined our definition of errors by focusing on minimizing the amount of error types while retaining the ability to capture the full possibility of simplification ablations. Notably, we specifically exclude a hallucination due to its ambiguous definition in related work (Ji et al., 2023), and instead define our error categories to capture any possible hallucination."
        },
        {
            "heading": "A.2.1 Conceptual Errors",
            "text": "We identify six types of errors in content, with errors primarily being related to information insertion.\nBad deletion. As the overwhelmingly most common error, a bad deletion removes necessary and relevant content to the main idea of the sentence. As discussed in \u00a7A.1.1, the threshold for \u2018relevancy\u2019 is ambiguous. Coreference. More precisely a failure in coreference or anaphora resolution (Maddela et al., 2021), this determines whether an explicit entity reference is removed. This error is only observed on a deletion of information.\nEXAMPLE Herbert Spencer\u2019s book makes the first...\nHis book makes the first. . .\nRepetition. Some trivially additional information which simply repeats knowledge already previously contained in the candidate sentence.\nEXAMPLE ... the New York City Police Department is a law enforcement agency ...\n... the New York City Police Department is a police department ...\nDespite successfully paraphrasing, police department, simply copies content from earlier in the sentence, instead of generating unique information.\nContradiction. A negation of the meaning of the original sentence. This notably includes modifying an existing phrase to contradict the original sentence:\nEXAMPLE ... the Watergate burglars were convicted ... ... the Watergate burglars were not convicted ...\nor generating new information making the sentence contradict itself:\nEXAMPLE Dextrose adds flavor and texture to dishes, although its consumption is known for negative consequences. Dextrose adds flavor, texture and nutrition to dishes, although its consumption is known for negative consequences.\nFactual Error. We asked annotators to use their commonsense knowledge and limited research to evaluate factuality in edits. Unlike contradiction, these claims introduce information which must be externally verified beyond the sentence context. Although factual content is an established focus for summarization evaluation (Pagnoni et al., 2021; Maynez et al., 2020), adequately retaining information (i.e. minimizing bad deletion) is a far greater concern for simplification (Devaraj et al., 2022).\nEXAMPLE Hilary Clinton was born in 1947.\nHilary Clinton was born in 1947 outside the United States.\nIn the context of work studying hallucination in LLMs, our contradiction and factual error categories can be interpreted as intrinsic and extrinsic hallucination respectively (Ji et al., 2023).\nIrrelevant. A sub-type of a hallucination failing to insert information related to the main idea of the sentence, recognizing the threshold for \u2018relevancy\u2019 is ambiguous (\u00a7A.1.1). For simplicity, we report irrelevancy alongside hallucination, as information insertion is generally a rare technique."
        },
        {
            "heading": "A.2.2 Syntactic Errors",
            "text": "Because syntactic edits are identified by the impact of information distribution, they do not need a finegrained error typology like conceptual edits, which make a diverse set of modifications. We simply observe each type as a failed attempted at their respective transformations.\nBad Reorder. Uses the same word-/phrase-level specification as quality reorder. We also observe that phrase-level reorder errors are almost exclusively observed to introduce a discontinuity to the syntax tree structure (Paetzold and Specia, 2013).\nBad Structure. We manually inspect structural errors according to the same sub-type specification as quality edits (\u00a7B).\nBad Sentence Split. Although sentence splitting is rarely rated as unhelpful, simplifications may unnecessarily segment ideas, or interrupt the flow of information."
        },
        {
            "heading": "A.2.3 Lexical Errors",
            "text": "Unrelated to information change, lexical errors evaluate primitive issues in fluency or wording.\nComplex Wording. An attempted paraphrase where the exact meaning is retained, but the replacement uses more complex semantics (also referred to as a hyponym, e.g. Stodden and Kallmeyer, 2022).\nEXAMPLE The researchers conducted an investigation.\nThe researchers conducted an assay.\nInformation Rewrite. Some substituted span whose content concerns the same subject, but fails to substitute the wording correctly, either through misrepresenting or falsely interpreting the information. Although similar to a combination of information deletion and information insertion, the edit is still attempting to represent the same content.\nGrammar Error. The edit violates grammatical convention. Past error analysis combines fluency and grammar into the same error type (Maddela et al., 2021) as the two are interrelated. Grammar errors are unique as they can co-occur with other errors, or occur alongside a high quality edit, as sentence fluency is independent from adequacy (Siddharthan, 2014)."
        },
        {
            "heading": "A.3 Edit Severity / Efficacy Levels",
            "text": "We provide examples of each severity level, which are also included as part of annotator training:\nEXAMPLE Severity: 1 - minor Like so many hyped books before it The Midnight Library excited me and gave me pause The Midnight Library excited me and gave me pause\nThe introductory clause \u2018Like so many hyped books before it,\u2019 situates the sentence within the context of \u2018hyped books.\u2019 However, it does not relate to the main idea of the sentence (the author\u2019s opinion on \u2018The Midnight Library\u2019).\nEXAMPLE Severity: 2 - somewhat Two security flaws, dubbed Meltdown and Spectre by researchers, were made public on 29 January 2018. Two security flaws, dubbed Meltdown and Spectre by researchers, were made public.\nAlthough the sentence retains its core meaning without \u2018on 29 January 2018\u2019, the specific reference of when \u2018Meltdown\u2019 and \u2018Spectre\u2019 were \u2018made public\u2019 is lost.\nEXAMPLE Severity: 3 - major If glycolysis evolved relatively late, it likely would not be as universal in organisms as it is. It likely would not be as universal in organisms as it is.\nSince the entity \u2018glycolysis\u2019 has been deleted, the coreference corresponding to the subject \u2018it\u2019 is lost."
        },
        {
            "heading": "A.4 Overall simplification score",
            "text": "Similar to MQM (Lommel et al., 2014), we collapse edit annotations into a simplification score to allow for direct system comparison. We calculate the sentence-level score as a weighted sum of edit ratings:\n\u2211 e\u2208E exp ( len(eC) + len(eS) len(C) + len(S) ) \u00b7 w(e) \u00b7 r(e)\nwhere S is the simplification of complex sentence C, E is the set of edits, eC and eS are the parts of edit e performed on C and S respectively, w(e) is the edit weight, r(e) is the edit rating (severity /\nefficacy), and len denotes character length.4 For weight scheme w(e), we fit a linear regression by considering the sentence-level human ratings gathered in SIMPEVAL2022 (Maddela et al., 2023) as a gold standard. As the type of simplification depends on the needs of each particular user group (Stajner, 2021), weights may be adjusted according to the simplification domain (Cemri et al., 2022; Basu et al., 2023; Joseph et al., 2023) or use case (Trienes et al., 2022)."
        },
        {
            "heading": "B Structural Edit Examples",
            "text": "Examples of each structural edit sub-type are listed in Table 5. We find training annotators to label structure change sub-type improved their ability to identify structure changes. We include morphological changes (e.g., tense change) as structure edits since these typically require multiple disconnected edits to perform and impact sentence-level meaning. Additionally, other work (Barancikova and Bojar, 2020), specifically Stodden and Kallmeyer (2022) annotate with a larger array of structural changes, notably including separate directions as distinct categories (e.g. singular \u2192 plural and plural \u2192 singular) and including change in sentiment and personal/impersonal form. We exclude these types as they almost never occur in the entirety of the ASSET corpus (Cardon et al., 2022). However,\n4We normalize the edit length and use exp to add weight for longer edits.\na case study in Italian simplification (Brunato et al., 2022) shows this structural edit distribution may vary when adapted to the needs of other languages. Similarly, German simplification often converts genitive to dative noun cases, a feature not seen in English simplification (Stodden and Kallmeyer, 2022)."
        },
        {
            "heading": "C Data Collection Details",
            "text": ""
        },
        {
            "heading": "C.1 Simplification Systems",
            "text": "Our main corpus of 700 simplifications are from the following diverse simplification approaches: MUSS (Martin et al., 2022), a BART-large model conditioned on explicit parameter tokens from Martin et al. (2020), fine-tuned on Wiki-Large (Zhang and Lapata, 2017) and mined paraphrase data. MUSS is the SOTA model before GPT-3.5. T5 (Raffel et al., 2020), an encoder-decoder transformer pre-trained on 745 GB of web text. We use T5-3B and T5-11B variants and fine-tune on the aligned Wiki-Auto dataset (Jiang et al., 2020), shown to be higher quality than Wiki-Large. GPT-3.5, a series of GPT-3 models pre-trained on text and code dated before Q4 2021. We use the best available text-davinci-003 model, based on InstructGPT (Ouyang et al., 2022), fine-tuned with human demonstrations and reinforcement learning with human feedback. We include both zeroand few-shot (5-shot) generation, using the same\nprompt setup as SIMPEVAL2022 (Maddela et al., 2023).\nHumans. We ask two in-house annotators to write simplifications for the 40 newly selected sentences, replicating instructions used in SIMPEVAL2022. We average the annotations of both human simplifications for dataset analysis.\nOur test set of 140 simplifications are from recent approaches, including open-source LLMs:\nT5 with ACCESS Tokens, we use the same training setup as our fine-tuned T5 model, but prepend the input with ACCESS control tokens (Martin et al., 2020): character length ratio, dependency tree depth ratio, character-level Levenshtein similarity, and inverse frequency ratio. During inference, we use 0.9 for the length ratio, and 0.75 for the other three control tokens, following the setup in (Maddela et al., 2023).\nAlpaca-7B (Taori et al., 2023), a fine-tuned LLaMA model (Touvron et al., 2023) on 52K GPT3.5 outputs generated using the Self-Instruct technique (Wang et al., 2023). As we find the prompt used for GPT-3.5 is too complex for Alpaca, we use the following prompt:\n\u201cRewrite the following complex sentence in order to make it easier to understand by non-native speakers of English.\u201d\nVicuna-7B (Chiang et al., 2023), a fine-tuned LLaMA model on 70K publicly shared ChatGPT conversations. As the training data for Vicuna includes prompts that are more diverse and complex than those used by Alpaca, Vicuna can manage longer prompts, but not at the level of GPT-3.5, so we use the following prompt:\n\u201cRewrite the following complex sentence in order to make it easier to understand by non-native speakers of English. The final simplified sentence needs to be grammatical, fluent, and retain the main ideas of its original counterpart without altering its meaning.\u201d\nChatGPT, an optimized chat variant of GPT-3.5, the model we use is gpt-3.5-turbo-0301. GPT-4, a large multimodal model that performs better than GPT-3.5 models. We use the version of gpt-4-0314.\nFor ChatGPT and GPT-4, we use the same prompt as GPT-3.5:\n\u201cRewrite the following complex sentence in order to make it easier to understand by non-native speakers of English. You can do so by replacing\ncomplex words with simpler synonyms (i.e. paraphrasing), deleting unimportant information (i.e. compression), and/or splitting a long complex sentence into several simpler ones. The final simplified sentence needs to be grammatical, fluent, and retain the main ideas of its original counterpart without altering its meaning.\u201d Humans. As existing automatic simplification evaluation metrics rely on human references, we include two human-written simplifications to use for metric evaluation, but do not collect annotations on these references.\nC.2 Interpreting Annotator Agreement\nAs the SIMPEVAL challenge dataset contains more edits than past simplification corpora, edit annotation becomes significantly more challenging as multiple groups of edits often overlap and simplifications contain more compression and sentencelevel transformations. Additionally, error-prone systems like MUSS make it challenging to disambiguate error and quality edits. Figure 8 illustrates an example of this disagreement, showing many of the same tokens are annotated, but with different edit spans. For example, observe the last clause in the sentence, which performs a rewrite:\nEXAMPLE that the fort stood out for its defenders\u2019 heroic resistance. and the defenders of the fort gave their lives to save the city.\nWe see three different, but valid understandings of this phrase: 1. Information was replaced - The information\nabout the defenders\u2019 resistance is inherently different then the defenders\u2019 giving their lives to save the city and is therefore an add/deletion pair.\n2. Information was retained, but paraphrased - The phrase heroic resistance being equivalent in meaning to gave their lives. 3. Subject was modified and information was replaced - The subject swap between the subject of the clause being the fort to being the defenders. The rest being an add/deletion pair.\nVarying interpretations of the same edit leads to natural disagreement. However, often a clear annotation exists and is not captured. For example, although we instructed annotators to create separate edits for overlapping syntax and conceptual edits, this occurred inconsistently in practice:\nEXAMPLE it was during the siege of the city of Elvas Don Luis de Haro attacked the city of Elvas\n1. Identified the edit as a structural change, because the noun siege was replaced with a verb, modifying the voice of the sentence 2. Identified a paraphrase, annotating siege as a more complex word than attacked 3. Correctly identified both edits occurred simultaneously\nWe find the largest source of disagreement comes from overlapping edits of multiple types, most often between structural changes and other types, because they often co-occur. Figure 9 demonstrates structural edits explain a significant portion of disagreement. Additionally, because structural edits are a composite edit, the same spans are captured by the structural edits\u2019 constituent spans and recalculating agreement using these spans, disagreement instead focuses on whether tokens are substituted.\nWithin individual sentences, we often observe multiple valid interpretations for span labeling, highlighting the inherit ambiguity in the task. Despite this, annotators still successfully communicated edit performance. All three annotators identified both the bad deletion and hallucination errors contained in the sentence. For the full SIMPEVAL dataset, we report error identification agreement in Table 6, finding syntax errors (e.g., bad structure, bad reorder) are far more difficult to identify than content or lexical errors. Particularly, complex wording and grammar errors exhibit both high fre-\nCo ref\nere nce\nCo ntr\nad ict\nion\nBa d S\nplit Irre lev an t\nBa d R\neo rde\nr\nCo mp\nlex W\nord ing\nBa d D\nele tio\nn Re pe titi on\nInf orm\nati on\nRe wr\nite\nBa d S\ntru ctu\nre\nNo Er\nror 2.0 1.5 1.0 0.5 0.0 0.5 1.0 1.5 2.0 Ou r S co re\nMUSS\nBa d S\ntru ctu\nre Ba d S plit\nRe pe\ntiti on\nBa d D\nele tio\nn\nCo mp\nlex W\nord ing Ba d R eo rde r No Er ror\nInf orm\nati on\nRe wr\nite\nT5 3B\nBa d D\nele tio\nn\nBa d S\ntru ctu\nre\nCo mp\nlex W\nord ing\nIrre lev\nan t Re pe titi on Ba d S plit\nInf orm\nati on\nRe wr\nite No Er ror\nT5 11B\nBa d S\nplit\nBa d R\neo rde\nr\nBa d S\ntru ctu\nre Irre lev an t Re pe titi on\nCo ntr\nad ict\nion\nCo mp\nlex W\nord ing\nInf orm\nati on\nRe wr\nite\nBa d D\nele tio\nn\nNo Er\nror\nZero-shot GPT-3.5\nCo ref\nere nce\nBa d S\nplit\nIrre lev\nan t\nBa d R\neo rde\nr\nCo mp\nlex W\nord ing Re pe titi on\nBa d D\nele tio\nn\nBa d S\ntru ctu\nre\nInf orm\nati on\nRe wr\nite No Er ror\nFew-shot GPT-3.5\nCo ntr\nad ict\nion\nBa d D\nele tio\nn\nBa d S\ntru ctu\nre Re pe titi on\nBa d R\neo rde\nr Ba d S plit Irre lev an t Co mp lex W ord ing No Er ror\nInf orm\nati on\nRe wr\nite\nHuman\nFigure 10: Average sentence-level score across error sentences for each system.\nquency and high agreement, as the definitions of these errors are unambiguous. Broadly, we find that high span-level agreement is not necessary for capturing overall, or even fine-grained sentence-level performance, a clear trade-off exists between the granularity of annotations and expected agreement."
        },
        {
            "heading": "D Further Analysis",
            "text": "Here, we report additional findings on the SIMPEVAL dataset and model performance, alongside observations about edit-level evaluation as a task. Figure 11 reports the average edit coverage by each edit operation and error type. We find paraphrases are typically annotated as pairs of a few words, while conceptual edits typically occur on the clause level and are annotated together. Surprisingly, structure changes often occurred as a few words:\nEXAMPLE MUSS ... Corbin has expanded his business to include agritourism, using his farm to host weddings ... ... Corbin\u2019s business also offers agritourism and he uses his farm to host weddings ...\nThe edit converts the beginning subordinate clause to a coordinate clause, yet only requires substituting a single word. Errors exhibited a significantly higher variance in size, which may be attributed to their sparsity, as no error except bad deletion occurs\nin more than 20% of outputs (Table 6). However, error sizes display the same trend as their quality counterparts, with conceptual errors typically being seen on the clause level. We also found single-word conceptual errors such as:\nEXAMPLE Zero-shot GPT-3.5 ... Arroyo released a statement that acted as an informal concession of sorts ...\n... Arroyo released a statement that was like a formal concession.\nEXAMPLE Few-shot GPT-3.5 The sentence included a fine of $400...\nThey imposed a fine of $400...\nWere less frequent than hallucinating entirely new phrasing or ideas. This may be promising for error detection as it implies error spans are often clausal and occur among many adjacent tokens. Quality and Error Are Interrelated. Figure 10 displays sentence-level scores for our error typology across systems on SIMPEVAL. We find the existence of an error to be a consistent predictor of a lower quality sentence, even in human simplifications. However, we find some errors correlate with a higher score (e.g. bad structure, information rewrite), but this may be attributed to the multiclause complex sentences in SIMPEVAL having a\na far greater number of positive edits when these corresponding errors occur. Broadly, we observe an inverse relationship between error and quality. As the error score increases (a function of the severity, frequency and size of errors), the quality must decrease.\nIncreased Edits Enables, But Does Not Guarantee Performance. Table 7 reports the mean and variance of sub-scores for the sentence-level SALSA score across each system. Edit-level scoring addresses the frequent evaluation concern that conservative systems may maximize their score by performing a minimal number of safe edits (AlvaManchego et al., 2021). The qualitatively conservative simplifications of T5 and zero-shot GPT-3.5 often score low because they fail to make many edits. SALSA distinguishes the MUSS simplifications with many successes, but more failures than other systems. We find the extent of sentence editing is not heuristic, but is a prerequisite for high\nperformance and that overall simplification performance is often determined by a small number of high-impact edits.\nSentence Length Impacts Edit Frequency. Previous linguistic annotation of the ASSET corpus (Cardon et al., 2022) reports that the number of modifications to a sentence does not correlate with input size. In Figure 13, we observe the same relationship on ASSET, however \u2013 because ASSET only represents simplifications of simpler sentences typically containing a single idea \u2013 when we extend the analysis to the more complex SIMPEVAL dataset, we see a clear relationship between the edit distance and the number of transformations in simplifications across all systems. This is also best exemplified by the split edit, which often signifies too many ideas are being contained within a single sentence. Figure 12 demonstrates the proportion of simplifications which exhibit a split across sentence lengths and edit distance. While split edits within ASSET were generally low, the much longer SIMPEVAL simplifications almost guaranteed all systems performed a sentence split. These findings highlight that performance measures should be length-agnostic, as to guarantee simplifications which simply contain more transformations due to a longer original sentence length are not arbitrarily rated as higher quality.\nComposite Edits. We report the breakdown of constituent edits in structure and split edits in Figure 15. Split edits typically need to rewrite the conjunction through inserting & deleting discourse tokens, while structure edits are typically performed some syntax transformation to the existing sentence tree, more often requiring substituted or reordered tokens.\nSALSA Test Set. Figure 16 reports the frequency of quality and error edits on the novel SALSA test set systems. While adding control tokens to T5 substantially improves the frequency of edits, we"
        },
        {
            "heading": "Zero-shot",
            "text": "find it still underperforms both MUSS and LLMs. Additionally, T5-11B makes a surprising increase in error frequency relative to the increase in the number of edits it performs relative to T5-3B. Language models demonstrate a smooth increase in\nedits, with the exception of GPT-4 making significantly less conceptual edits. Manual analysis reveals its conceptual edits are often sentence-level operations, which are not reflected in edit counts. The LLaMA-based Alpaca and Vicuna demonstrate surprisingly strong performance despite their relatively small size and training setup, even outperforming the fine-tuned simplification models.\nSALSA Dataset Statistics. We report full statistics on all 840 simplifications in Table 8. Similar to FRANK (Pagnoni et al., 2021), we asked annotators to note edits that could not be annotated, and we observe less than 0.5% of edits were not captured by one of our edit types. We consider the SALSA framework complete."
        },
        {
            "heading": "E Further Word-level QE Results",
            "text": "We include test set word-level F1 score on words in the original sentence, simplified sentence, and both sentences (same as Table 3) in Table 9. In the original sentence, only deletion edits are labeled. Thus, the performance in the original sentence column indicates the model\u2019s ability to identify quality or error deletion edits. The best-performing method, Ec-One, achieves over 50% in both quality and error F1. For the simplified sentence, which contains substitution and insertion edits, the model delivers better quality F1 but experiences a drop in error F1. This could be due to the higher proportion of error edits in deletion compared to substitution and insertion. In addition, the edit classification approach significantly improves the error F1 on the simplified sentence, compared to the tagging approaches, which reflects that tagging methods fail to capture multiple types of edits and those spanning both sentences like substitutions.\nF Implementation Details"
        },
        {
            "heading": "F.1 Generating Simplifications (\u00a73.1)",
            "text": "For all prompted models, we follow the hyperparameters of SIMPEVAL2022 (Maddela et al., 2023), using temperature=1.0 and top-p=0.95. For all T5 variants, we train them on the Wiki-Auto corpus (Jiang et al., 2020) using 8 A40 GPUs for 8 epochs with a batch size of 64. We use a learning rate of 3e-4 and AdamW (Loshchilov and Hutter, 2019) as the optimizer. For MUSS, we replicate the original setup (Martin et al., 2022). We use beam search with a beam size of 10 for these fine-tuned models."
        },
        {
            "heading": "F.2 Automatic Metrics (\u00a75)",
            "text": "Baseline Automatic Metrics. We use RoBERTa-large as the base model for BERTSCORE and the best available wmt21-comet-mqm as COMET-MQM. LENS-SALSA. Our implementation is based on\nthe reference-less COMETKIWI metric for machine translation (Rei et al., 2022). We modify their task setup of predicting binary quality labels for each output word y\u0302i \u2208 {OK, BAD} to a regression task using labels y\u0302i \u2208 [\u22123, 3], corresponding to each word rating in their SALSA annotations, as we find it performs better than using binary or three class labels in our preliminary study. Our regression task optimizes MSE loss on the word rating objective, rather than Cross Entropy Loss. The training objective can be formalized as:\nLsent(\u03b8) = 1\n2 (y \u2212 y\u0302(\u03b8))2\nLword(\u03b8) = \u2212 1\nn n\u2211 i=1 1 2 (yi \u2212 y\u0302i(\u03b8))2\nL(\u03b8) = \u03bbsLsent(\u03b8) + \u03bbwLword(\u03b8)\nBLE U SAR I BER TSC\nOR E COM ET-M QM LEN S LEN S-SA LSA Q ua lit y Lexical -0.185 0.030 0.015 0.086 0.289 0.284 Syntax -0.117 0.097 0.008 0.024 0.206 0.244 Conceptual -0.240 -0.147 -0.325 -0.187 -0.006 0.173\nE rr\nor Lexical -0.259 -0.162 -0.134 -0.004 -0.059 0.015 Syntax -0.147 -0.094 -0.136 -0.073 -0.042 -0.013 Conceptual -0.128 -0.099 -0.293 -0.169 -0.016 0.062\nA ll All Error -0.263 -0.190 -0.329 -0.170 -0.035 0.046 All Quality -0.201 0.056 -0.018 0.033 0.304 0.318 All Edits -0.286 -0.035 -0.235 -0.129 0.266 0.336\nTable 10: Pearson correlation between automatic metrics and SALSA sub-scores on the validation set, with test set performance reported in Table 2.\nwhere \u03bbs and \u03bbw weight word- and sentence-level losses. We experimented with custom weighting for edit ratings, but did not fine performance improvements. For fine-tuning, we set \u03bbw = 0.9.\nThe COMETKIWI design aggregates hidden states using a scalar mix module, and uses two feed forward networks for sentence- and wordlevel training. For pre-training, we optimize a RoBERTa-large model on the sentence-level SIMPEVAL training data used to train LENS (Maddela et al., 2023), with the training setup using only a single MSE loss to predict the sentence-level score (i.e., \u03bbs = 1, \u03bbw = 0). We follow COMETKIWI and freeze parameter updates for the RoBERTa encoder for the first epoch and use a learning rate of 1e-5 and 3e-5 for pre-training and fine-tuning respectively. We pre-train and fine-tune for 5 epochs, using the model with the highest validation set performance. We report the corresponding validation performance in Table 10."
        },
        {
            "heading": "F.3 Edit Classification (\u00a76).",
            "text": "All experiments are conducted using 2 A40 GPUs. We use the AdamW optimizer with a weight decay = 0.01, and implement our models using the Hugging Face Transformers. Learning rate are swept over 1e-5, 2e-5, 5e-5, 8e-5 for each method. Each run is trained for eight epochs with a batch of 32. This results in training times of less than five minutes per run for tagging methods and less than 20 minutes per run for the edit classification methods. We perform an evaluation of the validation set at each training step and use the model that achieved the highest validation performance on the test set.\nFor the word alignment model used in the twostage approach, we adopt the QA-based word aligner (Nagata et al., 2020), which formulates the task in a SQUAD style (Rajpurkar et al.,\n2018). We use RoBERTa-Large as the base model. We first pre-train it on monolingual word alignment datasets MultiMWA-Wiki and MultiMWANewsela from (Lan et al., 2021), and then fine-tune it on the SALSA annotations in the training set. During both pre-training and fine-tuning stages, we perform a learning rate sweep over {1e-5, 2e-5, 5e5, 8e-5} and train for 5 epochs, and save checkpoint at the end of every epoch. The highest evaluated checkpoint (pre-train for 2 epochs and fine-tune for 2 epochs) is selected for testing, achieving 81.03 F1 on the validation set.\nOn a side note, for the word that is tokenized into multiple tokens, we use its first token for prediction."
        },
        {
            "heading": "G Annotation Tutorial",
            "text": "We include screenshots to highlight the diversity of exercises and interactive elements in our detailed interface tutorial."
        }
    ],
    "title": "Dancing Between Success and Failure: Edit-level Simplification Evaluation using SALSA",
    "year": 2023
}