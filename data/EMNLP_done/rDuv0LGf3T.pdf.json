{
    "abstractText": "Multimodal Named Entity Recognition (MNER) on social media aims to enhance textual entity prediction by incorporating image-based clues. Existing studies mainly focus on maximizing the utilization of pertinent image information or incorporating external knowledge from explicit knowledge bases. However, these methods either neglect the necessity of providing the model with external knowledge, or encounter issues of high redundancy in the retrieved knowledge. In this paper, we present PGIM \u2014 a two-stage framework that aims to leverage ChatGPT as an implicit knowledge base and enable it to heuristically generate auxiliary knowledge for more efficient entity prediction. Specifically, PGIM contains a Multimodal Similar Example Awareness module that selects suitable examples from a small number of predefined artificial samples. These examples are then integrated into a formatted prompt template tailored to the MNER and guide ChatGPT to generate auxiliary refined knowledge. Finally, the acquired knowledge is integrated with the original text and fed into a downstream model for further processing. Extensive experiments show that PGIM outperforms state-of-the-art methods on two classic MNER datasets and exhibits a stronger robustness and generalization capability.1",
    "authors": [
        {
            "affiliations": [],
            "name": "Jinyuan Li"
        },
        {
            "affiliations": [],
            "name": "Han Li"
        },
        {
            "affiliations": [],
            "name": "Zhuo Pan"
        },
        {
            "affiliations": [],
            "name": "Di Sun"
        },
        {
            "affiliations": [],
            "name": "Jiahao Wang"
        },
        {
            "affiliations": [],
            "name": "Wenkun Zhang"
        },
        {
            "affiliations": [],
            "name": "Gang Pan"
        },
        {
            "affiliations": [],
            "name": "Tim Duncan"
        }
    ],
    "id": "SP:af80c7a2fb188d47fb23219b6566b09fdfc95d4e",
    "references": [
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners. Advances in neural information processing",
            "year": 2020
        },
        {
            "authors": [
                "Jason PC Chiu",
                "Eric Nichols."
            ],
            "title": "Named entity recognition with bidirectional lstm-cnns",
            "venue": "Transac-",
            "year": 2016
        },
        {
            "authors": [
                "Alexis Conneau",
                "Kartikay Khandelwal",
                "Naman Goyal",
                "Vishrav Chaudhary",
                "Guillaume Wenzek",
                "Francisco Guzm\u00e1n",
                "Edouard Grave",
                "Myle Ott",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Unsupervised cross-lingual representation learning at scale",
            "venue": "arXiv",
            "year": 2019
        },
        {
            "authors": [
                "Jia Deng",
                "Wei Dong",
                "Richard Socher",
                "Li-Jia Li",
                "Kai Li",
                "Li Fei-Fei."
            ],
            "title": "Imagenet: A large-scale hierarchical image database",
            "venue": "2009 IEEE conference on computer vision and pattern recognition, pages 248\u2013255. Ieee.",
            "year": 2009
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "arXiv preprint arXiv:1810.04805.",
            "year": 2018
        },
        {
            "authors": [
                "Zhiheng Huang",
                "Wei Xu",
                "Kai Yu."
            ],
            "title": "Bidirectional lstm-crf models for sequence tagging",
            "venue": "arXiv preprint arXiv:1508.01991.",
            "year": 2015
        },
        {
            "authors": [
                "Meihuizi Jia",
                "Lei Shen",
                "Xin Shen",
                "Lejian Liao",
                "Meng Chen",
                "Xiaodong He",
                "Zhendong Chen",
                "Jiaqi Li."
            ],
            "title": "Mner-qg: An end-to-end mrc framework for multimodal named entity recognition with query grounding",
            "venue": "arXiv preprint arXiv:2211.14739.",
            "year": 2022
        },
        {
            "authors": [
                "Jacob Devlin Ming-Wei Chang Kenton",
                "Lee Kristina Toutanova."
            ],
            "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of naacL-HLT, volume 1, page 2.",
            "year": 2019
        },
        {
            "authors": [
                "John Lafferty",
                "Andrew McCallum",
                "Fernando CN Pereira"
            ],
            "title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data",
            "year": 2001
        },
        {
            "authors": [
                "Dong-Ho Lee",
                "Akshen Kadakia",
                "Kangmin Tan",
                "Mahak Agarwal",
                "Xinyu Feng",
                "Takashi Shibuya",
                "Ryosuke Mitani",
                "Toshiyuki Sekiya",
                "Jay Pujara",
                "Xiang Ren"
            ],
            "title": "Good examples make a faster learner: Simple demonstration-based learning for low-resource ner",
            "year": 2021
        },
        {
            "authors": [
                "Junnan Li",
                "Dongxu Li",
                "Silvio Savarese",
                "Steven Hoi."
            ],
            "title": "Blip-2: Bootstrapping language-image pretraining with frozen image encoders and large language models",
            "venue": "arXiv preprint arXiv:2301.12597.",
            "year": 2023
        },
        {
            "authors": [
                "Tsung-Yi Lin",
                "Michael Maire",
                "Serge Belongie",
                "James Hays",
                "Pietro Perona",
                "Deva Ramanan",
                "Piotr Doll\u00e1r",
                "C Lawrence Zitnick."
            ],
            "title": "Microsoft coco: Common objects in context",
            "venue": "Computer Vision\u2013 ECCV 2014: 13th European Conference, Zurich,",
            "year": 2014
        },
        {
            "authors": [
                "Jiachang Liu",
                "Dinghan Shen",
                "Yizhe Zhang",
                "Bill Dolan",
                "Lawrence Carin",
                "Weizhu Chen"
            ],
            "title": "What makes good in-context examples for gpt-3? arXiv preprint arXiv:2101.06804",
            "year": 2021
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter."
            ],
            "title": "Decoupled weight decay regularization",
            "venue": "arXiv preprint arXiv:1711.05101.",
            "year": 2017
        },
        {
            "authors": [
                "Di Lu",
                "Leonardo Neves",
                "Vitor Carvalho",
                "Ning Zhang",
                "Heng Ji."
            ],
            "title": "Visual attention model for name tagging in multimodal social media",
            "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),",
            "year": 2018
        },
        {
            "authors": [
                "Seungwhan Moon",
                "Leonardo Neves",
                "Vitor Carvalho."
            ],
            "title": "Multimodal named entity recognition for short social media posts",
            "venue": "arXiv preprint arXiv:1802.07862.",
            "year": 2018
        },
        {
            "authors": [
                "Yasmin Moslem",
                "Rejwanul Haque",
                "Andy Way."
            ],
            "title": "Adaptive machine translation with large language models",
            "venue": "arXiv preprint arXiv:2301.13294.",
            "year": 2023
        },
        {
            "authors": [
                "Chengwei Qin",
                "Aston Zhang",
                "Zhuosheng Zhang",
                "Jiaao Chen",
                "Michihiro Yasunaga",
                "Diyi Yang"
            ],
            "title": "Is chatgpt a general-purpose natural language processing task solver? arXiv preprint arXiv:2302.06476",
            "year": 2023
        },
        {
            "authors": [
                "Ohad Rubin",
                "Jonathan Herzig",
                "Jonathan Berant."
            ],
            "title": "Learning to retrieve prompts for in-context learning",
            "venue": "arXiv preprint arXiv:2112.08633.",
            "year": 2021
        },
        {
            "authors": [
                "Erik F Sang",
                "Jorn Veenstra."
            ],
            "title": "Representing text chunks",
            "venue": "arXiv preprint cs/9907006.",
            "year": 1999
        },
        {
            "authors": [
                "Zhenwei Shao",
                "Zhou Yu",
                "Meng Wang",
                "Jun Yu."
            ],
            "title": "Prompting large language models with answer heuristics for knowledge-based visual question answering",
            "venue": "arXiv preprint arXiv:2303.01903.",
            "year": 2023
        },
        {
            "authors": [
                "Lin Sun",
                "Jiquan Wang",
                "Kai Zhang",
                "Yindu Su",
                "Fangsheng Weng."
            ],
            "title": "Rpbert: a text-image relation propagation-based bert model for multimodal ner",
            "venue": "Proceedings of the AAAI conference on artificial intelligence, volume 35, pages 13860\u201313868.",
            "year": 2021
        },
        {
            "authors": [
                "Hugo Touvron",
                "Thibaut Lavril",
                "Gautier Izacard",
                "Xavier Martinet",
                "Marie-Anne Lachaux",
                "Timoth\u00e9e Lacroix",
                "Baptiste Rozi\u00e8re",
                "Naman Goyal",
                "Eric Hambro",
                "Faisal Azhar"
            ],
            "title": "Llama: Open and efficient foundation language models",
            "year": 2023
        },
        {
            "authors": [
                "David Vilar",
                "Markus Freitag",
                "Colin Cherry",
                "Jiaming Luo",
                "Viresh Ratnakar",
                "George Foster."
            ],
            "title": "Prompting palm for translation: Assessing strategies and performance",
            "venue": "arXiv preprint arXiv:2211.09102.",
            "year": 2022
        },
        {
            "authors": [
                "Shuhe Wang",
                "Xiaofei Sun",
                "Xiaoya Li",
                "Rongbin Ouyang",
                "Fei Wu",
                "Tianwei Zhang",
                "Jiwei Li",
                "Guoyin Wang."
            ],
            "title": "Gpt-ner: Named entity recognition via large language models",
            "venue": "arXiv preprint arXiv:2304.10428.",
            "year": 2023
        },
        {
            "authors": [
                "Xinyi Wang",
                "Wanrong Zhu",
                "William Yang Wang."
            ],
            "title": "Large language models are implicitly topic models: Explaining and finding good demonstrations for in-context learning",
            "venue": "arXiv preprint arXiv:2301.11916.",
            "year": 2023
        },
        {
            "authors": [
                "Xinyu Wang",
                "Jiong Cai",
                "Yong Jiang",
                "Pengjun Xie",
                "Kewei Tu",
                "Wei Lu."
            ],
            "title": "Named entity and relation extraction with multi-modal retrieval",
            "venue": "arXiv preprint arXiv:2212.01612.",
            "year": 2022
        },
        {
            "authors": [
                "Xinyu Wang",
                "Min Gui",
                "Yong Jiang",
                "Zixia Jia",
                "Nguyen Bach",
                "Tao Wang",
                "Zhongqiang Huang",
                "Fei Huang",
                "Kewei Tu."
            ],
            "title": "Ita: Image-text alignments for multi-modal named entity recognition",
            "venue": "arXiv preprint arXiv:2112.06482.",
            "year": 2021
        },
        {
            "authors": [
                "Xinyu Wang",
                "Yong Jiang",
                "Nguyen Bach",
                "Tao Wang",
                "Zhongqiang Huang",
                "Fei Huang",
                "Kewei Tu."
            ],
            "title": "Improving named entity recognition by external context retrieving and cooperative learning",
            "venue": "arXiv preprint arXiv:2105.03654.",
            "year": 2021
        },
        {
            "authors": [
                "Xuwu Wang",
                "Junfeng Tian",
                "Min Gui",
                "Zhixu Li",
                "Jiabo Ye",
                "Ming Yan",
                "Yanghua Xiao."
            ],
            "title": "Promptmner: Prompt-based entity-related visual clue extraction and integration for multimodal named entity recognition",
            "venue": "Database Systems for Advanced Applica-",
            "year": 2022
        },
        {
            "authors": [
                "Xuwu Wang",
                "Jiabo Ye",
                "Zhixu Li",
                "Junfeng Tian",
                "Yong Jiang",
                "Ming Yan",
                "Ji Zhang",
                "Yanghua Xiao."
            ],
            "title": "Cat-mner: Multimodal named entity recognition with knowledge-refined cross-modal attention",
            "venue": "2022 IEEE International Conference on Multimedia and",
            "year": 2022
        },
        {
            "authors": [
                "Xiang Wei",
                "Xingyu Cui",
                "Ning Cheng",
                "Xiaobin Wang",
                "Xin Zhang",
                "Shen Huang",
                "Pengjun Xie",
                "Jinan Xu",
                "Yufeng Chen",
                "Meishan Zhang"
            ],
            "title": "Zeroshot information extraction via chatting with chatgpt",
            "venue": "arXiv preprint arXiv:2302.10205",
            "year": 2023
        },
        {
            "authors": [
                "Ikuya Yamada",
                "Akari Asai",
                "Hiroyuki Shindo",
                "Hideaki Takeda",
                "Yuji Matsumoto."
            ],
            "title": "Luke: Deep contextualized entity representations with entity-aware self-attention",
            "venue": "arXiv preprint arXiv:2010.01057.",
            "year": 2020
        },
        {
            "authors": [
                "Zhengyuan Yang",
                "Zhe Gan",
                "Jianfeng Wang",
                "Xiaowei Hu",
                "Yumao Lu",
                "Zicheng Liu",
                "Lijuan Wang."
            ],
            "title": "An empirical study of gpt-3 for few-shot knowledgebased vqa",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 3081\u2013",
            "year": 2022
        },
        {
            "authors": [
                "Jianfei Yu",
                "Jing Jiang",
                "Li Yang",
                "Rui Xia."
            ],
            "title": "Improving multimodal named entity recognition via entity span detection with unified multimodal transformer",
            "venue": "Association for Computational Linguistics.",
            "year": 2020
        },
        {
            "authors": [
                "Dong Zhang",
                "Suzhong Wei",
                "Shoushan Li",
                "Hanqian Wu",
                "Qiaoming Zhu",
                "Guodong Zhou."
            ],
            "title": "Multimodal graph fusion for named entity recognition with targeted visual guidance",
            "venue": "Proceedings of the AAAI conference on artificial intelligence, volume 35,",
            "year": 2021
        },
        {
            "authors": [
                "Qi Zhang",
                "Jinlan Fu",
                "Xiaoyu Liu",
                "Xuanjing Huang."
            ],
            "title": "Adaptive co-attention network for named entity recognition in tweets",
            "venue": "Proceedings of the AAAI conference on artificial intelligence, volume 32.",
            "year": 2018
        },
        {
            "authors": [
                "Xin Zhang",
                "Yong Jiang",
                "Xiaobin Wang",
                "Xuming Hu",
                "Yueheng Sun",
                "Pengjun Xie",
                "Meishan Zhang."
            ],
            "title": "Domain-specific ner via retrieving correlated samples",
            "venue": "arXiv preprint arXiv:2208.12995.",
            "year": 2022
        },
        {
            "authors": [
                "Fei Zhao",
                "Chunhui Li",
                "Zhen Wu",
                "Shangyu Xing",
                "Xinyu Dai."
            ],
            "title": "Learning from different text-image pairs: A relation-enhanced graph convolutional network for multimodal ner",
            "venue": "Proceedings of the 30th ACM International Conference on Multimedia, pages",
            "year": 2022
        },
        {
            "authors": [
                "Changmeng Zheng",
                "Zhiwei Wu",
                "Tao Wang",
                "Yi Cai",
                "Qing Li."
            ],
            "title": "Object-aware multimodal named entity recognition in social media posts with adversarial learning",
            "venue": "IEEE Transactions on Multimedia, 23:2520\u20132532.",
            "year": 2020
        },
        {
            "authors": [
                "Baohang Zhou",
                "Ying Zhang",
                "Kehui Song",
                "Wenya Guo",
                "Guoqing Zhao",
                "Hongbin Wang",
                "Xiaojie Yuan."
            ],
            "title": "A span-based multimodal variational autoencoder for semi-supervised multimodal named entity recognition",
            "venue": "Proceedings of the 2022 Conference",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Multimodal named entity recognition (MNER) has recently garnered significant attention (Lu et al., 2018). Users generate copious amounts of unstructured content primarily consisting of images and text on social media. The textual component in\n\u2217 corresponding authors. 1Our code is publicly available at https://github.com/\nJinYuanLi0012/PGIM\nthese posts possesses inherent characteristics associated with social media, including brevity and an informal style of writing. These unique characteristics pose challenges for traditional named entity recognition (NER) approaches (Chiu and Nichols, 2016; Devlin et al., 2018). To leverage the multimodal features and improve the NER performance, numerous previous works have attempted to align images and text implicitly using various attention mechanisms (Yu et al., 2020; Sun et al., 2021), but these Image-Text (I+T) paradigm methods have several significant limitations. Limitation 1. The feature distribution of different modalities exhibits variations, which hinders the model to learn aligned representations across diverse modalities. Limitation 2. The image feature extractors used in these methods are trained on datasets like ImageNet (Deng et al., 2009) and COCO (Lin et al., 2014), where the labels primarily consist of nouns rather than named entities. There are obvious deviations between the labels of these datasets and the named entities we aim to recognize. Given\nthese limitations, these multimodal fusion methods may not be as effective as state-of-the-art language models that solely focus on text.\nWhile MNER is a multimodal task, the contributions of image and text modalities to this task are not equivalent. When the image cannot provide more interpretation information for the text, the image information can even be discarded and ignored. In addition, recent studies (Wang et al., 2021b; Zhang et al., 2022) has shown that introducing additional document-level context on the basis of the original text can significantly improve the performance of NER models. Therefore, recent studies (Wang et al., 2021a, 2022a) aim to solve the MNER task using the Text-Text (T+T) paradigm. In these approaches, images are reasonably converted into textual representations through techniques such as image caption and optical character recognition (OCR). Apparently, the inter-text attention mechanism is more likely to outperform the cross-modal attention mechanism. However, existing second paradigm methods still exhibit certain potential deficiencies:\n(i) For the methods that solely rely on in-sample information, they often fall short in scenarios that demand additional external knowledge to enhance text comprehension.\n(ii) For those existing methods that consider introducing external knowledge, the relevant knowledge retrieved from external explicit knowledge base (e.g., Wikipedia) is too redundant. These low-relevance extended knowledge may even mislead the model\u2019s understanding of the text in some cases.\nRecently, the field of large language models (LLMs) is rapidly advancing with intriguing new findings and developments (Brown et al., 2020; Touvron et al., 2023). On the one hand, recent research on LLMs (Qin et al., 2023; Wei et al., 2023; Wang et al., 2023a) shows that the effect of the generative model in the sequence labeling task has obvious shortcomings. On the other hand, LLMs achieves promising results in various NLP (Vilar et al., 2022; Moslem et al., 2023) and multimodal tasks (Yang et al., 2022; Shao et al., 2023). These LLMs with in-context learning capability can be perceived as a comprehensive representation of internet-based knowledge and can offer highquality auxiliary knowledge typically. So we ask: Is it possible to activate the potential of ChatGPT\nin MNER task by endowing ChatGPT with reasonable heuristics?\nIn this paper, we present PGIM \u2014 a conceptually simple framework that aims to boost the performance of model by Prompting ChatGPT In MNER to generate auxiliary refined knowledge. As shown in Figure 1, the additional auxiliary refined knowledge generated in this way overcomes the limitations of (i) and (ii). We begin by manually annotating a limited set of samples. Subsequently, PGIM utilizes the Multimodal Similar Example Awareness module to select relevant instances, and seamlessly integrates them into a meticulously crafted prompt template tailored for MNER task, thereby introducing pertinent knowledge. This approach effectively harnesses the in-context few-shot learning capability of ChatGPT. Finally, the auxiliary refined knowledge generated by heuristic approach of ChatGPT is subsequently combined with the original text and fed into a downstream text model for further processing.\nPGIM outperforms all state-of-the-art models based on the Image-Text and Text-Text paradigms on two classical MNER datasets and exhibits a stronger robustness and generalization capability. Moreover, compared with some previous methods, PGIM is friendly to most researchers, its implementation requires only a single GPU and a reasonable number of ChatGPT invocations."
        },
        {
            "heading": "2 Related Work",
            "text": ""
        },
        {
            "heading": "2.1 Multimodal Named Entity Recognition",
            "text": "Considering the inherent characteristics of social media text, previous approaches (Moon et al., 2018; Zheng et al., 2020; Zhang et al., 2021; Zhou et al., 2022; Zhao et al., 2022) have endeavored to incorporate visual information into NER. They employ diverse cross-modal attention mechanisms to facilitate the interaction between text and images. Recently, Wang et al. (2021a) points out that the performance limitations of such methods are largely attributed to the disparities in distribution between different modalities. Despite Wang et al. (2022c) try to mitigate the aforementioned issues by using further refining cross-modal attention, training this end-to-end cross-modal Transformer architectures imposes significant demands on computational resources. Due to the aforementioned limitations, ITA (Wang et al., 2021a) and MoRe (Wang et al., 2022a) attempt to use a new paradigm to address MNER. ITA circumvents the challenge of multi-\nmodal alignment by forsaking the utilization of raw visual features and opting for OCR and image captioning techniques to convey image information. MoRe assists prediction by retrieving additional knowledge related to text and images from explicit knowledge base. However, none of these methods can adequately fulfill the requisite knowledge needed by the model to comprehend the text. The advancement of LLMs address the limitations identified in the aforementioned methods. While the direct prediction of named entities by LLMs in the full-shot case may not achieve comparable performance to task-specific models, we can utilize LLMs as an implicit knowledge base to heuristically generate further interpretations of text. This method is more aligned with the cognitive and reasoning processes of human."
        },
        {
            "heading": "2.2 In-context learning",
            "text": "With the development of LLMs, empirical studies have shown that these models (Brown et al., 2020) exhibit an interesting emerging behavior called In-Context Learning (ICL). Different from the paradigm of pre-training and then fine-tuning language models like BERT (Devlin et al., 2018), LLMs represented by GPT have introduced a novel in-context few-shot learning paradigm. This paradigm requires no parameter updates and can achieve excellent results with just a few examples from downstream tasks. Since the effect of ICL is strongly related to the choice of demonstration examples, recent studies have explored several effective example selection methods, e.g., similaritybased retrieval method (Liu et al., 2021; Rubin et al., 2021), validation set scores based selection (Lee et al., 2021), gradient-based method (Wang et al., 2023b). These results indicate that reasonable example selection can improve the performance of LLMs."
        },
        {
            "heading": "3 Methodology",
            "text": "PGIM is mainly divided into two stages. In the stage of generating auxiliary refined knowledge, PGIM leverages a limited set of predefined artificial samples and employs the Multimodal Similar Example Awareness (MSEA) module to carefully select relevant instances. These chosen examples are then incorporated into properly formatted prompts, thereby enhancing the heuristic guidance provided to ChatGPT for acquiring refined knowledge. (detailed in \u00a73.2). In the stage of entity prediction\nbased on auxiliary knowledge, PGIM combines the original text with the knowledge information generated by ChatGPT. This concatenated input is then fed into a transformer-based encoder to generate token representations. Finally, PGIM feeds the representations into the linear-chain Conditional Random Field (CRF) (Lafferty et al., 2001) layer to predict the probability distribution of the original text sequence (detailed in \u00a73.3). An overview of the PGIM is depicted in Figure 2."
        },
        {
            "heading": "3.1 Preliminaries",
            "text": "Before presenting the PGIM, we first formulate the MNER task, and briefly introduce the in-context learning paradigm originally developed by GPT-3 (Brown et al., 2020) and its adaptation to MNER.\nTask Formulation Consider treating the MNER task as a sequence labeling task. Given a sentence T = {t1, \u00b7 \u00b7 \u00b7 , tn} with n tokens and its corresponding image I , the goal of MNER is to locate and classify named entities mentioned in the sentence as a label sequence y = {y1, \u00b7 \u00b7 \u00b7 , yn}, where yi \u2208 Y are predefined semantic categories with the BIO2 tagging schema (Sang and Veenstra, 1999).\nIn-context learning in MNER GPT-3 and its successor ChatGPT (hereinafter referred to collectively as GPT) are autoregressive language models pretrained on a tremendous dataset. During inference, in-context few-shot learning accomplishes new downstream tasks in the manner of text sequence generation tasks on frozen GPT models. Concretely, given a test input x, its target y is predicted based on the formatted prompt p(h, C, x) as the condition, where h refers to a prompt head describing the task and in-context C = {c1, \u00b7 \u00b7 \u00b7 , cn} contains n in-context examples. All the h, C, x, y are text sequences, and target y = {y1, \u00b7 \u00b7 \u00b7 , yL} is a text sequence with the length of L. At each decoding step l, we have:\nyl = argmax yl pLLM(y l|p, y<l)\nwhere LLM represents the weights of the pretrained large language model, which are frozen for new tasks. Each in-context example ci = (xi, yi) consists of an input-target pair of the task, and these examples is constructed manually or sampled from the training set.\nAlthough the GPT-42 can accept the input of multimodal information, this function is only in the in-\n2https://openai.com/product/gpt-4\nternal testing stage and has not yet been opened for public use. In addition, compared with ChatGPT, GPT-4 has higher costs and slower API request speeds. In order to enhance the reproducibility of PGIM, we still choose ChatGPT as the main research object of our method. And this paradigm provided by PGIM can also be used in GPT-4. In order to enable ChatGPT to complete the imagetext multimodal task, we use advanced multimodal pre-training model to convert images into image captions. Inspired by PICa (Yang et al., 2022) and Prophet (Shao et al., 2023) in knowledge-based VQA, PGIM formulates the testing input x as the following template:\nText: t \\n Image: p \\n Question: q \\n Answer:\nwhere t, p and q represent specific test inputs. \\n stands for a carriage return in the template. Similarly, each in-context example ci is defined with similar templates as follows:\nText: ti\\n Image: pi\\n Question: q \\n Answer: ai\nwhere ti, pi, q and ai refer to an text-imagequestion-answer quadruple retrieved from predefined artificial samples. The complete prompt template of MNER consisting of a fixed prompt head, some in-context examples, and a test input is fed to ChatGPT for auxiliary knowledge generation."
        },
        {
            "heading": "3.2 Stage-1. Auxiliary Refined Knowledge Heuristic Generation",
            "text": "Predefined artificial samples The key to making ChatGPT performs better in MNER is to choose suitable in-context examples. Acquiring accurately annotated in-context examples that precisely reflect the annotation style of the dataset and provide a means to expand auxiliary knowledge poses a significant challenge. And directly acquiring such examples from the original dataset is not feasible.\nTo address this issue, we employ a random sampling approach to select a small subset of samples from the training set for manual annotation. Specifically, for Twitter-2017 dataset, we randomly sample 200 samples from training set for manual labeling, and for Twitter-2015 dataset, the number is 120. The annotation process comprises two main components. The first part involves identifying the named entities within the sentences, and the second part involves providing comprehensive justification by considering the image and text content, as well as relevant knowledge. For many possibilities encounter in the labeling process, what the annotator needs to do is to correctly judge and interpret the sample from the perspective of humans. For samples where image and text are related, we directly state which entities in the text are emphasized by the image. For samples where the image and text are unrelated, we directly declare that the image description is unrelated to the text. Through artifi-\ncial annotation process, we emphasize the entities and their corresponding categories within the sentences. Furthermore, we incorporate relevant auxiliary knowledge to support these judgments. This meticulous annotation process serves as a guide for ChatGPT, enabling it to generate highly relevant and valuable responses.\nMultimodel Similar Example Awareness Module Since the few-shot learning ability of GPT largely depends on the selection of in-context examples (Liu et al., 2021; Yang et al., 2022), we design a Multimodel Similar Example Awareness (MSEA) module to select appropriate in-context examples. As a classic multimodal task, the prediction of MNER relies on the integration of both textual and visual information. Accordingly, PGIM leverages the fused features of text and image as the fundamental criterion for assessing similar examples. And this multimodal fusion feature can be obtained from various previous vanilla MNER models.\nDenote the MNER dataset D and predefined artificial samples G as:\nD = {(ti, pi, yi)}Mi=1 G = {(tj , pj , yj)}Nj=1\nwhere ti, pi, yi refer to the text, image, and gold labels. The vanilla MNER model M trained on D mainly consists of a backbone encoder Mb and a CRF decoder Mc. The input multimodal imagetext pair is encoded by the encoder Mb to obtain multimodal fusion features H:\nH = Mb(t, p)\nIn previous studies, the fusion feature H after cross-attention projection into the highdimensional latent space was directly input to the decoder layer for the prediction of the result. Unlike them, PGIM chooses H as the judgment basis for similar examples. Because examples approximated in high-dimensional latent space are more likely to have the same mapping method and entity type. PGIM calculates the cosine similarity of the fused feature H between the test input and each predefined artificial sample. And top-N similar predefined artificial samples will be selected as incontext examples to enlighten ChatGPT generation auxiliary refined knowledge:\nI = argTopN j\u2208{1,2,...,N} HTHj \u2225H\u22252\u2225Hj\u22252\nI is the index set of top-N similar samples in G. The in-context examples C are defined as follows:\nC = {(tj , pj , yj) | j \u2208 I}\nIn order to efficiently realize the awareness of similar examples, all the multimodal fusion features can be calculated and stored in advance.\nHeuristics-enhanced Prompt Generation After obtaining the in-context example C, PGIM builds a complete heuristics-enhanced prompt to exploit the few-shot learning ability of ChatGPT in MNER.\nA prompt head, a set of in-context examples, and a testing input together form a complete prompt. The prompt head describes the MNER task in natural language according to the requirements. Given that the input image and text may not always have a direct correlation, PGIM encourages ChatGPT to exercise its own discretion. The incontext examples are constructed from the results C = {c1, \u00b7 \u00b7 \u00b7 , cn} of the MSEA module. For testing input, the answer slot is left blank for ChatGPT to generate. The complete format of the prompt template is shown in Appendix A.4."
        },
        {
            "heading": "3.3 Stage-2. Entity Prediction based on Auxiliary Refined Knowledge",
            "text": "Define the auxiliary knowledge generated by ChatGPT after in-context learning as Z = {z1, \u00b7 \u00b7 \u00b7 , zm}, where m is the length of Z. PGIM concatenates the original text T = {t1, \u00b7 \u00b7 \u00b7 , tn} with the obtained auxiliary refining knowledge Z as [T ;Z] and feeds it to the transformer-based encoder:\n{h1, \u00b7 \u00b7 \u00b7 , hn, \u00b7 \u00b7 \u00b7 , hn+m} = embed([T ;Z])\nDue to the attention mechanism employed by the transformer-based encoder, the token representationH = {h1, \u00b7 \u00b7 \u00b7 , hn} obtained encompasses pertinent cues from the auxiliary knowledge Z. Similar to the previous studies, PGIM feeds H to a standard linear-chain CRF layer, which defines the probability of the label sequence y given the input sentence T :\nP (y|T,Z) =\nn\u220f i=1\n\u03c8(yi\u22121, yi, hi)\u2211 y\u2032\u2208Y n\u220f i=1 \u03c8(y\u2032i\u22121, y \u2032 i, hi)\nwhere \u03c8(yi\u22121, yi, hi) and \u03c8(y\u2032i\u22121, y \u2032 i, hi) are potential functions. Finally, PGIM uses the negative\nlog-likelihood (NLL) as the loss function for the input sequence with gold labels y\u2217:\nLNLL(\u03b8) = \u2212 logP\u03b8(y\u2217|T,Z)"
        },
        {
            "heading": "4 Experiments",
            "text": ""
        },
        {
            "heading": "4.1 Settings",
            "text": "Datasets We conduct experiments on two public MNER datasets: Twitter-2015 (Zhang et al., 2018) and Twitter-2017 (Lu et al., 2018). These two classic MNER datasets contain 4000/1000/3257 and 3373/723/723 (train/development/test) image-text pairs posted by users on Twitter.\nModel Configuration PGIM chooses the backbone of UMT (Yu et al., 2020) as the vanilla MNER model to extract multimodal fusion features. This backbone completes multimodal fusion without too much modification. BLIP-2 (Li et al., 2023) as an advanced multimodal pre-trained model, is used for conversion from image to image caption. The version of ChatGPT used in experiments is gpt-3.5-turbo and sampling temperature is set to 0. For a fair comparison, PGIM chooses to use the same text encoder XLM-RoBERTalarge (Conneau et al., 2019) as ITA (Wang et al., 2021a), PromptMNER (Wang et al., 2022b), CAT-MNER (Wang et al., 2022c) and MoRe (Wang et al., 2022a).\nImplementation Details PGIM is trained by Pytorch on single NVIDIA RTX 3090 GPU. During training, we use AdamW (Loshchilov and Hutter, 2017) optimizer to minimize the loss function. We use grid search to find the learning rate for the embeddings within [1 \u00d7 10\u22126, 5 \u00d7 10\u22125]. Due to the different labeling styles of two datasets, the learning rates of Twitter-2015 and Twitter-2017 are finally set to 5\u00d7 10\u22126 and 7\u00d7 10\u22126. And we also use warmup linear scheduler to control the learning rate. The maximum length of the sentence input is set to 256, and the mini-batch size is set to 4. The model is trained for 25 epochs, and the model with the highest F1-score on the development set is selected to evaluate the performance on the test set. The number of in-context examples N in PGIM is set to 5. All of the results are averaged from 3 runs with different random seeds."
        },
        {
            "heading": "4.2 Main Results",
            "text": "We compare PGIM with previous state-of-the-art approaches on MNER in Table 1. The first group of methods includes BiLSTM-CRF (Huang et al.,\n2015), BERT-CRF (Devlin et al., 2018) as well as the span-based NER models (e.g., BERT-span, RoBERTa-span (Yamada et al., 2020)), which only consider original text. The second group of methods includes several latest multimodal approaches for MNER task: UMT (Yu et al., 2020), UMGF (Zhang et al., 2021), MNER-QG (Jia et al., 2022), R-GCN (Zhao et al., 2022), ITA (Wang et al., 2021a), PromptMNER (Wang et al., 2022b), CATMNER (Wang et al., 2022c) and MoRe (Wang et al., 2022a), which consider both text and corresponding images.\nThe experimental results demonstrate the superiority of PGIM over previous methods. PGIM surpasses the previous state-of-the-art method MoRe (Wang et al., 2022a) in terms of performance. This suggests that compared with the auxiliary knowledge retrieved by MoRe (Wang et al., 2022a) from Wikipedia, our refined auxiliary knowledge offers more substantial support. Furthermore, PGIM exhibits a more significant improvement in Twitter2017 compared with Twitter-2015. This can be attributed to the more complete and standardized labeling approach adopted in Twitter-2017, in contrast to Twitter-2015. Apparently, the quality of dataset annotation has a certain influence on the accuracy of MNER model. In cases where the dataset annotation deviates from the ground truth, accurate and refined auxiliary knowledge leads the model to prioritize predicting the truly correct entities, since the process of ChatGPT heuristically generating auxiliary knowledge is not affected by mislabeling. This phenomenon coincidentally highlights the robustness of PGIM. The ultimate objective of the MNER is to support downstream tasks effectively. Obviously, downstream tasks of MNER expect to receive MNER model outputs that are unaffected by irregular labeling in the training dataset. We further demonstrate this argument through a case study, detailed in the Appendix A.3."
        },
        {
            "heading": "4.3 Detailed Analysis",
            "text": "Impact of different text encoders on performance As shown in Table 2, We perform experiments by replacing the encoders of all XLMRoBERTalarge (Conneau et al., 2019) MNER methods with BERTbase (Kenton and Toutanova, 2019). BaselineBERT represents inputting original samples into BERT-CRF. All of the results are averaged from 3 runs with different random seeds. The marker * refers to significant test p-value <\n0.05 when comparing with ITA, CAT-MNER and MoReImage/Text. And \u2021 represents the results after we replace the text encoder in the MoRe official code with BERTbase.3 The experimental results show that PGIM achieves a greater performance improvement than the XLM-RoBERTalarge version\n3https://github.com/modelscope/AdaSeq/tree/ master/examples/MoRe\nexperiment, especially on the Twitter-2017 dataset. We think the reasons for this phenomenon are as follows: XLM-RoBERTalarge conceals the defects of previous MNER methods through its strong encoding ability, and these defects are further amplified after using BERTbase. For example, the encoding ability of BERTbase on long text is weaker than XLM-RoBERTalarge, and the additional knowledge retrieved by MoReImage/Text is much longer than PGIM. Therefore, as shown in Table 2 and Table 5, the performance loss of MoReImage/Text is larger than the performance loss of PGIM after replacing BERTbase.\nCompared with direct prediction of ChatGPT Table 3 presents the performance comparison between ChatGPT and PGIM in the few-shot scenario. VanillaGPT stands for no prompting, and PromptGPT denotes the selection of top-N similar samples for in-context learning. As shown in Appendix A.4, we employ a similar prompt template and utilize the same predefined artificial samples as in-context examples for the MSEA module to select from. But the labels of these samples are replaced by named entities in the text instead of auxiliary knowledge.\nThe BIO annotation method is not considered in this experiment because it is a little difficult for ChatGPT. Only the complete match will be considered, and only if the entity boundary and entity type are both accurately predicted, we judge it as a correct prediction.\nThe results show that the performance of ChatGPT on MNER is far from satisfactory compared with PGIM in the full-shot case, which once again confirms the previous conclusion of ChatGPT on NER (Qin et al., 2023). In other words, when we have enough training samples, only relying on ChatGPT itself will not be able to achieve the desired effect. The capability of ChatGPT shines in scenarios where sample data are scarce. Due to the in-context learning ability of ChatGPT, it can achieve significant performance improvement after learning a small number of carefully selected samples, and its performance increases linearly with the increase of the number of in-context samples. We conduct experiments to evaluate the performance of PGIM in few-shot case. For each few-shot experiment, we randomly select 3 sets of training data and train 3 times on each set to obtain the average result. The results show that after 10 prompts, ChatGPT performs better than PGIM in the fs-100 scenario on both datasets. This suggests that ChatGPT exhibits superior performance when confronted with limited training samples.\nEffectiveness of MSEA Module Table 4 demonstrates the effectiveness of the MSEA module. We use the auxiliary refined knowledge generated by ChatGPT after N in-context prompts to construct the datasets and train the model. The text encoder of the Baseline model is XLM-RoBERTalarge, and\nits input is the original text that does not contain any auxiliary knowledge. w/o MSEA represents a random choice of in-context examples. All results are averages of training results after three random initializations. Obviously, the addition of auxiliary refined knowledge can improve the effect of the model. And the addition of MSEA module can further improve the quality of the auxiliary knowledge generated by ChatGPT, which reflects the effectiveness of the MSEA module. An appropriate number of in-context examples can further improve the quality of auxiliary refined knowledge. But the number of examples is not the more the better. When ChatGPT is provided with an excessive number of examples, the quality of the auxiliary knowledge may deteriorate. One possible explanation for this phenomenon is that too many artificial examples introduce noise into the generation process of ChatGPT. As a pre-trained large language model, ChatGPT lacks genuine comprehension of the underlying logical implications in the examples. Consequently, an excessive number of examples may disrupt its original reasoning process.\nCase Study Through some case studies in Figure 3, we show how auxiliary refined knowledge can help improve the predictive performance of the model. The Baseline model represents that no auxiliary knowledge is introduced. MoReText and MoReImage denote the relevant knowledge of the input text and image retrieved using text retriever and image retriever, respectively. In PGIM, the auxiliary refined knowledge generated by ChatGPT is structured into two components: the first component provides a preliminary estimation of the named entity, and the second component offers a corresponding contextual explanation. In these examples, \"Leadership Course\", \"Big B\", \"Maxim\"\nand \"Mumbai BJP\" are all entities that were not accurately predicted by past methods. Because our auxiliary refined knowledge provides explicit explanations for such entities, PGIM makes the correct prediction."
        },
        {
            "heading": "5 Conclusion",
            "text": "In this paper, we propose a two-stage framework called PGIM and bring the potential of LLMs to MNER in a novel way. Extensive experiments show that PGIM outperforms state-of-the-art methods and considerably overcomes obvious problems in previous studies. Additionally, PGIM exhibits a strong robustness and generalization capability, and only necessitates a single GPU and a reasonable number of ChatGPT invocations. In our opinion, this is an ingenious way of introducing LLMs into MNER. We hope that PGIM will serve as a solid baseline to inspire future research on MNER and ultimately solve this task better.\nLimitations\nIn this paper, PGIM enables the integration of multimodal tasks into large language model by converting images into image captions. While PGIM achieves impressive results, we consider this TextText paradigm as a transitional phase in the development of MNER, rather than the ultimate solution. Because image captions are inherently limited in\ntheir ability to fully capture all the details of an image. This issue may potentially be further resolved in conjunction with the advancement of multimodal capabilities in language and vision models (e.g., GPT-4).\nEthics Statement\nIn this paper, we use publicly available Twitter2015, Twitter-2017 datasets for experiments. For the auxiliary refined knowledge, PGIM generates them using ChatGPT. Therefore, we trust that all data we use does not violate the privacy of any user."
        },
        {
            "heading": "Acknowledgments",
            "text": "This work was supported by the Natural Science Foundation of Tianjin (No.21JCYBJC00640) and by the 2023 CCF-Baidu Songguo Foundation (Research on Scene Text Recognition Based on PaddlePaddle)."
        },
        {
            "heading": "A Appendix",
            "text": "A.1 Generalization Analysis\nDue to the distinctive underlying logic of PGIM in incorporating auxiliary knowledge to enhance entity recognition, PGIM exhibits a stronger generalization capability that is not heavily reliant on specific datasets. Twitter-2015\u21922017 denotes the model is trained on Twitter-2015 and tested on Twitter-2017, vice versa. The results in Table 6 show that the generalization ability of PGIM is significantly improved compared with previous methods. This further validates the efficacy and superiority of our auxiliary refined knowledge in enhancing model performance.\nA.2 Comparison with MoRe\nAs the previous state-of-the-art method, MoRe retrieves relevant knowledge from Wikipedia to assist entity prediction. We experimentally compare the quality of auxiliary knowledge of MoRe and PGIM. The results are shown in Table 5. The baseline method solely relies on the original text without any incorporation of auxiliary information. MoReText and MoReImage denote the relevant knowledge of the input text and image retrieved using text retriever and image retriever, respectively. Ave.length represents the average length of the auxiliary knowledge in entire dataset. Memory indicates the GPU memory size required for training model. Ave.Improve represents the average result\nafter summing the improvement of each indicator compared with the baseline method. All models use XLM-RoBERTalarge (Conneau et al., 2019) as the text backbone with a fixed batch size of 4. The experimental results demonstrate that PGIM achieves performance improvement while requiring shorter average auxiliary knowledge length and consuming less memory. This observation highlights the lightweight nature of PGIM and further underscores the superiority of our auxiliary refined knowledge compared with auxiliary knowledge of MoRe sourced from Wikipedia.\nAdditionally, we observe that in certain cases, the introduction of auxiliary knowledge by MoRe can even lead to a deterioration in model performance. One possible explanation for this phenomenon is that the information retrieved from Wikipedia often contains redundant or irrelevant content. The first case in Figure 4 illustrates this phenomenon well. In this case, PGIM makes the correct prediction because the information re-\ntrieved by ChatGPT clearly states that \"Mumbai BJP refers to the Bharatiya Janata Party\". However, the information retrieved by MoReText from Wikipedia provides almost no assistance in recognition of named entities. MoRe alleviates this problem to some extent by introducing the Mixture of Experts (MoE) module in the post-processing stage. They fixed the parameters of MoReText and MoReImage, and trained the MoE module for 50 epochs on the basis of them. But as shown in Table 1 before, compared with MoReMoE, PGIM still shows better results without any post-processing.\nFurthermore, we also show an error prediction of PGIM in Figure 4. In this case, \"Bush\" is not a named entity that is hard to predict correctly. But since the additional knowledge retrieved by ChatGPT clearly states that \"Bush 41\" is a name of person, the prediction of PGIM is not in line with the gold label. This illustrates that the additional knowledge retrieved from ChatGPT can affect the final prediction of named entities to some extent. But the reason why MoReText can make correct prediction is obviously not related to the knowledge it retrieves from the Wikipedia, because \"Bush\" is not even mentioned in its knowledge. In fact, by using only the original text after masking the noise retrieved from the Wikipedia, the model can more easily predict correctly.\nIn summary, considering the relevance and length of retrieved information, using ChatGPT is obviously more suitable for this additional knowledge-based NER method than using Wikipedia. The information retrieved from Chat-\nGPT is generally unambiguous and directional, which causes it to significantly help predictions in most cases, and may also mislead predictions in rare cases. But the information retrieved from Wikipedia may mislead the original predictions in many cases.\nA.3 Predictions for mislabeled examples We observe that the annotation quality of the Twitter-2015 dataset is suboptimal. There have been a large number of errors and omissions in this dataset. This is the reason why the accuracy of Twitter-2015 has significantly decreased compared with Twitter-2017. However, as shown in Figure 5, since the first stage of ChatGPT heuristically generating auxiliary knowledge is not affected by mislabeling, PGIM correctly predicts those unlabeled entities. This also demonstrates the robustness of PGIM. As a future direction, we intend to reannotate the dataset to facilitate better development of the MNER task.\nA.4 Prompt template We present the template for prompting ChatGPT to generate answers. In Figure 6, PGIM guides ChatGPT for auxiliary refined knowledge generation. In-context examples and answers in the template are selected from predefined artificial samples by the MSEA module. In Figure 7, we guide ChatGPT to make direct predictions. In-context examples are selected from the same predefined artificial samples by the MSEA module. Note that the answers here are no longer human answers, but named entities in text."
        }
    ],
    "title": "Prompting ChatGPT in MNER: Enhanced Multimodal Named Entity Recognition with Auxiliary Refined Knowledge",
    "year": 2023
}