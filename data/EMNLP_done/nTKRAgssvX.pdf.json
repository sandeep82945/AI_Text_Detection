{
    "abstractText": "Temporal Knowledge Graph (TKG) reasoning, which focuses on leveraging temporal information to infer future facts in knowledge graphs, plays a vital role in knowledge graph completion. Typically, existing works for this task design graph neural networks and recurrent neural networks to respectively capture the structural and temporal information in KGs. Despite their effectiveness, in our practice, we find that they tend to suffer the issues of low training efficiency and insufficient generalization ability, which can be attributed to the over design of model architectures. To this end, this paper aims to figure out whether the current complex model architectures are necessary for temporal knowledge graph reasoning. As a result, we put forward a simple yet effective approach (termed SiMFy), which simply utilizes multilayer perceptron (MLP) to model the structural dependencies of events and adopts a fixed-frequency strategy to incorporate historical frequency during inference. Extensive experiments on realworld datasets demonstrate that our SiMFy can reach state-of-the-art performance with the following strengths: 1) faster convergence speed and better generalization ability; 2) a much smaller time consumption in the training process; and 3) better ability to capture the structural dependencies of events in KGs. These results provide evidence that the substitution of complex models with simpler counterparts is a feasible strategy.1",
    "authors": [
        {
            "affiliations": [],
            "name": "Zhengtao Liu"
        },
        {
            "affiliations": [],
            "name": "Lei Tan"
        },
        {
            "affiliations": [],
            "name": "Mengfan Li"
        },
        {
            "affiliations": [],
            "name": "Yao Wan"
        },
        {
            "affiliations": [],
            "name": "Hai Jin"
        },
        {
            "affiliations": [],
            "name": "Xuanhua Shi"
        }
    ],
    "id": "SP:e7525e123ea8b983258a2aa1ccb53cf187148d35",
    "references": [
        {
            "authors": [
                "Piero Andrea Bonatti",
                "Stefan Decker",
                "Axel Polleres",
                "Valentina Presutti."
            ],
            "title": "Knowledge graphs: New directions for knowledge representation on the semantic web (dagstuhl seminar 18371)",
            "venue": "Dagstuhl reports, volume 8. Schloss Dagstuhl-Leibniz-Zentrum",
            "year": 2019
        },
        {
            "authors": [
                "Antoine Bordes",
                "Nicolas Usunier",
                "Alberto Garc\u00edaDur\u00e1n",
                "Jason Weston",
                "Oksana Yakhnenko."
            ],
            "title": "Translating embeddings for modeling multirelational data",
            "venue": "Proceedings of the Advances in Neural Information Processing Systems 26, pages",
            "year": 2013
        },
        {
            "authors": [
                "Elizabeth Boschee",
                "Jennifer Lautenschlager",
                "Sean O\u2019Brien",
                "Steve Shellman",
                "James Starz",
                "Michael Ward"
            ],
            "title": "Icews coded event data",
            "venue": "Harvard Dataverse,",
            "year": 2015
        },
        {
            "authors": [
                "Weilin Cong",
                "Si Zhang",
                "Jian Kang",
                "Baichuan Yuan",
                "Hao Wu",
                "Xin Zhou",
                "Hanghang Tong",
                "Mehrdad Mahdavi"
            ],
            "title": "Do we really need complicated model architectures for temporal networks",
            "venue": "In Proceedings of The Eleventh International Conference on Learn-",
            "year": 2023
        },
        {
            "authors": [
                "Shib Sankar Dasgupta",
                "Swayambhu Nath Ray",
                "Partha P. Talukdar."
            ],
            "title": "Hyte: Hyperplane-based temporally aware knowledge graph embedding",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels,",
            "year": 2018
        },
        {
            "authors": [
                "Tim Dettmers",
                "Pasquale Minervini",
                "Pontus Stenetorp",
                "Sebastian Riedel."
            ],
            "title": "Convolutional 2d knowledge graph embeddings",
            "venue": "Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th Innovative Applications",
            "year": 2018
        },
        {
            "authors": [
                "Alberto Garc\u00eda-Dur\u00e1n",
                "Sebastijan Dumancic",
                "Mathias Niepert."
            ],
            "title": "Learning sequence encoders for temporal knowledge graph completion",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods",
            "year": 2018
        },
        {
            "authors": [
                "Michael Gutmann",
                "Aapo Hyv\u00e4rinen."
            ],
            "title": "Noisecontrastive estimation: A new estimation principle for unnormalized statistical models",
            "venue": "Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, pages 297\u2013304.",
            "year": 2010
        },
        {
            "authors": [
                "Zhen Han",
                "Peng Chen",
                "Yunpu Ma",
                "Volker Tresp."
            ],
            "title": "Explainable subgraph reasoning for forecasting on temporal knowledge graphs",
            "venue": "Proceedings of the 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May",
            "year": 2021
        },
        {
            "authors": [
                "Shizhu He",
                "Kang Liu",
                "Guoliang Ji",
                "Jun Zhao."
            ],
            "title": "Learning to represent knowledge graphs with gaussian embedding",
            "venue": "Proceedings of the 24th ACM International Conference on Information and Knowledge Management, CIKM 2015, Melbourne, VIC,",
            "year": 2015
        },
        {
            "authors": [
                "Yongquan He",
                "Peng Zhang",
                "Luchen Liu",
                "Qi Liang",
                "Wenyuan Zhang",
                "Chuang Zhang."
            ],
            "title": "Hip network: Historical information passing network for extrapolation reasoning on temporal knowledge graph",
            "venue": "Proceedings of the Thirtieth International",
            "year": 2021
        },
        {
            "authors": [
                "Zhiwei Hu",
                "V\u00edctor Guti\u00e9rrez-Basulto",
                "Zhiliang Xiang",
                "Xiaoli Li",
                "Ru Li",
                "Jeff Z. Pan."
            ],
            "title": "Type-aware embeddings for multi-hop reasoning over knowledge graphs",
            "venue": "Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJ-",
            "year": 2022
        },
        {
            "authors": [
                "Tingsong Jiang",
                "Tianyu Liu",
                "Tao Ge",
                "Lei Sha",
                "Baobao Chang",
                "Sujian Li",
                "Zhifang Sui."
            ],
            "title": "Towards time-aware knowledge graph completion",
            "venue": "Proceedings of the 26th International Conference on Computational Linguistics: Technical Papers, pages",
            "year": 2016
        },
        {
            "authors": [
                "Woojeong Jin",
                "Meng Qu",
                "Xisen Jin",
                "Xiang Ren."
            ],
            "title": "Recurrent event network: Autoregressive structure inference over temporal knowledge graphs",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP",
            "year": 2020
        },
        {
            "authors": [
                "Diederik P. Kingma",
                "Jimmy Ba."
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "Proceedings of the 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "Kalev Leetaru",
                "Philip A. Schrodt."
            ],
            "title": "Gdelt: Global data on events, location, and tone, 1979\u20132012",
            "venue": "Proceedings of the ISA Annual Convention, volume 2, pages 1\u201349. Citeseer.",
            "year": 2013
        },
        {
            "authors": [
                "Guo",
                "Xueqi Cheng"
            ],
            "title": "Complex evolutional",
            "year": 2022
        },
        {
            "authors": [
                "Welling"
            ],
            "title": "Modeling relational data with graph",
            "year": 2018
        },
        {
            "authors": [
                "Chao Shang",
                "Yun Tang",
                "Jing Huang",
                "Jinbo Bi",
                "Xiaodong He",
                "Bowen Zhou."
            ],
            "title": "End-to-end structureaware convolutional networks for knowledge base completion",
            "venue": "Proceedings of the Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The",
            "year": 2019
        },
        {
            "authors": [
                "Haohai Sun",
                "Shangyi Geng",
                "Jialun Zhong",
                "Han Hu",
                "Kun He."
            ],
            "title": "Graph Hawkes transformer for extrapolated reasoning on temporal knowledge graphs",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages",
            "year": 2022
        },
        {
            "authors": [
                "Haohai Sun",
                "Jialun Zhong",
                "Yunpu Ma",
                "Zhen Han",
                "Kun He."
            ],
            "title": "Timetraveler: Reinforcement learning for temporal knowledge graph forecasting",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP",
            "year": 2021
        },
        {
            "authors": [
                "Zhiqing Sun",
                "Zhi-Hong Deng",
                "Jian-Yun Nie",
                "Jian Tang."
            ],
            "title": "Rotate: Knowledge graph embedding by relational rotation in complex space",
            "venue": "Proceedings of the 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA,",
            "year": 2019
        },
        {
            "authors": [
                "Guoren Wang",
                "Yue Zeng",
                "Rong-Hua Li",
                "Hongchao Qin",
                "Xuanhua Shi",
                "Yubin Xia",
                "Xuequn Shang",
                "Liang Hong."
            ],
            "title": "Temporal graph cube",
            "venue": "IEEE Transactions on Knowledge and Data Engineering, pages 1\u201315.",
            "year": 2023
        },
        {
            "authors": [
                "Yuxiang Wang",
                "Arijit Khan",
                "Tianxing Wu",
                "Jiahui Jin",
                "Haijiang Yan."
            ],
            "title": "Semantic guided and response times bounded top-k similarity search over knowledge graphs",
            "venue": "Proceedings of the 36th IEEE International Conference on Data Engineer-",
            "year": 2020
        },
        {
            "authors": [
                "Lijie Xie",
                "Zhaoming Hu",
                "Xingjuan Cai",
                "Wensheng Zhang",
                "Jinjun Chen."
            ],
            "title": "Explainable recommendation based on knowledge graph and multiobjective optimization",
            "venue": "Complex & Intelligent Systems, 7:1241\u20131252.",
            "year": 2021
        },
        {
            "authors": [
                "Yi Xu",
                "Junjie Ou",
                "Hui Xu",
                "Luoyi Fu."
            ],
            "title": "Temporal knowledge graph reasoning with historical contrastive learning",
            "venue": "Proceedings of the ThirtySeventh AAAI Conference on Artificial Intelligence, AAAI 2023, Thirty-Fifth Conference on Innovative",
            "year": 2023
        },
        {
            "authors": [
                "Cunchao Zhu",
                "Muhao Chen",
                "Changjun Fan",
                "Guangquan Cheng",
                "Yan Zhang."
            ],
            "title": "Learning from history: Modeling temporal knowledge graphs with sequential copy-generation networks",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, vol-",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Knowledge Graphs (KGs), which represent events as triples (s, r, o), facilitate a wide range of natural language processing tasks, including semantic search (Bonatti et al., 2019; Wang et al., 2020), product recommendations (Xie et al., 2021), and question-answering systems (Saxena et al., 2020).\n1The source code and datasets are available at https:// github.com/CGCL-codes/SiMFy.\n\u2020Xuanhua Shi is the corresponding author.\nHowever, traditional KGs struggle to effectively handle facts that have temporal characteristics. Therefore, Temporal Knowledge Graphs (TKGs) have been introduced to tackle this challenge (Wang et al., 2023), which incorporate a time dimension t and store facts as quadruples (s, r, o, t), e.g., (Germany, negotiate, Russia, 2022-05).\nHowever, in real-world scenarios, TKGs are often incomplete, highlighting the vital importance of TKG reasoning, which aims to predict future facts by utilizing the temporal information. As illustrated in Figure 1, given the TKGs associated with Entity A at the timestamps of Time I, Time II, and Time III, our objective is to predict a quadruple containing an unknown entity, i.e., (A, negotiate, ?, Time IV). To achieve this goal, we can observe a significant structural dependency of facts along the timeline. That is, interactions between Entity A and Entity B at Time I could influence the interaction patterns at Time II, which, in turn, might provide clues for Time III. We also notice that historical events may recur. For instance, Entity A and Entity B had conflicts at both Time I and Time II timestamps. These observations provide us valuable insights into TKG reasoning.\nIn the TKG reasoning task, events to be predicted can typically be classified into two main types: historical events and unseen events (Han et al., 2021). Historical events, also known as\nrepetitive pattern, refer to facts that have already occurred in the historical KG sequence. Unseen events refer to events that have not occurred in the historical KG sequence. Many methods (Jin et al., 2020; Zhu et al., 2021) can predict historical events effectively by modeling the historical KG sequence auto-regressively. However, for unseen events, it is necessary to consider both the information of structural dependency and temporality of entities and relations. Typically, existing works (Liu et al., 2022; Li et al., 2022) apply Graph Neural Networks (GNNs) and Recurrent Neural Networks (RNNs) as encoders to capture the structural and temporal information in the historical KG sequence. Then, a translational model, such as ConvTransE (Shang et al., 2019), is used as a decoder to obtain predicted entities. Despite their effectiveness, these approaches are conceptually and technically complex due to their advanced model architectures. Moreover, these complex methods tend to suffer the issues of low training efficiency and insufficient generalization ability.\nIn this paper, we aim to answer the following research question: Are these complex model architectures indispensable for temporal knowledge graph reasoning? As a solution, we design a Simple MLP-Frequency-based model (SiMFy) to evaluate it against other complex baselines. Specifically, we use a one-layer MLP to jointly model entities and relations, capturing structural and temporal information in TKGs. This allows us to obtain embedding vectors for entities and relations, which are then used to calculate the similarity between queries and candidate entities, resulting in preliminary candidate entity scores. Next, we calculate the historical frequency scores of candidate entities based on their historical KG sequences. Finally, in the inference stage, the two scores mentioned above are combined using a coefficient \u03b1 to obtain the final entity scores. Through extensive experiments comparing it with existing state-of-the-art models, we find that our model can achieve comparable performance while having higher training efficiency and better generalization ability.\nFurthermore, we conduct a series of empirical studies to investigate the performance of SiMFy and existing complex models under specific conditions. We are the first to analyze the performance of MLP and GNNs in capturing unseen entities. Our findings indicate that MLP performs comparably to GNN in capturing structural dependency infor-\nmation for the TKG reasoning task. Furthermore, we investigate the question of whether historical frequency information should be incorporated into the model training process, which has not been explored before. Unlike the most current mainstream methods which directly incorporate historical frequency information of entities during training, SiMFy utilizes conceptually simple features to model the repetitive pattern of TKGs, which is fixed in training. Through empirical experiments, we validate the effectiveness of this fixed-frequency strategy adopted by SiMFy.\nThe contributions of this paper are as follows. \u2022 We design a simple MLP-based-only model,\ncalled SiMFy, which achieves state-of-theart performance on four widely-used datasets (i.e., ICEWS14, ICEWS18, ICEWS05-15, and GDELT), demonstrating the effectiveness of the simple model architecture. \u2022 We have performed extensive experiments to analyze the convergence speed, generalization ability, and training consumption of SiMFy and existing complex models. The empirical evidence demonstrates that a simple model architecture like SiMFy enjoys faster convergence, better generalization ability, and higher efficiency. \u2022 The performance of SiMFy could motivate future research to rethink the significance of the simpler model architecture and the potential value of MLP-based models in TKGs."
        },
        {
            "heading": "2 Related Work",
            "text": "Static KG Reasoning In static knowledge graphs, inference is done to deduce unknown facts from known ones. Existing approaches for inference can be categorized into embedding-based, tensor decomposition-based, and neural networkbased. Embedding-based methods, represented by the classic method TransE (Bordes et al., 2013), consider relations as translational transformations. Later models like KG2E (He et al., 2015) optimize the handling of diverse relations and improve model scalability. Tensor decomposition methods represent the graph as a tensor which is then decomposed. Models like RESCAL (Nickel et al., 2011) use this to capture interactions between entities and relations. As for the neural network models like ConvE (Dettmers et al., 2018) and KG-BART (Liu et al., 2021), ConvE enhances inference capabilities through deep feature learning and leveraging graph\nstructures, and KG-BART utilizes pretrained language models augmented with knowledge graphs to not only enhance inference but also improve the generation of commonsense-reasoned text.\nTemporal KG Reasoning Temporal knowledge graph inference, which takes into account the temporal evolution of events, generally falls into two categories: interpolation-based and extrapolationbased inference. Interpolation-based inference aims to guess unknown facts within a known time range. TTransE (Jiang et al., 2016) integrates temporal information into the TransE (Bordes et al., 2013) model using recursive neural networks, while HyTE (Dasgupta et al., 2018) designs a unique hyperplane to embed time into the entity-relation space. TeMP (Hu et al., 2022) addresses time sparsity and variability issues by combining neural message passing and temporal dynamic methods.\nOn the other hand, extrapolation-based inference, which forecasts unknown future facts, is garnering increasing attention. RE-NET (Jin et al., 2020) employs an encoder and aggregator to model past facts, while HIP (He et al., 2021) utilizes temporal, structural, and repetitive information. xERTE (Han et al., 2021) offers a novel framework for predicting future facts and CyGNet (Zhu et al., 2021) introduces a creative copy-mechanism used in natural language generation tasks before. Reinforce-\nment learning is adopted by CluSTeR (Li et al., 2021a), which infers answers from induced clues, and TimeTraveler (Sun et al., 2021), which uses historical knowledge graph snapshots for answer search. CEN (Li et al., 2022) adopts a length-aware convolutional network to model the KG sequence dynamically and GHT (Sun et al., 2022) is the first method to introduce a transformer into the TKG reasoning task. Finally, DA-Net (Liu et al., 2022) and CENET (Xu et al., 2023) propose unique event prediction models. DA-Net learns distributed attention to future events, while CENET distinguishes likely entities for a given query using a historical contrastive learning framework."
        },
        {
            "heading": "3 Problem Formulation",
            "text": ""
        },
        {
            "heading": "3.1 Temporal Knowledge Graph",
            "text": "A Temporal Knowledge Graph (TKG) G is a sequence of KGs (G0, G1, . . . ,Gt) arranged in order of their timestamp t. G = {E ,R}, where E stands for the set of entities, and R for the set of relations. Each Gt = {Et,Rt}, where Et \u2286 E and Rt \u2286 R are the sets of entities and relations at timestamp t respectively. In Gt, facts are represented as quadruples (s, r, o, t), where s, o \u2208 Et and r \u2208 Rt."
        },
        {
            "heading": "3.2 TKG Reasoning",
            "text": "TKG reasoning seeks to forecast either the subject entity s or the object entity o based on the historical\nKG sequence {G0, G1, . . . ,Gt}. When presented with a query of the form (?, r, o, t+\u2206t), the task is to identify the subject entity s, while for a query like (s, r, ?, t+\u2206t), the aim is to predict the object entity o. We use E \u2208 R|E|\u00d7d and R \u2208 R|R|\u00d7d to express the embeddings of all entities and all relations respectively. Boldfaced s, r,o are used to denote the embedding vectors of s, r, and o with a dimension of d. In our work, we specifically concentrate on the task of object entity prediction."
        },
        {
            "heading": "4 Our Approach",
            "text": "Here we elaborate our proposed Simple MLPFrequency-based model (SiMFy) for temporal graph reasoning. As illustrated in Figure 2, SiMFy mainly involves two modules: the Similarity Matching module and the Historical Frequency Learning module. These two modules generate corresponding scores for candidate entities. Afterward, a weight-based inference process is utilized to determine the final result of the reasoning. In the subsequent sections, we will provide a comprehensive introduction to our proposed method."
        },
        {
            "heading": "4.1 Similarity Matching",
            "text": "Given a query q = (s, r, ?, t + \u2206t), the Similarity Matching module is implemented by one-layerMLP to calculate the similarity between q and each candidate entity o to obtain the matching scores. Specifically, it generates a latent context vector Hs,r \u2208 R|E| for query q, which scores the similarity of different object entities with the query:\nHs,r = tanh (W [s, r] + b)E T (1)\nwhere s \u2208 E, r \u2208 R, and [s, r] denotes the concatenation of s and r. We use one-layer-MLP to aggregate the query\u2019s information. Here, W \u2208 Rd\u00d72d and b \u2208 Rd are trainable parameters. tanh is the activation function of the layer, then the layer\u2019s output is multiplied by E to obtain the final similarity vector, where each element represents the similarity score between the corresponding entity o \u2208 E and the query q. The learning objective of the similarity matching is to minimize the NCE loss (Gutmann and Hyv\u00e4rinen, 2010) L, as described below:\nL = \u2212 \u2211 q log exp (Hs,r (oi))\u2211 oj\u2208E exp (Hs,r (oj)) (2)\nwhere oi is the ground truth object entity corresponding to the given query q. Finally, we obtain\nthe following similarity score by utilizing the softmax function:\nS (s,r) sim = softmax(Hs,r) (3)"
        },
        {
            "heading": "4.2 Historical Frequency Learning",
            "text": "Given a query q = (s, r, ?, t+\u2206t) and the historical KG sequence {G0, G1, . . . ,Gt}, the Historical Frequency Learning module aims to obtain the historical frequency scores. Specifically, for every timestamp t\u2032 \u2264 t, we first investigate the frequencies of historical entities f (s,r)t\u2032 \u2208 R |E| , as follows:\nf (s,r) t\u2032 (o) = \u2211 x\u2208Gt\u2032 I[x = (s, r, o, t\u2032)] (4)\nwhere I[\u00b7] is an indicator function, yielding 1 if [\u00b7] is true and 0 otherwise. Then we add up the frequency information of all timestamps t\u2032 \u2264 t to obtain the historical frequency information of the query as follows:\nF (s,r) t+\u2206t = f (s,r) 0 + f (s,r) 1 + \u00b7 \u00b7 \u00b7+ f (s,r) t (5)\nwhere F(s,r)t+\u2206t \u2208 R|E| is an |E|-dimensional vector where each element represents the corresponding historical frequency of the candidate object entities. Finally, we obtain the following historical frequency score by utilizing the softmax function:\nS (s,r) freq = softmax(k \u00b7 F (s,r) t+\u2206t) (6)\nwhere k is a hyperparameter to balance the extremely small values."
        },
        {
            "heading": "4.3 Inference",
            "text": "In the inference stage, a coefficient \u03b1 \u2208 [0, 1] is integrated to balance the weight between the similarity score S(s,r)sim and the historical frequency score S (s,r) freq . These two scores are merged to determine the final probability distribution of candidate entities, then the model chooses the object with the highest probability as the final prediction, as defined below:\nP(o|s, r, t+\u2206t) = \u03b1\u00b7S(s,r)sim (o)+(1\u2212\u03b1)\u00b7S (s,r) freq (o)\not+\u2206t = argmaxo\u2208EP(o|s, r, t+\u2206t) (7)\nwhere P(o|s, r, t+\u2206t) is an |E|-dimensional vector which stores the final probability of all entities."
        },
        {
            "heading": "5 Experiments",
            "text": ""
        },
        {
            "heading": "5.1 Experimental Setup",
            "text": "Dataset During the evaluation, we adopt four publicly available TKG datasets: ICEWS14 (Li et al., 2021b), ICEWS05-15 (Garc\u00eda-Dur\u00e1n et al., 2018), ICEWS18 (Jin et al., 2020), and GDELT (Leetaru and Schrodt, 2013). The first three datasets which originate from the Integrated Crisis Early Warning System (Boschee et al., 2015) (ICEWS) comprise a diverse range of political facts accompanied by time annotations, such as (European Union, Praise or endorse, Kosovo, 2018/09/29). Global Database of Events, Language, and Tone (GDELT) is a much larger dataset that records data every 15 minutes. Following the dataset split strategy proposed by (Jin et al., 2020), we divide these datasets into training, validation, and test sets, adhering to an 80%, 10%, and 10% proportion by timestamps.\nEvaluation Metrics We evaluate our approach to the task of entity prediction, where the objective is to predict the absent object entity for a given entity-relation pair, assessing whether the ground truth entity ranks higher than other entities. We present the results in terms of Hits@1/3/10 and Mean Reciprocal Rank (MRR).\nDuring the evaluation stage, two settings are commonly employed: filtered setting and raw setting. In terms of the filtered setting, for each query, we treat all triples absent from the training, validation, and test sets to be negative samples. This implies that, when computing rankings, we disregard all triples known to be true.\nIn contrast, the raw setting does not involve fil-\ntering any triples, meaning that all possible triples are considered when calculating rankings. In the context of TKG reasoning task, each quadruple (s, r, o, t) is unique due to the inclusion of timestamp information. This suggests that the filtered setting holds little practical relevance when evaluating model performance. Therefore, we adopt the raw setting for our experiments.\nBaselines We assess how well our SiMFy model performs in comparison to three other types of previously proposed models: (1) Static reasoning methods, including RotatE (Sun et al., 2019), ConvTransE (Shang et al., 2019), ConvE (Dettmers et al., 2018), and R-GCN (Schlichtkrull et al., 2018). (2) Dynamic interpolated reasoning methods, including TTransE (Jiang et al., 2016), HyTE (Dasgupta et al., 2018), and TA-DistMult (Garc\u00eda-Dur\u00e1n et al., 2018). (3) Dynamic extrapolated reasoning methods, including xERTE (Han et al., 2021), RE-NET (Jin et al., 2020), CyGNet (Zhu et al., 2021), CENET (Xu et al., 2023), GHT (Sun et al., 2022), CEN (Li et al., 2022) and RE-GCN (Li et al., 2021b)."
        },
        {
            "heading": "5.2 Implementation Details",
            "text": "For the baselines of CyGNet, CENET, CEN, and RE-GCN, we rerun these models using their opensource code on four datasets with their default parameter settings. Since the code of GHT is not open-source, we provide the findings from their publication. It should be noted that we utilize the offline version of CEN to ensure fairness. Some results of static and dynamic interpolated reasoning approaches are adopted from (Li et al., 2021b).\nWe implement our SiMFy model using PyTorch.\nThe dimension of entity and relation embeddings is set to 200. We use Adam (Kingma and Ba, 2015) as the optimizer, with a learning rate of 0.001 and a weight decay of 0.00001. The hyperparameter k is set to 2 and the \u03b1 is set to 0.001. The batch size is set to 1024 and the training epoch is limited to 30. All experiments were conducted on a Tesla V100."
        },
        {
            "heading": "5.3 Model Results",
            "text": "Table 1 shows the experimental results of SiMFy compared with other baselines on four datasets. It is clear that SiMFy performs better than other baselines in most cases. For static reasoning methods, their results are very poor because they do not consider temporal information. Dynamic interpolated reasoning methods like TTransE only encode time information without considering the evolution of temporal KG sequences, so they cannot achieve good results. It is worth noting that for GCN-based models CEN and RE-GCN, as well as the models most pertinent to our model, RENET, CyGNet, and CENET, SiMFy outperforms them. This demonstrates the effectiveness of simple models based on MLPs and shows that MLPs can to some extent replace GCNs. Please refer to Section 6.3 for a more in-depth analysis."
        },
        {
            "heading": "5.4 Ablation Study",
            "text": "We perform ablation experiments on the ICEWS14 dataset to better understand the impact of each SiMFy module, and the results are displayed in Table 2. SiMFy w.o. HF refers to SiMFy without the Historical Frequency Learning module, while SiMFy w.o. SM represents SiMFy without the Similarity Matching module. It can be observed that SiMFy w.o. HF performs better than SiMFy w.o. SM. This is because the Similarity Matching module captures many unseen events in the TKGs, which also demonstrates the ability of MLP in understanding the structural dependencies of events. Therefore, SiMFy w.o. SM, which only considers historical repetitive patterns, results in a significant drop of 21.1% and 25.1% in terms of MRR and Hits@10 respectively compared to SiMFy. Despite\nhaving some capability to handle historical events, SiMFy w.o. HF also experiences a drop of 9% in performance, which further confirms the effectiveness of the Historical Frequency Learning module."
        },
        {
            "heading": "5.5 Case Study",
            "text": "To further illustrate 1) SiMFy\u2019s ability to predict unseen entities by capturing the structural dependency information of KGs, and 2) SiMFy\u2019s ability to predict repetitive events using historical frequency, we present two cases in the ICEWS18 dataset. \u2022 In the first case, the query is (Dharamvira\nGandhi, Criticize or denounce, ?, t), and the correct objective entity is Government (India). However, upon investigating the historical KG sequence of this query, we find that the triple (Dharamvira Gandhi, Criticize or denounce, Government (India)) has not occurred before. This indicates that there is no repetitive pattern for the quadruple (Dharamvira Gandhi, Criticize or denounce, Government (India), t), making it an unseen event with Government (India) as the corresponding unseen entity. Nevertheless, due to SiMFy\u2019s extraction of structural dependency information from the historical KG sequence, we find that the candidate entity Government (India) appears as the top first in the ranking list generated by the model. This indicates that SiMFy has successfully captured the correct unseen entity, despite its absence in the historical KGs. \u2022 In the second case, the query is (Zdravko Maric, Make statement, ?, t), and the correct objective entity is Government (Croatia). By analyzing the historical KG sequence of this query, we find that the triple (Zdravko Maric, Make statement, Government (Croatia)) has the highest frequency of occurrence, significantly surpassing other cases. SiMFy, utilizing the learned historical frequency information, recognizes this repetitive pattern and memorizes it. As a result, in the final ranking list generated by the model, the correct answer Government (Croatia) occupies the top position. This aligns with our real-world understanding that Zdravko Maric served as the Minister of Finance in the Croatian government, making statements on behalf of the government frequently."
        },
        {
            "heading": "6 More Empirical Results and Analysis",
            "text": "Here, we show more empirical results and analysis to reveal the strengths of our proposed SiMFy."
        },
        {
            "heading": "6.1 Convergence Speed and Generalization",
            "text": "To gain a deeper understanding of the model\u2019s performance, we will closely examine the dynamics of both training MRR and evaluation MRR. Following (Cong et al., 2023), we will also discuss the important generalization gap (the absolute difference between the MRR scores obtained during training and evaluation) of different models. In addition to SiMFy, we also use four existing complex baseline models, including CyGNet, RE-GCN, CENET, and CEN, which are most relevant to our work, as experimental comparisons. Moreover, based on RGCN, we also reconstruct two new GCN-based models as our comparative baselines. The first one is RGCN+ConvTransE, where ConvTransE (Shang et al., 2019) is used as the decoder. The second one is RGCN+MLP, using MLP as the decoder. Figure 4 illustrates the efficacy of the models on the most representative ICEWS05-15 dataset, from which the following conclusions can be drawn: \u2022 As the slope change of the training MRR curve\ncan reflect the convergence speed of the model (i.e., a slope close to 0 indicates model convergence), we can see that our model always converges after several epochs, indicating a very fast convergence speed. \u2022 After each epoch, we save separate models and\nevaluate their performance on the test set. Figure 4b illustrates that our model not only achieves the highest MRR but also maintains stability and smoothness in the curve after several epochs. In contrast, the MRR curves of other baselines show varying degrees of oscillation. \u2022 The generalization ability of a model refers to its performance on unseen data, that is, the adaptability of the model to new data. Therefore, the smaller the generalization gap, the better the model\u2019s generalization ability. As shown in Figure 4c, our model has the smallest generalization gap, showing its strong generalization ability. \u2022 Combining these three figures, we can also observe an interesting phenomenon. Models based on GCN, represented by CEN (Li et al., 2022), even if their performance on the test set fails to enhance after numerous epochs, their MRR on the training set keeps rising. This means that these GCN-based models have overfitting problems, which is also reflected in the increasing generalization gap."
        },
        {
            "heading": "6.2 Low Time Consumption",
            "text": "Compared to other complex models, SiMFy has much lower training time consumption because the model parameter numbers of SiMFy are much smaller. This could be attributed to the concise and effective model structure of SiMFy. Taking the typical GCN-based method as an example, our SiMFy is simply composed of an MLP layer that maps the input embedding to the output embedding. The GCN-based models also perform the multistep propagation operation, which will significantly increase the model parameters, especially when the depth of the GCN is increased.\nFor the ICEWS14, ICEWS18, and ICEWS05-15 benchmarks, we calculate the average training time consumption of our model SiMFy and other baselines over 30 epochs. As shown in Figure 3, the time cost is highest for the ICEWS05-15 dataset, followed by the ICEWS18 dataset, and the lowest for the ICEWS14 dataset. However, on all three\ndatasets, SiMFy exhibits the lowest time consumption compared to other baselines during the training process, making it more efficient and greatly improving resource utilization. Furthermore, we can observe that the training time consumption of SiMFy does not exhibit significant fluctuations across different datasets."
        },
        {
            "heading": "6.3 Ability to Capture Unseen Events",
            "text": "To predict the unseen events that do not appear in the historical KG sequence, a natural idea is using the historical evolutionary information of other events that interact with them, which is also known as the event structure dependency information. The mainstream view is that GCN can better capture this structural dependence, interestingly, we find that MLP and GCN have similar abilities to capture unseen events. Therefore, given the high training cost of GCN, we doubt whether it is necessary to apply it to TKG reasoning task. We collect all unseen events from the test set for the three ICEWS datasets, as indicated in Table 3, and then we evaluate the MLP module and the GCN, respectively. The results indicate that the performance of MLP is even slightly superior to that of GCN, which also validates the effectiveness of our MLP-based model."
        },
        {
            "heading": "6.4 Fixed Frequency in Training",
            "text": "SiMFy utilizes the historical frequency information of events only during the inference stage\n(fixed-frequency in training). However, many existing state-of-the-art methods, like CENET and CyGNet, combine event embeddings with historical frequency during the training process and iteratively update the model parameters. To validate which approach is more beneficial for the TKG reasoning task, we conduct comparative experiments on the ICEWS14 dataset by adopting the copy-mechanism-based learning strategy used in CyGNet and CENET. The results can be found in Table 4, MLP-ONLY consists of only the MLP module, MLP-F.F. adopts the fixed-frequency strategy during training, and MLP-COPY represents the model that incorporates the copy mechanism. It can be observed that MLP-F.F. outperforms MLPCOPY significantly, while MLP-COPY performs even slightly worse than the original MLP-ONLY. This suggests that MLP-COPY fails to effectively incorporate historical frequency information during the training process. Furthermore, by considering Figure 5, we can observe that MLP-COPY not only fails to enhance the model performance, but also significantly prolongs the training time for each epoch. This indirectly confirms the superiority of the fixed-frequency strategy adopted by SiMFy."
        },
        {
            "heading": "6.5 Discussion on the Depth of MLP",
            "text": "SiMFy uses a simple one-layer MLP to draw the final prediction. We also want to figure out whether the performance of the model will improve or worsen when the one-layer MLP is replaced with some deeper or more complicated neural structure. To answer this question, we conduct experiments to replace the one-layer MLP with deeper ones. The experimental results on the ICEWS14 dataset are presented in Table 5. From this table, we can see that as the structure of the model becomes more complex, its performance has hardly changed. This is because the complex model structure, while increasing computational costs, does not better capture the evolving information of entities and relations."
        },
        {
            "heading": "7 Conclusion",
            "text": "In this paper, we have proposed a new model, called SiMFy, for the TKG reasoning task. SiMFy is a conceptually straightforward method that simply combines MLP and historical frequency to model the temporal events in the TKGs. The experimental results demonstrate that SiMFy not only outperforms many existing complex methods but also exhibits faster convergence speed and better generalization ability. Our findings suggest that a welldesigned MLP-based model, such as SiMFy, can effectively address the limitations faced by complex architectures, making it a practical and efficient solution for the TKG reasoning task.\nLimitations\nOne limitation of this paper is that no in-depth exploration of the potential mutual influence between historical events and unseen events is considered, which means that events that have been repeatedly happening in the sequence of historical KGs may not happen in the future, and instead, new unseen events will take place. Another limitation lies in that the time span for calculating the frequency of\nhistorical events is too long, and a more precise time window is needed to better capture the longand short-term evolutionary patterns of events."
        },
        {
            "heading": "Acknowledgements",
            "text": "This work was supported in part by the National Key R&D Program of China under Grant 2020AAA0108501, and the Key Program of Hubei under Grant JD2023008."
        },
        {
            "heading": "A Appendix",
            "text": "A.1 Results with Filtered Metrics Table 6 provides the experimental results (with filtered metrics) of SiMFy compared with other baselines on ICEWS14, ICEWS18, ICEWS05-15, and GDELT datasets.\nA.2 Statistics of Datasets The detailed statistics of the ICEWS14, ICEWS18, ICEWS05-15, and GDELT datasets are presented in Table 7.\nA.3 Proportion of Unseen Events We conduct a statistical analysis on the proportion of unseen events in the test set of the ICEWS14, ICEWS18, ICEWS05-15, and GDELT datasets. The results are presented in Table 8.\nA.4 Supplementary Figures on Convergence Speed and Generalization\nWe provide the missing figures on the ICEWS14 and ICEWS18 datasets to supplement the Section 6.1. The results on ICEWS14 are shown in Figure 6 and the results on ICEWS18 are shown in Figure 7."
        }
    ],
    "title": "SiMFy: A Simple Yet Effective Approach for Temporal Knowledge Graph Reasoning",
    "year": 2023
}