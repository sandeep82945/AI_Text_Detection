{
    "abstractText": "Spatial reasoning over text is challenging as the models not only need to extract the direct spatial information from the text but also reason over those and infer implicit spatial relations. Recent studies highlight the struggles even large language models encounter when it comes to performing spatial reasoning over text. In this paper, we explore the potential benefits of disentangling the processes of information extraction and reasoning in models to address this challenge. To explore this, we design various models that disentangle extraction and reasoning (either symbolic or neural) and compare them with state-of-the-art (SOTA) baselines with no explicit design for these parts. Our experimental results consistently demonstrate the efficacy of disentangling, showcasing its ability to enhance models\u2019 generalizability within realistic data domains.",
    "authors": [
        {
            "affiliations": [],
            "name": "Roshanak Mirzaee"
        },
        {
            "affiliations": [],
            "name": "Parisa Kordjamshidi"
        }
    ],
    "id": "SP:2607ff838f86d9b45d4f81a437f237244552612b",
    "references": [
        {
            "authors": [
                "Yejin Bang",
                "Samuel Cahyawijaya",
                "Nayeon Lee",
                "Wenliang Dai",
                "Dan Su",
                "Bryan Wilie",
                "Holy Lovenia",
                "Ziwei Ji",
                "Tiezheng Yu",
                "Willy Chung",
                "Quyet V. Do",
                "Yan Xu",
                "Pascale Fung"
            ],
            "title": "A multitask, multilingual, multimodal evaluation of chatgpt on reasoning",
            "year": 2023
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners. Advances in neural information processing",
            "year": 2020
        },
        {
            "authors": [
                "Aakanksha Chowdhery",
                "Sharan Narang",
                "Jacob Devlin",
                "Maarten Bosma",
                "Gaurav Mishra",
                "Adam Roberts",
                "Paul Barham",
                "Hyung Won Chung",
                "Charles Sutton",
                "Sebastian Gehrmann"
            ],
            "title": "Palm: Scaling language modeling with pathways",
            "year": 2022
        },
        {
            "authors": [
                "Emmanuelle-Anna Dietz",
                "Steffen H\u00f6lldobler",
                "Raphael H\u00f6ps."
            ],
            "title": "A computational logic approach to human spatial reasoning",
            "venue": "2015 IEEE Symposium Series on Computational Intelligence, pages 1627\u20131634. IEEE.",
            "year": 2015
        },
        {
            "authors": [
                "Michael Grubinger",
                "Paul Clough",
                "Henning M\u00fcller",
                "Thomas Deselaers."
            ],
            "title": "The iapr tc-12 benchmark: A new evaluation resource for visual information systems",
            "venue": "International workshop ontoImage, volume 2.",
            "year": 2006
        },
        {
            "authors": [
                "Takeshi Kojima",
                "Shixiang Shane Gu",
                "Machel Reid",
                "Yutaka Matsuo",
                "Yusuke Iwasawa."
            ],
            "title": "Large language models are zero-shot reasoners",
            "venue": "Advances in neural information processing systems, 35:22199\u2013 22213.",
            "year": 2022
        },
        {
            "authors": [
                "Parisa Kordjamshidi",
                "Marie-Francine Moens",
                "Martijn van Otterlo."
            ],
            "title": "Spatial Role Labeling: Task definition and annotation scheme",
            "venue": "Proceedings of the Seventh conference on International Language Resources and Evaluation (LREC\u201910), pages",
            "year": 2010
        },
        {
            "authors": [
                "Parisa Kordjamshidi",
                "Taher Rahgooy",
                "Marie-Francine Moens",
                "James Pustejovsky",
                "Umar Manzoor",
                "Kirk Roberts."
            ],
            "title": "Clef 2017: Multimodal spatial role labeling (msprl) task overview",
            "venue": "International Conference of the Cross-Language Evaluation Forum for",
            "year": 2017
        },
        {
            "authors": [
                "Hung Le",
                "Truyen Tran",
                "Svetha Venkatesh."
            ],
            "title": "Self-attentive associative memory",
            "venue": "International Conference on Machine Learning, pages 5682\u20135691. PMLR.",
            "year": 2020
        },
        {
            "authors": [
                "Kenton Lee",
                "Luheng He",
                "Mike Lewis",
                "Luke Zettlemoyer."
            ],
            "title": "End-to-end neural coreference resolution",
            "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 188\u2013197, Copenhagen, Denmark. Association",
            "year": 2017
        },
        {
            "authors": [
                "Tsung-Yi Lin",
                "Priya Goyal",
                "Ross Girshick",
                "Kaiming He",
                "Piotr Doll\u00e1r."
            ],
            "title": "Focal loss for dense object detection",
            "venue": "Proceedings of the IEEE international conference on computer vision, pages 2980\u20132988.",
            "year": 2017
        },
        {
            "authors": [
                "Xiao Liu",
                "Da Yin",
                "Yansong Feng",
                "Dongyan Zhao."
            ],
            "title": "Things not written in text: Exploring spatial commonsense from visual signals",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),",
            "year": 2022
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter."
            ],
            "title": "Decoupled weight decay regularization",
            "venue": "arXiv preprint arXiv:1711.05101.",
            "year": 2017
        },
        {
            "authors": [
                "Vaibhav Mavi",
                "Anubhav Jangra",
                "Adam Jatowt."
            ],
            "title": "A survey on multi-hop question answering and generation",
            "venue": "arXiv preprint arXiv:2204.09140.",
            "year": 2022
        },
        {
            "authors": [
                "Ana Cristina Mendes",
                "Lu\u00edsa Coheur",
                "Paula Vaz Lobo."
            ],
            "title": "Named entity recognition in questions: Towards a golden collection",
            "venue": "LREC.",
            "year": 2010
        },
        {
            "authors": [
                "Pasquale Minervini",
                "Sebastian Riedel",
                "Pontus Stenetorp",
                "Edward Grefenstette",
                "Tim Rockt\u00e4schel."
            ],
            "title": "Learning reasoning strategies in end-to-end differentiable proving",
            "venue": "International Conference on Machine Learning, pages 6938\u20136949. PMLR.",
            "year": 2020
        },
        {
            "authors": [
                "Roshanak Mirzaee",
                "Parisa Kordjamshidi."
            ],
            "title": "Transfer learning with synthetic corpora for spatial role labeling and reasoning",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 6148\u20136165, Abu",
            "year": 2022
        },
        {
            "authors": [
                "Roshanak Mirzaee",
                "Hossein Rajaby Faghihi",
                "Qiang Ning",
                "Parisa Kordjamshidi."
            ],
            "title": "SPARTQA: A textual question answering benchmark for spatial reasoning",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for",
            "year": 2021
        },
        {
            "authors": [
                "Diego Moll\u00e1",
                "Menno Van Zaanen",
                "Daniel Smith"
            ],
            "title": "Named entity recognition for question answering",
            "year": 2006
        },
        {
            "authors": [
                "Maxwell Nye",
                "Michael Tessler",
                "Josh Tenenbaum",
                "Brenden M Lake."
            ],
            "title": "Improving coherence and consistency in neural sequence models with dualsystem, neuro-symbolic reasoning",
            "venue": "Advances in Neural Information Processing Systems, 34:25192\u2013",
            "year": 2021
        },
        {
            "authors": [
                "Batu Ozturkler",
                "Nikolay Malkin",
                "Zhen Wang",
                "Nebojsa Jojic."
            ],
            "title": "Thinksum: Probabilistic reasoning over sets using large language models",
            "venue": "arXiv preprint arXiv:2210.01293.",
            "year": 2022
        },
        {
            "authors": [
                "Rasmus Berg Palm",
                "Ulrich Paquet",
                "Ole Winther."
            ],
            "title": "Recurrent relational networks",
            "venue": "arXiv preprint arXiv:1711.08028.",
            "year": 2017
        },
        {
            "authors": [
                "Meng Qu",
                "Junkun Chen",
                "Louis-Pascal Xhonneux",
                "Yoshua Bengio",
                "Jian Tang."
            ],
            "title": "Rnnlogic: Learning logic rules for reasoning on knowledge graphs",
            "venue": "International Conference on Learning Representations.",
            "year": 2021
        },
        {
            "authors": [
                "Hossein Rajaby Faghihi",
                "Parisa Kordjamshidi."
            ],
            "title": "Time-stamped language model: Teaching language models to understand the flow of events",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational",
            "year": 2021
        },
        {
            "authors": [
                "Dan Shen",
                "Mirella Lapata."
            ],
            "title": "Using semantic roles to improve question answering",
            "venue": "Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),",
            "year": 2007
        },
        {
            "authors": [
                "Tao Shen",
                "Guodong Long",
                "Xiubo Geng",
                "Chongyang Tao",
                "Tianyi Zhou",
                "Daxin Jiang."
            ],
            "title": "Large language models are strong zero-shot retriever",
            "venue": "arXiv preprint arXiv:2304.14233.",
            "year": 2023
        },
        {
            "authors": [
                "Zhengxiang Shi",
                "Qiang Zhang",
                "Aldo Lipani."
            ],
            "title": "Stepgame: A new benchmark for robust multi-hop spatial reasoning in texts",
            "venue": "Proceedings of the Association for the Advancement of Artificial Intelligence, AAAI \u201922.",
            "year": 2022
        },
        {
            "authors": [
                "Rahil Soroushmojdehi",
                "Sina Javadzadeh",
                "Alessandra Pedrocchi",
                "Marta Gandolla"
            ],
            "title": "Transfer learning in hand movement intention detection based on surface electromyography signals",
            "venue": "Frontiers in Neuroscience,",
            "year": 2022
        },
        {
            "authors": [
                "Keith Stenning",
                "Michiel Van Lambalgen."
            ],
            "title": "Human reasoning and cognitive science",
            "venue": "MIT Press.",
            "year": 2012
        },
        {
            "authors": [
                "Sainbayar Sukhbaatar",
                "Arthur Szlam",
                "Jason Weston",
                "Rob Fergus."
            ],
            "title": "End-to-end memory networks",
            "venue": "arXiv preprint arXiv:1503.08895.",
            "year": 2015
        },
        {
            "authors": [
                "Sagar Gubbi Venkatesh",
                "Anirban Biswas",
                "Raviteja Upadrashta",
                "Vikram Srinivasan",
                "Partha Talukdar",
                "Bharadwaj Amrutur."
            ],
            "title": "Spatial reasoning from natural language instructions for robot manipulation",
            "venue": "2021 IEEE International Conference on Robotics",
            "year": 2021
        },
        {
            "authors": [
                "Jason Wei",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Maarten Bosma",
                "Ed Chi",
                "Quoc Le",
                "Denny Zhou."
            ],
            "title": "Chain of thought prompting elicits reasoning in large language models",
            "venue": "36th Conference on Neural Information Processing Systems (NeurIPS 2022).",
            "year": 2022
        },
        {
            "authors": [
                "Yue Zhang",
                "Quan Guo",
                "Parisa Kordjamshidi."
            ],
            "title": "Towards navigation by reasoning over spatial configurations",
            "venue": "Proceedings of Second International Combined Workshop on Spatial Language Understanding and Grounded Communication for Robotics,",
            "year": 2021
        },
        {
            "authors": [
                "Chen Zheng",
                "Parisa Kordjamshidi."
            ],
            "title": "Relational gating for\" what if\" reasoning",
            "venue": "arXiv preprint arXiv:2105.13449.",
            "year": 2021
        },
        {
            "authors": [
                "Rui Zhu",
                "Krzysztof Janowicz",
                "Ling Cai",
                "Gengchen Mai."
            ],
            "title": "Reasoning over higher-order qualitative spatial relations via spatially explicit neural networks",
            "venue": "International Journal of Geographical Information Science, pages 1\u201332.",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Despite the high performance of recent pretrained language models on question-answering (QA) tasks, solving questions that require multi-hop reasoning is still challenging (Mavi et al., 2022). In this paper, we focus on spatial reasoning over text which can be described as inferring the implicit1 spatial relations from explicit relations2 described in the text. Spatial reasoning plays a crucial role in diverse domains, including language grounding (Liu et al., 2022), navigation (Zhang et al., 2021), and human-robot interaction (Venkatesh et al., 2021). By studying this task, we can analyze both the reading comprehension and logical reasoning capabilities of models.\nPrevious work has investigated the use of general end-to-end deep neural models such as pretrained language models (PLM) (Mirzaee et al., 2021) in\n1By implicit, we mean indirect relations, not metaphoric usages or implicit meaning for the relations.\n2relationships between objects and entities in the environment, such as location, distance, and relative position.\nspatial question answering (SQA). PLMs show reasonable performance on the SQA problem and can implicitly learn spatial rules from a large set of training examples. However, the black-box nature of PLMs makes it unclear whether these models are making the abstractions necessary for spatial reasoning or their decisions are based solely on patterns observed in the data.\nAs a solution for better multi-hop reasoning, recent research has investigated the impact of using fine-grained information extraction modules such as Named Entity Recognition (NER) (Moll\u00e1 et al., 2006; Mendes et al., 2010), gated Entity/Relation (Zheng and Kordjamshidi, 2021) or semantic role labels (SRL) (Shen and Lapata, 2007; Faghihi et al., 2023) on the performance of models.\nOn a different thread, cognitive studies (Stenning and Van Lambalgen, 2012; Dietz et al., 2015) show when the given information is shorter, humans also find spatial abstraction and use spatial rules to infer implicit information. Figure 1 shows an example of such extractions. Building upon these findings, we aim to address the limitations of end-to-end models and capitalize on the advantages of fine-grained information extraction in solving SQA. Thus, we propose models which disentangle the language understanding and spatial reasoning computations as two separate components. Specifically, we first design a pipeline model that includes trained neural modules for extracting direct fine-grained spatial information from the text and performing symbolic spatial reasoning over them.\nThe second model is simply an end-to-end PLM that uses annotations used in extraction modules of pipeline model in the format of extra QA supervision. This model aims to demonstrate the advantages of using separate extraction modules compared to a QA-based approach while utilizing the same amount of supervision. Ultimately, the third model is an end-to-end PLM-based model on relation extraction tasks that has explicit latent layers to disentangle the extraction and reasoning inside the model. This model incorporates a neural spatial reasoner, which is trained to identify all spatial relations between each pair of entities.\nWe evaluate the proposed models on multiple SQA datasets, demonstrating the effectiveness of the disentangling extraction and reasoning approach in controlled and realistic environments. Our pipeline outperforms existing SOTA models by a significant margin on benchmarks with a controlled environment (toy tasks) while utilizing the same or fewer training data. However, in realworld scenarios with higher ambiguity of natural language for extraction and more rules to cover, our\nend-to-end model with explicit layers for extraction and reasoning performs better.\nThese results show that disentangling extraction and reasoning benefits deterministic spatial reasoning and improves generalization in realistic domains despite the coverage limitations and sensitivity to noises in symbolic reasoning. These findings highlight the potential of leveraging language models for information extraction tasks and emphasize the importance of explicit reasoning modules rather than solely depending on black-box neural models for reasoning."
        },
        {
            "heading": "2 Related Research",
            "text": "End-to-end model on SQA: To solve SQA tasks, recent research evaluates the performance of different deep neural models such as Memory networks (Shi et al., 2022; Sukhbaatar et al., 2015), Self-attentive Associative Memory (Le et al., 2020), subsymbolic fully connected neural network (Zhu et al., 2022), and Recurrent Relational Network (RRN) (Palm et al., 2017). Mirzaee and Kordjamshidi; Mirzaee et al. use transfer learning and provide large synthetic supervision that enhances the performance of PLMs on spatial question answering. However, the results show a large gap between models and human performance on humangenerated data. Besides, none of these models use explicit spatial semantics to solve the task. The only attempt towards integrating spatial semantics into spatial QA task is a baseline model introduced in (Mirzaee et al., 2021), which uses rulebased spatial semantics extraction for reasoning on bAbI (task 17) which achieves 100% accuracy without using any training data.\nExtraction and Reasoning: While prior research has extensively explored the use of end-to-end models for learning the reasoning rules (Minervini et al., 2020; Qu et al., 2021), there is limited discussion on separating the extraction and reasoning tasks. Nye et al. utilizes LMs to generate new sentences and extract facts while using some symbolic rules to ensure consistency between generated sentences. Similarly, ThinkSum (Ozturkler et al., 2022) uses LMs for knowledge extraction (Think) and separate probabilistic reasoning (Sum), which sums the probabilities of the extracted information. However, none of these works are on multi-step or spatial Reasoning."
        },
        {
            "heading": "3 Proposed Models",
            "text": "To understand the effectiveness of disentangling the extraction and reasoning modules, we provide three groups of models. The first model is a pipeline of extraction and symbolic reasoning (\u00a73.1), the second model is an end-to-end PLM that uses the same spatial information supervision but in a QA format (\u00a73.2), and the third model is an end-toend neural model with explicit layers of extraction and reasoning (\u00a73.3). We elaborate each of these models in the subsequent sections.\nTask The target task is spatial question answering (SQA), which assesses models\u2019 ability to comprehend spatial language and reason over it. Each example includes a textual story describing entities and their spatial relations, along with questions asking an implicit relation between entities (e.g., Figure 1). SQA benchmarks provide two types of questions: YN (Yes/No) queries about the existence of a relation between two groups of entities, and FR (Find Relation) seeks to identify all possible (direct/indirect) relations between them. The answer to these questions is chosen from a provided candidate list. For instance, the candidate list for FR questions can be a sublist of all relation types in Table 1."
        },
        {
            "heading": "3.1 Pipeline of Extraction and Reasoning",
            "text": "Here, we describe our suggested pipeline model designed for spatial question answering task, referred to as PISTAQ3. As shown in the extraction part of Figure 2, the spatial information is extracted first and forms a set of triplets for a story (Facts) and a question (Query). Then a coreference resolution module is used to connect these triplets to each other. Given the facts and queries, the spatial reasoner infers all implicit relations. The answer generator next conducts the final answer. Below we describe each module in more detail. Spatial Role Labeling (SPRL) is the task of identifying and classifying the spatial roles of phrases within a text (including the Trajector, Landmark, and Spatial Indicator) and formalizing their relations (Kordjamshidi et al., 2010). Here, we use the same SPRL modules as in (Mirzaee and Kordjamshidi, 2022). This model first computes the token representation of a story and its question using a BERT model. Then a BIO tagging layer is applied on the tokens representations using (O, B-\n3PIpeline model for SpaTiAl Question answering\nentity, I-entity, B-indicator, and I-indicator) tags. Finally, a softmax layer on the BIO tagger output selects the spatial entities4 (e.g., \u2018grey car\u2019 or \u2018plants\u2019 in Figure 2) and spatial indicators (e.g., \u2018in front of\u2019 in Figure 2).\nGiven the output of the spatial role extraction module, for each combination of (Trajector, Spatial indicator, Landmark) in each sentence, we create a textual input5 and pass it to a BERT model. To indicate the position of each spatial role in the sentence, we use segment embeddings and add 1 if it is a role position and 0 otherwise. The [CLS] output of BERT will be passed to a one-layer MLP that provides the probability for each triplet. To apply the logical rules on the triplets, we need to assign a relation type to each triplet. To this aim, we use another multiclassification layer on the same [CLS] token to identify the spatial types of the triplet. The classes are relation types in Table 1 alongside a class NaN for triplet with no spatial meaning. For instance, in Figure 2, (grey car, in front of, grey house) is a triplet with FRONT as its relation type while (grey house, in front of, grey car) is not a triplet and its relation type is NaN . We use a joint loss function for triplet and relation type classification to train the model. Coreference Resolution Linking the extracted triplets from the stories is another important step required in this task, as different phrases or pronouns may refer to same entity. To make such connections, we implement a coreference resolu-\n4Trajector/Landmark 5[CLS, traj, SEP, indic, SEP, land, SEP, sentence, SEP]\ntion model based on (Lee et al., 2017) and extract all antecedents for each entity and assign a unique id to them. In contrast to previous work, we have extended the model to support plural antecedents (e.g., two circles). More details about this model can be found in Appendix C.2. To find the mentions of the question entities in the story and create the queries, we use a Rule-based Coreference (R-Coref) based on exact/partial matching. In Figure 2, \u2018the car\u2019 in the question has the same id as \u2018the grey car\u2019 from the story\u2019s triplets. Logic-based Spatial Reasoner To do symbolic spatial reasoning, we use the reasoner from (Mirzaee and Kordjamshidi, 2022). This reasoner is implemented in Prolog and utilizes a set of rules on various relation types, as illustrated in Table 2. Given the facts and queries in Prolog format, the spatial reasoner can carry out the reasoning process and provide an answer to any given query. The reasoner matches variables in the program with concrete values and a backtracking search to explore different possibilities for each rule until a solution is found. As shown in Figure 2, the reasoner uses a FRONT and a BEHIND rules over the facts and generates the True response for the query."
        },
        {
            "heading": "3.2 PLMs Using SPRL Annotations",
            "text": "To have a fair comparison between the QA baselines and models trained on SPRL supervision, we design BERT-EQ6. We convert the SPRL annotation into extra YN questions7 asking about explicit relations between a pair of entities. To generate extra questions, we replace triplets from the SPRL annotation into the \u201cIs [Trajector] [Relation*] [Landmark]?\u201d template. The [Trajector] and [Landmark] are the entity phrases in the main sentence ignoring pronouns and general names (e.g., \u201can object/shape\u201d). The [Relation*] is a relation expression (examples presented in Table 1) for the\n6BERT+Extra Question 7This augmentation does not apply to FR type since it\ninquires about all relations between the two asked entities.\ntriplet relation type. To have equal positive and negative questions, we reverse the relation in half of the questions. We train BERT-EQ using both original and extra questions by passing the \u201cquestion+story\u201d into a BERT with answers classification layers."
        },
        {
            "heading": "3.3 PLMs with Explicit Extractions",
            "text": "As another approach, we aim to explore a model that disentangles the extraction and reasoning parts inside a neural model. Here, rather than directly predicting the answer from the output of PLMs (as typically done in the QA task), we introduce explicit layers on top of PLM outputs. These layers are designed to generate representations for entities and pairs of entities, which are then passed to neural layers to identify all relations. We call this model SREQA8, which is an end-to-end spatial relation extraction model designed for QA. Figure 3 illustrates the structure of this model.\nIn this model, we first select the entity mentions (Mj(E1)) from the BERT tokens representation and pass it to the extraction part shown in Figure 3a. Next, the model computes entity representation (M(E1)) by summing the BERT token representations of all entity\u2019s mentions and passing it to an MLP layer. Then for each pair of entities, a triplet is created by concatenating the pair\u2019s entities representations and the BERT [CLS] token representation. This triplet is passed through an MLP layer to compute the final pair representations. Next, in the reasoning part in Figure 3a, for each relation type in Table 1, we use a binary 2-layer MLP classifier to predict the probability of each relation between the pairs. We remove the inconsistent relations by selecting one with a higher probability at inference time, e.g., LEFT and RIGHT cannot be true at the same time. The final output is a list of all possible relations for each pair. This model is trained using the summation of Focal loss (Lin et al., 2017) of all relation classifiers.\n8Spatial Relation Extraction for QA\n...\na grey car\nthe carBE R\nT\n[C LS\n] ( St\nor y)\n: g re\ny ca\nr i s\npa rk\n.. . +\n(Q ue\nst io\nn) :\nM LP\n(E nt\nity re\np)\nM LP\n(T rip\nle t r\nep )\nLEFT Classifier T o k e n R e p r e s e n t a t i o n M1(E1) ... Mn(E1) M1(E2) Mn(E2) M - E 1 M - E 2 [C LS ] RIGHT Classifier\nNTPPI Classifier\nBEHIND Classifier\nAn sw\ner G\nen er\nat io\nn\nA re\nth e\npl an\nts b\neh in\nd th\ne ca\nr [ SE\nP] the plants\n1\nYes\np a i r\nExtraction Reasoning\n0\n0\n0\nplants Se le ct E nt\nity M\nen tio\nn\n(a) Model structure. First, entity mentions such as \u2018plants\u2019 and \u2018grey car\u2019 are selected from the BERT output and the entity representation is formed. Next, triplets like (\u2018plants\u2019, \u2018car\u2019, [CLS]) are generated and fed into the reasoning component. The collective output of all relation classifiers determines the relationships between each pair. *All hatched parts are trained end-to-end. The rest of the data is obtained from annotations or off-the-shelf modules.\nAll story relations (Training step#1) Question relation (Training step#2)\nEntity Mentions Relation Supervision Entity + Coref\nannotation\nTrained entity extraction + R-Coref\nSpRL + Coref annotation -> Spatial Reasoner\nQA Answer\nSource of\n(b) The source of supervision in each step of training. In step#1, we train the model on all story relations, and in step#2, we only train it on question relations.These modules and data are the same as the ones used in PISTAQ.\nFigure 3: The SREQA model with explicit neural layers to disentangle extraction and reasoning part.\nWe train SREQA in two separate steps. In the first step, the model is trained on a relation extraction task which extracts all direct and indirect relations between each pair of entities only from stories. The top row of Figure 3b shows the annotation and modules employed in this step to gather the necessary supervision. We use the entity and coreference annotation to select the entity mentions from the BERT output. To compute the relations supervision for each pair of entities, we employ the spatial reasoner from PISTAQ and apply it to the direct relations (triplets) from the SPRL annotation, which are connected to each other by coreference annotations. This step of training is only feasible for datasets with available SPRL and coreference annotations.\nIn the next step, we further train SREQA on extracting questions relation using QA supervision. As shown in the bottom row of Figure 3b, we employ the trained spatial role extraction model used in PISTAQ to identify the entities in the question and use R-Coref to find the mentions of these entities in the story. To obtain the relation supervision, we convert the question answers to relation labels. In FR, the label is similar to the actual answer, which is a list of all relations. In YN, the question\nrelation is converted to a label based on the Yes/No answer. For example, in Figure 3a, the question relation is \u2018BEHIND,\u2019 and the answer is Yes, so the label for the BEHIND classifier is 1.\nWe evaluate the SREQA model\u2019s performance in predicting the accurate answers of the test set\u2019s questions same as training step 2."
        },
        {
            "heading": "4 Experiments",
            "text": ""
        },
        {
            "heading": "4.1 Datasets",
            "text": "SPARTQA is an SQA dataset in which examples contain a story and multiple YN9 and FR questions that require multi-hop spatial reasoning to be answered. The stories in this dataset describe relations between entities in a controlled (toy task) environment. This dataset contains a large synthesized part, SPARTQA-AUTO, and a more complex small human-generated subset, SPARTQA-HUMAN. All stories and questions in this dataset also contain SPRL annotations. SPARTUN is an extension of SPARTQA with YN and FR questions containing SPRL annotations. Compared to SPARTQA, the vocabulary and relation types in this dataset are extended, and it covers more relation types, rules, and spatial expressions to describe relations between entities. RESQ is an SQA dataset with Yes/No questions over the human-generated text describing spatial relations in real-world settings. The texts of this dataset are collected from MSPRL dataset (Kordjamshidi et al., 2017) (See Figure 1), which describe some spatial relations in pictures of ImageCLEF (Grubinger et al., 2006). Also, the MSPRL dataset already contains some SPRL annotations. To answer some of the questions in this dataset, extra spatial-commonsense information is needed (e.g., a roof is on the top of buildings)."
        },
        {
            "heading": "4.2 Model Configurations & Baselines",
            "text": "We compare the models described in section 3 with the following baselines. Majority Baseline: This baseline selects the most frequent answer(s) in each dataset. GT-PISTAQ: This model uses ground truth (GT) values of all involved modules in PISTAQ to eliminate the effect of error propagation in the pipeline. This baseline is used to evaluate the alignments between the questions and story entities and the reasoning module in solving the QA task. It also\n9We ignore \u201cDont know\u201d answers in YN question and change them to No\ngives an upper bound for the performance of the pipeline model, as the extraction part is perfect. BERT: We select BERT as a candidate PLM that entangles the extraction and reasoning steps. In this model, the input of the \u201cquestion+story\u201d is passed to the BERT, and the [CLS] representation is used to do the answer classification. GPT3.5: GPT3.5 (Brown et al., 2020) baselines (GPT3.5 text-davinci-003) is selected as a candidate of generative larger language models which already passes many SOTAs in reasoning tasks (Bang et al., 2023; Kojima et al., 2022). We use Zero_shot and Few_shot (In-context learning with few examples) settings to evaluate this model on the human-generated benchmarks. We also evaluate the Chain-of-Thoughts (CoT) prompting method (Wei et al., 2022) to extend the prompts with manually-written reasoning steps. The format of the input and some prompt examples are presented in Appendix E.\nTransfer learning has already demonstrated significant enhancements in numerous deep learning tasks (Soroushmojdehi et al., 2022; Rajaby Faghihi and Kordjamshidi, 2021). Thus, when applicable, we further train models on SPARTUN synthetic data shown by \u201c*\u201d. The datasets\u2019 examples and statistics and more details of the experimental setups and configurations are provided in Appendix A and B. All codes are publicly available at https://github.com/RshNk73/PistaQ-SREQA."
        },
        {
            "heading": "5 Results and Discussion",
            "text": "Here, we discuss the influence of disentangling extraction and reasoning manifested in PISTAQ and SREQA models compared to various end-to-end models with no explicit design for these modules, such as BERT, BERT-EQ, and GPT3.5. Table 3 shows the list of these models with the sources of\ntheir supervision as well as extra off-the-shelf or rule-based modules employed in them.\nSince the performance of extraction modules, Spatial Role Labeling (SPRL) and Coreference Resolution (Coref), directly contribute to the final accuracy of the designed models, we have evaluated these modules and reported the results in Table 4. We choose the best modules on each dataset for experiments. For a detailed discussion on the performance of these modules, see Appendix C."
        },
        {
            "heading": "5.1 Result on Controlled Environment",
            "text": "Table 5 shows the performance of models on two auto-generated benchmarks, SPARTUN and SPARTQA-AUTO. We can observe that PISTAQ outperforms all PLM baselines and SREQA. This outcome first highlights the effectiveness of the extraction and symbolic reasoning pipeline compared to PLMs in addressing deterministic reasoning within a controlled environment. Second, it shows that disentangling extraction and reasoning as a pipeline works better than explicit neural layers in SQA with a controlled environment. The complexity of these environments is more related to conducting several reasoning steps and demands accurate logical computations where a rule-based reasoner excels. Thus, the result of PISTAQ with a rule-based reasoner module is also higher than SREQA with a neural reasoner.\nThe superior performance of PISTAQ over BERT suggests that SPRL annotations are more effective in the PISTAQ pipeline than when utilized in BERT-EQ in the form of QA supervision. Note that the extraction modules of PISTAQ achieve perfect results on auto-generated benchmarks while trained only on a quarter of the SPRL annotations\nas shown in Table 5. However, BERT-EQ uses all the original dataset questions and extra questions created from the full SPRL annotations.\nTable 6 demonstrates the results of models on SPARTQA-HUMAN with a controlled environment setting. As can be seen, our proposed pipeline, PISTAQ, outperforms the PLMs by a margin of 15% on YN questions, even though the extraction modules, shown in Table 4, perform low. This low performance is due to the ambiguity of human language and smaller training data. We also evaluate PISTAQ on SPARTQA-HUMAN FR questions using Macro_f1 score on all relation types. PISTAQ outperforms all other baselines on FR questions, except for BERT*.\nThere are two main reasons behind the inconsistency in performance between YN and FR question types. The first reason is the complexity of the YN questions, which goes beyond the basics of spatial reasoning and is due to using quantifiers (e.g., all circles, any object). While previous studies have demonstrated that PLMs struggle with quantifiers (Mirzaee et al., 2021), the reasoning module in PISTAQ can adeptly handle them without any performance loss. Second, further analysis indicates that PISTAQ predicts \u2018No\u2019 when a relationship is not extracted, which can be correct when the answer is \u2018No\u2019. However, in FR, a missed extraction causes a false negative which decreases F1 score."
        },
        {
            "heading": "5.2 Results on Real-world Setting",
            "text": "We select RESQ as an SQA dataset with realistic settings and present the result of models on this dataset in Table 7.\nTo evaluate PISTAQ on RESQ, we begin by adapting its extraction modules through training on the corresponding dataset. We train the SPRL modules on both MSPRL and SPARTUN, and the performance of these models is presented in Table 4. As the MSPRL dataset lacks coreference an-\nnotations, we employ the model trained on SPARTUN for this purpose. Rows 14 and 15 in Table 7 show the performance of the PISTAQ on RESQ is inferior compared to other baselines. To find the reason, we analyze the first 25 questions from the RESQ test set. We find that 18 out of 25 questions required spatial commonsense information and cannot be answered solely based on the given relations in the stories. From the remaining 7 questions, only 2 can be answered using the SPRL annotations provided in the MSPRL dataset. Some examples of this analysis are provided in Appendix D. Hence, the low performance of PISTAQ is attributed to first the absence of integrating commonsense information in this model and, second, the errors in the\nextraction modules, which are propagated to the reasoning modules.\nAs shown in Table 7, the best result on RESQ is achieved by SREQA* model. Compared to SREQA, SREQA* is trained on SPARTUN instead of MSPRL10 in the first step of the training. MSPRL lacks some SPRL and coreference annotations to answer RESQ questions. In the absence of this information, collecting the supervision for the first phase of training results in a significant number of missed relations. Therefore, as shown in row 11 of Table 7, employing MSPRL in the first training phase decreases the performance while replacing it with SPARTUN in SREQA* significantly enhances the results.\nSREQA* surpasses the PLMs trained on QA and QA+SPRL annotation, showcasing the advantage of the design of this model in utilizing QA and SPRL data within explicit extraction layers and the data preprocessing. Also, the better performance of this model compared to PISTAQ demonstrates how the end-to-end structure of SREQA can handle the errors from the extraction part and also can capture some rules and commonsense knowledge from RESQ training data that are not explicitly supported in the symbolic reasoner.\nIn conclusion, compared to PLMs, disentangling extraction and reasoning as a pipeline indicates su-\n10As mentioned, we use the MSPRL annotation for RESQ dataset.\nperior performance in deterministic spatial reasoning within controlled settings. Moreover, explicitly training the extraction module proves advantageous in leveraging SPRL annotation more effectively compared to using this annotation in QA format in the end-to-end training. Comparison between disentangling extraction and reasoning as a pipeline and incorporating them within an end-to-end model demonstrates that the end-to-end model performs better in realistic domains even better than PLMs. The end-to-end architecture of this model effectively enhances the generalization in the real-world setting and addresses some of the limitations of rule coverage and commonsense knowledge."
        },
        {
            "heading": "5.3 LLMs on Spatial Reasoning",
            "text": "Recent research shows the high performance of LLMs with zero/few_shot setting on many tasks (Chowdhery et al., 2022; Brown et al., 2020). However, (Bang et al., 2023) shows that ChatGPT (GPT3.5-Turbo) with zero_shot evaluation cannot perform well on SQA task using SPARTQAHUMAN test cases. Similarly, our experiments, as shown in Tables 6 and 7, show the lower performance of GPT3.5 (davinci) with zero/few_shot settings compared to human and our models PISTAQ and SREQA. Figure 4, shows an example of three LLMs, GPT3.5, GPT4 and PaLM2 on SPARTQA-HUMAN example11 (complete figure\n11Due to the limited resources, we only use GPT4 and PaLM2 on a few examples to evaluate their performance on\nincluding zero_shot examples is presented in Appendix E). Although Wei et al. shows that using CoT prompting improves the performance of PaLM on multi-step reasoning task, its spatial reasoning capabilities still does not meet the expectation."
        },
        {
            "heading": "5.3.1 LLMs as Extraction Module in PISTAQ",
            "text": "A recent study (Shen et al., 2023) shows that LLMs have a promising performance in information retrieval. Building upon this, we employ LLM, GPT3.5-Turbo with few_shot prompting to extract information from a set of SPARTQA-HUMAN and RESQ examples that do not necessitate commonsense reasoning for answering. The extracted information is subsequently utilized within the framework of PISTAQ.\nThe results, illustrated in the last row of Figure 4, highlight how the combination of LLM extraction and symbolic reasoning enables answering questions that LLMs struggle to address. Furthermore, Figure 5 provides a comparison between the trained BERT-based SPRL extraction modules and GPT3.5 with few_shot prompting in PISTAQ. It is evident that GPT3.5 extracts more accurate information, leading to correct answers. As we mentioned before, out of 25 sampled questions from RESQ, only 7 can be solved without relying on spatial commonsense information. Our experimental result shows that PISTAQ using LLM as extraction modules can solve all of these 7 questions.\nBased on these findings, leveraging LLMs in PISTAQ to mitigate errors stemming from the SPRL extraction modules rather than relying solely on LLMs for reasoning can be an interesting future research direction. This insight emphasizes the importance of considering new approaches for incorporating explicit reasoning modules whenever possible instead of counting solely on black-box\nSQA tasks.\nneural models for reasoning tasks."
        },
        {
            "heading": "6 Conclusion and Future Works",
            "text": "We investigate the benefits of disentangling the processes of extracting spatial information and reasoning over them. To this end, we devised a series of experiments utilizing PLMs for spatial information extraction coupled with a symbolic reasoner for inferring indirect relations. The outcomes of our experiments provide noteworthy insights: (1) Our observations in controlled experimental conditions demonstrate that disentangling extraction and symbolic reasoning, compared to PLMs, enhances the models\u2019 reasoning capabilities, even with comparable or reduced supervision. (2) Despite the acknowledged fragility of symbolic reasoning in real-world domains, our experiments highlight that employing explicit extraction layers and utilizing the same symbolic reasoner in data preprocessing enhances the reasoning capabilities of models. (3) Despite the limitations of LLMs in spatial reasoning, harnessing their potential for information extraction within a disentangled structure of Extraction and Reasoning can yield significant benefits. All of these results emphasize the advantage of disentangling the extraction and reasoning in spatial language understanding.\nIn future research, an intriguing direction is incorporating spatial commonsense knowledge using LLMs as an extraction module in the pipeline of extraction and reasoning. Additionally, the model\u2019s applicability extends beyond spatial reasoning, making it suitable for various reasoning tasks involving logical rules, such as temporal or arithmetic reasoning."
        },
        {
            "heading": "Acknowledgements",
            "text": "This project is partially supported by the National Science Foundation (NSF) CAREER award 202826. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation. We thank all reviewers for their helpful comments and suggestions. We would like to express our gratitude to Hossein Rajaby Faghihi for his valuable discussions and input to this paper.\nLimitations\nOur model is evaluated on a Spatial Reasoning task using specifically designed spatial logical rules.\nHowever, this methodology can be readily extended to other reasoning tasks that involve a limited set of logical rules, which can be implemented using logic programming techniques. The extraction modules provided in this paper are task-specific and do not perform well on other domains, but they can be fine-tuned on other tasks easily. Using LLM in the extraction phase can also deal with this issue. Also, using MSPRL annotation on RESQ(which this data is provided on) decreases the performance of our models. This annotation does not contain the whole existing relations in the context. The evaluation of the reasoning module is based on the existing datasets. However, we cannot guarantee that they cover all possible combinations between spatial rules and relation types. Many questions in RESQ need spatial commonsense to be answered. As a result, due to the limitation of our symbolic spatial reasoner, the performance of the pipeline model is much lower than what we expected. Due to the high cost of training GPT3.5 on large synthetic data, we cannot fine-tune the whole GPT3.5 and only provide the GPT3.5 with Few_shot learning on small human-generated benchmarks. Also, due to the limited access, we can only test PaLM2 and GPT4 on a few examples."
        },
        {
            "heading": "A Statistic Information",
            "text": "This section presents statistical information regarding dataset sizes and additional analyses conducted on the evaluation sets of human-generated datasets.\nTable 8 provides the number of questions in the training and evaluation sets of the SQA benchmarks. Tables 9 and 10 present the sentence and number of relation triplets within the SPRL annotation for each dataset, respectively. Table 11 illustrates a comprehensive breakdown of the size of Role and Relation sets in the MSPRL dataset.\nA.1 Analyzing SPARTQA-HUMAN YN\nWe conducted additional evaluations on the superior performance of PISTAQ over other baseline models on SPARTQA-HUMAN YN questions. As explained before, PISTAQ tends to predict No when\ninformation is not available, resulting in more No and fewer Yes predictions compared to other models, as presented in Table 12. The number of true positive predictions for PISTAQ is more than two other baselines, and as a result, it achieves higher accuracy.\nA.2 SPRL Annotations in MSPRL RESQ is built on the human-written context of MSPRL dataset which includes SPRL annotations. However, using this annotation in BERT-EQ and SREQA models causes lower performance (Check the result on Tablel 14). Our analysis shows that the\nSPRL annotations of MSPRL are not fully practical in our work due to two main reasons:\n1. Missed annotations: As shown in Figure 7, there are many missed annotations for each text, e.g., NTPP(bar, with, chair).\n2. No coreference : The coreference is not supported in this dataset, e.g., \u201cL2: it\u201d and \u201cT2: a bench\u201d are the same entity with different mentions, but they are mentioned with different ids. These missed coreferences result in fewer connections between entities and fewer inferred relations."
        },
        {
            "heading": "B Models and modules configuration",
            "text": "We use the huggingFace12 implementation of pretrained BERT base models, which have 768 hidden dimensions. All models are trained on the training set, evaluated on the dev set, and reported the result on the test set. For training, we train the model until no changes happen on the dev set and then store and use the best model on the dev set. We use AdamW ((Loshchilov and Hutter, 2017)), and learning rates from 2\u00d710\u22126, 2\u00d710\u22125 (depends on the task and datasets) on all models and modules. For the extraction modules and the baselines, we used the same configuration and setting as previous works (Mirzaee and Kordjamshidi, 2022). For SREQA models we use learning rates of 2 \u00d7 10\u22125, 4 \u00d7 10\u22126 for SREQA(story) and SREQA(question) respectively. To run the models we use machine with Intel Core i9-9820X (10 cores,\n12https://huggingface.co/transformers/v2.9.1/ model_doc/bert.html\n3.30 GHz) CPU and Titan RTX with NVLink as GPU.\nFor GPT3.5, we use Instruct-GPT, davinci00313. The cost for running GPT3.5 on the humangenerated benchmarks was 0.002$ per 1k tokens. For GPT3.5 as information extraction, we use GPT3.5 turbo (a.k.a ChatGPT) with a cost of 0.0001$ per 1k tokens. We also use the GPT4 playground in OpenAI and PaLM2 playground to find the prediction of examples in Figure 11."
        },
        {
            "heading": "C Extraction and Reasoning Modules",
            "text": "Here, we discuss each module used in PISTAQ and their performance including the Spatial Role Labeling (SPRL), Coreference Resolution, and Spatial reasoner.\nCLS T1 T2 Tn SEP\nA grey car is parking in front of a grey house\nT7\nSpatial Role (entity and spatial_indicator) extraction Extracting spatial entity and spatial indicators\nList of all spatial entities List of all spatial indicators\nSpatial Relation (triplet) Extraction\nCLS SEPTiT1 SEP Tj SEP TnTk+mTk SEP T1\nentity1 (Trajector)\nentity2 (Landmark) Indicator2 (spatial_indicator)\nSentence\n0 011 0 1 0 111 0 1\nSegment Embedding: [ 0 1 1 1 0 0 0 1 0 1 1 1 0 ]\nSpatial Type Cls (ftype)triplet cls (ftriplet)\ny'y\nO E E E OSP\nFigure 8: Spatial role labeling model includes two separately trained modules. E: entity, SP: spatial_indicators. As an example, triplet (a grey house, front , A grey car) is correct and the \u201cspatial_type = FRONT\u201d, and (A grey car, front, a grey house) is incorrect, and the \u201cspatial_type = NaN\u201d. Image from (Mirzaee and Kordjamshidi, 2022)\nC.1 Spatial Role Labeling (SPRL) The SPRL module, shown in Figure 8 is divided into three sub-modules, namely, spatial role extraction (SRole), spatial relation extraction (SRel)14, and spatial type classification (SType). We only\n13from https://beta.openai.com 14Since the questions(Q) and stories(S) have different annotations (questions have missing roles), we separately train and test the SRel and SType modules\nuse these modules on sentences that convey spatial information in each benchmark. To measure the performance of SPRL modules, we use the macro average of F1 measure for each label. These modules are evaluated on three datasets that provide SPRL annotations, MSPRL, SPARTQA, and SPARTUN. When training the SPRL module on auto-generated benchmarks, we achieved a performance of 100% using only a quarter of the training data, therefore we stopped further training.\nAs shown in Table 4, all SPRL sub-modules achieve a high performance on synthetic datasets, SPARTQA and SPARTUN. This good performance is because these datasets may contain less ambiguity in the natural language expressions. Therefore, the BERT-base models can easily capture the syntactic patterns needed for extracting the roles and direct relations from the large training set.\nC.2 Coreference Resolution (Coref) in Spatial Reasoning\nWe implement a coreference resolution model based on (Lee et al., 2017) to extract all antecedents for each entity (check Figure 9a). Compared to the previous works, we have the entities (phrase) annotations. Hence, we ignore the phrase scoring modules and use this annotation instead. We first collect all mentions of each predicted entity from spatial role extraction or role annotations, then assign an \u201cid\u201d to the same mentions and include that id in each triplet. For example, for BELOW(a cat,\na grey car), Front(the car, a church), id 1= a cat, 2 = a grey car, the car, and 3 = a church. So we create new triplets in the form of BELOW(1, 2) and Front(2, 3).\nTo train the model, we pair each mention with its previous antecedent and use cross-entropy loss to penalize the model if the correct pair is not chosen. For singletons and starting mention of objects, the model should return class 0, which is the [CLS] token. Since the previous model does not support the plural antecedent (e.g., two circles), we include that by considering shared entity in pairs like both (two circles, the black circle) and (two circles, the blue circle) are true pairs.\nAs an instance of the importance of coreference resolution in spatial reasoning, consider this context \u201cblock A has one black and one green circle. The black circle is above a yellow square. The yellow square is to the right of the green circle. Which object in block A is to the left of a yellow square\u201d The reasoner must know that the NTPPI(block A, one green circle) and RIGHT( the yellow square, the green circle) are talking about the same object to connect them via transitivity and find the answer.\nTo evaluate the coreference resolution module (Coref in Table 4), we compute the accuracy of the pairs predicted as Corefs. The Coref model achieves a high performance on all datasets. The performance is slightly lower on the SPARTQAHUMAN dataset when SPARTUN is employed for additional pre-training. However, we observed that there are many errors in the annotations in SPARTQA-HUMAN, and the pre-trained model is, in fact, making more accurate predictions than what is reflected in the evaluation.\nC.3 Logic-based Spatial Reasoner\nTo solely evaluate the performance of the logicbased reasoner, we use the \u201cGT-PISTAQ\u201d. We look into the errors of this model and categorize them based on the source of errors. The categories are\nmissing/wrong ground truth direct annotations (A), rule-based Coreference Error (C) in connecting the extracted information before passing to the reasoner, and the low coverage of spatial concepts in the reasoner (R). As is shown in Table 13, spatial Reasoner causes no errors for SPARTUN since the same reasoner has been used to generate it. However, the reasoner does not cover spatial properties of entities (e.g., right edge in \u201ctouching right edge\u201d) in SPARTQA and causes wrong predictions in those cases."
        },
        {
            "heading": "D SREQA on All Story Relations",
            "text": "Table 14 displays the results of the SREQA model trained and tested solely on all the story\u2019s relation extraction parts (step 1). During the evaluation, we also possess the same data preprocessing and gather annotations of all relations between stories\u2019 entities and select the best model based on performance on the development set.\nNotably, the performance on the humangenerated datasets, SPARTQA-HUMAN and RESQ, is significantly lower compared to the autogenerated datasets. As discussed in , the MSPRL datasets contain missed annotations, resulting in the omission of several relations from the stories\u2019 entities and incomplete training data for this phase. Similarly, the SPARTQA-HUMAN SPRL annotation, as discussed in Appendix C, exhibits some noise, particularly in coreference annotation, leading to similar issues as observed in MSPRL regarding annotation of all story relations.\nConsequently, this reduced performance in all story relation extraction impacts the overall performance of the main SREQA model trained using two steps; however, as illustrated in the results of SREQA* in Table7, which utilizes Spartun instead of MSPRL for training on all story\u2019s relations, the performance substantially improves on the RESQ dataset."
        },
        {
            "heading": "E Large Language Models (LLMs)",
            "text": "Figure 11 presents examples showcasing predictions made by three Large Language Models (LLMs): GPT3.5-DaVinci, GPT4, and PaLM2, on a story from the SPARTQA-HUMAN dataset. These examples demonstrate that while these models, specifically GPT4 and PaLM2, excel in multihop reasoning tasks, solving spatial question answering remains a challenging endeavor.\nTo evaluate the LLMs\u2019 performance on spatial reasoning, we use Zero_shot, Few_shot, and Few_shot+CoT. In the Zero_shot setting, the prompt given as input to the model is formatted as \u201cContext: story. Question: question?\u201d and the model returns the answer to the question.\nIn the Few_shot setting, we add two random examples from the training data with a story, all its questions, and their answers. Figure 12 depicts a prompt example for SPARTQA-HUMAN YN questions, passed to GPT3.5.\nFor Few_shot+CoT, we use the same idea as (Wei et al., 2022) and manually write the reasoning steps for eight questions (from two random stories). The input then is formatted as \u201cContext: story. Question: CoT. Answer. Asked Context: story. Question: question?\u201d. Figure 13 shows an example of these reasoning steps on RESQ.\nE.1 LLMs for Information Extraction\nAs discussed in Section 5.3.1, we utilize LLM, GPT3.5-Turbo, for information extraction from human-generated texts. The extraction process encompasses Entity, Relation, Relation Type, and coreference extractions from the story, as well as entity and relation extraction from the question. Additionally, LLM is employed to identify mentions of question entities within the text.\nWe construct multiple manually crafted prompt examples for each extraction task, as depicted in Figure 14. Subsequently, the extracted information is inputted into the reasoner module of PISTAQ to compute the answers.\nIn addition to our experiment, we attempted to incorporate LLMs as neural spatial reasoners but in a pipeline structure of extraction and reasoning. To do so, as illustrated in Figure 10, we add the extracted information of LLM with the written CoTs based on this extracted information to the prompt of a GPT3.5-DaVinci. The results, however, become even lower (62.62%) compared to GPT3.5-CoT with the main text (67.05%) when evaluated on\nthe RESQ dataset. This outcome highlights the superior ability of LLMs to capture information from natural language compared to structured data without fine-tuning."
        }
    ],
    "title": "Disentangling Extraction and Reasoning in Multi-hop Spatial Reasoning",
    "year": 2023
}