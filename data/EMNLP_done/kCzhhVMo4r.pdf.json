{
    "abstractText": "Pre-trained language models greatly improve the performance of various tasks but at a cost of high computation overhead. To facilitate practical applications, there are mainly two lines of research to accelerate model inference: model compression and dynamic computation (e.g., dynamic token pruning). Existing works either adopt these methods individually or simply apply dynamic computation approaches upon a compressed small language model. We argue that they are sub-optimal since the two approaches are separately designed so the compressed model may not be tailored for dynamic computation. To tackle this problem and make compressed small language models faster, we propose Length-Adaptive Distillation, a twostage knowledge distillation framework that aims to produce a customized small language model for dynamic token pruning. In the general distillation stage, we enforce the student to mimic and reconstruct the teacher\u2019s output based on the dynamically pruned representations. Then in the task-specific distillation stage, the student is further accustomed to token pruning while absorbing the task-specific knowledge. Experimental results on GLUE benchmark demonstrate that our method can make the small language model more customized for dynamic token pruning and achieve better speed-performance trade-off.",
    "authors": [
        {
            "affiliations": [],
            "name": "Chang Liu"
        },
        {
            "affiliations": [],
            "name": "Chongyang Tao"
        },
        {
            "affiliations": [],
            "name": "Jianxin Liang"
        },
        {
            "affiliations": [],
            "name": "Jiazhan Feng"
        },
        {
            "affiliations": [],
            "name": "Tao Shen"
        },
        {
            "affiliations": [],
            "name": "Quzhe Huang"
        },
        {
            "affiliations": [],
            "name": "Dongyan Zhao"
        }
    ],
    "id": "SP:6cc0aa6094ab5ac5e22b3e5c30682e9ea7f08bc1",
    "references": [
        {
            "authors": [
                "Haoli Bai",
                "Wei Zhang",
                "Lu Hou",
                "Lifeng Shang",
                "Jin Jin",
                "Xin Jiang",
                "Qun Liu",
                "Michael Lyu",
                "Irwin King."
            ],
            "title": "BinaryBERT: Pushing the limit of BERT quantization",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics",
            "year": 2021
        },
        {
            "authors": [
                "Luisa Bentivogli",
                "Peter Clark",
                "Ido Dagan",
                "Danilo Giampiccolo."
            ],
            "title": "The fifth pascal recognizing textual entailment challenge",
            "venue": "TAC.",
            "year": 2009
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners. Advances in neural information processing",
            "year": 2020
        },
        {
            "authors": [
                "Han Cai",
                "Chuang Gan",
                "Tianzhe Wang",
                "Zhekai Zhang",
                "Song Han."
            ],
            "title": "Once-for-all: Train one network and specialize it for efficient deployment",
            "venue": "arXiv preprint arXiv:1908.09791.",
            "year": 2019
        },
        {
            "authors": [
                "Daniel Cer",
                "Mona Diab",
                "Eneko Agirre",
                "I\u00f1igo LopezGazpio",
                "Lucia Specia."
            ],
            "title": "SemEval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation",
            "venue": "Proceedings of the 11th International Workshop on Semantic",
            "year": 2017
        },
        {
            "authors": [
                "Ting Chen",
                "Simon Kornblith",
                "Mohammad Norouzi",
                "Geoffrey Hinton."
            ],
            "title": "A simple framework for contrastive learning of visual representations",
            "venue": "International conference on machine learning, pages 1597\u20131607. PMLR.",
            "year": 2020
        },
        {
            "authors": [
                "Zihan Chen",
                "Hongbo Zhang",
                "Xiaoji Zhang",
                "Leqi Zhao."
            ],
            "title": "Quora question pairs",
            "venue": "URL https://www. kaggle. com/c/quora-question-pairs.",
            "year": 2018
        },
        {
            "authors": [
                "Mostafa Dehghani",
                "Stephan Gouws",
                "Oriol Vinyals",
                "Jakob Uszkoreit",
                "\u0141ukasz Kaiser."
            ],
            "title": "Universal transformers",
            "venue": "arXiv preprint arXiv:1807.03819.",
            "year": 2018
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
            "year": 2019
        },
        {
            "authors": [
                "William B. Dolan",
                "Chris Brockett."
            ],
            "title": "Automatically constructing a corpus of sentential paraphrases",
            "venue": "Proceedings of the Third International Workshop on Paraphrasing (IWP2005).",
            "year": 2005
        },
        {
            "authors": [
                "Angela Fan",
                "Edouard Grave",
                "Armand Joulin."
            ],
            "title": "Reducing transformer depth on demand with structured dropout",
            "venue": "arXiv preprint arXiv:1909.11556.",
            "year": 2019
        },
        {
            "authors": [
                "Hao Fu",
                "Shaojun Zhou",
                "Qihong Yang",
                "Junjie Tang",
                "Guiquan Liu",
                "Kaikui Liu",
                "Xiaolong Li."
            ],
            "title": "Lrc-bert: latent-representation contrastive knowledge distillation for natural language understanding",
            "venue": "Proceedings of the AAAI Conference on Artificial",
            "year": 2021
        },
        {
            "authors": [
                "Mitchell Gordon",
                "Kevin Duh",
                "Nicholas Andrews."
            ],
            "title": "Compressing BERT: Studying the effects of weight pruning on transfer learning",
            "venue": "Proceedings of the 5th Workshop on Representation Learning for NLP, pages 143\u2013155, Online. Association for Com-",
            "year": 2020
        },
        {
            "authors": [
                "Saurabh Goyal",
                "Anamitra Roy Choudhury",
                "Saurabh Raje",
                "Venkatesan Chakaravarthy",
                "Yogish Sabharwal",
                "Ashish Verma."
            ],
            "title": "Power-bert: Accelerating bert inference via progressive word-vector elimination",
            "venue": "International Conference on Machine Learn-",
            "year": 2020
        },
        {
            "authors": [
                "Yue Guan",
                "Zhengyi Li",
                "Jingwen Leng",
                "Zhouhan Lin",
                "Minyi Guo."
            ],
            "title": "Transkimmer: Transformer learns to layer-wise skim",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 7275\u2013",
            "year": 2022
        },
        {
            "authors": [
                "Shira Guskin",
                "Moshe Wasserblat",
                "Ke Ding",
                "Gyuwan Kim."
            ],
            "title": "Dynamic-tinybert: Boost tinybert\u2019s inference efficiency by dynamic sequence length",
            "venue": "arXiv preprint arXiv:2111.09645.",
            "year": 2021
        },
        {
            "authors": [
                "Shira Guskin",
                "Moshe Wasserblat",
                "Chang Wang",
                "Haihao Shen."
            ],
            "title": "Quala-minilm: a quantized length adaptive minilm",
            "venue": "arXiv preprint arXiv:2210.17114.",
            "year": 2022
        },
        {
            "authors": [
                "Yizeng Han",
                "Gao Huang",
                "Shiji Song",
                "Le Yang",
                "Honghui Wang",
                "Yulin Wang."
            ],
            "title": "Dynamic neural networks: A survey",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence.",
            "year": 2021
        },
        {
            "authors": [
                "Kaiming He",
                "Haoqi Fan",
                "Yuxin Wu",
                "Saining Xie",
                "Ross Girshick."
            ],
            "title": "Momentum contrast for unsupervised visual representation learning",
            "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 9729\u20139738.",
            "year": 2020
        },
        {
            "authors": [
                "Geoffrey Hinton",
                "Oriol Vinyals",
                "Jeff Dean."
            ],
            "title": "Distilling the knowledge in a neural network",
            "venue": "arXiv preprint arXiv:1503.02531.",
            "year": 2015
        },
        {
            "authors": [
                "Xiaoqi Jiao",
                "Yichun Yin",
                "Lifeng Shang",
                "Xin Jiang",
                "Xiao Chen",
                "Linlin Li",
                "Fang Wang",
                "Qun Liu."
            ],
            "title": "Tinybert: Distilling bert for natural language understanding",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4163\u20134174.",
            "year": 2020
        },
        {
            "authors": [
                "Xiaoqi Jiao",
                "Yichun Yin",
                "Lifeng Shang",
                "Xin Jiang",
                "Xiao Chen",
                "Linlin Li",
                "Fang Wang",
                "Qun Liu."
            ],
            "title": "TinyBERT: Distilling BERT for natural language understanding",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4163\u2013",
            "year": 2020
        },
        {
            "authors": [
                "Gyuwan Kim",
                "Kyunghyun Cho."
            ],
            "title": "Lengthadaptive transformer: Train once with length drop, use anytime with search",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Confer-",
            "year": 2021
        },
        {
            "authors": [
                "Sehoon Kim",
                "Sheng Shen",
                "David Thorsley",
                "Amir Gholami",
                "Woosuk Kwon",
                "Joseph Hassoun",
                "Kurt Keutzer."
            ],
            "title": "Learned token pruning for transformers",
            "venue": "arXiv preprint arXiv:2107.00910.",
            "year": 2021
        },
        {
            "authors": [
                "Zhenzhong Lan",
                "Mingda Chen",
                "Sebastian Goodman",
                "Kevin Gimpel",
                "Piyush Sharma",
                "Radu Soricut."
            ],
            "title": "Albert: A lite bert for self-supervised learning of language representations",
            "venue": "arXiv preprint arXiv:1909.11942.",
            "year": 2019
        },
        {
            "authors": [
                "Jianxin Liang",
                "Chang Liu",
                "Chongyang Tao",
                "Jiazhan Feng",
                "Dongyan Zhao."
            ],
            "title": "Attend, select and eliminate: Accelerating multi-turn response selection with dual-attention-based content elimination",
            "venue": "Findings of the Association for Computational",
            "year": 2023
        },
        {
            "authors": [
                "Chang Liu",
                "Chongyang Tao",
                "Jiazhan Feng",
                "Dongyan Zhao."
            ],
            "title": "Multi-granularity structural knowledge distillation for language model compression",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long",
            "year": 2022
        },
        {
            "authors": [
                "Chang Liu",
                "Chongyang Tao",
                "Jianxin Liang",
                "Tao Shen",
                "Jiazhan Feng",
                "Quzhe Huang",
                "Dongyan Zhao."
            ],
            "title": "Rethinking task-specific knowledge distillation: Contextualized corpus as better textbook",
            "venue": "Proceedings of the 2022 Conference on Empirical",
            "year": 2022
        },
        {
            "authors": [
                "Weijie Liu",
                "Peng Zhou",
                "Zhiruo Wang",
                "Zhe Zhao",
                "Haotang Deng",
                "Qi Ju."
            ],
            "title": "FastBERT: a selfdistilling BERT with adaptive inference time",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6035\u2013",
            "year": 2020
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Roberta: A robustly optimized bert pretraining approach",
            "venue": "arXiv preprint arXiv:1907.11692.",
            "year": 2019
        },
        {
            "authors": [
                "Zheng Liu",
                "Yingxia Shao."
            ],
            "title": "Retromae: Pretraining retrieval-oriented transformers via masked auto-encoder",
            "venue": "arXiv preprint arXiv:2205.12035.",
            "year": 2022
        },
        {
            "authors": [
                "I. Loshchilov",
                "F. Hutter."
            ],
            "title": "Fixing weight decay regularization in adam",
            "venue": "ArXiv, abs/1711.05101.",
            "year": 2017
        },
        {
            "authors": [
                "Ali Modarressi",
                "Hosein Mohebbi",
                "Mohammad Taher Pilehvar."
            ],
            "title": "AdapLeR: Speeding up inference by adaptive length reduction",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Pa-",
            "year": 2022
        },
        {
            "authors": [
                "Geondo Park",
                "Gyeongman Kim",
                "Eunho Yang."
            ],
            "title": "Distilling linguistic context for language model compression",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 364\u2013378, Online and Punta Cana, Dominican",
            "year": 2021
        },
        {
            "authors": [
                "Pranav Rajpurkar",
                "Jian Zhang",
                "Konstantin Lopyrev",
                "Percy Liang."
            ],
            "title": "SQuAD: 100,000+ questions for machine comprehension of text",
            "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383\u20132392, Austin,",
            "year": 2016
        },
        {
            "authors": [
                "Victor Sanh",
                "Lysandre Debut",
                "Julien Chaumond",
                "Thomas Wolf."
            ],
            "title": "Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter",
            "venue": "arXiv preprint arXiv:1910.01108.",
            "year": 2019
        },
        {
            "authors": [
                "Sheng Shen",
                "Zhen Dong",
                "Jiayu Ye",
                "Linjian Ma",
                "Zhewei Yao",
                "Amir Gholami",
                "Michael W Mahoney",
                "Kurt Keutzer."
            ],
            "title": "Q-bert: Hessian based ultra low precision quantization of bert",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, vol-",
            "year": 2020
        },
        {
            "authors": [
                "Richard Socher",
                "Alex Perelygin",
                "Jean Wu",
                "Jason Chuang",
                "Christopher D. Manning",
                "Andrew Ng",
                "Christopher Potts."
            ],
            "title": "Recursive deep models for semantic compositionality over a sentiment treebank",
            "venue": "Proceedings of the 2013 Conference on Empiri-",
            "year": 2013
        },
        {
            "authors": [
                "Siqi Sun",
                "Zhe Gan",
                "Yuwei Fang",
                "Yu Cheng",
                "Shuohang Wang",
                "Jingjing Liu."
            ],
            "title": "Contrastive distillation on intermediate representations for language model compression",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language",
            "year": 2020
        },
        {
            "authors": [
                "Chaofan Tao",
                "Lu Hou",
                "Wei Zhang",
                "Lifeng Shang",
                "Xin Jiang",
                "Qun Liu",
                "Ping Luo",
                "Ngai Wong."
            ],
            "title": "Compression of generative pre-trained language models via quantization",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational",
            "year": 2022
        },
        {
            "authors": [
                "Kaiser",
                "Illia Polosukhin"
            ],
            "title": "Attention is all",
            "year": 2017
        },
        {
            "authors": [
                "Furu Wei"
            ],
            "title": "MiniLMv2: Multi-head self",
            "year": 2021
        },
        {
            "authors": [
                "Jimmy Lin"
            ],
            "title": "DeeBERT: Dynamic early exiting",
            "year": 2020
        },
        {
            "authors": [
                "Jiahui Yu",
                "Thomas S Huang."
            ],
            "title": "Universally slimmable networks and improved training techniques",
            "venue": "Proceedings of the IEEE/CVF international conference on computer vision, pages 1803\u2013 1811.",
            "year": 2019
        },
        {
            "authors": [
                "Ofir Zafrir",
                "Guy Boudoukh",
                "Peter Izsak",
                "Moshe Wasserblat."
            ],
            "title": "Q8bert: Quantized 8bit bert",
            "venue": "2019 Fifth Workshop on Energy Efficient Machine Learning and Cognitive Computing-NeurIPS Edition (EMC2-NIPS), pages 36\u201339. IEEE.",
            "year": 2019
        },
        {
            "authors": [
                "Wangchunshu Zhou",
                "Canwen Xu",
                "Tao Ge",
                "Julian McAuley",
                "Ke Xu",
                "Furu Wei."
            ],
            "title": "Bert loses patience: Fast and robust inference with early exit",
            "venue": "Advances in Neural Information Processing Systems, 33:18330\u201318341.",
            "year": 2020
        },
        {
            "authors": [
                "Yukun Zhu",
                "Ryan Kiros",
                "Rich Zemel",
                "Ruslan Salakhutdinov",
                "Raquel Urtasun",
                "Antonio Torralba",
                "Sanja Fidler."
            ],
            "title": "Aligning books and movies: Towards story-like visual explanations by watching movies and reading books",
            "venue": "Proceedings of the IEEE in-",
            "year": 2015
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "With the rapid progress of pre-trained language models (PLMs) (Devlin et al., 2019; Liu et al., 2019; Brown et al., 2020), dramatic improvement has been achieved in a wide range of natural language understanding and generation tasks. Despite the remarkable performance those PLMs have\n\u2217 Corresponding author: Dongyan Zhao.\nachieved, they generally suffer from high computation overhead, which prevents them from being deployed into resource-scarce applications. Hence, there is an urgent need to accelerate inference speed while minimizing performance degradation.\nTo achieve this goal, great efforts have been made, among which model compression and dynamic computation are two major lines of research with different mechanisms. Model compression approaches typically convert the large model into a smaller one with reduced layer number and hidden dimension, while the computation graph of the compressed small model is fixed during inference. One of the effective approaches to achieve this goal is knowledge distillation from a large teacher model to a small student model (Jiao et al., 2020a; Wang et al., 2021; Liu et al., 2022a). As for dynamic computation methods, they don\u2019t change the architecture of the given language model and dynamically prune its computation graph during inference instead. There are two perspectives in dynamic computation for transformer-based (Vaswani et al., 2017) language models: depth-level early exiting (Liu et al., 2020; Xin et al., 2020; Zhou et al., 2020) and width-level token pruning (Goyal et al., 2020; Kim and Cho, 2021; Kim et al., 2021). We focus on token pruning methods in this paper.\nSince these two groups of methods achieve inference acceleration from two orthogonal perspectives, a natural idea is to combine them together to obtain faster inference speed. The simplest way to combine them is the pipeline approach (Kim and Cho, 2021; Guskin et al., 2021, 2022): preparing a compressed small language model first and applying dynamic computation algorithm upon it next. Though proved effective, we argue that the pipeline approach is sub-optimal since model compression\nand dynamic computation procedures are individually designed and may not be compatible with each other that much. As a result, the potential of such an approach has not been fully exploited.\nTo address this issue and achieve better speed-performance trade-off, we propose a novel knowledge distillation framework named LengthAdaptive Distillation (abbr. LAD) that transfers the knowledge from the large language model to a small language model while helping the small model to get adapted to dynamic token pruning. Drawing inspirations from Jiao et al. (2020b), we adopt a two-stage distillation paradigm where a general-purpose small language model is first distilled on large-scale general corpora and then taskspecific knowledge is injected on task-specific datasets. In general distillation, we mainly consider two issues: (1) how to transfer high-quality general knowledge from the teacher and (2) how to customize the student model for dynamic token pruning. For (1), we propose to transfer tokenlevel knowledge of two types: hidden representations that encode the contextual semantic information of tokens and attention dependency that is generally adopted as the indication of token importance (Goyal et al., 2020; Kim and Cho, 2021) for token pruning. The alignment of hidden representations between the teacher and the student is achieved by contrastive distillation and the matching of attention dependency is fulfilled by mean square error. While for (2), we employ a teacher with an unpruned computation graph to teach a student with dynamic token pruning where the importance of tokens is measured by the attention scores, and the retention configuration is sampled from a uniform distribution. As only the unpruned tokens have their representations in the student\u2019s last layer, we train the student by enforcing the remaining token representations to be close to the teacher\u2019s corresponding representations as well as to reconstruct the complete token representations of the teacher\u2019s. Then in task-specific distillation, we first employ data augmentation to enlarge the task datasets, then distill the student following the dynamic token pruning setting used in general distillation but transfer more task-relevant knowledge by enforcing the sentence embedding of the student to be close to the teacher via contrastive distillation.\nWe conduct experiments on GLUE benchmark (Wang et al., 2018). We first prove that our knowledge distillation method outperforms ad-\nvanced knowledge distillation methods on a standard setting (i.e., without token pruning). Moreover, we prove that with the help of the customized length-adaptive distillation, our methods successfully take advantage of both model compression and dynamic computation, and achieve dramatically better speed-performance trade-off compared with the pipeline approach. We release our implementation to facilitate future research1.\nTo sum up, our contributions are three folds: \u2022 We propose a two-stage knowledge distillation framework LAD that effectively combines model compression and dynamic computation to achieve faster speedup in inference. \u2022 We customize a small language model for dynamic token pruning by cast a length-adaptive setting on the student and enforcing it to mimic and reconstruct the teacher\u2019s representations. \u2022 We conduct comprehensive experiments on the GLUE benchmark and verify that our method can achieve a superior speed-performance trade-off compared with other methods."
        },
        {
            "heading": "2 Related Work",
            "text": ""
        },
        {
            "heading": "2.1 Language Model Compression",
            "text": "There are various methods to compress a large language model into a small one including pruning (Fan et al., 2019; Gordon et al., 2020), quantization (Zafrir et al., 2019; Shen et al., 2020; Bai et al., 2021), weight sharing (Dehghani et al., 2018; Lan et al., 2019), knowledge distillation (Hinton et al., 2015; Sanh et al., 2019; Jiao et al., 2020b) and so on. We focus on knowledge distillation in this paper, where a large model acts as the teacher and transfers its knowledge to a smaller student model. Jiao et al. (2020b) proposed general-thentask-specific distillation framework and transferred the knowledge in hidden states, attention matrices, and output logits in different distillation stages. Wang et al. (2020b, 2021) matched the attention dependencies derived from the query, the key and the value vectors in the self-attention module. Sun et al. (2020); Fu et al. (2021) employed contrastive distillation to match the hidden states. Park et al. (2021); Liu et al. (2022a) structured the knowledge as the relations of hidden states and enforced the relations between the teacher and the student to be consistent. Compared with existing works, our method not only addresses the issue of improving the performance of small language models with\n1https://github.com/EMNLP-LAD/LAD\nfixed computation graphs as they did, but also endows the small language model with good adaptation ability for dynamic token pruning to achieve improved inference efficiency."
        },
        {
            "heading": "2.2 Dynamic Computation",
            "text": "Different from language model compression methods that produce a small model with fixed computation graph, dynamic computation approaches fix the model architecture and achieve inference acceleration by dynamically pruning its computation graph (Han et al., 2021). Liu et al. (2020); Xin et al. (2020); Zhou et al. (2020) studied depth-level early exiting where samples with different difficulty are output from different layers (i.e., the more difficult the sample is, the more layers it would go through before getting its final output). While (Goyal et al., 2020; Kim and Cho, 2021; Kim et al., 2021; Ye et al., 2021; Guan et al., 2022; Modarressi et al., 2022; Liang et al., 2023) proposed widthlevel token pruning where unimportant tokens are progressively removed as the calculation goes from shallow to deep layers. Among the two lines of research, we focus on token pruning approaches. Recently, Guskin et al. (2021, 2022) explored the combination of dynamic token pruning and model compression to make more aggressive acceleration by simply applying token pruning upon a given compressed language model in a pipeline manner. Different from these works, we highlight the importance the adaptation from a fixed computation graph to a dynamic one for small language models, and propose a knowledge distillation framework to fulfill this goal."
        },
        {
            "heading": "3 Methodology",
            "text": "Our framework adopts a two-stage knowledge distillation paradigm (Jiao et al., 2020b) where a randomly initialized small language model is first distilled with general knowledge, and is then taught with task-specific knowledge. Along with the distillation process, the student not only absorbs the knowledge transferred from the teacher, but also gets adapted to dynamic token pruning. After the two distillation stages, we employ evolutionary search (Cai et al., 2019; Wang et al., 2020a; Kim and Cho, 2021) to obtain a set of length configurations for token pruning with different speedups. In the following subsections, we first introduce our knowledge distillation algorithms in general and task-specific distillation stages, then demonstrate\nhow to apply token pruning upon our model."
        },
        {
            "heading": "3.1 General Distillation",
            "text": "General distillation aims to transfer the general knowledge held in a pre-trained language model (i.e., RoBERTabase) to a randomly initialized small language model that has fewer layers and hidden dimensions with large-scale open-domain corpora (i.e., Wikipedia). Existing works (Sanh et al., 2019; Jiao et al., 2020b; Sun et al., 2020; Wang et al., 2021) have designed various effective algorithms by considering different knowledge sources, knowledge types, distance metrics, etc. However, they all focus on improving the evaluation performance of the student model in downstream tasks with fixed computation graphs. When directly adapting existing models to dynamic token pruning (Kim and Cho, 2021), we find that these models get poor performance when we adopt an aggressive pruning ratio. Therefore, to effectively combine model compression and dynamic token pruning to achieve a better speed-performance trade-off, we propose a novel distillation algorithm that not only transfers high-quality knowledge to the student but also helps the student to get adapted to dynamic token pruning, especially to a high-pruning extent. We first introduce what form of knowledge we prepare to transfer and how to transfer, then we present how to effectively get the student adapted to dynamic token pruning."
        },
        {
            "heading": "3.1.1 Knowledge Transfer",
            "text": "One of the vital problems in knowledge distillation is knowledge transfer. In transformer distillation, hidden representations (Jiao et al., 2020b), attention dependencies (Wang et al., 2020b), relations among representations (Park et al., 2021) all have been considered as useful knowledge to be transferred. Drawing on existing works, we first consider the hidden representations as the fundamental knowledge that mainly guides the learning of a student. Besides, considering that a majority of dynamic token pruning approaches (Goyal et al., 2020; Kim and Cho, 2021; Kim et al., 2021) have demonstrated that the attention scores derived from attention matrices of self-attention mechanism can act as a good indicator for token importance. Therefore, to produce a customized student model for token pruning, we additionally involve attention dependencies as the knowledge to be transferred.\nTo elaborate on our design in knowledge transfer, we first briefly describe the calculations of the\ntransformer model together with some necessary notations. Formally, given a sequence of input text, a sub-word tokenizer first splits it into n tokens x = [w1, w2, . . . , wn]. Then through an embedding layer, each token in x is converted as a dense vector through a lookup table, resulting in H0 = [h01,h 0 2, . . . ,h 0 n] where h 0 i \u2208 Rd. Then, the output of the embedding layer is passed to L stacked transformer (Vaswani et al., 2017) layers which are mainly comprised of a multi-head selfattention module and a position-wise feed-forward network. The self-attention module produces the attention matrices Al \u2208 Rh\u00d7n\u00d7n that encodes the dependencies among the input tokens using h attention heads. The output of the l-th layer is denoted as H l = [hl1,h l 2, . . . ,h l n].\nWith these notations, we now go back to the knowledge distillation setting. Given a teacher model with Lt layers and a student model with Ls layers, we feed the same text into them and can obtain the corresponding output hidden states {H lt}Ltl=0, {H l s}Lsl=0 and attention matrices {Alt}Ltl=1, {A l s}Lsl=1. We suppose the student\u2019s ls-th layer is aligned with the teacher\u2019s lt-th layer, then the outputs of the student (i.e., H lss and A ls s ) should be close to the teacher\u2019s (i.e., H ltt and A lt t ). For aligning hidden states, instead of aligning sentencelevel representations (Sun et al., 2020), we assume that token-level representations are more suitable for general distillation since they contain more fine-\ngrained knowledge. Drawing inspirations from contrastive learning (Chen et al., 2020; He et al., 2020; Tian et al., 2019), we design a contrastive objective to achieve this goal. For a hidden representation of the student hlss,i, we first conduct linear projection to get zlss,i \u2208 Rd that have the same hidden dimensions as the teacher model. The positive representation that it is enforced to match is the corresponding teacher\u2019s representation hltt,i, while there remain multiple choices for the negatives. Different from previous works that utilize the representations of tokens in the same input of text as negatives (Tao et al., 2022), we found that simply using randomly sampled token representations performs better. As demonstrated by previous works that contrastive learning requires a large number of negatives, we employ a memory queue (He et al., 2020) M = {hltt,j}mj=1 with memory size m to avoid the unaffordable memory cost brought by in-batch negatives with enlarged batch size (Chen et al., 2020). Then the token-level contrastive distillation objective for x is formulated as: Jhid = \u2212 n\u2211\ni=1\nlog exp(s(zlss,i,h lt t,i)/\u03c4)\u2211\nh lt t,j\u2208M\nexp(s(zlss,i,h lt t,j)/\u03c4)\n,\n(1) where \u03c4 is the temperature and s(\u00b7) denotes cosine similarity. Different from He et al. (2020) that using momentum encoder to make the stored representations stable and consistent during training,\nwe find that directly using the fixed teacher\u2019s representations without the help of momentum encoder yield better performance as the representations are naturally consistent. While for aligning attention dependencies, we follow Jiao et al. (2020b) to optimize the mean square error (MSE) between the attention matrices of the teacher and the student:\nJatt = \u2212MSE(Alss ,A lt t ). (2)\nThe overall objective for knowledge transfer is:\nJ (H lss ,Alss ,H lt t ,A lt t ) =Jhid(H lss ,H lt t )\n+Jatt(Alss ,Altt ). (3)"
        },
        {
            "heading": "3.1.2 Length-Adaptive Distillation",
            "text": "Based on the design of knowledge transfer, we then focus on the second goal: how to effectively get the student accustomed to token pruning while transferring general knowledge. We achieve this goal by employing a teacher without token pruning to teach the student with dynamic token pruning.\nToken Pruning In the standard setting of dynamic token pruning, there are two major issues: how to measure the importance of tokens in each layer and how to decide the number of tokens maintained in each layer. We follow the typical solution where the importance measurement is the sum of attention scores a token received from other tokens within a sentence (Goyal et al., 2020) and the length configuration for token pruning is obtained by sampling from a pre-defined range in training and using evolutionary search (Cai et al., 2019; Wang et al., 2020a; Kim and Cho, 2021) in inference. Given an input sequence, we first sample a length configuration N = [n1, n2, . . . , nLs ] for the student with ratio r with the sampling strategy proposed by Kim and Cho (2021) where nl denotes the number of tokens maintained in the l-th layer of the student. Hereby, the output of the student on its top layer only contains nLs token representations H\u0302Lss = [h\u0302 Ls z1 , h\u0302 Ls z2 , . . . , h\u0302 Ls znLs ] and the corresponding attention matrices A\u0302Lss formed by them, where Z = {z1, z2, . . . , znLs} is the original indices of the remaining tokens. Based on Z , we can also extract the corresponding teacher\u2019s hidden states in the teacher\u2019s lt-th layer H\u0302 ltt from H lt t and then calculate the attention matrices A\u0302ltt with them.\nTraining Based on the pruned student\u2019s representations, we propose two objectives that aim to get the student accustomed to token pruning. The\nfirst one encourages the remaining representations of the student to recover the representations of the whole input. To achieve this goal, we first construct the masked input which is the concatenation of two parts: (1) the remaining representations in the student\u2019s top layer after pruning, and (2) the query of the pruned representations constructed by adding the positional embedding of the pruned positions and the embedding of the mask token [M] token. The masked input is linearly projected to fit the hidden dimensions of the teacher. Then, we need to reconstruct the representations based on the masked input through a decoder. Instead of initializing a new decoder to predict the pruned tokens (Liu and Shao, 2022), we propose to take advantage of the teacher model by borrowing its last transformer layer as the decoder. This design not only enjoys the good ability of representation learning of the pre-trained teacher but also reduces the number of parameters that need to be optimized. The borrowed decoder is frozen during the training of the student. We denote the reconstructed outputs as H\u0303s and A\u0303s, and form the reconstruction loss as:\nLrec = J (H\u0303s, A\u0303s,HLtt ,A Lt t ) (4)\nThe second one is to align the remaining representations of the student with the corresponding ones of the teacher. Notice that in Eq. 3.1.2 we feed the representations of the Ls-th layer of the student to the last layer of the teacher, implicitly aligning the student\u2019s Ls-th layer with the Lt\u22121-th layer. Therefore, we form the alignment loss as:\nLalign = J (H\u0302Lss , A\u0302Lss , H\u0302Lt\u22121t , A\u0302 Lt\u22121 t ). (5)\nIn addition to the two objectives that aim to get the student accustomed to token pruning, we also transfer the knowledge without token pruning to stabilize training:\nLori = J (HLss ,ALss ,HLt\u22121t ,A Lt\u22121 t ). (6)\nThe overall objective for general distillation is the weighted sum of the three objectives:\nLGD = \u03bb1Lori + \u03bb2Lalign + \u03bb3Lrec. (7)\nWe name the model produced by general distillation as LADGD."
        },
        {
            "heading": "3.2 Task-Specific Distillation",
            "text": "Based on the model LADGD produced by general distillation, we inject task-specific knowledge with\nthe task-specific teacher, while further improving the customization for dynamic token pruning under the prediction mode of the task (e.g., classification). This distillation stage is on the training set of a downstream task. As the labeled task data is usually far less than general corpora, we follow Jiao et al. (2020b); Liu et al. (2022b) to conduct data augmentation (DA) to enlarge the task data. We focus on classification tasks in this paper, therefore in task-specific distillation, we focus on transferring the knowledge held in the start token (e.g., <s> in RoBERTa) instead of considering all tokens. Similar to general distillation, we sample the length configuration for token pruning with the same ratio r as general distillation for the student while keeping the teacher unpruned. The (projected) representations of the start token of the student and the teacher are denoted as z\u0302Lss,0 and h Lt t,0. We employ contrastive distillation to transfer the knowledge with the help of the memory queue:\nLTD = \u2212 log exp(s(z\u0302Lss,0,h Lt t,0)/\u03c4)\u2211\nhj\u2208M exp(s(z\u0302 Ls s,0,hj)/\u03c4)\n. (8)\nAfter distillation with Eq. 3.2, we fine-tune the model to finally adapted it to downstream tasks. We denote the model produced by task-specific distillation as LADTD or LADTD w/ DA depending on whether the training set of the task is augmented."
        },
        {
            "heading": "3.3 Configuration Search for Token Pruning",
            "text": "Given a small student model produced by our two-stage knowledge distillation framework, acceleration through dynamic token pruning can be achieved given a length configuration N = [n1, n2, . . . , nLs ] where nl denotes the number of remained tokens in layer l. Naturally, there is a trade-off between accuracy and efficiency with various choices of N and we need to find a series of optimal length configurations to facilitate various application scenarios. We achieve this goal by conducting evolutionary search (Cai et al., 2019) following Kim and Cho (2021). Specifically, we first initialize the candidate set of length configurations by random sampling, then iteratively construct new configurations by mutation and crossover operations and update the candidate set if some newly constructed configurations achieve better accuracy-efficiency trade-off. After searching, we obtain a set of optimal length configurations S = {N \u22171 ,N \u22172 , . . . ,N \u2217m} that lies in the accuracy-efficiency Pareto frontier. In our experi-\nments, we manually select one length configuration that achieves good accuracy as well as considerable speed-up from S and report the results."
        },
        {
            "heading": "4 Experiment",
            "text": ""
        },
        {
            "heading": "4.1 Datasets and Metrics",
            "text": "We conduct experiments on 8 generally adopted tasks from GLUE benchmark (Wang et al., 2018) following previous work (Jiao et al., 2020b; Wang et al., 2021), including 2 single sentence tasks:SST2 (Socher et al., 2013), CoLA (Warstadt et al., 2019), and 6 text pair tasks: MNLI (Williams et al., 2018), QNLI (Rajpurkar et al., 2016), RTE (Bentivogli et al., 2009), MRPC (Dolan and Brockett, 2005), STS-B (Cer et al., 2017), QQP (Chen et al., 2018). As for performance metrics, we report Spearman\u2019s rank correlation coefficient (Spear) on STS-B, Matthews correlation coefficient (Mcc) on CoLA, and accuracy (Acc) on the other 5 tasks. To evaluate the inference efficiency, we use FLOPs as the metric and employ torchprofile 2 as the tool to calculate following Kim and Cho (2021)."
        },
        {
            "heading": "4.2 Implementation Details",
            "text": "We adopt RoBERTa-base (Liu et al., 2019) as the teacher model and utilize a small transformer model with 6 layers, 384 hidden dimensions and 12 attention heads as the student following Wang et al. (2021). We conduct two-stage knowledge distillation. In general distillation, we prepare English Wikipedia and BookCorpus (Zhu et al., 2015) as the training corpora and set the max sequence length as 128. We set the size of memory queue m = 16384, the temperature \u03c4 = 0.07, the weights for different loss terms \u03bb1 = 1.0, \u03bb2 = \u03bb3 = 0.5. We randomly sample the token pruning ratio r from {0.1, 0.2, . . . , 0.7} for each training instance for length-adaptive distillation. We use AdamW (Loshchilov and Hutter, 2017) as the optimizer and train the student model with a batch size of 256, the learning rate as 6e-4, the maximum training steps as 400k, and the warmup ratio as 0.01. In task-specific distillation, we prepare two types of datasets: the original training set of each task, and their augmentation version where each sample is augmented using contextual world replacement with the same data augmentation setting of Jiao et al. (2020b). The hyperparameters m, \u03c4 and r are the same as general distillation. When using the original training sets, we train the student with a batch size of 32, the learning rate\n2https://github.com/zhijian-liu/torchprofile.\nas 3e-5, the maximum training epochs as 50 for CoLA and 20 for other tasks, and the warmup ratio as 0.1. When on the augmented training sets, the batch size is 256, the learning rate is 1e-4, the warmup ratio is 0.06, and the maximum training epochs are the same as on the original training sets. In the following fine-tuning stage, we choose the learning rate from {1e-6, 2e-6, 3e-6} and the batch size from {16, 32}. For dynamic token pruning, we use the same configuration of evolutionary search as Kim and Cho (2021)."
        },
        {
            "heading": "4.3 Baseline Methods",
            "text": "Baselines for General Distillation The first group of baselines is pure model compression methods using advanced knowledge distillation techniques. We implement TinyBERT (Jiao et al., 2020b) and MiniLMv2 (Wang et al., 2021), two representative knowledge distillation methods, under the same distillation setting (i.e., model size, training data, optimization hyperparameters, etc.) as ours. We compare two perspectives with these baselines: (1) the standard evaluation of model performance (i.e., fine-tuning on each task and testing without token pruning) and (2) the speedperformance trade-off under dynamic token pruning without any types of adaptation.\nBaselines for Task-Specific Distillation The second type of baseline is the adaptation ap-\nproach which adapts a given language model to dynamic token pruning. Following pipeline approaches (Guskin et al., 2021, 2022) that apply dynamic token pruning upon compressed small language models, we adopt the sandwich rule and inplace distillation (Yu and Huang, 2019) used in LAT (Kim and Cho, 2021) as the baseline (denoted as Ada) to be compared with our task-specific distillation method."
        },
        {
            "heading": "4.4 Overall Performance",
            "text": "We provide the overall evaluation results of the performance and the speedup of our method and baselines in Table 1. First, it can be observed from the second block that under standard evaluation setting (i.e., without token pruning), our model with general distillation (i.e., LADGD) consistently outperforms advanced general distillation methods on all tasks. With task-specific distillation and data augmentation, the performance can be further improved. Then we compare the performance of generally distilled models under the same token pruning setting in the third block. We find that LADGD can still outperform baselines on both performance and inference speedup under the selected pruning configurations. Finally, we compare the performance of our task-specific distillation method with existing pipeline approaches with adaptation to token pruning in the bottom block. It can be seen that LADTD not only achieves better performance\ncompared with LADGD but also outperforms baselines. With data augmentation, we achieve dramatically better performance, where we achieve 6.1\u00d7 speedup while improving the performance by 2.1% on average compared with static inference without token pruning using LADGD."
        },
        {
            "heading": "4.5 Discussions",
            "text": "There are several critical designs in our framework. We analyze these designs by drawing speedperformance curves under different settings on four representative tasks. We choose two low-resource tasks MRPC and RTE, one moderate-resource task QNLI, and one high-resource task MNLI.\nGeneral Distillation In general distillation, we introduce three objectives Lori, Lalign and Lrec weighted by \u03bb1, \u03bb2, \u03bb3 to jointly transfer general knowledge and get the student accustomed to token pruning. We first study the influence of different choices of these weights and find that setting \u03bb1 = \u03bb2 + \u03bb3 and \u03bb2 = \u03bb3 yield the best performance. Furthermore, we study the effectiveness of these objectives and plot Figure 2. Here we focus on the curves corresponding to the top 5 labels on the legend. Among these models, there are two models that are not obtained from specifically designed training for token pruning: MiniLMv2 and LADGD w/o align,rec. Compared with these two baselines, we find the introduction of Lalign and Lrec both improve the performance under token pruning and the combination of them performs better, verifying that our proposed length-adaptive distillation effectively help the student get accustomed to token pruning. It can also be observed that the improvement of our general distillation method over baselines under token pruning is more considerable on low-resource tasks (i.e., MRPC and RTE). The reason lies in that the fine-tuning steps on high-resource tasks are much more than on low-resource tasks, weakening the fitness for token\nFLOPs\n60\n70\n80\n90\nAc cu\nra cy\nMRPC\nLAD_TD, r=0.3 LAD_TD, r=0.5 LAD_TD, r=0.7 LAD_TD w/ DA, r=0.3 LAD_TD w/ DA, r=0.5 LAD_TD w/ DA, r=0.7\nFigure 3: The speed-performance curves of different pruning ratio on MRPC.\npruning learned by our general distillation. This issue can be well addressed by our task-specific distillation algorithm discussed in the following.\nTask-Specific Distillation Based on the generally distilled model, we further conduct taskspecific distillation on the training sets of downstream tasks with or without data augmentation (DA). From Figure 2 we find that task-specific distillation brings substantial improvement for all the tasks to a different extent. For low-resource tasks (i.e., MRPC and RTE), although LADTD significantly outperforms the baselines trained with no specification for token pruning (i.e., MiniLMv2 and LADGD w/o align,rec), it makes a slight improvement over LADGD due to limited task data. Hence with data augmentation, LADTD w/ DA performs dramatically better than LADTD. While for tasks with moderate or abundant amounts of instances (i.e., QNLI and MNLI), we find that LADTD brings considerable improvement and data augmentation is sort of the icing on the cake. These findings verify the effectiveness of task-specific distillation and prove that data augmentation is a solution to the data scarcity issue on low-resource tasks.\nThe Choice of Pruning Ratio Recall that in both general and task-specific distillation, the student is applied with token pruning with ratio r. To explore the effect of the ratio as well as to find the best choice, we adopt r \u2208 {0.3, 0.5, 0.7} in both general and task-specific distillation on MRPC. It can be observed from Figure 3 that the larger r is, the better speed-performance trade-off can be achieved whether the training set is augmented or not. More surprisingly, we also find that training with a larger pruning ratio brings consistent improvement at all speedups, which indicates that token pruning can be considered a kind of regularization in distillation that helps the student learn better."
        },
        {
            "heading": "5 Conclusion",
            "text": "In this paper, we propose a two-stage knowledge distillation framework LAD that transfers general and task-specific knowledge to the student while helping the student to get adapted to dynamic token pruning. We conduct comprehensive experiments on the GLUE benchmark. The evaluation results prove that our method can effectively take advantage of model compression and dynamic computation and achieve a superior speed-performance trade-off for inference acceleration.\nLimitations\nWe achieve superior speed-performance trade-off in inference acceleration by a two-stage knowledge distillation framework. In the first general distillation stage, in order to jointly transfer the general knowledge and get the student accustomed to dynamic token pruning, we introduce two calculation branches for the student. This design implies that the student needs to do two forward passes in one training iteration, increasing the computation overhead in training. In the future, we plan to explore how to unify the two computation branches to improve training efficiency."
        },
        {
            "heading": "Acknowledgements",
            "text": "We would like to thank the anonymous reviewers for their constructive comments. This work was supported by the National Key Research and Development Program of China (No. 2020AAA0106600).\nEthical Statement\nIn this paper, we propose a novel knowledge distillation framework that transfers knowledge to the\nstudent while customizing the student for dynamic token pruning. Our method doesn\u2019t introduce ethical issues. The datasets we used are publicly available and don\u2019t have any privacy issues."
        }
    ],
    "title": "Length-Adaptive Distillation: Customizing Small Language Model for Dynamic Token Pruning",
    "year": 2023
}