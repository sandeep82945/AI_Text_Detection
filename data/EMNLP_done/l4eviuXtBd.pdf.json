{
    "abstractText": "Pre-trained language models (LMs) have brought remarkable performance to numerous NLP tasks. However, they require significant resources and entail high computational costs for inference, making it challenging to deploy them in real-world and real-time systems. Existing early exiting methods aim to reduce computational complexity by selecting the layer at which to exit, but suffer from the limitation that they have to sequentially traverse through all layers prior to the selected exit layer, which lacks flexibility and degrades their performance. To solve this problem, we propose a homotopic and adaptive layer skipping fine-tuning method named HadSkip. HadSkip adaptively selects the layers to skip based on a predefined budget. Specifically, we introduce a learnable gate before each layer of the LM to determine whether the current layer should be skipped. To tackle various challenges in training brought by discrete gates and budget constraints, we propose a fine-grained initialization strategy and homotopic optimization strategy. We conduct extensive experiments on the GLUE benchmark, and experimental results demonstrate the proposed HadSkip outperforms all state-of-the-art baselines significantly.",
    "authors": [
        {
            "affiliations": [],
            "name": "Haoyu Wang"
        },
        {
            "affiliations": [],
            "name": "Yaqing Wang"
        },
        {
            "affiliations": [],
            "name": "Tianci Liu"
        },
        {
            "affiliations": [],
            "name": "Tuo Zhao"
        },
        {
            "affiliations": [],
            "name": "Jing Gao"
        }
    ],
    "id": "SP:cfc44cb06f819f12512147ed0e23c29d3ca12ff7",
    "references": [
        {
            "authors": [
                "Haoli Bai",
                "Wei Zhang",
                "Lu Hou",
                "Lifeng Shang",
                "Jing Jin",
                "Xin Jiang",
                "Qun Liu",
                "Michael Lyu",
                "Irwin King."
            ],
            "title": "Binarybert: Pushing the limit of bert quantization",
            "venue": "arXiv preprint arXiv:2012.15701.",
            "year": 2020
        },
        {
            "authors": [
                "Yoshua Bengio",
                "Nicholas L\u00e9onard",
                "Aaron Courville."
            ],
            "title": "Estimating or propagating gradients through stochastic neurons for conditional computation",
            "venue": "arXiv preprint arXiv:1308.3432.",
            "year": 2013
        },
        {
            "authors": [
                "Tianlong Chen",
                "Jonathan Frankle",
                "Shiyu Chang",
                "Sijia Liu",
                "Yang Zhang",
                "Zhangyang Wang",
                "Michael Carbin."
            ],
            "title": "The lottery ticket hypothesis for pretrained bert networks",
            "venue": "Advances in neural information processing systems, 33:15834\u201315846.",
            "year": 2020
        },
        {
            "authors": [
                "Yoni Choukroun",
                "Eli Kravchik",
                "Fan Yang",
                "Pavel Kisilev."
            ],
            "title": "Low-bit quantization of neural networks for efficient inference",
            "venue": "2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW), pages 3009\u20133018. IEEE.",
            "year": 2019
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "arXiv preprint arXiv:1810.04805.",
            "year": 2018
        },
        {
            "authors": [
                "Angela Fan",
                "Pierre Stock",
                "Benjamin Graham",
                "Edouard Grave",
                "R\u00e9mi Gribonval",
                "Herve Jegou",
                "Armand Joulin."
            ],
            "title": "Training with quantization noise for extreme model compression",
            "venue": "arXiv preprint arXiv:2004.07320.",
            "year": 2020
        },
        {
            "authors": [
                "Jonathan Frankle",
                "Michael Carbin."
            ],
            "title": "The lottery ticket hypothesis: Finding sparse, trainable neural networks",
            "venue": "arXiv preprint arXiv:1803.03635.",
            "year": 2018
        },
        {
            "authors": [
                "Zhengjie Gao",
                "Ao Feng",
                "Xinyu Song",
                "Xi Wu."
            ],
            "title": "Target-dependent sentiment classification with bert",
            "venue": "Ieee Access, 7:154290\u2013154299.",
            "year": 2019
        },
        {
            "authors": [
                "Alex Graves."
            ],
            "title": "Adaptive computation time for recurrent neural networks",
            "venue": "arXiv preprint arXiv:1603.08983.",
            "year": 2016
        },
        {
            "authors": [
                "Pengcheng He",
                "Xiaodong Liu",
                "Jianfeng Gao",
                "Weizhu Chen."
            ],
            "title": "Deberta: Decoding-enhanced bert with disentangled attention",
            "venue": "arXiv preprint arXiv:2006.03654.",
            "year": 2020
        },
        {
            "authors": [
                "Geoffrey Hinton",
                "Oriol Vinyals",
                "Jeff Dean."
            ],
            "title": "Distilling the knowledge in a neural network",
            "venue": "arXiv preprint arXiv:1503.02531.",
            "year": 2015
        },
        {
            "authors": [
                "Boren Hu",
                "Yun Zhu",
                "Jiacheng Li",
                "Siliang Tang."
            ],
            "title": "Smartbert: A promotion of dynamic early exiting mechanism for accelerating bert inference",
            "venue": "arXiv preprint arXiv:2303.09266.",
            "year": 2023
        },
        {
            "authors": [
                "Nanjiang Jiang",
                "Marie-Catherine de Marneffe."
            ],
            "title": "Evaluating bert for natural language inference: A case study on the commitmentbank",
            "venue": "Proceedings of the 2019 conference on empirical methods in natural language processing and the 9th interna-",
            "year": 2019
        },
        {
            "authors": [
                "Xiaoqi Jiao",
                "Yichun Yin",
                "Lifeng Shang",
                "Xin Jiang",
                "Xiao Chen",
                "Linlin Li",
                "Fang Wang",
                "Qun Liu."
            ],
            "title": "Tinybert: Distilling bert for natural language understanding",
            "venue": "arXiv preprint arXiv:1909.10351.",
            "year": 2019
        },
        {
            "authors": [
                "Jing Jin",
                "Cai Liang",
                "Tiancheng Wu",
                "Liqin Zou",
                "Zhiliang Gan."
            ],
            "title": "Kdlsq-bert: A quantized bert combining knowledge distillation with learned step size quantization",
            "venue": "arXiv preprint arXiv:2101.05938.",
            "year": 2021
        },
        {
            "authors": [
                "Yigitcan Kaya",
                "Sanghyun Hong",
                "Tudor Dumitras."
            ],
            "title": "Shallow-deep networks: Understanding and mitigating network overthinking",
            "venue": "International conference on machine learning, pages 3301\u20133310. PMLR.",
            "year": 2019
        },
        {
            "authors": [
                "Sehoon Kim",
                "Amir Gholami",
                "Zhewei Yao",
                "Michael W Mahoney",
                "Kurt Keutzer."
            ],
            "title": "I-bert: Integeronly bert quantization",
            "venue": "International conference on machine learning, pages 5506\u20135518. PMLR.",
            "year": 2021
        },
        {
            "authors": [
                "Dongjun Lee",
                "Sohee Yang",
                "Minjeong Kim."
            ],
            "title": "Claf: Open-source clova language framework",
            "venue": "https://github.com/naver/claf.",
            "year": 2019
        },
        {
            "authors": [
                "Xiaonan Li",
                "Yunfan Shao",
                "Tianxiang Sun",
                "Hang Yan",
                "Xipeng Qiu",
                "Xuanjing Huang."
            ],
            "title": "Accelerating bert inference for sequence labeling via early-exit",
            "venue": "arXiv preprint arXiv:2105.13878.",
            "year": 2021
        },
        {
            "authors": [
                "Chen Liang",
                "Haoming Jiang",
                "Zheng Li",
                "Xianfeng Tang",
                "Bin Yin",
                "Tuo Zhao."
            ],
            "title": "Homodistil: Homotopic task-agnostic distillation of pre-trained transformers",
            "venue": "arXiv preprint arXiv:2302.09632.",
            "year": 2023
        },
        {
            "authors": [
                "Liyuan Liu",
                "Chengyu Dong",
                "Xiaodong Liu",
                "Bin Yu",
                "Jianfeng Gao."
            ],
            "title": "Bridging discrete and backpropagation: Straight-through and beyond",
            "venue": "arXiv preprint arXiv:2304.08612.",
            "year": 2023
        },
        {
            "authors": [
                "Weijie Liu",
                "Peng Zhou",
                "Zhe Zhao",
                "Zhiruo Wang",
                "Haotang Deng",
                "Qi Ju."
            ],
            "title": "Fastbert: a selfdistilling bert with adaptive inference time",
            "venue": "arXiv preprint arXiv:2004.02178.",
            "year": 2020
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Roberta: A robustly optimized bert pretraining approach",
            "venue": "arXiv preprint arXiv:1907.11692.",
            "year": 2019
        },
        {
            "authors": [
                "Zejian Liu",
                "Fanrong Li",
                "Gang Li",
                "Jian Cheng."
            ],
            "title": "Ebert: Efficient bert inference with dynamic structured pruning",
            "venue": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 4814\u20134823.",
            "year": 2021
        },
        {
            "authors": [
                "Victor Sanh",
                "Lysandre Debut",
                "Julien Chaumond",
                "Thomas Wolf."
            ],
            "title": "Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter",
            "venue": "arXiv preprint arXiv:1910.01108.",
            "year": 2019
        },
        {
            "authors": [
                "Siqi Sun",
                "Yu Cheng",
                "Zhe Gan",
                "Jingjing Liu."
            ],
            "title": "Patient knowledge distillation for bert model compression",
            "venue": "arXiv preprint arXiv:1908.09355.",
            "year": 2019
        },
        {
            "authors": [
                "Zhiqing Sun",
                "Hongkun Yu",
                "Xiaodan Song",
                "Renjie Liu",
                "Yiming Yang",
                "Denny Zhou."
            ],
            "title": "Mobilebert: a compact task-agnostic bert for resource-limited devices",
            "venue": "arXiv preprint arXiv:2004.02984.",
            "year": 2020
        },
        {
            "authors": [
                "Hanlin Tang",
                "Xipeng Zhang",
                "Kai Liu",
                "Jianchen Zhu",
                "Zhanhui Kang."
            ],
            "title": "Mkq-bert: Quantized bert with 4-bits weights and activations",
            "venue": "arXiv preprint arXiv:2203.13483.",
            "year": 2022
        },
        {
            "authors": [
                "Surat Teerapittayanon",
                "Bradley McDanel",
                "HsiangTsung Kung."
            ],
            "title": "Branchynet: Fast inference via early exiting from deep neural networks",
            "venue": "2016 23rd International Conference on Pattern Recognition (ICPR), pages 2464\u20132469. IEEE.",
            "year": 2016
        },
        {
            "authors": [
                "Jiayi Tian",
                "Chao Fang",
                "Haonan Wang",
                "Zhongfeng Wang."
            ],
            "title": "Bebert: Efficient and robust binary ensemble bert",
            "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 1\u20135. IEEE.",
            "year": 2023
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin."
            ],
            "title": "Attention is all you need",
            "venue": "Advances in neural information processing systems, 30.",
            "year": 2017
        },
        {
            "authors": [
                "Haoyu Wang",
                "Fenglong Ma",
                "Yaqing Wang",
                "Jing Gao."
            ],
            "title": "Knowledge-guided paraphrase identification",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2021, pages 843\u2013853.",
            "year": 2021
        },
        {
            "authors": [
                "Zuxuan Wu",
                "Tushar Nagarajan",
                "Abhishek Kumar",
                "Steven Rennie",
                "Larry S Davis",
                "Kristen Grauman",
                "Rogerio Feris."
            ],
            "title": "Blockdrop: Dynamic inference paths in residual networks",
            "venue": "Proceedings of the IEEE conference on computer vision and pattern",
            "year": 2018
        },
        {
            "authors": [
                "Guangxuan Xiao",
                "Ji Lin",
                "Mickael Seznec",
                "Julien Demouth",
                "Song Han."
            ],
            "title": "Smoothquant: Accurate and efficient post-training quantization for large language models",
            "venue": "arXiv preprint arXiv:2211.10438.",
            "year": 2022
        },
        {
            "authors": [
                "Ji Xin",
                "Raphael Tang",
                "Jaejun Lee",
                "Yaoliang Yu",
                "Jimmy Lin."
            ],
            "title": "Deebert: Dynamic early exiting for accelerating bert inference",
            "venue": "arXiv preprint arXiv:2004.12993.",
            "year": 2020
        },
        {
            "authors": [
                "Ji Xin",
                "Raphael Tang",
                "Yaoliang Yu",
                "Jimmy Lin."
            ],
            "title": "Berxit: Early exiting for bert with better finetuning and extension to regression",
            "venue": "Proceedings of the 16th conference of the European chapter of the association for computational linguistics: Main",
            "year": 2021
        },
        {
            "authors": [
                "Ofir Zafrir",
                "Guy Boudoukh",
                "Peter Izsak",
                "Moshe Wasserblat."
            ],
            "title": "Q8bert: Quantized 8bit bert",
            "venue": "2019 Fifth Workshop on Energy Efficient Machine Learning and Cognitive Computing-NeurIPS Edition (EMC2-NIPS), pages 36\u201339. IEEE.",
            "year": 2019
        },
        {
            "authors": [
                "Wei Zhang",
                "Lu Hou",
                "Yichun Yin",
                "Lifeng Shang",
                "Xiao Chen",
                "Xin Jiang",
                "Qun Liu."
            ],
            "title": "Ternarybert: Distillation-aware ultra-low bit bert",
            "venue": "arXiv preprint arXiv:2009.12812.",
            "year": 2020
        },
        {
            "authors": [
                "Zhen Zhang",
                "Wei Zhu",
                "Jinfan Zhang",
                "Peng Wang",
                "Rize Jin",
                "Tae-Sun Chung."
            ],
            "title": "Pcee-bert: Accelerating bert inference via patient and confident early exiting",
            "venue": "Findings of the Association for Computational Linguistics: NAACL 2022, pages 327\u2013338.",
            "year": 2022
        },
        {
            "authors": [
                "Wangchunshu Zhou",
                "Canwen Xu",
                "Tao Ge",
                "Julian McAuley",
                "Ke Xu",
                "Furu Wei."
            ],
            "title": "Bert loses patience: Fast and robust inference with early exit",
            "venue": "Advances in Neural Information Processing Systems, 33:18330\u201318341.",
            "year": 2020
        },
        {
            "authors": [
                "Chenzhuo Zhu",
                "Song Han",
                "Huizi Mao",
                "William J Dally."
            ],
            "title": "Trained ternary quantization",
            "venue": "arXiv preprint arXiv:1612.01064.",
            "year": 2016
        },
        {
            "authors": [
                "Michael Zhu",
                "Suyog Gupta."
            ],
            "title": "To prune, or not to prune: exploring the efficacy of pruning for model compression",
            "venue": "arXiv preprint arXiv:1710.01878.",
            "year": 2017
        },
        {
            "authors": [
                "Wei Zhu."
            ],
            "title": "Leebert: Learned early exit for bert with cross-level optimization",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Vol-",
            "year": 2021
        },
        {
            "authors": [
                "2020 Fan et al",
                "2021 Jin et al",
                "Zhu"
            ],
            "title": "2016), pruning (Zhu and Gupta, 2017), and knowledge distillation",
            "venue": "(Hinton et al.,",
            "year": 2015
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Pre-trained language models (LMs), such as BERT (Devlin et al., 2018), RoBERTa (Liu et al., 2019), and DeBERTa (He et al., 2020), have significantly improved performance across various natural language processing (NLP) tasks, including paraphrase identification (Wang et al., 2021), natural language inference (Jiang and de Marneffe, 2019), sentiment classification (Gao et al., 2019), and so on. Pre-trained LMs are typically built by stacking transformer (Vaswani et al., 2017) layers or their variants, where the self-attention mechanism within each layer often exhibits high computational complexity. Consequently, the inference complexity of pre-trained LMs has been a bottle-\nneck for deploying them in latency-sensitive or latency-constrained scenarios (Xin et al., 2020).\nTo comply with the demand for inferenceefficient models, several approaches have been proposed to accelerate pre-trained LM inference. These methods include weight quantization (Zafrir et al., 2019; Kim et al., 2021), pruning (Liu et al., 2021), knowledge distillation (Sanh et al., 2019; Jiao et al., 2019), and early exiting (Xin et al., 2020; Zhou et al., 2020; Xin et al., 2021). Among them, early exiting methods become more appealing as they do not rely on specific hardware support, such as custom chips (Liu et al., 2023) or GPUs, and they do not require training an efficient model from scratch on large pre-training corpora, thus providing a more cost-effective solution. Early exiting methods attach a classifier to each transformer layer and aim to determine if the model should exit (stop inference) from the current layer based on a criterion or learned decision. Both the backbone model and the classifiers are jointly trained in the finetuning stage.\nAlthough early exiting methods enable the selection of a layer to exit based on the input sequence, they still require the input to pass through the transformer layers sequentially before exiting. This prevents the model from choosing desired uncontiguous layers. For example, with a 2-layer budget, for some input sequences, choosing the second and fifth layers of the model could achieve the most superior performance. However, early exiting methods can only traverse the layers sequentially and exit at most at the second layer, thereby missing out on the optimal combination of the second and fifth layers and potentially leading to degraded model accuracy as a result of suboptimal layer selection.\nTo address this issue, we propose a homotopic and adaptive layer skipping fine-tuning method named HadSkip. The proposed HadSkip selects which layers can be skipped based on the difficulty of input sequences and pre-defined budget.\nSpecifically, we introduce a binary learnable gate before each transformer layer of the LM to control whether the model can skip the current layer. This allows the model to use different gate values for different input sequences, satisfying the inference budget requirements. However, learning binary gates is a non-trivial task. The binary gates are discrete and it is difficult to optimize by gradient-based optimizers directly. Besides, the presence of a budget constraint can lead to early optimization collapse, particularly when the budget value is small. To tackle them, we design an elaborated three-stage training strategy to tackle this challenge. The first stage finetunes the backbone model and uses it to initialize the transformer layers. Next, these transformer layers are frozen and the gate parameters are initialized. In the third stage, the transformer layers and gates are trained jointly. To compute the gradient of gates, we employ a second-order gradient approximation method ReinMax (Liu et al., 2023). Further, we propose a homotopic optimization strategy, which converts the optimization process into a sequence of problems ranging from a large budget to a small budget. On one hand, this ensures a smoother and more stable optimization process. On the other hand, it avoids skipping a large number of layers at the beginning of training and losing valuable information from the pre-trained model.\nThe contributions of the paper are summarized as follows: 1) We propose a homotopic and adaptive layer skipping fine-tuning method to accelerate the inference of pre-trained LMs. It is more flexible and economical compared to existing LM inference acceleration methods. 2) We design a three-stage and homotopic optimization strategy. This strategy makes the optimization more stable and easier to solve. 3) We conduct extensive experiments on the GLUE benchmark. Results show that the proposed HadSkip outperforms baselines significantly and achieves a good efficiency-accuracy trade-off, e.g., preserving 95.7% of BERT\u2019s performance with only half the layers on average."
        },
        {
            "heading": "2 Related Work",
            "text": "Existing approaches for accelerating the inference of pre-trained language models can be broadly categorized into two types: 1) model compressionbased methods and 2) early exiting based methods. The proposed method is more relevant to early exiting based methods, which are discussed in more detail as follows. The discussion about the model compression-based methods can be found in the\nappendix. Early exiting methods enable the production of inference-efficient models for specific inference budgets during the fine-tuning stage. These methods draw inspiration from adaptive computation (Graves, 2016) techniques in recurrent neural networks and BranchyNet (Teerapittayanon et al., 2016) in computer vision. The fundamental concept behind early exiting methods is to identify the layer at which to exit early, rather than sequentially passing through all layers. Based on the criteria of choosing the exiting layer, they can be organized into three strategies. The first is scorebased early exiting. (Liu et al., 2020; Xin et al., 2020; Kaya et al., 2019) use the entropy of the prediction probability and the maximum of the predicted distribution as the score for exit determination respectively. The second is learning to exit. BERxiT (Xin et al., 2021) used a fully connected layer right after each layer to produce decisions if the model needs to exit from the current layer. The third is patience-based early exiting. For example, PABEE (Zhou et al., 2020), SENTEE (Li et al., 2021) and LeeBERT (Zhu, 2021) were designed to use scores of multiple layers to determine if the model could exit. Recently, PCEE-BERT (Zhang et al., 2022) also combined the score-based method with the patience-based early exiting method to improve accuracy. Despite these advancements, early exiting methods still require sequential traversal through multiple layers. However, for many input sequences, the optimal layers for inference could be non-contiguous. This lack of flexibility in early exiting methods can limit their performance. To address this limitation, inspired by BlockDrop (Wu et al., 2018), we propose a homotopic and adaptive layer skipping fine-tuning method named HadSkip, which can dynamically select which layers can be skipped based on the difficulty of input sequences and the pre-defined budget. Concurrently, SmartBERT (Hu et al., 2023) proposes to combine layer skipping and early exiting based on cross-layer contrastive learning."
        },
        {
            "heading": "3 Preliminaries",
            "text": "In this section, we first introduce the architecture of the pre-trained language model (PLM) and analyze its complexity for inference."
        },
        {
            "heading": "3.1 Pre-trained Language Model",
            "text": "A pre-trained language model usually consists of embedding layers, multiple transformer layers, and an output layer. Specifically, for an input sequence\nx, we obtain its corresponding embedding first:\ne = etoken(x) + epos(x) + eseg(x),\nwhere etoken, epos, and eseg are the token embedding layer, position embedding layer and segment embedding layer respectively. Then the embedding e will be fed into transformer layers, which can be formulated as h0 = e and\nhi = fi(hi\u22121; \u03b8i), i = 1, 2, ..., L, (1)\nwhere hi, \u03b8i, and fi are the output, parameters, and mapping function of the i-th transformer layer respectively. In the end, the output of the last transformer layer passes the output layer and generates logits, which can be represented as o = fout(hL). 3.2 Inference Complexity The inference complexity consists of the complexity of three parts: 1) embedding layers, 2) transformer layers, and 3) the output layer. The time complexity of both the embedding layers and the output layer is approximatelyO(d3). The complexity of transformer layers is O(4md2L + m2dL), where m represents the sequence length, d denotes the embedding dimensionality, and L signifies the number of transformer layers. Based on the analysis, it is evident that the complexity of transformer layers significantly exceeds that of the embedding layers and the output layer. Hence, this paper proposes to skip some transformer layers to reduce inference complexity, leveraging the expected number of transformer layers as a budget."
        },
        {
            "heading": "4 Methodology",
            "text": "Given an L-layer pre-trained language model (PLM) and a training set D = {(xi, yi)}ni=1, where xi is the input sequence, yi is the corresponding label and n is the number of training data, the aim is to train a model f(xi)\u2192 yi which can be efficient for inference while preserving the model performance."
        },
        {
            "heading": "4.1 Overview",
            "text": "To enhance model inference efficiency, we propose a homotopic and adaptive layer skipping finetuning method named HadSkip, which is shown in Fig. 1. The motivation behind HadSkip is to reduce the complexity of transformer layers by adaptively skipping certain layers based on the input sequence. We introduce gated transformer layers in which a binary gate is incorporated into each transformer layer to determine whether the current layer should be skipped (Section 4.2). Additionally, to address the non-differentiable nature of the binary gates and\nmeet the budget contraint, we propose a three-stage optimization strategy in Section 4.3."
        },
        {
            "heading": "4.2 Gated Transformer Layer",
            "text": "Based on the analysis in Section 3.2, we identify that the transformer layers are the bottleneck in terms of inference complexity. Since the difficulty of input sequences can vary due to factors such as vocabulary, sequence length, and rhetoric, it is unnecessary to utilize all L transformer layers for every input sequence. Using a smaller number of transformer layers may already yield accurate prediction results for certain simple sequences. Therefore, motivated by (Wu et al., 2018), we propose a gated transformer layer that adaptively selects layers to enhance inference efficiency.\nSpecifically, we introduce a binary gate before feeding input into each transformer layer. This gate determines whether the current transformer layer should be bypassed based on the output of the previous layer. Formally, the forward propagation can be represented as\nh0 = e,hi = fi(hi\u22121; \u03b8i) \u00b7 gi + hi\u22121 \u00b7 (1\u2212 gi), where gi = B(g(hi\u22121;\u03c9i)) \u2208 {0, 1} is the binary gate, B(\u00b7) : R \u2192 {0, 1} is a binarized function, g(\u00b7) is a one hidden layer feed-forward network, and \u03c9i is the gate parameter. If the value of gate gi is 1, then hi = fi(hi\u22121; \u03b8i), indicating that the current i-th transformer layer is preserved; otherwise, it is skipped."
        },
        {
            "heading": "4.3 Optimization Strategy",
            "text": "Considering a language model composed of stacked gated transformer layers, we need to impose a constraint on the maximum number of used layers, which should be approximately equal to a specified budget. The optimization problem can be formally defined as follows:\nmin \u0398\n1\nn \u2211 j \u2113(f(xj ; \u0398))\ns.t.E[gi] \u2248 1\nnL n\u2211 j=1 L\u2211 i=1 gji = s, (2)\nwhere \u0398 is the collection of model parameters consisting of transformer layer parameters, gate parameters, embedding layer parameters and output layer parameters, gji is the gate of i-th transformer layer with respect to input sequence xj , \u2113(\u00b7) is the loss function, the specific form of which will be demonstrated in Section 4.3 in detail, and s is the pre-given budget. The mean square error is employed as a penalty term in the problem in Eqn. 2,\nresulting in the following formulation:\nmin \u0398\n1 n \u2211 j \u2113(f(xj ; \u0398)) + \u03b2 ( 1 nL n\u2211 j=1 L\u2211 i=1 gji \u2212 s )2 , (3)\nwhere \u03b2 is a hyper-parameter. However, because of the non-differentiability of gji , the optimization process can easily converge to a suboptimal solution, making it challenging to solve directly. Additionally, due to the budget constraint, the model might prioritize skipping a significant number of layers first to meet the constraint. This behavior may lead to unstable optimization, and a loss of knowledge learned during the pretraining phase, which is usually stored in the first several layers of the model. To handle this problem, we propose a three-stage optimization strategy. In the initial two stages, we aim to find a good initialization for the HadSkip, while in the third stage, we propose the utilization of a homotopic optimization method, which enables learning of the gate values. The subsequent sections will provide an introduction to each of these three stages. \u2022 Stage I: Initialize Transformer Layer Parameters. Since transformer layer parameters are realvalued, they are generally easier to optimize compared to gate parameters. The discrepancy in optimization difficulty poses a challenge for model convergence. Motivated by this observation, we propose separate initialization design for transformer layer parameters and for gate parameters respectively. This section focuses on the initialization of transformer layer parameters. Specifically, we restrict our consideration to a vanilla language model fv(\u00b7) comprising transformer layers in order to avoid non-differentiable operations. The training process for fv involves standard fine-tuning of a language model, which can be formulated as\nmin \u0398v\n1\nn \u2211 j \u2113t(fv(x j ; \u0398v), y j), (4)\nwhere \u2113t(\u00b7, \u00b7) denotes the task-specific loss function, such as cross-entropy for classification, and \u0398v represents the parameters of fv, comprising transformer layer parameters, embedding layer parameters, and output layer parameters. The learned \u0398v is utilized to initialize the corresponding parameters of the model f , denoted as \u0398\\{\u03c9i}Li=1 \u2190 \u0398v. \u2022 Stage II: Initialize Gate Parameters. Following the initialization of transformer layer parameters, we freeze the parameters \u0398\\{\u03c9i}Li=1 and relax the original optimization problem in Eqn. 2 to achieve a coarse-grained initialization for gate parameters.\nThe expectation constraint is omitted and we solve the following optimization problem:\nmin {\u03c9i}Li=1\n1\nn \u2211 j \u2113(f(xj ; \u0398)).\nTo handle the non-differentiable problem, previous work usually leverages straight-through estimation (STE) (Bengio et al., 2013) to obtain approximate gradients. However, we observe that STE, although a first-order approximation (Liu et al., 2023), still presents challenges in stable backpropagation and may result in model collapse. To address this issue, we employ a second-order gradient approximation method called ReinMax (Liu et al., 2023). It integrates Heun\u2019s Method to approximate the gradient of B(\u00b7), which is shown in Algorithm 1 in the Appendix. \u2022 Stage III: Train LM and Gate Jointly. Following the completion of the first two stages, the proposed model has achieved a favorable initialization, and the gradient estimation is stable. However, in cases where the budget is limited and requires the LM to skip multiple layers, the substantial difference in model capacity leads to significant performance degradation. To alleviate this problem, we propose a homotopic optimization strategy that incorporates knowledge distillation. The knowledge distillation borrows knowledge from a finetuned vanilla LM to guide the proposed HadSkip learning via regularizing the consistency between their hidden representation and output predictions. Additionally, the homotopic optimization strategy utilizes a greedy method to facilitate smoother optimization. \u2022 Knowledge Distillation. We incorporate the following distillation losses into consideration: 1) the distillation loss of prediction Lp, 2) the distillation loss of hidden representations Lh, 3) the distillation loss of attention Latt, and 4) the distillation loss of embedding layer Lemb. Further elaboration on the distillation losses and the total loss can be found in Section B in the appendix. \u2022 Homotopic Optimization Strategy. While leveraging initialization and knowledge distillation can enhance the performance of HadSkip, optimizing the model remains challenging due to the significant capacity and representation power disparity between f and fv. To address this issue, we propose a homotopic optimization strategy that transforms the optimization into a sequence of problems, gradually progressing from easier to more challenging ones. Formally, we solve a series of optimiza-\ntion problems by replacing s with k in Eqn. 3 for k = s0, s1, . . . , sq, where s0 > s1 > . . . > sq = s. Hence, the result of the last optimization becomes the initialization for the subsequent optimization problem. Here, we observe that Eqn. 3 is not highly sensitive to the choice of \u03b2. Therefore, we employ a uniform \u03b2 value across all optimization problems. Additionally, we find seeting s0 = L, s1 = L \u2212 1, . . . , sq = s is a simple yet effective choice."
        },
        {
            "heading": "5 Experiment",
            "text": "In this section, we evaluate the proposed HadSkip and answer the following questions: RQ1) How does HadSkip perform compared to state-of-theart early exiting baselines? RQ2) What are the roles of initialization, knowledge distillation, and homotopic optimization strategy in model performance improvements respectively? RQ3) Can the proposed HadSkip generalize well with respect to different backbones? RQ4) Is it possible to combine the proposed HadSkip method with other model acceleration techniques? RQ5) How does the performance change with varying budget values? RQ6) Can the proposed HadSkip capture the difficulty differences among input sequences?"
        },
        {
            "heading": "5.1 Datasets and Experiment Settings",
            "text": "\u2022 Datasets Experiments are conducted on the benchmark GLUE dataset following the methodology of (Zhang et al., 2022; Sun et al., 2019). Specifically, we evaluate our approach on seven\nclassification tasks from the GLUE benchmark, namely MRPC, SST-2, RTE, QQP, QNLI, MNLI, and CoLA, which use F1, Accuracy, Accuracy, F1, Accuracy, Accuracy, and Matthews correlation coefficient as metrics respectively (the higher the better). More details about the datasets and evaluation metrics are provided in Table 5 in the Appendix. The performance is reported on the development sets, following the approach of (Zhang et al., 2022; Sun et al., 2019).\n\u2022 Baselines We adopt two types of baselines: 1) vanilla pre-trained language model BERT (Devlin et al., 2018), and 2) early exiting methods, including budgeted exiting (Zhang et al., 2022), BranchyNet (Teerapittayanon et al., 2016), Shallow-Deep (Kaya et al., 2019), BERxiT (Xin et al., 2021), PABEE (Sun et al., 2019), and PCEEBERT (Zhang et al., 2022). Since BERT does not have an inference speedup design, it serves as the performance ceiling for inference speedup methods. Budgeted exiting uses top K layers to make predictions based on the budget. BranchyNet, ShallowDeep, BERxiT, PABEE, and PCEE-BERT dynamically select a layer to exit according to the input sequence. Following (Zhang et al., 2022), we make early exiting baselines and proposed HadSkip expectedly use 3, 6, and 9 layers as budgets in experiments. Unless otherwise specified, all these methods utilize BERT as the backbone."
        },
        {
            "heading": "5.2 Performance Comparison",
            "text": "This section presents the performance of the baselines and the proposed HadSkip model, as shown in Table 1, addressing RQ1.\nThe performance results in Table 1 demonstrate that the proposed HadSkip surpasses all state-ofthe-art baselines on the GLUE datasets. Notably, under budget configurations of 3, 6, and 9 layers, the proposed HadSkip achieves substantial improvements in terms of average accuracy. Specifically, compared to the baselines, HadSkip exhibits performance gains of at least 10.7%, 4.8%, and 0.9% when using 3, 6, and 9 layers, respectively. Additionally, when compared to vanilla BERT, the proposed HadSkip retains 89.3%, 95.7%, and 99.3% of its performance when using 3, 6, and 9 layers, respectively. This demonstrates that HadSkip effectively accelerates model inference while preserving the performance of the pre-trained language model.\nWe also observe that as the budget decreases, the diversity in model performance becomes more pronounced. Specifically, when using 9 layers, the performance of the baselines, apart from the proposed HadSkip, exhibits minimal variation. However, with 3 and 6 layers, the performance gaps among different methods become more prominent. The proposed HadSkip demonstrates much more significant improvements under these two settings.\nSpecifically, on the CoLA dataset, the baselines struggle to provide accurate classification results when using 3 and 6 layers, while the proposed HadSkip remains effective. This indicates that the proposed HadSkip exhibits greater superiority over the baselines when operating under a smaller budget. Hence, in scenarios with limited resources, selecting layers adaptively may be more effective than choosing the exit layer."
        },
        {
            "heading": "5.3 Ablation Study",
            "text": "\u2022 Effectiveness of Gate Initialization. We conduct an analysis to answer RQ2 regarding gate initialization. To validate the role of the gate initialization, we compare HadSkip without the second stage to the complete HadSkip architecture. The comparison results are presented in Table 2. The experimental findings confirm the positive impact of gate initialization on model training. We observe that gate initialization provides greater performance improvement when the budget is small. However, for larger budgets, the benefits of gate initialization are limited. This can be attributed to the fact that with a larger budget, the model only needs to skip a few layers, which is relatively easier compared to skipping multiple layers. In such\ncases, a fine-grained warm-up strategy becomes unnecessary.\n\u2022 Effectiveness of Homotopic Optimization Strategy. We conduct an ablation study to answer RQ2 with respect to the homotopic optimization strategy. We compare the proposed HadSkip with HadSkip without the homotopic optimization strategy on the MRPC, MNLI, and CoLA datasets, as presented in Table 2. From the results in Table 2, we observe the effectiveness of the proposed homotopic optimization strategy in improving model performance. The homotopic optimization strategy leads to average improvements of 201.6%, 10.5%, and 4.2% when using 3, 6, and 9 layers, respectively. This trend highlights the greater importance of the homotopic optimization strategy for smaller budgets. When using 9 layers, the performance of HadSkip with and without the homotopic optimization strategy is similar for the MRPC and MNLI datasets. However, when using 3 layers, HadSkip without the homotopic optimization strategy fails to produce meaningful output for CoLA, whereas HadSkip still delivers competitive results. One potential reason for this difference is that the homotopic optimization strategy smooths the optimization problem and facilitates convergence to a better local optimum. \u2022 Effectiveness of Knowledge Distillation. In this paragraph, we do the ablation study to answer\nRQ2 regarding knowledge distillation. We conduct two comparison experiments: 1) HadSkip without knowledge distillation (KD) versus HadSkip, and 2) HadSkip using only prediction-based knowledge distillation versus HadSkip. The experimental results are presented in Table 2. Prediction-based knowledge distillation is widely employed in traditional teacher-student architectures, and the knowledge distillation loss function used in HadSkip is popular for compressing large language models. Compared to HadSkip without KD, HadSkip consistently improves model performance. For instance, HadSkip achieves improvements of 98.9%, 6.3%, and 1.2% when using 3, 6, and 9 layers, respectively. Compared to HadSkip with only prediction KD, HadSkip still shows improvements. This demonstrates that the losses of hidden representations, attention matrices, and embeddings effectively guide the model in learning how to select layers to skip. However, from Table 2, we observe that the performance improvements resulting from the distillation loss of prediction are greater than those from other distillation losses. This highlights the more significant role of the distillation loss of prediction in the proposed HadSkip."
        },
        {
            "heading": "5.4 Extension to RoBERTa",
            "text": "In this section, we employ RoBERTa (Liu et al., 2019) as the backbone to investigate RQ3. The results are presented in Table 3. We also consider DeeBERT, which utilizes RoBERTa as the backbone in their paper, as our baseline. As shown in Table 3, HadSkip+RoBERTa outperforms DeeBERT+RoBERTa. Specifically, for CoLA and MNLI, DeeBERT+RoBERTa fails to make correct predictions when using 3 and 6 layers. In contrast, the proposed HadSkip+RoBERTa achieves comparable performance to RoBERTa even with only 6 layers. These results demonstrate that HadSkip can effectively balance efficiency and accuracy when combined with different backbone models."
        },
        {
            "heading": "5.5 Combining with TinyBERT",
            "text": "In this section, we investigate RQ4 using TinyBERT (Jiao et al., 2019) as the backbone. Apart from early exiting methods, another popular approach to accelerate model inference is reducing the number of LM layers via knowledge distillation for LM compression. To demonstrate the complementarity of the proposed HadSkip with knowledge distillation model compression methods, we utilize a 6-layer TinyBERT, which compresses BERT using knowledge distillation into a\n6-layer small model, as the backbone and present the performance results in Table 4. Comparing HadSkip with a budget of 4 layers to TinyBERT (6 layers), we observe that HadSkip can preserve over 94.6% of the model performance. This indicates that HadSkip can effectively complement TinyBERT and further enhance the inference speed of TinyBERT. Moreover, when compared to the pretrained 4-layer TinyBERT, HadSkip with 4 layers achieves higher accuracy, and even HadSkip with 3 layers demonstrates similar performance. These results suggest that combining HadSkip with a compressed model may yield greater effectiveness than training a smaller compression model."
        },
        {
            "heading": "5.6 Sensitivity w.r.t. Budgets",
            "text": "We perform a hyperparameter experiment to address RQ5, focusing on different budgets for HadSkip using BERT-Large, BERT-base, and RoBERTa as backbones. BERT-Large is a 24-layer language model, while BERT-base and RoBERTa consist of 12 layers each. The results are presented in Fig. 6. It is evident from the figure that larger budgets lead to better performance. However, when the budget is less than half the total number of layers, the decrease in accuracy is significantly more pronounced compared to cases where the budget exceeds half the total number of layers. Specifically, as shown in Fig. 6, using half the total number of layers still preserves over 97% of the model\u2019s performance. This finding suggests that it is preferable to skip fewer than half the total number of layers."
        },
        {
            "heading": "5.7 Case Study",
            "text": "In this section, we conduct a case study to answer RQ6. To study if the proposed HadSkip can adaptively select layers based on input sequences, we visualize 1) the probability of each layer being used and 2) the probability of the number of layers being used on MNI, QNLI and SST2 datasets. We\nshow the visualization on Fig. 2 to Fig. 5. From Fig. 2 to Fig. 4, we observe diverse probabilities of each layer being utilized across different tasks and budgets. When utilizing 3 layers, the model tends to prefer specific layers. For example, the model favors the second and fifth layers on the MNLI dataset and the second, third, and seventh layers on the QNLI dataset. However, as the budget increases, the distribution of chosen layers becomes more uniform. According to Fig. 5, the number of layers utilized concentrates around the given budget, while the model also employs other numbers of layers near the budget. These figures confirm that the proposed model can select different layers for various input sequences."
        },
        {
            "heading": "6 Conclusion",
            "text": "In this paper, we propose a homotopic and adaptive layer skipping fine-tuning method named HadSkip for efficient inference. We introduce a learnable gate before each transformer layer of the pretrained language model to determine whether the model can skip the respective layer. For training the binary gate, we introduce a three-stage learning strategy to initialize and update model parameters. Additionally, we propose a homotopic optimization strategy to stabilize the model training. We also utilize knowledge distillation to guide model training and achieve an improved trade-off between efficiency and accuracy. We conducted extensive experiments on the GLUE benchmark, and the results demonstrate that HadSkip outperforms state-of-theart baselines. Moreover, HadSkip complements other acceleration methods, including TinyBERT.\nLimitations The proposed HadSkip introduces multiple hyperparameters including \u03b2, which might require additional effort to tune. Fortunately, we observe that the prediction performance is not sensitive to these hyperparameters, and therefore they can be easily\ntuned. In this paper, we set them to fixed values (e.g., one) and achieve good results.\nEthics Statement This paper proposes a homotopic and adaptive layer skipping fine-tuning method, called HadSkip. We show that the proposed HadSkip can be used for inference acceleration.We perform experiments on classification tasks using the GLUE benchmark. In all experiments, we utilize public benchmark datasets, models, and code. No ethical concerns were identified."
        },
        {
            "heading": "Acknowledgement",
            "text": "This work is supported in part by the US National Science Foundation under grant NSF IIS-1747614 and NSF IIS-2141037. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation."
        },
        {
            "heading": "A Related Work",
            "text": "Model Compression-based Methods. There are a number of well-explored model compression methods which can be used for accelerating model inference, such as weight quantization (Choukroun et al., 2019; Fan et al., 2020; Jin et al., 2021; Zhu et al., 2016), pruning (Zhu and Gupta, 2017), and knowledge distillation (Hinton et al., 2015). Weight quantization involves mapping model weights to low-precision integers and floating-point numbers, making them more hardware-friendly for computation. Specifically, (Xiao et al., 2022) proposed 8-bit quantization for BERT; (Tang et al., 2022) explored how to use 4 bits to quantize BERT; (Bai et al., 2020; Tian et al., 2023; Zhang et al., 2020) studied how to quantize BERT into 1-bit or 2-bits. Pruning, on the other hand, focuses on setting redundant parameters to zero to create a sparse network, enabling accelerated sparse matrix operations on specific hardware platforms. Existing pruning methods such as (Liu et al., 2021) and (Chen et al., 2020) proposed a dynamic structured pruning method and a lottery ticket hypothesis (Frankle and Carbin, 2018) based method respectively to learn a sparse network for BERT. Knowledge distillation, meanwhile, is to utilize a powerful large model (teacher model) to guide the learning of a lightweight model (student model). The lightweight student model usually has lower inference complexity compared to the teacher model. DistilBERT (Sanh et al., 2019), TinyBERT (Jiao et al., 2019), MobileBERT (Sun et al., 2020) and PKD (Sun et al., 2019) used knowledge distillation to learn a lightweight BERT. However, these compression-based methods have drawbacks. Firstly, they often require training compressed models from scratch, which can be computationally expensive. Secondly, weight quantization or pruning techniques rely on specialized hardware support, limiting their flexibility."
        },
        {
            "heading": "B Knowledge Distillation",
            "text": "Given a fine-tuned LM, i.e. fv(\u00b7) learned in the first stage, knowledge distillation is to enable the proposed f(\u00b7) to mimic the prediction and hidden representation of fv. Specifically, we consider the following losses: 1) The distillation loss of prediction is defined as\nLp = 1\nn \u2211 j DKL(fv(xi), f(xi)), (5)\nwhere DKL(\u00b7, \u00b7) is the KL divergence between the probability over the two outputs. The loss Lp regu-\nlarizes the prediction of HadSkip to be consistent with that of fv. 2) The distillation loss of hidden representations is to enables the hidden representations of f and fv to be similar, which can be formulated as\nLh = 1\nnL n\u2211 j=1 L\u2211 i=1 MSE(hji , vh j i ), (6)\nwhere hji , vh j i are the i-th layer hidden representation of f and fv with respect to input sequence xj respectively. 3) Similarly, the distillation loss of attention is to penalize the discrepancy between attention matrices of f and fv, which can be represented as\nLatt = 1\nnL n\u2211 j=1 L\u2211 i=1 MSE(Aji , vA j i ), (7)\nwhere Aji and vA j i are the i-th layer averaged attention matrices of f and fv with respect to input sequence xj respectively. 4) The distillation loss of embedding layer is\nLemb = 1\nn \u2211 j MSE(ej , ejv), (8)\nwhere ej and ejv are the embedding layer outputs of f and fv with respect to input sequence xj . In summary, the total loss function is obtained by combining it with the task-specific loss, which can be formulated as\nL =1 n \u2211 j \u2113(f(xj ; \u0398))\n=Lt + \u03b11Lp + \u03b12Lh + \u03b13Latt + \u03b14Lemb, where Lt = 1n \u2211 j \u2113t(f(x j ; \u0398), yj) is the taskspecific loss function, and \u03b11, \u03b12, \u03b13, \u03b14 are hyperparameters."
        },
        {
            "heading": "C Algorithm of Reinmax",
            "text": "Algorithm 1: Reinmax Input: gate function input g(hi\u22121;\u03c9i) Output: B[0]\n1 \u03c00 = Softmax(g(hi\u22121;\u03c9i)); 2 D = One_Hot(\u03c00); 3 \u03c01 = D+\u03c00 2 ; 4 \u03c01 = Softmax(stop_gradient(log(\u03c01)\u2212 g(hi\u22121;\u03c9i)) + g(hi\u22121;\u03c9i)); 5 \u03c02 = 2\u03c01 \u2212 12\u03c00; 6 B = \u03c02 \u2212 stop_gradient(\u03c02) +D;"
        },
        {
            "heading": "D Dataset",
            "text": "E Implementation Details We simply set \u03b11, \u03b12, \u03b13, \u03b14, and \u03b2 as 1 for all experiments. We select the learning rates from {5e\u22126, 1e\u22125, 2e\u22125, 3e\u22125, 5e\u22125} and batch size from {8, 16, 32, 64}. Experiments are conducted on four NIVIDA RTX A6000."
        }
    ],
    "title": "HadSkip: Homotopic and Adaptive Layer Skipping of Pre-trained Language Models for Efficient Inference",
    "year": 2023
}