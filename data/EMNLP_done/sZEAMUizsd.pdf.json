{
    "abstractText": "Post-training quantization (PTQ) of transformer language models faces significant challenges due to the existence of detrimental outliers in activations. We observe that these outliers are concentrated in specific channels and are asymmetric across channels. To address this issue, we propose the Outlier Suppression+ (OS+) framework, which contains the channel-wise shifting for asymmetry and channel-wise scaling for concentration. We show that these operations can be seamlessly migrated into subsequent modules while maintaining equivalence. Second, we propose a fast and stable scheme to calculate effective shifting and scaling values. The channel-wise shifting aligns the center of each channel for removal of outlier asymmetry. The channel-wise scaling quantitatively evaluates changes brought by migration and quantization for better quantization burden balance. We validate our OS+ under both standard and fine-grained quantization settings with models including BERT, OPT, BLOOM, BLOOMZ, and LLaMA. Comprehensive results across various tasks demonstrate the superiority of our approach. Especially, with standard quantization, OS+ can achieve near-floating-point performance on both small models and large language models on 8-bit and 6-bit. Besides, we establish a new state-of-the-art for 4-bit BERT with 15.5% improvement. Our code is available at https://github.com/ModelTC/ Outlier_Suppression_Plus.",
    "authors": [
        {
            "affiliations": [],
            "name": "Xiuying Wei"
        },
        {
            "affiliations": [],
            "name": "Yunchen Zhang"
        },
        {
            "affiliations": [],
            "name": "Yuhang Li"
        },
        {
            "affiliations": [],
            "name": "Xiangguo Zhang"
        },
        {
            "affiliations": [],
            "name": "Ruihao Gong"
        },
        {
            "affiliations": [],
            "name": "Jinyang Guo"
        },
        {
            "affiliations": [],
            "name": "Xianglong Liu"
        }
    ],
    "id": "SP:8fa1c154ee8bdc6f5de31e00ae60fe4e43918224",
    "references": [
        {
            "authors": [
                "Haoli Bai",
                "Wei Zhang",
                "Lu Hou",
                "Lifeng Shang",
                "Jing Jin",
                "Xin Jiang",
                "Qun Liu",
                "Michael Lyu",
                "Irwin King."
            ],
            "title": "Binarybert: Pushing the limit of bert quantization",
            "venue": "arXiv preprint arXiv:2012.15701.",
            "year": 2020
        },
        {
            "authors": [
                "Ron Banner",
                "Yury Nahshan",
                "Elad Hoffer",
                "Daniel Soudry"
            ],
            "title": "Aciq: analytical clipping for integer quantization of neural networks",
            "year": 2018
        },
        {
            "authors": [
                "Yelysei Bondarenko",
                "Markus Nagel",
                "Tijmen Blankevoort."
            ],
            "title": "Understanding and overcoming the challenges of efficient transformer quantization",
            "venue": "arXiv preprint arXiv:2109.12948.",
            "year": 2021
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners. Advances in neural information processing",
            "year": 2020
        },
        {
            "authors": [
                "Yaohui Cai",
                "Zhewei Yao",
                "Zhen Dong",
                "Amir Gholami",
                "Michael W Mahoney",
                "Kurt Keutzer."
            ],
            "title": "Zeroq: A novel zero shot quantization framework",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13169\u201313178.",
            "year": 2020
        },
        {
            "authors": [
                "Zhaowei Cai",
                "Nuno Vasconcelos."
            ],
            "title": "Rethinking differentiable search for mixed-precision neural networks",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2349\u20132358.",
            "year": 2020
        },
        {
            "authors": [
                "Mengzhao Chen",
                "Wenqi Shao",
                "Peng Xu",
                "Mingbao Lin",
                "Kaipeng Zhang",
                "Fei Chao",
                "Rongrong Ji",
                "Yu Qiao",
                "Ping Luo."
            ],
            "title": "Diffrate: Differentiable compression rate for efficient vision transformers",
            "venue": "arXiv preprint arXiv:2305.17997.",
            "year": 2023
        },
        {
            "authors": [
                "Jungwook Choi",
                "Zhuo Wang",
                "Swagath Venkataramani",
                "Pierce I-Jen Chuang",
                "Vijayalakshmi Srinivasan",
                "Kailash Gopalakrishnan."
            ],
            "title": "Pact: Parameterized clipping activation for quantized neural networks",
            "venue": "arXiv preprint arXiv:1805.06085.",
            "year": 2018
        },
        {
            "authors": [
                "Yoni Choukroun",
                "Eli Kravchik",
                "Fan Yang",
                "Pavel Kisilev."
            ],
            "title": "Low-bit quantization of neural networks for efficient inference",
            "venue": "2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW), pages 3009\u20133018. IEEE.",
            "year": 2019
        },
        {
            "authors": [
                "Matthieu Courbariaux",
                "Yoshua Bengio",
                "Jean-Pierre David."
            ],
            "title": "Binaryconnect: Training deep neural networks with binary weights during propagations",
            "venue": "Advances in neural information processing systems,",
            "year": 2015
        },
        {
            "authors": [
                "Tim Dettmers",
                "Mike Lewis",
                "Younes Belkada",
                "Luke Zettlemoyer"
            ],
            "title": "Llm. int8 (): 8-bit matrix multiplication for transformers at scale. arXiv preprint arXiv:2208.07339",
            "year": 2022
        },
        {
            "authors": [
                "Tim Dettmers",
                "Luke Zettlemoyer."
            ],
            "title": "The case for 4-bit precision: k-bit inference scaling laws",
            "venue": "arXiv preprint arXiv:2212.09720.",
            "year": 2022
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "arXiv preprint arXiv:1810.04805.",
            "year": 2018
        },
        {
            "authors": [
                "Zhen Dong",
                "Zhewei Yao",
                "Amir Gholami",
                "Michael W Mahoney",
                "Kurt Keutzer."
            ],
            "title": "Hawq: Hessian aware quantization of neural networks with mixedprecision",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 293\u2013",
            "year": 2019
        },
        {
            "authors": [
                "Steven K Esser",
                "Jeffrey L McKinstry",
                "Deepika Bablani",
                "Rathinakumar Appuswamy",
                "Dharmendra S Modha."
            ],
            "title": "Learned step size quantization",
            "venue": "arXiv preprint arXiv:1902.08153.",
            "year": 2019
        },
        {
            "authors": [
                "Angela Fan",
                "Pierre Stock",
                "Benjamin Graham",
                "Edouard Grave",
                "R\u00e9mi Gribonval",
                "Herve Jegou",
                "Armand Joulin."
            ],
            "title": "Training with quantization noise for extreme model compression",
            "venue": "arXiv preprint arXiv:2004.07320.",
            "year": 2020
        },
        {
            "authors": [
                "Elias Frantar",
                "Saleh Ashkboos",
                "Torsten Hoefler",
                "Dan Alistarh."
            ],
            "title": "Gptq: Accurate post-training quantization for generative pre-trained transformers",
            "venue": "arXiv preprint arXiv:2210.17323.",
            "year": 2022
        },
        {
            "authors": [
                "Leo Gao",
                "Stella Biderman",
                "Sid Black",
                "Laurence Golding",
                "Travis Hoppe",
                "Charles Foster",
                "Jason Phang",
                "Horace He",
                "Anish Thite",
                "Noa Nabeshima"
            ],
            "title": "The pile: An 800gb dataset of diverse text for language modeling",
            "venue": "arXiv preprint arXiv:2101.00027",
            "year": 2020
        },
        {
            "authors": [
                "Ruihao Gong",
                "Xianglong Liu",
                "Shenghu Jiang",
                "Tianxiang Li",
                "Peng Hu",
                "Jiazhen Lin",
                "Fengwei Yu",
                "Junjie Yan."
            ],
            "title": "Differentiable soft quantization: Bridging full-precision and low-bit neural networks",
            "venue": "The IEEE International Conference on Computer Vision",
            "year": 2019
        },
        {
            "authors": [
                "Cong Guo",
                "Yuxian Qiu",
                "Jingwen Leng",
                "Xiaotian Gao",
                "Chen Zhang",
                "Yunxin Liu",
                "Fan Yang",
                "Yuhao Zhu",
                "Minyi Guo."
            ],
            "title": "Squant: On-the-fly data-free quantization via diagonal hessian approximation",
            "venue": "arXiv preprint arXiv:2202.07471.",
            "year": 2022
        },
        {
            "authors": [
                "Cong Guo",
                "Jiaming Tang",
                "Weiming Hu",
                "Jingwen Leng",
                "Chen Zhang",
                "Fan Yang",
                "Yunxin Liu",
                "Minyi Guo",
                "Yuhao Zhu."
            ],
            "title": "Olive: Accelerating large language models via hardware-friendly outlier-victim pair quantization",
            "venue": "Matrix, 17(4.2):7\u20131.",
            "year": 2023
        },
        {
            "authors": [
                "Song Han",
                "Huizi Mao",
                "William J Dally."
            ],
            "title": "Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding",
            "venue": "arXiv preprint arXiv:1510.00149.",
            "year": 2015
        },
        {
            "authors": [
                "Geoffrey Hinton",
                "Oriol Vinyals",
                "Jeff Dean."
            ],
            "title": "Distilling the knowledge in a neural network",
            "venue": "arXiv preprint arXiv:1503.02531.",
            "year": 2015
        },
        {
            "authors": [
                "Edward J Hu",
                "Yelong Shen",
                "Phillip Wallis",
                "Zeyuan Allen-Zhu",
                "Yuanzhi Li",
                "Shean Wang",
                "Lu Wang",
                "Weizhu Chen."
            ],
            "title": "Lora: Low-rank adaptation of large language models",
            "venue": "arXiv preprint arXiv:2106.09685.",
            "year": 2021
        },
        {
            "authors": [
                "Itay Hubara",
                "Yury Nahshan",
                "Yair Hanani",
                "Ron Banner",
                "Daniel Soudry."
            ],
            "title": "Accurate post training quantization with small calibration sets",
            "venue": "International Conference on Machine Learning, pages 4466\u20134475. PMLR.",
            "year": 2021
        },
        {
            "authors": [
                "Benoit Jacob",
                "Skirmantas Kligys",
                "Bo Chen",
                "Menglong Zhu",
                "Matthew Tang",
                "Andrew Howard",
                "Hartwig Adam",
                "Dmitry Kalenichenko."
            ],
            "title": "Quantization and training of neural networks for efficient integer-arithmetic-only inference",
            "venue": "Proceedings of",
            "year": 2018
        },
        {
            "authors": [
                "Qing Jin",
                "Jian Ren",
                "Richard Zhuang",
                "Sumant Hanumante",
                "Zhengang Li",
                "Zhiyu Chen",
                "Yanzhi Wang",
                "Kaiyuan Yang",
                "Sergey Tulyakov."
            ],
            "title": "F8net: Fixed-point 8-bit only multiplication for network quantization",
            "venue": "arXiv preprint arXiv:2202.05239.",
            "year": 2022
        },
        {
            "authors": [
                "Sehoon Kim",
                "Amir Gholami",
                "Zhewei Yao",
                "Michael W Mahoney",
                "Kurt Keutzer."
            ],
            "title": "I-bert: Integeronly bert quantization",
            "venue": "International conference on machine learning, pages 5506\u20135518. PMLR.",
            "year": 2021
        },
        {
            "authors": [
                "Olga Kovaleva",
                "Saurabh Kulshreshtha",
                "Anna Rogers",
                "Anna Rumshisky."
            ],
            "title": "Bert busters: Outlier dimensions that disrupt transformers",
            "venue": "arXiv preprint arXiv:2105.06990.",
            "year": 2021
        },
        {
            "authors": [
                "Andrey Kuzmin",
                "Mart Van Baalen",
                "Yuwei Ren",
                "Markus Nagel",
                "Jorn Peters",
                "Tijmen Blankevoort."
            ],
            "title": "Fp8 quantization: The power of the exponent",
            "venue": "arXiv preprint arXiv:2208.09225.",
            "year": 2022
        },
        {
            "authors": [
                "Yann LeCun",
                "John Denker",
                "Sara Solla."
            ],
            "title": "Optimal brain damage",
            "venue": "Advances in neural information processing systems, 2.",
            "year": 1989
        },
        {
            "authors": [
                "Yanjing Li",
                "Sheng Xu",
                "Baochang Zhang",
                "Xianbin Cao",
                "Peng Gao",
                "Guodong Guo."
            ],
            "title": "Q-vit: Accurate and fully quantized low-bit vision transformer",
            "venue": "arXiv preprint arXiv:2210.06707.",
            "year": 2022
        },
        {
            "authors": [
                "Yuhang Li",
                "Xin Dong",
                "Wei Wang."
            ],
            "title": "Additive powers-of-two quantization: An efficient nonuniform discretization for neural networks",
            "venue": "arXiv preprint arXiv:1909.13144.",
            "year": 2019
        },
        {
            "authors": [
                "Yuhang Li",
                "Ruihao Gong",
                "Xu Tan",
                "Yang Yang",
                "Peng Hu",
                "Qi Zhang",
                "Fengwei Yu",
                "Wei Wang",
                "Shi Gu."
            ],
            "title": "Brecq: Pushing the limit of post-training quantization by block reconstruction",
            "venue": "International Conference on Learning Representations.",
            "year": 2021
        },
        {
            "authors": [
                "Ji Lin",
                "Jiaming Tang",
                "Haotian Tang",
                "Shang Yang",
                "Xingyu Dang",
                "Song Han."
            ],
            "title": "Awq: Activationaware weight quantization for llm compression and acceleration",
            "venue": "arXiv preprint arXiv:2306.00978.",
            "year": 2023
        },
        {
            "authors": [
                "Jing Liu",
                "Ruihao Gong",
                "Xiuying Wei",
                "Zhiwei Dong",
                "Jianfei Cai",
                "Bohan Zhuang."
            ],
            "title": "Qllm: Accurate and efficient low-bitwidth quantization for large language models",
            "venue": "arXiv preprint arXiv:2310.08041.",
            "year": 2023
        },
        {
            "authors": [
                "Jing Liu",
                "Zizheng Pan",
                "Haoyu He",
                "Jianfei Cai",
                "Bohan Zhuang."
            ],
            "title": "Ecoformer: Energy-saving attention with linear complexity",
            "venue": "Advances in Neural Information Processing Systems, 35:10295\u201310308.",
            "year": 2022
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Roberta: A robustly optimized bert pretraining approach",
            "venue": "arXiv preprint arXiv:1907.11692.",
            "year": 2019
        },
        {
            "authors": [
                "Stephen Merity",
                "Caiming Xiong",
                "James Bradbury",
                "Richard Socher"
            ],
            "title": "Pointer sentinel mixture models",
            "year": 2017
        },
        {
            "authors": [
                "Niklas Muennighoff",
                "Thomas Wang",
                "Lintang Sutawika",
                "Adam Roberts",
                "Stella Biderman",
                "Teven Le Scao",
                "M Saiful Bari",
                "Sheng Shen",
                "Zheng-Xin Yong",
                "Hailey Schoelkopf"
            ],
            "title": "Crosslingual generalization through multitask finetuning",
            "year": 2022
        },
        {
            "authors": [
                "Markus Nagel",
                "Rana Ali Amjad",
                "Mart Van Baalen",
                "Christos Louizos",
                "Tijmen Blankevoort."
            ],
            "title": "Up or down? adaptive rounding for post-training quantization",
            "venue": "International Conference on Machine Learning, pages 7197\u20137206. PMLR.",
            "year": 2020
        },
        {
            "authors": [
                "Markus Nagel",
                "Mart van Baalen",
                "Tijmen Blankevoort",
                "Max Welling."
            ],
            "title": "Data-free quantization through weight equalization and bias correction",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1325\u20131334.",
            "year": 2019
        },
        {
            "authors": [
                "Giovanni Puccetti",
                "Anna Rogers",
                "Aleksandr Drozd",
                "Felice Dell\u2019Orletta"
            ],
            "title": "Outliers dimensions that disrupt transformers are driven by frequency",
            "venue": "arXiv preprint arXiv:2205.11380",
            "year": 2022
        },
        {
            "authors": [
                "Haotong Qin",
                "Yifu Ding",
                "Mingyuan Zhang",
                "Qinghua Yan",
                "Aishan Liu",
                "Qingqing Dang",
                "Ziwei Liu",
                "Xianglong Liu."
            ],
            "title": "Bibert: Accurate fully binarized bert",
            "venue": "arXiv preprint arXiv:2203.06390.",
            "year": 2022
        },
        {
            "authors": [
                "Alec Radford",
                "Karthik Narasimhan",
                "Tim Salimans",
                "Ilya Sutskever"
            ],
            "title": "Improving language understanding by generative pre-training",
            "year": 2018
        },
        {
            "authors": [
                "Teven Le Scao",
                "Angela Fan",
                "Christopher Akiki",
                "Ellie Pavlick",
                "Suzana Ili\u0107",
                "Daniel Hesslow",
                "Roman Castagn\u00e9",
                "Alexandra Sasha Luccioni",
                "Fran\u00e7ois Yvon",
                "Matthias Gall\u00e9"
            ],
            "title": "Bloom: A 176bparameter open-access multilingual language model",
            "year": 2022
        },
        {
            "authors": [
                "Mingzhu Shen",
                "Feng Liang",
                "Ruihao Gong",
                "Yuhang Li",
                "Chuming Li",
                "Chen Lin",
                "Fengwei Yu",
                "Junjie Yan",
                "Wanli Ouyang."
            ],
            "title": "Once quantization-aware training: High performance extremely low-bit architecture search",
            "venue": "Proceedings of the IEEE/CVF Interna-",
            "year": 2021
        },
        {
            "authors": [
                "Sheng Shen",
                "Zhen Dong",
                "Jiayu Ye",
                "Linjian Ma",
                "Zhewei Yao",
                "Amir Gholami",
                "Michael W Mahoney",
                "Kurt Keutzer."
            ],
            "title": "Q-bert: Hessian based ultra low precision quantization of bert",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, vol-",
            "year": 2020
        },
        {
            "authors": [
                "Shaden Smith",
                "Mostofa Patwary",
                "Brandon Norick",
                "Patrick LeGresley",
                "Samyam Rajbhandari",
                "Jared Casper",
                "Zhun Liu",
                "Shrimai Prabhumoye",
                "George Zerveas",
                "Vijay Korthikanti"
            ],
            "title": "Using deepspeed and megatron to train megatron-turing nlg",
            "year": 2022
        },
        {
            "authors": [
                "Chaofan Tao",
                "Lu Hou",
                "Wei Zhang",
                "Lifeng Shang",
                "Xin Jiang",
                "Qun Liu",
                "Ping Luo",
                "Ngai Wong."
            ],
            "title": "Compression of generative pre-trained language models via quantization",
            "venue": "arXiv preprint arXiv:2203.10705.",
            "year": 2022
        },
        {
            "authors": [
                "Hugo Touvron",
                "Thibaut Lavril",
                "Gautier Izacard",
                "Xavier Martinet",
                "Marie-Anne Lachaux",
                "Timoth\u00e9e Lacroix",
                "Baptiste Rozi\u00e8re",
                "Naman Goyal",
                "Eric Hambro",
                "Faisal Azhar"
            ],
            "title": "Llama: Open and efficient foundation language models",
            "year": 2023
        },
        {
            "authors": [
                "Alex Wang",
                "Amanpreet Singh",
                "Julian Michael",
                "Felix Hill",
                "Omer Levy",
                "Samuel R Bowman."
            ],
            "title": "Glue: A multi-task benchmark and analysis platform for natural language understanding",
            "venue": "arXiv preprint arXiv:1804.07461.",
            "year": 2018
        },
        {
            "authors": [
                "Naigang Wang",
                "Jungwook Choi",
                "Daniel Brand",
                "Chia-Yu Chen",
                "Kailash Gopalakrishnan."
            ],
            "title": "Training deep neural networks with 8-bit floating point numbers",
            "venue": "Advances in neural information processing systems, 31.",
            "year": 2018
        },
        {
            "authors": [
                "Peisong Wang",
                "Qiang Chen",
                "Xiangyu He",
                "Jian Cheng."
            ],
            "title": "Towards accurate post-training network quantization via bit-split and stitching",
            "venue": "International Conference on Machine Learning, pages 9847\u20139856. PMLR.",
            "year": 2020
        },
        {
            "authors": [
                "Xiuying Wei",
                "Ruihao Gong",
                "Yuhang Li",
                "Xianglong Liu",
                "Fengwei Yu."
            ],
            "title": "Qdrop: Randomly dropping quantization for extremely low-bit post-training quantization",
            "venue": "International Conference on Learning Representations.",
            "year": 2022
        },
        {
            "authors": [
                "Xiuying Wei",
                "Yunchen Zhang",
                "Xiangguo Zhang",
                "Ruihao Gong",
                "Shanghang Zhang",
                "Qi Zhang",
                "Fengwei Yu",
                "Xianglong Liu."
            ],
            "title": "Outlier suppression: Pushing the limit of low-bit transformer language models",
            "venue": "arXiv preprint arXiv:2209.13325.",
            "year": 2022
        },
        {
            "authors": [
                "Hao Wu",
                "Patrick Judd",
                "Xiaojie Zhang",
                "Mikhail Isaev",
                "Paulius Micikevicius."
            ],
            "title": "Integer quantization for deep learning inference: Principles and empirical evaluation",
            "venue": "arXiv preprint arXiv:2004.09602.",
            "year": 2020
        },
        {
            "authors": [
                "Guangxuan Xiao",
                "Ji Lin",
                "Mickael Seznec",
                "Julien Demouth",
                "Song Han."
            ],
            "title": "Smoothquant: Accurate and efficient post-training quantization for large language models",
            "venue": "arXiv preprint arXiv:2211.10438.",
            "year": 2022
        },
        {
            "authors": [
                "Ke Xu",
                "Lei Han",
                "Ye Tian",
                "Shangshang Yang",
                "Xingyi Zhang."
            ],
            "title": "Eq-net: Elastic quantization neural networks",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1505\u2013 1514.",
            "year": 2023
        },
        {
            "authors": [
                "Zhewei Yao",
                "Reza Yazdani Aminabadi",
                "Minjia Zhang",
                "Xiaoxia Wu",
                "Conglong Li",
                "Yuxiong He."
            ],
            "title": "Zeroquant: Efficient and affordable post-training quantization for large-scale transformers",
            "venue": "arXiv preprint arXiv:2206.01861.",
            "year": 2022
        },
        {
            "authors": [
                "Zhewei Yao",
                "Xiaoxia Wu",
                "Cheng Li",
                "Stephen Youn",
                "Yuxiong He."
            ],
            "title": "Zeroquant-v2: Exploring post-training quantization in llms from comprehensive study to low rank compensation",
            "venue": "arXiv preprint arXiv:2303.08302.",
            "year": 2023
        },
        {
            "authors": [
                "Zhihang Yuan",
                "Chenhao Xue",
                "Yiqi Chen",
                "Qiang Wu",
                "Guangyu Sun."
            ],
            "title": "Ptq4vit: Post-training quantization framework for vision transformers",
            "venue": "arXiv preprint arXiv:2111.12293.",
            "year": 2021
        },
        {
            "authors": [
                "Ofir Zafrir",
                "Guy Boudoukh",
                "Peter Izsak",
                "Moshe Wasserblat."
            ],
            "title": "Q8bert: Quantized 8bit bert",
            "venue": "2019 Fifth Workshop on Energy Efficient Machine Learning and Cognitive Computing-NeurIPS Edition (EMC2-NIPS), pages 36\u201339. IEEE.",
            "year": 2019
        },
        {
            "authors": [
                "Aohan Zeng",
                "Xiao Liu",
                "Zhengxiao Du",
                "Zihan Wang",
                "Hanyu Lai",
                "Ming Ding",
                "Zhuoyi Yang",
                "Yifan Xu",
                "Wendi Zheng",
                "Xiao Xia"
            ],
            "title": "Glm-130b: An open bilingual pre-trained model",
            "venue": "arXiv preprint arXiv:2210.02414",
            "year": 2022
        },
        {
            "authors": [
                "Dongqing Zhang",
                "Jiaolong Yang",
                "Dongqiangzi Ye",
                "Gang Hua."
            ],
            "title": "Lq-nets: Learned quantization for highly accurate and compact deep neural networks",
            "venue": "Proceedings of the European conference on computer vision (ECCV), pages 365\u2013382.",
            "year": 2018
        },
        {
            "authors": [
                "Susan Zhang",
                "Stephen Roller",
                "Naman Goyal",
                "Mikel Artetxe",
                "Moya Chen",
                "Shuohui Chen",
                "Christopher Dewan",
                "Mona Diab",
                "Xian Li",
                "Xi Victoria Lin"
            ],
            "title": "Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068",
            "year": 2022
        },
        {
            "authors": [
                "Wei Zhang",
                "Lu Hou",
                "Yichun Yin",
                "Lifeng Shang",
                "Xiao Chen",
                "Xin Jiang",
                "Qun Liu."
            ],
            "title": "Ternarybert: Distillation-aware ultra-low bit bert",
            "venue": "arXiv preprint arXiv:2009.12812.",
            "year": 2020
        },
        {
            "authors": [
                "Xiangguo Zhang",
                "Haotong Qin",
                "Yifu Ding",
                "Ruihao Gong",
                "Qinghua Yan",
                "Renshuai Tao",
                "Yuhang Li",
                "Fengwei Yu",
                "Xianglong Liu."
            ],
            "title": "Diversifying sample generation for accurate data-free quantization",
            "venue": "In",
            "year": 2021
        },
        {
            "authors": [
                "Ritchie Zhao",
                "Yuwei Hu",
                "Jordan Dotzel",
                "Chris De Sa",
                "Zhiru Zhang."
            ],
            "title": "Improving neural network quantization without retraining using outlier channel splitting",
            "venue": "International conference on machine learning, pages 7543\u20137552. PMLR.",
            "year": 2019
        },
        {
            "authors": [
                "Barret Zoph",
                "Quoc V Le."
            ],
            "title": "Neural architecture search with reinforcement learning",
            "venue": "arXiv preprint arXiv:1611.01578.",
            "year": 2016
        },
        {
            "authors": [
                "Jin et al"
            ],
            "title": "Quantization of transformer language models. Recently, there has been a growing interest in the quantization of transformer language models",
            "venue": "In the context of QAT, Zafrir et al",
            "year": 2019
        },
        {
            "authors": [
                "Zhang"
            ],
            "title": "plores 8-bit quantization for BERT-like models. Shen et al. (2020) introduces group-wise quantization and studies mixed-precision quantization based on Hessian information",
            "year": 2022
        },
        {
            "authors": [
                "Kim"
            ],
            "title": "2021) approximates the nonlinear function in transformer architectures to enable integer-only inference. Fan et al. (2020) incorporates quantization noise for enhancement",
            "year": 2020
        },
        {
            "authors": [
                "Wei"
            ],
            "title": "2022b) identifies this feature lying",
            "year": 2022
        },
        {
            "authors": [
                "Wei"
            ],
            "title": "On fused models, we apply the calibration procedure. Particularly, on BERT models, due to the great variance of token range as discussed in Yao et al",
            "year": 2022
        },
        {
            "authors": [
                "99999]. OMSE (Choukroun"
            ],
            "title": "2019) minimizes the mean squared error between quantization and FP signals. PEG (Bondarenko et al., 2021) applies fine-grained quantization to problematic activation from a channel perspective",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Transformer language models (e.g., BERT, LLMs) have garnered significant attention due to their remarkable performance and scalable model size. These models have evolved from hundreds of millions of parameters (Devlin et al., 2018; Liu et al.,\n\u2217Corresponding author.\n2019; Radford et al., 2018) to hundreds of billions of parameters (Brown et al., 2020; Zhang et al., 2022; Smith et al., 2022). This necessitates the employment of compression techniques (Han et al., 2015; Hinton et al., 2015; Zoph and Le, 2016; LeCun et al., 1989) for practical deployment. Among these techniques, quantization (Jacob et al., 2018) has emerged as a general and primary paradigm for reducing both memory footprint and computation overhead.\nHowever, quantization, particularly post-training quantization (Choukroun et al., 2019; Banner et al., 2018; Wu et al., 2020) under the setting of limited data and GPU resources, has become increasingly challenging on these models (e.g., a 12% accuracy drop in BERT (Bondarenko et al., 2021) and catastrophic degradation in OPT-175B (Dettmers et al., 2022)). This is caused by the presence of detrimental outliers in activation (e.g., the range of distribution can be 80 in BERT and even 140 in OPTs), which prevents discrete numbers from accurately representing continuous ones.\nTo combat the bottleneck, researchers make indepth investigations and find that outliers mainly concentrate on certain channels. Some works (Bondarenko et al., 2021; Dettmers et al., 2022) suggest fine-grained quantization schemes and offer extra bit levels for outlier channels. Others (Wei et al., 2022b; Xiao et al., 2022) take the activation scaling to scale outliers and migrate scaling values to subsequent weights for FP equivalence. However, the former might hurt the quantization acceleration effect while the latter determines scaling values without the consideration of minimizing the change introduced by migration and quantization, which we find is sub-optimal. Meanwhile, we also identify a new outlier characteristic that previous works overlooked but is also responsible for the large tensor range.\nIn this paper, we propose the Outlier Suppression+ framework composed of channel-wise shift-\ning and scaling to effectively pursue better quantization performance while equivalently keeping the FP output. First, we find a new feature of outliers that they stay in asymmetric shape across channels (e.g., in Fig. 1a, one problematic channel on OPT-66B occupies the negative axis from -97 to -58 while another one has positive values ranging from 5.7 to 43). This outlier asymmetric presentation could cause a significantly wide distribution of tensor like 140 even composed of channels with relatively small ranges like 39. Thus, we propose the channel-wise shifting operation, which shifts the activation across channels to eliminate the impact of asymmetry. Together with channel-wise scaling for concentrated outliers, a unified migration pattern is introduced to seamlessly transfer the reversed effects of these operations to later modules to maintain equivalent FP models. Second, we devise deliberate schemes to determine effective shifting and scaling values. The shifting vector aligns the center of each channel, reducing the whole tensor range to its maximum channel range. The scaling values quantitatively minimize the interactive output change of the activation and weights induced by migration and quantization, achieving a balanced quantization burden with a fast and stable search procedure.\nOur algorithm can be carried out efficiently and enjoy affordability on real hardware, producing more quantization-friendly models in minutes and requiring no extra inference burden on LLMs. To this end, our main contributions can be summarized into three aspects:\n1. We find a new feature of outliers that show asymmetric shapes across channels and then propose the channel-wise shifting operation, along with taking channel-wise scaling for the outlier concentration attribute. A unified migration pattern that migrates their reversed effects to later modules is designed to guarantee an equivalent FP network. 2. We propose fast and stable ways to determine effective shifting and scaling values. Shifting values eliminate the asymmetry feature across channels while scaling values scale down outlier channels towards a quantitative optimization objective. 3. We assess the efficacy of our approach under both standard and fine-grained quantization settings. On standard one, OS+ achieves nearfloating-point performance on 8-bit and 6-bit BERT, OPTs, BLOOM, and BLOOMZ. On finegrained one, OS+ can surpass others by 9.41% on 4-bit LLaMA with per-token quantization and obtain lossless results on 4-bit OPT with per-group quantization."
        },
        {
            "heading": "2 Related work",
            "text": "Due to the space limit, we give the most relevant papers here and put a complete related work in the Appendix A. In the realm of PTQ, researchers have discovered that the poor performance of transformer language models should be attributed to extreme outliers in activations, which exhibit special characteristics from both channel and token aspects. Thus, we will introduce related works\nfrom the two aspects. Channel aspect. Outliers consistently emerge in certain channels over different inputs. Bondarenko et al. (2021) employs a per-embeddinggroup quantization scheme that uses different quantization parameters for distinct channel groups, while Dettmers et al. (2022) suggests utilizing FP16 representations for problematic channels holding signals over 6. Wei et al. (2022b) introduces an outlier suppression (OS) framework with one of components called Gamma Migration. Observing that outliers accumulate in certain channels, it adopts a scaling vector to scale outliers and migrates it to subsequent modules. Xiao et al. (2022) further proposes calculating scaling values by equalizing ranges between activations and weights and evaluates on large language models. Guo et al. (2023) discards normal values adjacent to outliers, making room for outliers with customized GPU support. To consider the standard quantization, we find that Wei et al. (2022b) and Xiao et al. (2022) still waste a large portion of quantization levels on the extreme outlier asymmetry across channels. Meanwhile, Wei et al. (2022b) simply views the scaling parameter in LayerNorm (LN) as the scaling vector for outliers, which might not always be consistent with the outlier distribution. Xiao et al. (2022) that adopts the heuristic way and obtains equalized ranges between activation and weights lacks quantitative evaluation of their output change induced by migration and quantization. Token aspect. Different tokens exhibit varying degrees of outliers. Dettmers et al. (2022); Yao et al. (2022) introduce a novel scheme called per-token quantization that dynamically computes quantization parameters for each token. Wei et al. (2022b) investigates the clipping impact of outliers and recommends finding an appropriate clipping range in a token-wise manner. In this paper, we focus on the channel aspect and might combine these techniques when necessary."
        },
        {
            "heading": "3 Preliminary",
            "text": "Basic Notations. We denote matrices as upper case letters (e.g., X) and vectors as lower case letters (e.g., x). Operator \u2299 and \u2298 represent element-wise multiplication and division for matrices or vectors. We use WX as matrix-matrix multiplication. Furthermore, Xt,j refers to the element of the t-th token and the j-th channel in transformer models. Q(\u00b7) denotes the quantization function.\nQuantization. We indicate standard quantization as per-tensor activation quantization, per-channel, or per-tensor weight quantization here because such schemes will not separate the integer matrix multiplication. Per-tensor means assigns quantization parameters for each tensor and per-channel for each output channel. Also, for some fine-grained ways, we mainly consider per-token (Yao et al., 2022) and per-group (Yao et al., 2023) here, which calculates quantization parameters in each token or group."
        },
        {
            "heading": "4 Method",
            "text": "We first present our equivalent shifting and scaling operations, then introduce ways to determine effective values for them."
        },
        {
            "heading": "4.1 Equivalent shifting and scaling",
            "text": "In this section, we comprehensively investigate outlier features, naturally introducing the design of shifting and scaling operations, followed by a unified migration pattern."
        },
        {
            "heading": "4.1.1 Outlier shifting and scaling",
            "text": "Channel-wise shifting. For transformers, especially LLMs, we find that outliers show asymmetric behavior among channels. Recall that in Fig. 1a, the 8725-th channel displays a hard negative interval (-97, -58), while another channel dominates a positive one (5.7, 43). Due to this asymmetry, even if the range of each channel is relatively small, such as 40 and 39 for outlier channels and minuscule values for normal channels, the range of the entire tensor can swell to a considerably large value (e.g., 140, ranging from -97 to 43), which negatively affects quantization performance.\nTo handle this issue, we propose channel-wise shifting, which can eliminate the impact of asymmetry by taking the following operation:\nX\u0303 \u2032 = X \u2212 z, (1)\nwhere z serves as a row vector (z \u2208 Rn) and shifts the activation for each channel. In this way, with a carefully designed z which we will introduce in Sec. 4.2.1, the new tensor X\u0303 \u2032 can get rid of the outlier asymmetry attribute. For example, by aligning the centers of each channel in Fig. 1b, the range can be reduced to 40 (the maximum channel range) from 140 (the large tensor range). Finally, note that this operation is not the conventional shifting operation for symmetric quantization, as it operates channel-wisely and provides better distribution for per-tensor quantization.\nChannel-wise scaling. Apart from the asymmetry feature across channels, there also exists the outlier concentration phenomenon (Wei et al., 2022b) that outliers predominantly accumulate in specific channels over various inputs. For example, the 8725-th and the 6354-th channels in Fig. 1a hold more aggressive values than others. Therefore, after shifting, we equip with the channel-wise scaling to narrow them down to further alleviate the quantization difficulty.\nX\u0303 = (X \u2212 z)\u2298 s. (2)\nIn the above equation, the row vector s \u2208 Rn scales the shifted tensor for each channel and brings final quantization-friendly activation X\u0303 . For example, in Fig. 1c, a tensor with a size of 10 can be obtained if we scale down channels with signals over 5. Detailed calculation of s will be given in Sec. 4.2.2. Implementation. It is easy to implement these operations. Take the output of LayerNorm Fig. 2 as an example, we only need to replace its linear transformation parameters \u03b2 and \u03b3 with (\u03b2\u2212z)\u2298s and \u03b3 \u2298 s to achieve shifting and scaling effects. For others, we can update parameters in the former DeQuant function."
        },
        {
            "heading": "4.1.2 Unified migration pattern",
            "text": "As mentioned in Eq. (1) and Eq. (2), we subtract z and divide s to make the problematic activation resilient to quantization. To keep an equivalent FP model, a unified migration pattern is proposed that transfers both reversed shifting and scaling vectors to subsequent modules. We demonstrate\nthe feasibility of this algorithm on two common structures. Linear Layer. First, we consider a prevalent scenario where a linear (convolutional) layer immediately follows. Reversing the above operations (i.e., (X\u0303 \u2299 s+ z)W\u22a4 + b) equals to updating the W \u2208 Rm,n and b \u2208 Rm in the next layer, given by\n(X\u0303 \u2299 s+ z)W\u22a4 + b\n= (X\u0303 \u2299 s)W\u22a4 + zW\u22a4 + b\n= X\u0303(W\u22a4 \u2299 s\u22a4) + (zW\u22a4 + b).\n(3)\nAccording to Eq. (3), weight and bias can absorb s and z, respectively, and thus becomes:\nW\u0303 = W \u2299  s1 s2 ... sn s1 s2 ... sn ... s1 s2 ... sn  , b\u0303 = zW\u22a4 + b.\n(4)\nFor example, Fig. 2(a) depicts the typical challenging activation (output of LayerNorm) in the attention structure, all following weights and biases can absorb the shifting and scaling signals without any extra computation burden. Residual connection. Second, we consider the case where a residual connection is applied after the LayerNorm structure (Post-LN) and fed into the quantized input. As shown in Fig. 2b, in addition to linear layer transformation, the identity function will be substituted with channel-wise multiplication and addition to maintain equivalence. We demonstrate that these increased calculations\nwill only incur a negligible inference burden in Sec. 5.5.\nFinally, because s and z serve as shared parameters across tokens and batches of data, the unified migration pattern can be well-implemented and produce the same output without additional computation most of the time."
        },
        {
            "heading": "4.2 Effective shifting and scaling",
            "text": "Based on the equivalent shifting and scaling operations, in this section, we propose a fast and stable scheme to pursue effective values."
        },
        {
            "heading": "4.2.1 Shifting values",
            "text": "The design of the shifting vector should eliminate the impact of asymmetry across channels. Thus, we devise to align the center of each channel to 0 so that the outlier channel will not occupy only the positive or negative side. In detail, z is defined as the average of the minimum and maximum signals in each channel, given by:\nzj = max(X:,j) + min(X:,j)\n2 , (5)\nWith the channel-wise shifting now, the tensor range reduces to the largest channel range, getting rid of being defined by asymmetric outliers."
        },
        {
            "heading": "4.2.2 Scaling values",
            "text": "The design of the scaling vector should further scale down outliers while bringing marginal impact on following weight quantization. The following parts introduce how to obtain it with the proposed optimization objective and procedure. Challenges. Recall that the equivalent transformation Eq. (4) also scales weights and potentially leads to inferior weight quantization, which requires us to calculate elaborate scaling values to reach a quantization balance between activation and weights. Nevertheless, we find previous works (Wei et al., 2022b; Xiao et al., 2022) either ignore the affected following weight or take a heuristic way that simply equalizes ranges of activation and weights. Unlike them, we think the key point is to minimize their interactive output change resulting from migration and quantization (a detailed analysis is available in Table 6). Hence, a new optimization objective is proposed. Optimization objective. We first study the simple case that the problematic activation acts as the input of one linear layer (e.g., Fig. 2b). Instead of minimizing quantization errors\nof activation and weight separately (i.e., mins E [ \u2225Q((X \u2212 z)\u2298 s)\u2212 (X \u2212 z)\u2298 s\u22252F ] and mins E [ \u2225Q(W \u2299 s)\u2212W \u2299 s\u22252F ] ), a task loss perspective is adopted by concerning their matrix multiplication output. We measure the output change after scaling and quantizing weight and activation to pursue effective factors, given by:\nmin s E[\u2225Q((X \u2212 z)\u2298 s)Q(W \u2299 s)\u22a4 + b\u0303\ufe38 \ufe37\ufe37 \ufe38 output after scaling and quantization\n\u2212 (XW\u22a4 + b)\ufe38 \ufe37\ufe37 \ufe38 original FP output \u22252F ], (6)\nwhere the mean squared error (MSE) is used to quantify the difference.\nMultiple linear layers: Furthermore, we study the case for multiple linear layers like the attention structure (Fig. 2a), where three weights will be multiplied by the same scaling vector and calculated with the same suppressed activation.\nIn this scenario, their matrix multiplication outputs produced by scaled and quantized matrices are marked as Q\u0303q, K\u0303q, V\u0303q, (Original outputs are denoted as Q,K,V ). Applying Eq. (6) to three linear layers separately and simply summing the losses can make it difficult to illustrate their different importance and usages. Therefore, we employ the attention mechanism as a post-process function to reasonably organize their scaling and quantization information, given by:\nmin s E[\u2225softmax(Q\u0303qK\u0303\u22a4q )V\u0303q \u2212 softmax(QK\u22a4)V \u22252F ]. (7) Normalization and masking are omitted for notation simplicity, and it can be seen that information from the first two linear layers has been encapsulated within the attention map. Optimization procedure. Toward the above objective, a fast and stable procedure is introduced to search the scaling vector. First, we find that scaling down only channels with outliers can bring better performance. Because channels with normal activations can exhibit more variation over different inputs, it can be difficult to find a decent scaling value for them. Also, considering that they are not responsible for low quantization performance, scaling them is not necessary. Second, we propose to optimize an alternate variable called outlier threshold t, which would squeeze only channels with an activation range over t into (\u2212t, t) and keep others intact (Fig. 2). Essentially, t here is used to specify\nwhich channel to scale down, the final scaled activation range, as well as the scaling values in the following weights.\nThis technique simplifies the complex problem with numerous variables s to a single variable t. Then we adopt the simple grid search for t to minimize the objective Eq. (6), Eq. (7). After getting the effective t, the scaling vector is calculated as:\nsj = max(1.0, max(X:,j \u2212 zj)\nt ). (8)"
        },
        {
            "heading": "5 Experiments",
            "text": "The evaluations are designed to show: I. satisfactory predictions of our OS+ for both small and large language models with standard quantization; II. consistent performance of OS+ on even lower-bit with fine-grained quantization; III. ablation study; III. analysis like computation complexity."
        },
        {
            "heading": "5.1 Set up",
            "text": "Quantization setting. Both the standard and finegrained quantization are considered. For the standard one, we take quantization nodes the same as in Wei et al. (2022b); NVIDIA (2022), always adopt per-tensor activation quantization, consider pertensor (fastest speed) and per-channel (high performance) weight quantization. For the fine-grained quantization, we adopt per-token (Yao et al., 2022) and per-group (Yao et al., 2023) quantization.\nNotation: We use INT8, INT6, INT4 to denote the bitwidth of activation and weight. Specifically, INT8* refers to per-tensor weight quantization. And per-token and per-group quantization will be marked in the table below. Models and tasks. We conduct experiments on both small and large language models. First, BERT models (base and large versions) are evaluated on the GLUE benchmark (Wang et al., 2018a). Second, four of the largest OPTs ranging from 13B to 175B, biggest BLOOM (Scao et al., 2022) and BLOOMZ (Muennighoff et al., 2022) boasting 176 billion parameters, and LLaMA (Touvron et al., 2023) models including 7B, 13B, 30B, 65B sizes are chosen as representatives. Zeroshot tasks including language modeling, multiple choice, commonsense reasoning, etc. are selected for evaluation. The evaluation code is based on lm-harness-evaluation1. Baselines. For BERT, we adopt classical PTQ techniques as baselines, including MinMax, Per-\n1 https://github.com/EleutherAI/lm-evaluation-harness\ncentile (Wu et al., 2020), OMSE (Choukroun et al., 2019), and recent works on BERT quantization including PEG (Bondarenko et al., 2021), and Outlier Suppresion (Wei et al., 2022b). For large models including OPT, BLOOM, and LLaMA, we mainly compare with recent works including ZeroQuant (Yao et al., 2022), and SmoothQuant (Xiao et al., 2022). For details, readers can refer to Appendix C. Implementation. We randomly select 128 samples from the training dataset, in-domain data for the GLUE benchmark, and PILE (Gao et al., 2020) dataset for zero-shot tasks. A batch of data is first used to calculate effective shifting and scaling vectors. Then, calibration is conducted. More details can be found in Appendix C."
        },
        {
            "heading": "5.2 Standard quantization with OS+",
            "text": "In this section, we show how OS+ can help standard quantization achieve satisfying results from both the small models and LLMs aspects.\nBERT. Table 1 gives prediction results of common PTQ algorithms. Most methods perform well on INT8* but fail on lower bits while our approach consistently achieves superior outcomes. Compared to Wei et al. (2022b), our method outperforms by 1.6% and 15.5% on 6-bit and 4-bit, respectively. In summary, our approach can achieve near-floating point performance on high bits and\nreduce the performance gap to 5.6% on 4-bit. OPT and BLOOM. With standard quantization, we list 8-bit and 6-bit accuracy in Table 2. It can be observed that OS+ outperforms ZeroQuant by a large margin. While SmoothQuant suffers from non-negligible accuracy drops on much harder settings like the 6-bit 175B model with significantly severe outliers, ours still gives enjoyable results, owning 32.5% upswings on HellaSwag task, 27.4% boost on PIQA. Results of BLOOM models indicate that their quantization challenges are less severe than OPTs with smaller accuracy drops across methods. Our approach still beats the best of others by about 2% points on 6-bit. To conclude, with standard quantization, ours is indeed close to FP results on 8-bit and exhibits around 1 point accuracy degradation on 6-bit."
        },
        {
            "heading": "5.3 Fine-grained quantization with OS+",
            "text": "Here, OS+ is combined with fine-grained quantization to validate its wide application and go extremely low bit setting like 4-bit quantization. Per-token Quantization. Per-token quantization (Yao et al., 2022), which customizes quantization parameters for individual tokens, can bring better predictions, especially for lower-bit quantization and longer output like WikiText2 (Merity et al., 2017). We opt for LLaMA models for validation. It\u2019s worth noting that the structure of LLaMA differs from others in its design of element-wise multiplication of two activations as the input to the final layer in FFN, potentially resulting in very\nlarge signals, even exceeding 600. Given such a challenge, we provide experiments both with and without quantization of this layer in Table 3 and Table 10, respectively. In both tables, we highlight our lossless performance on 6-bit quantization while SmoothQuant still suffers in Table 10. Also, it shows the superior performance of OS+ on 4-bit (e.g., 10.58% improvement on Winogrande, 10.04 PPL decrease on WikiText2).\nPer-group Quantization. Additionally, per-group quantization (Yao et al., 2023), which tailors quantization parameters for each group of elements, is a more fine-grained way. Recognizing the difficulties of 4-bit quantization for OPTs, we illustrate an example by adopting per-group quantization with relatively large group sizes of 1024 and 512. Fig. 3 shows that OS+ continues to outperform other methods and can be more competitive under harder cases such as a group size of 1024."
        },
        {
            "heading": "5.4 Ablation study",
            "text": "Design choices of scaling values. In this section, we compare different scaling vector designs. In Table 4, the second row displays results without attention post-processing Eq. (7). Summing the losses of multiple linear layers, as shown, proves unwise, resulting in performance declines of about 2% and 10% on OPTs. The third row removes the outlier threshold and instead learns scaling values directly. We find this process is unstable and requires suitable hyperparameters, causing failure on LLMs. As mentioned in Sec. 4.2.2, This instability may stem from suboptimal scaling values for normal channels with varying magnitudes.\nEffect of each operation. From Table 5, it can be observed clearly that by removing the shifting operation, the accuracy drops by about 1%-3% under difficult settings. This is because, without channelwise shifting that initially smooths the quantization challenge, scaling factors struggle to suppress outliers effectively while producing the tolerable weight quantization burden. Furthermore, when excluding scaling effects, performance decreases significantly, with even crashed results on LLMs."
        },
        {
            "heading": "5.5 Analysis",
            "text": "Different activation scaling. Because scaling values act in both the activation and weights, reducing quantization error for individual tensors can not guarantee the minimum output change, which encapsulates their information to later forward pass. For example, in Table 6, Outlier Suppression with fixed scaling values has the smallest quantization error for weight. SmoothQuant with a heuristic way has the smallest quantization error for activation. However, both of them did not bring the smallest quantization error for the output. This reveals the importance of directly optimizing according to the output, which is what our method exactly does. Thus, we can enjoy the best final performance. Model storage and accuracy. Inspired by a variety of models with diverse sizes, we also study the relationship between their storage and accuracy under quantization settings. Focusing on one kind of model with distinct quantization bit-width, Fig. 4 shows that 8-bit quantization which cuts storage by about half, can generally maintain original performance, and 6-bit quantization can lead to less performance drop on larger models. Moreover, considering fixed storage constraints, we discover that quantized big models typically outperform small FP models. These observations can relate to model robustness, which implies that large models can\nbenefit from compression more if special outliers are handled well. Computation Complexity. We explain our computation complexity of calibration and deployment phases. For the calibration process, OS+ is efficient, and able to generate scaling and shifting values in about 20 minutes for OPT-175B offline. Moreover, due to the equivalent transformation, our method does not demand additional training and can be applied in a post-training setting. For deployment, we discuss inference efficiency with latency performance evaluated using (NVIDIA, 2022). As mentioned before, our channel-wise shifting and scaling can be implemented by updating previous parameters, and be migrated to subsequent weights. For LLMs, our transformation does not introduce any extra computation burden and leads to favorable latency improvements, as demonstrated in a 1.5\u00d7 speedup in Fig. 5. Only BERT models additionally replace the identity function in the residual connection with channel-wise multiplication and addition. Such overhead is minimal, as shown in Fig. 5, resulting in comparable latency speedup."
        },
        {
            "heading": "6 Conclusion",
            "text": "We present the Outlier Suppression+ framework for addressing asymmetric and consistent outliers\nin LLMs and other transformers. Our framework is simple to use, consisting of both scaling and shifting operations, which can be efficiently and effectively implemented. Experiments demonstrate the efficacy of our methods for suppressing outliers.\nLimitations\nWhile we have observed features of outliers and devised methods to deal with them, the underlying reasons for their emergence and attributes have not been fully understood. This may require an in-depth analysis of the training pipeline, including the procedure and hyperparameters. Such investigations are time-consuming but can benefit both FP and quantized scenarios.\nEthics Statement\nOur Outlier Suppression+ framework aims to improve the quantization performance of transformer language models. It can boost the development of practical and green machine learning and does not incur extra ethical concerns."
        },
        {
            "heading": "Acknowledgment",
            "text": "We sincerely thank the anonymous reviewers for their sincere reviews and valuable suggestions to make this better. We also thank Qi Zhang for the insightful discussion and Jing Liu for helping to build the code of LLaMA. This work was supported by the National Natural Science Foundation of China (No. 62022009), National Natural Science Foundation of China (No. 62306025), the State Key Laboratory of Software Development Environment (SKLSDE-2022ZX-23)."
        },
        {
            "heading": "A Related work",
            "text": "Quantization. Compression has become more and more popular these days (Han et al., 2015; Hinton et al., 2015; Hu et al., 2021; Liu et al., 2022; Xu et al., 2023; Chen et al., 2023). One of its effective techniques called quantization (Jacob et al., 2018) employs low-bit representations for activation and weight in neural networks. Researchers categorize this approach into two pipelines: post-training quantization (PTQ) and quantization-aware training (QAT). QAT (Courbariaux et al., 2015; Choi et al., 2018; Esser et al., 2019; Li et al., 2019; Gong et al., 2019; Shen et al., 2021; Zhang et al., 2018) trains the quantized model end-to-end, necessitating significant GPU resources and the entire training dataset. In contrast, PTQ (Choukroun et al., 2019; Wu et al., 2020; Banner et al., 2018; Wang et al., 2020; Zhao et al., 2019; Nagel et al., 2019) only requires hundreds of samples and limited resource consumption, producing a calibrated model quickly. Recently, several works (Nagel et al., 2020; Hubara et al., 2021; Li et al., 2021; Wei et al., 2022a) proposed to adjust models slightly for improved PTQ performance. Besides, other types of quantization include zero-shot quantization without real calibration data (Cai et al., 2020; Zhang et al., 2021; Guo et al., 2022), mixed-precision with mixed bit-width (Dong et al., 2019; Cai and Vasconcelos, 2020), and FP8 data type (Wang et al., 2018b; Kuzmin et al., 2022; Micikevicius et al., 2022; Jin et al., 2022). Quantization of transformer language models. Recently, there has been a growing interest in the quantization of transformer language models. In the context of QAT, Zafrir et al. (2019) first explores 8-bit quantization for BERT-like models. Shen et al. (2020) introduces group-wise quantization and studies mixed-precision quantization based on Hessian information. Bai et al. (2020); Zhang et al. (2020); Qin et al. (2022) combine distillation strategies with quantization. Kim et al. (2021) approximates the nonlinear function in transformer architectures to enable integer-only inference. Fan et al. (2020) incorporates quantization noise for enhancement. Additionally, Tao et al. (2022) investigates the challenges of quantizing generative models.\nIn the realm of PTQ, researchers have discovered that the poor performance of these models should be attributed to extreme outliers in activations. These outliers exhibit special characteristics\nfrom both channel and token aspects. In terms of channels, outliers consistently emerge in certain channels over different inputs. Bondarenko et al. (2021) employs a per-embedding-group quantization scheme that uses different quantization parameters for distinct channel groups, while Dettmers et al. (2022) suggests utilizing FP16 representations for problematic channels holding signals over 6. Wei et al. (2022b) identifies this feature lying in LayerNorm\u2019s output and migrates the scaling parameter of LayerNorm to subsequent modules to attenuate outliers. Xiao et al. (2022) proposes calculating scaling values by equalizing ranges between activations and weights and evaluates on large language models. Guo et al. (2023) discards normal values adjacent to outliers, making room for outliers with customized GPU support. Compared to them, we design the scaling factors that concern the interactive results of troublesome activation and following weights to scale down channels with outliers offline. Also, we notice the asymmetric presentation of outliers and design a shifting operation. While we operate on corresponding channels between weights and activation, a later work (Liu et al., 2023) adopts the splitting and merging operations to transfer the quantization burden of outlier channels to opposite channels of weights, which might encourage us to design a more flexible technique without the same or opposite channel index requirement. In terms of tokens, different tokens exhibit varying degrees of outliers. Dettmers et al. (2022); Yao et al. (2022) introduce a novel scheme called per-token quantization that dynamically computes quantization parameters for each token. Wei et al. (2022b) investigates the clipping impact of outliers and recommends finding an appropriate clipping range in a token-wise manner.\nBesides, some studies focus on weight quantization, such as Dettmers and Zettlemoyer (2022); Frantar et al. (2022); Zeng et al. (2022); Lin et al. (2023) and some including Yuan et al. (2021); Li et al. (2022), investigate the quantization of Vision Transformer (ViT) models. Interestingly, several studies (Kovaleva et al., 2021; Puccetti et al., 2022) explore the underlying reasons for emerging outliers and trace them back to the pre-training phase."
        },
        {
            "heading": "B Supplementary experiments",
            "text": "BERT-base. We provide detailed results of BERTbase models on GLUE benchmarks in Table 7. Interestingly, we find that models which are sensitive\nto different learning hyperparameters during the fine-tuning phase, such as CoLA and RTE, also exhibit less favorable quantization outcomes. This suggests a possible relationship between quantization and robustness. BERT-large. We also conduct experiments on BERT-large models in Table 8. Results across methods indicate that quantizing BERT-large models is more challenging (e.g., MinMax suffers from a considerable accuracy drop (about 13%) on INT8* compared to BERT-base, and Outlier Suppression also fails on the 6-bit setting). Fortunately, with Outlier Suppression+, the results can be improved, yielding an 18.7% enhancement. OPT. Here, we provide results of OPTs on more tasks. Table 9 is the supplement for Table 2, which further shows consistent performance enhancement of OS+. LLaMA. Recall that we conduct experiments on LLaMA with two different settings in the finegrained quantization section. Table 10 gives the results when quantizing the special and challenging structure (the last layer of FFN) in LLaMA models. It can be observed that ours still earns near-floatingpoint performance on 6-bit quantization and beats others by about 5%\u223c14% in terms of averaged accuracy of the first four tasks, and even four times PPL decrease for WikiText2. By comparing with the easier setting Table 3, we find that the special structure with large signals really leads to much lower 4-bit outcomes across methods, especially\nfor MinMax and SmoothQuant, which makes us think of model design, training techniques, and efficient fine-tuning for quantization.\nC Implementation details\nC.1 OS+\nIn this section, we provide detailed descriptions of our implementation with the core part distilled in algorithm 1. BERT. On the GLUE benchmark, fine-tuned FP models are used for quantization. We randomly select 128 samples and set the batch size to 32. First, a batch of data is used to calculate the effective shifting and scaling signals for problematic activations, especially outputs after LayerNorm here. Then shifting and scaling vectors are fused into former operations and absorbed in later modules. On fused models, we apply the calibration procedure. Particularly, on BERT models, due to the great variance of token range as discussed in Yao et al. (2022); Wei et al. (2022b), we incorporate the Token-Wise Clipping proposed in Outlier Suppression which is an orthogonal technique and weakens outliers from the token aspect. OPTs. For OPTs, we quantize pre-trained models and evaluate them on zero-shot tasks. 128 samples are randomly extracted from one of the train datasets, namely the PILE dataset. As we have observed that LayerNorm produces severe asymmetric outliers on certain channels, the pro-\nposed method is applied here. After obtaining a more quantization-friendly model, the MinMax algorithm collects distribution statistics. Since diverse tokens do not have outliers of varying degrees on these models, advanced clipping techniques are not involved. BLOOM and BLOOMZ. The main pipeline is similar to OPTs. The only exception is using the Token-Wise Clipping as the calibration method because these models hold different outliers among different tokens. The clipping ratios are searched as 0.5% and 1.5% for 8-bit and 6-bit BLOOM, and 0.0% and 0.5% on BLOOMZ. LLaMA. The main pipeline is similar to OPTs with some small differences. First, we use the WikiText2 dataset for calibration. Second, as LLaMA does not have biases, introducing channel-wise shifting might incur a little overhead. Thus, for fair comparisons, we simply omit channel-wise shifting for LLaMA here. Third, when taking the harder setting that quantizes the last layer in FFN, the channel-wise scaling is also conducted thereby updating the quantization scale of up proj and weight parameters of down proj, which does not bring computation overhead during inference. Last,\nunlike OPTs, for tasks with normalized accuracy metrics, we report the normalized accuracy metric instead of the accuracy one to align the original paper (Touvron et al., 2023). This point has also been indicated in each table below.\nC.2 Baselines\nWe introduce the implementation details of baselines here. MinMax obtains the minimum and maximum statistics of the tensor for the quantization clipping range. Percentile (Wu et al., 2020) uses the activation distribution percentile as the quantization clipping range. Using the dev set, we search its hyper-parameters within [0.999, 0.9999, 0.99999]. OMSE (Choukroun et al., 2019) minimizes the mean squared error between quantization and FP signals. PEG (Bondarenko et al., 2021) applies fine-grained quantization to problematic activation from a channel perspective. Outlier Suppression (OS) (Wei et al., 2022b) uses fixed scaling factors to suppress outliers and further clips outliers in a token-wise manner. ZeroQuant (Yao et al., 2022) uses per-token quantization, assigning different quantization parameters to different tokens. This fine-grained scheme from\nthe token aspect also requires dynamic quantization. Meanwhile, for INT8*, we implement per-group weight quantization according to its description. SmoothQuant (Xiao et al., 2022) migrates scaling factors to later modules to smooth problematic activation. Their scaling factors equal the range between activation and weights. For lower bits, we also search its hyper-parameter \u03b1 according to its description for better performance."
        }
    ],
    "title": "Outlier Suppression+: Accurate quantization of large language models by equivalent and effective shifting and scaling",
    "year": 2023
}