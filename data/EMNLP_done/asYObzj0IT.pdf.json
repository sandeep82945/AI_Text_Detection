{
    "abstractText": "Recent advancements in natural language processing have demonstrated the efficacy of pretrained language models for various downstream tasks through prompt-based fine-tuning. In contrast to standard fine-tuning, which relies solely on labeled examples, prompt-based finetuning combines a few labeled examples (few shot) with guidance through prompts tailored for the specific language and task. For lowresource languages, where labeled examples are limited, prompt-based fine-tuning appears to be a promising alternative. In this paper, we compare prompt-based and standard finetuning for the popular task of text classification in Urdu and Roman Urdu languages. We conduct experiments using five datasets, covering different domains, and pre-trained multilingual transformers. The results reveal that significant improvement of up to 13% in accuracy is achieved by prompt-based fine-tuning over standard fine-tuning approaches. This suggests the potential of prompt-based fine-tuning as a valuable approach for low-resource languages with limited labeled data.",
    "authors": [
        {
            "affiliations": [],
            "name": "Faizad Ullah"
        },
        {
            "affiliations": [],
            "name": "Ubaid Azam"
        },
        {
            "affiliations": [],
            "name": "Ali Faheem"
        },
        {
            "affiliations": [],
            "name": "Faisal Kamiran"
        },
        {
            "affiliations": [],
            "name": "Asim Karim"
        }
    ],
    "id": "SP:88d8b359ee0ee9420415309088dda82639079d18",
    "references": [
        {
            "authors": [
                "Muhammad Pervez Akhter",
                "Zheng Jiangbin",
                "Irfan Raza Naqvi",
                "Mohammed Abdelmajeed",
                "Muhammad Tariq Sadiq"
            ],
            "title": "Automatic Detection",
            "year": 2020
        },
        {
            "authors": [
                "Askell"
            ],
            "title": "Language Models are Few-Shot",
            "year": 2020
        },
        {
            "authors": [
                "Kristina Toutanova"
            ],
            "title": "2019. BERT: Pre-training",
            "year": 2019
        },
        {
            "authors": [
                "Jesse Dodge",
                "Gabriel Ilharco",
                "Roy Schwartz",
                "Ali Farhadi",
                "Hannaneh Hajishirzi",
                "Noah Smith."
            ],
            "title": "Fine-tuning pretrained language models: Weight initializations, data orders, and early stopping",
            "venue": "arXiv preprint arXiv:2002.06305.",
            "year": 2020
        },
        {
            "authors": [
                "Tianyu Gao",
                "Adam Fisch",
                "Danqi Chen."
            ],
            "title": "Making pre-trained language models better few-shot learners",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natu-",
            "year": 2021
        },
        {
            "authors": [
                "Santiago Gonz\u00e1lez-Carvajal",
                "Eduardo C GarridoMerch\u00e1n."
            ],
            "title": "Comparing BERT against traditional machine learning text classification",
            "venue": "arXiv preprint arXiv:2005.13012.",
            "year": 2020
        },
        {
            "authors": [
                "Woojeong Jin",
                "Yu Cheng",
                "Yelong Shen",
                "Weizhu Chen",
                "Xiang Ren."
            ],
            "title": "A good prompt is worth millions of parameters: Low-resource prompt-based learning for vision-language models",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for",
            "year": 2022
        },
        {
            "authors": [
                "Dong-Ho Lee",
                "Akshen Kadakia",
                "Kangmin Tan",
                "Mahak Agarwal",
                "Xinyu Feng",
                "Takashi Shibuya",
                "Ryosuke Mitani",
                "Toshiyuki Sekiya",
                "Jay Pujara",
                "Xiang Ren"
            ],
            "title": "Good examples make a faster learner: Simple demonstration-based learning for low-resource NER",
            "year": 2022
        },
        {
            "authors": [
                "Pengfei Liu",
                "Weizhe Yuan",
                "Jinlan Fu",
                "Zhengbao Jiang",
                "Hiroaki Hayashi",
                "Graham Neubig."
            ],
            "title": "PreTrain, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing",
            "venue": "ACM Computing Surveys, 55(9):1\u201335.",
            "year": 2023
        },
        {
            "authors": [
                "Robert Logan IV",
                "Ivana Balazevic",
                "Eric Wallace",
                "Fabio Petroni",
                "Sameer Singh",
                "Sebastian Riedel."
            ],
            "title": "Cutting down on prompts and parameters: Simple few-shot learning with language models",
            "venue": "Findings of the Association for Computational Linguis-",
            "year": 2022
        },
        {
            "authors": [
                "Hammad Rizwan",
                "Muhammad Haroon Shakeel",
                "Asim Karim."
            ],
            "title": "Hate-speech and offensive language detection in Roman Urdu",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2512\u2013",
            "year": 2020
        },
        {
            "authors": [
                "Victor Sanh",
                "Lysandre Debut",
                "Julien Chaumond",
                "Thomas Wolf."
            ],
            "title": "DistilBERT, A Distilled Version Of BERT: Smaller, Faster, Cheaper and Lighter",
            "venue": "arXiv preprint arXiv:1910.01108.",
            "year": 2019
        },
        {
            "authors": [
                "Nathan Schucher",
                "Siva Reddy",
                "Harm de Vries."
            ],
            "title": "The power of prompt tuning for low-resource semantic parsing",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 148\u2013156, Dublin,",
            "year": 2022
        },
        {
            "authors": [
                "Jaehyung Seo",
                "Hyeonseok Moon",
                "Chanhee Lee",
                "Sugyeong Eo",
                "Chanjun Park",
                "Jihoon Kim",
                "Changwoo Chun",
                "Heuiseok Lim."
            ],
            "title": "Plain Template Insertion: Korean-Prompt-Based Engineering for FewShot Learners",
            "venue": "IEEE Access, 10:107587\u2013107597.",
            "year": 2022
        },
        {
            "authors": [
                "Chengyu Song",
                "Taihua Shao",
                "Kejing Lin",
                "Dengfeng Liu",
                "Siyuan Wang",
                "Honghui Chen."
            ],
            "title": "Investigating Prompt Learning for Chinese Few-Shot Text Classification with Pre-Trained Language Models",
            "venue": "Applied Sciences, 12(21):11117.",
            "year": 2022
        },
        {
            "authors": [
                "Yufei Wang",
                "Can Xu",
                "Qingfeng Sun",
                "Huang Hu",
                "Chongyang Tao",
                "Xiubo Geng",
                "Daxin Jiang."
            ],
            "title": "PromDA: Prompt-based data augmentation for lowresource NLU tasks",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational",
            "year": 2022
        },
        {
            "authors": [
                "Tianyi Zhang",
                "Felix Wu",
                "Arzoo Katiyar",
                "Kilian Q Weinberger",
                "Yoav Artzi."
            ],
            "title": "Revisiting Few-sample BERT Fine-tuning",
            "venue": "International Conference on Learning Representations.",
            "year": 2021
        },
        {
            "authors": [
                "Wanjun Zhong",
                "Yifan Gao",
                "Ning Ding",
                "Zhiyuan Liu",
                "Ming Zhou",
                "Jiahai Wang",
                "Jian Yin",
                "Nan Duan."
            ],
            "title": "Improving Task Generalization via Unified Schema Prompt",
            "venue": "arXiv preprint arXiv:2208.03229.",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Recent advancements in natural language processing (NLP) have highlighted the efficacy of pretrained language models (PLMs) in various downstream tasks, including text classification. PLMs, such as (Conneau et al., 2020; Devlin et al., 2019) and (Sanh et al., 2019), have revolutionized NLP by pre-training on extensive textual data to acquire language understanding and common knowledge. However, optimizing the performance of models for various languages, especially low-resource languages like Urdu and Roman Urdu, presents challenges that need to be addressed. Standard fine-tuning usually requires large amounts of taskspecific labeled examples to adapt the parameters of PLMs for robust performance. More recently, prompt-based fine-tuning (Gao et al., 2021) has\nemerged as a promising alternative for improving classification accuracy in low-resource language contexts (An, 2023; Lee et al., 2022; Jin et al., 2022; Schucher et al., 2022; Wang et al., 2022). Prompt-based fine-tuning combines a small set of annotated examples (few shot) with carefully designed language-specific prompts tailored to the task. These prompts explicitly guide the models, providing crucial context and information for precise predictions.\nThis paper presents an empirical evaluation of prompt-based fine-tuning with traditional standard fine-tuning for text classification in Urdu and Roman Urdu. The objective is to determine whether prompt-based fine-tuning surpasses the performance of standard fine-tuning of multilingual PLMs in few shot setting. To do this, we conduct experiments on five diverse datasets spanning different domains and encompassing various classification tasks. These datasets are carefully selected to represent the challenges and nuances of Urdu and Roman Urdu text classification. Additionally, we utilize three pre-trained multilingual transformers (1) BERT-Multilingual (Devlin et al., 2019), (2) DistilBERT (Sanh et al., 2019), and (3) XLM-RoBERTa (Conneau et al., 2020). These transformers have been widely used and proven effective in various NLP tasks across multiple languages. By incorporating numerous transformers, we aim to evaluate the robustness and generalizability of prompt-based fine-tuning across different architectures and language models.\nOur findings reveal a significant improvement of up to 13% in classification accuracy achieved by prompt-based fine-tuning over traditional approaches. This improvement highlights the effectiveness of prompt-based methods in capturing the complex linguistic characteristics and nuances inherent in Urdu texts and demonstrates its potential as a promising alternative for low-resource languages where limited labeled data is available. To\nthe best of our knowledge, there are no published works on prompt engineering or prompt-based finetuning for Urdu text classification. Our work comparing prompt and standard fine-tuning in Urdu can seed further research in this direction."
        },
        {
            "heading": "2 Literature Review",
            "text": "In the past few years, social media platforms have experienced an enormous surge in users. With the rise in digital media usage, there is an increasing demand for automated text classification in Urdu.\nRecently, various transfer learning and data augmentation approaches have been investigated for text classification, such as those described by (Banerjee et al., 2019; Azam et al., 2022; Gonz\u00e1lez-Carvajal and Garrido-Merch\u00e1n, 2020; Alam et al., 2023). These methods use pre-trained language models and fine-tune them on smaller datasets to enhance their performance on specific tasks. However, a recent technique in natural language processing called prompt engineering has recently gained attention from researchers (Liu et al., 2023; Gao et al., 2021). Designing prompts to guide language models improves model predictions by effectively utilizing contextual information. This technique achieves better results with minimal data, including short learning or zero-shot learning. Various types of prompts are used for text classification, namely Human Designed Prompts (Brown et al., 2020), Schema Prompts (Zhong et al., 2022), and Null Prompts (Logan IV et al., 2022).\nWhile prompt engineering research has primarily focused on the English language, there has been recent work exploring its effectiveness in other languages. For example, (Song et al., 2022) conducted research to determine if prompt engineering could improve text classification in Chinese. Their results showed that the use of prompts yielded positive results. Similarly, (Seo et al., 2022) applied prompt engineering techniques to Korean and found that it improved performance for various text classification tasks, including topic and semantic classification, even with few-shot learning. However, no known work has been done to study prompt learning for the Urdu language."
        },
        {
            "heading": "3 Experimental Design",
            "text": "In this section, we describe the datasets and the experimental setup employed to compare the performance of prompt-based and standard fine-tuning approaches for text classification in Urdu. Fine-\ntuning models on limited amounts of labeled data can introduce instability in execution and result in substantial performance variations depending on the choice of data splits (Zhang et al., 2021; Dodge et al., 2020). To generate robust results, we adopt a careful and comprehensive approach, as outlined below.\nWe aim to fine-tune a pre-trained language model L (standard and prompt-based) on task D with label space Y for Urdu and Roman Urdu text classification. Our goal is to develop effective learning strategies that generalize well to an unseen test set (xtestin , y\ntest) \u223c Dtest. In the fewshot setting, we have limited training examples per class. Let K denote the number of training examples per class, and |Y | denote the total number of classes in the task. Thus, the few-shot training set Dtrain consists of Ktot = K\u00d7 |Y | examples, where Dtrain = {(x(i)train, y (i) train)} Ktot\ni=1 . We utilize a development set Ddev to select the optimal model and tune hyper-parameters. The size of Ddev set is equal to the few-shot training set, i.e., |Ddev| = |Dtrain|.\nNow, given the language model L, our process begins by converting the input xin into a token sequence x\u0303, which is then mapped to a sequence of hidden vectors hk \u2208 Rd by the language model L. For example, in a binary sentiment classification task, we can construct a prompt using the input x = Yeh jaga bohat pyari hai. The prompt formulation would be\nxprompt = [CLS] x Yeh [MASK] hai. [SEP]\nIn the literature, various templates are utilized for prompt-based classification tasks (Gao et al., 2021). However, we find that \"Yeh [MASK] hai.\" template is performing better for Urdu and Roman Urdu. We do not mention the results for other templates due to lack of space in the paper.\nThe language model L is then responsible for determining whether it is more suitable to fill in the [MASK] position with \"khubsurat\" (beautiful) or \"fazool\" (useless) as depicted in Figure 1. This prompt-based methodology enables the model to autonomously complete prompts and make sentiment/classification predictions, enabling more efficient and accurate classification.\nFor standard fine-tuning, we use the token sequence\nxfine\u2212tune = [CLS]Yeh jaga bohat pyari hai.[SEP]\nTo construct the training and development sets for each dataset, we select K labeled examples\ngiven as follows: (\nYeh jaga bohat pyari hai. [SEP]\n[SEP] . \u06cc\u0631\u067e\u06cc\u0627 \u0628\u06c1\u062a \u062c\u06af\u06c1 \u06cc\u06c1\u06c1\u06d2\n[CLS] Yeh jaga bohat pyari hai. Ye hai. [SEP]\n[SEP] \u06c1\u06d2 \u06cc\u06c1 . \u06cc\u06c1\u062c\u06af\u06c1\u0628\u06c1\u062a\u06cc\u0631\u067e\u06cc\u0627\u06c1\u06d2 [CLS]\nInputTemplate\n(a) Standard Fine-tuning (b) Prompt-based Fine-tuning\nInput Template [CLS]\n[MASK]\n[MASK]\n[CLS]: \"This place is very beautiful.\"), (\nYeh jaga bohat pyari hai. [SEP]\n[SEP] . \u06cc\u0631\u067e\u06cc\u0627 \u0628\u06c1\u062a \u062c\u06af\u06c1 \u06cc\u06c1\u06c1\u06d2\n[CLS] Yeh jaga bohat pyari hai. Ye hai. [SEP]\n[SEP] \u06c1\u06d2 \u06cc\u06c1 . \u06cc\u06c1\u062c\u06af\u06c1\u0628\u06c1\u062a\u06cc\u0631\u067e\u06cc\u0627\u06c1\u06d2 [CLS]\nInput\n(a) Standard Fine-tuni (b) Prompt-based Fine-tuning\nInput Template [CLS]\n[MASK]\n[MASK]\n[CLS] : It is [MASK]), (Yeh [MASK] hai : It is [MASK]) ( CLS head\nYeh jaga bohat pyari hai. [SEP]\n[SEP] . \u06cc\u0631\u067e\u06cc\u0627 \u0628\u06c1\u062a \u062c\u06af\u06c1 \u06cc\u06c1\u06c1\u06d2\nlabel: positive\nlabel: negative\n\ud835\udc3f\ud835\udc4e\ud835\udc4f\ud835\udc52\ud835\udc59 \ud835\udc60\ud835\udc5d\ud835\udc4e\ud835\udc50\ud835\udc52 \ud835\udc4c\n[CLS] Yeh jaga bohat pyari hai. Ye hai. [SEP]\n[SEP] \u06c1\u06d2 \u06cc\u06c1 . \u06cc\u06c1\u062c\u06af\u06c1\u0628\u06c1\u062a\u06cc\u0631\u067e\u06cc\u0627\u06c1\u06d2 [CLS]\n\ud835\udc3f\ud835\udc4e\ud835\udc4f\ud835\udc52\ud835\udc59 \ud835\udc5a\ud835\udc4e\ud835\udc5d\ud835\udc5d\ud835\udc56\ud835\udc5b\ud835\udc54 \ud835\udc40(\ud835\udc4c)\nInputTemplate\n(a) Standard Fine-tuning (b) Prompt-based Fine-tuning\nInput Template [CLS]\nMLM\nhead\nMLM head\n[MASK]\n[MASK]\n(label: positive) \u062a\u0631\u0628\u0635\u0648 \u062e\u0648 (label: negative) \u0644\u0641\u0636\u0648\nkhubs rat (label: positive)\nfazool (label: negative)\n[CLS]\n: beautiful), (\nCLS head\nYeh jaga bohat pyari hai. [SEP]\n[SEP] . \u06cc\u0631\u067e\u06cc\u0627 \u0628\u06c1\u062a \u062c\u06af\u06c1 \u06cc\u06c1\u06c1\u06d2\nlabel: positive\nlabel: negative\n\ud835\udc3f\ud835\udc4e\ud835\udc4f\ud835\udc52\ud835\udc59 \ud835\udc60\ud835\udc5d\ud835\udc4e\ud835\udc50\ud835\udc52 \ud835\udc4c\n[CLS] Yeh jaga bohat pyari hai. Ye hai. [SEP]\n[SEP] \u06c1\u06d2 \u06cc\u06c1 . \u06cc\u06c1\u062c\u06af\u06c1\u0628\u06c1\u062a\u06cc\u0631\u067e\u06cc\u0627\u06c1\u06d2 [CLS]\n\ud835\udc3f\ud835\udc4e\ud835\udc4f\ud835\udc52\ud835\udc59 \ud835\udc5a\ud835\udc4e\ud835\udc5d\ud835\udc5d\ud835\udc56\ud835\udc5b\ud835\udc54 \ud835\udc40(\ud835\udc4c)\nInputTemplate\n(a) Standard Fine-tuning (b) Prompt-based Fine-tuning\nInput Template [CLS]\nMLM head\nMLM h ad\n[MASK]\n[MASK]\n(label: positive) \u062a\u0631\u0628\u0635\u0648 \u062e\u0648 (label: negative) \u0644\u0641\u0636\u0648\nkhubsurat (label: positive) f zo l (label: negative)\n[CLS]\n: useless)\nfrom Dtrain for each class, resulting in a total of Y \u00d7K labeled examples, where Y represents the total number of classes (labels) in the dataset. In our experiments, we consider K = 4, 8, 16 and 32. We also refer to this number as splits. The remaining examples from Dtrain are reserved for the test set (with no labels). To ensure fair evaluation, we perform multiple rounds of testing. We select samples randomly from the unlabeled test set not used in each round\u2019s training Dtrain and development Ddev sets. This process is repeated five times, allowing for a comprehensive evaluation of the models\u2019 performance across different test sets.\nWe evaluate classification performance using accuracy and macro F1-score. We report the mean and standard deviation of each measure over the five runs and for different splits. This approach allows us to draw robust conclusions regarding the effectiveness of prompt-based and standard finetuning approaches for text classification.\nPrompt-based fine-tuning involves using language-specific prompts tailored to Urdu and Roman Urdu to provide additional context to the models during training. This helps capture the intricacies of the languages and improve text classification performance. No prompts are used in standard fine-tuning, and the models rely solely on labeled examples. Comparing these approaches allows us to assess the impact of prompts in leveraging language-specific knowledge.\nWe used the Hugging Face1 library for standard fine-tuning of models, while for prompt-based fine-tuning, we employed the OpenPrompt2 library. OpenPrompt is an open-source framework designed explicitly for prompt learning, providing a comprehensive set of tools and resources for this approach. Our fine-tuning process (standard and prompt-based) utilizes a learning rate of 2e \u2212 5. The optimization method is AdamW (an Adam optimizer variant), and the loss function is CrossEntropy Loss. The number of epochs for each training is 10. Our study utilizes three pre-trained multilingual transformers, namely (1) xlm-roberta-base, (2) bert-base-multilingual-cased, and (3) distilbertbase-multilingual-cased, which are publicly available on HuggingFace library."
        },
        {
            "heading": "3.1 Datasets",
            "text": "Our study utilizes five distinct datasets that span different domains, including emotion and offensive language detection. Specifically, the Urdu Nastalique Emotions Dataset (UNED) (Bashir et al., 2023) consists of 1119 instances for emotion detection, featuring labels such as Neutral, Happy, Sad, Anger, Fear, and Love. The URDU OFFENSIVE DATASET (UOD) (Akhter et al., 2020) and Roman Urdu Dataset (RUD) (Akhter et al., 2020) play pivotal roles in offensive language detection, with instance counts of 2106 and 147116, respectively. These datasets employ Of-\n1https://huggingface.co/ 2https://thunlp.github.io/OpenPrompt/\nfensive and Non-Offensive labels. The Roman Urdu Emotion Detection Dataset (RUED) (Arshad et al., 2019), consisting of 2961 instances, facilitates emotion detection with labels including Anger, Sad, Happy, and Neutral. The Roman Urdu HateSpeech and Offensive Language Detection (RUHSOLD) (Rizwan et al., 2020) dataset comprising 10012 instances revolves around hate speech and offensive language detection, utilizing labels such as Hate Speech / Offensive Language and NonOffensive. RUHSOLD dataset originally used six labels Abusive/Offensive, Sexism, Religious Hate, Profane, and Normal. However, to make the tasks easier, we converted this multiclass problem to a binary class problem. The datasets selected for our experiments, however, represent both binary and multiclass text classification problems."
        },
        {
            "heading": "4 Results and Discussion",
            "text": "Table 1 shows the mean classification accuracy for prompt-based and standard fine-tuned pre-trained language models on different datasets. Mean accuracy is reported for splits of 4, 8, 16, and 32. In the UNED dataset, XLM-RoBERTa demonstrated the highest accuracy (after prompt-based fine-tuning) of 44.8% on 16 splits, outperforming all other models. Similarly, for RUD dataset, fine-tuned XLMRoBERTa remained the top performer with an accuracy of 84.8%. For the RUHSOLD dataset, finetuned XLM-RoBERTa consistently dominated in performance across all splits, achieving the highest accuracy of 63.8% on 32 splits.\nOn the RUED dataset, prompt-based fine-tuned XLM-RoBERTa outperformed other models with the highest accuracy of 35.6% for splits 32. However, for splits 8 and 16, prompt fine-tuned DistilBERT and BERT-Multilingual models performed better, respectively. In the case of the UOD dataset, the results were more varied. Prompt-based finetuned XLM-RoBERTa outperformed other models for splits 4 and 16. However, for splits 8 and 32, prompt fine-tuned BERT-Multilingual yielded better results compared to the other models.\nIn our experiments, we consistently observed that prompt fine-tuned models outperformed standard fine-tuned models achieving up to 13% absolute improvement and 5.44% average improvement in accuracy across all tasks. Additionally, it is noteworthy that as the data splits increased, the models\u2019 accuracy also increased, as evident from Figures 2, 3, 4, 5, and 6. These figures show the\nvariation of accuracy with split size for UOD, RUD, RUED, RUHSOLD, and UNED datasets, respectively, under prompt-based and standard fine-tuning of the respective best-performing models. In general, accuracy improves with an increase in split size. However, for prompt-based fine-tuned models, the increase in accuracy was more significant compared to standard fine-tuned models. It is also worth noting that standard fine-tuning almost always lags behind prompt-based fine-tuning for all models and datasets for up to 32 splits, confirming that for the limited number of labeled examples, prompt-based fine-tuning is preferable.\nUNED UOD RUD RUHSOLD RUED\n4 8 16 4 8 16 32 4 8 16 32 4 8 16 32 4 8 16 32\nStandard fine-tuning BERT-M 21.4 26.8 32.0 60.2 65.8 71.2 79.8 63.2 66.4 67.2 72.2 53.0 55.0 57.6 60.0 27.2 28.4 28.0 31.8 DistilBERT 18.8 20.8 23.4 58.6 57.4 68.4 80.0 55.0 64.2 63.2 71.0 49.2 55.6 56.6 60.4 27.4 29.8 31.6 25.4 XLM-R 19.0 27.4 32.0 54.4 49.0 74.6 81.2 55.0 59.8 66.6 75.2 54.2 56.2 57.8 60.2 19.4 23.0 26.8 26.6\nPrompt based fine-tuning BERT-M 23.2 27.4 36.2 60.4 68.2 77.4 89.2 63.8 70.6 70.6 77.6 54.2 53.8 58.4 61.8 26.8 29.0 32.2 30.4 DistilBERT 24.4 25.6 32.8 57.6 64.2 72.6 81.4 65.2 70.8 68.4 75.6 52.8 54.8 58.4 60.8 27.6 30.0 31.8 33.4 XLM-R 27.4 31.8 44.8 65.8 67.2 79.6 87.8 67.8 76.8 80.2 84.8 57.2 59.0 64.0 63.8 29.0 28.4 31.0 35.6\nTable 1: Mean accuracy comparison of standard and prompt-based fine-tuning for Urdu and Roman Urdu datasets using K = 4, 8, 16 and 32, where K represents the number of samples per class. The standard deviation of these experiments is given in Appendix A.\nWe present additional experimental results in the Appendices. Appendix A gives the standard deviations corresponding to the mean accuracy values reported in Table 1. Appendix B presents the mean F1-score of all the experiments on different models and datasets. Appendix C studies the impact of fine-tuning by comparing the performance of zeroshot (no fine-tuning) and 4-shot prompt-based and standard fine-tuned models."
        },
        {
            "heading": "5 Conclusion",
            "text": "In this paper, we compare prompt-based fine-tuning with standard fine-tuning for Urdu and Roman Urdu text classification when restricted to dozens of training examples only. We perform experi-\nments using several multilingual pre-trained language models on different classification datasets. Regardless of the training set size or the specific classification task, prompt-based fine-tuning consistently outperforms standard fine-tuning, highlighting its robustness and generalizability. The clear prediction superiority of prompt-based approaches, coupled with its generally lower computational cost, makes it an attractive alternative to traditional fine-tuning methods for low-resource languages. The insights gained from this study can inspire future research and encourage the adoption of prompt-based techniques in other low-resource languages.\nLimitations\nIt is essential to note that the presented experiments and results are relevant to Urdu and Roman Urdu. Consequently, the generalizability of the findings to other languages remains to be determined. The effectiveness and performance of the approach in diverse linguistic contexts may vary significantly. Thus, care should be exercised when inferring the results to languages beyond Urdu and Roman Urdu. Another significant factor to consider is the reliance of prompt-based techniques on domain expertise. The success of prompt-based fine-tuning heavily hinges upon formulating appropriate prompts that adequately capture the desired semantic information. Effective prompts require a deep understanding of the language, context, and the specific task. However, this process can introduce subjectivity and potential bias, as prompt design involves making subjective decisions and assumptions. These subjective elements may influence the performance of the approach and limit its objectivity in specific scenarios.\nThe current findings provide valuable insights into the utility of prompt-based fine-tuning for lowresource languages and text classification. Future\nstudies should investigate the performance of this approach across a broader range of languages and tasks, considering different linguistic characteristics and data availability. It is also essential to acknowledge that the evaluation of the proposed approach was focused solely on text classification. The applicability and performance of prompt-based fine-tuning in other NLP tasks, such as named entity recognition, sentiment analysis, or machine translation, still need to be explored. Therefore, caution should be exercised when attempting to generalize the findings to other NLP domains, as the effectiveness of prompt-based fine-tuning may vary depending on the task and its linguistic properties.\nThe limitations outlined in this study highlight the need for research and improvement in promptbased fine-tuning. While the approach shows promise for low-resource languages and text classification, its generalizability, subjectivity in a prompt design, limited task scope, and broader data scarcity challenges necessitate further investigation and refinement. Addressing these limitations will enhance the applicability and effectiveness of prompt-based fine-tuning in diverse language settings and NLP tasks.\nEthics Statement\nWe have carefully considered our study\u2019s ethical implications and taken the measures into account. Data privacy and confidentiality were strictly maintained throughout the research process. We consciously tried to mitigate biases and subjectivity in prompt design and analysis. Our approach is designed to assist human decision-making rather than replace it, emphasizing the importance of human involvement. We have given due consideration to the ethical aspects of prompt-based fine-tuning, including fairness and privacy issues. Our work contributes to advancing NLP knowledge and benefits various stakeholders. Overall, this study aligns with established ethical standards and promotes the responsible application of prompt-based finetuning in low-resource languages and text classification."
        },
        {
            "heading": "A Standard Deviation of Accuracy",
            "text": "Table 2 shows the standard deviation of accuracy over 5 runs for different approaches, models, and datasets. The standard deviation corresponds to the mean accuracy reported in Table 1."
        },
        {
            "heading": "B F1-score Comparison",
            "text": "Table 3 gives the mean F1-score for prompt-based and standard fine-tuning using different models and on different datasets. It is observed that F1-score results follow the same trend as that for accuracy results given in Table 1. Thus, despite significant class imbalance in some datasets the performance of text classification measured via accuracy and macro F1-score is fairly consistent.\nUNED UOD RUD RUHSOLD RUED\n4 8 16 4 8 16 32 4 8 16 32 4 8 16 32 4 8 16 32\nStandard fine-tuning BERT-M 6.06 3.96 7.0 4.20 2.28 2.38 3.56 3.42 0.89 3.11 2.38 3.46 4.63 5.02 2.82 4.60 4.66 5.04 2.78 DistilBERT 3.27 3.96 4.27 1.94 4.72 6.42 2.0 4.84 2.48 3.34 2.23 2.04 1.81 1.14 1.51 6.58 5.67 5.17 3.28 XLM-R 4.24 3.78 7.0 8.35 0.0 7.02 3.76 6.40 8.19 4.03 3.34 1.48 4.91 4.20 10.9 2.50 5.87 3.76 5.59\nPrompt based fine-tuning BERT-M 2.48 2.30 2.48 4.92 3.34 10.4 1.78 5.40 1.34 2.07 1.51 4.02 2.16 2.30 3.70 5.06 4.18 7.72 6.65 DistilBERT 2.70 4.03 3.11 4.15 5.80 4.50 5.72 2.77 1.64 4.15 1.14 4.43 2.16 5.12 2.68 4.33 2.34 5.35 3.43 XLM-R 2.19 3.03 7.01 9.47 6.05 5.72 2.77 5.35 2.77 4.20 2.94 3.76 3.08 4.52 3.70 4.24 9.39 2.0 6.76\nStandard fine-tuning BERT-M 16.8 25.2 27.8 57.4 64.8 71.2 79.8 62.0 65.6 67.0 72.0 46.6 52.0 56.4 59.6 23.0 25.4 27.0 29.2 DistilBERT 10.8 15.2 17.4 50.6 50.4 66.8 79.8 45.6 63 59.4 70.6 42.2 46.6 54.0 56.2 22 21.4 25.2 22.6 XLM-R 10.0 23.8 27.8 43.4 33 73.2 80.8 45.2 52.6 62.8 75.0 46.0 40.8 56.8 56.4 12.4 15.2 22.4 23.4\nPrompt based fine-tuning BERT-M 20.4 23.4 32.8 59.2 67.6 76.0 89.2 63.4 69.8 70.0 77.4 52.6 51.4 54.4 61.8 22.8 25.0 28.0 26.8 DistilBERT 20.4 22.8 29.4 55.2 63.4 72.4 81.4 64.2 70.6 67.8 75.6 51.8 53.2 57.6 60.6 24.6 27.2 27.8 30.0 XLM-R 25.6 28.8 45.0 63.6 66.0 79.6 87.8 66.4 76.8 79.8 84.8 55.8 57.0 61.8 63.4 26.8 24.4 30.0 30.4"
        },
        {
            "heading": "C Accuracy and F1-Score Comparison for Zero-Shot and 4-Shot",
            "text": "Tables 4 and 5 present accuracy and F1-score, respectively, for zero-shot and fine-tuned with four examples on all five datasets. Results are shown for standard and prompt-based techniques using 3 pre-trained language models. It is clear from the results that zero-shot prompt-based predictions lag behind those for 4-shot prompt-based fine-tuned predictions on all the datasets. In other words, a performance boost is obtained by employing promptbased fine-tuning even with 4 training examples. This underscores our argument that prompt-based fine-tuning holds significant potential in resourceconstrained scenarios and is particularly relevant in the case of the Urdu language."
        }
    ],
    "title": "Comparing Prompt-Based and Standard Fine-Tuning for Urdu Text Classification",
    "year": 2023
}