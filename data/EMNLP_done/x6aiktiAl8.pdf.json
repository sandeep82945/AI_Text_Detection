{
    "abstractText": "Despite the excellent performance of visionlanguage pre-trained models (VLPs) on conventional VQA task, they still suffer from two problems: First, VLPs tend to rely on language biases in datasets and fail to generalize to outof-distribution (OOD) data. Second, they are inefficient in terms of memory footprint and computation. Although promising progress has been made in both problems, most existing works tackle them independently. To facilitate the application of VLP to VQA tasks, it is imperative to jointly study VLP compression and OOD robustness, which, however, has not yet been explored. This paper investigates whether a VLP can be compressed and debiased simultaneously by searching sparse and robust subnetworks. To this end, we systematically study the design of a training and compression pipeline to search the subnetworks, as well as the assignment of sparsity to different modality-specific modules. Our experiments involve 3 VLPs, 2 compression methods, 4 training methods, 2 datasets and a range of sparsity levels. Our results show that there indeed exist sparse and robust subnetworks, which are competitive with the debiased full VLP and clearly outperform the debiasing SoTAs with fewer parameters on OOD datasets VQA-CP v2 and VQA-VS.1",
    "authors": [
        {
            "affiliations": [],
            "name": "Qingyi Si"
        },
        {
            "affiliations": [],
            "name": "Yuanxin Liu"
        },
        {
            "affiliations": [],
            "name": "Zheng Lin"
        },
        {
            "affiliations": [],
            "name": "Peng Fu"
        },
        {
            "affiliations": [],
            "name": "Yanan Cao"
        },
        {
            "affiliations": [],
            "name": "Weiping Wang"
        }
    ],
    "id": "SP:737c4992de37d8004cb6d49e708bd77b7578290a",
    "references": [
        {
            "authors": [
                "Aishwarya Agrawal",
                "Dhruv Batra",
                "Devi Parikh."
            ],
            "title": "Analyzing the behavior of visual question answering models",
            "venue": "arXiv preprint arXiv:1606.07356.",
            "year": 2016
        },
        {
            "authors": [
                "Aishwarya Agrawal",
                "Dhruv Batra",
                "Devi Parikh",
                "Aniruddha Kembhavi."
            ],
            "title": "Don\u2019t just assume; look and answer: Overcoming priors for visual question answering",
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages",
            "year": 2018
        },
        {
            "authors": [
                "Peter Anderson",
                "Xiaodong He",
                "Chris Buehler",
                "Damien Teney",
                "Mark Johnson",
                "Stephen Gould",
                "Lei Zhang."
            ],
            "title": "Bottom-up and top-down attention for image captioning and visual question answering",
            "venue": "Proceedings of the IEEE conference on computer vision",
            "year": 2018
        },
        {
            "authors": [
                "Stanislaw Antol",
                "Aishwarya Agrawal",
                "Jiasen Lu",
                "Margaret Mitchell",
                "Dhruv Batra",
                "C Lawrence Zitnick",
                "Devi Parikh."
            ],
            "title": "Vqa: Visual question answering",
            "venue": "Proceedings of the IEEE international conference on computer vision, pages 2425\u20132433.",
            "year": 2015
        },
        {
            "authors": [
                "Yoshua Bengio",
                "Nicholas L\u00e9onard",
                "Aaron C. Courville."
            ],
            "title": "Estimating or propagating gradients through stochastic neurons for conditional computation",
            "venue": "CoRR, abs/1308.3432.",
            "year": 2013
        },
        {
            "authors": [
                "Remi Cadene",
                "Corentin Dancette",
                "Matthieu Cord",
                "Devi Parikh"
            ],
            "title": "Rubi: Reducing unimodal biases for visual question answering",
            "venue": "Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "Long Chen",
                "Xin Yan",
                "Jun Xiao",
                "Hanwang Zhang",
                "Shiliang Pu",
                "Yueting Zhuang."
            ],
            "title": "Counterfactual samples synthesizing for robust visual question answering",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Tianlong Chen",
                "Jonathan Frankle",
                "Shiyu Chang",
                "Sijia Liu",
                "Yang Zhang",
                "Zhangyang Wang",
                "Michael Carbin."
            ],
            "title": "The lottery ticket hypothesis for pretrained BERT networks",
            "venue": "NeurIPS, pages 15834\u2013 15846.",
            "year": 2020
        },
        {
            "authors": [
                "Xinlei Chen",
                "Hao Fang",
                "Tsung-Yi Lin",
                "Ramakrishna Vedantam",
                "Saurabh Gupta",
                "Piotr Doll\u00e1r",
                "C Lawrence Zitnick."
            ],
            "title": "Microsoft coco captions: Data collection and evaluation server",
            "venue": "arXiv preprint arXiv:1504.00325.",
            "year": 2015
        },
        {
            "authors": [
                "Christopher Clark",
                "Mark Yatskar",
                "Luke Zettlemoyer."
            ],
            "title": "Don\u2019t take the easy way out: Ensemble based methods for avoiding known dataset biases",
            "venue": "arXiv preprint arXiv:1909.03683.",
            "year": 2019
        },
        {
            "authors": [
                "Yang Ding",
                "Jing Yu",
                "Bang Liu",
                "Yue Hu",
                "Mingxin Cui",
                "Qi Wu."
            ],
            "title": "Mukea: Multimodal knowledge extraction and accumulation for knowledge-based visual question answering",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pat-",
            "year": 2022
        },
        {
            "authors": [
                "Zi-Yi Dou",
                "Yichong Xu",
                "Zhe Gan",
                "Jianfeng Wang",
                "Shuohang Wang",
                "Lijuan Wang",
                "Chenguang Zhu",
                "Pengchuan Zhang",
                "Lu Yuan",
                "Nanyun Peng"
            ],
            "title": "An empirical study of training end-to-end vision-and-language transformers",
            "year": 2022
        },
        {
            "authors": [
                "Mengnan Du",
                "Subhabrata Mukherjee",
                "Yu Cheng",
                "Milad Shokouhi",
                "Xia Hu",
                "Ahmed Hassan Awadallah."
            ],
            "title": "What do compressed large language models forget? robustness challenges in model compression",
            "venue": "CoRR, abs/2110.08419.",
            "year": 2021
        },
        {
            "authors": [
                "Zhiyuan Fang",
                "Jianfeng Wang",
                "Xiaowei Hu",
                "Lijuan Wang",
                "Yezhou Yang",
                "Zicheng Liu."
            ],
            "title": "Compressing visual-linguistic model via knowledge distillation",
            "venue": "ICCV, pages 1408\u20131418. IEEE.",
            "year": 2021
        },
        {
            "authors": [
                "Jonathan Frankle",
                "Michael Carbin."
            ],
            "title": "The lottery ticket hypothesis: Finding sparse, trainable neural networks",
            "venue": "ICLR. OpenReview.net.",
            "year": 2019
        },
        {
            "authors": [
                "Yonggan Fu",
                "Qixuan Yu",
                "Yang Zhang",
                "Shang Wu",
                "Xu Ouyang",
                "David D. Cox",
                "Yingyan Lin."
            ],
            "title": "Drawing robust scratch tickets: Subnetworks with inborn robustness are found within randomly initialized networks",
            "venue": "NeurIPS, pages 13059\u201313072.",
            "year": 2021
        },
        {
            "authors": [
                "Trevor Gale",
                "Erich Elsen",
                "Sara Hooker."
            ],
            "title": "The state of sparsity in deep neural networks",
            "venue": "CoRR, abs/1902.09574.",
            "year": 2019
        },
        {
            "authors": [
                "Zhe Gan",
                "Yen-Chun Chen",
                "Linjie Li",
                "Tianlong Chen",
                "Yu Cheng",
                "Shuohang Wang",
                "Jingjing Liu",
                "Lijuan Wang",
                "Zicheng Liu."
            ],
            "title": "Playing lottery tickets with vision and language",
            "venue": "AAAI, pages 652\u2013660. AAAI Press.",
            "year": 2022
        },
        {
            "authors": [
                "Tejas Gokhale",
                "Pratyay Banerjee",
                "Chitta Baral",
                "Yezhou Yang."
            ],
            "title": "Mutant: A training paradigm for out-of-distribution generalization in visual question answering",
            "venue": "arXiv preprint arXiv:2009.08566.",
            "year": 2020
        },
        {
            "authors": [
                "Mitchell A. Gordon",
                "Kevin Duh",
                "Nicholas Andrews."
            ],
            "title": "Compressing BERT: studying the effects of weight pruning on transfer learning",
            "venue": "RepL4NLP@ACL, pages 143\u2013155.",
            "year": 2020
        },
        {
            "authors": [
                "Yash Goyal",
                "Tejas Khot",
                "Douglas Summers-Stay",
                "Dhruv Batra",
                "Devi Parikh."
            ],
            "title": "Making the v in vqa matter: Elevating the role of image understanding in visual question answering",
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern",
            "year": 2017
        },
        {
            "authors": [
                "Gabriel Grand",
                "Yonatan Belinkov."
            ],
            "title": "Adversarial regularization for visual question answering: Strengths, shortcomings, and side effects",
            "venue": "NAACL HLT 2019, page 1.",
            "year": 2019
        },
        {
            "authors": [
                "Shupeng Gui",
                "Haotao Wang",
                "Haichuan Yang",
                "Chen Yu",
                "Zhangyang Wang",
                "Ji Liu."
            ],
            "title": "Model compression with adversarial robustness: A unified optimization framework",
            "venue": "NeurIPS, pages 1283\u20131294.",
            "year": 2019
        },
        {
            "authors": [
                "Yangyang Guo",
                "Liqiang Nie",
                "Zhiyong Cheng",
                "Qi Tian",
                "Min Zhang."
            ],
            "title": "Loss re-scaling vqa: revisiting the language prior problem from a classimbalance view",
            "venue": "IEEE Transactions on Image Processing, 31:227\u2013238.",
            "year": 2021
        },
        {
            "authors": [
                "Song Han",
                "Jeff Pool",
                "John Tran",
                "William Dally."
            ],
            "title": "Learning both weights and connections for efficient neural network",
            "venue": "Advances in Neural Information Processing Systems 28, pages 1135\u20131143. Curran Associates, Inc.",
            "year": 2015
        },
        {
            "authors": [
                "Xinzhe Han",
                "Shuhui Wang",
                "Chi Su",
                "Qingming Huang",
                "Qi Tian."
            ],
            "title": "Greedy gradient ensemble for robust visual question answering",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1584\u20131593.",
            "year": 2021
        },
        {
            "authors": [
                "Geoffrey E. Hinton."
            ],
            "title": "Training products of experts by minimizing contrastive divergence",
            "venue": "Neural Comput., 14(8):1771\u20131800.",
            "year": 2002
        },
        {
            "authors": [
                "Robin Jia",
                "Percy Liang."
            ],
            "title": "Adversarial examples for evaluating reading comprehension systems",
            "venue": "arXiv preprint arXiv:1707.07328.",
            "year": 2017
        },
        {
            "authors": [
                "Xiaoqi Jiao",
                "Yichun Yin",
                "Lifeng Shang",
                "Xin Jiang",
                "Xiao Chen",
                "Linlin Li",
                "Fang Wang",
                "Qun Liu."
            ],
            "title": "Tinybert: Distilling BERT for natural language understanding",
            "venue": "EMNLP (Findings), pages 4163\u20134174.",
            "year": 2020
        },
        {
            "authors": [
                "Chenchen Jing",
                "Yuwei Wu",
                "Xiaoxun Zhang",
                "Yunde Jia",
                "Qi Wu."
            ],
            "title": "Overcoming language priors in vqa via decomposed linguistic representations",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, pages 11181\u201311188.",
            "year": 2020
        },
        {
            "authors": [
                "Gouthaman Kv",
                "Anurag Mittal."
            ],
            "title": "Reducing language biases in visual question answering with visually-grounded question encoder",
            "venue": "European Conference on Computer Vision, pages 18\u201334. Springer.",
            "year": 2020
        },
        {
            "authors": [
                "Zhenzhong Lan",
                "Mingda Chen",
                "Sebastian Goodman",
                "Kevin Gimpel",
                "Piyush Sharma",
                "Radu Soricut."
            ],
            "title": "ALBERT: A lite BERT for self-supervised learning of language representations",
            "venue": "ICLR. OpenReview.net.",
            "year": 2020
        },
        {
            "authors": [
                "Chenliang Li",
                "Haiyang Xu",
                "Junfeng Tian",
                "Wei Wang",
                "Ming Yan",
                "Bin Bi",
                "Jiabo Ye",
                "Hehong Chen",
                "Guohai Xu",
                "Zheng Cao",
                "Ji Zhang",
                "Songfang Huang",
                "Fei Huang",
                "Jingren Zhou",
                "Luo Si"
            ],
            "title": "mplug: Effective and efficient vision-language learning",
            "year": 2022
        },
        {
            "authors": [
                "Junnan Li",
                "Ramprasaath Selvaraju",
                "Akhilesh Gotmare",
                "Shafiq Joty",
                "Caiming Xiong",
                "Steven Chu Hong Hoi."
            ],
            "title": "Align before fuse: Vision and language representation learning with momentum distillation",
            "venue": "Advances in neural information processing systems,",
            "year": 2021
        },
        {
            "authors": [
                "Liunian Harold Li",
                "Mark Yatskar",
                "Da Yin",
                "Cho-Jui Hsieh",
                "Kai-Wei Chang."
            ],
            "title": "Visualbert: A simple and performant baseline for vision and language",
            "venue": "arXiv preprint arXiv:1908.03557.",
            "year": 2019
        },
        {
            "authors": [
                "Wei Li",
                "Can Gao",
                "Guocheng Niu",
                "Xinyan Xiao",
                "Hao Liu",
                "Jiachen Liu",
                "Hua Wu",
                "Haifeng Wang."
            ],
            "title": "Unimo: Towards unified-modal understanding and generation via cross-modal contrastive learning",
            "venue": "arXiv preprint arXiv:2012.15409.",
            "year": 2020
        },
        {
            "authors": [
                "Zhuohan Li",
                "Eric Wallace",
                "Sheng Shen",
                "Kevin Lin",
                "Kurt Keutzer",
                "Dan Klein",
                "Joseph E. Gonzalez."
            ],
            "title": "Train large, then compress: Rethinking model size for efficient training and inference of transformers",
            "venue": "CoRR, abs/2002.11794.",
            "year": 2020
        },
        {
            "authors": [
                "Chen Liang",
                "Simiao Zuo",
                "Minshuo Chen",
                "Haoming Jiang",
                "Xiaodong Liu",
                "Pengcheng He",
                "Tuo Zhao",
                "Weizhu Chen."
            ],
            "title": "Super tickets in pre-trained language models: From model compression to improving generalization",
            "venue": "ACL/IJCNLP, pages 6524\u2013",
            "year": 2021
        },
        {
            "authors": [
                "Zujie Liang",
                "Haifeng Hu",
                "Jiaying Zhu."
            ],
            "title": "Lpf: A language-prior feedback objective function for debiased visual question answering",
            "venue": "arXiv preprint arXiv:2105.14300.",
            "year": 2021
        },
        {
            "authors": [
                "Zujie Liang",
                "Weitao Jiang",
                "Haifeng Hu",
                "Jiaying Zhu."
            ],
            "title": "Learning to contrast the counterfactual samples for robust visual question answering",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
            "year": 2020
        },
        {
            "authors": [
                "Yuanxin Liu",
                "Zheng Lin",
                "Fengcheng Yuan."
            ],
            "title": "ROSITA: refined BERT compression with integrated techniques",
            "venue": "AAAI, pages 8715\u20138722. AAAI Press.",
            "year": 2021
        },
        {
            "authors": [
                "Yuanxin Liu",
                "Fandong Meng",
                "Zheng Lin",
                "Peng Fu",
                "Yanan Cao",
                "Weiping Wang",
                "Jie Zhou."
            ],
            "title": "Learning to win lottery tickets in BERT transfer via task-agnostic mask training",
            "venue": "CoRR, abs/2204.11218.",
            "year": 2022
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter."
            ],
            "title": "Decoupled weight decay regularization",
            "venue": "arXiv preprint arXiv:1711.05101.",
            "year": 2017
        },
        {
            "authors": [
                "Christos Louizos",
                "Max Welling",
                "Diederik P. Kingma."
            ],
            "title": "Learning sparse neural networks through l_0 regularization",
            "venue": "ICLR (Poster). OpenReview.net.",
            "year": 2018
        },
        {
            "authors": [
                "Rabeeh Karimi Mahabadi",
                "James Henderson."
            ],
            "title": "Simple but effective techniques to reduce biases",
            "venue": "arXiv preprint arXiv:1909.06321, 9.",
            "year": 2019
        },
        {
            "authors": [
                "Arun Mallya",
                "Dillon Davis",
                "Svetlana Lazebnik."
            ],
            "title": "Piggyback: Adapting a single network to multiple tasks by learning to mask weights",
            "venue": "ECCV, volume 11208 of Lecture Notes in Computer Science, pages 72\u201388. Springer.",
            "year": 2018
        },
        {
            "authors": [
                "Varun Manjunatha",
                "Nirat Saini",
                "Larry S Davis."
            ],
            "title": "Explicit bias discovery in visual question answering models",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9562\u20139571.",
            "year": 2019
        },
        {
            "authors": [
                "Kenneth Marino",
                "Mohammad Rastegari",
                "Ali Farhadi",
                "Roozbeh Mottaghi."
            ],
            "title": "Ok-vqa: A visual question answering benchmark requiring external knowledge",
            "venue": "Proceedings of the IEEE/cvf conference on computer vision and pattern recognition, pages",
            "year": 2019
        },
        {
            "authors": [
                "Tom McCoy",
                "Ellie Pavlick",
                "Tal Linzen."
            ],
            "title": "Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference",
            "venue": "ACL, pages 3428\u20133448. Association for Computational Linguistics.",
            "year": 2019
        },
        {
            "authors": [
                "Paul Michel",
                "Omer Levy",
                "Graham Neubig"
            ],
            "title": "Are sixteen heads really better than one",
            "venue": "In NeurIPS,",
            "year": 2019
        },
        {
            "authors": [
                "Yulei Niu",
                "Kaihua Tang",
                "Hanwang Zhang",
                "Zhiwu Lu",
                "Xian-Sheng Hua",
                "Ji-Rong Wen."
            ],
            "title": "Counterfactual vqa: A cause-effect look at language bias",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12700\u2013",
            "year": 2021
        },
        {
            "authors": [
                "Sai Prasanna",
                "Anna Rogers",
                "Anna Rumshisky."
            ],
            "title": "When BERT plays the lottery, all tickets are winning",
            "venue": "EMNLP, pages 3208\u20133229.",
            "year": 2020
        },
        {
            "authors": [
                "Evani Radiya-Dixit",
                "Xin Wang."
            ],
            "title": "How fine can fine-tuning be? learning efficient language models",
            "venue": "AISTATS, volume 108 of Proceedings of Machine Learning Research, pages 2435\u20132443. PMLR.",
            "year": 2020
        },
        {
            "authors": [
                "Sainandan Ramakrishnan",
                "Aishwarya Agrawal",
                "Stefan Lee."
            ],
            "title": "Overcoming language priors in visual question answering with adversarial regularization",
            "venue": "arXiv preprint arXiv:1810.03649.",
            "year": 2018
        },
        {
            "authors": [
                "Vivek Ramanujan",
                "Mitchell Wortsman",
                "Aniruddha Kembhavi",
                "Ali Farhadi",
                "Mohammad Rastegari."
            ],
            "title": "What\u2019s hidden in a randomly weighted neural network? In CVPR, pages 11890\u201311899",
            "venue": "Computer Vision Foundation / IEEE.",
            "year": 2020
        },
        {
            "authors": [
                "Shaoqing Ren",
                "Kaiming He",
                "Ross Girshick",
                "Jian Sun."
            ],
            "title": "Faster r-cnn: Towards real-time object detection with region proposal networks",
            "venue": "Advances in neural information processing systems, 28:91\u201399.",
            "year": 2015
        },
        {
            "authors": [
                "Victor Sanh",
                "Lysandre Debut",
                "Julien Chaumond",
                "Thomas Wolf."
            ],
            "title": "Distilbert, a distilled version of BERT: smaller, faster, cheaper and lighter",
            "venue": "CoRR, abs/1910.01108.",
            "year": 2019
        },
        {
            "authors": [
                "Victor Sanh",
                "Thomas Wolf",
                "Alexander M. Rush."
            ],
            "title": "Movement pruning: Adaptive sparsity by finetuning",
            "venue": "NeurIPS, pages 20378\u201320389.",
            "year": 2020
        },
        {
            "authors": [
                "Tal Schuster",
                "Darsh J. Shah",
                "Yun Jie Serene Yeo",
                "Daniel Filizzola",
                "Enrico Santus",
                "Regina Barzilay."
            ],
            "title": "Towards debiasing fact verification models",
            "venue": "EMNLP/IJCNLP, pages 3417\u20133423. Association for Computational Linguistics.",
            "year": 2019
        },
        {
            "authors": [
                "Vikash Sehwag",
                "Shiqi Wang",
                "Prateek Mittal",
                "Suman Jana."
            ],
            "title": "HYDRA: pruning adversarially robust neural networks",
            "venue": "NeurIPS.",
            "year": 2020
        },
        {
            "authors": [
                "Qingyi Si",
                "Zheng Lin",
                "Mingyu Zheng",
                "Peng Fu",
                "Weiping Wang."
            ],
            "title": "Check it again: Progressive visual question answering via visual entailment",
            "venue": "arXiv preprint arXiv:2106.04605.",
            "year": 2021
        },
        {
            "authors": [
                "Qingyi Si",
                "Yuanxin Liu",
                "Fandong Meng",
                "Zheng Lin",
                "Peng Fu",
                "Yanan Cao",
                "Weiping Wang",
                "Jie Zhou."
            ],
            "title": "Towards robust visual question answering: Making the most of biased samples via contrastive learning",
            "venue": "ArXiv, abs/2210.04563.",
            "year": 2022
        },
        {
            "authors": [
                "Qingyi Si",
                "Fandong Meng",
                "Mingyu Zheng",
                "Zheng Lin",
                "Yuanxin Liu",
                "Peng Fu",
                "Yanan Cao",
                "Weiping Wang",
                "Jie Zhou."
            ],
            "title": "Language prior is not the only shortcut: A benchmark for shortcut learning in vqa",
            "venue": "arXiv preprint arXiv:2210.04692.",
            "year": 2022
        },
        {
            "authors": [
                "Qingyi Si",
                "Yuchen Mo",
                "Zheng Lin",
                "Huishan Ji",
                "Weiping Wang."
            ],
            "title": "Combo of thinking and observing for outside-knowledge vqa",
            "venue": "arXiv preprint arXiv:2305.06407.",
            "year": 2023
        },
        {
            "authors": [
                "Siqi Sun",
                "Yu Cheng",
                "Zhe Gan",
                "Jingjing Liu."
            ],
            "title": "Patient knowledge distillation for BERT model compression",
            "venue": "EMNLP/IJCNLP, pages 4322\u20134331.",
            "year": 2019
        },
        {
            "authors": [
                "Hao Tan",
                "Mohit Bansal."
            ],
            "title": "Lxmert: Learning cross-modality encoder representations from transformers",
            "venue": "arXiv preprint arXiv:1908.07490.",
            "year": 2019
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N. Gomez",
                "Lukasz Kaiser",
                "Illia Polosukhin."
            ],
            "title": "Attention is all you need",
            "venue": "NIPS, pages 5998\u20136008.",
            "year": 2017
        },
        {
            "authors": [
                "Peng Wang",
                "An Yang",
                "Rui Men",
                "Junyang Lin",
                "Shuai Bai",
                "Zhikang Li",
                "Jianxin Ma",
                "Chang Zhou",
                "Jingren Zhou",
                "Hongxia Yang."
            ],
            "title": "Ofa: Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework",
            "venue": "ICML.",
            "year": 2022
        },
        {
            "authors": [
                "Wenhui Wang",
                "Hangbo Bao",
                "Li Dong",
                "Furu Wei."
            ],
            "title": "Vlmo: Unified vision-language pre-training with mixture-of-modality-experts",
            "venue": "arXiv preprint arXiv:2111.02358.",
            "year": 2021
        },
        {
            "authors": [
                "Zirui Wang",
                "Jiahui Yu",
                "Adams Wei Yu",
                "Zihang Dai",
                "Yulia Tsvetkov",
                "Yuan Cao."
            ],
            "title": "Simvlm: Simple visual language model pretraining with weak supervision",
            "venue": "arXiv preprint arXiv:2108.10904.",
            "year": 2021
        },
        {
            "authors": [
                "Zhiquan Wen",
                "Guanghui Xu",
                "Mingkui Tan",
                "Qingyao Wu",
                "Qi Wu."
            ],
            "title": "Debiased visual question answering from feature and sample perspectives",
            "venue": "Advances in Neural Information Processing Systems, 34:3784\u20133796.",
            "year": 2021
        },
        {
            "authors": [
                "Scao",
                "Sylvain Gugger",
                "Mariama Drame",
                "Quentin Lhoest",
                "Alexander M. Rush."
            ],
            "title": "Transformers: State-of-the-art natural language processing",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System",
            "year": 2020
        },
        {
            "authors": [
                "Canwen Xu",
                "Wangchunshu Zhou",
                "Tao Ge",
                "Ke Xu",
                "Julian J. McAuley",
                "Furu Wei."
            ],
            "title": "Beyond preserved accuracy: Evaluating loyalty and robustness of BERT compression",
            "venue": "EMNLP (1), pages 10653\u2013 10659. Association for Computational Linguistics.",
            "year": 2021
        },
        {
            "authors": [
                "Shaokai Ye",
                "Xue Lin",
                "Kaidi Xu",
                "Sijia Liu",
                "Hao Cheng",
                "Jan-Henrik Lambrechts",
                "Huan Zhang",
                "Aojun Zhou",
                "Kaisheng Ma",
                "Yanzhi Wang."
            ],
            "title": "Adversarial robustness vs",
            "venue": "model compression, or both? In ICCV, pages 111\u2013120. IEEE.",
            "year": 2019
        },
        {
            "authors": [
                "Michael Zeng",
                "Luowei Zhou",
                "Pengchuan Zhang."
            ],
            "title": "Florence: A new foundation model for computer vision",
            "venue": "arXiv preprint arXiv:2111.11432.",
            "year": 2021
        },
        {
            "authors": [
                "Ofir Zafrir",
                "Guy Boudoukh",
                "Peter Izsak",
                "Moshe Wasserblat."
            ],
            "title": "Q8BERT: quantized 8bit BERT",
            "venue": "EMC2@NeurIPS, pages 36\u201339. IEEE.",
            "year": 2019
        },
        {
            "authors": [
                "Pengchuan Zhang",
                "Xiujun Li",
                "Xiaowei Hu",
                "Jianwei Yang",
                "Lei Zhang",
                "Lijuan Wang",
                "Yejin Choi",
                "Jianfeng Gao."
            ],
            "title": "Vinvl: Revisiting visual representations in vision-language models",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and",
            "year": 2021
        },
        {
            "authors": [
                "Wei Zhang",
                "Lu Hou",
                "Yichun Yin",
                "Lifeng Shang",
                "Xiao Chen",
                "Xin Jiang",
                "Qun Liu."
            ],
            "title": "Ternarybert: Distillation-aware ultra-low bit BERT",
            "venue": "EMNLP, pages 509\u2013521. Association for Computational Linguistics.",
            "year": 2020
        },
        {
            "authors": [
                "Yuan Zhang",
                "Jason Baldridge",
                "Luheng He."
            ],
            "title": "PAWS: paraphrase adversaries from word scrambling",
            "venue": "NAACL-HLT, pages 1298\u20131308. Association for Computational Linguistics.",
            "year": 2019
        },
        {
            "authors": [
                "Mengjie Zhao",
                "Tao Lin",
                "Fei Mi",
                "Martin Jaggi",
                "Hinrich Sch\u00fctze."
            ],
            "title": "Masking as an efficient alternative to finetuning for pretrained language models",
            "venue": "EMNLP, pages 2226\u20132241.",
            "year": 2020
        },
        {
            "authors": [
                "Si et al",
                "Wen"
            ],
            "title": "2021) and the open-domain VQA (Marino et al., 2019) method MuKEA (Ding et al., 2022)). In this paper, we therefore mainly use lxmert as the backbone model and extend several debiasing methods to it",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Visual Question Answering (VQA) (Antol et al., 2015) is an important task at the intersection of CV and NLP. In the last decade, deep neural networks have made promising progress in VQA. However, recent studies (Agrawal et al., 2016; Manjunatha et al., 2019) have found that VQA models are prone to dataset biases. As a result, they always suffer from sharp performance drops when faced with outof-distribution (OOD) test datasets, whose answer distributions are different from the training set.\n\u2217Equal contribution. \u2020 Corresponding author: Zheng Lin. 1The codes can be found at https://github.com/\nPhoebusSi/Compress-Robust-VQA.\nAlthough large-scale vision-language pretrained models (VLPs) achieve further improvements in the in-distribution (ID) VQA benchmark (Goyal et al., 2017), they also fail to address the dataset-bias problem (Agrawal et al., 2018), e.g., lxmert (Tan and Bansal, 2019) suffers a 23.26% drop between ID and OOD accuracy. At the same time, the improvement brought by VLPs is partly due to their large model size, which increases the computational cost of deploying VQA models. To facilitate the application of VLPs to VQA tasks, the two problems should be addressed simultaneously. However, existing researches mostly focus on each of them separately.\nThe dataset-bias problem in VQA is well studied by numerous debiasing methods based on conventional small-scale models(Anderson et al., 2018; Cadene et al., 2019). Their main solution (Cadene et al., 2019; Clark et al., 2019; Liang et al., 2021b; Mahabadi and Henderson, 2019) is to regularize the loss according to the bias degree of training samples. In terms of the increased computational cost, a line of recent efforts have been made to compress pre-trained language models (PLMs) in the NLP\nfield (Chen et al., 2020b; Li et al., 2020a,b; Liang et al., 2021a; Liu et al., 2021, 2022; Prasanna et al., 2020) and VLPs for visual-linguistic tasks (Fang et al., 2021; Gan et al., 2022). They show that large-scale PLMs and VLPs can be compressed into lightweight models without degrading performance. Refer to App. A for more related work.\nThis paper jointly studies the compression and debiasing problems of VLP for the VQA task. To this end, we combine the existing debiasing and pruning methods to establish a training and compression pipeline, and conduct extensive experiments with the pre-trained lxmert, which is the most popular VLP in VQA, under different OOD settings. We show that there exist sparse lxmert subnetworks that are more robust than the full model, which suggests that the goal of OOD robustness and computational efficiency can be achieved simultaneously.\nWe also present a comprehensive study on the design of the training and compression pipeline, as well as the assignment of sparsity to different model modules, to identify subnetworks with better OOD generalization. Our findings highlight the importance of 1) Employing a two-stage training and compression pipeline and integrating the debiasing objective throughout the entire process. 2) If there are two debiasing methods working well with the full model, training the full model with the relatively poor-performing one and compressing it with the better one. 3) Assigning modality-specific sparsity to different modules of VLP.\nOur main contributions are as follows: (1) We present the first (to our knowledge) systematic study on sparsity and OOD robustness for VLPs. (2) Our empirical studies on the training and compression pipeline and sparsity assignment can serve as a valuable guideline for the future design of VLP subnetwork searching methods. (3) We obtain subnetworks that outperform existing debiasing SoTAs in terms of the trade-off between accuracy and model size on OOD datasets VQA-CP v2 and VQA-VS (see Fig. 1, Tab. 1 and Tab. 2)."
        },
        {
            "heading": "2 Method",
            "text": ""
        },
        {
            "heading": "2.1 VLP Architecture and Subnetworks",
            "text": "This section takes lxmert as an example to introduce how we extract subnetworks. Lxmert contains an embedding layer, a visual fc layer, a pooler layer, a VQA-specific classifier and a stack of Transformer layers, which involve three encoders: lan-\nguage encoder (Lenc), object relationship encoder (Renc) and cross-modality encoder (Cenc).\nWe adopt unstructured pruning to obtain a compressed version (i.e., a subnetwork) of the original VLPs. Specifically, given a VLP f(\u03b8) with parameters \u03b8, we apply a binary pruning mask m \u2208 {0, 1}|\u03b8| to the model parameters, which gives rise to f(m\u2299\u03b8), where\u2299 is the element-wise product. The parameters to be pruned are:\n\u03b8pr = {Wemb,Wvis-fc,Wplr} \u222a \u03b8Lenc \u222a \u03b8Renc \u222a \u03b8Xenc (1)\nwhere Wemb, Wvis-fc and Wplr are the weights of embedding layer, vision fc layer and pool layer, \u03b8Lenc \u222a\u03b8Renc \u222a\u03b8Xenc are the parameters of Transformer layers. More details of lxmert can be found in App. B.1. Another model visualBERT (Li et al., 2019), which is also used in our experiments, will be introduced in App. B.2."
        },
        {
            "heading": "2.2 Pruning Methods",
            "text": "We consider two representative pruning methods, i.e., magnitude-based pruning (Han et al., 2015) and mask training (Louizos et al., 2018; Ramanujan et al., 2020; Sanh et al., 2020; Sehwag et al., 2020).\nMagnitude-based Pruning approximates the importance of model parameters based on their absolute values and eliminates the less important ones. We adopt the basic version of magnitude-based pruning, i.e., one-shot magnitude pruning (OMP). OMP can optionally be combined with further finetuning of the pruned subnetwork to recover the performance drop.\nMask Training directly optimizes the binary pruning mask m towards the given objectives. Specifically, each weight matrix W \u2208 Rdi\u00d7do is associated with two mask matrices, namely a binary mask m \u2208 {0, 1}di\u00d7do and a real-valued mask m\u0302 \u2208 Rdi\u00d7do . In the forward propagation, m is computed from m\u0302 through binarization:\nmi,j = { 1 if m\u0302i,j \u2265 \u03d5 0 else\n(2)\nwhere \u03d5 is the threshold. Then, the original weight matrix W is replaced with a pruned one m\u2299W. When it comes to backward propagation, we follow (Liu et al., 2022; Mallya et al., 2018; Radiya-Dixit and Wang, 2020; Zhao et al., 2020) and use the straight-through estimator (Bengio et al., 2013) to estimate the gradients of m\u0302 using the gradients of\nm, and then update m\u0302 as m\u0302\u2190 m\u0302\u2212 \u03b7 \u2202L\u2202m , where \u03b7 is the learning rate.\nWe initialize m\u0302 according to the magnitudes of the pre-trained weights of lxmert. This strategy is shown to be more effective than random initialization for pre-trained language models (Liu et al., 2022; Radiya-Dixit and Wang, 2020) and we also validate this in our experiments with lxmert (see App. C.2). Specifically, m\u0302 is initialized as:\nm\u0302i,j = { 0 if Wi,j is pruned by OMP \u03b1\u00d7 \u03d5 else (3)\nwhere \u03b1 \u2265 1 is a hyper-parameter. At initialization, we set the threshold \u03d5 = 0.01 (any other value with the same order of magnitude should also be fine). To ensure that the subnetwork satisfies the given sparsity, \u03d5 is re-computed every tm training steps."
        },
        {
            "heading": "2.3 Debiasing Methods",
            "text": "The deabising methods in VQA usually contain a main model and a biased model. The biased model, which learns the language bias, is used to measure the training samples\u2019 bias degree and adjust the training loss for the main model. We experiment with SoTAs debiasing methods, i.e., LMH (Clark et al., 2019), RUBi (Cadene et al., 2019) and LPF (Liang et al., 2021b), of which LMH is widely studied for the OOD scenario of VQA (Chen et al., 2020a; Liang et al., 2020; Si et al., 2021) and NLU (Jia and Liang, 2017; McCoy et al., 2019; Schuster et al., 2019; Zhang et al., 2019). For comparison, we also describe the binary cross-entropy here.\nBinary Cross-Entropy (BCE) computes the cross-entropy between the predicted distribution pm (from main model) and the soft target score of each ground-truth t, as:\nLbce = t \u00b7 log(\u03b4(pm)) + (1\u2212 t) \u00b7 log(1\u2212 \u03b4(pm))] (4)\nwhere \u03b4 denotes the sigmoid function. Learned-Mixin +H (LMH) adds a biased model to learn biases during training, as follows:\np\u0302deb = softmax(log(pm) + g(h)log(pb)) g(h) = softplus(w \u00b7 h) (5)\nwhere pb and pm are the predicted distribution of biased model and main model, respectively. g(h) determines how much to trust the learned biases, based on lxmert\u2019s last hidden representation h. Following (Clark et al., 2019), we directly use the answers\u2019 frequency under each question type as\npb 2. To prevent pb from being ignored, LMH also adds an entropy penalty item R in the final loss:\nLlmh = t \u00b7 log(\u03b4(p\u0302deb)) + (1\u2212 t) \u00b7 log(1\u2212 \u03b4(p\u0302deb))] +R (6)\nRUBi adopts a training strategy similar to LMH to regularize the main model\u2019s probability, and uses standard cross-entropy as the training loss:\np\u0302deb = softmax(pm \u00b7 \u03b4(pb))\nLrubi = \u2212 1\nN N\u2211 k log(p\u0302deb) [ak] (7)\nLPF measures the bias degree as \u03b1k = pb [ak] to regularize the loss of the main model:\nLlpf = \u22121 N N\u2211 k (1\u2212 \u03b1k)\u03b3 log(softmax(pm)) [ak] (8)\nwhere the \u03b3 is a tunable hype-parameter."
        },
        {
            "heading": "2.4 Problem Formulation",
            "text": "Given the pre-trained lxmert f(\u03b8pt), our goal is to find a subnetwork f (m\u2299 \u03b8ft) that satisfies a target sparsity level s and maximizes the OOD performance:\nmaxm,\u03b8ft (EOOD (f (m\u2299 \u03b8ft))) , s.t. \u2225m\u22250 |\u03b8pr| = (1\u2212 s) (9)\nwhere EOOD denotes OOD evaluation, \u2225\u22250 is the L0 norm and |\u03b8pr| is the total number of parameters in \u03b8pr. This goal is achieved by searching the optimal m and \u03b8ft in model training and compression.\nEq. 9 only specifies the overall sparsity. In this work, we also explore a finer-grained control over sparsity, which allocates different sparsity to different modules of lxmert, given that the overall sparsity is satisfied. Concretely, we consider three modules from different modalities, i.e., the language module, the visual module and the cross-modality module. The constraint in the optimization problem is then rewritten as3:\ns.t. \u2225mLan\u22250 |\u03b8Lan| = (1\u2212 sL), \u2225mV is\u22250 |\u03b8V is| = (1\u2212 sR), \u2225mX\u22250 |\u03b8Xenc | = (1\u2212 sX), sL \u00b7 |\u03b8Lan| |\u03b8pr| + sR \u00b7 |\u03b8V is| |\u03b8pr| + sX \u00b7 |\u03b8Xenc | |\u03b8pr| = s (10) where \u03b8Lan = \u03b8LEnc \u222a{Wemb}, \u03b8V is = \u03b8REnc \u222a {Wvis-fc} and \u03b8XEnc are model parameters of\n2We use the same pb in our implementation of LMH, RUBi and LPF. More details of LMH can be found in App. B.3\n3For simplicity, the pooler layer\u2019s parameters(0.5M) are not included in eq. 10. We directly set it to the target sparsity s.\nthe language module, visual module and crossmodality encoder, respectively. mLan, mV is and mX are the binary masks for the three modules, respectively. sL, sR and sX are the target sparsity levels for the three modules, respectively.\nIf not otherwise specified, we set the sparsity of every weight matrix to target sparsity. For example, if s = 70% and there is no modality-specific constraint, then all weight matrices are at 70% (uniform sparsity). If sL = 50%, then all weight matrices in \u03b8Lan are at 50% sparsity, while sR and sX could be different (modality-specific sparsity)."
        },
        {
            "heading": "2.5 Training and Compression Pipeline",
            "text": "We define two notations: FL(f(\u03b8)) denotes training f(\u03b8) using loss L \u2208 {Lbce,Llmh}. PpL(f(\u03b8)) denotes pruning f(\u03b8) using method p \u2208 {OMP,mask train} and loss L (if applicable), which outputs a pruning mask m. A typical training and compression pipeline involves three stages:\nStage1: Full Model Fine-tuning. The pretrained lxmert f(\u03b8pt) is fine-tuned using loss L, which produces f(\u03b8ft) = FL(f(\u03b8)).\nStage2: Model Compression. The fine-tuned lxmert f(\u03b8ft) is compressed and we get the subnetwork f (m\u2299 \u03b8ft), where m = PpL(f(\u03b8ft)).\nStage3: Further Fine-tuning (optional). The subnetwork f(m\u2299 \u03b8ft) is further fine-tuned using loss L\u2032, and gets f(m\u2299\u03b8\u2032ft) = FL\u2032 (f(m\u2299\u03b8ft))."
        },
        {
            "heading": "3 Experiments",
            "text": "In this section, we mainly investigate three questions: (1) How does compression affect lxmert\u2019s OOD generalization ability? (2) How to design the training and pruning pipeline to achieve a good sparsity-performance trade-off? (3) How to assign sparsity to different modality-specific modules?"
        },
        {
            "heading": "3.1 Datasets, Model and Implementation",
            "text": "We conduct experiments on the OOD benchmarks VQA-CP v2 (Agrawal et al., 2018) and VQA-VS (Si et al., 2022b) that evaluate the robustness of VQA systems, with the accuracy-based evaluation metric (Antol et al., 2015). A more detailed discussion of the difference between the two datasets is shown in Sec. 3.5. We thoroughly study the above three questions on VQA-CP-v2, which is widely used in the literature on debiasing VQA systems (refer to Sec. 3.2, 3.3 and 3.4 ). Then, based on the findings, we further explore the more challenging VQA-VS (Si et al., 2022b) (refer to Sec. 3.5 ). For VLP, we adopt the lxmert-base-uncased model (Tan and Bansal, 2019) released by huggingface (Wolf et al., 2020). All the results are averaged over 4 random seeds. More information of the model and implementation details are shown in App. B.4."
        },
        {
            "heading": "3.2 Effect of Compression on OOD Accuracy",
            "text": "Subnetworks from BCE Fine-tuned lxmert. We compress the BCE fine-tuned lxmert using OMP and mask training and introduce either Lbce or Llmh in the pruning (for mask training) or further fine-tuning process (for OMP).\nThe results are shown in the upper row of Fig. 2. We can derive several observations: 1) When no debiasing methods are used, the subnetworks of \u201cmask train(bce)\" and \u201cOMP + bce ft\" improve over the full lxmert by 1.35% \u223c 2.79%, even at up to 70% sparsity. This implies that lxmert is overparameterized and pruning may remove some parameters related to the bias features. 2) \u201cmask train(lmh)\" and \u201cOMP + lmh ft\" achieve further performance boost, exceeding full lxmert by a large margin (11.05% \u223c 14.02%). Since mask training does not change the value of parameters, the\nresults of \u201cmask train (lmh)\" implicate that the biased \u201cfull lxmert(bce)\" already contains sparse and robust subnetworks (across 10% \u223c 90% sparsity). 3) \u201cmask train\" outperforms \u201cOMP\" in general, which suggests that directly optimizing the subnetwork structure is more effective than debiasing a compressed subnetwork by further fine-tuning.\nSubnetworks from lxmert Fine-tuned with Debiasing Methods. From the lower row of Fig. 2, we can find that: 1) For the full lxmert, the OOD performance is obviously promoted with the LMH debiasing method. 2) Unlike lxmert(bce) subnetworks, lxmert(lmh) subnetworks do not exhibit significant improvement over the full model. However, the \u201cmask train(lmh)\" and \u201cOMP + lmh ft\" subnetworks, which preserve the lxmert(lmh)\u2019s performance at up to 50% sparsity, can serve as an efficient alternative to the LMH fine-tuned full lxmert. 3) \u201cmask train(bce)\" and \u201cOMP + bce ft\" clearly underperform their lmh counterparts, which suggests that it is important to use the debiasing method in pruning and subnetwork further finetuning even when the full model is already trained with the debiasing method.\nFig. 3 compares the subnetworks fine-tuned with LMH, LPF and RUBi. We find that: The subnetworks found using LMH consistently outperform those found by LPF and RUBi across different sparsity levels. Therefore, to save computing resources, we mainly use the best performing LMH in the following experiments and analysis."
        },
        {
            "heading": "3.3 Training and Compression Pipeline",
            "text": "In this section, we study the proper design of the training and compression pipeline, under the basic framework described in Sec. 2.5. Here we focus on the mask training compression method, as it\nhas been shown to generally outperform OMP with further fine-tuning. Our main observations can be described from three perspectives:\nFirst, it is recommended to introduce the debiasing loss across Stage1, Stage2 and (if applicable) Stage3. The reason is three-fold: 1) As shown by Fig. 4, the subnetworks at 10%, 30% and 70% sparsity levels have better performance when starting from lxmert(lmh), as compared with the lxmert(bce). At 90% sparsity, \u201clxmert(lmh) + mask train(lmh)\" underperforms \u201clxmert(bce) + mask train(lmh)\" (see App. C.3 for reasons), but the Accuracy gap is small. Therefore, adopting Llmh in Stage1 is a better choice than Lbce, especially when the subnetworks are not at extremely high sparsity. 2) As we discussed in the previous section, introducing Llmh in the mask training process (Stage2) substantially outperforms Lbce for both lxmert(lmh) and lxmert(bce). 3) When both Stage1 and Stage2 adopt the BCE loss, further finetuning the subnetworks with LMH loss in Stage3 can significantly boost the performance, as shown by the results of \u201clxmert(bce) + mask train(bce)\" w/o ft and w/ lmh ft in Fig. 4.\nSecond, Stage3 is unnecessary if it adopts the same training objective as Stage2. Comparing the blue and red (or cyan) bars in Fig. 4, we can see that further fine-tuning with the same training objective generally degrades the performance of \u201clxmert(lmh) + mask train(lmh)\", \u201clxmert(bce) + mask train(lmh)\" and \u201clxmert(bce) + mask train(bce)\". This phenomenon suggests that Stage3 can be eliminated to save computation cost.\nThird, it is recommended to use different debiasing methods in the two stages and leave the better one to Stage2. As shown in Fig. 5, although LPF and RUBi are less effective in debi-\nasing the full model than LMH, \u201clpf+lmh\"4 and \u201crubi+lmh\" are superior to \u201clmh+lmh\". In contrast, when reversing the debiasing methods used in the two stages, \u201clmh+rubi\" and \u201clmh+lpf\" exhibit worse performance, suggesting that the better debiasing method should be used in Stage2. Additionally, \u201clpf+lmh\" is superior to \u201crubi+lmh\", which indicates that using a better debiasing objective in Stage1 is helpful when we have multiple choices different from the Stage2 objective. We also experiment with another VLP model, visualBERT (Li et al., 2019), and find that \u201clpf+lmh\" still performs the best as in Fig. 7."
        },
        {
            "heading": "3.4 Modality-specific Sparsity",
            "text": ""
        },
        {
            "heading": "Pruning Each Single Modality-specific Module.",
            "text": "Since lxmert uses different modules to encode the multi-modal data, it is intuitive to hypothesize that different modules of lxmert may capture the language bias to different extents. To validate this hypothesis, we compress the language, visual and cross-modality modules, respectively. As presented\n4\u201clpf+lmh\" denotes \u201clxmert(lpf) + mask train(lmh)\"\nby Fig. 6, the compression of different modalityspecific modules indeed exhibits different effects.\nWhen the full model is lxmert(bce) (the orange and cyan lines), compressing the language or crossmodality module has a positive effect on the OOD performance, and the accuracy generally improves as sparsity increases from 10% to 90%. By contrast, compressing the visual module results in inferior results than compressing the other two modules, even if the number of remaining parameters is larger (note that the visual module has a smaller number of parameters than the other two modules). These results suggest that, for the biased lxmert(bce), the language and cross-modality modules capture more training set bias than the visual module, which supports the above hypothesis.\nIn terms of \u201clxmert(lmh) + mask train(lmh)\" (the red line), although compression does not lead to performance improvement like compressing lxmert(bce), the results also demonstrate that the language and cross-modality modules are more compressible than the visual module.\nSearching for Appropriate Modality-specific Sparsity. Motivated by the above findings, we search for appropriate modality-specific sparsity by performing mask training with a variety of sparsity configurations (see App. C.4) for the three modules while keeping the overall sparsity the same.\nAs we can see in Fig. 8, at 50% and 70% overall sparsity, the configuration that achieves the best result assigns slightly higher sparsity to the language and cross-modality modules and significantly lower sparsity to the visual module, as compared with uniform sparsity. This phenomenon is in accordance with the findings in Fig. 6, implicating that compressing the three modules uniformly is suboptimal (at 50% \u223c 70% sparsity) and the language and cross-modality modules should be compressed to\na larger extent than the visual module. At 90% sparsity, the sparsity configuration\u2019s comfort zone is in the proximity of the uniform point. Further increasing the sparsity of the language and crossmodality modules result in performance decline or only minor improvements. This is because 90% sparsity already approaches the compression upper bound, even for the language and cross-modality modules.\nFig. 9 shows a more direct comparison between the uniform and modality-specific sparsity. We also introduce another baseline \u201cmatrix-specific sparsity\", which ranks all the model parameters, instead of the parameters in each weight matrix. This also results in different sparsity levels for different weight matrices, while there is no explicit control over the modality-specific sparsity. We can see that\nmodality-specific sparsity achieves the best results across the three overall sparsity levels from 50% to 90%, demonstrating its superiority. Besides, the results also suggest that, although simply allowing different matrices to have different sparsity is more flexible than uniform sparsity, it is not conducive to the final performance."
        },
        {
            "heading": "3.5 Exploration on VQA-VS",
            "text": "VQA-CP v2 is widely used in the literature of debiasing VQA systems. However, it only considers the question-type-based bias. To account for other potential biases, VQA-VS constructs several types of OOD test sets according to different shortcuts (e.g., keyword and key object). As a result, VQA-VS is more challenging and allows us to analyze the results on different biases. In this section, we search sparse and robust lxmert subnetworks in VQA-VS based on the major findings obtained from VQACP v2.\nThe Effect of Compression. Fig. 10 shows the results of full lxmert and subnetworks on VQA-VS. We can see that: 1) When using the BCE objective, we can identify sparse \u201cbce+bce\" subnetworks that are comparable with full lxmert (bce). 2) Different from VQA-CP v2, full lxmert (lmh) only slightly\noutperforms full lxmert (bce) in the OOD setting of VQA-VS, and underperforms in the ID setting. 3) The \u201clmh+lmh\"5 subnetworks improve over full lxmert (lmh) on both ID and OOD test sets, across a wide range of sparsity levels, suggesting that lxmert can also be simultaneously compressed and debiased on VQA-VS.\nThe Effect of Modality-specific Sparsity. Fig. 10 also shows that compressing different modalityspecific modules has different effect on VQA-VS, as in VQA-CP v2. The language module is the most compressible while compressing the visual module results in the sharpest performance decline. To compare modality-specific sparsity and uniform sparsity, we directly inherit the sparsity configuration selected in Sec. 3.4 on VQA-CP v2. Fig. 11 shows that modality-specific sparsity consistently outperform uniform sparsity, except for 90% sparsity in the ID setting."
        },
        {
            "heading": "3.6 Comparison with Debiasing SoTAs",
            "text": "In this section, we will compare the best training and compression solutions identified in the previous sections with the current SoTA debiasing methods.\nTab. 1 shows the results on VQA-CP v2. We find that: 1) The accuracy of our methods (10% lxmert and 30% lxmert) beats the previous non-VLP debi-\n5Since most debiasing methods (e.g., LPF and RUBi) fail on VQA-VS (see Tab.2), we only use LMH in VQA-VS. However, combining LMH and other effective debiasing methods in different stages may further outperform \u201clmh+lmh\", as found in VQA-CP v2. We leave it for future work.\nasing SoTAs with 1.55% and 5.79%, respectively, with fewer or similar amounts of parameters, establishing new state-of-the-arts. 2) Our methods (30% lxmert and 50% lxmert) outperform the debiased full lxmert, even with much fewer parameters. 3) Full lxmert(lpf) and full lxmert(lmh) are good at different question types, which can partly explain why combining them in different stages produces more robust subnetworks.\nWe also add experiments on a more recent VLP mPLUG (Li et al., 2022). We adopt the base version of mPLUG, fine-tune it on the VQA-CP v2 training set and then conduct pruning using mask training. Since mPLUG formulas VQA as a text generation task, we adopt the LPF debiasing method. Note that LMH and RUBi cannot be directly applied to debias text generation models, because they are designed for classification loss over a fixed number of classes. As shown in the bottom rows of Tab. 1, the mPLUG trained with standard cross-entropy (CE) loss can be simultaneously compressed (to 50%) and debiased (+5.48 Acc). The mPLUG trained with LPF debiasing loss can also be compressed to 50% with a slight accuracy decline. These results demonstrate that the findings and techniques present in our work can be generalized to more advanced VLPs.\nResults on VQA-VS are presented in Tab. 2. We can observe that: 1) Our methods \u201cbce+bce\" 10% lxmert and \u201clmh+lmh\" 30% lxmert outperform all the non-VLP debiasing methods in both ID and OOD settings, with similar or fewer parameters. 2) Except for LMH, other debiasing methods underperform BCE in OOD-mean. LMH improves the OOD accuracy at the cost of ID accuracy decline. 3) The \u201clmh+lmh\" subnetworks\n(even with 50% remaining parameters) obviously improve the ID performance of lxmert (lmh) and retain comparable OOD performance. 4) Compared with \u201cbce+bce\", the OOD advantage of \u201clmh+lmh\" outweighs its ID disadvantage at 50% to 90% parameters. With fewer remaining parameters, the overall performance of \u201cbce+bce\" is superior."
        },
        {
            "heading": "4 Conclusion",
            "text": "To facilitate the application of VLP-based VQA systems, this paper presents the first joint study on the compression and debiasing problems of VLP for the VQA task. Through extensive experiments with three VLPs (i.e., lxmert, visualBERT and mPLUG), we analyze the impact of compression on the OOD generalization ability. We present a comprehensive study on the design of the training and compression pipeline for a good sparsity-performance trade-off, and provide some valuable findings about the assignment of sparsity to different modality-specific modules. The compressed lxmert subnetworks in this paper outperform the SoTA debiasing methods with fewer or similar model parameter counts."
        },
        {
            "heading": "Limitations",
            "text": "Although we have empirically verified that the adoption of modality-specific sparsity is beneficial for the search for more robust subnetworks, our work still does not provide a solution on how to determine the optimal sparsity assignment effectively and efficiently. We invite follow-up studies to further address it in future work."
        },
        {
            "heading": "Acknowledgement",
            "text": "This work was supported by National Natural Science Foundation of China (No. 61976207) and National Social Science Foundation of China (No. 21AZD145)."
        },
        {
            "heading": "A More Related Work",
            "text": ""
        },
        {
            "heading": "A.1 Overcoming Dataset Bias in VQA",
            "text": "Most VQA systems heavily rely on the information of the question to predict answers no matter the content of the given image. That is they learned the language biases in datasets. They are not robust and always perform poor in the OOD setting where the language biases they learned in training set are invalid for test set. To promote the development of models that overcome such problem, VQA-CP v2 (Goyal et al., 2017) is proposed and has become the standard OOD benchmark in VQA. Currently, the widely used debiasing methods can be roughly grouped into non-data-augmentation (Clark et al., 2019; Liang et al., 2021b; Mahabadi and Henderson, 2019) and data-augmentation methods (Chen et al., 2020a; Gokhale et al., 2020). The former applies a biased model (trained with question only) to\nregularize the model training and thus prevent learning from question. The latter generates samples to balance the training data and directly erase the biases in the training set. However, the augmented data also increase the training cost, and overcoming the language-bias problem remaining the original dataset biases unchanged still remains a major challenge (Liang et al., 2021b; Niu et al., 2021). Thus, we only focus on non-data-augmentation methods, such as LMH (Clark et al., 2019), RUBi (Cadene et al., 2019) and LPF (Liang et al., 2021b). Very recently, VQA-VS6 (Si et al., 2022b) is proposed to explore the varying types of dataset biases. We also use this dataset to study how the training and compression pipeline affect different dataset biases.\nA.2 Vision-Language Pre-trained Models\nRecently, VLPs (Dou et al., 2022; Li et al., 2021, 2020a; Wang et al., 2021a,b; Zhang et al., 2021; Si et al., 2023; Li et al., 2022) based on the Transformer backbone (Vaswani et al., 2017) have achieved encouraging success. Specially, OFA (Wang et al., 2022) and Florence (Yuan et al., 2021) establish the SoTA on the in-distribution VQA v2. To learn better cross-modality representations and vision-language alignment, they are trained with large-scale pre-training data and generally have huge model capacity. Among them, lxmert (Tan and Bansal, 2019) is the most widely used VLP as the backbone model in VQA field (e.g., some dataaugmentation debiasing methods (Gokhale et al., 2020; Si et al., 2021; Wen et al., 2021) and the open-domain VQA (Marino et al., 2019) method MuKEA (Ding et al., 2022)). In this paper, we therefore mainly use lxmert as the backbone model and extend several debiasing methods to it for indepth research on compressing and debiasing. For completeness, we also conduct experiments on the popular VLP visualBERT (Li et al., 2019)."
        },
        {
            "heading": "A.3 Model Compression and Robustness",
            "text": "Model compression techniques for Transformerbased pre-trained models are well developed (mainly around BERT), including pruning (Gale et al., 2019; Gordon et al., 2020; Michel et al., 2019), knowledge distillation (Jiao et al., 2020; Sanh et al., 2019; Sun et al., 2019), parameter sharing (Lan et al., 2020) and quantization (Zafrir et al., 2019; Zhang et al., 2020). Inspired by lottery ticket\n6Both VQA-VS and VQA-CP v2 datasets are licensed under Commons Attribution 4.0 International License.\nhypothesis (Frankle and Carbin, 2019), many recent studies show that BERT can be pruned to a sparse subnetwork after (Gale et al., 2019) and before fine-tuning (Chen et al., 2020b; Liang et al., 2021a; Liu et al., 2022; Prasanna et al., 2020), without performance degrading. On this basis, we extend the pruning paradigm to the fine-tuned lxmert for OOD scenario in VQA, which incorporates the debiasing methods when fine-tuning and pruning. In the NLP and CV fields, some recent efforts have also been made to study model compression and robustness to adversarial attacks (Fu et al., 2021; Gui et al., 2019; Sehwag et al., 2020; Xu et al., 2021; Ye et al., 2019) and spurious correlations (Du et al., 2021; Xu et al., 2021) (which is more common than the worst-case adversarial attack). Dataset-bias problem is a typical symptom of spurious correlations and poses a challenge to VQA models. We are the first to thoroughly investigate the sparsity and OOD robustness for VLPs in VQA."
        },
        {
            "heading": "B More Details of Model and Implementation",
            "text": ""
        },
        {
            "heading": "B.1 lxmert Architecture and Subnetworks",
            "text": "For lxmert, the embedding layer and visual fc layer map language-modality input (token sequences obtained by WordPiece tokenizer) and visionmodality input (36 object features obtained by Faster R-CNN (Ren et al., 2015)) into the samedimension space. The pooler layer connects the Transformer top layer and the classifier. The Transformer layers involve three encoders 7: language encoder (Lenc), object relationship encoder (Renc) and cross-modality encoder (Cenc), and are usually composed of attention modules and feed-forward networks (FFN).\nThe attention modules have four kinds of weight matrices, i.e., the query, key and value matrices WQ,K,V \u2208 Rdmodel\u00d7dmodel , and the output matrix WO \u2208 Rdmodel\u00d7dmodel . FFN contains two linear layers Win \u2208 Rdmodel\u00d7dFFN , Wout \u2208 RdFFN\u00d7dmodel .\nWe adopt unstructured pruning to obtain a compressed version (i.e., a subnetwork) of the original VLPs. Specifically, given a VLP f(\u03b8) with\n7Each Transformer layer of the language encoder and object relationship encoder has a multi-head self-attention module and a feed-forward network (FFN). Each Transformer layer of the cross-modality encoder has a language self-attention module, a visual self-attention module and a multi-head crossattention module. Only the language self-attention and visual self-attention modules are followed by FFN. All the weight matrices of Transformer layers are summarized in eq. 12.\nparameters \u03b8, we apply a binary pruning mask m \u2208 {0, 1}|\u03b8| to the model parameters, which gives rise to f(m \u2299 \u03b8), where \u2299 is the elementwise product. For lxmert, we focus on the embedding layer, visual fc layer, pooler layer and Transformer layers of which the parameters are pre-trained, while the classifier is excluded. The language encoder, visual encoder, cross-modality encoder have T , I and X Transformer layers respectively. The parameters to be pruned are:\n\u03b8pr = {Wemb,Wvis-fc,Wplr} \u222a \u03b8Lenc \u222a \u03b8Renc \u222a \u03b8Xenc (11) where Wemb, Wvis-fc and Wplr are the weights of embedding layer, vision fc layer and pool layer, \u03b8Lenc \u222a\u03b8Renc \u222a\u03b8Xenc are the parameters of Transformer layers:\n\u03b8Lenc = { WtQL ,W t KL ,WtVL ,W t OL ,WtinL ,W t outL }T t=1\n\u03b8Renc = { WiQR ,W i VR ,WiKR ,W i OR ,WiinR ,W i outR }I i=1 \u03b8Xenc = {WxQCX ,W x KCX ,WxKCX ,W x OCX ,\nWxQCL ,W x KCL ,WxVCL ,W x OCL ,WxinCL ,W x outCL , WxQCR ,W x KCR ,WxKCR ,W x OCR ,WxinCR ,W x outCR} X x=1\n(12) where CX , CL and CR are the language selfattention, visual self-attention and cross-attention modules respectively."
        },
        {
            "heading": "B.2 visualBERT Architecture and Subnetworks",
            "text": "Similar to lxmert, visualBERT is composed of an embedding layer, a visual projection layer, a pooler\nlayer, a stack of Transformer layers. Differently, visualBERT\u2019s Transformer layers only involve a single encoder (Venc). The parameters of visualBERT to be pruned are:\n\u03b8pr = {Wemb,Wplr} \u222a \u03b8Venc (13)\nwhere Wemb and Wplr are the weights of embedding layer and pool layer, \u03b8Venc are the parameters of Transformer layers:\n\u03b8Venc = { WvQL ,W v KL ,WvVL ,W v OL ,WvinL ,W v outL }V v=1\n(14) where V = 12."
        },
        {
            "heading": "B.3 LMH details",
            "text": "LMH takes a step further based on Produce of Experts (PoE) (Hinton, 2002), which simply combines the predicted distributions of the main model and the biasd model as follows:\np\u0302deb = softmax(log(pm) + log(pb)) (15)\nwhere pb is the predicted distribution of biased model, and indicates the bias degree of the sample. In this way, when a sample is heavily biased, that is, pb is large, the main model will not output a large pm for it during training. Following (Clark et al., 2019), we directly use the answers\u2019 frequency under each question type as pb.\nTo selectively adjust the main model\u2019s behavior, LMH adds a learn function g to explicitly deter-\nmine how much to trust the learned biases:\np\u0302deb = softmax(log(pm) + g(h)log(pb)) g(h) = softplus(w \u00b7 h) (16)\nwhere h is the cross-modality representation from the last hidden layer of lxmert, w is trainable. To prevent pb being ignored, LMH also adds an entropy penalty item R, and the final loss is computed as:\nLlmh = t \u00b7 log(\u03b4(p\u0302deb)) + (1\u2212 t) \u00b7 log(1\u2212 \u03b4(p\u0302deb))] +R (17)"
        },
        {
            "heading": "B.4 Model and Implementation Details",
            "text": "Lxmert has about 202M parameters, and 197.7M parameters are involved in the pruning process (4.5M parameters are left to the classifier). The three modules from different modalities, namely the language module, the visual module and the cross-modality module, contain 83.1M, 35.3M and 78.8M parameters respectively. We train the models for 20 epochs with a batch size of 128 on two Tesla-V100-32G or 256 on A100-80GB. The AdamW (Loshchilov and Hutter, 2017) optimizer is adopted with a learning rate of 5e-5. Our codes are based on the huggingface transformers library (Wolf et al., 2020). We adopt visualBERT of its coco-pre version which is pre-trained with COCO (Chen et al., 2015) dataset."
        },
        {
            "heading": "C More Experiments on VQA-CP v2",
            "text": ""
        },
        {
            "heading": "C.1 Performance of Subnetworks on Three Types of Questions",
            "text": "Subnetworks from BCE Fine-tuned lxmert. For the three types of questions, as shown in the right three plots of Fig. 12 (upper), we find that: 1) The performance on \"Num\" questions is sensitive to the varying sparsity levels while that on \"Y/N\" questions is relatively stable in general except at 90% sparsity. Specially, with the increase of sparsity, the performance on \"Num\" questions of \"mask train(lmh)\" and \"OMP + lmh ft\" counterintuitively greatly promote. This shows that language biases for the \"Num\" questions exist in a large proportion of the parameters of biased lxmert. 2) For the \"Other\" questions, debiasing methods have little gain on the performance of subnetworks. For example, the performance of \"mask train(lmh)\" is similar with that of \"mask train(bce)\". This indicates that the language biases for \"Other\" questions is minor\nin training set. Therefore, \"Other\" questions request more reasoning than debiasing. 3)There is a sharp decline of all the subnetworks\u2019 performance on \"Other\" questions from 70% \u223c 90% sparsity. We conjecture that this is because reducing the model\u2019s capacity too drastically hurt the reasoning ability which is necessary to answer the \"Other\" questions correctly.\nSubnetworks from LMH Fine-tuned lxmert. The right three plots of Fig. 12 (lower) shows the performance of LMH fine-tuned lxmert subnetworks on different types of questions. For the \u201cNum\" questions, when compressing LMH finetuned lxmert (the grey and maroon lines), the performance of subnetworks no longer rises with sparsity growth. This demonstrates that language biases for the \u201cNum\" questions exist in a much smaller proportion of the parameters of debiased lxmert than that of biased lxmert. For \u201cOther\" questions, \u201clxmert(bce) + mask train(lmh)\" is consistently superior to \u201clxmert(lmh) + mask train(lmh)\", which demonstrates that further debiasing the debiased full lxmert in the pruning process sacrifices the reasoning ability."
        },
        {
            "heading": "C.2 The Effect of Different Initialization Strategies of m\u0302 for Mask Training",
            "text": "We conduct experiments with different subnetworks to validate the effectiveness of initializing m\u0302 according to the magnitudes of lxmert\u2019s pretrained weights. From Fig. 13, it can be seen that \"lxmert(bce) + mask train(bce)\", \"lxmert(bce) + mask train(lmh)\", \"lxmert(lmh) + mask train(bce)\" (dashed lines) consistently outperform \"lxmert(bce) + rand-init mask train(bce)\", \"lxmert(bce) + randinit mask train(lmh)\", \"lxmert(lmh) + rand-init\nmask train(bce)\" (full lines) at all sparsity levels. As the sparsity increases, the gaps widen. This shows the initialization strategy we adopt is more effective than random initialization."
        },
        {
            "heading": "C.3 A Close Look at The Performance of Subnetworks at 90% Sparsity",
            "text": "From Fig. 14, we derive two abnormal observations at the extremely high sparsity, i.e., 90%: 1) Pruning with \"OMP + lmh ft\" (pink and grey lines) is better than pruning with \"mask train(lmh)\" (cyan and brown lines). 2) Starting from \"lxmert(bce)\" (pink and cyan lines) is better than starting from \"lxmert(lmh)\" (grey and brown lines). The two observations at 90% sparsity are contrary to other sparsity. For the first observation, we conjecture that this is because mask training (which involves binarization and gradient estimation) is more difficult to optimize at 90% compared with further fine-tuning of the OMP subnetworks. The second observation can be explained by that: Further debiasing the debiased full lxmert in the pruning process slightly sacrifices the performance on \"Other\" questions, which require more reasoning ability than debiasing ability (as shown in the rightmost two plots of Fig. 12). Therefore, at the extremely high sparsity, when the benefits of debiasing on \"Y/N\" and \"Num\" questions are small, the performance penalty on \"Other\" questions results in a drop in \"Overall\" accuracy. Nevertheless, the gaps between \"lxmert(lmh) + mask train(lmh)\" and the other two pipelines are small at 90% sparsity."
        },
        {
            "heading": "C.4 Sparsity Configurations for the Three Modality-specific Modules",
            "text": "For the overall target sparsity of 50% and 70%, we adopt the following procedure to search the comfortable zone for the modality-specific sparsity:\nFirst, we traverse [10%, 30%, 50%, 70%, 90%] (i.e., step size of 20%) to assign modality-specific sparsity for any two modules, and compute the modality-specific sparsity for the remaining one8 according to eq. 10 in the main paper. From the experimental results of these sparsity configurations, we can determine the approximate range where the pruned subnetworks perform better.\nSecond, we use the same method to traverse the reduced range with a smaller step size of 5%. In this way, we can determine the most comfortable zone for the modality-specific sparsity.\nSimilarly, when the overall target sparsity is 90%, we directly traverse 80% \u223c 98% with a step size of 2% to search the most comfortable zone of the modality-specific sparsity."
        },
        {
            "heading": "D More Experiments on VQA-VS",
            "text": "D.1 Performance on varying OOD test sets of VQA-VS"
        },
        {
            "heading": "The Effect of Compression without Debiasing",
            "text": "For simplicity, we categorize the nine OOD test sets into 3 categories of different modalities, i.e., language-based (OOD-lang), visual-based (OODvis) and cross-modality (OOD-crsM) ones. We report the average accuracy of each category, as well as the IID accuracy and the average accuracy of all OOD test sets (OOD-mean) in Fig. 15.\nThe upper part of Fig. 15 shows the performance of subnetworks compressed without debiasing method, it can be seen that: 1) All subnetworks obtained by pruning all three modules underperform \u201cfull model(bce)\" in ID test set. This is because the ID performance relies on memory ability, which is positively related to the parameter quantity. 2) The subnetworks obtained by pruning the language module consistently outperform the full model on OOD-mean, OOD-lang and OODcrsM test sets, which are related to the language bias. This indicates that the language module of lxmert is slightly overparameterized. 3) In contrast, pruning other modules causes a negative impact on OOD performance. Especially, pruning visual modules also results in a sharp OOD-vis accuracy drop, indicating that the visual module of lxmert is not suitable for compression.\nThe Effect of Compression with Debiasing The lower part of Fig. 15 shows the VQA-VS perfor-\n8We exclude the configurations where the computed sparsity for the remaining module is greater than 1 or smaller than 0.\nmance of \u201clxmert(lmh) + mask train(lmh)\"9, which performs the best on VQA-CP v2. We can observe that: 1) Pruning any modules can improve ID performance over the debiased full model (\"full model(lmh)\"). This is because debiasing methods improve OOD performance at the cost of ID performance, while our pipeline alleviates such ID performance decline by compressing some harmful parameters. 2) Similarly, pruning any lxmert modules with a small sparsity (e.g., 0.2 and 0.4) also improves the OOD-mean performance. This demonstrates the existence of sparse and robust lxmert subnetworks on VQA-VS. 3) Especially, subnetworks obtained by compressing the language module consistently perform better than subnetworks obtained by pruning other modules and the debiased full model (except on OOD-vis), since the dataset biases tend to be learned by the language module. 4) However, pruning on any module fails to improve the OOD-vis accuracy, as the debiasing method LMH is designed for the language bias.\n9Note that most debiasing methods fail on VQA-VS (Si et al., 2022b), such as LPF and RUBi. We thus do not discuss them in this section.\nD.2 The Effect of Modality-specific Sparsity on varying OOD test sets of VQA-VS\nWe directly use the modality-specific sparsity selected by the experiments of Sec. 3.4 in the main paper on VQA-CP v2. Fig. 16 shows that the subnetworks with modality-specific sparsity always outperform those with uniform sparsity except for 90% sparsity on ID test set, which validates that different modules should be compressed with different sparsity. Besides, when the overall sparsity is too large or too small, the benefits of the assignment of modality-specific sparsity will decrease accordingly. Note that the phenomenon of OOD-vis is different from other OOD test sets as the sparsity increases, since the debiasing methods LMH is designed for the language biases."
        }
    ],
    "title": "Compressing and Debiasing Vision-Language Pre-Trained Models for Visual Question Answering",
    "year": 2023
}