{
    "abstractText": "Memes are a widely popular tool for web users to express their thoughts using visual metaphors. Understanding memes requires recognizing and interpreting visual metaphors with respect to the text inside or around the meme, often while employing background knowledge and reasoning abilities. We present the task of meme captioning and release a new dataset, MEMECAP. Our dataset contains 6.3K memes along with the title of the post containing the meme, the meme captions, the literal image caption, and the visual metaphors. Despite the recent success of vision and language (VL) models on tasks such as image captioning and visual question answering, our extensive experiments using state-of-the-art VL models show that they still struggle with visual metaphors, and perform substantially worse than humans.",
    "authors": [
        {
            "affiliations": [],
            "name": "EunJeong Hwang"
        },
        {
            "affiliations": [],
            "name": "Vered Shwartz"
        }
    ],
    "id": "SP:c2b4cd29655ee2bcc494719d883ed006eda8ff78",
    "references": [
        {
            "authors": [
                "Ehsan Aghazadeh",
                "Mohsen Fayyaz",
                "Yadollah Yaghoobzadeh."
            ],
            "title": "Metaphors in pre-trained language models: Probing and generalization across datasets and languages",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational",
            "year": 2022
        },
        {
            "authors": [
                "Arjun R. Akula",
                "Brendan Driscoll",
                "Pradyumna Narayana",
                "Soravit Changpinyo",
                "Zhiwei Jia",
                "Suyash Damle",
                "Garima Pruthi",
                "Sugato Basu",
                "Leonidas Guibas",
                "William T. Freeman",
                "Yuanzhen Li",
                "Varun Jampani"
            ],
            "title": "Metaclue: Towards comprehensive",
            "year": 2023
        },
        {
            "authors": [
                "gooei",
                "Marianne Monteiro",
                "Jacob Menick",
                "Sebastian Borgeaud",
                "Andrew Brock",
                "Aida Nematzadeh",
                "Sahand Sharifzadeh",
                "Mikolaj Binkowski",
                "Ricardo Barreira",
                "Oriol Vinyals",
                "Andrew Zisserman",
                "Karen Simonyan"
            ],
            "title": "Flamingo: a visual language model",
            "year": 2022
        },
        {
            "authors": [
                "Malihe Alikhani",
                "Piyush Sharma",
                "Shengjie Li",
                "Radu Soricut",
                "Matthew Stone."
            ],
            "title": "Cross-modal coherence modeling for caption generation",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6525\u20136535, On-",
            "year": 2020
        },
        {
            "authors": [
                "Nitzan Bitton-Guetta",
                "Yonatan Bitton",
                "Jack Hessel",
                "Ludwig Schmidt",
                "Yuval Elovici",
                "Gabriel Stanovsky",
                "Roy Schwartz"
            ],
            "title": "Breaking common sense: Whoops! a vision-and-language benchmark of synthetic and compositional images",
            "year": 2023
        },
        {
            "authors": [
                "Branislav Buchel."
            ],
            "title": "Internet memes as means of communication",
            "venue": "Brno: Masaryk University.",
            "year": 2012
        },
        {
            "authors": [
                "Tuhin Chakrabarty",
                "Yejin Choi",
                "Vered Shwartz."
            ],
            "title": "It\u2019s not rocket science: Interpreting figurative language in narratives",
            "venue": "Transactions of the Association for Computational Linguistics, 10:589\u2013606.",
            "year": 2022
        },
        {
            "authors": [
                "Tuhin Chakrabarty",
                "Debanjan Ghosh",
                "Adam Poliak",
                "Smaranda Muresan."
            ],
            "title": "Figurative language in recognizing textual entailment",
            "venue": "Findings of the Association for Computational Linguistics: ACLIJCNLP 2021, pages 3354\u20133361, Online. Association",
            "year": 2021
        },
        {
            "authors": [
                "Tuhin Chakrabarty",
                "Arkady Saakyan",
                "Olivia Winn",
                "Artemis Panagopoulou",
                "Yue Yang",
                "Marianna Apidianaki",
                "Smaranda Muresan."
            ],
            "title": "I spy a metaphor: Large language models and diffusion models co-create visual metaphors",
            "venue": "Findings of ACL.",
            "year": 2023
        },
        {
            "authors": [
                "Tuhin Chakrabarty",
                "Xurui Zhang",
                "Smaranda Muresan",
                "Nanyun Peng."
            ],
            "title": "MERMAID: Metaphor generation with symbolism and discriminative decoding",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for",
            "year": 2021
        },
        {
            "authors": [
                "Stoica",
                "Eric P. Xing"
            ],
            "title": "Vicuna: An opensource chatbot impressing gpt-4 with 90% chatgpt quality",
            "year": 2023
        },
        {
            "authors": [
                "Minjin Choi",
                "Sunkyung Lee",
                "Eunseong Choi",
                "Heesoo Park",
                "Junhyuk Lee",
                "Dongwon Lee",
                "Jongwuk Lee."
            ],
            "title": "MelBERT: Metaphor detection via contextualized late interaction using metaphorical identification theories",
            "venue": "Proceedings of the 2021 Conference of",
            "year": 2021
        },
        {
            "authors": [
                "Ellen Dodge",
                "Jisup Hong",
                "Elise Stickles."
            ],
            "title": "MetaNet: Deep semantic automatic metaphor analysis",
            "venue": "Proceedings of the Third Workshop on Metaphor in NLP, pages 40\u201349, Denver, Colorado. Association for Computational Linguistics.",
            "year": 2015
        },
        {
            "authors": [
                "Charles Forceville."
            ],
            "title": "Pictorial metaphor in advertising",
            "venue": "Psychology Press.",
            "year": 1996
        },
        {
            "authors": [
                "Jack Hessel",
                "Ana Marasovic",
                "Jena D. Hwang",
                "Lillian Lee",
                "Jeff Da",
                "Rowan Zellers",
                "Robert Mankoff",
                "Yejin Choi."
            ],
            "title": "Do androids laugh at electric sheep? humor \u201cunderstanding\u201d benchmarks from the new yorker caption contest",
            "venue": "Proceedings of the",
            "year": 2023
        },
        {
            "authors": [
                "thosh Rajamanickam",
                "Georgios Antoniou",
                "Ekaterina Shutova",
                "Helen Yannakoudakis",
                "Vlad Sandulescu",
                "Umut Ozertem",
                "Patrick Pantel",
                "Lucia Specia",
                "Devi Parikh"
            ],
            "title": "The hateful memes challenge: Competition report",
            "venue": "In Proceedings of the NeurIPS",
            "year": 2021
        },
        {
            "authors": [
                "Junnan Li",
                "Dongxu Li",
                "Silvio Savarese",
                "Steven Hoi"
            ],
            "title": "Blip-2: Bootstrapping language-image pretraining with frozen image encoders and large language models",
            "year": 2023
        },
        {
            "authors": [
                "Chin-Yew Lin."
            ],
            "title": "ROUGE: A package for automatic evaluation of summaries",
            "venue": "Text Summarization Branches Out, pages 74\u201381, Barcelona, Spain. Association for Computational Linguistics.",
            "year": 2004
        },
        {
            "authors": [
                "Vicente Ordonez",
                "Girish Kulkarni",
                "Tamara Berg."
            ],
            "title": "Im2text: Describing images using 1 million captioned photographs",
            "venue": "Advances in Neural Information Processing Systems, volume 24. Curran Associates, Inc.",
            "year": 2011
        },
        {
            "authors": [
                "Kishore Papineni",
                "Salim Roukos",
                "Todd Ward",
                "WeiJing Zhu."
            ],
            "title": "Bleu: a method for automatic evaluation of machine translation",
            "venue": "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311\u2013318, Philadelphia,",
            "year": 2002
        },
        {
            "authors": [
                "Jingnong Qu",
                "Liunian Harold Li",
                "Jieyu Zhao",
                "Sunipa Dev",
                "Kai-Wei Chang"
            ],
            "title": "Disinfomeme: A multimodal dataset for detecting meme intentionally spreading out disinformation",
            "year": 2022
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark",
                "Gretchen Krueger",
                "Ilya Sutskever"
            ],
            "title": "Learning transferable visual models from natural language",
            "year": 2021
        },
        {
            "authors": [
                "Robert Kaczmarczyk",
                "Jenia Jitsev"
            ],
            "title": "Laion5b: An open large-scale dataset for training next generation image-text models",
            "year": 2022
        },
        {
            "authors": [
                "Kate Scott."
            ],
            "title": "Memes as multimodal metaphors: A relevance theory analysis",
            "venue": "Pragmatics & Cognition, 28(2):277\u2013298.",
            "year": 2021
        },
        {
            "authors": [
                "Chhavi Sharma",
                "William Paka",
                "Scott",
                "Deepesh Bhageria",
                "Amitava Das",
                "Soujanya Poria",
                "Tanmoy Chakraborty",
                "Bj\u00f6rn Gamb\u00e4ck"
            ],
            "title": "Task Report: Memotion Analysis 1.0 @SemEval 2020: The Visuo-Lingual Metaphor",
            "venue": "In Proceedings of the 14th International",
            "year": 2020
        },
        {
            "authors": [
                "Piyush Sharma",
                "Nan Ding",
                "Sebastian Goodman",
                "Radu Soricut."
            ],
            "title": "Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning",
            "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Lin-",
            "year": 2018
        },
        {
            "authors": [
                "Shivam Sharma",
                "Atharva Kulkarni",
                "Tharun Suresh",
                "Himanshi Mathur",
                "Preslav Nakov",
                "Md. Shad Akhtar",
                "Tanmoy Chakraborty"
            ],
            "title": "Characterizing the entities in harmful memes: Who is the hero, the villain, the victim",
            "year": 2023
        },
        {
            "authors": [
                "Kevin Stowe",
                "Nils Beck",
                "Iryna Gurevych."
            ],
            "title": "Exploring metaphoric paraphrase generation",
            "venue": "Proceedings of the 25th Conference on Computational Natural Language Learning, pages 323\u2013336, Online. Association for Computational Linguistics.",
            "year": 2021
        },
        {
            "authors": [
                "Roman Suvorov",
                "Elizaveta Logacheva",
                "Anton Mashikhin",
                "Anastasia Remizova",
                "Arsenii Ashukha",
                "Aleksei Silvestrov",
                "Naejin Kong",
                "Harshith Goka",
                "Kiwoong Park",
                "Victor Lempitsky"
            ],
            "title": "Resolution-robust large mask inpainting with fourier",
            "year": 2021
        },
        {
            "authors": [
                "Kohtaro Tanaka",
                "Hiroaki Yamane",
                "Yusuke Mori",
                "Yusuke Mukuta",
                "Tatsuya Harada."
            ],
            "title": "Learning to evaluate humor in memes based on the incongruity theory",
            "venue": "Proceedings of the Second Workshop on When Creative AI Meets Conversational AI, pages",
            "year": 2022
        },
        {
            "authors": [
                "William Yang Wang",
                "Miaomiao Wen."
            ],
            "title": "I can has cheezburger? a nonparanormal approach to combining textual and visual information for predicting and generating popular meme descriptions",
            "venue": "Proceedings of the 2015 Conference of the North Amer-",
            "year": 2015
        },
        {
            "authors": [
                "Jason Wei",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Maarten Bosma",
                "brian ichter",
                "Fei Xia",
                "Ed Chi",
                "Quoc V Le",
                "Denny Zhou"
            ],
            "title": "Chain-of-thought prompting elicits reasoning in large language models",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Bo Xu",
                "Tingting Li",
                "Junzhe Zheng",
                "Mehdi Naseriparsa",
                "Zhehuan Zhao",
                "Hongfei Lin",
                "Feng Xia."
            ],
            "title": "Met-meme: A multimodal meme dataset rich in metaphors",
            "venue": "Proceedings of the 45th International ACM SIGIR Conference on Research and Develop-",
            "year": 2022
        },
        {
            "authors": [
                "Ron Yosef",
                "Yonatan Bitton",
                "Dafna Shahaf"
            ],
            "title": "Irfl: Image recognition of figurative language",
            "year": 2023
        },
        {
            "authors": [
                "Dongyu Zhang",
                "Minghao Zhang",
                "Heting Zhang",
                "Liang Yang",
                "Hongfei Lin."
            ],
            "title": "MultiMET: A multimodal dataset for metaphor understanding",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th Inter-",
            "year": 2021
        },
        {
            "authors": [
                "Wang",
                "Luke Zettlemoyer"
            ],
            "title": "Opt: Open pretrained transformer language models",
            "year": 2022
        },
        {
            "authors": [
                "Tianyi Zhang",
                "Varsha Kishore",
                "Felix Wu",
                "Kilian Q. Weinberger",
                "Yoav Artzi"
            ],
            "title": "Bertscore: Evaluating text generation with bert",
            "year": 2020
        },
        {
            "authors": [
                "Zhuosheng Zhang",
                "Aston Zhang",
                "Mu Li",
                "Hai Zhao",
                "George Karypis",
                "Alex Smola."
            ],
            "title": "Multimodal chain-of-thought reasoning in language models",
            "venue": "arXiv preprint arXiv:2302.00923.",
            "year": 2023
        },
        {
            "authors": [
                "Deyao Zhu",
                "Jun Chen",
                "Xiaoqian Shen",
                "Xiang Li",
                "Mohamed Elhoseiny"
            ],
            "title": "2023a. Minigpt-4: Enhancing vision-language understanding with advanced large language models",
            "year": 2023
        },
        {
            "authors": [
                "Wanrong Zhu",
                "Jack Hessel",
                "Anas Awadalla",
                "Samir Yitzhak Gadre",
                "Jesse Dodge",
                "Alex Fang",
                "Youngjae Yu",
                "Ludwig Schmidt",
                "William Yang Wang",
                "Yejin Choi"
            ],
            "title": "Multimodal c4: An open, billion-scale corpus of images interleaved with text",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Web users frequently communicate their thoughts and feelings online using memes (Buchel, 2012; Tanaka et al., 2022). Memes are created by taking an existing widespread image and attaching new meaning to it by altering the text inside the image. For example, in Figure 1, Tom cat is a metaphor for the person who posted the meme and the cats he is shaking hands with represent his two regular followers who always like his posts. This incongruity between the image and the text makes memes humorous (Tanaka et al., 2022).\nBecause of their complementary nature, interpreting the meaning of a meme requires understanding both the visual and text modalities. Moreover, memes are often posted on social media platforms along with additional text, such as \u201cone of them is my alt\u201d in Fig. 1, which is further needed to understand the meme.\nRecently, there is a surge of vision and language (VL) models (e.g. Alayrac et al., 2022; Li et al., 2023; OpenAI, 2023). VL models have shown remarkable capabilities in generating detailed and\naccurate descriptions of images in both zero-shot and in-context setups. Such models are first pretrained on language-only and vision-only datasets, and then trained on tasks such as image captioning and visual question answering, where the redundancy between the vision and language is used to embed them in a shared space. For example, the majority of image captions in existing datasets describe what is depicted in the image, at most adding subjective interpretations or inferences about the story behind the image (Alikhani et al., 2020). In contrast, there is little work on visual metaphors to date (Zhang et al., 2021; Chakrabarty et al., 2023).\nIn this paper, we are investigating whether VL models can successfully interpret memes. We propose the task of meme captioning, in which models are presented with a meme along with its title (e.g. the title of the post containing the meme), and is tasked with generating a concise caption describing\nthe meaning of the meme. This task goes beyond object recognition and language understanding. It is challenging due to the metaphorical role of the visual content of the meme (Scott, 2021). For example, in Fig. 1, the model needs to recognize that Tom cat is merely a metaphor for the meme poster, and that handshaking signals appreciation. The literal content of the image, such as Tom or the handshake, should not be part of the meme caption. Recognizing and interpreting such metaphors involve detecting facial expressions, the tone expressed in the texts, making commonsense inferences, and more (Bitton-Guetta et al., 2023).\nTo that end, we collected a meme captioning dataset MEMECAP, containing 6,384 memes along with their captions. Each meme is also annotated with the literal image description (e.g. \u201cTom cat is shaking hands with two small cats and smiling\u201d), and the visual metaphors (e.g. Tom is a metaphor for the meme poster).\nWe establish comprehensive baseline performances with recent large-scale VL models, in various training setups (e.g. zero-shot, few-shot, finetuning), and inputs (i.e. meme, title, literal image captions, and metaphors). Human evaluation of the generated captions shows that models are far from humans in captioning memes. In particular, models tend to ignore important visual or textual elements, and instead, repeat the text inside the meme or make up fake elements. Our findings merit future research on this task. 1"
        },
        {
            "heading": "2 Background",
            "text": ""
        },
        {
            "heading": "2.1 Metaphors",
            "text": "Most work on metaphors is limited to textual metaphors, and pertains to collecting resources (Dodge et al., 2015), detecting or interpreting metaphorical expressions in context (Choi et al., 2021; Chakrabarty et al., 2021a; Aghazadeh et al., 2022; Chakrabarty et al., 2022), and metaphor generation (Stowe et al., 2021; Chakrabarty et al., 2021b).\nRecently, there has been interest in visual metaphors. Visual metaphors occur when a target concept is compared to another visual element (vehicle) (Forceville, 1996). MultiMET (Zhang et al., 2021) and Met-Meme (Xu et al., 2022) are two datasets of text-image pairs with annotations for the existence and types of metaphors, sentiment,\n1Our code and data are available at: https://github.com/eujhwang/meme-cap\nand more. Chakrabarty et al. (2023) tested image generation models on prompts involving a visual metaphor such as \u201cMy bedroom is a pigsty\u201d. They found the unsatisfactory performance can be improved by using a large language model (LLM) to interpret the visual metaphors and add details to the prompt, such as \u201cmessy bedroom\u201d. Akula et al. (MetaCLUE; 2023) introduces a set of tasks pertaining to visual metaphors in synthetic images, such as retrieval and captioning.\nFinally, the Image Recognition of Figurative Language dataset (IRFL; Yosef et al., 2023) presents an idiom, metaphor, or simile, along with 4 images, with the goal of selecting the image that matches the figurative expression. The distractors consist of an image that depicts the expression literally, for example a picture of a cheetah for the simile \u201cas fast as a cheetah\u201d. This dataset is challenging for state-of-the-art VL models."
        },
        {
            "heading": "2.2 Memes",
            "text": "Recent work on memes focused on detecting hateful or harmful content in memes (Kiela et al., 2021; Qu et al., 2022; Sharma et al., 2023), classifying memes to humorous or not (Tanaka et al., 2022), and analyzing the sentiment of memes (Sharma et al., 2020). Earlier work automatically generated a text to go inside the meme (Wang and Wen, 2015), and the memes themselves (e.g. the ImgFlip575K dataset).2\nAlthough MultiMET (Zhang et al., 2021) does not focus specifically on memes, the images were collected from a range of sources including social media, which contains memes. The similar MetMeme dataset (Xu et al., 2022) focuses on memes. Differently from our work, both datasets contain annotations for visual metaphors while MEMECAP also contains meme captions."
        },
        {
            "heading": "2.3 Other Image Datasets",
            "text": "The WHOOPS benchmark (Bitton-Guetta et al., 2023) consists of unconventional human-created and machine-generated images that defy commonsense (e.g. an image of \u201cAlbert Einstein holding a smartphone\u201d), along with their textual descriptions. It\u2019s meant to be used for image captioning, image-text matching, visual question answering, and explanation generation. In contrast, our work focuses on memes, and tests models on their ability to interpret real memes posted by web users.\n2https://github.com/schesa/ImgFlip575K_Dataset\nAnother multi-modal benchmark which is similar to ours is the New Yorker Cartoon Caption Contest (Hessel et al., 2023). This benchmark involves 3 tasks: matching a caption to a cartoon, evaluating the quality of the caption, and explaining the joke. While both memes and cartoons use a combination of visual and textual elements to convey humor, memes are based on recognizable images that are modified and repurposed to create new meanings based on shared cultural knowledge. Cartoons, on the other hand, are originally drawn illustrations, often in the form of comic strips, that convey a more complex narrative. Further, while Hessel et al. (2023) focus on discriminative matching (i.e. selecting the more appropriate caption) and generating an explanation, in this paper we present a generative task, i.e. generating a caption to describe a meme."
        },
        {
            "heading": "3 The MEMECAP Dataset",
            "text": "The overall data collection and annotation process is illustrated in Figure 2. We collected memes (Sec 3.1) and crowdsourced their captions (Sec 3.2). We present the data splits and statistics in Sec 3.3."
        },
        {
            "heading": "3.1 Memes",
            "text": "We scraped memes from Reddit using the publicly available API.3 In particular, we focused on the subreddit /r/memes and collected posts that contained a meme with a post title. To ensure that the text and image are complementary, we manually examined the memes and excluded memes that lacked any text or contained an excessive number of characters. To exclude offensive content from the dataset, we filtered out memes with profanity in the text using the Google banned word list.4 We also filtered out images with sexual content, for which the NudeNet Classifier returned an unsafe score higher than 0.9.5\n3https://www.reddit.com/dev/api/ 4 https://github.com/coffee-and-fun/google-profanity-words 5https://github.com/notAI-tech/NudeNet"
        },
        {
            "heading": "3.2 Captions",
            "text": "We conducted two rounds of annotations to obtain the captions. In the first round, we collected the literal image descriptions, disregarding the text in the memes, while in the second round, we collected the meme caption along with the visual metaphors.\nLiteral Image Captions. We asked workers to caption the image, disregarding the text. For example, a suitable literal image caption for Figure 1 is \u201cTom cat is shaking hands with two small cats and smiling\u201d. To prevent biasing the workers with the text inside the meme, we identified and removed the text in the meme using the LaMa inpainting tool (Suvorov et al., 2021). We collected one caption for each meme, which we manually verified.\nMeme Captions. We showed a second set of annotators the full meme, title, and literal image caption, and asked them to provide a meme caption. This HIT included two steps. First, workers were asked to indicate for each term in the literal image caption whether it was used metaphorically, and if so, what was the target of the metaphor (e.g., \u201cTom cat\u201d is a metaphor for the meme poster). We then instructed the workers to write a concise caption describing the meaning that the meme poster was trying to convey, while excluding the metaphor vehicles (e.g., not mentioning Tom). We collected one caption for each meme in the training set, and 2 to 4 captions for memes in the test set.\nBoth rounds of annotations were conducted on Amazon Mechanical Turk (MTurk). To ensure the quality of annotations, we required that workers were located in English-speaking countries (e.g. US, UK, Canada, Australia, and New Zealand), had an acceptance rate of at least 98% on 5,000 prior HITs, and passed a qualification test similar to the task.\nWe excluded from the dataset any memes that workers in each of the rounds marked as offensive, sexual, hateful, or uninterpretable."
        },
        {
            "heading": "3.3 Final Dataset",
            "text": "We clustered the examples in the dataset based on the vector representation of their meme captions using OPT2.7b (Zhang et al., 2022). To ensure the diversity of topics in both the training and test sets, we then sampled 10% of the memes from each cluster and assigned them to the test set, and the rest of the memes into the training and validation\nset.6 Table 1 shows the statistics of our dataset."
        },
        {
            "heading": "3.4 Types of Metaphors",
            "text": "We manually analyzed 28 memes along with their metaphor annotations.\nMeme Type. First, following Zhang et al. (2021) and Xu et al. (2022), we categorized the memes into three categories: text dominant and image dominant, where the text or the image respectively may be enough to understand the metaphor, and complementary, where both modalities are required. We added a fourth category for memes that had no metaphor, i.e. whose meaning is conveyed explicitly in the text. The left part of Figure 3 shows that the 44% of memes are complementary, but each of the other categories is also prominent with 19%.\nWe then looked at the human annotations we obtained in Sec 3.2 for the metaphors in each meme. We looked at the vehicle, i.e. the visual element\n6Note that our dataset doesn\u2019t contain duplicate memes.\nused to convey the metaphorical meaning, as well as the target, i.e. the meaning itself.\nMetaphor Vehicle Type. The middle part of Fig 3 shows that the most common vehicle is a person or a character, followed by objects (such as the trophy), facial expressions or gestures (such as the surprised look on the man\u2019s face), and actions.\nMetaphor Target Type. The types of targets are displayed in the right part of Fig 3. The majority of the metaphors describe either a behavior or stance towards a certain topic, or the meme poster themselves (with a person vehicle, such as Drake). Other categories are an approach or a concept (for which the meme poster expresses a certain stance), another person, and a \u201cdesire vs. reality\u201d meme such as the drowning meme illustrated in Fig 3."
        },
        {
            "heading": "4 Experimental Setup",
            "text": "We report the performance of various baselines on MEMECAP. All models are tasked with generating\na meme caption, and are based on pre-trained VL or language models (Sec 4.1), but may differ by their inputs and number of training examples (Sec 4.2)."
        },
        {
            "heading": "4.1 Models",
            "text": "We experiment with two state-of-the-art VL models that can generate text conditioned on both text and images, as well as one language model.\nOpen Flamingo. Flamingo was initialized with a pre-trained LLM and a pre-trained vision model, and further trained on vision and language tasks, keeping the pre-trained models frozen. The interaction between the two modalities is facilitated with a gated cross-attention dense block. Since the original model is not publicly available, we use the open version, OpenFlamingo-9B (Awadalla et al., 2023). OpenFlamingo is built on top of LLaMA 7B (Touvron et al., 2023) and CLIP ViT/L-14 (Radford et al., 2021), and was trained on 5M samples from the Multimodal C4 dataset (Zhu et al., 2023b) and 10M samples from LAION-2B (Schuhmann et al., 2022).\nMiniGPT4. MiniGPT4 (Zhu et al., 2023a) is similarly composed of frozen pre-trained language and vision models, and it employs a single projection layer to align the visual and language features. Since GPT4\u2019s architecture and training data remain a mystery, we utilize MiniGPT4 as an alternative to GPT4 (OpenAI, 2023).7 It has similar capabilities to GPT-4 in understanding and generating the context (Zhu et al., 2023a). For its language model, MiniGPT4 uses Vicuna (Chiang et al., 2023), which is built on top of LLaMA13B and performs on par with ChatGPT (OpenAI, 2023). For its vision component, it uses BLIP-2 (Li et al., 2023), which consists of CLIP ViT-G/14 and a Q-Former architecture. MiniGPT4 was trained on various multimodal datasets, including images\n7The version of GPT-4 available through the OpenAI API doesn\u2019t support images.\nfrom LAION (Schuhmann et al., 2022), Conceptual Captions (Sharma et al., 2018), and SBU (Ordonez et al., 2011).\nLLaMA LLaMA (Touvron et al., 2023) is a transformer-based language model that was trained on trillions of tokens from exclusively publiclyavailable data. The LLaMA-13B model outperforms GPT-3 (Brown et al., 2020) on most benchmarks. We use the LLaMA-7B model, which achieves comparable performance to the LLaMA13B model on most benchmarks. Since LLaMA is a language model rather than a VL model, its access to the visual content is through the image caption and the OCR text alone."
        },
        {
            "heading": "4.2 Evaluation Setup",
            "text": "Inputs. We test the models with different input settings. In the setup which is the most comparable to humans, we provide the models with the meme and title. We also experiment with setups that aid the model. One such input is the image caption, which can help the model focus on the language modality and ignore the image. The second such input is the text inside the meme, that we extracted using EasyOCR,8 which helps the model focus on the visual aspects of the image and includes the text inside the image as part of the language input. We incrementally added each of these inputs.\nLearning Setups. We evaluate all models in a zero-shot setup. Flamingo and LLaMA enable incontext learning, so we experiment with 4, 8, and 12 shots. An example prompt (including the meme, title, image caption, and text inside the meme) is illustrated in Figure 4. MiniGPT4 works in a chat format, so rather than in-context learning, we use it in either a zero-shot setup, or fine-tuned on our training set.\nLastly, motivated by Chakrabarty et al. (2023) and Zhang et al. (2023), we also tested models in a\n8https://github.com/JaidedAI/EasyOCR\nChain of Thought (CoT) style prompting (Wei et al., 2022). In our case, we elicit multi-step reasoning from the LLM by providing the visual metaphors, using the following prompt:\n<image>This is a meme with the title \u201c{title}\u201d. The image description is \u201c{image caption}\u201d. The following text is written inside the meme: \u201c{OCR text}\u201d. What is the meme poster trying to convey? Rationale: \u201c{keyword1}\u201d is a metaphor for \u201c{meaning1}\u201d. \u201c{keyword2}\u201d is a metaphor for \u201c{meaning2}\u201d. Answer:"
        },
        {
            "heading": "5 Results",
            "text": "We evaluated the performance of the various models with both automatic metrics (Sec 5.1) and human evaluation (Sec 5.2). We show that the vision and language modalities are complementary through ablation tests (Sec 5.3)."
        },
        {
            "heading": "5.1 Automatic Evaluation",
            "text": "To evaluate the quality of the generated captions, we use standard metrics for automatic evaluation of generative tasks: BLEU (Papineni et al., 2002) ROUGE (Lin, 2004), and BERTScore (Zhang et al., 2020) (using microsoft/deberta-xlarge-mnli). BLEU and ROUGE are based on n-gram overlap between the generated captions and human-written reference captions, while BERTScore measures the semantic similarities between the two.\nTable 2 shows the performance of the various models and input setups in terms of these metrics. For the few-shot setup, we show the best performance across (4, 8, and 12 shots). See Appendix A for the full results.\nModels. Flamingo dominates MiniGPT4 across all metrics, with a gap of 15, 12, and 6 points in BLEU, ROUGE, and BertScore respectively for the best setups. This is likely due to the lengthy captions generated by MiniGPT4, despite the prompt including the instruction to generate a single sentence. Finally, the LLaMA model is highly competitive with Flamingo despite not having access to the image itself. It appears that the image captions and OCR text provide sufficient information.\nLearning Setups. The Flamingo performance significantly improves from the zero-shot to fewshot setting, and continues to improve from 4 to\n8 shots but slightly decreases at 12 shots (see Appendix A). MiniGPT4 achieved better performance in the zero-shot setup, while fine-tuning its last layer significantly decrease the performance. As we show in Sec 5.2, while the fine-tuned model learns to generate short captions, it tends to hallucinate more. We hypothesize that fine-tuning only the last layer is ineffective.\nInputs. In the few-shot setups, the best performance is achieved with as many of the inputs as possible, i.e. including both the image caption and the OCR text, despite the redundancy with the visual inputs. This might be due to suboptimal crossmodal interaction in VL models. While prior work showed that explicitly stating the metaphors helps image generation models generate better images (Chakrabarty et al., 2023), we did not see a similar gain in meme captioning."
        },
        {
            "heading": "5.2 Human Evaluation",
            "text": "We focused on the models with the full set of inputs except for the rationales (meme+title+img cap+OCR text) and evaluated the performance of all models (focusing on 4-shots for the few-shot setups), with respect to the following criteria:\n\u2022 Correctness: Does the caption correctly convey the meaning the meme poster wanted to convey?\n\u2022 Appropriate Length: Is the caption length appropriate for conveying the meaning (i.e. it is not too verbose)?\n\u2022 Visual Completeness: Does the caption describe all the important elements in the image?\n\u2022 Textual Completeness: Does the caption describe all the important elements in the text inside the meme and the title text?\n\u2022 Faithfulness: Are all the elements of the caption supported by either the visual or text elements (i.e. there are no made-up elements)?\nWe randomly sampled 30 memes along with their model-generated and human-written captions. The annotation was performed by students in the lab, and we took the majority vote across 3 annotators. Figure 5 shows the performance according to the human evaluation. All models perform significantly worse than humans, except for appropriate length criteria, with 36.6, 29.3, 24.5, and 18.4 point differences on correctness, textual completeness, visual completeness, and faithfulness respectively.\nModels. Model performances differ by criteria. Flamingo and LLaMA are more correct and faithful, while MiniGPT4 is more visually complete.\nLearning Setups. For Flamingo, the few-shot models improve in textual and visual completeness upon the zero-shot model, but not in terms of correctness and faithfulness. This may suggest that while access to examples improves the model\u2019s understanding of the task, it might also confuse it with information irrelevant to the target meme. LLaMA doesn\u2019t gain any performance improvements from in-context examples, likely for the same reason. Without the visual features, it might struggle even more to separate the text (title, image caption, and OCR) of the different examples.\nMiniGPT4 zero-shot is very verbose, but the fine-tuned model learns to output captions in the\nlength of its training examples. Unfortunately, these captions are far worse than those of the zeroshot model in all criteria. The zero-shot version generates verbose captions that include a lot of information, often conveying the correct meaning along with irrelevant information such as literal descriptions of the image. Conversely, the fine-tuned version adapts to the \u201ccorrect\u201d length but it often fails to focus on the relevant parts, leading to incorrect or incomplete captions. We hypothesize that the frozen language and vision model may not have enough information about interpreting memes, and simply fine-tuning the last projection layer of the model is not enough to produce high-quality captions. This conclusion is consistent with Zhou et al. (2023), according to which most knowledge in LLM is learned during the pre-training stage.\nCommon Errors. Figure 6 shows two examples of meme captions generated by Flamingo 4-shot along with the types of errors they exhibit. The top example demonstrates an unfaithful caption because neither the meme nor the title conveys anything about being successful in life. The bottom example illustrates a common error in which the model copies text from inside the meme while ignoring important visual elements. In this case, Spongebob\u2019s smile indicates the meme poster\u2019s positive attitude towards reading old and long forum threads, but the model-generated caption misses it. Another common error (not illustrated here) occurs when the model treats visual elements too literally, failing to interpret the metaphor. Finally, in some cases, the model might lack sufficient background knowledge to correctly interpret the meme."
        },
        {
            "heading": "5.3 Ablation Tests",
            "text": "The analysis in Sec 3.4 shows that interpreting most memes in MEMECAP will require understanding both the visual and text modalities. We are interested in the extent that models make use of each modality. To that end, we perform an ablation test to exclude each modality. Table 3 presents the results in terms of automatic metrics.\nIn most cases, the best performance is achieved with both modalities. For Flamingo (zero-shot and few-shot), excluding the meme results in more decrease in performance than excluding the title, in-\ndicating that the model relies more on the visual modality than the information provided by the title. The same is true for LLaMA (in both settings), for which excluding the image caption yields worse performance. This is expected since the title is typically secondary in informativeness to the meme. In addition, Flamingo still has access to the text inside the meme via visual features.\nConversely, MiniGPT4 exhibits a higher dependency on textual modality, resulting in a significant decrease when the title is not provided. Since\nMiniGPT4 shows higher textual and visual completeness when the OCR text is provided (\u00a75.2), we hypothesize that MiniGPT4 makes limited usage of the visual modality."
        },
        {
            "heading": "6 Conclusion",
            "text": "We present MEMECAP, the first meme captioning dataset. MEMECAP is challenging for the existing VL models, as it requires recognizing and interpreting visual metaphors, and ignoring the literal visual elements. The experimental results using state-ofthe-art VL models indeed show that such models are still far from human performance. In particular, they tend to treat visual elements too literally and copy text from inside the meme. Our work opens up interesting future research on recognizing visual metaphors, interpreting them with respect to a textual context, and generating meme captions that are complete with respect to both modalities without creating fake elements.\nLimitations\nQuality of Metaphor Annotations. We put our best efforts into manually verifying the collected data, and indeed the human performance in Section 5.2 shows the human-written captions are of high quality. With that said, we noticed that the quality of the visual metaphors is inconsistent. We believe that while people are capable of explaining a meme, they don\u2019t always know to map the visual vehicles into textual targets. This likely explains why adding the metaphors as inputs didn\u2019t improve the performance.\nSubjectivity and Background Knowledge. The meme captioning task involves employing background knowledge which may vary between annotators. To that end, we manually checked the meme captions to minimize the number of incorrect captions in the dataset. In addition, there is some level of subjectivity with respect to the evaluation criteria for the meme caption quality. For this reason, we ensured a high quality of annotations by having in-house annotators that could ask clarification questions, but some subjectivity still remains.\nEthics Statement\nData All the datasets used in our work are publicly available. Our dataset is collected from Reddit and may contain offensive, hateful, or sexual content. Despite our best efforts to filter them out as\ndescribed in Section 3, we found people have different criteria for what they perceive as offensive, hateful, or sexual, and thus, such content may still exist in our data.\nData Collection We use Amazon Mechanical Turk to collect 6.3K image descriptions and 7.7K meme captions. We paid $0.03 for the image captioning task and $0.16 for the meme captioning task. The annotators were compensated with an average hourly wage of $13, which is comparable to the US minimum wage. We did not collect any personal information from annotators.\nModels Our dataset may include some offensive content or mild expletives and this can amplify potentially biased and unethical answers. In addition, the large pre-trained VL models we used for the experiments are trained on a large-scale publicly available web corpus and may also bring some bias when generating sentences."
        },
        {
            "heading": "Acknowledgements",
            "text": "This work was funded, in part, by the Vector Institute for AI, Canada CIFAR AI Chairs program, an NSERC discovery grant, and a research gift from AI2."
        },
        {
            "heading": "A Additional Experimental Results",
            "text": "We show the full experimental results in Table 4."
        }
    ],
    "title": "MEMECAP: A Dataset for Captioning and Interpreting Memes",
    "year": 2023
}