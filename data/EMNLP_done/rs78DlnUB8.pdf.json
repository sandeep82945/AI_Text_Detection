{
    "abstractText": "Curriculum learning provides a systematic approach to training. It refines training progressively, tailors training to task requirements, and improves generalization through exposure to diverse examples. We present a curriculum learning approach that builds on existing knowledge about text and graph complexity formalisms for training with text graph data. The core part of our approach is a novel data scheduler, which employs \u201cspaced repetition\u201d and complexity formalisms to guide the training process. We demonstrate the effectiveness of the proposed approach on several text graph tasks and graph neural network architectures. The proposed model gains more and uses less data; consistently prefers text over graph complexity indices throughout training, while the best curricula derived from text and graph complexity indices are equally effective; and it learns transferable curricula across GNN models and datasets. In addition, we find that both nodelevel (local) and graph-level (global) graph complexity indices, as well as shallow and traditional text complexity indices play a crucial role in effective curriculum learning.",
    "authors": [
        {
            "affiliations": [],
            "name": "Nidhi Vakil"
        },
        {
            "affiliations": [],
            "name": "Hadi Amiri"
        }
    ],
    "id": "SP:e930323e977e5ccb5e958e9b156b6b6266bb7783",
    "references": [
        {
            "authors": [
                "Sweta Agrawal",
                "Marine Carpuat."
            ],
            "title": "An imitation learning curriculum for text editing with nonautoregressive models",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 7550\u2013",
            "year": 2022
        },
        {
            "authors": [
                "Hadi Amiri",
                "Timothy Miller",
                "Guergana Savova."
            ],
            "title": "Repeat before forgetting: Spaced repetition for efficient and effective training of neural networks",
            "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages",
            "year": 2017
        },
        {
            "authors": [
                "Yoshua Bengio",
                "J\u00e9r\u00f4me Louradour",
                "Ronan Collobert",
                "Jason Weston."
            ],
            "title": "Curriculum learning",
            "venue": "Proceedings of the 26th annual international conference on machine learning.",
            "year": 2009
        },
        {
            "authors": [
                "Karsten M Borgwardt",
                "Hans-Peter Kriegel."
            ],
            "title": "Shortest-path kernels on graphs",
            "venue": "Fifth IEEE in-",
            "year": 2005
        },
        {
            "authors": [
                "Thibault Castells",
                "Philippe Weinzaepfel",
                "Jerome Revaud."
            ],
            "title": "Superloss: A generic loss for robust curriculum learning",
            "venue": "Advances in Neural Information Processing Systems.",
            "year": 2020
        },
        {
            "authors": [
                "Mohamed Elgaar",
                "Hadi Amiri."
            ],
            "title": "HuCurl: Human-induced curriculum discovery",
            "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1862\u20131877, Toronto, Canada. Association for",
            "year": 2023
        },
        {
            "authors": [
                "Justin Gilmer",
                "Samuel S Schoenholz",
                "Patrick F Riley",
                "Oriol Vinyals",
                "George E Dahl."
            ],
            "title": "Neural message passing for quantum chemistry",
            "venue": "International conference on machine learning, pages 1263\u20131272. PMLR.",
            "year": 2017
        },
        {
            "authors": [
                "Alex Graves",
                "Marc G Bellemare",
                "Jacob Menick",
                "Remi Munos",
                "Koray Kavukcuoglu."
            ],
            "title": "Automated curriculum learning for neural networks",
            "venue": "international conference on machine learning, pages 1311\u2013 1320. PMLR.",
            "year": 2017
        },
        {
            "authors": [
                "Aric Hagberg",
                "Pieter Swart",
                "Daniel S Chult."
            ],
            "title": "Exploring network structure, dynamics, and function using networkx",
            "venue": "Technical report, Los Alamos National Lab.(LANL), Los Alamos, NM (United States).",
            "year": 2008
        },
        {
            "authors": [
                "Will Hamilton",
                "Zhitao Ying",
                "Jure Leskovec."
            ],
            "title": "Inductive representation learning on large graphs",
            "venue": "Advances in neural information processing systems.",
            "year": 2017
        },
        {
            "authors": [
                "Weihua Hu",
                "Matthias Fey",
                "Marinka Zitnik",
                "Yuxiao Dong",
                "Hongyu Ren",
                "Bowen Liu",
                "Michele Catasta",
                "Jure Leskovec."
            ],
            "title": "Open graph benchmark: Datasets for machine learning on graphs",
            "venue": "Advances in neural information processing systems, 33:22118\u201322133.",
            "year": 2020
        },
        {
            "authors": [
                "Lu Jiang",
                "Zhengyuan Zhou",
                "Thomas Leung",
                "Li-Jia Li",
                "Li Fei-Fei."
            ],
            "title": "Mentornet: Learning datadriven curriculum for very deep neural networks on corrupted labels",
            "venue": "International Conference on Machine Learning.",
            "year": 2018
        },
        {
            "authors": [
                "Hisashi Kashima",
                "Koji Tsuda",
                "Akihiro Inokuchi."
            ],
            "title": "Marginalized kernels between labeled graphs",
            "venue": "Proceedings of the 20th international conference on machine learning (ICML-03), pages 321\u2013328.",
            "year": 2003
        },
        {
            "authors": [
                "Thomas N Kipf",
                "Max Welling."
            ],
            "title": "Semisupervised classification with graph convolutional networks",
            "venue": "International Conference on Learning Representations.",
            "year": 2017
        },
        {
            "authors": [
                "Thomas N. Kipf",
                "Max Welling."
            ],
            "title": "Semisupervised classification with graph convolutional networks",
            "venue": "5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings.",
            "year": 2017
        },
        {
            "authors": [
                "Julia Kreutzer",
                "David Vilar",
                "Artem Sokolov."
            ],
            "title": "Bandits don\u2019t follow rules: Balancing multi-facet machine translation with multi-armed bandits",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2021, pages 3190\u20133204, Punta",
            "year": 2021
        },
        {
            "authors": [
                "Nils M Kriege",
                "Fredrik D Johansson",
                "Christopher Morris."
            ],
            "title": "A survey on graph kernels",
            "venue": "Applied Network Science, 5(1):1\u201342.",
            "year": 2020
        },
        {
            "authors": [
                "John P. Lalor",
                "Hong Yu."
            ],
            "title": "Dynamic data selection for curriculum learning via ability estimation",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 545\u2013555, Online. Association for Computational Linguistics.",
            "year": 2020
        },
        {
            "authors": [
                "Bruce W Lee",
                "Yoo Sung Jang",
                "Jason Lee."
            ],
            "title": "Pushing on text readability assessment: A transformer meets handcrafted linguistic features",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 10669\u2013",
            "year": 2021
        },
        {
            "authors": [
                "Bruce W. Lee",
                "Yoo Sung Jang",
                "Jason Lee."
            ],
            "title": "Pushing on text readability assessment: A transformer meets handcrafted linguistic features",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 10669\u2013",
            "year": 2021
        },
        {
            "authors": [
                "Haoyang Li",
                "Xin Wang",
                "Wenwu Zhu."
            ],
            "title": "Curriculum Graph Machine Learning: A Survey",
            "venue": "arXiv e-prints, page arXiv:2302.02926.",
            "year": 2023
        },
        {
            "authors": [
                "Fenglin Liu",
                "Shen Ge",
                "Xian Wu."
            ],
            "title": "Competencebased multimodal curriculum learning for medical report generation",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference",
            "year": 2021
        },
        {
            "authors": [
                "Yue Liu",
                "Xihong Yang",
                "Sihang Zhou",
                "Xinwang Liu",
                "Zhen Wang",
                "Ke Liang",
                "Wenxuan Tu",
                "Liang Li",
                "Jingcan Duan",
                "Cancan Chen."
            ],
            "title": "Hard sample aware network for contrastive deep graph clustering",
            "venue": "Proceedings of the AAAI conference on artificial",
            "year": 2023
        },
        {
            "authors": [
                "Adyasha Maharana",
                "Mohit Bansal."
            ],
            "title": "On curriculum learning for commonsense reasoning",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
            "year": 2022
        },
        {
            "authors": [
                "Andrew Kachites McCallum",
                "Kamal Nigam",
                "Jason Rennie",
                "Kristie Seymore."
            ],
            "title": "Automating the construction of internet portals with machine learning",
            "venue": "Information Retrieval, 3(2):127\u2013163.",
            "year": 2000
        },
        {
            "authors": [
                "Emmanouil Antonios Platanios",
                "Otilia Stretcu",
                "Graham Neubig",
                "Barnabas Poczos",
                "Tom Mitchell."
            ],
            "title": "Competence-based curriculum learning for neural machine translation",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the",
            "year": 2019
        },
        {
            "authors": [
                "Shreyas Saxena",
                "Oncel Tuzel",
                "Dennis DeCoste."
            ],
            "title": "Data parameters: A new family of parameters for learning a differentiable curriculum",
            "venue": "Advances in Neural Information Processing Systems.",
            "year": 2019
        },
        {
            "authors": [
                "Burr Settles",
                "Brendan Meeder."
            ],
            "title": "A trainable spaced repetition model for language learning",
            "venue": "Proceedings of the 54th annual meeting of the association for computational linguistics (volume 1: Long papers), pages 1848\u20131858.",
            "year": 2016
        },
        {
            "authors": [
                "Diana Sousa",
                "Andr\u00e9 Lam\u00farias",
                "Francisco M Couto."
            ],
            "title": "A silver standard corpus of human phenotypegene relations",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Lan-",
            "year": 2019
        },
        {
            "authors": [
                "Swabha Swayamdipta",
                "Roy Schwartz",
                "Nicholas Lourie",
                "Yizhong Wang",
                "Hannaneh Hajishirzi",
                "Noah A. Smith",
                "Yejin Choi."
            ],
            "title": "Dataset cartography: Mapping and diagnosing datasets with training dynamics",
            "venue": "Proceedings of the 2020 Conference on Empirical",
            "year": 2020
        },
        {
            "authors": [
                "Nidhi Vakil",
                "Hadi Amiri."
            ],
            "title": "Generic and trendaware curriculum learning for relation extraction",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
            "year": 2022
        },
        {
            "authors": [
                "Petar Veli\u010dkovi\u0107",
                "Guillem Cucurull",
                "Arantxa Casanova",
                "Adriana Romero",
                "Pietro Li\u00f2",
                "Yoshua Bengio."
            ],
            "title": "Graph attention networks",
            "venue": "International Conference on Learning Representations.",
            "year": 2018
        },
        {
            "authors": [
                "S Vichy N Vishwanathan",
                "Nicol N Schraudolph",
                "Risi Kondor",
                "Karsten M Borgwardt."
            ],
            "title": "Graph kernels",
            "venue": "Journal of Machine Learning Research, 11:1201\u20131242.",
            "year": 2010
        },
        {
            "authors": [
                "Hui Wang",
                "Kun Zhou",
                "Xin Zhao",
                "Jingyuan Wang",
                "Ji-Rong Wen."
            ],
            "title": "Curriculum pre-training heterogeneous subgraph transformer for top-n recommendation",
            "venue": "ACM Transactions on Information Systems, 41(1):1\u201328.",
            "year": 2023
        },
        {
            "authors": [
                "Yiwei Wang",
                "Wei Wang",
                "Yuxuan Liang",
                "Yujun Cai",
                "Bryan Hooi."
            ],
            "title": "Curgraph: Curriculum learning for graph classification",
            "venue": "Proceedings of the Web Conference 2021.",
            "year": 2021
        },
        {
            "authors": [
                "Xiaowen Wei",
                "Xiuwen Gong",
                "Yibing Zhan",
                "Bo Du",
                "Yong Luo",
                "Wenbin Hu."
            ],
            "title": "Clnode: Curriculum learning for node classification",
            "venue": "Proceedings of the Sixteenth ACM International Conference on Web Search and Data Mining, pages 670\u2013678.",
            "year": 2023
        },
        {
            "authors": [
                "Benfeng Xu",
                "Licheng Zhang",
                "Zhendong Mao",
                "Quan Wang",
                "Hongtao Xie",
                "Yongdong Zhang."
            ],
            "title": "Curriculum learning for natural language understanding",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages",
            "year": 2020
        },
        {
            "authors": [
                "Cheng Yang",
                "Deyu Bo",
                "Jixi Liu",
                "Yufei Peng",
                "Boyu Chen",
                "Haoran Dai",
                "Ao Sun",
                "Yue Yu",
                "Yixin Xiao",
                "Qi Zhang"
            ],
            "title": "Data-centric graph learning: A survey",
            "venue": "arXiv preprint arXiv:2310.04987",
            "year": 2023
        },
        {
            "authors": [
                "Wentao Zhang",
                "Zheyu Lin",
                "Yu Shen",
                "Yang Li",
                "Zhi Yang",
                "Bin Cui."
            ],
            "title": "Deep and flexible graph neural architecture search",
            "venue": "Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research,",
            "year": 2022
        },
        {
            "authors": [
                "Xuan Zhang",
                "Pamela Shapiro",
                "Gaurav Kumar",
                "Paul McNamee",
                "Marine Carpuat",
                "Kevin Duh."
            ],
            "title": "Curriculum learning for domain adaptation in neural machine translation",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the",
            "year": 2019
        },
        {
            "authors": [
                "Tianyi Zhou",
                "Shengjie Wang",
                "Jeff A Bilmes."
            ],
            "title": "Curriculum learning by dynamic instance hardness",
            "venue": "Advances in Neural Information Processing Systems.",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Message passing (Gilmer et al., 2017) is a widely used framework for developing graph neural networks (GNNs), where node representations are iteratively updated by aggregating the representations of neighbors (a subgraph) and applying neural network layers to perform non-linear transformation of the aggregated representations. We hypothesize that topological complexity of subgraphs or linguistic complexity of text data can affect the efficacy of message passing techniques in text graph data, and propose to employ such complexity formalisms in a novel curriculum learning framework for effective training of GNNs. Examples of graph and text complexity formalisms are node centrality and connectivity (Kriege et al., 2020); and word rarity and type token ratio (Lee et al., 2021a) respectively.\nIn Curriculum learning (CL) (Bengio et al., 2009) data samples are scheduled in a meaningful difficulty order, typically from easy to hard, for iterative training. CL approaches have been successful in various areas (Graves et al., 2017; Jiang et al., 2018; Castells et al., 2020), including NLP (Settles and Meeder, 2016; Amiri et al., 2017; Zhang et al., 2019; Lalor and Yu, 2020; Xu et al., 2020; Kreutzer et al., 2021; Agrawal and Carpuat, 2022; Maharana and Bansal, 2022). Existing approaches use data properties such as sentence length, word rarity or syntactic features (Platanios et al., 2019; Liu et al., 2021); and model properties such as training loss and its variations (Graves et al., 2017; Zhou et al., 2020) to order data samples for training. However, other types of complexity formalisms such as those developed for graph data are largely underexplored. Recently, Wang et al. (2021) proposed to estimate graph difficulty based on intra- and inter-class distributions of embeddings, realized through neural density estimators. Wei et al. (2023) employed a selective training strategy that targets nodes with diverse label distributions among their neighbors as difficult to learn nodes. Vakil and Amiri (2022) used loss trajectories to estimate the emerging difficulty of subgraphs and weighted sample losses for data scheduling.We encourage readers to see (Li et al., 2023; Yang et al., 2023) for recent surveys on graph CL approaches.\nWe propose that existing knowledge about text and graph complexity can inform better curriculum development for text graph data. For example, a training node pair that shares many common neighbors is expected to be easier for link prediction than a local bridge1 that lacks common neighbors. Motivated by this rationale, we present a complexityguided CL approach for text graphs (TGCL), which employs multiview complexity formalisms to space training samples over time for iterative training. It advances existing research as follows:\n1An edge that is not part of a triangle in the graph.\n\u2022 a new curriculum learning framework that employs graph and text complexity formalisms for training GNNs on text graph data, and\n\u2022 insights into the learning dynamics of GNNs, i.e., which complexity formalisms are learned by GNNs during training.\nWe conduct extensive experiments on real-world datasets and across GNN models, focusing on link prediction and node classification tasks in text graphs. The proposed model gains 5.1 absolute points improvement in average score over the stateof-the-art model, across datasets and GNN models, while using 39.2% less data for node classification than high-performing baselines. The results show that both node-level (local) and graph-level (global) complexity indices play a crucial role in training. More interestingly, although the best curricula derived from text and graph complexity indices are equally effective, the model consistently prefers text over graph complexity indices throughout all stages of training. Finally, the curricula learned by the model are transferable across GNN models and datasets2."
        },
        {
            "heading": "2 Method",
            "text": "A curriculum learning approach should estimate the complexity of input data, determine the pace of introducing samples based on difficulty, and schedule data samples for training. As Figure 1 shows, TGCL tackles these tasks by quantifying sample difficulty through complexity formalisms (\u00a72.1), gradually introducing training samples to GNNs\n2Code and data are available at https://clu.cs.uml. edu/tools.html\nbased on a flexible \u201ccompetence\u201d function (\u00a72.2), and employing different data schedulers that order training samples for learning with respect to model behavior (\u00a72.3). By integrating these components, TGCL establishes curricula that are both data-driven and model-dependent. In what follows, we present approaches for addressing these tasks."
        },
        {
            "heading": "2.1 Complexity Formalisms",
            "text": "Graph complexity (Kashima et al., 2003; Vishwanathan et al., 2010; Kriege et al., 2020) indices are derived from informative metrics from graph theory, such as node degree, centrality, neighborhood and graph connectivity, motif and graphlet features, and other structural features from random walk kernels or shortest-path kernels (Borgwardt and Kriegel, 2005). We use 26 graph complexity indices to compute the complexity score of data instances in graph datasets, see Table 1, and details in Appendix A.1. Since data instances for GNNs are subgraphs, we compute complexity indices for each input subgraph. For tasks involving more than one subgraph (e.g., link prediction), we aggregate complexity scores of the subgraphs through an order-invariant operation such as sum().\nLinguistic Complexity Lee et al. (2021a) implemented various linguistics complexity features for readability assessment. We use 14 traditional and shallow linguistics indices such as Smog index, Coleman Liau Readability Score, and sentence length-related indices as text complexity indices in our study. See Appendix A.2 for details. We normalize complexity scores for each text and graph index using the L2 norm."
        },
        {
            "heading": "2.2 Competence for Gradual Inclusion",
            "text": "Following the core principle of curriculum learning (Bengio et al., 2009), we propose to gradually increase the contribution of harder samples as training progresses. Specifically, we derive the competence function c(t) that determines the top fraction of training samples that the model is allowed to use for training at time step t. We derive a general form of c(t) by assuming that the rate of competence\u2013 the rate by which new samples are added to the current training data\u2013is equally distributed across the remaining training time:\ndc(t)\ndt = 1\u2212 c(t) 1\u2212 t , (1)\nwhere t \u2208 [0, 1] is the normalized value of the current training time step, with t = 1 indicating the time after which the learner is fully competent. Solving this differential equation, we obtain:\u222b\n1\n1\u2212 c(t) dc(t) =\n\u222b 1\n1\u2212 t c(t), (2)\nwhich results in c(t) = 1\u2212 exp(b)(1\u2212 t) for some constant b. Assuming the initial competence c(t = 0) is c0 and final competence c(t = 1) is 1, we obtain the following linear competence function:\nc(t) = min ( 1, 1\u2212 (1\u2212 c0)(1\u2212 t) ) . (3)\nWe modify the above function by allowing flexibility in competence so that models can use\nlarger/smaller fraction of training data than what the linear competence allows at different stages of training. This consideration results in a more general form of competence function:\nc(t) = min ( 1, (1\u2212 (1\u2212 c0)(1\u2212 t)) 1 \u03b1 ) , (4)\nwhere \u03b1 > 0 specifies the rate of change for competence during training. As Figure 2 shows, a larger \u03b1 quickly increases competence, allowing the model to use more data after a short initial training with easier samples. We expect such curricula to be more suitable for datasets with lower prevalence of easier samples than harder ones, so that the learner do not spend excessive time on the small set of easy samples at earlier stages of training. On the other hand, a smaller \u03b1 results in a curriculum that allows more time for learning from easier samples. We expect such curricula to be more suitable for datasets with greater prevalence of easier samples, as it provides sufficient time for the learner to assimilate the information content in easier samples before gradually moving to harder ones."
        },
        {
            "heading": "2.3 Spaced Repetition for Ordering Samples",
            "text": "Spaced repetition is a learning technique that involves reviewing and revisiting information at intervals over time. We propose to use spaced repetition to schedule training data for learning. Specifically, we develop schedulers that determine (data from) which complexity indices should be used for training at each time. For this purpose, we learn a delay parameter for each index, which signifies the number of epochs by which the usage of data from the index should be delayed before re-introducing the index into the training process. The schedulers dynamically (during training) increase or decrease the delay for indices based on the difficulty of learning their top c(t) samples by the GNN model.\nAs Algorithm 1 shows, the model first computes complexity indices for training and validation samples, and sorts the samples for each index according to a pre-defined order. All indices are initialized\nAlgorithm 1: TGCL Scheduler input :\nL: Complexity indices M: GNN Model D: Training data of size n V: Validation data of size m S: Index sort order(s)\noutput : Trained model M\u2217\n1 LDi \u2190 Complexity of training data based on index i 2 LVi \u2190 Complexity of validation data based on index i 3 LDi \u2190 sort(LDi , S), \u2200i 4 LVi \u2190 sort(LVi , S), \u2200i 5 \u03b4i = 1, \u2200i \u2208 L #initialize delay for indices 6 for t\u2190 0 to E do 7 current_batch\u2190 {i: \u03b4i <= 1} 8 delayed_batch\u2190 {i: \u03b4i > 1} 9 c(t)\u2190 competence from Eq (4)\n10 for i \u2208 current_batch do 11 Train M with top n\u00d7 c(t) samples in LDi 12 end 13 for i \u2208 delayed_batch do 14 \u03b4i \u2190 \u03b4i \u2212 1 15 end 16 for i \u2208 current_batch do 17 s\u2190 top m\u00d7 c(t) samples in LVi 18 di \u2190 loss(s) 19 ai \u2190 prediction_score(s) 20 \u03b3 \u2190 validation_performance(s) 21 \u03b4i \u2190 compute_delay(i, \u03b7, d, a, \u03b3) 22 end 23 end\nwith a delay of one, \u03b4i = 1, \u2200i. At every iteration, the model divides the indices into two batches: the current batch, those with an estimated delay \u03b4i \u2264 1 iteration; and the delayed batch, those with \u03b4i > 1. Indices in the current batch are those that the scheduler is less confident about their learning by the GNN and those in the delayed batch are indices that are better learned by the GNN. At each iteration, the scheduler prioritizes indices in the current batch by training the GNN using their top c(t) fraction of samples, see (4), while samples of the delayed indices are not used for training. After each iteration, all delay values are updated."
        },
        {
            "heading": "2.3.1 Delay Estimation",
            "text": "We develop schedulers that assign greater delays to indices that contain relatively easier samples within their top c(t) samples. Our intuition is that these samples are already learned to a satisfactory degree, thus requires less frequent exposure during training. Delaying such indices can result in better allocation of training resources by preventing unnecessary repetition of already learned samples and potentially directing training toward areas where model\u2019s generalization can be further improved.\nFor this purpose, we first develop a scheduler f() to estimate the optimized sample-level delay\nAlgorithm 2: Compute Optimized Delay input :\ni: Index di: Loss vector ai: Probability score vector \u03b7: Recall threshold \u03b3: Current model performance on val. data\noutput :\u03b4i: Delay for index i 1 \u03c4\u0302i \u2190 calculate optimal \u03c4 using (12) 2 t\u0302i \u2190 calculate optimal delay using (5) 3 \u03b4i \u2190 \u2211 j t\u0302ij\n|t\u0302i| using (6)\n4 return \u03b4i\nt\u0302i (Amiri et al., 2017) for the top c(t) samples of each index i in the current batch. We learn the delays such that model performance on the samples is maintained or improved after the delay:\nt\u0302i = argmaxtij ,j\u2208Qti\n( f ( dij\u00d7tij\n\u03b3 , \u03c4\u0302i\n) \u2212 \u03b7 )2 ,(5)\nwhere Qti is the top c(t) fraction of samples of index i at iteration t, di is the instantaneous losses of these samples, ti is the delay for these samples (a vector to be learned), \u03b3 is the performance of the current model on validation data, and \u03b7 \u2208 (0, 1) is the expected model performance on samples in Qti after the delay. f() is a non-increasing function of xi = di\u00d7ti\u03b3 , and is responsible for assigning greater delays (ti) to easier samples (smaller di) in stronger networks (greater \u03b3) (Amiri et al., 2017). Intuitively, (5) estimates the maximum delay t\u0302i for the samples in Qti such that, with a probability of \u03b7, the performance of the model is maintained or improved for these samples at iteration e+ t\u0302i. The hyperparameter \u03c4 controls the rate of decay for f , which is optimized using the achieved model performance in hindsight, see \u00a72.3.2. The delay for each index i is obtained by averaging the optimal delays of its top c(t) samples (Qti) as follows:\n\u03b4i = 1\n|t\u0302i| \u2211 j\u2208Qti t\u0302ij . (6)\nIn addition, indices in the delayed batch are not used for training at current iteration and thus their delays are reduced by one, Line 14, Algorithm 1. We note that, given the improvement gain by the GNN model as training progresses, the above approach is conservative and provides a lower bound of the optimal delays for indices."
        },
        {
            "heading": "2.3.2 Scheduling Functions",
            "text": "A good scheduler should assign greater delays to easier samples in stronger models. Therefore, we\ncan use any non-increasing function of xi = di\u00d7ti\u03b3 . We consider the following functions:\nflap (xi, \u03c4i) = exp(\u2212xi\u03c4i) (7)\nfsec (xi, \u03c4i) = 2\nexp(\u2212\u03c4ix2i ) + exp(\u03c4ix2i ) (8)\nfcos (xi, \u03c4i) =\n{ 1 2 cos (\u03c4i\u03c0xi) + 1 xi < 1 \u03c4i\n0 otherwise\n(9)\nfqua (xi, \u03c4i) = { 1\u2212 \u03c4ix2i x2i < 1\u03c4i 0 otherwise\n(10)\nflin (xi, \u03c4i) = { 1\u2212 \u03c4ixi xi < 1\u03c4i 0 otherwise\n(11)\nFor each index i, we estimate the optimal value of the hyperparameter \u03c4i using information from previous iteration. Specifically, given the sample loss and validation performance from the previous iteration for the top c(t \u2212 1) samples of index i (Qt\u22121i ), and the current accuracy of the GNN model on these samples (pi), we estimate \u03c4i as:\n\u03c4\u0302i = argmin\u03c4i (f (xi, \u03c4i)\u2212 pi) 2 , (12)\n\u2200j \u2208 Qe\u22121i , pij >= \u03b7.\nSee the steps for delay estimation in Algorithm 2."
        },
        {
            "heading": "2.4 Base GNN Models",
            "text": "Our approach can be used to train any GNN model. We consider four models for experiments: GraphSAGE (Hamilton et al., 2017), graph convolutional network (GCN) (Kipf and Welling, 2017a), graph attention networks (GAT) (Velic\u030ckovic\u0301 et al., 2018), and graph text neural network (GTNN) (Vakil and Amiri, 2022). GraphSAGE is a commonly-used model that learns node embeddings by aggregating the representation of neighboring nodes through an order-invariant operation. GCN is an efficient and scalable approach based on convolution neural networks which directly operates on graphs. GAT extends GCN by employing self-attention layers to identify informative neighbors while aggregating their information. GTNN extends GraphSAGE for NLP tasks by directly using node representations at the prediction layer as auxiliary information, alleviating information loss in the iterative process of learning node embeddings in GNNs."
        },
        {
            "heading": "3 Experiments",
            "text": ""
        },
        {
            "heading": "3.1 Datasets",
            "text": "Ogbn-arxiv from Open Graph Benchmark (Hu et al., 2020) is a citation network of computer sci-\nence articles. Each paper is provided with an embedding vector of size 128, obtained from average word embeddings of the title and abstract of the paper and categorized into one of the 40 categories.\nCora (McCallum et al., 2000) is a relatively small citation network, in which papers are categorized into one of the seven subject categories and is provided with a feature word vector obtained from the content of the paper.\nCiteseer (Kipf and Welling, 2017b) a citation network of scientific articles, in which nodes are classified six classes. We use the same data split as reported in (Zhang et al., 2022).\nGene, Disease, Phenotype Relation (GDPR) (Vakil and Amiri, 2022) is a large scale dataset for predicting causal relations between genes and diseases from their text descriptions. Each node in the graph is a gene or disease and an edge represents (causal) relation between genes and diseases.\nGene Phenotype Relation (PGR) (Sousa et al., 2019) is a dataset for extracting relations between gene and phenotypes (symptoms) from short sentences. Each node in the graph is a gene, phenotype or disease and an edge represents a relation between its end points. Since the original dataset does not contain a validation split, we generate a validation set from training data through random sampling, while leaving the test data intact. The data splits will be released with our method."
        },
        {
            "heading": "3.2 Baselines",
            "text": "In addition to the GNN models described in \u00a72.4, we use the following curriculum learning baselines for evaluation:\nCompetence CL (CCL) (Platanios et al., 2019) is a competence-based CL approach that gradually introduces the data in increasing order of difficulty to the model according to a competence function. The model only works with one difficulty score, which we provide by summing the complexity indices for each training sample.\nSuperLoss (SL) (Castells et al., 2020) is a CL framework that determines the difficulty of samples by comparing them against the loss value of their corresponding batches. It assigns greater weights to easy samples and gradually introduces harder examples as training progresses.\nCurGraph (Wang et al., 2021) is a CL approach for GNNs that computes difficulty scores based on the intra- and inter-class distributions of embeddings, realized through a neural density estimator, and develops a smooth-step function to gradually use harder samples in training. We implemented this approach by closely following the paper.\nTrend SL (Vakil and Amiri, 2022) extends SuperLoss by discriminating easy and hard samples based on their recent loss trajectories. Similar to SL, Trend-SL can be used with any loss function.\nCLNode (Wei et al., 2023) employs a selective training strategy that estimates sample difficulty based on the diversity in the label of neighboring nodes and identifies mislabeled difficult nodes by analyzing their node features. CLNode implements an easy to hard transition curriculum."
        },
        {
            "heading": "3.3 Settings",
            "text": "In the competence function (4), we set the value of \u03b1 from [0.2, 5]. In (5) and (12), we set \u03b7 from [0.6, 1) with step size of 0.1 for link prediction and from [0.7, 1) with the step size of 0.5 for node classification. The best kernel for the datasets in the reported results are cos, qua, lap, qua, sec, and the best value of \u03b7 is 0.7, 0.9, 0.8, 0.75, 0.9 for GDPR, PGR, Ogbn-Arxiv, Cora and Citeseer respectively. We consider a maximum number of 100 and 500 training iterations for link prediction and node classification respectively. In addition, we order samples for each index in four different ways, ascending/descending (low/high to high/low complexity), and medium ascending/descending (where instances are ordered based on their absolute distance to the standard Normal distribution mean of the complexity scores in ascending/descending or-\nder). We evaluate models based on the F1 score for link prediction and accuracy for node prediction task using (Buitinck et al., 2013). Finally, we run all experiments on a single A100 40GB GPU."
        },
        {
            "heading": "3.4 Main Results",
            "text": "Table 2 shows the performance of our approach for link prediction and node classification tasks using four GNN models. The performance of all models on link prediction and node classification significantly decreases when we switch from GTNN to any other GNN as encoder, which indicates additional text features as auxiliary information in GTNN are useful for effective learning.See Appendix A.6 for the performance of kernel functions.\nFor both link prediction and node classification tasks, most curricula improve the performance compared to standard training (No-CL in Table 2). On an average, TGCL performs better than other CL baselines with most GNN models. CurGraph show lower performance than other curricula and NoCL in almost all settings, except when used with GraphSAGE, GCN and GAT on GDPR. The lower overall performance of CurGraph and CLNode may be due to the static order of samples or monotonic curricula imposed by the model, or our implementation of CurGraph. The large performance gains of TGCL on node classification and link prediction datasets against No-CL indicates the importance of the information from difficulty indices, their systematic selection, timely delays, and revisiting indices progressively during training, which help the model generalize better."
        },
        {
            "heading": "4 Curricula Introspection",
            "text": "We conduct several analysis on TGCL scheduler to study its behavior and shed light on the reasons for its improved performance. Due to space limitations, we conduct these experiments on one representative dataset from each task, PGR for link prediction and Ogbn-Arxiv for node classification."
        },
        {
            "heading": "4.1 Learning Dynamics",
            "text": "For these experiments, we divide training iterations into three phases: Phase-1 (early training, the first 33% of iterations), Phase-2 (mid training, the second 33% of iterations), and Phase-3 (late training, the last 33% of iterations). We report the number of times graph complexity indices appeared in the current batch at each phase. We group indices based on their types and definitions as reported in Table 1 to ease readability.\nFigures 3a and 3b show the results. The frequency of use for indices follows a decreasing trend. This is expected as in the initial phase the model tends to have lower accuracy in its predictions, resulting in higher loss values. Consequently, the scheduler assigns smaller delays to most indices, ensuring that they appear in the current batch at the early stage of training. However, as the model improves its prediction accuracy, the scheduler becomes more flexible with delays during the latter stages of training. This increased flexibility allows the scheduler to adjust the delay values dynamically and adapt to the learning progress of the model. In addition, the results show that the model focuses on both local and global indices (degree and centrality respectively) for link prediction, while it prioritizes local indices (degree and basic) over global indices for node classification throughout the training. See Appendix A.4 for detailed results."
        },
        {
            "heading": "4.2 TGCL Gains More and Uses Less Data",
            "text": "In standard training, a model uses all its n training examples per epoch, resulting in a total number of n\u00d7 E updates. TGCL uses on an average 39.2% less training data for node classification for GTNN model, by strategically delaying indices throughout the training. Figure 4 shows the average number of examples used by different CL models for training across training iteration, computed across all node classification datasets. Our model TGCL uses less data as the training progresses, the standard training (No-CL) and some other curricula such as SL and SL-Trend uses all training data at each iteration. CCL, apart from TGCL, uses less data compared to other CL models. An intriguing observation is that despite both CCL and TGCL are allowed to use\nmore data as training progresses, TGCL uses less data by strategically delaying indices and avoiding unnecessary repetition of samples that have already been learned, resulting in better training resources and reduced redundancy.\nIn spaced repetition, a spacing effect is observed when the difference between subsequent reviews of learning materials increases as learning progresses. As a result, the percent of the indices used by model for training should decrease as the model gradually becomes stronger. Figure 5 illustrates this behavior exhibited by TGCL. This results demonstrates that the delays assigned by the scheduler effectively space out the data samples over time, leading to an effective training process."
        },
        {
            "heading": "4.3 TGCL Prioritizes Linguistics Features",
            "text": "For this analysis, we calculate linguistic indices (detailed in \u00a72.1 and Appendix A.2) from the paper titles in Ogbn-Arxiv. We augment the graph indices with linguistics indices and retrain our top performing model, GTNN, on Ogbn-Arxiv to assess the importance of linguistics indices in the training process. The resulting accuracy is 76.4, which remains\nunchanged compared to using only graph indices. However, we observe that the model consistently prefers linguistic indices (Coleman Liau Readability and sentence length related indices), followed by the degree based indices, throughout all phases of the training. Figure 6 shows the contribution of linguistic and graph indices in different phases of training. While linguistic indices do not lead to an accuracy beyond 76.4, they are consistently prioritized by TGCL over graph indices. Incorporating additional linguistic indices have the potential to further enhance performance."
        },
        {
            "heading": "4.4 TGCL Learns Transferable Curricula",
            "text": "We study the transferability of curricula learned by TGCL across datasets and models. For these experiments, we track the curriculum (competence values and indices used for training at every iteration) of a source dataset and apply the curriculum to a target dataset using GTNN as the base model. Table 3 shows learned curricula are largely transferable across dataset, considering the performance of No-CL as the reference. We note that the slight reduction in performance across datasets (compared to the source curricula), can be negligible considering the significant efficiency that can be gained through the adoption of free curricula (39.2% less training data, see \u00a74.2). Table 4 shows the curricula learned by TGCL can be transferred across GNN models, and in some cases improves the performance, e.g., GAT to GCN. Further analysis on these results is the subject of our future works."
        },
        {
            "heading": "5 Related Work",
            "text": "In Curriculum learning (CL) (Bengio et al., 2009) data samples are scheduled in a meaningful difficulty order, typically from easy to hard, for iterative training. In graph machine learning, Wang et al. (2021) introduced CurGraph, a curriculum learning method designed for sub-graph classification. This\nmodel assesses the difficulty of samples by analyzing both intra-class and inter-class distributions of sub-graph embeddings. It then organizes the training instances, by first exposing easier sub-graphs and gradually introducing more challenging ones. Wei et al. (2023) adopted a selective training strategy, targeting nodes with diverse label distributions among their neighbors as particularly challenging to learn. Liu et al. (2023) proposed HSAN, which clusters graphs using curriculum and contrastive learning and measures the difficulty of training pairs using attribute and structural similarity and use weights to select hard negative samples. Wang et al. (2023) proposed an approach called CHEST to improve recommendation using heterogeneous graph data and combine local and global context information to guide curriculum development.\nIn contrast to static curriculum approaches, Saxena et al. (2019) proposed a dynamic curriculum approach that automatically assigns confidence scores to samples based on their estimated difficulty. However this model requires additional trainable parameters. To address this limitation, Castells et al. (2020) introduced the SuperLoss framework to calculate optimal confidence scores for each instance using a closed-form solution. In (Vakil and Amiri, 2022), we extended SuperLoss to incorporate trend information at the sample level. We utilized loss trajectories to estimate the emerging difficulty of subgraphs and employed weighted sample losses for data scheduling in order to create effective curricula for training GNNs and understanding their\nlearning dynamics. Current curriculum learning methodologies in NLP rely on data properties, e.g., sentence length, word rarity, or syntactic features (Platanios et al., 2019; Liu et al., 2021), or annotation disagreement (Elgaar and Amiri, 2023); as well as model properties such as training loss and its variations (Graves et al., 2017; Amiri et al., 2017) to sequence data samples for training. Elgaar and Amiri (2023) developed a curriculum discovery framework based on prior knowledge of sample difficulty, utilized annotation entropy and loss values. They concluded that curricula based on easy-to-hard or hard-to-easy transition are often at the risk of under-performing, effective curricula are often non-monotonic, and curricula learned from smaller datasets perform well on larger datasets.\nOther instances of curriculum learning for textual data have primarily centered on machine translation and language comprehension. For instance, Agrawal and Carpuat (2022) introduced a framework for training non-autoregressive sequence-tosequence models for text editing. Additionally, Maharana and Bansal (2022) designed various curriculum learning approaches where the teacher model assesses the difficulty of each training example by considering factors such as question-answering probability, variability, and out-of-distribution measures. Other notable work in various domain includes (Graves et al., 2017; Jiang et al., 2018; Castells et al., 2020; Settles and Meeder, 2016; Amiri et al., 2017; Zhang et al., 2019; Lalor and Yu, 2020; Xu et al., 2020; Kreutzer et al., 2021) which have contributed to its broader adoption."
        },
        {
            "heading": "6 Conclusion and Future Work",
            "text": "We introduce a novel curriculum learning approach for text graph data and graph neural networks, inspired by spaced repetition. By leveraging text and graph complexity formalisms, our approach determines the optimized timing and order of training samples. The model establishes curricula that are both data-driven and model- or learner-dependent. Experimental results demonstrate significant performance improvements in node classification and link prediction tasks when compared to strong baseline methods. Furthermore, our approach offers potential for further enhancements by incorporating additional complexity indices, exploring different scheduling functions and model transferability, and extending its applicability to other domains.\nBroader Impacts\nThe advancements in curriculum learning signal a promising direction for the optimization of training processes within NLP and graph data. Based on the principles of \u201cspaced repetition\u201d and text and graph complexity measures, the proposed work enhances the efficiency of training and improves model generalization capabilities. This is particularly crucial for applications reliant on graph representations of text, such as social network analysis, recommendation systems, and semantic web. Furthermore, the method\u2019s ability to derive transferable curricula across different models and datasets suggests a more applicable strategy, potentially enabling seamless integration and deployment across varied NLP applications and domains.\nLimitation The proposed approach relies on the availability of appropriate complexity formalisms. If the selected indices do not capture the desired complexity, the curricula may not be optimally designed. The approach primarily focuses on text graph data and graph neural networks, and the results may not directly apply to other types of data or architectures. The estimation of optimized time and order for training samples introduces additional computational overhead. This can be a limitation in scenarios where real-time training is required, e.g., in processing streaming data of microposts."
        },
        {
            "heading": "A Appendix",
            "text": "A.1 Graph Indices Definition\nBelow are the list of 26 indices which we consider for TGCL. All these indices are computed on the subgraph of the node or an edge. These definition and code to calculate the indices, we used Networkx package (Hagberg et al., 2008).\n\u2022 Degree: The number of immediate neighbors of a node in a graph.\n\u2022 Treewidth min degree: The treewidth of an graph is an integer number which quantifies, how far the given graph is from being a tree.\n\u2022 Average neighbor degree: Average degree of the neighbors of a node is computed as:\n1 |Ni| \u2211 j\u2208Ni kj\nwhere Ni is the set of neighbors of node i and kj is the degree of node j.\n\u2022 Degree mixing matrix: Given the graph, it calculates joint probability, of occurrence of node degree pairs. Taking the mean, gives the degree mixing value representing the given graph.\n\u2022 Average degree connectivity: Given the graph, it calculates the average of the nearest neighbor degree of nodes with degree k. We choose the highest value of k obtained from the calculation and used its connectivity value as the complexity index score.\n\u2022 Degree assortativity coefficient: Given the graph, assortativity measures the similarity of connections in the graph with respect to the node degree.\n\u2022 Katz centrality: The centrality of a node, i, computed based on the centrality of its neighbors j. Katz centrality computes the relative influence of a node within a network by measuring taking into account the number of immediate neighbors and number of walks between node pairs. It is computed as follows:\nxi = \u03b1 \u2211 j Aijxj + \u03b2\nwhere xi is the Katz centrality of node i, A is the adjacency matrix of Graph G with eigenvalues \u03bb. The parameter \u03b2 controls the initial centrality and \u03b1 < 1 / \u03bbmax.\n\u2022 Degree centrality: Given the graph, the degree centrality for a node is the fraction of nodes connected to it.\n\u2022 Closeness centrality: The closeness of a node is the distance to all other nodes in the graph or in the case that the graph is not connected to all other nodes in the connected component containing that node. Given the subgraph and the nodes, added the values of the nodes to find the complexity index value.\n\u2022 Eigenvector centrality: Eigenvector centrality computes the centrality for a node based on the centrality of its neighbors. The eigenvector centrality for node i is Ax = \u03bbx. where A is the adjacency matrix of the graph G with eigenvalue \u03bb.\n\u2022 Group Degree centrality: Group degree centrality of a group of nodes S is the fraction of non-group members connected to group members.\n\u2022 Ramsey R2: This computes the largest clique and largest independent set in the graph G. We calculate the index value by multiplying number of largest cliques to number of largest independent set.\n\u2022 Average clustering: The local clustering of each node in the graph G is the fraction of triangles that exist over all possible triangles in its neighborhood. The average clustering coefficient of a graph G is the mean of local clusterings.\n\u2022 Resource allocation index: For nodes i and j in a subgraph, the resource allocation index is defined as follows:\u2211\nk\u2208(Ni \u22c2 Nj)\n1\n|Nk| ,\nwhich quantifies the closeness of target nodes based on their shared neighbors.\n\u2022 Subgraph density: The density of an undirected subgraph is computed as follows:\ne\nv(v \u2212 1) ,\nwhere e is the number of edges and v is the number of nodes in the subgraph.\n\u2022 Local bridge: A local bridge is an edge that is not part of a triangle in the subgraph. We take the number of local bridges in a subgraph as a complexity score.\n\u2022 Number of nodes: Given the graph G, number of nodes in the graph is chosen as the complexity score.\n\u2022 Number of Edges: Given the graph G, number of edges in the graph is chosen as the complexity score.\n\u2022 Large clique size: Given the graph G, the size of a large clique in the graph is chosen as the complexity score.\n\u2022 Common neighbors: Given the graph and the nodes, it finds the number of common neighbors between the pair of nodes. We chose number of common neighbors as the complexity score.\n\u2022 Subgraph connectivity: is measured by the minimum number of nodes that must be removed to disconnect the subgraph.\n\u2022 Local node connectivity: Local node connectivity for two non adjacent nodes s and t is the minimum number of nodes that must be removed (along with their incident edges) to disconnect them. Given the subgraph and the nodes, gives the single value which we used as complexity score.\n\u2022 Minimum Weighted Dominating Set: For a graph G = (V,E), the weighted dominating set problem is to find a vertex set S \u2286 V such that when each vertex is associated with a positive number, the goal is to find a dominating set with the minimum weight.\n\u2022 Weighted vertex cover index: The weighted vertex cover problem is to find a vertex cover S\u2013a set of vertices that include at least one endpoint of every edge of the subgraph\u2013that has the minimum weight. This index and the weight of the cover S is defined by\u2211\ns\u2208S w(s), where w(s) indicates the weight of s. Since w(s) = 1, \u2200s in our unweighted subgraphs, the problem will reduce to finding a vertex cover with minimum cardinality.\n\u2022 Minimum edge dominating set: Minimum edge dominating set approximate solution to the edge dominating set.\n\u2022 Minimum maximal matching: Given a graph G = (V,E), a matching M in G is a set of pairwise non-adjacent edges; that is, no two edges share a common vertex. That is, out of all maximal matchings of the graph G, the smallest is returned. We took the length of the set as the complexity index.\nA.2 Linguistics Indices\nBelow are the list of linguistic (Lee et al., 2021b) indices used in our experiments. We follow (Lee et al., 2021b) to measure all scores.\nTraditional Formulas (TraF) These features computes the readability score of the given text based on the content, complexity of its vocabulary and syntactic information. Readability can be defined as the ease with which a reader can understand a written text.\n\u2022 Gunning Fog Count score: The Gunning fog index is a readability test for English writing. It commonly used to confirm that text can be read easily by the intended audience. It is computed as follows: 0.4 [( words\nsentences\n) + 100 ( complexwords\nwords )] where words is the number of words, sentences is the number of sentences, and complexwords is the number of complex words\n\u2022 New Automated readability index: The automated readability index is a readability test for texts, which determines the understandability of a text. It is computed as follows:\n4.71 [(\ncharacters words\n) + 0.5 ( words\nsentences )] where characters is the number of letters and numbers, words is the number of spaces, and sentences is the number of sentences.\n\u2022 Flesch Kincaid Grade level: This readability test is design to determine how difficult is the given text to understand. It can be computed as follows:\n0.39 [(\nwords sentences\n) + 11.8 ( syllables words )] where words is the total number of words, sentences is the total number of sentences, and syllables is the total number of syllables\n\u2022 Linsear Write Formula score: It is a readability metric for text originally developed to calculate the readability of technical manuals. It can be computed as follows:\nAlgorithm 3: Compute Linsear Write score 1 Initialize r = 0 2 For each easy word, defined as word with 2 syllables\nor less r = r + 1 3 For each hard word, defined as word with 3 syllables\nor more r = r + 3\nr = r\nsentences\nwhere sentences = number of sentences in 100 word sample\n4 if r > 20, LinsearWritescore = r 2 5 if r =< 20, LinsearWritescore = r 2 \u2212 1\n\u2022 Coleman Liau Readability Score: The Coleman\u2013Liau index is calculated as follows:\n0.0588 \u2217 L\u2212 0.296 \u2217 S \u2212 15.8 where L is the average number of letters per 100 words, S is the average number of sentences per 100 words\n\u2022 SMOG Index: SMOG index can be calculated as follows: 1.0430 \u2217 (polysyllables \u2217 30\nsentences )1/2 + 3.1291\nShallow Features (ShaF) These features captures the surface level difficulties. Features used are as follows:\n\u2022 Average count of characters per token: The average count of characters per token is taken as the complexity score.\n\u2022 Average count of characters per sentence: The average count of characters per sentence is taken as the complexity score.\n\u2022 Average count of syllables per token: The average count of syllables per token is taken as the complexity score.\n\u2022 Average count of syllables per sentence: The average count of characters per syllables is taken as the complexity score.\n\u2022 Sentence length: computed by count of token per sentence\n\u2022 Token sentence ratio: computed by the log of total count of tokens divided by the log of total count of sentences.\n\u2022 Token sentence multiply: computed by the total count of tokens multiply by the total count of sentences, and its square root.\nA.3 TGCL Exploits All Ranking Orders\nFigures 7a and 7b show the distribution of indices used for training based on to their ordering strategy (see \u00a72.1 and Algorithm 1) during different phases of training. As mentioned before, complexity scores can be sorted in four ways: ascending, descending, medium ascending, and medium descending orders, which represent easy-to-hard or hard-to-easy order types. The results on the PGR dataset show that in the initial phase of training the scheduler uses all order types, while emphasizing most on ascending followed by descending orders. And, in the mid and late training phases, the model prioritizes the ascending difficulty order over the other orders with a fairly larger difference in usage. The results on Ogbn-Arxiv show that TGCL relies equally on all order types during its training with a slightly greater emphasis on descending order at the latter stages of training. The relatively significant use of medium ascending order, especially\nat the early stage of training, is in line with recent studies showing the importance of using mediumlevel samples for effective training (Swayamdipta et al., 2020).\nA.4 Fine-grained Index Analysis\nFigure 8 shows fine-grained analysis for OgbnArxiv when linguistic indices are included along with the graph indices. In the Phase-1, scheduler focuses on all the indices with more frequency of SraF based indices from linguistics, and Ramsey and degree based indices from graph features. In the Phase-2, the overall use of all the indices is reduced and it focuses more on readability indices (TraF) from linguistics features, and uses Ramsey R2 more from the graph indices. In Phase-3, scheduler uses very less indices at the end of the training and focuses on the average count of characters per sentence from linguistic features and degree assortaivity coefficient from the graph indices.\nAs shown in the Figure 9 for PGR dataset, centrality and degree based indices are used more frequently. Closeness centrality, density, and degree assortativity coefficient indices are used more frequently in Phase-1 and Phase-2, initial part and middle part of training. In the final phase of the training Phase-3, scheduler focuses on closeness centrality and degree connectivity based indices more frequently.\nAs shown in the Figure 10, for Ogbn-Arxiv dataset, basic and degree based indices are used more frequently. In the initial phase of training, as the threshold is high (\u03b7) scheduler uses all available indices. In the Phase-2, scheduler uses degree centrality, Ramsey R2 and degree assortativity coefficient more frequently. In Phase-3 scheduler uses local bridge, degree and degree centrality more frequently compare to the other indices.\nA.5 Selection Process for Complexity Indices\nTo avoid the over representation of similar indices, we group indices based on their similarity. For this purpose, we compute the Pearson Co-relation coefficient between complexity scores of each pair of indices and create an n \u00d7 n correlation matrix (where, n is the total number of indices). We use this co-relation matrix as an input to K-means and empirically group the indices into 10 clusters. We randomly select one metric from each cluster for use in our curriculum framework. See indices with \u22c6 labels in Table 1. Here, \u22c6 indicates the indices\nwere used in one of the dataset used in the experiments.\nA.6 Detailed results Table 5 shows the performance of TGCL model with different kernel functions for all the datasets on GTNN as the base model."
        }
    ],
    "title": "Complexity-Guided Curriculum Learning for Text Graphs",
    "year": 2023
}