{
    "abstractText": "As dialogue systems become more popular, evaluation of their response quality gains importance. Engagingness highly correlates with overall quality and creates a sense of connection that gives human participants a more fulfilling experience. Although qualities like coherence and fluency are readily measured with well-worn automatic metrics, evaluating engagingness often relies on human assessment, which is a costly and time-consuming process. Existing automatic engagingness metrics evaluate the response without the conversation history, are designed for one dataset, or have limited correlation with human annotations. Furthermore, they have been tested exclusively on English conversations. Given that dialogue systems are increasingly available in languages beyond English, multilingual evaluation capabilities are essential. We propose that large language models (LLMs) may be used for evaluation of engagingness in dialogue through prompting, and ask how prompt constructs and translated prompts compare in a multilingual setting. We provide a prompt-design taxonomy for engagingness and find that using selected prompt elements with LLMs, including our comprehensive definition of engagingness, outperforms state-of-the-art methods on evaluation of engagingness in dialogue across multiple languages.1",
    "authors": [
        {
            "affiliations": [],
            "name": "Amila Ferron"
        },
        {
            "affiliations": [],
            "name": "Ameeta Agrawal"
        }
    ],
    "id": "SP:c9c7ca6a2d3bb7371f3839760ba63acbcc94742a",
    "references": [
        {
            "authors": [
                "Daniel Adiwardana",
                "Minh-Thang Luong",
                "David R. So",
                "Jamie Hall",
                "Noah Fiedel",
                "Romal Thoppilan",
                "Zi Yang",
                "Apoorv Kulshreshtha",
                "Gaurav Nemade",
                "Yifeng Lu",
                "Quoc V. Le."
            ],
            "title": "Towards a human-like opendomain chatbot",
            "venue": "CoRR, abs/2001.09977.",
            "year": 2020
        },
        {
            "authors": [
                "Vibhav Agarwal",
                "Pooja Rao",
                "Dinesh Babu Jayagopi."
            ],
            "title": "Towards code-mixed Hinglish dialogue generation",
            "venue": "Proceedings of the 3rd Workshop on Natural Language Processing for Conversational AI, pages 271\u2013280, Online. Association for Computational Lin-",
            "year": 2021
        },
        {
            "authors": [
                "Jessica Joelle Alexander",
                "Camilla Semlov Andersson."
            ],
            "title": "F\u00e6llesskab and belonging",
            "venue": "M\u00edriam Juan-Torres Gonz\u00e1lez, Yvette Tetteh, and EJ Toppin, editors, Paper Series: On Belonging in Europe. Othering Belonging Institute.",
            "year": 2022
        },
        {
            "authors": [
                "Deborah Cohen",
                "Moonkyung Ryu",
                "Yinlam Chow",
                "Orgad Keller",
                "Ido Greenberg",
                "Avinatan Hassidim",
                "Michael Fink",
                "Yossi Matias",
                "Idan Szpektor",
                "Craig Boutilier",
                "Gal Elidan"
            ],
            "title": "Dynamic planning in openended dialogue using reinforcement learning",
            "year": 2022
        },
        {
            "authors": [
                "Pierre Colombo",
                "Maxime Peyrard",
                "Nathan Noiry",
                "Robert West",
                "Pablo Piantanida"
            ],
            "title": "The glass ceiling of automatic evaluation in natural language generation",
            "year": 2022
        },
        {
            "authors": [
                "Mingkai Deng",
                "Bowen Tan",
                "Zhengzhong Liu",
                "Eric Xing",
                "Zhiting Hu"
            ],
            "title": "Compression, transduction, and creation: A unified framework for evaluating",
            "year": 2021
        },
        {
            "authors": [
                "Patrick Fernandes",
                "Aman Madaan",
                "Emmy Liu",
                "Ant\u00f3nio Farinhas",
                "Pedro Henrique Martins",
                "Amanda Bertsch",
                "Jos\u00e9 G.C. de Souza",
                "Shuyan Zhou",
                "Tongshuang Wu",
                "Graham Neubig",
                "Andr\u00e9 F.T. Martins"
            ],
            "title": "Bridging the gap: A survey",
            "year": 2023
        },
        {
            "authors": [
                "Jinlan Fu",
                "See-Kiong Ng",
                "Zhengbao Jiang",
                "Pengfei Liu"
            ],
            "title": "Gptscore: Evaluate as you desire",
            "year": 2023
        },
        {
            "authors": [
                "Mingqi Gao",
                "Jie Ruan",
                "Renliang Sun",
                "Xunjian Yin",
                "Shiping Yang",
                "Xiaojun Wan"
            ],
            "title": "Human-like summarization evaluation with chatgpt",
            "year": 2023
        },
        {
            "authors": [
                "Mingqi Gao",
                "Xiaojun Wan"
            ],
            "title": "Social biases in automatic evaluation metrics for nlg",
            "year": 2022
        },
        {
            "authors": [
                "Nan Gao",
                "Mohammad Saiedur Rahaman",
                "Wei Shao",
                "Flora D. Salim"
            ],
            "title": "Investigating the reliability of self-report data in the wild: The quest for ground truth",
            "year": 2021
        },
        {
            "authors": [
                "Sebastian Gehrmann",
                "Elizabeth Clark",
                "Thibault Sellam"
            ],
            "title": "Repairing the cracked foundation: A survey of obstacles in evaluation practices for generated text",
            "year": 2022
        },
        {
            "authors": [
                "Sarik Ghazarian",
                "Ralph Weischedel",
                "Aram Galstyan",
                "Nanyun Peng."
            ],
            "title": "Predictive engagement: An efficient metric for automatic evaluation of opendomain dialogue systems",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, 34(05):7789\u2013",
            "year": 2020
        },
        {
            "authors": [
                "Tom Kocmi",
                "Christian Federmann"
            ],
            "title": "Large language models are state-of-the-art evaluators of translation quality",
            "year": 2023
        },
        {
            "authors": [
                "Chin-Yew Lin."
            ],
            "title": "ROUGE: A package for automatic evaluation of summaries",
            "venue": "Text Summarization Branches Out, pages 74\u201381, Barcelona, Spain. Association for Computational Linguistics.",
            "year": 2004
        },
        {
            "authors": [
                "Chia-Wei Liu",
                "Ryan Lowe",
                "Iulian Serban",
                "Mike Noseworthy",
                "Laurent Charlin",
                "Joelle Pineau."
            ],
            "title": "How NOT to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation",
            "venue": "Proceedings of",
            "year": 2016
        },
        {
            "authors": [
                "Yang Liu",
                "Dan Iter",
                "Yichong Xu",
                "Shuohang Wang",
                "Ruochen Xu",
                "Chenguang Zhu"
            ],
            "title": "G-eval: Nlg evaluation using gpt-4 with better human alignment",
            "year": 2023
        },
        {
            "authors": [
                "Varvara Logacheva",
                "Mikhail Burtsev",
                "Valentin Malykh",
                "Vadim Polulyakh",
                "Aleksandr Seliverstov."
            ],
            "title": "Convai dataset of topic-oriented human-to-chatbot dialogues",
            "venue": "The NIPS \u201917 Competition: Building Intelligent Systems, pages 47\u201357, Cham. Springer",
            "year": 2018
        },
        {
            "authors": [
                "Zheheng Luo",
                "Qianqian Xie",
                "Sophia Ananiadou"
            ],
            "title": "Chatgpt as a factual inconsistency evaluator for text summarization",
            "year": 2023
        },
        {
            "authors": [
                "Shikib Mehri",
                "Maxine Eskenazi."
            ],
            "title": "Unsupervised evaluation of interactive dialog with DialoGPT",
            "venue": "Proceedings of the 21th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 225\u2013235, 1st virtual meeting. Association for",
            "year": 2020
        },
        {
            "authors": [
                "Shikib Mehri",
                "Maxine Eskenazi."
            ],
            "title": "USR: An unsupervised and reference free evaluation metric for dialog generation",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 681\u2013707, Online. Association for",
            "year": 2020
        },
        {
            "authors": [
                "Subhabrata Mukherjee",
                "Arindam Mitra",
                "Ganesh Jawahar",
                "Sahaj Agarwal",
                "Hamid Palangi",
                "Ahmed Awadallah"
            ],
            "title": "Orca: Progressive learning from complex explanation traces of gpt-4",
            "year": 2023
        },
        {
            "authors": [
                "Jekaterina Novikova",
                "Ond\u0159ej Du\u0161ek",
                "Amanda Cercas Curry",
                "Verena Rieser."
            ],
            "title": "Why we need new evaluation metrics for NLG",
            "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2241\u20132252, Copen-",
            "year": 2017
        },
        {
            "authors": [
                "Paul Christiano",
                "Jan Leike",
                "Ryan Lowe"
            ],
            "title": "Training language models to follow instructions with human feedback",
            "year": 2022
        },
        {
            "authors": [
                "Kishore Papineni",
                "Salim Roukos",
                "Todd Ward",
                "WeiJing Zhu."
            ],
            "title": "Bleu: A method for automatic evaluation of machine translation",
            "venue": "Proceedings of the",
            "year": 2002
        },
        {
            "authors": [
                "John A. Powell",
                "Stephen Menendian."
            ],
            "title": "On belonging: An introduction to othering belonging in europe",
            "venue": "M\u00edriam Juan-Torres Gonz\u00e1lez, Yvette Tetteh, and EJ Toppin, editors, Paper Series: On Belonging in Europe. Othering Belonging Institute.",
            "year": 2022
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Tao Xu",
                "Greg Brockman",
                "Christine McLeavey",
                "Ilya Sutskever"
            ],
            "title": "Robust speech recognition via large-scale weak supervision",
            "year": 2022
        },
        {
            "authors": [
                "Ricardo Rei",
                "Ana C Farinha",
                "Chrysoula Zerva",
                "Daan van Stigt",
                "Craig Stewart",
                "Pedro Ramos",
                "Taisiya Glushkova",
                "Andr\u00e9 F.T. Martins",
                "Alon Lavie."
            ],
            "title": "Are references really needed? unbabel-IST 2021 submission for the metrics shared task",
            "venue": "Pro-",
            "year": 2021
        },
        {
            "authors": [
                "Ricardo Rei",
                "Craig Stewart",
                "Ana C Farinha",
                "Alon Lavie."
            ],
            "title": "Unbabel\u2019s participation in the WMT20 metrics shared task",
            "venue": "Proceedings of the Fifth Conference on Machine Translation, pages 911\u2013920, Online. Association for Computational Linguistics.",
            "year": 2020
        },
        {
            "authors": [
                "Christopher Richardson",
                "Sudipta Kar",
                "Anjishnu Kumar",
                "Anand Ramachandran",
                "Omar Zia Khan",
                "Zeynab Raeesy",
                "Abhinav Sethy."
            ],
            "title": "Learning to retrieve engaging follow-up queries",
            "venue": "EACL 2023.",
            "year": 2023
        },
        {
            "authors": [
                "Mario Rodr\u00edguez-Cantelar",
                "Chen Zhang",
                "Chengguang Tang",
                "Ke Shi",
                "Sarik Ghazarian",
                "Jo\u00e3o Sedoc",
                "Luis Fernando D\u2019Haro",
                "Alexander Rudnicky"
            ],
            "title": "Overview of robust and multilingual automatic evaluation metrics for open-domain dialogue systems",
            "year": 2023
        },
        {
            "authors": [
                "Robert Rosenman",
                "Vidhura Tennekoon",
                "Laura G. Hill."
            ],
            "title": "Measuring bias in self-reported data",
            "venue": "International Journal of Behavioural and Healthcare Research, 2(4):320\u2013332.",
            "year": 2011
        },
        {
            "authors": [
                "Abigail See",
                "Stephen Roller",
                "Douwe Kiela",
                "Jason Weston."
            ],
            "title": "What makes a good conversation? how controllable attributes affect human judgments",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computa-",
            "year": 2019
        },
        {
            "authors": [
                "Xinyue Shen",
                "Zeyuan Chen",
                "Michael Backes",
                "Yang Zhang"
            ],
            "title": "In chatgpt we trust? measuring and characterizing the reliability of chatgpt",
            "year": 2023
        },
        {
            "authors": [
                "Katherine Stasaski",
                "Marti A. Hearst"
            ],
            "title": "Pragmatically appropriate diversity for dialogue evaluation",
            "year": 2023
        },
        {
            "authors": [
                "Ekaterina Svikhnushina",
                "Pearl Pu"
            ],
            "title": "Approximating human evaluation of social chatbots with prompting",
            "year": 2023
        },
        {
            "authors": [
                "Marilyn A. Walker",
                "Diane J. Litman",
                "Candace A. Kamm",
                "Alicia Abella."
            ],
            "title": "PARADISE: A framework for evaluating spoken dialogue agents",
            "venue": "35th Annual Meeting of the Association for Computational Linguistics and 8th Conference of the Eu-",
            "year": 1997
        },
        {
            "authors": [
                "Jiaan Wang",
                "Yunlong Liang",
                "Fandong Meng",
                "Zengkui Sun",
                "Haoxiang Shi",
                "Zhixu Li",
                "Jinan Xu",
                "Jianfeng Qu",
                "Jie Zhou"
            ],
            "title": "Is chatgpt a good nlg evaluator? a preliminary study",
            "year": 2023
        },
        {
            "authors": [
                "Yida Wang",
                "Pei Ke",
                "Yinhe Zheng",
                "Kaili Huang",
                "Yong Jiang",
                "Xiaoyan Zhu",
                "Minlie Huang."
            ],
            "title": "A large-scale chinese short-text conversation dataset",
            "venue": "Natural Language Processing and Chinese Computing: 9th CCF International Conference, NLPCC",
            "year": 2020
        },
        {
            "authors": [
                "Guangxuan Xu",
                "Ruibo Liu",
                "Fabrice Harel-Canada",
                "Nischal Reddy Chandra",
                "Nanyun Peng."
            ],
            "title": "EnDex: Evaluation of dialogue engagingness at scale",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2022, pages 4884\u20134893, Abu",
            "year": 2022
        },
        {
            "authors": [
                "Zhou Yu",
                "Leah Nicolich-Henkin",
                "Alan W Black",
                "Alexander Rudnicky."
            ],
            "title": "A Wizard-of-Oz study on a non-task-oriented dialog systems that reacts to user engagement",
            "venue": "Proceedings of the 17th Annual Meeting of the Special Interest Group on Discourse",
            "year": 2016
        },
        {
            "authors": [
                "Weizhe Yuan",
                "Graham Neubig",
                "Pengfei Liu."
            ],
            "title": "Bartscore: Evaluating generated text as text generation",
            "venue": "Advances in Neural Information Processing Systems, volume 34, pages 27263\u201327277. Curran Associates, Inc.",
            "year": 2021
        },
        {
            "authors": [
                "Chen Zhang",
                "Yiming Chen",
                "Luis Fernando D\u2019Haro",
                "Yan Zhang",
                "Thomas Friedrichs",
                "Grandee Lee",
                "Haizhou Li"
            ],
            "title": "DynaEval: Unifying turn and dialogue level evaluation",
            "venue": "In Proceedings of the 59th Annual Meeting of the Association",
            "year": 2021
        },
        {
            "authors": [
                "Chen Zhang",
                "Luis Fernando D\u2019Haro",
                "Qiquan Zhang",
                "Thomas Friedrichs",
                "Haizhou Li"
            ],
            "title": "2022a. FineDeval: Fine-grained automatic dialogue-level evaluation",
            "venue": "In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2022
        },
        {
            "authors": [
                "Qingyu Zhang",
                "Xiaoyu Shen",
                "Ernie Chang",
                "Jidong Ge",
                "Pengke Chen"
            ],
            "title": "2022b. Mdia: A benchmark for multilingual dialogue generation",
            "year": 2022
        },
        {
            "authors": [
                "Saizheng Zhang",
                "Emily Dinan",
                "Jack Urbanek",
                "Arthur Szlam",
                "Douwe Kiela",
                "Jason Weston"
            ],
            "title": "Personalizing dialogue agents: I have a dog, do you have pets too",
            "venue": "In Proceedings of the 56th Annual Meeting of the Association for Computational Lin-",
            "year": 2018
        },
        {
            "authors": [
                "Tianyi Zhang",
                "Varsha Kishore",
                "Felix Wu",
                "Kilian Q. Weinberger",
                "Yoav Artzi."
            ],
            "title": "Bertscore: Evaluating text generation with bert",
            "venue": "International Conference on Learning Representations.",
            "year": 2020
        },
        {
            "authors": [
                "Tiancheng Zhao",
                "Ran Zhao",
                "Maxine Eskenazi."
            ],
            "title": "Learning discourse-level diversity for neural dialog models using conditional variational autoencoders",
            "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume",
            "year": 2017
        },
        {
            "authors": [
                "Ming Zhong",
                "Yang Liu",
                "Da Yin",
                "Yuning Mao",
                "Yizhu Jiao",
                "Pengfei Liu",
                "Chenguang Zhu",
                "Heng Ji",
                "Jiawei Han."
            ],
            "title": "Towards a unified multidimensional evaluator for text generation",
            "venue": "Proceedings of the 2022 Conference on Empirical Meth-",
            "year": 2022
        },
        {
            "authors": [
                "Hao Zhou",
                "Chujie Zheng",
                "Kaili Huang",
                "Minlie Huang",
                "Xiaoyan Zhu."
            ],
            "title": "KdConv: A Chinese multi-domain dialogue dataset towards multi-turn knowledge-driven conversation",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Com-",
            "year": 2020
        },
        {
            "authors": [
                "Pei Zhou",
                "Hyundong Cho",
                "Pegah Jandaghi",
                "Dong-Ho Lee",
                "Bill Yuchen Lin",
                "Jay Pujara",
                "Xiang Ren."
            ],
            "title": "Reflect, not reflex: Inference-based common ground improves dialogue response quality",
            "venue": "Proceedings of the 2022 Conference on Empirical Meth-",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Dialogue systems are becoming more popular, but their quality is usually evaluated in terms of metrics such as fluency, coherence, or sensibility. Recent advancements in large language model-based dialogue systems enable high levels of proficiency, thus shifting the emphasis from fluency evaluation to more nuanced aspects such as engagingness, which has emerged as an important quality of dialogue systems (Yu et al., 2016; Zhou et al., 2022;\n1Code available at https://github.com/PortNLP/MEEP\nCohen et al., 2022). Development of models that produce more engaging responses can be supported by automatic metrics to complement human annotation.\nDespite the ground-breaking work of existing metrics for engagingness, they evaluate the response without the conversation history (Xu et al., 2022), or are designed for a specific dataset (Liu et al., 2023), and are not highly correlated with human annotations (Ghazarian et al., 2020). Although multi-dimensional automatic metrics are desirable, they have limited success with complex qualities like engagement (Mehri and Eskenazi, 2020a; Deng et al., 2021; Zhang et al., 2022a). Enthusiasm for multi-dimensional evaluation is balanced by calls to develop metrics that measure specific dialogue qualities in order to complement existing metrics (Gehrmann et al., 2022; Mehri et al., 2022; Colombo et al., 2022).\nFurthermore, dialogue systems are increasingly available in languages beyond English so it is important to be able to test systems in these languages (Rodr\u00edguez-Cantelar et al., 2023). With the rise of virtual assistants, the importance of engaging dialogue (Richardson et al., 2023) and multilingualism has increased significantly, as they need to actively assist users, facilitate knowledge exploration, and foster effective communication, making engagingness a vital parameter in evaluating dialogues. Our research therefore develops an automatic metric for engagingness that can evaluate dialogue responses in multiple languages.\nWe establish a comprehensive definition of engagingness in conversation and construct an extensive range of prompts designed with our dimensions of engagingness and prompt engineering techniques (see Figure 1). These prompts are employed across several LLMs, which show impressive capabilities in many tasks and have had limited exploration into their use for automatic evaluation of dialogue.\nTo our knowledge, this is the first work to extensively test prompt design for the dedicated evaluation of engagingness in dialogue in a multilingual setting. We test the prompts first on Englishonly datasets, and select the best-performing set of prompting methods to test against four strong baseline metrics on Spanish-language and simplified Chinese-language datasets. We also test translated versions of these prompts. We find that our proposed framework \u2013 MEEP: Metric for Engagingness Evaluation using Prompting \u2013 can be effective for evaluation of enganginess in dialogue in multiple languages with the broader implication that LLMs can be used for evaluation of complex qualities of dialogue in multiple languages through prompting alone.\nOur contributions are as follows:\n\u2022 A thorough definition of engagingness using five dialogue engagement dimensions informed by previous engagingness research and a linguistics-based approach. It frames the goals of engagingness in conversation in terms of human-aligned dimensions independent of existing datasets and provides a more nuanced target for the development of engaging dialogue systems.\n\u2022 A novel method for measuring engagingness in dialogue using a set of prompt engineering approaches that improve LLM performance.\n\u2022 An extensive evaluation across ten datasets and three languages (English, Spanish, and Chinese)."
        },
        {
            "heading": "2 Related Work",
            "text": "Automatic Dialogue Metrics From the development of PARADISE, the earliest automatic metric to evaluate dialogue (Walker et al., 1997), reference-based automatic metrics such as BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), and BERTSCORE (Zhang et al., 2020) have been used extensively for evaluation of dialogue but have weak correlation with human judgments (Liu et al., 2016; Novikova et al., 2017), and do not suit the multiplicity of valid responses possible in any conversation (Zhao et al., 2017; Mehri et al., 2022).\nAutomatic Metrics for Engagingness in Dialogue Ghazarian et al. (2020) use engagingness as a measure of dialogue system performance in the first attempt to learn engagingness scores at the utterance level from annotated data. This BERT-based metric achieves modest correlations with human annotated scores, and leaves room for future improvement. A recent effort at creating a more robust automatic metric for engagingness is ENDEX (Xu et al., 2022). This method uses several features of a Reddit-based dataset to learn engagingness scores and evaluate the responses in a turn-level dataset without considering the dialogue context. Some multi-dimensional dialogue metrics include engagingness as one of the dimensions measured (Mehri and Eskenazi, 2020a; Zhang et al., 2021), however, the engagingness scores for these metrics are not highly correlated with human evaluation. Other multidimensional dialogue metrics use results from human-directed questions about interestingness to evaluate engagingness for an imprecise approximation (Deng et al., 2021; Zhong et al., 2022; Liu et al., 2023).\nMultilingual Automatic Dialogue Metrics Multilingual dialogue models are commonly evaluated using similarity measures such as SacreBLEU, BERTSCORE, BLEU, and perplexity (Agarwal et al., 2021; Zhang et al., 2022b). Development of non-similarity-based automatic metrics for a multilingual context remains an open question for research. The recent DSTC11 shared task proposal for Task 4 acknowledges this lack as motivation for the task (Rodr\u00edguez-Cantelar et al., 2023).\nLarge Language Models for NLG Evaluation\nGao et al. (2023) find that for text summarization evaluation, ChatGPT provides scores in the style of human evaluation, and that adding dimension definitions leads to slight improvements in correlation with human annotations. GEMBA (Kocmi and Federmann, 2023) achieves state-of-the-art performance for machine translation evaluation using GPT-3.5 and larger. Most attempts to study the effectiveness of prompting in the context of automatic evaluation use modified versions of annotator instructions as prompts (Kocmi and Federmann, 2023; Wang et al., 2023; Luo et al., 2023; Fu et al., 2023; Gao et al., 2023). Prompt techniques improve through the use of clarifying examples (Yuan et al., 2021) or assigning LLMs roles (Shen et al., 2023). Svikhnushina and Pu (2023) use new dialogues generated by an LLM interacting with a chatbot that are then scored with the same LLM.\nLarge Language Models for Evaluation of Dialogue G-EVAL (Liu et al., 2023), evaluates summarization and dialogue response generation, using prompts that include definitions of the task and quality to be evaluated, Chain of Thought reasoning, and a scoring function. GPTSCORE (Fu et al., 2023) provides an adaptable metric using zero-shot instruction with a prompt template that includes the task specification and a definition of the aspect to be evaluated. Each has only been tested on one (English) dataset."
        },
        {
            "heading": "3 Engagingness in Dialogue",
            "text": "Engagingness is central to our conception of conversation, as demonstrated by the prevalence of the word \u201cconversation\u201d as a collocate2 of the verb \u201cengage\u201d (COCA). What then is engagingness? In prior research, several metrics conflate engagingness with interestingness (Deng et al., 2021; Zhong et al., 2022; Liu et al., 2023). Others provide no definition and the training of these metrics rely instead on the implicit judgements from the training set annotators (Logacheva et al., 2018). We posit that there is no standard definition of engagingness in NLP because of the subjectivity of human judgment. Humans, like models, cannot be relied upon for self reporting (Gao et al., 2021; Rosenman et al., 2011). We put forward a definition of engagingness that is useful for our goal: to create an evaluation tool that will improve dialogue model performance.\n2A collocate is a word that appears in conjunction (or in the same context) as the other word with greater than random frequency.\nTo ground our definition, we start with the dictionary. Dictionary definitions identify three consistent aspects of the quality of being engaging: attention, interest, and participation (Merriam-Webster, 2023; Wiktionary, 2023; Dictionary, 2023). A reply is engaging when it gets the user\u2019s attention, interests the user, and incites participation from the user. These three aspects form the core of what we expect an engagingness metric to measure.\nWe propose the following five subdimensions of engagingness:\n\u2022 Response Diversity: Engaging responses are diverse, not repetitive and generic. When a model produces a standard or generic response, it can constrain the responses the user can reasonably produce (Stasaski and Hearst, 2023), creating predictable dialogue which can limit engagingness.\n\u2022 Interactional Quality: Engaging responses encourage a response from the user. A definitionally necessary component of engagement is participation, so signs of elicited participation in the user, such as 1) the presence of any response, and 2) the presence of a high-quality response, demonstrate this aspect.\n\u2022 Interestingness: We pose interestingness as a necessary component of engagingness. Research has commonly used interestingness as a proxy for engagingness (Deng et al., 2021; Zhong et al., 2022; DSTC11 2023; Liu et al., 2023), and we see high correlation between scores for engagingness and interestingness in the FED dataset (Mehri and Eskenazi, 2020a) (see Figure 4). However, we do not believe that there is a one-to-one correspondence between interestingness and engagingness. Interestingness captures factual appeal (Mehri and Eskenazi, 2020b), while definitions of engagingness emphasize active participation. Both\ncontribute to meaningful conversations from different perspectives.\n\u2022 Contextual Specificity: Engaging responses are specific to the conversational context. This is another aspect of avoiding generic user responses. See et al. (2019) show that controlling for specificity improves the engagingness of their dialogue model.\n\u2022 Othering: Engaging responses create a sense of belonging, not one of othering (Powell and Menendian, 2022; Alexander and Andersson, 2022). Even an interesting conversation can quickly turn dull when we feel misunderstood, unheard, and looked down on. Conversations where rapport is established, on the other hand, are often enjoyable even in the absence of content. This aspect of engagingness is particularly underexplored in the research."
        },
        {
            "heading": "4 MEEP: Metric for Engagingness Evaluation using Prompting",
            "text": "We design several categories of prompts. A straightforward naive prompt acts as the basis against which we compare our more developed prompt sets. We adapt dialogue annotator directions into prompts for the human-directed prompt set and our own proposed sub-dimensions of engagingness form the third prompt set. For each prompt set, we create a version casting the LLM in a role. Table 1 provides an overview of the prompt types, and the full text of each prompt is available in Appendix A. We experiment with prompts in English as well as Spanish and Chinese.\nNaive Prompt (Naive) We use the Naive prompt as a baseline comparator from which to develop more performant prompts. Kocmi and Federmann (2023) report best performance from the least constrained prompting method, implying that this\nnaive prompt approach is valid as its own path of inquiry, and a similar prompt is used in Wang et al. (2023).\nHuman-directed Prompts (HD) Since LLMs are developed to approximate natural language and natural linguistic interaction, we theorize that an evaluation prompt styled after the same instructions that human annotators were given should lead to increased performance over the Naive prompt.\nPrompts With Our Proposed Dimensions of Engagingness (MEEP) We incorporate each of our five subdimensions of engagingness into a word or phrase in order to form the definition of engagingness within the prompt. This is demonstrated in Figure 3.\n\u201cSuch As\u201d and Role Assignment (SA, R) Yuan et al. (2021) find that even when controlling for the presence of example responses, the phrase \u201csuch as\u201d increases performance in the domain of machine translation evaluation. When added to our prompts, it is accompanied by an example response. Role assignment is theorized to increase the quality of the responses generated by dialogue models (Svikhnushina and Pu, 2023), and we explore its utility in dialogue response evaluation. We incorporate role assignment using methods specific to the model used. When possible, we make the assignment directly in the API call, using the \u201csystem\u201d option. If this method is not supported by\nthe model, the role assignment is prefixed to the prompt text.\nTranslation (\u2020) We translate a selection of our prompts from English into Spanish and Chinese, to match the language of the dialogues evaluated. Spanish text was manually translated, and Chinese translations are with GPT-4 and checked by a human translator for quality."
        },
        {
            "heading": "5 Experiments",
            "text": "We first test our prompt combinations using two LLMs on the English-language FED dataset. Five examples from these tests are randomly selected, on which we perform a qualitative analysis. We then select the best performing prompts to test over six multilingual turn-level datasets, two in each of English, Spanish, and Chinese. For these tests, we evaluate the prompts in English as well as the versions translated into the language of the dataset. The scores reported by each LLM/prompt style pair over six datasets are correlated with the humanannotated scores to evaluate the proposed methods and four strong baseline methods. A final set of experiments evaluates the performance of the highestperforming prompts on dialogue-level datasets in English and Chinese and correlates with humanannotated scores. These are compared with the strongest baseline method from turn-level testing."
        },
        {
            "heading": "5.1 Large Language Models",
            "text": "For our base model, we select text-davinci-003 (GPT-3.5) (Ouyang et al., 2022), gpt-3.5-turbo-0301 (ChatGPT), and gpt-3.5-turbo-0613 (ChatGPT0613) from the GPT-series of LLMs3 (Radford et al., 2022). We set the temperature to 0, top_p to 1, and n to 1. Presence penalty and frequency penalty are set to 0, logit bias to null, and we request the best of 1. We also use LLaMA2 (Touvron et al., 2023) in the 7B size with the chat_completion function with max_seq_len set to 1024, max_gen_len set to None, temperature set to 0, max_batch_size set to 1, and top_p set to 1. We do not set a max length for generation because the responses are long and the location of scores within the returned text is unpredictable.\n3We note that ChatGPT is available at the time of writing as gpt-3.5-turbo in the free version, and gpt-4 in the paid version. We take the liberty of using ChatGPT as the shorthand for gpt-3.5-turbo-0301, the free version at the time of testing."
        },
        {
            "heading": "5.2 Benchmark Datasets",
            "text": "We use four existing datasets, all from the 11th Dialogue System Technology Challenge, Track 44 (DSTC11 2023). Challenge organizers machine translated two English turn-level datasets into Spanish and Chinese, and two Chinese dialoguelevel datasets into English to produce a total of ten datasets spanning three languages. Annotations from the original datasets are used to test the datasets in the additional languages.\nFED The Fine-grained Evaluation of Dialogue (FED) dataset (Mehri and Eskenazi, 2020a) provides annotations of English-language dialogues first collected by Adiwardana et al. (2020) by way of multi-dimensional evaluations per dialogue turn, including engagingness as one of the evaluated dimensions.\nSEE The Persona-See (SEE) dataset (See et al., 2019) expands the PersonaChat (Zhang et al., 2018) task to provide human annotations at the turn-level, evaluating multiple dimensions of conversation per dialogue. They consider enjoyment to be an equivalent measure of engagement, relying on users\u2019 active involvement and positive perception. We regard it as a reliable proxy for engagingness. Originally containing over 3,000 annotated dialogues, this is much larger than the other datasets, seen in Table 2. For efficiency and consistency of scale, we randomly select a subset of 300 dialogues for comparative analysis. These same 300 dialogues are used in each of English, Spanish, and Chinese.\nKDCONV The Knowledge-driven Conversation (KDCONV) dataset consists of Chinese-language dialogues between two humans who are given knowledge graphs from which to draw their responses (Zhou et al., 2020).\nLCCC The large-scale cleaned Chinese conversation dataset (LCCC) contains conversations from Chinese social media posts of users heuristically identified to be human (Wang et al., 2020). These dialogues are combined with those from publicly available databases and cleaned.\nDialogues for KDCONV and LCCC were manually annotated at the dialogue-level for engagingness and machine translated into English for the DSTC11 Track 4 Challenge.\nTranslation Quality Analysis We briefly look at dataset translation quality to assess the validity of\n4https://chateval.org/dstc11\nthe annotations as an accurate measure of engagingness for translated datasets. This informs analysis of our test results in Section 6.2. Translations into Spanish and Chinese were obtained by the DSTC11 organizers with the MS Azure service5 and Tencent Machine Translation service6, respectively.\nThe datasets include four measures of translation quality for every utterance: two quality estimator metrics from COMET versions from 2020 (Rei et al., 2020) and 2021 (Rei et al., 2021), and two cosine similarity measures from embeddings generated from the original utterance and translation. Two methods of embedding generation were used. Details are available in Appendix B. We average the provided translation quality scores across all utterances for each translated dataset and list them in Table 2. The Spanish-language datasets score consistently higher than the Chinese-language datasets."
        },
        {
            "heading": "5.3 Baselines",
            "text": "The results of our proposed method are compared against four recent baselines.\nENDEX evaluates the engagingness of dialogues based on human reactions (Xu et al., 2022). This approach uses fine-tuned RoBERTa-large models on datasets evaluated at the turn level, measuring dimensions of engagement including attentional, behavioral, emotional, and task-based aspects of replies. Each response is given a binary score for engagingness. Due to the inherent constraints of ENDEX, we encountered difficulties in adapting it to languages other than English.\nUNIEVAL employs T5-Large as the underlying model to assess any dialogue dimension, including those that are not seen during training (Zhong\n5https://azure.microsoft.com/en-us/products/ cognitive-services/translator/\n6https://www.tencentcloud.com/products/tmt\net al., 2022). It accomplishes this by formulating single evaluation dimensions as boolean questionanswering (QA) tasks. In evaluating the engagingness of a response, UNIEVAL takes three parameters: the conversational context, an additional context (such as an intriguing fact), and the response itself. To facilitate testing with the FED dataset which does not include an additional fact, an empty string is used as the additional context. We use UNIEVAL in a multilingual context where it had not been previously evaluated.\nGPTSCORE evaluates dialogue response generation at the turn-level across eight dimensions, including engagingness (Fu et al., 2023). Their prompt for evaluation of engagingness is included in Appendix A. Our approximation of their methodology is adapted from their code for text summarization evaluation. A prompt that includes task definition, identification of the quality to be evaluated (engagingness), and the dialogue, followed by \u201cAnswer: Yes.\u201d, is passed to the LLM with temperature=0, max_tokens=0, logprobs=0, echo=True, and n=0. The log probabilities returned for the token \u2019Yes\u2019 become the score for engagingness.\nG-EVAL is an LLM-based evaluator comprising a prompt, Chain-of-Thought reasoning, and a scoring function (Liu et al., 2023). It achieves strong correlations with human evaluations using GPT-3.5 and GPT-4 models in NLG tasks, including the assessment of engagingness in dialogues. We use their GPT-3.5 model, which shows higher correlations, and modify the prompt for testing datasets without additional context, as seen in Appendix A."
        },
        {
            "heading": "6 Results and Analysis",
            "text": ""
        },
        {
            "heading": "6.1 Effect of Prompt Styles",
            "text": "Effect of MEEP Average correlations of system output and human annotations for each prompt are shown in Table 3. Prompts including MEEP generally give higher correlation than experiments not including MEEP in the prompts. The method with lowest correlation is the human-directed prompt, with average correlations of 0.420 (S) and 0.429 (P). The MEEP-based prompts have average correlations of 0.492 (S) and 0.481 (P). This indicates that including our fine-grained subdimensions of engagingness in prompting for evaluation of engagingness is an important component of an effective evaluation method.\nEffect of \u2018such as\u2019 As seen in Table 3, prompts with our engagingness dimensions (MEEP) and the use of \u2018such as\u2019 have the highest and second highest correlations, in 5 of 8 measures. Prompts including \u2018such as\u2019 have average correlations of 0.506 (S) and 0.492 (P). Their performance dominates both with and without defining the system role.\nEffect of system role In Table 3, we see that the use of system role has mixed impact. Performance for ChatGPT and ChatGPT0613 improves with system role in most cases, while performance varies for LLaMA2-7B and generally falls for GPT-3.5. Defining the system role gives the most consistent gains in performance when used with ChatGPT on prompts that do not include MEEP. The inconsistent improvements with the use of system role may be because it offers a contextual shift that is not as closely aligned as other methods, like MEEP. This more approximate alignment at times moves the model to a less-aligned position than it would be if using a better aligned method alone. GPT-3.5\nalmost entirely performs worse with the prepended system role, indicating that the system role prompt is not beneficial when used with GPT-3.5 in this setting. This combined with mixed performance when used on LLaMA2-7B suggests that it may add a level of complexity to which the simpler models cannot rise.\nComparing Models Considering that ChatGPT is an interactive LLM specifically trained on conversational data, we are curious to see whether it brings additional gains to our task. In Table 3, we see that ChatGPT has an average of 0.023 (S) and 0.012 (P) higher correlations than GPT-3.5 across all English-language turn-level experiments. ChatGPT0613 has similar though slightly lower aggregate performance to that of ChatGPT. LLaMA has 0.151 (S) and 0.302 (P) lower average performance than GPT3.5, considering statistically significant results only. GPT-3.5 has better performance than ChatGPT and ChatGPT0613 for the simplest prompts, when used without system role assignment, otherwise ChatGPT is the model that is most aligned with human annotations."
        },
        {
            "heading": "6.2 Performance in a Multilingual Setting",
            "text": ""
        },
        {
            "heading": "6.2.1 Turn-level Evaluation",
            "text": "Table 4 reports Spearman coefficients from experiments on multilingual turn-level datasets with prompts in English, Spanish, and Chinese (Pearson coefficients are available in Appendix C). Our highest-performing English prompts have an average of 0.064 higher correlation than the highest-performing baseline metric (G-EVAL or GPTSCORE) for that dataset. Average increases of the highest-correlated English prompt over the highest-correlated baseline are 0.060, 0.082, and 0.050 for the English, Spanish, and Chineselanguage datasets, respectively.\nResults indicate that correlation using our method is highest for English-language dialogues, followed by Spanish-language dialogues, with Chinese-language dialogues having the lowest correlations. We would expect to see comparable performance because they are all high-resource languages. The unexpected results may be related to translation quality of the datasets, which is lower for the Chinese datasets as seen in Table 2. A comparison of results for the Spanish and Chinese datasets in Figure 4 shows that translating a prompt into the language of the dataset does not consistently improve correlation, but our best scores for the Spanish and Chinese datasets are nevertheless seen with translated prompts. The ChatGPT0613 model provides several of these highest correlations, indicating that improved system role capabilities with this model may have included multilingual system role training."
        },
        {
            "heading": "6.2.2 Dialogue-level Evaluation",
            "text": "Results for dialogue-level evaluation are in Table 5. Achieving statistically significant results is less consistent on the dialogue-level than on the turnlevel datasets, especially for the translated Englishlanguage versions. Despite this, we can see better performance with our prompts than with the baseline.\nQualitative Analysis An example from qualitative analysis is provided in Table 6, with more examples in Appendix D. The randomly-selected examples show mixed consistency with general trends observed with testing. We note that MEEP prompts improve performance over aggregate results from HD and Naive prompts, although exceptions can be found. The effect of the system role is difficult to discern at this scale. ChatGPT performs better or similarly to GPT-3.5 in four out of five examples, holding with the pattern exhibited in the data.\nWe see a pattern of GPT-3.5 more often giving scores that are higher than the ground truth annotations. When we average the scores for the topperforming models, GPT-3.5 and ChatGPT, across all the model/prompt combinations used in Table 3, we find that GPT-3.5 produces scores appreciably higher than ChatGPT, with averages of 0.8138 and\n0.7307, respectively. This indicates that GPT-3.5 has a positive bias in evaluation when compared to ChatGPT, which may reveal underlying limitations in the model\u2019s capability."
        },
        {
            "heading": "7 Discussion",
            "text": "Insights from our experiments showcase the power of our proposed LLM-based evaluator as follows:\n\u2022 Our MEEP dimensions of engagingness improve alignment with human annotations and are an effective component of an LLM-based metric, improving over state-of-the-art metrics in evaluation of engagingness.\n\u2022 Clear improvement is seen with the use of such as with clarifying examples in this context and we conclude that examples written in this form will improve dialogue evaluation performance with these LLMs.\n\u2022 Defining the system role offers improvement when used with ChatGPT or ChatGPT0613 and prompts that are naive. It does not improve performance when used with LLaMA27B, or GPT-3.5.\n\u2022 ChatGPT performs better than GPT-3.5 with more complex prompts. When prompts are in their simplest form, GPT-3.5 has higher correlation with human annotations. LLaMA gives highest correlations when used with the\nnaive prompt. We infer that to see an increase with the more powerful model, the context must be appropriately set, as with MEEP+SA, and that simpler prompts are more appropriate for smaller LLMs.\n\u2022 The results for our MEEP prompts used on multilingual datasets show improvement over state-of-the-art baselines in the evaluation of engagingness in dialogue across Chinese, Spanish, and English. This is also consistent across turn-level and dialogue-level datasets.\n\u2022 In the multilingual setting, for turn-level dialogues, automatic evaluation is most strongly correlated with human evaluation when the prompt is in the language of the dialogues."
        },
        {
            "heading": "8 Conclusion and Future Work",
            "text": "We propose a novel method for estimating engagingness in dialogue using LLMs as automatic evaluators. Our method \u2013 MEEP \u2013 outperforms stateof-the-art baselines, and when used in prompts containing a \u2018such as\u2019 phrase with examples, leads to better correlation with human annotated scores. This performance is demonstrated in a multilingual context, using prompts and dialogues in English, Spanish, and Chinese and for both turn-level and dialogue-level evaluation. Our findings indicate that there is promise in the evaluation of other complex dialogue qualities \u2013 such as a model\u2019s ability to provide emotional support \u2013 through a similar use of prompting with LLMs. We see opportunities for future work in the development and use of non-translated datasets in languages other than English, with annotations for a well-defined measure of engagingness. In the future, we would like to continue to explore lighter models like LLaMA (Touvron et al., 2023) or ORCA (Mukherjee et al., 2023) with our method for a more energy-efficient approach (Appendix E).\nEthical Considerations\nThe use of LLM-based metrics in evaluating language models raises concerns regarding potential bias (Gao and Wan, 2022) and self-reinforcement (Liu et al., 2023). Language models like GPTs are trained on large datasets, which may contain biases and inaccuracies that can impact evaluation tasks. This is particularly important in the context of self-AI feedback (Fernandes et al., 2023), where LLM-based metrics may prefer LLM-generated\ntexts, potentially reinforcing biases. More specifically, the dialogues and annotations are produced by crowd workers with unknown backgrounds. Ideally, they would represent a wide range of ethnic, racial, and economic backgrounds, with varied dialects. We are not aware of the composition of the workers selected. Since their annotations become our ground-truth, there is a possibility that we base our evaluation of engagingness on that of a population that does not fully represent world-wide diversity. Differences in dialect, tone, or wordiness can be interpreted uniquely depending on cultural or individual preferences. By potentially limiting our definition of engagingness to what is seen in these datasets, we admit the possibility of training dialogue systems that are engaging to a limited population, limiting the accessibility of the tools that use these systems.\nWhile more engaging dialogue systems have promising applications in domains like virtual assistants and medical support, the use of evaluation metrics beyond evaluation can lead to unintended consequences. Additionally, ethical issues arise due to the lack of transparency of AI models, as well as privacy and data security concerns when handling sensitive information. It is crucial to be aware of these considerations and prioritize the ethical and responsible use of LLMs and evaluation metrics.\nLimitations\nOur research on engagingness in conversations is limited to existing datasets available for this specific quality. We see promise in the creation of a dataset with intentionally diverse perspectives on engagingness for the development of evaluation metrics that can represent the plurality of user backgrounds.\nOur evaluation of prompt styles is not exhaustive, as the possibilities are vast. We limit our prompt styles to those found to be useful in existing research, or with a strong theoretical basis to support them. We leave for further exploration the evaluation of a wider range of prompt styles.\nIn Spanish- and Chinese-language dialogue evaluation, our findings are limited to the evaluation of translations from English. For a more robust multilingual evaluation, we would use datasets created in each language and evaluated by speakers of those languages.\nOur experiments with LLaMA use only the\nsmallest version due to limited resources. The lack of transparency in AI models presents a challenge as it hampers a comprehensive understanding of the assessment process. These limitations highlight the importance of further exploration, diverse datasets, and increased transparency to strengthen the validity and applicability of our research findings on engagingness in conversations."
        },
        {
            "heading": "Acknowledgements",
            "text": "We thank the anonymous reviewers and the PortNLP group for their insightful comments and suggestions. We express gratitude to Yufei Tao and Russell Scheinberg for assistance with translation, and to Aekta Shah, PhD for suggesting othering and belonging as a possible impact on engagingness. This research was supported by the National Science Foundation under Grant No. CRII:RI2246174."
        },
        {
            "heading": "A Full Text of Prompts",
            "text": ""
        },
        {
            "heading": "B Translation Quality Metrics",
            "text": "COMET-20 and COMET-21 are quality estimator metrics from the COMET models wmt20-comet-qe-da-v2 (Rei et al., 2020) and wmt21-comet-qe-mqm (Rei et al., 2021). CosSim1 and CosSim2 measure cosine similarity of the original utterance and the translation after generating embeddings for both with the SentenceTransformer library 7. They use the multilingual models distiluse-base-multilingual-cased-v1, and paraphrase-xlm-r-multilingual-v1 respectively.\n7https://www.sbert.net/"
        },
        {
            "heading": "C Results for Multilingual Datasets - Pearson Coefficients",
            "text": ""
        },
        {
            "heading": "D Select Examples from English-language FED dataset",
            "text": ""
        },
        {
            "heading": "E Carbon Emissions",
            "text": "Researchers are actively considering environmental implications and making efforts to address and reduce the effects associated with the deployment of large-scale NLP models. CodeCarbon.io is a dedicated emission tracker library designed to quantify carbon emissions accurately. NLP techniques always vary in accuracy and generalizability depending upon hardware variations. We addressed this by accounting for our hardware specification and recorded reliable emissions estimations and programmatic energy usage readings from CodeCarbon. The total energy consumed (E) is determined using the following formula:\nE(kWh) = 1.103 \u2217 codecarbon kWh\nWith our CO2 emission results, we converted our emissions at the time of submission to humanunderstandable emission parameters like \u201cmiles driven by an average gasoline-powered passenger vehicle\u201d using EPA Greenhouse Gas Equivalencies Calculator and found that our total emission for our core research phase was about 0.16 miles \u201cdriven by an average gasoline-powered passenger vehicle\u201d. This does not include energy usage by OpenAI to service our API calls. It is also noteworthy that our GPU energy consumption is 0.854 kWh which is comparable to 0.0001 \u201cbarrels of oil consumed\u201d, whereas a full masked language model training cost 1200 times higher. We also analyze that energy usage and efficiency are essentially a function of running time, assuming the same hardware."
        }
    ],
    "title": "MEEP: Is this Engaging? Prompting Large Language Models for Dialogue Evaluation in Multilingual Settings",
    "year": 2023
}