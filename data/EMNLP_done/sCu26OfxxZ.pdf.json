{
    "abstractText": "In this paper, we propose a novel negotiation dialogue agent designed for the online marketplace. Our agent is integrative in nature i.e, it possesses the capability to negotiate on price as well as other factors, such as the addition or removal of items from a deal bundle, thereby offering a more flexible and comprehensive negotiation experience. We create a new dataset called Integrative Negotiation Dataset (IND) to enable this functionality. For this dataset creation, we introduce a new semi-automated data creation method, which combines defining negotiation intents, actions, and intent-action simulation between users and the agent to generate potential dialogue flows. Finally, the prompting of GPT-J, a state-of-the-art language model, is done to generate dialogues for a given intent, with a human-in-the-loop process for postediting and refining minor errors to ensure high data quality. We employ a set of novel rewards, specifically tailored for the negotiation task to train our Negotiation Agent, termed as the Integrative Negotiation Agent (INA). These rewards incentivize the chatbot to learn effective negotiation strategies that can adapt to various contextual requirements and price proposals. By leveraging the IND, we train our model and conduct experiments to evaluate the effectiveness of our reward-based dialogue system for negotiation. Our results demonstrate that the proposed approach and reward system significantly enhance the agent\u2019s negotiation capabilities. The INA successfully engages in integrative negotiations, displaying the ability to dynamically adjust prices and negotiate the inclusion or exclusion of items in a bundle deal1.",
    "authors": [
        {
            "affiliations": [],
            "name": "Zishan Ahmad"
        },
        {
            "affiliations": [],
            "name": "Suman Saurabh"
        },
        {
            "affiliations": [],
            "name": "Vaishakh Sreekanth Menon"
        },
        {
            "affiliations": [],
            "name": "Asif Ekbal"
        },
        {
            "affiliations": [],
            "name": "Roshni Ramnani"
        },
        {
            "affiliations": [],
            "name": "Anutosh Maitra"
        }
    ],
    "id": "SP:6342a78c99dbaf1afd596554e0ad4b64e43cb8b7",
    "references": [
        {
            "authors": [
                "Satanjeev Banerjee",
                "Alon Lavie."
            ],
            "title": "Meteor: An automatic metric for mt evaluation with improved correlation with human judgments",
            "venue": "Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summariza-",
            "year": 2005
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot",
            "year": 2020
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "arXiv preprint arXiv:1810.04805.",
            "year": 2018
        },
        {
            "authors": [
                "Peyman Faratin",
                "Carles Sierra",
                "Nick R Jennings."
            ],
            "title": "Negotiation decision functions for autonomous agents",
            "venue": "Robotics and Autonomous Systems, 24(34):159\u2013182.",
            "year": 1998
        },
        {
            "authors": [
                "Robert H Guttman",
                "Pattie Maes."
            ],
            "title": "Agentmediated integrative negotiation for retail electronic commerce",
            "venue": "International Workshop on AgentMediated Electronic Trading, pages 70\u201390. Springer.",
            "year": 1998
        },
        {
            "authors": [
                "He He",
                "Derek Chen",
                "Anusha Balakrishnan",
                "Percy Liang."
            ],
            "title": "Decoupling strategy and generation in negotiation dialogues",
            "venue": "arXiv preprint arXiv:1808.09637.",
            "year": 2018
        },
        {
            "authors": [
                "Ari Holtzman",
                "Jan Buys",
                "Li Du",
                "Maxwell Forbes",
                "Yejin Choi."
            ],
            "title": "The curious case of neural text degeneration",
            "venue": "arXiv preprint arXiv:1904.09751.",
            "year": 2019
        },
        {
            "authors": [
                "Ehsan Hosseini-Asl",
                "Bryan McCann",
                "Chien-Sheng Wu",
                "Semih Yavuz",
                "Richard Socher."
            ],
            "title": "A simple language model for task-oriented dialogue",
            "venue": "Advances in Neural Information Processing Systems, 33:20179\u2013 20191.",
            "year": 2020
        },
        {
            "authors": [
                "Klaus Krippendorff"
            ],
            "title": "Computing krippendorff\u2019s alpha-reliability",
            "year": 2011
        },
        {
            "authors": [
                "Romain Laroche",
                "Aude Genevay"
            ],
            "title": "The negotiation dialogue",
            "year": 2016
        },
        {
            "authors": [
                "Mike Lewis",
                "Denis Yarats",
                "Yann N Dauphin",
                "Devi Parikh",
                "Dhruv Batra."
            ],
            "title": "Deal or no deal? end-to-end learning for negotiation dialogues",
            "venue": "arXiv preprint arXiv:1706.05125.",
            "year": 2017
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter."
            ],
            "title": "Decoupled weight decay regularization",
            "venue": "arXiv preprint arXiv:1711.05101.",
            "year": 2017
        },
        {
            "authors": [
                "David Premack",
                "Guy Woodruff"
            ],
            "title": "Does the chimpanzee have a theory of mind? Behavioral and brain",
            "year": 1978
        },
        {
            "authors": [
                "Alec Radford",
                "Jeffrey Wu",
                "Rewon Child",
                "David Luan",
                "Dario Amodei",
                "Ilya Sutskever"
            ],
            "title": "Language models are unsupervised multitask learners",
            "venue": "OpenAI blog,",
            "year": 2019
        },
        {
            "authors": [
                "John Schulman",
                "Filip Wolski",
                "Prafulla Dhariwal",
                "Alec Radford",
                "Oleg Klimov."
            ],
            "title": "Proximal policy optimization algorithms",
            "venue": "arXiv preprint arXiv:1707.06347.",
            "year": 2017
        },
        {
            "authors": [
                "Leigh L Thompson",
                "Jiunwen Wang",
                "Brian C Gunia."
            ],
            "title": "Negotiation",
            "venue": "Annual review of psychology, 61:491\u2013515.",
            "year": 2010
        },
        {
            "authors": [
                "Ben Wang",
                "Aran Komatsuzaki"
            ],
            "title": "Gpt-j-6b: A 6 billion parameter autoregressive language model",
            "year": 2021
        },
        {
            "authors": [
                "Xiaoyang Wang",
                "Chen Li",
                "Jianqiao Zhao",
                "Dong Yu."
            ],
            "title": "Naturalconv: A chinese dialogue dataset towards multi-turn topic-driven conversation",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 14006\u201314014.",
            "year": 2021
        },
        {
            "authors": [
                "Qingyang Wu",
                "Yichi Zhang",
                "Yu Li",
                "Zhou Yu."
            ],
            "title": "Alternating recurrent dialog model with large-scale pre-trained language models",
            "venue": "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume,",
            "year": 2021
        },
        {
            "authors": [
                "Runzhe Yang",
                "Jingxiao Chen",
                "Karthik Narasimhan."
            ],
            "title": "Improving dialog systems for negotiation with personality modeling",
            "venue": "arXiv preprint arXiv:2010.09954.",
            "year": 2020
        },
        {
            "authors": [
                "Haolan Zhan",
                "Yufei Wang",
                "Tao Feng",
                "Yuncheng Hua",
                "Suraj Sharma",
                "Zhuang Li",
                "Lizhen Qu",
                "Gholamreza Haffari."
            ],
            "title": "Let\u2019s negotiate! a survey of negotiation dialogue systems",
            "venue": "arXiv preprint arXiv:2212.09072.",
            "year": 2022
        },
        {
            "authors": [
                "Tianyi Zhang",
                "Varsha Kishore",
                "Felix Wu",
                "Kilian Q Weinberger",
                "Yoav Artzi."
            ],
            "title": "Bertscore: Evaluating text generation with bert",
            "venue": "arXiv preprint arXiv:1904.09675.",
            "year": 2019
        },
        {
            "authors": [
                "Ran Zhao",
                "Oscar J Romero",
                "Alex Rudnicky."
            ],
            "title": "Sogo: a social intelligent negotiation dialogue system",
            "venue": "Proceedings of the 18th International Conference on intelligent virtual agents, pages 239\u2013246.",
            "year": 2018
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "In an online marketplace, customers and sellers engage in discussions involving product inquiry and bargaining before reaching a common consensus\n1Codes and dataset available at https://github. com/zishan-ai/neg and https://www.iitp.ac.in/ ~ai-nlp-ml/resources.html#INA\n(He et al., 2018). In such a setting, negotiation between the customer and the seller is a core facet of discourse that ultimately decides the profit of sale and customer satisfaction. Negotiation on the price of a product is very common, however, customers have an open-ended approach to negotiation often also involving negotiation on certain aspects related to the deal. For example, while buying a chair the customer may negotiate a deal without the cushions, or even negotiate between delivery and in-store pick-up. As a result, a dialogue system for negotiation in an online marketplace should be capable of engaging in negotiation on different aspects such as price, product, and delivery. Additionally, such a system should also be capable of responding to product inquiries with relevant and knowledge-grounded information.\nA systemic survey conducted by (Zhan et al., 2022) discussed various datasets, evaluation metrics, and methodologies in common literature. From this, it can be implied that bargain in the marketplace typically follows a \"Distributive\" strategy where each party involved aims to maximize their gain rather than mutually benefiting outcomes. This strategy follows a win-lose model, where one party can gain only if the other party loses. The CraigslistBargains dataset (He et al., 2018) is the most prominent dataset in the price bargain domain with other datasets having less than 1,000 dialogues. This dataset contains dialogues between two human agents assigned the role of customer and seller negotiating over a product on Craigslist, the strategy used in the dialogues are largely distributive in nature. In contrast to a distributive approach, an \"Integrative\" approach to negotiation aims to reach a win-win situation by understanding the other party\u2019s needs and reaching a mutually satisfying consensus. It has been shown that an integrative approach to negotiation in retail e-commerce is more effective and leads to better customer satisfaction, than distributive approaches (Guttman\nand Maes, 1998) that typically utilize agents that negotiate only on price. It is common in online marketplaces for products to have several items, such as a \"A chair and its cushion\", a negotiation agent that is capable of satisfying customers that only want select items from the product such as customers that only want a chair or customers that only want a cushion is beneficial since the agent better understands customer requirements and may lead to win-win outcomes. Hence, treating a product as a \"bundle\" of items that customers can choose is a more integrative approach than treating the product as a single entity.\nTo incorporate this integrative approach, in this paper, we propose a novel dialogue system for negotiation in the online marketplace domain, which can respond to customers\u2019 inquiries and engage in negotiation with the customer. Unlike existing systems (He et al., 2018) that primarily focus on negotiation over the price of a product, our system follows a more integrative approach wherein negotiation involves different aspects such as adding or removing products from the aforementioned \"bundle\" of products, the price of the bundle, and the delivery of the product. Datasets for negotiation such as the CraigslistBargains dataset do not explicitly model the product as a bundle of smaller items. Hence, we construct a dataset (IND) consisting of integrative negotiation dialogues where the deal is modeled as a bundle of products. To avoid complete manual data creation, we design prompts for the GPT-J model (Wang and Komatsuzaki, 2021) to generate integrative negotiation utterances. To ensure the dataset\u2019s quality, we use humans in the loop for minor edits and filtering of the generated dialogues.\nUsing the constructed dataset, we build an integrative negotiation-powered dialogue agent (INA) using a supervised learning (SL) + reinforcement learning (RL) approach. To train our system, we leverage a novel reward function and maximize it using PPO loss (Schulman et al., 2017) to ensure aspects of negotiation consistency, negotiation power, and intent consistency. As per our knowledge, this is the first attempt to build an integrative-negotiation based dialogue system. Therefore we present a pioneering effort in developing an integrative-negotiation-based dialogue system, making several key contributions. First, we introduce a new task of integrative negotiation, expanding the scope of dialogue system research. Sec-\nond, we propose an efficient approach for automatically generating data with minimal manual intervention, addressing the challenge of data scarcity in certain domains. This contribution will drive the development of more robust dialogue systems. Third, we create a unique dataset of integrative negotiation dialogues. Finally, we leverage the strengths of both supervised and reinforcement learning to construct a powerful dialogue system empowered by integrative negotiation strategies."
        },
        {
            "heading": "2 Related Work",
            "text": "Thompson et al. (2010) studied the effects of various intra-personal processes, such as mood, and interpersonal processes, such as emotion, on negotiation outcomes. They defined integrative negotiation as \"the extent to which the negotiated outcome satisfies the interests of both parties in a way that the outcome cannot be improved upon without hurting one or more of the parties involved\". They also reported that the studies on the effectiveness of computer-mediated negotiation with respect to face-to-face negotiation give mixed results. Laroche and Genevay (2016) stated the importance of user adaptation in negotiation dialogue systems by performing experiments using different policies on simulated users in a newly designed negotiation dialogue game. Zhao et al. (2018) proposes a semiautomatic negotiation wherein a dialogue manager decides the intent after which a natural language generator presents conversational strategies to a human expert that writes the final utterance. Lewis et al. (2017) prepares a dataset and proposes end-toend dialogue systems for \"multi-issue bargaining\". In this type of bargaining, two agents are presented with a set of items and asked to assign each item to one agent, each agent is also given a value function to decide the value of an item. He et al. (2018) prepares the CraiglistBargains dataset where two human agents negotiate over the price of a product listed on Craigslist, further, they decouple negotiation strategy and dialogue generation by proposing a dialogue manager to decide the intent of the next utterance and a generator that uses the intent to generate the utterance. Following this work, Yang et al. (2020) proposes a framework to integrate \"Theory of mind\" (Premack and Woodruff, 1978) for inferring personality types to enhance negotiation dialogues.\nUnlike these previous works, our proposed negotiation agent (INA) is capable of doing integrative\nnegotiation. Our agent is not only capable of negotiation with respect to the price of an item but can also modify the deal to better suit the customer\u2019s preference. Similarly, our agent can also handle the customization of a deal proposed by the customer and decide on accepting or rejecting the deal. These capabilities are currently absent in any negotiation agent."
        },
        {
            "heading": "3 Dataset Creation",
            "text": "We construct the IND dataset for the task of integrative negotiation. To save on human effort and resources , we come up with a novel mechanism based on prompting a large language model for dataset creation. We keep human annotators in the loop only for making minor edits and filtering the automatically generated dialogues to ensure the quality of the conversations. The overall process consists of creating a skeleton of dialogues by dynamically deciding the correct intent for any arbitrary conversation. Our overall dataset creation process consists of 5 steps: (i). Background Data Creation, (ii). Intent Definition, (iii). Dialogue Flow Generation, (iv). Prompting for Dialogue Generation, and (v). Data Correction."
        },
        {
            "heading": "3.1 Background Data Creation",
            "text": "Although our method can be adapted to any product negotiation, we mainly focus on a list of 10 different electronic items: (i). Air Conditioning, (ii). Television, (iii). Refrigerator, (iv). Oven, (v). Washing Machine, (vi). Printer, (vii). Smart Phone, (viii). Laptop, (ix). Tablet, and (x). Camera. Along with these products, the deal bundle consists of a set of accessories related to the product. Therefore, our background database consists of the following information, such as Product Name, Product Description, Product Features, Price, Accessory List, and Accessory Description."
        },
        {
            "heading": "3.2 Intent Definition",
            "text": "In order to build a robust negotiation system it is vital to define intents that can cover a diverse range of scenarios during negotiation. For an integrative negotiation agent, the scenario in the scope of the agent is not just price negotiation, but also itemlevel negotiation in the given bundle. To cover these properties, we come up with the following intents2:\n2Example utterances for each intent provided in Table 6 of the appendix.\n\u2022 Greet: The utterances with general greetings like welcome and thank you come under this category.\n\u2022 Ask: This intent is triggered when a user explicitly asks for information about an item or the ongoing negotiation.\n\u2022 Inform: The agent may use the \u2019inform\u2019 intent to share detailed information about the products or services involved in the negotiation.\n\u2022 Ask-Clarification: This intent captures the user\u2019s intention to seek further explanation or clarification regarding certain aspects of the negotiation or the overall deal according to the current negotiation state.\n\u2022 Negotiate-Price-Increase: This intent indicates that the agent is seeking to increase the pricing terms of a product or service during the negotiation process.\n\u2022 Negotiate-Price-Decrease: This intent indicates that the agent is seeking to decrease the pricing terms of a product or service during the negotiation process.\n\u2022 Negotiate-Price-NoChange: This is an intent by the agent in a negotiation system indicating the system\u2019s intention to propose or assert that the price of a product or service should remain unchanged during the negotiation process. This is ideally done by highlighting the value and fairness of the current deal.\n\u2022 Negotiate-Add-X: This intent by the agent or user refers to the intention to propose or suggest the addition of a specific item or feature to enhance the value of a product or service during the negotiation process. This may or may not lead to an increase in the price of the deal.\n\u2022 Negotiate-Remove-X: This intent by the agent or user in refers to the intention to propose or suggest the removal of a specific item or feature from the deal in the negotiation process. This may or may not lead to a decrease in the price of the deal.\n\u2022 Accept: This refers to the agent or user\u2019s intention to agree or accept a proposal, offer, or condition reached during the negotiation process.\n\u2022 Reject: This refers to the agent or user\u2019s intention to agree or reject a proposal, offer, or condition reached during the negotiation process.\nThe above intents can occur either individually or in combination with other intents (e.g.: Greet-Ask)."
        },
        {
            "heading": "3.3 Dialogue Flow Generation",
            "text": "Our dialogue flow generator module assumes that the dialogue flow (intent sequence) during negotiation can be random. However, we also put some obvious constraints on this dataset-generation process. One simple constraint is that the conversation would be initiated by the customer with a greet intent. This greet intent could be accompanied by a request for clarification or one of the \u2018negotiate\u2019 intents for the customer. The agent can respond by the inform intent or one of the agent \u2018negotiate\u2019 intents.\nFor all the deal bundles, we maintain negotiation details of the ongoing deal with the customer, which consist of: (i). Minimum Seller price, (ii). Current Seller price, (iv). Tolerance value (tol) and (iii). Current Customer price. To enforce the integrative nature of our agent, we limit only price-based negotiations to d turns after which the \u2018Negotiate-Add-X\u2019 or \u2018Negotiate-Remove-X\u2019 intents would take over. To propose a price for the next turn, we assume that a decay in price difference (increment for customer and decrement for seller) over dialogue turns. This is in line with Faratin et al. (1998) where a similar function is used to model the price negotiation between the customer and seller. Equations 1 and 2 are used\nfor the computation of the proposed price by customer (Pb) or seller (Ps) at dialogue turn t. In the equations, k is a constant to control the rate of price change from one turn to the next. If it k is larger there will higher rate of concession, at a low value the rate of concession provided by the seller is low. For our setting, we have assumed a higher k value for the seller and a lower k for the customer, considering the customer is strict with their budget.\nPst = Pbt\u22121 + (Pst\u22121 \u2212 Pbt\u22121)e\u2212kt (1)\nPbt = Pst\u22121 \u2212 (Pst\u22121 \u2212 Pbt\u22121)e\u2212kt (2) The seller will choose intent \u2018Accept\u2019 when the customer offered price is less than or equal to the amount Pst\u2212 tol \u2217Pst. The customer will choose intent \u2018Reject\u2019 when the conversation has crossed the negotiation deadline, and the seller is no more ready to lower the bundle price. The dialogue flow terminates with the acknowledgment of \u2018accept\u2019 intent or the \u2018reject\u2019 intent."
        },
        {
            "heading": "3.4 Prompting for Dialogue Generation",
            "text": "We design few-shot prompts (Brown et al., 2020)3 for each intent, with around four shots for each prompt (due to the token limit of 2,048 in GPT-J). Each shot contains three parts, a description of the task, a summary of the relevant information from the dialogue, and an utterance following the intent, all in a natural language format. The summary of the relevant information is designed considering the intent flow of the previous utterances of the dialogue. The description of the task is the sentences\n3Example prompts provided in Section B.1 of the Appendix\nin the prompt that explains the situation and the goal of the intent, for instance, the task description for the \"Acknowledge acceptance\" intent is \"A customer has agreed to purchase a product from a seller, the seller wants to thank the customer and proceed with the transaction\". The utterance following the intent is a manually designed utterance following the task description and the information summary of the shot.\nThe flow generation module creates an ordered list of intents along with relevant information for each intent, for instance, for the intent \"NegotiateAdd-X\" the item to be added is mentioned, and for \"Negotiate-Price-Decrease\" the price to be proposed is mentioned. Our algorithm uses the list created by the flow generation module to create a shot that is augmented to the prompt of the respective intent, this prompt is then passed to the GPT-J model to produce the utterance."
        },
        {
            "heading": "3.5 Data Correction",
            "text": "To ensure the quality of the automatically generated dataset, we implemented manual correction and filtration steps. We engaged three human experts who possess post-graduate qualifications and have two years of experience in the field. Their instructions were to make edits to the generated dialogues in order to ensure grounding in the provided background database, intent, action, and negotiation flow. Additionally, any utterances produced by the agent that referred to its own experiences or feel-\nings, pretending to be human, were to be rephrased or removed (to maintain authenticity). The experts were also responsible for correcting minor grammatical errors. Furthermore, they were asked to rate the fluency of each utterance on a scale of 0-2, where 0 represented non-fluency and 2 indicated complete fluency. Dialogues containing utterances rated as 0 fluency were dropped from the dataset. These measures were implemented to uphold the quality standards of the dataset."
        },
        {
            "heading": "4 Dataset Statistic",
            "text": "The statistics of the dataset created are given in Table 1. The dataset has a total of 4,163 utterances and we follow an 80:12:8 split between train, test, and validation sets. The average number of turns per dialogue in the dataset is 13 and the number of unique words in the dataset, excluding numbers is 12,219, both these metrics are comparable to the metrics in the Craigslist Bargain dataset (avg. turns:9; unique words:11,799). Following (Wang et al., 2021), to automatically measure the variability of conversations of our dataset, we compute BLEU-1 and METEOR scores between the utterances. We obtain low BLEU-1 and METEOR scores of 0.08 and 0.05, respectively, indicating high variability between the utterances in IND. We ask three human experts to rate the \u2018engagingness\u2019 and \u2018fairness\u2019 of dialogues on a scale of 1 to 3 (higher the better). The dialogues obtained an average rating of 2.17 for \u2018engagingness\u2019 and 2.26 for"
        },
        {
            "heading": "5 Methodology",
            "text": "To force a language model to negotiate with the user while following its own price goal as well as approach, we fine-tune it using a novel-designed reward function in a reinforcement learning setting. Here, first, a pre-trained language model (GPT-2-medium) is fine-tuned in a supervised setting using traditional cross-entropy loss between the ground truth and predicted utterances probability distributions. For a supervised dialogue dataset D = {d0, d1, .., dN}, where, d = {a0, u0, .., ai, ui, .., aT\u22121, uT\u22121} - a multi-turn dialogue with ui + cxti (ui - user\u2019s utterance at ith turn and cxti = {a0, u0, .., ai\u22121}) as input and ai (agent\u2019s utterance at ith turn) as output. The supervised learning dialogue model \u03c1\u03b8(d) can be expressed as:\n\u03c1\u03b8(d) = T\u22121\u220f T=0 \u03c1u(ui|u<i, a<i)\u03c1a(ui|u<=i, a<i)\n(3) where \u03c1u and \u03c1a are the user\u2019s and agent\u2019s utterances probability distributions. This trained SLDM is fine-tuned in an RL setting using the PPO loss formulated as below:\nLCLIP(\u03b8) = E\u0302[min(prr(\u03b8)A\u0302r, clip(pry(\u03b8),\n1\u2212 \u03b5, 1 + \u03b5)A\u0302r)] (4)\nwhere prr(\u03b8) = Pnew\u03b8 /Pold\u03b8 . \u03b5 and A\u0302y denote the clipping range and normalized rewards, respectively. Finally, parameters\u2019 updation is done as follows:\n\u03b8k+1 = argmax \u03b8 E s,a\u223cP\u03b8k\n[LCLIP] (5)\nHere, normalized rewards are obtained by a novel designed reward function (R) incorporating intent consistency reward (R1), price gap reward\n4The overall inter-annotator agreement using Krippendorff\u2019s alpha (Krippendorff, 2011) was found to be 0.84\n(R2), negotiation strategy reward (R3) and interactiveness (R4) in generated responses. R intuitively nudges SLDM towards these aspects by providing appropriate respective aspect penalization/reward for generated responses. For example, if the model generates intent inconsistent response then R3 will penalize the model to discourage it from generating a similar type of content. All five rewards can be written as: Intent consistency: In a negotiation system with complex intents there can often be divergence between the predicted intent and the intent of the generated utterance. To enforce this consistency, we propose Intent Consistency (IC) reward. This reward function is implemented by first training a BERT model (Devlin et al., 2018) on the training set of IND for the task of intent prediction. This task is modeled as a classification task where the input to the BERT model is an agent utterance at turn t, Uat, and the expected output is the intent of the utterance Iat. The accuracy of the trained intent classifier is 71.2%. We use the [CLS] token for computing the probability distribution of the intent classes. We sample the probability value Pit of the intent predicted i by our end-to-end SLDM dialogue model and use it as R1 (Eq. 6).\nR1 = Pit(ut) (6)\nPrice Gap Reward: The purpose of negotiation is to find a win-win solution for both the customer and the seller. The winning scenario for a seller would be as little reduction in the initially proposed price as possible. In line with this logic, we propose a Price Gap (PG) reward. This reward is simply the fraction of the initial proposed price by the agent Pai and the final selling price after negotiation Paf (Eq 7). The higher the final price the greater the reward.\nR2 = Paf Pai\n(7)\nNegotiation Strategy Reward: A successful negotiation might not always entail deal acceptance. In cases where the customer wants to go below the minimum selling price of the agent Pa\u2212min it would not be judicious for the seller to satisfy the customer. In such situations where the negotiation could result in a win-lose situation, the deal should be rejected. Hence, the success criterion of the negotiation lies in not just acceptance of the deal but also the fairness of the deal. To ensure that our negotiation succeeds only in win-win scenarios we design the Negotiation Strategy (NS) reward.\nR3 = F ( Pb \u2212 Pa\u2212min\nPa\u2212min )G(Intentf ) (8)\nG(Intentf ) =\n{ 1, Intentf = accept\n\u22121, Intentf = reject (9)\nF (x) =\n{ 0, x < 0\nex, x \u2265 0 (10)\nIn the above equations, Pb is the customer\u2019s proposed price, and Intentf \u2208 {Accept, Reject} is the final intent in the conversation used to capture the negotiation result. The reward incentivizes acceptance of a deal when the negotiated price is within the limit of a minimum price for the seller, and rejection when the negotiated price is below this minimum price. Interactiveness: To ensure interactiveness, repetitions, and conversation blackholes are penalized such that system can engage the user for a longer duration with interactive responses. To penalize the generation of similar utterances for a given intent in the dialogue we use Equation 11.\nR4 = 1\u2212\n\u2211i=m i=1 vink .v in i\n|vink ||v in i |\nm (11)\nwhere vink is the vector (bag of words) representing the generated utterance with intent in. vini to v in m are the vectors representing the previously generated utterances in the dialogue with the same intent. The final normalized reward function R is formulated as:\nR = \u03b31R1 + \u03b32R2 + \u03b33R3 + \u03b34R4 (12)\nwith \u03b31 + \u03b32 + \u03b33 + \u03b34 = 1."
        },
        {
            "heading": "6 Experiments",
            "text": ""
        },
        {
            "heading": "6.1 Evaluation Metrics",
            "text": "To properly assess INA\u2019s performance, we perform both automatic and manual evaluations. In automatic evaluation to measure the surface similarity with the gold responses, we compute METEOR (Banerjee and Lavie, 2005). For semantic similarity, we compute BERT Score (BS-F1) (Zhang et al., 2019) and Word Mover distance (WM). We also report the Perplexity (PPL) and the average response length (R-LEN) of the generated responses.\nHuman evaluations were conducted by three postgraduate evaluators who possess proficiency in similar tasks. Each evaluator interacted with the proposed system 15 times and assessed the conversations based on: (i). Negotiation Consistency (NCon): It is the measure of consistency (absence of arbitrariness) in the negotiation approach within a dialogue (ii). Bargaining Efficacy (B-Eff): It measures the ability of the negotiation system to present compelling arguments, reasoning, or incentives that influence the other party\u2019s decision-making process., (iii). Outcome fairness (O-fair): It assesses the fairness or equity of the final outcomes reached during the negotiation process., (iv). Dialoguefluency (D-F): It measures the overall grammatical correctness of the generated responses, and (v). Dialogue-Engagingness (D-E): Measures the extent to which a conversation or dialogue is interesting, captivating, and able to hold the attention of the participants. The evaluators assigned scores on a scale of 1 to 3 for each metric (The higher the better)."
        },
        {
            "heading": "7 Results and Analysis",
            "text": "Automatic Evaluation: It can be noticed from Table 2 that the proposed INA performs better than all the four baselines viz. ARDM, ARDM + BK (Background Knowledge), ARDM + In (Intent), and Neg-TOD (Hosseini-Asl et al., 2020), in terms of all the five metrics viz. METEOR, BS-F1, WM, PPL, and R-LEN. For the evaluation metrics measuring similarity (of the generated utterance) with the gold utterance i.e METEOR, BS-F1 and WM, INA attains scores of 0.43, 0.86 and 0.57, respectively. The obtained scores are significant improvements <0.141, 0.042, 0.04>, <0.158, 0.032, 0.04>, <0.144, 0.032, 0.03> and <0.137, 0.029, 0.03> over the baselines, ARDM, ARDM+BK, ARDM+In, and NegTOD, respectively.\nIt can also be inferred that the difference of BSF1, and WM scores decrease in the following order: INA>INA-NS>INA-PG>INA-I. This shows the importance of task-specific rewards in our proposed system INA.\nIt can also be observed from Table 2 that INA obtains lower (better) PPL = 1.56 score than that of ARDM, ARDM+BK, ARDM+In, and NegTod with a difference of 1.39, 1.19, 1.24, and 1.37 points, respectively. Further, we obtain a score of R-LEN = 39.93 is also better than that of ARDM, ARDM+BK, ARDM+In, and Neg-TOD with a dif-\nference of 15.72, 2.28, 13.5, and 1.76, respectively. This indicates that the INA is able to generate longer responses, hence, showcasing more engagingness with the user. It can be due to the incorporation of all four rewards where R1, R2, and R3 play the crucial role in handling negotiation and price consistency, and R4 helps in maintaining the non-repetitiveness, hence, driving the agent to build the rapport with a user as well as be on the goal by generating diverse and interactive negotiation responses.\nHuman Evaluation: Table 3 shows the human evaluation results for all the eight models viz. ARDM, ARDM+BK, ARDM+In, NegTOD, INAIC, INA-PG, INA-NS and INA-I. It may be noted that INA yields better scores for N-Con, B-Eff, O-fair, D-F, and D-E compared to the baselines. Scores of N-Con: 2.4, D-F: 2.8, and D-E: 2.6 shows that the intent-consistency (IC) and interactiveness (I) rewards play a crucial role in obtaining consistent, fluent, and engaging, responses as compared to other models. Further, in terms of B-Eff and O-fair, INA attains a score of 1.8 for both. The ablation of the price-gap (PG) and negotiationstrategy (NS) rewards showcases the importance of these rewards in terms of B-Eff and O-fair. Therefore, it can be inferred that employing intent consistency, price gap, and negotiation strategy rewards help in a more consistent, persuasive, and overall fair negotiation with the customer."
        },
        {
            "heading": "8 Conclusion",
            "text": "In this paper, we have presented a novel dialogue agent for negotiation (INA) in the online marketplace domain, focusing on an integrative approach that goes beyond price negotiations. Our system can respond to customer inquiries and engage in negotiations that encompass various aspects, such as modifying product bundles, adjusting prices, and arranging product delivery. Unlike existing systems that mainly concentrate on price negotiations, our approach provides a more comprehensive and versatile solution. To achieve this, we constructed a dataset of negotiation dialogues (IND) where the product is represented as a bundle of smaller items. To minimize manual effort in data creation, we employed prompts for the GPT-J model to generate integrative negotiation utterances. Using IND, we developed a INA through a combination of supervised learning and reinforcement learning. Our training process incorporated a novel reward function that suits the negotiation task, which we optimized using the Proximal Policy Optimization (PPO) loss. Our results show that INA is able to perform integrative negotiations with the customer and enable engaging negotiations that can lead to a win-win deal for the seller and the customer.\nIn the future, it would be interesting to explore the role of the customer persona like age, gender, hobbies, etc. during negotiation."
        },
        {
            "heading": "9 Limitations",
            "text": "Our data creation steps and modeling have some limitations. First, to create the data, GPT-J is used which requires a large GPU memory size (here, 40 GB). Another limitation of GPT-J is that it has a context window of 2,048 tokens, which constrains our prompting mechanism. Within this context window, we need to fit background data as well as dialogue history with a few shot examples. This allows us to only go for a maximum of 4 shots while prompting leading to some hallucinations in the created data which needed to be fixed manually."
        },
        {
            "heading": "10 Ethical Considerations",
            "text": "Since negotiation by nature entails bargain with the customer, it should be done ethically. Our integrative approach to negotiation gives greater flexibility to the customer and hence leads to a win-win scenario in negotiation. Our negotiation is not aimed at as a zero-sum game where a party has to lose in order for the other to win. The customer at any point of the conversation can reject the deal and thus is not compelled to continue with the negotiation if it does not suit them.\nThe dataset created in this work will be made available only after filling and signing an agreement declaring that the data will be used only for research purposes. The annotation, filtering/editing of data, and manual evaluations were done by human experts, who are regular employees of our research group and are paid in accordance with the institute\u2019s policy. There are no other issues to declare."
        },
        {
            "heading": "11 Acknowledgement",
            "text": "Authors acknowledge the grant received from Accenture LLP for the project T\u201dConversational Agents with Negotiation and Influencing ability\u201d."
        },
        {
            "heading": "A Appendix",
            "text": "A.1 Implementation Details For generating the INA corpus, GPT-J model (Wang and Komatsuzaki, 2021) with 6 billion parameters was used. INA is trained in an RL framework by employing a fine-tuned GPT-2 small (Radford et al., 2019) model (117 million parameters) on our proposed IND dataset. For dialogue flow generation, we keep the value of d as 2. In each iteration of RL-training, n = 3 candidate responses are generated. It is selected as per PPL score, after experimenting with different values i.e. n = 2, 3, 4, 5, 10. Nucleus sampling (Holtzman et al., 2019) with temperature T = 0.8 and probability p = 0.9 is used to decode the generated utterances. INA trained is done using seed_value = 10, human_reward = 10, max_candidate_length = 50 with optimizer = AdamW (Loshchilov and Hutter, 2017) and learning rate \u03b1 = 2e \u2212 05, \u03b5 = 0.2 and epochs = 17. The reward weight combination of 0.2, 0.2, 0.3, 0.2 are chosen as the final weights for \u03b31, \u03b32, \u03b33, and \u03b34 respectively.\nA.1.1 Specifications of Computational Resource\nTo train the MLE-loss-based conversational model, and proposed INA, following configurations are used:\n\u2022 GPU: A100-PCIE-40GB.\n\u2022 CUDA Support: CUDA 11.x (or later.\n\u2022 Memory clock: 1215 MHz.\n\u2022 Total board power: 250 W.\n\u2022 GPU clocks: Base: 765 MHz, Boost: 1410 MHz.\n\u2022 Memory Size: 40 GB.\n\u2022 Memory Type: HBM2.\n\u2022 Bus Width: 5120 bits."
        },
        {
            "heading": "B Dataset",
            "text": "We ensure that the utterances in INA are grounded on the background knowledge consisting of product and deal details. Table 5 shows example utterances from our dataset for different agent intents. It can clearly be observed that the utterances are wellgrounded in the background knowledge and do contain factual hallucinations.\nTable 6 shows example utterances for each intent defined in Section 3. The table also shows examples of custom intents obtained through the combination of the defined intents.\nB.1 Prompts Each prompt contains around 4 shots and each shot contains a task description, required context, and an example utterance.\nAn example shot for the intent \"Negotiate-PriceNoChange\" is as follows:\n\"A seller is negotiating with a customer for a laptop called Dell X8, it has 16 GB ram, 11-inch screen and Intel i7 processor. The seller ideally wants it for $770 and is not willing to reduce the price.\"\n\u201d\u2019The customer tells \"I appreciate that you need to make a profit on this item, but unfortunately, $770 is above my budget for a laptop. I was ideally hoping to purchase the Dell X8 for $500, but I\u2019m willing to negotiate up to $570 if necessary. Is there any way you could lower the price to meet me somewhere in the middle?\"\u201d\u2019\n\"(Remember, the seller cannot go lower than $770) The seller endorses the product by saying.\"\n\"<start> While I appreciate your offer, I cannot go as low as $570. I cannot lower the price further since the laptop is high-end and is well worth $770. It has 16 GB ram and an Intel i7 processor, making it ideal for heavy duty applications. I\u2019m sure that you would be pleased with it even for $770!\"\nHere, we provide context in terms of the previous utterance of the customer and the product description. The task mentioned is to endorse the product and not go below a particular price.\nAn example shot for the intent \"NegotiateRemove-X\" is as follows:\n\"A customer is negotiating with a seller about a product. The customer wants to ask for another deal to the seller.\"\n\"The initial deal was a laptop called Dell X8, it has 16 GB ram, 11-inch screen and Intel i7 processor along with a gaming mouse. The price for this deal was $800. The customer wants to remove the gaming mouse from the deal.\"\n\"The customer asks for the new deal by saying.\" \"<start> I do not really need the mouse, is it possible to just sell me the laptop?\" Here the task description is mentioned explicitly in 3 lines. The context is provided as the previous deal.\nC INA Generated Conversation Sample\nIn Table 7 we show one sample interaction between a human and INA. The negotiation is for a bundle deal of a TV, TV Stand, and Extension Cord. The interaction shows the capability of our dialogue agent (INA) in handling and integrative negotiation."
        }
    ],
    "title": "INA: An Integrative Approach for Enhancing Negotiation Strategies with Reward-Based Dialogue System",
    "year": 2023
}