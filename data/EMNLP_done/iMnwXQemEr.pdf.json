{
    "abstractText": "This study utilizes Independent Component Analysis (ICA) to unveil a consistent semantic structure within embeddings of words or images. Our approach extracts independent semantic components from the embeddings of a pre-trained model by leveraging anisotropic information that remains after the whitening process in Principal Component Analysis (PCA). We demonstrate that each embedding can be expressed as a composition of a few intrinsic interpretable axes and that these semantic axes remain consistent across different languages, algorithms, and modalities. The discovery of a universal semantic structure in the geometric patterns of embeddings enhances our understanding of the representations in embeddings.",
    "authors": [
        {
            "affiliations": [],
            "name": "Hiroaki Yamagiwa"
        },
        {
            "affiliations": [],
            "name": "Momose Oyama"
        },
        {
            "affiliations": [],
            "name": "Hidetoshi Shimodaira"
        }
    ],
    "id": "SP:e6fdde3ff9ee37aa571d16be2af4beb99be431ce",
    "references": [
        {
            "authors": [
                "Prince Osei Aboagye",
                "Yan Zheng",
                "Chin-Chia Michael Yeh",
                "Junpeng Wang",
                "Zhongfang Zhuang",
                "Huiyuan Chen",
                "Liang Wang",
                "Wei Zhang",
                "Jeff M. Phillips."
            ],
            "title": "Quantized wasserstein procrustes alignment of word embedding spaces",
            "venue": "Proceedings of the 15th",
            "year": 2022
        },
        {
            "authors": [
                "Saleh Albahli",
                "Awais Awan",
                "Tahira Nazir",
                "Aun Irtaza",
                "Ali Alkhalifah",
                "Waleed Albattah."
            ],
            "title": "A deep learning method dcwr with hanet for stock market prediction using news articles",
            "venue": "Complex & Intelligent Systems, 8(3):2471\u20132487.",
            "year": 2022
        },
        {
            "authors": [
                "David Alvarez-Melis",
                "Tommi S. Jaakkola."
            ],
            "title": "Gromov-wasserstein alignment of word embedding spaces",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31 - November 4, 2018,",
            "year": 2018
        },
        {
            "authors": [
                "Sanjeev Arora",
                "Yuanzhi Li",
                "Yingyu Liang",
                "Tengyu Ma",
                "Andrej Risteski."
            ],
            "title": "Linear algebraic structure of word senses, with applications to polysemy",
            "venue": "Transactions of the Association for Computational Linguistics, 6:483\u2013495.",
            "year": 2018
        },
        {
            "authors": [
                "Mikel Artetxe",
                "Gorka Labaka",
                "Eneko Agirre."
            ],
            "title": "Learning principled bilingual mappings of word embeddings while preserving monolingual invariance",
            "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, EMNLP",
            "year": 2016
        },
        {
            "authors": [
                "Mikel Artetxe",
                "Gorka Labaka",
                "Eneko Agirre."
            ],
            "title": "Learning bilingual word embeddings with (almost) no bilingual data",
            "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, ACL 2017, Vancouver, Canada, July 30 -",
            "year": 2017
        },
        {
            "authors": [
                "Mikel Artetxe",
                "Gorka Labaka",
                "Eneko Agirre."
            ],
            "title": "A robust self-learning method for fully unsupervised cross-lingual mappings of word embeddings",
            "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018,",
            "year": 2018
        },
        {
            "authors": [
                "Anthony J Bell",
                "Terrence J Sejnowski."
            ],
            "title": "The \u201cindependent components\u201d of natural scenes are edge filters",
            "venue": "Vision research, 37(23):3327\u20133338.",
            "year": 1997
        },
        {
            "authors": [
                "Peter J. Bickel",
                "Gil Kur",
                "Boaz Nadler."
            ],
            "title": "Projection pursuit in high dimensions",
            "venue": "Proceedings of the National Academy of Sciences, 115(37):9151\u20139156.",
            "year": 2018
        },
        {
            "authors": [
                "Michael W Browne."
            ],
            "title": "An overview of analytic rotation in exploratory factor analysis",
            "venue": "Multivariate behavioral research, 36(1):111\u2013150.",
            "year": 2001
        },
        {
            "authors": [
                "Elia Bruni",
                "Nam-Khanh Tran",
                "Marco Baroni."
            ],
            "title": "Multimodal distributional semantics",
            "venue": "Journal of Artificial Intelligence Research, 49:1\u201347.",
            "year": 2014
        },
        {
            "authors": [
                "Tyler Chang",
                "Zhuowen Tu",
                "Benjamin Bergen."
            ],
            "title": "The geometry of multilingual language model representations",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 119\u2013136, Abu Dhabi, United Arab Emirates.",
            "year": 2022
        },
        {
            "authors": [
                "Ciprian Chelba",
                "Tom\u00e1s Mikolov",
                "Mike Schuster",
                "Qi Ge",
                "Thorsten Brants",
                "Phillipp Koehn",
                "Tony Robinson."
            ],
            "title": "One billion word benchmark for measuring progress in statistical language modeling",
            "venue": "INTERSPEECH 2014, 15th Annual Conference of the In-",
            "year": 2014
        },
        {
            "authors": [
                "Ethan A. Chi",
                "John Hewitt",
                "Christopher D. Manning."
            ],
            "title": "Finding universal grammatical relations in multilingual BERT",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5564\u20135577, Online. Association",
            "year": 2020
        },
        {
            "authors": [
                "Charles B Crawford",
                "George A Ferguson."
            ],
            "title": "A general rotation criterion and its use in orthogonal rotation",
            "venue": "Psychometrika, 35(3):321\u2013332.",
            "year": 1970
        },
        {
            "authors": [
                "Jan Engler",
                "Sandipan Sikdar",
                "Marlene Lutz",
                "Markus Strohmaier."
            ],
            "title": "SensePOLAR: Word sense aware interpretability for pre-trained contextual word embeddings",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2022, pages 4607\u2013",
            "year": 2022
        },
        {
            "authors": [
                "Kawin Ethayarajh."
            ],
            "title": "How contextual are contextualized word representations? Comparing the geometry of BERT, ELMo, and GPT-2 embeddings",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the",
            "year": 2019
        },
        {
            "authors": [
                "Manaal Faruqui",
                "Yulia Tsvetkov",
                "Dani Yogatama",
                "Chris Dyer",
                "Noah A. Smith."
            ],
            "title": "Sparse overcomplete word vector representations",
            "venue": "Proceedings",
            "year": 2015
        },
        {
            "authors": [
                "Lev Finkelstein",
                "Evgeniy Gabrilovich",
                "Yossi Matias",
                "Ehud Rivlin",
                "Zach Solan",
                "Gadi Wolfman",
                "Eytan Ruppin."
            ],
            "title": "Placing search in context: The concept revisited",
            "venue": "ACM Transactions on information systems, 20(1):116\u2013131.",
            "year": 2002
        },
        {
            "authors": [
                "Daniela Gerz",
                "Ivan Vuli\u0107",
                "Felix Hill",
                "Roi Reichart",
                "Anna Korhonen."
            ],
            "title": "SimVerb-3500: A large-scale evaluation set of verb similarity",
            "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2173\u20132182.",
            "year": 2016
        },
        {
            "authors": [
                "Yoav Goldberg."
            ],
            "title": "Neural network methods for natural language processing",
            "venue": "Synthesis lectures on human language technologies, 10(1):1\u2013309.",
            "year": 2017
        },
        {
            "authors": [
                "Edouard Grave",
                "Piotr Bojanowski",
                "Prakhar Gupta",
                "Armand Joulin",
                "Tom\u00e1s Mikolov."
            ],
            "title": "Learning word vectors for 157 languages",
            "venue": "Proceedings of the Eleventh International Conference on Language Resources and Evaluation, LREC 2018, Miyazaki,",
            "year": 2018
        },
        {
            "authors": [
                "Edouard Grave",
                "Armand Joulin",
                "Quentin Berthet."
            ],
            "title": "Unsupervised alignment of embeddings with wasserstein procrustes",
            "venue": "The 22nd International Conference on Artificial Intelligence and Statistics, AISTATS 2019, 16-18 April 2019, Naha, Okinawa,",
            "year": 2019
        },
        {
            "authors": [
                "Lei Han",
                "Mengqi Ji",
                "Lu Fang",
                "Matthias Nie\u00dfner."
            ],
            "title": "Regnet: Learning the optimization of direct image-to-image pose registration",
            "venue": "CoRR, abs/1812.10212.",
            "year": 2018
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun."
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016, pages 770\u2013778. IEEE",
            "year": 2016
        },
        {
            "authors": [
                "Felix Hill",
                "Roi Reichart",
                "Anna Korhonen."
            ],
            "title": "Simlex-999: Evaluating semantic models with (genuine) similarity estimation",
            "venue": "Computational Linguistics, 41(4):665\u2013695.",
            "year": 2015
        },
        {
            "authors": [
                "Timo Honkela",
                "Aapo Hyv\u00e4rinen",
                "Jaakko J V\u00e4yrynen."
            ],
            "title": "Wordica\u2014emergence of linguistic representations for words by independent component analysis",
            "venue": "Natural Language Engineering, 16(3):277\u2013 308.",
            "year": 2010
        },
        {
            "authors": [
                "Peter J Huber."
            ],
            "title": "Projection pursuit",
            "venue": "The annals of Statistics, pages 435\u2013475.",
            "year": 1985
        },
        {
            "authors": [
                "A. Hyvarinen."
            ],
            "title": "Fast and robust fixed-point algorithms for independent component analysis",
            "venue": "IEEE Transactions on Neural Networks, 10(3):626\u2013634.",
            "year": 1999
        },
        {
            "authors": [
                "Aapo Hyv\u00e4rinen",
                "Erkki Oja."
            ],
            "title": "Independent component analysis: Algorithms and applications",
            "venue": "Neural networks, 13(4-5):411\u2013430.",
            "year": 2000
        },
        {
            "authors": [
                "Agnan Kessy",
                "Alex Lewin",
                "Korbinian Strimmer."
            ],
            "title": "Optimal whitening and decorrelation",
            "venue": "The American Statistician, 72(4):309\u2013314.",
            "year": 2018
        },
        {
            "authors": [
                "Guillaume Lample",
                "Alexis Conneau",
                "Marc\u2019Aurelio Ranzato",
                "Ludovic Denoyer",
                "Herv\u00e9 J\u00e9gou"
            ],
            "title": "Word translation without parallel data",
            "venue": "In 6th International Conference on Learning Representations,",
            "year": 2018
        },
        {
            "authors": [
                "Guy Lev",
                "Benjamin Klein",
                "Lior Wolf."
            ],
            "title": "In defense of word embedding for generic text representation",
            "venue": "Natural Language Processing and Information Systems: 20th International Conference on Applications of Natural Language to Information",
            "year": 2015
        },
        {
            "authors": [
                "Ze Liu",
                "Yutong Lin",
                "Yue Cao",
                "Han Hu",
                "Yixuan Wei",
                "Zheng Zhang",
                "Stephen Lin",
                "Baining Guo."
            ],
            "title": "Swin transformer: Hierarchical vision transformer using shifted windows",
            "venue": "2021 IEEE/CVF International Conference on Computer Vision, ICCV 2021,",
            "year": 2021
        },
        {
            "authors": [
                "Hongyin Luo",
                "Zhiyuan Liu",
                "Huanbo Luan",
                "Maosong Sun."
            ],
            "title": "Online learning of interpretable word embeddings",
            "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1687\u20131692, Lisbon, Portugal. As-",
            "year": 2015
        },
        {
            "authors": [
                "Thang Luong",
                "Richard Socher",
                "Christopher Manning."
            ],
            "title": "Better word representations with recursive neural networks for morphology",
            "venue": "Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 104\u2013113.",
            "year": 2013
        },
        {
            "authors": [
                "Matt Mahoney."
            ],
            "title": "About the test data",
            "venue": "http:// mattmahoney.net/dc/textdata.html.",
            "year": 2011
        },
        {
            "authors": [
                "Alireza Makhzani",
                "Brendan Frey."
            ],
            "title": "K-sparse autoencoders",
            "venue": "arXiv preprint arXiv:1312.5663.",
            "year": 2013
        },
        {
            "authors": [
                "Binny Mathew",
                "Sandipan Sikdar",
                "Florian Lemmerich",
                "Markus Strohmaier."
            ],
            "title": "The polar framework: Polar opposites enable interpretability of pre-trained word embeddings",
            "venue": "Proceedings of The Web Conference 2020.",
            "year": 2020
        },
        {
            "authors": [
                "Tom\u00e1s Mikolov",
                "Kai Chen",
                "Greg Corrado",
                "Jeffrey Dean."
            ],
            "title": "Efficient estimation of word representations in vector space",
            "venue": "1st International Conference on Learning Representations, ICLR 2013, Scottsdale, Arizona, USA, May 2-4, 2013, Workshop Track Pro-",
            "year": 2013
        },
        {
            "authors": [
                "Tom\u00e1s Mikolov",
                "Quoc V. Le",
                "Ilya Sutskever."
            ],
            "title": "Exploiting similarities among languages for machine translation",
            "venue": "CoRR, abs/1309.4168.",
            "year": 2013
        },
        {
            "authors": [
                "Tom\u00e1s Mikolov",
                "Ilya Sutskever",
                "Kai Chen",
                "Greg Corrado",
                "Jeffrey Dean."
            ],
            "title": "Distributed representations of words and phrases and their compositionality",
            "venue": "Advances in Neural Information Processing Systems, pages 3111\u20133119.",
            "year": 2013
        },
        {
            "authors": [
                "Jiaqi Mu",
                "Pramod Viswanath."
            ],
            "title": "All-but-the-top: Simple and effective postprocessing for word representations",
            "venue": "International Conference on Learning Representations.",
            "year": 2018
        },
        {
            "authors": [
                "Brian Murphy",
                "Partha Talukdar",
                "Tom Mitchell."
            ],
            "title": "Learning effective and interpretable semantic models using non-negative sparse embedding",
            "venue": "Proceedings of COLING 2012, pages 1933\u20131950, Mumbai, India. The COLING 2012 Organizing Committee.",
            "year": 2012
        },
        {
            "authors": [
                "Tom\u00e1\u0161 Musil",
                "David Mare\u010dek."
            ],
            "title": "Independent components of word embeddings represent semantic features",
            "venue": "arXiv:2212.09580.",
            "year": 2022
        },
        {
            "authors": [
                "Sungjoon Park",
                "JinYeong Bak",
                "Alice Oh."
            ],
            "title": "Rotated word vector representations and their interpretability",
            "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 401\u2013411, Copenhagen, Denmark. Associ-",
            "year": 2017
        },
        {
            "authors": [
                "F. Pedregosa",
                "G. Varoquaux",
                "A. Gramfort",
                "V. Michel",
                "B. Thirion",
                "O. Grisel",
                "M. Blondel",
                "P. Prettenhofer",
                "R. Weiss",
                "V. Dubourg",
                "J. Vanderplas",
                "A. Passos",
                "D. Cournapeau",
                "M. Brucher",
                "M. Perrot",
                "E. Duchesnay"
            ],
            "title": "Scikit-learn: Machine learning",
            "year": 2011
        },
        {
            "authors": [
                "Telmo Pires",
                "Eva Schlinger",
                "Dan Garrette."
            ],
            "title": "How multilingual is multilingual BERT? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4996\u20135001, Florence, Italy",
            "venue": "Association for Computational Linguis-",
            "year": 2019
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark",
                "Gretchen Krueger",
                "Ilya Sutskever"
            ],
            "title": "Learning transferable visual models from natural language",
            "year": 2021
        },
        {
            "authors": [
                "Kira Radinsky",
                "Eugene Agichtein",
                "Evgeniy Gabrilovich",
                "Shaul Markovitch."
            ],
            "title": "A word at a time: Computing word relatedness using temporal semantic analysis",
            "venue": "Proceedings of the 20th International Conference on World Wide Web, page 337\u2013346.",
            "year": 2011
        },
        {
            "authors": [
                "Olga Russakovsky",
                "Jia Deng",
                "Hao Su",
                "Jonathan Krause",
                "Sanjeev Satheesh",
                "Sean Ma",
                "Zhiheng Huang",
                "Andrej Karpathy",
                "Aditya Khosla",
                "Michael Bernstein",
                "Alexander C. Berg",
                "Li Fei-Fei."
            ],
            "title": "ImageNet Large Scale Visual Recognition Challenge",
            "venue": "International",
            "year": 2015
        },
        {
            "authors": [
                "Robyn Speer"
            ],
            "title": "rspeer/wordfreq: v3.0. https: //doi.org/10.5281/zenodo.7199437",
            "year": 2022
        },
        {
            "authors": [
                "Jianlin Su",
                "Jiarun Cao",
                "Weijie Liu",
                "Yangyiwen Ou."
            ],
            "title": "Whitening sentence representations for better semantics and faster retrieval",
            "venue": "CoRR, abs/2103.15316.",
            "year": 2021
        },
        {
            "authors": [
                "Anant Subramanian",
                "Danish Pruthi",
                "Harsh Jhamtani",
                "Taylor Berg-Kirkpatrick",
                "Eduard H. Hovy."
            ],
            "title": "SPINE: sparse interpretable neural embeddings",
            "venue": "Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th inno-",
            "year": 2018
        },
        {
            "authors": [
                "Fei Sun",
                "J. Guo",
                "Yanyan Lan",
                "Jun Xu",
                "Xueqi Cheng."
            ],
            "title": "Sparse word embeddings using l1 regularized online learning",
            "venue": "International Joint Conference on Artificial Intelligence.",
            "year": 2016
        },
        {
            "authors": [
                "William Timkey",
                "Marten van Schijndel."
            ],
            "title": "All bark and no bite: Rogue dimensions in transformer language models obscure representational quality",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages",
            "year": 2021
        },
        {
            "authors": [
                "Hugo Touvron",
                "Piotr Bojanowski",
                "Mathilde Caron",
                "Matthieu Cord",
                "Alaaeldin El-Nouby",
                "Edouard Grave",
                "Gautier Izacard",
                "Armand Joulin",
                "Gabriel Synnaeve",
                "Jakob Verbeek",
                "Herv\u00e9 J\u00e9gou"
            ],
            "title": "Resmlp: Feedforward networks for image classification",
            "year": 2023
        },
        {
            "authors": [
                "Ross Wightman"
            ],
            "title": "Pytorch image models",
            "year": 2019
        },
        {
            "authors": [
                "Chao Xing",
                "Dong Wang",
                "Chao Liu",
                "Yiye Lin."
            ],
            "title": "Normalized word embedding and orthogonal transform for bilingual word translation",
            "venue": "NAACL HLT 2015, The 2015 Conference of the North American Chapter of the Association for Computational Lin-",
            "year": 2015
        },
        {
            "authors": [
                "Yukun Zhu",
                "Ryan Kiros",
                "Richard S. Zemel",
                "Ruslan Salakhutdinov",
                "Raquel Urtasun",
                "Antonio Torralba",
                "Sanja Fidler."
            ],
            "title": "Aligning books and movies: Towards story-like visual explanations by watching movies and reading books",
            "venue": "2015 IEEE Interna-",
            "year": 2015
        },
        {
            "authors": [
                "MUSE (Lample"
            ],
            "title": "2018) to aid in the mapping of English words to their counterparts in other languages7. Given that the vocabulary of 157langsfastText is substantial, containing 2,000,000",
            "year": 2018
        },
        {
            "authors": [
                "ford",
                "Ferguson"
            ],
            "title": "this measure can be used to find an optimal R by minimizing f\u03ba(ZR), and use ZR. Different values of \u03ba correspond to different rotation methods, such as quartimax (\u03ba = 0), varimax (\u03ba = 1/n), parsimax",
            "year": 2001
        },
        {
            "authors": [
                "Grave"
            ],
            "title": "2018) utilized in Section 4, we employed fastText by MUSE (Lample et al., 2018). We refer to these word embeddings as 157langs-fastText and MUSE-fastText, and chose some of the languages",
            "venue": "fastText",
            "year": 2018
        },
        {
            "authors": [
                "Lample"
            ],
            "title": "2018) instead of the standard k-NN method. The top-1 accuracy was computed as the frequency of finding the correct translation word",
            "year": 2018
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Embeddings play a fundamental role in representing meaning. However, there are still many aspects of embeddings that are not fully understood. For instance, issues such as the dimensionality of embeddings, their interpretability, and the universal properties shared by embeddings trained with different algorithms or in different modalities pose important challenges in practical applications.\nDiscussions and research have explored the topics of low-dimensionality and interpretability of embeddings (Goldberg, 2017). Proposals have been made for learning and post-processing methods that incorporate constraints, aiming to achieve sparse embeddings or acquire semantic axes. Additionally, research has focused on aligning embeddings trained in different languages through various transformations. However, in contrast to this ongoing trend, our specific focus lies on the intrinsic independence present within embeddings.\n\u2217 The first two authors contributed equally to this work. Our code and data are available at https://github.\ncom/shimo-lab/Universal-Geometry-with-ICA.\nIn this research, we post-process embeddings using Independent Component Analysis (ICA), providing a new perspective on these issues (Hyv\u00e4rinen and Oja, 2000). There are limited studies that have applied ICA to a set of word embeddings, with only a few exceptions (Lev et al., 2015; Albahli et al., 2022; Musil and Marec\u030cek, 2022). There has also been a study that applied ICA to wordcontext matrices instead of distributed representations (Honkela et al., 2010). Although it has received less attention in the past, using ICA al-\nlows us to extract independent semantic components from a set of word embeddings. By leveraging these components as the embedding axes, we anticipate that each word can be represented as a composition of intrinsic (inherent in the original embeddings) and interpretable (sparse and consistent) axes. Our experiment suggests that the number of dimensions needed to represent each word is considerably less than the actual dimensions of the embeddings, enhancing the interpretability.\nFig. 1 shows an example of independent semantic components that are extracted by ICA. Each axis has its own meaning, and a word is represented as a combination of a few axes. Furthermore, Figs. 2a and 3a show that the semantic axes found by ICA are almost common across different languages when we applied ICA individually to the embeddings of each language. This result is not limited to language differences but also applies\nwhen the embedding algorithms or modalities (i.e., word or image) are different.\nPrincipal Component Analysis (PCA) has traditionally been used to identify significant axes in terms of variance, but it falls short in comparison to ICA; the patterns are less clear for PCA in Figs. 2b and 3b. Embeddings are known to be anisotropic (Ethayarajh, 2019), and their isotropy can be greatly improved by post-processes such as centering the mean, removing the top principal components (Mu and Viswanath, 2018), standardization (Timkey and van Schijndel, 2021), or whitening (Su et al., 2021), which can also lead to improved performance in downstream tasks. While the whitening obtained by PCA provides isotropic embeddings regarding the mean and covariance of components, notably, ICA has succeeded in discovering distinctive axes by focusing on the anisotropic information left in the third and higher-order mo-\nments of whitened embeddings."
        },
        {
            "heading": "2 Background",
            "text": ""
        },
        {
            "heading": "2.1 Interpretability in word embeddings",
            "text": "The interpretability of individual dimensions in word embedding is challenging and has been the subject of various studies.\nA variety of studies have adopted explicit constraints during re-training or post-processing of embeddings to improve sparsity and interpretability. This includes the design of loss functions (Arora et al., 2018), the introduction of constraint conditions by sparse overcomplete vectors (Faruqui et al., 2015), and re-training using k-sparse autoencoders (Makhzani and Frey, 2013; Subramanian et al., 2018), sparse coding (Murphy et al.,\n2012; Luo et al., 2015), or \u21131-regularization (Sun et al., 2016). Additionally, the sense polar approach designs objective functions to make each axis of BERT embeddings interpretable at both ends (Engler et al., 2022; Mathew et al., 2020).\nHowever, our study takes a distinct approach. We do not rely on explicit constraints utilized in the aforementioned methods. Instead, we leverage transformations based on the inherent information within the embeddings. Our motivation aligns with that of Park et al. (2017) and Musil and Marec\u030cek (2022), aiming to incorporate interpretability into each axis of word vectors. Similar to previous studies (Honkela et al., 2010; Musil and Marec\u030cek, 2022), we have confirmed that interpretable axes are found by applying ICA to a set of embeddings."
        },
        {
            "heading": "2.2 Cross-lingual embeddings",
            "text": "Cross-lingual mapping. To address the task of cross-lingual alignment, numerous methodologies have been introduced to derive cross-lingual mappings. Supervised techniques that leverage translation pairs as training data have been proposed, such as the linear transformation approach (Mikolov et al., 2013b). Studies by Xing et al. (2015) and Artetxe et al. (2016) demonstrated enhanced performance when constraining the transformation matrices to be orthogonal. Furthermore, Artetxe et al. (2017) proposed a method for learning transformations from a minimal data set. As for unsupervised methods that do not leverage translation pairs for training, Lample et al. (2018) proposed an approach incorporating adversarial learning, while Artetxe et al. (2018) introduced a robust self-learning method. Additionally, unsupervised methods employing optimal transportation have been presented: Alvarez-Melis and Jaakkola (2018) introduced a method utilizing the GromovWasserstein distance, while studies by Grave et al. (2019) and Aboagye et al. (2022) suggested methods that employ the Wasserstein distance.\nMultilingual language models. Studies have demonstrated that a single BERT model, pretrained with a multilingual corpus, acquires crosslingual grammatical knowledge (Pires et al., 2019; Chi et al., 2020). Further research has also been conducted to illustrate how such multilingual models express cross-lingual knowledge through embeddings (Chang et al., 2022).\nOur approach. These cross-lingual studies, even the \u2018unsupervised\u2019 mapping, utilize embeddings\nfrom multiple languages for training. Unlike these, we apply ICA to each language individually and identify the inherent semantic structure in each without referencing other languages. Thus ICA, as well as PCA, is an unsupervised transformation in a stronger sense. In our study, the embeddings from multiple languages and the translation pairs are used solely to verify that the identified semantic structure is shared across these languages. While it is understood from previous research that there exists a shared structure in embeddings among multiple languages (i.e., their shapes are similar), the discovery in our study goes beyond that by revealing the universal geometric patterns of embeddings with intrinsic interpretable axes (i.e., clarifying the shapes of embedding distributions; see Fig. 4)."
        },
        {
            "heading": "3 ICA: Revealing the semantic structure in the geometric patterns of embeddings",
            "text": "We analyzed word embeddings using ICA. It was discovered that they possess inherent interpretability and sparsity. ICA can unveil these properties of embeddings."
        },
        {
            "heading": "3.1 PCA-transformed embeddings",
            "text": "Before explaining ICA, we briefly explain PCA, widely used for dimensionality reduction and whitening, or sphering, of feature vectors.\nThe pre-trained embedding matrix is represented as X \u2208 Rn\u00d7d, where X is assumed to be centered; it is preprocessed so that the mean of each column is zero. Here, n represents the number of embeddings, and d is the number of embedding dimensions. The i-th row of X, denoted as xi \u2208 Rd, corresponds to the word vector of the i-th word, or the embedding computed by an image model from the i-th image.\nIn PCA, the transformed embedding matrix Z \u2208 Rn\u00d7d is computed using algorithms such as Singular Value Decomposition (SVD) of X. This process identifies the directions that explain the most variance in the data. The transformation can be expressed using a transformation matrix A \u2208 Rd\u00d7d as follows:\nZ = XA.\nThe columns of Z are called principal components. The matrix Z is whitened, meaning that each column has a variance of 1 and all the columns are uncorrelated. In matrix notation, Z\u22a4Z/n = Id, where Id \u2208 Rd\u00d7d represents the identity matrix."
        },
        {
            "heading": "3.2 ICA-transformed embeddings",
            "text": "In Independent Component Analysis (ICA), the goal is to find a transformation matrix B \u2208 Rd\u00d7d such that the columns of the resulting matrix S \u2208 Rn\u00d7d are as independent as possible. This transformation is given by:\nS = XB.\nThe columns of S are called independent components. The independence of random variables is a stronger condition than uncorrelatedness, and when random variables are independent, it implies that they are uncorrelated with each other. While both PCA and ICA produce whitened embeddings, their scatterplots appear significantly different, as observed in Fig. 5; refer to Appendix B for more details. While PCA only takes into account the first and second moments of random variables (the mean vector and the variance-covariance matrix), ICA aims to achieve independence by incorporating the third moment (skewness), the fourth moment (kurtosis) and higher-order moments through non-linear contrast functions (Fig. 6).\nIn the implementation of FastICA1, PCA is used as a preprocessing step for computing Z, and\nS = ZRica\nis actually computed2, and we seek an orthogonal matrix Rica that makes the columns of S as independent as possible (Hyv\u00e4rinen and Oja, 2000). The linear transformation with an orthogonal matrix involves only rotation and reflection of the zi vectors, ensuring that the resulting matrix S is also whitened, meaning that the embeddings of ICA, as well as those of PCA, are isotropic with respect to the variance-covariance matrix (Appendix A).\nAccording to the central limit theorem, when multiple variables are added and mixed together, they tend to approach a normal distribution. Therefore, in ICA, an orthogonal matrix Rica is computed to maximize a measure of non-Gaussianity for each column in S, aiming to recover independent variables (Hyv\u00e4rinen and Oja, 2000). This idea is rooted in the notion of \u2018projection pursuit\u2019 (Huber, 1985), a long-standing idea in the field. Since the normal distribution maximizes entropy among probability distributions with fixed mean and variance, measures of non-Gaussianity are interpreted as approximations of neg-entropy.\n1We used FastICA in Scikit-learn (Pedregosa et al., 2011). 2Since we can express S = XARica, and thus specifying\nRica is equivalent to specifying B = ARica."
        },
        {
            "heading": "3.3 Interpretability and low-dimensionality of ICA-transformed embeddings",
            "text": "Fig. 1 illustrates that the key components of a word embedding are formed by specific axes that capture the meanings associated with each word. Specifically, this figure showcases five axes selected from the ICA-transformed word embeddings, that is, five columns of S. The word embeddings X have a dimensionality of d = 300 and were trained on the text8 corpus using Skip-gram with negative sampling. For details of the experiment, refer to Appendix B.\nInterpretability. Each of these axes represents a distinct meaning and can be interpreted by examining the top words based on their normalized component values. For example, words like dishes, meat, noodles have high values on axis 16, while words like cars, car, ferrari have high values on axis 26. We labeled each axis with the word having the highest component value, enclosed in brackets like [dishes] for axis 16, and [cars] for axis 26.\nLow-dimensionality. The meaning of a word is approximately represented by the combination of a few axes. For example, the word ferrari has large values on [cars] (axis 26) and [italian] (axis 34). This indicates that the meaning of the word ferrari\nis approximately represented by these two axes. Quantitative evaluation is provided in Section 6."
        },
        {
            "heading": "4 Universality across languages",
            "text": "This section examines the results of conducting ICA on word embeddings, each trained individually from different language corpora. Interestingly, the meanings of the axes discovered by ICA appear to be the same across all languages. For a detailed description of the experiment, refer to Appendix C.\nSetting. We utilized the fastText embeddings by Grave et al. (2018), each trained individually on separate corpora for 157 languages. In this experiment, we used seven languages: English (EN), Spanish (ES), Russian (RU), Arabic (AR), Hindi (HI), Chinese (ZH), and Japanese (JA). The dimensionality of each embedding is d = 300. For each language, we selected n = 50,000 words, and computed the PCA-transformed embeddings Zlang and the ICA-transformed embeddings Slang for each of the seven centered embedding matrices Xlang (lang \u2208 {EN,ES,RU,AR,HI,ZH, JA}).\nWe then performed the permutation of axes to find the best alignment of axes between languages. The matching is measured by the cross-correlation coefficients between languages.\nResults. The upper panels of Fig. 3 display the cross-correlation coefficients between the first 100 axes of English and those of Spanish embeddings. The significant diagonal elements and negligible off-diagonal elements observed in Fig. 3a suggest a strong alignment of axes, indicating a good correspondence between the ICA-transformed embeddings. Conversely, the less pronounced diagonal elements and non-negligible off-diagonal elements observed in Fig. 3b indicate a less favorable alignment between the PCA-transformed embeddings.\nThe semantic axes identified by ICA, referred to as \u2018independent semantic axes\u2019 or \u2018independent semantic components\u2019, appear to be nearly universal, regardless of the language. In Fig. 2a, heatmaps visualize the ICA-transformed embeddings. The upper panels display the top 5 words for each of the first 100 axes in English and their corresponding translations in the other languages. It is evident that words sharing the same meaning in different languages are represented on the corresponding axes. The lower panels show the first 5 axes with their corresponding words. ICA identified axes in each language related to first names, ships-and-sea, country names, plants, and meals as independent semantic components. Overall, the heatmaps for all languages exhibit very similar patterns, indicating a shared set of independent semantic axes in the ICA-transformed embeddings across languages."
        },
        {
            "heading": "5 Universality in algorithm and modality",
            "text": "We expand the analysis from the previous section to two additional settings. The first setting involves comparing fastText and BERT, while the second setting involves comparing multiple image models and fastText simultaneously."
        },
        {
            "heading": "5.1 Contextualized word embeddings",
            "text": "Setting. Sentences included in the One Billion Word Benchmark (Chelba et al., 2014) were processed using a BERT-base model to generate contextualized embeddings for n = 100,000 tokens, each with a dimensionality of d = 768. We computed PCA and ICA-transformed embeddings for both the BERT embeddings and the English fastText embeddings. As with the cross-lingual case of Section 4, the axes are aligned between BERT and fastText embeddings by permuting the axes based on the cross-correlation coefficients. Further details can be found in Appendix D.1.\nResults. In Fig. 7a, the heatmaps for fastText and BERT exhibit strikingly similar patterns, indicating a shared set of independent semantic axes in the ICA-transformed embeddings for both fastText and BERT. The lower heatmaps show the first five axes with meanings first names, community, ships-andsea, verb, and number. Furthermore, the middle panel in Fig. 3a demonstrates a good alignment of axes between fastText and BERT embeddings. On the other hand, PCA gives a less favorable alignment, as seen in Figs. 3b and Fig. 7b."
        },
        {
            "heading": "5.2 Image embeddings",
            "text": "Setting. We randomly sampled images from the ImageNet dataset (Russakovsky et al., 2015), which consists of 1000 classes. For each class, we collected 100 images, resulting in a total of n = 100,000 images. These images were passed through the five pre-trained image models listed in Table 1, and we obtained embeddings from the layer just before the final layer of each model.\nAmong these models, we selected a specific model of ViT-Base (Dosovitskiy et al., 2021) as our reference image model. This particular ViT-Base model was trained with a focus on aligning with text em-\nbeddings (Radford et al., 2021). We computed PCA and ICA-transformed embeddings for the five image models. As with the cross-lingual case of Section 4, the axes are aligned between ViT-Base and each of the other four image models by permuting the axes based on the cross-correlation coefficients. Additionally, to align the axes between ViT-Base and English fastText, we permuted the axes based on the cross-correlation coefficients that were computed using ImageNet class names and fastText vocabulary as a means of linking the two modalities. Further details can be found in Appendix D.2.\nResults. In Fig. 8a, the heatmaps of the five image models and fastText exhibit similar patterns, indicating a shared set of independent semantic axes in the ICA-transformed embeddings for both images and words. While we expected a good alignment between ViT-Base and fastText, it is noteworthy that we also observe a good alignment of ResMLP (Touvron et al., 2023) and Swin Transformer (Liu et al., 2021) with fastText. Furthermore, Fig. 3a demonstrates a very good alignment of axes between ViT-Base and ResMLP. This suggests that the independent semantic components captured by ICA are not specific to a particular image model but are shared across multiple models. On the other hand, PCA gives a less favorable alignment, as seen in Figs. 3b and 8b."
        },
        {
            "heading": "6 Quantitative evaluation",
            "text": "We quantitatively evaluated the interpretability (Section 6.1) and low-dimensionality (Section 6.2) of ICA-transformed embeddings comparing with other whitening methods (PCA, ZCA) as well as a well-known rotation method (varimax). These baseline methods are described in Appendix E.1. In the monolingual experiments, we used 300- dimensional word embeddings trained using the SGNS model on the text8 corpus, as outlined in Appendix B. Furthermore, we assessed the crosslingual performance (Section 6.3) of PCA and ICAtransformed embeddings, along with two other supervised baseline methods."
        },
        {
            "heading": "6.1 Interpretability: word intrusion task",
            "text": "We conducted the word intrusion task (Sun et al., 2016; Park et al., 2017) in order to quantitatively evaluate the interpretability of axes. In this task, we first choose the top k words from each axis, and then evaluate the consistency of their word meaning based on the identifiability of the intruder word. For instance, consider a word group of k = 5, namely, {windows, microsoft, linux, unix, os} with the consistent theme of operating systems. Then, hamster should be easily identified as an intruder. Details are presented in Appendix E.3.\nResults. The experimental results presented in Table 2 show that the top words along the axes of ICA-transformed embeddings exhibit more consistent meanings compared to those of other methods. This confirms the superior interpretability of axes in ICA-transformed embeddings."
        },
        {
            "heading": "6.2 Low-dimensionality: analogy task & word similarity task",
            "text": "We conducted analogy tasks and word similarity tasks using a reduced number of components from the transformed embeddings. Specifically, we evaluated how well the transformed embedding retains semantic information even when reducing the nonzero components from the least significant ones. For each whitened embedding, we retained the k most significant components unchanged while setting the remaining components to zero. The specific axes we used depend on each embedding. The performance was evaluated for the number of axes k ranging from 1 to 300.\nResults. Fig. 9 demonstrates that the ICAtransformed embedding has the highest average performance throughout the entire dataset. Detailed settings and results, including those for unwhitening cases, are presented in Appendix E.4. These experimental results show that the ICA-transformed embedding effectively represents word meanings using only a few axes."
        },
        {
            "heading": "6.3 Universality: cross-lingual alignment",
            "text": "In Fig. 2a, we visually inspected the crosslingual alignment obtained by permutating ICAtransformed embeddings, and we observed remarkably good alignment across languages. In this sec-\ntion, we go beyond that by thoroughly examining the performance of the alignment task. Details are presented in Appendix E.5.\nDatasets. In addition to \u2018157langs-fastText\u2019 used in Section 4 (Grave et al., 2018), we used \u2018MUSEfastText\u2019, pre-aligned embeddings across languages (Lample et al., 2018). The source language is English (EN), and the target languages are Spanish (ES), French (FR), German (DE), Italian (IT), and Russian (RU). For each language, we selected 50,000 words.\nSupervised baselines. We used two supervised baselines that learn a transformation matrix W \u2208 Rd\u00d7d by leveraging both the source embedding X and the target embedding Y. Each row of X and Y corresponds to a translation pair. We minimized \u2225XW \u2212 Y\u222522 by the least squares (LS) method (Mikolov et al., 2013b) or the Procrustes (Proc) method with the constraint that W is an orthogonal matrix (Xing et al., 2015; Artetxe et al., 2016).\nRandom transformation. In addition to the original fastText embeddings, we considered embeddings transformed by a random matrix Q \u2208 Rd\u00d7d that involves random rotation and random scaling. Specifically, for each X, we independently generated Q to compute XQ.\nResults. Table 3 shows the top-1 accuracy of the cross-lingual alignment task. The values are averaged over the five target languages.\nLS consistently performed the best, or nearly the best, across all the settings because it finds the\noptimal mapping from the source language to the target language by leveraging both the embeddings as well as translation pairs. Therefore, we consider LS as the reference method in this experiment. Proc performed similarly to LS in the original embeddings, but its performance deteriorated with the random transformation. The original word embeddings had very similar geometric arrangements across languages, but the random transformation distorted the arrangements so that the orthogonal matrix in Proc was not able to recover the original arrangements.\nICA generally performed well, despite being an unsupervised method. In particular, ICA was not affected by random transformation and performed as well as or better than Proc. The higher performance of ICA for MUSE than for 157langs is likely due to the fact that MUSE is pre-aligned. On the other hand, PCA performed extremely poorly in all the settings. This demonstrates the challenge of crosslingual alignment for unsupervised transformations and highlights the superiority of ICA.\nThe observations from this experiment can be summarized as follows. ICA was able to identify independent semantic axes across languages. Furthermore, ICA demonstrated robust performance even when the geometric arrangements of embeddings were distorted. Despite being an unsupervised transformation method, ICA achieved impressive results and performed comparably to the supervised baselines."
        },
        {
            "heading": "7 Conclusion",
            "text": "We have clarified the universal semantic structure in the geometric patterns of embeddings using ICA by leveraging anisotropic distributions remaining in the whitened embeddings. We have verified that the axes defined by ICA are interpretable and that embeddings can be effectively represented in lowdimensionality using a few of these components. Furthermore, we have discovered that the meanings of these axes are nearly universal across different languages, algorithms, and modalities. Our findings are supported not only by visual inspection of the embeddings but also by quantitative evaluation, which confirms the interpretability, lowdimensionality, and universality of the semantic structure. The results of this study provide new insights for pursuing the interpretability of models. Specifically, it can lead to the creation of interpretable models and the compression of models.\nLimitations\n\u2022 Due to the nature of methods that identify semantic axes by linearly transforming embeddings, the number of independent semantic components is limited by the dimensionality of the original embeddings.\n\u2022 ICA transforms the data in such a way that the distribution of each axis deviates from the normal distribution. Therefore, ICA is not applicable if the original embeddings follow a multivariate normal distribution. However, no such issues were observed for the embeddings considered in this paper.\n\u2022 In Section 4, we utilized translation pairs to confirm the shared semantic structure across languages. Consequently, without access to such translation pairs, it becomes infeasible to calculate correlation coefficients and achieve successful axis matching. Therefore, in the future, we intend to investigate whether it is possible to perform matching by comparing distributions between axes using optimal transport or other methods without relying on translation pairs.\n\u2022 When comparing heatmaps of embeddings across languages in Fig. 2, we looked at five words from each axis. Thus only a small fraction of vocabulary words were actually examined for verifying the shared structure of geometric patterns of embeddings. Although this issue is already compensated by the plot of cross-correlations in Fig. 3, where a substantial fraction of vocabulary words were examined, we seek a better way to verify the shared structure in future work. For example, the scatter plots in Fig. 15 may help us understand the entire structure of word embedding distributions.\nEthics Statement\nThis study complies with the ACL Ethics Policy."
        },
        {
            "heading": "Acknowledgements",
            "text": "We would like to thank Sho Yokoi for the discussion and anonymous reviewers for their helpful advice. This study was partially supported by JSPS KAKENHI 22H05106, 23H03355, JST CREST JPMJCR21N3."
        },
        {
            "heading": "A Whitening and isotropic embeddings",
            "text": "In addition to the whitened embeddings Z obtained through PCA, we also consider various other whitened embeddings (Kessy et al., 2018). They\ncan be represented in the form of linear transformations using an orthogonal matrix R:\nY = ZR.\nThese transformed embeddings Y with any R are again whitened3, meaning that the embeddings are isotropic with respect to the variance-covariance matrix. Also, the centering step of whitening makes the embeddings centered, and the transformed embeddings Y with any R are again centered4, meaning that the embeddings are isotropic with respect to the mean vector. However, the linear transformation cannot make the embeddings isotropic with respect to the third and higher-order moments.\nThe row vectors yi = (yi1, . . . , yid) and zi = (zi1, . . . , zid) of Y and Z, respectively, satisfy the equation5:\n\u27e8yi,yj\u27e9 = \u27e8zi, zj\u27e9,\nwhere \u27e8a,b\u27e9 = \u2211d\nk=1 akbk represents the inner product. Therefore, the inner products of embeddings are preserved under this transformation, indicating that the performance of tasks based on inner products, such as those using cosine similarity, remains unchanged."
        },
        {
            "heading": "B Details of experiment in Section 3",
            "text": "We summarize the details of the embeddings used in Figure 1 and the monolingual quantitative evaluations in Sections 6.1 and 6.2.\nCorpus. We used the text8 (Mahoney, 2011), which is an English corpus data with the size of N = 17.0\u00d7106 tokens and |V | = 254\u00d7103 vocabulary words. We used all the tokens separated by spaces. The frequency of word w \u2208 V in the corpus is denoted as p(w), where \u2211 w\u2208V p(w) = 1.\nTraining of the SGNS model. Word embeddings were trained6 by optimizing the same objective\n3In general, for an orthogonal matrix R, i.e., R\u22a4R = Id, consider a transformed matrix Y = ZR. Then Y is whitened, because Y\u22a4Y/n = (ZR)\u22a4ZR/n = R\u22a4Z\u22a4ZR/n = R\u22a4R = Id.\n4For centered embeddings Z, the mean vector is (1n/n) \u22a4Z = 0\u22a4d , where 1n = (1, . . . , 1) \u22a4 \u2208 Rn and 0d = (0, . . . , 0) \u22a4 \u2208 Rd. Then, for any orthogonal matrix R, (1n/n) \u22a4Y = (1n/n) \u22a4ZR = 0\u22a4d R = 0 \u22a4 d .\n5Since yi = ziR, we have \u27e8yi,yj\u27e9 = yiy\u22a4j = ziR(zjR) \u22a4 = ziRR \u22a4z\u22a4j = ziz \u22a4 j = \u27e8zi, zj\u27e9.\n6We used AMD EPYC 7702 64-Core Processor (64 cores \u00d7 2). In this setting, the CPU time is estimated at about 12 hours.\nfunction used in Mikolov et al. (2013c). Parameters used to train SGNS are summarized in Table 4. The learning rate shown is the initial value, which we decreased linearly to the minimum value of 1.0 \u00d7 10\u22124 during the learning process. The negative sampling distribution was proportional to the 3/4-th power of the word frequency, p(w)3/4. The elements of xi were initialized by the uniform distribution over [\u22120.5, 0.5] divided by the dimensionality of the embedding, and the elements of x\u2032i were initialized by zero.\nDiscarding low-frequency words. After computing the embeddings, we discarded words w that appeared less than 10 times in the text8 corpus. The new vocabulary V \u2032 consists of the 47,134 words that appeared 10 times or more in the text8 corpus.\nResampling word embeddings. We resampled the word w with probability q(w) when preparing the data matrix X \u2208 Rn\u00d7d. As an example, we will explain the procedure when employing q(w) \u221d p(w) for w \u2208 V \u2032. First, we randomly sampled 100,000 words from V \u2032 with replacement using the weight q(w). This resulted in the selection of 14,942 unique words. Subsequently, we added 15,058 words from the remaining unselected words, ordered by descending frequency, to reach a total of 30,000 unique words. Each row of X represents the embeddings of the n = 115,058 words selected through the aforementioned process.\nSelection of resampling weight. We considered resampling weights in the form of q(w) \u221d p(w)\u03b1, where \u03b1 takes values from the candidate set \u03b1 \u2208 {1/2, 3/4, 1}. We conducted an experiment to determine the optimal value of \u03b1. For each q(w), we prepared the data matrix X using the resampling method explained above. Additionally, we created an unweighted X with n = |V \u2032|. We then computed ICA-transformed embeddings and evaluated the performance on the analogy and word similarity tasks using a reduced number of components, which are explained in detail in Section 6.2 and Appendix E.4. The results are shown in Fig. 10. We\nobserved that either \u03b1 = 3/4 or \u03b1 = 1 is the best, and we decided that using \u03b1 = 1 is appropriate for ease of implementation in general.\nSelection of the number of FastICA iterations. We demonstrate that our analysis remains unaffected by changes in the number of iterations in FastICA. Specifically, we evaluated the embeddings obtained by varying the number of iterations (100, 200, 1000, 10000) in FastICA. The evaluation was performed on the word intrusion task, analogy task, and word similarity task. The results are presented in Table 5 and Fig. 11. In both experiments, we observed that reducing the number of FastICA iterations slightly diminished task performance, although the difference was very small. Therefore, we conclude that changing the number of FastICA iterations did not significantly impact the results.\nICA-transformed embeddings. We utilized the implementation of FastICA, with the default setting\nexcept for the number of iterations set to 10,000 and a tolerance of 10\u221210. The contrast function used was G(x) = log cosh(x). It should be noted that the embeddings obtained through ICA have arbitrariness in the sign of each axis and the order of the axes. We calculated the skewness of each axis and flipped the sign of axes as necessary to ensure a positive sign of skewness. We then sorted the axes in descending order of skewness. When visualizing embeddings or selecting word sets, we normalized each embedding to have a unit norm for facilitating interpretation unless otherwise specified.\nDetails of Fig. 1 in Section 1. To illustrate the additive compositionality of embeddings, the words in the heatmap were selected as follows. For each of the six combinations of axes {[dishes], [cars], [films]} \u00d7 {[italian], [japanese]}, we selected 20 words with the largest sum of the two component values. From these 20 words, we chose the top five words based on the second-largest component value among the five axes. As a result, a total of 6 \u00d7 5 = 30 words were selected in this process.\nNext, we created scatterplots of the normalized ICA-transformed embeddings for all the ten combinations of two axes selected from {[dishes], [cars], [films], [italian], [japanese]}. Two of these scatterplots are shown in Fig. 1, and the remaining eight are presented in Fig. 12.\nScatter plots of ICA and PCA-transformed word embeddings. To visualize the transformed embedding, scatterplots are displayed with selected pairs of axes. Fig. 13a shows the ICA-transformed embeddings given in Section 3.2, plotting specified columns of S, with the axis numbers sorted in descending order of skewness. Fig. 13b shows the PCA-transformed embeddings given in Section 3.1, plotting specified columns of Z, with the axis numbers sorted in descending order of variance. Colors indicate word frequency in the corpus, with warmer colors being more frequent. Unlike other visualizations, each embedding is not normalized to have a unit norm. The plots in Fig. 5 in Sectoin 3 are superpositions of the plots in Figs. 13a and 13b, but the frequency information was omitted.\nThe distributions of these two types of word embedding have exactly the same shape in Rd because they can be transformed into each other by rotation as S = ZRica. However, there are significant differences in the word distributions on each axis. The distribution of ICA-transformed embeddings exhibits anisotropy with a heavy-tailed shape along each axis, thereby characterizing the meaning of each axis. The distribution of PCA-transformed embeddings is more isotropic and lacks specific characterization of each axis, except for the top few axes that encode word frequency as pointed out by Mu and Viswanath (2018). Interestingly, pronounced frequency bias is not observed in the axes of ICA-transformed embeddings.\nThe spiky shape of the distribution of word embeddings observed in Fig. 13a is illustrated in Fig. 4. As Figs. 2a and 3a suggest, the distribution of word embeddings across multiple languages has a nearly common shape, so they can be mapped by orthogonal transformations.\nMeasures of non-Gaussianity. Fig. 6 in Sectoin 3 displays four non-Gaussianity measures. Let X denote a random variable representing the component on a specified axis, and let Z denote a Gaussian random variable; here the symbols X and Z are not related to X and Z. Since both PCA and ICA-transformed embeddings are whitened, we assume that X and Z have a mean of zero and a variance of one; E(X) = E(Z) = 0 and E(X2) = E(Z2) = 1, where E() denotes the expectation.\n\u2022 The first measure is the skewness E(X3). If it is negative, we flip the sign of the axis so that E(X3) \u2265 0. Since E(Z3) = 0, a large value of skewness indicates a deviation from Gaussianity.\n\u2022 The second measure is the kurtosis E(X4)\u2212 E(Z4), where E(Z4) = 3. We observed that it is nonnegative for most of the components of embeddings, indicating that their distributions are more spread out with heavy tails than the normal distribution.\n\u2022 The third measure labeled logcosh is defined as {E(G(X)) \u2212 E(G(Z))}2 with the contrast function G(x) = log cosh(x) and E(G(Z)) = 0.374567207491438. This measure is used as the objective function in the implementation of FastICA.\n\u2022 The fourth measure labeled Gaussian is {E(G(X)) \u2212 E(G(Z))}2 with the contrast function G(x) = \u2212 exp(\u2212x2/2) and E(G(Z)) = \u22121/ \u221a 2.\nProperties of the last two measures are well studied in Hyvarinen (1999)."
        },
        {
            "heading": "C Details of experiment in Section 4",
            "text": "The procedure described for this cross-lingual experiment serves as a template for the other experiments in the following sections.\nDataset. We employed the fastText embeddings trained for 157 languages by Grave et al. (2018), referred to as \u2018157langs-fastText\u2019 in this study. We also made use of the dictionaries provided by MUSE (Lample et al., 2018) to aid in the mapping of English words to their counterparts in other languages7. Given that the vocabulary of 157langsfastText is substantial, containing 2,000,000 words, our initial step was to lowercase and select nonduplicate words included in both the English-toother language and other language-to-English dictionaries provided by MUSE. These dictionaries contain 6,500 unique source words from the combined train and test sets. Subsequently, from the 2,000,000 words, those not yet chosen were selected based on their frequency. We determined the frequency using the Python package wordfreq (Speer, 2022), which provides word frequencies across various languages. The vocabulary was then capped at 50,000 words for each language.\nPCA and ICA-transformed embeddings. We then implemented PCA and ICA transformations on the fastText embeddings, each containing 50,000 words per language. We used PCA and FastICA in Scikit-learn (Pedregosa et al., 2011) with ICA performed for a maximum of 10,000 iterations. Finally, we computed the skewness for each axis and flipped the sign of the axis if necessary to ensure positive skewness. These PCA and ICA transformations were applied to each language individually to identify the inherent semantic structure within each language without referencing other languages. The following steps are devoted to verifying that the identified semantic structure in\n7Here, we solely utilized the dictionaries provided by MUSE and did not make use of the embeddings included in MUSE. The multi-lingual embeddings in MUSE are prealigned across languages, which makes them unsuitable for this particular experiment.\nthe axes of ICA-transformed embeddings is shared across these languages.\nWord translation pairs. As the first step of the verification, we established the correspondence between the embeddings of English and the embeddings of other languages in the 157langs-fastText dataset. This linking information between embeddings is necessary to compute cross-correlation coefficients. For the purpose of illustration, we will explain the procedure using the language pair of English and Spanish. First, we gathered all pairs of English and Spanish words from the train set of the MUSE dictionaries, including both the English-toSpanish and Spanish-to-English translation pairs. Next, we filtered out any pairs where either the English word or the Spanish word was not included in the vocabulary set of 50,000 words prepared for each language. The total number of pairs collected was 11,965, where a word may be included in multiple translation pairs. The number of unique English words obtained from this process was 4,991. The results of applying this procedure to the six target languages are presented in Table 6, where the source language is English.\nAlignment of axes via permutation. Next, we established a correspondence between the axes of\nEnglish word embeddings and those of other languages by appropriately permuting the indices of axes. This involves permuting the columns of the transformed embedding matrix. For illustrative purposes, we will explain the procedure using English and Spanish word embeddings. Since both the English and Spanish word embeddings have dimensions of 300, we computed a total of 300\u00d7300 cross-correlation coefficients using the translation pairs of English words and their corresponding Spanish words. From these cross-correlation coefficients, we identified matched pairs of axes with high correlations in a greedy fashion, starting from the highest correlation. The columns of Spanish word embeddings were then permuted to match those of English word embeddings. This procedure was applied to all other languages, ensuring that their axes align with those of English.\nReordering axes. Subsequently, we computed the average correlation coefficients of the aligned axes to determine the reordering of the axes based on their degree of similarity. For each language, we calculated 300 correlation coefficients between the axes and the corresponding axes of English. These correlation coefficients were then averaged across Spanish, Russian, Arabic, Hindi, Chinese, and Japanese. Finally, we reordered the axes in descending order based on the average correlation coefficient. The cross-correlations between the aligned axes as well as the average correlation coefficients are shown in Fig. 14.\nDiagnosing the axis alignment. For both ICAtransformed and PCA-transformed embeddings, we demonstrated the 100\u00d7 100 cross-correlation coefficients between the first 100 axes of reordered English and Spanish word embeddings in Fig. 3. The diagonal elements represent the correlation coefficients between the aligned pairs of axes, while the off-diagonal elements represent the correlation coefficients between unaligned axes. In the case of ICA-transformed embeddings, as depicted in Fig. 3a, it is clear that the diagonal elements exhibit significantly positive values, while the majority of the off-diagonal elements are close to zero. Thus, ICA gives a strong alignment of the axes. In contrast, for PCA-transformed embeddings, as shown in Fig. 3b, the diagonal elements are smaller compared to ICA, and a considerable number of off-diagonal elements deviate significantly from zero. Thus, PCA gives a less favorable alignment.\nWord selection for visualization. After reordering the axes, we normalized all the embeddings to ensure that their norm is equal to 1. We focused on the first 100 axes for further analysis. We limited the selection to words that have translation pairs in the complete set of MUSE in all languages, including English, Spanish, Russian, Arabic, Hindi, Chinese, and Japanese. From this restricted set, we selected the top 5 words for each axis of the English word embeddings based on their largest component values. For the remaining languages, we selected words from the translation pairs. To ensure diversity, we excluded duplicates that occur in both singular and plural forms of nouns.\nAnalyzing the heatmaps. In Fig. 2, we presented heatmaps that illustrate the components of the normalized 500-word embeddings selected through this procedure for both ICA-transformed and PCAtransformed embeddings.\nIn Fig. 2a, which represents the ICA-transformed embeddings, we observe that the top 5 words for each axis have significant values along that axis. This leads to a diagonal pattern of large values due to the sparsity in other components. This consistent pattern is observed in all seven languages. The lower panels of the figure magnify the first five axes, allowing us to identify the top 5 words chosen for each axis. It is evident that each axis has a specific semantic relevance. For example, the first axis corresponds to first names, and we observe that such semantically meaningful axes can be effectively aligned across languages.\nConversely, in Fig. 2b representing the PCAtransformed embeddings, it can be observed that the top 500 words do not exhibit significant values along the diagonal. Additionally, in the magnified heatmaps depicting the 25 words, when compared to Fig. 2a, the semantics of the words for each axis appear to be more ambiguous.\nVisualization via scatterplots along the five axes. In the heatmaps, we visualized embeddings for 25 words in each language. To overcome the limitation of the heatmap, which only allows us to view a small subset of words, we utilized scatterplots to visualize all the words in the vocabulary. In Fig. 15, we projected the normalized ICA-transformed embeddings into two dimensions using the same five axes as those used in the heatmaps.\nSimilarly to the heatmaps, the meaning of each axis can be interpreted based on the words arranged\nalong the axis. When viewed as a whole, the distribution of embeddings for each language exhibits a distinctive shape with spikes along axes. This spiky shape is universally observed across all languages.\nWe should discuss whether this spiky shape is real or not. ICA seeks axes that maximize nonGaussianity (Hyv\u00e4rinen and Oja, 2000). More generally, projection pursuit aims to find \u2018interesting\u2019 projections of high-dimensional data (Huber, 1985). However, these methods may detect apparent structures that are not statistically significant, particularly when \u03b3 = d/n is large (Bickel et al., 2018). In the case of cross-lingual word embeddings, where d = 300 and n = 50,000, \u03b3 = 0.006 \u226a 1 is very small. Therefore, it can be said that the chances of detecting apparent non-Gaussian structures are quite low. Taking into account the discovery of numerous common axes across all languages, it can be argued that the universal shape in the crosslingual embedding distributions is real.\nThe signature of ICA-transformed embeddings. To investigate the characteristics of ICAtransformed word embeddings for each language, we plotted two measures of non-Gaussianity, namely skewness and kurtosis, along each axis in Fig. 16. Summary statistics for the skewness and kurtosis of the embeddings are given in Table 7. These results allow us to discern deviations from the isotropy of the distributions of word embeddings. All the kurtosis values for all axes in all languages were positive, indicating that the distributions are spreading more than the normal distribution. Although a general trend is observed in the plots, there are differences depending on the language. In particular, English shows the highest non-Gaussianity, indicating a most spiky shape. In contrast, Chinese and Hindi have the lowest non-Gaussianity and smoother shapes. It remains unclear whether these differences are languagespecific or induced by the embedding training process."
        },
        {
            "heading": "D Details of experiment in Section 5",
            "text": "D.1 Contextualized word embeddings\nDataset. We used bert-base-uncased, a pretrained BERT model from the huggingface transformers library. This model was pre-trained on the BookCorpus (Zhu et al., 2015) and English Wikipedia. Sentences from the One Billion Word Benchmark (Chelba et al., 2014) were sequentially\ninputted into BERT, generating 100,000 tokens, including [CLS] and [SEP]. Unlike static word embeddings like those from fastText, BERT yields dynamic word embeddings. Consequently, the same word can have different embeddings, labeled sequentially in their order of appearance, for instance, as sea_0, sea_1, and so on.\nPCA and ICA-transformed embeddings. Similar to the cross-lingual experiments, we computed PCA-transformed and ICA-transformed embeddings for the obtained BERT embeddings. All the axes of the transformed embeddings were adjusted to have positive skewness.\nAlignment of axes via permutation. We utilized the same English fastText embeddings used in Appendix C along with the BERT embeddings. We paired the fastText words with the BERT-labeled tokens. If the same word appeared k times in the BERT tokens, we created k pairs with that word. To adjust for this effect, the correlation coefficients were computed using the inverse frequency weight, 1/k. Given that the dimensionality of fastText embeddings is 300 while that of BERT is 768, we greedily matched pairs of axes based on the most\nsignificant correlation coefficients, thereby selecting 300 pairs of axes.\nReordering axes. Subsequently, we reordered the axes in descending order of the correlation coefficients between the aligned axes of fastText and BERT. The cross-correlation coefficients between the first 100 axes are presented in the middle panels of Fig. 3. ICA gives a strong alignment of the axes, while PCA gives a less favorable alignment.\nWord selection for visualization. We normalized each embedding to have a norm of 1. We limited BERT tokens to those included in the fastText vocabulary. We selected the top 5 words from the fastText axis based on their component values. To examine the diversity of words, duplicates in singular and plural forms of nouns were disregarded. To verify the variations in dynamic word embeddings, we randomly selected 3 BERT embeddings for each word, instead of selecting those with the three largest component values. In the heatmaps, these three BERT embeddings were placed in descending order based on the component values. We performed this process for the initial 100 axes of both the ICA-transformed and PCA-transformed embeddings, and we presented the heatmaps of the components of the embeddings when selecting 500 words from fastText and 1,500 tokens from BERT.\nAnalyzing the heatmaps. In Fig. 7a, which illustrates the ICA-transformed embeddings, we observe similar patterns to the cross-lingual experiment. In the upper panels, representing 500 words and 1,500 tokens, the components of the top 5 words and sampled 15 tokens for each axis exhibit large values. This is due to the sparsity of the remaining components, resulting in significant values along the diagonal. Moreover, in the lower panels, we can observe consistent component patterns across most pairs of axes for the 25 words and 75 tokens.\nOn axis-1 of fastText embeddings, words such as people and those are relatively ambiguous and context-dependent. Considering that even in static fastText, their component values are smaller compared to other axes, it might be more challenging for a single axis to exhibit more significant components in dynamic embeddings of BERT.\nOn axis-2 of BERT embeddings, the component value for shore_0 is nearly zero, while the component values for shore_1 and shore_2 are large. Upon reviewing the sentences containing\nthese words, shore_0 appeared in the verb phrase shore up, while shore_1 and shore_2 were used to refer to the land along the edge of a sea. Therefore, axis-2 effectively represents a consistent meaning related to ships-and-sea.\nConversely, in Fig. 7b, which illustrates PCAtransformed embeddings, we did not observe the characteristics seen in Fig. 7a, mirroring the findings from the cross-lingual scenario.\nD.2 Image embeddings\nDataset. Beyond examining the universality across different languages and distinct word embedding models, we also investigated the universality across different image models. For the 1000 classes in ImageNet (Russakovsky et al., 2015), we assembled a dataset comprising 100,000 images, randomly selecting 100 images per class. As for the image models, we chose ViT-Base (Dosovitskiy et al., 2021) which is the backbone of CLIP (Radford et al., 2021), ResMLP (Touvron et al., 2023), Swin Transformer (Liu et al., 2021), ResNet (He et al., 2016), and RegNet (Han et al., 2018). The feature embeddings of these image models were extracted from their penultimate layer. We utilized the pre-trained weights from the huggingface PyTorch Image Models (Wightman, 2019) for these investigations, and Table 1 outlines the model types, weight types, and the dimensionalities of the embeddings.\nPCA and ICA-transformed embeddings. Just as we did for fastText and BERT word embeddings, we computed PCA-transformed and ICAtransformed embeddings for image embeddings. All the axes of the transformed embeddings were adjusted to have positive skewness.\nAlignment of axes via permutation. Similar to the cross-lingual experiment, where English fastText served as the reference model, we used ViTBase as the reference model among the image models. In the process of computing the correlation coefficients between ViT-Base and another image model, the embeddings for the same image were treated as a pair. To align the axes of the ViT-Base embeddings with those of the other four image models, we employed the greedy matching approach based on the cross-correlation coefficients. Unlike the cross-lingual scenario, the image model embeddings have different dimensions. Therefore, we extracted only the axes from the\nViT-Base model that matched all the other models. As a result, there were 292 axes for the PCAtransformed embeddings and 276 axes for the ICAtransformed embeddings that were common among all the models.\nDiagnosing the axis alignment. As an example, we presented the cross-correlation coefficients between the first 100 axes of the ViT-Base and ResMLP-12 models for both the PCA-transformed and ICA-transformed embeddings in the bottom panels of Fig. 3. As observed in previous experiments, the alignment between the ViT-Base and ResMLP-12 models is evident for the ICAtransformed embeddings. However, for the PCAtransformed embeddings, the alignment appears to be less clear and more ambiguous.\nAlignment with fastText. Next, we considered the correspondence between the axes of ViT-Base embeddings and English fastText embeddings. It is important to note that the class names in ImageNet are not individual words but sentences, such as \u2018king snake, kingsnake\u2019. We parsed the class names into separate words and searched for those words in the vocabulary of English fastText, such as \u2018king\u2019 and \u2018snake\u2019. For each ImageNet class, we randomly sampled 100 images, resulting in 100 pairs for \u2018king\u2019 and 100 pairs for \u2018snake\u2019 in the case of \u2018king snake, kingsnake\u2019. If a class name did not contain any of the words present in the vocabulary, that particular class was excluded from further analysis. When calculating the cross-correlation coefficients between the axes of ViT-Base embeddings and English fastText embeddings, each image-word pair was weighted inversely proportional to its frequency. Utilizing these cross-correlation coefficients, we employed the greedy matching approach to align the axes of ViT embeddings with the axes of fastText embeddings.\nReordering axes. Lastly, we rearranged the aligned axes of ViT-Base and fastText embeddings to ensure the correlation coefficients of the aligned axes are in descending order. The axes of other image models, previously matched with ViT-Base axes, were also rearranged according to the order of the ViT-Base axes.\nAnalyzing the heatmaps. We normalized each embedding to have a norm of 1. For each axis of ViT-Base, we selected the top 5 classes that have the highest average component values. For\neach selected class, we randomly chose 3 images from the 100 images and sorted them in descending order based on the component value. The class name was parsed to find words in the English fastText vocabulary, and we selected the word with the largest component value on the corresponding axis. This process was applied to the first 100 axes for both PCA-transformed and ICA-transformed embeddings. The resulting heatmaps of the embeddings are displayed in Fig. 8.\nFrom Fig. 8a for ICA-transformed embeddings, in the case of the 1,500 images and 500 words presented in the upper panels, the 15 images and 5 words on each axis exhibit significant values on that particular axis, much like in the cross-lingual and BERT experiments. This is because other components are sparse, causing large values to appear on the diagonal. The lower panels illustrate the heatmaps of the embeddings of 75 images and 25 words corresponding to the first five axes. For instance, axis-2 of ViT-Base is oriented toward alcohol-related concepts, the component values are large for images of bottles and beers. Models such as ResMLP-12, Swin-S, and RegNetY-200MF also manifest larger component values on axis-2 for images related to alcoholic beverages. When class names are partitioned into words, words selected on axis-2 of fastText include beer, bottle, and wine. The significant component values on the corresponding axes suggest a successful alignment between the axes of the image models and fastText.\nConversely, in Fig. 8b for PCA-transformed embeddings, the correspondence of axes is not evident. Furthermore, the interpretation of the top 5 classes associated with ViT-Base axes remains unclear.\nIt is important to note that ViT-Base, extracted from the pre-trained CLIP, aligns well with other models such as ResMLP-12 and Swin-S. This observation suggests that the decent alignment of ViTBase with fastText is not merely a result of CLIP learning from both image and text."
        },
        {
            "heading": "E Details of experiment in Section 6",
            "text": "E.1 Whitened embeddings We introduce several whitening methods below. In Appendix A, we mentioned that they can be represented as\nY = ZR\nfrom the embeddings Z obtained through PCA and an orthogonal matrix R. Although these whitened embeddings do not differ in terms of performance\non tasks based on inner products, they do differ in terms of interpretability, low-dimensionality, and cross-lingual performance. This study aims to identify the inherently interpretable subspace within pre-trained embeddings and analyze their sparsity and interpretability. Consequently, the baselines were chosen with this perspective in mind. In other words, embeddings learned directly from a corpus, which incorporate explicit sparsity and interpretability objectives in their optimization, are beyond the scope of this research.\nPCA. Let \u03a3 = X\u22a4X/n be the covariance matrix of the row vectors of X. In one implementation of PCA, eigendecomposition is performed as \u03a3 = UD2U\u22a4, where U is an orthogonal matrix consisting of the eigenvectors and D2 is a diagonal matrix consisting of the eigenvalues. Using these, the transformed matrix Z is computed as8:\nZ = XUD\u22121.\nAnother implementation directly computes Z from the singular value decomposition X = ZDU\u22a4.\nICA. As mentioned in Section 3.2, ICA is represented as S = ZRica with the orthogonal matrix Rica. Thus S is whitened.\nZCA-Mahalanobis whitening. The whitening transformation that minimizes the total squared distance between X and Y is computed as:\nYzca = X\u03a3 \u22121/2,\nwhere \u03a3\u22121/2 := UD\u22121U\u22a4 (Bell and Sejnowski, 1997; Kessy et al., 2018). This can be expressed9 as Yzca = ZRzca, where Rzca = U\u22a4. Since the columns of U represent the directions of principal components, Yzca simply rescales the original X along these directions without introducing any rotation.\nCrawford-Ferguson rotation family. A family of measures for the parsimony of matrix Y is proposed by Crawford and Ferguson (1970) as f\u03ba(Y) = (1 \u2212 \u03ba) \u2211n i=1 \u2211d j=1 \u2211d k \u0338=j y 2 ijy 2 ik +\n\u03ba \u2211d\nk=1 \u2211n i=1 \u2211n j \u0338=i y 2 iky 2 jk, where 0 \u2264 \u03ba \u2264 1 is a parameter. Although initially proposed for post-processing the factor-loading matrix in factor analysis (Craw8Z = XA with A = UD\u22121 in Section 3.1. 9By noting X = ZDU\u22a4, we have Yzca = X\u03a3\u22121/2 = (ZDU\u22a4)(UD\u22121U\u22a4) = ZDD\u22121U\u22a4 = ZU\u22a4.\nford and Ferguson, 1970; Browne, 2001), this measure can be used to find an optimal R by minimizing f\u03ba(ZR), and use ZR. Different values of \u03ba correspond to different rotation methods, such as quartimax (\u03ba = 0), varimax (\u03ba = 1/n), parsimax (\u03ba = (d \u2212 1)/(n + d \u2212 2)), or factor parsimony (\u03ba = 1).\nIf Z is a whitened matrix, the resulting matrix Y = ZR is almost the same regardless of the choice of \u03ba. This is because the second term of f\u03ba(Y) satisfies \u2211d k=1 \u2211n i=1 \u2211n j \u0338=i y 2 iky 2 jk =\ndn2 \u2212 \u2211d\nk=1 \u2211n i=1 y 4 ik = dn 2(1 + Op(n \u22121))\nas the rotated matrix Y is also whitened, i.e.,\u2211n i=1 y 2 ik/n = 1. Therefore, the second term is almost constant with respect to Y, and the result of the minimization is not significantly influenced by the value of \u03ba.\nE.2 Unwhitened embeddings\nWe introduced some embeddings obtained by rotating the centered embeddings X without rescaling.\nPCA. The diagonal elements of D represent the standard deviations of X in the directions of the principal components. Simply rescaling Z by these standard deviations results in the same scaling as X, yielding\nXpca := ZD = XU.\nICA. Since S = ZRica, we define\nXica := XpcaRica = X(URica).\nIt should be noted that columns of S are intended to be independent random variables, while this is not the case for columns of Xica.\nZCA. Given that Yzca = ZU\u22a4, we define\nXzca := XpcaU \u22a4 = X,\nwhich brings us back to the original X. This explains that ZCA involves only scaling without rotation.\nCrawford-Ferguson rotation family. We simply apply the optimization procedure to X. Specifically, we find an optimal R by minimizing f\u03ba(XR), and use XR. For unwhitened matrix X, the minimization of f\u03ba(XR) depends on the value of \u03ba.\nE.3 Interpretability: word intrusion task Selection of the intruder word. Our objective is to assess the interpretability of the word embeddings Y \u2208 Rn\u00d7d, where each row vector yi \u2208 Rd corresponds to a word wi. In order to select the wintruder(a) for the set of top k words of each axis a \u2208 {1, . . . , d}, denoted as topk(a), we randomly chose a word from a pool of words that satisfy both of the following criteria simultaneously: (i) the word ranks in the lower 50% in terms of the component value on the axis a, and (ii) it ranks in the top 10% in terms of the component value on some axis other than a.\nEvaluation metric. We adopted the following metric proposed by Sun et al. (2016).\nDistRatio = 1\nd d\u2211 a=1 InterDist(a) IntraDist(a)\nIntraDist(a) = \u2211\nwi,wj\u2208topk(a) wi \u0338=wj\ndist(wi, wj)\nk(k \u2212 1)\nInterDist(a) = \u2211\nwi\u2208topk(a)\ndist(wi, wintruder(a))\nk\nIn this formula, we defined dist(wi, wj) = \u2225yi \u2212 yj\u2225. Here, IntraDist(a) denotes the average distance between the top k words, and InterDist(a) represents the average distance between the top words and the intruder word. The score is higher when the intruder word is further away from the set topk(a). Therefore, this score serves as a quantitative measure of the ability to identify the intruder word, thus it is used as a measure of the consistency of the meaning of the top k words and the interpretability of axes.\nResults for Crawford-Ferguson rotation family. Table 8 shows the DistRatio for whitened embeddings with the four different choices of \u03ba value. As we have discussed in Appendix E.1, there is no significant difference between the four rotation methods. So, we presented the result for the wellknown varimax rotation in Table 2 of Section 6.1.\nE.4 Low-dimensionality: analogy task & word similarity task\nAnalogy task. We used the Google analogy dataset (Mikolov et al., 2013a), which includes 14\ntypes of word relations for the analogy task. Each task is composed of four words that follow the relation w1 : w2 = w3 : w4. Using w1, w2 and w3, we calculated w3 + w2 \u2212 w1 and identified the top\n10 words with the highest cosine similarity to see if w4 is included in them.\nWord similarity task. We used MEN (Bruni et al., 2014), WS353 (Finkelstein et al., 2002), MTurk (Radinsky et al., 2011), RW (Luong et al., 2013), SimLex999 (Hill et al., 2015), and SimVerb3500 (Gerz et al., 2016). They provide word pairs along with human-rated similarity scores. As the evaluation metric, we used the Spearman rank correlation coefficient between the human ratings and the cosine similarity of the word embeddings.\nReducing the non-zero components. For each transformed embedding y = (y1, . . . , yd) and a specified value of k, we retained only the k most significant components of y. For example, if the components are |y1| \u2265 |y2| \u2265 \u00b7 \u00b7 \u00b7 \u2265 |yd|, then we used (y1, y2, . . . , yk, 0, . . . , 0) \u2208 Rd, where the d\u2212 k least significant components are replaced by zero.\nResults. The detailed results of the experiments presented in Section 6.2 are shown in Table 9 for\nwhitened embeddings and Tables 10, 11 for unwhitened embeddings. These results are derived from varying the number of non-zero components k, set to k = 1, 10, 100, and 300 for each embedding. The performance of a specific case in analogy and word similarity tasks is also illustrated in Fig. 17.\nFor k = 300, all the components of the 300- dimensional word vectors were used as they are. Note that the performance for k = 300 is identical for all the whitened (or unwhitened) embeddings, because the difference is only their rotations, and both analogy tasks and similarity tasks are based on the inner product of embeddings.\nAlthough there is a tendency for accuracy to decrease when reducing the number of non-zero components, it can be confirmed that the degree of decrease is smaller when using ICA compared to the other methods. The specific tasks depicted in Fig. 17 are those that achieved the highest performance at k = 300 in Table 9; capital-commoncountries has the highest top-10 accuracy 0.97 in the analogy tasks, and WS353 has the highest Spearman correlation 0.71 in the word similarity tasks.\nResults with embeddings that are transformed by the Crawford-Ferguson rotation family are shown in Table 12 for whitened embeddings and Table 13 for unwhitened embeddings. For whitened embeddings, there is no significant difference between the four rotation methods as discussed in Appendix E.1. So, we presented the results for the well-known varimax rotation in Section 6.2 and Table 9. For unwhitened embeddings, quartimax, varimax, and parsimax were similarly good. This result is consistent with the findings of Park et al. (2017). The best rotation was identified as boldface in Table 13\nfor the analogy task and word similarity task at each k, and the selected rotation method was used in Tables 10, 11.\nE.5 Cross-lingual alignment\nDatasets and visual inspection. In addition to the fastText by Grave et al. (2018) utilized in Section 4, we employed fastText by MUSE (Lample et al., 2018). We refer to these word embeddings as 157langs-fastText and MUSE-fastText, and chose some of the languages common to these two datasets. For the cross-lingual alignment task, English (EN) was designated as the source language, while Spanish (ES), French (FR), German (DE), Italian (IT), and Russian (RU) were specified as the target languages. Following the same procedure as in Appendix C, we limited the vocabulary size to 50,000 in each language. The embeddings for the six languages are visualized in Fig. 18, by applying the same procedure as in Fig. 2.\nApplying a random transformation. Note that MUSE-fastText already has pre-aligned word embeddings across languages. To resolve any such relationship across languages, we applied a random transformation to embeddings. For each embedding matrix X \u2208 Rn\u00d7d, we generated a random matrix Q \u2208 Rd\u00d7d to compute XQ. The random matrix was generated independently as\nQ = MLN,\nwhere all the elements of M,N \u2208 Rd\u00d7d and L = diag(l1, . . . , ld) \u2208 Rd\u00d7d are distributed independently. Specifically, the elements are Mij , Nij \u223c\nN (0, 1/d), the normal distribution with mean 0 and variance 1/d, and li \u223c Exp(1), the exponential distribution with mean 1. The random matrices M and N primarily induce rotation because they are roughly orthogonal matrices, while L induces random scaling.\nWord translation pairs. We established the correspondence between the embeddings of English and the embeddings of other languages in the 157langs-fastText and MUSE-fastText datasets. To accomplish this, we followed the procedure outlined in Appendix C, but now we applied it to both the train set and the test set of MUSE dictionaries. The results of applying this procedure to the five target languages are presented in Table 14, where the source language is English. The train pairs were used for training supervised baselines and also for computing cross-correlation coefficients. The test pairs were used for computing the top-1 accuracy.\nSupervised baselines. Two supervised baselines were considered to learn a linear transformation from the source embedding X to the target embedding Y. We rearranged the word embeddings so that each row of X and Y corresponds to a translation pair, i.e., the meaning of the i-th row xi corresponds to that of yi. We then computed the optimal transformation matrix W \u2208 Rd\u00d7d that solves the least squares (LS) problem (Mikolov et al., 2013b):\nmin W\u2208Rd\u00d7d\n\u2225XW \u2212Y\u222522.\nIn the optimization of the Procrustes (Proc) problem, the transformation matrix W was restricted to an orthogonal matrix. Although LS is more flexible, the performance of cross-lingual alignment can possibly be improved by Proc (Xing et al., 2015; Artetxe et al., 2016). In these supervised methods, the two embeddings X and Y underwent centering and normalization as preprocessing steps.\nCross-lingual alignment methods. We considered both supervised and unsupervised transformations for cross-lingual alignment from the source language to the target languages. In the supervised transformation methods, LS and Proc, we first trained the linear transformation using both the source and target embeddings.\nIn the unsupervised transformation methods, PCA and ICA, we first applied the transformation individually to each language and then permuted\nthe axes based on the cross-correlation (see Appendix C). Although PCA and ICA are unsupervised transformations, the axis permutation is supervised because cross-correlation coefficients are computed from the embeddings of both languages.\nEvaluation metric. For each word in the source language, we computed the transformed embedding and found the closest embedding from the target language in terms of cosine similarity. To mitigate the hubness problem, we used the CSLS\nmethod (Lample et al., 2018) instead of the standard k-NN method. The top-1 accuracy was computed as the frequency of finding the correct translation word.\nResults. The top-1 accuracy for all the five target languages is shown in Table 15, and only the average value is shown in Table 3. We used two datasets: 157langs-fastText and MUSE-fastText. Two types of embeddings were considered: one using the original word embeddings for all languages, and the other applying a random transformation to all the embeddings. The conclusions obtained in Section 6.3 regarding the average results of crosslingual alignment hold true when considering each of the target languages."
        }
    ],
    "title": "Discovering Universal Geometry in Embeddings with ICA",
    "year": 2023
}