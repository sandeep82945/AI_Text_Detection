{
    "abstractText": "Comparative Opinion Quintuple Extraction (COQE) aims to predict comparative opinion quintuples from comparative sentences. These quintuples include subject, object, shareable aspect, comparative opinion, and preference. The existing pipeline-based COQE method fails in error propagation. In addition, the complexity and insufficient amounts of annotated data hinder the performance of COQE models. In this paper, we introduce a novel approach called low-resource comparative opinion quintuple extraction by Data Augmentation with Prompting (DAP). Firstly, we present an end-to-end model architecture better suited to the data augmentation method from triplets to quintuples and can effectively avoid error propagation. Additionally, we introduce a data-centric augmentation approach that leverages the robust generative abilities of ChatGPT and integrates transfer learning techniques. Experimental results over three datasets (Camera, Car, Ele) demonstrate that our approach yields substantial improvements and achieves state-of-the-art results. The source code and data are publicly released at: https://github.com/qtxu-nlp/COQE-DAP.",
    "authors": [
        {
            "affiliations": [],
            "name": "Qingting Xu"
        },
        {
            "affiliations": [],
            "name": "Yu Hong"
        },
        {
            "affiliations": [],
            "name": "Fubang Zhao"
        },
        {
            "affiliations": [],
            "name": "Kaisong Song"
        },
        {
            "affiliations": [],
            "name": "Yangyang Kang"
        },
        {
            "affiliations": [],
            "name": "Jiaxiang Chen"
        },
        {
            "affiliations": [],
            "name": "Guodong Zhou"
        }
    ],
    "id": "SP:45b94c805c82df21d8c81a04c526fc6200b16df9",
    "references": [
        {
            "authors": [
                "Arya Aftab",
                "Alireza Morsali",
                "Shahrokh Ghaemmaghami",
                "Benoit Champagne."
            ],
            "title": "Lightsernet: A lightweight fully convolutional neural network for speech emotion recognition",
            "venue": "Proceedings",
            "year": 2022
        },
        {
            "authors": [
                "Atsushi Ando",
                "Yumiko Murata",
                "Ryo Masumura",
                "Satoshi Suzuki",
                "Naoki Makishima",
                "Takafumi Moriya",
                "Takanori Ashihara",
                "Hiroshi Sato"
            ],
            "title": "Customer satisfaction estimation using unsupervised representation learning with multi-format prediction loss",
            "year": 2022
        },
        {
            "authors": [
                "Jatin Arora",
                "Sumit Agrawal",
                "Pawan Goyal",
                "Sayan Pathak."
            ],
            "title": "Extracting entities of interest from comparative product reviews",
            "venue": "Proceedings of the 2017 ACM on Conference on Information and Knowledge Management, pages 1975\u20131978.",
            "year": 2017
        },
        {
            "authors": [
                "Zhuang Chen",
                "Tieyun Qian."
            ],
            "title": "Enhancing aspect term extraction with soft prototypes",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 2107\u20132117.",
            "year": 2020
        },
        {
            "authors": [
                "Corinna Cortes",
                "Vladimir Vapnik."
            ],
            "title": "Supportvector networks",
            "venue": "Machine learning, 20:273\u2013297.",
            "year": 1995
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "arXiv preprint arXiv:1810.04805.",
            "year": 2018
        },
        {
            "authors": [
                "Rotem Dror",
                "Gili Baumer",
                "Segev Shlomov",
                "Roi Reichart."
            ],
            "title": "The hitchhiker\u2019s guide to testing statistical significance in natural language processing",
            "venue": "Proceedings of the 56th annual meeting of the association for computational linguistics, pages",
            "year": 2018
        },
        {
            "authors": [
                "Marzieh Fadaee",
                "Arianna Bisazza",
                "Christof Monz."
            ],
            "title": "Data augmentation for low-resource neural machine translation",
            "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 567\u2013573.",
            "year": 2017
        },
        {
            "authors": [
                "Junliang Guo",
                "Xu Tan",
                "Di He",
                "Tao Qin",
                "Linli Xu",
                "Tie-Yan Liu."
            ],
            "title": "Non-autoregressive neural machine translation with enhanced decoder input",
            "venue": "Proceedings of the AAAI conference on artificial intelligence, volume 33, pages 3723\u20133730.",
            "year": 2019
        },
        {
            "authors": [
                "Nitin Jindal",
                "Bing Liu."
            ],
            "title": "Identifying comparative sentences in text documents",
            "venue": "Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval-SIGIR\u201906, pages 244\u2013251. ACM Press.",
            "year": 2006
        },
        {
            "authors": [
                "Nitin Jindal",
                "Bing Liu."
            ],
            "title": "Mining conparative sentences and relations",
            "venue": "Proceeding of 21th National Conference on Artifitial Intelligence, 2006, pages 1331\u20131336.",
            "year": 2006
        },
        {
            "authors": [
                "Douglas H Johnson."
            ],
            "title": "The insignificance of statistical significance testing",
            "venue": "The journal of wildlife management, pages 763\u2013772.",
            "year": 1999
        },
        {
            "authors": [
                "Wiltrud Kessler",
                "Jonas Kuhn"
            ],
            "title": "Detection of product comparisons-how far does an out-of-the-box semantic role labeling system take you",
            "venue": "In Proceedings of the 2013 conference on empirical methods in natural language processing,",
            "year": 2013
        },
        {
            "authors": [
                "Wiltrud Kessler",
                "Jonas Kuhn."
            ],
            "title": "A corpus of comparisons in product reviews",
            "venue": "Proceedings of the Ninth International Conference on Language Resources and Evaluation, pages 2242\u20132248.",
            "year": 2014
        },
        {
            "authors": [
                "John Lafferty",
                "Andrew McCallum",
                "Fernando CN Pereira"
            ],
            "title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data",
            "year": 2001
        },
        {
            "authors": [
                "Ziheng Liu",
                "Rui Xia",
                "Jianfei Yu."
            ],
            "title": "Comparative opinion quintuple extraction from product reviews",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3955\u20133965.",
            "year": 2021
        },
        {
            "authors": [
                "Nianzu Ma",
                "Sahisnu Mazumder",
                "Hao Wang",
                "Bing Liu."
            ],
            "title": "Entity-aware dependency-based deep graph attention network for comparative preference classification",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Lin-",
            "year": 2020
        },
        {
            "authors": [
                "Dae Hoon Park",
                "Catherine Blake."
            ],
            "title": "Identifying comparative claim sentences in full-text scientific articles",
            "venue": "Proceedings of the workshop on detecting structure in scholarly discourse, pages 1\u20139.",
            "year": 2012
        },
        {
            "authors": [
                "Kim Schouten",
                "Flavius Frasincar."
            ],
            "title": "Survey on aspect-level sentiment analysis",
            "venue": "IEEE Transactions on Knowledge and Data Engineering, pages 813\u2013 830.",
            "year": 2015
        },
        {
            "authors": [
                "Rico Sennrich",
                "Barry Haddow",
                "Alexandra Birch."
            ],
            "title": "Neural machine translation of rare words with subword units",
            "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1715\u20131725. Association for Compu-",
            "year": 2016
        },
        {
            "authors": [
                "Kang Liu. Songbo Tan"
            ],
            "title": "Overview of chinese opinion analysis evaluation",
            "year": 2013
        },
        {
            "authors": [
                "Suge Wang",
                "Hongxia Li",
                "Xiaolei Song."
            ],
            "title": "Automatic semantic role labeling for chinese comparative sentences based on hybrid patterns",
            "venue": "2010 international conference on artificial intelligence and computational intelligence, volume 1, pages 378\u2013382.",
            "year": 2010
        },
        {
            "authors": [
                "Wei Wang",
                "Tiejun Zhao",
                "Guodong Xin",
                "Yuyong Xu."
            ],
            "title": "Extraction of comparative elements using conditional random fields",
            "venue": "Acta Automatica Sinica, 41(8):1385\u20131393.",
            "year": 2015
        },
        {
            "authors": [
                "Jason Wei",
                "Kai Zou."
            ],
            "title": "Eda: Easy data augmentation techniques for boosting performance on text classification tasks",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint",
            "year": 2019
        },
        {
            "authors": [
                "Wenxuan Zhang",
                "Xin Li",
                "Yang Deng",
                "Lidong Bing",
                "Wai Lam."
            ],
            "title": "A survey on aspect-based sentiment analysis: tasks, methods, and challenges",
            "venue": "IEEE Transactions on Knowledge and Data Engineering, pages 1\u201320.",
            "year": 2022
        },
        {
            "authors": [
                "Haotian Zhu",
                "Denise Mak",
                "Jesse Gioannini",
                "Fei Xia."
            ],
            "title": "Nlpstattest: A toolkit for comparing nlp system performance",
            "venue": "Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th Interna-",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "COQE is an essential subfield of Natural Language Processing (NLP). Its primary objective is to extract five specific components from comparative sentences, namely: subject, object, shareable aspect, comparative opinion, and preference, as defined in (Liu et al., 2021). For example, in the sentence \u201cLike the viewfinder, the Nikon D80 has the same sensor as the D200.\u201d, \u201cNikon D80\u201d and \u201cD200\u201d are respectively the subject and object entities, the aspect term is \u201csensor\u201d, the opinion word is \u201csame\u201d, and the comparative preference is \u201cEqual\u201d. COQE plays a crucial role in various applications, such as comparative opinion mining (Jindal and\n\u2217 The authors contributed equally to this work and therefore are considered as co-corresponding authors.\nLiu, 2006b; Wang et al., 2010; Ma et al., 2020), sentiment analysis (Schouten and Frasincar, 2015; Zhang et al., 2022; Aftab et al., 2022), and customer satisfaction estimation (Ando et al., 2022).\nExisting pipeline-based method (Liu et al., 2021) suffers from error propagation. The heavy reliance on extensive annotated data poses a bottleneck in the training process. To address the aforementioned issues, we propose a data augmentation method with prompting for low-resource COQE. Firstly, we propose a BERT-based (Devlin et al., 2018) end-toend deep learning model as our backbone to avoid error propagation. Although existing LLMs such as ChatGPT possess rich linguistic knowledge and impressive generative capabilities, they encounter difficulties in generating satisfactory quintuple examples due to the inherent complexity of COQE. In this paper, we propose to develop a lightweight data augmentation, where the triple examples are required to be generated for augmentation, instead of the unabridged quintuple examples. It is relatively easy for ChatGPT to produce some qualified triple examples rather than quintuple. Additionally, we leverage these generated triple examples to warm up the end-to-end extraction model before training it over the benchmark quintuple dataset. To summarize, the main contributions of our work are as follows: \u2022 We introduce an end-to-end model framework to suit data augmentation methods better and avoid error propagation. Additionally, we propose a twostage data augmentation approach for low-resource COQE, leveraging the generative capabilities of ChatGPT and the transfer learning method. \u2022 Experimental results demonstrate that our approach yields substantial improvements compared to the baseline and the current state-of-the-art model, resulting in a new highest performance on three COQE datasets. Furthermore, we conduct further analyses and supplementary experiments to verify the effectiveness of our approach."
        },
        {
            "heading": "2 Approach",
            "text": ""
        },
        {
            "heading": "2.1 Task Definition",
            "text": "COQE aims to predict quintuples {sub, obj, asp, op, pre} from the given sentence X = {x1, x2, ..., xn} (n denotes the number of tokens). sub and obj represent entities that serve as the subject and object in a comparison relation, respectively. asp refers to the shareable aspect term of two compared entities. Additionally, op denotes comments or opinions expressed regarding the aspect term of subject and object entities. There are four categories of preference (pre) to be considered, including Better, Equal, Different and Worse."
        },
        {
            "heading": "2.2 End-to-end Model",
            "text": "We utilize BERT (Devlin et al., 2018) as the sentence encoder. Using a BPE tokenizer (Sennrich et al., 2016), we obtain context-aware representations for each token in the input sentence X .\nH = BERT (X) (1)\nWe employ a non-autoregressive (Guo et al., 2019) decoder to generate the quintuples, which remove the dependence on previous target tokens from the input of the decoder. Specifically, we randomly initialize a vector representation Q to represent a quintuple. In each decoder layer, we update the representation of Q using the formula (2). In this paper, we utilize a l-layer transformer decoder for non-autoregressive generation, and in our experiments, we set l to 3.\nQl = Decoder(H,Ql\u22121) (2)\nGiven the output of the final decoder layer, we employ a classifier and four pointer networks to extract quintuples. Each pointer network is responsible for identifying the start and end position of one element within a quadruple. We calculate the classification probabilities using formula (3) and\nthe extraction probabilities for the quadruple using formula (4).\npc = softmax(WcQl + b) (3)\npei = softmax(V \u22a4 i tanh(Weq l i +WhH)) (4)\nwhere Wc, We, Wh, b and V are all trainable parameters. qli is the i-th embedding output by the final decoder layer.\nWe optimize the combined objective function during training. Ltotal comprises classification and extraction loss using the cross-entry loss function.\nLtotal = N\u2211\nn=1\n( log pc +\nK=8\u2211 k=1 log pek\n) (5)\nwhere N denotes the number of initialized Q, K denotes the number of loss functions computations required for a quadruple, which is equal to 8."
        },
        {
            "heading": "2.3 Data Augmentation for Transfer",
            "text": "Currently, there is a lack of datasets that are specifically annotated for subject, object and aspect in comparative sentences. In this paper, we introduce a data-centric method to leverage the rich linguistic knowledge within ChatGPT and further enhance COQE performance. ChatGPT generates a dataset containing triplets {sub, obj, asp}. In this section, we take the Camera dataset as an example. A multi-stage approach is needed to generate proper sentences for automatic annotation in the dataset. Building the triplet dataset involves four steps, as depicted in Figure 1. \u2022 Obtaining Aspect Terms Firstly, we count the unique aspect terms separately from each dataset. The number of distinct aspect terms for three datasets is provided in Table 2. \u2022 Generating Triplets We generate triplets based on the aspect terms obtained from step 1. Specifically, we invoke the ChatGPT API and design an appropriate prompt to generate three triplets.\n\u2022 Generating Sentences Based on the statistics from the Camera dataset, 221 sentences contain triplets {sub, obj, asp}. In contrast, there are 202 sentences with only binary combinations such as {sub, obj}, 139 sentences with {sub, asp}, and 52 sentences with {obj, asp}. To ensure diversity in the generated sentences by ChatGPT, we prioritize the first three scenarios. We design specific prompts for each scenario. For example, in the first scenario, we establish the prompt as shown in Table 1. \u2022 Data Filtering and Processing Despite the provided constraints, ChatGPT may still generate some samples that do not meet the specifications. Therefore, before automatic labeling, matching the generated sentences with the given triplets is essential. Only the sentences that successfully match the triplets should be labeled automatically.\nWe train the backbone model using newly constructed triplet data to obtain feature representations. Subsequently, we employ transfer learning techniques to fine-tune the gold quintuples based on the obtained representations."
        },
        {
            "heading": "3 Experiments",
            "text": ""
        },
        {
            "heading": "3.1 Datasets and Evaluation Metrics",
            "text": "Datasets We evaluate our method on three datasets. Camera is an English corpus (Liu et al., 2021). It builds upon the prior work of Kessler et al.(Kessler and Kuhn, 2014) by providing additional annotation for comparative sentences with comparative opinions and preferences. Besides, Liu et al. (Liu et al., 2021) construct two Chinese datasets specifically designed for comparative opinion quintuple extraction. They extend the COAE (Songbo Tan, 2013) dataset by building upon it and providing ad-\nditional annotations for data points regarding comparative opinions and preferences. The statistics of the three datasets are shown in Table 2.\nEvaluation Metrics We evaluate all models through Precision (P ), Recall (R), and F1 metrics. Additionally, we employ three matching strategies to evaluate prediction performance: exact-match (Ex), proportional-match (Pr) and binary-math (Bi) evaluation. The details of the three matching strategies are as follows:\nEx = { 0 \u2203 (pi \u0338= gi) 1 otherwise\n(6)\nPr =\n{ 0 \u2203 (gi \u2229 pi = \u2205)\u2211\ni len(gi\u2229pi)\u2211 i len(gi) otherwise (7)\nBi = { 0 \u2203 (gi \u2229 pi = \u2205) 1 otherwise\n(8)\nwhere, gi and pi denote the i-th element in the gold and predicted quintuple result, respectively. The index i ranges from 1 to 5. We report the average performance based on three runs, utilizing shuffled random seeds for each run."
        },
        {
            "heading": "3.2 Compared Models",
            "text": "We compare with the following models: MSSVM+CRF first propose comparative sentence identification. They utilize SVM (Cortes and Vapnik, 1995) for identifying comparative sentences and CRF (Lafferty et al., 2001) for extracting comparative elements (Jindal and Liu, 2006a).\nMSCRF employ a CRF-based model for comparative sentence identification and comparative element extraction (Wang et al., 2015).\nMSLSTM introduce a multi-stage framework utilizing LSTM as the text encoder. Firstly, they identify comparative sentences and extract comparative elements. Subsequently, they combine and filter these elements. Finally, they classify valid quadruples into four categories Liu et al. (2021).\nMSBERT is a a modified version of MSLSTM. For MSBERT, Liu et al. (2021) choose BERT as the model\u2019s text encoder."
        },
        {
            "heading": "3.3 Main Results",
            "text": "We show the performance over three test sets in Table 3. It can be observed that DAP yields substantial improvements on three datasets compared to all the current SoTA or our backbone (E2E). In particular, even E2E is superior to the performance of MSBERT . This also proves that the end-to-end model can effectively avoid error propagation. Besides, compared to the current SOTA results, DAP leads to F1 score improvements of 7.70%, 6.38%, and 8.51% on the Camera, Car, and Ele datasets, respectively. This shows that the performance of COQE can be effectively improved by introducing external knowledge contained in LLM."
        },
        {
            "heading": "3.4 Cross-Domain Experiments",
            "text": "We follow Liu et al. (2021) to evaluate the crossdomain generalization ability of our method. We conduct cross-domain experiments on two Chinese datasets, where cross-domain refers to using training and validation sets from the source domain (SOU) and a test set from the target domain (TAR). In Table 4, \u201cEle \u2192 Car\u201d denotes electronic domain serves as the SOU, car domain is the TAR.\nIt can be observed that our method DAP yields\nsuperior cross-domain experimental results surpassing previous COQE approaches, owing to our approach\u2019s generalization of data transfer."
        },
        {
            "heading": "3.5 Significance Test",
            "text": "We perform a statistical significance test (abbr., SST) to validate the reliability of our method. The sampling-based P-value (Johnson, 1999) is used as the metric for measuring significance levels. On the other hand, to provide a comprehensive insight into the significance, we conduct the practical significance test (abbr., PST) (Zhu et al., 2020), which is more reliable than SST. Cohen\u2019s D-value is used as the metric of PST. It is noteworthy that, in SST, the reported P-value below 0.05 (i.e., 5.0E-02) indicates significant improvement, otherwise insignificant (Dror et al., 2018). Similarly, in PST, the reported Cohen\u2019s D-value exceeds 1 indicates a significant improvement.\nThe results of the SST and PST are presented in Table 5. It can be found that the P-values of SST are lower than the threshold, while Cohen\u2019s D-value of PST is higher than the threshold. This demonstrates that DAP yields significant improvements."
        },
        {
            "heading": "4 Related Work",
            "text": ""
        },
        {
            "heading": "4.1 Comparative Sentence Analysis",
            "text": "Comparison-oriented information extraction has attracted considerable research interest. Jindal and Liu (2006a) first introduce the concept of comparative sentences and implement comparative sentence discrimination based on rules and SVM (Cortes and Vapnik, 1995). Park and Blake (2012) explore an extensive set of syntactic and semantic features, and employ three different classifiers to identify comparative sentences. The recent studies concentrate on fine-grained component analysis and parsing upon comparative sentences (Kessler and Kuhn, 2013; Arora et al., 2017; Ma et al., 2020). In particular, Liu et al. (2021) propose a novel task called comparative opinion quintuple extraction, which aims to extract quintuples from the given comparative sentences."
        },
        {
            "heading": "4.2 Data Augmentation",
            "text": "Data augmentation is a technique that expands and diversifies training datasets by applying various transformation or modification approaches to the existing data. Fadaee et al. (2017) use backtranslation as a data augmentation method for generating synthetic parallel sentences, thereby enhancing the performance of low-resource neural machine translation systems. Wei and Zou (2019) use synonym replacement, random insertion, random deletion and synonym swapping to increase the diversity of training data. The approaches contribute to enhancing the robustness of text classifier. Chen and Qian (2020) develop a prototype generator for data augmentation, where internal and external prototypes are adopted."
        },
        {
            "heading": "5 Conclusion and Future Work",
            "text": "To avoid error propagation, we design an end-toend model. Additionally, we propose a data-centric augmentation approach using the powerful generative capability of ChatGPT. The performance on three datasets achieves SoTA. Future work will focus on integrating existing annotated triplet data with automatically generated domain-specific data.\nLimitations\nDespite achieving a new state-of-the-art performance, our model still has several limitations. One obvious limitation of the method is that the original three data sets contain a certain proportion of multiple comparison sentences, making predictions more difficult. In this paper, we only improve performance by introducing external knowledge and extracting the quintuple from the difficult to the easy. Future work can concentrate on tackling the COQE problem specifically from the perspective of multiple comparative sentences"
        },
        {
            "heading": "Acknowledgements",
            "text": "The research is supported by National Key R&D Program of China (2020YFB1313601), National Natural Science Foundation of China (62376182, 62076174, 62106039)."
        }
    ],
    "title": "Low-Resource Comparative Opinion Quintuple Extraction by Data Augmentation with Prompting",
    "year": 2023
}