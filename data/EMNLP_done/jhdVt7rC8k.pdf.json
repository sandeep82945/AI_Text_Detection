{
    "abstractText": "Large Language Models (LLMs) have shown remarkable performances on a wide range of natural language understanding and generation tasks. We observe that the LLMs provide effective priors in exploiting linguistic shortcuts for temporal and causal reasoning in Video Question Answering (VideoQA). However, such priors often cause suboptimal results on VideoQA by leading the model to overrely on questions, i.e., linguistic bias, while ignoring visual content. This is also known as \u2018ungrounded guesses\u2019 or \u2018hallucinations\u2019. To address this problem while leveraging LLMs\u2019 prior on VideoQA, we propose a novel framework, Flipped-VQA, encouraging the model to predict all the combinations of \u27e8V, Q, A\u27e9 triplet by flipping the source pair and the target label to understand their complex relationships, i.e., predict A, Q, and V given a VQ, VA, and QA pairs, respectively. In this paper, we develop LLaMA-VQA by applying Flipped-VQA to LLaMA, and it outperforms both LLMs-based and non-LLMs-based models on five challenging VideoQA benchmarks. Furthermore, our Flipped-VQA is a general framework that is applicable to various LLMs (OPT and GPT-J) and consistently improves their performances. We empirically demonstrate that Flipped-VQA not only enhances the exploitation of linguistic shortcuts but also mitigates the linguistic bias, which causes incorrect answers over-relying on the question. Code is available at https: //github.com/mlvlab/Flipped-VQA.",
    "authors": [
        {
            "affiliations": [],
            "name": "Dohwan Ko"
        },
        {
            "affiliations": [],
            "name": "Ji Soo Lee"
        },
        {
            "affiliations": [],
            "name": "Wooyoung Kang"
        },
        {
            "affiliations": [],
            "name": "Byungseok Roh"
        },
        {
            "affiliations": [],
            "name": "Hyunwoo J. Kim"
        }
    ],
    "id": "SP:59cd5d3f8607263c26af24fdfae6843f23689ed4",
    "references": [
        {
            "authors": [
                "Jean-Baptiste Alayrac",
                "Jeff Donahue",
                "Pauline Luc",
                "Antoine Miech",
                "Iain Barr",
                "Yana Hasson",
                "Karel Lenc",
                "Arthur Mensch",
                "Katherine Millican",
                "Malcolm Reynolds"
            ],
            "title": "Flamingo: a visual language model for few-shot learning",
            "venue": "NeurIPS",
            "year": 2022
        },
        {
            "authors": [
                "Shyamal Buch",
                "Crist\u00f3bal Eyzaguirre",
                "Adrien Gaidon",
                "Jiajun Wu",
                "Li Fei-Fei",
                "Juan Carlos Niebles."
            ],
            "title": "Revisiting the\" video\" in video-language understanding",
            "venue": "CVPR.",
            "year": 2022
        },
        {
            "authors": [
                "Remi Cadene",
                "Corentin Dancette",
                "Matthieu Cord",
                "Devi Parikh"
            ],
            "title": "Rubi: Reducing unimodal biases for visual question answering",
            "venue": "NeurIPS",
            "year": 2019
        },
        {
            "authors": [
                "Shanqing Cai",
                "Subhashini Venugopalan",
                "Katrin Tomanek",
                "Ajit Narayanan",
                "Meredith R Morris",
                "Michael P Brenner."
            ],
            "title": "Context-aware abbreviation expansion using large language models",
            "venue": "NAACL.",
            "year": 2022
        },
        {
            "authors": [
                "Jiangjie Chen",
                "Wei Shi",
                "Ziquan Fu",
                "Sijie Cheng",
                "Lei Li",
                "Yanghua Xiao."
            ],
            "title": "Say what you mean! large language models speak too positively about negative commonsense knowledge",
            "venue": "ACL.",
            "year": 2023
        },
        {
            "authors": [
                "Mark Chen",
                "Jerry Tworek",
                "Heewoo Jun",
                "Qiming Yuan",
                "Henrique Ponde de Oliveira Pinto",
                "Jared Kaplan",
                "Harri Edwards",
                "Yuri Burda",
                "Nicholas Joseph",
                "Greg Brockman"
            ],
            "title": "Evaluating large language models trained on code",
            "year": 2021
        },
        {
            "authors": [
                "Feng Cheng",
                "Xizi Wang",
                "Jie Lei",
                "David Crandall",
                "Mohit Bansal",
                "Gedas Bertasius."
            ],
            "title": "Vindlu: A recipe for effective video-and-language pretraining",
            "venue": "CVPR.",
            "year": 2023
        },
        {
            "authors": [
                "Seongho Choi",
                "Kyoung-Woon On",
                "Yu-Jung Heo",
                "Ahjeong Seo",
                "Youwon Jang",
                "Minsu Lee",
                "ByoungTak Zhang."
            ],
            "title": "Dramaqa: Character-centered video story understanding with hierarchical qa",
            "venue": "AAAI.",
            "year": 2021
        },
        {
            "authors": [
                "John Joon Young Chung",
                "Ece Kamar",
                "Saleema Amershi."
            ],
            "title": "Increasing diversity while maintaining accuracy: Text data generation with large language models and human interventions",
            "venue": "ACL.",
            "year": 2023
        },
        {
            "authors": [
                "Peter Clark",
                "Isaac Cowhey",
                "Oren Etzioni",
                "Tushar Khot",
                "Ashish Sabharwal",
                "Carissa Schoenick",
                "Oyvind Tafjord."
            ],
            "title": "Think you have solved question answering? try arc, the ai2 reasoning challenge",
            "venue": "arXiv preprint arXiv:1803.05457.",
            "year": 2018
        },
        {
            "authors": [
                "Karl Cobbe",
                "Vineet Kosaraju",
                "Mohammad Bavarian",
                "Mark Chen",
                "Heewoo Jun",
                "Lukasz Kaiser",
                "Matthias Plappert",
                "Jerry Tworek",
                "Jacob Hilton",
                "Reiichiro Nakano"
            ],
            "title": "Training verifiers to solve math word problems",
            "venue": "arXiv preprint arXiv:2110.14168",
            "year": 2021
        },
        {
            "authors": [
                "Yu Fei",
                "Yifan Hou",
                "Zeming Chen",
                "Antoine Bosselut."
            ],
            "title": "Mitigating label biases for in-context learning",
            "venue": "ACL.",
            "year": 2023
        },
        {
            "authors": [
                "Difei Gao",
                "Luowei Zhou",
                "Lei Ji",
                "Linchao Zhu",
                "Yi Yang",
                "Mike Zheng Shou."
            ],
            "title": "Mist: Multi-modal iterative spatial-temporal transformer for long-form video question answering",
            "venue": "CVPR.",
            "year": 2023
        },
        {
            "authors": [
                "Dan Hendrycks",
                "Collin Burns",
                "Steven Basart",
                "Andy Zou",
                "Mantas Mazeika",
                "Dawn Song",
                "Jacob Steinhardt."
            ],
            "title": "Measuring massive multitask language understanding",
            "venue": "ICLR.",
            "year": 2021
        },
        {
            "authors": [
                "Namgyu Ho",
                "Laura Schmid",
                "Se-Young Yun."
            ],
            "title": "Large language models are reasoning teachers",
            "venue": "ACL.",
            "year": 2022
        },
        {
            "authors": [
                "Marius Hobbhahn",
                "Tom Lieberum",
                "David Seiler."
            ],
            "title": "Investigating causal understanding in llms",
            "venue": "NeurIPS Workshop.",
            "year": 2022
        },
        {
            "authors": [
                "Jordan Hoffmann",
                "Sebastian Borgeaud",
                "Arthur Mensch",
                "Elena Buchatskaya",
                "Trevor Cai",
                "Eliza Rutherford",
                "Diego de Las Casas",
                "Lisa Anne Hendricks",
                "Johannes Welbl",
                "Aidan Clark"
            ],
            "title": "Training compute-optimal large language models",
            "year": 2022
        },
        {
            "authors": [
                "Edward J Hu",
                "Yelong Shen",
                "Phillip Wallis",
                "Zeyuan Allen-Zhu",
                "Yuanzhi Li",
                "Shean Wang",
                "Lu Wang",
                "Weizhu Chen."
            ],
            "title": "Lora: Low-rank adaptation of large language models",
            "venue": "ICLR.",
            "year": 2021
        },
        {
            "authors": [
                "Yunseok Jang",
                "Yale Song",
                "Youngjae Yu",
                "Youngjin Kim",
                "Gunhee Kim."
            ],
            "title": "Tgif-qa: Toward spatiotemporal reasoning in visual question answering",
            "venue": "CVPR.",
            "year": 2017
        },
        {
            "authors": [
                "Menglin Jia",
                "Luming Tang",
                "Bor-Chun Chen",
                "Claire Cardie",
                "Serge Belongie",
                "Bharath Hariharan",
                "SerNam Lim."
            ],
            "title": "Visual prompt tuning",
            "venue": "ECCV.",
            "year": 2022
        },
        {
            "authors": [
                "Dongfu Jiang",
                "Xiang Ren",
                "Bill Yuchen Lin."
            ],
            "title": "Llm-blender: Ensembling large language models with pairwise ranking and generative fusion",
            "venue": "ACL.",
            "year": 2023
        },
        {
            "authors": [
                "Pin Jiang",
                "Yahong Han."
            ],
            "title": "Reasoning with heterogeneous graph alignment for video question answering",
            "venue": "AAAI.",
            "year": 2020
        },
        {
            "authors": [
                "Yao Jin",
                "Guocheng Niu",
                "Xinyan Xiao",
                "Jian Zhang",
                "Xi Peng",
                "Jun Yu."
            ],
            "title": "Knowledge-constrained answer generation for open-ended video question answering",
            "venue": "AAAI.",
            "year": 2023
        },
        {
            "authors": [
                "Chen Ju",
                "Tengda Han",
                "Kunhao Zheng",
                "Ya Zhang",
                "Weidi Xie."
            ],
            "title": "Prompting visual-language models for efficient video understanding",
            "venue": "ECCV.",
            "year": 2022
        },
        {
            "authors": [
                "Ehsan Kamalloo",
                "Nouha Dziri",
                "Charles LA Clarke",
                "Davood Rafiei."
            ],
            "title": "Evaluating open-domain question answering in the era of large language models",
            "venue": "ACL.",
            "year": 2023
        },
        {
            "authors": [
                "Emre K\u0131c\u0131man",
                "Robert Ness",
                "Amit Sharma",
                "Chenhao Tan."
            ],
            "title": "Causal reasoning and large language models: Opening a new frontier for causality",
            "venue": "arXiv preprint arXiv:2305.00050.",
            "year": 2023
        },
        {
            "authors": [
                "Seonhoon Kim",
                "Seohyeong Jeong",
                "Eunbyul Kim",
                "Inho Kang",
                "Nojun Kwak."
            ],
            "title": "Self-supervised pretraining and contrastive representation learning for multiple-choice video qa",
            "venue": "AAAI.",
            "year": 2021
        },
        {
            "authors": [
                "Dohwan Ko",
                "Ji Soo Lee",
                "Miso Choi",
                "Jaewon Chu",
                "Jihwan Park",
                "Hyunwoo J Kim."
            ],
            "title": "Openvocabulary video question answering: A new benchmark for evaluating the generalizability of video question answering models",
            "venue": "ICCV.",
            "year": 2023
        },
        {
            "authors": [
                "Thao Minh Le",
                "Vuong Le",
                "Svetha Venkatesh",
                "Truyen Tran."
            ],
            "title": "Hierarchical conditional relation networks for multimodal video question answering",
            "venue": "IJCV.",
            "year": 2021
        },
        {
            "authors": [
                "Jie Lei",
                "Tamara L Berg",
                "Mohit Bansal."
            ],
            "title": "Revealing single frame bias for video-and-language learning",
            "venue": "ACL.",
            "year": 2023
        },
        {
            "authors": [
                "Jie Lei",
                "Licheng Yu",
                "Mohit Bansal",
                "Tamara L Berg."
            ],
            "title": "Tvqa: Localized, compositional video question answering",
            "venue": "EMNLP.",
            "year": 2018
        },
        {
            "authors": [
                "Jie Lei",
                "Licheng Yu",
                "Tamara L Berg",
                "Mohit Bansal."
            ],
            "title": "What is more likely to happen next? videoand-language future event prediction",
            "venue": "EMNLP.",
            "year": 2020
        },
        {
            "authors": [
                "Junnan Li",
                "Dongxu Li",
                "Silvio Savarese",
                "Steven Hoi."
            ],
            "title": "Blip-2: Bootstrapping language-image pretraining with frozen image encoders and large language models",
            "venue": "ICML.",
            "year": 2023
        },
        {
            "authors": [
                "KunChang Li",
                "Yinan He",
                "Yi Wang",
                "Yizhuo Li",
                "Wenhai Wang",
                "Ping Luo",
                "Yali Wang",
                "Limin Wang",
                "Yu Qiao."
            ],
            "title": "Videochat: Chat-centric video understanding",
            "venue": "arXiv preprint arXiv:2305.06355.",
            "year": 2023
        },
        {
            "authors": [
                "Kunchang Li",
                "Yali Wang",
                "Yizhuo Li",
                "Yi Wang",
                "Yinan He",
                "Limin Wang",
                "Yu Qiao."
            ],
            "title": "Unmasked teacher: Towards training-efficient video foundation models",
            "venue": "ICCV.",
            "year": 2023
        },
        {
            "authors": [
                "Lei Li",
                "Yuwei Yin",
                "Shicheng Li",
                "Liang Chen",
                "Peiyi Wang",
                "Shuhuai Ren",
                "Mukai Li",
                "Yazheng Yang",
                "Jingjing Xu",
                "Xu Sun"
            ],
            "title": "2023d. M3it: A largescale dataset towards multi-modal multilingual instruction tuning",
            "venue": "arXiv preprint arXiv:2306.04387",
            "year": 2023
        },
        {
            "authors": [
                "Xiang Lorraine Li",
                "Adhiguna Kuncoro",
                "Jordan Hoffmann",
                "Cyprien de Masson d\u2019Autume",
                "Phil Blunsom",
                "Aida Nematzadeh"
            ],
            "title": "A systematic investigation of commonsense knowledge in large language models",
            "year": 2022
        },
        {
            "authors": [
                "Xingxuan Li",
                "Liying Cheng",
                "Qingyu Tan",
                "Hwee Tou Ng",
                "Shafiq Joty",
                "Lidong Bing."
            ],
            "title": "Unlocking temporal question answering for large language models using code execution",
            "venue": "arXiv preprint arXiv:2305.15014.",
            "year": 2023
        },
        {
            "authors": [
                "Hanmeng Liu",
                "Ruoxi Ning",
                "Zhiyang Teng",
                "Jian Liu",
                "Qiji Zhou",
                "Yue Zhang."
            ],
            "title": "Evaluating the logical reasoning ability of chatgpt and gpt-4",
            "venue": "arXiv preprint arXiv:2304.03439.",
            "year": 2023
        },
        {
            "authors": [
                "Stephanie Long",
                "Tibor Schuster",
                "Alexandre Pich\u00e9",
                "ServiceNow Research"
            ],
            "title": "Can large language models build causal graphs? arXiv preprint arXiv:2303.05279",
            "year": 2023
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter."
            ],
            "title": "Decoupled weight decay regularization",
            "venue": "arXiv preprint arXiv:1711.05101.",
            "year": 2017
        },
        {
            "authors": [
                "Yulei Niu",
                "Kaihua Tang",
                "Hanwang Zhang",
                "Zhiwu Lu",
                "Xian-Sheng Hua",
                "Ji-Rong Wen."
            ],
            "title": "Counterfactual vqa: A cause-effect look at language bias",
            "venue": "CVPR.",
            "year": 2021
        },
        {
            "authors": [
                "Aaron van den Oord",
                "Yazhe Li",
                "Oriol Vinyals."
            ],
            "title": "Representation learning with contrastive predictive coding",
            "venue": "arXiv preprint arXiv:1807.03748.",
            "year": 2018
        },
        {
            "authors": [
                "Batu Ozturkler",
                "Nikolay Malkin",
                "Zhen Wang",
                "Nebojsa Jojic."
            ],
            "title": "Thinksum: Probabilistic reasoning over sets using large language models",
            "venue": "ACL.",
            "year": 2023
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark"
            ],
            "title": "Learning transferable visual models from natural language supervision",
            "venue": "In ICML",
            "year": 2021
        },
        {
            "authors": [
                "Sainandan Ramakrishnan",
                "Aishwarya Agrawal",
                "Stefan Lee."
            ],
            "title": "Overcoming language priors in visual question answering with adversarial regularization",
            "venue": "NeurIPS.",
            "year": 2018
        },
        {
            "authors": [
                "Swarnadeep Saha",
                "Peter Hase",
                "Nazneen Rajani",
                "Mohit Bansal."
            ],
            "title": "Are hard examples also harder to explain? a study with human and model-generated explanations",
            "venue": "EMNLP.",
            "year": 2022
        },
        {
            "authors": [
                "Keisuke Sakaguchi",
                "Ronan Le Bras",
                "Chandra Bhagavatula",
                "Yejin Choi."
            ],
            "title": "Winogrande: An adversarial winograd schema challenge at scale",
            "venue": "Communications of the ACM.",
            "year": 2021
        },
        {
            "authors": [
                "Qingyu Tan",
                "Hwee Tou Ng",
                "Lidong Bing."
            ],
            "title": "Towards benchmarking and improving the temporal reasoning capability of large language models",
            "venue": "ACL.",
            "year": 2023
        },
        {
            "authors": [
                "Hugo Touvron",
                "Thibaut Lavril",
                "Gautier Izacard",
                "Xavier Martinet",
                "Marie-Anne Lachaux",
                "Timoth\u00e9e Lacroix",
                "Baptiste Rozi\u00e8re",
                "Naman Goyal",
                "Eric Hambro",
                "Faisal Azhar"
            ],
            "title": "Llama: Open and efficient foundation language models",
            "year": 2023
        },
        {
            "authors": [
                "Ben Wang",
                "Aran Komatsuzaki."
            ],
            "title": "GPT-J6B: A 6 Billion Parameter Autoregressive Language Model",
            "venue": "https://github.com/kingoflolz/ mesh-transformer-jax.",
            "year": 2021
        },
        {
            "authors": [
                "Jinpeng Wang",
                "Yixiao Ge",
                "Rui Yan",
                "Yuying Ge",
                "Kevin Qinghong Lin",
                "Satoshi Tsutsui",
                "Xudong Lin",
                "Guanyu Cai",
                "Jianping Wu",
                "Ying Shan"
            ],
            "title": "2023a. All in one: Exploring unified video-language pretraining",
            "year": 2023
        },
        {
            "authors": [
                "Lei Wang",
                "Wanyu Xu",
                "Yihuai Lan",
                "Zhiqiang Hu",
                "Yunshi Lan",
                "Roy Ka-Wei Lee",
                "Ee-Peng Lim."
            ],
            "title": "Plan-and-solve prompting: Improving zeroshot chain-of-thought reasoning by large language models",
            "venue": "ACL.",
            "year": 2023
        },
        {
            "authors": [
                "Yi Wang",
                "Kunchang Li",
                "Yizhuo Li",
                "Yinan He",
                "Bingkun Huang",
                "Zhiyu Zhao",
                "Hongjie Zhang",
                "Jilan Xu",
                "Yi Liu",
                "Zun Wang"
            ],
            "title": "Internvideo: General video foundation models via generative and discriminative learning. arXiv preprint arXiv:2212.03191",
            "year": 2022
        },
        {
            "authors": [
                "Yiming Wang",
                "Zhuosheng Zhang",
                "Rui Wang."
            ],
            "title": "Element-aware summarization with large language models: Expert-aligned evaluation and chain-ofthought method",
            "venue": "ACL.",
            "year": 2023
        },
        {
            "authors": [
                "Bo Wu",
                "Shoubin Yu",
                "Zhenfang Chen",
                "Joshua B Tenenbaum",
                "Chuang Gan."
            ],
            "title": "Star: A benchmark for situated reasoning in real-world videos",
            "venue": "NeurIPS.",
            "year": 2021
        },
        {
            "authors": [
                "Junbin Xiao",
                "Xindi Shang",
                "Angela Yao",
                "Tat-Seng Chua."
            ],
            "title": "Next-qa: Next phase of questionanswering to explaining temporal actions",
            "venue": "CVPR.",
            "year": 2021
        },
        {
            "authors": [
                "Junbin Xiao",
                "Pan Zhou",
                "Tat-Seng Chua",
                "Shuicheng Yan."
            ],
            "title": "Video graph transformer for video question answering",
            "venue": "ECCV.",
            "year": 2022
        },
        {
            "authors": [
                "Dejing Xu",
                "Zhou Zhao",
                "Jun Xiao",
                "Fei Wu",
                "Hanwang Zhang",
                "Xiangnan He",
                "Yueting Zhuang."
            ],
            "title": "Video question answering via gradually refined attention over appearance and motion",
            "venue": "ACM Multimedia.",
            "year": 2017
        },
        {
            "authors": [
                "Antoine Yang",
                "Antoine Miech",
                "Josef Sivic",
                "Ivan Laptev",
                "Cordelia Schmid."
            ],
            "title": "Just ask: Learning to answer questions from millions of narrated videos",
            "venue": "ICCV.",
            "year": 2021
        },
        {
            "authors": [
                "Antoine Yang",
                "Antoine Miech",
                "Josef Sivic",
                "Ivan Laptev",
                "Cordelia Schmid."
            ],
            "title": "Zero-shot video question answering via frozen bidirectional language models",
            "venue": "NeurIPS.",
            "year": 2022
        },
        {
            "authors": [
                "Qinghao Ye",
                "Guohai Xu",
                "Ming Yan",
                "Haiyang Xu",
                "Qi Qian",
                "Ji Zhang",
                "Fei Huang."
            ],
            "title": "Hitea: Hierarchical temporal-aware video-language pre-training",
            "venue": "arXiv preprint arXiv:2212.14546.",
            "year": 2022
        },
        {
            "authors": [
                "Shoubin Yu",
                "Jaemin Cho",
                "Prateek Yadav",
                "Mohit Bansal."
            ],
            "title": "Self-chained image-language model for video localization and question answering",
            "venue": "arXiv preprint arXiv:2305.06988.",
            "year": 2023
        },
        {
            "authors": [
                "Weijiang Yu",
                "Haoteng Zheng",
                "Mengfei Li",
                "Lei Ji",
                "Lijun Wu",
                "Nong Xiao",
                "Nan Duan."
            ],
            "title": "Learning from inside: Self-driven siamese sampling and reasoning for video question answering",
            "venue": "NeurIPS.",
            "year": 2021
        },
        {
            "authors": [
                "Zhou Yu",
                "Dejing Xu",
                "Jun Yu",
                "Ting Yu",
                "Zhou Zhao",
                "Yueting Zhuang",
                "Dacheng Tao."
            ],
            "title": "Activitynet-qa: A dataset for understanding complex web videos via question answering",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence.",
            "year": 2019
        },
        {
            "authors": [
                "Rowan Zellers",
                "Ari Holtzman",
                "Yonatan Bisk",
                "Ali Farhadi",
                "Yejin Choi"
            ],
            "title": "Hellaswag: Can a machine really finish your sentence",
            "year": 2019
        },
        {
            "authors": [
                "Rowan Zellers",
                "Ximing Lu",
                "Jack Hessel",
                "Youngjae Yu",
                "Jae Sung Park",
                "Jize Cao",
                "Ali Farhadi",
                "Yejin Choi."
            ],
            "title": "Merlot: Multimodal neural script knowledge models",
            "venue": "NeurIPS.",
            "year": 2021
        },
        {
            "authors": [
                "Cheng Zhang",
                "Stefan Bauer",
                "Paul Bennett",
                "Jiangfeng Gao",
                "Wenbo Gong",
                "Agrin Hilmkil",
                "Joel Jennings",
                "Chao Ma",
                "Tom Minka",
                "Nick Pawlowski"
            ],
            "title": "Understanding causality with large language models: Feasibility and opportunities",
            "year": 2023
        },
        {
            "authors": [
                "Hang Zhang",
                "Xin Li",
                "Lidong Bing."
            ],
            "title": "Videollama: An instruction-tuned audio-visual language model for video understanding",
            "venue": "arXiv preprint arXiv:2306.02858.",
            "year": 2023
        },
        {
            "authors": [
                "Michael JQ Zhang",
                "Eunsol Choi."
            ],
            "title": "Situatedqa: Incorporating extra-linguistic contexts into qa",
            "venue": "EMNLP.",
            "year": 2021
        },
        {
            "authors": [
                "Renrui Zhang",
                "Jiaming Han",
                "Aojun Zhou",
                "Xiangfei Hu",
                "Shilin Yan",
                "Pan Lu",
                "Hongsheng Li",
                "Peng Gao",
                "Yu Qiao."
            ],
            "title": "Llama-adapter: Efficient fine-tuning of language models with zero-init attention",
            "venue": "arXiv preprint arXiv:2303.16199.",
            "year": 2023
        },
        {
            "authors": [
                "Susan Zhang",
                "Stephen Roller",
                "Naman Goyal",
                "Mikel Artetxe",
                "Moya Chen",
                "Shuohui Chen",
                "Christopher Dewan",
                "Mona Diab",
                "Xian Li",
                "Xi Victoria Lin"
            ],
            "title": "Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Large Language Models (LLMs) have exhibited an impressive ability to free-form generation tasks and multi-choice question-answering tasks in natural language processing (Chung et al., 2023; Jiang et al., 2023; Fei et al., 2023; Cai et al., 2022; Chen et al., 2023; Saha et al., 2022). These LLMs have\n\u2217Equal contribution. \u2020Corresponding author.\nachieved human-level performance on a wide range of challenging tasks like professional & academic QA (Hendrycks et al., 2021), science QA (Clark et al., 2018), mathematics QA (Cobbe et al., 2021), code generation (Chen et al., 2021), and commonsense reasoning (Zellers et al., 2019; Sakaguchi et al., 2021) since they are pretrained with largescale corpora (e.g., CommonCrawl, Bookcorpus, and Wikipedia) which entail massive human knowledge. With such pretraining data, usually comprising a series of contexts, LLMs are trained to predict the next token given the preceding tokens. Therefore, LLMs learn to predict the next context given a series of contexts during pretraining, so they implicitly learn temporal and causal reasoning ability.\nTo assess LLMs\u2019 temporal and causal reasoning ability, we explore a popular multi-modal understanding task, Video Question Answering (VideoQA), which requires the model to predict the correct answer (A) given a video (V) and question (Q) pair. Recent challenging VideoQA benchmarks demand the model to answer the question which asks temporal and causal relationships, e.g., the next event of a video or the reason why a scene happens. We observe that LLMs effectively handle\nsuch challenging VideoQA benchmarks by leveraging their strong prior knowledge of temporal and causal reasoning learned from the pretraining phase. For example, in Fig. 1a, LLMs correctly answer causal questions solely based on the text question and options without referring to the visual content by exploiting linguistic shortcut. Also, Fig. 1b shows that a language-only QA model equipped with a larger language model, e.g., LLaMA 33B, denoted by QA 33B outperforms a VideoQA model with OPT 125M trained with full \u27e8V, Q, A\u27e9 on causal and temporal questions by a large margin of 13%. Although LLMs\u2019 prior knowledge is effective for addressing complex temporal and causal questions, this sometimes leads to suboptimal answers when the model overly depends on inaccurate linguistic prior, i.e., linguistic bias, while ignoring the visual content. This is known as the \u2018hallucination problem\u2019 of visual question-answering models equipped with LLMs (Alayrac et al., 2022). Although in the literature linguistic shortcut and linguistic bias are interchangeably used, in this paper, we use the former specifically when the linguistic prior is correct and the latter otherwise.\nHere, we propose a novel learning framework, Flipped-VQA, predicting all the combinations of \u27e8V, Q, A\u27e9 triplet by flipping the source pair and the target label, i.e., VQ \u2192 A (main task), VA \u2192 Q, and QA \u2192 V (auxiliary tasks). In other words, to understand complex relationships between the video, question, and answer, LLMs are asked to additionally predict the question given a video-answer pair and the video given a questionanswer pair by leveraging their knowledge of temporal and causal reasoning. In our experiments, we develop LLaMA-VQA by applying Flipped-VQA to LLaMA (Touvron et al., 2023) and it outperforms other baselines on five challenging VideoQA benchmark datasets: NExT-QA (Xiao et al., 2021), STAR (Wu et al., 2021), DramaQA (Choi et al., 2021), VLEP (Lei et al., 2020), and TVQA (Lei et al., 2018) with a small number of learnable parameters (only 0.06% of total model parameters). Furthermore, Flipped-VQA improves the performance of GPT-J (Wang and Komatsuzaki, 2021) and OPT (Zhang et al., 2022), implying that our framework is generally applicable to other decoder-only LLMs. We empirically demonstrate that Flipped-VQA encourages LLMs to exploit linguistic shortcuts by leveraging their prior knowledge and also mitigates linguistic bias which causes\nincorrect answer over-relying on the question. To sum up, our contributions are as follows:\n\u2022 We investigate that pretrained LLMs\u2019 knowledge is a strong prior for temporal and causal reasoning on challenging VideoQA.\n\u2022 We propose a novel framework, Flipped-VQA, to efficiently fine-tune LLMs on VideoQA by reasoning and understanding the complex relationships of \u27e8V, Q, A\u27e9 triplet, using LLMs\u2019 prior knowledge of temporal and causal reasoning. Flipped-VQA requires LLMs to perform three tasks: VQ \u2192 A, VA \u2192 Q, and QA \u2192 V, and we combine these objectives as LLMs\u2019 language generation objective.\n\u2022 LLaMA-VQA trained by Flipped-VQA, outperforms the baselines on five challenging VideoQA benchmark datasets. Also, our experiments demonstrate that Flipped-VQA is generally applicable to various decoder-only LLMs and consistently improves their performances.\n\u2022 Our extensive analyses show that FlippedVQA is effective in exploiting linguistic shortcuts to answer the question based on LLMs\u2019 prior knowledge and alleviating the linguistic bias by increasing the utilization of visual contents."
        },
        {
            "heading": "2 Related works",
            "text": "LLMs for temporal and causal reasoning. Exposed to a wide range of corpora during pretraining, LLMs perform diverse reasoning tasks (Liu et al., 2023; Wang et al., 2023c; Ozturkler et al., 2023; Ho et al., 2022; Li et al., 2022; Wang et al., 2023c; K\u0131c\u0131man et al., 2023). Particularly, a line of works (Zhang et al., 2023a; Li et al., 2023e; Kamalloo et al., 2023; Wang et al., 2023b; Tan et al., 2023) focuses on the temporal and causal reasoning skills of LLMs. For temporal reasoning, Zhang and Choi (2021) assesses LLMs by asking opendomain time-sensitive questions in both closed and open settings. Similarly, Hobbhahn et al. (2022) investigates the ability through querying subject and relation pairs of different time periods. Furthermore, some works have also explored the causal capabilities by evaluating whether LLMs can understand the causal implications given in the sentence. Long et al. (2023) uses simple causal graphs and determines if LLMs can understand the relationship\nbetween nodes. In this work, we further examine the temporal and causal reasoning skills of LLMs expanded to the multi-modal setting of challenging VideoQA.\nLLMs for multi-modal understanding. Various lines of work attempt to incorporate different modalities into LLMs to leverage the models\u2019 generation power and knowledge in performing multimodal downstream tasks. There exist various techniques (Hu et al., 2021; Jia et al., 2022; Ju et al., 2022) in the literature to bridge the gap between different modalities. For instance, Flamingo (Alayrac et al., 2022) ingests visual content into the frozen Chinchilla (Hoffmann et al., 2022) through a Perceiver Resampler. LLaMA-Adapter (Zhang et al., 2023c) fine-tunes LLaMA by applying linear projection along with the adaption of prompts to incorporate the visual information. Recently, there have been several approaches to develop a LLMsbased video chat model trained on massive multimodal instruction tuning dataset that enables comprehensive understanding across different modalities (Zhang et al., 2023b; Li et al., 2023b,d). Specifically, VideoChat (Zhang et al., 2023b) proposes video-centric instruction dataset that primarily emphasizes spatio-temporal reasoning and causal rela-\ntionships present in the video. Video Question Answering (VideoQA). VideoQA aims to answer natural language questions given a video that requires multi-modal understanding and reasoning skills on different semantic levels. Previous VideoQA benchmarks (Xu et al., 2017; Jang et al., 2017) target short videos and ask questions based on visual facts such as location and objects/attributes. In contrast, more recent benchmarks (Xiao et al., 2021; Lei et al., 2020, 2018; Wu et al., 2021; Choi et al., 2021) tend to tackle temporal and causal questions referencing a longer video. Specifically, NExT-QA (Xiao et al., 2021) requires uncovering the cause/intention of a certain event (e.g., Why did ...) or reasoning about subsequent actions in the video (e.g., What/How ... do/react after ...)1. In this work, we address these challenging VideoQA benchmarks through LLMs\u2019 temporal and causal reasoning abilities."
        },
        {
            "heading": "3 Method",
            "text": "We present Flipped-VQA, a simple yet effective framework for Video Question Answering (VideoQA) that leverages the LLMs\u2019 prior knowl-\n1Further details with examples of NExT-QA are provided in Sec. D.\nedge of temporal and causal reasoning. In addition to the target task of predicting an answer A given a video V and a question Q (i.e., VQ \u2192 A), our framework flips the role of inputs and outputs, requiring the model to predict V given QA and Q given VA. We apply our framework to LLaMA (Touvron et al., 2023) and develop LLaMA-VQA but it is applicable to any decoderonly LLMs. In this section, we first describe the overall architecture of LLaMA-VQA and then introduce our objective. The overall architecture is illustrated in Fig. 2."
        },
        {
            "heading": "3.1 LLaMA-VQA",
            "text": "LLaMA-VQA is built on LLaMA with a few additional learnable parameters. First, LLaMA-VQA adopts a learnable linear layer f to project the visual embeddings, extracted from the frozen visual encoder CLIP ViT/L14 (Radford et al., 2021), to LLaMA\u2019s text token embedding space, see Fig. 2. Specifically, given a raw video xv, a sequence of visual tokens is calculated as v = [v1, . . . , vNv ] = f(CLIP(xv)) \u2208 RNv\u00d7D, where Nv is the number of video frames and D is a feature dimension. Second, as in LLaMA-Adapter (Zhang et al., 2023c), we additionally adopt several trainable adapter tokens p = [p1, . . . , pNp ] which are prepended to the key and value of each self-attention layer, where Np is the number of adapter tokens. Further descriptions of LLaMA-Adapter are provided in Sec. C. So the number of trainable parameters of LLaMA-VQA 7B is 4.5M, only 0.06% of total parameters of LLaMA 7B. With such a few trainable parameters, LLaMA-VQA effectively preserves LLMs\u2019 prior knowledge and leverages it in exploiting linguistic shortcuts for VideoQA.\nThe question q = [q1, . . . , qNq ] \u2208 RNq\u00d7D and answer a = [a1, . . . , aNa ] \u2208 RNa\u00d7D tokens are extracted from raw question xq and answer xa texts by a tokenizer, where Nq and Na are the numbers of question and answer tokens respectively. The input prompt with visual tokens for LLaMA-VQA is provided in Tab. 1, where v and q serve as prefix tokens, see Sec. B for further prompt details. For simplicity, we omit the tokens for the prompt template (e.g., \u2018Video:\u2019 and \u2018Question:\u2019) and only consider content tokens (e.g., \u2018<v1>\u2019 and \u2018<question>\u2019) in our equations. Note that q \u2208 RNq\u00d7D represents only the question tokens and choice tokens are omitted in following notations. Then, token sequences v, q, and a are concatenated and fed to\nLLaMA, and the output feature is calculated as:\n[hv,hq,ha] = LLaMA([v,q,a],p), (1)\nwhere hv is a sequence of output features, i.e., hv = [hv1, . . . , h v Nv\n], and hq,ha are similarly defined."
        },
        {
            "heading": "3.2 Flipped-VQA",
            "text": "To utilize LLMs\u2019 temporal and causal reasoning abilities, we here present Flipped-VQA, consisting of three objectives, for reasoning the complex relationship between video, question, and answer of VideoQA. VQ \u2192 A. Predicting an answer given a videoquestion pair is the primary task of VideoQA. Its objective function is formulated as:\nLvqa = \u2212 logP (a|v,q)\n= \u2212 Na\u22121\u2211 t=0 logP (at+1|v,q, a\u2264t), (2)\nwhere v and q are given as prefix to generate the answer a. Note that P (a1|v,q, a\u22640) := P (a1|v,q). Then, the probability in Eq. (2) is calculated as:\nP (at+1|v,q, a\u2264t) = Softmax(Linear(hat )). (3)\nAt the inference phase, the model predicts the answer as:\na\u0302 = argmax a\u2208A\nP (a|v,q), (4)\nwhere A is a set of candidate answers, i.e., choices. We now flip the role of inputs and outputs and define two auxiliary tasks: question generation and video prediction. VA \u2192 Q. Similar to Lvqa, we also encourage the model to generate the question from the video and\nanswer as:\nLvaq = \u2212 logP (q|v,a)\n= \u2212 Nq\u22121\u2211 t=0 logP (qt+1|v,a, q\u2264t), (5)\nwhere P (qt+1|v,a, q\u2264t) = Softmax(Linear(hqt )). By Eq. (5), LLaMA-VQA has to generate the question which derives the answer from the video, leveraging its prior knowledge of temporal and causal reasoning. QA \u2192 V. Another flipped task is video prediction given a question and an answer. It is formulated as:\nLqav = \u2212 logP (v|q,a)\n= \u2212 Nv\u22121\u2211 t=0 logP (vt+1|q,a, v\u2264t). (6)\nIn contrast to the text generation loss in Eq. (2) and Eq. (5), which selects a token among the fixed vocabulary set (discrete space), it is too challenging to generate a video. So we instead adopt InfoNCE (Oord et al., 2018) to maximize the mutual information between the input frame feature vt+1 \u2208 RD and the output feature (hvt \u2208 RD) of LLaMA-VQA. Then, the likelihood in Eq. (6) is calculated as:\nP (vt+1|q,a, v\u2264t) = exp(vt+1 \u22a4hvt )\u2211Nv i=1 exp(vi \u22a4hvt ) , (7)\nwhere hv0 is the token representation right before the start of visual tokens. This encourages the model to predict the order of video frames given preceding frames, i.e., next frame prediction, by analyzing the question and answer with LLMs\u2019 prior knowledge. This formulation enables video prediction via a unified text-generation-based QA model with minimum modification.\nWe combine all three objectives, which are LLMs\u2019 language generation losses and its variant. Finally, we train LLaMA-VQA with the following loss:\nLFlipped-VQA = Lvqa + Lvaq + Lqav. (8)\nWe accumulate gradients of three different objectives and then update the learnable parameters. Remarks. We observe that the objectives of the primary task and auxiliary tasks can be interpreted as learning posterior and likelihood, respectively. For instance, by the Bayes rule, we have\nP (a|v,q) \u221d P (q|v,a)P (a|v). Hence, learning likelihood P (q|v,a) via the question generation given a video and an answer benefits the primary task of predicting the answer given a video and a question, which is the posterior probability P (a|v,q). Similarly, the same argument holds for video prediction; P (a|v,q) \u221d P (v|q,a). These relationships explain why training a VQA model with flipped tasks boosts the performance of the target task. More detailed discussion is provided in Sec. E."
        },
        {
            "heading": "4 Experiments",
            "text": "We verify the effectiveness of our framework to leverage the powerful prior knowledge induced by an LLM. For a thorough analysis, our framework is applied to various LLMs: LLaMA (7B, 13B, and 33B), OPT (125M \u223c 6.7B), GPT-J (6B). We conduct experiments and analyses to answer the following research questions: Q1. Do LLMs possess the knowledge of temporal and causal reasoning? Q2. Is Flipped-VQA effective for dealing with challenging VideoQA? Q3. How does Flipped-VQA alleviate linguistic bias? Datasets. We experiment on five multiple-choice VideoQA benchmark datasets (NExT-QA, STAR, DramaQA, VLEP, and TVQA) which require challenging temporal and causal reasoning abilities. Further experimental settings and implementation details are provided in Sec. A and Sec. B."
        },
        {
            "heading": "4.1 Temporal and causal reasoning of LLMs",
            "text": "We investigate LLMs\u2019 strong prior of temporal and causal reasoning to answer Q1 by comparing our framework with both LLMs-based and non-LLMsbased models for VideoQA. Comparison of various sizes of LLMs. We first conduct the experiment on various LLMs sizes to verify the effectiveness of LLMs\u2019 temporal and causal reasoning ability on VideoQA in Fig. 3. Note that Flipped-VQA is not applied in this experiment to show that LLMs already possess strong reasoning ability themselves. In Fig. 3a, we evaluate various sizes of LLMs trained with entire \u27e8V, Q, A\u27e9 triplets and the result shows that the performances on causal and temporal questions are dramatically improved as the model size increases. On the other hand, the performance gain of descriptive questions is relatively smaller than causal and temporal ques-\ntions. The performance gap between descriptive and causal questions has decreased from 17.2% on 125M to 1.1% on 33B.\nAlso, to verify the LLMs\u2019 prior knowledge of temporal and causal reasoning, we evaluate LLMs trained with only \u27e8Q, A\u27e9 pairs by forcing the model to solely rely on the question. In Fig. 3b, with only linguistic information (i.e., question), the performance of causal questions is lower than descriptive questions on 125M, but it significantly improves as the model size increases and outperforms the descriptive question accuracy on 33B by a margin of 6.6%. Without visual content, this model already outperforms MIST (Gao et al., 2023) in Tab. 2, a non-LLMs-based model, in terms of causal and temporal question types. These results suggest that larger LLMs possess more powerful prior of causal and temporal reasoning obtained during pretraining, and such prior plays a significant role in exploiting linguistic shortcuts for complex VideoQA.\nComparison with non-LLMs-based models. In Tab. 2, we then show the results of LLaMA-VQA in comparison to non-LLMs-based models on five challenging VideoQA benchmark datasets. Our LLaMA-VQA outperforms all the baselines across various datasets by a significant margin, especially on causal and temporal questions. For example, in NExT-QA, the performance gain in descriptive questions type is marginal compared to InternVideo, but it yields more than 10% improvements in both temporal and causal questions. Also, in STAR, LLaMA-VQA surpasses MIST on all question types resulting in an 11.5% increase in total accuracy. Particularly, the performance on sequence questions, which ask for temporal reasoning about consecutive actions, is improved by a large margin of 13.7%. These results highlight that LLMs-based LLaMA-VQA achieves remarkable capability, especially on temporal and causal reasoning questions compared to non-LLMs-based models, by mainly leveraging its pretrained prior (introducing only 4.5M learnable parameters). Comparison with LLMs-based models. We also explore larger LLaMA-VQA (\u223c 33B) and compare them with LLMs-based models on NExT-QA in Tab. 3. Our LLaMA-VQA 33B outperforms BLIP2 and SeViLA in terms of the total accuracy only with 9.2M trainable parameters. Specifically, the performance gain on causal and temporal questions\nis 2% and 3.2% compared to SeViLA. On the other hand, the accuracy of LLaMA-VQA on descriptive questions is lower than baselines since they were further pretrained with large-scale image-caption pair datasets, which boosts the descriptiveness capability of visual content. Finally, as the model size of LLaMA-VQA increases (7B \u2192 33B), the performance on causal and temporal questions is increased by 3.5% and 3.4%, respectively implying that larger LLMs have more powerful temporal and causal reasoning capabilities."
        },
        {
            "heading": "4.2 Flipped-VQA on challenging VideoQA",
            "text": "We here discuss Q2 by analyzing the effectiveness of Flipped-VQA on challenging VideoQA. Ablation studies of Flipped-VQA. Tab. 4 shows the ablation studies of Flipped-VQA on various LLMs (OPT, GPT-J, and LLaMA). Compared to the baseline LLMs with Lvqa, introducing a question generation objective Lvaq improves the performances by 3.7%, 1.6%, and 2.5% in NExT-QA on OPT, GPT-J, and LLaMA, respectively. This result demonstrates that generating intricate questions of NExT-QA given a video-answer pair encourages LLMs to leverage their temporal and causal reasoning knowledge to predict the answer. In addition, further improvement is observed by adapting video predicting objective Lqav that helps to understand the order of visual contents based on the question and answer. The accuracy of GPT-J is increased by a margin of 2.9%, 3.6%, and 1.6% respectively on NExT-QA, STAR, and DramaQA. Overall, each component of Flipped-VQA improves the performance across various LLMs, implying that FlippedVQA is an effective training objective for LLMs to deal with challenging VideoQA by leveraging their pretrained knowledge.\nUnlike solely using Lvqa to perform the main\ntask, Flipped-VQA accumulates gradients from three different objectives. So we conduct an additional experiment by increasing the gradient accumulation steps three times more than the baseline, i.e., we additionally train Lvqa for 15 epochs while the others are trained for 5 epochs. In Tab. 4, the performance of Lvqa with 15 epochs is on par with the one with 5 epochs across various LLMs and datasets. This suggests that the performance gain of Flipped-VQA does not come from the increased gradient accumulation or training schedule, but comes from the capability of LLMs\u2019 prior knowledge exploited by Lvaq and Lqav. Qualitative results of generated questions. Fig. 4 illustrates examples of questions generated by LLaMA-VQA conditioned on the video and answer with the objective of Lvaq, in the NExT-QA validation set. We observe that the majority of \u27e8V, Q, A\u27e9 triplets with generated questions are plausible enough to answer, while the generated questions depict different aspects from the video than the originals. For instance of the causal question in Fig. 4a, LLaMA-VQA combines the visual content, a man waterskiing, with the answer \u201chold on to long rope\u201d and expresses as the man is \u201cmaintain[ing] balance\u201d. Note that the idea of \u201ckeep[ing] contact\u201d in the original question aligns with the idea of \u201cmaintain[ing] his balance\u201d, so there is no difficulty for the model to answer the generated question based on the given video. Hence it shows how LLMs are using their pretrained knowledge to generate the question appropriate to the given\nvideo and answer. More interestingly, LLaMA-VQA is capable of understanding intricate interactions present in the video. For the temporal question in Fig. 4b, unlike the original question that asks for the action after the girl \u201cstop[s] hitting the board\u201d, the generated question asks for the action after \u201clook[ing] down\u201d. This reveals that LLaMA-VQA understands the interaction between objects and the sequential events in the video, and thus it can generate temporal questions adequate to the answer. These results suggest that LLaMA-VQA successfully understands the complex relationship between the video, question, and answer with Flipped-VQA by leveraging its prior knowledge of temporal and causal reasoning."
        },
        {
            "heading": "4.3 Flipped-VQA for mitigating linguistic bias",
            "text": "We observed that Flipped-VQA is crucial to address linguistic bias. So we finally analyze how Flipped-VQA alleviates linguistic bias (Q3) by providing detailed quantitative and qualitative analysis. Quantitative results of bias mitigation. Although LLMs\u2019 strong prior of temporal and causal reasoning is beneficial to exploit linguistic shortcuts for challenging questions, this sometimes leads to suboptimal results by forcing the model to overly rely on the text question while ignoring the visual content. This problem, linguistic bias, is commonly observed in visual question answering (Niu et al., 2021; Ramakrishnan et al., 2018; Cadene et al., 2019). Our extensive analyses show that FlippedVQA mitigates the linguistic bias while effectively leveraging linguistic shortcut. We first analyze how effectively LLaMA-VQA exploits linguistic shortcuts. Specifically, Tab. 5 shows that Flipped-VQA improves the accuracy of LLaMA-VQA on NExTQA by 4.6% when the linguistic prior is correct. It is measured by the following equation:\nP ( Y\u0302A|V,Q = Y \u2223\u2223\u2223Y\u0302A|Q = Y ) . (9)\nHere, the correctness of linguistic prior is defined as the accuracy of the QA model that predicts answers solely based on the language, i.e., P (Y\u0302A|Q = Y ). Secondly, we analyze how effectively LLaMA-VQA mitigates linguistic bias by the following metric:\nP ( Y\u0302A|V,Q = Y \u2223\u2223\u2223Y\u0302A|Q \u0338= Y ) . (10) This measures the accuracy of the VQA model when the linguistic prior is wrong, i.e., Y\u0302A|Q \u0338= Y . Tab. 5 shows that Flipped-VQA improves the accuracy on the samples with linguistic bias (inaccurate linguistic priors) by 3.1%. Our in-depth analysis of attention and embeddings also shows that FlippedVQA encourages LLMs to leverage more visual content and better align the visual embedding space with LLMs\u2019 text embedding space, see Sec. F for details. Qualitative results of bias mitigation. We here further analyze the effectiveness of Flipped-VQA in mitigating linguistic bias with qualitative results. Given incorrect linguistic prior, i.e., Y\u0302A|Q \u0338= Y , in other words, when the prediction by the languageonly model is wrong, our model trained with Flipped-VQA better rejects the wrong linguistic bias than the one trained without Flipped-VQA. For example in Fig. 5a the language-only model outputs \u201cman draws on ground\u201d for the causal question \u201cWhy are there lines everywhere on the ground?\u201d. LLaMA-VQA trained without Flipped-VQA fails to reject the wrong linguistic prior and chooses the\nplausible-sounding answer based on the common knowledge in the pretrained language model without referring to visual content. This can be viewed as the hallucination and ungrounded guess problem observed in Alayrac et al. (2022). On the other hand, LLaMA-VQA trained with Flipped-VQA refers to the visual content that depicts a plane leaving traces on the snow as it is taking off and successfully predicts the actual cause \u201cplane take off\u201d. The enhanced attention between the answer and visual tokens supports that Flipped-VQA encourages the model to refer to visual content, see Sec. F for more details.\nSimilarly, LLMs\u2019 temporal prior occasionally disrupts identifying the action followed by an event. For the temporal question in Fig. 5b, the act of wiping off follows the act of \u201cpick[ing] up the towel\u201d in general. Hence, the language-only model Y\u0302A|Q and LLaMA-VQA Y\u0302A|V,Q without FlippedVQA predict \u201crub the baby\u2019s face.\u201d In contrast, the proposed method with Flipped-VQA accurately predicts \u201ckeeps it back\u201d by referring to the video. These results demonstrate that Flipped-VQA encourages the answers grounded on visual information and mitigates linguistic bias."
        },
        {
            "heading": "5 Conclusion",
            "text": "In this paper, we investigate the large language models\u2019 (LLMs\u2019) temporal and causal reasoning abilities on the challenging multi-modal Video Question Answering (VideoQA) task. We observe that the larger LLMs possess more powerful prior knowledge of temporal and causal reasoning in addressing complex VideoQA. Moreover, we propose a novel framework, Flipped-VQA, that effectively leverages the LLMs\u2019 knowledge of temporal and causal reasoning on understanding the complex \u27e8V, Q, A\u27e9 triplet by introducing three generative objectives: Lvqa, Lvaq, and Lqav. Our in-depth analyses show that Flipped-VQA not only enhances the exploitation of linguistic shortcuts but also mitigates linguistic bias that causes hallucination and ungrounded guess problems. Acknowledgments. This work was partly supported by ICT Creative Consilience program (IITP2023-2020-0-01819) supervised by the IITP, the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIT) (NRF-2023R1A2C2005373), and KakaoBrain corporation.\nLimitations\nWe propose a Flipped-VQA which can be widely adaptable to decoder-only LLMs by improving their performances on challenging VideoQA. Flipped-VQA effectively leverages LLMs\u2019 prior knowledge of temporal and causal reasoning with generative objectives of next token prediction. Extending it to encoder-decoder LLMs with an objective other than the next token prediction can be interesting. Also, although the number of trainable parameters of LLaMA-VQA is only 4.5M \u223c 9.2M, the total number of parameters is inherently large (7B \u223c 33B), which mainly comes from backbone LLMs, LLaMA. This leads to massive memory usage during training/fine-tuning and inference."
        },
        {
            "heading": "A Dataset details",
            "text": "NExT-QA (Xiao et al., 2021) consists of three types of questions. Causal questions ask for the intentions of earlier actions or reasons for succeeding ones. Temporal questions determine the relationships between actions that are solely based on the sequence of occurrence (e.g. what ... do after/before/while ...). Descriptive questions focus\non visible contents such as places, and objects/attributes. About 5K videos of average 44s and 48K QA pairs with five answer candidates are given. Further examples of each question type are provided in Sec. D. DramaQA (Choi et al., 2021) features video story understanding with hierarchical difficulty levels. The level is determined by the required length of the clip (shot or scene) and the number of logical reasoning steps to answer the question. The dataset contains 24K video clips and 18K QA pairs with five answer candidates. Average video lengths are 3.7s for the shot and 91.3s for the scene. STAR (Wu et al., 2021) is designed for situational reasoning with questions that tackle interaction, sequence, prediction, and feasibility of events. There exist 60K QA pairs with four answer candidates and 22K video clips. VLEP (Lei et al., 2020) uses TV shows and YouTube Vlogs with an average of 6.1s that contain rich physical interactions and dialogues between people. The challenge is to determine which of two future events is likely to occur in the given video (with dialogue). It is comprised of 29K QA pairs with 10K video clips. TVQA (Lei et al., 2018) is built on long video clips (60-90s) of six different TV shows with various social interactions and activities. It provides dialogues for each video with 153K QA pairs and 22K video clips.\nB Implementation details\nTraining details. LLaMA-VQA is trained for five epochs on all datasets with a batch size of 32. LLaMA-VQA 7B and 13B are trained with 8 \u00d7 A6000 GPUs and LLaMA-VQA 33B is trained with 8 \u00d7 A100 GPUs. AdamW optimizer (Loshchilov and Hutter, 2017) is used with \u03b2 = (0.9, 0.95). We search learning rate and weight decay in [0.05, 0.1] and [0.15, 0.25], respectively. Following LLaMA-Adapter, for each layer of LLMs, 10 adapter tokens are used, i.e.Np = 10. The number of video frames Nv is set to 10. Each frame is resized by 224 \u00d7 224 and fed into CLIP VIT-L/14 (Radford et al., 2021) to extract frame features. The total sequence length of the concatenated visual, question, and answer tokens, Nv +Nq +Na, is 128, 128, 384, 256, and 512 for NExT-QA, STAR, DramaQA, VLEP, and TVQA respectively. Each dataset optionally provides dialogues. We append dialogues as prefix tokens for\nVLEP and TVQA. Lvaq is not applied in VLEP since questions of all samples in VLEP are consistent to \u201cWhich event is more likely to happen right after?\u201d. Prompt details. The general input prompt of LLaMA-VQA is provided in Tab. 1. Also, Tab. 6, Tab. 7, and Tab. 8 provides detailed input prompt of each task in Flipped-VQA, respectively. In those tables, non-prefix tokens, which the model needs to generate, are highlighted in red and the rest are prefix tokens."
        },
        {
            "heading": "C LLaMA-Adapter",
            "text": "LLaMA-Adapter (Zhang et al., 2023c) adopts a set of learnable adapter tokens p = [p1, . . . , pNp ] \u2208 RNp\u00d7D to efficiently fine-tune LLaMA (Touvron et al., 2023), where Np is the number of adapter tokens and D is a feature dimension. The adapter tokens are then concatenated as prefix tokens for the key and value of each self-attention layer, formulated as:\nQ = Linearq([v,q,a]) \u2208 RN , K = Lineark([p,v,q,a]) \u2208 RNp+N , V = Linearv([p,v,q,a]) \u2208 RNp+N ,\n(11)\nwhere N = Nv + Nq + Na. Note in our work, according to each objective of Flipped-VQA, the order of v, q, and a is permuted. Then, the scaled dot-product between Q and K is calculated as:\nS = QK\u22a4/ \u221a D \u2208 RN\u00d7(Np+N). (12)\nS in Eq. (12) can be divided into two groups as:\nS = [SNp , SN ]\u22a4, (13)\nwhere SNp \u2208 RN\u00d7Np is the attention scores of adapter tokens p and SN \u2208 RN\u00d7N is for v, q, a tokens.\nMoreover, to adjust the contribution of newly adopted tokens p at the beginning of training, LLaMA-Adapters introduce a zero-init attention gate g as:\nS = [Softmax(SNp) \u00b7 g,Softmax(SN )]\u22a4, (14)\nwhere g is initialized to zero. Eq. (14) preserves the LLMs\u2019 knowledge at the beginning of training and gradually increases the influence of adapter tokens p. Finally, the output of LLaMA-Adapter is as follows:\nH = Linear(SV ) \u2208 RN\u00d7D. (15)"
        },
        {
            "heading": "D NExT-QA",
            "text": "In NExT-QA, causal and temporal questions account for 48% and 29% respectively of the dataset. Specifically, there exist three types of questions: Causal, Temporal, and Descriptive. In general, questions and answers for temporal and causal types are longer than descriptive ones.\nCausal questions seek for event A which happens in advance of event B and is also responsible for B\u2019s occurrence in the video. These questions are broken down into asking \u201chow\u201d such an event\noccurred or \u201cwhy\u201d the object acts in a certain way. For instance, Fig. 6 (left) asks \u201cWhy did the shortest girl cover her mouth near the end of the video?\u201d.\nTemporal questions are closely related to causality but require reasoning solely based on the sequence of occurrence (present, previous, or next actions) and further ask to focus on interactions of multiple objects. For example, a question regarding the previous action asks \u201cWhat does the man in red do before the man in white cut the ribbon?\u201d in Fig. 6 (middle).\nLastly, descriptive questions tend to ask about the video in general (i.e., the places, objects/attributes, and main actions/events). For instance, Fig. 6 (right) asks \u201cWhat is the relationship between the guy in red and the rest of the people in black?\u201d."
        },
        {
            "heading": "E Discussion on Flipped-VQA",
            "text": "The primary task of VideoQA, predicting the answer given the video and question, can be rewritten as:\nP (a|v,q) = P (v|a,q)P (a|q) P (v|q)\n\u221d P (v|a,q). (16)\nIn Eq. (16), we observe that the auxiliary task of predicting visual tokens v given q and a can be viewed as maximum likelihood estimation (i.e., P (v|a,q)) and the primary task as the maximum a posterior (MAP) estimation (i.e., P (a|v,q)), respectively.\nSimilarly, the auxiliary task of predicting the question token q given a and v can be correlated with the primary task as:\nP (a|v,q) = P (q|a,v)P (a|v) P (q|v)\n\u221d P (q|a,v). (17)\nTherefore, by maximizing the likelihoods of auxiliary tasks, LFlipped-VQA in Eq. (8) aims to strengthen the performance on the main task."
        },
        {
            "heading": "F Embedding space alignment of Flipped-VQA",
            "text": "To bridge the visual encoder embedding space with LLMs text embedding space, we adopt a simple linear projection layer f . We observe that without Flipped-VQA, f is not trained effectively as the visual tokens v are just used as prefix tokens which are excluded from the generation target of LLMs. However, with Flipped-VQA which directly propagates the loss to the visual tokens (red arrows of Lqav in Fig. 2), f is trained to align visual encoder embedding space with frozen LLMs text token embedding space. We visualize the embedding space of question tokens q and visual tokens v with and without Flipped-VQA in Fig. 7a, and show that the embedding space of visual tokens with FlippedVQA (orange) is closer to LLMs\u2019 text embedding space (red) compared to the one without FlippedVQA (blue).\nFurthermore, in Fig. 7b, we plot the attention score between the answer query tokens and visual key tokens, i.e., measuring how much visual tokens v affect answer tokens a. As training proceeds, the attention score of both LLaMA-VQA with and without Flipped-VQA gradually increases, representing that the model leverages more visual content to answer the question. However, after the entire training iterations, the attention score with\nFlipped-VQA (red) is three times larger than the one without Flipped-VQA (blue), indicating that Flipped-VQA plays a key role in transferring the visual representation into the LLMs embedding space and enhances the utilization of visual content on LLMs. These results demonstrate that FlippedVQA encourages text-only trained LLMs to understand visual content by utilizing the strong representation power of a pretrained visual encoder, effectively aligning the visual embedding space with the text embedding space."
        },
        {
            "heading": "G Further quantitative results",
            "text": "We conduct an additional experiment on two generation-based VideoQA benchmark datasets: NExT-QA open-form generation (Xiao et al., 2021) and ActivityNet-QA (Yu et al., 2019). WUPS and Accuracy are used for evaluation metrics in NExTQA open-form generation and ActivityNet-QA, respectively. In Tab. 9, our LLaMA-VQA outperforms non-LLMs-based models HGA and KcGA by a large margin. Also, compared with LLMsbased Flamingo which is further trained with 2.1B external visual-text pairs, the performance gain is 0.8%. Furthermore, in Tab. 10, LLaMA-VQA also outperforms all the baselines although those use large-scale visual-text data for extra training in ActivityNet-QA."
        },
        {
            "heading": "H Further qualitative results",
            "text": "Question generation of Flipped-VQA. We here show further qualitative examples of generated\nquestions by Lvaq in Fig. 8. For the example in the middle of the first row, the generated question successfully describes the video, where the man moves across the muddy area, so the model can answer \u201cjeep\u201d based on this question. Also, LLaMAVQA generates questions by referring to different timestamps of the video. In the right example of the first row, the original question asks the reason why the lioness bends its head in the middle of the video to lead to the answer \u201cdrink water\u201d. However, the lioness also touches the river to drink water at the beginning of the video, and the generated question asks the reason for touching the river to obtain the answer \u201cdrink water\u201d. This suggests that Lvaq of Flipped-VQA helps to understand the complex relationship between video, question, and answer with LLMs\u2019 prior knowledge. Bias alleviation of Flipped-VQA. In addition to the examples in Fig. 5, we provide further qualitative results that Flipped-VQA enables the alleviation of linguistic bias. For the left example of the first row in Fig. 9, the model trained with questionanswer pairs (Y\u0302A|Q) predicts \u201cmicrophone\u201d for the temporal question \u201cWhat are the men holding on to when they speak?\u201d, based on the prior knowledge that people usually use the microphone when they are speaking. LLaMA-VQA trained without Flipped-VQA still fails to reject the wrong linguistic bias and predicts the same answer. On the other hand, LLaMA-VQA trained with Flipped-VQA accurately outputs the answer \u201cglasses\u201d by taking into account the video where the men are holding glasses, resulting in the reduction of hallucination problems with the mitigation of linguistic bias."
        }
    ],
    "title": "Large Language Models are Temporal and Causal Reasoners for Video Question Answering",
    "year": 2023
}