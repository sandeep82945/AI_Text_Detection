{
    "abstractText": "We investigate the potential misuse of modern Large Language Models (LLMs) for generating credible-sounding misinformation and its subsequent impact on information-intensive applications, particularly Open-Domain Question Answering (ODQA) systems. We establish a threat model and simulate potential misuse scenarios, both unintentional and intentional, to assess the extent to which LLMs can be utilized to produce misinformation. Our study reveals that LLMs can act as effective misinformation generators, leading to a significant degradation (up to 87%) in the performance of ODQA systems. Moreover, we uncover disparities in the attributes associated with persuading humans and machines, presenting an obstacle to current human-centric approaches to combat misinformation. To mitigate the harm caused by LLM-generated misinformation, we propose three defense strategies: misinformation detection, vigilant prompting, and reader ensemble. These approaches have demonstrated promising results, albeit with certain associated costs. Lastly, we discuss the practicality of utilizing LLMs as automatic misinformation generators and provide relevant resources and code to facilitate future research in this area.1",
    "authors": [
        {
            "affiliations": [],
            "name": "Yikang Pan"
        },
        {
            "affiliations": [],
            "name": "Liangming Pan"
        },
        {
            "affiliations": [],
            "name": "Wenhu Chen"
        },
        {
            "affiliations": [],
            "name": "Preslav Nakov"
        },
        {
            "affiliations": [],
            "name": "Min-Yen Kan"
        },
        {
            "affiliations": [],
            "name": "William Yang Wang"
        }
    ],
    "id": "SP:93c7fecf11d6a5338da9e3e74c7662e2a73c72ac",
    "references": [
        {
            "authors": [
                "Aaditya Bhat"
            ],
            "title": "Gpt-wiki-intro (revision 0e458f5)",
            "year": 2023
        },
        {
            "authors": [
                "Ben Buchanan",
                "Andrew Lohn",
                "Micah Musser."
            ],
            "title": "Truth, lies, and automation: How language models could change disinformation",
            "venue": "Center for Security and Emerging Technology.",
            "year": 2021
        },
        {
            "authors": [
                "Anderson",
                "Andreas Terzis",
                "Kurt Thomas",
                "Florian Tram\u00e8r."
            ],
            "title": "Poisoning Web-Scale Training Datasets is Practical",
            "venue": "ArXiv:2302.10149 [cs].",
            "year": 2023
        },
        {
            "authors": [
                "Souradip Chakraborty",
                "Amrit Singh Bedi",
                "Sicheng Zhu",
                "Bang An",
                "Dinesh Manocha",
                "Furong Huang."
            ],
            "title": "On the Possibilities of AI-Generated Text Detection",
            "venue": "ArXiv:2304.04736 [cs].",
            "year": 2023
        },
        {
            "authors": [
                "Hung-Ting Chen",
                "Michael J.Q. Zhang",
                "Eunsol Choi."
            ],
            "title": "Rich Knowledge Sources Bring Complex Knowledge Conflicts: Recalibrating Models to Reflect Conflicting Evidence",
            "venue": "ArXiv:2210.13701",
            "year": 2022
        },
        {
            "authors": [
                "Elizabeth Clark",
                "Tal August",
                "Sofia Serrano",
                "Nikita Haduong",
                "Suchin Gururangan",
                "Noah A. Smith."
            ],
            "title": "All that\u2019s \u2019human\u2019 is not gold: Evaluating human evaluation of generated text",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2021
        },
        {
            "authors": [
                "Elizabeth Clark",
                "Tal August",
                "Sofia Serrano",
                "Nikita Haduong",
                "Suchin Gururangan",
                "Noah A. Smith."
            ],
            "title": "All That\u2019s \u2019Human\u2019 Is Not Gold: Evaluating Human Evaluation of Generated Text",
            "venue": "ArXiv:2107.00061 [cs].",
            "year": 2021
        },
        {
            "authors": [
                "Thibault Formal",
                "Carlos Lassance",
                "Benjamin Piwowarski",
                "St\u00e9phane Clinchant."
            ],
            "title": "SPLADE v2: Sparse Lexical and Expansion Model for Information Retrieval",
            "venue": "ArXiv:2109.10086 [cs].",
            "year": 2021
        },
        {
            "authors": [
                "Yi R. Fung",
                "Christopher Thomas",
                "Revanth Gangi Reddy",
                "Sandeep Polisetty",
                "Heng Ji",
                "Shih-Fu Chang",
                "Kathleen R. McKeown",
                "Mohit Bansal",
                "Avi Sil."
            ],
            "title": "Infosurgeon: Cross-media fine-grained information consistency checking for fake news detection",
            "venue": "An-",
            "year": 2021
        },
        {
            "authors": [
                "Josh A. Goldstein",
                "Girish Sastry",
                "Micah Musser",
                "Renee DiResta",
                "Matthew Gentzel",
                "Katerina Sedova."
            ],
            "title": "Generative language models and automated influence operations: Emerging threats and potential mitigations",
            "venue": "CoRR, abs/2301.04246.",
            "year": 2023
        },
        {
            "authors": [
                "Shane Greenstein",
                "Feng Zhu"
            ],
            "title": "Is Wikipedia Biased",
            "venue": "American Economic Review,",
            "year": 2012
        },
        {
            "authors": [
                "Maur\u00edcio Gruppi",
                "Benjamin D. Horne",
                "Sibel Adal\u0131."
            ],
            "title": "NELA-GT-2021: A Large Multi-Labelled News Dataset for The Study of Misinformation in News Articles",
            "venue": "ArXiv:2203.05659 [cs].",
            "year": 2022
        },
        {
            "authors": [
                "Kelvin Guu",
                "Kenton Lee",
                "Zora Tung",
                "Panupong Pasupat",
                "Mingwei Chang."
            ],
            "title": "Retrieval Augmented Language Model Pre-Training",
            "venue": "Proceedings of the 37th International Conference on Machine Learning, pages 3929\u20133938. PMLR. ISSN: 2640-3498.",
            "year": 2020
        },
        {
            "authors": [
                "Sebastian Hofst\u00e4tter",
                "Jiecao Chen",
                "Karthik Raman",
                "Hamed Zamani."
            ],
            "title": "FiD-Light: Efficient and Effective Retrieval-Augmented Text Generation",
            "venue": "ArXiv:2209.14290 [cs].",
            "year": 2022
        },
        {
            "authors": [
                "Wen-tau Yih"
            ],
            "title": "Dense Passage Retrieval",
            "year": 2020
        },
        {
            "authors": [
                "Haoran Li",
                "Dadi Guo",
                "Wei Fan",
                "Mingshi Xu",
                "Yangqiu Song."
            ],
            "title": "Multi-step Jailbreaking Privacy Attacks on ChatGPT",
            "venue": "ArXiv:2304.05197 [cs].",
            "year": 2023
        },
        {
            "authors": [
                "Xinyi Li",
                "Yongfeng Zhang",
                "Edward C. Malthouse."
            ],
            "title": "A Preliminary Study of ChatGPT on News Recommendation: Personalization, Provider Fairness, Fake News",
            "venue": "ArXiv:2306.10702 [cs].",
            "year": 2023
        },
        {
            "authors": [
                "Jimmy Lin",
                "Xueguang Ma",
                "Sheng-Chieh Lin",
                "JhengHong Yang",
                "Ronak Pradeep",
                "Rodrigo Nogueira."
            ],
            "title": "Pyserini: A Python Toolkit for Reproducible Information Retrieval Research with Sparse and Dense Representations",
            "venue": "Proceedings of the 44th",
            "year": 2021
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
            "venue": "ArXiv:1907.11692 [cs].",
            "year": 2019
        },
        {
            "authors": [
                "Shayne Longpre",
                "Kartik Perisetla",
                "Anthony Chen",
                "Nikhil Ramesh",
                "Chris DuBois",
                "Sameer Singh."
            ],
            "title": "Entity-Based Knowledge Conflicts in Question Answering",
            "venue": "ArXiv:2109.05052 [cs].",
            "year": 2022
        },
        {
            "authors": [
                "Alexandra Luccioni",
                "Joseph Viviano."
            ],
            "title": "What\u2019s in the Box? An Analysis of Undesirable Content in the Common Crawl Corpus",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint",
            "year": 2021
        },
        {
            "authors": [
                "Eric Mitchell",
                "Yoonho Lee",
                "Alexander Khazatsky",
                "Christopher D. Manning",
                "Chelsea Finn."
            ],
            "title": "DetectGPT: Zero-Shot Machine-Generated Text Detection using Probability Curvature",
            "venue": "ArXiv:2301.11305 [cs].",
            "year": 2023
        },
        {
            "authors": [
                "OpenAI."
            ],
            "title": "GPT-4 technical report",
            "venue": "CoRR, abs/2303.08774.",
            "year": 2023
        },
        {
            "authors": [
                "Paul F. Christiano",
                "Jan Leike",
                "Ryan Lowe."
            ],
            "title": "Training language models to follow instructions with human feedback",
            "venue": "CoRR, abs/2203.02155.",
            "year": 2022
        },
        {
            "authors": [
                "Liangming Pan",
                "Wenhu Chen",
                "Min-Yen Kan",
                "William Yang Wang."
            ],
            "title": "ContraQA: Question Answering under Contradicting Contexts",
            "venue": "ArXiv:2110.07803 [cs].",
            "year": 2021
        },
        {
            "authors": [
                "Emmanouil Papadogiannakis",
                "Panagiotis Papadopoulos",
                "Evangelos P. Markatos",
                "Nicolas Kourtellis."
            ],
            "title": "Who Funds Misinformation? A Systematic Analysis of the Ad-related Profit Routines of Fake News sites",
            "venue": "ArXiv:2202.05079 [cs].",
            "year": 2023
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J. Liu."
            ],
            "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
            "venue": "ArXiv:1910.10683 [cs, stat].",
            "year": 2020
        },
        {
            "authors": [
                "Stephen Robertson",
                "Hugo Zaragoza."
            ],
            "title": "The Probabilistic Relevance Framework: BM25 and Beyond",
            "venue": "Foundations and Trends\u00ae in Information Retrieval, 3(4):333\u2013389.",
            "year": 2009
        },
        {
            "authors": [
                "Vinu Sankar Sadasivan",
                "Aounon Kumar",
                "Sriram Balasubramanian",
                "Wenxiao Wang",
                "Soheil Feizi"
            ],
            "title": "Can AI-Generated Text be Reliably Detected? ArXiv:2303.11156 [cs",
            "year": 2023
        },
        {
            "authors": [
                "Zhihong Shao",
                "Minlie Huang."
            ],
            "title": "Answering Open-Domain Multi-Answer Questions via a Recallthen-Verify Framework",
            "venue": "ArXiv:2110.08544 [cs].",
            "year": 2022
        },
        {
            "authors": [
                "Harald Stiff",
                "Fredrik Johansson."
            ],
            "title": "Detecting computer-generated disinformation",
            "venue": "International Journal of Data Science and Analytics, 13(4):363\u2013 383.",
            "year": 2022
        },
        {
            "authors": [
                "Dan Su",
                "Mostofa Patwary",
                "Shrimai Prabhumoye",
                "Peng Xu",
                "Ryan Prenger",
                "Mohammad Shoeybi",
                "Pascale Fung",
                "Anima Anandkumar",
                "Bryan Catanzaro."
            ],
            "title": "Context Generation Improves Open Domain Question Answering",
            "venue": "ArXiv:2210.06349 [cs].",
            "year": 2022
        },
        {
            "authors": [
                "Orion Weller",
                "Aleem Khan",
                "Nathaniel Weir",
                "Dawn Lawrie",
                "Benjamin Van Durme."
            ],
            "title": "Defending Against Poisoning Attacks in Open-Domain Question Answering",
            "venue": "ArXiv:2212.10002 [cs].",
            "year": 2022
        },
        {
            "authors": [
                "Teven Le Scao",
                "Sylvain Gugger",
                "Mariama Drame",
                "Quentin Lhoest",
                "Alexander M. Rush."
            ],
            "title": "HuggingFace\u2019s Transformers: State-of-the-art Natural Language Processing",
            "venue": "ArXiv:1910.03771 [cs].",
            "year": 2020
        },
        {
            "authors": [
                "Wenhao Yu",
                "Dan Iter",
                "Shuohang Wang",
                "Yichong Xu",
                "Mingxuan Ju",
                "Soumya Sanyal",
                "Chenguang Zhu",
                "Michael Zeng",
                "Meng Jiang."
            ],
            "title": "Generate rather than Retrieve: Large Language Models are Strong Context Generators",
            "venue": "ArXiv:2209.10063 [cs].",
            "year": 2022
        },
        {
            "authors": [
                "Rowan Zellers",
                "Ari Holtzman",
                "Hannah Rashkin",
                "Yonatan Bisk",
                "Ali Farhadi",
                "Franziska Roesner",
                "Yejin Choi"
            ],
            "title": "Defending against neural fake",
            "year": 2019
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Recently, large language models (LLMs) (Brown et al., 2020; Ouyang et al., 2022; OpenAI, 2023) have demonstrated exceptional language generation capabilities across various domains. On one hand, these advancements offer significant benefits to everyday life and unlock vast potential in diverse fields such as healthcare, law, education, and science. On the other hand, however, the growing accessibility of LLMs and their enhanced capacity to produce credibly-sounding text also raise\n\u2217Equal Contribution. 1We release the resources at https://github.com/\nMexicanLemonade/LLM-Misinfo-QA.\nconcerns regarding their potential misuse for generating misinformation. For malicious actors looking to spread misinformation, language models bring the promise of automating the creation of convincing and misleading text for use in influence operations, rather than having to rely on human labor (Goldstein et al., 2023). The deliberate distribution of misinformation can lead to significant societal harm, including the manipulation of public opinion, the creation of confusion, and the promotion of detrimental ideologies.\nAlthough concerns regarding misinformation have been discussed in numerous reports related to AI safety (Zellers et al., 2019; Buchanan et al., 2021; Kreps et al., 2022; Goldstein et al., 2023), there remains a gap in the comprehensive study of the following research questions: (1) To what extent can modern LLMs be utilized for generating credible-sounding misinformation? (2) What potential harms can arise from the dissemination of neural-generated misinformation in informationintensive applications, such as information retrieval\nand question-answering? (3) What mitigation strategies can be used to address the intentional misinformation pollution enabled by LLMs?\nIn this paper, we aim to answer the above questions by establishing a threat model, as depicted in Figure 1. We first simulate different potential misuses of LLMs for misinformation generation, which include: (1) the unintentional scenario where misinformation arises from LLM hallucinations, and (2) the intentional scenario where a malicious actor seeks to generate deceptive information targeting specific events. For example, malicious actors during the COVID-19 pandemic attempt to stir public panic with fake news for their own profits (Papadogiannakis et al., 2023). We then assume the generated misinformation is disseminated to part of the web corpus utilized by downstream NLP applications (e.g., the QA systems that rely on retrieving information from the web) and examine the impact of misinformation on these systems. For instance, we investigate whether intentional misinformation pollution can mislead QA systems into producing false answers desired by the malicious actor. Lastly, we explore three distinct defense strategies to mitigate the harm caused by LLM-generated misinformation including prompting, misinformation detection, and majority voting.\nOur results show that (1) LLMs are excellent controllable misinformation generators, making them prone to potential misuse (\u00a7 3), (2) deliberate synthetic misinformation significantly degrades the performance of open-domain QA (ODQA) systems, showcasing the threat of misinformation for downstream applications (\u00a7 4), and (3) although we observe promising trends in our initial attempts to defend against the aforementioned attacks, misinformation pollution is still a challenging issue that demands further investigation (\u00a7 5).\nIn summary, we investigate a neglected potential misuse of modern LLMs for misinformation generation and we present a comprehensive analysis of the consequences of misinformation pollution for ODQA. We also study different ways to mitigate this threat, which setups a starting point for researchers to further develop misinformation-robust NLP applications. Our work highlights the need for continued research and collaboration across disciplines to address the challenges posed by LLMgenerated misinformation and to promote the responsible use of these powerful tools."
        },
        {
            "heading": "2 Related Work",
            "text": "Combating Model-Generated Misinformation The proliferation of LLMs has brought about an influx of non-factual data, including both intentional disinformation (Goldstein et al., 2023) and unintentional inaccuracies, known as \u201challucinations\u201d (Ji et al., 2022). The realistic quality of such synthetically-generated misinformation presents a significant challenge for humans attempting to discern fact from fiction (Clark et al., 2021a). In response to this issue, a growing body of research has begun to focus on the detection of machinegenerated text (Stiff and Johansson, 2022; Mitchell et al., 2023; Sadasivan et al., 2023; Chakraborty et al., 2023). However, these methods remain limited in their precision and scope. Concurrently, there are efforts to curtail the production of harmful, biased, or baseless information by LLMs.2 These attempts, though well-intentioned, have shown vulnerabilities, with individuals finding methods to bypass them using specially designed \u201cjail-breaking\u201d prompts (Li et al., 2023a). Our research diverges from prior studies that either concentrate on generation or detection, as we strive to create a comprehensive threat model that encompasses misinformation generation, its influence on downstream tasks, and potential countermeasures.\nData Pollution Retrieval-augmented systems have demonstrated strong performance in knowledgeintensive tasks, including ODQA (Lewis et al., 2021; Guu et al., 2020). However, these systems are intrinsically vulnerable to \u201cdata pollution\u201d, i.e., the training data or the corpus they extract information from could be a mixture of both factual and fabricated content. This risk remained underexplored, as the current models mostly adopt a reliable external knowledge source (such as Wikipedia) (Karpukhin et al., 2020; Hofst\u00e4tter et al., 2022) for both training and evaluation. However, this ideal scenario may not always be applicable in the real world, considering the rapid surge of machinegenerated misinformation. Our work takes a pioneering step in exploring the potential threat posed by misinformation to QA systems. Unlike prior work on QA system robustness under synthetic perturbations like entity replacements (Pan et al., 2021; Longpre et al., 2022; Chen et al., 2022; Weller et al., 2022; Hong et al., 2023), we focus on the threat of realistic misinformation with modern LLMs.\n2https://platform.openai.com/docs/guides/ moderation"
        },
        {
            "heading": "3 Generating Misinformation with LLMs",
            "text": "In this section, we delve into the potential misuse of modern LLMs for creating seemingly credible misinformation. However, misinformation generation is a vast and varied topic to study. Therefore, in this paper, we concentrate on a particular scenario as follows: a malicious actor, using a misinformation generator denoted by G, seeks to fabricate a false article P \u2032 in response to a specific target question Q (for instance, \u201cWho won the 2020 US Presidential Election?\u201d). With the help of LLMs, the fabricated article P \u2032 could be a counterfeit news piece that incorrectly reports Trump as the winner. In the following, we will first introduce the misinformation generator and then delineate four distinct strategies that a malicious actor might employ to misuse LLMs for generating misinformation."
        },
        {
            "heading": "3.1 GPT-3.5 as Misinformation Generator",
            "text": "Prior works have attempted to generate fake articles using large pre-trained sequence-to-sequence (Seq2Seq) models (Zellers et al., 2019; Fung et al., 2021; Huang et al., 2022). However, the articles generated by these approaches occasionally make grammar and commonsense mistakes, making them not deceptive enough to humans. To simulate a realistic threat, we use GPT-3.5 (text-davinci-003) as the misinformation generator due to its exceptional ability to generate coherent and contextually appropriate text in response to given prompts. Detailed configurations of the generator are in Appendix A."
        },
        {
            "heading": "3.2 Settings for Misinformation Generation",
            "text": "The inputs chosen by users for LLMs can vary greatly, resulting in differences in the quality, style, and content of the generated text. When creating misinformation, propagandists may employ manipulative instructions to fabricate audacious falsehoods, whereas ordinary users might unintentionally receive non-factual information from harmless queries. In order to simulate various demographics of misinformation producers, we have devised four distinct settings to prompt LLMs for misinformation. Figure 2 showcases examples of misinformation generated in each scenario.\nTo be specific, we prompt the misinformation generator G (GPT-3.5) in a zero-shot fashion. The prompt p is composed of two parts: the instruction text pinstr and the target text ptgt. The former controls the overall properties of the generated text\n(e.g. length, style, and format), while the latter specifies the topic. In the following, we introduce the four different settings under this scheme and illustrate the detailed prompt design.\nGENREAD.3 This setting directly prompts GPT3.5 to generate a document that is ideally suited to answer a given question. In this context, pinstr is framed as \u201cGenerate a background document to answer the following question:\u201d, while ptgt includes only the question. LLMs are expected to generate factual content to address the question. However, in practice, they can be susceptible to hallucinations, resulting in the creation of content that strays from reality. This setting mirrors scenarios where LLM\u2019s hallucinations inadvertently introduce misinformation.\nCTRLGEN In this setting, we also prompt LLMs to produce a context passage for answering the given question. However, we additionally input a predetermined non-factual opinion. In this setting, pinstr is: \u201cGenerate a background document in support of the given opinion to the question.\u201d, while ptgt contains the target question and the non-factual fact or opinion. In this way, we intend to simulate the real-world disinformation and propaganda creation process where the malicious actors have some predetermined fabricated fact in mind (e.g., Trump won the 2020 presidential election) and attempt to generate an article that reflects the fact (e.g., fake news that reports Trump\u2019s victory).\nREVISE In this setting, we provide a humanwritten factual article for LLMs to use as a reference. Then, we prompt the LLM to revise the article to inject the predetermined non-factual fact or opinion. We set pinstr as: \u201cGiven the following passage, modify as few details as possible to make it support the given opinion to the question.\u201d. ptgt is then a real-world passage pertinent to the target question, together with the question and the predetermined opinion.\nREIT The previous settings all aim at generating articles that appear authentic to humans. However, there are cases where malicious actors aim to generate misinformation to compromise downstream models, such as QA systems. In these situations, the generated article does not necessarily have to appear realistic, as long as it can effectively manipulate the model (e.g., altering the QA system\u2019s\n3We borrow the name from (Yu et al., 2022)\noutput). We simulate this type of misuse by setting pinstr to: \u201cGiven the question and a predefined response, rephrase the response in ten different ways.\u201d In this case, ptgt comprises the target question and the predetermined misinformation."
        },
        {
            "heading": "4 Polluting ODQA with Misinformation",
            "text": "We then explore the potential damages that can result from the spread of LLM-generated misinformation, with a particular emphasis on Open-domain Question Answering (ODQA) applications. ODQA systems operate on a retriever-reader model, which involves first identifying relevant documents from a large evidence corpus, then predicting an answer based on these documents.\nWe introduce the concept of misinformation pollution, wherein LLM-generated misinformation is deliberately infused into the corpus used by the ODQA model. This mirrors the growing trend of LLM-generated content populating the web data used by downstream applications. Our goal is to evaluate the effects of misinformation pollution on various ODQA models, with a particular interest in whether or not such pollution could influence these QA systems to generate incorrect answers as per the intentions of a potential malicious actor."
        },
        {
            "heading": "4.1 Datasets",
            "text": "We construct two ODQA datasets for our exploration by adapting existing QA datasets.\nNQ-1500 We first use the Natural Questions (Kwiatkowski et al., 2019) dataset, a widely-used ODQA benchmark derived from Wikipedia. To minimize experimental costs, we selected a random sample of 1,500 questions from the original test set. In line with prior settings, we employed the Wikipedia dump from December 30, 2018 (Karpukhin et al., 2020) as the corpus for evidence retrieval.\nCovidNews We also conduct our study on a news-centric QA dataset that covers real-world topics that are more vulnerable to misinformation pollution, where malicious actors might fabricate counterfeit news in an attempt to manipulate news-oriented QA systems. We base our study on the StreamingQA (Li\u0161ka et al., 2022) dataset, a large-scale QA dataset for news articles. We filter the dataset using specific keywords adapted from Gruppi et al. (2022) and a timestamp filter of Jan. 2020 to Dec. 20204, allowing us to isolate\n4This timeframe was selected primarily due to GPT\u2019s knowledge limitations regarding events post-2021.\n1,534 questions related to COVID-19 news. For the evidence corpus, we utilize the original news corpus associated with StreamingQA, along with the WMT English News Corpus from 20205."
        },
        {
            "heading": "4.2 ODQA Systems",
            "text": "We conduct experiments on four distinctive types of retrieve-and-read ODQA systems, classified based on the choice of the retrievers and the readers.\nRetrievers For retrievers, we use BM25 (Robertson and Zaragoza, 2009) and Dense Passage Retriever (DPR) (Karpukhin et al., 2020), representing sparse and dense retrieval mechanisms respectively, which are the mainstream of the current ODQA models. BM25 is a traditional probabilistic model for information retrieval that remains a robust baseline in retrieval tasks. Although sparse retrievers may fall short in capturing complex semantics, they excel at handling simple queries, thus forming the backbone of several contemporary retrieval systems (Formal et al., 2021). Conversely, DPR leverage learned embeddings to discern implicit semantics within sentences, outpacing sparse retrievers in most retrieval tasks.\nReaders For readers, we use Fusion-in-Decoder (FiD) (Izacard and Grave, 2021) and GPT-3.5 (text-davinci-003). FiD is a T5-based (Raffel et al., 2020) reader, which features utilizing multiple passages at once to predict answers compared to concurrent models, yielding outstanding performance. Considering that answering questions with conflicting information might diverge from the training objectives of current MRC models, we also experimented with GPT-3.5 as a reader to leverage its extensive training set and flexibility. Additional model configurations are in Appendix A."
        },
        {
            "heading": "4.3 Misinformation Pollution",
            "text": "We then conduct misinformation pollution on the corpus for both NQ-1500 and CovidNews. For each question, we generate one fake document to be injected into the corresponding natural corpus, separately under each setting introduced in Section 3.2. We then evaluate ODQA under both the clean and polluted corpora, using the standard Exact Match (EM) to measure QA performance.\nThe statistics of the clean corpus and the polluted corpora for each setting are presented in Table 1.\n5https://statmt.org/wmt20/translation-task. html\nSetting NQ-1500 CovidNews Size % Size % CLEAN 21M - 3.3M -\nGENREAD 4.1K 0.02% 4.5K 0.1% CTRLGEN 1.7K <0.01% 3.9K 0.1% REVISE 2.3K 0.02% 2.7K 0.1% REIT 3.0K 0.01% 3.3K 0.1%\nTable 1: The size of the clean corpus and the number / percentage of fake passages injected into the clean corpus for each setting. We employ the 100-word split of a document as the unit to measure the size.\nThe volumes of injected fake passages, as indicated in the percentage column, are small in scale compared to the size of the original corpora."
        },
        {
            "heading": "4.4 Main Results",
            "text": "We evaluate the performance of different ODQA systems under two settings: one using an unpolluted corpus (CLEAN) and the other using a misinformation-polluted corpus, which is manipulated using different misinformation generation methods (CTRLGEN, REVISE, REIT, GENREAD). We present the performance of QA models in Table 2, in which we configured a fixed number of retrieved context passages for each reader6.\nWe identify four major findings. 1. Our findings indicate that misinformation poses a significant threat to ODQA systems. When subjected to three types of deliberate misinformation pollution \u2014 namely, CTRLGEN, REVISE, and REIT\u2014 all ODQA systems demonstrated a huge decline in performance as fake passages infiltrated the corpus. The performance drop ranges from 14% to 54% for DPR-based models and ranges from 20% to 87% for BM25-based models. Even under the GENREAD scenario, where misinformation is inadvertently introduced through hallucinations, we noted a 5% and 15% decrease in ODQA performance for the best-performing model (DPR+FiD) on NQ-1500 and CovidNews, respectively. These reductions align with our expectations, given that ODQA systems, trained on pristine data, are predisposed towards retrieving seemingly relevant information, without the capacity to discern the veracity of that information. This reveals the vulnerability of current ODQA systems to misinformation pollution, a risk that emanates both from intentional\n6We conducted additional experiments on the QA systems\u2019 performance with respect to the size of context passages used, which we explained in Appendix D.\nattacks by malicious entities and unintentional hallucinations introduced by LLMs.\n2. The strategy of reiterating misinformation (REIT) influences machine perception more effectively, even though such misinformation tends to be more easily discernible to human observers. We observed that the REIT pollution setting outstripped all others by significant margins. This striking impact corroborates our expectations as we essentially flood machine readers with copious amounts of seemingly vital evidence, thereby distracting them from authentic information. Considering that machine readers are primarily conditioned to extract answer segments from plausible sources \u2014 including generative readers \u2014 it is logical for such attack mechanisms to attain superior performance. The simplicity and easy implementation of this attack method underlines the security vulnerabilities inherent in contemporary ODQA systems.\n3. To further understand how misinformation pollution affects ODQA systems, we present in Table 7 the proportions of the questions where at least one fabricated passage was among the top-K retrieved documents. We find that LLM-generated misinformation is quite likely to be retrieved by both the BM25 and the DPR retriever. This is primarily because the retrievers prioritize the retrieval of passages that are either lexically or semantically aligned with the question, but they lack the capability to discern the authenticity of the information. We further reveal that REVISE is superior to GENREAD in producing fake passages that are more likely to be retrieved, and sparse retrievers\nare particularly brittle to deliberate misinformation pollution, e.g., REIT. The detailed configurations and findings are in Appendix B.\n4. Questions without dependable supporting evidence are more prone to manipulation. Comparing the performance differentials across the two test sets, we notice a more pronounced decline in system performance on the CovidNews test set. Our hypothesis for this phenomenon lies in the relative lack of informational depth within the news domain as opposed to encyclopedias. Subsequent experiments corroborate that the WMT News Corpus indeed provides fewer and less pertinent resources for answering queries. We delve into this aspect in greater detail in Appendix C.\nMoreover, we discover that the generated texts in the GENREAD setting have a significantly more detrimental effect on the CovidNews benchmark compared to the NQ-1500 benchmark. This highlights the uneven capabilities of GPT-3.5 in retaining information across diverse topics. We postulate that this may be partially due to the training procedure being heavily reliant on Wikipedia data, which could potentially induce a bias towards Wikipediacentric knowledge in the model\u2019s output."
        },
        {
            "heading": "5 Defense Strategies",
            "text": "A fundamental approach to mitigate the negative impacts of misinformation pollution involves the development of a resilient, misinformation-aware QA system. Such a system would mirror human behavior in its dependence on trustworthy external\nsources to provide accurate responses. In our pursuit of this, we have explored three potential strategies. In the following sections, we will succinctly outline the reasoning behind each strategy, present our preliminary experimental results, and discuss their respective merits and drawbacks. Details on experimental configurations are in Appendix A.\nDetection Approach The initial strategy entails incorporating a misinformation detector within the QA system, equipped to discern model-generated content from human-authored ones. To test this approach, we have employed a RoBERTa-based classifier (Liu et al., 2019), fine-tuned specifically for this binary classification task. For acquiring the training and testing data, we leveraged the NQ1500 DPR retrieval result, randomly partitioning the first 80% for training, and reserving the remaining 20% for testing. For each query, we used the top-10 context passages, amounting to 12,000 training instances and 3,000 testing instances. Training the above detector assumes the accessibility of the in-domain NQ-1500 data. Acknowledging the practical limitations of in-domain training data, we also incorporated an existing dataset of GPT3 completions based on Wikipedia topics to train an out-of-domain misinformation detector.\nVigilant Prompting LLMs have recently exhibited a remarkable ability to follow human instructions when provided with suitable prompting (Ouyang et al., 2022). We aim to investigate whether this capability can be extended to follow directives aimed at evading misinformation. Our experimental design utilizes GPT-3.5 as the reader, employing QA prompts that include an additional caution regarding misinformation. For example, the directive given to the reader might read: \u201cDraw upon the passages below to answer the subsequent\nquestion concisely. Be aware that a minor portion of the passages may be designed to mislead you.\u201d\nReader Ensemble Traditionally in ODQA, all retrieved context passages are concatenated before being passed to the reader. This approach may cause the model to become distracted by the presence of misinformation. In response to this, we propose a \u201cdivide-and-vote\u201d technique. Firstly, we segregate the context passages into k groups based on their relevance to the question. Each group of passages is then used by a reader to generate an answer. Subsequently, we apply majority voting on the resulting k candidate responses a1, a2, ..., ak to calculate the voted answer(s) av, using the formula av = argmax\naj\n(\u2211k i=1 I(ai = aj) ) . Through this\nvoting strategy, we aim to minimize the impact of misinformation by limiting the influence of individual information sources on answer prediction.\nMain Results The performance of detectors trained on both in-domain and out-of-domain data is illustrated in Table 3, revealing significant variances. In-domain trained detectors consistently deliver high AUROC scores (91.4%-99.7%), whereas out-of-domain trained classifiers show only slight improvements over random guessing (50.7%-64.8%). Despite the impressive results obtained with in-domain detectors, expecting a sufficient quantity of in-domain training data to always be available is impractical in real-world scenarios. This is due to our lack of knowledge regarding the specific model malicious actors may use to generate misinformation. Additionally, our out-of-domain training data, despite being deliberately selected to match the genre, topic, and length of the detection task\u2019s targets, yielded disappointing results. This underscores the challenge of training a versatile, effective misinformation detector.\nIncorporating additional information through prompting GPT readers yielded inconsistent out-\ncomes, as indicated in Table 4. This variation may be attributable to the dearth of data and the absence of tasks similar to the ones during the GPT-3.5 training phase. The voting strategy yielded benefits, albeit with attached costs. Voting consistently achieved better effectiveness compared with the prompting strategy, as demonstrated in Table 4. It is essential to note, however, that the deployment of multiple readers in the Voting approach necessitates additional resources. Despite the potential for concurrent processing of multiple API calls, the cost per question escalates linearly with the number of context passages used, rendering the method financially challenging at a large scale.\nDoes Reading more Contexts help? Intuitively, a straightforward way to counteract the proliferation of misinformation in ODQA is to diminish its prevalence, or in other words, to decrease the ratio of misinformation that the QA systems are exposed to. A viable method of achieving this is by retrieving a larger number of passages to serve as contexts for the reader. This approach has demonstrated potential benefits in several ODQA systems that operate on a clean corpus (Izacard and Grave, 2021; Lewis et al., 2021). To explore its effectiveness against misinformation, we evaluate the QA performance using different amount of context passages given to readers.\nFigure 3 shows the relation between context size used in readers and the QA performance. Instead of reporting the absolute EM score, we report the relative EM drop compared with the EM score under the clean corpus setting to measure the impact of misinformation pollution. Interestingly, our results show that increasing the context size has minimal, if not counterproductive, effects in mitigating the performance decline caused by misinformation. This aligns with the previous observation that ODQA readers rely on a few highly relevant contexts, regardless of the entire volume of contexts to make the prediction (Chen et al., 2022). A straightforward strategy of \u201cdiluting\u201d the misinformation by increasing the context size is not an effective way to defend against misinformation pollution.\nSummary In our exploration of three strategies to safeguard ODQA systems against misinformation pollution, we uncover promising effects through the allocation of additional resources. These include using in-domain detection training and engaging multiple readers to predict answers\nvia a voting mechanism. Nonetheless, the development of a cost-effective and resilient QA system capable of resisting misinformation still demands further research and exploration."
        },
        {
            "heading": "6 Discussion",
            "text": "In previous sections, we established a comprehensive threat model that encompasses misinformation generation, its resultant pollution, and potential defense strategies. While our research primarily resides in simulation-based scenarios, it has shed light on numerous potential risks posed by misinformation created by large language models. If left unaddressed, these risks could substantially undermine the current information ecosystem and have detrimental impacts on downstream applications. In this section, we offer a discussion on the practical implications of misinformation pollution in a real-world web environment. Our focus is on three crucial factors: information availability, the associated costs, and the integrity of web-scale corpora.\nInformation Availability Our misinformation generation methods only require minimal additional information, such as a relevant real passage (REVISE), or a modest amount of knowledge about the targeted system (REIT). Given the relative ease of producing misinformation with LLMs, we forecast that misinformation pollution is likely to become an imminent threat to the integrity of the web environment in the near future. It is, therefore, critical to pursue both technological countermeasures and regulatory frameworks to mitigate this threat.\nAssociated Costs The cost of the misinformation attack depends on factors like language model training and maintenance, data storage, and computing resources. We focus on the dominant cost in our experiments: OpenAI\u2019s language models API fees. We estimate producing one fake document (200 words) costs $0.01 to $0.04 using text-davinci-003, significantly lower than hiring human writers . This cost-efficient misinformation production scheme likely represents the disinformation industry\u2019s future direction.\nIntegrity of Web-scale Corpora The quality of web-scale corpora is important for downstream applications. However, web-scale corpora are known to contain inaccuracies, inconsistencies, and biases (Kumar et al., 2016; Greenstein and Zhu, 2012). Large-scale corpora are especially vulnerable to misinformation attacks. Decentralized cor-\npora with data in URLs risk attackers hijacking expired domains and tampering with contents (Carlini et al., 2023). Centralized corpora, such as the Common Crawl suffer from unwanted data as well (Luccioni and Viviano, 2021); even for manually maintained ones like Wikipedia, it is still possible for misinformation to slip in (Carlini et al., 2023)."
        },
        {
            "heading": "7 Conclusion and Future Work",
            "text": "We present an evaluation of the practicality of utilizing Language Model Models (LLMs) for the automated production of misinformation and we examine their potential impact on knowledge-intensive applications. By simulating scenarios where actors deliberately introduce false information into knowledge sources for question-answering systems, we discover that machines are highly susceptible to synthetic misinformation, leading to a significant decline in their performance. We further observe that machines\u2019 performance deteriorates even further when exposed to intricately crafted falsehoods. In response to these risks, we propose three partial solutions as an initial step toward mitigating the impact of LLM misuse and we encourage further research into this problem.\nOur future research directions for extending this work could take three paths. Firstly, while we have thus far only illustrated the potential dangers of misinformation generated by LLMs in ODQA systems, this threat model could be employed to assess risk across a broader spectrum of applications. Secondly, the potential of LLMs to create more calculated forms of misinformation, such as hoaxes, rumors, or propagandistic falsehoods, warrants a separate line of inquiry. Lastly, there is an ongo-\ning need for further research into the development of cost-effective and robust QA systems that can effectively resist misinformation.\nLimitations\nDespite the remarkable capabilities of GPT-3.5 (text-davinci-003) in generating high-quality textual content, one must not disregard its inherent limitations. Firstly, the reproducibility of its outputs presents a significant challenge. In order to mitigate this issue, we shall make available all prompts and generated documents, thereby facilitating the replication of our experiments. Secondly, the cost associated with GPT-3.5 is an order of magnitude greater than that of some of its contemporaries, such as ChatGPT (gpt-turbo-3.5), which inevitably constrained the scope of our investigations. The focus of this research lies predominantly on a selection of the most emblematic and pervasive QA systems and LLMs. Nonetheless, the findings derived from our analysis may not necessarily be applicable to other systems or text generators. For instance, QA systems employing alternative architectures, as demonstrated by recent works (Shao and Huang, 2022; Su et al., 2022), may exhibit increased robustness against the proliferation of misinformation.\nEthics Statement\nWe decide to publicly release our model-generated documents and the prompts used for creating them, despite the potential for misuse and generating harmful disinformation. We believe open sourcing is important and we justify our decision as follows.\nFirstly, since our model relies on the readily available OpenAI API, replicating our production process is feasible without access to the code. Our objective is to raise awareness and encourage action by investigating the consequences of misusing large language models. We aim to inform the public, policymakers, and developers about the need for responsible and ethical implementation.\nSecondly, our choice to release follows a similar approach taken with Grover (Zellers et al., 2019)7, a powerful detector and advanced generator of AIgenerated fake news. The authors of Grover argue that threat modeling, including a robust generator or simulation of the threat, is crucial to protect against potential dangers. In our research, we establish an effective threat model for ODQA in the context of misinformation. Future studies can build upon the transparency of our model, further enhancing our proposed defense techniques for AI safety."
        },
        {
            "heading": "Acknowledgements",
            "text": "This work was supported by the National Science Foundation Award #2048122. The views expressed are those of the authors and do not reflect the official policy or position of the US government. This research is also supported by the Ministry of Education, Singapore, under its MOE AcRF TIER 3 Grant (MOE-MOET32022-0001)."
        },
        {
            "heading": "A Configuration Details",
            "text": "Here we elaborate on the specific configurations used in our experiments for clarity.\nRationale behind the prompt template design. Table 5 summarizes the aforementioned four misinformation generation settings. We categorize their key features based on three dimensions: 1) maliciousness: if the setting intends to deliberately elicit misinformation, 2) resourcefulness: if the setting demands resources unavailable to regular users, such as a real factual article, and 3) customization: if the generated documents are designed to influence the target audience (in our case, machines). We believe these settings cover the common potential scenarios of misusing LLMs for misinformation generation.\nGenerator. We employed the default hyperparameter for GPT-3.5 (text-davinci-003) in all text-generation experiments, specified in the OpenAI API documentation8.\nPrompt template for plausible answer production. Since correct answers were not needed, we relied solely on the parametric memory of language models. The prompt we used is: Generate a false answer to the given question. It should be of short (less than five words in general) and look plausible, compared to the reference answer. Question: Question\nReference Answers: Answer\nTo ensure our plausible answers produced were indeed incorrect, we repeated the answer generation process five times and randomly sampled one answer that does not match any reference answer.\nODQA Model details.\n\u2022 BM25: We employed the implementation from (Lin et al., 2021).\n\u2022 DPR: We used the checkpoint provided by the original DPR repository9, trained on five QA datasets including NQ.\n\u2022 FiD: We used the FiD-large checkpoint provided by the original FiD repository10 trained on NQ.\n8https://platform.openai.com/docs/ api-reference/completions/create\n9https://github.com/facebookresearch/DPR/tree/ main\n10https://github.com/facebookresearch/FiD\nDetails of the defense strategies. For the detecting-based method, We employed the RobertaForSequenceClassification checkpoint provided by huggingface (Wolf et al., 2020). For both in-domain and out-of-domain classifiers, we used 12,000 context passages for training. We sampled data from a dataset containing both actual Wikipedia snippets and Wikipedia completions generated by GPT-3(Bhat, 2023), which share many similar properties with text generated in our experiments. The model is configured for 3 epochs of training with a learning rate of 0.001.\nFor the prompting-based method, we designed five different misinformation-aware prompts, and report the average EM score across these prompts. We drew inspiration from concurrent works (Hong et al., 2023; Li et al., 2023b) and engineering experience11, then utilized ChatGPT to produce five prompts in accordance to one human written example. We report each prompt and its respective performance in table 6. For the voting method, we explored various configurations of the number of readers k and the number of context passages used for each reader n. We report the best-performing configuration where k = 5 readers and n = 10 passages for each reader based on preliminary experiments."
        },
        {
            "heading": "B Retrieval Performance",
            "text": "Retrieval pollution. Table 7 presents the percentages of questions in our two benchmarks that contain synthetic disinformation, known as \u201cpoisoned\u201d questions. A question is considered poisoned if it includes at least one synthetic passage retrieved in the top-K passages.\nMachine-generated misinformation could easily infiltrate information retrieval systems. Comparing between generation settings, REVISE outperforms GENREAD in producing information more likely to be retrieved, thanks to its \u2018gold template\u2019 that its misinformation is based on. However, we observed that REIT degraded QA performance the most, which highlights the security risks of a deliberate attack on automated systems. Sparse retrievers are particularly brittle to targeted misinformation. When targeting the sparse retriever BM25, REIT can poison more than 90% of questions using only 10 context passages, and over 95% of questions using 100 context passages, indicating\n11https://www.promptingguide.ai/risks/ adversarial#add-defense-in-the-instruction\nthe fragility of sparse retrieval under deliberate attacks."
        },
        {
            "heading": "C Analysis on Corpus Quality",
            "text": "We conduct a brief analysis of the two corpora under our study regarding their informativeness to ODQA tasks, as shown in Table 8. Since the utilization of context passages containing gold answers (commonly referred to as gold evidence) is critical, we intend to find out about the qualities of gold evidence in both corpora. Specifically, we measure question coverage (Recall@100), volume (average mentions of answers) and relevance (average rank of the first gold evidence) of gold evidence in the two corpora. Using the DPR-retrieval result of 100 context passages, we observe that CovidNews corpus (News) provides significantly less informative evidence for answering questions, making it a challenging QA task. Furthermore, these topics are more susceptible to plausible assertions at the manipulation of propagandists for a deficit of\ncounterpart factual evidence."
        },
        {
            "heading": "D Human Detection of Generated Misinformation",
            "text": "We conducted a small-scale human study to explore the detectability of misinformation. We randomly sampled 50 fake documents generated under CTRLGEN and 50 corresponding most relevant Wikipedia passages. We employed experimental settings described in (Clark et al., 2021b), where participants need to rate each text on a 4-point scale. We hired three college students to each annotate the 100 documents. Similar to their findings, we found humans cannot reliably differentiate machine-generated misinformation from their Wikipedia counterparts, with an average overall accuracy of 57%. Note that we observed great performance improvements in the second half of experiments for all participants, which could mean that 57% is an overestimation of human capabilities since the participants displayed signs of learning\nthe sampled data in our experiments."
        }
    ],
    "title": "On the Risk of Misinformation Pollution with Large Language Models",
    "year": 2023
}