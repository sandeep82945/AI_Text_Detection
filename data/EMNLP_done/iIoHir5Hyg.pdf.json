{
    "abstractText": "Accurate knowledge selection is critical in knowledge-grounded dialogue systems. Towards a closer look at it, we offer a novel perspective to organize existing literature, i.e., knowledge selection coupled with, after, and before generation. We focus on the third underexplored category of study, which can not only select knowledge accurately in advance, but has the advantage to reduce the learning, adjustment, and interpretation burden of subsequent response generation models, especially LLMs. We propose GATE, a generator-agnostic knowledge selection method, to prepare knowledge for subsequent response generation models by selecting context-related knowledge among different knowledge structures and variable knowledge requirements. Experimental results demonstrate the superiority of GATE, and indicate that knowledge selection before generation is a lightweight yet effective way to facilitate LLMs (e.g., ChatGPT) to generate more informative responses.",
    "authors": [
        {
            "affiliations": [],
            "name": "Lang Qin"
        },
        {
            "affiliations": [],
            "name": "Yao Zhang"
        },
        {
            "affiliations": [],
            "name": "Hongru Liang"
        },
        {
            "affiliations": [],
            "name": "Jun Wang"
        },
        {
            "affiliations": [],
            "name": "Zhenglu Yang"
        }
    ],
    "id": "SP:a8c7e033dff7774b458a054c0a9e26fa567183e0",
    "references": [
        {
            "authors": [
                "Jiaqi Bai",
                "Ze Yang",
                "Jian Yang",
                "Hongcheng Guo",
                "Zhoujun Li."
            ],
            "title": "Kinet: Incorporating relevant facts into knowledge-grounded dialog generation",
            "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing, 31:1213\u20131222.",
            "year": 2023
        },
        {
            "authors": [
                "Shaked Brody",
                "Uri Alon",
                "Eran Yahav"
            ],
            "title": "How attentive are graph attention networks",
            "venue": "In International Conference on Learning Representations",
            "year": 2022
        },
        {
            "authors": [
                "Xiuyi Chen",
                "Fandong Meng",
                "Peng Li",
                "Feilong Chen",
                "Shuang Xu",
                "Bo Xu",
                "Jie Zhou."
            ],
            "title": "Bridging the gap between prior and posterior knowledge selection for knowledge-grounded dialogue generation",
            "venue": "Proceedings of the 2020 Conference on Empirical",
            "year": 2020
        },
        {
            "authors": [
                "Michael Denkowski",
                "Alon Lavie."
            ],
            "title": "Meteor universal: Language specific translation evaluation for any target language",
            "venue": "Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 376\u2013380, Baltimore, Maryland, USA. Association",
            "year": 2014
        },
        {
            "authors": [
                "Emily Dinan",
                "Stephen Roller",
                "Kurt Shuster",
                "Angela Fan",
                "Michael Auli",
                "Jason Weston."
            ],
            "title": "Wizard of Wikipedia: Knowledge-powered conversational agents",
            "venue": "Proceedings of the International Conference on Learning Representations (ICLR).",
            "year": 2019
        },
        {
            "authors": [
                "Nouha Dziri",
                "Andrea Madotto",
                "Osmar Za\u00efane",
                "Avishek Joey Bose."
            ],
            "title": "Neural path hunter: Reducing hallucination in dialogue systems via path grounding",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Process-",
            "year": 2021
        },
        {
            "authors": [
                "Mihail Eric",
                "Nicole Chartier",
                "Behnam Hedayatnia",
                "Karthik Gopalakrishnan",
                "Pankaj Rajan",
                "Yang Liu",
                "Dilek Hakkani-Tur."
            ],
            "title": "Multi-sentence knowledge selection in open-domain dialogue",
            "venue": "Proceedings of the 14th International Conference on Natural",
            "year": 2021
        },
        {
            "authors": [
                "Marjan Ghazvininejad",
                "Chris Brockett",
                "Ming-Wei Chang",
                "Bill Dolan",
                "Jianfeng Gao",
                "Wen-tau Yih",
                "Michel Galley."
            ],
            "title": "A knowledge-grounded neural conversation model",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, 32.",
            "year": 2018
        },
        {
            "authors": [
                "Maarten Grootendorst"
            ],
            "title": "Keybert: Minimal keyword extraction with bert",
            "year": 2020
        },
        {
            "authors": [
                "Jaehun Jung",
                "Bokyung Son",
                "Sungwon Lyu."
            ],
            "title": "AttnIO: Knowledge Graph Exploration with In-andOut Attention Flow for Knowledge-Grounded Dialogue",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing",
            "year": 2020
        },
        {
            "authors": [
                "Mike Lewis",
                "Yinhan Liu",
                "Naman Goyal",
                "Marjan Ghazvininejad",
                "Abdelrahman Mohamed",
                "Omer Levy",
                "Veselin Stoyanov",
                "Luke Zettlemoyer"
            ],
            "title": "BART: Denoising sequence-to-sequence pre-training for natural language",
            "year": 2020
        },
        {
            "authors": [
                "Sha Li",
                "Mahdi Namazifar",
                "Di Jin",
                "Mohit Bansal",
                "Heng Ji",
                "Yang Liu",
                "Dilek Hakkani-Tur."
            ],
            "title": "Enhancing knowledge selection for grounded dialogues via document semantic graphs",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the",
            "year": 2022
        },
        {
            "authors": [
                "Rongzhong Lian",
                "Min Xie",
                "Fan Wang",
                "Jinhua Peng",
                "Hua Wu."
            ],
            "title": "Learning to select knowledge for response generation in dialog systems",
            "venue": "Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI 2019, Macao, China,",
            "year": 2019
        },
        {
            "authors": [
                "Chin-Yew Lin."
            ],
            "title": "ROUGE: A package for automatic evaluation of summaries",
            "venue": "Text Summarization Branches Out, pages 74\u201381, Barcelona, Spain. Association for Computational Linguistics.",
            "year": 2004
        },
        {
            "authors": [
                "Zhibin Liu",
                "Zheng-Yu Niu",
                "Hua Wu",
                "Haifeng Wang."
            ],
            "title": "Knowledge aware conversation generation with explainable reasoning over augmented graphs",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the",
            "year": 2019
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter."
            ],
            "title": "Decoupled weight decay regularization",
            "venue": "arXiv preprint arXiv:1711.05101.",
            "year": 2017
        },
        {
            "authors": [
                "Qingyu Lu",
                "Baopu Qiu",
                "Liang Ding",
                "Liping Xie",
                "Dacheng Tao"
            ],
            "title": "Error analysis prompting enables human-like translation evaluation in large language models: A case study on chatgpt",
            "year": 2023
        },
        {
            "authors": [
                "Zheheng Luo",
                "Qianqian Xie",
                "Sophia Ananiadou"
            ],
            "title": "Chatgpt as a factual inconsistency evaluator for text summarization",
            "year": 2023
        },
        {
            "authors": [
                "Chuan Meng",
                "Pengjie Ren",
                "Zhumin Chen",
                "Zhaochun Ren",
                "Tengxiao Xi",
                "Maarten de Rijke."
            ],
            "title": "Initiative-aware self-supervised learning for knowledge-grounded conversations",
            "venue": "Proceedings of the 44th International ACM SIGIR Conference on",
            "year": 2021
        },
        {
            "authors": [
                "Chuan Meng",
                "Pengjie Ren",
                "Zhumin Chen",
                "Weiwei Sun",
                "Zhaochun Ren",
                "Zhaopeng Tu",
                "Maarten de Rijke."
            ],
            "title": "Dukenet: A dual knowledge interaction network for knowledge-grounded conversation",
            "venue": "Proceedings of the 43rd International ACM SIGIR",
            "year": 2020
        },
        {
            "authors": [
                "Seungwhan Moon",
                "Leonardo Neves",
                "Vitor Carvalho."
            ],
            "title": "Multimodal named entity recognition for short social media posts",
            "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Lan-",
            "year": 2018
        },
        {
            "authors": [
                "Seungwhan Moon",
                "Pararth Shah",
                "Anuj Kumar",
                "Rajen Subba."
            ],
            "title": "OpenDialKG: Explainable conversational reasoning with attention-based walks over knowledge graphs",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational",
            "year": 2019
        },
        {
            "authors": [
                "Kishore Papineni",
                "Salim Roukos",
                "Todd Ward",
                "WeiJing Zhu."
            ],
            "title": "Bleu: a method for automatic evaluation of machine translation",
            "venue": "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311\u2013318, Philadelphia,",
            "year": 2002
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J. Liu."
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "J. Mach. Learn. Res., 21(1).",
            "year": 2020
        },
        {
            "authors": [
                "Nils Reimers",
                "Iryna Gurevych."
            ],
            "title": "SentenceBERT: Sentence embeddings using Siamese BERTnetworks",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natu-",
            "year": 2019
        },
        {
            "authors": [
                "Rajdeep Sarkar",
                "Mihael Arcan",
                "John McCrae"
            ],
            "title": "KG-CRuSE: Recurrent walks over knowledge graph",
            "year": 2022
        },
        {
            "authors": [
                "Kurt Shuster",
                "Spencer Poff",
                "Moya Chen",
                "Douwe Kiela",
                "Jason Weston."
            ],
            "title": "Retrieval augmentation reduces hallucination in conversation",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2021, pages 3784\u20133803, Punta Cana, Do-",
            "year": 2021
        },
        {
            "authors": [
                "Leslie N. Smith",
                "Nicholay Topin."
            ],
            "title": "Superconvergence: very fast training of neural networks using large learning rates",
            "venue": "Artificial Intelligence and Machine Learning for Multi-Domain Operations Applications, volume 11006, page 1100612. Interna-",
            "year": 2019
        },
        {
            "authors": [
                "Yiming Tan",
                "Dehai Min",
                "Yu Li",
                "Wenbo Li",
                "Nan Hu",
                "Yongrui Chen",
                "Guilin Qi"
            ],
            "title": "Evaluation of chatgpt as a question answering system for answering complex questions",
            "year": 2023
        },
        {
            "authors": [
                "Yi-Lin Tuan",
                "Sajjad Beygi",
                "Maryam Fazel-Zarandi",
                "Qiaozi Gao",
                "Alessandra Cervone",
                "William Yang Wang."
            ],
            "title": "Towards large-scale interpretable knowledge graph reasoning for dialogue systems",
            "venue": "Findings of the Association for Computational Lin-",
            "year": 2022
        },
        {
            "authors": [
                "Jiaan Wang",
                "Yunlong Liang",
                "Fandong Meng",
                "Zengkui Sun",
                "Haoxiang Shi",
                "Zhixu Li",
                "Jinan Xu",
                "Jianfeng Qu",
                "Jie Zhou"
            ],
            "title": "Is chatgpt a good nlg evaluator? a preliminary study",
            "year": 2023
        },
        {
            "authors": [
                "Jason Wei",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Maarten Bosma",
                "Brian Ichter",
                "Fei Xia",
                "Ed Chi",
                "Quoc Le",
                "Denny Zhou"
            ],
            "title": "Chain-of-thought prompting elicits reasoning in large language models",
            "year": 2023
        },
        {
            "authors": [
                "Ronald J. Williams."
            ],
            "title": "Simple statistical gradientfollowing algorithms for connectionist reinforcement learning",
            "venue": "Mach. Learn., 8(3\u20134):229\u2013256.",
            "year": 1992
        },
        {
            "authors": [
                "Qiang Xue",
                "Tetsuya Takiguchi",
                "Yasuo Ariki."
            ],
            "title": "Building a knowledge-based dialogue system with text infilling",
            "venue": "Proceedings of the 23rd Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 237\u2013243, Edinburgh, UK. As-",
            "year": 2022
        },
        {
            "authors": [
                "Chenxu Yang",
                "Zheng Lin",
                "Jiangnan Li",
                "Fandong Meng",
                "Weiping Wang",
                "Lanrui Wang",
                "Jie Zhou."
            ],
            "title": "TAKE: Topic-shift aware knowledge sElection for dialogue generation",
            "venue": "Proceedings of the 29th International Conference on Computational Linguistics,",
            "year": 2022
        },
        {
            "authors": [
                "Xueliang Zhao",
                "Wei Wu",
                "Can Xu",
                "Chongyang Tao",
                "Dongyan Zhao",
                "Rui Yan."
            ],
            "title": "Knowledgegrounded dialogue generation with pre-trained language models",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language",
            "year": 2020
        },
        {
            "authors": [
                "Hao Zhou",
                "Minlie Huang",
                "Yong Liu",
                "Wei Chen",
                "Xiaoyan Zhu."
            ],
            "title": "EARL: Informative knowledge-grounded conversation generation with entity-agnostic representation learning",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Knowledge-grounded dialogue systems generate informative responses by incorporating external knowledge, such as unstructured documents and structured knowledge graphs (Ghazvininejad et al., 2018; Lian et al., 2019). This generation process requires an agent to select context-related knowledge to support high user engagement. Taking Figure 1 (a) as an example, the knowledge \u201cMark Boal wrote Zero Dark Thirty\u201d contributes to a high-quality response compared with the knowledge \u201cZero Dark Thirty has genre War film\u201d, when the user focuses on \u201cwho wrote Zero Dark Thirty\u201d.\nMany efforts have been devoted to selecting context-relevant knowledge, and we classify them\n\u2217Corresponding author.\ninto three categories based on the occasion when knowledge selection is performed. The first category is co-selection (Zhao et al., 2020; Tuan et al., 2022; Bai et al., 2023) wherein knowledge selection and response generation are executed in a coupled manner (c.f. Figure 1 (b)-\u2780). Although this category of approach is efficient and has been extensively researched, it is costly to learn and is difficult to interpret and adjust when errors arise in the generated responses. The second category is post-selection (Dziri et al., 2021; Xue et al., 2022), namely, the knowledge is selected after the generation and is used to correct the knowledge error in the generated response (c.f. Figure 1 (b)-\u2781). This category is skilled in adjusting local errors yet has minimal impact on enhancing the informativeness of the response.\nSurprisingly, the third category, pre-selection, has been under-explored by previous research. This category performs knowledge selection as an independent model before response generation (c.f. Figure 1 (b)-\u2782). In this paper, we pay attention to this study, not only because of the overlook by current work but also because selecting knowledge accurately in advance can provide the potential to reduce the learning, adjustment, and interpretation burden of subsequent response generation models, especially for large language models (LLMs). To select knowledge for preparation, in addition to the accuracy required for knowledge selection, there are two key issues that need to be addressed:\n\u2022 Different knowledge structure. An ideal knowledge selector should be able to tackle different knowledge structures, such as unstructured knowledge represented by text and structured knowledge represented by knowledge graphs.\n\u2022 Variable knowledge requirement. Typically, the\nnumber of knowledge required to generate an informative response is neither one nor a fixed number but a variable number. For example, when the dialogue is about an award-winning film rather than a little-known one, there is much more relevant knowledge involved. An ideal knowledge selector should be able to dynamically adapt the number of selected knowledge.\nTo resolve the above issues, we propose GATE, a Generator-AgnosTic knowledgE selection method to prepare knowledge for subsequent various response generation models, e.g., BART (Lewis et al., 2020) and T5 (Raffel et al., 2020). GATE has the ability to confront both unstructured and structured knowledge and to adapt the number of desired knowledge according to different dialogue contexts. Specifically, GATE consists of: knowledge structure unifying, knowledge scoring, and knowledge pool size adapting module. Further, we employ a reinforcement learning (RL) framework to train GATE, optimizing the reward of selecting appropriate knowledge in both quality and quantity. The experimental results on two datasets demonstrate the superiority of GATE on knowledge selection, and that GATE can facilitate the response generation model (including ChatGPT) to generate more informative responses. We believe that GATE provides a lightweight and efficient solution, which is a potentially viable way for reducing the learning, adjustment, and interpretation burden of LLMs. In summary, the main contributions of this work are as follows.\n\u2022 We introduce a novel perspective to organize the\nliterature of knowledge selection in knowledgegrounded dialogue, i.e., knowledge selection coupled with, after, and before generation. Besides, we point out that the third category of study, though under-explored, has advantages to reduce the learning, adjustment, and interpretation burden of subsequent response generation models.\n\u2022 We propose GATE, a generator-agnostic knowledge selection method, to prepare knowledge for subsequent response generators by selecting context-related knowledge among different knowledge structures and variable knowledge requirements.\n\u2022 We conduct experiments to demonstrate the superiority of GATE, and find that knowledge preselection is a lightweight and effective way to facilitate ChatGPT to generate more informative responses."
        },
        {
            "heading": "2 Related Work",
            "text": "Knowledge selection is a crucial step in the knowledge-grounded dialogue system. Our work provides a new perspective to review the literature of knowledge-grounded dialogue\u2014\u2014based on different time points of knowledge selection in the response generation process (c.f., Figure 1(b)). Knowledge selection coupled with Generation This knowledge selection category denotes that the knowledge selection and response generation processes are modeled to be executed concurrently in a single model, as shown in Figure 1 (b)-\u2780. For unstructured knowledge, the Co-Selection process is an interactive matching process between the\ndialogue and the documents (Meng et al., 2020, 2021). Dinan et al. (2019) utilizes dot product attention to select the most relevant knowledge. Bai et al. (2023) improves selection by enhancing knowledge\u2019s dense representation. For structured knowledge, the selection process can be viewed as a multi-hop reasoning procedure on a graph, which is subsequently followed by a two-stage architecture for response generation (Liu et al., 2019; Zhou et al., 2021; Tuan et al., 2022). However, the construction or training of the above methods is tied to the generation model. In contrast, GATE is \u201cplugand-play\u201d and can enhance response generation for various generation models.\nKnowledge selection after Generation This knowledge selection category denotes that knowledge selection is executed as an independent model after response generation, as shown in Figure 1 (b)-\u2781. Post-selection is dedicated to correcting potential knowledge errors in the response. Dziri et al. (2021) address hallucinations in responses by replacing them with correct knowledge obtained from the knowledge graph. Xue et al. (2022) employ text infilling to incorporate retrieved knowledge into incomplete responses. However, these methods would diminish the fluency and naturalness of responses, whereas GATE does not compromise generation models.\nKnowledge selection before Generation This knowledge selection category denotes that knowledge selection is executed as an independent model before response generation, as shown in Figure 1 (b)-\u2782. Pre-selection is dedicated to improving the accuracy of knowledge selection and further enhancing the quality of response generation. A few works have actually employed Pre-Selection without emphasizing or providing a formal definition. Jung et al. (2020) implement graph-based reasoning through attention flows to select knowledge (i.e., paths in the graph). Eric et al. (2021) collected an augmented dataset and proposed a ranking-generation pipeline to evaluate it. Li et al. (2022) constructs semantic graphs of textual knowledge and performs selection based on node similarity. Yang et al. (2022) proposed a topic-shift aware knowledge selector to utilize the role-initiative information to help select knowledge. However, the above methods are designed to address specific knowledge types, while GATE can select knowledge across different knowledge bases.\nIn summary, existing methods are constrained\nto specific knowledge types or generation models, significantly limiting the generalization ability, and rely on a fixed-size knowledge pool, which could undermine the performance of response (Shuster et al., 2021). In contrast, GATE operates in a PreSelection category that can handle diverse knowledge types and adapt the knowledge pool size to enhance various generation models."
        },
        {
            "heading": "3 Methodology",
            "text": ""
        },
        {
            "heading": "3.1 Overview",
            "text": "We first formulate the knowledge-grounded dialogue as follows: at t-turn conversation, given the dialogue history Xt and currently accessible knowledge base K, the system generates response Yt based on the dialogue-relevant knowledge set Kt \u2286 K. Then, the knowledge pre-selection task that we focus on refers to how to select a more useful knowledge set K\u2217t from the knowledge set Kt before the response generation process. \u201cUseful\u201d here means that K\u2217t can help the agent generate high-quality responses.\nTo select a more useful knowledge set K\u2217t , GATE performs three efforts:\n\u2022 Unifying knowledge of diverse structure types (unstructured and structured), to improve the generalization and flexibility of GATE.\n\u2022 Scoring knowledge to help select the knowledge that is more relevant to the desired response.\n\u2022 Adapting knowledge pool size to provide appropriately sized knowledge sets for subsequent response generation models.\nWe will introduce the details of GATE in Section 3.2, the reinforcement learning framework for GATE in Section 3.3, and the optimization and training details of GATE in Section 3.4.\n3.2 GATE\nKnowledge Structure Unifying GATE first unifies knowledge of diverse structure types to improve its generalization and flexibility. In general, there are two types of knowledge structures used by the knowledge-grounded dialogue system: unstructured and structured knowledge. Unstructured knowledge generally exists in the form of documents, and structured knowledge typically uses knowledge graphs. Graph structures are lengthy in modeling the association information between\nknowledge and can help to select useful knowledge more accurately. Moreover, graph structures can enhance the interpretability of the knowledge selection process. Therefore, GATE uniformly transforms all the diverse types of knowledge into a graph structure, which has two types of nodes, i.e., process node and knowledge node.\nFor document-based unstructured knowledge, it naturally has a topic \u226b article title \u226b sentence hierarchy. GATE presents this hierarchy as a graph, where topics and article titles are used as process nodes in the graph, sentences are used as knowledge nodes, and nodes are connected to nodes by edges if there is a containment relation, as shown in Figure 2 (a). For the structured knowledge graph, GATE keeps its original structure unchanged, and all the original nodes are used as process nodes. GATE will additionally add knowledge nodes 1. For the triple (ei, rij , ej) formed by two entity nodes (ei and ej) and the relation edge (rij) between them, GATE merges them into a single knowledge node and connects them to the head entity (ei) in this triple, as shown in Figure 2 (b). Knowledge Scoring After unifying the knowledge structure, GATE next scores each process node in the graph to help select the knowledge that is more relevant to the desired response.\nFirstly, a subgraph Adjt is obtained based on the valid state transition target of the Agent. As shown in Figure 3, the encoding of the Agent\u2019s state is concatenated with the encoding of each process node in the subgraph. We utilize a Graph Attention Network (GAT) (Brody et al., 2022) to score each node and then sample to obtain the target for state\n1It has been demonstrated that using fact triples can help dialogue systems generate high-quality responses better than separate entities and relations. (Dziri et al., 2021; Sarkar et al., 2022).\ntransition:\nscoren = GAT([St; en] | for n in Adjt) . (1)\nThe knowledge nodes of sorted process nodes form Kt. Considering the guiding role of process node scores in knowledge selection, we calculate node attention weights based on the score distribution using MLP. These weights are utilized in dot-product calculations with the Agent state and knowledge encoding to determine the score of each knowledge in Kt. Knowledge Pool Size Adapting GATE determines the knowledge pool\u2019s appropriate size by analyzing the node score distribution variance. The topranked knowledge is selected to constitute K\u2217t , where M(\u00b7) maps the input to the interval [0,1]:\n|K\u2217t | = |Kt| \u2217M (\n1\n1 -Var (scoren)\n) . (2)\n3.3 RL Framework for GATE\nGraph-based reasoning improves the interpretability of knowledge selection. Due to the extensive use and success of reinforcement learning in this field, we formulate the knowledge selection process as a Markov Decision Process and employ reinforcement learning for graph-based reasoning. State We employ SentenceBert (Reimers and Gurevych, 2019) to perform static encoding of the nodes and knowledge within the graph structure outlined in Section 3.2. We encode each piece of knowledge attached to a node as {eki} |eki | i=1 , and use the mean-pooling of these encodings as the node\u2019s\nencoding: en = MeanPool ( {eki} |eki | i=1 ) . We\nleverage KeyBert (Grootendorst, 2020) to extract keywords Wt from the conversation history X1:t\u22121 and user statement Xt. Then, we use a modified multi-head hierarchical modality attention mechanism (Moon et al., 2018) to process all input information: x = Attention (X1:t\u22121,Xt,Wt).\nAfter initializing and encoding the graph, we employ GAT to update the encoding of the entire graph at the beginning of the Agent\u2019s traversal: {eni} |eni | i=1 = GAT ( {eni} |eni | i=1 ) .\nAs the Agent traverses the graph, we update its state considering the node ent that the Agent is located at time t:\nSt = Attention (x;St\u22121; ent) . (3)\nAction The Agent\u2019s traversal on the graph is a Markov Decision Process with a maximum step count of T . The Agent\u2019s action encompasses moving to the next node and constructing the corresponding knowledge pool. The action space is the set of one-hop neighbors of the current node. Reward To accomplish our objective of enhancing the quality and quantity of knowledge selection, we optimize the model in three ways:\n\u2022 Enhancing the Agent\u2019s ability to traverse the graph and reach the correct process node, we have RNode as positive if the Agent halts at the correct process node and negative otherwise.\n\u2022 Improving the ability to select appropriate knowledge from the knowledge pool, we design RGold:\nRGold = Max (1 -\u03b1 \u2217 r (kgold) , - 1) , (4)\nwhere r refers to the rank of ground-truth knowledge kgold in K\u2217t , and RGold is negative if not select kgold.\n\u2022 Adaptively determining an appropriate knowledge pool size. If the model selects the correct knowledge with a high probability, excessive knowledge can be deemed redundant. Otherwise, expanding the knowledge pool is necessary to enhance knowledge effectiveness. Hence, we design RPool as RGold/|K\u2217t |.\nThe complete reward function is as follows:\nRst = RNode +RGold +RPool. (5)\nPolicy Network We design a policy network \u03c0\u03b8 (at | st) = P (at | st; \u03b8) to obtain the probability distribution of actions, where \u03b8 represents the model parameters utilized in the Knowledge Scoring process and RL Framework."
        },
        {
            "heading": "3.4 Optimization and Training",
            "text": "The optimization objective of our policy network is to maximize the expected cumulative reward:\nJ(\u03b8) = Ea\u223c\u03c0(a|s;\u03b8) (\u2211 t Rst ) . (6)\nWe use the following stochastic gradient based on the REINFORCE algorithm (Williams, 1992) to optimize \u03b8:\n\u2207\u03b8J(\u03b8) \u2248 \u2207\u03b8 \u2211 t Rst log \u03c0\u03b8 (at | st) . (7)\nIn order to effectively utilize the available supervised information inherent in the graph, we integrate node loss LNode and knowledge loss LKnowledge using standard cross-entropy. We derive LWalk by taking the negative value of J(\u03b8). Then, the complete loss function is as follows:\nL = LWalk + LNode + LKnowledge. (8)\nThe introduced model uniquely incorporates the aforementioned selection methods while maintaining its independence from any specific generator."
        },
        {
            "heading": "4 Experiments",
            "text": "We conduct experiments in knowledge selection and response generation to investigate the effectiveness of knowledge Pre-Selection in GATE. We also analyze the auxiliary capability of knowledge Pre-Selection on the generation model and the advantages of GATE adaptive knowledge pool size."
        },
        {
            "heading": "4.1 Datasets",
            "text": "We conduct experiments on Wizard of Wikipedia (WoW) (Dinan et al., 2019) and OpenDialKG (Moon et al., 2019) datasets for unstructured and structured knowledge bases. The statistics of the two datasets are presented in Appendix A.\nIn WoW, two participants take roles as a wizard and an apprentice. The wizard selects proper knowledge (sentence) from Wikipedia for the response. WoW split test set into Seen and Unseen based on topics. OpenDialKG is a parallel corpus comprising open-domain dialogues and a knowledge graph. The reasoning path of each turn is annotated, enabling participants to utilize graph information during the conversation.\nDue to the absence of an official split in OpenDialKG, we follow WoW by dividing the dataset into Seen/Unseen categories based on topics."
        },
        {
            "heading": "4.2 Baselines",
            "text": "We choose the following four types of baselines: 1) Trivial baselines\nRandom: it randomly selects knowledge from the candidate pool;\nSemantic: it selects knowledge based on semantic similarity between dialogue and knowledge.\n2) Methods using Co-Selection\nTMNet (Dinan et al., 2019): it combines memory network architecture and Transformer to encode dialogue and knowledge for response generation;\nSKT++ (Chen et al., 2020): it utilizes a Posterior Information Prediction Module and a Knowledge Distillation-Based Training Strategy;\nMIKE (Meng et al., 2021): it introduces an initiative discriminator for knowledge selection;\nKnowledGPT (Zhao et al., 2020): it utilizes pre-trained language models and optimizes via unsupervised learning;\nKINET (Bai et al., 2023): it introduces a negative-enhanced knowledge approximator and a curriculum knowledge sampler;\nDiffKG (Tuan et al., 2022): it employs Transformer to generate relation sequences on KG and generates responses based on retrieved entities.\n3) Methods using Post-Selection\nNPH (Dziri et al., 2021): it retrieves correct entities by crafting a query signal propagated over a graph to refine hallucination in response.\n4) Methods using Pre-Selection\nDialKG (Moon et al., 2019): it models the symbolic transitions as structured traversal on KG and predicts entities with a graph path decoder;\nAttnIO (Jung et al., 2020): it flexibly adjusts the nodes and edges of focus based on dialogue context via attention flow.\nAmong the aforementioned baselines, TMNet, SKT++, MIKE, KnowledGPT, and KINET are utilized for unstructured knowledge, whereas DialKG, AttnIO, DiffKG, and NPH are utilized for structured knowledge."
        },
        {
            "heading": "4.3 Metrics",
            "text": "GATE strives to improve the accuracy of knowledge selection and further enhance the quality of response generation. Following previous works (Jung et al., 2020; Meng et al., 2021; Bai et al., 2023), we employ 1) ROUGE (Lin, 2004), BLEU (Papineni et al., 2002), Meteor (Denkowski and Lavie, 2014), and unigram overlap (F1) (Dinan et al., 2019) to evaluate the quality of the generated responses; and\n2) Recall@k, which calculates the percentage of ground-truth knowledge included in the top-k selections, to evaluate the knowledge selection accuracy."
        },
        {
            "heading": "4.4 Implementation Details",
            "text": "We employ AdamW optimizer (Loshchilov and Hutter, 2017) with weight decay 0.12 and OneCycle policy (Smith and Topin, 2019). The activation function is LeakyReLU with a negative slope of 0.21. GATE is trained on a single RTX A5000. We use the same pre-trained checkpoints as the state-ofthe-art approaches to ensure fairness. Specifically, we employ BART2 and T53. The code is available at https://github.com/qinlang14/GATE."
        },
        {
            "heading": "4.5 Results and Observations",
            "text": "Overall Performance Table 1 and Table 2 show that GATE markedly outperforms all baselines in terms of accuracy of knowledge selection and quality of the generated responses. To ensure the reliability of the results, we conduct additional experiments using the original data from OpenDialKG as Figure 4. We make the following observations:\n\u2022 GATE outperforms previous SOTA methods in 2https://huggingface.co/facebook/bart-base 3https://huggingface.co/allenai/ unifiedqa-t5-base\nknowledge selection as measured by R@1. Our reinforcement learning-based graph reasoning method for unified knowledge types effectively selects relevant knowledge.\n\u2022 GATE outperforms models utilizing the same generation module (e.g., Bart, T5) across multiple metrics. The knowledge selected by GATE significantly enhances generation modules.\n\u2022 Specifically, in Table 1, the NPH method utilizing Post-Selection achieves considerable results through entity replacement but still falls short compared to using GATE\u2019s Pre-selection. Table 2 demonstrates that Co-Selection methods such as TMNet and KINET are constrained by the pretrained models, whereas GATE can be generatoragnostic that selects relevant knowledge without being reliant on the generation model.\nPerformance w.r.t. ChatGPT+GATE To evaluate the auxiliary capability of our model, we input the knowledge selected by GATE into ChatGPT as supplementary information. We provide conversation history and instruct ChatGPT to generate responses using the selected knowledge or relying solely on its internal knowledge. Considering the cost of human evaluation, we follow recent works (Wang et al., 2023; Luo et al., 2023) to assess responses by ChatGPT. Detailed prompts are in Appendix B. The\nresults in Table 3 demonstrate that, although similar to the findings in Shuster et al. (2021) that utilizing redundant knowledge may impair engagement, the knowledge selected by GATE significantly enhances the information quality of responses. This improvement persists even when employing ChatGPT, already known for its powerful performance and vast implicit knowledge. Moreover, the inconsistency in engaging may stem from different knowledge quantities: the WoW dataset with complete sentences and overly detailed knowledge, causing rigid responses, and the OpenDialKG dataset with concise triplets, causing engaging responses. Performance w.r.t. Adaptive Pool Size Figure 5 shows the response performance under different knowledge pool sizes and illustrates that GATE determines the appropriate size for dialogues. The range of knowledge pool sizes is up to 50 due to the limitation of input tokens that the generation model can accept. In WoW, sentences as knowl-\nedge have higher informativeness, resulting in a relatively smaller number of required knowledge pieces. In OpenDialKG, triplets have less information, requiring a larger knowledge pool to obtain satisfactory results. \u201cGATE(Fixed)\u201d demonstrates that a certain amount of knowledge is necessary for better responses. \u201cGATE\u201d demonstrates that our method can select an appropriate number of knowledge pieces, resulting in high-quality responses. GATE offers improved flexibility and effectiveness compared to previous works that solely utilize a fixed and inadequate selection of top-k knowledge."
        },
        {
            "heading": "4.6 Ablation Study",
            "text": "As outlined in Section 3.4, the loss function comprises three components: walk, node, and knowledge. Table 5 presents the respective influence of each component on knowledge selection. Our model attains optimal performance by leveraging the synergistic effect of three components within the loss function. In particular, LKnowledge serves as the universal optimization objective for this task, acting as the cornerstone for knowledge selection accuracy. LWalk corresponds to the loss in the reinforcement learning of our model, while LNode represents the loss in node selection after unifying the knowledge types. The combined influence of these two specialized components empower our model to achieve significantly higher accuracy than using LKnowledge alone."
        },
        {
            "heading": "4.7 Case Study",
            "text": "As shown in Table 4, we conduct a case study on a sample from OpenDialKG. The dialogue topic shifted from movies to books, requiring the agent to provide recommendations of works by a specified author. DiffKG provides an appealing response but incorrectly \u201cconcatenates\u201d the triples in its CoSelection process, as the true author of \u201cConfederacy of Dunces\u201d is John Kennedy Toole. NPH utilizes Post-Selection based on the response from GATE, resulting in the disruption of sentence semantics, which is an inherent drawback of entity substitution methods. In contrast, GATE acquires relevant knowledge through Pre-Selection and produces an accurate and fluent response."
        },
        {
            "heading": "5 Conclusion",
            "text": "This paper offers a novel perspective to organize the literature on knowledge selection in knowledgegrounded dialogue systems, i.e., knowledge selection coupled with, after, and before generation. This paper focuses on the third category and proposes GATE, a generator-agnostic knowledge selection method, which prepares knowledge for subsequent response generation models by selecting context-related knowledge among different knowledge structures and variable knowledge requirements. The experimental results demonstrate the superiority of GATE on knowledge selection, and that GATE can facilitate the response generation model (including ChatGPT) to generate more informative responses.\nLimitation\nDespite we have conducted experiments demonstrating the remarkable ability of GATE in improving the performance of ChatGPT and producing more informative responses, there is still ample scope for further exploration regarding the contribution of our model to LLMs. For example, We can try to combine GATE with more advanced prompts techniques developed recently (Wei et al., 2023; Tan et al., 2023; Lu et al., 2023) to facilitate more LLMs. We believe this will be an effective way to amplify the capability of GATE."
        },
        {
            "heading": "Acknowledgements",
            "text": "This work was supported in part by the National Natural Science Foundation of China (No.62206191, No.62306156 and No.62106091);\nin part by the China Postdoctoral Science Foundation (No.2021TQ0222 and No.2021M700094); in part by the Natural Science Foundation of Sichuan (No.2023NSFSC0473); in part by the Shandong Provincial Natural Science Foundation (No.ZR2021MF054); and in part by the Fundamental Research Funds for the Central Universities, Sichuan University (No.2023SCU12089) and Nankai University (No.63231183)."
        },
        {
            "heading": "A Dataset Statistics",
            "text": "As shown in Table 6, WoW has 22,311 conversations and a test set split into Seen and Unseen based on topics. Consistent with previous research, we employ the knowledge retrieved for the last two turns as the knowledge pool. OpenDialKG has 15,673 conversations and a knowledge graph that contains 100K entities and 1.1M facts. The reasoning path of each turn is annotated, enabling participants to utilize graph information during the conversation. We maintain the train/valid/test sets in a 70%/15%/15% ratio, consistent with previous works."
        },
        {
            "heading": "B Prompts for ChatGPT",
            "text": "Tables 7 and Tables 8 contain the prompts for generation and evaluation to derive the results presented in Table 3."
        },
        {
            "heading": "C Supplementary Ablation Study",
            "text": "Table 9 presents the extended results of Table 5, measuring the performance on R@1/5/10. The three components have demonstrated effectiveness in improving the overall selection performance rather than solely focusing on enhancing R@1.\nAs demonstrated in Table 10, our proposed node attention and the corresponding dot product calculation effectively enhance R@1/5. This suggests that the node representations obtained through the aggregation of knowledge embeddings possess robust expressive capabilities. The scoring mechanism during the agent\u2019s traversal process and the corresponding selection strategy are effective, particularly evident in the case of WoW Test Unseen, which suggests that the guiding role of node scores in knowledge selection remains effective even for zero-shot topics. The decrease in R@10 performance could be attributed to the amplification of knowledge score variance by node attention. Moreover, the enhancement in knowledge selection accuracy contributes to improving the quality of response generation."
        },
        {
            "heading": "D Case Study for WoW",
            "text": "To ensure the credibility of the comparisons, we conducted a case study using samples previously discussed in the relevant literature (Bai et al., 2023). As shown in Table 4, the dialogue topic is \u201cVeterinary physician\u201d and the Wizard is asked the question, \u201cWhat makes you want to be a veterinarian?\u201d.\nIn the ground-truth data, the Wizard responds with \u201cwanting to help animals\u201d based on the description of the veterinary role in selected knowledge. In contrast, MIKE\u2019s generated response incorrectly assumes that the Wizard is already a veterinarian, disregarding the dialogue context. KnowledGPT\u2019s response lacks confidence and fails to provide adequate reasoning. While KINET offers reasons for pursuing a career as a veterinarian, its emphasis on the \u201cclinical environment\u201d does not align with the preceding context and relevant knowledge. Conversely, GATE provides a comprehensive and contextually appropriate answer regarding \u201cbecoming a veterinarian,\u201d demonstrating its capability to effectively select suitable knowledge for generating coherent and engaging responses."
        }
    ],
    "title": "Well Begun is Half Done: Generator-agnostic Knowledge Pre-Selection for Knowledge-Grounded Dialogue",
    "year": 2023
}