{
    "abstractText": "Despite exciting recent results showing visionlanguage systems\u2019 capacity to reason about images using natural language, their capacity for video reasoning remains underexplored. We motivate framing video reasoning as the sequential understanding of a small number of keyframes, thereby leveraging the power and robustness of vision-language while alleviating the computational complexities of processing videos. To evaluate this novel application, we introduce VIP1, an inference-time challenge dataset designed to explore models\u2019 reasoning capabilities through video chain-of-thought. Inspired by visually descriptive scene plays, we propose two formats for keyframe description: unstructured dense captions and structured scene descriptions that identify the focus, action, mood, objects, and setting (FAMOuS) of the keyframe. To evaluate video reasoning, we propose two tasks: Video Infilling and Video Prediction, which test abilities to generate multiple intermediate keyframes and predict future keyframes, respectively. We benchmark GPT-4, GPT-3, and VICUNA on VIP, demonstrate the performance gap in these complex video reasoning tasks, and encourage future work to prioritize language models for efficient and generalized video reasoning.",
    "authors": [
        {
            "affiliations": [],
            "name": "Vaishnavi Himakunthala"
        },
        {
            "affiliations": [],
            "name": "Andy Ouyang"
        },
        {
            "affiliations": [],
            "name": "Daniel Rose"
        },
        {
            "affiliations": [],
            "name": "Ryan He"
        },
        {
            "affiliations": [],
            "name": "Alex Mei"
        },
        {
            "affiliations": [],
            "name": "Yujie Lu"
        },
        {
            "affiliations": [],
            "name": "Chinmay Sonar"
        },
        {
            "affiliations": [],
            "name": "Michael Saxon"
        },
        {
            "affiliations": [],
            "name": "William Yang Wang"
        }
    ],
    "id": "SP:18d8278836b9ebef1ccaebc03eef833ef8a9106b",
    "references": [
        {
            "authors": [
                "Sami Abu-El-Haija",
                "Nisarg Kothari",
                "Joonseok Lee",
                "Paul Natsev",
                "George Toderici",
                "Balakrishnan Varadarajan",
                "Sudheendra Vijayanarasimhan"
            ],
            "title": "Youtube8m: A large-scale video classification benchmark",
            "year": 2016
        },
        {
            "authors": [
                "Jean-Baptiste Alayrac",
                "Jeff Donahue",
                "Pauline Luc",
                "Antoine Miech",
                "Iain Barr",
                "Yana Hasson",
                "Karel Lenc",
                "Arthur Mensch",
                "Katherine Millican",
                "Malcolm Reynolds"
            ],
            "title": "Flamingo: a visual language model for few-shot learning",
            "venue": "Advances in Neural",
            "year": 2022
        },
        {
            "authors": [
                "Max Bain",
                "Arsha Nagrani",
                "G\u00fcl Varol",
                "Andrew Zisserman."
            ],
            "title": "Frozen in time: A joint video and image encoder for end-to-end retrieval",
            "venue": "IEEE International Conference on Computer Vision. 5",
            "year": 2021
        },
        {
            "authors": [
                "Aanisha Bhattacharya",
                "Yaman K Singla",
                "Balaji Krishnamurthy",
                "Rajiv Ratn Shah",
                "Changyou Chen."
            ],
            "title": "A video is worth 4096 tokens: Verbalize story videos to understand them in zero shot",
            "venue": "3",
            "year": 2023
        },
        {
            "authors": [
                "Byoung-Tak Zhang"
            ],
            "title": "Multimodal dual attention",
            "year": 2018
        },
        {
            "authors": [
                "Kyung-Min Kim",
                "Min-Oh Heo",
                "Seong-Ho Choi",
                "Byoung-Tak Zhang."
            ],
            "title": "Deepstory: Video story qa by deep embedded memory networks",
            "venue": "Proceedings of the 26th International Joint Conference on Artificial Intelligence, IJCAI\u201917, page 2016\u20132022.",
            "year": 2017
        },
        {
            "authors": [
                "Wonjae Kim",
                "Bokyung Son",
                "Ildoo Kim."
            ],
            "title": "Vilt: Vision-and-language transformer without convolution or region supervision",
            "venue": "2",
            "year": 2021
        },
        {
            "authors": [
                "Ranjay Krishna",
                "Kenji Hata",
                "Frederic Ren",
                "Li Fei-Fei",
                "Juan Carlos Niebles."
            ],
            "title": "Dense-captioning events in videos",
            "venue": "International Conference on Computer Vision (ICCV). 5",
            "year": 2017
        },
        {
            "authors": [
                "Jie Lei",
                "Licheng Yu",
                "Mohit Bansal",
                "Tamara Berg."
            ],
            "title": "TVQA: Localized, compositional video question answering",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1369\u20131379, Brussels, Belgium.",
            "year": 2018
        },
        {
            "authors": [
                "Jie Lei",
                "Licheng Yu",
                "Tamara Berg",
                "Mohit Bansal."
            ],
            "title": "TVQA+: Spatio-temporal grounding for video question answering",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8211\u20138225, Online. Association",
            "year": 2020
        },
        {
            "authors": [
                "Jie Lei",
                "Licheng Yu",
                "Tamara L Berg",
                "Mohit Bansal."
            ],
            "title": "What is more likely to happen next? videoand-language future event prediction",
            "venue": "arXiv preprint arXiv:2010.07999. 3",
            "year": 2020
        },
        {
            "authors": [
                "Bo Li",
                "Yuanhan Zhang",
                "Liangyu Chen",
                "Jinghao Wang",
                "Jingkang Yang",
                "Ziwei Liu."
            ],
            "title": "Otter: A multi-modal model with in-context instruction tuning",
            "venue": "arXiv preprint arXiv:2305.03726. 6",
            "year": 2023
        },
        {
            "authors": [
                "Junnan Li",
                "Dongxu Li",
                "Caiming Xiong",
                "Steven Hoi."
            ],
            "title": "Blip: Bootstrapping language-image pretraining for unified vision-language understanding and generation",
            "venue": "ICML. 2",
            "year": 2022
        },
        {
            "authors": [
                "Junnan Li",
                "Yongkang Wong",
                "Qi Zhao",
                "Mohan S. Kankanhalli."
            ],
            "title": "Video storytelling: Textual summaries for events",
            "venue": "IEEE Transactions on Multimedia, 22(2):554\u2013565. 5",
            "year": 2020
        },
        {
            "authors": [
                "Haotian Liu",
                "Chunyuan Li",
                "Qingyang Wu",
                "Yong Jae Lee."
            ],
            "title": "Visual instruction tuning",
            "venue": "2, 5",
            "year": 2023
        },
        {
            "authors": [
                "T. Maharaj",
                "N. Ballas",
                "A. Rohrbach",
                "A. Courville",
                "C. Pal."
            ],
            "title": "A dataset and exploration of models for understanding video data through fill-in-the-blank question-answering",
            "venue": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2017
        },
        {
            "authors": [
                "Antoine Miech",
                "Dimitri Zhukov",
                "Jean-Baptiste Alayrac",
                "Makarand Tapaswi",
                "Ivan Laptev",
                "Josef Sivic."
            ],
            "title": "HowTo100M: Learning a Text-Video Embedding by Watching Hundred Million Narrated Video Clips",
            "venue": "ICCV. 2, 3, 5",
            "year": 2019
        },
        {
            "authors": [
                "Jonghwan Mun",
                "Paul Hongsuck Seo",
                "Ilchae Jung",
                "Bohyung Han."
            ],
            "title": "Marioqa: Answering questions by watching gameplay videos",
            "venue": "Proceedings of the IEEE International Conference on Computer Vision, pages 2867\u20132875. 2, 3",
            "year": 2017
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark",
                "Gretchen Krueger",
                "Ilya Sutskever"
            ],
            "title": "Learning transferable visual models from natural language",
            "year": 2021
        },
        {
            "authors": [
                "Nils Reimers",
                "Iryna Gurevych."
            ],
            "title": "SentenceBERT: Sentence embeddings using Siamese BERTnetworks",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natu-",
            "year": 2019
        },
        {
            "authors": [
                "Daniel Rose",
                "Vaishnavi Himakunthala",
                "Andy Ouyang",
                "Ryan He",
                "Alex Mei",
                "Yujie Lu",
                "Michael Saxon",
                "Chinmay Sonar",
                "Diba Mirza",
                "William Yang Wang."
            ],
            "title": "Visual chain of thought: Bridging logical gaps with multimodal infillings",
            "venue": "2",
            "year": 2023
        },
        {
            "authors": [
                "Michael Saxon",
                "William Yang Wang."
            ],
            "title": "Multilingual conceptual coverage in text-to-image models",
            "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 4831\u20134848, Toronto, Canada.",
            "year": 2023
        },
        {
            "authors": [
                "Sainbayar Sukhbaatar",
                "Arthur Szlam",
                "Jason Weston",
                "Rob Fergus."
            ],
            "title": "Weakly supervised memory networks",
            "venue": "ArXiv, abs/1503.08895. 3",
            "year": 2015
        },
        {
            "authors": [
                "Makarand Tapaswi",
                "Yukun Zhu",
                "Rainer Stiefelhagen",
                "Antonio Torralba",
                "Raquel Urtasun",
                "Sanja Fidler."
            ],
            "title": "Movieqa: Understanding stories in movies through question-answering",
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern",
            "year": 2016
        },
        {
            "authors": [
                "Peng Wang",
                "An Yang",
                "Rui Men",
                "Junyang Lin",
                "Shuai Bai",
                "Zhikang Li",
                "Jianxin Ma",
                "Chang Zhou",
                "Jingren Zhou",
                "Hongxia Yang."
            ],
            "title": "Ofa: Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework",
            "venue": "CoRR,",
            "year": 2022
        },
        {
            "authors": [
                "Xin Wang",
                "Jiawei Wu",
                "Junkun Chen",
                "Lei Li",
                "YuanFang Wang",
                "William Yang Wang."
            ],
            "title": "Vatex: A large-scale, high-quality multilingual dataset for video-and-language research",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer",
            "year": 2019
        },
        {
            "authors": [
                "Zhenhailong Wang",
                "Manling Li",
                "Ruochen Xu",
                "Luowei Zhou",
                "Jie Lei",
                "Xudong Lin",
                "Shuohang Wang",
                "Ziyi Yang",
                "Chenguang Zhu",
                "Derek Hoiem",
                "Shih-Fu Chang",
                "Mohit Bansal",
                "Heng Ji"
            ],
            "title": "Language models with image descriptors are strong few-shot",
            "year": 2022
        },
        {
            "authors": [
                "Jason Wei",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Maarten Bosma",
                "brian ichter",
                "Fei Xia",
                "Ed H. Chi",
                "Quoc V Le",
                "Denny Zhou"
            ],
            "title": "2022. Chain of thought prompting elicits reasoning in large language models",
            "venue": "In Advances in Neural Information Processing Systems",
            "year": 2022
        },
        {
            "authors": [
                "Jialian Wu",
                "Jianfeng Wang",
                "Zhengyuan Yang",
                "Zhe Gan",
                "Zicheng Liu",
                "Junsong Yuan",
                "Lijuan Wang"
            ],
            "title": "Grit: A generative region-to-text transformer for object understanding",
            "venue": "arXiv preprint arXiv:2212.00280",
            "year": 2022
        },
        {
            "authors": [
                "Jun Xu",
                "Tao Mei",
                "Ting Yao",
                "Yong Rui."
            ],
            "title": "Msrvtt: A large video description dataset for bridging video and language",
            "venue": "IEEE International Conference on Computer Vision and Pattern Recognition (CVPR). IEEE International Conference on Computer",
            "year": 2016
        },
        {
            "authors": [
                "Kexin Yi",
                "Chuang Gan",
                "Yunzhu Li",
                "Pushmeet Kohli",
                "Jiajun Wu",
                "Antonio Torralba",
                "Joshua B. Tenenbaum."
            ],
            "title": "CLEVRER: collision events for video representation and reasoning",
            "venue": "ICLR. 3",
            "year": 2020
        },
        {
            "authors": [
                "Zhou Yu",
                "Dejing Xu",
                "Jun Yu",
                "Ting Yu",
                "Zhou Zhao",
                "Yueting Zhuang",
                "Dacheng Tao."
            ],
            "title": "Activitynet-qa: A dataset for understanding complex web videos via question answering",
            "venue": "AAAI, pages 9127\u20139134. 2",
            "year": 2019
        },
        {
            "authors": [
                "Rowan Zellers",
                "Ximing Lu",
                "Jack Hessel",
                "Youngjae Yu",
                "Jae Sung Park",
                "Jize Cao",
                "Ali Farhadi",
                "Yejin Choi."
            ],
            "title": "Merlot: Multimodal neural script knowledge models",
            "venue": "Advances in Neural Information Processing Systems, volume 34, pages 23634\u201323651. Curran",
            "year": 2021
        },
        {
            "authors": [
                "Kuo-Hao Zeng",
                "Tseng-Hung Chen",
                "Ching-Yao Chuang",
                "Yuan-Hong Liao",
                "Juan Carlos Niebles",
                "Min Sun."
            ],
            "title": "Leveraging video descriptions to learn video question answering",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, 31(1). 2",
            "year": 2017
        },
        {
            "authors": [
                "Tianyi Zhang",
                "Varsha Kishore",
                "Felix Wu",
                "Kilian Q. Weinberger",
                "Yoav Artzi"
            ],
            "title": "Bertscore: Evaluating text generation with bert",
            "venue": "In International Conference on Learning Representations",
            "year": 2020
        },
        {
            "authors": [
                "Luowei Zhou",
                "Chenliang Xu",
                "Jason J. Corso."
            ],
            "title": "Towards automatic learning of procedures from web instructional videos",
            "venue": "Proceedings of the ThirtySecond AAAI Conference on Artificial Intelligence and Thirtieth Innovative Applications of Artificial",
            "year": 2018
        },
        {
            "authors": [
                "Xingyi Zhou",
                "Rohit Girdhar",
                "Armand Joulin",
                "Philipp Kr\u00e4henb\u00fchl",
                "Ishan Misra."
            ],
            "title": "Detecting twenty-thousand classes using image-level supervision",
            "venue": "ECCV. 4, 5",
            "year": 2022
        },
        {
            "authors": [
                "Wanrong Zhu",
                "An Yan",
                "Yujie Lu",
                "Wenda Xu",
                "Xin Wang",
                "Miguel Eckstein",
                "William Yang Wang."
            ],
            "title": "Visualize before you write: Imagination-guided openended text generation",
            "venue": "Findings of the Association for Computational Linguistics: EACL 2023, pages",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Constituting 65% of all internet traffic in 2023, videos are an area of huge potential for the next chapter of leveraging artificial intelligence (Fu et al., 2021; Zellers et al., 2021; Fu et al., 2023a). For example, Video Question Answering (Lei et al., 2018) and Video Summarization (Xu et al., 2016) are two existing datasets that empirically evaluate video models. Yet, they do not assess more challenging tasks, such as reasoning through specific relationships between multiple frames. Just\n*Denotes equal contribution. 1https://github.com/vaishnaviHimakunthala/VIP\nlike how humans understand videos by processing frames across time steps, AI\u2019s ability to accomplish multi-frame reasoning is a core task for video understanding.\nMulti-frame video reasoning is bottlenecked by the sheer computing resources needed to process videos, which typically contain 24 frames per second and can vary widely. However, intelligible videos tend to have little variation from frame to frame. Akin to selecting principle components on the axes containing the highest orthogonal variance, picking a small sample of keyframes can capture much of the video\u2019s meaning. Understanding a video via extracted keyframes poses the challenge of multi-hop reasoning, which requires stronger language model capabilities.\nTo evaluate this challenging multi-hop multiframe video reasoning, we elicit video chain-ofthought (VIDEOCOT). We propose two tasks to test such capabilities in existing models \u2013 Video Infilling and Video Prediction. In the Video Infilling setting, the goal is to predict the masked keyframes\u2019 descriptions when given the previous and next keyframes\u2019 descriptions in sequence as context, following the spirit of masked language modeling. In the Video Prediction, the goal is to predict the descriptions of the next keyframes in the sequence when given a set of previous keyframes\u2019 descriptions, similar to the next token prediction task. These tasks can help analyze video generation and whether video models truly understand the dynamic relations between subsequent video frames, given the variable context gap between keyframes.\nTo benchmark our proposed Video Infilling and Prediction tasks, we construct a dataset by proposing an automated method to extract keyframes. In addition to the frame itself, we propose two textual representations \u2013 unstructured dense captions and FAMOuS structured scene descriptions (Figure 1). The unstructured captions are intended to extract more significant and visually descriptive informa-\ntion compared to existing captioning systems, a necessary component to provide enough context for more challenging tasks. In addition, we systematically create structured scene descriptions from these unstructured dense captions by extracting the frame\u2019s focus, action, mood, objects, and setting using weak human supervision for quality assurance. The FAMOuS categories are inspired by play scripts, which maintain much of the visual detail of the unstructured captions while providing a clear, structured way to reason through visual tasks with high degrees of freedom.\nWe propose the following contributions:\n\u2022 We systematically collect an inference-time challenge dataset of keyframes for video reasoning augmented with two textual representations: unstructured dense captions for visually-descriptive information and FAMOuS scene descriptions for structured reasoning.\n\u2022 We propose the Video Infilling and Video Prediction tasks to benchmark the video chain-ofthought capabilities in existing models.\n\u2022 We empirically demonstrate that existing models have the potential for multi-hop multi-frame video reasoning tasks but have a significant area for improvement as future work."
        },
        {
            "heading": "2 Related Work",
            "text": "AI Reasoning. Large language models (LLMs) demonstrate considerable gains on existing reasoning benchmarks with strategies such as chainof-thought (COT) (Wei et al., 2022) and fewshot demonstrations (Brown et al., 2020). Visionlanguage models (VLM) (Alayrac et al., 2022; Chowdhery et al., 2022; Driess et al., 2023) have furthered LLMs\u2019 capabilities by adding the visual modality to perform tasks such as visually-guided text generation (Rose et al., 2023; Zhu et al., 2023), vision question-answering (Wang et al., 2022a; Kim et al., 2021), and image captioning (Li et al., 2022; Liu et al., 2023). We believe the logical next step is to extend these existing models to the video domain. VIP\u2019S annotated collection of extracted keyframes from real-world videos offers a resource to evaluate reasoning abilities within the video domain.\nDatasets for Video Understanding. Existing video datasets are often limited by domain specificity or require a supplementary representation (e.g., audio, text) (Lei et al., 2018, 2020a; Tapaswi et al., 2016; Miech et al., 2019; Kim et al., 2017; Mun et al., 2017). These datasets also provide simplifications as textual summaries (Xu et al., 2016; Guadarrama et al., 2013) or a single video frame (Yu et al., 2019; Zeng et al., 2017; Maharaj et al.,\n2017), which by themselves can be sufficient to complete the task. While some datasets consider the multi-frame component (Jang et al., 2017; Yi et al., 2020; Mun et al., 2017; Fu et al., 2023b) for higher order complexity, VIP differs in trying to reduce the computational intensity of video reasoning without reducing the task difficulty and generality. VIP is a dataset of real-life videos that spans a breadth of domains and assesses multi-hop, multiframe video reasoning without requiring significant computation to train on videos.\nTextual Representations of Videos. Early video models are trained with visual keyframes and textual questions as input and return textual answers as output (Sukhbaatar et al., 2015; Kim et al., 2017; Jang et al., 2017). Then, researchers started to unify the video, keyframe, and text embedding spaces (Miech et al., 2019; Kim et al., 2018; Zellers et al., 2021; Kim et al., 2017; Guadarrama et al., 2013; Bhattacharya et al., 2023). VidIL leverages the contemporary in-context inference paradigm with few-shot demonstrations, including frames, captions, and visual tokens to prompt language models to solve VidL tasks (Lei et al., 2020b; Wang et al., 2022b). In contrast to these existing works, VIP introduces textual representations at the keyframe level and then leverages them to reason about specific video segments using VideoCOT."
        },
        {
            "heading": "3 VIP Dataset Construction",
            "text": "To construct the dataset, we first outsource the video corpus to stem from the YouTube-8m dataset (Abu-El-Haija et al., 2016), whose diverse, realistic videos with human-labeled topics align well with our desiderata. To effectively enable multi-frame video reasoning, we downsample visually static categories such as weather, which may not contain much change throughout the video. The weights of each category2 is described in Figure 2.\nThen, to reduce the computational complexity of video processing, we reduce a video into a set of keyframes that seek to capture the video\u2019s meaning (\u00a73.1). To accommodate the limitations of existing models, we also generate two forms of visually-descriptive, textual representations\u2013 unstructured dense captions and FAMOuS scene descriptions (\u00a73.2). Figure 3 summarizes this automated pipeline, which reduces the cost ordinarily spent to manually construct such a dataset.\n2Videos may be in multiple categories."
        },
        {
            "heading": "3.1 Representative Keyframe Selection",
            "text": "Selecting Video Frames. The bottleneck to models in the video modality is the computational intensity. We select video frames that best capture the overall video content to mitigate this issue. Instead of training a model to choose a dynamic number of keyframes \u2013 which would be computationally expensive \u2013 we design an algorithm to prune semantically similar keyframes (Algorithm 1). However, selecting keyframes in this manner comes with the tradeoff that too many frames can introduce redundancy while too few can remove critical context. We choose a large set of candidate keyframes to balance these considerations, which we then dynamically prune. We employ an off-the-shelf keyframe extractor instead of learning a model ourselves. We choose to use KATNA3 as the baseline keyframe extraction tool as it is open-sourced and easy to onboard. KATNA selects keyframes by leveraging the differences in LUV colorspace, brightness, contrast, blur, and k-means clustering of images.\n3https://github.com/keplerlab/katna\nAlgorithm 1: frame_extract(v, c, f ) Data: video v, ints c, f of the candidate and finalized\nkeyframe counts, respectively. Result: List of f finalized keyframes from v. Extract initial frames and embeddings:\n1 k1, . . . , kn \u2190 Katna(v, c) 2 t1, . . . , tn \u2190 CLIP (Detic(k1, . . . , kn)) 3 i1, . . . , in \u2190 CLIP (k1, . . . , kn) 4 while len(k) > f do Remove frame with highest adjacent similarity: 5 for j in range(len(k)) do 6 costext \u2190 cos(tj , tj\u22121, tj+1) 7 cosimage \u2190 cos(ij , ij\u22121, ij+1) 8 scores[j]\u2190 mean(costext, cosimage) 9 remove k[s] where s = argmaxs(scores[s])\n10 return k1, . . . , kf\nFigure 4: frame_extract returns a list of f selected keyframes from a video v. First, we extract c candidates using Katna. These keyframes are embedded using CLIP in the image space; Detic extracts objects from the keyframes into a textual representation, which are also embedded with CLIP. Then, we iteratively prune the keyframe with the highest cosine similarity with adjacent frames until f keyframes remain.\nPruning Redundant Frames. Once baseline candidate keyframes are selected, we prune them by removing low-quality, semantically similar frames. First, we remove blurry keyframes with low Laplacian scores, which indicate the absence of intensity changes. Then, we use object detection models DETIC (Zhou et al., 2022) and GRIT (Wu et al., 2022) to filter keyframes that contain minimal objects, which indicates blurriness as these models are quite sensitive to all background objects.\nAfter removing low-quality frames, we use CLIP\n(Radford et al., 2021) to create embeddings for the keyframe image and its list of detected objects and positions from the previous step in the pipeline. This combination helps us compare frames using pixel similarity and object invariance. We take the average cosine similarity score for the keyframe\u2019s image and object embeddings compared to the surrounding keyframes and prune the frames with the highest similarity. As people tend to be the primary subject of these videos, we add an additional check only to prune keyframes containing people if either of the surrounding frames also includes people."
        },
        {
            "heading": "3.2 Textual Representations of Keyframes",
            "text": "Next, to complement the keyframe images, we construct two textual representations of scenes: an unstructured, dense caption that provides visually descriptive insight into the scene; and second, a FAMOuS scene description that offers a structured approach to the reasoning process. We first generate the dense caption, which we then use to extract specific information for the structured scene description. These frame descriptions allow for leveraging existing LLM/VLM capabilities for video reasoning and generation.\nUnstructured, Dense Captions. To create visually descriptive frame descriptions, we first extract three things from each keyframe: a caption, an object list, and a dense caption list. Together, these outputs paint a visual description of the keyframe \u2013 the object list and the dense captions describe the focus, objects, and setting, while the caption details the focus, action, and mood. We specifically use\nDETIC (Zhou et al., 2022), a tool that accurately detects objects without much detail, to simply list the frame\u2019s objects. To extract more descriptive, objectlevel detail for high-quality scene descriptions, we use GRIT (Wu et al., 2022), which returns dense captions describing each object. Finally, we obtain the keyframe\u2019s overall caption using LLAVA (Liu et al., 2023).\nBecause these individual models are prone to hallucination (Dai et al., 2023), we ensure the accuracy of our unstructured descriptions by engineering DETIC and GRIT to return confidence scores for each of their outputs. Additionally, we utilize the Wiki descriptions of each video topic in the YouTube-8M dataset to extract a grounding list of baseline objects using GPT-4. Finally, we feed in all of the extracted outputs into GPT-4 to generate the final dense caption.\nFAMOUS Structured Scene Descriptions. Structure can improve the reasoning ability of a model by providing concrete targets. To provide structure, we take inspiration from scene plays which clearly label and describe the scene. Specifically, we identify and extract the focus, action, mood, objects, and setting from the dense caption using GPT-4, categories which should capture the most important visual information in a concise, structured manner."
        },
        {
            "heading": "3.3 Dataset Contributions",
            "text": "The VIP dataset is the first to evaluate multi-hop video reasoning via a video-chain of thought. This novel paradigm promotes efficient and robust video reasoning through automated keyframe extraction (Algorithm 1) over a breadth of domains (Figure 2). Our two textual representations of keyframes (Figure 1) add significantly granularity to videos (with\nan average caption length of 114 tokens) compared to traditional video caption datasets (Table 1). This enables reasoning on more specific visual and semantic changes which occur between frames, more closely mimicking how humans process videos by thinking frame by frame.\nTo ensure the quality of our collected dataset, we verify correctness via crowdsourcing on Amazon Mechanical Turk (Appendix A). Workers are paid to evaluate the quality of structured scene descriptions and edit those of low quality. Unstructured dense captions are corrected using the validated structured scene descriptions with GPT-4 and verified with another round of human evaluation."
        },
        {
            "heading": "4 Video Reasoning Tasks",
            "text": "Taking inspiration from existing natural language tasks, we propose two tasks for videos that explore a model\u2019s multi-frame reasoning capabilities. The Video Infilling task requires models to predict a set of keyframes given the preceding and following frames, akin to masked language modeling for keyframes. Video Prediction tasks models to predict the most likely sequence of frames to follow a given series of frames - parallel to the text completion task. Video infilling and prediction of keyframes are two general tasks with several downstream contexts that can benefit from video understanding and completion.\nTo concretely define the tasks below, we represent the sequence of chronological keyframes as k1, . . . , kn, their respective unstructured dense captions as u1, . . . , un, and FAMOuS structured scene descriptions as s1, . . . , sn."
        },
        {
            "heading": "4.1 Video Infilling Task",
            "text": "Suppose a subsequence of frames ki, . . . , kj is masked. In the video infilling task, the target is for\na model to learn to reconstruct these masked frames using preceding context frames ki\u2212n, . . . , ki\u22121 and following context frames kj+1, . . . , kj+n where n is the number of frames provided as context. Without loss of generality, this task follows for both textual representations using u and s as inputs and outputs instead of k. In the multimodal setting, we can use pairs (k, u) or (k, s) as inputs and outputs.\nThis task requires models to capture a scene\u2019s temporal variations and transitions, including changes in visual elements, object positions, and contextual factors. Furthermore, the task\u2019s difficulty scales two-fold. First, decreasing the context window n will reduce the ability to leverage hints from surrounding keyframes to infill informatively; combined with the necessity to perform multi-hop reasoning between each pair of frames in sequence, insufficient context could result in training divergence. Second, increasing the number of frames to predict in sequence between i, j also raises similar challenges as too large a gap could add several degrees of freedom, resulting in significant infilling variability. Successfully predicting intermediate keyframes may illuminate models\u2019 abilities to reason through the dynamic evolution of scenes and identify critical deltas in videos."
        },
        {
            "heading": "4.2 Video Prediction Task",
            "text": "Suppose we are given a sequence of context frames ki\u2212n, . . . , ki. In the video prediction task, we aim to predict the f following frames ki+1, . . . , kf . Without loss of generality, this task follows for the unimodal text and multimodal representations. Much like the infilling task, the difficulty increases by decreasing the context window or increasing the prediction span. Since the prediction task only provides past context, predicting a longer sequence following may be harder as the possibilities increase exponentially."
        },
        {
            "heading": "5 Experiments",
            "text": "Setup. Although it would be ideal to benchmark multi-modal language models on our proposed tasks, the current pre-trained models (e.g., Open Flamingo and Otter (Awadalla et al., 2023; Li et al.,\n2023)) are not designed to accommodate multiple image inputs off-the-shelf. As a result, we chose to benchmark the video infilling and prediction tasks as a language task, generating keyframes as represented by dense captions or FAMOUS descriptions. We use GPT-3, GPT-4, and VICUNA4 as leading models, with in-context inference using one demonstration in both our infilling and prediction tasks. To mitigate hallucination, we leverage greedy decoding. In each task, the goal is to infill or predict three intermediate or subsequent keyframes, respectively. Evaluation metrics are computed as the mean of these three generated keyframes compared to the ground truth. Results are reported using one prompt, but a follow-up analysis shows prompt stability through low-variation among other prompts (Table 2).\nMetrics. We use three standard text comparison metrics: ROUGEL, BERTSCORE (Zhang* et al., 2020), and SENTENCEBERT (Reimers and Gurevych, 2019). ROUGEL is best suited for tasks aimed to generate text that exactly matches the ground truth. BERTSCORE leverages BERT embeddings, which utilize the surrounding context. SENTENCEBERT is similar to BERTSCORE but computes the similarity of texts using sentencelevel embeddings instead of word embeddings. These metrics combined provide initial scope into keyframe generation performance from both the semantic and contextual perspective.\n4We use the pre-trained VICUNA-13B checkpoint."
        },
        {
            "heading": "5.1 Primary Results",
            "text": "We break down the primary results (Table 3) into four key points.\nNumber of Context Frames. Although the output size is fixed, we investigate how varying the input size affects the complexity of the VIP tasks. Consistent with intuition, we observe higher performance given additional context. However, the performance boost with each additional keyframe is marginal. With scores for all three metrics significantly lower than other tasks of similar spirit, it appears that our multi-hop, multi-frame prediction task is quite challenging using only textual representations for existing state-of-the art language models. This low baseline performance may overshadow the change in difficulty as a result of varying context frames.\nDense Captions vs FAMOUS Descriptions. Dense captions consistently show stronger performance using our selected metrics than FAMOUS descriptions. As our evaluation metrics emphasize word similarity, they may favor dense captions which contain filler words used to form complete sentences. In the FAMOUS structure, descriptions are broken down by category, which reduces the verbosity, thereby increasing the difficulty for word comparison metrics.\nInfilling vs Prediction Tasks. We consistently observe that models have stronger performance on the infilling task compared to the prediction task. To most fairly compare the two tasks, compare the Infilling-1 task, which aims to predict three intermediate frames given one predecessor and one\nsuccessor keyframe, with the Prediction-2 task, which aims to predict the three keyframes following the two context frames. Aside from a few nonsignificant outliers, these models perform better across all metrics and both textual representations. This is inline with intuition as bidirectional context reduces the complexity of the problem.\nIndividual Model Performance. Across models, the performance does not follow any obvious trends, which is surprising considering the size difference in the open-source VICUNA compared to the human-reinforced GPT-3 and GPT-4 models. By metrics, we observe GPT-3 and VICUNA performs slightly better performances on ROGUEL compared to GPT-4, suggesting exact word consistency may be better in these earlier models. GPT-4\u2019s and GPT-3\u2019s edge in SENTENCEBERT suggest their generations may align more semantically than an off-the-shelf VICUNA."
        },
        {
            "heading": "5.2 FAMOUS Component Analysis",
            "text": "We decompose model performance on FAMOUS structured scene descriptions on a component level (Table 4) for the Prediction-3 task, which aims to predict the three keyframes following the three input keyframes. Through ROUGEL, we observe models perform significantly better identifying objects, compared to the other four components. Comparing BERTSCORE, models appear to semantically align with the ground truth on the focus component better and more poorly in understanding of the keyframe\u2019s action. Finally, the SENTENCEBERT results suggest that models better maintain overall sentence similarity when considering components of the image\u2019s environment, such as mood, objects, and setting. These trends\nhighlight that reasoning through textual representations for basic components such as keyframe focus and objects is a strength of language models, while reasoning about more dynamic components such as the action necessitates a more intricate understanding of the keyframes and could benefit from a video representation."
        },
        {
            "heading": "5.3 Causal Aspect Analysis",
            "text": "We examine the difference in performance between physical and social causal reasoning (Table 5). A task necessitates physical causal reasoning when the video changes stem from external, tangible forces, like a wave crashing on a beach. Conversely, a task involves social causal reasoning when video changes result from social cues, such as a character becoming joyful during a surprise party. Observation of the results show that social causal reasoning tasks scored higher on BERTSCORE while physical causal reasoning tasks scored higher on SENTENCEBERT. These results may be an outgrowth of the FAMOuS Component Analysis \u00a75.2, where a consistent character focus and objects present in many social scenarios yield higher token-level similarity with BERTSCORE. By contrast, the consistent environmental qualities like action or mood\u2013 present in many physical scenarios\u2013 result in a greater SENTENCEBERT score."
        },
        {
            "heading": "5.4 Domain Analysis",
            "text": "We also outline the overall results from all experiments corresponding to the different visual domains of our videos in Table 6. Although we found several categories to be noisy due to low sample sizes, certain categories like Games perform well, while others like Jobs & Education fall behind. We hypothesize that the availability of domain-\nspecific training data as well as intrinsic dimensionality needed to model interactions within these topics jointly contribute to such observations."
        },
        {
            "heading": "5.5 Qualitative Observations",
            "text": "Figure 5 provides a visual depiction of the outputs generated by GPT-4 and VICUNA for the prediction task. Inspecting the depicted outputs from both models, it\u2019s evident that they lack some semantic congruence with the ground truth, underscoring the limitations that language model-based approaches face in in video reasoning. Figure 7 and Figure 5 further demonstrate impressive early performance using video-chain of thought, though the examples\u2019 strikingly similar output suggests some overfitting to training data. Despite the observable limitations, it\u2019s clear that the language models have a clear baseline video understanding. Still, both the quantitative and qualitative axes highlight that only using unimodal language doesn\u2019t generalize their strong language task performance to VIP\u2019s video reasoning tasks, which naturally opens several areas for subsequent research threads."
        },
        {
            "heading": "6 Future Work",
            "text": "In this paper, we aim to lay the groundwork for exploring the challenging topic of multi-frame, multihop reasoning within the existing capabilities of deep learning models. Naturally, this opens several directions for exciting future work.\nIn the language model space, we benchmark the performance of several leading models using textual representations of keyframes for video-related reasoning following a standard in-context inference procedure with few-shot demonstrations. This invites the opportunity to discover more targeted inference-time techniques using language models or vision-language models to improve the performance of video reasoning tasks beyond a general paradigm. Similarly, additional training-time ef-\nfort could be worthwhile through fine-tuning or a more traditional train-validate-test paradigm to learn skills beyond the general pre-trained learnset. In this vein, collecting additional data samples could improve the feasibility of these research threads.\nBeyond the language modality, bridging the video reasoning task end-to-end with video is a longer-term research direction with immediate benefits in animation. Our paper reduces the video reasoning task into a language task with a textual output. Image synthesis would be an immediate step to reconstruct the keyframe image. Then, video synthesis from a set of images would naturally follow. Finally, unifying these disjoint tasks could benefit from error reduction and improved usability.\nAs video reasoning is a new space, developing robust evaluation metrics would be a valuable contribution. Some desirable but difficult properties to consider in this area include the ability to capture both the spatial and temporal invariance that could occur through videos, as multiple interchangeable actions are plausible within different areas of the frame and sequences.\nFinally, our general video reasoning tasks pose the prospect of efficient transfer learning where improving on such a task could benefit several new applications, similar to the contemporary boom of language technologies."
        },
        {
            "heading": "7 Conclusion",
            "text": "We present the inference-time Video Infilling and Prediction dataset to evaluate models\u2019 video reasoning abilities by performing a video chain-ofthought on video keyframes. To collect this dataset, we introduce a novel pipeline to systematically extract keyframes and generate corresponding textual representations \u2013 unstructured dense captions and structured FAMOUS scene descriptions. We benchmark state-of-the-art language models on VIP tasks and evaluate their ability to generate these textual representations of keyframes through a video chain-of-thought. These models display potential to perform video-related reasoning yet have significant potential to improve. By testing multi-hop, multi-frame reasoning abilities on sparse keyframes, we hope to promote research into developing models that support real-world video understanding and video generation while being resource efficient.\nLimitations\nModel Selection for Benchmarking. The primary limitation of our work is that current multimodal models do not support multiple image inputs per input query as a trivial use-case. As a result, despite our paper proposing a novel dataset intended for video-related reasoning, we currently only benchmark large language models. We encourage future research to explore the reasoning capabilities for video scene description generation.\nAutomation Process. While our work aims to systematically generate samples to evaluate the video-related reasoning capabilities of existing AI systems, we acknowledge the potential for error when using other AI systems to generate such examples. As a result, we add a layer of human supervision where crowd workers are used to first to classify whether generated scene descriptions are sufficiently correct. Then, we must make use of expert annotators to manually correct the generated scene descriptions that were flagged as poor quality for quality assurances purposes of this dataset.\nEthics Statement\nPotential for Bias. We acknowledge the potential for bias in the data collection process and inference task design. We have taken a number of steps to mitigate bias, including ensuring diversity in video content selection, and regularly reviewing and refining the annotation process to minimize any unconscious bias. In addition, we are committed to addressing and correcting any biases that may arise during the evaluation and analysis of VIP model performance. Our dataset is restricted to English captions, thereby containing a bias toward culturally Western scenes. In a multilingual setting differential behaviors between language classes would probably be observed (Saxon and Wang, 2023).\nCrowdsourcing. Crowdsourcing via the Amazon Mechanical Turk platform was utilized for conducting human evaluation. To ensure reliable results, we restricted participation to workers based in Australia, Canada, New Zealand, the United Kingdom, or the United States, with a minimum HIT approval rating of 98%. Compensation for scene description checking and correction was set at a rate of $15 per hour. Our data collection is classified as an approved exempt protocol from the Institutional Review Board. Details of the interface can be found in Appendix A."
        },
        {
            "heading": "Acknowledgements",
            "text": "We thank our reviewers for their supportive feedback. This material is based upon work supported in part by the National Science Foundation under Grants #1821415 and #2048122 REU. The authors are solely responsible for the contents of the paper, and the opinions expressed in this publication do not necessarily reflect the official policy or position of associated funding agencies or past or present employers of the authors. The contents of this paper are not intended to provide, and should not be relied upon for, investment advice."
        },
        {
            "heading": "A Appendix",
            "text": "A.1 Human Evaluation Interface We demonstrate the interface of our human evaluation in scene description checking in Figure 9. We employ manual procedures to guarantee the exclusion of personal information and the absence of offensive content during human evaluations."
        }
    ],
    "title": "Let\u2019s Think Frame by Frame with VIP: A Video Infilling and Prediction Dataset for Evaluating Video Chain-of-Thought",
    "year": 2023
}