{
    "abstractText": "Conditional story generation is significant in human-machine interaction, particularly in producing stories with complex plots. While Large language models (LLMs) perform well on multiple NLP tasks, including story generation, it is challenging to generate stories with both complex and creative plots. Existing methods often rely on detailed prompts to guide LLMs to meet target conditions, which inadvertently restrict the creative potential of the generated stories. We argue that leveraging information from exemplary human-written stories facilitates generating more diverse plotlines. Delving deeper into story details helps build complex and credible plots. In this paper, we propose a retrievalauGmented stoRy generation framework with a fOrest of eVidEnce (GROVE) to enhance stories\u2019 complexity. We build a retrieval repository for target conditions to produce few-shot examples to prompt LLMs. Additionally, we design an \u201casking-why\u201d prompting scheme that extracts a forest of evidence, providing compensation for the ambiguities that may occur in the generated story. This iterative process uncovers underlying story backgrounds. Finally, we select the most fitting chains of evidence from the evidence forest and integrate them into the generated story, thereby enhancing the narrative\u2019s complexity and credibility. Experimental results and numerous examples verify the effectiveness of our method.",
    "authors": [
        {
            "affiliations": [],
            "name": "Zhihua Wen"
        },
        {
            "affiliations": [],
            "name": "Zhiliang Tian"
        },
        {
            "affiliations": [],
            "name": "Wei Wu"
        },
        {
            "affiliations": [],
            "name": "Yuxin Yang"
        },
        {
            "affiliations": [],
            "name": "Yanqi Shi"
        },
        {
            "affiliations": [],
            "name": "Zhen Huang"
        },
        {
            "affiliations": [],
            "name": "Dongsheng Li"
        }
    ],
    "id": "SP:0ae615ede1ee29c6aeaecdb9e29d1734c65582a4",
    "references": [
        {
            "authors": [
                "Amal Alabdulkarim",
                "Siyan Li",
                "Xiangyu Peng."
            ],
            "title": "Automatic story generation: Challenges and attempts",
            "venue": "WNU, pages 72\u201383, Virtual. Association for Computational Linguistics.",
            "year": 2021
        },
        {
            "authors": [
                "Arwa I. Alhussain",
                "Aqil M. Azmi."
            ],
            "title": "Automatic story generation: A survey of approaches",
            "venue": "ACM Comput. Surv., 54(5).",
            "year": 2021
        },
        {
            "authors": [
                "Prithviraj Ammanabrolu",
                "Wesley Cheung",
                "William Broniec",
                "Mark O. Riedl"
            ],
            "title": "Automated storytelling via causal, commonsense plot ordering",
            "year": 2020
        },
        {
            "authors": [
                "Yiming Cui",
                "Ziqing Yang",
                "Xin Yao."
            ],
            "title": "Efficient and effective text encoding for chinese llama and alpaca",
            "venue": "arXiv preprint arXiv:2304.08177.",
            "year": 2023
        },
        {
            "authors": [
                "Angela Fan",
                "Mike Lewis",
                "Yann Dauphin."
            ],
            "title": "Hierarchical neural story generation",
            "venue": "ACL, pages 889\u2013898, Melbourne, Australia. Association for Computational Linguistics.",
            "year": 2018
        },
        {
            "authors": [
                "Steven Y. Feng",
                "Varun Gangal",
                "Jason Wei",
                "Sarath Chandar",
                "Soroush Vosoughi",
                "Teruko Mitamura",
                "Eduard Hovy."
            ],
            "title": "A survey of data augmentation approaches for NLP",
            "venue": "ACL-IJCNLP, pages 968\u2013988, Online. Association for Computational Linguistics.",
            "year": 2021
        },
        {
            "authors": [
                "Tianyu Gao",
                "Adam Fisch",
                "Danqi Chen."
            ],
            "title": "Making pre-trained language models better few-shot learners",
            "venue": "ACL-IJCNLP, pages 3816\u20133830, Online. Association for Computational Linguistics.",
            "year": 2021
        },
        {
            "authors": [
                "Jian Guan",
                "Fei Huang",
                "Zhihao Zhao",
                "Xiaoyan Zhu",
                "Minlie Huang."
            ],
            "title": "A knowledge-enhanced pretraining model for commonsense story generation",
            "venue": "Transactions of the Association for Computational Linguistics, 8:93\u2013108.",
            "year": 2020
        },
        {
            "authors": [
                "Jian Guan",
                "Yansen Wang",
                "Minlie Huang."
            ],
            "title": "Story ending generation with incremental encoding and commonsense knowledge",
            "venue": "AAAI, 33:6473\u20136480.",
            "year": 2019
        },
        {
            "authors": [
                "Jian Guan",
                "Yansen Wang",
                "Minlie Huang."
            ],
            "title": "Story ending generation with incremental encoding and commonsense knowledge",
            "venue": "AAAI, AAAI\u201919/IAAI\u201919/EAAI\u201919. AAAI Press.",
            "year": 2019
        },
        {
            "authors": [
                "Han Guo",
                "Bowen Tan",
                "Zhengzhong Liu",
                "Eric Xing",
                "Zhiting Hu."
            ],
            "title": "Efficient (soft) Q-learning for text generation with limited good data",
            "venue": "Findings of EMNLP, pages 6969\u20136991, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.",
            "year": 2022
        },
        {
            "authors": [
                "Thomas Hartvigsen",
                "Saadia Gabriel",
                "Hamid Palangi",
                "Maarten Sap",
                "Dipankar Ray",
                "Ece Kamar."
            ],
            "title": "ToxiGen: A large-scale machine-generated dataset for adversarial and implicit hate speech detection",
            "venue": "ACL, pages 3309\u20133326, Dublin, Ireland. Association",
            "year": 2022
        },
        {
            "authors": [
                "Amr Hendy",
                "Mohamed Abdelrehim",
                "Amr Sharaf",
                "Vikas Raunak",
                "Mohamed Gabr",
                "Hitokazu Matsushita",
                "Young Jin Kim",
                "Mohamed Afify",
                "Hany Hassan Awadalla"
            ],
            "title": "How good are gpt models at machine translation? a comprehensive evaluation",
            "year": 2023
        },
        {
            "authors": [
                "Marzena Karpinska",
                "Nader Akoury",
                "Mohit Iyyer."
            ],
            "title": "The perils of using Mechanical Turk to evaluate open-ended text generation",
            "venue": "EMNLP, pages 1265\u20131285, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.",
            "year": 2021
        },
        {
            "authors": [
                "Takeshi Kojima",
                "Shixiang (Shane) Gu",
                "Machel Reid",
                "Yutaka Matsuo",
                "Yusuke Iwasawa"
            ],
            "title": "Large language models are zero-shot reasoners",
            "venue": "In NeurIPS,",
            "year": 2022
        },
        {
            "authors": [
                "Pratyush Kumar"
            ],
            "title": "Large language models humanize technology",
            "year": 2023
        },
        {
            "authors": [
                "Mike Lewis",
                "Yinhan Liu",
                "Naman Goyal",
                "Marjan Ghazvininejad",
                "Abdelrahman Mohamed",
                "Omer Levy",
                "Veselin Stoyanov",
                "Luke Zettlemoyer"
            ],
            "title": "BART: Denoising sequence-to-sequence pre-training for natural language",
            "year": 2020
        },
        {
            "authors": [
                "Belinda Z. Li",
                "Maxwell Nye",
                "Jacob Andreas."
            ],
            "title": "Language modeling with latent situations",
            "venue": "Findings of ACL 2023, pages 12556\u201312571, Toronto, Canada. Association for Computational Linguistics.",
            "year": 2023
        },
        {
            "authors": [
                "Pengfei Liu",
                "Weizhe Yuan",
                "Jinlan Fu",
                "Zhengbao Jiang",
                "Hiroaki Hayashi",
                "Graham Neubig."
            ],
            "title": "Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing",
            "venue": "ACM Comput. Surv., 55(9).",
            "year": 2023
        },
        {
            "authors": [
                "Albert Lu",
                "Hongxin Zhang",
                "Yanzhe Zhang",
                "Xuezhi Wang",
                "Diyi Yang."
            ],
            "title": "Bounding the capabilities of large language models in open text generation with prompt constraints",
            "venue": "Findings of EACL, pages 1982\u20132008, Dubrovnik, Croatia. Association",
            "year": 2023
        },
        {
            "authors": [
                "Huanru Henry Mao",
                "Bodhisattwa Prasad Majumder",
                "Julian McAuley",
                "Garrison Cottrell."
            ],
            "title": "Improving neural story generation by targeted common sense grounding",
            "venue": "EMNLP-IJCNLP, pages 5988\u20135993, Hong Kong, China. Association for Computational",
            "year": 2019
        },
        {
            "authors": [
                "Paul F Christiano",
                "Jan Leike",
                "Ryan Lowe."
            ],
            "title": "Training language models to follow instructions with human feedback",
            "venue": "Advances in Neural Information Processing Systems, volume 35, pages 27730\u201327744. Curran Associates, Inc.",
            "year": 2022
        },
        {
            "authors": [
                "Nanyun Peng",
                "Marjan Ghazvininejad",
                "Jonathan May",
                "Kevin Knight."
            ],
            "title": "Towards controllable story generation",
            "venue": "Proceedings of the First Workshop on Storytelling, pages 43\u201349, New Orleans, Louisiana. Association for Computational Linguistics.",
            "year": 2018
        },
        {
            "authors": [
                "Xiangyu Peng",
                "Siyan Li",
                "Sarah Wiegreffe",
                "Mark Riedl."
            ],
            "title": "Inferring the reader: Guiding automated story generation with commonsense reasoning",
            "venue": "Findings of EMNLP, pages 7008\u20137029, Abu Dhabi, United Arab Emirates. Association for Com-",
            "year": 2022
        },
        {
            "authors": [
                "Wentao Qin",
                "Dongyan Zhao."
            ],
            "title": "Retrieval, selection and writing: A three-stage knowledge grounded storytelling model",
            "venue": "Natural Language Processing and Chinese Computing: 11th CCF International Conference, NLPCC 2022, Guilin, China, Septem-",
            "year": 2022
        },
        {
            "authors": [
                "Alec Radford",
                "Jeff Wu",
                "Rewon Child",
                "David Luan",
                "Dario Amodei",
                "Ilya Sutskever"
            ],
            "title": "Language models are unsupervised multitask learners",
            "year": 2019
        },
        {
            "authors": [
                "Hannah Rashkin",
                "Asli Celikyilmaz",
                "Yejin Choi",
                "Jianfeng Gao."
            ],
            "title": "PlotMachines: Outlineconditioned generation with dynamic plot state tracking",
            "venue": "EMNLP, pages 4274\u20134295, Online. Association for Computational Linguistics.",
            "year": 2020
        },
        {
            "authors": [
                "Nils Reimers",
                "Iryna Gurevych."
            ],
            "title": "SentenceBERT: Sentence embeddings using Siamese BERTnetworks",
            "venue": "EMNLP-IJCNLP, pages 3982\u20133992, Hong Kong, China. Association for Computational Linguistics.",
            "year": 2019
        },
        {
            "authors": [
                "Gaurav Sahu",
                "Pau Rodriguez",
                "Issam Laradji",
                "Parmida Atighehchian",
                "David Vazquez",
                "Dzmitry Bahdanau."
            ],
            "title": "Data augmentation for intent classification with off-the-shelf large language models",
            "venue": "Proceedings of the 4th Workshop on NLP for Conver-",
            "year": 2022
        },
        {
            "authors": [
                "Victor Sanh",
                "Lysandre Debut",
                "Julien Chaumond",
                "Thomas Wolf"
            ],
            "title": "Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter",
            "year": 2020
        },
        {
            "authors": [
                "Shouvon Sarker",
                "Lijun Qian",
                "Xishuang Dong"
            ],
            "title": "Medical data augmentation via chatgpt: A case study on medication identification and medication event classification",
            "year": 2023
        },
        {
            "authors": [
                "Timo Schick",
                "Hinrich Sch\u00fctze."
            ],
            "title": "Few-shot text generation with natural language instructions",
            "venue": "EMNLP, pages 390\u2013402, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.",
            "year": 2021
        },
        {
            "authors": [
                "Freda Shi",
                "Xinyun Chen",
                "Kanishka Misra",
                "Nathan Scales",
                "David Dohan",
                "Ed H. Chi",
                "Nathanael Sch\u00e4rli",
                "Denny Zhou."
            ],
            "title": "Large language models can be easily distracted by irrelevant context",
            "venue": "Proceedings of the 40th International Conference on Machine",
            "year": 2023
        },
        {
            "authors": [
                "Weiwei Sun",
                "Lingyong Yan",
                "Xinyu Ma",
                "Pengjie Ren",
                "Dawei Yin",
                "Zhaochun Ren"
            ],
            "title": "Is chatgpt good at search? investigating large language models as re-ranking agent",
            "year": 2023
        },
        {
            "authors": [
                "Pradyumna Tambwekar",
                "Murtaza Dhuliawala",
                "Lara J. Martin",
                "Animesh Mehta",
                "Brent Harrison",
                "Mark O. Riedl."
            ],
            "title": "Controllable neural story plot generation via reward shaping",
            "venue": "IJCAI. International Joint Conferences on Artificial Intelligence Organization.",
            "year": 2019
        },
        {
            "authors": [
                "Rohan Taori",
                "Ishaan Gulrajani",
                "Tianyi Zhang",
                "Yann Dubois",
                "Xuechen Li",
                "Carlos Guestrin",
                "Percy Liang",
                "Tatsunori B. Hashimoto."
            ],
            "title": "Stanford alpaca: An instruction-following llama model",
            "venue": "https:// github.com/tatsu-lab/stanford_alpaca.",
            "year": 2023
        },
        {
            "authors": [
                "Boshi Wang",
                "Xiang Deng",
                "Huan Sun."
            ],
            "title": "Iteratively prompt pre-trained language models for chain of thought",
            "venue": "EMNLP, pages 2714\u20132730, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.",
            "year": 2022
        },
        {
            "authors": [
                "Su Wang",
                "Greg Durrett",
                "Katrin Erk"
            ],
            "title": "2020a. Narrative interpolation for generating and understanding stories",
            "year": 2020
        },
        {
            "authors": [
                "Wei Wang",
                "Hai-Tao Zheng",
                "Zibo Lin."
            ],
            "title": "Selfattention and retrieval enhanced neural networks for essay generation",
            "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 8199\u20138203. IEEE.",
            "year": 2020
        },
        {
            "authors": [
                "Xinpeng Wang",
                "Han Jiang",
                "Zhihua Wei",
                "Shanlin Zhou."
            ],
            "title": "CHAE: Fine-grained controllable story generation with characters, actions and emotions",
            "venue": "COLING, pages 6426\u20136435, Gyeongju, Republic of Korea. International Committee on Computational",
            "year": 2022
        },
        {
            "authors": [
                "Yuxin Wang",
                "Jieru Lin",
                "Zhiwei Yu",
                "Wei Hu",
                "B\u00f6rje F. Karlsson"
            ],
            "title": "Open-world story generation with structured knowledge enhancement: A comprehensive survey",
            "year": 2023
        },
        {
            "authors": [
                "Jason Wei",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Maarten Bosma",
                "Brian Ichter",
                "Fei Xia",
                "Ed Chi",
                "Quoc Le",
                "Denny Zhou"
            ],
            "title": "Chain-of-thought prompting elicits reasoning in large language models",
            "year": 2023
        },
        {
            "authors": [
                "Jason Wei",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Maarten Bosma",
                "brian ichter",
                "Fei Xia",
                "Ed Chi",
                "Quoc V Le",
                "Denny Zhou"
            ],
            "title": "Chain-of-thought prompting elicits reasoning in large language models",
            "venue": "In NeurIPS,",
            "year": 2022
        },
        {
            "authors": [
                "Yuqiang Xie",
                "Yue Hu",
                "Yunpeng Li",
                "Guanqun Bi",
                "Luxi Xing",
                "Wei Peng."
            ],
            "title": "Psychology-guided controllable story generation",
            "venue": "COLING, pages 6480\u2013 6492, Gyeongju, Republic of Korea. International Committee on Computational Linguistics.",
            "year": 2022
        },
        {
            "authors": [
                "Zhuohan Xie",
                "Trevor Cohn",
                "Jey Han Lau"
            ],
            "title": "Can very large pretrained language models learn storytelling with a few examples",
            "year": 2023
        },
        {
            "authors": [
                "An Yang",
                "Quan Wang",
                "Jing Liu",
                "Kai Liu",
                "Yajuan Lyu",
                "Hua Wu",
                "Qiaoqiao She",
                "Sujian Li."
            ],
            "title": "Enhancing pre-trained language representations with rich knowledge for machine reading comprehension",
            "venue": "ACL, pages 2346\u20132357, Florence, Italy. Association",
            "year": 2019
        },
        {
            "authors": [
                "Shunyu Yao",
                "Dian Yu",
                "Jeffrey Zhao",
                "Izhak Shafran",
                "Thomas L. Griffiths",
                "Yuan Cao",
                "Karthik Narasimhan"
            ],
            "title": "Tree of thoughts: Deliberate problem solving with large language models",
            "year": 2023
        },
        {
            "authors": [
                "Kang Min Yoo",
                "Dongju Park",
                "Jaewook Kang",
                "Sang-Woo Lee",
                "Woomyoung Park."
            ],
            "title": "GPT3Mix: Leveraging large-scale language models for text augmentation",
            "venue": "Findings of EMNLP, pages 2225\u20132239, Punta Cana, Dominican Republic. Association for",
            "year": 2021
        },
        {
            "authors": [
                "Wanyue Zhai",
                "Jonathan Rusert",
                "Zubair Shafiq",
                "Padmini Srinivasan"
            ],
            "title": "Can large language models be an alternative to human evaluation",
            "year": 2023
        },
        {
            "authors": [
                "Yiming Zhang",
                "Shi Feng",
                "Chenhao Tan."
            ],
            "title": "Active example selection for in-context learning",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 9134\u2013 9148, Abu Dhabi, United Arab Emirates. Association",
            "year": 2022
        },
        {
            "authors": [
                "Yushi Zhang",
                "Yan Yang",
                "Ming Gu",
                "Feng Gao",
                "Chengcai Chen",
                "Liang He."
            ],
            "title": "Ceg: A joint model for causal commonsense events enhanced story ending generation",
            "venue": "PLOS ONE, 18.",
            "year": 2023
        },
        {
            "authors": [
                "Zhuosheng Zhang",
                "Aston Zhang",
                "Mu Li",
                "Alex Smola"
            ],
            "title": "Automatic chain of thought prompting in large language models",
            "year": 2022
        },
        {
            "authors": [
                "Yan Zhao",
                "Lu Liu",
                "Chunhua Liu",
                "Ruoyao Yang",
                "Dong Yu."
            ],
            "title": "From plots to endings: A reinforced pointer generator for story ending generation",
            "venue": "Natural Language Processing and Chinese Computing, pages 51\u201363, Cham. Springer International Publish-",
            "year": 2018
        },
        {
            "authors": [
                "Wangchunshu Zhou",
                "Yuchen Eleanor Jiang",
                "Ethan Wilcox",
                "Ryan Cotterell",
                "Mrinmaya Sachan"
            ],
            "title": "Controlled text generation with natural language instructions",
            "year": 2023
        },
        {
            "authors": [
                "Yongchao Zhou",
                "Andrei Ioan Muresanu",
                "Ziwen Han",
                "Keiran Paster",
                "Silviu Pitis",
                "Harris Chan",
                "Jimmy Ba"
            ],
            "title": "2023b. Large language models are human-level prompt engineers",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Conditional automatic storytelling, generating a story that satisfies specific target conditions, has gained significant attention in the natural language processing community (Kumar, 2023). Generating stories with complex plots is particularly crucial as it creates engaging stories of human-level quality for various applications, such as AI novelists and AI playwrights (Alhussain and Azmi, 2021).\n\u2217Corresponding Authors.\nStory generation is an active research area where existing studies approach it from two directions: enhancing controllability and incorporating commonsense knowledge (Alabdulkarim et al., 2021). To satisfy target constraints, researchers enhance the controllability of generation models (Zhou et al., 2023a). Rashkin et al. (2020) follow an outline of the plots to generate stories. Wang et al. (2022b) propose a BART-based (Lewis et al., 2020) model to generate stories according to the fine-grained personalized guidance. Additionally, to produce fluent and coherent storylines, researchers investigate incorporating commonsense knowledge into generation (Wang et al., 2020a; Guan et al., 2020; Zhang et al., 2023). Peng et al. (2022) introduce commonsense inference into GPT-2-based (Radford et al., 2019) model to improve narritive coherence. Qin and Zhao (2022) combine knowledge retrieval, knowledge selection, and story generation together to make the generated story more reasonable. The above studies focus on improving controllability and logical coherence but rarely explore the generation of stories with complex plots.\nLarge Language Models (LLMs) learn commonsense knowledge from massive texts and develop strong abilities to follow human instructions (Ouyang et al., 2022; OpenAI, 2023; Taori et al., 2023). Thus, LLM-based prompt learning generates fluent and coherent stories with high controllability (Lu et al., 2023; Xie et al., 2023; Yao et al., 2023). Lu et al. (2023) prompt GPT3 (Brown et al., 2020) with combinations of multiple target conditions. Xie et al. (2023) demonstrate that by using prompts, GPT-3 generates higherquality stories than other state-of-the-art (SOTA) models. Typically, LLMs generate stories based on a given prompt (i.e. text spans or a few sentences) and the outputs are continuations of the given texts. However, a recurring issue emerges in the LLM-based prompting approaches to generate complex stories: there is a struggle to balance\nthe complexity and creativity within the generated stories (Alabdulkarim et al., 2021; Wang et al., 2023). To prompt LLMs to generate stories with complex plots, users often need to detail control signals within the prompt. This approach presents a dilemma: the more control information provided, the more likely it is that the generated story will focus solely on describing the given content, thus constraining the story\u2019s potential creativity.\nWe argue that leveraging the information (e.g. story background and plots) from exemplary human stories facilitates generating more diverse plots. Delving into story details enriches the narrative with the necessary information, thereby helping to build complex and credible storylines.\nIn this paper, we propose a retrieval-auGmented complex stoRy generation framework with a fOrest of eVidEnce (GROVE), which leverages existing stories and evidence to generate and rewrite stories for more complex plots. We construct a retrieval repository that enables the LLM to learn diverse plots and common patterns from human-written stories. This assists the LLM in generating stories with complex plots. Moreover, we design an \u201casking-why\u201d 1 prompting scheme that iteratively builds an evidence forest addressing the ambiguities found in the story from various perspectives. The evidence forest refers to a collection or set of evidence trees that are generated to supplement a story in GROVE. Each evidence tree consists of nodes representing pieces of evidence and edges connecting them. The root node of the tree represents an ambiguous or unclear part in the generated story, while the non-root nodes represent additional information that provides clarity and background details to the nodes above them in the tree. Finally, we select the optimal chains of evidence from the evidence forest and integrate them into the generated story, thereby enhancing its complexity and credibility. Our method is not intended to replace any specifically designed prompts or techniques currently employed in the field. Instead, we propose a flexible and generalizable framework that enables LLMs to generate stories with complex plots, complementing existing methods.\nOur contributions are threefold: 1) We develop a retrieval-augmented framework for generating stories with complex plots by prompting an LLM; 2) We introduce an \u201casking-why\u201d prompting scheme\n1We call the prompting method \u201casking-why\u201d because it requires the LLM to justify why particular ambiguities make sense in the generated story.\nto generate a forest of evidence and rewrite the original story based on the optimal evidence chains; 3) Our approach achieves SOTA performance on quantities of testing cases. Detailed analyses validate the effectiveness of our approach."
        },
        {
            "heading": "2 Related work",
            "text": ""
        },
        {
            "heading": "2.1 Story Generation",
            "text": "Research on automatic story generation can be classified into two categories: enhancing controllability and incorporating commonsense knowledge (Alabdulkarim et al., 2021). Researchers explore both ending-focused approach (Zhao et al., 2018; Guan et al., 2019a) and storyline-focused approach (Peng et al., 2018) to improve the controllability of generated stories. The ending-focused approach aims to generate a story with a specific desired ending. Tambwekar et al. (2019) apply reinforcement learning to optimize the pre-trained model to generate story plots that consistently reach a specified ending for the story. Wang et al. (2020a) leverage an interpolation model based on GPT-2 to produce coherent narratives with user-specified target endings. Lu et al. (2023) explore the generation ability of GPT-3 based on different prompts. The aim of storyline-focused approaches is to make the generated story follow an outline of the plot (Rashkin et al., 2020; Fan et al., 2018). Wang et al. (2022b) propose a BART-based (Lewis et al., 2020) model to generate stories with desired characters, actions, and emotions. Xie et al. (2022) consider psychological state chains of protagonists and propose a psychology-guided controllable story generation system.\nAnother line of work involves the study of incorporating commonsense into story generation either explicitly (Yang et al., 2019; Guan et al., 2020; Mao et al., 2019) or implicitly (Wang et al., 2020a; Guan et al., 2020). Researchers explicitly leverage additional data by incorporating a commonsense knowledge graph into the model encoding (Guan et al., 2019b; Wang et al., 2020b) or using a plot graph based on commonsense descriptions (Ammanabrolu et al., 2020). Implicit knowledge stored in model parameters is also helpful in producing stories. LLMs learn from large amounts of texts, thereby gaining a rich understanding of commonsense knowledge to generate stories. Xie et al. (2023) randomly sample few-shot demonstrations to GPT-3 to guide story generation. Yao et al. (2023) instruct LLM to make multiple plans\nand vote for the best plan to generate stories. Our work is also based on LLMs. However, unlike existing LLM-based approaches for story generation that prompt LLMs with manually chosen cases, GROVE automatically retrieves similar examples to instruct the LLM."
        },
        {
            "heading": "2.2 LLM-based Prompting Learning",
            "text": "In the context of LLMs, prompting refers to a user inputting a text string to the model, eliciting a response from the LLM according to the input (Liu et al., 2023; Li et al., 2023). To fully leverage LLMs in downstream tasks, researchers propose to carefully design prompts either manually (Brown et al., 2020; Hendy et al., 2023; Schick and Sch\u00fctze, 2021) or automatically (Gao et al., 2021; Zhou et al., 2023b; Guo et al., 2022). Wang et al. (2022a) explore an iterative prompting framework, which progressively elicits knowledge from language models by prompting automatically. Wei et al. (2023) find that the Chain-ofThought (CoT) prompting, a kind of prompt that instructs the model to provide a rationale for its answer, shows advantages in complex arithmetic and reasoning tasks. Zhang et al. (2022b) classify CoT prompting into three paradigms: Zero-ShotCoT (Kojima et al., 2022), Manual-CoT (Wei et al., 2022), and Auto-CoT (Zhang et al., 2022b). ZeroShot-CoT involves adding a prompt like \u201cLet\u2019s consider the following step-by-step\u201d to the test question, which helps LLMs consider problems more logically. Manual-CoT (Wei et al., 2023) is a fewshot prompting method that provides manual reasoning demonstrations to the LLMs. Zhang et al. (2022b) propose Auto-CoT to construct demonstrations with questions and reasoning chains automatically. Yao et al. (2023) propose Tree-of-Thoughts (ToT) prompting to improve LLM\u2019s performance by voting for different reasoning. These studies approach a task by deconstructing it into multiple steps and executing them sequentially. In contrast, our approach initially completes the entire task, and then iteratively refines and improves it."
        },
        {
            "heading": "2.3 LLM-based Data Augmentation",
            "text": "Researchers investigate generating pseudo data to alleviate the issue of data scarcity (Feng et al., 2021; Plu\u0161c\u030cec and \u0160najder, 2023) for tasks including knowledge distilling (Sanh et al., 2020; Sun et al., 2023), event classification (Sarker et al., 2023) and harmful content detection (Hartvigsen et al., 2022). Yoo et al. (2021) combine text perturbation, pseudo-\nlabeling, and knowledge distillation to generate realistic text samples with LLMs. Sahu et al. (2022) create prompts from available examples and feed them to LLMs to generate training data for intent classification. Our work is another attempt to leverage LLMs for data augmentation that uses an LLM to extract narrative attributes from existing stories."
        },
        {
            "heading": "3 Method",
            "text": ""
        },
        {
            "heading": "3.1 Overview",
            "text": "Fig. 1 presents an overview of our framework. GROVE consists of three parts: (1) Retrieval Repository builds a repository R with humanwritten stories and the associated desired control signals. (2) Evidence Forest Construction via Asking Why retrieves and generates stories with inputs (steps 1 to 3 in Fig. 1) and recursively grows a forest of evidence in support of the story (steps 4 and 5). (3) Evidence Chains-supported Story Rewriting selects optimal evidence chains from the evidence forest, which are the most relevant to the target conditions, and employs them to enrich the story (steps 6 to 8 in Fig. 1).\nTo generate a story, our framework receives a set of target conditions C = {c1, . . . , cm}, which comprise multiple text spans to express the desired mood, plot, genre, subject, etc. By employing an LLM, we aim to generate an interesting story with complex plots satisfying the control conditions."
        },
        {
            "heading": "3.2 Retrieval Repository",
            "text": "We construct a retrieval repository consisting of multiple key-value retrieval items, where the key represents the target conditions of a story, and the value is the story itself. To construct the repository, we use a set of conditions extracted from the value (i.e. story) as the key.The repository acts as a source of inspiration and provides the LLM with a rich set of story examples to draw from. It helps the LLM to learn from various narrative attributes and incorporate them into its generated stories, facilitating the generation of complex storylines."
        },
        {
            "heading": "3.2.1 Repository Construction",
            "text": "To construct the repository, we collect raw stories from the Internet, using these as values, and employ LLM to extract target conditions from the story as keys. To facilitate this extraction, we query the LLM using human-written prompts for each type of target condition.\nWe obtain the values for retrieval items by collecting a large set of raw stories from the Internet, denoted as D = {d1, d2, . . . , dn}. To obtain the corresponding keys for these values, we take two steps: prompt construction and condition extraction. Specifically, (1) In the prompt construction step, we construct a prompt template prompt(.) for each type of target condition. This template serves as a natural language query asking for the specific condition within a given story. Recall that C = {c1, . . . , cm} comprises a series of different types of target control conditions, including story plot, subject, etc, where each control condition ci is described by a text span. Note that the \u201cplot\u201d condition specifies the storylines that must appear in the story, rather than defining a limited set of allowable plots. For example, for the target condition \u201csubject\u201d, the associated prompt template can be:\n\u201cHere is a story: [STORY]. Answer the following question based on the above story: give a list of distinctive subjects this story is trying to portray.\u201d, where \u201c[STORY]\u201d represents the given story content. We detail the prompt templates for all target conditions in App. F. (2) In the condition extraction step, for each story dj in D, we use LLM to extract each condition c\u0303i by feeding prompti(dj) into the LLM.\nEach story dj , along with its extracted conditions C\u0303j = {c\u03031, . . . , c\u0303m}, constitutes an item (C\u0303j , dj) in the retrieval repository. Ultimately, the repository R consists of pairs of stories and their extracted conditions: R = {(C\u0303j , dj)}|D|j=1 ."
        },
        {
            "heading": "3.2.2 Repository Retrieval",
            "text": "The retrieval process searches for the most similar condition sets and returns their corresponding stories from the retrieval repository. The search is based on the semantic similarity between the target control conditions and the items\u2019 keys in R.\nSpecifically, during inference, given a target condition set C, we define a recommendation score s for each story. To obtain s, we calculate the cosine similarity between the semantic vector of each condition in C\u0303 and that of its corresponding condition in C, and then sum up the cosine similarity scores for all conditions:\ns = m\u2211 i cos (f (c\u0303i) , f (ci)) ,\nwhere f(.) is an off-the-shelf semantic encoder2. We sort all stories in R based on their recommenda-\n2We use SBERT (Reimers and Gurevych, 2019) in our experiment.\ntion scores s and return the top-k highest-ranking retrieval items, along with their stories, represented as W = {(C\u0303j , dj)}kj=1."
        },
        {
            "heading": "3.3 Evidence Forest Construction via Asking Why",
            "text": "We employ an LLM to generate an initial story and then iteratively ask the LLM to construct a forest of evidence that supplements the story. The intuition is that referring to the retrieved story incentivizes the LLM to produce diverse new stories. This may result in a lack of concrete supporting details and appear hollow, which makes the story less credible and informative. To address this issue, we design an iterative asking-why prompting scheme to recursively collect pieces of evidence, thus enriching and clarifying ambiguous parts in complex plots.\nThe algorithm first generates an initial story generation then generates the unclear parts (named \u201cambiguity\u201d) in the initial story, and finally collects the evidence by iteratively asking why. Firstly, to generate the initial story, we construct an incontext learning prompt using the retrieval results in W and the target conditions C. Then, we feed this prompt into the LLM to obtain an initial story. Secondly, to discover the unclear parts in the story, we instruct the LLM to generate N relevant \u201cambiguities\u201d for the initial story concerning the target conditions, where ambiguity is a significant drawback that decreases the story\u2019s credibility. For example, an ambiguity can be an unclear motivation or a seemingly illogical plot. We prompt the LLM to generate ambiguity as: \u201cHere is a story: [STORY]. When analyzing fictional stories, it is okay to mention the negative aspects. Pretend to be a writer, and without further ado, point out N missing background information in the story with N simple sentences one by one.\u201d Finally, to collect evidence, we propose to iteratively ask why questions to LLM. By asking why, we instruct the LLM to provide b pieces of evidence that compensate for the initial story. For each ambiguity, we recursively ask the \u201cwhy\u201d question for I iterations and obtain an evidence tree {E , A}, where E is the set of nodes and A represents the set of edges. In an evidence tree, the root node represents an ambiguity, and non-root nodes are pieces of evidence that provide additional information to the nodes connected to them in the upper layer. We define an evidence chain E\u0304 = {e0, . . . , eI} is a path from a tree\u2019s root node (e0 representing the ambiguity) to\na leaf node that comprises a sequence of I pieces of evidence, where each piece of evidence supplements the preceding one. To perform asking-why on each node, we concatenate its corresponding evidence chain with a pre-defined prompt template and feed it to the LLM. The template for asking why can be: \u201cHere is a story: [STORY]. A missing detail is: [EVIDENCE CHAIN]. Except for pure coincidence, point out b factual pieces of background information that compensate the story one by one. Each additional piece of information should be in one short sentence and only contain factual information without opinions or judgments.\u201dAs there are N ambiguities, we obtain an evidence forest F = {{E1, A1}, . . . , {EN , AN }}.\nOur iterative asking-why prompting scheme explores multiple possibilities by prompting the LLM to supplement new evidence obtained from the last iteration. In this way, we create an evidence forest F to support the initial story. We can adjust the amount of information by increasing b and the number of iterations I ."
        },
        {
            "heading": "3.4 Evidence Chains-supported Story Rewriting",
            "text": "The LLM selects the optimal evidence chain from each tree to incorporate into the original story. The intuition for story rewriting is to address its ambiguities by incorporating relevant pieces of evidence into the story to provide the necessary information.\nWe select evidence chains to augment the story in two steps.\n\u2022 Evidence chains selection. For each evidence tree, we first concatenate all the evidence chains before feeding them into the LLM. Then, we prompt the LLM to select the most suitable evidence chain to add to the initial story. The selection is based on the relevance between chains and the initial story. We repeat this process on all trees in the evidence forest F and obtain N evidence chains, denoted as {E\u0304 , A}NI=1.\n\u2022 Story rewriting. We instruct the LLM to incorporate the information from {E\u0304 , A}Ni=1 into the initial story to rewrite the final version of the story. The prompt template is: \u201cHere is a story: [STORY]. Here is the missing background information: [EVIDENCE CHAINS]. Pretend to be a writer and complete the story by including the given information. Modify\nthe necessary sentences in the story and repeat the unrelated parts to include the given background information.\u201d\nRecall that each piece of evidence is a node in the tree with b child nodes (except for the external nodes). These child nodes support it in different ways, making the information from those child nodes mutually exclusive. If we incorporate multiple chains into the story that are from the same tree, it would contain contradictory pieces of evidence from one node\u2019s different child nodes, compromising the logical consistency of the narrative. Therefore, to ensure logical coherence and avoid potential contradictions, we select only one evidence chain from each tree for inclusion in the final story."
        },
        {
            "heading": "4 Experiments",
            "text": ""
        },
        {
            "heading": "4.1 Experimental Settings",
            "text": "Datasets. Following Lu et al. (2023), we consider plot, mood, genre, and subject as target conditions. plot describes the events that must appear in the generated story. mood defines the expected emotional response of the reader after reading the generated story. genre dictates the desired story type and subject indicates the subjects that should be mentioned in the story. We randomly draw 50 prompts from the testing set of the WritingPrompt dataset for plot. Following Lu et al. (2023), we consider happy, angry, fearful, and sad for mood. For genre, we consider historical fiction, literary fiction, and science fiction. Lover, cat, and survivor are for subject. We experiment with all combinations of these four types of control components across 1800 testing cases (derived from 50 prompts, 4 moods, 3 genres, and 3 subjects), ensuring that each type of condition only appears once in each case. Besides, we use the 1.5K unlabeled movie plot summaries from the IMDB movie details dataset 3 to build the retrieval repository. Evaluation Metrics. We follow the common practice in automatic story generation for evaluation (Karpinska et al., 2021; Zhai et al., 2023) and measure the following 4 aspects:(1) Grammar: How grammatically correct is the text of the story? (2) Coherence: How well do the sentences in the story fit together? (3) Likability: How enjoyable or appealing is the story to the readers? (4) Relevance: How closely does the story align with\n3www.kaggle.com/datasets/txgg123/imdb-movie-details\nthe target constraints? Additionally, we propose two new metrics tailored for evaluating stories with complex plots: (5) Complexity: How complex is the plot structure in the story? (6) Creativity: How creative is the story\u2019s plot design?\nWe evaluate stories on these six metrics using a 5-point Likert scale with human evaluation. We hire three evaluators with Master\u2019s degrees in English Literature from a commercial company to independently evaluate 100 randomly sampled stories paired with instructions4. As Zhai et al. (2023) found that LLMs can serve as a cheap alternative for human evaluation, we evaluate each story in Sec. 4.4 by querying ChatGPT three times using the instructions in Zhai et al. (2023). We calculate the average scores and variances for human and model-based evaluation respectively. Baselines. We compare our method against five baselines: Human (Fan et al., 2018) is written ground truth stories under the same prompts. ICL (Xie et al., 2023) explicitly instructs an LLM to generate a story to satisfy target conditions and contain many interesting plots. CoT follows the Chain-of-Thought prompting strategy (Wei et al., 2022), where the LM is asked to follow specific instructions, generate a story, identify the missing information, and iteratively revise the story to include missing backgrounds step by step. Prompt-E performs prompt engineering by modifying the instruction of ICL to obtain 4 variants that explicitly require creativity and complexity from the generated story and taking the average of their performance for evaluation. Specifically, it adds \u201cGenerate a complex and creative story\u201d, \u201cGenerate a detailed and complex story\u201d, \u201cEnsure that the story is creative and rich in plots.\u201d, and \u201cGenerate a long and complex story\u201d to the ICL instruction, respectively. Story-S prompts the LLM to generate a story multiple times. For each sample, we add up the scores of six metrics to obtain the overall score and select the story with the highest overall score as the final story. We use ChatGPT as the base model for all the above methods for a fair comparison (implementation details are in App. C. As the API to access GPT-4 model is hard to apply, which limits our ability to conduct large-scale experiments for direct comparison. However, the theoretical underpinnings and method of our work remain applicable to GPT-4.\n4We do not use the commonly adopted AMT since Karpinska et al. (2021) found that their results are questionable."
        },
        {
            "heading": "4.2 Overall Performance",
            "text": "Tab. 1 shows the results of all methods of human evaluation in story generation. GROVE achieves the best performance on almost all metrics. Human baseline underperforms other automatic approaches that are based on the same LLM (i.e. ChatGPT), indicating the strong ability of LLM to produce high-quality stories. ICL generates a story by instructing the LLM with target conditions. It achieves the highest performance in Relevance and Grammar, indicating ChatGPT\u2019s strong ability to follow instructions and generate correct English. Compared to GROVE, ICL produces the least complex stories under both human and automatic evaluation, which means that directly instructing the LLM with target conditions struggles to generate complex stories. CoT self-improves the story in a single output step-by-step by asking and answering questions about the initial story and rewriting it to obtain the final story. CoT generates slightly more complex and likable stories than ICL. It shows that Chain-of-Thought prompting is effective in improving the story\u2019s complexity and fondness. However, CoT is still worse than GROVE on all metrics because it is challenging for ChatGPT to execute all steps in a single output5. We find in our experiments that the story quality of Story-S is highly inconsistent among different generations. Besides, even if it is possible to obtain complex and creative stories, Story-S cannot diversify or prolong one particular user-desired story. GROVE benefits from the retrieval-augmented approach and the asking-why prompting scheme. Since it introduces more complex plots, GROVE inevitably incorporates additional information into the story that may be irrelevant to the target conditions. While producing slightly less relevant stories than ICL, it still scores high on Relevance and achieves the best performance in the rest metrics. We also conduct prompt engineering on GROVE and obtain GROVE (Prompt-E), which further improves its story generation ability."
        },
        {
            "heading": "4.3 Ablation study",
            "text": "Tab. 2 shows the human evaluation results of the ablation studies on our proposed components and verifies their effectiveness. \u2212 Retrieve generates stories without referring to relevant few-shot ex-\n5We calculate that CoT fails to finish the required number of iterations for evidence generation before generating the final story in more than 40% of the testing cases.\namples. \u2212 Retrieve underperforms GROVE in all metrics, especially on Relevance. The performance drop indicates that the retrieval enhances the understanding of desirable outputs and helps generate coherent and relevant stories. \u2212 Rewrite skips the evidence-based story rewriting, which deepens the storylines and explores untold backgrounds and indepth rationale. The drop in Complexity shows that the stories generated without rewriting lack a certain depth and complexity, thus validating the importance of evidence-based rewriting in enriching story complexity. As \u2212 Rewrite generates without exploring deeper details, resulting in stories that more closely stick to the given instruction, thus demonstrating a slight advantage over GROVE in Relevance. However, the Relevance of GROVE remains high, even while scoring high in Complexity. \u2212 Select omits to select optimal evidence chains and incorporate all evidence into the final stories. The lack of evidence filtration introduces unrelated and conflicting information, producing verbose and illogical stories with decreased Coherence and Relevance. Furthermore, the drop in Complexity and Creativity indicates the importance of the selection process in refining the stories\u2019 complexity and originality. Given the initial stories, \u2212 Evidence directly instructs the LLM to include the necessary details to revise the stories. \u2212 Evidence enables story revisions. However, without explicitly prompting the LLM with the necessary information, it may add insignificant details that hardly improve the story quality. + Evidence increases the number of evidence trees thereby improving Complexity, while possibly making the generated stories too specific, thereby affecting Likability. Inserting a fixed complex story into the prompt (Fixed Story) leads to unstable performance. It decreases the stories\u2019 Coherence, Likability, and Relevance, echoing the discoveries of Shi et al. (2023) and Zhang et al. (2022a) that irrelevant or random samples distract LLM, thereby hurting performance."
        },
        {
            "heading": "4.4 Generalization Ability",
            "text": "We verify the generalization ability of GROVE on a much smaller open-source LLM (i.e. Alpaca-Plus7B (Cui et al., 2023)). Due to the high computational costs of many LLMs, exploring smaller models provides a more affordable option for smaller teams. We apply GROVE on Alpaca-Plus-7B (Alpaca) to compare with a simple baseline ICL in Tab. 3 and showcase its generation results to com-\npare with that of ChatGPT in Tab. 4 (see full results in Tab. 9 and Tab. 10). GROVE improves the generation quality of Alpaca on almost all metrics. For the same instruction, ChatGPT generates an initial story satisfying all desired control conditions. In contrast, the initial story generated by Alpaca only meets part of the target conditions (subject and genre) and struggles to satisfy the rest (plot and mood). Both final stories incorporate the information from the evidence chains, however, ChatGPT fuses the information in a more natural and coherent way. The performance gap between the two models is understandable because Alpaca\u2019s scale of model size and training data is much smaller than ChatGPT, thus possessing a relatively limited capacity to handle multiple, complex instructions. Despite the relatively poor controllability of Alpaca, GROVE is still helpful in providing complex stories with rich information."
        },
        {
            "heading": "4.5 Plot Enumeration Analysis",
            "text": "We propose a model-based evaluation method to calculate the average number of story plots for dif-\nferent baselines, which verifies that GROVE generates complex stories with rich plots (see Tab. 5). We randomly sample 100 stories generated by each baseline respectively. Then, we construct a prompt template that instructs the LLM to generate a list of plots for each story. The prompt template is:\n\u201cHere is a story: [STORY]. Give me an organized list of sentences, each of which describes one plot.\u201d We fill the template with each story, feed it into the LLM, and count the number of plots from the LLM\u2019s output. Finally, for each method, we calculate the average number of plots across all tested stories. GROVE outperforms other methods in generating complex stories, with the highest average number of plots per story. This underscores GROVE\u2019s effectiveness in creating multiple complex and engaging storylines, thereby enhancing the depth and variety of story generation."
        },
        {
            "heading": "4.6 Plagiarism Detection",
            "text": "We evaluate the potential intellectual property infringements of our generated stories through Ngram overlap and plagiarism detection. The N-\ngram overlap results show that our generated seldom directly copied text spans from retrieved stories. We apply a commercial plagiarism detection service6 that categorizes text similarities as Identical, Minor Changes, Paraphrased, and Omitted Words. All the results are zero, indicating no infringements. The results in Tab. 6 demonstrate that GROVE generates innovative and original stories, respecting the intellectual property of the reference stories."
        },
        {
            "heading": "5 Conclusion",
            "text": "We propose GROVE, a retrieval-augmented framework to generate stories with complex plots by iter-\n6app.copyleaks.com\native asking-why prompting. We build a retrieval repository with existing stories by extracting desired controlling information to inspire a new generation. We design an algorithm that iteratively prompts LLM to obtain a forest of evidence that compensates for the generated story. Moreover, we revise the story by referring to the most relevant chains of evidence. Experimental results show that our proposed method outperforms strong baselines in ensuring story complexity and creativity."
        },
        {
            "heading": "6 Acknowledgement",
            "text": "This work is supported by the following foundations: the National Natural Science Foundation of China under Grant No.62025208 and No.62306330, and the Xiangjiang Laboratory Foundation under Grant No.22XJ01012."
        },
        {
            "heading": "A Limitations",
            "text": "Our approach heavily relies on the capability of the underlying LLM, which means improvements in story complexity and coherence may be constrained by the LLM\u2019s inherent limitations. Furthermore, it may introduce bias if certain types of evidence or narrative chains are favored by the LLM, impacting the diversity of generated stories. We encourage future works to analyze the inner workings of LLMs and more effective control strategies for better understanding and utilization."
        },
        {
            "heading": "B Ethics Statement",
            "text": "Our proposed method aims to generate a complex story with given target conditions. We hope that our work can inspire future studies on controllable text generation. We acknowledge that GROVE poses potential harm when it is used with malicious intentions. Firstly, one may use GROVE to guide the generation to produce biased or harmful information. Secondly, using existing stories for retrieval may violate copyright laws if improperly handled, so attention to data sourcing and fair use principles is essential. Appropriate debiasing measures and content moderation strategies can alleviate these potential negative impacts. We encourage future research to study these issues.\nC Implementation Details\nFor our experiments with ChatGPT, we access the gpt-3.5-turbo model by API calls. we set the number of few-shot examples k to 1. During story generation, We set the number of ambiguities N , the number of iterations I , and the evidence number b to 2. We resample another prompt from the WritingPrompt dataset when ChatGPT repeatedly refuses to generate a story for the given prompt. We calculate the average of their performance on the automatic evaluation. During the automatic evaluation, we construct the instructions for Complexity and Creativity following the same format in Zhai et al. (2023) and keep feeding the same instruction to ChatGPT until it provides a rating. In Sec. 4.4, we set N and I to 1 for Alpaca and ChatGPT for a fair comparison. We adopt the nucleus sampling scheme with p set to 0.73 and generation temperature set to 0.72."
        },
        {
            "heading": "D Automatic Evaluations",
            "text": "We show automatic evaluation results in Tab. 7 and Tab. 8. The overall performance of GROVE surpasses most baselines. Different parts of GROVE are crucial to its effectiveness."
        },
        {
            "heading": "E Case Study",
            "text": "We demonstrate the retrieved stories in Tab. 11 and generated stories in Tab. 12 and Tab. 13. GROVE produces stories with more creative and complex plots. Since ICL is unaware of the necessary evidence, it occasionally omits story backgrounds and character traits, which are essential for reader comprehension and engagement (see Tab. 12). CoT may fail to finish all required steps to generate a story in a single-round interaction (see Sec 4.2 for details), thus generating stories with limited improvements (see Tab. 13). With retrieval and evidence-based story rewriting, GROVE consistently produces complex stories supported by necessary details.\nF Important Instructions\nWe demonstrate our prompt templates in Tab. 14. In our experiment, since the dataset provides the story genres, we do not need to extract them from LLM."
        }
    ],
    "title": "GROVE: A Retrieval-augmented Complex Story Generation Framework with A Forest of Evidence",
    "year": 2023
}