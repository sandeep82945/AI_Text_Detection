{
    "abstractText": "Pre-trained language models (PLMs) are often deployed as cloud services, enabling users to upload textual data and perform inference remotely. However, users\u2019 personal text often contains sensitive information, and sharing such data directly with the service providers can lead to serious privacy leakage. To address this problem, we introduce a novel privacy-preserving inference framework called TextMixer, which prevents plaintext leakage during the inference phase. Inspired by k-anonymity, TextMixer aims to obfuscate a user\u2019s private input by mixing it with multiple other inputs, thereby confounding potential privacy attackers. To achieve this, our approach involves: (1) proposing a novel encryption module, Privacy Mixer, which encrypts input from three distinct dimensions: mixing, representation, and position. (2) adopting a pre-trained Multi-input Multi-output network to handle mixed representations and obtain multiple predictions. (3) employing a Privacy Demixer to ensure only the user can decrypt the real output among the multiple predictions. Furthermore, we explore different ways to automatically generate synthetic inputs required for mixing. Experimental results on token and sentence classification tasks demonstrate that TextMixer greatly surpasses existing privacy-preserving methods in both performance and privacy.",
    "authors": [
        {
            "affiliations": [],
            "name": "Xin Zhou"
        },
        {
            "affiliations": [],
            "name": "Yi Lu"
        },
        {
            "affiliations": [],
            "name": "Ruotian Ma"
        },
        {
            "affiliations": [],
            "name": "Tao Gui"
        },
        {
            "affiliations": [],
            "name": "Qi Zhang"
        },
        {
            "affiliations": [],
            "name": "Xuanjing Huang"
        }
    ],
    "id": "SP:0f3f255d4d27f6ed3788a5b266d990599cb3bd6e",
    "references": [
        {
            "authors": [
                "Tianyu Chen",
                "Hangbo Bao",
                "Shaohan Huang",
                "Li Dong",
                "Binxing Jiao",
                "Daxin Jiang",
                "Haoyi Zhou",
                "Jianxin Li",
                "Furu Wei."
            ],
            "title": "THE-X: Privacy-preserving transformer inference with homomorphic encryption",
            "venue": "Findings of the Association for Computational",
            "year": 2022
        },
        {
            "authors": [
                "Maximin Coavoux",
                "Shashi Narayan",
                "Shay B Cohen."
            ],
            "title": "Privacy-preserving neural representations of text",
            "venue": "arXiv preprint arXiv:1808.09408.",
            "year": 2018
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "arXiv preprint arXiv:1810.04805.",
            "year": 2018
        },
        {
            "authors": [
                "Ye Dong",
                "Wen jie Lu",
                "Yancheng Zheng",
                "Haoqi Wu",
                "Derun Zhao",
                "Jin Tan",
                "Zhicong Huang",
                "Cheng Hong",
                "Tao Wei",
                "Wenguang Chen"
            ],
            "title": "Puma: Secure inference of llama-7b in five minutes",
            "year": 2023
        },
        {
            "authors": [
                "David Evans",
                "Vladimir Kolesnikov",
                "Mike Rosulek"
            ],
            "title": "A pragmatic introduction to secure multi-party computation. Foundations and Trends\u00ae in Privacy and Security, 2(2-3):70\u2013246",
            "year": 2018
        },
        {
            "authors": [
                "Craig Gentry."
            ],
            "title": "Fully homomorphic encryption using ideal lattices",
            "venue": "Proceedings of the Forty-First Annual ACM Symposium on Theory of Computing, STOC \u201909, page 169\u2013178, New York, NY, USA. Association for Computing Machinery.",
            "year": 2009
        },
        {
            "authors": [
                "Marton Havasi",
                "Rodolphe Jenatton",
                "Stanislav Fort",
                "Jeremiah Zhe Liu",
                "Jasper Snoek",
                "Balaji Lakshminarayanan",
                "Andrew M Dai",
                "Dustin Tran."
            ],
            "title": "Training independent subnetworks for robust prediction",
            "venue": "arXiv preprint arXiv:2010.06610.",
            "year": 2020
        },
        {
            "authors": [
                "Johannes H\u00f6hmann",
                "Achim Rettinger",
                "Kai Kugler."
            ],
            "title": "Invbert: Text reconstruction from contextualized embeddings used for derived text formats of literary works",
            "venue": "arXiv preprint arXiv:2109.10104.",
            "year": 2021
        },
        {
            "authors": [
                "Shlomo Hoory",
                "Amir Feder",
                "Avichai Tendler",
                "Sofia Erell",
                "Alon Peled-Cohen",
                "Itay Laish",
                "Hootan Nakhost",
                "Uri Stemmer",
                "Ayelet Benjamini",
                "Avinatan Hassidim"
            ],
            "title": "Learning and evaluating a differentially private pre-trained language model",
            "year": 2021
        },
        {
            "authors": [
                "Yangsibo Huang",
                "Zhao Song",
                "Danqi Chen",
                "Kai Li",
                "Sanjeev Arora."
            ],
            "title": "TextHide: Tackling data privacy in language understanding tasks",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1368\u20131382, Online. Association",
            "year": 2020
        },
        {
            "authors": [
                "Marija Jegorova",
                "Chaitanya Kaul",
                "Charlie Mayor",
                "Alison Q O\u2019Neil",
                "Alexander Weir",
                "Roderick MurraySmith",
                "Sotirios A Tsaftaris"
            ],
            "title": "Survey: Leakage and privacy at inference time",
            "venue": "IEEE Transactions on Pattern Analysis and Machine",
            "year": 2022
        },
        {
            "authors": [
                "Dacheng Li",
                "Rulin Shao",
                "Hongyi Wang",
                "Han Guo",
                "Eric P Xing",
                "Hao Zhang."
            ],
            "title": "Mpcformer: fast, performant and private transformer inference with mpc",
            "venue": "arXiv preprint arXiv:2211.01452.",
            "year": 2022
        },
        {
            "authors": [
                "Yitong Li",
                "Timothy Baldwin",
                "Trevor Cohn."
            ],
            "title": "Towards robust and privacy-preserving text representations",
            "venue": "arXiv preprint arXiv:1805.06093.",
            "year": 2018
        },
        {
            "authors": [
                "Chin-Yew Lin."
            ],
            "title": "Rouge: A package for automatic evaluation of summaries",
            "venue": "Text summarization branches out, pages 74\u201381.",
            "year": 2004
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov"
            ],
            "title": "Roberta: A robustly optimized bert pretraining approach",
            "year": 2019
        },
        {
            "authors": [
                "Zhijian Liu",
                "Zhanghao Wu",
                "Chuang Gan",
                "Ligeng Zhu",
                "Song Han."
            ],
            "title": "Datamix: Efficient privacypreserving edge-cloud inference",
            "venue": "Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part",
            "year": 2020
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter"
            ],
            "title": "Decoupled weight decay regularization",
            "year": 2019
        },
        {
            "authors": [
                "Lingjuan Lyu",
                "Xuanli He",
                "Yitong Li."
            ],
            "title": "Differentially private representation for nlp: Formal guarantee and an empirical study on privacy and fairness",
            "venue": "arXiv preprint arXiv:2010.01285.",
            "year": 2020
        },
        {
            "authors": [
                "Lingjuan Lyu",
                "Xuanli He",
                "Yitong Li."
            ],
            "title": "Differentially private representation for NLP: Formal guarantee and an empirical study on privacy and fairness",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020, pages",
            "year": 2020
        },
        {
            "authors": [
                "Andrew L. Maas",
                "Raymond E. Daly",
                "Peter T. Pham",
                "Dan Huang",
                "Andrew Y. Ng",
                "Christopher Potts."
            ],
            "title": "Learning word vectors for sentiment analysis",
            "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational",
            "year": 2011
        },
        {
            "authors": [
                "Vishvak Murahari",
                "Ameet Deshpande",
                "Carlos E Jimenez",
                "Izhak Shafran",
                "Mingqiu Wang",
                "Yuan Cao",
                "Karthik Narasimhan."
            ],
            "title": "Mux-plms: Pretraining language models with data multiplexing",
            "venue": "arXiv preprint arXiv:2302.12441.",
            "year": 2023
        },
        {
            "authors": [
                "Vishvak Murahari",
                "Carlos Jimenez",
                "Runzhe Yang",
                "Karthik Narasimhan."
            ],
            "title": "Datamux: Data multiplexing for neural networks",
            "venue": "Advances in Neural Information Processing Systems, 35:17515\u2013 17527.",
            "year": 2022
        },
        {
            "authors": [
                "Xudong Pan",
                "Mi Zhang",
                "Shouling Ji",
                "Min Yang."
            ],
            "title": "Privacy risks of general-purpose language models",
            "venue": "2020 IEEE Symposium on Security and Privacy (SP), pages 1314\u20131331. IEEE.",
            "year": 2020
        },
        {
            "authors": [
                "Jeffrey Pennington",
                "Richard Socher",
                "Christopher Manning."
            ],
            "title": "GloVe: Global vectors for word representation",
            "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1532\u20131543, Doha, Qatar.",
            "year": 2014
        },
        {
            "authors": [
                "Richard Plant",
                "Dimitra Gkatzia",
                "Valerio Giuffrida."
            ],
            "title": "CAPE: Context-aware private embeddings for private language learning",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7970\u20137978, Online and",
            "year": 2021
        },
        {
            "authors": [
                "Chen Qu",
                "Weize Kong",
                "Liu Yang",
                "Mingyang Zhang",
                "Michael Bendersky",
                "Marc Najork."
            ],
            "title": "Natural language understanding with privacy-preserving bert",
            "venue": "Proceedings of the 30th ACM International Conference on Information & Knowledge Management,",
            "year": 2021
        },
        {
            "authors": [
                "Alexandre Ram\u00e9",
                "R\u00e9my Sun",
                "Matthieu Cord."
            ],
            "title": "Mixmo: Mixing multiple inputs for multiple outputs via deep subnetworks",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 823\u2013833.",
            "year": 2021
        },
        {
            "authors": [
                "Congzheng Song",
                "Ananth Raghunathan."
            ],
            "title": "Information leakage in embedding models",
            "venue": "Proceedings of the 2020 ACM SIGSAC Conference on Computer and Communications Security, pages 377\u2013390.",
            "year": 2020
        },
        {
            "authors": [
                "Congzheng Song",
                "Vitaly Shmatikov."
            ],
            "title": "Overlearning reveals sensitive attributes",
            "venue": "arXiv preprint arXiv:1905.11742.",
            "year": 2019
        },
        {
            "authors": [
                "Latanya Sweeney."
            ],
            "title": "k-anonymity: A model for protecting privacy",
            "venue": "International journal of uncertainty, fuzziness and knowledge-based systems, 10(05):557\u2013570.",
            "year": 2002
        },
        {
            "authors": [
                "Erik F. Tjong Kim Sang",
                "Fien De Meulder."
            ],
            "title": "Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition",
            "venue": "Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003, pages 142\u2013",
            "year": 2003
        },
        {
            "authors": [
                "Ralph Weischedel",
                "Martha Palmer",
                "Mitchell Marcus",
                "Eduard Hovy",
                "Sameer Pradhan",
                "Lance Ramshaw",
                "Nianwen Xue",
                "Ann Taylor",
                "Jeff Kaufman",
                "Michelle Franchini"
            ],
            "title": "Linguistic Data Consortium, Philadel",
            "venue": "Ontonotes release",
            "year": 2013
        },
        {
            "authors": [
                "Xiang Yue",
                "Minxin Du",
                "Tianhao Wang",
                "Yaliang Li",
                "Huan Sun",
                "Sherman S.M. Chow."
            ],
            "title": "Differential privacy for text analytics via natural text sanitization",
            "venue": "Findings, ACL-IJCNLP 2021.",
            "year": 2021
        },
        {
            "authors": [
                "Xiang Zhang",
                "Junbo Zhao",
                "Yann LeCun."
            ],
            "title": "Character-level convolutional networks for text classification",
            "venue": "Advances in neural information processing systems, 28.",
            "year": 2015
        },
        {
            "authors": [
                "Mengxin Zheng",
                "Qian Lou",
                "Lei Jiang."
            ],
            "title": "Primer: Fast private transformer inference on encrypted data",
            "venue": "arXiv preprint arXiv:2303.13679.",
            "year": 2023
        },
        {
            "authors": [
                "Xin Zhou",
                "Jinzhu Lu",
                "Tao Gui",
                "Ruotian Ma",
                "Zichu Fei",
                "Yuran Wang",
                "Yong Ding",
                "Yibo Cheung",
                "Qi Zhang",
                "Xuanjing Huang."
            ],
            "title": "TextFusion: Privacypreserving pre-trained model inference via token fusion",
            "venue": "Proceedings of the 2022 Conference on",
            "year": 2022
        },
        {
            "authors": [
                "Tjong Kim Sang",
                "De Meulder"
            ],
            "title": "2003) is a popular benchmark dataset for named entity recognition (NER) tasks. It consists of English and German news articles annotated with named entity labels",
            "year": 2003
        },
        {
            "authors": [
                "C this"
            ],
            "title": "Discussion about TextFusion TextFusion (Zhou et al., 2022) aims to fuse token representations and prevent privacy attackers from collecting ideal data and training a one-to",
            "year": 2022
        },
        {
            "authors": [
                "Loshchilov",
                "Hutter"
            ],
            "title": "2019) and a linear learning rate scheduler, as suggested by the Hugging Face default setup. For all defense methods, we choose Laplace noise to disturb the representation and train 30 epochs to guarantee convergence",
            "year": 2019
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "The emergence of pre-trained language models (PLMs) (Devlin et al., 2018; Liu et al., 2019; Brown et al., 2020) has significantly increased the demand for cloud inference services. By uploading their textual inputs to the cloud, users can benefit from PLMs and achieve superior performance on various\n\u2217Equal contribution. \u2020Corresponding authors.\nNLP tasks, without requiring high-performance hardware or expertise. However, the widespread adoption of PLM services also introduces a risk of privacy leakage (Jegorova et al., 2022). Users\u2019 textual inputs may contain sensitive information, including names, schedules, and even business secrets. Leaking such private information to service providers is unacceptable to most users.\nTo address users\u2019 privacy concerns and comply with relevant laws1, service providers can provide privacy-preserving inference services. Many sought to make users query PLM services with word representations rather than plaintext. But digital word representations still need additional privacy protection because recent studies (Pan et al., 2020; Song and Raghunathan, 2020) have revealed that standard word representation can be easily restored to its original word under embedding inversion attacks (H\u00f6hmann et al., 2021).\nOne solution is to apply cryptographic techniques to the PLM (Hao et al.; Li et al., 2022; Zheng et al., 2023; Chen et al., 2022), but these methods often come with significant communi-\n1https://www.consilium.europa.eu/en/policies/dataprotection/data-protection-regulation/\ncation costs and computational time, making it difficult to apply them in real-world scenarios. Another potential solution is to remove the private information in word representations (Pan et al., 2020) through adversarial training (Li et al., 2018; Coavoux et al., 2018; Plant et al., 2021) and differential privacy (Lyu et al., 2020a; Hoory et al., 2021; Yue et al., 2021). However, the private information in our scenario pertains to each word in the user\u2019s plaintext. Removing the word information from word representations is highly unnatural and often leads to crucial performance degradation (Zhou et al., 2022).\nIn this paper, we propose a novel privacypreserving inference paradigm named TextMixer. Our method is inspired by k-anonymity (Sweeney, 2002), a classic privacy-preserving technique that ensures each individual\u2019s information cannot be distinguished from at least k-1 other individuals. For that purpose, TextMixer aims to make each real word representation indistinguishable by mixing it with multiple synthetic representations during the inference phase, rather than removing word information, as shown in Figure 1.\nHowever, directly mixing the representations of k inputs leads to information interference, and normal PLMs cannot handle the mixed inputs to acquire the desired output. To address the above problems, we resort to Multi-input Multioutput (MIMO) network (Ram\u00e9 et al., 2021; Murahari et al., 2022, 2023), allows us to process mixed representations with minimal information interference and obtain multiple predictions simultaneously. We further design a privacy mixer to enhance TextMixer\u2019s privacy, which encrypts the real inputs from three dimensions. For each forward pass, the privacy mixer applies anonymous mixing, representation perturbation, and position shuffling to encrypt the real inputs. Only the user holds the correct keys to decrypt the real prediction from mixed representation. In this way, TextMixer preserves the privacy of both the input and prediction. We also conduct an investigation into different methods for generating synthetic inputs. Our contribution can be summarized as follows2:\n\u2022 We propose TextMixer, a privacy-preserving inference method that obfuscates privacy attacks by mixing private input with synthetic inputs.\n2We release our code at https://github.com/ LuLuLuyi/TextMixer\n\u2022 We propose a mixing-centric encryption method and explore several approaches for synthetic input generation.\n\u2022 Experimental results on various token and sentence classification datasets demonstrate TextMixer\u2019s effectiveness in privacy and performance."
        },
        {
            "heading": "2 Methodology",
            "text": "In this section, we present a novel privacypreserving inference framework TextMixer, which achieves k-anonymity (Sweeney, 2002) by mixing multiple inputs during the inference phase."
        },
        {
            "heading": "2.1 Overview",
            "text": "We take an example to show how k-anonymity contributes to privacy-preserving inference. To hide the private text \"John\", TextMixer mixes the word representation of \"John\" with k-1 synthetic ones (such as \"Mike\", \"Smith\", \"Johnson\", etc.). Only the mixed representation is shared with privacy attackers, making it difficult for privacy attackers to identify which word is real. As shown in Figure 2, our method consists of three steps: (1) User first generates k-1 synthetic representations (\u00a72.5) and inputs k representations into the Privacy Mixer (\u00a72.3), which uses various encryption methods with mixing as the core to combine the real and synthetic inputs, creating mixed and encrypted representations; (2) Cloud server receives the mixed representations, and inputs them into a pre-trained MIMO network (\u00a72.2), which can handle mixed representations formed by multiple inputs and output their outputs; (3) User uses a Privacy DeMixer (\u00a72.4) to decrypt the desired prediction from the multiple outputs locally. In the following subsection, we provide a detailed introduction to each part of TextMixer. We also conduct a theoretical privacy analysis of TextMixer in Appendix B."
        },
        {
            "heading": "2.2 MIMO Network",
            "text": "We first adopt the mixing process of MIMO Network (Murahari et al., 2023) (called vanilla mixing here) to our scenarios. Vanilla mixing transforms different inputs into separate spaces before mixing, thus reducing representation interference between multiple inputs.\nGiven a user\u2019s real input sequence Xr = [xr1, ...,x r n] with n word representations, and k \u2212 1 synthetic inputs {Xsi}k\u22121i=1 where Xsi =\n[xsi1 , ...,x si n ], we aims to mix these representation to generate Xmix = [xmix1 , ...,x mix n ]. For simplicity, we denote k words to be mixed at the i-th position as x1:ki = [x r i ,x s1 i , ...,x sk\u22121 i ]. To avoid misunderstanding, we use \"position\" to represent the word position in an input sequence, and \"order\" to represent the order of k inputs. For example, xji means the word representation at i-th position from j-th input. To ensure that the mixed representation still retains the individual semantics of each input, we need to perform transformation before mixing. Transformation maps word representations from different inputs to separate spaces and consequently reduces interference between their representations. The process of transformation and mixing can be defined as:\nxmixi = 1\nk k\u2211 j=1 xji \u2299 e j , (1)\nwhere xji is j-th representation in x 1:k i and e j \u2208 Rd \u223c N (0, I) is transformation vector sampled from a standard multivariate Gaussian distribution.\nNext, we input mixed representations into a MIMO network to obtain multiple predictions. Then we demix the real prediction from the multiple predictions, as shown in \u00a72.4. The structure of MIMO network is the same as BERT, its ability to handle mixed representation comes\nfrom the over-parameterized network and MIMO pre-training (Murahari et al., 2023)."
        },
        {
            "heading": "2.3 Privacy Mixer",
            "text": "However, vanilla mixing process is not a good privacy protector due to the lack of randomness. For example, equation 1 shows that different inputs will be mapped to different spaces. If the real input is always mapped to a fixed space, privacy attackers can easily identify this fixed pattern, thereby restoring the real input from the mixed representation (shown in Table 2). Thus, we design privacy mixer, which extends the vanilla mixing into a real encryption strategy from three dimensions: anonymous mixing, perturbed representation, and random position.\nMixing Encryption. Inspired by k-anonymity, this encryption aims to make each real representation xri indistinguishable from synthetic ones {xsji } k\u22121 j=1 . As mentioned before, the real input cannot be mapped to a fixed space, thus we shuffle inputs to get a random mixing order, allowing the real input to be anonymous:\nRME \u00b7 x1:ki = [x sk\u22121 i , ...,x r i ,x s1 i ], (2)\nwhere x1:ki \u2208 Rk\u00d7d and RME \u2208 {0, 1}k\u00d7k is a random permutation matrix. Equation 2 represents only one possible shuffling result. RME\nis generated by the user\u2019s side and not shared with the cloud.\nRepresentation Encryption. To obscure the representation itself, we add random perturbations to each representation of a random input, as shown in Figure 2. Following Plant et al. (2021), we adopt Laplace Noise as the random perturbation. Formally, the process of mixing encryption and representation encryption for x1:ki is:\nx Enc(1:k) i = RME \u00b7 [x r i ,x s1 i , ...,x s\u03b1 i + Lap(\u03f5), ...,x sk\u22121 i ],\n(3)\nwhere \u03f5 is a hyperparameter controlling the scale of noise, smaller values of correspond to a larger scale of noise. Lap(\u03f5) can be added to any representation, including real input xri .\nThen we mix k representation xEnc(1:k)i at each position i, according to equation 1. Finally, we obtain the mixed representations Xmix \u2208 Rn\u00d7d.\nPosition Encryption. To further enhance the privacy of Xmix, position encryption aims to reduce its human readability. We utilize a random permutation matrix, denoted as RPE \u2208 {0, 1}n\u00d7n, to shuffle word positions in Xmix:\nXmixPE = RPE \u00b7Xmix = [xmixn ,xmix1 ...,xmixn\u22121]. (4) In this way, even if privacy attackers can restore some text from mixed representation, they cannot obtain a readable sequence. We show that XmixPE and Xmix are equivalent for PLMs in Appendix D. Similar to Mixing Encryption, only the user holds the private key RPE .\nFinally, users upload encrypted representations, XmixPE , to the cloud for further computation. A pretrained MIMO network processes the XmixPE and sends back the final representation HmixPE \u2208 Rn\u00d7d to the user\u2019s side for de-mixing the real prediction."
        },
        {
            "heading": "2.4 Privacy DeMixer",
            "text": "The goal of Privacy DeMixer is to decrypt the encrypted final representations HmixPE and obtain the desired prediction, with the help of user\u2019s personal keys RPE and RME . First, we use RPE to restore original word position:\nHmix = R\u22121PE \u00b7H mix PE , (5)\nwhere Hmix = [hmix1 , ...,h mix n ]. Next, we de-mix these mixed final representations Hmix to get final representations corresponding to different inputs.\nFor simplicity, we only consider the representation in position i. De-mixing means extracting k final representations h1:ki \u2208 Rk\u00d7d from hmixi \u2208 Rd, where h1:ki are the final representations of different inputs x1:ki . The final representation of j-th input in x1:ki can be obtained by:\nhji = DeMix ( [hmixi ;d j ] ) , (6)\nwhere DeMix is a two layer MLP and dj \u2208 Rd is private vectors to de-mix the j-th representation from mixed representation and [\u2217; \u2217] means concatenation. The de-mixing ability of DeMix and dj comes from pre-training (Murahari et al., 2023). We show that DeMix and dj are not helpful in restoring the real input in \u00a74.3.\nAfter de-mixing, we restore an ordered set h1:ki = [h 1 i , ...,h k i ], then we can use RME to decrypt the original order by R\u22121ME \u00b7h1:ki , obtaining the real input\u2019s final representation Hr \u2208 Rn\u00d7d. Subsequently, Hr is fed into the classification layer to obtain the prediction desired by the user."
        },
        {
            "heading": "2.5 Synthetic Input Generating",
            "text": "As mentioned before, our method requires k-1 synthetic inputs for mixing. We now explore principled ways of generating synthetic input.\nGenerating with Real Data. The most intuitive method is to select the real training data as the synthetic data. Given a real input sentence, we randomly sample k\u2212 1 sentences from the training set and encrypt these sentences.\nGenerating with Similarity. In this approach, we leverage the similarity for synthetic input generating. Given a real word xr, we select the closest word ws in the embedding space as its synthetic word:\nws = argmaxk\u22121||w \u2212 xr||, (7)\nwhere w is the embedding matrix of PLM.\nGenerating with Input Itself. This method is inspired by Zhou et al. (2022) that fuses representation within the same sentence. Given a real input Xr = [xr1, ...,x r n], for each x r i , we random sample k \u2212 1 representations from Xr (excluding xri itself) as the synthetic inputs."
        },
        {
            "heading": "2.6 Training Details",
            "text": "TextMixer supports both token-level and sentencelevel classification tasks. During the training\nprocess, we randomly sample k inputs from the training set and only enable representation encryption. We add random noise to a random input sequence and then mix all the inputs together. The mixed input is sent to a pre-trained MIMO network to get the final representation. Without any decryption process, we directly use DeMix to obtain the final representation of each input, which is then fed into the output layer to predict the task label. In addition to the task loss, a retrieval loss is added to promote the network\u2019s ability to handle multiple inputs (Murahari et al., 2023). The final loss can be represented as follows:\nLretrieval(x1:k) = \u2211 i \u2212logP (wji |h j i ), (8)\nL =Ltask + \u03b1Lretrieval, (9)\nwhere \u03b1 is a hyperparameter, hji is final representation of the j-th sentence at position i, and wji is the word corresponding to h j i . During the inference phase, we can directly enable all encryption strategies and switch the synthetic input generation methods without retraining."
        },
        {
            "heading": "3 Experimental Setup",
            "text": ""
        },
        {
            "heading": "3.1 Datasets",
            "text": "We conduct experiments on four representative datasets. Sentence classification: we select IMDB (Maas et al., 2011) for sentiment analysis and AGNews (Zhang et al., 2015) for topic classification. Token classification: we select NER (Named Entity Recognition) as the main task, including CoNLL2003 (Tjong Kim Sang and De Meulder, 2003) and Ontonotes (Weischedel et al., 2013). These tasks are highly relevant to realworld scenarios and datasets often involve sensitive information such as names and locations. Thus they serve as suitable benchmarks for evaluating both privacy and performance. The details of these datasets are in Appendix A."
        },
        {
            "heading": "3.2 Baselines",
            "text": "For a thorough comparison, we select the widely used privacy attack and defense baselines.\nAttack Baselines. Following Zhou et al. (2022), we select three privacy attack baselines. KNN (Qu et al., 2021) selects the closest word in the embedding space as the real word. InvBert (H\u00f6hmann et al., 2021) trains an embedding inversion model that takes word representations\nas input and outputs each representation\u2019s original word one-to-one. MLC (Song and Raghunathan, 2020) is similar to InvBert, the difference is that MLC performs multi-label classification tasks, predicting all the words that have appeared without emphasizing the order.\nDefense Baselines. DPNR (Lyu et al., 2020b) uses differential privacy to obfuscate representations. CAPE (Plant et al., 2021) further eliminates privacy information by incorporating adversarial training and differential privacy. SanText+ (Yue et al., 2021) uses differential privacy and word frequency to replace sensitive words. TextFusion (Zhou et al., 2022) employs word fusion and misleading training to hinder privacy attackers from training targeted embedding inversion models. We attack it with a special InvBert due to different settings, as shown in Appendix C. DataMix (Liu et al., 2020) proposes to mix and encrypt the image with random coefficients."
        },
        {
            "heading": "3.3 Evaluation Metrics",
            "text": "We consider privacy metrics at both the word level and the sentence level. At the word level, we use the Top-k, which refers to the proportion of real words among the top k predictions generated by the attack methods for a given representation. At the sentence level, we use the RougeL (Lin, 2004), a text generation metric, to measure the similarity between the restored sentence and the original sentence. For MLC Attack, we use the TokenHit (Zhou et al., 2022), which treats the words in real input as a set, calculating the percentage of predicted words in the raw words."
        },
        {
            "heading": "3.4 Implementation Details",
            "text": "We show three important implementation details below, and present the remaining details in Appendix G. (1) All attack and defense methods are performed at the embedding layer, before any transformer modules. Users only need to convert words into embeddings and send them to the cloud. (2) The pre-trained MIMO network is based on BERT-base, thus we also chose BERT-base as the backbone to train task models for all baselines, as well as the inversion models for InvBert and MLC. (3) Inversion models for each dataset are trained in its training set."
        },
        {
            "heading": "4 Experimental Results",
            "text": "In this section, we show the results of privacy and task performance in \u00a74.1 and the ablation study in \u00a74.2. We also design three specific privacy attacks for TextMixer and show the attack results in \u00a74.3. The inference cost is shown in Appendix F."
        },
        {
            "heading": "4.1 Main Results",
            "text": "Main results on baselines and TextMixer are listed in Table 1. From the result, we can see that (1) Normal word representations without any defense method suffer from privacy leakage risk. Almost all privacy attacks achieve a 100% success rate in fine-tuning, which means that privacy is completely leaked to attackers. (2) Preserving privacy in the embedding layer is an extremely challenging task. Despite compromising significant performance, most baselines are unable to provide satisfactory privacy protection. This is due to the inherent contradiction of removing private word information from word representations, as discussed in the introduction. (3) Our proposed TextMixer achieves better task performance and\nlower privacy leakage than most baselines across all datasets. With the help of our proposed encryption strategy and MIMO network, TextMixer avoids contradictory behavior, thus achieving good performance while preserving privacy.\nPrevious works usually require deploying multiple transformer layers in users\u2019 devices to achieve satisfying performance. But when applied to the embedding layer, we observe that most baselines are difficult to provide privacy protection. CAPE and DPNR usually need to trade a lot of task performance for the ability to protect privacy, which can be attributed to the inherent contradiction that we discussed above. SanText+ relies on word replacement to protect privacy, but word replacement cannot handle token-level tasks, which require much token information, and tasks with long sequences, which require lots of words to be replaced, such as IMDB. TextFusion fuses most words to reduce Top-1, resulting in serious performance degradation. But the performance of Top-5 and MLC of TextFusion remains high, indicating privacy risk is still high under our strict\nprivacy setting. DataMix fails to perform well on most datasets, and we believe there are two reasons. First, its encryption mechanism is friendly to convolutional networks but not suitable for PLMs that involve more non-linear operations. Second, DataMix is designed for images, whereas we require encrypting a representation sequence, which is a harder task. These two difficulties cause it to fail on PLMs and textual data."
        },
        {
            "heading": "4.2 Ablation Study",
            "text": "In this subsection, we explore the effect of encryption strategy and synthetic input generation. We also show the effect of mixed inputs number and noise scale in Appendix E.\nEffect of encryption strategy. From Table 2, we can see that mixing Encryption plays an important role in privacy. Without mixing encryption, privacy attackers can easily identify the space of real input and accurately restore the original words, resulting in high privacy leakage. Representation Encryption prevents privacy attackers by adding noise to representation. Similar to the previous work, adding noise hurts task performance, but an appropriate noise would not result in much performance loss. This is acceptable as it contributes largely to privacy. Position Encryption is not designed to prevent attackers from restoring words, but it can significantly reduce readability, resulting in a substantial decrease in Rouge scores. From the above results, it can be observed that our proposed encryption techniques reduce privacy leakage risks from different perspectives while having minimal impact on performance.\nEffect of Synthetic Inputs. From Figure 3, we surprisingly find that the similarity-based methods do not confuse the inversion model but instead lead to more severe privacy leakage. We speculate\nthat these synthetic inputs differ significantly from the words that would appear in the real world. During the training process, the powerful inversion model gradually eliminates this irrelevant information and identifies the feature of the real input, resulting in a successful attack. Real Data and Input Itself achieve first and second rankings in preserving privacy. Both of these synthetic inputs are generated based on real data, which enlightens us that the closer synthetic inputs approximate real-world scenarios, the greater their potential to mislead privacy attackers. The substitution of synthetic inputs has only a marginal impact on performance, both in token-level tasks and sentence-level tasks, indicating users can replace the synthetic inputs generation method at any time without the need for retraining."
        },
        {
            "heading": "4.3 Stronger Privacy Attacks",
            "text": "Although TextMixer can defend the powerful embedding inversion attack, there are still concerns about potential privacy leakage under stronger privacy attacks. Therefore, we design some attacks for our encryption method.\nDeMixing Attack is designed for Mixing Encryption. We verify whether DeMix and dj , which are used to demix the outputs, are helpful to demix the mixed input. We assume the privacy attacker knows the exact order of the real input. For instance, they know the j-th input is the real one, thus they can use DeMix and dj to demix the mixed input and obtain the word representations of j-th input. Then\nthey use an InvBert to predict real words and train DeMix, dj and InvBert jointly. However, attack results (DMA) shown in Table 3 show that DeMix is not helpful for restoring real input, this attack performs even worse than using InvBert directly. We speculate that this is because DeMix is trained to demix the output instead of input, it strongly interferes with the inversion model, making it impossible to attack successfully at all. Position Inversion Attack is designed for Position Encryption. It aims to restore the original word position from the encrypted input. Similar to InvBert, we train a model to predict the original position of each encrypted representation and show attack results (PIA) in Table 3. The accuracy of restoring the original position is only 50%, and the readability of such a sentence remains low. Within TextMixer, it is not a threat since embedding inversion attacks can only restore a few words from encrypted representations. Cross Inversion Attack is designed for synthetic input generation, which verifies whether the InvBert trained on one synthetic input generation method works better on another one. From Table 4, we can see that all transferred InvBerts perform worse than InvBert trained and tested in the same synthetic input indicating its poor generalization ability. Privacy attacks\u2019 performance primarily depends on the synthetic input used during testing."
        },
        {
            "heading": "5 Related Work",
            "text": ""
        },
        {
            "heading": "5.1 Plaintext Privacy during Inference",
            "text": "Privacy Threats. Most PLM services require users to upload their plaintext data to the cloud to perform PLM inference, which leads to a serious risk of privacy leakage (Jegorova et al., 2022). Privacy attackers could be the service provider themselves, they aim to collect users\u2019 textual input for their own purposes. For example, they can use these data to train a better model or extract users\u2019 private information, such as personal details and confidential business information, even if it is prohibited by law. Recent literature (Song\nand Shmatikov, 2019; Pan et al., 2020) shows that even uploading word representations can still leak privacy, as the embedding inversion attack (H\u00f6hmann et al., 2021) can restore word representations to their original words.\nPrivacy-preserving Methods. To avoid users\u2019 private plaintext from leaking to privacy attackers, many existing works adopt cryptographic techniques such as Homomorphic Encryption (Gentry, 2009; Chen et al., 2022) and Secure Multiparty Computation (Evans et al., 2018; Li et al., 2022) into PLM (Hao et al.; Zheng et al., 2023). Although theoretically guaranteed, these methods typically require much higher time and communication costs than standard PLM. For example, Hao et al. shows that an encrypted BERTtiny model requires about 50 seconds and 2GB of communication cost to process a 30 words sentence. An alternative that does not require additional inference costs is to employ adversarial training (Li et al., 2018; Coavoux et al., 2018; Plant et al., 2021) and differential privacy (Lyu et al., 2020a; Hoory et al., 2021; Yue et al., 2021) to remove private information in representations. However, the perturbed word representations, which lack sufficient information for privacy attackers to identify their original words, are also insufficient for the model to achieve high performance on downstream tasks (Zhou et al., 2022). Therefore, in this work, we propose mixing inputs rather than removing word information to preserve plaintext privacy during the inference phase.\nComparison with Similar Works. Huang et al. (2020) propose TextHide, which mixes up inputs during the training phase to protect the privacy of training data. Our work focuses on users\u2019 plaintext privacy and mixes inputs during the inference phase, which brings new challenges. Liu et al. (2020) propose Datamix to mix multiple inputs during inference and encrypt them with random coefficients. Our encryption method is different from Datamix, and Datamix is designed for image data and convolutional networks, which not performs well in text data and PLM. Recently, Zhou et al. (2022) propose TextFusion, which fuses parts of word representations within the same sentence to prevent privacy attackers from training a targeted inversion model. Differently, our method mixes representations from different sentences and our privacy setting is more challenging. We discuss\nit in Appendix C."
        },
        {
            "heading": "5.2 Multi-input Multi-output Network",
            "text": "Multi-input Multi-output network aims to use one neural network to process multiple inputs and obtain their predictions in one forward pass. Havasi et al. (2020) and Ram\u00e9 et al. (2021) try to improve robustness by ensembling multiple predictions in a MIMO convolutional network. Murahari et al. (2022) propose a transformer-based MIMO model for efficient inference and further improve its performance through pre-training (Murahari et al., 2023). Our work does not focus on robustness or efficiency but rather utilizes the MIMO network as a tool to construct partial encryption strategies, thereby achieving privacy-preserving inference."
        },
        {
            "heading": "6 Conclusion",
            "text": "In this paper, we propose a novel privacypreserving inference framework TextMixer. Our framework is inspired by k-anonymity and makes real input indistinguishable by mixing multiple inputs during the inference phase. Specifically, TextMixer adopts a pre-trained MIMO network, which can process representation composed of multiple inputs and obtain multiple outputs simultaneously, to handle mixed representations. We further propose the privacy mixer to encrypt not only the mixing but also the two dimensions of representation and word position. We also explore various ways to automatically generate synthetic inputs. At the embedding layer, TextMixer outperforms the existing privacy-preserving methods by a large margin, regardless of token and sentence classification tasks. The detailed ablation and analysis experiments also show TextMixer is an effective privacy-preserving inference framework.\nLimitations\nWe have summarized three limitations of TextMixer. (1) TextMixer is designed to protect plaintext privacy, and we have only conducted experiments to evaluate its privacy under embedding inversion attacks. However, TextMixer provides the k-anonymity by mixing multiple inputs, it has the potential to address other privacy concerns, such as resisting Attribute Inference Attacks. (2) TextMixer\u2019s privacy guarantee is based on k-anonymity, which is not as rigorous as homomorphic encryption. Although TextMixer\u2019s\nprivacy has been empirically verified through the powerful and widely used embedding inversion attack, and we have provided privacy proof in Appendix B. (3) TextMixer has only been evaluated at the BERT-like model and has not been migrated to larger models like GPT-3. Limited by computational resources, we can only use the pre-trained MIMO network provided by Murahari et al. (2023). However, we believe that service providers can train more powerful MIMO networks with more parameters and extend them to more tasks, such as text generation."
        },
        {
            "heading": "Acknowledgements",
            "text": "The authors wish to thank the anonymous reviewers for their helpful comments. This work was partially funded by National Natural Science Foundation of China (No.62206057,62076069,61976056), Shanghai Rising-Star Program (23QA1400200), Shanghai Academic Research Leader Program 22XD1401100, and Natural Science Foundation of Shanghai (23ZR1403500)."
        },
        {
            "heading": "A Datasets",
            "text": "we conduct experiments on 4 classification tasks and the statistics of datasets in our experiments are shown in Table 5. For all the datasets, we use the test data for evaluation. We illustrate the details of each dataset as follows:\nIMDB (Maas et al., 2011) contains a large collection of movie reviews along with their corresponding sentiment labels, indicating whether the review is positive or negative.\nAGNEWS (Zhang et al., 2015) provides a balanced distribution of articles across the World, Sports, Business, and Science/Technology, making it suitable for training and evaluating text classification models.\nCoNLL2003 (Tjong Kim Sang and De Meulder, 2003) is a popular benchmark dataset for named entity recognition (NER) tasks. It consists of English and German news articles annotated with named entity labels such as person names, locations, organizations, and miscellaneous entities.\nOntoNotes5 (Weischedel et al., 2013) contains text from various domains, including news, conversations, and web data, and is annotated with detailed named entity labels such as person names, locations, organizations, and more.\nDataset # Train #Test #Labels #Average Length"
        },
        {
            "heading": "B Privacy Analysis",
            "text": "Given a real input X = [x1,x2, ...,xn] with length n and its synthetic inputs {Xi}ki=1.\nMixing Encryption Mixing encryption mixes k word representations based on Eq. 1 and Eq. 2. There are a total of k possible word representations (1 real + k1 synthetic). All representations are assumed to be equally likely. Besides that, The mixed representation is more confusing than the isolated representation, which further reduces the success rate of attacks. Hence, the probability of correctly guessing the real input is less than the reciprocal of the total number of inputs, P(Mix_Attack) \u2264 1k .\nRepresentation Encryption Representation encryption adds random noise to a random representation before mixing. \"This encryption method draws on differential privacy, which increases the level of confusion in the representation. Although it is difficult to analyze the theoretical privacy protection capability, we provide empirical results on the impact of different noise scales on privacy and performance in Appendix E.\nPosition Encryption Given a shuffled sentence of length n, there are n! (n factorial) ways to arrange n items. All arrangements are assumed to be equally likely. Hence, the probability of correctly guessing the original order should be the reciprocal of the total number of arrangements: P(Pos_Attack) = 1 n! . Although attackers can exploit rules such as grammar and positional information to eliminate and increase the success rate of attacks, there would still be many confusing candidates left. We also designed privacy attacks in \u00a74.2 to verify this."
        },
        {
            "heading": "C Discussion about TextFusion",
            "text": "TextFusion (Zhou et al., 2022) aims to fuse token representations and prevent privacy attackers from collecting ideal data and training a one-toone inversion model. In their privacy setting, privacy attackers come primarily from third parties. However, our setting is stricter than TextFusion, service providers can also be potential privacy attackers, they release the model weights, can design special attack methods, and collect any training data if they want. We propose a simple attack method and use it in our experiments. For token-level tasks, we train an inversion model that only restores unfused tokens and directly uses this model to attack. For sentence-level tasks, we train an inversion to restore one original word from fused representations. As shown in Table 1, the success rates of both attack methods are considerably high, indicating that our scenario necessitates stronger methods for privacy protection."
        },
        {
            "heading": "D Discussion about Position Encryption",
            "text": "In \u00a72.3, we mention that XmixPE and X mix are equivalent for PLM, which means that even if the word positions are shuffled, it does not affect the subsequent model inference. Here we provide a brief explanation, we use a Bert-like model, and no relative position embedding is used. As a\nresult, the position information is only provided by position embedding, which is added to each input before mixing. During the inference phase, the subsequent calculations (self-attention, softmax, feed-forward layer, etc.) are independent of the explicit representation position and only depend on the implicit positional features within the representation. As a result, we can shuffle the mixed representations without any impact on performance. However, the shuffled word positions can reduce the readability for humans, which confuses privacy attackers and enhances privacy."
        },
        {
            "heading": "E Effect of Mixed Inputs Number and Noise Scale",
            "text": "In this section, we show the utility-privacy trade-off caused by the number of mixed inputs and noise scale. We conducted experiments on CoNLL03, and Table 6 shows the experimental results for mixed inputs numbers of [2, 5, 10] and noise parameters \u03f5 of [1, 0.5, 0.2]. lower \u03f5 means larger noise. It can be observed that increasing the noise scale appropriately can significantly prevent privacy leakage (64.1 -> 32.24 for N=2) with an acceptable decrease in performance (89.5 - > 88.22 for N=2). In addition to this, we can observe that incorporating mixed inputs results in a more favorable balance between utility and privacy, underscoring the significance of mixing."
        },
        {
            "heading": "F Comparison of Inference Cost",
            "text": "To show the efficiency of TextMixer, we compare the inference cost of different types of privacypreserving methods, including Fine-tuning, encryption-based methods, TextFusion and TextMixer. The communication cost is how much data the users send. Because inference time can vary depending on different platforms and implementations, we use both inference time and FLOPs (floating-point operations) as two metrics to measure computational costs. We use a sentence of length 128 to query a bert-base model, all experiments are conducted on the same CPU unless otherwise specified.\nFrom Table 7, we can find that TextMixer only introduces a few additional computation and communication costs compared to fine-tuning. The fast encryption-based method, PUMA, still requires 33.91 seconds and 10.77GB communication cost, which is not convenient in real applications.\nG Implementation Details\nThe pre-trained MIMO network is released by (Murahari et al., 2023)3, BERT-base are used as pretrained MIMO network to implement TextMixer. To ensure consistency, all baseline methods are implemented using the BERT-base model. All the experiments are conducted on NVIDIA GeForce RTX 3090. Implementation details and the HyperParameters of both attack and defense methods are introduced as follows.\nDefence Methods Our implementation is based on the Hugging Face Transformer models 4 and is replication with publicly available code. At training time, we use the AdamW optimizer\n3https://github.com/princeton-nlp/datamux-pretraining/ 4https://huggingface.co/\n(Loshchilov and Hutter, 2019) and a linear learning rate scheduler, as suggested by the Hugging Face default setup. For all defense methods, we choose Laplace noise to disturb the representation and train 30 epochs to guarantee convergence. We prioritize privacy while considering performance when selecting the most favorable outcomes from the experimental results for reporting. The optimized hyperparameters, which yielded the best results, are presented in Table 8.\nFor TextMixer, the hyperparameters we tune include the number of mixing instances and the scale of Laplace noise of Representation Encryption. We train TextMixer with the number of instances N in [2, 5, 10] and the scale of Laplace noise \u03f5 in [1, 2, 2.5, 5, 6, 7, 8, 10]. For all datasets, we use Real Data as the synthetic inputs due to the better privacy-performance trade-off.\nFor DPNR5, we take the scale of Laplace noise and the nullification rate for word dropout strategy as hyperparameters, we search the scale of the noise \u03f5 in [0.2, 1, 2, 10, 20] and the nullification rate nu in [0, 0.1, 0.3, 0.5].\nFor CAPE6, during the progress of adversarial training weight, the coefficient \u03bbadv for the adversarial training loss is searched within the range of [0.01, 0.05, 0.1, 0.5, 1, 5] and the scale of the noise \u03f5 in [0.2, 1, 2, 10, 20].\nFor Santext+7, our approach aligns with the author\u2019s methodology, utilizing GloVe (Pennington et al., 2014) for word substitution guidance. By default, the probability of non-sensitive words being replaced is set at 0.3 (denoted as p), while the percentage of sensitive words to be sanitized is set at 0.9 (denoted as w). We conduct a search for privacy parameter \u03b1 within the range of [1, 2, 3].\n5https://github.com/xlhex/dpnlp 6https://github.com/NapierNLP/CAPE 7https://github.com/xiangyue9607/SanText\nFor DataMix, we implement the method ourselves based on the original paper, we search the number of mixing instances in the range of [2, 4, 8].\nFor TextFusion, we focus on adjusting the misleading weight during the fusion process. we vary the misleading weight \u03bbml within the range of [0.05, 0.5, 1, 5, 10, 15].To enhance privacy on sentence-level tasks, we increase the weight of the misleading loss term.\nAttack Methods To carry out the KNN Attack in our implementation, we utilized the embedding matrix of the BERT-base model. This embedding matrix was employed to compute the Euclidean distance between the client representations. In the case of InvBert Attack and MLC Attack, we employ the BERT-base model to construct the inversion model and train 20 epochs to guarantee convergence. We also perform a search for the optimal learning rate within the range of [1e-4, 1e5, 2e-5, 1e-6]. For baselines, we use a learning rate of 2e-5 to train the inversion model. For TextMixer, the best learning rate we tuned is 1e-4 in most cases."
        }
    ],
    "title": "TextMixer: Mixing Multiple Inputs for Privacy-Preserving Inference",
    "year": 2023
}