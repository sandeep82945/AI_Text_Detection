{
    "abstractText": "Current variational dialog models have employed pre-trained language models (PLMs) to parameterize the likelihood and posterior distributions. However, the Gaussian assumption made on the prior distribution is incompatible with these distributions, thus restricting the diversity of generated responses. These models also suffer from posterior collapse, i.e., the decoder tends to ignore latent variables and directly access information captured in the encoder through the cross-attention mechanism. In this work, we propose Dior-CVAE, a hierarchical conditional variational autoencoder (CVAE) with diffusion priors to address these challenges. We employ a diffusion model to increase the complexity of the prior distribution and its compatibility with the distributions produced by a PLM. Also, we propose memory dropout to the cross-attention mechanism, which actively encourages the use of latent variables for response generation. Our method requires parameters that are comparable to those of previous studies while maintaining comparable inference time, despite the integration of the diffusion model. Overall, experiments across two commonly used open-domain dialog datasets show that our method can generate more diverse responses even without largescale dialog pre-training. Code is available at https://github.com/UKPLab/dior-cvae.",
    "authors": [
        {
            "affiliations": [],
            "name": "Tianyu Yang"
        },
        {
            "affiliations": [],
            "name": "Thy Thy Tran"
        },
        {
            "affiliations": [],
            "name": "Iryna Gurevych"
        }
    ],
    "id": "SP:b0ccb4db44de5405284246f4218f5445cba07a9d",
    "references": [
        {
            "authors": [
                "Jacob Austin",
                "Daniel D. Johnson",
                "Jonathan Ho",
                "Daniel Tarlow",
                "Rianne van den Berg."
            ],
            "title": "Structured denoising diffusion models in discrete state-spaces",
            "venue": "Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information",
            "year": 2021
        },
        {
            "authors": [
                "Hareesh Bahuleyan",
                "Lili Mou",
                "Olga Vechtomova",
                "Pascal Poupart."
            ],
            "title": "Variational attention for sequence-to-sequence models",
            "venue": "Proceedings of the 27th International Conference on Computational Linguistics, pages 1672\u20131682, Santa Fe, New Mexico,",
            "year": 2018
        },
        {
            "authors": [
                "Siqi Bao",
                "Huang He",
                "Fan Wang",
                "Hua Wu",
                "Haifeng Wang."
            ],
            "title": "PLATO: Pre-trained dialogue generation model with discrete latent variable",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 85\u201396, Online.",
            "year": 2020
        },
        {
            "authors": [
                "Samuel R. Bowman",
                "Luke Vilnis",
                "Oriol Vinyals",
                "Andrew Dai",
                "Rafal Jozefowicz",
                "Samy Bengio."
            ],
            "title": "Generating sentences from a continuous space",
            "venue": "Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning, pages 10\u201321,",
            "year": 2016
        },
        {
            "authors": [
                "Zefeng Cai",
                "Zerui Cai."
            ],
            "title": "Pcvae: Generating prior context for dialogue response generation",
            "venue": "Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI-22, pages 4065\u20134071. International Joint Conferences on",
            "year": 2022
        },
        {
            "authors": [
                "Chaotao Chen",
                "Jinhua Peng",
                "Fan Wang",
                "Jun Xu",
                "Hua Wu."
            ],
            "title": "Generating multiple diverse responses with multi-mapping and posterior mapping selection",
            "venue": "Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence,",
            "year": 2019
        },
        {
            "authors": [
                "Wei Chen",
                "Yeyun Gong",
                "Song Wang",
                "Bolun Yao",
                "Weizhen Qi",
                "Zhongyu Wei",
                "Xiaowu Hu",
                "Bartuer Zhou",
                "Yi Mao",
                "Weizhu Chen",
                "Biao Cheng",
                "Nan Duan"
            ],
            "title": "DialogVED: A pre-trained latent variable encoder-decoder model for dialog response",
            "year": 2022
        },
        {
            "authors": [
                "Rewon Child."
            ],
            "title": "Very deep vaes generalize autoregressive models and can outperform them on images",
            "venue": "9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net.",
            "year": 2021
        },
        {
            "authors": [
                "Meier-Hellstern",
                "Douglas Eck",
                "Jeff Dean",
                "Slav Petrov",
                "Noah Fiedel."
            ],
            "title": "Palm: Scaling language modeling with pathways",
            "venue": "Journal of Machine Learning Research, 24(240):1\u2013113.",
            "year": 2023
        },
        {
            "authors": [
                "Rich\u00e1rd Cs\u00e1ky",
                "Patrik Purgai",
                "G\u00e1bor Recski."
            ],
            "title": "Improving neural conversational models with entropy-based data filtering",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5650\u20135669, Florence, Italy.",
            "year": 2019
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
            "year": 2019
        },
        {
            "authors": [
                "Angela Fan",
                "Mike Lewis",
                "Yann Dauphin."
            ],
            "title": "Hierarchical neural story generation",
            "venue": "Proceedings",
            "year": 2018
        },
        {
            "authors": [
                "Le Fang",
                "Chunyuan Li",
                "Jianfeng Gao",
                "Wen Dong",
                "Changyou Chen."
            ],
            "title": "Implicit deep latent variable models for text generation",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International",
            "year": 2019
        },
        {
            "authors": [
                "Hao Fu",
                "Chunyuan Li",
                "Xiaodong Liu",
                "Jianfeng Gao",
                "Asli Celikyilmaz",
                "Lawrence Carin."
            ],
            "title": "Cyclical annealing schedule: A simple approach to mitigating KL vanishing",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the",
            "year": 2019
        },
        {
            "authors": [
                "Jun Gao",
                "Wei Bi",
                "Xiaojiang Liu",
                "Junhui Li",
                "Guodong Zhou",
                "Shuming Shi."
            ],
            "title": "A discrete CVAE for response generation on short-text conversation",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the",
            "year": 2019
        },
        {
            "authors": [
                "Sergey Golovanov",
                "Rauf Kurbanov",
                "Sergey Nikolenko",
                "Kyryl Truskovskyi",
                "Alexander Tselousov",
                "Thomas Wolf."
            ],
            "title": "Large-scale transfer learning for natural language generation",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Com-",
            "year": 2019
        },
        {
            "authors": [
                "Shansan Gong",
                "Mukai Li",
                "Jiangtao Feng",
                "Zhiyong Wu",
                "Lingpeng Kong."
            ],
            "title": "Diffuseq: Sequence to sequence text generation with diffusion models",
            "venue": "The Eleventh International Conference on Learning Representations.",
            "year": 2023
        },
        {
            "authors": [
                "Ian Goodfellow",
                "Jean Pouget-Abadie",
                "Mehdi Mirza",
                "Bing Xu",
                "David Warde-Farley",
                "Sherjil Ozair",
                "Aaron Courville",
                "Yoshua Bengio."
            ],
            "title": "Generative adversarial networks",
            "venue": "Communications of the ACM, 63(11):139\u2013144.",
            "year": 2020
        },
        {
            "authors": [
                "Xiaodong Gu",
                "Kyunghyun Cho",
                "Jung-Woo Ha",
                "Sunghun Kim."
            ],
            "title": "Dialogwae: Multimodal response generation with conditional wasserstein autoencoder",
            "venue": "7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA,",
            "year": 2019
        },
        {
            "authors": [
                "Seungju Han",
                "Beomsu Kim",
                "Buru Chang."
            ],
            "title": "Measuring and improving semantic diversity of dialogue generation",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2022, pages",
            "year": 2022
        },
        {
            "authors": [
                "Jonathan Ho",
                "Ajay Jain",
                "Pieter Abbeel."
            ],
            "title": "Denoising diffusion probabilistic models",
            "venue": "Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020,",
            "year": 2020
        },
        {
            "authors": [
                "Jonathan Ho",
                "Tim Salimans."
            ],
            "title": "Classifier-free diffusion guidance",
            "venue": "NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications.",
            "year": 2021
        },
        {
            "authors": [
                "Ari Holtzman",
                "Jan Buys",
                "Li Du",
                "Maxwell Forbes",
                "Yejin Choi."
            ],
            "title": "The curious case of neural text degeneration",
            "venue": "8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net.",
            "year": 2020
        },
        {
            "authors": [
                "Emiel Hoogeboom",
                "Alexey A. Gritsenko",
                "Jasmijn Bastings",
                "Ben Poole",
                "Rianne van den Berg",
                "Tim Salimans."
            ],
            "title": "Autoregressive diffusion models",
            "venue": "The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29,",
            "year": 2022
        },
        {
            "authors": [
                "Emiel Hoogeboom",
                "Didrik Nielsen",
                "Priyank Jaini",
                "Patrick Forr\u00e9",
                "Max Welling."
            ],
            "title": "Argmax flows and multinomial diffusion: Learning categorical distributions",
            "venue": "Advances in Neural Information Processing Systems 34: Annual Conference on Neural",
            "year": 2021
        },
        {
            "authors": [
                "Edward J Hu",
                "yelong shen",
                "Phillip Wallis",
                "Zeyuan AllenZhu",
                "Yuanzhi Li",
                "Shean Wang",
                "Lu Wang",
                "Weizhu Chen"
            ],
            "title": "2022a. LoRA: Low-rank adaptation of large language models",
            "venue": "In International Conference on Learning Representations",
            "year": 2022
        },
        {
            "authors": [
                "Jinyi Hu",
                "Xiaoyuan Yi",
                "Wenhao Li",
                "Maosong Sun",
                "Xing Xie."
            ],
            "title": "Fuse it more deeply! a variational transformer with layer-wise latent variable inference for text generation",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Asso-",
            "year": 2022
        },
        {
            "authors": [
                "Mohit Iyyer",
                "Varun Manjunatha",
                "Jordan Boyd-Graber",
                "Hal Daum\u00e9 III."
            ],
            "title": "Deep unordered composition rivals syntactic methods for text classification",
            "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th",
            "year": 2015
        },
        {
            "authors": [
                "Valentin Khrulkov",
                "Gleb V. Ryzhakov",
                "Andrei Chertkov",
                "Ivan V. Oseledets."
            ],
            "title": "Understanding DDPM latent codes through optimal transport",
            "venue": "The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5,",
            "year": 2023
        },
        {
            "authors": [
                "Diederik Kingma",
                "Tim Salimans",
                "Ben Poole",
                "Jonathan Ho."
            ],
            "title": "Variational diffusion models",
            "venue": "Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December",
            "year": 2021
        },
        {
            "authors": [
                "Diederik P. Kingma",
                "Jimmy Ba."
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings.",
            "year": 2015
        },
        {
            "authors": [
                "Diederik P. Kingma",
                "Max Welling."
            ],
            "title": "Autoencoding variational bayes",
            "venue": "2nd International Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings.",
            "year": 2014
        },
        {
            "authors": [
                "Guillaume Klein",
                "Yoon Kim",
                "Yuntian Deng",
                "Jean Senellart",
                "Alexander Rush."
            ],
            "title": "OpenNMT: Opensource toolkit for neural machine translation",
            "venue": "Proceedings of ACL 2017, System Demonstrations, pages 67\u201372, Vancouver, Canada. Association for Compu-",
            "year": 2017
        },
        {
            "authors": [
                "Alexej Klushyn",
                "Nutan Chen",
                "Richard Kurle",
                "Botond Cseke",
                "Patrick van der Smagt."
            ],
            "title": "Learning hierarchical priors in vaes",
            "venue": "Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019,",
            "year": 2019
        },
        {
            "authors": [
                "Mike Lewis",
                "Yinhan Liu",
                "Naman Goyal",
                "Marjan Ghazvininejad",
                "Abdelrahman Mohamed",
                "Omer Levy",
                "Veselin Stoyanov",
                "Luke Zettlemoyer"
            ],
            "title": "BART: Denoising sequence-to-sequence pre-training for natural language",
            "year": 2020
        },
        {
            "authors": [
                "Bohan Li",
                "Junxian He",
                "Graham Neubig",
                "Taylor BergKirkpatrick",
                "Yiming Yang."
            ],
            "title": "A surprisingly effective fix for deep latent variable modeling of text",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the",
            "year": 2019
        },
        {
            "authors": [
                "Chunyuan Li",
                "Xiang Gao",
                "Yuan Li",
                "Baolin Peng",
                "Xiujun Li",
                "Yizhe Zhang",
                "Jianfeng Gao."
            ],
            "title": "Optimus: Organizing sentences via pre-trained modeling of a latent space",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Process-",
            "year": 2020
        },
        {
            "authors": [
                "Jiwei Li",
                "Michel Galley",
                "Chris Brockett",
                "Jianfeng Gao",
                "Bill Dolan."
            ],
            "title": "A diversity-promoting objective function for neural conversation models",
            "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computa-",
            "year": 2016
        },
        {
            "authors": [
                "Xiang Li",
                "John Thickstun",
                "Ishaan Gulrajani",
                "Percy S Liang",
                "Tatsunori B Hashimoto."
            ],
            "title": "Diffusionlm improves controllable text generation",
            "venue": "Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Process-",
            "year": 2022
        },
        {
            "authors": [
                "Yanran Li",
                "Hui Su",
                "Xiaoyu Shen",
                "Wenjie Li",
                "Ziqiang Cao",
                "Shuzi Niu."
            ],
            "title": "DailyDialog: A manually labelled multi-turn dialogue dataset",
            "venue": "Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers),",
            "year": 2017
        },
        {
            "authors": [
                "Chia-Wei Liu",
                "Ryan Lowe",
                "Iulian Serban",
                "Mike Noseworthy",
                "Laurent Charlin",
                "Joelle Pineau."
            ],
            "title": "How NOT to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation",
            "venue": "Proceedings of",
            "year": 2016
        },
        {
            "authors": [
                "Guangyi Liu",
                "Zeyu Feng",
                "Yuan Gao",
                "Zichao Yang",
                "Xiaodan Liang",
                "Junwei Bao",
                "Xiaodong He",
                "Shuguang Cui",
                "Zhen Li",
                "Zhiting Hu."
            ],
            "title": "Composable text controls in latent space with odes",
            "venue": "ArXiv preprint, abs/2208.00638.",
            "year": 2022
        },
        {
            "authors": [
                "Justin Lovelace",
                "Varsha Kishore",
                "Chao Wan",
                "Eliot Shekhtman",
                "Kilian Weinberger."
            ],
            "title": "Latent diffusion for language generation",
            "venue": "ArXiv preprint, abs/2212.09462.",
            "year": 2022
        },
        {
            "authors": [
                "Tien-Ching Luo",
                "Jen-Tzung Chien."
            ],
            "title": "Variational dialogue generation with normalizing flows",
            "venue": "ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 7778\u20137782. IEEE.",
            "year": 2021
        },
        {
            "authors": [
                "Anastasia Malashina."
            ],
            "title": "Entropy analysis of ngrams and estimation of the number of meaningful language texts",
            "venue": "cyber security applications. In Proceedings of the Entropy 2021: The Scientific Tool of the 21st Century, Basel, Switzerland. MDPI.",
            "year": 2021
        },
        {
            "authors": [
                "Shikib Mehri",
                "Maxine Eskenazi"
            ],
            "title": "Unsupervised evaluation of interactive dialog with DialoGPT",
            "venue": "In Proceedings of the 21th Annual Meeting of the Special Interest Group on Discourse and Dialogue,",
            "year": 2020
        },
        {
            "authors": [
                "Djordje Miladinovic",
                "Kumar Shridhar",
                "Kushal Jain",
                "Max Paulus",
                "Joachim M Buhmann",
                "Carl Allen."
            ],
            "title": "Learning to drop out: An adversarial approach to training sequence vaes",
            "venue": "Advances in Neural Information Processing Systems 35: Annual Confer-",
            "year": 2022
        },
        {
            "authors": [
                "Swaroop Mishra",
                "Daniel Khashabi",
                "Chitta Baral",
                "Hannaneh Hajishirzi."
            ],
            "title": "Cross-task generalization via natural language crowdsourcing instructions",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume",
            "year": 2022
        },
        {
            "authors": [
                "Alexander Quinn Nichol",
                "Prafulla Dhariwal."
            ],
            "title": "Improved denoising diffusion probabilistic models",
            "venue": "Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine",
            "year": 2021
        },
        {
            "authors": [
                "Kishore Papineni",
                "Salim Roukos",
                "Todd Ward",
                "WeiJing Zhu."
            ],
            "title": "Bleu: a method for automatic evaluation of machine translation",
            "venue": "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311\u2013318, Philadelphia,",
            "year": 2002
        },
        {
            "authors": [
                "Weizhen Qi",
                "Yu Yan",
                "Yeyun Gong",
                "Dayiheng Liu",
                "Nan Duan",
                "Jiusheng Chen",
                "Ruofei Zhang",
                "Ming Zhou."
            ],
            "title": "ProphetNet: Predicting future n-gram for sequence-to-SequencePre-training",
            "venue": "Findings of the Association for Computational Linguistics:",
            "year": 2020
        },
        {
            "authors": [
                "Danilo Jimenez Rezende",
                "Shakir Mohamed."
            ],
            "title": "Variational inference with normalizing flows",
            "venue": "Proceedings of the 32nd International Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015, volume 37 of JMLR Workshop and Con-",
            "year": 2015
        },
        {
            "authors": [
                "Ananya B. Sai",
                "Akash Kumar Mohankumar",
                "Siddhartha Arora",
                "Mitesh M. Khapra."
            ],
            "title": "Improving dialog evaluation with a multi-reference adversarial dataset and large scale pretraining",
            "venue": "Transactions of the Association for Computational Linguistics, 8:810\u2013",
            "year": 2020
        },
        {
            "authors": [
                "Stanislau Semeniuta",
                "Aliaksei Severyn",
                "Erhardt Barth."
            ],
            "title": "A hybrid convolutional variational autoencoder for text generation",
            "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural",
            "year": 2017
        },
        {
            "authors": [
                "Iulian Vlad Serban",
                "Alexander G. Ororbia",
                "Joelle Pineau",
                "Aaron Courville."
            ],
            "title": "Piecewise latent variables for neural variational text processing",
            "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 422\u2013432,",
            "year": 2017
        },
        {
            "authors": [
                "Iulian Vlad Serban",
                "Alessandro Sordoni",
                "Ryan Lowe",
                "Laurent Charlin",
                "Joelle Pineau",
                "Aaron C. Courville",
                "Yoshua Bengio."
            ],
            "title": "A hierarchical latent variable encoder-decoder model for generating dialogues",
            "venue": "Proceedings of the Thirty-First AAAI Conference",
            "year": 2017
        },
        {
            "authors": [
                "Xiaoyu Shen",
                "Hui Su",
                "Yanran Li",
                "Wenjie Li",
                "Shuzi Niu",
                "Yang Zhao",
                "Akiko Aizawa",
                "Guoping Long."
            ],
            "title": "A conditional variational framework for dialog generation",
            "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Vol-",
            "year": 2017
        },
        {
            "authors": [
                "Kihyuk Sohn",
                "Honglak Lee",
                "Xinchen Yan."
            ],
            "title": "Learning structured output representation using deep conditional generative models",
            "venue": "Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Sys-",
            "year": 2015
        },
        {
            "authors": [
                "Casper Kaae S\u00f8nderby",
                "Tapani Raiko",
                "Lars Maal\u00f8e",
                "S\u00f8ren Kaae S\u00f8nderby",
                "Ole Winther."
            ],
            "title": "Ladder variational autoencoders",
            "venue": "Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems",
            "year": 2016
        },
        {
            "authors": [
                "Nitish Srivastava",
                "Geoffrey Hinton",
                "Alex Krizhevsky",
                "Ilya Sutskever",
                "Ruslan Salakhutdinov."
            ],
            "title": "Dropout: A simple way to prevent neural networks from overfitting",
            "venue": "Journal of Machine Learning Research, 15(56):1929\u20131958.",
            "year": 2014
        },
        {
            "authors": [
                "Robin Strudel",
                "Corentin Tallec",
                "Florent Altch\u00e9",
                "Yilun Du",
                "Yaroslav Ganin",
                "Arthur Mensch",
                "Will Grathwohl",
                "Nikolay Savinov",
                "Sander Dieleman",
                "Laurent Sifre"
            ],
            "title": "Self-conditioned embedding diffusion for text generation",
            "venue": "ArXiv preprint,",
            "year": 2022
        },
        {
            "authors": [
                "Bin Sun",
                "Shaoxiong Feng",
                "Yiwei Li",
                "Jiamou Liu",
                "Kan Li."
            ],
            "title": "Generating relevant and coherent dialogue responses using self-separated conditional variational AutoEncoders",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational",
            "year": 2021
        },
        {
            "authors": [
                "Bin Sun",
                "Yitong Li",
                "Fei Mi",
                "Weichao Wang",
                "Yiwei Li",
                "Kan Li"
            ],
            "title": "Towards diverse, relevant and coherent open-domain dialogue generation via hybrid latent variables",
            "venue": "In Thirty-Seventh AAAI Conference on Artificial Intelligence,",
            "year": 2023
        },
        {
            "authors": [
                "Tianxiang Sun",
                "Junliang He",
                "Xipeng Qiu",
                "Xuanjing Huang."
            ],
            "title": "BERTScore is unfair: On social bias in language model-based metrics for text generation",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2022
        },
        {
            "authors": [
                "Tianyi Tang",
                "Junyi Li",
                "Wayne Xin Zhao",
                "Ji-Rong Wen."
            ],
            "title": "MVP: Multi-task supervised pre-training for natural language generation",
            "venue": "Findings of the Association for Computational Linguistics: ACL 2023, pages 8758\u20138794, Toronto, Canada. Associa-",
            "year": 2023
        },
        {
            "authors": [
                "Hugo Touvron",
                "Louis Martin",
                "Kevin Stone",
                "Peter Albert",
                "Amjad Almahairi",
                "Yasmine Babaei",
                "Nikolay Bashlykov",
                "Soumya Batra",
                "Prajjwal Bhargava",
                "Shruti Bhosale"
            ],
            "title": "Llama 2: Open foundation and fine-tuned chat models",
            "year": 2023
        },
        {
            "authors": [
                "Arash Vahdat",
                "Jan Kautz."
            ],
            "title": "Nvae: A deep hierarchical variational autoencoder",
            "venue": "Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020,",
            "year": 2020
        },
        {
            "authors": [
                "Arash Vahdat",
                "Karsten Kreis",
                "Jan Kautz."
            ],
            "title": "Score-based generative modeling in latent space",
            "venue": "Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-",
            "year": 2021
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N. Gomez",
                "Lukasz Kaiser",
                "Illia Polosukhin."
            ],
            "title": "Attention is all you need",
            "venue": "Advances in Neural Information Processing Systems 30: Annual Conference on Neural",
            "year": 2017
        },
        {
            "authors": [
                "Elena Voita",
                "David Talbot",
                "Fedor Moiseev",
                "Rico Sennrich",
                "Ivan Titov."
            ],
            "title": "Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computa-",
            "year": 2019
        },
        {
            "authors": [
                "Jiannan Xiang",
                "Yahui Liu",
                "Deng Cai",
                "Huayang Li",
                "Defu Lian",
                "Lemao Liu"
            ],
            "title": "Assessing dialogue",
            "year": 2021
        },
        {
            "authors": [
                "Zichao Yang",
                "Diyi Yang",
                "Chris Dyer",
                "Xiaodong He",
                "Alex Smola",
                "Eduard Hovy."
            ],
            "title": "Hierarchical attention networks for document classification",
            "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computa-",
            "year": 2016
        },
        {
            "authors": [
                "Peiyu Yu",
                "Sirui Xie",
                "Xiaojian Ma",
                "Baoxiong Jia",
                "Bo Pang",
                "Ruiqi Gao",
                "Yixin Zhu",
                "Song-Chun Zhu",
                "Ying Nian Wu."
            ],
            "title": "Latent diffusion energy-based model for interpretable text modelling",
            "venue": "Proceedings of the 39th International Conference on Machine",
            "year": 2022
        },
        {
            "authors": [
                "Saizheng Zhang",
                "Emily Dinan",
                "Jack Urbanek",
                "Arthur Szlam",
                "Douwe Kiela",
                "Jason Weston"
            ],
            "title": "Personalizing dialogue agents: I have a dog, do you have pets too",
            "venue": "In Proceedings of the 56th Annual Meeting of the Association for Computational Lin-",
            "year": 2018
        },
        {
            "authors": [
                "Tiancheng Zhao",
                "Kyusong Lee",
                "Maxine Eskenazi."
            ],
            "title": "Unsupervised discrete sentence representation learning for interpretable neural dialog generation",
            "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume",
            "year": 2018
        },
        {
            "authors": [
                "Tiancheng Zhao",
                "Ran Zhao",
                "Maxine Eskenazi."
            ],
            "title": "Learning discourse-level diversity for neural dialog models using conditional variational autoencoders",
            "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume",
            "year": 2017
        },
        {
            "authors": [
                "DELLA (Hu"
            ],
            "title": "2022b): the original model is a GPT-2-based HCVAE; we reproduced the model for the two evaluation datasets and replaced GPT-2 by BART for a fair comparison",
            "year": 2022
        },
        {
            "authors": [
                "\u2022 DialogVED (Chen"
            ],
            "title": "2022): a Transformer VAE pre-trained on large-scale dialog data in order to improve DRG. We use -sample to denote sampling from the top-k tokens with the top-p probabilities at",
            "year": 2022
        }
    ],
    "sections": [
        {
            "text": "In this work, we propose Dior-CVAE, a hierarchical conditional variational autoencoder (CVAE) with diffusion priors to address these challenges. We employ a diffusion model to increase the complexity of the prior distribution and its compatibility with the distributions produced by a PLM. Also, we propose memory dropout to the cross-attention mechanism, which actively encourages the use of latent variables for response generation. Our method requires parameters that are comparable to those of previous studies while maintaining comparable inference time, despite the integration of the diffusion model. Overall, experiments across two commonly used open-domain dialog datasets show that our method can generate more diverse responses even without largescale dialog pre-training. Code is available at https://github.com/UKPLab/dior-cvae."
        },
        {
            "heading": "1 Introduction",
            "text": "Due to the open nature of open-domain dialogs, i.e., their diverse topics and the lack of specific goals, a dialog context can be followed by multiple responses, presenting a one-to-many complex relationship (Cs\u00e1ky et al., 2019). This relationship usually poses a significant challenge to sequenceto-sequence dialog generation models that are inherently deterministic, i.e., can not produce different responses given the same dialog. Although different decoding strategies such as nucleus sampling (Holtzman et al., 2020) have been introduced\nto bring stochasticity, these strategies mostly perform on the token level and thus might harm the fluency of the generated responses.\nConditional variational autoencoders (CVAEs) (Sohn et al., 2015) have been used to bring diversity (Zhao et al., 2017; Shen et al., 2017; Serban et al., 2017a; Chen et al., 2018, 2022; Sun et al., 2021, 2023). CVAEs draw latent variables from an assumed prior distribution conditioned on the dialog context and use these latent variables to guide the generative process. These latent variables often capture potential dialog topics, implicit conversational intents, or different styles of responses (Zhao et al., 2017).\nOne main challenge occurs due to the simple prior distribution, which is assumed to be the isotropic Gaussian distribution. The Gaussian assumption is oversimplified compared to the complex relationship between a dialog context and its potential responses. The Gaussian distribution is also incompatible with the expressive likelihood and posterior distributions, which are parameterized by pre-trained language models. The oversimplification and incompatibility consequently restrict the generated responses to a relatively small region of the latent space (Chen et al., 2019; Gu et al., 2019). In other words, the generated re-\nsponses could be different in textual form but not in topic or intent (Fig. 1). Several studies introduce more complex prior distributions by using a neural network (NN) to sample implicit latent representations (Fang et al., 2019), or by using normalizing flows (Luo and Chien, 2021). While diffusion models have been shown to provide better priors than Gaussians and normalizing flows (Vahdat et al., 2021), they have not been used to parameterize the prior distribution for variational dialog generation.\nAnother major challenge of CVAEs is the wellknown posterior collapse problem (Bowman et al., 2016), especially when incorporating the PLMs based on the Transformer encoder-decoder architecture (Vaswani et al., 2017). Latent variables can be easily neglected by the expressive decoder (Bowman et al., 2016) or bypassed by the cross-attention mechanism between the encoder and decoder (Bahuleyan et al., 2018). Previous studies attempt to mitigate this problem by weakening the decoder (Semeniuta et al., 2017; Zhao et al., 2017) or controlling the weight of the Kullback\u2013Leibler (KL) divergence term (Fu et al., 2019; Li et al., 2019). Forcing the entanglement of latent variables in the decoding process has also been proposed to address the problem (Hu et al., 2022b). Different from these methods, several dropout methods have been proposed to address posterior collapse, without the need for additional training parameters (Srivastava et al., 2014; Iyyer et al., 2015; Miladinovic et al., 2022).\nIn this work, we propose Dior-CVAE, which employs a diffusion model to parameterize the prior distribution of a hierarchical CVAE model. The diffusion model can provide a more expressive distribution than the typical isotropic Gaussian (Ho et al., 2020). Meanwhile, the proposed model uses a Transformer-based encoder-decoder PLM to compute the posterior and likelihood distributions and derive the hierarchical latent variables. To alleviate the posterior collapse problem in Transformer-based CVAEs, we introduce memory dropout into the cross-attention mechanism of the decoder, which strengthens the role of latent variables in dialog response generation. Our method necessitates comparable parameters compared to prior studies and maintains comparable inference time despite of the additional diffusion model. Extensive experiments on the DailyDialog and PersonaChat datasets show better performance of our model over existing response generation meth-\nods without large-scale dialog pre-training. Our human evaluation further validates that the proposed model can generate more diverse responses with high quality."
        },
        {
            "heading": "2 Problem Statement and Background",
            "text": ""
        },
        {
            "heading": "2.1 Dialog Response Generation",
            "text": "Dialog response generation (DRG) aims at generating a response given a dialog context. The dialog context c consists of a sequence of N tokens c = [c]N1 , which is the concatenation of the history utterances separated by a special token (</s>). The response r consists of K tokens, r = [r]K1 ."
        },
        {
            "heading": "2.2 Conditional Variational Autoencoders",
            "text": "Conditional Variational Autoencoders (CVAEs) learn a conditional generative model by introducing the latent variables in the form of p(r, z|c) = p\u03c8(z|c) p\u03b8(r|z, c) where p\u03c8(z|c) is the prior distribution of the latent variable z given the dialog context c and p\u03b8(r|z, c) is the likelihood or decoder that generates a response r given latent variables z and the dialog context c. Since the true posterior p(z|r, c) is intractable, the generative model is often trained with an approximated posterior distribution or encoder\nq\u03d5(z|r, c) . To approximate more dynamic distributions, CVAEs often use neural networks (NNs) to parameterize the prior, posterior, and likelihood distributions by \u03c8 , \u03d5 and \u03b8 respectively.\nTraining. CVAEs are trained to maximize the Evidence Lower BOund (ELBO), i.e., minimize the upper bound of negative log-likelihood. The CVAE loss (LCVAE) consists of a reconstruction loss (LRC) and the Kullback-Leibler divergence (LKL). The reconstruction loss corresponds to the cross entropy between the expected and generated response. The KL divergence aligns the posterior distribution q\u03d5(z|r, c) with the prior p\u03c8(z|c) .\nLCVAE = LRC + LKL = E[ \u2212 log p\u03b8(r|z, c) ]\n+ KL( q\u03d5(z|r, c) || p\u03c8(z|c) ).\n(1)\nCVAEs have shown great potential to improve the diversity of generated responses with the latent variables z, which can represent the underlying factors such as topics, intentions, and styles associated with different responses (Zhao et al., 2017)."
        },
        {
            "heading": "2.3 Diffusion Models",
            "text": "Given an observation of data x0, different from CVAEs, diffusion models (Ho et al., 2020) learn the data distribution p(x0) by reversing a diffusion process. The diffusion (forward) process is a Markov chain that corrupts the sampled data x0 by gradually adding random noise to it:\nq(xt|xt\u22121) = N ( \u221a\n1\u2212 \u03b2txt\u22121, \u03b2tI) (2) where \u03b21:T are the pre-defined noise variances, \u03b2t \u2208 (0, 1) at time step t. When \u03b2t \u2192 T , the data distribution will be corrupted to N (0, I). By defining \u03b1t = \u220ft i=1(1 \u2212 \u03b2i), we can directly get xt by adding noise to the input as follows:\nq(xt|x0) = N ( \u221a \u03b1tx0, (1\u2212 \u03b1t)I) (3)\nwhere \u03b1t \u2208 (0, 1). Given access to the original data x0, the forward process can be inverted analytically\np(xt\u22121|xt,x0) = N (ft(xt,x0), \u03c32t I) (4) where \u03c3t can be derived from \u03b2t (Ho et al., 2020), ft(xt,x0) has a closed form (Ho et al., 2020) parameterized by t. However, since the original data x0 is not available in the actual generation process, (i.e., the response is supposed to be generated), we can not directly use Eq. (4) to sample data. We thus approximate ft(\u00b7) using an NN with the parameter\u03c6, namely denoising network. The training objective of the denoising network can be defined as:\nEt,x0,xt [ 1\n2\u03c32t ||f\u03c6(xt, t)\u2212 x0||\n] (5)\nwhere t \u223c Uniform({1, \u00b7 \u00b7 \u00b7 , T}), xt \u223c q(xt|x0). For inference, we can use the trained denoising network f\u03c6(xt, t) to build the usable inversion p\u03c6(xt\u22121|xt) \u2248 p(xt\u22121|xt, f\u03c6(xt, t)), referring to\nlines 16-17 of Alg. 1, and get new high-quality data by sampling from it iteratively."
        },
        {
            "heading": "3 Our Method \u2013 Dior-CVAE",
            "text": "We present Dior-CVAE, a hierarchical CVAE model based on an encoder-decoder Transformer, with four improvements (Fig. 2). First, we enhance the computation of hierarchical latent variables with attention mechanism (\u00a73.1). These variables are then infused into the decoder via self- and cross-attention (\u00a73.2). We then introduce memory dropout during training to alleviate posterior collapse, a well-known problem in CVAEs (\u00a73.3). Most importantly, we parameterize the prior distribution using a diffusion model for more flexible and compatible representations than an isotropic Gaussian (\u00a73.4). Finally, we introduce the objective for the end-to-end training and describe the training and inference process (\u00a73.5)."
        },
        {
            "heading": "3.1 Hierarchical Latent Variables",
            "text": "Hierarchical CVAEs (S\u00f8nderby et al., 2016; Klushyn et al., 2019; Vahdat and Kautz, 2020; Child, 2021) increase the expressiveness of the approximated prior and posterior by splitting the latent variables into L groups z = {z1, \u00b7 \u00b7 \u00b7 , zL}. The prior and approximated posterior of the latent variables z can be factorized as:\np\u03c8(z|c) = \u220fL\nl=1 p\u03c8l(z\nl|z<l, c) (6)\nq\u03d5(z|r, c) = \u220fL\nl=1 q\u03d5l(z\nl|z<l, r, c). (7)\nwhere \u03c8l, \u03d5l denote parameters of the l-th layer. When l = 1, p\u03c81(z1|z<1, c) = p\u03c81(z1|c). The same applies for q\u03d51(z1|z<1, r, c) = q\u03d5l(z1|r, c). The detailed building process of the posterior\n(Eq. (7)) and the prior (Eq. (6)) distribution will be introduced in the following content and the \u00a73.4, respectively.\nIn this work, we employ an encoder-decoder PLM with L encoder (Enc) and L decoder (Dec) layers to build the hierarchical CVAE. Each layer corresponds to a group of latent variables. We denote the hidden output by the l-th encoder layer as HEnclc = Encl(H Encl\u22121 c ) \u2208 RN\u00d7d. We construct the mean and variance of the approximated posterior q\u03d5l(z\nl|z<l, r, c) as follows:[ \u00b5lq\u03d5\nlog(\u03c3lq\u03d5)\n] = FNN(  z<leEnclc eEnclr ) (8) where FNN refers to a fully-connected feed forward NN with tanh as the activation function, [\u00b7] denotes the concatenation of the representations. The mean and variance can be used to sample latent variables zl using the re-parameterization trick (Kingma et al., 2021) to enable back-propagation. The inputs to the FNN are computed as follows.\nWe aggregate information from the latent variables of the lower layers to get the z<l in the Eq. (8):\nz<l = FNN( [ Linear(z<l\u22121) Linear(zl\u22121) ] ) (9)\nwhere Linear denotes a fully-connected NN without activation function.\nWe construct the representation eEnclc in the Eq. (8) from each encoder layer by attending over all outputs of that layer:\neEnclc = Att(H Encl c ) (10)\nwhere Att refers to the attention mechanism (Yang et al., 2016). We can obtain the representation eEnclr in the Eq. (8) following the same method."
        },
        {
            "heading": "3.2 Hierarchical Latent Variables Infusion",
            "text": "We prepend the latent variables as prefix embeddings to the input of the self-attention mechanism in each decoder layer, following previous work (Chen et al., 2022):\nSelfAttl([Linear(zl),HDecl\u22121 ]) (11)\nThe latent variables can then be iteratively infused to generate the subsequent tokens. Differently, the cross attention takes the output of the final encoder layer HEncLc as memory. Crossattention has shown its importance in the Transformer model, resulting in more quality degradation when pruned (Voita et al., 2019). We thus\nprepend the latent variables to the (encoder) memory, which is passed to the cross-attention mechanism.\nXAttl([Linear(zl),HEncLc ]) (12)\nIntuitively, the latent variables can serve as additional memory for the decoder. The model facilitates the next token generation with the deep infusion of latent variables. However, latent variables and (encoder) memory theoretically contain overlapping information. The latent variables may be ignored during the generation process, causing posterior collapse. This drawback can be mitigated by the memory dropout introduced in \u00a73.3."
        },
        {
            "heading": "3.3 Memory Dropout",
            "text": "In this work, we propose memory dropout to address posterior collapse problem. The memory dropout aims at encouraging the use of the latent variables in the decoder. In particular, we apply random dropout to the hidden state: hEncLci of the memory where i \u2208 [1, N ] while keeping the latent variables. Subsequently, the input of the crossattention in each decoder layer becomes:\nXAttl([Linear(zl), memdrop(HEncLc )]) (13) where memdrop(\u00b7) denotes the memory dropout operation with a certain probability. Concretely, this is done by randomly masking out some hidden states from the memory of the cross-attention mechanism. Compared with previous methods, our memory dropout does not introduce any additional trainable parameters (Miladinovic et al., 2022)."
        },
        {
            "heading": "3.4 Diffusion Prior",
            "text": "We parameterize the prior distribution p\u03c8(z|c) defined in the Eq. (6) with a diffusion model to improve its complexity. The conditional information, dialog context c, can be introduced as an additional input for the denoising network, as f\u03c6(xt, t, c). During training, latent variables sampled from the approximated posterior distribution are used as the input data x0 := z (Eq. (7)). The diffusion model is trained to imitate the posterior distribution. During inference, the denoising network conditioned on the dialog context is used to sample representations of latent variables.\nSpecifically, to condition on the latent variables in lower layers as done in the inference of the posterior distribution (\u00a73.1) and the dialog context c, we concatenate the latent variables sampled from the posterior distribution and conditional representa-\nAlgorithm 1 Dior-CVAE Inference Input Dialog context c, # timestep T , noise schedule [\u03b1t]T1 , sampling hyperparameter [\u03c3t]T1 Model Denoising model f\u03c6(\u00b7) Output Response r 1: HEnc0c \u2190 c \u25b7 Embed the tokens 2: for l = 1, ..., L do 3: HEnclc = Encl(H Encl\u22121 c )\n4: Get eEnclc through H Encl c according Eq. (10) 5: end for 6: Get ec by concatenating [e Encl c ] L 1 according Eq. (14). 7: zT \u2208 Rd \u223c N (0, I) \u25b7 Sample noise 8: for t = T, ..., 1 do 9: z = (1 + w) \u2217 f\u03c6(ec, t, zt)\u2212 w \u2217 f\u03c6(0, t, zt)\n10: if t == 1 then 11: return z 12: end if 13: \u03f5 \u2208 Rd \u223c N (0, I) 14: \u03f5\u0303t = zt\u2212 \u221a \u03b1tz\u221a 1\u2212\u03b1t 15: zt\u22121 = \u221a \u03b1t\u22121z+ \u221a 1\u2212 \u03b1t\u22121 \u2212 \u03c32t \u03f5\u0303t + \u03c3t\u03f5 16: end for 17: Split z into [zl]L1 \u2208 Rd/L. 18: r = Dec([zl]L1 ,H EncL c )\ntions from all layers following the below equation: z = [z1 \u00b7 \u00b7 \u00b7 zL]\u22a4\nec = [e Enc1 c \u00b7 \u00b7 \u00b7 eEncLc ]\u22a4\n(14)\nThe sinusoidal position embeddings (Vaswani et al., 2017) are adopted to represent timestep t to get the time embedding pe(t), which is first added to the conditional representation and then concatenated with the noisy latent variables zt to form the input of the denoising network. Thus, the denoising network can be defined as:\nf\u03c6(ec, t, zt) = FNN(Linear\n([ pe(t) + ec\nzt\n]) )\n(15) The general diffusion model described in \u00a72.3 can only model the unconditional distribution. To obtain a conditional diffusion model, we follow the the classifier-free guidance paradigm (Ho and Salimans, 2021), where we train a conditional and an unconditional diffusion model simultaneously by replacing the conditional representation ec by a zero vector with a probability \u03b7 during training. During inference, the output interpolation of these two models with weight w is used as the final prior representation, referring to line 9 of Alg. 1."
        },
        {
            "heading": "3.5 End-to-end Training",
            "text": "As mentioned in \u00a72.2, the training objective of CVAEs consists of the reconstruction and the KL divergence losses. To learn also the latent diffusion prior simultaneously, we follow Vahdat\nAlgorithm 2 Training Dior-CVAE Input Dataset D, # timestep T , noise schedule [\u03b1t]T1 Model Denoising model f\u03c6(\u00b7) 1: while not converged do 2: (c, r) \u223c D, z<1 = 0 \u25b7 Sample data, init latent 3: for l = 1, ..., L do 4: HEnclc = Encl(H Encl\u22121 c ) \u25b7 Context embed.\n5: eEnclc = Att(H Encl c ) \u25b7 Eq. (10) 6: HEnclr = Encl(H Encl\u22121 r ) \u25b7 Response embed. 7: eEnclr = Att(H Encl r ) \u25b7 Eq. (10) 8: Get \u00b5lq\u03d5 , log(\u03c3 l q\u03d5) from Eq. (8) 9: zl \u223c N (\u00b5lq\u03d5 ,\u03c3 l q\u03d5) 10: end for 11: z = [z1 \u00b7 \u00b7 \u00b7 zL]\u22a4 \u25b7 Eq. (14) 12: t \u223c Uniform({1, ..., T}) \u25b7 Sample timestep 13: zt \u223c N ( \u221a \u03b1tz, (1\u2212 \u03b1t)I) \u25b7 Sample z at time t 14: \u03c9 \u223c Uniform([0, 1]) 15: if \u03c9 < \u03b7 then 16: ec = 0 17: else 18: ec = [eEnc1c \u00b7 \u00b7 \u00b7 eEncLc ]\u22a4 \u25b7 Eq. (14) 19: end if 20: L = LRC + Lneg\u2212xent + Lxent \u25b7 Eq. (16) 21: Calculate gradients and update parameters 22: end while\net al. (2021) to decompose the KL loss into its negative entropy (Lneg\u2212xent) and cross-entropy (Lxent) losses. The reconstruction and the negative entropy terms can be calculated using the reparameterization trick (Kingma and Welling, 2014). The cross-entropy term can be further expressed with the regression loss (Lreg) of the denoising diffusion model defined in Eq. (5) (Vahdat et al., 2021). The final loss of Dior-CVAE is as follows:\nL = LRC + LKL = LRC + Lneg\u2212xent + Lxent = LRC + Lneg\u2212xent + Lreg = E[ \u2212 log p\u03b8(r|z, c) ]\n+ E[ log q\u03d5(z|r, c) ] + E [\n1 2\u03c32t\n||f\u03c6(ec, t, zt, )\u2212 z|| ]\n(16)\nTraining (Alg. 2). The latent variables are sampled from the approximate posterior distribution of each layer where the parameters of the distribution are calculated through the layer-wise conditional representation and reference response representation. In addition to being fed into the decoder to generate the target response, the latent variables are also used as the input data x0 in the diffusion model to train the diffusion model to imitate the posterior distribution.\nInference (Alg. 1). Specifically, to generate the response for a given dialog context, we first encode the dialog context and get the conditional representation from each layer of the encoder. The representations are then concatenated as one of the inputs to the denoising network. Starting from the final step T, we first sample the latent variables zT from the standard Gaussian distribution. Then we iteratively denoise the latent variables conditioned on the concatenated conditional representations using the denoising network until step 1 when we get the latent variables z. We split z into L parts, resulting in z1, \u00b7 \u00b7 \u00b7 , zL and feed them into each layer of the decoder along with the memory to generate the response."
        },
        {
            "heading": "4 Experiments",
            "text": "This section gives a brief overview of our experimental settings. We refer to appendices A to C for a full set of hyperparameters, data statistics, and formal definitions of metrics, respectively.\nImplementation. Our model was developed using the OpenNMT-py library (Klein et al., 2017). We employed BART (Lewis et al., 2020) as the backbone PLM, with the max sequence length set as 1024 and 50 diffusion steps.\nDatasets & Metrics. We trained and evaluated the proposed model on the DailyDialog (Li et al., 2017) and Persona-Chat (Zhang et al., 2018) datasets. DailyDialog is a collection of English dialogs about daily life, while Persona-Chat additionally includes personas of the speakers. We follow previous work in reporting the lexical similarity of the references and generated responses using BLEU-1/2 (Papineni et al., 2002) and the lexical diversity calculated by Distinct-1/2 (Li et al., 2016) which computes the ratio of distinct n-grams in the generated responses.\nIn addition, we employ Entropy-1/2/3 (Malashina, 2021) to measure meaningful information in the generated responses. Since lexical-overlapping metrics have shown great limitations for text generation, we then employ model-based metrics for better evaluation, including BERTScore (BTS) (Sun et al., 2022) and FED (Mehri and Eskenazi, 2020). BERTScore (BTS) measures the semantic similarity of the reference and generated responses (Devlin et al., 2019). We also employ FED (Mehri and Eskenazi, 2020) which measures 18 fine-grained qualities of\nthe generated response, including the relevancy, coherency, diversity, and understandability.\nAnalysis For diversity analysis of the generated responses, we sample 100 dialogs in the intersection of DailyDialog and DailyDialog++ (Sai et al., 2020), which has multiple references for each dialog, namely DailyDialog-100.\nBaselines. We compare Dior-CVAE with the state-of-the-art models for variational dialog generation. Overall, the baselines are mostly the Transformer-based models pre-trained on the largescale corpus. One of the critical differences between these models is whether they are pre-trained on a large-scale dialog dataset. More details about the baselines can be seen in the Appx. D."
        },
        {
            "heading": "5 Experimental Results",
            "text": ""
        },
        {
            "heading": "5.1 Main Results",
            "text": "Tab. 1 presents the main evaluation results on the test sets of DailyDialog and Persona-chat. On DailyDialog, our model surpasses all baselines by a large margin for all metrics, while getting comparable performance as the models pre-trained on large-scale dialog data such as PLATO and DialogVED. The higher performance demonstrates the expressive representation capability of the diffusion priors in combination with the PLMs to generate high-quality responses. The proposed model can bring the generative distribution closer to the true dialog distribution. Regarding Persona-chat, once again, Dior-CVAE, with fewer parameters, mostly achieves better diversity than SoTA models. Compared to models with large-scale dialog pretraining, the results are inconclusive, with higher results than PLATO but slightly lower than DialogVED. The inconsistent results indicate the potential positive impact of dialog pre-training but investigation is required for further understanding.\nFurther analysis. We additionally report in Tab. 2 the n-gram Entropy scores and evaluation results of the model-based metrics. The Entropy scores show that our proposed method can generate more diverse responses compared to the large-scale dialog pre-trained model \u2013 DialogVED. BERTScore (BTS) focuses on semantic similarity brought by the cosine similarity between the contextual embeddings of a reference and a generated response. We can see a similar trend in BERTScore (BTS) on the DailyDialog dataset\ncompared to that of the lexical-overlapping metrics BLEU-n. For both metrics, our method achieves higher scores than DialogVED. Also, the higher FED score by our model indicates the higher quality of generated responses on multiple dimensions, including the relevance, coherence, diversity, and understandability of the responses."
        },
        {
            "heading": "5.2 Ablation Study",
            "text": "We conduct an ablation analysis on the validation set of the DailyDialog validation set (Tab. 3). We compare Dior-CVAE with the following ablated variants: w/o diffusion: The diffusion model is removed and the prior distribution is assumed to be the isotropic Gaussian distribution; w/o memory dropout: memory dropout is removed; w/o selfattention infusion: Eq. (11) is not computed; w/o cross-attention infusion: Eq. (12) is not computed; w/o PLM: random initialization of the Transformer encoder and decoder is used instead of BART.\nWe observe that the diffusion prior and latent variable infusion greatly contribute to both metrics that evaluate the coherence and the diversity of the generated responses. Unlike the above two components, memory dropout mainly contributes\nto diversity (Distinct-1/2) while slightly harming lexical-overlapping scores ( BLEU-1/2). While memory dropout is designed to promote diversity, generated responses then can be diverse from the references, leading to a slight decrease in lexicaloverlapping scores. We further generate responses by sampling different latent variables to assess the effects of memory dropout, the analysis can be found in Appx. H. We also ablate the PLM to assess whether generic large-scale pre-training is useful for dialog generation. The great performance drop after removing PLM highlights the importance of pre-training. Interestingly, the diversity remains relatively high even without using PLM, which is greatly attributed to the diffusion model."
        },
        {
            "heading": "5.3 Human Evaluation",
            "text": "Since automatic metrics for open-domain text generation may not be consistent with human perceptions (Liu et al., 2016), we also conduct a human evaluation on the DailyDailog-100 subset with the help of three expert annotators. All annota-\ntors have an NLP background. For each dialog, we sample five responses from Dior-CVAE and DialogVED. For quality, each annotator is asked to judge the quality with regards to the following four criteria: Coherent (COH), Informative (INF), Safety (SAF), and Engagement (ENG) on a 3-point Likert scale. We describe the criteria details in Appx. E. Furthermore, we automatically mark responses that do not violate any criteria as valid, i.e., only a maximum of 5 generated responses are valid. For the diversity evaluation, annotators are asked to annotate the number of distinct meanings among the valid responses. Results of the human evaluation are reported in Tab. 4. Compared to DialogVED, our method not only generates higher quality but also more diverse responses."
        },
        {
            "heading": "5.4 Perplexity of Multiple References",
            "text": "To further verify that our proposed model can generate more diverse responses, we calculate the perplexity of multiple different responses given the same context (Fig. 3). In particular, given a dialog context, we sample one to five human references from DailyDialog-100 (x-axis). We calculate the averaged perplexity of our method and its ablation without diffusion priors (i.e., with Gaussian priors) on the sampled human references. We also compute the cosine similarity between every two reference responses for the same dialog context using BERT (set as 1 when there is only one reference).\nFrom the cosine similarity shown by the blue curve, we can see that the human-labeled responses for the same dialog context are semantically different from each other. We can notice that the perplexity scores by the ablation without diffusion priors are significantly higher than our method. This indicates that diffusion models can better approximate multiple potential responses given a dialog context which are semantically different."
        },
        {
            "heading": "5.5 Analysis with Llama-2",
            "text": "In this section, we compare our model with Llama2 (Touvron et al., 2023), one of the most recent SoTA Large Language Models (LLMs) with bil-\nlions of parameters (Brown et al., 2020; Touvron et al., 2023; Chowdhery et al., 2023). Specifically, we prompt the Llama-2 to generate responses given a dialog history through in-context learning (ICL) (Brown et al., 2020) and instruction tuning (Mishra et al., 2022). A parameter-efficient fine-tuning method (LoRA) (Hu et al., 2022a) is used for the instruction tuning.\nThe evaluation results are shown in Tab. 5. DiorCVAE performs better on the BLEU-1/2 metrics while LLAMA-2 gets higher Dist-1/2 scores. This indicates that the responses generated by the LLM have the best lexical diversity. While BERTScore (BTS) focuses on semantic similarity between the reference and generated responses, our method also gets the best performance on this. The generated responses by Dior-CVAE match better with the reference responses. In contrast, LLAMA-2 gets higher FED scores, suggesting that the responses generated by LLMs may have better quality on multiple criteria. This also emphasizes the functionality of large-scale pre-training for dialog systems."
        },
        {
            "heading": "6 Related Work",
            "text": "Variational dialog generation. Conditional variational autoencoders (CVAE) (Kingma and Welling, 2014; Sohn et al., 2015) achieved impressive results to address the safe and commonplace\nresponse problem in the dialogue generation task by representing dialog contexts in the latent space (Zhao et al., 2017; Shen et al., 2017; Serban et al., 2017b; Chen et al., 2018). One limitation is the oversimplified Gaussian assumption of the prior and the posterior distributions. Several studies (Serban et al., 2017a; Zhao et al., 2018; Gao et al., 2019; Cai and Cai, 2022) introduce discrete latent variables to improve the complexity of these distributions. Further studies use more advanced generative models like Generative Adversarial Network (Goodfellow et al., 2020; Gu et al., 2019; Khan et al., 2020) or Normalizing Flows (Rezende and Mohamed, 2015; Luo and Chien, 2021).\nDiffusion models for text generation. Adapting diffusion models to natural language remains an open challenge due to the inherently discrete nature of texts. These studies can be divided into discrete and continuous. Notable work (Austin et al., 2021; Hoogeboom et al., 2021, 2022) directly defines a forward diffusion process on discrete data. Other work has adapted diffusion models in the word embedding space and presented them as an alternative to auto-regressive LMs (Li et al., 2022; Gong et al., 2023; Strudel et al., 2022). Differently, diffusion models have been studied in the latent space to complement existing PLMs (Liu et al., 2022; Lovelace et al., 2022; Yu et al., 2022), which condition on a pre-defined set of labels. In our approach, we investigate how to incorporate diffusion priors in variational dialog generation."
        },
        {
            "heading": "7 Conclusion & Future Work",
            "text": "We proposed Dior-CVAE, an approach for variational dialog generation, which incorporates a diffusion model to produce a more informative and expressive prior distribution. Our method is based on a hierarchical conditional variational autoencoder (CVAE), which derives latent variables from every encoder layer and fuses them into the corresponding decoder layers as hierarchical latent memory. A pre-trained language model, BART, is employed to estimate the posterior and likelihood distributions of the CVAE. The proposed approach approximates the one-to-many complex relationship of dialog response generation, i.e., multiple potential responses given a dialog context. The approach does not require more parameters than previous work, and the inference time remains comparable regardless of the introduction of the diffusion model. We also propose memory dropout to\nalleviate the posterior collapse problem in training Transformer-based CVAEs.\nOur experiments across two commonly used dialog datasets show that the proposed method can generate diverse responses without relying on largescale dialog pre-training. This work suggests the effectiveness of using a diffusion model to parameterize the prior distribution in Transformer-based CVAEs for dialog response generation. Future work on diffusion models for text generation in general should be explored given their potential.\nLimitations\nOne limitation of this work is the instability of the training process due to the high variance of the time step sampling operation in the diffusion model (Eq. (5)). An advanced pre-defined noise variances scheme for timestep sampling (Vahdat et al., 2021; Nichol and Dhariwal, 2021) will be explored to address this problem. Future work also aims at understanding information captured in the latent space produced by diffusion models (Khrulkov et al., 2023), towards an interpretable latent space.\nThis work only considers BART, a Transformer encoder-decoder architecture, as our backbone PLM. However, many recent SoTA large language models (LLMs) are decoder-only architecture, further experiments are required to use these LLMs with diffusion priors.\nEthics Considerations\nThis work is fine-tuned and evaluated on two publicly available datasets that are widely used as benchmarks for open-domain dialog generation. However, due to the use of a PLM, the fine-tuned models may inherit biases from large-scale pretraining. Safety filtering and/or debiasing methods should be considered while using the proposed approach in real-world applications."
        },
        {
            "heading": "Acknowledgement",
            "text": "This work has been funded by the European Union under the Horizon Europe grant No 101070351 (SERMAS) and by the German Federal Ministry of Education and Research (BMBF) under the promotional reference 13N15897 (MISRIK). We also thank our internal and anonymous reviewers for their constructive comments on this paper."
        },
        {
            "heading": "A Hyperparameters",
            "text": "Dior-CVAE uses BART (Lewis et al., 2020) as the backbone PLM which consists of a 6-layer encoder and a 6-layer decoder. The size of hidden state is 768. The max length of the input tokens is 1024 and the max length the generated response is set as 200. To compare with the baselines, the size of latent variable is set as 64.\nWe use Adam optimizer (Kingma and Ba, 2015) with a learning rate of 5\u00d7 e\u22124 on the Daily-Dialog dataset and 1 \u00d7 e\u22124 on the Persona-Chat dataset. We train the model for 500,000 steps, which takes around 78 hours in total. The learning rate schedule is set according to the study from Vaswani et al..\nWarmup steps of the optimization is set as 20,000 and 40,000 on the Daily-Dialog and Persona-Chat, respectively. Besides, we utilize KL annealing tricks to mitigate the posterior problem, the weight of KL term in the ELBO increases to 1 in 20,000 steps linearly. In the computation of negative log loss, the label smoothing value is set as 0.1. We use the dynamic batching mechanism in the OpenNMT package. The batch size measured by tokens is 4096. After every 5,000 optimization steps, we evaluate the model on the validation set. After completing the optimization, we select the checkpoint that obtained the best validation results to evaluate on the testing set and report the result. We conduct our experiments on one Nvidia Telsa V100 32G GPU.\nFor memdrop, we tried different dropout probabilities from the range [0.1, 0.2, 0.3, \u00b7 \u00b7 \u00b7 , 1.0]. We finally used 0.7 for dropout as it gives the highest performance on the validation set. In the diffusion prior, the number of diffusion steps during inference is set to 50. It was chosen from the range [50, 100, 150, 200] following the same standard as described above. We set the variance schedule to constants increasing linearly from \u03b21 = 5\u22126 to \u03b2M = 10\n\u22123. In the decoding process, for the beam search decoding method, the beam width is set as 5. For the sampling method, we set K as 50 and p as 0.9. All experiments are run only once due to resource constraints, with random seed set to 1234."
        },
        {
            "heading": "B Data statistics",
            "text": "Detailed dataset statistics can be seen in Tab. 6, where #Examples denotes the number of the dialog data example in the dataset, #Turns denotes the average number of turns, #Tokens means the average number of tokens in the dialog context and the response, respectively. We pre-process the multiturn dialog to many single-turn dialogs as input to the model following DialogVED (Chen et al., 2022), where the dialog history is concatenated as a whole sequence with a special token [SEP] functioning as a separation mark to separate different turns. We use two special tokens, madeupword01 and madeupword02 from the BART model as the speaker indicator tokens. #Train, #Valid, and #Test denotes the number of the single-turn dialog in the training set, validation set and testing set of each dataset."
        },
        {
            "heading": "C Automatic Evaluation Metrics",
            "text": "In this section, we present the formal definitions of all metrics used in this paper. The BLEU-n score is defined as\nBLEU-n = BP \u00b7 exp\n( n\u2211\ni=1\nwi log pi\n)\nBP =\n{ 1 if c > r\ne1\u2212r/c if c \u2264 r\n(17)\nwhere BP is the brevity penalty term, c denotes the length of the candidate generated response and r denotes the effective reference corpus length, wi denotes the positive weights summing to one. pi is the geometric average of the modified n-gram precisions, which is defined as pn =\u2211\nC\u2208{Candidates} \u2211\nn-gram\u2208C CountClip(n-gram)\u2211 C\u2032\u2208{Candidates} \u2211 n-gram\u2032\u2208C\u2032 Count(n-gram\u2032)\n(18) where Count(n-gram\u2032) denotes the the number of n-gram occurrences in the generated response, CountClip(n-gram) denotes the number of times the generated response\u2019s n-gram appears in the reference.\nDistinct-n score is calculated by countering the number of distinct unigrams and bigrams in generated responses. The value is scaled by total number of generated tokens to avoid favoring long sentences. Formally, it\u2019s defined as\nDistinct-1 = CountUnique(1-gram)\nCount(1-gram)\nDistinct-2 = CountUnique(2-gram)\nCount(2-gram) where CountUnique(n-gram) denotes the number of unique n-gram in the sentence. In this paper, we focus on Inter-Distinct score, namely the distinct score of generated responses in the whole test set.\nThe concept of \"Entropy-n\" is commonly used in information theory to quantify the average amount of information or uncertainty contained in a sequence of symbols of length \"n\". It measures the predictability or randomness of a sequence. Formally, it is defined as\nEntropy-n = \u2212 \u2211 \u03c9\u2208\u2126 p(\u03c9) log(p(\u03c9))\nwhere \u2126 is the set of all the kinds of n-gram subsequence in a generated response. p(\u03c9) denotes the normalized frequency of occurrence of the n-gram subsequence \u03c9.\nFor model-based metrics, we can directly get the evaluation result from the evaluation model. The FBD metric is a fine-grained evaluation metric that can provide evaluation scores for 17 aspects. We only take the aspects that are the most relevant to this paper, including Relevant, Correct, Coherent, Error Recovery, Consistent and Diverse, and calculate an average to get the final FBD score."
        },
        {
            "heading": "D Model Comparisons",
            "text": "We compare Dior-CVAE with the state-of-the-art models for variational dialog generation:\n\u2022 LIC (Golovanov et al., 2019): a PLM fine-tuned on the open-domain dialog datasets. \u2022 ProphetNet (Qi et al., 2020): a PDG pre-trained on predicting more than one future tokens. \u2022 DRESS (Han et al., 2022): a PLM fine-tuned to produce a balanced semantic distribution over the generated responses.\nWe also prepare and evaluate these baselines:\n\u2022 iVAEMI (Fang et al., 2019): an implicit VAE model based on LSTMs that uses a NN to produce the posterior distribution. \u2022 Optimus (Li et al., 2020): a pre-trained Transformer VAE for text generation. \u2022 MVP+S (Tang et al., 2023): a multi-task supervised pre-trained model for text generation. \u2022 DELLA (Hu et al., 2022b): the original model is a GPT-2-based HCVAE; we reproduced the model for the two evaluation datasets and replaced GPT-2 by BART for a fair comparison.\nAdditionally, we include results of the models taking advantage of large-scale dialog pre-training:\n\u2022 PLATO (Bao et al., 2020): a large-scale pretrained DRG model that uses a discrete latent variable to address the one-to-many problem. \u2022 DialogVED (Chen et al., 2022): a Transformer VAE pre-trained on large-scale dialog data in order to improve DRG.\nWe use -sample to denote sampling from the top-k tokens with the top-p probabilities at each decoding step (Fan et al., 2018; Holtzman et al., 2020) and -beam to indicate beam search."
        },
        {
            "heading": "E Human Evaluation",
            "text": "This section introduces the questions corresponding to four criteria used in our human evaluation. Each criterion are rated with a 3-point Likert scale.\n\u2022 Coherence (COH): is the response relevant to the dialog?\n\u2022 Informativeness (INF): does the response provide correct information?\n\u2022 Safety (SAF): is the response safe to read?\n\u2022 Engagement (ENG): do you want to have a conversation with this speaker?\nThe first three criteria are turn-based evaluation and the last one is evaluated on dialog-level.\nF Inference Speed\nAlthough there are concerns about the low speed of inference about the diffusion model especially in the synthetic image generation task. And there are many studies trying to improve the speed of inference of the diffusion model. While in this paper, the inference speed of our model should not be a major problem. This slow sampling nature of diffusion exists in synthetic image generation, where the number of diffusion steps is often set to 1000 \u2013 4000, and the dimension of the latent variables is relatively large (e.g., 128x128). In our method, the number of diffusion steps is set to 50 (< 1,000) and the dimension of the latent variable is set to 64. The inference speed evaluated by generated tokens per second can be seen in the Tab. 7 We can see that the inference speed of Dior-CVAE doesn\u2019t drop significantly compared with the other two models."
        },
        {
            "heading": "G Realization of the Diffusion Process",
            "text": "In this section, we show the effect of the latent variable at each diffusion step. Specifically, for a model setting where the diffusion steps is set to 50, we perform the denoising of 1, 20, 30, 40 and 50 steps, respectively and then input the denoised latent variable to the decoder to see the generation result. From the generated text we can see that as the denosing step increases, the generated response can become more relevant to the dialog context. One of the generation results can be seen in Tab. 8."
        },
        {
            "heading": "H Effect of the Memory Dropout",
            "text": "Our proposed memory dropout method is used to alleviate the problem of posterior collapse. The direct result of the posterior collapse is that the latent variable has no effect on the text generation process. To further verify the effect of the memory dropout, we sample from the prior distribution 5 times for a given context and then perform the decoding process subsequently. We then obtain the embedding of 5 generated responses using the pretrained BERT\nmodel and calculate the average cosine similarity between the responses. Experimental results on the test set of DailyDialog can be seen in the Tab. 9. An extremely high similarity means that all the five responses are almost the same no matter the how the sample latent variable changes. We can see that as the dropout rate increases, the similarity decreases, which can demonstrate the effect of the memory dropout method."
        },
        {
            "heading": "I Case Study",
            "text": "We show more generation examples by Dior-CVAE and DialogVED on the test set of DailyDialog in this section. The dialog context and generated responses can be seen in Tab. 11. From the results, we can see that Dior-CVAE can generate more diverse responses than the SoTA model DialogVED."
        }
    ],
    "title": "Dior-CVAE: Pre-trained Language Models and Diffusion Priors for Variational Dialog Generation",
    "year": 2023
}