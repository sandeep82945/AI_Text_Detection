{
    "abstractText": "Automatic Speech Recognition (ASR) systems are instrumental across various applications, with their performance being critically tied to user satisfaction. Conventional evaluation metrics for ASR systems produce a singular aggregate score, which is insufficient for understanding specific system vulnerabilities. Therefore, we aim to address the limitations of the previous ASR evaluation methods by introducing the Korean Error Explainable Benchmark Dataset for ASR and Post-processing (KEBAP). KEBAP enables comprehensive analysis of ASR systems at both speechand text levels, thereby facilitating a more balanced assessment encompassing speech recognition accuracy and user readability. KEBAP provides 37 newly defined speech-level resources incorporating diverse noise environments and speaker characteristics categories, also presenting 13 distinct textlevel error types. This paper demonstrates detailed statistical analyses of colloquial noise categories and textual error types. Furthermore, we conduct extensive validation and analysis on commercially deployed ASR systems, providing valuable insights into their performance. As a more fine-grained and real-world-centric evaluation method, KEBAP contributes to identifying and mitigating potential weaknesses in ASR systems.",
    "authors": [
        {
            "affiliations": [],
            "name": "Seonmin Koo"
        },
        {
            "affiliations": [],
            "name": "Chanjun Park"
        },
        {
            "affiliations": [],
            "name": "Jinsung Kim"
        },
        {
            "affiliations": [],
            "name": "Jaehyung Seo"
        },
        {
            "affiliations": [],
            "name": "Sugyeong Eo"
        },
        {
            "affiliations": [],
            "name": "Hyeonseok Moon"
        },
        {
            "affiliations": [],
            "name": "Heuiseok Lim"
        }
    ],
    "id": "SP:2697ad0f4fae01afd2457052c7ca2199d1132fed",
    "references": [
        {
            "authors": [
                "Petar Aleksic",
                "Cyril Allauzen",
                "David Elson",
                "Aleksandar Kracun",
                "Diego Melendo Casado",
                "Pedro J Moreno."
            ],
            "title": "Improved recognition of contact names in voice commands",
            "venue": "2015 IEEE International Conference on Acoustics, Speech and Signal",
            "year": 2015
        },
        {
            "authors": [
                "Petar Aleksic",
                "Mohammadreza Ghodsi",
                "Assaf Michaely",
                "Cyril Allauzen",
                "Keith Hall",
                "Brian Roark",
                "David Rybach",
                "Pedro Moreno"
            ],
            "title": "Bringing contextual information to google speech recognition",
            "year": 2015
        },
        {
            "authors": [
                "Rosana Ardila",
                "Megan Branson",
                "Kelly Davis",
                "Michael Kohler",
                "Josh Meyer",
                "Michael Henretty",
                "Reuben Morais",
                "Lindsay Saunders",
                "Francis Tyers",
                "Gregor Weber."
            ],
            "title": "Common voice: A massivelymultilingual speech corpus",
            "venue": "Proceedings of the",
            "year": 2020
        },
        {
            "authors": [
                "Youssef Bassil",
                "Paul Semaan."
            ],
            "title": "Asr contextsensitive error correction based on microsoft n-gram dataset",
            "venue": "arXiv preprint arXiv:1203.5262.",
            "year": 2012
        },
        {
            "authors": [
                "Theresa Breiner",
                "Swaroop Ramaswamy",
                "Ehsan Variani",
                "Shefali Garg",
                "Rajiv Mathews",
                "Khe Chai Sim",
                "Kilol Gupta",
                "Mingqing Chen",
                "Lara McConnaughey."
            ],
            "title": "Userlibri: A dataset for asr personalization using only text",
            "venue": "arXiv preprint arXiv:2207.00706.",
            "year": 2022
        },
        {
            "authors": [
                "Hui Bu",
                "Jiayu Du",
                "Xingyu Na",
                "Bengu Wu",
                "Hao Zheng."
            ],
            "title": "Aishell-1: An open-source mandarin speech corpus and a speech recognition baseline",
            "venue": "2017 20th conference of the oriental chapter of the international coordinating committee on speech",
            "year": 2017
        },
        {
            "authors": [
                "Joon Son Chung."
            ],
            "title": "Naver at activitynet challenge 2019\u2013task b active speaker detection (ava)",
            "venue": "arXiv preprint arXiv:1906.10555.",
            "year": 2019
        },
        {
            "authors": [
                "Gon\u00e7alo M Correia",
                "Andr\u00e9 FT Martins."
            ],
            "title": "A simple and effective approach to automatic postediting with transfer learning",
            "venue": "arXiv preprint arXiv:1906.06253.",
            "year": 2019
        },
        {
            "authors": [
                "Wenliang Dai",
                "Samuel Cahyawijaya",
                "Tiezheng Yu",
                "Elham J Barezi",
                "Peng Xu",
                "Cheuk Tung Yiu",
                "Rita Frieske",
                "Holy Lovenia",
                "Genta Winata",
                "Qifeng Chen"
            ],
            "title": "Ci-avsr: A cantonese audio-visual speech datasetfor in-car command recognition",
            "year": 2022
        },
        {
            "authors": [
                "John Evershed",
                "Kent Fitch."
            ],
            "title": "Correcting noisy ocr: Context beats confusion",
            "venue": "Proceedings of the First International Conference on Digital Access to Textual Cultural Heritage, pages 45\u201351.",
            "year": 2014
        },
        {
            "authors": [
                "Jinjuan Feng",
                "Andrew Sears."
            ],
            "title": "Using confidence scores to improve hands-free speech based navigation in continuous dictation systems",
            "venue": "ACM Transactions on Computer-Human Interaction (TOCHI), 11(4):329\u2013356.",
            "year": 2004
        },
        {
            "authors": [
                "Zorik Gekhman",
                "Dina Zverinski",
                "Jonathan Mallinson",
                "Genady Beryozkin."
            ],
            "title": "RED-ACE: Robust error detection for ASR using confidence embeddings",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2022
        },
        {
            "authors": [
                "Zorik Gekhman",
                "Dina Zverinski",
                "Jonathan Mallinson",
                "Genady Beryozkin."
            ],
            "title": "Red-ace: Robust error detection for asr using confidence embeddings",
            "venue": "arXiv preprint arXiv:2203.07172.",
            "year": 2022
        },
        {
            "authors": [
                "Yuan Gong",
                "Jin Yu",
                "James Glass."
            ],
            "title": "Vocalsound: A dataset for improving human vocal sounds recognition",
            "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 151\u2013155. IEEE.",
            "year": 2022
        },
        {
            "authors": [
                "Jinxi Guo",
                "Tara N Sainath",
                "Ron J Weiss."
            ],
            "title": "A spelling correction model for end-to-end speech recognition",
            "venue": "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 5651\u20135655. IEEE.",
            "year": 2019
        },
        {
            "authors": [
                "Ji-Wan Ha",
                "Hyun Sub Sim."
            ],
            "title": "A comparison study of interjectional characteristics between people who stutter and people who do not stutter",
            "venue": "Communication Sciences and Disorders, 13(3):438\u2013453.",
            "year": 2008
        },
        {
            "authors": [
                "Oleksii Hrinchuk",
                "Mariya Popova",
                "Boris Ginsburg."
            ],
            "title": "Correction of automatic speech recognition with transformer sequence-to-sequence model",
            "venue": "Icassp 2020-2020 ieee international conference on acoustics, speech and signal processing (icassp),",
            "year": 2020
        },
        {
            "authors": [
                "Seonmin Koo",
                "Chanjun Park",
                "Jaehyung Seo",
                "Seungjun Lee",
                "Hyeonseok Moon",
                "Jungseob Lee",
                "Heuiseok Lim."
            ],
            "title": "K-nct: Korean neural grammatical error correction gold-standard test set using novel error type classification criteria",
            "venue": "IEEE Access, 10:118167\u2013",
            "year": 2022
        },
        {
            "authors": [
                "Egor Lakomkin",
                "Sven Magg",
                "Cornelius Weber",
                "Stefan Wermter."
            ],
            "title": "Kt-speech-crawler: Automatic dataset construction for speech recognition from youtube videos",
            "venue": "arXiv preprint arXiv:1903.00216.",
            "year": 2019
        },
        {
            "authors": [
                "Myunghoon Lee",
                "Hyeonho Shin",
                "Dabin Lee",
                "SungPil Choi."
            ],
            "title": "Korean grammatical error correction based on transformer with copying mechanisms and grammatical noise implantation methods",
            "venue": "Sensors, 21(8):2658.",
            "year": 2021
        },
        {
            "authors": [
                "Yichong Leng",
                "Xu Tan",
                "Linchen Zhu",
                "Jin Xu",
                "Renqian Luo",
                "Linquan Liu",
                "Tao Qin",
                "Xiangyang Li",
                "Edward Lin",
                "Tie-Yan Liu."
            ],
            "title": "Fastcorrect: Fast error correction with edit alignment for automatic speech recognition",
            "venue": "Advances in Neural Information Pro-",
            "year": 2021
        },
        {
            "authors": [
                "Junwei Liao",
                "Sefik Emre Eskimez",
                "Liyang Lu",
                "Yu Shi",
                "Ming Gong",
                "Linjun Shou",
                "Hong Qu",
                "Michael Zeng."
            ],
            "title": "Improving readability for automatic speech recognition transcription",
            "venue": "Transactions on Asian and Low-Resource Language Information Pro-",
            "year": 2020
        },
        {
            "authors": [
                "Junwei Liao",
                "Yu Shi",
                "Yong Xu."
            ],
            "title": "Automatic speech recognition post-processing for readability: Task, dataset and a two-stage pre-trained approach",
            "venue": "IEEE Access, 10:117053\u2013117066.",
            "year": 2022
        },
        {
            "authors": [
                "Anirudh Mani",
                "Shruti Palaskar",
                "Sandeep Konam."
            ],
            "title": "Towards understanding asr error correction for medical conversations",
            "venue": "Proceedings of the first workshop on natural language processing for medical conversations, pages 7\u201311.",
            "year": 2020
        },
        {
            "authors": [
                "Anirudh Mani",
                "Shruti Palaskar",
                "Nimshi Venkat Meripo",
                "Sandeep Konam",
                "Florian Metze."
            ],
            "title": "Asr error correction and domain adaptation using machine translation",
            "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal",
            "year": 2020
        },
        {
            "authors": [
                "Andrew Morris",
                "Viktoria Maier",
                "Phil Green"
            ],
            "title": "From wer and ril to mer and wil: improved evaluation measures for connected speech recognition",
            "year": 2004
        },
        {
            "authors": [
                "Courtney Napoles",
                "Keisuke Sakaguchi",
                "Matt Post",
                "Joel Tetreault."
            ],
            "title": "Ground truth for grammatical error correction metrics",
            "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Confer-",
            "year": 2015
        },
        {
            "authors": [
                "Thi-Tuyet-Hai Nguyen",
                "Mickael Coustaty",
                "Antoine Doucet",
                "Adam Jatowt",
                "Nhu-Van Nguyen."
            ],
            "title": "Adaptive edit-distance and regression approach for post-ocr text correction",
            "venue": "Maturity and Innovation in Digital Libraries: 20th International Confer-",
            "year": 2018
        },
        {
            "authors": [
                "Thi Tuyet Hai Nguyen",
                "Adam Jatowt",
                "Nhu-Van Nguyen",
                "Mickael Coustaty",
                "Antoine Doucet."
            ],
            "title": "Neural machine translation with bert for post-ocr error detection and correction",
            "venue": "Proceedings of the ACM/IEEE joint conference on digital libraries in 2020, pages",
            "year": 2020
        },
        {
            "authors": [
                "Vassil Panayotov",
                "Guoguo Chen",
                "Daniel Povey",
                "Sanjeev Khudanpur."
            ],
            "title": "Librispeech: an asr corpus based on public domain audio books",
            "venue": "2015 IEEE international conference on acoustics, speech and signal processing (ICASSP), pages 5206\u20135210.",
            "year": 2015
        },
        {
            "authors": [
                "Kishore Papineni",
                "Salim Roukos",
                "Todd Ward",
                "WeiJing Zhu."
            ],
            "title": "Bleu: a method for automatic evaluation of machine translation",
            "venue": "Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 311\u2013318.",
            "year": 2002
        },
        {
            "authors": [
                "Chanjun Park",
                "Jaehyung Seo",
                "Seolhwa Lee",
                "Chanhee Lee",
                "Hyeonseok Moon",
                "Sugyeong Eo",
                "Heui-Seok Lim."
            ],
            "title": "Bts: Back transcription for speech-to-text post-processor using text-to-speech-to-text",
            "venue": "Proceedings of the 8th Workshop on Asian Translation",
            "year": 2021
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Tao Xu",
                "Greg Brockman",
                "Christine McLeavey",
                "Ilya Sutskever."
            ],
            "title": "Robust speech recognition via large-scale weak supervision",
            "venue": "International Conference on Machine Learning, pages 28492\u201328518. PMLR.",
            "year": 2023
        },
        {
            "authors": [
                "Yongmei Shi",
                "Lina Zhou."
            ],
            "title": "Supporting dictation speech recognition error correction: the impact of external information",
            "venue": "Behaviour & Information Technology, 30(6):761\u2013774.",
            "year": 2011
        },
        {
            "authors": [
                "Myung-Sun Shin",
                "Jong-Bok Ahn",
                "Hyun-Wook Nam",
                "Do-Ha Kwon."
            ],
            "title": "A study of dysfluency characteristics in normal adults and children in monologue",
            "venue": "Speech Sciences, 12(3):49\u201357.",
            "year": 2005
        },
        {
            "authors": [
                "Claytone Sikasote",
                "Antonios Anastasopoulos."
            ],
            "title": "BembaSpeech: A speech recognition corpus for the Bemba language",
            "venue": "Proceedings of the Thirteenth Language Resources and Evaluation Conference, pages 7277\u20137283, Marseille, France. European Lan-",
            "year": 2022
        },
        {
            "authors": [
                "Bernhard Suhm",
                "Brad Myers",
                "Alex Waibel."
            ],
            "title": "Multimodal error correction for speech user interfaces",
            "venue": "ACM transactions on computer-human interaction (TOCHI), 8(1):60\u201398.",
            "year": 2001
        },
        {
            "authors": [
                "Wei Tang",
                "Matthew Lease."
            ],
            "title": "Semi-supervised consensus labeling for crowdsourcing",
            "venue": "SIGIR 2011 workshop on crowdsourcing for information retrieval (CIR), pages 1\u20136.",
            "year": 2011
        },
        {
            "authors": [
                "Longshaokan Wang",
                "Maryam Fazel-Zarandi",
                "Aditya Tiwari",
                "Spyros Matsoukas",
                "Lazaros Polymenakos"
            ],
            "title": "Data augmentation for training dialog models",
            "year": 2020
        },
        {
            "authors": [
                "Sihui Wang",
                "Tom Gunter",
                "David VanDyke."
            ],
            "title": "On modelling uncertainty in neural language generation for policy optimisation in voice-triggered dialog assistants",
            "venue": "2nd Workshop on Conversational AI: Today\u2019s Practice and Tomorrow\u2019s Poten-",
            "year": 2018
        },
        {
            "authors": [
                "Jason D Williams",
                "Steve Young."
            ],
            "title": "Partially observable markov decision processes for spoken dialog systems",
            "venue": "Computer Speech & Language, 21(2):393\u2013 422.",
            "year": 2007
        },
        {
            "authors": [
                "J.P. Woodard"
            ],
            "title": "journal = Workshop on standardisation for speech I/O technology, Naval Air Development Center, Warminster, PA title = An information theoretic measure of speech recognition performance Nelson, J.T",
            "year": 1982
        },
        {
            "authors": [
                "Soyoung Yoon",
                "Sungjoon Park",
                "Gyuwan Kim",
                "Junhee Cho",
                "Kihyo Park",
                "Gyu Tae Kim",
                "Minjoon Seo",
                "Alice Oh."
            ],
            "title": "Towards standardizing korean grammatical error correction: Datasets and annotation",
            "venue": "arXiv preprint arXiv:2210.14389.",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Automatic speech recognition (ASR) is a task that recognizes speech and converts it into text, and it is getting more and more attention with the development of voice interface applications and devices such as Alexa, Siri, and Cortana (Williams and Young, 2007; Wang et al., 2018, 2020). In the real world, the ASR result has a trade-off between recognition accuracy1 and user readability. Even\n\u2217 Equally contributed, \u2021 Corresponding author 1Recognition accuracy is the measure of accurately perceiving phonemes as they are externally expressed, regardless of user input quality (Liao et al., 2022).\nif the ASR model accurately recognizes the input voice, the user\u2019s readability may decrease. This is because humans do not always utter perfect sentences in the real world (e.g., incomplete utterances, sighs, etc.). To achieve balanced ASR results in this trade-off situation, it is required to consider both recognition accuracy and user readability.\nIn terms of recognition accuracy, various ASR evaluation metrics such as word error rate (WER) (Woodard and Nelson) and character error rate (CER) (Morris et al., 2004) have been prevalent. Also, readability is considered in ASR post-processing (ASRP) tasks, where the goal is to improve the clarity and comprehension of speech recognition outputs without modifying the underlying model architecture (Mani et al., 2020b; Liao et al., 2020; Leng et al., 2021). The ASRP task relies on quantitative metrics, such as BLEU (Papineni et al., 2002) and GLEU (Napoles et al., 2015), similar to the ASR evaluation using WER and CER.\nHowever, it is crucial to recognize that even if\nquantitative evaluation scores are similar, the qualitative aspects of the ASR results may not necessarily align. Conventional research methods, which focus on accuracy or user readability, compute quantitative scores based on the degree of alignment between inputs and outputs. This approach falls short in classifying potential error types or pinpointing the model\u2019s specific weaknesses, thus lacking explanatory power for real-world ASR model outputs. This deficiency hinders the establishment of clear directions for model improvement. To this end, datasets that aim to enhance the explanatory power of ASR evaluations by considering noisy environments or speaker characteristics have been published recently (Sikasote and Anastasopoulos, 2022; Lakomkin et al., 2019; Gong et al., 2022; Dai et al., 2022). However, these datasets still focus on accuracy and provide a limited set of error types, thereby leading to insufficiency in diagnosing specific weaknesses within ASR models.\nTherefore, we introduce the novel Korean Error Explainable Benchmark Dataset for ASR and Postprocessing (KEBAP)2. It encompasses speechlevel distraction-based resources and text-level error types relevant to real-world ASR applications. These diverse error types of KEBAP can lead to improved explanatory capability compared to the previous examination methods, as illustrated in Figure 1. In particular, speech-level noise types are bifurcated into two categories: noisy environments and speaker characteristics, comprising 37 distinct types. Additionally, KEBAP includes 13 types of textual errors pertinent in ASR contexts. The dataset stands out in its authenticity since all speech samples are recorded by human speakers, and background noises are derived from real-world environments. Also, We annotate the difficulty levels to all types, enhancing the interpretability of the ASR model.\nWe employ KEBAP to conduct an empirical analysis of the correlations between speech-level noise types and textual error types. Moreover, leveraging ChatGPT (OpenAI-Blog, 2022), we explore the potential of language models in discovering the vulnerabilities of ASR models. Our observations highlight KEBAP\u2019s significant interpretability of ASR model diagnostics and shed light on the pressing need for research on diagnostic tasks for ASR systems. Our work sets the stage for more real-\n2Our KEBAP dataset is publicly available at https:// github.com/seonminkoo/KEBAP\nworld-oriented evaluations of ASR systems and can contribute to the advancements in this domain."
        },
        {
            "heading": "2 KEBAP",
            "text": ""
        },
        {
            "heading": "2.1 Why KEBAP?",
            "text": "In the real-world scenario, mitigating the tradeoff between recognition accuracy and user readability is crucial. To address this, we propose KEBAP, emphasizing the importance of considering both aspects. A detailed explanation is as follows. Firstly, in real-world speech recognition, it is essential to consider the accuracy of model and end-user satisfaction simultaneously. To facilitate this, we propose to map the accuracy of the ASR model to \u2018speech-level noises\u2019 and user readability to \u2018text-level errors\u2019 to mitigate this inherent tradeoff. From the perspective of the accuracy of the ASR model, it should output the recognition results \u2018as heard,\u2019 regardless of the quality of the user-provided input. Conversely, from the standpoint of the end-user receiving the result, satisfaction increases when the output is presented in a refined state, despite any errors in the initial input. For instance, if a speaker stammers during their speech, the ASR model would likely deem its output more accurate if it recognizes and outputs all the words uttered. However, this would likely result in lower readability from the user\u2019s perspective.\nIn addition, previous research lacks an adequate number of error types for a detailed diagnosis. Since benchmarks measure performance with quantitative metrics, it is crucial to subdivide characteristics for a more detailed diagnosis. In industry contexts, communication between model and service teams is critical. When there\u2019s an issue with the model, clear criteria for the data flywheel significantly facilitate communication. That is, distinguishing the error type criteria for speech- and textlevel aids in detailed diagnosis for model improvement. However, conventional benchmark datasets lack sufficient error types for detailed model analysis, leading to extensive usage of human evaluation in real-world settings. Humans can cope using commonsense, even if the criteria are unclear, but existing benchmarks with limited error types fall short. Hence, to solve the explainability issue, we must define error type criteria that consider both the speech- and text-level and create benchmarks to achieve human-level explainability.\nTo enhance the explanatory power of the validation process for ASR models, we define errors\nat speech- and text-level and propose KEBAP, a benchmark that considers various potential errorprone environments in real-world scenarios"
        },
        {
            "heading": "2.2 Speech-Level Noise Type",
            "text": "Error types at the speech-level refer to factors that trigger inaccuracies in speech recognition situations. For example, identical utterances may be challenging to recognize due to background noise (Sikasote and Anastasopoulos, 2022). Additionally, even in quiet environments, individuals do not consistently articulate perfect sentences and each speaker has unique characteristics that may negatively influence speech recognition (Gong et al., 2022).\nTable 1 illustrates the speech-level error type classification criteria considering these characteristics. The speech-level error types allow the classification of two main categories (noisy environment and characteristics of interlocutor) and more detailed error types, with 24 sub-types for noise error\nand 13 for speaker characteristics. Considering environments inundated with noise, it does not represent a quiet recording situation but rather a condition intertwined with noise. Realworld scenarios frequently involve inputs replete with ambient noise (Sikasote and Anastasopoulos, 2022). Reflecting on these practical situations where voice interface applications and devices are deployed, we propose an enhanced categorization scheme that closely follows the classification in the AI-HUB\u2019s noisy environment speech recognition dataset 3 which are representative Korean data platform. We divide the noisy environment errors into 11 nuanced subcategories, including home appliances, where recognition is impaired due to surrounding appliance noise; individual transportation, which includes instances with ambient transportation noise; street, covering situations with disruptive street noise; cafe/restaurant, addressing cases with the cafe or restaurant ambient noise;\n3https://www.aihub.or.kr/\nmarket/shopping mall, indicating instances with market or shopping mall noise; public transportation, comprising cases with subway or bus noise; terminal, reflecting instances with terminal noise; construction site, for cases hindered by construction site noise; factory, indicating instances with factory noise; nature ambient, for cases disturbed by natural sounds. Lastly, we include an etc. category for instances where recognition is affected by external noise types not encompassed in the previous categories.\nConsidering speaker characteristics, recognition can be hampered due to the individual traits of the recorder. Inspired by studies on idiolectal elements in the field of psycholinguistics (Ha and Sim, 2008; Shin et al., 2005), we propose a nuanced categorization comprising 13 detailed subcategories. The details description of the subcategories are described in Appendix B."
        },
        {
            "heading": "2.3 Text-Level Error Type",
            "text": "Text-level error types refer to issues that emerge in speech recognition results and must be addressed by post-processing. Since the output of the speech recognizer serves as the input for downstream tasks, it is one of the most significant factors influencing end-user satisfaction. By improving the performance of downstream tasks through quality input and diagnosing the performance of post-processing models through detailed error types, it is possible to enhance end-user satisfaction.\nExisting datasets that detail error types, such as grammatical error correction (GEC) datasets, do not consider speech recognition situations (Koo et al., 2022; Yoon et al., 2022). Therefore, we re-\nconfigure the Korean GEC dataset, K-NCT, to suit speech recognition situations. The existing K-NCT dataset includes errors that only occur at the textlevel and not in speech situations (Koo et al., 2022). Hence, errors that do not have vocal characteristics are removed.\nTable 2 illustrates the text-level error type classification criteria considering speech recognition situations, including 13 text-level errors that can occur in speech recognition situations. Detailed explanations for each text-level error type can be found in Appendix C"
        },
        {
            "heading": "2.4 KEBAP Construction Process",
            "text": "In this work, we propose a comprehensive data construction guideline for the ASR and ASRP dataset, grounded in the application of a GEC dataset. Our methodology encompasses build text-level error corpus, speech recording, noise synthesis, and difficulty annotation. For the efficiency of the task, we choose the \u2018consensus labeling\u2019 method (Tang and Lease, 2011), in which a human overseer, who possesses an elevated degree of task completion, serves as a quality controller. During the progression of the task, any outcomes that do not conform to the established guidelines are promptly dismissed and subsequently reconstructed.\nStep 1: Build Text-Level Error Corpus In this study, we employ a human-curated GEC dataset, which encompasses various text-level error types (Koo et al., 2022). Considering the inapplicability of the standard GEC benchmark dataset in a speech recognition setting, we selectively compose a text-level error types dataset by human evaluation.\nWe assess whether the given error types are valid or invalid in the context of speech recognition situations by human evaluators. Invalid types are filtered out, and the type structure is reconfigured In particular, we extract 13 categories that resonate with speech recognition scenarios (e.g., honorific colloquial expression) and reorganize their hierarchy for ease of labeling. Consequently, our refined dataset includes data reflecting 13 error types relevant to speech recognition contexts.\nSubsequently, we authenticate the quality of the filtered dataset focusing on the alignment between labels and text, and the inclusion of text-level errors with a specific consideration of the speech recognition context. Validation processes proceed with a human supervisor, priorily trained with each error type. Evaluators are presented with an erroneous sentence, its correct counterpart, and a specified error type with the corresponding error span indicated. They are then tasked with assessing whether the sentence contains the presented error types. Sentences deemed to be incorrect are appropriately amended. This procedural framework ensures the generation of a high-quality dataset.\nStep 2: Speech Recording In the second phase, we request that recording participants incorporate characteristics of interlocutor errors into their recordings by presenting them with speech-level errors and transcription relevant to the respective error types. At most 3 error types are presented, which could include an instance of \u2018no error type\u2019, indicating clean data. The placement of the error within the sentence is non-specific, with the ensurance that it includes only the errors specified. The recording environment should be ensured to be quiet without background noise. Each recorder is instructed to speak as naturally as possible, emulating their speech patterns when interacting with a voice interface application in real-world scenarios. After completing the recording, participants have the opportunity to listen to their own voice, and if they determine that the speech does not meet the criteria, they can re-record it. Participants are required to go through the process of listening to their recorded speech in order to complete the recording task. The detailed information about the workers can be found in Appendix D.\nStep 3: Synthesis of Background Noise In the next stage, we incorporate background noise into the recording to reflect the noise environment er-\nror in the proposed speech-level. The background noise used for this integration is derived directly from recordings of the identified environments. We ensure that the collected noise spans a duration longer than that of the recording file, fostering noise diversity. To mimic real-world situations, we conduct both single and multiple noise syntheses while filtering out instances that are unlikely to co-occur. During noise synthesis, the noise is integrated as though it is ambient background noise, designed to be audible at the onset of the voice file. Noise is composited into the recording by randomly excising sections, thus ensuring variation within sounds, even when they are categorized under the same noise type.\nStep 4: Difficulty Annotation Difficult data for ASR models refers to data that is not frequently encountered in the training data and is imbalanced, varying depending on the user (Aleksic et al., 2015a). Therefore, we annotate the difficulty of the data to enable a detailed assessment of the model\u2019s coverage ability. To this end, we employ a framework that distinguishes between utterances considered easier for ASR and those deemed harder or more noisy for ASR (Breiner et al., 2022). We extend this framework to include the tagging of difficulty using a Likert scale by human annotators. Humans listen to audio file and select score based on evaluation criteria. We ask humans, \u2018How difficult is it to recognize the presented speech accurately as the same as the transcript?\u2019 Scores range from 1 (very easy) to 5 (very difficult). Three evaluators assess each audio file, and the average score is selected as the difficulty level of the data. This allows for a detailed analysis of the model\u2019s performance. The details of construction process described in Appendix D."
        },
        {
            "heading": "3 KEBAP Analysis",
            "text": ""
        },
        {
            "heading": "3.1 Text-level Distribution",
            "text": "We filter out cases that cannot occur in speech recognition situations, such as typing language errors caused by keyboard language switching, in the GEC dataset. After the filtering process, the textlevel distribution is shown in Table 3. It includes 2,478 instances of errors and correct sentences, including text-level errors. The statistical information for the text is provided in Table 4"
        },
        {
            "heading": "3.2 Speech-level Distribution",
            "text": "KEBAP consists of a total of 2,478 speech files, transcriptions, and speech-level noise types. Figure 2-(a) illustrates the data distribution for speechlevel. KEBAP is composed of a total of 24,021.82 seconds of speech. It includes an average speech duration of 9.69 seconds, with the shortest file being 3.8 seconds and the longest file being 27.68 seconds. Although transcription sentences are composed of single sentences, their lengths can vary depending on speaker characteristics, such as pauses (silent) or mutters, even for sentences of the same length. This allows for the inclusion of speech files of varying durations, covering the characteristics of diverse users who use ASR systems."
        },
        {
            "heading": "3.3 Difficulty Distribution",
            "text": "Overall, the average Krippendorff\u2019s \u03b1 for interannotator agreement of each annotation level is 0.476. The label distribution of the collected data is\nshown in Figure 2-(b). To more accurately diagnose the model\u2019s capability, we enhance its interpretability by tagging the difficulty level of the data. Since the perceived difficulty of the same data may vary among individuals, we determine the difficulty of each data based on the average difficulty annotation provided by three evaluators. The difficulty ratings for the data are generally concentrated between 1 and 2, but there is also a significant presence of ratings at 5. This indicates that the dataset includes a range of difficulty levels, which we believe will be beneficial for assessing the performance of ASR models."
        },
        {
            "heading": "3.4 Category Distribution",
            "text": "Each data includes single or multiple speech-level characteristics. Figure 3 shows the distribution of each category of speech-level. The speech-level errors can be broadly classified into two main categories: noisy environment and characteristics of the interlocutor. Our dataset encompasses various combinations of characteristics within each category and also includes cross-category combinations, providing a diverse range of error types. Noisy environment and characteristics of the interlocutor represent mutually exclusive types, while co-occurrence indicates cases where two characteristics occur simultaneously. When considering only noisy environments, 1 and 2 characteristics account for 40.02% each, and 3 characteristics account for 19.96%.\nNoisy environments that cannot occur simultaneously are not included. When considering only the characteristics of the interlocutor, 1 and 2 characteristics account for 39.98% each, and 3 characteristics account for 20.04%. Co-occurrence occurs in 49.88% of cases for 2 characteristics and 50.12% of cases for 3 characteristics. This demonstrates the presence of a diverse range of error levels, both in terms of types and quantities. Actual workers recorded the data, and the background noise was collected directly from real-world environments, ensuring high quality. There is no synthetic audio involved in the recordings."
        },
        {
            "heading": "4 Efficacy Validation for KEBAP",
            "text": "In this section, we assess the specific capabilities of commercialized ASR models using KEBAP. To achieve this, we conduct a detailed correlation analysis of commercialized systems such as Google Cloud Speech-to-Text (Aleksic et al., 2015b) and CLOVA Speech (Chung, 2019). We examine the correlation between speech-level noise types and text-level errors, aiming for a granular understanding. We comprehensively validate the model\u2019s capabilities by considering both speech- and text-level aspects. We verify whether the LLM possesses the necessary qualities as a diagnostic model through the error type classification task."
        },
        {
            "heading": "4.1 Analysis of Correlation in ASR models",
            "text": "Table 5 shows the evaluation results of ASR models. Based on conventional evaluation metrics such as WER (Woodard and Nelson) and CER (Morris et al., 2004), we observe similar performance between the two ASR models. However, even though the quantitative evaluation results may be similar, it does not necessarily mean that the qualitative aspects of the ASR model\u2019s outputs are also similar. This makes it challenging to identify the specific\nweaknesses of the model, hindering the establishment of directions for model improvement. To enhance interpretability, we analyze the tendency of text-level error propagation at the speech level for ASR model. To clearly understand the impact of each speech-level category on the text-level, we sample data that includes a single speech-level noise type. The results of the ASR model are labeled by humans trained in explanations and examples of text-level errors.\nFigure 4 illustrates the correlation between the noisy environment at the speech level and textlevel errors. In the speech-level category, we have grouped similar types together, excluding miscellaneous types. In the case of text-level punctuation errors, we consider only those instances where peri-\nods (\u2018.\u2019) are missing in all sentences, leading to the omission of other punctuation marks such as question marks (\u2018?\u2019) or exclamation points (\u2018!\u2019). This specific condition allows us to focus on scenarios where the absence of periods directly affects the presence of other punctuation marks in the transcriptions.\nBoth the Google and Naver ASR systems exhibit significant error propagation in the domain of public transportation. Specifically, for Google, there is a high correlation between speech-level errors and text-level errors in punctuation, spacing, and replace. On the other hand, Clova shows a strong correlation between speech-level errors and textlevel errors in punctuation, spacing, and addition. Furthermore, google showed robustness in the nature ambient, but Clova showed relatively more text errors.\nFigure 5 shows the correlation between speechlevel characteristics of interlocutor and text-level errors. For Google, the presence of pause (silent) in speech had a significant impact on the occurrence of remove errors in the transcriptions, while word repetition contributed to the occurrence of addition errors. In the case of Clova, overall, a higher number of errors were observed compared to Google. Particularly, hyperfluency had the most significant impact on the occurrence of addition errors in the transcriptions.\nThis analysis provides valuable insights into the correlation between speech-level noise, particularly noisy environments, and text-level errors in the Google and Clova ASR systems. The varying impact of different types of speech-level characteristics on text-level errors highlights the need for further granularity in categorizing these types. Even if models demonstrate similar performance, the individual capabilities of each model can differ. This demonstrates that KEBAP helps enhance the interpretability of ASR model verification 4."
        },
        {
            "heading": "4.2 Adequacy of Synthesized Noise",
            "text": "Table 6 shows the performance before and after noise synthesis. Experimental results show that for Google, the WER is 0.49, the CER is 0.23 before noise synthesis, and the WER is 0.68 and the CER is 0.41 after noise synthesis. For Clova, the WER before noise synthesis is 0.53, and the CER is 0.19,\n4Additionally, KEBAP provides difficulty information, enabling even more fine-grained analysis. Appendix E includes the detailed analysis results based on diverse levels of difficulty.\nwhile the WER after noise synthesis is 0.71 and the CER is 0.43. These results are interpretable in that the resources we provide are high-quality and helpful."
        },
        {
            "heading": "4.3 Examination of ASR models through LLM",
            "text": "With recent advancements in Large Language Model (LLM) development, most tasks are converging towards LLM-based approaches. In this study, we explore the potential of using ChatGPT (OpenAI-Blog, 2022) as a diagnostic tool for ASR results. Understanding error types is essential for verifying the models, and to measure this understanding, we perform an error type classification task. ChatGPT is utilized to classify text-level error types based on provided sentences in a few-shot setup. The specific prompt used for this experiment is listed in Appendix F.\nWe task ChatGPT with classifying all text-level errors occurring in the ASR results. However, as seen in the examples (please refer to Appendix F.2), it is evident that ChatGPT not only misclassifies text-level errors but also struggles more when multiple errors are present within a sentence. Although LLMs are converging towards covering various tasks, they exhibit limitations in performing diagnostic tasks for commercial systems. This indicates that while various tasks may converge with LLM, the diagnostic domain for the proposed model is far from convergence with LLMs, highlighting the need for further research."
        },
        {
            "heading": "5 Conclusion",
            "text": "In the real-world, ASR results involve a trade-off between recognition accuracy and user readability, thus requiring a balanced consideration of these factors. To provide guidance for improving model performance, it is necessary to enhance interpretability, which entails considering both speech-level accuracy and text-level user readability. To this end, we propose Korean Error Explainable Benchmark\nDataset for ASR and Post-processing (KEBAP) for diagnosing and validating models by segmenting error types while considering both speech- and text-level. To facilitate the construction process, we utilize a GEC dataset that includes text-level errors and structure the process into validation, recording, synthesis of background noise, and difficulty tagging stages, employing consensus labeling within each stage to enhance the efficiency and quality of the task. We performed a detailed diagnostic analysis of the commercialization systems using KEBAP. Furthermore, the proposed task falls into a domain that is challenging for ChatGPT to cover, and it indicates the need for further research to achieve a closer approximation to real-world diagnostics. We demonstrated that KEBAP contributes to enhancing the interpretability of the model\u2019s weaknesses.\nLimitations\nThis study has the limitation of only building data for the Korean language. Additionally, as this paper proposes a new task, it was not able to conduct extensive quantitative analyses by comparing it with existing models, which remains a limitation. However, this paper made a contribution by proposing new data and tasks and making them publicly available.\nEthics Statement\nWe discuss the main ethical considerations of KEBAP benchmark we presented: (1) Privacy. KEBAP benchmark is constructed to acquire factual dataset, and does not contain privacy issues. (2) Human evaluation. During data evaluation process, we paid human workers the legal wage determined by the average time of evaluation and local labor compensation standards. We also guided them to take a rest when they are in a state of fatigue during work. (3) Potential problems. While principled measures are taken to ensure the quality of the dataset, there might still be potential problems with the dataset quality."
        },
        {
            "heading": "Acknowledgements",
            "text": "This research was supported by the MSIT(Ministry of Science and ICT), Korea, under the ITRC(Information Technology Research Center) support program(IITP-2023-2018-0-01405) supervised by the IITP(Institute for Information & Communications Technology Planning &\nEvaluation). This work was supported by Institute of Information & communications Technology Planning & Evaluation(IITP) grant funded by the Korea government(MSIT) (No. 2020-0-00368, A Neural-Symbolic Model for Knowledge Acquisition and Inference Techniques). This work was supported by Institute for Information & communications Technology Planning & Evaluation(IITP) grant funded by the Korea government(MSIT) (No. 2022-0-00369, (Part 4) Development of AI Technology to support Expert Decision-making that can Explain the Reasons/Grounds for Judgment Results based on Expert Knowledge)"
        },
        {
            "heading": "A Related Works and Background",
            "text": "Post-processing Model Post-processing serves an important role in quality enhancement across various fields by modifying the distorted output into appropriate statements. For instance, in the field of optical character recognition (OCR), conventional approaches such as manual, lexical, and statistical methods have been used (Evershed and Fitch, 2014; Nguyen et al., 2018). More recently, language models like BERT have been employed for error detection in tasks like named entity recognition (NER) and are performed through characterlevel machine translation (Nguyen et al., 2020).\nAs another field, machine translation (MT) often utilizes the following methods. Post-processing research is being carried out in automatic post-editing (APE) to improve translation quality by adopting transfer learning (Correia and Martins, 2019). Concurrently, in the grammatical error correction (GEC) field, transformers and the copy mechanism are used to correct spelling and grammatical errors in MT results (Lee et al., 2021). Studies that define error types to construct test sets or utilize an automatic grammatical error annotation system to create datasets also exist to improve Korean GEC studies (Koo et al., 2022; Yoon et al., 2022). Likewise, the study on post-processing is actively explored in a wide range of fields and holds significance in terms of enhancing the quality of output results. This can also be of significant importance in the field of Automatic speech recognition (ASR), which is discussed in the following section.\nASR Post-Processing Model ASR postprocessing (ASRP) involves the detection and correction of errors in the output of an ASR, distinguishing it from simple error correction in that it considers user-friendliness as an additional aspect. This approach can improve the final quality of statements without modifying the ASR system structure. For instance, in specialized fields like the medical domain, attempts have been made to eliminate punctuation errors in ASR through post-processing (Mani et al., 2020a). Prior research has primarily focused on providing information that allows humans to manually rectify erroneous segments, proposing alternative words for correction or creating an environment conducive to modification (Suhm et al., 2001; Feng and Sears, 2004). External information, such as word alternative hypothesis, noisy context,\nand accurate context, is provided to assist in post-processing for error correction (Shi and Zhou, 2011). In particular, Bassil and Semaan (2012) use the N-gram dataset for ASR errors to detect and correct errors automatically. Models such as LSTM-based or Transformer-based sequence-to-sequence architectures are adopted to correct the speech recognition results while considering the semantics and spelling (Guo et al., 2019; Hrinchuk et al., 2020).\nRecent studies strive to improve ASRP performance by utilizing the results derived from ASR. Gekhman et al. (2022a) introduce the ASR confidence embedding (ACE) layer to the encoder of the ASR model to jointly encode the confidence scores and transcribed text into a contextualized representation. To mitigate the time and cost-related challenges associated with the parallel data required for training, Park et al. (2021) employ Text-to-speech (TTS) and Speech-to-text (STT) technologies to construct parallel data.\nASR dataset The availability of suitable datasets is imperative for the active progression of ASRP. Previously, post-processing studies have been conducted with ASR datasets. Panayotov et al. (2015) organize the two labels in the ASR dataset that denote the quality of speech recognition, classified into \u2018clean\u2019 and \u2018other\u2019 categories, providing valuable assistance in the analysis. Ardila et al. (2020) constuct comprehensive ASR dataset that includes demographic metadata such as age, sex, and accent to provide a wider representation.\nTranscription hypotheses obtained by decoding audio data using an ASR model are used to align hypothesis words with the reference (correct) transcription. The process of labeling errors and nonerrors is facilitated by employing the minimum edit distance (Gekhman et al., 2022b). In the context of Chinese language datasets, a significant dataset is available for speech recognition systems, labeled with audio devices and recording environments (Bu et al., 2017). Gekhman et al. (2022b) build a dataset by aligning hypothesis words with the reference (correct) transcription through a transcription hypothesis obtained by decoding audio data with an ASR model and labeling errors and nonerrors using minimum edit distance. In the context of Chinese, a large-scale dataset is available for speech recognition systems labeled with audio device information and recording environments (Bu et al., 2017).\nTo mitigate the problem of insufficient training\ndata, methodologies that synthesize data via data augmentation methods have been proposed (Liao et al., 2022). However, the overall quality of the data is more crucial than the size. Specifically, the detailed datasets that consider both speechand text-level like the real world are absent. Consequently, we aim to construct the ASR PostProcessing dataset, which contemplates audio- and text-level for the first time."
        },
        {
            "heading": "B Description of Speech-Level Noise Type",
            "text": "Pause (silent) category captures instances where silence intervenes mid-utterance before completion\u2014for instance, when \u2018I am eating\u2019 is articulated as \u2018I am... eating\u2019. Filled pause represents cases characterized by the habitual insertion of filler sounds during pauses, as in utterances supplemented by sounds such as \u2018um... uh... so I\u2019. Interjection category encompasses instances where one or more words or phrases irrelevant to the intended message are interjected, evident in utterances like \u2018Okay I see, but you know\u2019. Parenthetical category includes instances where grammatically correct, but semantically neutral phrases are inserted\u2014for instance, utterances incorporating phrases such as \u2018you know\u2019 and \u2018I mean\u2019. Unfinished interlocutor category denotes cases where the utterance concludes prematurely\u2014for instance, when \u2018I am eating\u2019 is truncated to \u2018I am...\u2019. Word repetition category signifies instances where the same word is iterated, as in saying \u2018Hello\u2019 as \u2018Hello Hello\u2019. Syllable repetition category characterizes cases where the same syllable is iterated\u2014for instance, when \u2018Hello\u2019 is articulated as \u2018He-hello\u2019. Phoneme repetition category encapsulates instances where the same phoneme is repeated, such as saying \u2018Hello\u2019 as \u2018Hel-llo\u2019. Sustained category accounts for instances where part of an utterance is elongated, exemplified in \u2018Is that so\u2014right?\u2019. Hyperfluency category represents instances of excessive verbosity. Mutter category includes cases where utterances are murmured in an indistinct manner, as in \u2018That.. is.. like that...\u2019. Dynamic error category encompasses instances where syllable articulation strength is incongruous with the intended utterance, or instances that are challenging to comprehend at the human-level. Finally, speaking rate category accounts for instances where rapid speech pace hinders comprehension at a human-level."
        },
        {
            "heading": "C Description of Text-Level Error Type",
            "text": "Spacing encapsulates instances contravening standard spacing conventions. Punctuation entails cases where punctuation is omitted or misapplied in Korean sentences\u2014for instance, when \u2018Can I teach?\u2019 is interpreted as \u2018Can I teach.\u2019 Numerical encompasses cases where number conversion fails, such as when \u2018Ahead of the three-month schedule\u2019 is interpreted as \u2018Bill 2, 3-month schedule\u2019.\nSpelling and Grammar consists of ten detailed subcategories. Remove designates cases where some word components are not recognized, or endings or particles are missing\u2014for example, when \u2018The champion is in the final\u2019 is misinterpreted as \u2018Champion final\u2019. Addition involves cases where the same word is repeated or unutilized particles or endings are appended. For instance, when \u2018World\u2019s fruits, fish, and meat\u2019 is interpreted as \u2018World\u2019s world\u2019s fruits, fish, and meat\u2019. Replace refers to instances where one word is substituted with another\u2014for example, when \u2018Apply the filter.\u2019 is interpreted as \u2018Wear the pizza\u2019. Separation refers to instances where consonants and vowels in the target utterance are separated, exemplified when \u2018The discount applies as it is.\u2019 is interpreted as \u2018Discount app - lise as it is.\u2019. Foreign word conversion refers to cases where words deviate from standard foreign word pronunciation or some syllables are incorrectly converted from English to Korean or vice versa. For example, when \u2018Brazil\u2019s Samba Festival\u2019 is interpreted as \u2018Brazil\u2019s SsamBap Festival,\u2019 or \u2018I prefer to use ATM.\u2019 is interpreted as \u2018I prefer to use hm.\u2019.\nSpelling is bifurcated into two types: Graphemeto-Phoneme (G2P) and Consonant vowel conversion. G2P pertains to instances where a character is recognized per its pronunciation. Consonant vowel conversion refers to instances where phonemic units are incorrectly spelled. Post-position refers to cases where different particles are used or omitted\u2014for example, when \u2018Ordinary high school students\u2019 is interpreted as \u2018Ordinary at high school students.\u2019 Syntax involves cases where the grammatical interpretation remains valid, but the semantic interpretation varies. Finally, neologism refers to cases where the target word and its meaning and pronunciation are dissimilar and are not included in Korean vocabulary."
        },
        {
            "heading": "D Human Annotation",
            "text": "D.1 Crowd-sourcing and Compensation We recruited individuals who are native speakers of Korean and selectively hired candidates suitable for the task through validation questions. Every employee has been fairly remunerated at least a rate of 140 KRW per task. It is expected that each worker will complete 2-3 questions within a minute, guaranteeing a minimum compensation of 16,800 KRW per hour. Comparatively, the minimum hourly wage in South Korea for 2023 is 9,620 KRW. The guidelines for annotation and the user interface are illustrated in Figure 6 and Figure 7.\nD.2 Annotation Guidelines and Interface\nD.3 Annotation Demographics The detailed demographic information is provided in Table 7"
        },
        {
            "heading": "E Analysis of Correlation by Difficulty Levels",
            "text": "We believe that providing difficulty information facilitates the analysis of weaknesses in ASR models. We extracted an equal number of samples for each difficulty level and analyzed them. Figure 8, Figure 9, and Figure 10 show the correlation between the noisy environment at the speech level and textlevel errors in diverse difficulty settings. Figure 11, Figure 12, and Figure 13 illustrate the correlation between speech-level characteristics of interlocutor and text-level errors in diverse difficulty settings. Spa/Punc/Num represent spacing, punctuation, and numerical, respectively. Rem/Add/Rep/Sep indicate remove, addition, replace, and separation, respectively. FWC/G2P/CVC correspond to foreign word conversion, grapheme-to-phoneme, and consonant vowel conversion, respectively. PP/Syn/Neo signify post-position, syntax, and neologism, respectively.\nAnalyzing the details based on different difficulty levels can be employed to enhance the interpretability of the ASR model. For example, in the case of Google, experimental results show that the correlation from \u2018Terminal\u2019 speech-level type to \u2018Punctuation\u2019 text-level type is strong for easy level, \u2018Construction site\u2019 speech-level type to \u2018Addition\u2019 text-level type for medium level, and \u2018Terminal\u2019 speech-level type to \u2018Replace\u2019 or \u2018Remove\u2019 textlevel type for hard level. For Clova, the tendency of \u2019Replace\u2019 text-level type in \u2019Individual transportation\u2019 speech-level type is strongest at easy level, and it is strongly related to \u2019Syntax\u2019 and \u2019Replace\u2019 text-level type at medium level. At the hard level, it has a strong tendency to \u2019Remove\u2019 and \u2019Syntax\u2019 text-level types."
        },
        {
            "heading": "F LLM for Validation",
            "text": "F.1 Prompts of ChatGPT\nF.2 Examples of Classified Types"
        }
    ],
    "title": "KEBAP: Korean Error Explainable Benchmark Dataset for ASR and Post-processing",
    "year": 2023
}