{
    "abstractText": "Scaling dense PCFGs to thousands of nonterminals via a low-rank parameterization of the rule probability tensor has been shown to be beneficial for unsupervised parsing. However, PCFGs scaled this way still perform poorly as a language model, and even underperform similarly-sized HMMs. This work introduces SimplePCFG, a simple PCFG formalism with independent left and right productions. Despite imposing a stronger independence assumption than the low-rank approach, we find that this formalism scales more effectively both as a language model and as an unsupervised parser. As an unsupervised parser, our simple PCFG obtains an average F1 of 65.1 on the English PTB, and as a language model, it obtains a perplexity of 119.0, outperforming similarly-sized lowrank PCFGs. We further introduce FlashInside, a hardware IO-aware implementation of the inside algorithm for efficiently scaling simple PCFGs.",
    "authors": [
        {
            "affiliations": [],
            "name": "Wei Liu"
        },
        {
            "affiliations": [],
            "name": "Songlin Yang"
        },
        {
            "affiliations": [],
            "name": "Yoon Kim"
        },
        {
            "affiliations": [],
            "name": "Kewei Tu"
        }
    ],
    "id": "SP:1ef0f001886b445132d5377451ef52ed486e431c",
    "references": [
        {
            "authors": [
                "Steven Cao",
                "Nikita Kitaev",
                "Dan Klein."
            ],
            "title": "Unsupervised parsing via constituency tests",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 4798\u20134808, Online. Association for Computational",
            "year": 2020
        },
        {
            "authors": [
                "Tianqi Chen",
                "Bing Xu",
                "Chiyuan Zhang",
                "Carlos Guestrin."
            ],
            "title": "Training deep nets with sublinear memory cost",
            "venue": "CoRR, abs/1604.06174.",
            "year": 2016
        },
        {
            "authors": [
                "Justin T. Chiu",
                "Yuntian Deng",
                "Alexander M. Rush."
            ],
            "title": "Low-rank constraints for fast inference in structured models",
            "venue": "Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021,",
            "year": 2021
        },
        {
            "authors": [
                "Shay B. Cohen",
                "Giorgio Satta",
                "Michael Collins."
            ],
            "title": "Approximate PCFG parsing using tensor decomposition",
            "venue": "Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language",
            "year": 2013
        },
        {
            "authors": [
                "Michael Collins."
            ],
            "title": "Three generative, lexicalised models for statistical parsing",
            "venue": "35th Annual Meeting of the Association for Computational Linguistics and 8th Conference of the European Chapter of the",
            "year": 1997
        },
        {
            "authors": [
                "Tri Dao",
                "Daniel Y. Fu",
                "Stefano Ermon",
                "Atri Rudra",
                "Christopher R\u00e9."
            ],
            "title": "Flashattention: Fast and memory-efficient exact attention with io-awareness",
            "venue": "NeurIPS.",
            "year": 2022
        },
        {
            "authors": [
                "Andrew Drozdov",
                "Subendhu Rongali",
                "Yi-Pei Chen",
                "Tim O\u2019Gorman",
                "Mohit Iyyer",
                "Andrew McCallum"
            ],
            "title": "Unsupervised parsing with S-DIORA: Single tree encoding for deep inside-outside recursive autoencoders",
            "venue": "In Proceedings of the 2020 Conference",
            "year": 2020
        },
        {
            "authors": [
                "Andrew Drozdov",
                "Patrick Verga",
                "Yi-Pei Chen",
                "Mohit Iyyer",
                "Andrew McCallum."
            ],
            "title": "Unsupervised labeled parsing with deep inside-outside recursive autoencoders",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Pro-",
            "year": 2019
        },
        {
            "authors": [
                "Jason Eisner."
            ],
            "title": "Inside-outside and forwardbackward algorithms are just backprop (tutorial paper)",
            "venue": "Proceedings of the Workshop on Structured Prediction for NLP, pages 1\u201317, Austin, TX. Association for Computational Linguistics.",
            "year": 2016
        },
        {
            "authors": [
                "Jason Eisner",
                "Giorgio Satta."
            ],
            "title": "Efficient parsing for bilexical context-free grammars and head automaton grammars",
            "venue": "Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics, pages 457\u2013464, College Park, Maryland,",
            "year": 1999
        },
        {
            "authors": [
                "Jason M. Eisner."
            ],
            "title": "Three new probabilistic models for dependency parsing: An exploration",
            "venue": "COLING 1996 Volume 1: The 16th International Conference on Computational Linguistics.",
            "year": 1996
        },
        {
            "authors": [
                "Dan Friedman",
                "Alexander Wettig",
                "Danqi Chen."
            ],
            "title": "Finding dataset shortcuts with grammar induction",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 4345\u20134363, Abu Dhabi, United Arab Emirates.",
            "year": 2022
        },
        {
            "authors": [
                "Daniel J. Hsu",
                "Sham M. Kakade",
                "Percy Liang."
            ],
            "title": "Identifiability and unmixing of latent parse trees",
            "venue": "Advances in Neural Information Processing Systems 25: 26th Annual Conference on Neural Information Processing Systems 2012. Proceedings of a meet-",
            "year": 2012
        },
        {
            "authors": [
                "Xiang Hu",
                "Haitao Mi",
                "Liang Li",
                "Gerard de Melo."
            ],
            "title": "Fast-R2D2: A pretrained recursive neural network based on pruned CKY for grammar induction and text representation",
            "venue": "Proceedings of the 2022",
            "year": 2022
        },
        {
            "authors": [
                "Lifeng Jin",
                "William Schuler."
            ],
            "title": "Grounded PCFG induction with images",
            "venue": "Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language",
            "year": 2020
        },
        {
            "authors": [
                "Taeuk Kim."
            ],
            "title": "Revisiting the practical effectiveness of constituency parse extraction from pre-trained language models",
            "venue": "Proceedings of the 29th International Conference on Computational Linguistics, pages 5398\u20135408, Gyeongju, Republic of Korea. In-",
            "year": 2022
        },
        {
            "authors": [
                "Yoon Kim."
            ],
            "title": "Sequence-to-sequence learning with latent neural grammars",
            "venue": "Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pages",
            "year": 2021
        },
        {
            "authors": [
                "Yoon Kim",
                "Chris Dyer",
                "Alexander Rush."
            ],
            "title": "Compound probabilistic context-free grammars for grammar induction",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2369\u20132385, Florence, Italy. Asso-",
            "year": 2019
        },
        {
            "authors": [
                "Dan Klein",
                "Christopher Manning."
            ],
            "title": "Corpusbased induction of syntactic structure: Models of dependency and constituency",
            "venue": "Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL-04), pages 478\u2013485,",
            "year": 2004
        },
        {
            "authors": [
                "Boyi Li",
                "Rodolfo Corona",
                "Karttikeya Mangalam",
                "Catherine Chen",
                "Daniel Flaherty",
                "Serge J. Belongie",
                "Kilian Q. Weinberger",
                "Jitendra Malik",
                "Trevor Darrell",
                "Dan Klein"
            ],
            "title": "Does unsupervised grammar induction need pixels? CoRR, abs/2212.10564",
            "year": 2022
        },
        {
            "authors": [
                "Jiaxi Li",
                "Wei Lu."
            ],
            "title": "Contextual distortion reveals constituency: Masked language models are implicit parsers",
            "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 5208\u20135222, Toronto,",
            "year": 2023
        },
        {
            "authors": [
                "Anji Liu",
                "Honghua Zhang",
                "Guy Van den Broeck."
            ],
            "title": "Scaling up probabilistic circuits by latent variable distillation",
            "venue": "The Eleventh International Conference on Learning Representations.",
            "year": 2023
        },
        {
            "authors": [
                "Chao Lou",
                "Kewei Tu."
            ],
            "title": "Improving grammarbased sequence-to-sequence modeling with decomposition and constraints",
            "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 1918\u2013",
            "year": 2023
        },
        {
            "authors": [
                "Mitchell P. Marcus",
                "Beatrice Santorini",
                "Mary Ann Marcinkiewicz."
            ],
            "title": "Building a large annotated corpus of english: The penn treebank",
            "venue": "Comput. Linguistics, 19(2):313\u2013330.",
            "year": 1993
        },
        {
            "authors": [
                "Tom\u00e1s Mikolov",
                "Anoop Deoras",
                "Stefan Kombrink",
                "Luk\u00e1s Burget",
                "Jan Cernock\u00fd."
            ],
            "title": "Empirical evaluation and combination of advanced language modeling techniques",
            "venue": "INTERSPEECH 2011, 12th Annual Conference of the International Speech Com-",
            "year": 2011
        },
        {
            "authors": [
                "Mark Paskin."
            ],
            "title": "Grammatical bigrams",
            "venue": "Advances in Neural Information Processing Systems, volume 14. MIT Press.",
            "year": 2001
        },
        {
            "authors": [
                "Robert Peharz",
                "Steven Lang",
                "Antonio Vergari",
                "Karl Stelzner",
                "Alejandro Molina",
                "Martin Trapp",
                "Guy Van den Broeck",
                "Kristian Kersting",
                "Zoubin Ghahramani."
            ],
            "title": "Einsum networks: Fast and scalable learning of tractable probabilistic circuits",
            "venue": "Pro-",
            "year": 2020
        },
        {
            "authors": [
                "Alexander Rush."
            ],
            "title": "Torch-struct: Deep structured prediction library",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 335\u2013342, Online. Association for Computational Linguistics.",
            "year": 2020
        },
        {
            "authors": [
                "Djam\u00e9 Seddah",
                "Sandra K\u00fcbler",
                "Reut Tsarfaty."
            ],
            "title": "Introducing the SPMRL 2014 shared task on parsing morphologically-rich languages",
            "venue": "Proceedings of the First Joint Workshop on Statistical Parsing of Morphologically Rich Languages and Syntactic Anal-",
            "year": 2014
        },
        {
            "authors": [
                "Yikang Shen",
                "Zhouhan Lin",
                "Chin-Wei Huang",
                "Aaron Courville."
            ],
            "title": "Neural Language Modeling by Jointly Learning Syntax and Lexicon",
            "venue": "Proceedings of ICLR.",
            "year": 2018
        },
        {
            "authors": [
                "Yikang Shen",
                "Shawn Tan",
                "Alessandro Sordoni",
                "Aaron Courville."
            ],
            "title": "Ordered Neurons: Integrating Tree Structures into Recurrent Neural Networks",
            "venue": "Proceedings of ICLR.",
            "year": 2019
        },
        {
            "authors": [
                "Yikang Shen",
                "Yi Tay",
                "Che Zheng",
                "Dara Bahri",
                "Donald Metzler",
                "Aaron Courville."
            ],
            "title": "StructFormer: Joint unsupervised induction of dependency and constituency structure from masked language modeling",
            "venue": "Proceedings of the 59th Annual Meeting of the",
            "year": 2021
        },
        {
            "authors": [
                "Philippe Tillet",
                "Hsiang-Tsung Kung",
                "David D. Cox."
            ],
            "title": "Triton: an intermediate language and compiler for tiled neural network computations",
            "venue": "Proceedings of the 3rd ACM SIGPLAN International",
            "year": 2019
        },
        {
            "authors": [
                "Bailin Wang",
                "Ivan Titov",
                "Jacob Andreas",
                "Yoon Kim."
            ],
            "title": "Hierarchical phrase-based sequence-tosequence learning",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 8211\u20138229, Abu Dhabi, United",
            "year": 2022
        },
        {
            "authors": [
                "Zhiyang Xu",
                "Andrew Drozdov",
                "Jay Yoon Lee",
                "Tim O\u2019Gorman",
                "Subendhu Rongali",
                "Dylan Finkbeiner",
                "Shilpa Suresh",
                "Mohit Iyyer",
                "Andrew McCallum"
            ],
            "title": "Improved latent tree induction with distant supervision via span constraints",
            "year": 2021
        },
        {
            "authors": [
                "Nianwen Xue",
                "Fei Xia",
                "Fu-Dong Chiou",
                "Martha Palmer."
            ],
            "title": "The penn chinese treebank: Phrase structure annotation of a large corpus",
            "venue": "Nat. Lang. Eng., 11(2):207\u2013238.",
            "year": 2005
        },
        {
            "authors": [
                "Songlin Yang",
                "Roger P Levy",
                "Yoon Kim."
            ],
            "title": "Unsupervised discontinuous constituency parsing with mildly context-sensitive grammars",
            "venue": "Proceedings of ACL.",
            "year": 2023
        },
        {
            "authors": [
                "Songlin Yang",
                "Wei Liu",
                "Kewei Tu."
            ],
            "title": "Dynamic programming in rank space: Scaling structured inference with low-rank HMMs and PCFGs",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational",
            "year": 2022
        },
        {
            "authors": [
                "Songlin Yang",
                "Yanpeng Zhao",
                "Kewei Tu."
            ],
            "title": "Neural bi-lexicalized PCFG induction",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing",
            "year": 2021
        },
        {
            "authors": [
                "Songlin Yang",
                "Yanpeng Zhao",
                "Kewei Tu."
            ],
            "title": "PCFGs can do better: Inducing probabilistic contextfree grammars with many symbols",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics:",
            "year": 2021
        },
        {
            "authors": [
                "Youngmin Yi",
                "Chao-Yue Lai",
                "Slav Petrov",
                "Kurt Keutzer."
            ],
            "title": "Efficient parallel CKY parsing on GPUs",
            "venue": "Proceedings of the 12th International Conference on Parsing Technologies, pages 175\u2013185, Dublin, Ireland. Association for Computational Lin-",
            "year": 2011
        },
        {
            "authors": [
                "Songyang Zhang",
                "Linfeng Song",
                "Lifeng Jin",
                "Haitao Mi",
                "Kun Xu",
                "Dong Yu",
                "Jiebo Luo."
            ],
            "title": "Learning a grammar inducer from massive uncurated instructional videos",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Process-",
            "year": 2022
        },
        {
            "authors": [
                "Songyang Zhang",
                "Linfeng Song",
                "Lifeng Jin",
                "Kun Xu",
                "Dong Yu",
                "Jiebo Luo."
            ],
            "title": "Video-aided unsupervised grammar induction",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human",
            "year": 2021
        },
        {
            "authors": [
                "Yanpeng Zhao",
                "Ivan Titov."
            ],
            "title": "Visually grounded compound PCFGs",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 4369\u20134379, Online. Association for Computational Linguistics.",
            "year": 2020
        },
        {
            "authors": [
                "Hao Zhu",
                "Yonatan Bisk",
                "Graham Neubig."
            ],
            "title": "The Return of Lexical Dependencies: Neural Lexicalized PCFGs",
            "venue": "Transactions of the Association for Computational Linguistics, 8:647\u2013661.",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Despite the improvements in unsupervised parsing obtained through scaling neural probabilistic context-free grammars (PCFGs), their language model performance scales less favorably compared to, for example, hidden Markov models (HMMs) and neural language models. On the Penn Treebank, a neural PCFG with 30 nonterminals and 60 preterminals obtains \u2248 250 perplexity (Kim et al., 2019), and while scaling neural PCFGs to thousands of states via a low-rank parameterization can improve perplexity to \u2248 170 (Yang et al., 2022), this still lags behind a similarly-sized HMM, which obtains \u2248 130 perplexity (Chiu et al., 2021), despite the fact that HMMs are a subclass of PCFGs\nThis work proposes SimplePCFG, a simple PCFG formalism with independent left and right productions. We find that this simple PCFG scales\n\u2217Equal contribution. Code: https://github.com/sustcsonglin/TN-PCFG.\nmore effectively (in terms of both language modeling and unsupervised parsing) than previous approaches which scale PCFGs by factorizing the rule probability tensor into low-rank components (Yang et al., 2021b, 2022). In particular, we find that simple PCFGs can obtain significantly lower perplexity in language modeling while achieving higher unsupervised parsing performance compared to lowrank PCFGs with a similar number of nonterminals, achieving a near state-of-the-art unsupervised parsing performance on the Penn Treebank with an F1 of 65.1. We further describe a hardware-efficient IO-aware implementation of the inside algorithm, dubbed FlashInside, to facilitate scalable learning of simple PCFGs."
        },
        {
            "heading": "2 Simple PCFGs",
            "text": "A PCFG can be defined by a 6-tuple G = (S,N ,P,\u03a3,R, \u03c0), where S is the distinguished start symbol, N/P/\u03a3 are a finite set of nonterminal/pre-terminal/terminal symbols,1 R is a set of production rules of the form,\nS \u2192 A, A \u2208 N A \u2192 BC, A \u2208 N , B,C \u2208 N \u222a P T \u2192 w, T \u2208 P, w \u2208 \u03a3\nand \u03c0 : R \u2192 [0, 1] maps rules to their associated probabilities. In simple PCFGs, we decompose \u03c0A\u2192BC into \u03c0B\u21b6A \u00b7 \u03c0A\u21b7C , effectively assuming that left and right children are generated independently. 2 We denote L,R \u2208 R|N |\u00d7|N | as the matrix representation of \u03c0B\u21b6A and \u03c0A\u21b7C , and apply a neural parameterization over these matrices to compute the rule probabilities (Kim et al., 2019). See Appendix A for details.\n1For brevity we do not distinguish between N and P for the rest of the paper.\n2This formalism has been discussed in Hsu et al. (2012), i.e., PCFG-I. We also experimented with PCFG-IE, where L = R, but found it necessary to distinguish between L and R to achieve good performance.\nComparing simple vs. low-rank PCFGs. The previous approach to scaling HMMs and PCFGs to thousands of nontermals is parameterizing the rule probability tensor T \u2208 R|N |\u00d7|N |\u00d7|N | to be lowrank (Chiu et al., 2021; Yang et al., 2021b, 2022). Low-rank PCFGs can be viewed as introducing a new latent variable, namely a \u201crank variable\u201d R, to decompose \u03c0A\u2192BC into \u2211 R \u03c0A\u2192R\u03c0B\u21b6R\u03c0R\u21b7C , as shown in Fig. 1, where the tensor/matrix representations of \u03c0A\u2192BC , \u03c0A\u2192R, \u03c0B\u21b6R, \u03c0R\u21b7C are T,U,V,W, respectively. Yang et al. (2022, Sect. 4.2) show that a low-rank PCFG can be reparameterized as a simple PCFG with independent left/right productions by marginalizing nonterminal variables and viewing the rank variables as new nonterminal variables. As such, low-rank PCFGs parameterize L,R in a more restrictive manner: L = VUT ,R = WUT . We speculate that the shared UT would restrict the expressiveness of lowrank PCFGs and thus hinder optimization, which motivates our simple PCFGs."
        },
        {
            "heading": "3 A Hardware-efficient Inside Algorithm",
            "text": ""
        },
        {
            "heading": "3.1 The inside algorithm for simple PCFGs",
            "text": "The inside algorithm for simple PCFGs has the following recursive formula:\n\u03b2Aij = \u2211\nB,C\u2208N \u03c0B\u21b6A \u00b7 \u03c0A\u21b7C \u2211 i<k<j \u03b2Bik \u00b7 \u03b2Ckj\n= \u2211\ni<k<j (\u2211 B\u2208N \u03c0B\u21b6A \u00b7 \u03b2Bik ) \ufe38 \ufe37\ufe37 \ufe38\n\u03b7Aik\n(\u2211 C\u2208N \u03c0A\u21b7C \u00b7 \u03b2Ckj ) \ufe38 \ufe37\ufe37 \ufe38\n\u03b6Akj\nwhere \u03b2Aij is the inside probability for span (A, i, j) with the base case \u03b2Aii = \u03c0A\u2192wi . We cache \u03b7 A ij , \u03b6 A ij\nto avoid repeated computation, similar to Cohen et al. (2013) and Yang et al. (2022). The resulting complexity is O(l3|N | + l2|N |2) where l is sentence length.\nVector form. We abuse the notation to have \u03b2ij ,\u03b7ij , \u03b6ij \u2208 R|N |. Then we can write \u03b2ij =\u2211\ni<k<j \u03b7ik \u2299 \u03b6kj and \u03b7ij = L\u03b2ij , \u03b6ij = R\u03b2ij , where \u2299 is the element-wise product."
        },
        {
            "heading": "3.2 FlashInside",
            "text": "It is necessary to implement the inside algorithm on GPUs efficiently to facilitate scaling of simple PCFGs. We introduce FlashInside, a hardwareefficient IO-aware implementation of the inside algorithm in the spirit of FlashAttention (Dao et al., 2022). FlashInside comprises of four main techniques:\nSpan-level parallelism. Given the span width w, the inside probability vector \u03b2i(i+w) could be computed in parallel for different starting position i (Yi et al., 2011, Sect. 4.2).\nThe log-einsum-exp trick. To improve numerical stability, it is common to use the \u201clog-sum-exp\u201d trick. For example, letting oij = log\u03b2ij ,aij = log \u03b7ij , bij = log \u03b6ij , we have\noij = x \u22c6 + log \u2211 i<k<j exp(aik + bkj \u2212 x\u22c6) (1)\nwhere x\u22c6 = maxi<k<j(aik + bkj) \u2208 R|N |. Using log-sum-exp could be expensive when computing aij and bij , so we resort to the \u201clog-einsum-exp\u201d trick (Peharz et al., 2020, Sect. 3.2),\naij = x \u2020 + log ( L exp(oij \u2212 x\u2020) ) bij = x \u2020 + log ( R exp(oij \u2212 x\u2020)\n) where x\u2020 = maxoij \u2208 R.3 This allows us to leverage matrix multiplication operators, which are highly optimized on GPUs, to compute L exp(oij \u2212 x\u2020) and R exp(oij \u2212 x\u2020).\nKernel fusion. The above computation involves many element-wise operations and is thus memorybounded. Loading and storing these vectors multiple times would cause significant IO-cost (Dao et al., 2022). We reduce the IO-cost by fusing these operations whenever possible. Concretely, when\n3We abuse the notation for broadcasting vector-scalar addition/subtraction.\ncomputing exp(oij\u2212x\u2020), we perform max,\u2212, exp in the same kernel that computes oij ; and we compute aij , bij at once by\n[aij |bij ] = x\u2020 + log ( [L|R] exp(oij \u2212 x\u2020) ) followed by fused element-wise log and addition operations.\nRecomputation. While it possible to rely on automatic differentiation (AD) to backpropagate through the inside algorithm (Eisner, 2016), this can be memory-inefficient since AD would save all the intermediate results in the DP computation, which are not needed. For example, in Eq. 1 the partial differentiation between oij and aik, bkj is given by,\n\u03b4oij \u03b4aik = \u03b4oij \u03b4bkj = exp (aik + bkj \u2212 x\u22c6)\u2211 k\u2032 exp ( aik\u2032 + bk\u2032,j \u2212 x\u22c6 ) =\nexp (aik + bkj \u2212 x\u22c6) exp (oij \u2212 x\u22c6) = exp (aik + bkj \u2212 oij)\nIn the backward pass, we could recompute exp(aik + bkj \u2212 oij) without the need to store exp(aik + bkj \u2212 x\u22c6) in the forward pass, thus saving memory 4. We found that this manual backpropagation led to a slight decrease in running speed but greatly increased memory savings, and thus use it for all our experiments.\nSpeed comparison Table 1 shows running speed and memory footprint measured under a single NVIDIA-A40 GPU, where we compare against the standard log-sum-exp implementation of the inside algorithm which only leverages span-level\n4This is also known as gradient checkpointing (Chen et al., 2016).\nparallelism (e.g., in Torch-Struct (Rush, 2020)). We can see that the use of log-einsum-exp trick significantly accelerate the running speed and reduce the memory footprint. FlashInside uses the kernel fusion and recomputation techniques in addition, resulting in further improvement, especially on larger grammars and longer sentences."
        },
        {
            "heading": "4 Experimental Setup",
            "text": "Datasets. We conduct experiments on the Penn Treebank (PTB) (Marcus et al., 1993) dataset with two different splits: one for language modeling (Mikolov et al., 2011), and one for unsupervised parsing (Shen et al., 2018, 2019). We also evaluate our model on Chinese Treebank 5.1 (CTB) (Xue et al., 2005) and German and French treebanks from SPRML (Seddah et al., 2014).\nBaselines. Our HMM baselines include neural HMM (NHMM) (Chiu et al., 2021) , LHMM (Chiu et al., 2021), and Rank HMM (Yang et al., 2022). Our PCFG baselines include Neural/Compound PCFG (N/C-PCFG) (Kim et al., 2019), TN-PCFG (Yang et al., 2021b) and Rank PCFG (Yang et al., 2022). \u2020 denotes our reimplementation. For Rank PCFG we use rank size 4096. See Appendix B for more implementation details.\nEvaluation. We use perplexity (ppl) to evaluate language modeling and sentence-level F1 (S-F1) (Kim et al., 2019) to evaluate unsupervised parsing."
        },
        {
            "heading": "5 Results",
            "text": "We compare our simple neural PCFG (SN-PCFG) to the baseline models. Table 2 shows the language modeling performance on PTB. SN-PCFG obtains significantly lower perplexity than Rank PCFG, and outperforms similarly-sized HMMs. This indicates that simple PCFGs provide a viable path\ntowards scaling PCFGs, despite the strict independence assumption.\nTable 4 and 3 show the unsupervised parsing performance. SN-PCFG consistently outperforms Rank PCFG in S-F1 while obtaining much lower perplexity. We also experiment with the compound version of simple PCFGs (SC-PCFG) which uses an auxiliary sentence-level vector to model sentence-level properties and uses variational infer-\nence for learning (see the appendix for the full parameterization). We find that SN-PCFG performs better on English while SC-PCFG achieves the best parsing performance in languages other than English. We remark that the compound parameterization is reported to be not compatible with low-rank parameterization probably due to optimization issues (Yang et al., 2021b). This work successfully scales compound PCFGs to thousands of states, which could be useful in some settings such as multimodal grammar induction which condition on vector representations of side information (Zhao and Titov, 2020; Jin and Schuler, 2020; Zhang et al., 2021, 2022; Li et al., 2022).\nSimple PCFG vs. Neural PCFG. Despite the better scalablity of simple PCFGs, we find that under the same number of nonterminal (i.e., 128), SN-PCFG expectedly underperforms N-PCFG in both language modeling and unsupervised parsing (Table 4) due to the stronger independence assumption that is necessary for scaling. Nevertheless, N-PCFG does not scale well and (for example) runs into memory issues even with just 256 nonterminals, while SN-PCFG can scale to 8192 nonterminals on a single A40 GPU.\nSimple PCFG vs. Rank PCFG. Recall that the rank PCFG and simple PCFG share an identical dynamic programming structure. The rank variable in the rank PCFG amounts to the nonterminal variable in the simple PCFG. Consequently, if we align the rank size in the rank PCFG with the nonterminal size in the simple PCFG, we achieve parity in terms of memory footprint and computational speed within the dynamic programming computation. In our experiments, we opt for a rank size of 4096 in the low-rank PCFG. The results, as presented in tables 2-4, showcase the worse perfor-\nmance of the rank PCFG when compared to SNPCFG with 4096 nonterminals. Interestingly, this work was motivated by our observation that merely augmenting the rank size of PCFG falls short in bridging the performance gap between HMMs and PCFGs in language modeling. This resulted in our exploring alternative parameterizations, culminating in the straightforward independent left/right productions based parameterization which yields superior results in both language modeling and unsupervised parsing."
        },
        {
            "heading": "6 Related Work",
            "text": "Independence assumptions are frequently made in grammar learning for tractability and scalability. Simple PCFGs assume independent generation of left and right children, thus resembling splithead dependency grammars (Eisner, 1996; Collins, 1997; Eisner and Satta, 1999; Paskin, 2001; Klein and Manning, 2004). We have shown that trading expressiveness (of grammar formalism) for scalablity is beneficial, and this idea could be applied to other complex grammar formalism of high parsing complexity, such as mildly context-sensitive grammars (Yang et al., 2023), synchronized grammars (Kim, 2021; Wang et al., 2022; Friedman et al., 2022; Lou and Tu, 2023) and lexicalized grammars (Zhu et al., 2020; Yang et al., 2021a)."
        },
        {
            "heading": "7 Conclusion",
            "text": "In this work we explore a simpler variant of PCFGs (SimplePCFG) that shows better scaling properties than previous approaches in terms of both language modeling and unsupervised parsing performance. We also introduce a hardware-aware version of the inside algorithm (FlashInside) which improves over existing vectorized GPU implementations.\nLimitations\nWe have successfully bridged the gap between HMMs and PCFGs in language modeling. However, a significant disparity remains between PCFGs and neural models like Transformers. While we recognize the potential of our hardwareefficient inside algorithm implementation for conducting large-scale language modeling experiments, our aim is not to position PCFGs as direct rivals to neural models, given the intrinsic limitations arising from PCFG\u2019s strong context-free independence assumption. Our main objective is to enhance unsupervised PCFG learning, with a cen-\ntral focus on optimizing the sentence log marginal likelihood objective function.\nSimple PCFGs, due to their restrictive grammar nature, require many nonterminals for optimal performance. However, we observe diminishing returns while scaling up simple PCFGs. This phenomena is common in scaling up latent-variable models and future work might consider leveraging the technique from Liu et al. (2023) to mitigate this issue.\nWhen scaling up simple PCFGs, the computation of grammar rule probabilities could also be expensive, especially when constructing the emission probability matrix of size R|P|\u00d7|V|. Compound parameterization exacerbates this issue since each sentence will have its own set of grammar rule probabilities. Consequently, we only used up to 2048 nonterminals in our SC-PCFG experiments.\nAcknowlgedment\nThis study was supported by the National Natural Science Foundation of China (61976139) and by funds from an MIT-IBM Watson AI Lab grant."
        },
        {
            "heading": "A Neural Parameterization",
            "text": "We present the neural parameterization of our simple neural pcfg and simple compound pcfg. We use EG = {wN |N \u2208 {S} \u222a N \u222a P} to denote symbol embeddings for simple PCFG and use function gr(\u00b7; \u03b8) = \u03c0r parameterized by \u03b8 to denote neural parameterization function.\nSimple Neural PCFG We use neural networks to parameterize these rule probabilities. The neural parameterization of all rule probabilities \u03c0r starts from corresponding symbol embeddings in EG . Parameterization function gr(\u00b7; \u03b8) can be formulated as gr(EG ; \u03b8) in simple neural PCFG, which takes from one of these forms:\n\u03c0S\u2192A = exp\n( u\u22a4Af1 (wS) )\u2211 A\u2032\u2208N exp ( u\u22a4A\u2032f1 (wS)\n) , \u03c0B\u21b6A = exp ( f2 ( w\u22a4B ) f3 (wA) )\u2211 B\u2032\u2208N\u222aP exp ( f2 ( w\u22a4B\u2032 ) f3 (wA)\n) , \u03c0A\u21b7C = exp ( f4 ( w\u22a4C ) f3 (wA) )\u2211 C\u2032\u2208N\u222aP exp ( f4 ( w\u22a4C\u2032 ) f3 (wA)\n) , \u03c0T\u2192w = exp ( u\u22a4wf5 (wT ) )\u2211 w\u2032\u2208\u03a3 exp ( u\u22a4w\u2032f5 (wT )\n) where f1, f5 are two-layer residual networks; f2, f3, f4 are one-linear-layer with ReLU activation function and residual connections. We highlight the usefulness of sharing symbol embedding\nacross different grammar rules; and the use of residual connections in f2, f3, f4.\nSimple Compound PCFG Similar to compound PCFGs, we parameterize our simple compound PCFGs with a latent variable z \u223c p(z). We replace three rule probabilities \u03c0B\u21b6A, \u03c0B\u21b6A, and \u03c0T\u2192w with \u03c0r = gr(z,EG ; \u03b8), while leaving the remaining rule probabilities as gr(EG ; \u03b8). The function \u03c0r = gr(z,EG ; \u03b8) can take one of the following forms:\n\u03c0B\u21b6A = exp\n( f2 ( w\u22a4B ) f \u20323 ([wA; z]) )\u2211 B\u2032\u2208N\u222aP exp ( f2 ( w\u22a4B\u2032 ) f \u20323 ([wA; z])\n) , \u03c0A\u21b7C = exp ( f4 ( w\u22a4C ) f \u20323 ([wA; z]) )\u2211 C\u2032\u2208N\u222aP exp ( f4 ( w\u22a4C\u2032 ) f \u20323 ([wA; z])\n) , \u03c0T\u2192w = exp ( u\u22a4wf \u2032 5 ([wT ; z]) )\u2211 w\u2032\u2208\u03a3 exp ( u\u22a4w\u2032f \u2032 5 ([wT ; z])\n) where f \u20323, f \u2032 5 are neural networks which are sim-\nilar to f3, f5 but with different input shape.\nB Implementation Details\nWe follow Mikolov et al. (2011) to preprocess PTB language modeling split data. For other datasets, we use the preprocessing from Yang et al. (2021b).\nWe implement our model based on the codebase of Yang et al. (2022). And most hyperparameters follow their settings. We use Xavier normal initialization to initialize neural networks. Our model is optimized by Adam optimizer with \u03b21 = 0.75, \u03b22 = 0.999, and learning rate 0.002. All dimensions of symbol embeddings are set to 512. Our FlashInside is implemented with Triton (Tillet et al., 2019) 5, an open-source python-like GPU programming language. For the latent variable z in SC-PCFG, we follow the implementation of Kim et al. (2019) which applies a max-pooling layer over the hidden states of the BiLSTM to obtain sentence representation and generates 64- dimensional mean vectors \u00b5(w) and log-variances log\u03c3(w) by leveraging an affine layer.\nWe run all experiments on NVIDIA V100 and NVIDIA A40. All experimental results are averaged from four runs.\n5https://github.com/openai/triton"
        }
    ],
    "title": "Simple Hardware-Efficient PCFGs with Independent Left and Right Productions",
    "year": 2023
}