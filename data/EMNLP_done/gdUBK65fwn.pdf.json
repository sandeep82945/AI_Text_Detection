{
    "abstractText": "The success of large language models (LLMs), like GPT-4 and ChatGPT, has led to the development of numerous cost-effective and accessible alternatives that are created by finetuning open-access LLMs with task-specific data (e.g., ChatDoctor) or instruction data (e.g., Alpaca). Among the various fine-tuning methods, adapter-based parameter-efficient fine-tuning (PEFT) is undoubtedly one of the most attractive topics, as it only requires fine-tuning a few external parameters instead of the entire LLMs while achieving comparable or even better performance. To enable further research on PEFT methods of LLMs, this paper presents LLMAdapters, an easy-to-use framework that integrates various adapters into LLMs and can execute these adapter-based PEFT methods of LLMs for different tasks. The framework includes state-of-the-art open-access LLMs such as LLaMA, BLOOM, and GPT-J, as well as widely used adapters such as Series adapters, Parallel adapter, Prompt-based learning and Reparametrization-based methods. Moreover, we conduct extensive empirical studies on the impact of adapter types, placement locations, and hyper-parameters to the best design for each adapter-based methods. We evaluate the effectiveness of the adapters on fourteen datasets from two different reasoning tasks, Arithmetic Reasoning and Commonsense Reasoning. The results demonstrate that using adapter-based PEFT in smaller-scale LLMs (7B) with few extra trainable parameters yields comparable, and in some cases superior, performance to powerful LLMs (175B) in zero-shot inference on both reasoning tasks. The code and datasets can be found in https://github. com/AGI-Edgerunners/LLM-Adapters.",
    "authors": [
        {
            "affiliations": [],
            "name": "Zhiqiang Hu"
        },
        {
            "affiliations": [],
            "name": "Lei Wang"
        },
        {
            "affiliations": [],
            "name": "Yihuai Lan"
        },
        {
            "affiliations": [],
            "name": "Wanyu Xu"
        },
        {
            "affiliations": [],
            "name": "Ee-Peng Lim"
        },
        {
            "affiliations": [],
            "name": "Lidong Bing"
        },
        {
            "affiliations": [],
            "name": "Xing Xu"
        },
        {
            "affiliations": [],
            "name": "Soujanya Poria Roy"
        },
        {
            "affiliations": [],
            "name": "Ka-Wei Lee"
        }
    ],
    "id": "SP:0e54bbad6aaf25350ba298e891e315215b88f2dd",
    "references": [
        {
            "authors": [
                "Gupta"
            ],
            "title": "Intrinsic dimensionality explains the",
            "year": 2020
        },
        {
            "authors": [
                "Yonatan Bisk",
                "Rowan Zellers",
                "Ronan Le Bras",
                "Jianfeng Gao",
                "Yejin Choi."
            ],
            "title": "Piqa: Reasoning about physical commonsense in natural language",
            "venue": "ThirtyFourth AAAI Conference on Artificial Intelligence.",
            "year": 2020
        },
        {
            "authors": [
                "Jiaao Chen",
                "Aston Zhang",
                "Xingjian Shi",
                "Mu Li",
                "Alex Smola",
                "Diyi Yang."
            ],
            "title": "Parameterefficient fine-tuning design spaces",
            "venue": "arXiv preprint arXiv:2301.01821.",
            "year": 2023
        },
        {
            "authors": [
                "Christopher Clark",
                "Kenton Lee",
                "Ming-Wei Chang",
                "Tom Kwiatkowski",
                "Michael Collins",
                "Kristina Toutanova."
            ],
            "title": "BoolQ: Exploring the surprising difficulty of natural yes/no questions",
            "venue": "Proceedings of the 2019 Conference of the North American Chap-",
            "year": 2019
        },
        {
            "authors": [
                "Peter Clark",
                "Isaac Cowhey",
                "Oren Etzioni",
                "Tushar Khot",
                "Ashish Sabharwal",
                "Carissa Schoenick",
                "Oyvind Tafjord."
            ],
            "title": "Think you have solved question answering? try arc, the ai2 reasoning challenge",
            "venue": "arXiv:1803.05457v1.",
            "year": 2018
        },
        {
            "authors": [
                "Karl Cobbe",
                "Vineet Kosaraju",
                "Mohammad Bavarian",
                "Jacob Hilton",
                "Reiichiro Nakano",
                "Christopher Hesse",
                "John Schulman."
            ],
            "title": "Training verifiers to solve math word problems",
            "venue": "arXiv preprint arXiv:2110.14168.",
            "year": 2021
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "arXiv preprint arXiv:1810.04805.",
            "year": 2018
        },
        {
            "authors": [
                "Ali Edalati",
                "Marzieh S. Tahaei",
                "Ivan Kobyzev",
                "V. Nia",
                "James J. Clark",
                "Mehdi Rezagholizadeh."
            ],
            "title": "Krona: Parameter efficient tuning with kronecker adapter",
            "venue": "ArXiv, abs/2212.10650.",
            "year": 2022
        },
        {
            "authors": [
                "Cheng Fu",
                "Hanxian Huang",
                "Xinyun Chen",
                "Yuandong Tian",
                "Jishen Zhao."
            ],
            "title": "Learn-to-share: A hardware-friendly transfer learning framework exploiting computation and parameter sharing",
            "venue": "Proceedings of the 38th International Conference on Ma-",
            "year": 2021
        },
        {
            "authors": [
                "Junxian He",
                "Chunting Zhou",
                "Xuezhe Ma",
                "Taylor BergKirkpatrick",
                "Graham Neubig."
            ],
            "title": "Towards a unified view of parameter-efficient transfer learning",
            "venue": "arXiv preprint arXiv:2110.04366.",
            "year": 2021
        },
        {
            "authors": [
                "Junxian He",
                "Chunting Zhou",
                "Xuezhe Ma",
                "Taylor BergKirkpatrick",
                "Graham Neubig."
            ],
            "title": "Towards a unified view of parameter-efficient transfer learning",
            "venue": "International Conference on Learning Representations.",
            "year": 2022
        },
        {
            "authors": [
                "Shwai He",
                "Liang Ding",
                "Daize Dong",
                "Jeremy Zhang",
                "Dacheng Tao."
            ],
            "title": "SparseAdapter: An easy approach for improving the parameter-efficiency of adapters",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2022, pages 2184\u20132190,",
            "year": 2022
        },
        {
            "authors": [
                "James Henderson",
                "Sebastian Ruder"
            ],
            "title": "Compacter: Efficient low-rank hypercomplex adapter layers",
            "venue": "In Advances in Neural Information Processing Systems",
            "year": 2021
        },
        {
            "authors": [
                "Mohammad Javad Hosseini",
                "Hannaneh Hajishirzi",
                "Oren Etzioni",
                "Nate Kushman."
            ],
            "title": "Learning to solve arithmetic word problems with verb categorization",
            "venue": "EMNLP, pages 523\u2013533.",
            "year": 2014
        },
        {
            "authors": [
                "Neil Houlsby",
                "Andrei Giurgiu",
                "Stanislaw Jastrzebski",
                "Bruna Morrone",
                "Quentin de Laroussilhe",
                "Andrea Gesmundo",
                "Mona Attariyan",
                "Sylvain Gelly."
            ],
            "title": "Parameter-efficient transfer learning for nlp",
            "venue": "International Conference on Machine Learning.",
            "year": 2019
        },
        {
            "authors": [
                "Edward J. Hu",
                "Yelong Shen",
                "Phillip Wallis",
                "Zeyuan Allen-Zhu",
                "Yuanzhi Li",
                "Shean Wang",
                "Weizhu Chen."
            ],
            "title": "Lora: Low-rank adaptation of large language models",
            "venue": "ArXiv, abs/2106.09685.",
            "year": 2021
        },
        {
            "authors": [
                "Takeshi Kojima",
                "Shixiang Shane Gu",
                "Machel Reid",
                "Yutaka Matsuo",
                "Yusuke Iwasawa."
            ],
            "title": "Large language models are zero-shot reasoners",
            "venue": "arXiv preprint arXiv:2205.11916.",
            "year": 2022
        },
        {
            "authors": [
                "Rik Koncel-Kedziorski",
                "Hannaneh Hajishirzi",
                "Ashish Sabharwal",
                "Oren Etzioni",
                "Siena Dumas Ang."
            ],
            "title": "Parsing algebraic word problems into equations",
            "venue": "Transactions of the Association for Computational Linguistics, 3:585\u2013597.",
            "year": 2015
        },
        {
            "authors": [
                "Rik Koncel-Kedziorski",
                "Subhro Roy",
                "Aida Amini",
                "Nate Kushman",
                "Hannaneh Hajishirzi."
            ],
            "title": "MAWPS: A math word problem repository",
            "venue": "Proceedings of NAACL, pages 1152\u20131157.",
            "year": 2016
        },
        {
            "authors": [
                "Brian Lester",
                "Rami Al-Rfou",
                "Noah Constant."
            ],
            "title": "The power of scale for parameter-efficient prompt tuning",
            "venue": "ArXiv, abs/2104.08691.",
            "year": 2021
        },
        {
            "authors": [
                "Xiang Lisa Li",
                "Percy Liang."
            ],
            "title": "Prefix-tuning: Optimizing continuous prompts for generation",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language",
            "year": 2021
        },
        {
            "authors": [
                "Wang Ling",
                "Dani Yogatama",
                "Chris Dyer",
                "Phil Blunsom."
            ],
            "title": "Program induction by rationale generation: Learning to solve and explain algebraic word problems",
            "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics",
            "year": 2017
        },
        {
            "authors": [
                "Sourab Mangrulkar",
                "Sylvain Gugger",
                "Lysandre Debut",
                "Younes Belkada",
                "Sayak Paul."
            ],
            "title": "Peft: Stateof-the-art parameter-efficient fine-tuning methods",
            "venue": "https://github.com/huggingface/peft.",
            "year": 2022
        },
        {
            "authors": [
                "Yuning Mao",
                "Lambert Mathias",
                "Rui Hou",
                "Amjad Almahairi",
                "Hao Ma",
                "Jiawei Han",
                "Wen tau Yih",
                "Madian Khabsa."
            ],
            "title": "Unipelt: A unified framework for parameter-efficient language model tuning",
            "venue": "ArXiv, abs/2110.07577.",
            "year": 2021
        },
        {
            "authors": [
                "Niklas Muennighoff",
                "Thomas Wang",
                "Lintang Sutawika",
                "Adam Roberts",
                "Stella Biderman",
                "Teven Le Scao",
                "M Saiful Bari",
                "Sheng Shen",
                "Zheng-Xin Yong",
                "Hailey Schoelkopf"
            ],
            "title": "Crosslingual generalization through multitask finetuning",
            "year": 2022
        },
        {
            "authors": [
                "OpenAI."
            ],
            "title": "Introducing chatgpt",
            "venue": "https://openai. com/blog/chatgpt.",
            "year": 2022
        },
        {
            "authors": [
                "OpenAI."
            ],
            "title": "GPT-4 technical report",
            "venue": "CoRR, abs/2303.08774.",
            "year": 2023
        },
        {
            "authors": [
                "Arkil Patel",
                "Satwik Bhattamishra",
                "Navin Goyal"
            ],
            "title": "Are NLP models really able to solve simple math word problems",
            "venue": "In Proceedings of NAACL,",
            "year": 2021
        },
        {
            "authors": [
                "Jonas Pfeiffer",
                "Ivan Vulic",
                "Iryna Gurevych",
                "Sebastian Ruder."
            ],
            "title": "Mad-x: An adapter-based framework for multi-task cross-lingual transfer",
            "venue": "Conference on Empirical Methods in Natural Language Processing.",
            "year": 2020
        },
        {
            "authors": [
                "Chengwei Qin",
                "Aston Zhang",
                "Zhuosheng Zhang",
                "Jiaao Chen",
                "Michihiro Yasunaga",
                "Diyi Yang"
            ],
            "title": "Is chatgpt a general-purpose natural language processing task solver? arXiv preprint arXiv:2302.06476",
            "year": 2023
        },
        {
            "authors": [
                "Yujia Qin",
                "Xiaozhi Wang",
                "Yusheng Su",
                "Yankai Lin",
                "Ning Ding",
                "Jing Yi",
                "Weize Chen",
                "Zhiyuan Liu",
                "Juanzi Li",
                "Lei Hou"
            ],
            "title": "Exploring universal intrinsic task subspace via prompt tuning. arXiv e-prints, pages arXiv\u20132110",
            "year": 2021
        },
        {
            "authors": [
                "Subhro Roy",
                "Dan Roth."
            ],
            "title": "Solving general arithmetic word problems",
            "venue": "arXiv preprint arXiv:1608.01413.",
            "year": 2016
        },
        {
            "authors": [
                "Keisuke Sakaguchi",
                "Ronan Le Bras",
                "Chandra Bhagavatula",
                "Yejin Choi."
            ],
            "title": "Winogrande: An adversarial winograd schema challenge at scale",
            "venue": "Communications of the ACM, 64(9):99\u2013106.",
            "year": 2021
        },
        {
            "authors": [
                "Maarten Sap",
                "Hannah Rashkin",
                "Derek Chen",
                "Ronan LeBras",
                "Yejin Choi."
            ],
            "title": "Socialiqa: Commonsense reasoning about social interactions",
            "venue": "arXiv preprint arXiv:1904.09728.",
            "year": 2019
        },
        {
            "authors": [
                "Yongliang Shen",
                "Kaitao Song",
                "Xu Tan",
                "Dongsheng Li",
                "Weiming Lu",
                "Yueting Zhuang."
            ],
            "title": "Hugginggpt: Solving AI tasks with chatgpt and its friends in huggingface",
            "venue": "CoRR, abs/2303.17580.",
            "year": 2023
        },
        {
            "authors": [
                "Yi-Lin Sung",
                "Jaemin Cho",
                "Mohit Bansal."
            ],
            "title": "Lst: Ladder side-tuning for parameter and memory efficient transfer learning",
            "venue": "ArXiv, abs/2206.06522.",
            "year": 2022
        },
        {
            "authors": [
                "Rohan Taori",
                "Ishaan Gulrajani",
                "Tianyi Zhang",
                "Yann Dubois",
                "Xuechen Li",
                "Carlos Guestrin",
                "Percy Liang",
                "Tatsunori B. Hashimoto."
            ],
            "title": "Stanford alpaca: An instruction-following llama model",
            "venue": "https:// github.com/tatsu-lab/stanford_alpaca.",
            "year": 2023
        },
        {
            "authors": [
                "Hugo Touvron",
                "Thibaut Lavril",
                "Gautier Izacard",
                "Xavier Martinet",
                "Marie-Anne Lachaux",
                "Timoth\u00e9e Lacroix",
                "Baptiste Rozi\u00e8re",
                "Naman Goyal",
                "Eric Hambro",
                "Faisal Azhar"
            ],
            "title": "Llama: Open and efficient foundation language models",
            "year": 2023
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin."
            ],
            "title": "Attention is all you need",
            "venue": "Advances in neural information processing systems, pages 5998\u20136008.",
            "year": 2017
        },
        {
            "authors": [
                "Tu Vu",
                "Brian Lester",
                "Noah Constant",
                "Rami Al-Rfou",
                "Daniel Cer."
            ],
            "title": "Spot: Better frozen model adaptation through soft prompt transfer",
            "venue": "arXiv preprint arXiv:2110.07904.",
            "year": 2021
        },
        {
            "authors": [
                "Ben Wang",
                "Aran Komatsuzaki."
            ],
            "title": "GPT-J6B: A 6 Billion Parameter Autoregressive Language Model",
            "venue": "https://github.com/kingoflolz/ mesh-transformer-jax.",
            "year": 2021
        },
        {
            "authors": [
                "Lei Wang",
                "Wanyu Xu",
                "Yihuai Lan",
                "Zhiqiang Hu",
                "Yunshi Lan",
                "Roy Ka-Wei Lee",
                "Ee-Peng Lim."
            ],
            "title": "Planand-solve prompting: Improving zero-shot chain-ofthought reasoning by large language models",
            "venue": "arXiv preprint arXiv:2305.04091.",
            "year": 2023
        },
        {
            "authors": [
                "Yaqing Wang",
                "Subhabrata Mukherjee",
                "Xiaodong Liu",
                "Jing Gao",
                "Ahmed Hassan Awadallah",
                "Jianfeng Gao."
            ],
            "title": "Adamix: Mixture-of-adapter for parameter-efficient tuning of large language models",
            "venue": "ArXiv, abs/2205.12410.",
            "year": 2022
        },
        {
            "authors": [
                "Li Yunxiang",
                "Li Zihan",
                "Zhang Kai",
                "Dan Ruilong",
                "Zhang You."
            ],
            "title": "Chatdoctor: A medical chat model fine-tuned on llama model using medical domain knowledge",
            "venue": "arXiv preprint arXiv:2303.14070.",
            "year": 2023
        }
    ],
    "sections": [
        {
            "text": "The success of large language models (LLMs), like GPT-4 and ChatGPT, has led to the development of numerous cost-effective and accessible alternatives that are created by finetuning open-access LLMs with task-specific data (e.g., ChatDoctor) or instruction data (e.g., Alpaca). Among the various fine-tuning methods, adapter-based parameter-efficient fine-tuning (PEFT) is undoubtedly one of the most attractive topics, as it only requires fine-tuning a few external parameters instead of the entire LLMs while achieving comparable or even better performance. To enable further research on PEFT methods of LLMs, this paper presents LLMAdapters, an easy-to-use framework that integrates various adapters into LLMs and can execute these adapter-based PEFT methods of LLMs for different tasks. The framework includes state-of-the-art open-access LLMs such as LLaMA, BLOOM, and GPT-J, as well as widely used adapters such as Series adapters, Parallel adapter, Prompt-based learning and Reparametrization-based methods. Moreover, we conduct extensive empirical studies on the impact of adapter types, placement locations, and hyper-parameters to the best design for each adapter-based methods. We evaluate the effectiveness of the adapters on fourteen datasets from two different reasoning tasks, Arithmetic Reasoning and Commonsense Reasoning. The results demonstrate that using adapter-based PEFT in smaller-scale LLMs (7B) with few extra trainable parameters yields comparable, and in some cases superior, performance to powerful LLMs (175B) in zero-shot inference on both reasoning tasks. The code and datasets can be found in https://github. com/AGI-Edgerunners/LLM-Adapters."
        },
        {
            "heading": "1 Introduction",
            "text": "Large language models (LLMs), such as ChatGPT (OpenAI, 2022) and GPT-4 (OpenAI, 2023),\n\u2217Corresponding author.\nhave demonstrated unprecedented performance across various natural language processing (NLP) tasks (Qin et al., 2023) and multi-modal tasks (Shen et al., 2023). These LLMs often possess sizes exceeding hundreds of billions of parameters and are closed-source. Consequently, this has spurred the development of accessible and cost-effective alternatives such as LLaMA (Touvron et al., 2023). These alternatives involve fine-tuning open-source LLMs utilizing either task-specific data (e.g., ChatDoctor (Yunxiang et al., 2023)) or instructional data (e.g., Alpaca (Taori et al., 2023)). However, full-model fine-tuning (FFT) is computationally and storage-intensive, thereby presenting significant challenges in practical implementation.\nPrior to the emergence of FFT of LLMs (e.g., LLaMA), a compelling solution called parameterefficient fine-tuning (PEFT) (Houlsby et al., 2019) has been proposed in the NLP field, specifically for pre-trained models (e.g., BERT (Devlin et al., 2018)), offering a promising approach for efficiently fine-tuning LLMs. The advantage of PEFT lies in its ability to fine-tune only a small set of external parameters rather than the entire backbone model while still achieving comparable or even superior performance (Mangrulkar et al., 2022). Moreover, PEFT can effectively mitigate catastrophic forgetting in comparison to FFT (Wang et al., 2022). As shown in Table 1, the advantage of PEFT has resulted in the developing of diverse PEFT modules, encompassing series adapters (Houlsby et al., 2019; Wang et al., 2022; He et al., 2022b; Fu et al., 2021), parallel adapters (He et al., 2022a), reparameterizationbased methods (Hu et al., 2021; Edalati et al., 2022), and prompt-based learning methods (Lester et al., 2021; Li and Liang, 2021).\nBy incorporating these PEFT modules into backbone models (i.e., LLMs), we can capitalize on the remarkable capabilities of backbone models without requiring extensive computational resources.\nThis opens up opportunities for a broader range of applications, enabling even those with limited access to high-performance computing to harness the power of LLMs in their specific tasks. Despite the success of PEFT for pre-trained models, it remains unclear which PEFT module, in combination with which layer and hyperparameter configuration, is most suitable for a given task or dataset when meeting LLMs (e.g., LLaMA (Touvron et al., 2023)). Therefore, further investigation is needed to determine the optimal PEFT setup that maximizes performance across different tasks and datasets.\nMotivated by this, in this paper, we conduct a comprehensive empirical study of PEFT of three representative open-source LLMs, including BLOOM (Muennighoff et al., 2022), GPT-J (Wang and Komatsuzaki, 2021), and LLaMA (Touvron et al., 2023). Specifically, we undertake an empirical study to address the following three research questions: (i) What is the optimal placement and configuration of different PEFT methods? (ii) How\u2019s the performance of different adapters across downstream tasks? And (iii) What are the differences in performance between in-distribution (ID) and out-of-distribution (OOD) scenarios for PEFT methods? The findings of our study are as follows:\n1. The optimal placement for the series adapter, parallel adapter, and LoRA is after the MLP layers, parallel with the MLP layers, and located after both the Attention layers and MLP layers simultaneously, respectively;\n2. Smaller language models with the PEFT approach can attain competitive or superior performance on specific tasks compared to larger language models. For instance, LLaMA-13B with LoRA can outperform GPT-3.5 (>175B) on MultiArith, AddSub, and SingleEq ;\n3. The ID fine-tuned LLaMA-13B with adapters outperforms ChatGPT on commonsense reasoning tasks indicating that smaller language models have the potential to outperform larger language models on specific tasks with ID fine-tuning data.\nOur contributions can be summarized as follows:\n\u2022 We conduct a comprehensive empirical study of various PEFT methods applied in different open-source LLMs.\n\u2022 To facilitate our empirical study, we construct two high-quality training datasets to enhance PEFT performance in math reasoning and commonsense reasoning tasks.\n\u2022 We develop a user-friendly framework, LLM-Adapter, seamlessly integrates diverse adapters into LLMs, empowering researchers to implement adapter-based PEFT methods for a wide range of tasks.\n\u2022 We conduct extensive experiments to answer the three research questions to serve as inspiration for future research."
        },
        {
            "heading": "2 PEFT Overview",
            "text": "In this section, we provide a brief overview of four parameter-efficient fine-tuning (PEFT) methods: prompt-based learning, reparametrization-based methods, series adapters, and parallel adapters. (Li and Liang, 2021; Hu et al., 2021; Houlsby et al., 2019; He et al., 2022a)\nPrompt-based learning. As shown in Figure 1(a), prompt-based learning transforms the discrete optimization problem of finding the optimal hard prompt into a continuous (soft) prompt. To achieve this, Lester et al. (2021) proposed the concept of prompt tuning, where a trainable tensor is added as a prefix to the input embeddings. Another approach called Prefix-Tuning(Li and Liang, 2021) independently explored the addition of soft prompts to the hidden states of all layers. Intrinsic Prompt Tuning (Qin et al., 2021) employs an autoencoder to compress and decompress the soft\nprompt. We take learnable vectors incorporated into the attention layer as an example of promptbased learning, which can be formulated as follows:\nHo = Attn(HiWQ, [PK ;HiWK ], [PV ;HiWV ]), (1)\nwhere Hi \u2208 RT\u00d7d and Ho \u2208 RT\u00d7d are the input and output of the attention layer respectively. Note that T is the maximum input length and d is the vector dimension. PK \u2208 RL\u00d7d and PV \u2208 RL\u00d7d are the learnable vectors for PEFT. L is the number of learnable tokens, which is discussed in the experiment section in detail. Q,K, V denote the query, key, value vectors of th attention module, respectively.\nReparametrization-based method. This type of methods aim to transform network weights using a low-rank technique. This approach effectively reduces the number of trainable parameters while preserving the ability to handle high-dimensional matrices. Intrinsic SAID (Aghajanyan et al., 2020) investigates the intrinsic dimensionality of finetuning within a low-rank subspace. LoRA (Hu et al., 2021) introduces a simple approach to update the parameters of a weight matrix by decomposing it into a product of two low-rank matrices. KronA (Edalati et al., 2022) improves upon the matrix factorization aspect of LoRA by utilizing the Kronecker product in its technique. We take LoRA as an example of Reparametrization-based learning, which can be formulated below:\nHo = HiW0 +Hi\u2206W = HiW0 +HiBA, (2)\nwhere W0 \u2208 Rd\u00d7d can be any pre-trained weight matrix, including weights in the MLP or Attention layer. B \u2208 Rr\u00d7d and A \u2208 Rr\u00d7d are lower-rank matrix intended for covering \u2206W . r \u226a d is an important hyper-parameter for LoRA.\nSeries Adapter. Series adapters involve incorporating additional learnable modules in a sequential manner within a specific sublayer. In their study, Houlsby et al. (2019) proposed integrating fully-connected networks after the attention and FFN layers in the Transformer model (Vaswani et al., 2017). Another finding by Pfeiffer et al. (2020) revealed that achieving comparable performance is possible by inserting the adapter solely after the self-attention layer, instead of using two adapters per transformer block. AdaMix (Wang et al., 2022) introduces a method that utilizes multiple series adapters in a mixture-of-experts (MoE) fashion. Compacter (Henderson et al., 2021) utilizes the Kronecker product, low-rank matrices, and parameter sharing across layers to generate adapter weights. This technique aims to reduce the computational complexity associated with the adapters while maintaining their performance. Series Adapter can be formulated as follows:\nHo \u2190 Ho + f(HoWdown)Wup, (3)\nwhere the output Ho of a specific layer, such as the MLP layer, is first down-projected by Wdown \u2208 Rd\u00d7r to a lower dimension r, and then up-projected back by Wup \u2208 Rr\u00d7d to the original dimension d. f is a non-linear function. We discuss the choice of r in the experiment Section.\nParallel Adapter. Parallel adapters (He et al., 2022a) aim to incorporate additional learnable modules in parallel with distinct sublayers within the backbone model. The parallel adapter can be formulated below:\nHo \u2190 Ho + f(HiWdown)Wup, (4)\nwhere Hi (Ho) is the input (output) of a specific layer. Expanding on this concept, the Multi-head Parallel Adapter takes it a step further by using parallel adapters to modify the outputs of head attention. On the other hand, the Scaled Parallel Adapter is a variant that applies the composition and insertion format of LoRA (Hu et al., 2021) to adapters. Another approach, called Ladder Side-Tuning (Sung et al., 2022), involves training a lightweight ladder side network. This network accepts intermediate activations from the backbone networks through shortcut connections (ladders)."
        },
        {
            "heading": "3 Experiment Setup",
            "text": ""
        },
        {
            "heading": "3.1 Benchmarks",
            "text": "We conduct extensive empirical studies on fourteen benchmark datasets from two categories of reasoning problems: Arithmetic Reasoning: (1) the GSM8K (Cobbe et al., 2021) dataset consists of high quality linguistically diverse grade school math word problems created by human problem writers, (2) the SVAMP (Patel et al., 2021) benchmark consists of one-unknown arithmetic word problems for up-to-4 grade level students by making simple changes to a set of problems from another existing dataset, (3) the MultiArith (Roy and Roth, 2016) dataset of math word problems requiring multiple reasoning steps and operations, (4) the AddSub (Hosseini et al., 2014) dataset of addition and subtraction arithmetic word problems, (5) the AQuA (Ling et al., 2017) dataset of algebraic word problems with natural language rationales, and (6) the SingleEq (Koncel-Kedziorski et al., 2015) dataset of grade-school algebra word problems that map to single equations with varying length; Commonsense Reasoning: (1) the BoolQ (Clark et al., 2019) dataset is a questionanswering dataset for yes/no questions containing 15942 examples. These questions are naturally occurring and generated in unprompted and unconstrained settings, (2) the PIQA (Bisk et al., 2020) dataset of questions with two solutions requiring physical commonsense to answer, (3) the SIQA\n(Sap et al., 2019) focuses on reasoning about people\u2019s actions and their social implications, (4) the HellaSwag dataset of commonsense NLI questions including a context and several endings which complete the context, (5) the WinoGrande (Sakaguchi et al., 2021) dataset is formulated as a fill-in-ablank task with binary options, and the goal is to choose the right option for a given sentence which requires commonsense reasoning, (6) the ARC-c and (7) the ARC-e are the Challenge Set and Easy Set of ARC (Clark et al., 2018) dataset of genuine grade-school level, multiple-choice science questions, and (8) the OBQA dataset contains questions requiring multi-step reasoning, use of additional common and commonsense knowledge, and rich text comprehension. Table 2 shows the dataset statistics."
        },
        {
            "heading": "3.2 Fine-tuning Data Collection",
            "text": "In order to perform fine-tuning on adapters, we acquire two high-quality training datasets specifically designed for math reasoning and commonsense reasoning. Table 2 reveals that only GSM8K and AQuA datasets provide training sets for arithmetic reasoning. To enhance the diversity of our data, we incorporate the training sets from GSM8K, MAWPS, MAWPS-single (KoncelKedziorski et al., 2016), and select 1000 examples from AQuA for the purpose of collecting the finetuning data. However, it is worth noting that the chosen datasets solely offer equations and corresponding answers. In order to augment the reasoning capabilities of our model, particularly in terms of providing step-by-step rationales, we leverage ChatGPT as the teacher model. By utilizing zeroshot chain-of-thought prompts, ChatGPT generates\nreasoning steps. We have included the specific prompt templates used to collect the math reasoning dataset in Appendix A.1. To ensure the quality of the data, we eliminate samples that contain incorrect answers. As a result, we obtain a set of 10K math reasoning samples, referred to as Math10K, which we consider for further analysis and finetuning.\nTo facilitate fine-tuning in the domain of commonsense reasoning, we construct fine-tuning data by formatting the training sets from BoolQ, PIQA, SIQA, HellaSwag, WinoGrande, ARC-e, ARC-c, and OBQA with pre-defined templates. As each dataset in the commonsense reasoning domain entails distinct tasks, we adopt a structured template by initially describing the task\u2019s goal, followed by the corresponding content and answer. The template utilized for creating the fine-tuning data can be found in A.2. Upon completion of this process, we obtain a collection of 170K commonsense reasoning samples, which we refer to as Commonsense170K. These datasets will be made publicly available to encourage further research and exploration in this area."
        },
        {
            "heading": "3.3 Implementations",
            "text": "To facilitate the seamless utilization of PEFT methods in both research and practical applications, we have developed a user-friendly framework, LLMAdapter. LLM-Adapters seamlessly integrates diverse adapters into LLMs, empowering researchers to implement adapter-based PEFT methods for a wide range of tasks. We utilize LLaMA (7B, 13B) (Touvron et al., 2023), BLOOMz (7B) (Muennighoff et al., 2022), and GPT-J (6B) (Wang and Komatsuzaki, 2021) as the base models for our experiments. As for the four categories of PEFT methods, we select Prefix-Tuning (Li and Liang, 2021), Series Adapter (Houlsby et al., 2019), LoRA (Hu et al., 2021), and Parallel adapter (He et al., 2022a) as representative candidates to examine their efficacy. For consistency across all fine-tuning experiments, we maintain a batch size of 16. The learning rate for Prefix-Tuning is set to 3e-2, while the rest of the methods adopt a learning rate of 3e-4. Each of the PEFT methods is fine-tuned for three epochs on the fine-tuning datasets. It is important to note that we fine-tune a single model for either the math or commonsense reasoning task, and subsequently evaluate its performance across all corresponding datasets."
        },
        {
            "heading": "4 Experiment Results",
            "text": ""
        },
        {
            "heading": "4.1 Placement and Configuration",
            "text": "To address the research question, \u201cWhat is the optimal placement and configuration for various types of adapters?\u201d, we employ LLaMA-7B as the base model to assess different adapter settings within the context of the math reasoning task. Our empirical study begins by determining the most effective placement for the Series Adapter, Parallel Adapter, and LoRA. Prefix-Tuning is excluded from this analysis since its placement is predetermined. For the Series Adapter, we explore its placement options after the multi-head attention layers, MLP layers, or both of them. As for the Parallel Adapter and LoRA, we integrate them into the multi-head attention layers, MLP layers, or both of them, in order to assess their respective performances. The detailed results on each dataset are shown in Appendix A.3. Figure 2 shows the average accuracy on math reasoning datasets. We can observe that for the Series Adapter, the best position is to place it after the MLP layers, achieving an average accuracy of 59.5% on the math reasoning datasets. As for the Parallel Adapter, when we place it within the MLP layers, it achieves the best performance of 61.7%. Regarding LoRA, we need to insert it simultaneously into both the Multi-head Attention layers and MLP layers to achieve the best performance of 60%.\nIn order to determine the optimal configuration of various adapters, we conduct an analysis of the most crucial variable for each type of the PEFT methods. We compare the average accuracy on math reasoning datasets. The placement of adapters follows the optimal settings derived from the placement analysis. Regarding Prefix-tuning, we assess the performance with different numbers of virtual tokens (vt) set at [10, 20, 30, 40]. For Series and\nParallel Adapters, we evaluate the impact of the bottleneck size (bn) with values of [64, 128, 256, 512]. For LoRA, we examine the influence of different rank values (r) at [4, 8, 16, 32]. The detailed results for each dataset can be found in Appendix A.4. Figure 3 presents the average accuracy of different variables on math reasoning datasets. It can be noted that when the number of virtual tokens in Prefix-Tuning is set to 10, Prefix-Tuning attains an average accuracy of 42.0% on math reasoning datasets. By configuring the bottleneck dimension to 256, Series and Parallel Adapter demonstrate the highest level of performance. However, when the bottleneck size is increased to 512, the accuracy of both Series and Parallel Adapter decreases. The typical setting for LoRA rank is set to 8, but we\nhave discovered that a larger rank can enhance the performance of LoRA. When the rank is increased from 8 to 32, the average accuracy of LoRA increases from 60.0% 61.9%.\nIn order to enhance the breadth of our research findings, we conducted additional experiments involving the placement of adapters on various LLMs such as GPT-J and BLOOMz. These experiments were conducted across different model sizes, specifically 7B and 13B parameters. Furthermore, we extended our investigation to encompass diverse tasks, including Commonsense tasks. This approach enabled us to generalize our observations across a wider spectrum of LLMs, sizes, and tasks, thus providing a more comprehensive understanding of the adapter placement strategies. The detailed experiment results can be found in Appendix A.3\nBased on our comprehensive placement and configuration analysis, we have determined the optimal settings for each adapter, which will be consistently employed throughout the subsequent experiments.\n\u2022 For Prefix-Tuning, we establish the number of virtual tokens at 10.\n\u2022 For Series and Parallel Adapter, we seamlessly incorporate them into the MLP layers, configuring the bottleneck size to 256.\n\u2022 Regarding LoRA, we seamlessly integrate it into both the Multi-head Attention layers and the MLP layers with rank 32."
        },
        {
            "heading": "4.2 Arithmetic Reasoning",
            "text": "In order to evaluate the effectiveness of adapters on the Arithmetic Reasoning task, we conducted\na study where adapters are fine-tuned on the Math10K dataset and subsequently evaluated on six different math reasoning datasets. As our baseline, we utilize the GPT-3.5 model, specifically the text-Davinci-003 variant, for Zero-shot CoT according to Kojima et al. (2022). The results of the GPT-3.5 model can be found in Wang et al. (2023). Table 3 reports the performance of different PEFT methods and the baseline. On average, the GPT-3.5 model (175B) outperforms adapter-based PEFT LLMs in terms of accuracy. However, for simpler math reasoning datasets such as MultiArith, AddSub, and SingleEq, adapter-based methods like LLaMA-13B with LoRA outperform GPT-3.5. Notably, LLaMA-13B with LoRA achieves an average accuracy of 65.4%, which is approximately 92.8% of the performance exhibited by GPT-3.5. This suggests that with sufficient task-specific training data, adapter-based PEFT of smaller LLMs has the potential to achieve performance comparable to that of extremely large language models. The utilization of adapter-based PEFT yields superior performance by smaller language models compared to GPT-3.5 specifically in simpler tasks such as MultiArith, AddSub, and SingleEq. However, challenges persist in more complex tasks like GSM8K and SVAMP, which require a higher level of language comprehension and proficiency from the underlying base model, thereby resulting in a discernible performance gap. Regarding the different adapters employed, LoRA achieves remarkable performance while utilizing significantly\nfewer trainable parameters. This implies that excessive learnable parameters may not be necessary for task-specific fine-tuning. Overall, these findings demonstrate the potential for adapter-based PEFT of smaller LLMs to achieve high performance on specific tasks with few trainable parameters."
        },
        {
            "heading": "4.3 Commonsense Reasoning",
            "text": "Additionally, we assess the efficacy of various PEFT methods for commonsense reasoning tasks. The adapters undergo fine-tuning using the Commonsense170K dataset. Our baseline models for commonsense reasoning include GPT-3 (175B), PaLM (540B), and ChatGPT. The results for GPT3 and PaLM can be found in the study by Touvron et al. (2023). To evaluate ChatGPT\u2019s performance in commonsense reasoning, we employ the gpt-3.5-turbo API with a zero-shot CoT. The zero-shot CoT prompts align with the template used for collecting our commonsense fine-tuning dataset, as outlined in Appendix A.2. Table 4 presents the performance of the PEFT methods utilizing different LLMs alongside the baselines. Remarkably, LLaMA-13B with Series Adapter, Parallel Adapter, and LoRA outperform all the baselines, including ChatGPT, which has been hailed as the most impressive LLM to date. LLaMA-13B with Parallel Adapter achieves an average accuracy of 81.5%, representing a 4.5% improvement over ChatGPT. It is worth noting that all the training sets from the commonsense reasoning datasets are included in the fine-tuning data Commonsense170K. Furthermore, we observe that the performance of the PEFT\nmethods is influenced by the underlying capabilities of the base models. LLaMA-7B and LLaMA13B demonstrate superior commonsense reasoning abilities compared to the BLOOMz and GPT-J models."
        },
        {
            "heading": "4.4 ID and OOD Analysis",
            "text": "When comparing the performance of PEFT methods on math reasoning and commonsense reasoning tasks, we can observe that PEFT methods exhibit more remarkable results in the realm of commonsense reasoning. Moving forward, we will analyze the factors contributing to this phenomenon from both the in-distribution (ID) and out-of-distribution (OOD) perspectives. In the context of commonsense reasoning, the fine-tuning data set, Commonsense170K, encompasses all the training sets from the commonsense reasoning datasets. Notably, PEFT methods have demonstrated the ability to outperform ChatGPT. This observation implies that, by utilizing ID fine-tuning data, smaller language models like LLaMA-13B could surpass larger language models such as ChatGPT and PaLM in specific downstream tasks. However, when considering math reasoning tasks, the fine-tuning data set, Math10K, only includes the training sets of GSM8K and AQuA. In this regard, it has been observed that PEFT methods, particularly LLaMA-13B with LoRA, exhibit superior performance compared to GPT-3.5 on MultiArith, AddSub, and SingleEq. These findings suggest that\nPEFT methods can enhance the math reasoning abilities of LLMs and can be successfully applied to OOD datasets. Nonetheless, when evaluating the performance of PEFT methods on the ID datasets GSM8K and AQuA, a performance gap is still evident compared to GPT-3.5. This discrepancy is likely due to the higher complexity of GSM8K and AQuA datasets in terms of math reasoning, while the reasoning capabilities of smaller LLMs remain limited. Consequently, identifying strategies to improve the performance of PEFT methods on complex math reasoning tasks represents a potential avenue for future research."
        },
        {
            "heading": "5 Qualitative Study",
            "text": "The previous sections have presented the quantitative analysis. In this section, we will provide qualitative examples to demonstrate the quality of outputs from different models. Table 5 displays a randomly selected question from GSM8K along with the outputs of ChatGPT and LLaMA-13B models using various PEFT methods. More detailed examples can be found in Appendix A.5. ChatGPT demonstrates a comprehensive understanding of the question and generates two steps, \"(36 * 2/3) = 24 square feet\" and \"(24 * 24) = 576 mosaic tiles,\" effectively solving the problem. However, the language understanding ability of LLaMA-13B-Prefix models is limited, leading LLaMA-13B-Prefix to take the wrong direction in the first step. On the other hand, LLaMA-13B\nwith Series Adapter produces a high-quality answer by providing the crucial two steps and performing the correct calculations to obtain the accurate result. Interestingly, LLaMA-13B-Parallel and LLaMA-13B-LoRA generate almost identical rationales. However, LLaMA-13B-Parallel produces an incorrect answer due to a calculation error, stating \"24 sq ft x 24 mosaic tiles per sq ft = 600 mosaic tiles\". In general, when equipped with taskspecific fine-tuning data, smaller language models like LLaMA-13B can generate impressive, highquality answers that are comparable to those produced by ChatGPT."
        },
        {
            "heading": "6 Conclusion",
            "text": "In this paper, we develop a user-friendly framework, LLM-Adapter, seamlessly integrates diverse adapters into LLMs, empowering researchers to implement adapter-based PEFT methods for a wide range of tasks. To evaluate different PEFT methods on downstream tasks, we construct two highquality fine-tuning datasets to enhance PEFT performance on math reasoning and commonsense reasoning tasks. By utilizing the LLM-Adapter toolkit and the constructed fine-tuning datasets, we conduct a comprehensive empirical study and find the answer of research questions on the optimal placement and configuration of different PEFT methods, the impact of adapter architectures, and the influence of ID and OOD scenarios. We hope this work will encourage further research on PEFT methods for LLMs."
        },
        {
            "heading": "7 Limitations",
            "text": "There are two limitations to this work. Firstly, due to constrained computing resources, we were unable to evaluate the performance of larger language models such as LLaMA-33B and LLaMA-65B. It is anticipated that these larger models, possessing enhanced language understanding capabilities, would yield superior performance. Secondly, this paper does not delve into the exploration of combining different adapters. Given the extensive search space associated with the combination of various PEFT methods, we intend to explore this direction in future research endeavors."
        },
        {
            "heading": "A Appendix",
            "text": "A.1 Math Reasoning Prompt Templates\nWe utilize ChatGPT to collect the math reasoning data for fine-tuning. Table 6 show the prompt template used to query ChatGPT. The expression \"Please give the steps\" is employed to guide ChatGPT to generate reasoning steps, thus, we can use the rationale information to fine-tune adapters. \"Give the arabic numerals as the answer.\" is utilized to guide ChatGPT to generate arabic numbers as the final answer making it easier to extract the answer from the outputs.\nA.2 Commonsense Data Templates\nAs each dataset in the commonsense reasoning domain entails distinct tasks, we adopt a structured template by initially describing the task\u2019s goal, followed by the corresponding content and answer. Table 7 shows the templates used to collect commonsense reasoning data for fine-tuning.\nA.3 Placement Analysis\nTable 8 shows the performance regarding the placement of adapters in various locations on math reasoning datasets. The fine-tuning dataset utilized for this study is Math10K. Meanwhile, the base models employed is LLaMA-7B. We can observe that for the Series Adapter, the best position is to place it after the MLP layers, achieving an average accuracy of 59.5% on the math reasoning datasets. As for the Parallel Adapter, when we place it within the MLP layers, it achieves the best performance of 61.7%. Regarding LoRA, we need to insert it simultaneously into both the Multi-head Attention layers and MLP layers to achieve the best performance of 60%.\nIn order to enhance the breadth of our research findings, we conducted additional experiments involving the placement of adapters on various LLMs such as GPT-J and BLOOMz. These experiments were conducted across different model sizes, specifically 7B and 13B parameters. Furthermore, we extended our investigation to encompass diverse tasks, including Commonsense tasks. This approach enabled us to generalize our observations across a wider spectrum of LLMs, sizes, and tasks, thus providing a more comprehensive understanding of the adapter placement strategies.\nA.3.1 Various LLMs\nInitially, our evaluation focused on comparing the placement of adapters in the context of GPT-J-6B and BLOOMz-7B models, specifically concerning Arithmetic Reasoning tasks. The subsequent Table 9 and Table 10 displays the accuracy attained on Arithmetic Reasoning datasets.\nThe bold figures represent the optimal adapter placements for GPT-J-6B and BLOOMz-7B models. Specifically, for both GPT-J-6B and BLOOMz7B, the series adapter demonstrated optimal performance when placed after the MLP layers. Similarly, the parallel adapter exhibited its best results when positioned in parallel with the MLP layers. Lastly, the LoRA adapter showed superior performance when located after both the Attention layers and the MLP layers concurrently. This observation aligns seamlessly with the assertions made in this paper.\nA.3.2 Various Size\nThis evaluation focuses on assessing various adapter placements with LLaMA-13B in the context of Arithmetic Reasoning tasks, aiming to compare the outcomes with those obtained from LLaMA-7B. Table 11 presents the accuracy achieved on Arithmetic Reasoning datasets, highlighting the comparative analysis between the two model configurations.\nThe optimal placement of series adapter, parallel adapter, and LoRA is also consistent with LLaMA7B reported in the paper.\nA.3.3 Various Tasks\nIn order to ascertain the generalizability of the placement strategies identified in the context of Arithmetic Reasoning tasks to other domains such as Commonsense reasoning, we conducted a comprehensive evaluation of various adapter placements with LLaMA-7B. Table 12 presents the accuracy scores achieved on Commonsense Reasoning datasets, providing valuable insights into the effectiveness of the adapter placements in this specific task.\nOur observations reveal that the optimal placement of series adapters, parallel adapters, and LoRA with LLaMA-7B for Commonsense reasoning aligns with their placement in Arithmetic reasoning tasks. These findings demonstrate a consistent pattern in adapter placement across various models, sizes, and task types, emphasizing the stability and reliability of our results.\nA.4 Configuration Analysis Table 13 shows the accuracy comparison regarding different settings of variable for PEFT methods on math reasoning datasets. The fine-tuning dataset used for this study is Math10K. It can be noted that when the number of virtual tokens in PrefixTuning is set to 10, Prefix-Tuning attains an average accuracy of 42.0% on math reasoning datasets. By configuring the bottleneck dimension to 256, Series and Parallel Adapter demonstrate the highest level of performance. However, when the bottleneck size is increased to 512, the accuracy of both Series and Parallel Adapter decreases. The typical setting for LoRA rank is set to 8, but we have discovered that a larger rank can enhance the performance of LoRA. Remarkably, when the rank is increased to 32, LoRA achieves an accuracy of 61.9%.\nA.5 Qualitative Examples We will show examples randomly sampled from math reasoning and commonsense reasoning datasets in this section."
        }
    ],
    "title": "LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models",
    "year": 2023
}