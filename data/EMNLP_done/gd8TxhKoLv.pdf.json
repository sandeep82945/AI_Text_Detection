{
    "abstractText": "Rich and diverse knowledge-bases (KB) are foundational building blocks for online knowledge sharing communities such as StackOverflow and Quora, and applications such as conversational assistants (aka chatbots). A popular format for knowledge bases is questionanswer pairs (or FAQs), where questions are designed to accurately match a multitude of queries. In this paper, we address the problem of automatic creation of such Q&A-based knowledge bases from domain-specific, longform textual content (e.g., web articles). Specifically, we consider the problem of question generation, which is the task of generating questions given a paragraph of text as input, with a goal to achieve both diversity and fidelity of the generated questions. Towards this goal we propose PROTEGE, a diverse question generation framework which consists of (1) a novel encoder-decoder based Large Language Model (LLM) architecture which can take a variety of prompts and generate a diverse set of candidate questions, and (2) a hill-climbing algorithm that maximizes a sub-modular objective function to balance diversity with fidelity. Through our experiments on three popular public Q&A datasets, we demonstrate that PROTEGE improves diversity by +16% and fidelity by +8% over diverse beam search and prompt-based baselines.",
    "authors": [
        {
            "affiliations": [],
            "name": "Vinayak S Puranik"
        },
        {
            "affiliations": [],
            "name": "Anirban Majumder"
        },
        {
            "affiliations": [],
            "name": "Vineet Chaoji"
        }
    ],
    "id": "SP:cde98fae77000bba0c625eb3992c482e5604fb0c",
    "references": [
        {
            "authors": [
                "Chris Alberti",
                "Daniel Andor",
                "Emily Pitler",
                "Jacob Devlin",
                "Michael Collins."
            ],
            "title": "Synthetic QA corpora generation with roundtrip consistency",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6168\u20136173, Flo-",
            "year": 2019
        },
        {
            "authors": [
                "Jimmy Lei Ba",
                "Jamie Ryan Kiros",
                "Geoffrey E. Hinton."
            ],
            "title": "Layer normalization",
            "venue": "Cite arxiv:1607.06450.",
            "year": 2016
        },
        {
            "authors": [
                "Francis Bach."
            ],
            "title": "Learning with Submodular Functions: A Convex Optimization Perspective",
            "venue": "Now Publishers Inc., Hanover, MA, USA.",
            "year": 2013
        },
        {
            "authors": [
                "Satanjeev Banerjee",
                "Alon Lavie."
            ],
            "title": "METEOR: An automatic metric for MT evaluation with improved correlation with human judgments",
            "venue": "Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Transla-",
            "year": 2005
        },
        {
            "authors": [
                "Emanuela Boros",
                "Jose G. Moreno",
                "Antoine Doucet."
            ],
            "title": "Event detection as question answering with entity information",
            "venue": "CoRR, abs/2104.06969.",
            "year": 2021
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann"
            ],
            "title": "Language models are few-shot learners",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Wei-Lin Chiang",
                "Zhuohan Li",
                "Zi Lin",
                "Ying Sheng",
                "Zhanghao Wu",
                "Hao Zhang",
                "Lianmin Zheng",
                "Siyuan Zhuang",
                "Yonghao Zhuang",
                "Joseph E. Gonzalez",
                "Ion Stoica",
                "Eric P. Xing"
            ],
            "title": "Vicuna: An opensource chatbot impressing gpt-4 with 90%* chatgpt",
            "year": 2023
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
            "year": 2019
        },
        {
            "authors": [
                "Zi-Yi Dou",
                "Pengfei Liu",
                "Hiroaki Hayashi",
                "Zhengbao Jiang",
                "Graham Neubig."
            ],
            "title": "GSum: A general framework for guided neural abstractive summarization",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for",
            "year": 2021
        },
        {
            "authors": [
                "Xinya Du",
                "Junru Shao",
                "Claire Cardie."
            ],
            "title": "Learning to ask: Neural question generation for reading comprehension",
            "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1342\u20131352,",
            "year": 2017
        },
        {
            "authors": [
                "Ehsan Elhamifar",
                "Guillermo Sapiro",
                "Ren\u00e9 Vidal."
            ],
            "title": "Finding exemplars from pairwise dissimilarities via simultaneous sparse recovery",
            "venue": "Advances in Neural Information Processing Systems, volume 25. Curran Associates, Inc.",
            "year": 2012
        },
        {
            "authors": [
                "Alexander Fabbri",
                "Patrick Ng",
                "Zhiguo Wang",
                "Ramesh Nallapati",
                "Bing Xiang."
            ],
            "title": "Template-based question generation from retrieved sentences for improved unsupervised question answering",
            "venue": "Proceedings of the 58th Annual Meeting of the Association",
            "year": 2020
        },
        {
            "authors": [
                "Seungju Han",
                "Beomsu Kim",
                "Buru Chang."
            ],
            "title": "Measuring and improving semantic diversity of dialogue generation",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2022, pages 934\u2013950, Abu Dhabi, United Arab Emirates. Associ-",
            "year": 2022
        },
        {
            "authors": [
                "Michael Heilman",
                "Noah A. Smith."
            ],
            "title": "Good question! statistical ranking for question generation",
            "venue": "Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages",
            "year": 2010
        },
        {
            "authors": [
                "Karl Moritz Hermann",
                "Tomas Kocisky",
                "Edward Grefenstette",
                "Lasse Espeholt",
                "Will Kay",
                "Mustafa Suleyman",
                "Phil Blunsom."
            ],
            "title": "Teaching machines to read and comprehend",
            "venue": "Advances in Neural Information Processing Systems, volume 28. Curran Associates,",
            "year": 2015
        },
        {
            "authors": [
                "Rafal Jozefowicz",
                "Oriol Vinyals",
                "Mike Schuster",
                "Noam Shazeer",
                "Yonghui Wu"
            ],
            "title": "Exploring the limits of language modeling",
            "year": 2016
        },
        {
            "authors": [
                "Kalpesh Krishna",
                "Mohit Iyyer."
            ],
            "title": "Generating question-answer hierarchies",
            "venue": "Association for Computational Linguistics.",
            "year": 2019
        },
        {
            "authors": [
                "Kalpesh Krishna",
                "Aurko Roy",
                "Mohit Iyyer."
            ],
            "title": "Hurdles to progress in long-form question answering",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
            "year": 2021
        },
        {
            "authors": [
                "Uszkoreit",
                "Quoc Le",
                "Slav Petrov."
            ],
            "title": "Natural questions: A benchmark for question answering research",
            "venue": "Transactions of the Association for Computational Linguistics, 7:452\u2013466.",
            "year": 2019
        },
        {
            "authors": [
                "Patrick Lewis",
                "Ethan Perez",
                "Aleksandra Piktus",
                "Fabio Petroni",
                "Vladimir Karpukhin",
                "Naman Goyal",
                "Heinrich K\u00fcttler",
                "Mike Lewis",
                "Wen-tau Yih",
                "Tim Rockt\u00e4schel",
                "Sebastian Riedel",
                "Douwe Kiela"
            ],
            "title": "Retrieval-augmented generation for knowledge",
            "year": 2020
        },
        {
            "authors": [
                "Jiwei Li",
                "Michel Galley",
                "Chris Brockett",
                "Jianfeng Gao",
                "Bill Dolan."
            ],
            "title": "A diversity-promoting objective function for neural conversation models",
            "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computa-",
            "year": 2016
        },
        {
            "authors": [
                "Chin-Yew Lin",
                "Franz Josef Och."
            ],
            "title": "Automatic evaluation of machine translation quality using longest common subsequence and skip-bigram statistics",
            "venue": "Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL-",
            "year": 2004
        },
        {
            "authors": [
                "Zhouhan Lin",
                "Minwei Feng",
                "C\u00edcero Nogueira dos Santos",
                "Mo Yu",
                "Bing Xiang",
                "Bowen Zhou",
                "Yoshua Bengio."
            ],
            "title": "A structured self-attentive sentence embedding",
            "venue": "ArXiv, abs/1703.03130.",
            "year": 2017
        },
        {
            "authors": [
                "Gary Marcus"
            ],
            "title": "The next decade in ai: Four steps towards robust artificial intelligence",
            "year": 2020
        },
        {
            "authors": [
                "G.L. Nemhauser",
                "L.A. Wolsey",
                "M.L. Fisher."
            ],
            "title": "An analysis of approximations for maximizing submodular set functions\u2013i",
            "venue": "Math. Program., 14(1):265\u2013294.",
            "year": 1978
        },
        {
            "authors": [
                "Tri Nguyen",
                "Mir Rosenberg",
                "Xia Song",
                "Jianfeng Gao",
                "Saurabh Tiwary",
                "Rangan Majumder",
                "Li Deng."
            ],
            "title": "Ms marco: A human generated machine reading comprehension dataset",
            "venue": "CoRR, abs/1611.09268.",
            "year": 2016
        },
        {
            "authors": [
                "Tri Nguyen",
                "Mir Rosenberg",
                "Xia Song",
                "Jianfeng Gao",
                "Saurabh Tiwary",
                "Rangan Majumder",
                "Li Deng."
            ],
            "title": "MS MARCO: A human generated machine reading comprehension dataset",
            "venue": "CoRR, abs/1611.09268.",
            "year": 2016
        },
        {
            "authors": [
                "Kishore Papineni",
                "Salim Roukos",
                "Todd Ward",
                "WeiJing Zhu."
            ],
            "title": "Bleu: a method for automatic evaluation of machine translation",
            "venue": "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311\u2013318, Philadelphia,",
            "year": 2002
        },
        {
            "authors": [
                "Guilherme Penedo",
                "Quentin Malartic",
                "Daniel Hesslow",
                "Ruxandra Cojocaru",
                "Alessandro Cappelli",
                "Hamza Alobeidli",
                "Baptiste Pannier",
                "Ebtesam Almazrouei",
                "Julien Launay"
            ],
            "title": "The refinedweb dataset for falcon llm: Outperforming curated corpora with web",
            "year": 2023
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J. Liu."
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "Journal of Machine Learning Research,",
            "year": 2020
        },
        {
            "authors": [
                "Pranav Rajpurkar",
                "Robin Jia",
                "Percy Liang."
            ],
            "title": "Know what you don\u2019t know: Unanswerable questions for SQuAD",
            "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 784\u2013789,",
            "year": 2018
        },
        {
            "authors": [
                "Pranav Rajpurkar",
                "Jian Zhang",
                "Konstantin Lopyrev",
                "Percy Liang."
            ],
            "title": "SQuAD: 100,000+ questions for machine comprehension of text",
            "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383\u20132392, Austin,",
            "year": 2016
        },
        {
            "authors": [
                "Sathish Reddy",
                "Dinesh Raghu",
                "Mitesh M. Khapra",
                "Sachindra Joshi."
            ],
            "title": "Generating natural language question-answer pairs from a knowledge graph using a RNN based question generation model",
            "venue": "Proceedings of the 15th Conference of the European Chap-",
            "year": 2017
        },
        {
            "authors": [
                "Yiping Song",
                "Rui Yan",
                "Yansong Feng",
                "Yaoyuan Zhang",
                "Dongyan Zhao",
                "Ming Zhang."
            ],
            "title": "Towards a neural conversation model with diversity net using determinantal point processes",
            "venue": "Proceedings of the",
            "year": 2018
        },
        {
            "authors": [
                "Katherine Stasaski",
                "Marti Hearst."
            ],
            "title": "Semantic diversity in dialogue with natural language inference",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
            "year": 2022
        },
        {
            "authors": [
                "Xingwu Sun",
                "Jing Liu",
                "Yajuan Lyu",
                "Wei He",
                "Yanjun Ma",
                "Shi Wang."
            ],
            "title": "Answer-focused and positionaware neural question generation",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3930\u20133939, Brus-",
            "year": 2018
        },
        {
            "authors": [
                "Guy Tevet",
                "Jonathan Berant."
            ],
            "title": "Evaluating the evaluation of diversity in natural language generation",
            "venue": "CoRR, abs/2004.02990.",
            "year": 2020
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N. Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin."
            ],
            "title": "Attention is all you need",
            "venue": "Proceedings of the 31st International Conference on Neural Information Processing Sys-",
            "year": 2017
        },
        {
            "authors": [
                "Ashwin Vijayakumar",
                "Michael Cogswell",
                "Ramprasaath Selvaraju",
                "Qing Sun",
                "Stefan Lee",
                "David Crandall",
                "Dhruv Batra."
            ],
            "title": "Diverse beam search for improved description of complex scenes",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2018
        },
        {
            "authors": [
                "Ashwin K. Vijayakumar",
                "Michael Cogswell",
                "Ramprasaath R. Selvaraju",
                "Qing Sun",
                "Stefan Lee",
                "David J. Crandall",
                "Dhruv Batra."
            ],
            "title": "Diverse beam search: Decoding diverse solutions from neural sequence models",
            "venue": "CoRR, abs/1610.02424.",
            "year": 2016
        },
        {
            "authors": [
                "Thomas Wolf",
                "Lysandre Debut",
                "Victor Sanh",
                "Julien Chaumond",
                "Clement Delangue",
                "Anthony Moi",
                "Pierric Cistac",
                "Tim Rault",
                "R\u00e9mi Louf",
                "Morgan Funtowicz",
                "Jamie Brew."
            ],
            "title": "Huggingface\u2019s transformers: State-of-the-art natural language processing",
            "venue": "CoRR,",
            "year": 2019
        },
        {
            "authors": [
                "Xusen Yin",
                "Jonathan May",
                "Li Zhou",
                "Kevin Small."
            ],
            "title": "Question generation for supporting informational query intents",
            "venue": "CoRR, abs/2010.09692.",
            "year": 2020
        },
        {
            "authors": [
                "Tianyi Zhang",
                "Varsha Kishore",
                "Felix Wu",
                "Kilian Q. Weinberger",
                "Yoav Artzi"
            ],
            "title": "Bertscore: Evaluating text generation with bert",
            "year": 2020
        },
        {
            "authors": [
                "Wenzheng Zhang",
                "Wenyue Hua",
                "Karl Stratos."
            ],
            "title": "Entqa: Entity linking as question answering",
            "venue": "CoRR, abs/2110.02369.",
            "year": 2021
        },
        {
            "authors": [
                "Yizhe Zhang",
                "Michel Galley",
                "Jianfeng Gao",
                "Zhe Gan",
                "Xiujun Li",
                "Chris Brockett",
                "Bill Dolan."
            ],
            "title": "Generating informative and diverse conversational responses via adversarial information maximization",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2018
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "In a data rich era, identifying, extracting and generating responses to user\u2019s questions has become the next challenge. While search engines provide a simple interface for users to get responses to their queries, getting answers to complex queries still remains a challenge (Krishna et al., 2021). As a result, specialized knowledge bases that extract and store question-answer pairs have become prevalent.\nMany applications rely on a knowledge base of generated question-answer pairs, to ensure reliable, accurate and as close to human-generated information to their users. For instance, a key frustration\nfor online shopping is the difficulty in identifying the right product that suits the requirement. For high-consideration products such as laptops and smartphones, at times customers lack the human touch that they would otherwise experience in an offline store where trained sales agents can explain features of each product and provide high-level guidance to select the right one. The sales agent can proactively query the customer to understand her requirement, help refine her needs and finally recommend the right products. In order to bridge the gap between online and offline shopping experience, multi-turn goal-oriented dialog systems, also known as chatbots offer a promising direction. Chatbots help users to familiarize technical concepts, acquire domain knowledge, get recommended products that they are likely to buy, closely mimicking the offline shopping experience.\nTowards curating a large scale knowledge bank, Large Language Models (LLMs) (Devlin et al., 2019; Raffel et al., 2020; Brown et al., 2020) have shown remarkable success in learning in-depth knowledge from data. They do so without access to any external memory as the knowledge is imbibed in the model parameters. While this is fascinating, on the downside, the model may hallucinate (Marcus, 2020) and generate answers that are factually incorrect. As a result, modern chatbot systems employ a Retrieval-Augmented Generation (RAG) architecture (Lewis et al., 2020) that has two components: a) an encoder-decoder network that does the natural language understanding of user queries and language generation and b) a back-end knowledgebase (KB) that indexes relevant bits of information for the task at hand. The encoder maps user inputs to a dense representation which is used to query the KB and retrieve evidences. The evidences, as well as the input, are fed to the decoder to generate the final response.\nScalable generation of knowledge-bases is fundamental to the success of the RAG and the underly-\ning chatbot application. In this work, we investigate automatic generation of a knowledge-base. Unlike traditional RAG system (Lewis et al., 2020; Izacard et al., 2022) that indexes webpages and large documents, we look for a knowledge-base in the form of question-answer pairs. Not only this helps us improve the accuracy of evidence retrieval, but also allows rich applications to be easily built on top of it e.g., suggesting related questions, navigation of the KB etc, for improved customer experience. Our focus is on educational questions, i.e., questions that help user familiarize with product concepts (\u201cWhat is the difference between SSD and HDD?\u201d) and use-case guidance (\u201cWhat is the recommended configuration for a gaming laptop?\u201d).\nQuestion generation is the task of generating questions given a paragraph of text as input. Question generation quality can be attributed to two characteristics: a) Fidelity that measures the semantic coherence of generated questions and our ability to answer them from the input paragraph, and b) Diversity which measures lexical and semantic dissimilarity between generated questions. Many previous works (Rajpurkar et al., 2018, 2016; Kwiatkowski et al., 2019) have addressed the task of generating questions from text. While it is essential to generate questions that are of high fidelity, for knowledge-base completion, it is imperative to have a diverse question set. To promote diversity, current question generation models rely on beam search. The resulting set, however contains many structurally similar questions with minor lexical changes that warrant the same answer. There has been prior work (Elhamifar et al., 2012; Song et al., 2018; Vijayakumar et al., 2018) in NLP on diversity. In particular, Song et al. (Song et al., 2018) addresses diversity via Determinantal Point Processes (DPP) for neural conversation models, it can be adapted for question generation task.\nWhile these approaches are helpful in maximizing diversity, they fall short in terms of generating high fidelity output. Naively borrowing these techniques may allow the model to hallucinate and generate questions that are not answerable from the input paragraph. Addressing the task of diverse question generation through the lens of monotone sub-modular function (Bach, 2013) alleviates this problem and provide additional benefits. On one hand, this formulation provides flexibility in controlling diversity and fidelity of the output. On the other hand, we can leverage a well-known greedy\nalgorithm (Nemhauser et al., 1978) to generate a near optimal set of questions, therefore, increasing yield and quality simultaneously.\nWe propose PROTEGE (PROmpT-based divErse question GEneration), a diverse question generation framework which consists of two stages (1) a novel encoder-decoder based LLM architecture which can take a variety of prompts and generate a diverse set of candidate questions, and (2) a greedy hill-climbing algorithm that maximizes a sub-modular objective function to balance diversity with fidelity. We demonstrate that PROTEGE improves diversity by +16% and fidelity by +8% while also improving text generation metrics, over strong baselines. Our experiments on three popular public Q&A datasets indicate that PROTEGE consistently outperforms both diverse beam search-based and prompt-based baselines."
        },
        {
            "heading": "2 PROTEGE: Prompted Question Generation",
            "text": "Question generation models take a source context x represented as a sequence of sentences x = (x1, x2 \u00b7 \u00b7 \u00b7 ), pass them through an encoder to learn its latent representation and finally through a decoder to generate the output question y = (y1, y2 \u00b7 \u00b7 \u00b7 ) with one word yi at a time. Given training data-set D = {(x, y)}, the model parameters \u03b8 are learned by maximizing the likelihood function \u2211 (x,y)\u2208D log Pr(y | x; \u03b8) The encoder and decoder are implemented as Transformer networks (Vaswani et al., 2017). The encoder consists of Ne layers where each layer contains a self-attention and feed-forward block. The encoder takes an input x \u2208 RB\u00d7S\u00d7F and passes it through all the encoder blocks to generate an output h = ENCODER(x) \u2208 RB\u00d7S\u00d7F . 1 The nth encoder block is a Transformer layer TRANSFORMER(x(n\u22121)) which takes the input x(n\u22121) from previous layer and generates the output x(n). The encoder blocks are applied in a sequence and finally we get the output h = x(Ne). To generate the ith output word yi+1, we take the previous words y\u00b7j = y1\u00b7\u00b7\u00b7i and the encoder output h and pass them through Nd decoder blocks. Each decoder block contains a self-attention, cross-attention and feedforward layer. The decoder blocks are also applied in a sequence and at the final layer it emits the next word yi+1 = y (Nd) \u00b7j .\n1Here B is the minibatch size, S is the sequence length and F denotes the embedding dimension."
        },
        {
            "heading": "2.1 Controlled Generation",
            "text": "For controlled generation of questions, we feed the input document along with various types of prompts to the encoder. This requires some modification to the standard encoder-decoder architecture. We use two encoders: one for the document (or, the context) and the other for the prompt signals (Dou et al., 2021). Similar to Transformer architecture, each encoder has 1 + Ne layers where the first Ne layers use shared parameters \u0398e for the context and prompt. The final encoder layers consist of an additional Transformer block for the context and prompt inputs with individual parameters \u0398c,\u0398p respectively. More specifically, given context xc and prompt xp, we run the following computation on the encoder side,\nhc = TRANSFORMER (ENCODER (xc,\u0398e) ,\u0398c)\nhp = TRANSFORMER (ENCODER (xp,\u0398e) ,\u0398p)\nOur decoder attends to both the context and prompt signals hc, hp. We achieve it by modifying the standard architecture as follows. Unlike standard decoder, each decoder layer attends to both context and prompt embeddings from the encoder via crossattention layers and their combined output is fed to\nthe feed-forward network. More specifically, each decoder layer performs the following computation,\nd (n) in = LN ( y (n\u22121) \u00b7j + SA(y (n\u22121) \u00b7j ) ) (1)\nd(n)c = LN ( d (n) in + CA(hc, d (n) in ) ) (2)\nd(n)p = LN ( d (n) in + CA(hp, d (n) in ) ) (3)\nd (n) out = MIXUP ( d(n)c , d (n) p ) (4)\ny (n) \u00b7j = LN ( d (n) out + FF(d (n) out) ) (5)\nHere LN, SA, CA, FF are abbreviations for layernorm (Ba et al., 2016), cross-attention (Vaswani et al., 2017), self-attention (Vaswani et al., 2017) and feedforward layers. MIXUP is an aggregation layer that combines the context and prompt cross-attention outputs d(n)c , d (n) p to generate d (n) out i.e. d(n)out = \u03bb T \u00b7 [d(n)c , d(n)p ]. We propose various ways to implement MIXUP: a) treat \u03bb as a tunable hyper-parameter, b) learn \u03bb as a free parameter or via attention (Lin et al., 2017) weights. Note that by setting \u03bb = [1, 0], we recover the standard decoder. More details about the architecture choice are described in Appendix A."
        },
        {
            "heading": "2.2 Prompt Signals",
            "text": "We use two types of prompts: a) keyword-based: we define an entity dictionary based on domain knowledge search keywords such as, brands, features etc. Entities from this dictionary can be used as a prompt, b) sentence-based: we identify informative sentences from the context and use them as prompt input. Further, there are two strategies to compute the prompts: a) HEURISTIC: extract prompts from the context based on manually defined rules or ML models, b) ORACLE: extract prompts from both context and the ground-truth question. Note that the ORACLE strategy requires the ground-truth and hence, can be used only during training whereas HEURISTIC can be used during both training and inference. While using ORACLE during training and HEURISTIC in inference leads to train, test mismatch of distribution of prompts, our hypothesis is that it will help establish strong correlation between the prompts and the generated questions. In Appendix B we describe the various prompt signals we have used for our experiments."
        },
        {
            "heading": "2.3 Balancing Diversity and Fidelity of Questions",
            "text": "At the end of the first stage of PROTEGE, we have generated a diverse set of questions by varying the prompt input to the model. However, in practice, some of these questions may be irrelevant, i.e., they can\u2019t be answered from the current context. In the second stage, we leverage an algorithm that selects a subset of questions which is both relevant and diverse. Let\u2019s assume that the previous step has generated N questions Q = {q1, q2 \u00b7 \u00b7 \u00b7 , qN} from a document D. The objective of the current step is to select a subset Q\u2032 \u2286 Q of size k, that maximizes \u03b6(Q\u2032, D) =\n\u03b7 \u00b7 diversity(Q\u2032)+ (1\u2212 \u03b7) \u00b7 relevance(Q\u2032, D) (6)\nHere \u03b7 is a hyper-parameter that balances the relevance (i.e., fidelity) and diversity. We discuss various choices for implementing the diversity and relevance functions. The relevance of a question set Q is determined via answerability i.e. how likely the question can be answered from the given context. The answerability of a question set Q\u2032 is calculated as fa(Q\u2032, D) = \u2211 q\u2208Q\u2032 AE(D, q) where AE is an answerability model build on top of standard LLM encoders such as BERT (Devlin et al., 2019). We use n-grams to define diversity of a question bank. Let zn(q) denote the set of n-grams in q after removing stop-words. We define diversity as diversity(Q\u2032) = \u2211 n\u2208{1,2,3} | \u222aq\u2208Q\u2032 zn(q)|.\nNote that the diversity expression promotes unique n-grams across questions and has been used as standard metric to measure diversity of text generated by LLMs in prior works, such as (Zhang et al., 2018). It can be noted that the diversity function is sub-modular (Bach, 2013) which makes the objective function \u03b6(Q\u2032, D) sub-modular as well. Although maximization of a sub-modular function is NP-Hard, it is well-known that the algorithm that greedily picks each item has provably good approximation guarantee (Nemhauser et al., 1978)."
        },
        {
            "heading": "3 Experiments",
            "text": ""
        },
        {
            "heading": "3.1 Datasets",
            "text": "A supervised dataset for the question generation task typically consists of question and answer pairs along with a \u201ccontext\u201d input. In order to prove the efficacy of our approach for a specific domain of \u2018shopping guidance\u2019, we curate a custom dataset, termed SEARCHQA, by extracting QA pairs from a third-party search engine. We submit customized\nshopping guidance queries to the search engine and extract questions, answer snippets and URLs from the search results. We further pre-process the extracted content to form question, answer, context triplets. We also leverage three popular benchmark Q&A datasets namely, (1) SQUAD 2.0 (Rajpurkar et al., 2018), (2) NQ Natural Questions dataset (Kwiatkowski et al., 2019), and (3) MS MARCO (Nguyen et al., 2016a). In Appendix C we describe the pre-processing logic used to create the question, answer, context triplets from raw datasets. Table 7 in Appendix C lists the dataset statistics."
        },
        {
            "heading": "3.2 Implementation details",
            "text": "Our models are based on the popular T5 (Text-toText Transfer Transformer) (Raffel et al., 2020) architecture. T5 models closely follow the encoderdecoder Transformer implementation originally proposed in (Vaswani et al., 2017) with minor modifications. For baseline models (section 3.3), we use the vanilla T5ForConditionalGeneration implementation from the HuggingFace Transformer library (Wolf et al., 2019). For our prompt-based controlled generation models we extend the vanilla implementation by including (as described in section 2.1) (1) an additional encoder for the prompt input which shares parameters with the original encoder, (2) a new cross-attention block in the decoder which is initialized with pre-trained weights from the original cross-attention block. Hyper-parameter settings. To make it feasible to train a large number of models, for all our experiments we use the t5-small variant with 60MM parameters as the base implementation. We use a learning rate of 5e-5, epsilon of 1e-8 with AdamW optimizer. We use a sequence length of 512. We train all models up to 10 epochs with a training batch size of 4 and choose the checkpoint with the best performance on the validation set. We train our models on a single GPU of an AWS EC2 instance with a GPU memory of 64GB."
        },
        {
            "heading": "3.3 Baselines",
            "text": "BASELINE-BEAM Only the context is passed as input (without any additional prompts) and Diverse Beam Search (DBS) (Vijayakumar et al., 2016) is used to generate top-k questions. Diversity parameters num_beam_groups and diversity_penalty are fine-tuned by optimizing for diversity metrics through a grid search. BASELINE-PROMPT Prompts are concatenated\nwith context to form a single input to vanilla T5. For a fair evaluation, we use exactly the same set of prompts used in the corresponding controlled generation (PROTEGE) model. For instance, suppose PROTEGE model uses ground-truth question entities as ORACLE prompts (training) and context entities as HEURISTIC prompts (inference), the exact same strategy is used for this baseline as well."
        },
        {
            "heading": "3.4 Metrics",
            "text": "Diversity metrics. We evaluate on two popular n-gram based lexical diversity metrics (1) Distinctn (Li et al., 2016), which measures the percentage of unique n-grams out of total number of n-grams in a set of generated questions. We report Dist-1, Dist-2 and Dist-3 metrics, (2) Entropy-n (Zhang et al., 2018), which measures how evenly the ngram distribution is for a given question set. These two metrics are popularly used in literature to evaluate lexical diversity of generated responses (Zhang et al., 2018; Han et al., 2022; Stasaski and Hearst, 2022; Tevet and Berant, 2020). To measure semantic diversity we report a BERTScore (Zhang et al., 2020), which is measured as the average BERTScore of each pair of generated questions. BERTScore measures the semantic similarity between a pair of generated sentences, hence lower the average BERTScore better the diversity. Fidelity metrics. To report a fidelity (or \u201canswerability\u201d) metric, we train a separate BERT-based model that takes a context and a question and outputs a probability score for the question being answerable from the context. The ROC AUC of this BERT model was observed to be 0.84. We tune the threshold of this model to operate at a precision of 85%, corresponding to a recall of 30%. A higher bar on the precision allows us to select questions which are highly likely to be answerable\nfrom the context, at the cost of missing out on other answerable questions. We compute the answerability score for each generated question and report the average. NLG metrics. Finally, to evaluate the \u201ccloseness\u201d of generated questions with respect to the ground-truth questions we also report standard NLG metrics popular in literature, namely: (1) METEOR (Banerjee and Lavie, 2005) which is measured as a harmonic mean of unigram precision and recall, (2) BLEU-4, a cumulative 4-gram BLEU (Papineni et al., 2002) score, which is an evaluation of matching grams of specific order (1- gram, 2-gram etc.) (3) ROUGE-L, a version of ROUGE (Lin and Och, 2004), which measures the longest common subsequence (LCS) between the generated and reference text."
        },
        {
            "heading": "4 Results",
            "text": "Diversity results. For the SEARCHQA dataset, among several choices for prompt signals (described in section 2.2) we highlight the best results obtained in this section and describe the trade-offs among the choices in section 4.1. In all our tables we highlight the first best result and underline the second best result. Note that \u2191 for a metric indicates higher values are preferred whereas \u2193 indicates lower values are preferred. For SEARCHQA dataset, we observe in table 1 that across all the metrics our algorithm (PROTEGE) does significantly better than both baselines. On Dist-1, Dist-2 and Dist-3 metrics, PROTEGE does 16%, 18%, 13%, respectively, better than the second best result. On Ent-1, Ent-2 and Ent-3 metrics, PROTEGE shows an improvement of 12%, 13% and 11%, respectively, compared to the second best result. PROTEGE also reduces BERTScore by 7%, and improves Fidelity by 8%. PROTEGE generates almost double the\nnumber of unique questions compared to BASELINE-PROMPT. Thus, given the same context PROTEGE generates a higher number of unique questions which are better both in terms of diversity and fidelity, compared to baselines.\nFor the benchmark datasets, we observe in Table 1 that across datasets, PROTEGE improves on all the diversity metrics (Dist-n & Ent-n) when compared to both the baselines. For example, on the Dist-1 metric, compared to the second best (which is consistently BASELINE-PROMPT), PROTEGE shows an improvement of 17%, 13% and 12%, respectively, for SQUAD, NQ and MS MARCO. On fidelity, compared to the second best, PROTEGE performs 9%, 9% and 4% better, respectively, for SQUAD, NQ and MS MARCO. Significant reduction is also observed on BERTScore. PROTEGE generates 1 to 3 unique questions (on an average) more than BASELINE-PROMPT.\nNLG results. We present metrics separately for top-1, top-2 and top-3 generated questions. The metrics for top-k is computed using the question (among top-k) which results in the maximum METEOR score with reference to the ground-truth question. As described earlier, for BASELINEBEAM we use beam search to generate the top-k questions, while for BASELINE-PROMPT we pick the top-k questions based on generation score. For PROTEGE we select the top-k questions returned by our second-stage algorithm (section 2.3, which greedily selects the question that maximizes diversity and fidelity.\nFor the SEARCHQA dataset, in table 2 we observe that for the top-1 question the best metrics are obtained from the BASELINE-BEAM model. From our model\u2019s point of view this is expected as the topmost question is selected based on diversity and\nfidelity objectives, and hence need not be closest to the reference ground-truth. However, as we allow PROTEGE to select more questions (top-2 and top3) the model often generates a question closer to the ground-truth, which shows in top-2/top-3 results where PROTEGE does better than both the baselines in matching with the reference. In other words, if we allow top-2 questions, PROTEGE shows the best performance with an improvement of 1.1% in METEOR (but, shows second best performance in BLEU-4 and ROUGE-L). Similarly, for top-3 questions the corresponding improvements are +1.7%, +0.2% for METEOR and BLEU-4 scores. We observe similar trends for SQUAD among benchmark datasets.\nFor the NQ and MS MARCO datasets, although PROTEGE shows a significant improvement over baselines on diversity metrics, improvements are not observed on NLG metrics. We explain our hypothesis for this observation in Appendix D.\nHuman evaluation. We performed human evaluation to compare the quality of top-k generated questions between PROTEGE and BASELINEBEAM. Annotators were asked to label each set of generated questions (for a given context) w.r.t., a) Readability (no. of readable & meaningful questions), b) Diversity (no. of semantically unique questions), c) Fidelity (no. of questions answerable from the context). In figure 2 we observe that PROTEGE improves on BASELINE-BEAM with an absolute improvement of 5% on readability, 32% on diversity, 36% on answerability. Appendix I describes the details of the human audits."
        },
        {
            "heading": "4.1 Ablation studies",
            "text": "Effect of prompt signals. For the SEARCHQA dataset we experiment with a variety of (keywordbased and sentence-based) prompt signals as de-\nscribed in 2.2. In table 3 we present the effects of prompt signals on diversity metrics.\nTraining prompt (ORACLE) Inference prompt (HEURISTIC) Dist-1 \u2191 BERT Score \u2193 Fidelity \u2191\nBaseline 0.5379 0.9009 0.8408\nAnswer text Context span (size=1) 0.7351 0.8235 0.9085\nAnswer text Context span (size=2) 0.6925 0.8405 0.9005\nAnswer text Context-span (size=4) 0.6268 0.8730 0.8817\nAnswer keywords Keywords from context span (size=1) 0.6931 0.8419 0.8977\nAnswer keywords Keywords from context span (size=2) 0.5735 0.8578 0.8906\nAnswer keywords Keywords from context span (size=4) 0.6215 0.8829 0.8684\nQuestion entities Context entities 0.7459 0.8234 0.8765\nTable 3: Effect of prompt signals on diversity metrics.\nAcross all prompt choices, PROTEGE does better than BASELINE-BEAM on all metrics. Answer text (with a context span of size 1 during inference) performs the best on BERTScore and Fidelity metrics and second best on Dist-1 metric. Question entities shows the best performance on Dist-1 metric, which is due to the fact that the model is trained to generate a question around the specific entity passed as a prompt. Based on these results, we typically use answer text as a preferred choice for the prompt. Detailed metrics are in Appendix F.\nEffect of ORACLE prompting. Across datasets, ORACLE prompting yields the best performance in terms of matching the ground-truth question. (Refer figure 4 and table 11 in Appendix G). This ablation shows the efficacy of our architecture in incorporating the prompt when generating a question, i.e., providing the \u201cexact\u201d prompt elicits a question which is relatively closer to the ground-truth.\nEffect of greedy algorithm. As described in section 2.3, our algorithm takes the candidate set of questions generated in the first stage (prompt-based controlled generation) and in the second stage performs a greedy algorithm, at each step optimizing\nfor both diversity and fidelity. In table 4 we see that as an effect of this greedy algorithm, across datasets both diversity and fidelity metrics show a marked improvement. On an average, post-greedy Dist-1 metric improves by 13% and Fidelity improves by 9%. Further, in Appendix H we show the effects of greedy algorithm on all the diversity and NLG metrics.\nDiversity versus fidelity. Our algorithm to balance diversity and fidelity of questions (section 2.3) allows us to control the trade-off between diversity and fidelity through the \u03b7 parameter. Figure 3 shows how controlling the \u03b7 parameter allows us to operate at different points for diversity and fidelity. Low \u03b7 results in high fidelity, while high \u03b7 results in high diversity. For our experiments we used an \u03b7 around 0.5 to achieve the right trade-off. Table 8 in Appendix E shows the full metrics as a result of varying \u03b7."
        },
        {
            "heading": "4.2 Qualitative study",
            "text": "In Table 5 we provide qualitative examples of questions generated by PROTEGE when compared with BASELINE-BEAM output. Due to paucity of space we do not include the context input which is fed to both the models. The second column shows the output of BASELINE-BEAM given the context input alone. The third column is a sample of the prompts which are fed to PROTEGE model (along with con-\ntext). We have shown examples of prompt keywords (e.g., first row), as well as prompt sentences (e.g., second row). Finally last column shows the output of PROTEGE model given the prompt and context as input. We observe that given a context PROTEGE leverages the prompts effectively in generating diverse questions when compared to BASELINE-BEAM output. Especially, when sentences are passed as prompts they often appear to be answers to the generated question."
        },
        {
            "heading": "5 Related Work",
            "text": "Rule-based (Heilman and Smith, 2010; Fabbri et al., 2020) and DNN based (Sun et al., 2018; Yin et al., 2020) models are used for question generation from text corpora. Answer extraction (Rajpurkar et al., 2016; Kwiatkowski et al., 2019) or machine comprehension (Hermann et al., 2015; Jozefowicz et al., 2016) is a branch of NLP where the goal is to extract answer snippet from text documents given a question as input. In both cases, either the question or the answer is given as input. QA extraction models (Alberti et al., 2019; Du et al., 2017; Reddy et al., 2017; Krishna and Iyyer, 2019) are generally pipeline-based which generates the question and the answer in a sequence. Boros et al. (Boros et al., 2021) uses a question-answering system to detect specific events in textual content (e.g., tweets, blogs). In this context, the entity information is used to frame template-based question (e.g., Where did the [attack] happen?) where attack is an event of interest). Zhang et al. (Zhang et al., 2021) propose combining entity linkage with a QA system. However, our objective is differ-\nent as we enrich the QA extraction technique by augmenting it with entity level metadata."
        },
        {
            "heading": "6 Conclusion and Future Work",
            "text": "In this paper we present PROTEGE, a transformer based two-stage question generation framework based on prompts that balances diversity of the generated questions with their fidelity. Through extensive experiments on multiple datasets we show that PROTEGE significantly improves diversity (by +16%) and fidelity (by +8%) compared to strong baselines. As a future work, we will extend our models to simultaneously generate both questions and answers. In preliminary experiments on the task of extracting answers for questions from a given context, we have observed that providing the \u201centities\u201d in the question as additional prompt signals to a BERT-based model improves the answer extraction quality by up to +4.2% in F1 score. Similar applications to other NLG tasks such as document summarization and FAQ creation are possible using the framework proposed in our paper. Extension of our work to non English languages is part of future work.\nLimitations\nOne limitation of PROTEGE is that it is tightly integrated with existing transformer architecture. Therefore to test its efficacy with Large Language Models (LLMs), we would need access to the pretrained model parameters. While this is possible for publicly available Large Language Models (LLMs) such as Vicuna (Chiang et al., 2023), Falcon (Penedo et al., 2023) and LLaMA (Tou-\nvron et al., 2023), we will miss out state-of-the-art LLMs such as GPT-4 (OpenAI, 2023) and ChatGPT 2. Further our approach requires large GPU cluster to train which may lead to higher carbon emission.\nExperimental evidences suggest that when context span is used as prompt, our model may hallucinate or mention incomplete product names or product family. For example, instead of \u201cCore i7 12700K CPU\u201d, it may generate a question with \u201cCore i7 12700 CPU\u201d which is ambiguous (i7 12700K CPU has a base frequency of 3.6 GHz in comparison to 2.1 GHz for i7 12700F). Generating questions with fully-qualified product names will be a direction of our future work."
        },
        {
            "heading": "A Architecture Choices",
            "text": "The architecture for controlled generation is described in Section 2.1. We once again highlight our key modifications to the standard encoder-decoder architecture in order to achieve our objective of controlled question generation:\n\u2022 Introduction of an additional encoder stack (with 1 + Ne layers) for the \u201cprompt\u201d input. The first Ne layers share the parameters with the encoder stack for the \u201ccontext\u201d input. The final layer (a transformer block) has its own individual parameters. This choice is not only to reduce computations, but also due to the fact that the difference between context and prompt signals is expected be at a higher-level which is captured only at the top layer of the encoder.\n\u2022 Modification of each decoder block to attend to the \u201ccontext\u201d and \u201cprompt\u201d embedding via separate cross-attention layers. The output of the cross-attention blocks are combined using a MIXUP operation and fed to the feedforward layer. It is important to note that this is a fundamental change to the standard decoder architecture. The intuition for this architecture is that the cross-attention with the prompt signal influences the decoder to focus on certain topics while the cross-attention with the context influences the decoder to improve relevance of the questions generated w.r.t. the context. Together the aggregated cross-attention influences the model to generate a question around the prompt while remaining true to the information present in the context. This aggregation of the decoder crossattention with the context and prompt embed-\nding happens throughout all the decoder layers.\nOne of our baselines, BASELINE-PROMPT, can be viewed as a combination of \u201ccontext\u201d and \u201cprompt\u201d tokens at the input level (i.e., early fusion) via cross-attention. We also performed limited experiments to combine the encoder outputs for the context (hc) and prompt (hp) (i.e., mid-level fusion) and observed it to perform worse than BASELINEPROMPT. Our experiments also suggested that a single cross-attention and MIXUP was not sufficient to guarantee faithfulness of the generated questions w.r.t. the prompt signal, and hence needs to be repeated in each decoder layer."
        },
        {
            "heading": "B Prompt signals",
            "text": "In table 6 we describe the various prompt signals we have used for our experiments."
        },
        {
            "heading": "C Dataset details",
            "text": "Search QA (SEARCHQA): We start by creating a set of custom templates for shopping guidance queries (e.g., things to consider when buying a <category>, main features of a <usecase> <category>). We expand the templates by populating the slots to create a seed set of search queries (e.g., main features of a gaming laptop). For each query, we submit a search request to a third-party search engine and extract questions, answer snippets and URLs. Further, we extract the textual content from the URLs. We thus collect a dataset of size ~100K. After filtering out rows where we are (a) unable to extract the URL content (b) unable to locate the answer in the extracted URL content we are left with ~60K datapoints. Finally, for each question, answer, URL datapoint, starting from the textual content extracted from the URL we extract multiple paragraphs (each containing the answer in different locations) to create a \u201ccontext\u201d input (data augmentation). Thus, each question, answer, URL datapoint expands into N question, answer, context datapoints. From the URLs set we create two mutually exclusive set of domain names, one each for train and test datasets (to ensure that models generalize across unseen domains), which allows us to create a training dataset with ~100K rows, and a dev and test datasets each with ~5K rows each.\nSQuAD 2.0(SQUAD): The Stanford Question Answering Dataset 2.0 (Rajpurkar et al., 2016) is a public dataset consisting of crowdsourced questions on a selection of Wikipedia articles. The\ndataset consists of a paragraph/context, a set of questions relevant to the context and for each question, an answer which is a phrase from within the context. We ignore unanswerable questions (where is_impossible = True). We split the original train dataset into train (~80K rows) and dev (~5K rows) by splitting based on titles. We sample from the original validation set (consisting of ~10K unique datapoints) to create a test set (~5K rows).\nGoogle Natural Questions (NQ): Natural Questions (Kwiatkowski et al., 2019) is a collection of real user questions submitted to Google and answers gathered from Wikipedia by annotators. From the original dataset we parse the question_text, the long_answer (and treat it as a context) and the short_answer (and treat it as an answer) which is usually a phrase from within the context (except for yes/no answers). We filter out contexts that contain HTML tables (<Table>) and also filter out very long contexts (>= 20 sentences). We sample from the original training data of ~307K datapoints to create a train (~120K rows) and dev (~5K rows). We similarly parse the original ~7.8K validation set to create a test set (~3.5K rows).\nMS MARCO: MS MARCO (Microsoft Machine Reading Comprehension) (Nguyen et al., 2016b) is a large scale collection of datasets (machine reading comprehension, passage ranking, etc.) of which we leverage the question answering dataset. Queries (questions) are sampled from Bing logs and 10 most relevant passages for the query are generated. Human annotators then tag passages that contain an answer to the question and identify the answers from the relevant passages. From the original dataset, for a given query we randomly select one answer and then randomly sample 3 passages (selecting one passage that contains the answer and two passages that do not contain the answer), shuffle and concatenate the passages to form our input context. We sample from the original train and dev datasets to create a train (~100K rows), dev (~5K rows) and test (~5K rows)."
        },
        {
            "heading": "D NQ & MS MARCO observations",
            "text": "For the NQ and MS MARCO datasets, although PROTEGE shows a significant improvement over baselines on diversity metrics, improvements are not observed on NLG metrics. In the case of NQ and MS MARCO datasets, the answer is often a short phrase (specifically, in NQ we use the \u201cshort answer\u201d provided in the dataset). During inference,\nwe select the top-k phrases/keywords from the context, using an unsupervised keyword detection algorithm (RAKE), as prompt signals. Hence, for most examples the specific phrase/keyword which is part of the ground-truth answer does not always get selected. Due to this reason, although we generate a number of answerable and diverse questions from the context, we may not necessarily generate a question semantically similar to the ground-truth question. On the other hand, the baseline techniques take as input only the context and no other controlling signal and hence, are more likely to generate a question similar to the ground-truth. This issue is not seen with SEARCHQA and SQUAD datasets, where answers are usually complete sentences which are picked as a candidate prompt signals during inference. Further we note that for all datasets, including NQ and MS MARCO, when the model is provided with the exact ground-truth answer-based prompt, it indeed generates a question semantically closer to the ground-truth question as observed in the Section 4.1 (\u201cEffect of ORACLE prompting\u201d).\nBelow we provide examples to illustrate why on NQ and MS MARCO datasets the performance of PROTEGE is lower than baselines on NLG metrics. As seen in the examples below the question generated by PROTEGE depends on the phrase/keyword that gets selected during inference, which need not\nnecessarily match the phrase/keyword that elicits ths ground-truth question.\nExample in NQ: Context: The American Civil War was fought in the United States from 1861 to 1865... Ground-truth question: who took part in the american civil war Ground-truth short answer: nationalists of the Union\nPrompt Generated question\npresident abraham lincoln who was president when the civil war began\nunited states where did the civil war take place in\nExample in MS MARCO: Context: Jesse James (VII) Producer | Actor. At first glance, Jesse James is the consummate biker rebel... Ground-truth question: who is married to jesse james Ground-truth short answer: Karla James, Janine Lindemulder, Sandra Bullock and Alexi DeJoria.:"
        },
        {
            "heading": "E Ablation: Balancing diversity vs fidelity",
            "text": "Table 8 shows the full metrics as a result of varying \u03b7.\nPrompt Generated question\nJesse James (VII) Producer | Actor.\nwho is jesse james\nTattoos, knives, goatee, black t-shirts and skulls all around him what is jesse james famous for"
        },
        {
            "heading": "F Ablation: Effect of prompt signals",
            "text": "In tables 9 and 10 we present the complete set of diversity and NLG metrics based on the choice of prompt signals. Specifically for the NLG metrics, regardless of the choice of prompt signal for top-1 question, BASELINE-BEAM generates questions closest to ground-truth followed by answer keywords. For top-2 and top-3, the best strategy in general is to pass answer keywords as prompts during training and keywords from context spans of size 2 or 4 during inference. The best result with answer keywords is better than the best result with answer text, indicating that model benefits more when passed keywords rather than full text as guidance signal. Among different context span sizes passing a larger window (2 or 4 sentences) leads to better results. The worst performing is question entities possibly because model tends to overfit on the specific prompted entities, while it generalizes when passed with a larger window of keywords/sentences."
        },
        {
            "heading": "G Ablation: ORACLE prompting",
            "text": "Figure 4 and table 11 (with the complete set metrics for ORACLE vs HEURISTIC prompting) shows that on an average there is a 40+% improvement in METEOR metrics from HEURISTIC prompting compared to ORACLE prompting."
        },
        {
            "heading": "H Ablation: Effect of greedy algorithm",
            "text": "Table 12 shows the effect of greedy algorithm on the full set of diversity metrics. As expected, across all datasets diversity metrics improve with greedy algorithm.\nIn table 13 we observe that post-greedy top-1 METEOR reduces for some datasets. This is expected as the generated question from the first stage is often replaced by a question which displays high diversity and fidelity. However, at top-2 and top-3 the METEOR slightly increases (except for NQ) indicating that the greedy algorithm implicitly favors the question more closer to the ground-truth (which is also expected to be answerable) as long as it improves the diversity."
        },
        {
            "heading": "I Audit SOP",
            "text": "We perform human audits with 2 auditors to compare the generated questions between PROTEGE and BASELINE-BEAM (for a sample of ~200 contexts). For each data point, auditors record the following details regarding the top-k generated questions: (A) Are the questions readable and meaningful (i.e., well-formed and complete sentences)? (B) Out of the readable questions, how many questions are semantically unique (measures semantic diversity)? (C) Out of the readable questions, how many questions are answerable from the context (measures fidelity)? In case of conflicts on any of the labels, a third auditor re-verifies the decision to resolve the conflict. Finally, we take a cumulative count for each aspect and measure the percentage of readable, unique and answerable questions.\nDataset Model Training prompt(ORACLE)\nSEARCHQA\nDataset Model Training prompt(ORACLE)\nSEARCHQA\nBASELINE-BEAM PROTEGE (HEURISTIC) PROTEGE (ORACLE)\nDataset Prompt Choice(Training) METEOR top-1 BLEU-4 top-1 ROUGE-L top-1 METEOR top-1 BLEU-4 top-1 ROUGE-L top-1 METEOR top-1 BLEU-4 top-1 ROUGE-L top-1\nSEARCHQA Answer-Text\n0.3088 0.3794 0.3375 0.2821 0.3560 0.3102 0.3221 0.4043 0.3518\nAnswer-Keywords 0.2938 0.3627 0.3162 0.3107 0.3915 0.3353\nQuestion-Entities 0.2275 0.2929 0.2502 0.4213 0.5032 0.4564\nSQUAD Answer-Text 0.1678 0.2346 0.2083 0.1593 0.2289 0.2053 0.2777 0.3442 0.3133\nNQ Answer-Text 0.3998 0.4634 0.4307 0.3089 0.3965 0.3391 0.4224 0.4763 0.4518\nMS MARCO Answer-Text 0.4217 0.4712 0.4637 0.3788 0.4234 0.4120 0.4940 0.5364 0.5226"
        }
    ],
    "title": "PROTEGE: Prompt-based Diverse Question Generation from Web Articles",
    "year": 2023
}