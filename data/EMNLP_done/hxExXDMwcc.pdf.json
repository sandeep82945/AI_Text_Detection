{
    "abstractText": "The patient-centered medical dialogue systems strive to offer diagnostic interpretation services to users who are less knowledgeable about medical knowledge, through emphasizing the importance of providing responses specific to the patients. It is difficult for the large language models (LLMs) to guarantee the specificity of responses in spite of its promising performance even in some tasks in medical field. Inspired by in-context learning, we propose PlugMed, a Plug-and-Play Medical Dialogue System, for addressing the challenge. PlugMed is equipped with a prompt generation (PG) module and a response ranking (RR) module to enhances LLMs\u2019 dialogue strategies for improving the specificity of the responses. The PG module is used to stimulate the imitative ability of LLMs by providing them with real dialogues from similar patients as prompts. The RR module incorporates fine-tuned small model as response filter to enable the selection of appropriate responses generated by LLMs. Furthermore, we introduce a new evaluation method based on matching both user\u2019s intent and high-frequency medical term to effectively assess the specificity of the responses. We conduct experimental evaluations on three medical dialogue datasets, and the results, including both automatic and human evaluation, demonstrate the effectiveness of our approach.",
    "authors": [
        {
            "affiliations": [],
            "name": "Chengfeng Dou"
        },
        {
            "affiliations": [],
            "name": "Zhi JinB"
        },
        {
            "affiliations": [],
            "name": "Wenpin JiaoB"
        },
        {
            "affiliations": [],
            "name": "Haiyan Zhao"
        },
        {
            "affiliations": [],
            "name": "Yongqiang Zhao"
        },
        {
            "affiliations": [],
            "name": "Zhenwei Tao"
        }
    ],
    "id": "SP:197283756fc330de06f4e25bdd0702a59e0c8613",
    "references": [
        {
            "authors": [
                "Pawe\u0142 Budzianowski",
                "Tsung-Hsien Wen",
                "Bo-Hsiang Tseng",
                "Inigo Casanueva",
                "Stefan Ultes",
                "Osman Ramadan",
                "Milica Ga\u0161i\u0107."
            ],
            "title": "Multiwoz\u2013a large-scale multi-domain wizard-of-oz dataset for task-oriented dialogue modelling",
            "venue": "arXiv preprint",
            "year": 2018
        },
        {
            "authors": [
                "Boxing Chen",
                "Colin Cherry."
            ],
            "title": "A systematic comparison of smoothing techniques for sentencelevel BLEU",
            "venue": "Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 362\u2013367, Baltimore, Maryland, USA. Association for Compu-",
            "year": 2014
        },
        {
            "authors": [
                "Derek Chen",
                "Kun Qian",
                "Zhou Yu."
            ],
            "title": "Stabilized in-context learning with pre-trained language models for few shot dialogue state tracking",
            "venue": "arXiv preprint arXiv:2302.05932.",
            "year": 2023
        },
        {
            "authors": [
                "Wei Chen",
                "Zhiwei Li",
                "Hongyi Fang",
                "Qianyuan Yao",
                "Cheng Zhong",
                "Jianye Hao",
                "Qi Zhang",
                "Xuanjing Huang",
                "Jiajie Peng",
                "Zhongyu Wei."
            ],
            "title": "A Benchmark for Automatic Medical Consultation System: Frameworks, Tasks and Datasets",
            "venue": "Bioinformat-",
            "year": 2022
        },
        {
            "authors": [
                "Qingxiu Dong",
                "Lei Li",
                "Damai Dai",
                "Ce Zheng",
                "Zhiyong Wu",
                "Baobao Chang",
                "Xu Sun",
                "Jingjing Xu",
                "Zhifang Sui."
            ],
            "title": "A survey for in-context learning",
            "venue": "arXiv preprint arXiv:2301.00234.",
            "year": 2022
        },
        {
            "authors": [
                "Tongfeng Guan",
                "Hongying Zan",
                "Xiabing Zhou",
                "Hongfei Xu",
                "Kunli Zhang."
            ],
            "title": "Cmeie: Construction and evaluation of chinese medical information extraction dataset",
            "venue": "Natural Language Processing and Chinese Computing: 9th CCF International Con-",
            "year": 2020
        },
        {
            "authors": [
                "Zan Hongying",
                "Li Wenxin",
                "Zhang Kunli",
                "Ye Yajuan",
                "Chang Baobao",
                "Sui Zhifang."
            ],
            "title": "Building a pediatric medical corpus: Word segmentation and named entity annotation",
            "venue": "Chinese Lexical Semantics: 21st Workshop, CLSW 2020, Hong Kong, China,",
            "year": 2021
        },
        {
            "authors": [
                "Alex Howard",
                "William Hope",
                "Alessandro Gerada"
            ],
            "title": "Chatgpt and antimicrobial advice: the end of the consulting infection doctor? The Lancet Infectious Diseases, 23(4):405\u2013406",
            "year": 2023
        },
        {
            "authors": [
                "Yushi Hu",
                "Chia-Hsuan Lee",
                "Tianbao Xie",
                "Tao Yu",
                "Noah A Smith",
                "Mari Ostendorf."
            ],
            "title": "In-context learning for few-shot dialogue state tracking",
            "venue": "arXiv preprint arXiv:2203.08568.",
            "year": 2022
        },
        {
            "authors": [
                "Ziwei Ji",
                "Nayeon Lee",
                "Rita Frieske",
                "Tiezheng Yu",
                "Dan Su",
                "Yan Xu",
                "Etsuko Ishii",
                "Ye Jin Bang",
                "Andrea Madotto",
                "Pascale Fung."
            ],
            "title": "Survey of hallucination in natural language generation",
            "venue": "ACM Computing Surveys, 55(12):1\u201338.",
            "year": 2023
        },
        {
            "authors": [
                "Jinhyuk Lee",
                "Wonjin Yoon",
                "Sungdong Kim",
                "Donghyeon Kim",
                "Sunkyu Kim",
                "Chan Ho So",
                "Jaewoo Kang."
            ],
            "title": "Biobert: a pre-trained biomedical language representation model for biomedical text mining",
            "venue": "Bioinformatics, 36(4):1234\u20131240.",
            "year": 2020
        },
        {
            "authors": [
                "Young-Jun Lee",
                "Chae-Gyun Lim",
                "Ho-Jin Choi."
            ],
            "title": "Does GPT-3 generate empathetic dialogues? a novel in-context example selection method and automatic evaluation metric for empathetic dialogue generation",
            "venue": "Proceedings of the 29th International",
            "year": 2022
        },
        {
            "authors": [
                "Mike Lewis",
                "Yinhan Liu",
                "Naman Goyal",
                "Marjan Ghazvininejad",
                "Abdelrahman Mohamed",
                "Omer Levy",
                "Ves Stoyanov",
                "Luke Zettlemoyer"
            ],
            "title": "Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension",
            "year": 2019
        },
        {
            "authors": [
                "Dongdong Li",
                "Zhaochun Ren",
                "Pengjie Ren",
                "Zhumin Chen",
                "Miao Fan",
                "Jun Ma",
                "Maarten de Rijke."
            ],
            "title": "Semi-supervised variational reasoning for medical dialogue generation",
            "venue": "Proceedings of the 44th International ACM SIGIR Conference on Re-",
            "year": 2021
        },
        {
            "authors": [
                "Dongdong Li",
                "Zhaochun Ren",
                "Pengjie Ren",
                "Zhumin Chen",
                "Miao Fan",
                "Jun Ma",
                "Maarten de Rijke."
            ],
            "title": "Semi-supervised variational reasoning for medical dialogue generation",
            "venue": "Proceedings of the 44th International ACM SIGIR Conference on Re-",
            "year": 2021
        },
        {
            "authors": [
                "Zekun Li",
                "Wenhu Chen",
                "Shiyang Li",
                "Hong Wang",
                "Jing Qian",
                "Xifeng Yan."
            ],
            "title": "Controllable dialogue simulation with in-context learning",
            "venue": "arXiv preprint arXiv:2210.04185.",
            "year": 2022
        },
        {
            "authors": [
                "Chin-Yew Lin."
            ],
            "title": "ROUGE: A package for automatic evaluation of summaries",
            "venue": "Text Summarization Branches Out, pages 74\u201381, Barcelona, Spain. Association for Computational Linguistics.",
            "year": 2004
        },
        {
            "authors": [
                "Xinzhu Lin",
                "Xiahui He",
                "Qin Chen",
                "Huaixiao Tou",
                "Zhongyu Wei",
                "Ting Chen."
            ],
            "title": "Enhancing dialogue symptom diagnosis with global attention and symptom graph",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Pro-",
            "year": 2019
        },
        {
            "authors": [
                "Wenge Liu",
                "Jianheng Tang",
                "Yi Cheng",
                "Wenjie Li",
                "Yefeng Zheng",
                "Xiaodan Liang."
            ],
            "title": "Meddg: an entity-centric medical consultation dataset for entity-aware medical dialogue generation",
            "venue": "Natural Language Processing and Chinese Computing:",
            "year": 2022
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Roberta: A robustly optimized BERT pretraining approach",
            "venue": "CoRR, abs/1907.11692.",
            "year": 2019
        },
        {
            "authors": [
                "Renqian Luo",
                "Liai Sun",
                "Yingce Xia",
                "Tao Qin",
                "Sheng Zhang",
                "Hoifung Poon",
                "Tie-Yan Liu."
            ],
            "title": "BioGPT: generative pre-trained transformer for biomedical text generation and mining",
            "venue": "Briefings in Bioinformatics, 23(6). Bbac409.",
            "year": 2022
        },
        {
            "authors": [
                "Nicholas Meade",
                "Spandana Gella",
                "Devamanyu Hazarika",
                "Prakhar Gupta",
                "Di Jin",
                "Siva Reddy",
                "Yang Liu",
                "Dilek Hakkani-T\u00fcr."
            ],
            "title": "Using in-context learning to improve dialogue safety",
            "venue": "arXiv preprint arXiv:2302.00871.",
            "year": 2023
        },
        {
            "authors": [
                "Long Ouyang",
                "Jeffrey Wu",
                "Xu Jiang",
                "Diogo Almeida",
                "Carroll Wainwright",
                "Pamela Mishkin",
                "Chong Zhang",
                "Sandhini Agarwal",
                "Katarina Slama",
                "Alex Ray"
            ],
            "title": "Training language models to follow instructions with human",
            "year": 2022
        },
        {
            "authors": [
                "Alec Radford",
                "Jeffrey Wu",
                "Rewon Child",
                "David Luan",
                "Dario Amodei",
                "Ilya Sutskever"
            ],
            "title": "Language models are unsupervised multitask learners",
            "venue": "OpenAI blog,",
            "year": 2019
        },
        {
            "authors": [
                "Nils Reimers",
                "Iryna Gurevych"
            ],
            "title": "Sentence-bert: Sentence embeddings using siamese bert-networks",
            "year": 2019
        },
        {
            "authors": [
                "Julian Risch",
                "Timo M\u00f6ller",
                "Julian Gutsch",
                "Malte Pietsch."
            ],
            "title": "Semantic answer similarity for evaluating question answering models",
            "venue": "arXiv preprint arXiv:2108.06130.",
            "year": 2021
        },
        {
            "authors": [
                "Shamik Roy",
                "Raphael Shu",
                "Nikolaos Pappas",
                "Elman Mansimov",
                "Yi Zhang",
                "Saab Mansour",
                "Dan Roth."
            ],
            "title": "Conversation style transfer using few-shot learning",
            "venue": "arXiv preprint arXiv:2302.08362.",
            "year": 2023
        },
        {
            "authors": [
                "Teven Le Scao",
                "Angela Fan",
                "Christopher Akiki",
                "Ellie Pavlick",
                "Suzana Ili\u0107",
                "Daniel Hesslow",
                "Roman Castagn\u00e9",
                "Alexandra Sasha Luccioni",
                "Fran\u00e7ois Yvon",
                "Matthias Gall\u00e9"
            ],
            "title": "Bloom: A 176bparameter open-access multilingual language model",
            "year": 2022
        },
        {
            "authors": [
                "Karan Singhal",
                "Shekoofeh Azizi",
                "Tao Tu",
                "S Sara Mahdavi",
                "Jason Wei",
                "Hyung Won Chung",
                "Nathan Scales",
                "Ajay Tanwani",
                "Heather Cole-Lewis",
                "Stephen Pfohl"
            ],
            "title": "Large language models encode clinical knowledge",
            "venue": "arXiv preprint arXiv:2212.13138",
            "year": 2022
        },
        {
            "authors": [
                "Yixuan Su",
                "Lei Shu",
                "Elman Mansimov",
                "Arshit Gupta",
                "Deng Cai",
                "Yi-An Lai",
                "Yi Zhang."
            ],
            "title": "Multi-task pre-training for plug-and-play task-oriented dialogue system",
            "venue": "arXiv preprint arXiv:2109.14739.",
            "year": 2021
        },
        {
            "authors": [
                "Haipeng Sun",
                "Junwei Bao",
                "Youzheng Wu",
                "Xiaodong He"
            ],
            "title": "Mars: Semantic-aware contrastive learning for end-to-end task-oriented dialog",
            "year": 2022
        },
        {
            "authors": [
                "Ilya Sutskever",
                "Oriol Vinyals",
                "Quoc V Le."
            ],
            "title": "Sequence to sequence learning with neural networks",
            "venue": "Advances in neural information processing systems,",
            "year": 2014
        },
        {
            "authors": [
                "Chen Tang",
                "Hongbo Zhang",
                "Tyler Loakman",
                "Chenghua Lin",
                "Frank Guerin"
            ],
            "title": "Terminology-aware medical dialogue generation",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2023
        },
        {
            "authors": [
                "Mina Valizadeh",
                "Natalie Parde."
            ],
            "title": "The AI doctor is in: A survey of task-oriented dialogue systems for healthcare applications",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 6638\u2013",
            "year": 2022
        },
        {
            "authors": [
                "Deeksha Varshney",
                "Aizan Zafar",
                "Niranshu Kumar Behera",
                "Asif Ekbal."
            ],
            "title": "Knowledge graph assisted end-to-end medical dialog generation",
            "venue": "Artificial Intelligence in Medicine, page 102535.",
            "year": 2023
        },
        {
            "authors": [
                "Deeksha Varshney",
                "Aizan Zafar",
                "Niranshu Kumar Behera",
                "Asif Ekbal."
            ],
            "title": "Knowledge grounded medical dialogue generation using augmented graphs",
            "venue": "Scientific Reports, 13(1):3310.",
            "year": 2023
        },
        {
            "authors": [
                "Weizhi Wang",
                "Zhirui Zhang",
                "Junliang Guo",
                "Yinpei Dai",
                "Boxing Chen",
                "Weihua Luo."
            ],
            "title": "Task-oriented dialogue system as natural language generation",
            "venue": "Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Infor-",
            "year": 2022
        },
        {
            "authors": [
                "Zhongyu Wei",
                "Qianlong Liu",
                "Baolin Peng",
                "Huaixiao Tou",
                "Ting Chen",
                "Xuanjing Huang",
                "Kam-fai Wong",
                "Xiangying Dai."
            ],
            "title": "Task-oriented dialogue system for automatic diagnosis",
            "venue": "Proceedings of the 56th Annual Meeting of the Association for",
            "year": 2018
        },
        {
            "authors": [
                "Yixuan Weng."
            ],
            "title": "A large chinese medical cqa",
            "venue": "https://github.com/WENGSYX/CMCQA.",
            "year": 2022
        },
        {
            "authors": [
                "Yuan Xia",
                "Jingbo Zhou",
                "Zhenhui Shi",
                "Chao Lu",
                "Haifeng Huang."
            ],
            "title": "Generative adversarial regularized mutual information policy gradient framework for automatic diagnosis",
            "venue": "Proceedings of the AAAI conference on artificial intelligence, volume 34,",
            "year": 2020
        },
        {
            "authors": [
                "Yunyi Yang",
                "Yunhao Li",
                "Xiaojun Quan."
            ],
            "title": "Ubar: Towards fully end-to-end task-oriented dialog system with gpt-2",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 14230\u2013 14238.",
            "year": 2021
        },
        {
            "authors": [
                "Guangtao Zeng",
                "Wenmian Yang",
                "Zeqian Ju",
                "Yue Yang",
                "Sicheng Wang",
                "Ruisi Zhang",
                "Meng Zhou",
                "Jiaqi Zeng",
                "Xiangyu Dong",
                "Ruoyu Zhang",
                "Hongchao Fang",
                "Penghui Zhu",
                "Shu Chen",
                "Pengtao Xie"
            ],
            "title": "MedDialog: Large-scale medical dialogue datasets",
            "year": 2020
        },
        {
            "authors": [
                "Hongbo Zhang",
                "Junying Chen",
                "Feng Jiang",
                "Fei Yu",
                "Zhihong Chen",
                "Jianquan Li",
                "Guiming Chen",
                "Xiangbo Wu",
                "Zhiyi Zhang",
                "Qingying Xiao",
                "Xiang Wan",
                "Benyou Wang",
                "Haizhou Li."
            ],
            "title": "Huatuogpt, towards taming language models to be a doctor",
            "venue": "arXiv",
            "year": 2023
        },
        {
            "authors": [
                "Tianyi Zhang",
                "Varsha Kishore",
                "Felix Wu",
                "Kilian Q Weinberger",
                "Yoav Artzi."
            ],
            "title": "Bertscore: Evaluating text generation with bert",
            "venue": "arXiv preprint arXiv:1904.09675.",
            "year": 2019
        },
        {
            "authors": [
                "Li"
            ],
            "title": "Since our task does not involve interaction with the database, we omit the second step but employ D instead of DB. Considering that the dataset lacks labeled dialogue states and actions, we adopt the approach",
            "year": 2021
        }
    ],
    "sections": [
        {
            "text": "The patient-centered medical dialogue systems strive to offer diagnostic interpretation services to users who are less knowledgeable about medical knowledge, through emphasizing the importance of providing responses specific to the patients. It is difficult for the large language models (LLMs) to guarantee the specificity of responses in spite of its promising performance even in some tasks in medical field. Inspired by in-context learning, we propose PlugMed, a Plug-and-Play Medical Dialogue System, for addressing the challenge. PlugMed is equipped with a prompt generation (PG) module and a response ranking (RR) module to enhances LLMs\u2019 dialogue strategies for improving the specificity of the responses. The PG module is used to stimulate the imitative ability of LLMs by providing them with real dialogues from similar patients as prompts. The RR module incorporates fine-tuned small model as response filter to enable the selection of appropriate responses generated by LLMs. Furthermore, we introduce a new evaluation method based on matching both user\u2019s intent and high-frequency medical term to effectively assess the specificity of the responses. We conduct experimental evaluations on three medical dialogue datasets, and the results, including both automatic and human evaluation, demonstrate the effectiveness of our approach."
        },
        {
            "heading": "1 Introduction",
            "text": "As a key task in health conversational assistants, Medical Dialogue Generation aims to automatically generate informative responses to the users. It\u2019s better for such dialogues to be medical knowledgeable, patient-specific and context-aware, as the patients may be less knowledgeable about medical knowledge and their dialogue contexts may vary.\nPrior studies (Li et al., 2021a; Varshney et al., 2023b,a) have emphasized the critical role of domain knowledge in medical dialogue generation,\nmany of which integrate general medical knowledge from knowledge bases into the models by utilizing knowledge injection mechanism. As the scale of large language models (LLMs) continues to expand, the lack of knowledge is being alleviated. This is evident in the performance of LLMs such as Instruct GPT (Ouyang et al., 2022), which outperform small models based on fine-tuning (Singhal et al., 2022) on medical question answering tasks without any additional training.\nHowever, LLMs have been criticized for lacking medical diagnostic logic to the patient, which can lead to untargeted and even risky response suggestions (Howard et al., 2023; Zhang et al., 2023). For instance, as illustrated in Figure (1), when a patient claims to have a particular ailment and seeks medication, the doctor\u2019s priority is to first investigate the underlying disease to make targeted suggestions. In contrast, LLMs are more inclined to give straightforward medical advice, rather than further gathering patient information to give accurate advice. Once LLMs give overconfident advice, it is difficult for patients to discern the effectiveness and safety of such advice.\nResearches have demonstrated that in-context\nlearning (ICL) possesses the ability to impact the LLMs\u2019 conversational style and mitigate prejudice and toxicity concerns (Roy et al., 2023; Meade et al., 2023) by demonstrating a few examples. This phenomenon is due to the powerful imitation learning and few-shot capabilities of LLMs, by learning patterns from a small number of samples and applying them in generation (Dong et al., 2022).\nTaking inspiration from these studies, we propose leveraging ICL to shape the LLMs\u2019 dialogue strategies and accordingly design a Plug-and-Play Medical Dialogue System, named PlugMed, which embodies two crucial components: a Prompt Generation (PG) Module and a Response Ranking (RR) Module. Specifically, PlugMed uses the PG module to identify examples by considering information from both global and local views. From global view, the PG module choose relevant examples for ensuring that the model acquires a comprehensive understanding of the entire dialogue process by exploiting the similarity with the entire dialogue history. Conversely, from local view, the PG module priorities recent utterances to capture the most relevant information for generating responses. To further maximize advantages of both the global and local views, PlugMed uses the RR module to autonomously select the most appropriate response for the ongoing dialogue through utilizing a finetuned small model.\nAnother critical consideration is about appropriate automatic evaluation metrics for medical dialogue systems. Previous studies (Varshney et al., 2023b; Zhang et al., 2023) only relied on opendomain dialogue evaluation methods. However, as indicated in (Ji et al., 2023), these evaluation methods may be unreliable in task-oriented scenarios. To gain a comprehensive understanding of the system\u2019s real-world performance, we undertake a thorough evaluation that is twofold: the intent accuracy and the high-frequency medical term accuracy. Here, the intent accuracy is used to evaluate the reasonableness of the dialogue actions adopted by the system, and the high-frequency medical term accuracy focuses on measuring the presence of essential medical information in the system\u2019s responses.\nWe evaluate our approach on three widely used large medical datasets, i.e., Meddg, MedDialogue and Kamed datasets. Both automatic and human evaluations show that our approach can substantially improve the specificity of LLMs. Our contributions can be summarized as follows:\n\u2022 An ICL-based approach that enhances LLMs to generate responses that conform to the diagnostic strategy.\n\u2022 The comprehensive evaluation metrics for medical dialogue automation that take both the intent accuracy and the high-frequency medical term accuracy into account.\n\u2022 Experiments demonstrating the key elements of automated medical diagnosis."
        },
        {
            "heading": "2 Methodology",
            "text": ""
        },
        {
            "heading": "2.1 Overview",
            "text": "We propose a framework as shown in Figure (2), which employs ICL to guide the LLM towards generating high-quality replies, where the Prompt Generation (PG) Module and the Response Ranking (RR) Module are two key components. The PG Module accepts the dialogue history as input and outputs multiple In-context prompts, along with a single Instruct prompt. Based on these prompts, the LLM generate multiple system responses. Then, the RR Module utilizes a small language model (SLM) to select the best response. We will elaborate these two components in the following subsections."
        },
        {
            "heading": "2.2 Prompt Generation Module",
            "text": "The PG module uses a multi-strategy retrieval framework to retrieve examples similar to the input sample and then employ them to generate prompts."
        },
        {
            "heading": "2.2.1 Basic Ideas",
            "text": "As shown in \u2018Dialogue History\u2019 at the left part of Figure (2), the main idea is as follows. Firstly, from the global view, we considers the entire history of the dialogue, and the global retriever retrieves the similar dialogues from the the training set of the dataset. Secondly, considering the global view is susceptible to distractions caused by the abundance of irrelevant information, which may lead to inappropriate retrieved examples, we include the local view for enhancing the relevance of the retrieval. Concretely, the local retriever first extracts the patient\u2019s symptom information from past conversations, serving as the initial filter for the samples. Considering the recent rounds of conversations hold the utmost relevance to the systemgenerated responses, we then utilize these conversations as query to select examples. This effectively mitigate the interference of irrelevant information.\nThirdly, we employ the retrieved examples from both views to take advantage of their respective strengths."
        },
        {
            "heading": "2.2.2 Implementation",
            "text": "Global Retriever. The global retriever utilizes the full context of dialogue history as a query for searching samples. It employs Sentence-Bert (SBERT) (Reimers and Gurevych, 2019) to encode the query and examples, and then utilizes cosine similarity to identify the closest examples.\nLocal Retriever. The local retriever retrieves samples in terms of symptoms and recent utterances. For getting symptoms, we develop a medical dialogue summary model that utilizes the ICMSMRG (Chen et al., 2022) dataset in conjunction with BART (Lewis et al., 2019) as the backbone model. This model enables the extraction of the chief complaint, containing the patient symptom information, and we use SBERT as the encoder of chief complaints to provide embeddings for the following operations. We first encode the chief complaints of examples and use the K-Means algorithm to divide them into K groups. Then, we extract the embeddings of each cluster centers to serve as the symptom index for querying. When performing the search, we first retrieve the candidate examples by computing the cosine similarity between the embeddings of the sample\u2019s chief complaint and the symptom index. Then, we use SBERT to retrieve examples from the candidates based on recent dialogue utterances.\nPrompt Generator. The prompt generator generates two types of prompts, as depicted in Figure (2):\n\u2018Instruct prompt\u2019 and \u2018In-context prompts\u2019. Each In-context prompt corresponds to a distinct example retrieval strategy.\nIt is needed to compress examples to include more demonstrations, given the input length constraint imposed by the LLM, when generating the in-context prompts. Drawing inspiration from Hu et al. (2022), for each example, we keep only the most recent rounds of conversations, and we restrict the maximum conversation length to no more than n. Moreover, we employ the previous mentioned chief complaints (up to m characters long) as the dialogue abstract, replacing the excluded history to achieve dialogue compression.\nThe Instruct prompt includes the full context, and we include the instruction before the history to prompt LLM to act as a doctor. We use this prompt to activate the zero-shot capability of LLMs. Appendix E gives some examples of both prompts."
        },
        {
            "heading": "2.3 Response Ranking Module",
            "text": "Through the experimentation, we observe that LLMs tends to generate a significant number of medical terms during dialogue, resulting in more comprehensive responses. However, LLMs often exhibit overconfidence, as they question patients less frequently, resulting in a decrease in the overall quality of responses. On the contrary, small language models (SLMs) that are fine-tuned on a dialogue corpus behave cautiously and tend to include a limited number of medical terms in their output. This phenomenon is illustrated in Figure (3). Acknowledging the complementary nature of LLMs and SLMs, we propose using a SLM to evaluate the responses of the LLM.\nSpecifically, for a given sample with a dialogue history h, we use perplexity as the score of the response r generated by the LLM. We compute the score using the following equation:\ns(r) = \u22121 l l\u2211 i=1 log p(ri | r<i, h; \u03b8) (1)\nHere, l represents the length of response r, and \u03b8 denotes the model parameter of the SLM. This evaluation model uses an encoder-decoder architecture, where h is input to the encoder side of the model, and r<i is input to the decoder side for calculating the generation probabilities. We select the response with the lowest score as the system output."
        },
        {
            "heading": "3 Automatic Evaluation Metrics",
            "text": "We observe that previous studies (Varshney et al., 2023b,a; Zeng et al., 2020) employ the metrics for open domain dialogue tasks that often cannot effectively measure system performance in task-oriented settings (Ji et al., 2023; Risch et al., 2021). From the results given in Figure (3), we observe that the LLM exhibits overconfidence. To judge whether our approach alleviates the problem, it becomes crucial to evaluate the dialogue actions taken by the LLM. It is equally important to evaluate the dialogue content generated by LLM. Hence, we introduce two metrics that consider both intent and the usage of high-frequency medical terms."
        },
        {
            "heading": "3.1 Intent Evaluation",
            "text": "The intent accuracy (Int) is used to assess the conformity of the system\u2019s response to the ground-truth in terms of dialogue actions. We train a medical dialogue intent classifier for calculating Int, and Int is calculated by the following formula:\nInt = 1\nN N\u2211 i=1 f(Predi, Goldeni) (2)\nHere, N represents the total number of samples. Predi and Goldeni denote the model\u2019s predicted response and the corresponding actual response, respectively. The function f(\u00b7, \u00b7) evaluates the intentions, which are extracted by the aforementioned intent classifier, assigning a value of 1 if the intentions of the two responses are the same, and a value of 0 otherwise. Appendix A.1 presents the implementation details of the intent classifier."
        },
        {
            "heading": "3.2 Medical Term Evaluation",
            "text": "We evaluate the completeness and correctness of the responses using the micro-f1 score, which measures the overlap of high-frequency medical terms between the ground-truth and the prediction. We need to avoid relying solely on exact matching when measuring the term overlap. For instance, different doctors may prescribe different medications for some diseases that may have same effect on the patient. Employing exact matching alone may result in an underestimation of the performance exhibited by models that possess the ability to generate diverse treatment options.\nHence, we introduce a novel approach called Top-n Match (TnM) to address the problem. Concretely, let T = {t1, t2, ..., t|T |} be a set consisting of |T | terms, and let s be a similarity score function that satisfies 0 \u2264 s(ti, tj) \u2264 1 for all ti, tj \u2208 T . For a given term ti \u2208 T , we define Sni (s, ti) \u2286 T as the set of n terms that are closest to ti based on the similarity function s. We say that ti and tj are Top-N Match if and only if Sni (s, ti) \u2229 Snj (s, tj) \u0338= \u2205. TnM with different n can represent the term-matching scores for different similarity levels. We present the f1-score results using T3M configurations. Appendix A.2 and A.3 will give more details about the term extraction and matching."
        },
        {
            "heading": "4 Experiments",
            "text": "This section delineates the evaluation setup and the results of the proposed approach in the context of\nmedical dialogue generation."
        },
        {
            "heading": "4.1 Datasets and Evaluation Metrics",
            "text": "We conduct experiments on three large datasets for our evaluation. 1) The Meddg dataset (Liu et al., 2022) which is collected from Doctor Chunyu1 and consists of 17,864 dialogue sessions. 2) The MedDialogue-CN dataset (Zeng et al., 2020) which is collected from HaoDaifu2 and comprises 38,723 dialogues without any provided annotations. 3) The KaMed dataset (Li et al., 2021b) which is also collected from Doctor Chunyu, but at a larger scale, containing 63,754 dialogue sessions. Statistics of three datasets are presented in Table (1).\nTo evaluate the quality of generation, we employ the Rouge-L (Lin, 2004) and Bert-Score (Zhang et al., 2019) to measure the overall similarity between the generated text and ground-truth. We also utilize micro-F1 scores for entity matching in T3M settings to assess medical term correctness based on the aforementioned definitions. Additionally, we employ the INT metric to measure the accuracy of the intended responses. The validity analysis of each metric can be found in the Section 4.8."
        },
        {
            "heading": "4.2 Implementations",
            "text": "Our approach employs BLOOM as the foundation model, and to ensure reproducibility, we avoid any form of sampling and instead utilize a greedy decoding strategy. We generate a set of four prompts for a given sample, including an Instruction prompt (referred to as \u201cVanilla\u201d) and three In-context prompts. These prompts are as follows: 1) Vanilla: This strategy instructs the model to act as a doctor by prefixing the samples with an instruction. 2) Global View: This strategy involves looking for examples through a global retriever, using full context as query. 3) Local Primary: In this strategy, we first consider the patient\u2019s chief\n1https://www.chunyuyisheng.com/ 2https://haodf.com\ncomplaint to retrieve examples with similar symptoms. From these examples, a number of samples are randomly selected to generate the prompt. This strategy corresponds to the initial step of the local retriever\u2019s two-step search. 4) Local Secondary: This strategy searches for examples through a local retriever and utilizes a full two-step search. All examples are from the training set of the dataset.\nLimited by input length, the In-context prompt contains 4 examples, while each example is limited to the last 140 (m = 20, n = 120) tokens. When constructing the symptom index using the K-Means algorithm, we set the number of cluster centers to 100 and iterate 20 times. During response ranking, BART serves as the scoring model. We conduct the experiments using PyTorch3 and Huggingface Inference API4."
        },
        {
            "heading": "4.3 Baselines",
            "text": "Fine-tuning Based Baselines. These baselines utilize small language models as the backbone, trained on the aforementioned datasets for the medical dialogue generation task, which include the following models: 1) Bart (Lewis et al., 2019), a well-known encoder-decoder model that is recognized for its text generation capabilities. 2) Mars (Sun et al., 2022), an advanced model explicitly crafted for the MultiWoZ (Budzianowski et al., 2018) dataset, renowned for its exceptional performance in addressing the Task-Oriented Dialogue (TOD) task. We migrated Mars into our tasks and trained it to focus on the generation of medical terms. Appendix B gives the further information on the migration process.\nLLM Baselines. These baselines employ large language models to generate dialogue responses. Our comparison targets include: 1) Bloom (Scao et al., 2022), a widely used open-source multilingual language model with 176 billion parameters. 2) Bloomz (Scao et al., 2022), an instruction-tuned model derived from Bloom and specializes in zeroshot tasks. In addition to these models that use Instruct-prompt as input, we also include two baselines that utilize In-context prompts as input: 1) ICL Rand, which selects a set of dialogue examples randomly to construct the prompt. 2) ICL Sbert, which utilizes Sbert (Reimers and Gurevych, 2019) to encode the dialogue history and find the\n3https://pytorch.org/ 4https://huggingface.co/docs/api-inference/index\nclosest examples to the given sample. Both baselines use Bloom as the fundamental model."
        },
        {
            "heading": "4.4 Overall Performance",
            "text": "Table (2) present the automatic evaluation results on Meddg, MedDialog, and KaMed. Remarkably, PlugMed consistently attains the top-ranking positions across a majority of the metrics and achieves best performance in T3M, outperforming even the strongest baseline. Meanwhile, among all baselines leveraging the LLM, PlugMed generates responses with the most reasonable intent.\nOur analysis uncovers interesting observations. Firstly, we discovered that fine-tuned small models tend to have higher INT scores but lower term matching-related scores. This suggests that while these models excel in emulating the dialogue actions of doctors, they often struggle to generate appropriate medical terminology due to their limited medical knowledge. Secondly, we observed that Bloomz performed worse than Bloom, indicating that the instruct-tuning process may compromise the diagnostic capability of the model. More details can be found in Section 4.6."
        },
        {
            "heading": "4.5 Ablation Study",
            "text": "To investigate the influence of different prompt generation strategies on system responses and the efficacy of the RR module, we conduct ablation experiments. The experimental results, shown in Table (3), indicate that both the global and local view effectively enhance the quality of the LLM output. Moreover, we observe a substantial en-\nhancement in the quality of system responses due to the integration of the RR module. This observation underscores the efficacy of the fine-tuned SLM in effectively evaluating the LLM\u2019s output.\nFurthermore, we observe that Global View demonstrates comparable performance to PlugMed when assessed using the MedDialogue dataset. However, PlugMed significantly surpasses Global View on the Meddg and KaMed datasets. We attribute this discrepancy to the MedDialogue dataset\u2019s characteristic of having shorter dialog histories, averaging only 4.76 rounds. Conversely, Global View\u2019s retrieval effectiveness diminishes as the samples\u2019 length increases, as elaborated in Section 2.2.1, making it less effective than PlugMed on the other two datasets. This underscores the synergistic relationship between Global View and Local View, highlighting their complementary strengths.\nTo delve deeper into the contributions of these strategies to the outcomes, we utilize the RR module to rank the responses corresponding to each strategy, documenting the percentage of times each strategy achieves the top rank. The results are illustrated in Figure (4). Based on the findings depicted in the figure, we can infer that the responses generated by the Vanilla strategy exhibit significantly inferior quality compared to those produced by the other three strategies. Furthermore, the probability of being selected by the remaining three strategies is approximately equal, indicating their complementary nature."
        },
        {
            "heading": "4.6 Analysis of Generalization Capacity",
            "text": "We conducted experiments using ChatGPT and Bloomz to assess the effectiveness of our approach in terms of generalization. The results, as presented in Table (3) for ChatGPT, revealed that our approach successfully enhances ChatGPT\u2019s performance in completing medical conversations. However, it was observed that ChatGPT\u2019s capability for multi-round conversations lags behind that of\nBloom. This discrepancy can be attributed to the safety protocols integrated into ChatGPT by OpenAI. Specifically, ChatGPT often advises patients to seek professional assistance instead of providing diagnoses, which affects its multi-round conversation potential. Conversely, we noticed that our approach has minimal impact on Bloomz, as detailed in Appendix C. This disparity can be attributed to Bloomz\u2019s training dataset, bigscience/xP3 5, which primarily comprises single-round Q&A tasks, making it less adaptable to multi-round conversations. To summarize, the generalization ability of our method is influenced by the model\u2019s pre-training task, and improving our method\u2019s effectiveness entails pre-training models based on multi-round dialogues and reducing safety interventions."
        },
        {
            "heading": "4.7 Human Evaluation",
            "text": "We manually evaluate seven selected dialogue models to conduct a comprehensive comparison. We ensure a thorough analysis by randomly selecting 100 samples from each dataset. Each sample is examined by three physicians who assign scores based on the criteria outlined in Table (4). The evaluators check the responses in the following order: Role Consistency, Empathy, Correctness, Neces-\n5https://huggingface.co/datasets/bigscience/xP3\nsity, and Richness. This order also corresponds to the importance of the check items, and subsequent items only had test significance if the model satisfied the previous check item. Therefore, we apply the following rules for scoring. If a target response successfully pass a particular test, it receives a score of 20. However, if the target response does not pass a specific test, it is assigned a score of 0 for all subsequent check items.\nWe compute the average score of each model on the dataset and summarize the evaluation results in Table (5). Our analysis reveals that PlugMed exhibits the highest performance based on human evaluations. However, there still exists a significant gap between the responses generated by the models and the human responses, indicating that the models have not fully grasped the diagnostic capabilities. Additionally, we observe that BART performs well in the manual evaluation, primarily because we give a very low priority to richness. Appendix D provides a case study for illustrating this finding."
        },
        {
            "heading": "4.8 Analysis of Evaluation Metrics",
            "text": "Within this section, we have included an evaluation of the metric\u2019s reliability. To gauge the quality of these metrics, we employed the Pearson correlation\ncoefficient to quantify their alignment with human evaluations. A score approaching 1 indicates a stronger metric performance. The corresponding outcomes can be found in Table (5).\nOur examination indicates that the INT metric surpasses all others in performance, with BertScore coming in a close second. These two metrics exhibit stronger correlations with human assessment, indicating the superiority of a semantically based evaluation. Simultaneously, this outcome implies that the model\u2019s intent should align closely with the corpus. Lower scores for BLEU (Chen and Cherry, 2014) and higher scores for Rouge-L suggest a preference among individuals for more comprehensive model responses over precisionoriented ones. Lastly, based on the outcomes of T1M (exact match), T3M, and T5M, it becomes evident that factoring in term similarity is imperative when calculating term matching scores."
        },
        {
            "heading": "5 Related Work",
            "text": ""
        },
        {
            "heading": "5.1 Medical Dialogue Systems",
            "text": "According to system architecture, medical dialogue systems can be of two types: the pipeline and the end-to-end systems (Valizadeh and Parde, 2022). The pipeline systems usually involve four steps: the natural language understanding, the dialogue state\ntracking, the dialogue action generation, and the natural language generation. Wei et al. (2018); Xia et al. (2020) propose to learn the dialogue policies for automated diagnosis using reinforcement learning. In other studies, Lin et al. (2019) proposes to model the associations between symptoms by constructing a symptom graph, aiming to enhance symptom diagnosis performance. Li et al. (2021a) proposes to use symptoms and diseases as keys to generate responses instead of dialogue states and actions based on a knowledge graph.\nThe end-to-end models, usually a sequence-tosequence architecture (Sutskever et al., 2014), first attracted attention. The fine-tuning pre-trained models, such as GPT-2 (Radford et al., 2019), have been proven effective in task-based dialogue scenarios, as demonstrated by Su et al. (2021); Yang et al. (2021); Wang et al. (2022). BioBERT (Lee et al., 2020) and BioGPT (Luo et al., 2022) try to improve the performance by employing pre-training on medical corpora. Varshney et al. (2023b,a); Tang et al. (2023) propose explicitly using the Unified Medical Language System (UMLS) to incorporate knowledge in the dialogue generation process.\nIn summary, existing studies mainly emphasize the role of knowledge enhancement based on small models. In contrast, our work is conducted on knowledge-rich LLMs, emphasizing the enhancement of dialogue strategies."
        },
        {
            "heading": "5.2 ICL for Dialogue",
            "text": "As LLMs continue to advance, ICL has emerged as a new paradigm in natural language processing. The exploration of ICL\u2019s ability to evaluate and infer LLMs has become a prominent trend (Dong et al., 2022). Some studies have tried to apply ICL in dialogue generation. Roy et al. (2023) proposes a two-stage style transfer framework to leverage ICL for dialogue style transfer. Meade et al. (2023); Lee et al. (2022) employ a retrieval-based framework to mitigate bias and toxicity in chatbot-generated responses, guiding the model towards safer and more responsible dialogue. Hu et al. (2022); Chen et al. (2023) propose a method for long dialogue compression, enabling each prompt to contain more examples and improving dialogue state tracking performance. ICL has also been utilized for unsupervised generation of dialogue data in certain contexts (Li et al., 2022), expanding the potential applications of this approach.\nOverall, the aforementioned works concentrate\non either example retrieval or dialogue compression. In contrast, our work combines the two techniques for integrating multiple retrieval strategies."
        },
        {
            "heading": "6 Conclusion",
            "text": "In this paper, we use in-context learning to develop a patient-centered medical dialogue model. To this end, we introduce a Prompt Generation module capable of generating LLM input from both global and local views. Additionally, we construct a Response Ranking module using a supervised trained small model to filter the LLM output. Experimental results indicate that the responses generated by PlugMed exhibit a greater inclusion of comprehensive medical terms and PlugMed yields more accurate dialogue intents than other large language model based methods.\nLimitations\nBased on human evaluation, we have identified shortcomings in PlugMed\u2019s diagnostic efficiency. This suggests that PlugMed has difficulty rapidly identifying the patient\u2019s disease, which may lead to an increase in average conversational discourse and harm the patient\u2019s experience. Our future work will prioritize addressing this issue."
        },
        {
            "heading": "7 Acknowledgement",
            "text": "Our work is supported by the National Key Research and Development Program of China (Project Number: 2020AAA0109400). We kindly appreciate all the researchers who provide valuable insights, discussions, and comments on this work."
        },
        {
            "heading": "A Details of Automatic Evaluation",
            "text": "A.1 Intent Classifier\nTo compute Int, we employ the IMCS-IR dataset (Chen et al., 2022) and utilize Robertalarge (Liu et al., 2019) as the backbone model for training the classifier. To process a response, we concatenate it with the dialogue history and input the combined sequence into the model for classification. The input format is structured as follows:\n<s> [dialogue history] </s> [response]\nWe extract the hidden layer vector of the \u2018<s>\u2018 token as the embedding representation for the response, which is subsequently classified using a two-layer neural network with a hidden dimension of 768. The intentions considered are presented in Table (6), and the model achieves an accuracy of 0.86 on the validation set.\nA.2 Term Extraction\nFollowing the implementation outlined in (Li et al., 2021a), we employ word matching to extract highfrequency terms from responses. To accomplish this, we initially construct a glossary of medical high-frequency words. We then appropriately expand the glossary to ensure a satisfactory hit rate for term matching. Our approach is detailed as follows.\nInitialization of the glossary. We utilized the Chinese medical high-frequency vocabulary acquired from THUOCL6 as the initial glossary, comprising 18,749 frequently employed terms in doctor-patient communication.\nGlossary expansion. We utilize the skip-gram algorithm to expand the list of high-frequency terms.\n6https://github.com/thunlp/THUOCL\nOur approach involves using the skip-gram algorithm to discover synonyms of the initial vocabulary and incorporate them into the vocabulary. This method helps us identify aliases and common names of the terms and facilitates the calculation of the TnM f1-score by providing term similarity information. To accomplish this, we train skipgram word embeddings using the CMCQA (Weng, 2022) corpus, which consists of 1.3 million complete conversations, 19.83 million sentences, and 650 million tokens. Our skip-gram algorithm employs a word vector dimension of 300, a window size of 5, a sub-sample ratio of 3, and undergoes 8 rounds of training. Using the obtained word vectors, we enrich the high-frequency terminology list by adding the 10 synonyms with the closest semantic meanings for each term. Subsequently, we save the term vectors obtained from the training to facilitate the calculation of similarity scores.\nDiscussion. It is worth to note that Meddg (Liu et al., 2022) and a few other datasets propose similar evaluation metrics based on medical entity matching. This raises the question of why we are considering medical terminology matching instead of medical entity matching. We chose terminology matching instead of entity matching because we observe several issues.\nFirstly, most medical named entity recognition (NER) datasets (Hongying et al., 2021; Guan et al., 2020) focus on recognizing a limited range of entity types, which often excludes entities crucial to the consultation process, such as etiology related to eating habits. Secondly, since our target audience is patients without medical expertise, the dialogue content incorporates numerous colloquial words. This poses challenges for NER models that are primarily designed for professional terminology. Consequently, the traditional NER model can only identify a limited number of entities within the dataset. For example, in the report of Meddg, an average sentence contains only 0.56 entities, which hampers the evaluation of the dialogue content.\nTherefore, we believe that evaluating highfrequency medical terms would be a more logical approach. The glossary we selected offering an 81.9% coverage of the labeled entity classes in Meddg and featuring an average of 2.13 entities per utterance. Table (7) illustrates the comparison between Meddg\u2019s original entity extraction and our improved term extraction, clearly showing that our extracted terms better cover the dialogue content.\nA.3 Term Matching\nLet Ai = {ai1, .., ain} represent the set of n terms in the system response i, and Bi = {bi1, ..., bim} represent the set of m terms in the standard response i. We use the cosine similarity of the skip-gram vector corresponding to the term as the required similarity score for Top-n Match. To calculate the f1-score, we define the following term types:\n\u2022 Truth-Positive (TP): For any aiu \u2208 Ai, if there exists biv \u2208 Bi Top-n Matches with it, then we classify aiu as a TP term.\n\u2022 False-Positive (FP): If aiu is not a TP term, it is classified as an FP term.\n\u2022 False-Negative (FN): If biv does not top-n match with any aiu \u2208 Ai, we classify biv as an FN term.\nThen the calculation formula for micro-f1 is as follows, where N denotes the number of samples, and i denotes the index of the samples.\nP = \u2211N i TPi\u2211\ni TPi + \u2211 i FPi (3)\nR = \u2211N i TPi\u2211\ni TPi + \u2211 i FNi (4)\nF = 2PR\nP +R (5)\nB Implementation of Mars\nThe overview of the migration process are shown in Figure (5). Mars (Sun et al., 2022) incorporates two decoders, namely the Dialogue State Decoder and the Action State Decoder, which both utilize a Shared Encoder. The original workflow of Mars is as follows: 1) Utilize the Shared Encoder to encode the context C and user input U , and employ the Dialogue State Decoder to decode the dialogue state D. 2) Employ the dialogue state to retrieve the corresponding entity DB from the database. 3) Utilize Shared Encoder to encode C, U , and DB, and employ the Action State Decoder to decode the dialogue action A and dialogue response R.\nSince our task does not involve interaction with the database, we omit the second step but employ D instead of DB. Considering that the dataset lacks labeled dialogue states and actions, we adopt the approach of Li et al. (2021a) and utilize highfrequency medical vocabulary found in sentences as the states and actions. Our subsequent experiments show that this approach facilitates the model to generate more medical terms while improving the fluency of the output. We keep the other settings the same as the original configuration.\nC ICL on Bloomz\nThis section explores the adaptability of Bloomz to In-Context Learning. We observed that the impact\nof ICL on BLoomz was minimal. Adjusting the order of the examples or changing the number of examples did not result in significant changes in the model\u2019s performance. To illustrate this phenomenon, we provide the details of the following experiment.\nC.1 Experimental Setup\nIn order to evaluate the impact of In-Context Learning (ICL) on the performance of Bloom and Bloomz, we conducted experiments on the validation set of the KaMed dataset. For each sample in the validation set, we generated 10 prompts, where each prompt consisted of a random number of randomly selected examples. The maximum number of examples included in a prompt was limited to less than 4. Using these 10 prompts, we made predictions using Bloom and Bloomz models respectively. We then count the number of non-repeats of the system\u2019s responses.\nC.2 Experimental Result\nThe test results are presented in Figure (6). The horizontal axis of the figure represents the number of unique outputs obtained from the 10 prompts per sample, while the vertical axis represents the percentage of samples in the dataset that have that specific number of unique outputs. From the figure, we can observe that Bloom generates almost different answers for ICL prompts containing different examples. In contrast, Bloomz generates more duplicate answers, indicating that Bloomz is less influenced by the sample. The observation that Bloomz\u2019s dialogue strategy is less malleable, suggests that the Instruct-tuning process may compromise the model\u2019s In-Context Learning capabilities on the medical dialogue generation task."
        },
        {
            "heading": "D Case Study",
            "text": "We selected a representative sample from the KaMed dataset to facilitate an interpretable comparison of different models. The corresponding results are presented in Table (8). In the scenario depicted within this sample, the patient\u2019s mother provided limited information to the doctor, which is insufficient to support an accurate diagnosis. Consequently, the doctor made the decision to gather more necessary symptom information.\nOur observations reveal that the Bart model exhibit an awareness of the information gap. However, Bart inquire about a known symptom, indicating that the small model lacks certain medical common sense and doesn\u2019t know what information to collect. Mars places greater emphasis on generating pertinent medical terms. In the absence of other\ninformation, Mars choose to reiterate the patient\u2019s words in an attempt to enhance the likelihood of a terminological hit. Unfortunately, this approach result in the generation of responses of lower quality. The LLM produce lengthier responses, while ICL Rand and ICL Sbert attempt to provide a diagnosis directly. On the other hand, Bloom and Bloomz generate false and biased utterances, respectively, indicating a dearth of diagnostic strategies within these larger models. In contrast, PlugMed generate responses similar to the ground-turth, thus suggesting the effectiveness of our proposed method."
        },
        {
            "heading": "E Prompt Format",
            "text": "Our experiments employ two prompts: the Instruct prompt, shown in Figure (7), and the In-context prompt, shown in Figure (8). The Instruct prompt consists of a concise statement of the task goal, followed by direct input of all conversation histories into the model, serving as a test for the model\u2019s zero-shot capability. On the other hand, the Incontext prompt involves presenting 4 examples prior to the samples to activate the model\u2019s imitation ability."
        }
    ],
    "title": "PlugMed: Improving Specificity in Patient-Centered Medical Dialogue Generation using In-Context Learning",
    "year": 2023
}