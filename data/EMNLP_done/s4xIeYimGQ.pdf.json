{
    "abstractText": "Recently, with the chain of thought (CoT) prompting, large language models (LLMs), e.g., GPT-3, have shown strong reasoning ability in several natural language processing tasks such as arithmetic, commonsense, and logical reasoning. However, LLMs with CoT require multi-step prompting and multi-token prediction, which is highly sensitive to individual mistakes and vulnerable to error accumulation. The above issues make the LLMs need the ability to verify the answers. In fact, after inferring conclusions in some thinking decision tasks, people often check them by re-verifying steps to avoid some mistakes. In this paper, we propose and prove that LLMs also have similar self-verification abilities. We take the conclusion obtained by CoT as one of the conditions for solving the original problem. By performing a backward verification of the answers that LLM deduced for itself, we can obtain interpretable answer validation scores to select the candidate answer with the highest score. Experimental results demonstrate that the proposed method can improve the reasoning performance on various arithmetic, commonsense, and logical reasoning datasets. Our code is publicly available at: https://github.com/WENGSYX/ Self-Verification.",
    "authors": [
        {
            "affiliations": [],
            "name": "Yixuan Weng"
        },
        {
            "affiliations": [],
            "name": "Minjun Zhu"
        },
        {
            "affiliations": [],
            "name": "Fei Xia"
        },
        {
            "affiliations": [],
            "name": "Bin Li"
        },
        {
            "affiliations": [],
            "name": "Shizhu He"
        },
        {
            "affiliations": [],
            "name": "Shengping Liu"
        },
        {
            "affiliations": [],
            "name": "Bin Sun"
        },
        {
            "affiliations": [],
            "name": "Kang Liu"
        },
        {
            "affiliations": [],
            "name": "Jun Zhao"
        }
    ],
    "id": "SP:c189e75e9307ac4f7c27e7e184ecb42d3296bb6d",
    "references": [
        {
            "authors": [
                "Ekin Aky\u00fcrek",
                "Dale Schuurmans",
                "Jacob Andreas",
                "Tengyu Ma",
                "Denny Zhou."
            ],
            "title": "What learning algorithm is in-context learning? investigations with linear models",
            "venue": "arXiv preprint arXiv:2211.15661.",
            "year": 2022
        },
        {
            "authors": [
                "Aida Amini",
                "Saadia Gabriel",
                "Peter Lin",
                "Rik KoncelKedziorski",
                "Yejin Choi",
                "Hannaneh Hajishirzi."
            ],
            "title": "Mathqa: Towards interpretable math word problem solving with operation-based formalisms",
            "venue": "north american chapter of the association for com-",
            "year": 2019
        },
        {
            "authors": [
                "Patel Arkil",
                "Bhattamishra Satwik",
                "Goyal Navin"
            ],
            "title": "Are nlp models really able to solve simple math word problems",
            "year": 2021
        },
        {
            "authors": [
                "Simran Arora",
                "Avanika Narayan",
                "Mayee F. Chen",
                "Laurel J. Orr",
                "Neel Guha",
                "Kush Bhatia",
                "Ines Chami",
                "Frederic Sala",
                "Christopher R\u00e9"
            ],
            "title": "Ask me anything: A simple strategy for prompting language models",
            "year": 2022
        },
        {
            "authors": [
                "Akari Asai",
                "Hannaneh Hajishirzi."
            ],
            "title": "Logicguided data augmentation and regularization for consistent question answering",
            "venue": "meeting of the association for computational linguistics.",
            "year": 2020
        },
        {
            "authors": [
                "Chandra Bhagavatula",
                "Ronan Le Bras",
                "Chaitanya Malaviya",
                "Keisuke Sakaguchi",
                "Ari Holtzman",
                "Hannah Rashkin",
                "Doug Downey",
                "Scott Wen tau Yih",
                "Yejin Choi."
            ],
            "title": "Abductive commonsense reasoning",
            "venue": "Learning.",
            "year": 2019
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners. Advances in neural information processing",
            "year": 2020
        },
        {
            "authors": [
                "Morikawa",
                "Alec Radford",
                "Matthew Knight",
                "Miles Brundage",
                "Mira Murati",
                "Katie Mayer",
                "Peter Welinder",
                "Bob McGrew",
                "Dario Amodei",
                "Sam McCandlish",
                "Ilya Sutskever",
                "Wojciech Zaremba."
            ],
            "title": "Evaluating large language models trained on code",
            "venue": "CoRR,",
            "year": 2021
        },
        {
            "authors": [
                "Bing Qin",
                "Ting Liu"
            ],
            "title": "A survey of chain of thought reasoning: Advances, frontiers and future",
            "year": 2023
        },
        {
            "authors": [
                "Karl Cobbe",
                "Vineet Kosaraju",
                "Mohammad Bavarian",
                "Jacob Hilton",
                "Reiichiro Nakano",
                "Christopher Hesse",
                "John Schulman."
            ],
            "title": "Training verifiers to solve math word problems",
            "venue": "arXiv preprint arXiv:2110.14168.",
            "year": 2021
        },
        {
            "authors": [
                "Xiang Deng",
                "Yu Su",
                "Alyssa Lees",
                "You Wu",
                "Cong Yu",
                "Huan Sun."
            ],
            "title": "Reasonbert: Pre-trained to reason with distant supervision",
            "venue": "empirical methods in natural language processing.",
            "year": 2021
        },
        {
            "authors": [
                "Shehzaad Dhuliawala",
                "Mojtaba Komeili",
                "Jing Xu",
                "Roberta Raileanu",
                "Xian Li",
                "Asli Celikyilmaz",
                "Jason Weston."
            ],
            "title": "Chain-of-verification reduces hallucination in large language models",
            "venue": "ArXiv, abs/2309.11495.",
            "year": 2023
        },
        {
            "authors": [
                "Paul Erd."
            ],
            "title": "On a new law of large numbers",
            "venue": "J. Anal. Math, 23(103111):8.",
            "year": 1970
        },
        {
            "authors": [
                "Yao Fu",
                "Hao Peng",
                "Ashish Sabharwal",
                "Peter Clark",
                "Tushar Khot"
            ],
            "title": "Complexity-based prompting for multi-step reasoning",
            "year": 2022
        },
        {
            "authors": [
                "Luyu Gao",
                "Aman Madaan",
                "Shuyan Zhou",
                "Uri Alon",
                "Pengfei Liu",
                "Yiming Yang",
                "Jamie Callan",
                "Graham Neubig"
            ],
            "title": "Pal: Program-aided language models",
            "year": 2023
        },
        {
            "authors": [
                "Mor Geva",
                "Daniel Khashabi",
                "Elad Segal",
                "Tushar Khot",
                "Dan Roth",
                "Jonathan Berant."
            ],
            "title": "Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies",
            "venue": "Transactions of the Association for Computational Linguistics.",
            "year": 2021
        },
        {
            "authors": [
                "Yuxian Gu",
                "Jiaxin Wen",
                "Hao Sun",
                "Yi Song",
                "Pei Ke",
                "Chujie Zheng",
                "Zheng Zhang",
                "Jianzhu Yao",
                "Lei Liu",
                "Xiaoyan Zhu",
                "Minlie Huang"
            ],
            "title": "Eva2.0: Investigating open-domain chinese dialogue systems with large-scale pre-training",
            "venue": "Machine Intelligence",
            "year": 2023
        },
        {
            "authors": [
                "Charles R Harris",
                "K Jarrod Millman",
                "St\u00e9fan J Van Der Walt",
                "Ralf Gommers",
                "Pauli Virtanen",
                "David Cournapeau",
                "Eric Wieser",
                "Julian Taylor",
                "Sebastian Berg",
                "Nathaniel J Smith"
            ],
            "title": "Array programming with numpy",
            "year": 2020
        },
        {
            "authors": [
                "Namgyu Ho",
                "Laura Schmid",
                "Se-Young Yun"
            ],
            "title": "Large language models are reasoning teachers",
            "year": 2022
        },
        {
            "authors": [
                "Mohammad Javad Hosseini",
                "Hannaneh Hajishirzi",
                "Oren Etzioni",
                "Nate Kushman."
            ],
            "title": "Learning to solve arithmetic word problems with verb categorization",
            "venue": "empirical methods in natural language processing.",
            "year": 2014
        },
        {
            "authors": [
                "Minghao Hu",
                "Yuxing Peng",
                "Zhen Huang",
                "Dongsheng Li."
            ],
            "title": "A multi-type multi-span network for reading comprehension that requires discrete reasoning",
            "venue": "empirical methods in natural language processing.",
            "year": 2019
        },
        {
            "authors": [
                "Tianbo Ji",
                "Chenyang Lyu",
                "Gareth Jones",
                "Liting Zhou",
                "Yvette Graham"
            ],
            "title": "Qascore \u2013 an unsupervised unreferenced metric for the question generation evaluation",
            "year": 2022
        },
        {
            "authors": [
                "Takeshi Kojima",
                "Shixiang Shane Gu",
                "Machel Reid",
                "Yutaka Matsuo",
                "Yusuke Iwasawa."
            ],
            "title": "Large language models are zero-shot reasoners",
            "venue": "Advances in Neural Information Processing Systems.",
            "year": 2022
        },
        {
            "authors": [
                "Rik Koncel-Kedziorski",
                "Hannaneh Hajishirzi",
                "Ashish Sabharwal",
                "Oren Etzioni",
                "Siena Dumas Ang."
            ],
            "title": "Parsing algebraic word problems into equations",
            "venue": "Transactions of the Association for Computational Linguistics.",
            "year": 2015
        },
        {
            "authors": [
                "Nate Kushman",
                "Yoav Artzi",
                "Luke Zettlemoyer",
                "Regina Barzilay."
            ],
            "title": "Learning to automatically solve algebra word problems",
            "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages",
            "year": 2014
        },
        {
            "authors": [
                "Bin Li",
                "Encheng Chen",
                "Hongru Liu",
                "Yixuan Weng",
                "Bin Sun",
                "Shutao Li",
                "Yongping Bai",
                "Meiling Hu."
            ],
            "title": "More but correct: Generating diversified and entity-revised medical response",
            "venue": "arXiv preprint arXiv:2108.01266.",
            "year": 2021
        },
        {
            "authors": [
                "Jierui Li",
                "Lei Wang",
                "Jipeng Zhang",
                "Yan Wang",
                "Bing Tian Dai",
                "Dongxiang Zhang."
            ],
            "title": "Modeling intrarelation in math word problems with different functional multi-head attentions",
            "venue": "meeting of the association for computational linguistics.",
            "year": 2019
        },
        {
            "authors": [
                "Yifei Li",
                "Zeqi Lin",
                "Shizhuo Zhang",
                "Qiang Fu",
                "Bei Chen",
                "Jian-Guang Lou",
                "Weizhu Chen."
            ],
            "title": "On the advance of making language models better reasoners",
            "venue": "arXiv preprint arXiv:2206.02336.",
            "year": 2022
        },
        {
            "authors": [
                "Hunter Lightman",
                "Vineet Kosaraju",
                "Yura Burda",
                "Harrison Edwards",
                "Bowen Baker",
                "Teddy Lee",
                "Jan Leike",
                "John Schulman",
                "Ilya Sutskever",
                "Karl Cobbe."
            ],
            "title": "Let\u2019s verify step by step",
            "venue": "ArXiv, abs/2305.20050.",
            "year": 2023
        },
        {
            "authors": [
                "Wang Ling",
                "Dani Yogatama",
                "Chris Dyer",
                "Phil Blunsom."
            ],
            "title": "Program induction by rationale generation : Learning to solve and explain algebraic word problems",
            "venue": "meeting of the association for computational linguistics.",
            "year": 2017
        },
        {
            "authors": [
                "Jian Liu",
                "Leyang Cui",
                "Hanmeng Liu",
                "Dandan Huang",
                "Yile Wang",
                "Yue Zhang."
            ],
            "title": "Logiqa: A challenge dataset for machine reading comprehension with logical reasoning",
            "venue": "international joint conference on artificial intelligence.",
            "year": 2020
        },
        {
            "authors": [
                "Jinghui Lu",
                "Rui Zhao",
                "Brian Mac Namee",
                "Dongsheng Zhu",
                "Weidong Han",
                "Fei Tan"
            ],
            "title": "What makes pre-trained language models better zero/few-shot learners",
            "year": 2022
        },
        {
            "authors": [
                "Aman Madaan",
                "Shuyan Zhou",
                "Uri Alon",
                "Yiming Yang",
                "Graham Neubig"
            ],
            "title": "Language models of code are few-shot commonsense learners",
            "year": 2022
        },
        {
            "authors": [
                "Paul Christiano",
                "Jan Leike",
                "Ryan Lowe"
            ],
            "title": "Training language models to follow instructions with human feedback",
            "year": 2022
        },
        {
            "authors": [
                "Arkil Patel",
                "Satwik Bhattamishra",
                "Navin Goyal."
            ],
            "title": "Are nlp models really able to solve simple math word problems",
            "venue": "north american chapter of the association for computational linguistics.",
            "year": 2021
        },
        {
            "authors": [
                "Xinyu Pi",
                "Qian Liu",
                "Bei Chen",
                "Morteza Ziyadi",
                "Zeqi Lin",
                "Yan Gao",
                "Qiang Fu",
                "Jian-Guang Lou",
                "Weizhu Chen"
            ],
            "title": "Reasoning like program executors",
            "year": 2022
        },
        {
            "authors": [
                "David L Poole",
                "Alan K Mackworth."
            ],
            "title": "Artificial Intelligence: foundations of computational agents",
            "venue": "Cambridge University Press.",
            "year": 2010
        },
        {
            "authors": [
                "Shuofei Qiao",
                "Yixin Ou",
                "Ningyu Zhang",
                "Xiang Chen",
                "Yunzhi Yao",
                "Shumin Deng",
                "Chuanqi Tan",
                "Fei Huang",
                "Huajun Chen"
            ],
            "title": "Reasoning with language model prompting: A survey",
            "year": 2022
        },
        {
            "authors": [
                "Alec Radford",
                "Jeffrey Wu",
                "Rewon Child",
                "David Luan",
                "Dario Amodei",
                "Ilya Sutskever"
            ],
            "title": "Language models are unsupervised multitask learners",
            "venue": "OpenAI blog,",
            "year": 2019
        },
        {
            "authors": [
                "Ayoub",
                "Jeff Stanway",
                "Lorrayne Bennett",
                "Demis Hassabis",
                "Koray Kavukcuoglu",
                "Geoffrey Irving."
            ],
            "title": "Scaling language models: Methods, analysis & insights from training gopher",
            "venue": "CoRR, abs/2112.11446.",
            "year": 2021
        },
        {
            "authors": [
                "Subhro Roy",
                "Dan Roth."
            ],
            "title": "Solving general arithmetic word problems",
            "venue": "arXiv: Computation and Language.",
            "year": 2016
        },
        {
            "authors": [
                "Abulhair Saparov",
                "He He"
            ],
            "title": "Language models are greedy reasoners: A systematic formal analysis of chain-of-thought",
            "year": 2022
        },
        {
            "authors": [
                "Jianhao Shen",
                "Yichun Yin",
                "Lin Li",
                "Lifeng Shang",
                "Xin Jiang",
                "Ming Zhang",
                "Qun Liu."
            ],
            "title": "Generate & rank: A multi-task framework for math word problems",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2021, pages 2269\u20132279.",
            "year": 2021
        },
        {
            "authors": [
                "Yibin Shen",
                "Cheqing Jin."
            ],
            "title": "Solving math word problems with multi-encoders and multi-decoders",
            "venue": "international conference on computational linguistics.",
            "year": 2020
        },
        {
            "authors": [
                "Yu Hou",
                "Yufang Hou",
                "Yuntao Bai",
                "Zachary Seid",
                "Zhuoye Zhao",
                "Zijian Wang",
                "Zijie J. Wang",
                "Zirui Wang",
                "Ziyi Wu"
            ],
            "title": "Beyond the imitation game: Quantifying and extrapolating the capabilities of language models",
            "year": 2022
        },
        {
            "authors": [
                "Tian-Xiang Sun",
                "Xiang-Yang Liu",
                "Xi-Peng Qiu",
                "Xuan-Jing Huang."
            ],
            "title": "Paradigm shift in natural language processing",
            "venue": "Machine Intelligence Research, 19(3):169\u2013183.",
            "year": 2022
        },
        {
            "authors": [
                "Xingwu Sun",
                "Jing Liu",
                "Yajuan Lyu",
                "Wei He",
                "Yanjun Ma",
                "Shi Wang."
            ],
            "title": "Answer-focused and positionaware neural question generation",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3930\u20133939, Brus-",
            "year": 2018
        },
        {
            "authors": [
                "Alon Talmor",
                "Jonathan Herzig",
                "Nicholas Lourie",
                "Jonathan Berant."
            ],
            "title": "Commonsenseqa: A question answering challenge targeting commonsense knowledge",
            "venue": "north american chapter of the association for computational linguistics.",
            "year": 2018
        },
        {
            "authors": [
                "Boshi Wang",
                "Sewon Min",
                "Xiang Deng",
                "Jiaming Shen",
                "You Wu",
                "Luke Zettlemoyer",
                "Huan Sun."
            ],
            "title": "Towards understanding chain-of-thought prompting: An empirical study of what matters",
            "venue": "Annual Meeting of the Association for Computational Linguistics.",
            "year": 2022
        },
        {
            "authors": [
                "Lei Wang",
                "Yan Wang",
                "Deng Cai",
                "Dongxiang Zhang",
                "Xiaojiang Liu."
            ],
            "title": "Translating a math word problem to a expression tree",
            "venue": "empirical methods in natural language processing.",
            "year": 2018
        },
        {
            "authors": [
                "Siyuan Wang",
                "Wanjun Zhong",
                "Duyu Tang",
                "Zhongyu Wei",
                "Zhihao Fan",
                "Daxin Jiang",
                "Ming Zhou",
                "Nan Duan."
            ],
            "title": "Logic-driven context extension and data augmentation for logical reasoning of text",
            "venue": "Findings of the Association for Computational Lin-",
            "year": 2022
        },
        {
            "authors": [
                "Xinyi Wang",
                "Wanrong Zhu",
                "Michael Saxon",
                "Mark Steyvers",
                "William Yang Wang"
            ],
            "title": "2023a. Large language models are implicitly topic models: Explaining and finding good demonstrations for in-context learning",
            "year": 2023
        },
        {
            "authors": [
                "Xuezhi Wang",
                "Jason Wei",
                "Dale Schuurmans",
                "Quoc V Le",
                "Ed H. Chi",
                "Sharan Narang",
                "Aakanksha Chowdhery",
                "Denny Zhou."
            ],
            "title": "Self-consistency improves chain of thought reasoning in language models",
            "venue": "The Eleventh International Conference on Learning",
            "year": 2023
        },
        {
            "authors": [
                "Xuezhi Wang",
                "Jason Wei",
                "Dale Schuurmans",
                "Quoc V Le",
                "Ed H. Chi",
                "Sharan Narang",
                "Aakanksha Chowdhery",
                "Denny Zhou."
            ],
            "title": "Self-consistency improves chain of thought reasoning in language models",
            "venue": "The Eleventh International Conference on Learning",
            "year": 2023
        },
        {
            "authors": [
                "Jason Wei",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Maarten Bosma",
                "Ed Chi",
                "Quoc Le",
                "Denny Zhou."
            ],
            "title": "Chain of thought prompting elicits reasoning in large language models",
            "venue": "arXiv preprint arXiv:2201.11903.",
            "year": 2022
        },
        {
            "authors": [
                "Yixuan Weng",
                "Bin Li",
                "Fei Xia",
                "Minjun Zhu",
                "Bing Sun",
                "Shizhu He",
                "Kang Liu",
                "Jun Zhao."
            ],
            "title": "Large language models need holistically thought in medical conversational qa",
            "venue": "ArXiv, abs/2305.05410.",
            "year": 2023
        },
        {
            "authors": [
                "Yixuan Weng",
                "Minjun Zhu",
                "Fei Xia",
                "Bin Li",
                "Shizhu He",
                "Kang Liu",
                "Jun Zhao"
            ],
            "title": "Mastering symbolic operations: Augmenting language models with compiled neural networks",
            "year": 2023
        },
        {
            "authors": [
                "Fei Xia",
                "Bin Li",
                "Yixuan Weng",
                "Shizhu He",
                "Kang Liu",
                "Bin Sun",
                "Shutao Li",
                "Jun Zhao."
            ],
            "title": "Medconqa: Medical conversational question answering system based on knowledge graphs",
            "venue": "Conference on Empirical Methods in Natural Language Processing.",
            "year": 2022
        },
        {
            "authors": [
                "Yisheng Xiao",
                "Lijun Wu",
                "Junliang Guo",
                "Juntao Li",
                "Min Zhang",
                "Tao Qin",
                "Tie-yan Liu"
            ],
            "title": "A survey on non-autoregressive generation for neural machine translation and beyond",
            "year": 2022
        },
        {
            "authors": [
                "Yichong Xu",
                "Chenguang Zhu",
                "Shuohang Wang",
                "Siqi Sun",
                "Hao Cheng",
                "Xiaodong Liu",
                "Jianfeng Gao",
                "Pengcheng He",
                "Michael Zeng",
                "Xuedong Huang"
            ],
            "title": "Human parity on commonsenseqa: Augmenting self-attention with external attention",
            "year": 2022
        },
        {
            "authors": [
                "Ori Yoran",
                "Alon Talmor",
                "Jonathan Berant."
            ],
            "title": "Turning tables: Generating examples from semistructured tables for endowing language models with reasoning skills",
            "venue": "arXiv: Computation and Language.",
            "year": 2021
        },
        {
            "authors": [
                "Weihao Yu",
                "Zihang Jiang",
                "Yanfei Dong",
                "Jiashi Feng."
            ],
            "title": "Reclor: A reading comprehension dataset requiring logical reasoning",
            "venue": "international conference on learning representations.",
            "year": 2020
        },
        {
            "authors": [
                "Zihan Yu",
                "Liang He",
                "Zhen Wu",
                "Xinyu Dai",
                "Jiajun Chen"
            ],
            "title": "Towards better chain-of-thought prompting strategies: A survey",
            "year": 2023
        },
        {
            "authors": [
                "Zhuosheng Zhang",
                "Aston Zhang",
                "Mu Li",
                "Alex Smola"
            ],
            "title": "Automatic chain of thought prompting in large language models",
            "year": 2022
        },
        {
            "authors": [
                "Denny Zhou",
                "Nathanael Sch\u00e4rli",
                "Le Hou",
                "Jason Wei",
                "Nathan Scales",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Claire Cui",
                "Olivier Bousquet",
                "Quoc Le",
                "Ed Chi"
            ],
            "title": "Least-to-most prompting enables complex reasoning in large language models",
            "year": 2023
        },
        {
            "authors": [
                "Minjun Zhu",
                "Bin Li",
                "Yixuan Weng",
                "F. Xia."
            ],
            "title": "A knowledge storage and semantic space alignment method for multi-documents dialogue generation",
            "venue": "Workshop on Document-grounded Dialogue and Conversational Question Answering.",
            "year": 2022
        },
        {
            "authors": [
                "Minjun Zhu",
                "Yixuan Weng",
                "Shizhu He",
                "Kang Liu",
                "Jun Zhao."
            ],
            "title": "Reasonchainqa: Text-based complex question answering with explainable evidence chains",
            "venue": "ArXiv, abs/2210.08763.",
            "year": 2022
        },
        {
            "authors": [
                "Minjun Zhu",
                "Yixuan Weng",
                "Shizhu He",
                "Cunguang Wang",
                "Kang Liu",
                "Li Cai",
                "Jun Zhao."
            ],
            "title": "Learning to build reasoning chains by reliable path retrieval",
            "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "The ability of reasoning in the process of thinking and decision-making is an essential aspect of human intelligence. Recently, chain of thought (CoT) prompting (Wei et al., 2022) has been a good way to solve the arithmetic, commonsense, and logical reasoning tasks with large language models (LLMs), which help the LLMs simulating the human thinking process when solving complex natural language processing (NLP) tasks. CoT guides LLMs to generate a series of intermediate reasoning steps to address complex problems rather than just predict\n\u2217 : These authors contribute this work equally; \u2020 : Corresponding author.\na final answer. This approach has been shown the advance performances on several challenging NLP tasks, even when using only a few or no training samples (Madaan et al., 2022; Saparov and He, 2022; Fu et al., 2022; Gu et al., 2023).\nAlthough CoT can enable LLMs to solve complex reasoning tasks, it is highly sensitive to individual mistakes and vulnerable to error accumulation (Shen et al., 2021). If a tiny mistake occurs, it can change the meaning deviations of the whole statement (Xiao et al., 2022), leading to incorrect answers (Cobbe et al., 2021). That is especially problematic in using CoT for addressing multi-step precise reasoning (such as mathematical calculation). Due to the lack of the error correction mechanism, it is difficult for the LLMs to obtain correct results from the possible errors in multiple steps reasoning. Detecting and mitigating errors is essential to improve reasoning capabilities.\nPrevious methods resolve the above issue by training another verifier to evaluate the correctness of the model\u2019s output (Shen et al., 2021; Li et al., 2022). However, there are some drawbacks in these work. On the one hand, training a verifier requires a lot of human annotations and additional fine-tuned models, which limits its widespread use in other tasks and domains. On the other hand, the verifier fine-tuned by a language model is not easily explainable, making it difficult to assess the model\u2019s reliability based on its output scores. Therefore, the challenge of obtaining a better reasoner based on the LLMs is to get a verifier that can avoid manual annotation and additional training, so that it can be better extended and migrated to other fields and tasks.\nTo address this challenge and overcome the limitations of training verifiers, we propose utilizing LLMs as reasoners with self-verification for selecting better prediction results. In numerous decision-making tasks, humans often perform selfverification of inferred conclusions to mitigate mis-\nQ: Jackie has 10 apples. Adam has 8 apples. How many more apples does Jackie have than Adam? A: 2\n\u5177\u4f53\u65b9\u6cd5\n\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd: \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\n\ufffd1 \ufffd2 \ufffd\ufffd\nJackie have 2 apples than Adam.\n(\ufffd1 \u2227 \ufffd2) \u22a2 \ufffd\ufffd (\ufffd1 \u2227 \ufffd\ufffd) \u22a2 \ufffd2\n(\ufffd2 \u2227 \ufffd\ufffd) \u22a2 \ufffd1 [Adam has 8 apples. Jackie have 2 apples than Adam.] \u2192 [Jackie has 10 apples]\n[Jackie has 10 apples. Jackie have 2 apples than Adam.] \u2192 [Adam has 8 apples.]\n\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\n\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\nFigure 1: The answer of a question can be verified by masking and predicting the conditions of the original contexts. To mimic the self-verification ability of human, we predict the accuracy of fC by predicting the original conditions f1 or f2 is right or not based on this conclusion.\ntakes (Poole and Mackworth, 2010). In this paper, we propose and demonstrate that LLMs possess a similar self-verification ability, the better reasoning with CoT is carried out in the following two steps, Forward Reasoning and Backward Verification. Specifically, in Forward Reasoning, LLM reasoners generate candidate answers using CoT, and the question and candidate answers form different conclusions to be verified. And in Backward Verification, We mask the original condition and predict its result using another CoT. We rank candidate conclusions based on a verification score, which is calculated by assessing the consistency between the predicted and original condition values. For example, as shown in Figure 1, by taking f2 and fC as conditions to predict the value of condition attribute in f\u03021, the correctness of fC can be evaluated by comparing the consistency of values of the predicted f\u03021 and the original f1 in verification.\nOur method employs LLMs for self-verification with only a few prompts, eliminating the need for fine-tuning or gradient updating. This approach enables automatic verification of multiple candidate answers and corresponding conclusions, mitigating deviations from the correct thought chain in the original CoT. Our verification score arises from evaluating each step during the backward verification phase, rather than from the direct output of a neural network model (Cobbe et al., 2021; Li et al., 2022), enhancing the explainability of prediction outcomes and solution processes (Li et al., 2021; Yu et al., 2023; Zhu et al., 2023). We conducted experiments on various open-source datasets for mathematical reasoning, common sense, and logical reasoning tasks, achieving results beyond the baseline (e.g., 60.8 \u2192 65.1 on GSM8K, 91.01 \u2192 93.40 on SingleEq). In addition, we also attempt to combine our method with some approaches to improving\nforward reasoning, such as self-consistency (Wang et al., 2023b) and Least-to-Most (Zhou et al., 2023). The experimental results show that our method also improves upon these forward reasoning approaches.\nOur contributions are summarized as follows:\n1. We propose and prove that large language models (LLMs) can self-verify their prediction results. The proposed method can provide interpretable verification scores without the need for train additional verifiers.\n2. We have conducted extensive of experiments with multiple LLMs, and the experimental results on multiple mathematical, commonsense, and logical reasoning datasets achieve a significant improvement compared to the baseline.\n3. We introduced True-False Item Verification for General Tasks in the backward verification stage and proposed Condition Mask Verification based on the characteristics of Arithmetic Tasks. Our method can be applied to a wide range of reasoning datasets, potentially paving the way for self-validation to become a new paradigm following pre-training and prompt learning, thus motivating further exploration of the capabilities of LLMs."
        },
        {
            "heading": "2 Related Work",
            "text": "Language Model Reasoning. It has been extensively studied in order to evaluate the various reasoning abilities of language models (Arora et al., 2022; Madaan et al., 2022; Sun et al., 2022), including arithmetic reasoning (Koncel-Kedziorski et al., 2015; Roy and Roth, 2016; Patel et al., 2021; Cobbe et al., 2021), commonsense reasoning (Talmor et al., 2018; Bhagavatula et al., 2019; Geva\net al., 2021; Zhu et al., 2022b), and logical reasoning (Liu et al., 2020; Yu et al., 2020). To solve these reasoning tasks, researchers have utilized pre-trained language reasoning models (Asai and Hajishirzi, 2020; Deng et al., 2021; Xia et al., 2022) or fine-tuned general LLMs (Cobbe et al., 2021). Early work attempted to solve complex reasoning tasks using Seq2Seq models (Wang et al., 2018; Li et al., 2019). Later, specialized encoderdecoder architectures were designed to improve reasoning performance (Shen and Jin, 2020; Zhu et al., 2022a). More recent work has suggested to adopt pre-training tasks to improve arithmetic reasoning ability (Yoran et al., 2021; Wang et al., 2022b). However, these methods require a significant amount of human annotation. In this paper, we proposed to obtain answers automatically and verify them in multiple reasoning tasks.\nIn-context Learning. Large language models such as GPT-3 exhibit impressive few-shot learning ability (Lu et al., 2022; Qiao et al., 2022), and closely approximate the predictors computed by gradient descent (Aky\u00fcrek et al., 2022). It requires only filling a few exemplars into context as prompts and without the need for finetuning on a dataset of training examples (Wang et al., 2022a; Weng et al., 2023a). However, this approach struggles with tasks requiring complex reasoning (Rae et al., 2021), which drives researchers to explore other prompting strategies. CoT (Wei et al., 2022) is a chained reasoning approach that inserts a multi-step reasoning path before generating the final answer. Wang et al. (2023c) proposed a selfconsistency decoding strategy to vote on the reasoning path, and Kojima et al. (2022) demonstrated that LLMs could as zero-shot reasoners through the prompt \u201cLet\u2019s think step-by-step\u201d. These methods focus on constructing the CoT but ignore the high sensitivity of LLMs to individual mistakes in generating these chains, so some of these conclusions by CoT may be unreliable (Dhuliawala et al., 2023; Chu et al., 2023; Weng et al., 2023b). In this paper, we proved that LLMs can self-verify their conclusions.\nAnswer Verification. It is a common method for evaluating and reordering candidate answers with a trained language understanding model. Kushman et al. (2014) train a classifier to select the best answer from candidate answers, while Roy and Roth (2016) train a global scoring model to guide the search process for better answers. Shen et al.\n(2021) proposed the joint training of answer generation and rank with language model. Cobbe et al. (2021) and Lightman et al. (2023) fine-tunes language model as a verifier, which calculates tokenlevel and solution-level verification scores for a predicate result. However, the above method all need additional annotations. In our work, we do not require training examples and can provide an explainable verification score."
        },
        {
            "heading": "3 The Proposed Method",
            "text": "The proposed method can be used to verify prediction results. As shown in Figure 2, the process mainly consists of two steps. The first step, forward reasoning, is similar to the normal CoT, except that multiple candidate answers are generated through sampling decoding. In the second step, we calculate the verification scores for each candidate\u2019s answer by the self-verification method, and the answer with the highest score is selected as the final answer."
        },
        {
            "heading": "3.1 Forward Reasoning",
            "text": "In forward reasoning, the LLM reasoners generate candidate answers with the chain of thought prompting. We augment the input with several CoT prompts similar to the original query and then send it to the LLM. The LLM then performs sampling decoding to generate multiple candidates for verification.\nAs shown in Figure 2, for a reasoning task, the large language model LLM is given a question X which is accompanied by a chain of thought set C. In few-shot setting, the whole prompt also contains other question-CoT prompt-answer tuples. The input X can be further subdivided into X = {f1, f2, . . . , fR, q}, where each fi is a condition (fact), and q is a question, both represented as natural language clause or sub-sentences.\nSpecifically, in order to generate step-by-step solutions with CoT, we followed Wei et al. (2022) and designed CoT prompt set C for the reasoning dataset (e.g., the GSM8K dataset), which contains n samples, each sample has the question X\u0307 , chain of thout t\u0307, and the answer y\u0307. These samples are used as the input of test-time. Each example in C is concatenated as a prompt:\nC = (X\u03070, t\u03070, y\u03070); (X\u03071, t\u03071, y\u03071); . . . ; (X\u0307n, t\u0307n, y\u0307n)\nTherefore, LLM is required to follow the prompt of C to generate the chain of thought tCoT before generating the final answer y:\nP(y|C,X ) = P(tCoT|C,X )\u00d7 P)(y|C,X , tCoT)\nTo ensure the diversity of different answers, we adapt sampling decoding (Radford et al., 2019) to generate multiple y for K times. Specifically, sampling decoding is a random decoding method, which can select the next word by sampling from a probability distribution over the possible words at each step. Multiple candidate answers can be obtained when repeatedly using sampling decoding. For example, we generate \u201c18\u201d and \u201c2\u201d as candidate answers in the example of Figure 2."
        },
        {
            "heading": "3.2 Backward Verification",
            "text": "Step 1 may generate multiple different answers, this step is used to verify and select the best answer. Backward verification involves several sub-steps. First, the original question with each candidate\u2019s answer is rewritten as a conclusion and then supplemented as a new condition (incarnadine color in Figure 2). Then, we considered two methods to construct new questions. In the general QA task, the True-False Item Verification is given based on all the conditions, asking the LLM whether these conditions are mutually satisfied, it has a broad applicability. In Arithmetic reasoning tasks, as the definite condition masks can indicate the reasoning direction of the language model, we propose the Condition Mask Verification method to design questions for the verification stage. Finally,\nwe perform multiple experiments to compute the verification score by comparing the consistency between the predicted condition value and the original masked condition value, and select the candidate answer with the highest score as the final answer."
        },
        {
            "heading": "3.2.1 Rewritten Candidate Conclusion",
            "text": "Besides, we rewrite the original question with the candidate\u2019s answer as a conclusion and then supplement it as a new condition in the backward verification step. Specifically, we use the instruction prompt \u201cPlease change the questions and answers into complete declarative sentences [q] The answer is [y]\u201d to change q and y into new declarative sentence fY by LLM. As shown in Figure 2, we can rewrite the question and conclusion as \u201cJackie has 18 apples more than Adam\u201d."
        },
        {
            "heading": "3.2.2 Condition Masking",
            "text": "For question generation, the diversity of the problems makes it difficult to balance the need for coherence and fact consistency between questions and answers in practical operation (Sun et al., 2018; Ji et al., 2022). To tackle this issue, we included clear questions asking the language model to accurately predict.\nTrue-False Item Verification (TFV). This approach can be applied to a wide range of reasoning QA tasks. We directly add \"Do it is correct (True or False)?\" after all the conditions, requiring the LLM to self-evaluate the correctness of these conditions.\nCondition Mask Verification (CMV). Further,\nwe use regular expressions to filter out specific conditions, such as numbers, and then mask them in turn. If we do not mask all conditions but randomly select a condition, unnecessary conditions may be masked, which will significantly impact the verification answer. For example, \u201cDana worked 9 hours on Friday, 10 hours on Saturday, and 3 hours on Sunday. She earns $13 per hour. How much money did Dana earn in weekend?\u201d, since condition 1 (9 hours) does not affect the conclusion, it is difficult to predict it correctly. We replace all occurrences of f in the original X with \u201cX\u201d in turn, and ask LLM to re-predict it. Then we rewrite the question. For example, we might find a value in f1 and replace it with \u201cX\u201d. We can then add \u201cWhat is the answer of \u2018X\u2019?\u201d to the end of the new question, effectively turning it into an equation. This technique helps to guide the language model towards the correct answer."
        },
        {
            "heading": "3.2.3 Verification Score Calculation",
            "text": "This backward verification chain of thought is similar to solving an equation. We design a chain of thought prompt, like forward reasoning, to guide LLM in generating a solving process. We input the newly constructed sentences into LLM. For TFV, we can directly count the number of answers that are True as the score, and for CMV, we will match its final result with the masked condition.\nDue to the limited performance of LLM itself, if the condition is verified only once in the backward verification step, it is easy to have the same score, resulting in a lack of differentiation. To address this, we repeat the sampling decoding process P times, so that the verification score can more accurately reflect the model\u2019s confidence for a given conclusion (Erd, 1970).\nThe verification score is calculated as follows:\nScorey =\n{\u2211P p=1( \u2211R r=1 1(LLMp(X\u2212fr+fY )=fr)) TFV\u2211P\np=1(1(LLMp(X+fY ))) CMV\nWhere 1(\u2022) is an indicator function. Finally, we select the one with the highest verification score from the K candidate answers generated as a result.\nOutput = argmax k\u2208[0,K] (Scorek)\nFor example for CMV, in Figure 2.3)Verification, we match the results generated by the selfverification of LLM with the masked conditions.\nThere is one \u201c10\u201d in the conclusion of A1, so the verification score is 1. There are four correct results in A2, so the verification score is 4, and we finally choose A2, which has the highest verification score, as the final conclusion."
        },
        {
            "heading": "4 Experiment Setting",
            "text": ""
        },
        {
            "heading": "4.1 Task and Dataset",
            "text": "We evaluated eight datasets on three reasoning tasks: arithmetic reasoning, commonsense reasoning, and logical reasoning. These datasets are highly heterogeneous in terms of their input formats (see Appendix A.2 for the detailed description of each dataset. Examples of different datasets are given in Table 7 of Appendix A.4).\n\u2022 Arithmetic Reasoning. We performed experiments on the following 6 arithmetic datasets: SingleEq (Koncel-Kedziorski et al., 2015), AddSub (Hosseini et al., 2014), MultiArith (Roy and Roth, 2016), AQUA-RAT (Ling et al., 2017), GSM8K (Cobbe et al., 2021), and SVAMP (Arkil et al., 2021).\n\u2022 Commonsense Reasoning. CommonsenseQA (CSQA) (Talmor et al., 2018) is the most typical dataset of the task, which requires commonsense knowledge about the world to accurately answer questions with complex meanings.\n\u2022 Logical Reasoning. Date Understanding (DU) (Srivastava et al., 2022) involves inferring a date from a given context."
        },
        {
            "heading": "4.2 Model",
            "text": "We conducted experiments to evaluate the original GPT-3 (Chen et al., 2021) (code-davinci-001) model and the Instruct-GPT model (Ouyang et al., 2022) (code-davinci-002). Additionally, we conducted analysis experiments with public GPT-3 (Brown et al., 2020). All prediction results of different reasoning tasks and datasets are obtained by OpenAI\u2019s API 1. Appendix A.3 shows the reproducibility statement."
        },
        {
            "heading": "4.3 Prompts",
            "text": "We conducted all experiments in the few-shot setting without any fine-tuning of the original LLM To ensure a fair comparison, we used the same\n1OpenAI\u2019s API: https://openai.com/api/\nprompts as in Wei et al. (2022) for forward reasoning. We made several changes of the prompts for backward verification (the details are shown in Appendix A.5)."
        },
        {
            "heading": "4.4 Implementation",
            "text": "In each experiment, we perform CoT prompting on the LLMs, then LLMs generate conclusions (answers) by sampling decoding without top-k truncation. When forward reasoning, we generated K = 5 candidate answers (conclusions). In backward verification, each candidate conclusion generated P = 10 times, and the maximum token length of each decoding was 168. After LLM generates the output, we only select the part of the text that conforms to the conclusion format. Appendix A.1 shows the specific strategy for different tasks. In addition, to ensure a fair comparison, we ran each experiment three times and calculated the average result."
        },
        {
            "heading": "5 Result and Analysis",
            "text": "The main experimental results are shown in Table 1. The table shows that the proposed self-verification method (SV) can improve previous methods in all datasets. Our method achieved a new stateof-the-art (SOTA) performance in six of these eight datasets. Appendix A.4 shows specific examples of language model self-verification for each dataset. Additionally, we observed that self-verification led to an average increase of 2.33% in the highperforming Instruct-GPT model, which indicates that the model with strong forward reasoning capabilities also benefits from the self-verification mechanism. The detailed experimental conclusions and analysis are described as follows:\nThe current self-verification method is more suitable for arithmetic reasoning tasks than other reasoning tasks. We find that the average performance improvement of arithmetic reasoning tasks (1.67%/2.84% \u2191) is higher than that of other reasoning tasks (0.62%/0.78% \u2191) in Table 1. We believe the reason is that it is easier to find the required mask conditions for arithmetic reasoning\n0.4B 7B 175B (A) GSM8K\n0\n20\n40\n60\n80\n100\n0.4B 7B 175B (B) Multiarith\n0\n20\n40\n60\n80\n100\n0.4B 7B 175B (C) SVAMP\n0\n20\n40\n60\n80\n100 Self-Verification\nCoT\n(a) Problem solve rate (%) in difference size models. The text-ada-001 (0.4B), text-babbage-001 (1.3B), text-curie-001 (7B) and text-davinci-002 (175B) models are used respectively.\n0.4B 1.3B 7B 175B -1.0\n-0.5\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nSe lf -V er ic at io n Su bt ra ct C oT GSM8K Multiarith SVAMP\n(b) Subtract the problem solve rate (%) of CoT from the problem solve rate (%) of self-verification in different size models. The pink area means that the use of self-verification has a negative impact.\nFigure 3: The self-verification ability of models with different sizes.\ntasks, but other reasoning tasks used TFV that cannot determine the exact conditions. In the future, we will consider the targeted condition selection and masking for other reasoning tasks."
        },
        {
            "heading": "The self-validation method can be combined",
            "text": "with improved methods for forward reasoning. We report the results of combining selfconsistency or PoL at the bottom of Table 1 separately. Specifically, for combining self-consistency, we use the Top-2 candidate results obtained from self-consistency in the Forward Reasoning stage and then use self-validation to re-rank the candidate results; for combining PAL, we require the generation of runnable programs in Forward Reasoning to obtain candidate answers. We find that this approach still can achieve better performance than self-consistency, demonstrating that self-verification can be combined with a series of existing methods for improving forward calculation to achieve further gains. We believe that the self-\nverification can re-rank candidate answers from the perspective of backward validation, providing more robust results.\nLarger language models are better reasoners with self-verification. Figure 3 shows the capability of GPT-3 models with parameters ranging from 0.4B to 175B. The experimental results suggest that the self-verification ability tends to be more robust as the number of parameters increases, aligning with the limited experimental results in Wei et al. (2022). This indicates that self-verification is an emergent property that arises in larger models, where stronger reasoning capacities allow them to derive reliable and accurate conclusions during the context learning process (Ho et al., 2022; Wang et al., 2023a). Consequently, their performance in the backward verification phase is also enhanced. However, smaller models are prone to generate erroneous text during the generation process, and augment them with self-verification abilities presents a challenge.\nWith the different number of few-shots, the reasoning ability of models using selfverification has significantly improved. Figure 4 demonstrate the impact of different sample sizes on three arithmetic reasoning datasets. We observe that the self-verification method exhibits greater robustness with smaller samples, even with only 2-shots (At this time, it has 99.6% performance of 8-shot, while CoT has only 98.7%). In addition, we find that even with only four samples (2 CoT samples + 2 self-verification samples), self-verification outperforms the CoT with eight samples, which\nhighlights the importance of answer verification in scenarios of limited data."
        },
        {
            "heading": "The more verification conditions are used, the",
            "text": "better self-verification reasoning ability. We observed the effect of using the single conditional mask on six different arithmetic datasets for Condition Mask Verificat in Figure 5. As each number in these datasets\u2019 input can be thought of as a condition, we can study the impact of increasing the number of validation conditions. In most experiments, we found that the multi-condition mask performed better than the single-condition mask, and both performed better than the original CoT. These results suggest that the accuracy of verification scores improves as the number of available conditions increases.\nMasked conditions can guide the LLMs to reason more effectively. As shown in Figure 6, we compared the results of using CMV (Conditional Masked Verification) and TFV (Token Form Verification) for self-verification. We found that the performance of CMV is generally better than TFV. We believe this is because the lack of explicit goals can lead to a lack of use of existing conclusions, so CMV is more helpful in stimulating the self-\nverification ability of the model. However, due to its simplicity, TFV can be applied to a variety of tasks (including common sense reasoning and logical reasoning, both with improvements compared to the CoT baseline) for self-verification, making it highly adaptable to different scenarios.\nFewer computational resources can also improve performance through self-verification. In Figure 7, we show the results of changing the number of P generated in Backward Verification. We find that even when P = 2, only a small increase in computational overhead is needed, and there is still an improvement in CoT baseline. Considering that performance starts to slowly increase when P is\nincreased to 10, we recommend choosing an appropriate value for P (e.g. P=10) to achieve a balance between performance and resource consumption."
        },
        {
            "heading": "Analysis of the accuracy of the verification",
            "text": "results. We conducted additional analysis of the InstructGPT results in Table 1 under a new setting where we only considered cases in the forward inference phase that contained one correct answer and N incorrect answers (where N>=1 and N<=4). The results in Table 2 provide further evidence that the proposed self-verification technique can effectively improve the accuracy of commonsense reasoning models. Across all 6 datasets, the verification accuracy is consistently and considerably higher than both the random guessing baseline and the standalone CoT model accuracy. For example, on the challenging GSM8K dataset, the verification stage obtains 58.9% accuracy, substantially outperforming the 53.4% CoT accuracy and 35.7% random guess accuracy. The largest accuracy gains are witnessed on the MultiArith and SingleEq datasets, where the verification stage lifts the accuracy by 17.8% and 23.5% respectively over the CoT model. This indicates that the self-verification technique is particularly adept at rectifying errors made by the CoT model on arithmetic and symbolic equation problems. The consistent accuracy improvements demonstrate that allowing the model to verify its own predictions provides a simple yet effective way to enhance commonsense reasoning. These comprehensive results validate self-verification as a promising approach to refine LLMs and reduce reasoning errors."
        },
        {
            "heading": "6 Conclusion",
            "text": "In this study, we show that large language models have a strong ability to self-verification, allowing\nthem to assess the conclusions they generate accurately. We propose a novel method that uses self-verification to generate interpretable scores for ranking results in few-shot tasks. Our approach demonstrates the potential of using self-verification to improve the accuracy and reliability of large language models in reasoning tasks. By relying on the self-verification ability of large language models, we significantly improved the accuracy of three types of reasoning tasks. All in all, we posit that the self-verification capability of large language models will have an extensive positive impact, as it enables the enhancement of their reasoning abilities through a simple process of self-verification."
        },
        {
            "heading": "Limitations",
            "text": "Our self-verification method relies on large language models (LLMs). It provides few-shot prompts to guide the model in verifying its own results, but it is worth noting that these prompts are artificially constructed and may introduce bias. The effectiveness of our method is limited by the presence of accurate answers within the candidate conclusions generated by the LLM, our experiments demonstrate that the capability of self-verification depends on the model\u2019s reasoning abilities, which means it is challenging to augment the reasoning performance of smaller language models, while the benefits are greater for high-performing models (in Figure 3). For the sake of usability, our method focuses on the conclusions derived from the reasoning rather than the reasoning process itself, and is thus not suitable for evaluating the LLM\u2019s inference procedure. Additionally, the method necessitates generating multiple candidate inference chains and conclusions, leading to increased computational costs; however, we demonstrate that only a minimal increase (merely 1x) in expenditure is required to substantially enhance the model\u2019s inference capabilities (in Figure 7)."
        },
        {
            "heading": "Acknowledgements",
            "text": "This work was supported by the National Key R&D Program of China (No.2022ZD0118501) and the National Natural Science Foundation of China (No.62376270, No.U1936207, No.61976211, No.62171183). This work was supported by the Strategic Priority Research Program of Chinese Academy of Sciences (No.XDA27020100), Youth Innovation Promotion Association CAS, and OPPO Research Fund."
        },
        {
            "heading": "A Appendix",
            "text": ""
        },
        {
            "heading": "A.1 Answer Cleansing",
            "text": "Our answer cleaning strategy is consistent with Wang et al. (2023b) and Zhang et al. (2022). The first number after selecting \u201cThe answer is\u201d is regarded as an output for arithmetic tasks, and we use Numpy (Harris et al., 2020) to compare it with the standard answer. For multiple choice tasks, we regard the first capital letter as output.\nFor the \u201cTrue-False Item Verification\u201d, we use \u201cTrue or False\u201d to select answer. Table 3 summarizes a list of answer cleansing approaches used across all the experiments."
        },
        {
            "heading": "A.2 Dataset Details",
            "text": "Our method is evaluated on eight benchmark datasets that cover arithmetic reasoning, commonsense reasoning, and logical reasoning tasks. The statistics of the datasets are shown in Table 6.\nWe list the details for all datasets used in this paper.\n\u2022 GSM8K: https://github.com/openai/ grade-school-math\n\u2022 SingleEq: https://gitlab.cs. washington.edu/ALGES/TACL2015\n\u2022 AddSub: https://www.cs.washington. edu/nlp/arithmetic\n\u2022 MultiArith: http://cogcomp.cs. illinois.edu/page/resource_view/98\n\u2022 AQUA-RAT: https://github.com/ deepmind/AQuA\n\u2022 SVAMP: https://github.com/ arkilpatel/SVAMP\n\u2022 CSQA: https://www.tau-nlp.org/ commonsenseqa\n\u2022 Data Understanding: https://github. com/google/BIG-bench"
        },
        {
            "heading": "A.3 Reproducibility Statement",
            "text": "All our experiments in the main text were run using the OpenAI API on November 20th to December 20th, 2022. The main experiment was run on November 25th to December 10th, the singlecondition rxperiment was run on November 20th to 25th, the Few CoT prompts experiment was run on December 12th, the True-False Item Verification experiment was run on December 12th to 15th, the different sizes models experiment was run on December 16th, and the computational reasource experiment was run on December 18th."
        },
        {
            "heading": "A.4 Additional Experiment Results",
            "text": "In Table 5, we show whether to generate real examples of multiple condition masks. We found that if only the first condition mask is used, the score is zero, and multiple evidence masks can obtain more accurate verification scores.\nThen, Table shows the difference between (A) Conditional Masked Verification and (B) TrueFalse Item Verification. The difference between the two lies in whether the condition mask is correct and the template for the question.\nFinally, we generated some selfverification examples using the Instruct-GPT (code-davinci-002) model. As show in Table 7.\nAnswer Format\nAnswer Cleansing Approach\nPseudo Code (Example in Pytorch 3.7)\nNumber Pick up the first number encountered in the text. pred = pred.replace(\",\", \"\") pred = [s for s in re.findall(r\u2019 -?\\d+\\.?\\d*\n\u2019, pred)] pred = pred [0]\nMultipleChoice\nPick up the first large letter encountered in the text. pred = re.findall(r\u2019A|B|C|D|E\u2019, pred) pred = pred [0]\nTrue or False Pick up the first \"True\" or \"False\" encountered in the text after removing unnecessary letters. pred = pred.lower() pred = re.sub(\"\\\"|\\ \u2019|\\n|\\.|\\s|\\:|\\,\",\"\u2423\", pred) pred = pred.split(\"\u2423\") pred = [i for i in pred if i in (\"True\", \"\nFalse\")] pred = pred [0]\nYes or No Pick up the first \"yes\" or \"no\" encountered in the text after removing unnecessary letters. pred = pred.lower() pred = re.sub(\"\\\"|\\ \u2019|\\n|\\.|\\s|\\:|\\,\",\"\u2423\", pred) pred = pred.split(\"\u2423\") pred = [i for i in pred if i in (\"yes\", \"no\n\")] pred = pred [0]\nFree Format Just remove unnecessary letters, such as \".\". pred = re.sub(\"\\\"|\\ \u2019|\\n|\\.|\\s\",\"\", pred)\nTable 3: Detail description of answer cleansing.\n\u5df2\u7ecf\u5b8c\u6210\u7684\u5b9e\u9a8c\n4. \u4e0d\u540c\u5f62\u5f0f\u7684\u53cd\u5411\u9a8c\u8bc1\u5f71\u54cd\n\u53ef\u4ee5\u60f3\u8c61\u6c42\u201dX\u201c\u662f\u4e00\u79cd\u201d\u586b\u7a7a\u9898\u201c\u7684\u5f62\u5f0f\u8fdb\u884c\u53cd\u5411\u9a8c\u8bc1\uff0c\n\u53e6\u4e00\u79cd\u53cd\u5411\u9a8c\u8bc1\u7684\u8303\u5f0f\u4e3a\u201d\u5224\u65ad\u9898\u201c\uff0c\u76f4\u63a5\u7ed9\u5b9a\u7ed3\u8bba\uff0c\u5224\u65ad\u6b64\u7ed3\u8bba\u662f\u5426\u6b63\u786e"
        },
        {
            "heading": "A.5 Forward reasoning prompt",
            "text": "For fairness, we used the same CoT prompts cue as Wei et al. (2022) in forward reasoning.\nAs show in Table 8, Table 9, Table 10, Table 11."
        },
        {
            "heading": "A.6 Backward Verification prompt",
            "text": "When backward verification, we manually wrote some examples referring to Wei et al. (2022).\nAs show in Table 12, Table 13, Table 14, Table 15."
        },
        {
            "heading": "A.7 True-False Item Verification prompt",
            "text": "In order to test True-False Item Verification, we designed some prompts to guide the LLMs to consider whether the conclusion is wrong.\nAs show in Table 16, Table 17.\nEXAMPLE Q: Tim wanted to make lemonade for a pool party. For a gallon of lemonade, his recipe called for 1 cup of fresh lemon juice. He found that 6 lemons would yield 1 cup of juice. He figured he would need to make 4 gallons of lemonade for\nEXAMPLE 2 (Condition is not required) Q: Dana worked 9 hours on Friday, 10 hours on Saturday, and 3 hours on Sunday. She earns $13 per hour. How much money did Dana earn in weekend?\nTable 7 \u2013 Continued from previous page\nDataset Question Self-Verification\nQ: Four students scored a total of 251 points in a board game. Naomi scored 68 of the points. Yuri scored \u2019X\u2019 more than half as many points as Naomi and Brianna scored 17 points more than Naomi. Jojo scored 54 points. Four students scored a total of 251 points. Naomi scored 68 points. Yuri scored X points. Brianna scored 17 points more than Naomi. Jojo scored 54 points. So: 68 + X + 68 + 17 + 54 = 251, X = 251 - 68 - 68 - 17 - 54, X = 94. The answer is 94. [\u2717](GT : 10)\nSingleEq Q: Sam grew 4 watermelons, but the rabbits ate \u2019X\u2019 watermelons. Sam have 1 watermelon left. A:Sam grew 4 watermelons. But the rabbits ate X watermelons. So, we can write the following equation: 4 - X = 1, X = 4 - 1, X = 3. The answer is 3. [\u2713]\nQ: Melanie, Benny, Sandy, and Jessica each have \u2019X\u2019 dozen Pokemon cards. They have 432 pokemon cards in all. A: Melanie has X dozen Pokemon cards. Benny has X dozen Pokemon cards. Sandy has X dozen Pokemon cards. Jessica has X dozen Pokemon cards. So, they have 4 X dozen Pokemon cards in total. X * 4 = 432, X = 432 / 4, X = 108. The answer is 108. [\u2717](GT : 9)\nAddsub Q: Karin \u2019s science class weighed plastic rings for an experiment . They found that the orange ring weighed \u2019X\u2019 ounce , the purple ring weighed 0.3333333333333333 ounce , and the white ring weighed 0.4166666666666667 ounce . The total weight of the plastic rings is 0.8333333333333334. A: The orange ring weighed 0.08333333333333333 ounce. the purple ring weighed 0.3333333333333333 ounce. and the white ring weighed 0.4166666666666667 ounce. The total weight of the plastic rings is 0.8333333333333334. 0.08333333333333333 + 0.3333333333333333 + 0.4166666666666667 = 0.8333333333333334 0.8333333333333334 - 0.4166666666666667 = 0.4166666666666667 0.4166666666666667 - 0.3333333333333333 = 0.08333333333333334 The answer is 0.08333333333333334. [\u2713]\nQ: There were 6 roses in the vase . Mary cut some roses from her flower garden . There are now \u2019X\u2019 roses in the vase . She cut 10 roses. A: There were originally 6 roses in the vase. Mary cut 10 roses from her flower garden. So there are now X roses in the vase. <code> 6 - 10 = X X = -4 </code> The answer is -4. [\u2717](GT : 16)\nMultiArith Q: Luke was organizing his baseball cards in a binder with 3 on each page. If he had 3 new cards and \u2019X\u2019 old cards to put in the binder, he use 4 pages. A: Luke had 3 new cards and X old cards. He would use 4.0 pages. So, we can write the following equation: 3 + X = 3 * 4.0, 3 + X = 12, X = 12 - 3 X = 9. The answer is 9. [\u2713]\nQ: Will bought 7 boxes of chocolate candy and gave 3 to his little brother. If each box has \u2019X\u2019 pieces inside it, Will still have 16 pieces. A: Will bought 7 boxes of chocolate candy. Each box has X pieces inside it. He gave 3 boxes to his little brother. So he has 4 boxes left. 7 * X = Y, Y - 3 * X = 16, Y = 16 + 3 * X, Y = 16 + 3 * X. The answer is 16. [\u2717](GT : 4)\nContinued on next page"
        },
        {
            "heading": "PROMPT FOR AQUA ALGEBRAIC WORD PROBLEMS",
            "text": "Q: John found that the average of 15 numbers is 40. If 10 is added to each number then the mean of the numbers is? Answer Choices: (a) 50 (b) 45 (c) 65 (d) 78 (e) 64\nA: If 10 is added to each number, then the mean of the numbers also increases by 10. So the new mean would be 50. The answer is (a).\nQ: If a / b = 3/4 and 8a + 5b = 22,then find the value of a. Answer Choices: (a) 1/2 (b) 3/2 (c) 5/2 (d) 4/2 (e) 7/2\nA: If a / b = 3/4, then b = 4a / 3. So 8a + 5(4a / 3) = 22. This simplifies to 8a + 20a / 3 = 22, which means 44a / 3 = 22. So a is equal to 3/2. The answer is (b).\nQ: A person is traveling at 20 km/hr and reached his destiny in 2.5 hr then find the distance? Answer Choices: (a) 53 km (b) 55 km (c) 52 km (d) 60 km (e) 50 km\nA: The distance that the person traveled would have been 20 km/hr * 2.5 hrs = 50 km. The answer is (e).\nQ: How many keystrokes are needed to type the numbers from 1 to 500? Answer Choices: (a) 1156 (b) 1392 (c) 1480 (d) 1562 (e) 1788\nA: There are 9 one-digit numbers from 1 to 9. There are 90 two-digit numbers from 10 to 99. There are 401 three-digit numbers from 100 to 500. 9 + 90(2) + 401(3) = 1392. The answer is (b)."
        },
        {
            "heading": "PROMPT FOR DATE UNDERSTANDING",
            "text": "Q: The concert was scheduled to be on 06/01/1943, but was delayed by one day to today. What is the date 10 days ago in MM/DD/YYYY?\nA: One day after 06/01/1943 is 06/02/1943, so today is 06/02/1943. 10 days before today is 05/23/1943. So the answer is 05/23/1943.\nQ: It is 4/19/1969 today. What is the date 24 hours later in MM/DD/YYYY? A: Today is 04/19/1969. 24 hours later is one day after today, which would be 04/20/1969. So the answer is 04/20/1969.\nQ: Jane thought today is 3/11/2002, but today is in fact Mar 12, which is 1 day later. What is the date 24 hours later in MM/DD/YYYY?\nA: Today is 03/12/2002. So the date 24 hours later will be 03/13/2002. So the answer is 03/13/2002.\nQ: Jane was born on the last day of Feburary in 2001. Today is her 16-year-old birthday. What is the date yesterday in MM/DD/YYYY?\nA: The last day of February is the 28th, so Jane was born on 02/28/2001. Today is her 16-year old birthday, so today is 02/28/2017. So yesterday was 02/27/2017. So the answer is 02/27/2017.\nTable 10: Few-shot exemplars for full chain of thought prompt for math word problems. This set of exemplars was used for all math word problem datasets except AQuA."
        },
        {
            "heading": "PROMPT FOR MATH WORD PROBLEMS",
            "text": "Q: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\nA: There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6. The answer is 6.\nQ: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot? A: There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. The answer is 5.\nQ: Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total? A: Originally, Leah had 32 chocolates. Her sister had 42. So in total they had 32 + 42 = 74. After eating 35, they had 74 - 35 = 39. The answer is 39.\nQ: Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops did Jason give to Denny?\nA: Jason started with 20 lollipops. Then he had 12 after giving some to Denny. So he gave Denny 20 - 12 = 8. The answer is 8.\nQ: Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does he have now? A: Shawn started with 5 toys. If he got 2 toys each from his mom and dad, then that is 4 more toys. 5 + 4 = 9. The answer is 9.\nQ: There were nine computers in the server room. Five more computers were installed each day, from monday to thursday. How many computers are now in the server room?\nA: There were originally 9 computers. For each of 4 days, 5 more computers were added. So 5 * 4 = 20 computers were added. 9 + 20 is 29. The answer is 29.\nQ: Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many golf balls did he have at the end of wednesday?\nA: Michael started with 58 golf balls. After losing 23 on tuesday, he had 58 - 23 = 35. After losing 2 more, he had 35 - 2 = 33 golf balls. The answer is 33.\nQ: Olivia has $23. She bought five bagels for $3 each. How much money does she have left? A: Olivia had 23 dollars. 5 bagels for 3 dollars each will be 5 x 3 = 15 dollars. So she has 23 - 15 dollars left. 23 - 15 is 8. The answer is 8.\nTable 11: Few-shot exemplars for full chain of thought prompt for CSQA. There are newlines between the answer choices that are omitted in the table for space reasons."
        },
        {
            "heading": "PROMPT FOR CSQA",
            "text": "Q: What do people use to absorb extra ink from a fountain pen? Answer Choices: (a) shirt pocket (b) calligrapher\u2019s hand (c) inkwell (d) desk drawer (e) blotter\nA: The answer must be an item that can absorb ink. Of the above choices, only blotters are used to absorb ink. So the answer is (e).\nQ: What home entertainment equipment requires cable? Answer Choices: (a) radio shack (b) substation (c) television (d) cabinet\nA: The answer must require cable. Of the above choices, only television requires cable. So the answer is (c).\nQ: The fox walked from the city into the forest, what was it looking for? Answer Choices: (a) pretty flowers (b) hen house (c) natural habitat (d) storybook\nA: The answer must be something in the forest. Of the above choices, only natural habitat is in the forest. So the answer is (b).\nQ: Sammy wanted to go to where the people were. Where might he go? Answer Choices: (a) populated areas (b) race track (c) desert (d) apartment (e) roadblock\nA: The answer must be a place with a lot of people. Of the above choices, only populated areas have a lot of people. So the answer is (a).\nQ: Where do you put your grapes just before checking out? Answer Choices: (a) mouth (b) grocery cart (c)super market (d) fruit basket (e) fruit market\nA: The answer should be the place where grocery items are placed before checking out. Of the above choices, grocery cart makes the most sense for holding grocery items. So the answer is (b).\nQ: Google Maps and other highway and street GPS services have replaced what? Answer Choices: (a) united states (b) mexico (c) countryside (d) atlas\nA: The answer must be something that used to do what Google Maps and GPS services do, which is to give directions. Of the above choices, only atlases are used to give directions. So the answer is (d).\nQ: Before getting a divorce, what did the wife feel who was doing all the work? Answer Choices: (a) harder (b) anguish (c) bitterness (d) tears (e) sadness"
        },
        {
            "heading": "PROMPT FOR AQUA ALGEBRAIC WORD PROBLEMS",
            "text": "Q: \"John found that the average of 15 numbers is \u2019X\u2019. If 10 is added to each number then the mean of the numbers is 50.\" What is the answer of \u2019X\u2019?\nA: If 10 is added to each number, then the mean of the numbers also increases by 10. The new mean would be 50., X + 10 = 50, X = 40. The answer is 40.\nQ: \"If a / b = \u2019X\u2019 and 8a + 5b = 22, then the value of a is 3/2.\" What is the answer of \u2019X\u2019? A: If a / b = X, then 8a + 5b = 22 and a = 3/2, so , 8 * 3/2 + 5b = 22, 5b = 22 - 12 = 10, b = 2, X = a / b = 3/2 / 2 = 3/4. The answer is 3/4.\nQ: \"A person is traveling at \u2019X\u2019 km/hr and reached his destiny in 2.5 hr then find the distance is 50km.\" What is the answer of \u2019X\u2019?\nA: The distance that the person traveled would have been , X km/hr * 2.5 hrs = 50 k, X = 20. The answer is 20.\nQ: \"There were \u2019X\u2019 computers in the server room. Five more computers were installed each day, from monday to thursday. There are 29 computers in the server room.\" What is the answer of \u2019X\u2019?\nA: There were originally X computers. For each of 4 days, 5 more computers were added. So 5 * 4 = 20 computers were added. And there are 29 computers. X + 20 = 29, X = 29 - 20, X = 9. The answer is 9."
        },
        {
            "heading": "PROMPT FOR DATE UNDERSTANDING",
            "text": "Q: \"The first day of \u2019X\u2019 is a Tuesday, and today is the first Monday of 2019. Today is 01/07/2019.\" What is the answer of \u2019X\u2019?\nA: If today is the first Monday of 2019 and today is 01/07/2019. So The first day of 2019 is a Tuesday. The answer is 2019.\nQ: \"The concert was scheduled to be on \u2019X\u2019/01/1943, but was delayed by one day to today. 10 days ago is 05/23/1943.\" What is the answer of \u2019X\u2019?\nA: 10 days ago is 05/23/1943, and the concert was delayed by one day to today, so today is 06/02/1943. So the concert was scheduled to be on 06/01/1943 The answer is 06.\nQ: \"It is \u2019X\u2019/19/1969 today. 24 hours later is 04/20/1969.\" What is the answer of \u2019X\u2019? A: 24 hours later is 04/20/1969. So today is 04/19/1969. The answer is 04.\nQ: \"Jane thought today is \u2019X\u2019/12/2002, but today is in fact Mar 12, which is 1 day later. 24 hours later is 03/13/2002.\" What is the answer of \u2019X\u2019?\nA: 24 hours later is 03/13/2002. So today is 03/12/2002. The answer is 03.\nQ: \"Jane was born on the last day of Feburary in \u2019X\u2019. Today is her 16-year-old birthday. Yesterday is 02/27/2017\" What is the answer of \u2019X\u2019?\nA: Yesterday is 02/27/2017, so today is 02/28/2017, Jane was born on 02/28/2001. The answer is 2001.\nTable 14: Few-shot exemplars for full chain of thought prompt for math word problems when backward verification. This set of exemplars was used for all math word problem datasets except AQuA."
        },
        {
            "heading": "PROMPT FOR MATH WORD PROBLEMS",
            "text": "Q: \"There are \u2019X\u2019 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. The grove workers planted 6 trees today.W\u0308hat is the answer of \u2019X\u2019?\nA: There are X trees originally. The grove workers planted 6 trees today. Then there were 21 trees after some more were planted. So, we can write the following equation:X + 6 = 21, X = 21 - 6, X = 15. The answer is 15.\nQ: \"If there are \u2019X\u2019 cars in the parking lot and 2 more cars arrive, There are 5 cars in the parking lot.\" What is the answer of \u2019X\u2019?\nA: There are originally X cars. 2 more cars arrive and there are 5 cars finally. So: X + 2 = 5, X = 5 - 2, X = 3. The answer is 3.\nQ: \"Leah had \u2019X\u2019 chocolates and her sister had 42. If they ate 35, they have 39 pieces left in total.\" What is the answer of \u2019X\u2019?\nA: Originally, Leah had X chocolates. Her sister had 42. So in total they had: X + 42 = Y. After eating 35, they had 39, so, Y = 35 + 39, Y = 74, X + 42 = 74, X = 74 - 42, X = 32. The answer is 32.\nQ: \"Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has \u2019X\u2019 lollipops. Jason gave Denny 8 lollipops.\" What is the answer of \u2019X\u2019?\nA: Jason started with 20 lollipops. Then he had X after giving some to Denny and gave Denny 8. 20 - X = 8, X = 12. The answer is 12.\nQ: \"Shawn has \u2019X\u2019 toys. For Christmas, he got two toys each from his mom and dad. He has 9 toys now.\" What is the answer of \u2019X\u2019?\nA: Shawn started with X toys. If he got 2 toys each from his mom and dad, then that is 4 more toys. X + 4 = 9, X = 9 - 4, X = 5. The answer is 5.\nQ: \"There were \u2019X\u2019 computers in the server room. Five more computers were installed each day, from monday to thursday. There are 29 computers in the server room.\" What is the answer of \u2019X\u2019?\nA: There were originally X computers. For each of 4 days, 5 more computers were added. So 5 * 4 = 20 computers were added. And there are 29 computers. X + 20 = 29, X = 29 - 20, X = 9. The answer is 9.\nQ: \"Michael had 58 golf balls. On tuesday, he lost \u2019X\u2019 golf balls. On wednesday, he lost 2 more. He had 33 golf balls at the end of Wednesday.\" What is the answer of \u2019X\u2019?\nA: Michael started with 58 golf balls. After losing X on tuesday and he lost 2 more on wednesday, He had 33 golf balls. So, we can write the following equation: 58 - X - 2 = 33, 58 - X = 35, X = 23. The answer is 23.\nQ: \"Olivia has $\u2019X\u2019. She bought five bagels for $3 each. She has 8 dollars left.\" What is the answer of \u2019X\u2019? A: Olivia had X dollars. 5 bagels for 3 dollars each will be 5 x 3 = 15 dollars. She has 8 dollars left finally. X - 15 = 8, X = 8 + 15, X = 23. The answer is 23.\nTable 15: Few-shot exemplars for full chain of thought prompt for CSQA when backward verification. There are newlines between the answer choices that are omitted in the table for space reasons."
        },
        {
            "heading": "PROMPT FOR CSQA",
            "text": "Q: \"People use blotter to absorb extra ink from a fountain pen.\" Judge whether this statement is normal (yes or no). A: The Blotter is used to absorb extra ink from a fountain pen. The answer is Yes.\nQ: \"Television requires cable.\" Judge whether this statement is normal (yes or no). A: The Television is an electrical appliance, it needs electricity, so it requires cable. The answer is Yes.\nQ: \"The fox walked from the city into the forest, it was looking for a hen house.\" Judge whether this statement is normal (yes or no).\nA: The hen house is not in the forest, so the fox does not go to the hen house. The answer is No.\nQ: \"Sammy wanted to go to where the people were. He might go populated areas.\" Judge whether this statement is normal (yes or no).\nA: There are many people in the populated areas, so they really go here. The answer is Yes.\nQ: \"The grapes are put in the fruit market just before checking out.\" Judge whether this statement is normal (yes or no). A: The answer should be the place where grocery items are placed before checking out. But the fruit market is not suitable place where grocery items are placed. The answer is No.\nQ: \"Google Maps and other highway and street GPS services have replaced the united states.\" Judge whether this statement is normal (yes or no).\nA: The united states is a country and Google Maps is a map, so Google Maps cannot replace the united states. The answer is No.\nQ: \"The wife who was doing all the work felt bitterness before getting a divorce.\" Judge whether this statement is normal (yes or no).\nA: The wife divorced who was doing all the work. So she felt bitterness. The answer is Yes.\nTable 16: Few-shot exemplars for full chain of thought prompt for AQuA when True-False Item Verification."
        },
        {
            "heading": "PROMPT FOR AQUA ALGEBRAIC WORD PROBLEMS",
            "text": "Q: \"John found that the average of 15 numbers is 40. If 10 is added to each number then the mean of the numbers is 50.\" Do it is correct (True or False)?\nA: The new mean would be 50. The average of 15 numbers is 4, if 10 is added to each number, then the mean of the numbers also increases by 10. 50 - 40 = 10. The answer is True.\nQ: \"If a / b = 3/4 and 8a + 5b = 22, then the value of a is 3.\" Do it is correct (True or False)? A: If a is 3, a / b = 3/4, so b = 4. then 8a + 5b = 8 * 2 + 5 * 4 = 36, but 8a + 5b = 22 The answer is False.\nQ: \"A person is traveling at 20 km/hr and reached his destiny in 2.5 hr then find the distance is 65km.\" Do it is correct (True or False)?\nA: If 65km is driven at 20km/hr, so the driving time is 65km / 20km/hr = 3.25h, but he destiny in 2.5 hr. The answer is False.\nQ: \"There were 9 computers in the server room. Five more computers were installed each day, from monday to thursday. There are 29 computers in the server room.\" Do it is correct (True or False)?\nA: There are 29 computers in the server room. For each of 4 days, 5 more computers were added. 5 * 4 = 20 computers were added. So there were originally 9 computers. The answer is True.\nTable 17: Few-shot exemplars for full chain of thought prompt for math word problems when True-False Item Verification. This set of exemplars was used for all math word problem datasets except AQuA."
        },
        {
            "heading": "PROMPT FOR MATH WORD PROBLEMS",
            "text": "Q: \u2019There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. The grove workers planted 4 trees today.\u2019 Do it is correct (True or False)?\nA: If the Grove workers will plant 4 trees today and there will be 21 trees after they are done. 21 - 4 = 17, there are 17 trees in the grove, but actually there are 15 trees, 17 != 15, which is different from the theme. The answer is False.\nQ: \u2019If there are 3 cars in the parking lot and 2 more cars arrive, There are 5 cars in the parking lot.\u2019 Do it is correct (True or False)?\nA: If there will be 5 cars in the parking lot, subtract 2 cars that will arrive, 5 - 2 = 3, so there are 2 cars in the parking lot, which is consistent with the theme. The answer is True.\nQ: \u2019Leah had 32 chocolates and her sister had 42. If they ate 35, they have 39 pieces left in total.\u2019 Do it is correct (True or False)?\nA: If there are 39 pieces of chocolates and 35 pieces of chocolate are eaten, Leah and her sister have 39 + 35 = 74 in total. Her sister\u2019s had 42, so Leah had 74 - 42 = 32, which is consistent with the theme. The answer is True.\nQ: \u2019Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. Jason gave Denny 6 lollipops.\u2019 Do it is correct (True or False)?\nA: If Jason gave Denny 6 lollipops, and Jason now has 12 lollipops, so Jason originally had 6+12=18 lollipops, 18 != 20, which is different from the theme. The answer is False.\nQ: \u2019Shawn has five toys. For Christmas, he got two toys each from his mom and dad. He has 9 toys now.\u2019 Do it is correct (True or False)?\nA: If Shawn now has 9 toys and his parents gaven him two each, then he originally had 9 - 2 - 2 = 5, which is consistent with the theme. The answer is True.\nQ: \u2019There were nine computers in the server room. Five more computers were installed each day, from monday to thursday. There are 18 computers in the server room.\u2019 Do it is correct (True or False)?\nA: Now there are 18 computers in the server room. For each of 4 days, 5 more computers were added. So 5 * 4 = 20 computers were added. So there were 18 - 20= -2 in the server room originally, -2 != 9, which is different from the theme. The answer is False.\nQ: \u2019Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. He had 40 golf balls at the end of Wednesday.\u2019 Do it is correct (True or False)?\nA: If Michael had 40 golf balls on Wednesday, he had 40+2=42 on Tuesday because he lost 2 golf balls on Wednesday. Due to lost 23 balls on Tuesday, he should have 42+23=65 on Monday, but in fact Michael has 58 golf balls original, which is different from the theme. The answer is False.\nQ: \u2019Olivia has $23. She bought five bagels for $3 each. She has 8 dollars left.\u2019 Do it is correct (True or False)? A: If Olivia had $8 left and she bought five bagels for $3 each, so costs 5 * 3 = 15, so there was 8 + 15 = 23, which is consistent with the theme. The answer is True."
        }
    ],
    "title": "Large Language Models are Better Reasoners with Self-Verification",
    "year": 2023
}