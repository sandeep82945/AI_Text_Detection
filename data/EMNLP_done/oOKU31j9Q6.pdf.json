{
    "abstractText": "Semantic Change Detection (SCD) of words is an important task for various NLP applications that must make time-sensitive predictions. Some words are used over time in novel ways to express new meanings, and these new meanings establish themselves as novel senses of existing words. On the other hand, Word Sense Disambiguation (WSD) methods associate ambiguous words with sense ids, depending on the context in which they occur. Given this relationship between WSD and SCD, we explore the possibility of predicting whether a target word has its meaning changed between two corpora collected at different time steps, by comparing the distributions of senses of that word in each corpora. For this purpose, we use pretrained static sense embeddings to automatically annotate each occurrence of the target word in a corpus with a sense id. Next, we compute the distribution of sense ids of a target word in a given corpus. Finally, we use different divergence or distance measures to quantify the semantic change of the target word across the two given corpora. Our experimental results on SemEval 2020 Task 1 dataset show that word sense distributions can be accurately used to predict semantic changes of words in English, German, Swedish and Latin.",
    "authors": [
        {
            "affiliations": [],
            "name": "Xiaohang Tang"
        },
        {
            "affiliations": [],
            "name": "Yi Zhou"
        },
        {
            "affiliations": [],
            "name": "Taichi Aida"
        },
        {
            "affiliations": [],
            "name": "Procheta Sen"
        },
        {
            "affiliations": [],
            "name": "Danushka Bollegala"
        }
    ],
    "id": "SP:a8be869f0d3c4131230db9fafe607e9d73c229f5",
    "references": [
        {
            "authors": [
                "Taichi Aida",
                "Danushka Bollegala."
            ],
            "title": "Unsupervised semantic variation prediction using the distribution of sibling embeddings",
            "venue": "Proc. of the 61st Annual Meeting of the Association for Computational Linguistics (Findings of ACL 2023).",
            "year": 2023
        },
        {
            "authors": [
                "Taichi Aida",
                "Mamoru Komachi",
                "Toshinobu Ogiso",
                "Hiroya Takamura",
                "Daichi Mochihashi."
            ],
            "title": "A comprehensive analysis of PMI-based models for measuring semantic differences",
            "venue": "Proceedings of the 35th Pacific Asia Conference on Language, In-",
            "year": 2021
        },
        {
            "authors": [
                "Reem Alatrash",
                "Dominik Schlechtweg",
                "Jonas Kuhn",
                "Sabine Schulte im Walde."
            ],
            "title": "CCOHA: Clean corpus of historical American English",
            "venue": "Proceedings of the Twelfth Language Resources and Evaluation Conference. European Language Resources",
            "year": 2020
        },
        {
            "authors": [
                "Ehsaneddin Asgari",
                "Christoph Ringlstetter",
                "Hinrich Sch\u00fctze."
            ],
            "title": "EmbLexChange at SemEval-2020 task 1: Unsupervised embedding-based detection of lexical semantic changes",
            "venue": "Proceedings of the Fourteenth Workshop on Semantic Evaluation. Inter-",
            "year": 2020
        },
        {
            "authors": [
                "Pierpaolo Basile",
                "Annalina Caputo",
                "Tommaso Caselli",
                "Pierluigi Cassotti",
                "Rossella Varvara."
            ],
            "title": "Diacrita @ evalita2020: Overview of the evalita2020 diachronic lexical semantics (diacr-ita) task",
            "venue": "Valerio Basile, Danilo Croce, Maria Di Maro, and",
            "year": 2020
        },
        {
            "authors": [],
            "title": "DiaSense at SemEval-2020 task",
            "year": 2020
        },
        {
            "authors": [
                "Tomas Mikolov"
            ],
            "title": "Enriching word vectors with",
            "year": 2017
        },
        {
            "authors": [
                "Kristina Toutanova"
            ],
            "title": "2019. BERT: Pre-training",
            "year": 2019
        },
        {
            "authors": [
                "Haim Dubossarsky",
                "Simon Hengchen",
                "Nina Tahmasebi",
                "Dominik Schlechtweg."
            ],
            "title": "Timeout: Temporal referencing for robust modeling of lexical semantic change",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Com-",
            "year": 2019
        },
        {
            "authors": [
                "Christiane Fellbaum",
                "George Miller."
            ],
            "title": "WordNet: An electronic lexical database",
            "venue": "MIT press. https://ieeexplore.ieee.org/book/6267389.",
            "year": 1998
        },
        {
            "authors": [
                "Mario Giulianelli",
                "Marco Del Tredici",
                "Raquel Fern\u00e1ndez."
            ],
            "title": "Analysing lexical semantic change with contextualised word representations",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. Associa-",
            "year": 2020
        },
        {
            "authors": [
                "Mario Giulianelli",
                "Andrey Kutuzov",
                "Lidia Pivovarova."
            ],
            "title": "Do not fire the linguist: Grammatical profiles help language models detect semantic change",
            "venue": "Proceedings of the 3rd Workshop on Computational Approaches to Historical Language Change.",
            "year": 2022
        },
        {
            "authors": [
                "William L. Hamilton",
                "Jure Leskovec",
                "Dan Jurafsky."
            ],
            "title": "Diachronic word embeddings reveal statistical laws of semantic change",
            "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for",
            "year": 2016
        },
        {
            "authors": [
                "Renfen Hu",
                "Shen Li",
                "Shichen Liang."
            ],
            "title": "Diachronic sense modeling with deep contextualized word embeddings: An ecological view",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. Association for Computa-",
            "year": 2019
        },
        {
            "authors": [
                "Eric H Huang",
                "Richard Socher",
                "Christopher D Manning",
                "Andrew Y Ng."
            ],
            "title": "Improving word representations via global context and multiple word prototypes",
            "venue": "Proceedings of ACL. pages 873\u2013882. https://aclanthology.org/P12-1092.pdf.",
            "year": 2012
        },
        {
            "authors": [
                "Ignacio Iacobacci",
                "Mohammad Taher Pilehvar",
                "Roberto Navigli."
            ],
            "title": "Sensembed: Learning sense embeddings for word and relational similarity",
            "venue": "Proceedings of ACL. pages 95\u2013105. https://aclanthology.org/P15-1010.pdf.",
            "year": 2015
        },
        {
            "authors": [
                "Slav Petrov"
            ],
            "title": "Temporal analysis of lan",
            "year": 2014
        },
        {
            "authors": [
                "Phil Blunsom"
            ],
            "title": "Mind the gap",
            "year": 2021
        },
        {
            "authors": [
                "Daniel Loureiro",
                "Francesco Barbieri",
                "Leonardo Neves",
                "Luis Espinosa Anke",
                "Jose Camacho-collados."
            ],
            "title": "TimeLMs: Diachronic language models from Twitter",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics:",
            "year": 2022
        },
        {
            "authors": [
                "Daniel Loureiro",
                "Alipio Jorge."
            ],
            "title": "Language Modelling Makes Sense: Propagating Representations through WordNet for Full-Coverage Word Sense Disambiguation",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computa-",
            "year": 2019
        },
        {
            "authors": [
                "Jean-Philippe Magu\u00e9."
            ],
            "title": "Changements s\u00e9mantiques et cognition: diff\u00e9rentes m\u00e9thodes pour diff\u00e9rentes \u00e9chelles temporelles",
            "venue": "Ph.D. thesis, Universit\u00e9 Lumi\u00e8re-Lyon II.",
            "year": 2005
        },
        {
            "authors": [
                "Matej Martinc",
                "Petra Kralj Novak",
                "Senja Pollak."
            ],
            "title": "Leveraging contextual embeddings for detecting diachronic semantic shift",
            "venue": "Proceedings of the Twelfth Language Resources and Evaluation Conference. European Language Resources",
            "year": 2020
        },
        {
            "authors": [
                "Tomas Mikolov",
                "Kai Chen",
                "Jeffrey Dean."
            ],
            "title": "Efficient estimation of word representation in vector space",
            "venue": "Proc. of International Conference on Learning Representations.",
            "year": 2013
        },
        {
            "authors": [
                "George A. Miller",
                "Claudia Leacock",
                "Randee Tengi",
                "Ross T. Bunker."
            ],
            "title": "A semantic concordance",
            "venue": "Human Language Technology: Proceedings of a Workshop Held at Plainsboro, New Jersey, March 2124, 1993. https://www.aclweb.org/anthology/H93-",
            "year": 1993
        },
        {
            "authors": [
                "Syrielle Montariol",
                "Matej Martinc",
                "Lidia Pivovarova."
            ],
            "title": "Scalable and interpretable semantic change detection",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Hu-",
            "year": 2021
        },
        {
            "authors": [
                "Roberto Navigli",
                "Simone Paolo Ponzetto."
            ],
            "title": "Babelnet: The automatic construction, evaluation and application of a wide-coverage multilingual semantic network",
            "venue": "Artificial intelligence 193:217\u2013250.",
            "year": 2012
        },
        {
            "authors": [
                "Arvind Neelakantan",
                "Jeevan Shankar",
                "Alexandre Passos",
                "Andrew McCallum."
            ],
            "title": "Efficient nonparametric estimation of multiple embeddings per word in vector space",
            "venue": "Proceedings of EMNLP. pages 1059\u20131069. https://aclanthology.org/D14-",
            "year": 2014
        },
        {
            "authors": [
                "G. Palshikar."
            ],
            "title": "Simple algorithms for peak detection in time series",
            "venue": "Proc. of the 1st International Conference on Advanced Data Analysis, Business Analytics and Intelligence. pages 1\u201313.",
            "year": 2009
        },
        {
            "authors": [
                "Mohammad Taher Pilehvar",
                "Jose Camacho-Collados."
            ],
            "title": "WiC: the word-in-context dataset for evaluating context-sensitive meaning representations",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computa-",
            "year": 2019
        },
        {
            "authors": [
                "Ond\u0159ej Pra\u017e\u00e1k",
                "Pavel P\u0159ib\u00e1\u0148",
                "Stephen Taylor",
                "Jakub Sido."
            ],
            "title": "UWB at SemEval-2020 task 1: Lexical semantic change detection",
            "venue": "Proceedings of the Fourteenth Workshop on Semantic Evaluation. International Committee for Computational Linguistics,",
            "year": 2020
        },
        {
            "authors": [
                "Joseph Reisinger",
                "Raymond Mooney."
            ],
            "title": "Multiprototype vector-space models of word meaning",
            "venue": "Proceedings of NAACL. pages 109\u2013117. https://aclanthology.org/N10-1013.pdf.",
            "year": 2010
        },
        {
            "authors": [
                "Kiamehr Rezaee",
                "Daniel Loureiro",
                "Jose CamachoCollados",
                "Mohammad Taher Pilehvar."
            ],
            "title": "On the cross-lingual transferability of contextualized sense embeddings",
            "venue": "Proceedings of the 1st Workshop on Multilingual Representation Learning. As-",
            "year": 2021
        },
        {
            "authors": [
                "Guy D. Rosin",
                "Ido Guy",
                "Kira Radinsky."
            ],
            "title": "Time masking for temporal language models",
            "venue": "Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining. Association for Computing Machinery, New",
            "year": 2022
        },
        {
            "authors": [
                "Guy D. Rosin",
                "Kira Radinsky."
            ],
            "title": "Temporal attention for language models",
            "venue": "Findings of the Association for Computational Linguistics: NAACL 2022. Association for Computational Linguistics, Seattle, United States, pages 1498\u2013",
            "year": 2022
        },
        {
            "authors": [
                "Sascha Rothe",
                "Hinrich Sch\u00fctze."
            ],
            "title": "AutoExtend: Extending Word Embeddings to Embeddings for Synsets and Lexemes",
            "venue": "Proceedings of ACL-IJCNLP. pages 1793\u20131803. https://aclanthology.org/P15-1173.pdf.",
            "year": 2015
        },
        {
            "authors": [
                "David Rother",
                "Thomas Haider",
                "Steffen Eger."
            ],
            "title": "CMCE at SemEval-2020 task 1: Clustering on manifolds of contextualized embeddings to detect historical meaning shifts",
            "venue": "Proceedings of the Fourteenth Workshop on Semantic Evaluation. Interna-",
            "year": 2020
        },
        {
            "authors": [
                "Ana Sabina Uban",
                "Alina Maria Cristea",
                "Anca Daniela Dinu",
                "Liviu P Dinu",
                "Simona Georgescu",
                "Laurentiu Zoicas."
            ],
            "title": "CoToHiLi at LSCDiscovery: the role of linguistic features in predicting semantic change",
            "venue": "Proceedings of the 3rd",
            "year": 2022
        },
        {
            "authors": [
                "Bianca Scarlini",
                "Tommaso Pasini",
                "Roberto Navigli."
            ],
            "title": "SensEmBERT: Context-Enhanced Sense Embeddings for Multilingual Word Sense Disambiguation",
            "venue": "Proceedings of AAAI. volume 34(05), pages 8758\u20138765.",
            "year": 2020
        },
        {
            "authors": [
                "Bianca Scarlini",
                "Tommaso Pasini",
                "Roberto Navigli."
            ],
            "title": "With more contexts comes better performance: Contextualized sense embeddings for all-round word sense disambiguation",
            "venue": "Proceedings of EMNLP. Online, pages 3528\u20133539.",
            "year": 2020
        },
        {
            "authors": [
                "Dominik Schlechtweg",
                "Barbara McGillivray",
                "Simon Hengchen",
                "Haim Dubossarsky",
                "Nina Tahmasebi."
            ],
            "title": "SemEval-2020 task 1: Unsupervised lexical semantic change detection",
            "venue": "Proceedings of the Fourteenth Workshop on Semantic",
            "year": 2020
        },
        {
            "authors": [
                "Milan Straka",
                "Jana Strakov\u00e1"
            ],
            "title": "Tokenizing, POS tagging, lemmatizing and parsing UD 2.0 with UDPipe",
            "venue": "In Proceedings of the CoNLL",
            "year": 2017
        },
        {
            "authors": [
                "Zhaochen Su",
                "Zecheng Tang",
                "Xinyan Guan",
                "Lijun Wu",
                "Min Zhang",
                "Juntao Li."
            ],
            "title": "Improving temporal generalization of pre-trained language models with lexical semantic change",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural",
            "year": 2022
        },
        {
            "authors": [
                "Nina Tahmasebi",
                "Lars Borina",
                "Adam Jatowtb."
            ],
            "title": "Survey of computational approaches to lexical semantic change detection",
            "venue": "Computational approaches to semantic change 6:1.",
            "year": 2021
        },
        {
            "authors": [
                "Stephen Ullmann."
            ],
            "title": "The Principles of Semantics",
            "venue": "Blackwell.",
            "year": 1959
        },
        {
            "authors": [
                "Shyam Upadhyay",
                "Kai-Wei Chang",
                "Matt Taddy",
                "Adam Kalai",
                "James Zou."
            ],
            "title": "Beyond bilingual: Multi-sense word embeddings using multilingual context",
            "venue": "Proceedings of the 2nd Workshop on Representation Learning for NLP. Association for Compu-",
            "year": 2017
        },
        {
            "authors": [
                "Luke Vilnis",
                "Andrew McCallum."
            ],
            "title": "Word representations via gaussian embedding",
            "venue": "Proceedings of the 3rd International Conference on Learning Representations. San Diego, CA, USA.",
            "year": 2015
        },
        {
            "authors": [
                "Zijun Yao",
                "Yifan Sun",
                "Weicong Ding",
                "Nikhil Rao",
                "Hui Xiong."
            ],
            "title": "Dynamic word embeddings for evolving semantic discovery",
            "venue": "WSDM 2018. pages 673\u2013681. https://doi.org/10.1145/3159652.3159703.",
            "year": 2018
        },
        {
            "authors": [
                "Arda Y\u00fcksel",
                "Berke U\u011furlu",
                "Aykut Ko\u00e7."
            ],
            "title": "Semantic change detection with gaussian word embeddings",
            "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing 29:3349\u20133361. https://doi.org/10.1109/TASLP.2021.3120645.",
            "year": 2021
        },
        {
            "authors": [
                "Yi Zhou",
                "Masahiro Kaneko",
                "Danushka Bollegala."
            ],
            "title": "Sense embeddings are also biased \u2013 evaluating social biases in static and contextualised sense embeddings",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Vol-",
            "year": 2022
        },
        {
            "authors": [
                "C different"
            ],
            "title": "Previously Proposed SCD Methods We compare SSCS against the following prior SCD methods on the SemEval-2020 Task 1 English data. BERT + Time Tokens",
            "year": 2020
        },
        {
            "authors": [
                "pendent",
                "Rosin",
                "Radinsky"
            ],
            "title": "2022) proposed to use them simultaneously. They also use the cosine distance for prediction and report that this model achieves current state-of-the-art performance in semantic change detection",
            "year": 2022
        },
        {
            "authors": [
                "Embeddings: Y\u00fcksel"
            ],
            "title": "2021) extend word2vec to create Gaussian embeddings (Vilnis and McCallum, 2015) for target words independently in each corpus. They then learn a mapping between the two vector spaces for computing",
            "year": 2015
        },
        {
            "authors": [
                "CMCE: Rother"
            ],
            "title": "Clustering on Manifolds of Contextualised Embeddings (CMCE) where they use mBERT embeddings with dimensionality reduction to represent target",
            "year": 2020
        },
        {
            "authors": [
                "Asgari"
            ],
            "title": "Thresholds found with Bayesian optimisation for the classification task on English datasets. for the binary classification",
            "year": 2020
        },
        {
            "authors": [
                "fasttext (Bojanowski"
            ],
            "title": "2017) to create word embeddings from each corpora separately, and measure the cosine similarity between a word and a selected fixed set of pivotal words to represent a word in a corpus using the distribution over those",
            "year": 2017
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "SCD of words over time has provided important insights for diverse fields such as linguistics, lexicography, sociology, and information retrieval (IR) (Traugott and Dasher, 2001; Cook and Stevenson, 2010; Michel et al., 2011; Kutuzov et al., 2018). For example, in IR one must know the seasonal association of keywords used in user queries to provide relevant results pertaining to a particular time period. Moreover, it has been shown that the performance of publicly available pretrained LLMs declines over time when applied to emerging data (Su et al., 2022; Loureiro et al., 2022; Lazaridou et al., 2021) because they are trained\nusing a static snapshot. Moreover, Su et al. (2022) showed that the temporal generalisation of LLMs is closely related to their ability to detect semantic variations of words.\nA word is often associated with multiple senses as listed in dictionaries, corresponding to its different meanings. Polysemy (i.e. coexistence of several possible meanings for one word) has been shown to statistically correlate with the rate of semantic change in prior work (Bre\u0301al, 1897; Ullmann, 1959; Mague\u0301, 2005). For example, consider the word cell, which has the following three noun senses according to the WordNet1: (a) cell%1:03:00 \u2013 the basic structural and functional unit of all organisms, (b) cell%1:06:04 \u2013 a handheld mobile radiotelephone for use in an area divided into small sections, and (c) cell%1:06:01 \u2013 a room where a prisoner is kept. Here, the WordNet sense ids for each word sense are shown in boldface font. Mobile phones were first produced in the late 1970s and came into wider circulation after 1990. Therefore, the sense (b) is considered as a more recent association compared to (a) and (c). Given two sets of documents, one sampled before 1970 and one after, we would expect to encounter (b) more frequently in the latter set. Likewise, articles on biology are likely to contain (a). As seen from this example the sense distributions of a word in two corpora provide useful information about its possible meaning changes over time.\nGiven this relationship between the two tasks SCD and WSD, a natural question arises \u2013 is the word sense distribution indicative of semantic changes of words? To answer this question, we design and evaluate an unsupervised SCD method that uses only the word sense distributions to predict whether the meaning associated with a target word w has changed from one text corpora C1 to another C2. For the ease of future references, we name this method Sense-based Semantic Change\n1https://wordnet.princeton.edu/\nScore (SSCS). Given a target word w, we first disambiguate each occurrence of w in each corpus. For this purpose, we measure the similarity between the contextualised word embedding of w, obtained from a pre-trained Masked Language Model (MLM), from the contexts containing w with each of the pre-trained static sense embeddings corresponding to the different senses of w. Next, we compute the sense distribution of w in each corpus separately. Finally, we use multiple distance/divergence measures to compare the two sense distributions of w to determine whether its meaning has changed from C1 to C2.\nTo evaluate the possibility of using word senses for SCD, we compare the performance of SSCS against previously proposed SCD methods using the SemEval-2020 Task 1 (unsupervised lexical SCD) (Schlechtweg et al., 2020) benchmark dataset. This task has two subtasks: (1) a binary classification task, where for a set of target words, we must decide which words lost or gained sense(s) from C1 to C2, and (2) a ranking task, where we must rank target words according to their degree of lexical semantic change from C1 to C2. We apply SSCS on two pre-trained static sense embeddings, and six distance/divergence measures. Despite the computationally lightweight and unsupervised nature of SSCS, our experimental results show that it surprisingly outperforms most previously proposed SCD methods for English, demonstrating the effectiveness of word sense distributions for SCD. Moreover, evaluations on German, Latin and Swedish show that this effectiveness holds in other languages as well, although not to the same levels as in English. We hope our findings will motivate future methods for SCD to explicitly incorporate word sense related information. Source code implementation for reproducing our experimental results is publicly available.2"
        },
        {
            "heading": "2 Related Work",
            "text": "Semantic Change Detection: SCD is modelled in the literature as the unsupervised task of detecting words whose meanings change between two given time-specific corpora (Kutuzov et al., 2018; Tahmasebi et al., 2021). In recent years, several shared tasks have been held (Schlechtweg et al., 2020; Basile et al., 2020; Kutuzov and Pivovarova, 2021), where participants are required to predict the\n2https://github.com/LivNLP/ Sense-based-Semantic-Change-Prediction\ndegree or presence of semantic changes for a given target word between two given corpora, sampled from different time periods. Various methods have been proposed to map vector spaces from different time periods, such as initialisation (Kim et al., 2014), alignment (Kulkarni et al., 2015; Hamilton et al., 2016), and joint learning (Yao et al., 2018; Dubossarsky et al., 2019; Aida et al., 2021).\nExisting SCD methods can be broadly categorised into two groups: (a) methods that compare word/context clusters (Hu et al., 2019; Giulianelli et al., 2020; Montariol et al., 2021), and (b) methods that compare embeddings of the target words computed from different corpora sampled at different time periods (Martinc et al., 2020; Beck, 2020; Kutuzov and Giulianelli, 2020; Rosin et al., 2022). Rosin and Radinsky (2022) recently proposed a temporal attention mechanism, which achieves SoTA performance for SCD. However, their method requires additional training of the entire MLM with temporal attention, which is computationally expensive for large MLMs and corpora.\nThe change of the grammatical profile (Kutuzov et al., 2021; Giulianelli et al., 2022) of a word, created using its universal dependencies obtained from UDPipe (Straka and Strakova\u0301, 2017), has shown to correlate with the semantic change of that word. However, the accuracy of the grammatical profile depends on the accuracy of the parser, which can be low for resource poor languages and noisy texts. Sabina Uban et al. (2022) used polysemy as a feature for detecting lexical semantic change discovery. The distribution of the contextualised embeddings of a word over its occurrences (aka. sibling embeddings) in a corpus has shown to be an accurate representation of the meaning of that word in a corpus, which can be used to compute various semantic change detection scores (Kutuzov et al., 2022; Aida and Bollegala, 2023). XLLEXEME (Cassotti et al., 2023) is a supervised SCD method where a bi-encoder model is trained using WiC (Pilehvar and Camacho-Collados, 2019) dataset to discriminate whether a target word appears in different senses in a pair of sentences. XLLEXEME reports SoTA SCD results for English, German, Swedish and Russian.\nSense Embeddings: Sense embedding learning methods represent different senses of an ambiguous word with different vectors. The concept of multiprototype embeddings to represent word senses was introduced by Reisinger and Mooney (2010).\nThis idea was further extended by Huang et al. (2012), who combined both local and global contexts in their approach. Clustering is used in both works to categorise contexts of a word that belong to the same meaning. Although the number of senses a word can take depends on that word, both approaches assign a predefined fixed number of senses to all words. To address this limitation, Neelakantan et al. (2014) introduced a non-parametric model, which is able to dynamically estimate the number of senses for each word.\nAlthough clustering-based approaches can allocate multi-prototype embeddings to a word, they still suffer from the fact that the embeddings generated this way are not linked to any sense inventories (Camacho-Collados and Pilehvar, 2018). On the other hand, knowledge-based methods obtain sense embeddings by extracting sense-specific information from external sense inventories, such as the WordNet (Fellbaum and Miller, 1998) or the BabelNet3 (Navigli and Ponzetto, 2012). Chen et al. (2014) extended word2vec (Mikolov et al., 2013) to learn sense embeddings using WordNet synsets. Rothe and Schu\u0308tze (2015) made use of the semantic relationships in WordNet to embed words into a shared vector space. Iacobacci et al. (2015) used the definitions of word senses in BabelNet and conducted WSD to extract contextual information that is unique to each sense.\nRecently, contextualised embeddings produced by MLMs have been used to create sense embeddings. To achieve this, Loureiro and Jorge (2019) created LMMS sense embeddings by averaging over the contextualised embeddings of the sense annotated tokens from SemCor (Miller et al., 1993). Scarlini et al. (2020a) proposed SenseEmBERT (Sense Embedded BERT), which makes use of the lexical-semantic information in BabelNet to create sense embeddings without relying on senseannotated data. ARES (context-AwaRe EmbeddinS) (Scarlini et al., 2020b) is a knowledge-based method for generating BERT-based embeddings of senses by means of the lexical-semantic information available in BabelNet and Wikipedia. ARES and LMMS embeddings are the current SoTA sense embeddings."
        },
        {
            "heading": "3 Sense-based Semantic Change Score",
            "text": "SSCS consists of two steps. First, in \u00a73.1, we compute the distribution of word senses associated with\n3https://babelnet.org/\na target word in a corpus. Second, in \u00a73.2, we use different distance (or divergence) measures to compare the sense distributions computed for the same target word from different corpora to determine whether its meaning has changed between the two corpora."
        },
        {
            "heading": "3.1 Computing Sense Distributions",
            "text": "We represent the meaning expressed by a target word w in a corpus C by the distribution of w\u2019s word senses, p(zw|w, C). As explained in \u00a71, our working hypothesis is that if the meaning of w has changed from C1 to C2, then the corresponding sense distributions of w, p(zw|w, C1) will be different from p(zw|w, C2). Therefore, we first estimate p(zw|w, C) according to the probabilistic model illustrated in the plate diagram in Figure 1. We consider the corpus, C to be a collection of |C| sentences from which a sentence s is randomly sampled according to p(s|C). Next, for each word w in vocabulary V that appears in s, we randomly sample a sense zw from its set of sense ids Zw. As shown in Figure 1, we assume the sense that a word takes in a sentence to be independent of the other sentences in the corpus, which enables us to factorise p(z|w, C) as in (1).\np(zw|w, C) = \u2211\ns\u2208C(w)\np(zw|w, s)p(s|C) (1)\nHere, C(w) is the subset of sentences in C where w occurs. We assume p(s|C) to be uniform and set it to be 1/|C|, where |C| is the number of sentences in C.\nFollowing the prior works that use static sense embeddings for conducting WSD, the similarity between the pre-trained static sense embedding zw of the sense zw of w, and the contextualised word embedding f(w, s) of w in s (obtained from\na pre-trained MLM) can be used as the confidence score for predicting whether w takes the sense zw in s. Specifically, the LMMS and ARES sense embeddings we use in our experiments are computed using BERT (Devlin et al., 2019) as the back-end, producing aligned vector spaces where we can compute the confidence scores using the inner-product as given by (2).\np(zw|w, s) = \u27e8zw,f(w, s)\u27e9\u2211\nz\u2032w\u2208Zw \u27e8z \u2032 w,f(w, s)\u27e9\n(2)\nIn WSD, an ambiguous word is typically assumed to take only a single sense in a given context. Therefore, WSD methods assign the most probable sense z\u2217w to w in s, where z \u2217 w = argmax zw\u2208Zw p(zw|w, s). However, not all meanings of a word might necessarily map to a single word sense due to the incompleteness of sense inventories. For example, a novel use of an existing word might be a combination of multiple existing senses of that word rather than a novel sense. Therefore, it is important to consider the sense distribution over word senses, p(zw|w, s), instead of only the most probable sense. Later in \u00a75.1, we experimentally study the effect of using top-k senses of a word in a given sentence."
        },
        {
            "heading": "3.2 Comparing Sense Distributions",
            "text": "Following the procedure described in \u00a73.1, we independently compute the distributions p(zw|w, C1) and p(zw|w, C2) respectively from C1 and C2. Next, we compare those two distributions using different distance measures, d(p(zw|w, C1), p(zw|w, C2)), to determine whether the meaning of w has changed between C1 and C2. For this purpose, we use five distance measures (i.e. Cosine, Chebyshev, Canberra, Bray-Curtis, Euclidean) and two divergence measures (i.e. Jensen-Shannon (JS), Kullback\u2013Leibler (KL)) in our experiments. For computing distance measures, we consider each sense zw as a dimension in a vector space where the corresponding value is set to p(zw|w, C). The definitions of those measures are given in Appendix A."
        },
        {
            "heading": "4 Experiments",
            "text": "Data and Evaluation Metrics: We use the SemEval-2020 Task 1 dataset (Schlechtweg et al., 2020) to evaluate SCD of words over time for English, German, Swedish and Latin in two subtasks:\nbinary classification and ranking. In the classification subtask, the words in the evaluation set must be classified as to whether they have semantically changed over time. Classification Accuracy (i.e. percentage of the correctly predicted words in the set of test target words) is used as the evaluation metric for this task.\nTo predict whether a target word w has its meaning changed, we use Bayesian optimisation to find a threshold on the distance, d(p(zw|w, C1), p(zw|w, C2)), between the sense distributions of w computed from C1 and C2. Specifically, we use the Adaptive Experimentation Platform4 to find the threshold that maximises the classification accuracy on a randomly selected heldout portion of words from the SemEval dataset, reserved for validation purposes. We found that using Bayesian optimisation is more efficient than conducting a linear search over the parameter space. We repeat this threshold estimation process five times and use the averaged parameter values in the remainder of the experiments.\nIn the ranking subtask, the words in the evaluation set must be sorted according to the degree of semantic change. Spearman\u2019s rank correlation coefficient (\u03c1 \u2208 [\u22121, 1]) between the human-rated gold scores and the induced ranking scores is used as the evaluation metric for this subtask. Higher \u03c1 values indicate better SCD methods.\nStatistics of the data used in our experiments are shown in Table 4 in Appendix B. The English dataset includes two corpora from different centuries extracted from CCOHA (Alatrash et al., 2020). Let us denote the corpora collected from the early 1800s and late 1900s to early 2000s respectively by C1 and C2. For each language, its test set has 30-48 target words that are selected to indicate whether they have undergone a semantic change between the two time periods. These words are annotated by native speakers indicating whether their meanings have changed over time and if so the degree of the semantic change.\nSense Embeddings: We use two pretrained sense embeddings in our experiments: LMMS (Loureiro and Jorge, 2019) and ARES (Scarlini et al., 2020b). For English monolingual experiments we use the 2048-dimensional\n4https://ax.dev/\nLMMS5 and ARES6 embeddings computed using bert-large-cased,7 which use WordNet sense ids. For the multilingual experiments, we use the 768-dimensional ARES embeddings computed using multilingual-bert,8 which uses BabelNet sense ids. As a baseline method that does not use sense embeddings, we use the English WSD implementation in NLTK9 to predict WordNet sense-ids for the target words, when computing the sense distributions, p(zw|s) in (1).\nHardware and Hyperparameters: We used a single NVIDIA RTX A6000 and 64 GB RAM in our experiments. It takes ca. 7 hours to compute semantic change scores for all target words in the 15.3M sentences in the SemEval datasets for all languages we consider. The only hyperparameter in SSCS is the threshold used in the binary classification task, which is tuned using Bayesian Optimisation. The obtained thresholds for the different distance measures are shown in Table 5 in Appendix B."
        },
        {
            "heading": "5 Results",
            "text": "5.1 Effect of Top-k Senses\nThe correct sense of a word might not necessarily be ranked as the top-1 because of two reasons: (a) the sense embeddings might not perfectly encode all sense related information, and (b) the contextualised word embeddings that we use to compute the inner-product with sense embeddings might en-\n5https://github.com/danlou/LMMS/tree/ LMMS ACL19\n6http://sensembert.org/ 7https://huggingface.co/bert-large-cased 8https://huggingface.co/\nbert-base-multilingual-cased 9https://www.nltk.org/howto/wsd.html\ncode information that is not relevant to the meaning of the target word in the given context. Therefore, there is some benefit of not strictly limiting the sense distribution only to the top-1 ranked sense, but to consider k(\u2265 1) senses for a target word in a given context. However, when we increase k, we will consider less likely senses of w in a given context s, thereby introducing some noise in the computation of p(zw|w, C).\nWe study the effect of using more than one sense in a given context to compute the sense distribution of a target word. Specifically, we sort the senses in the descending order of p(zw|w, s), and select the top-k ranked senses to represent w in s. Setting k = 1 corresponds to considering the most probable sense, i.e. argzw\u2208Zw max p(zw|w, s). In Figure 2, we plot the accuracy (for the binary classification subtask) and \u03c1 (for the ranking subtask) obtained using LMMS sense embeddings and JS divergence measure against k. For this experiment, we use a randomly held-out set of English words from the SemEval dataset. From Figure 2, we see that both accuracy and \u03c1 increase initially with k up to a maximum at k = 2, and then start to drop. Following this result, we limit the sense distributions to the top 2 senses for the remainder of the experiments reported in this paper."
        },
        {
            "heading": "5.2 English Monolingual Results",
            "text": "Aida and Bollegala (2023) showed that the performance of an SCD method depends on the metric used for comparisons. Therefore, in Table 1 we show SCD results for English target words with different distance/divergence measures using LMMS, ARES sense embeddings against the NLTK WSD baseline. We see that LMMS sense embeddings coupled with JS divergence report the best performance for both the binary classification and ranking subtasks across all settings, whereas ARES sense embeddings with JS divergence obtain similar accuracy for the binary classification subtask. Comparing the WSD methods, we see that NLTK is not performing as well as the sense embedding-based methods (i.e. LMMS and ARES) in terms of Spearman\u2019s \u03c1 for the ranking subtask. Although NLTK matches the performance of ARES on the classification subtask, it is still below that of LMMS. Moreover, the best performance for NLTK for the classification subtask is achieved with multiple metrics such as Cosine, Bray-Curtis, Euclidean and KL. Therefore, we conclude that the sense embedding-\nbased sense distribution computation is superior to that of the NLTK baseline.\nAmong the different distance/divergence measures used, we see that JS divergence measure performs better than the other measures. In particular, for both ARES and LMMS, JS outperforms other measures for both subtasks, and emerges as the overall best metric for SCD. On the other hand, Cosine distance, which has been used as a baseline in much prior work on semantic change detection (Rosin et al., 2022) performs poorly for the ranking subtask although it does reasonably well on the classification subtask. Rosin et al. (2022) predicted semantic changes by thresholding the cosine distance. They used peak detection methods (Palshikar, 2009) to determine this threshold, whereas we use Bayesian optimisation methods."
        },
        {
            "heading": "5.3 English SCD Results",
            "text": "We compare SSCS against the following prior SCD methods on the SemEval-2020 Task 1 English data. Due to space limitations, further details of those methods are given in Appendix C.\nBERT + Time Tokens + Cosine is the method\nproposed by Rosin et al. (2022) that fine-tunes pretrained BERT-base models using time tokens. BERT + APD was proposed by Kutuzov and Giulianelli (2020) that uses the averages pairwise cosine distance. Based on this insight, Aida and Bollegala (2023) evaluate the performance of BERT + TimeTokens + Cosine with the average pairwise cosine distance computed using pre-trained BERT-base as the MLM. BERT+DSCD is the sibling Distribution-based SCD (DSCD) method proposed by Aida and Bollegala (2023).\nA Temporal Attention mechanism was proposed by Rosin and Radinsky (2022) where they add a trainable temporal attention matrix to the pretrained BERT models. Because their two proposed methods (fine-tuning with time tokens and temporal attention) are independent, Rosin and Radinsky (2022) proposed to use them simultaneously, which is denoted by BERT + Time Tokens + Temporal Attention. Yu\u0308ksel et al. (2021) extended word2vec to create Gaussian embeddings (Vilnis and McCallum, 2015) for target words independently from each corpus\nRother et al. (2020) proposed Clustering on Manifolds of Contextualised Embeddings (CMCE) where they use mBERT embeddings with dimensionality reduction to represent target words, and then apply clustering algorithms to find the different sense clusters. CMCE is the current SoTA for the binary classification task. Asgari et al. (2020) proposed EmbedLexChange, which uses fasttext (Bojanowski et al., 2017) to create word embeddings from each corpora separately, and measures the cosine similarity between a word and a fixed set of pivotal words to represent a word in a corpus using the distribution over those pivots.\nUWB (Praz\u030ca\u0301k et al., 2020) learns separate word embeddings for a target word from each corpora and then use Canonical Correlation Analysis (CCA) to align the two vector spaces. UWB was ranked 1st for the binary classification subtask at the official SemEval 2020 Task 1 competition.\nIn Table 2, we compare our SSCS with JS using LMMS sense embeddings (which reported the best performance according to Table 1) against prior work. For prior SCD methods, we report performance from the original publications, without rerunning those methods. However, not all prior SCD methods evaluate on both binary classification and ranking subtasks as we do in this paper, which is indicated by N/A (not available) in Table 2. XL-\nLEXEME is a supervised SCD method that is the current SoTA on this dataset.\nFrom Table 2, we see that SSCS obtains competitive results for both binary classification and ranking subtasks on the SemEval-2020 Task 1 English dataset, showing the effectiveness of word sense information for SCD. It matches the performance of CMCE for the binary classification subtask, while outperforming Temporal attention with Time Token fine-tuning (Rosin and Radinsky, 2022) on the ranking subtask.\nAlthough models such as CMCE and EmbedLexChange have good performance for the binary classification subtask, their performance on the ranking subtask is poor. Both of those methods learn static word embeddings for a target word independently from each corpus. Therefore, those methods must first learn comparable distributions before a distance measure can be used to calculate a semantic change score for a target word. As explained above, CMCE learns CCA-based vector space alignments, while EmbedLexChange uses the cosine similarity over a set of fixed pivotal words selected from the two corpora. Both vector space alignments and pivot selection are error prone, and add additional noise to SCD. On the other hand, SSCS uses the same MLM and sense embeddings on both corpora when computing the sense distributions, thus obviating the need for any costly vector space alignments.\nBoth TimeToken and Temporal Attention methods require retraining a transformer model (i.e. BERT models are used in the original papers). TimeToken prepends each sentence with a timestamp, thereby increasing the input length, which results in longer training times with transformer-\nbased LLMs. On the other hand, Temporal Attention increases the number of parameters in the transformer as it uses an additional timespecific weight matrix. Interestingly, from Table 2 we see that SSCS outperforms both those methods convincingly despite not requiring any fine-tuning/retraining of the sense embeddings nor MLMs, which is computationally attractive.\nSSCS (which is unsupervised) does not outperform XL-LEXEME (which is trained on WiC data) for the ranking subtask. In particular, we see a significant performance gap between XL-LEXEME and the rest of the unsupervised methods, indicating that future work on SCD should explore the possibility of incorporating some form a supervision to further improve performance. Although in SSCS, we used pre-trained static sense embeddings without any further fine-tuning, we could have used WiC data to select the classification threshold. During inference time, XL-LEXEME computes the average pair-wise cosine distance between the embeddings of sentences that contain the target word (which we are interested in predicting whether its meaning has changed over time), selected from each corpora. However, as already discussed in \u00a7 5.2, JS divergence outperforms cosine distance for SCD. Therefore, it would be an interesting future research direction would be to incorporate the findings from unsupervised SCD to further improve performance in supervised SCD methods."
        },
        {
            "heading": "5.4 Multilingual SCD Results",
            "text": "To evaluate the effectiveness of word sense distributions for detecting semantic change of words in other languages, we use the 768- dimensional ARES multilingual sense embed-\ndings,10 trained using BabelNet concept ids. We use bert-base-multilingual-cased11 as the multilingual MLM in this evaluation because it is compatible with the ARES multilingual sense embeddings. For evaluations, we use the ranking subtask data in the SemEval 2020 Task 1 for German, Swedish and Latin.\nIn Figure 3 we compare \u03c1 values obtained using different distance/divergence measures. We see that JS performs best for English, KL for German and Latin, whereas Canberra for Swedish. Overall, the divergence-based measures (i.e. KL and JS) report better results than distance-based measures across languages, except in Swedish. Various factors affect the performance such as the coverage and sparseness of sense distributions, size of the corpora in each time period, and the number of test target words in each evaluation dataset. Therefore, it is difficult to attribute the performance differences across languages purely to the different distance/divergence measures used to compare the sense distributions.\nThe performance for non-English languages is much lower compared to that for English. This is due to three main reasons: (a) the limited sense coverage in BabelNet for non-English languages (especially Latin and Swedish in this case), (b) the accuracy of ARES sense embedding for German and Latin being lower compared to that for English, and (c) the multilingual contextualised embeddings\n10http://sensembert.org/resources/ares embedding.tar.gz\n11https://huggingface.co/ bert-base-multilingual-cased\nobtained from mBERT has poor coverage for Latin. Although more language-specialised MLMs are available such as GermanBERT12, LatinBERT,13 and SwedishBERT14, we must have compatible sense embeddings to compute the sense distributions. Learning accurate multilingual sense embeddings is an active research area (Rezaee et al., 2021; Upadhyay et al., 2017) on its own and is beyond the scope of this paper which focuses on SCD."
        },
        {
            "heading": "5.5 Qualitative Analysis",
            "text": "Figure 4 shows example sense distributions and the corresponding JS divergence scores for the words plane (a word that has changed meaning according to SemEval annotators, giving a rating of 0.882) and pin (a word that has not changed its meaning, with a rating of 0.207) from the SemEval English binary classification subtask. We see that the two distributions for plane are significantly different from each other (the second peak at sense-id 5 vs. 6, respectively in C1 and C2), as indicated by a high JS divergence (i.e. 0.221). On the other hand, the sense distributions for pin are similar, resulting in a relatively smaller (i.e. 0.027) JS divergence. This result supports our claim that sense distributions provide useful clues for SCD of words.\nIn Table 3, we show the top- and bottom-8 ranked words according to their semantic change scores in the SemEval English dataset. We compare the ranks assigned to words according to SSCS against the NLTK baseline (used in Table 1) and DSCD (Aida and Bollegala, 2023). From Table 3 we see that for 6 (i.e. plane, tip, graft, record, stab, head) out of the top-8 ranked words with a semantic change between the corpora, SSCS assigns equal or lower ranks than either of NLTK or DSCD. Moreover, we see that SSCS assigns lower ranks to words that have not changed meaning across corpora.\nAs an error analysis, let us consider risk, which is assigned a higher rank (8) incorrectly by SSCS, despite not changing its meaning. Further investigations (see Appendix D) reveal that the sense distributions for risk computed from the two corpora are indeed very similar, except that C2 has two additional senses not present in C1. However, those additional senses are highly similar to ones\n12https://huggingface.co/ bert-base-german-cased\n13https://github.com/dbamman/latin-bert 14https://huggingface.co/KB/\nbert-base-swedish-cased\npresent in C1 and imply the semantic invariance of risk. Explicitly incorporating sense similarity into SSCS could further improve its performance."
        },
        {
            "heading": "6 Conclusion",
            "text": "We proposed, SSCS, a sense distribution-based method for predicting the semantic change of a word from one corpus to another. SSCS obtains good performance among the unsuupervised methods for both binary classification and ranking subtasks for the English unsupervised SCD on the Se-\nmEval 2020 Task 1 dataset. The experimental results highlight the effectiveness of using word sense distribution to detect semantic changes of words in different languages."
        },
        {
            "heading": "Acknowledgements",
            "text": "Danushka Bollegala holds concurrent appointments as a Professor at University of Liverpool and as an Amazon Scholar. This paper describes work performed at the University of Liverpool and is not associated with Amazon."
        },
        {
            "heading": "7 Limitations",
            "text": "An important limitation in our proposed sense distribution-based SCD method is its reliance on sense labels. Sense labels could be obtained using WSD tools (as demonstrated by the use of NLTK baseline in our experiments) or by performing WSD using pre-trained static sense embeddings with contextualised word embeddings, obtained from pre-trained MLMs (as demonstrated by the use of LMMS and ARES in our experiments). Even if the WSD accuracy is not high for the top-1 predicted sense, SSCS can still accurately predict SCD because it uses the sense distribution for a target word, and not just the top-1 predicted sense. Moreover, SSCS uses the sense distribution of a target word over the entire corpus and not for a single sentence. Both WSD and sense embedding learning are active research topics in NLP (Bevilacqua and Navigli, 2020). We can expect the performance of WSD tools and sense embeddings to improve further in the future, which will further improve the SCD accuracy of SSCS.\nAlthough we evaluated the performance of SSCS in German, Latin and Swedish in addition to English, this is still a limited set of languages. How-\never, conducting a large-scale multilingual evaluation for SCD remains a formidable task due to the unavailability of human annotated semantic change scores/labels for words in many different languages. As demonstrated by the difficulties in data annotation tasks during the SemEval 2020 Task 1 on Unsupervised SCD (Schlechtweg et al., 2020), it is difficult to recruit native speakers for all languages of interest. Indeed for Latin it has been reported that each test word was annotated by only a single expert because it was not possible to recruit native speakers via crowd-sourcing for Latin, which is not a language in active usage. Therefore, we can expect similar challenges when evaluating SCD performance for rare or resource poor languages, with limited native speakers. Moreover, providing clear annotation guidelines for the semantic changes of a word in a corpus is difficult, especially when the semantic change happens gradually over a longer period of time.\nThe semantic changes of words considered in SemEval 2020 Task 1 dataset span relatively longer time periods, such as 50-200 years. Although it is possible to evaluate the performance of SCD methods for detecting semantic changes of words that happen over a longer period, it is unclear from the evaluations on this dataset whether SSCS can detect more short term semantic changes. For example, the word corona gained a novel meaning in 2019 with the wide spread of COVID-19 pandemic compared to its previous meanings (e.g. sun\u2019s corona rings and a beer brand). We believe that it is important for an SCD method to accurately detect such short term semantic changes, especially when used in applications such as information retrieval, where keywords associated with user interests vary over a relatively shorter period of time (e.g. seasonality related queries can vary over a few weeks to a few months)."
        },
        {
            "heading": "8 Ethical Considerations",
            "text": "We considered the problem of SCD of words across corpora, sampled at different points in time. To evaluate our proposed method, SSCS, against previously proposed methods for SCD we use the publicly available SemEval 2020 Task 1 datasets. We are unaware of any social biases or other ethical issues reported regarding this dataset. Moreover, we did not collect, annotate, or distribute any datasets as part of this work. Therefore, we do not foresee any ethical concerns regarding our work.\nHaving said that, we would like to point out that we are using pre-trained MLMs and static sense embeddings in SSCS. MLMs are known to encode unfair social biases such as gender- or race-related biases (Basta et al., 2019). Moreover, Zhou et al. (2022) showed that static sense embeddings also encode unfair social biases. Therefore, it is unclear how such biases would affect the SCD performance of SSCS. On the other hand, some gender-related words such as gay have changed their meaning over the years (e.g. offering fun and gaiety vs. someone who is sexually attracted to persons of the same sex). The ability to correctly detect such changes will be important for NLP models to make fair and unbiased decisions and generate unbiased responses when interacting with human users in real-world applications."
        },
        {
            "heading": "A Distance and Divergence Measures",
            "text": "We describe the distance and divergence measures d used in our experiments to compare the sense distributions p1(z) = p(zw|w, C1) and p2(z) = p(zw|w, C2) computed respectively from C1 and C2.\nKullback-Liebler (KL) Divergence:\nKL(p1||p2) = \u2211\nz\u2208Zw\n(p1(z) log\n( p1(z)\np2(z)\n) (3)\nJensen-Shannon (JS) Divergence:\nJS(p1||p2)\n= 1\n2 KL (p1||q) +\n1 2 KL (p2||q) (4)\nHere,\nq(z) = 1\n2 (p1(z) + p2(z)) (5)\nBray-Curtis Distance:\nd(p1, p2) = \u2211 z\u2208Zw |p1(z)\u2212 p2(z)|\u2211 z\u2208Zw |p1(z) + p2(z)|\n(6)\nCanberra Distance:\nd(p1, p2) = \u2211\nz\u2208Zw\n|p1(z)\u2212 p2(z)| |p1(z) + p2(z)|\n(7)\nChebyshev Distance: d(p1, p2) = max\nz\u2208Zw |p1(z)\u2212 p2(z)| (8)\nCosine Distance: d(p1, p2) = 1\u2212 \u2211\nz\u2208Zw p1(z)p2(z)\u221a\u2211 z\u2208Zw p1(z) 2 \u221a\u2211 z\u2208Zw p1(z) 2\n(9)\nEuclidean Distance:\nd(p1, p2) = \u221a\u2211 z (p1(z)\u2212 p2(z))2 (10)"
        },
        {
            "heading": "B Experimental Settings",
            "text": "Statistics of the SemEval 2020 Task 1 Unsupervised Semantic Change Detection Dataset is shown in Table 4. Thresholds found with Bayesian optimisation for the classification task are listed in Table 5. We used random seed of 42 while performing the Bayesian optimisation. From Table 5 we see that the classification thresholds that are learnt for different divergence/distance metrics are largely different."
        },
        {
            "heading": "C Previously Proposed SCD Methods",
            "text": "We compare SSCS against the following prior SCD methods on the SemEval-2020 Task 1 English data.\nBERT + Time Tokens + Cosine : Rosin et al. (2022) performed fine-tuning of the published pretrained BERT-base models using time tokens. They add a time token (e.g. <2023>) to the beginning of a sentence. In the fine-tuning step, the models use two types of masked language modelling objectives: 1) predicting the masked time tokens from given contexts, and 2) predicting the masked tokens from given contexts with time tokens. They make predictions with the average distance of the target token probabilities or the cosine distance of the average sibling embeddings. According to their results, the cosine distance achieves better performance than the average distance of the probabilities. Therefore, we use cosine distance as the distance metric for this method.\nBERT + APD : Kutuzov and Giulianelli (2020) report that the average pairwise cosine distance outperforms the cosine distance. Based on this insight, Aida and Bollegala (2023) evaluate the performance of BERT + TimeTokens + Cosine with the average pairwise cosine distance computed using pre-trained BERT-base as the MLM.\nBERT+DSCD : Aida and Bollegala (2023) proposed a Distribution-based Semantic Change Detection (DSCD), considering distributions of sibling embeddings (sibling distribution). During prediction, they sample an equal number of target word vectors from the sibling distribution (approximated by a multivariate diagonal Gaussian) for each time period and calculate the average distance. They report that the Chebyshev distance function achieves the best performance.\nTemporal Attention : Rosin and Radinsky (2022) proposed a temporal attention mechanism, where they add a trainable temporal attention matrix to the pretrained BERT models. Subsequently, additional training is performed on the target cor-\npus. They use the cosine distance following their earlier work (Rosin et al., 2022).\nBERT + Time Tokens + Temporal Attention : Because their two proposed methods (fine-tuning with time tokens and temporal attention) are independent, Rosin and Radinsky (2022) proposed to use them simultaneously. They also use the cosine distance for prediction and report that this model achieves current state-of-the-art performance in semantic change detection.\nGaussian Embeddings: Yu\u0308ksel et al. (2021) extend word2vec to create Gaussian embeddings (Vilnis and McCallum, 2015) for target words independently in each corpus. They then learn a mapping between the two vector spaces for computing a semantic change score.\nCMCE: Rother et al. (2020) proposed Clustering on Manifolds of Contextualised Embeddings (CMCE) where they use mBERT embeddings with dimensionality reduction to represent target words, and then apply clustering algorithms to find the different sense clusters. CMCE is the current SoTA\nfor the binary classification task.\nEmbedLexChange: Asgari et al. (2020) used fasttext (Bojanowski et al., 2017) to create word embeddings from each corpora separately, and measure the cosine similarity between a word and a selected fixed set of pivotal words to represent a word in a corpus using the distribution over those pivots. They used KL divergence to measure the similarity between the two distributions associated with a target word to compute a semantic change score.\nUWB: Praz\u030ca\u0301k et al. (2020) learnt separate word embeddings for a target word from each corpora and then use Canonical Correlation Analysis (CCA) to align the two vector spaces. They use the average cosine similarity between the two embeddings over all target words as the threshold for predicting semantic changes of words. UWB was ranked 1st for the binary classification subtask at the official SemEval 2020 Task 1 competition."
        },
        {
            "heading": "D Error Analysis",
            "text": "As a concrete example of computing semantic change scores using sense distributions where SSCS assigned a low JS divergence value using LMMS incorrectly to a word that had not changed its meaning, we consider the word risk. The sense distributions, p(zw|w,C1) and p(zw|w,C2), of risk in the two corpora, respectively C1 and C2 are shown in Figure 5. The word risk is considered as a noun in the SemEval 2020 Task 1 dataset, and the annotator-assigned semantic change score is 0, indicating that it has not changed meaning between the two corpora. Looking at the sense distributions in Figure 5, we see that they are almost similar with two peaks at sense-ids 2 and 4. However, two additional senses, corresponding sense-id 3 (\u2019risk%1:07:02::\u2019, risk of exposure, the probability of being exposed to an infectious agent) and sense-id 5 (\u2019risk%1:07:01::\u2019, expose to a chance of loss or damage) can be observed in Figure 5b in C2. According to the sense definitions in the WordNet for those two senses, they are very similar (i.e. expressing different types of risks), which could be considered to be a case where the meaning of risk remains largely the same in the two corpora. Because of this reason the JS divergence score between the two distributions tends to be higher at 0.2139, giving it a lower rank (rank 8 among 37 words in Table 3, indicating that risk has changed its meaning). This is a typical example where although a word might take different senses in different corpora (or within different sentences in the same corpora), some of those senses could be highly similar and could be mapped to the same meaning for the purpose of SCD."
        }
    ],
    "title": "Can Word Sense Distribution Detect Semantic Changes of Words?",
    "year": 2023
}