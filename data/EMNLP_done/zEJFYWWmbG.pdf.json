{
    "abstractText": "Instruction-tuned large language models (LLMs), such as ChatGPT, have led to promising zero-shot performance in discriminative natural language understanding (NLU) tasks. This involves querying the LLM using a prompt containing the question, and the candidate labels to choose from. The question-answering capabilities of ChatGPT arise from its pre-training on large amounts of human-written text, as well as its subsequent fine-tuning on human preferences, which motivates us to ask: Does ChatGPT also inherit humans\u2019 cognitive biases? In this paper, we study the primacy effect of ChatGPT: the tendency of selecting the labels at earlier positions as the answer. We have two main findings: i) ChatGPT\u2019s decision is sensitive to the order of labels in the prompt; ii) ChatGPT has a clearly higher chance to select the labels at earlier positions as the answer. We hope that our experiments and analyses provide additional insights into building more reliable ChatGPT-based solutions. We release the source code at https: //github.com/wangywUST/PrimacyEffectGPT.",
    "authors": [
        {
            "affiliations": [],
            "name": "Yiwei Wang"
        },
        {
            "affiliations": [],
            "name": "Yujun Cai"
        },
        {
            "affiliations": [],
            "name": "Muhao Chen"
        },
        {
            "affiliations": [],
            "name": "Yuxuan Liang"
        },
        {
            "affiliations": [],
            "name": "Bryan Hooi"
        }
    ],
    "id": "SP:c0eadf0a05e7a5fc5a7b3124922de600910f8a78",
    "references": [
        {
            "authors": [
                "Khaled Albishre",
                "Mubarak Albathan",
                "Yuefeng Li."
            ],
            "title": "Effective 20 newsgroups dataset cleaning",
            "venue": "2015 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology (WI-IAT), volume 3, pages 98\u2013101. IEEE.",
            "year": 2015
        },
        {
            "authors": [
                "Christoph Alt",
                "Aleksandra Gabryszak",
                "Leonhard Hennig."
            ],
            "title": "TACRED revisited: A thorough evaluation of the TACRED relation extraction task",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1558\u2013",
            "year": 2020
        },
        {
            "authors": [
                "Solomon E Asch."
            ],
            "title": "Forming impressions of personality",
            "venue": "The Journal of Abnormal and Social Psychology, 41(3):258.",
            "year": 1946
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners. Advances in neural information processing",
            "year": 2020
        },
        {
            "authors": [
                "I\u00f1igo Casanueva",
                "Tadas Temcinas",
                "Daniela Gerz",
                "Matthew Henderson",
                "Ivan Vulic."
            ],
            "title": "Efficient intent detection with dual sentence encoders",
            "venue": "Proceedings of the 2nd Workshop on NLP for ConvAI - ACL 2020. Data avail-",
            "year": 2020
        },
        {
            "authors": [
                "I\u00f1igo Casanueva",
                "Tadas Tem\u010dinas",
                "Daniela Gerz",
                "Matthew Henderson",
                "Ivan Vuli\u0107."
            ],
            "title": "Efficient intent detection with dual sentence encoders",
            "venue": "arXiv preprint arXiv:2003.04807.",
            "year": 2020
        },
        {
            "authors": [
                "Jiawei Chen",
                "Hande Dong",
                "Xiang Wang",
                "Fuli Feng",
                "Meng Wang",
                "Xiangnan He."
            ],
            "title": "Bias and debias in recommender system: A survey and future directions",
            "venue": "ACM Transactions on Information Systems, 41(3):1\u201339.",
            "year": 2023
        },
        {
            "authors": [
                "Wei-Lin Chiang",
                "Zhuohan Li",
                "Zi Lin",
                "Ying Sheng",
                "Zhanghao Wu",
                "Hao Zhang",
                "Lianmin Zheng",
                "Siyuan Zhuang",
                "Yonghao Zhuang",
                "Joseph E. Gonzalez",
                "Ion Stoica",
                "Eric P. Xing"
            ],
            "title": "Vicuna: An opensource chatbot impressing gpt-4 with 90%* chatgpt",
            "year": 2023
        },
        {
            "authors": [
                "Aakanksha Chowdhery",
                "Sharan Narang",
                "Jacob Devlin",
                "Maarten Bosma",
                "Gaurav Mishra",
                "Adam Roberts",
                "Paul Barham",
                "Hyung Won Chung",
                "Charles Sutton",
                "Sebastian Gehrmann"
            ],
            "title": "Palm: Scaling language modeling with pathways",
            "year": 2022
        },
        {
            "authors": [
                "William D Crano."
            ],
            "title": "Primacy versus recency in retention of information and opinion change",
            "venue": "The Journal of Social Psychology, 101(1):87\u201396.",
            "year": 1977
        },
        {
            "authors": [
                "Qingxiu Dong",
                "Lei Li",
                "Damai Dai",
                "Ce Zheng",
                "Zhiyong Wu",
                "Baobao Chang",
                "Xu Sun",
                "Jingjing Xu",
                "Zhifang Sui."
            ],
            "title": "A survey for in-context learning",
            "venue": "arXiv preprint arXiv:2301.00234.",
            "year": 2022
        },
        {
            "authors": [
                "Yu Fei",
                "Yifan Hou",
                "Zeming Chen",
                "Antoine Bosselut."
            ],
            "title": "Mitigating label biases for in-context learning",
            "venue": "arXiv preprint arXiv:2305.19148.",
            "year": 2023
        },
        {
            "authors": [
                "Jack FitzGerald",
                "Christopher Hench",
                "Charith Peris",
                "Scott Mackie",
                "Kay Rottmann",
                "Ana Sanchez",
                "Aaron Nash",
                "Liam Urbach",
                "Vishesh Kakarala",
                "Richa Singh"
            ],
            "title": "Massive: A 1m-example multilingual natural language understanding dataset",
            "year": 2022
        },
        {
            "authors": [
                "Bent Fuglede",
                "Flemming Topsoe."
            ],
            "title": "Jensenshannon divergence and hilbert space embedding",
            "venue": "International Symposium onInformation Theory, 2004. ISIT 2004. Proceedings., page 31. IEEE.",
            "year": 2004
        },
        {
            "authors": [
                "Jinglong Gao",
                "Xiao Ding",
                "Bing Qin",
                "Ting Liu."
            ],
            "title": "Is chatgpt a good causal reasoner? a comprehensive evaluation",
            "venue": "arXiv preprint arXiv:2305.07375.",
            "year": 2023
        },
        {
            "authors": [
                "Ridong Han",
                "Tao Peng",
                "Chaohao Yang",
                "Benyou Wang",
                "Lu Liu",
                "Xiang Wan."
            ],
            "title": "Is information extraction solved by chatgpt? an analysis of performance, evaluation criteria, robustness and errors",
            "venue": "arXiv preprint arXiv:2305.14450.",
            "year": 2023
        },
        {
            "authors": [
                "Mubin Ul Haque",
                "Isuru Dharmadasa",
                "Zarrin Tasnim Sworna",
                "Roshan Namal Rajapakse",
                "Hussain Ahmad."
            ],
            "title": " i think this is the most disruptive technology\": Exploring sentiments of chatgpt early adopters using twitter data",
            "venue": "arXiv preprint",
            "year": 2022
        },
        {
            "authors": [
                "Sebastian Kr\u00fcgel",
                "Andreas Ostermaier",
                "Matthias Uhl."
            ],
            "title": "The moral authority of chatgpt",
            "venue": "arXiv preprint arXiv:2301.07098.",
            "year": 2023
        },
        {
            "authors": [
                "Bo Li",
                "Gexiang Fang",
                "Yang Yang",
                "Quansen Wang",
                "Wei Ye",
                "Wen Zhao",
                "Shikun Zhang."
            ],
            "title": "Evaluating chatgpt\u2019s information extraction capabilities: An assessment of performance, explainability, calibration, and faithfulness",
            "venue": "arXiv preprint arXiv:2304.11633.",
            "year": 2023
        },
        {
            "authors": [
                "Cong Li."
            ],
            "title": "Primacy effect or recency effect? a long-term memory test of super bowl commercials",
            "venue": "Journal of Consumer Behaviour: An International Research Review, 9(1):32\u201344.",
            "year": 2010
        },
        {
            "authors": [
                "Yao Lu",
                "Max Bartolo",
                "Alastair Moore",
                "Sebastian Riedel",
                "Pontus Stenetorp."
            ],
            "title": "Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity",
            "venue": "arXiv preprint arXiv:2104.08786.",
            "year": 2021
        },
        {
            "authors": [
                "Yubo Ma",
                "Yixin Cao",
                "YongChing Hong",
                "Aixin Sun"
            ],
            "title": "Large language model is not a good few-shot information extractor, but a good reranker for hard samples! arXiv preprint arXiv:2303.08559",
            "year": 2023
        },
        {
            "authors": [
                "Kamil Malinka",
                "Martin Peres\u00edni",
                "Anton Firc",
                "Ondrej Hujnak",
                "Filip Janus"
            ],
            "title": "On the educational impact of chatgpt: Is artificial intelligence ready to obtain a university degree",
            "venue": "In Proceedings of the 2023 Conference on Innovation and Technology",
            "year": 2023
        },
        {
            "authors": [
                "Long Ouyang",
                "Jeffrey Wu",
                "Xu Jiang",
                "Diogo Almeida",
                "Carroll Wainwright",
                "Pamela Mishkin",
                "Chong Zhang",
                "Sandhini Agarwal",
                "Katarina Slama",
                "Alex Ray"
            ],
            "title": "Training language models to follow instructions with human",
            "year": 2022
        },
        {
            "authors": [
                "Chen Qian",
                "Fuli Feng",
                "Lijie Wen",
                "Chunping Ma",
                "Pengjun Xie."
            ],
            "title": "Counterfactual inference for text classification debiasing",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Confer-",
            "year": 2021
        },
        {
            "authors": [
                "Jack W Rae",
                "Sebastian Borgeaud",
                "Trevor Cai",
                "Katie Millican",
                "Jordan Hoffmann",
                "Francis Song",
                "John Aslanides",
                "Sarah Henderson",
                "Roman Ring",
                "Susannah Young"
            ],
            "title": "Scaling language models: Methods, analysis & insights from training gopher",
            "year": 2021
        },
        {
            "authors": [
                "Shaden Smith",
                "Mostofa Patwary",
                "Brandon Norick",
                "Patrick LeGresley",
                "Samyam Rajbhandari",
                "Jared Casper",
                "Zhun Liu",
                "Shrimai Prabhumoye",
                "George Zerveas",
                "Vijay Korthikanti"
            ],
            "title": "Using deepspeed and megatron to train megatron-turing nlg",
            "year": 2022
        },
        {
            "authors": [
                "George Stoica",
                "Emmanouil Antonios Platanios",
                "Barnab\u00e1s P\u00f3czos."
            ],
            "title": "Re-tacred: Addressing shortcomings of the tacred dataset",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 13843\u201313850.",
            "year": 2021
        },
        {
            "authors": [
                "Teo Susnjak."
            ],
            "title": "Applying bert and chatgpt for sentiment analysis of lyme disease in scientific literature",
            "venue": "arXiv preprint arXiv:2302.06474.",
            "year": 2023
        },
        {
            "authors": [
                "Chris Sweeney",
                "Maryam Najafian."
            ],
            "title": "A transparent framework for evaluating unintended demographic bias in word embeddings",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1662\u20131667.",
            "year": 2019
        },
        {
            "authors": [
                "Lydia Tan",
                "Geoff Ward."
            ],
            "title": "A recency-based account of the primacy effect in free recall",
            "venue": "Journal of Experimental Psychology: Learning, Memory, and Cognition, 26(6):1589.",
            "year": 2000
        },
        {
            "authors": [
                "Romal Thoppilan",
                "Daniel De Freitas",
                "Jamie Hall",
                "Noam Shazeer",
                "Apoorv Kulshreshtha",
                "Heng-Tze Cheng",
                "Alicia Jin",
                "Taylor Bos",
                "Leslie Baker",
                "Yu Du"
            ],
            "title": "Lamda: Language models for dialog applications",
            "venue": "arXiv preprint arXiv:2201.08239",
            "year": 2022
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin."
            ],
            "title": "Attention is all you need",
            "venue": "Advances in neural information processing systems, pages 5998\u20136008.",
            "year": 2017
        },
        {
            "authors": [
                "Zhen Wan",
                "Fei Cheng",
                "Zhuoyuan Mao",
                "Qianying Liu",
                "Haiyue Song",
                "Jiwei Li",
                "Sadao Kurohashi."
            ],
            "title": "Gpt-re: In-context learning for relation extraction using large language models",
            "venue": "arXiv preprint arXiv:2305.02105.",
            "year": 2023
        },
        {
            "authors": [
                "Fei Wang",
                "Wenjie Mo",
                "Yiwei Wang",
                "Wenxuan Zhou",
                "Muhao Chen."
            ],
            "title": "A causal view of entity bias in (large) language models",
            "venue": "arXiv preprint arXiv:2305.14695.",
            "year": 2023
        },
        {
            "authors": [
                "Yiwei Wang",
                "Muhao Chen",
                "Wenxuan Zhou",
                "Yujun Cai",
                "Yuxuan Liang",
                "Dayiheng Liu",
                "Baosong Yang",
                "Juncheng Liu",
                "Bryan Hooi"
            ],
            "title": "Should we rely on entity mentions for relation extraction? debiasing relation extraction with counterfactual analysis",
            "year": 2022
        },
        {
            "authors": [
                "Jason Wei",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Maarten Bosma",
                "Fei Xia",
                "Ed Chi",
                "Quoc V Le",
                "Denny Zhou"
            ],
            "title": "Chain-of-thought prompting elicits reasoning in large language models",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Xiang Wei",
                "Xingyu Cui",
                "Ning Cheng",
                "Xiaobin Wang",
                "Xin Zhang",
                "Shen Huang",
                "Pengjun Xie",
                "Jinan Xu",
                "Yufeng Chen",
                "Meishan Zhang"
            ],
            "title": "Zeroshot information extraction via chatting with chatgpt",
            "venue": "arXiv preprint arXiv:2302.10205",
            "year": 2023
        },
        {
            "authors": [
                "Liuyu Xiang",
                "Guiguang Ding",
                "Jungong Han."
            ],
            "title": "Learning from multiple experts: Self-paced knowledge distillation for long-tailed classification",
            "venue": "European Conference on Computer Vision, pages 247\u2013 263. Springer.",
            "year": 2020
        },
        {
            "authors": [
                "Chenhan Yuan",
                "Qianqian Xie",
                "Sophia Ananiadou."
            ],
            "title": "Zero-shot temporal relation extraction with chatgpt",
            "venue": "arXiv preprint arXiv:2304.05454.",
            "year": 2023
        },
        {
            "authors": [
                "Bowen Zhang",
                "Daijun Ding",
                "Liwen Jing"
            ],
            "title": "How would stance detection techniques evolve after the launch of chatgpt? arXiv preprint arXiv:2212.14548",
            "year": 2022
        },
        {
            "authors": [
                "Yuhao Zhang",
                "Victor Zhong",
                "Danqi Chen",
                "Gabor Angeli",
                "Christopher D Manning."
            ],
            "title": "Position-aware attention and supervised data improve slot filling",
            "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 35\u2013",
            "year": 2017
        },
        {
            "authors": [
                "Zihao Zhao",
                "Eric Wallace",
                "Shi Feng",
                "Dan Klein",
                "Sameer Singh"
            ],
            "title": "Calibrate before use: Improving few-shot performance of language models",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Qihuang Zhong",
                "Liang Ding",
                "Juhua Liu",
                "Bo Du",
                "Dacheng Tao."
            ],
            "title": "Can chatgpt understand too? a comparative study on chatgpt and fine-tuned bert",
            "venue": "arXiv preprint arXiv:2302.10198.",
            "year": 2023
        },
        {
            "authors": [
                "Wenxuan Zhou",
                "Muhao Chen."
            ],
            "title": "An improved baseline for sentence-level relation extraction",
            "venue": "pages 161\u2013168.",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Humans tend to recall information presented at the start of a list better than information at the middle or end. This phenomenon is known as the primacy effect (Asch, 1946), which is a cognitive bias that relates to humans\u2019 attention spans (Crano, 1977), rehearsal (Tan and Ward, 2000), and memory systems (Li, 2010). Similarly, in advertisement systems and search engines, humans tend to interact with items in higher positions regardless of the items\u2019 actual relevance (Chen et al., 2023). Primacy effect influences humans\u2019 behaviors to make unfair decisions. Similarly, if it exists in machine learning models, it may lead to worse performance.\n\u2217Equal contribution.\nRecently, instruction-tuned large language models (LLMs), represented by ChatGPT (OpenAI, 2022), have received wide attention on their capabilities of imitating humans in question-answering and problem-solving. However, this underlying behavioral similarity between ChatGPT and humans naturally leads to an intriguing question: Is ChatGPT also affected by the primacy effect?\nChatGPT provides a convenient way to achieve the discriminative natural language understanding (NLU) (Li et al., 2023; Wei et al., 2023; Yuan et al., 2023). People only need to list the labels in the prompt and asking ChatGPT to select the label(s) that match the input text. In this work, to analyze the primacy effect of ChatGPT, we start by testing with random label shuffling, i.e., shuffling labels listed in the prompt before every prediction. We compare the predictions on the same instance with two different label orders. Then, we count the predicted label indices on many instances with label shuffling. The motivation is that: a fair NLU model should give the same prediction on an input instance regardless of how the labels are ordered; consequently, it should produce uniformly distributed label indices under label shuffling for\nany instance. Through extensive experiments with a series of NLU datasets, we find that\n\u2022 ChatGPT\u2019s prediction is sensitive to the order of labels in the prompt. Specifically, ChatGPT\u2019s prediction changes after a label shuffling on 87.9% of the instances in TACRED.\n\u2022 ChatGPT is affected by the primacy effect: ChatGPT tends to select labels in earlier positions in the prompt (see Fig. 1), which present clear bias with respect to the label order.\nOn the whole, our work contributes to a better understanding of ChatGPT\u2019s behaviors and building more faithful ChatGPT-based NLU solutions."
        },
        {
            "heading": "2 Primacy Effect of ChatGPT",
            "text": "In this section, we first introduce the general prompt design of ChatGPT in discriminative natural language understanding (NLU). Then, we analyze the primacy effect of ChatGPT using label shuffling in prompts."
        },
        {
            "heading": "2.1 Prompts for ChatGPT",
            "text": "Prompts are a key component to the effective use of ChatGPT on discriminative NLU tasks (Wei et al., 2023; Yuan et al., 2023). Generally, prompts for such tasks involve two key components: (i) label definitions, and (ii) a task description and input text (see an example in Fig. 2).\nChatGPT\u2019s capability of understanding instructions in the prompt benefits from its training with human feedback (OpenAI, 2022), but this also creates the risk of inheriting humans\u2019 cognitive biases. In this paper, we discuss a cognitive bias in ChatGPT: the primacy effect, which indicates the tendency of selecting labels in earlier positions in the prompt."
        },
        {
            "heading": "2.2 Analysis with Label Shuffling",
            "text": "Analyzing the primacy effect requires us to distill the effects of label orders in the prompts. However, this is non-trivial because there are many factors influencing ChatGPT\u2019s decisions, such as the input text and label definitions. In our work, to distinguish the primacy effect of ChatGPT from other factors, we conduct random shuffling for labels listed in the prompts. Specifically, before every prediction, we shuffle the labels as visualized in Fig. 3. Label shuffling erases the discriminative semantics of the specific label orders in the prompts.\nIdeally, a fair model should return the same prediction when labels are shuffled, and consequently, the predicted label index should follow a uniform distribution under random shuffling.\nNext, we introduce our two ways of using random label shuffling to analyze ChatGPT.\nPrediction Comparison on an Instance A reliable and consistent classifier is expected to consistently choose the same label for the same instance irrespective of the label order. To evaluate such consistency of ChatGPT, we perform the random shuffling for the same instance twice to produce two prompts. We feed these two prompts to ChatGPT and compare the corresponding two predictions with each other. We apply the above process to all the test instances and compute the fraction of the instances where the prediction changed after label shuffling. The higher the fraction is, the more sensitive ChatGPT is to the label order.\nStatistics of Predicted Indices Taking a further step, we perform statistical analysis on the predicted indices for instances where the prediction changed after label shuffling. If ChatGPT does not have any preference on the label orders, its predicted label indices should be uniformly distributed. By comparing the predicted label index distribution of ChatGPT to the uniform distribution, we can assess its fairness and preferences regarding label orders."
        },
        {
            "heading": "3 Experiments",
            "text": "We analyze the primacy effect based on the aforementioned strategies using three relation extraction datasets and an intent detection dataset."
        },
        {
            "heading": "3.1 Experiment Setup",
            "text": "We mainly chose relation extraction and intent detection tasks in our experiments since these tasks naturally come with adequately sized decision spaces to illustrate the underlying primacy effect of labels. For relation extraction, we experiment on three benchmark datasets including TACRED (Zhang et al., 2017), TACREV (Alt et al., 2020), and Re-TACRED (Stoica et al., 2021). For intent detection, we conducted experiments on Banking77 (Casanueva et al., 2020a) and MASSIVE (FitzGerald et al., 2022). MASSIVE (FitzGerald et al., 2022) is a parallel dataset of massive utterances with annotations for the Natural Language Understanding tasks of intent prediction. Utterances span 60 intents.\nWe additionally conducted experiments on the NLP datasets: GoEmotions (Demszky et al., 2020) and 20 Newsgroups (Albishre et al., 2015) for a more comprehensive evaluation. GoEmotions (Demszky et al., 2020) is a dataset for fine-grained emotion classification. It is a corpus of 58k carefully curated comments extracted from Reddit, with human annotations for 27 emotion categories and a neutral one. The 20 Newsgroups (Albishre et al., 2015) dataset is a collection of approximately 20,000 newsgroup documents, partitioned across 20 different newsgroups.\nWe follow the existing work (Wei et al., 2023; Li et al., 2023) to apply ChatGPT to these tasks via the OpenAI API gpt-3.5-turbo. Specifically, we set the temperature as 0.0 to minimize the randomness of ChatGPT\u2019s outputs. For comparison, we adopt the existing work (Casanueva et al., 2020b; Zhou and Chen, 2022) to fine-tune the BERT model with an MLP classification head."
        },
        {
            "heading": "3.2 Consistency under Label Shuffling",
            "text": "First, we observe the low consistency of ChatGPT confronted under label shuffling. As shown in Table 1, ChatGPT changes its label prediction after label shuffling in over 85% of the test instances on the TACRED, TACREV, and Re-TACRED datasets, and in 35.7% of instances on Banking77. Also, ChatGPT changes its label prediction after label shuffling in over 69% of the test instances on the datasets of GoEmotions and in more than 30% of instances on MASSIVE and 20 Newsgroups. In contrast, the fine-tuned BERT classifier maintains consistent predictions after label shuffling. This discrepancy challenges the widely-held belief that ChatGPT can comprehend human instructions and provide consistent responses. One possible explanation is that ChatGPT\u2019s understanding of the prompt is obtained by training on human-labeled data, which inherits humans\u2019 cognitive bias of treating labels at different positions unfairly.\nIt is worth noting that the ratio of instances with changed predictions is consistently high across the relation extraction datasets but lower on intent detection. This discrepancy can be attributed to the fact that information extraction tasks are shown to be challenging for ChatGPT and other LLMs (Wang et al., 2023; Li et al., 2023). In more difficult tasks, ChatGPT lacks sufficient discriminative semantic understanding from the input text and may be more affected by the label order."
        },
        {
            "heading": "3.3 Primacy Effect of ChatGPT",
            "text": "The empirical results in Section 3.2 indicate that ChatGPT\u2019s predictions are affected by label order. To deeper delve into the effects of label orders on ChatGPT, we analyze the distribution of predicted label indices (e.g., if the prediction is the first label, the label index is 1), as introduced in Section 2.2. We visualize the distributions in Fig. 4. Notably,\nthe distribution of ChatGPT\u2019s predictions consistently deviates from the uniform distribution, displaying a consistent bias towards smaller indices across different datasets. In contrast, BERT exhibits no preference for label orders and consistently demonstrates a uniform distribution in its predicted label indices.\nWe term this tendency of ChatGPT as the primacy effect, where the model tends to favor the labels presented earlier in the prompt. The magnitude of these primacy effects varies across tasks, as illustrated in Fig. 4. Notably, the influence of primacy effects is higher in more challenging tasks. This observation aligns with the results discussed in Sec. 3.3, wherein the impact of primacy effects is greater when ChatGPT tackles more difficult tasks. In the next section, we will quantitatively analyze the primacy effects of ChatGPT."
        },
        {
            "heading": "3.4 Evaluation on Fairness",
            "text": "The fairness of a trained model can be assessed by examining the imbalance or skewness in its predictions (Sweeney and Najafian, 2019). Following prior studies (Xiang et al., 2020; Sweeney and Najafian, 2019; Qian et al., 2021; Wang et al., 2022), we employ the JS divergence (Fuglede and\nTopsoe, 2004) as the metric to evaluate how imbalanced/skewed/unfair a prediction P is. The measurement is symmetric (i.e., JS(P\u2225U) = JS(U\u2225P )) and strictly scoped.\nTo evaluate the label order bias of ChatGPT, we compute the average relative label order imbalance (LOI): LOI is defined as the JS divergence between the predicted label index distribution P and the uniform distribution U :\nLOI = JS(P (x|x \u2208 D), U), (1)\nwhere x represents an input instance, D is the test set, P (x) is the predicted label index, and U is the uniform distribution. LOI captures the disparity between the predicted indices and a uniform distribution.\nWe conduct the fairness evaluation following the experimental settings described in Section 3.3, and the results are presented in Table 2. The findings demonstrate that ChatGPT exhibits unfair treatment of label indices when making relation label predictions for input texts. Furthermore, the degree of unfairness increases with the task\u2019s difficulty, which aligns with the empirical results discussed in Sections 3.2 and 3.3. In contrast, BERT demonstrates significantly better fairness, as its predictions are not influenced by label orders.\nWe additionally test the performance of ChatGPT with CoT (Chain-of-thoughts) (Wei et al., 2022). With CoT, ChatGPT still exhibits the primacy effect. The above results show that with or without CoT, ChatGPT consistently exhibits the primacy effect. A reason for this phenomenon could be that the CoT encourages the LLMs for \u201cslow thinking\u201d about the question but does not neces-\nsarily mitigate the cognitive bias in the reasoning steps of CoT."
        },
        {
            "heading": "4 Related Work",
            "text": "Large Language Models (LLMs) (Brown et al., 2020; Rae et al., 2021; Thoppilan et al., 2022; Smith et al., 2022), such as GPT-3 (Brown et al., 2020), LaMDA (Thoppilan et al., 2022) and PaLM (Chowdhery et al., 2022), refer to large scale pretrained models that contain more than a hundred billion parameters. Based on the highly parallelizable Transformer architecture (Vaswani et al., 2017), these Large Language models have shown powerful capability to produce reasonable results with very few samples or task descriptions as input.\nA key milestone in the development process of LLMs is ChatGPT, which is developed by OpenAI based on InstructGPT(Ouyang et al., 2022). ChatGPT is able to interact with humans through multiple turns of dialogue, understand user intent, accomplish instructions, and return human-like responses. This attracts huge attention from research field, motivating numerous recent work (Zhang et al., 2022; Ma et al., 2023; Wan et al., 2023; Zhong et al., 2023; Susnjak, 2023) to utilize ChatGPT to different tasks.\nAs ChatGPT is a proprietary model, and OpenAI does not disclose its training specifics, researchers are actively investigating its associated implications and capabilities. There has been some work analyzing the performance, robustness, faithfulness, and explain-ability of ChatGPT (Gao et al., 2023; Han et al., 2023; Li et al., 2023). For example, (Malinka et al., 2023) investigates the educational integrity of ChatGPT and evaluates the ChatGPT\u2019s abilities to solve assignments of various levels in computer security specialization. (Haque et al., 2022) and (Kr\u00fcgel et al., 2023) investigate the ethical risks of ChatGPT.\nBefore ChatGPT, LLMs\u2019 inference has been accompanied by in-context learning (ICL) which adds a few demonstrations in the prompt (Dong et al., 2022; Fei et al., 2023). Accordingly, some work investigates the effects of demonstration orders for the LLMs before ChatGPT (Lu et al., 2021). (Zhao et al., 2021) finds the majority label, recency, and common token biases of LLMs\u2019 ICL.\nDifferent from the above work, we focus on a new phenomenon of ChatGPT: the primacy effect, which is the tendency of selecting the first labels as\nthe answer. The primary effect seriously influences ChatGPT\u2019s fairness. Collectively, our findings provide a new understanding of how ChatGPT works given the instructional prompts."
        },
        {
            "heading": "5 Conclusion",
            "text": "While previous work often takes ChatGPT as a universal method applicable to all text-related tasks, we argue that its flexibility comes with the risk of inheriting human\u2019s cognitive biases. These biases lead to unfair judgments which can affect the performance of the machine learning model. This work studies a cognitive bias of ChatGPT: primacy effects. We propose a simple yet effective label shuffling method to analyze the influence of label orders on ChatGPT. We discover the primacy effect of ChatGPT and finds that it highly influences the fairness of ChatGPT in NLU. Our work contributes to a better understanding of the behaviors of ChatGPT and building more faithful solutions with ChatGPT in NLU applications.\nLimitation\nOur work has a few potential limitations. Firstly, we primarily evaluate the primacy effect of ChatGPT, which is one of the most widely-used instruction-legacy models for each task. It would be beneficial to assess this effect on other LLMs models (such as Google Bard, vicuna (Chiang et al., 2023)) and explore additional tasks to examine this primacy effect. Secondly, this work focused on analyzing the primacy effect of ChatGPT through experiments. We encourage further studies to propose effective solutions that can mitigate the negative impacts associated with the primacy effect."
        },
        {
            "heading": "Acknowledgement",
            "text": "The authors would like to thank the anonymous reviewers for their discussion and feedback. Yiwei Wang and Bryan Hooi are supported by NUS ODPRT Grant A-0008067-00-00, NUS ODPRT Grant R252-000-A81-133, and Singapore Ministry of Education Academic Research Fund Tier 3 under MOEs official grant number MOE2017T3-1-007. Muhao Chen is supported by the NSF Grant IIS 2105329, the NSF Grant ITE 2333736, the DARPA MCS program under Contract No. N660011924033 with the United States Office Of Naval Research, a Cisco Research Award, two Amazon Research Awards, and a Keston Research Award."
        }
    ],
    "title": "Primacy Effect of ChatGPT",
    "year": 2023
}