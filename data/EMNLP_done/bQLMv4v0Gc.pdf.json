{
    "abstractText": "Metaphor identification aims at understanding whether a given expression is used figuratively in context. However, in this paper we show how existing metaphor identification datasets can be gamed by fully ignoring the potential metaphorical expression or the context in which it occurs. We test this hypothesis in a variety of datasets and settings, and show that metaphor identification systems based on language models without complete information can be competitive with those using the full context. This is due to the construction procedures to build such datasets, which introduce unwanted biases for positive and negative classes. Finally, we test the same hypothesis on datasets that are carefully sampled from natural corpora and where this bias is not present, making these datasets more challenging and reliable.",
    "authors": [
        {
            "affiliations": [],
            "name": "Joanne Boisson"
        },
        {
            "affiliations": [],
            "name": "Jose Camacho-Collados"
        }
    ],
    "id": "SP:d49dd9811833746535d133f6f1967a79979b46c0",
    "references": [
        {
            "authors": [
                "Israa Alghanmi",
                "Luis Espinosa Anke",
                "Steven Schockaert."
            ],
            "title": "Probing pre-trained language models for disease knowledge",
            "venue": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 3023\u20133033, Online. Association for",
            "year": 2021
        },
        {
            "authors": [
                "Dan Assaf",
                "Yair Neuman",
                "Yohai Cohen",
                "Shlomo Argamon",
                "Newton Howard",
                "Mark Last",
                "Ophir Frieder",
                "Moshe Koppel."
            ],
            "title": "Why \"dark thoughts\" aren\u2019t really dark: A novel algorithm for metaphor identification",
            "venue": "2013 IEEE Symposium on Computational",
            "year": 2013
        },
        {
            "authors": [
                "Julia Birke",
                "Anoop Sarkar."
            ],
            "title": "A clustering approach for nearly unsupervised recognition of nonliteral language",
            "venue": "11th Conference of the European Chapter of the Association for Computational Linguistics.",
            "year": 2006
        },
        {
            "authors": [
                "Samuel R. Bowman",
                "Gabor Angeli",
                "Christopher Potts",
                "Christopher D. Manning."
            ],
            "title": "A large annotated corpus for learning natural language inference",
            "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages",
            "year": 2015
        },
        {
            "authors": [
                "Eileen R. Cardillo",
                "Christine Watson",
                "Anjan Chatterjee."
            ],
            "title": "Stimulus needs are a moving target: 240 additional matched literal and metaphorical sentences for testing neural hypotheses about metaphor",
            "venue": "Behavior Research Methods, 49(2):471\u2013483.",
            "year": 2017
        },
        {
            "authors": [
                "E.R. Cardillo",
                "G.L. Schmidt",
                "A. Kranjec",
                "A. Chatterjee"
            ],
            "title": "Stimulus design is an obstacle course: 560 matched literal and metaphorical sentences for testing neural hypotheses about metaphor",
            "year": 2010
        },
        {
            "authors": [
                "Tuhin Chakrabarty",
                "Xurui Zhang",
                "Smaranda Muresan",
                "Nanyun Peng."
            ],
            "title": "MERMAID: Metaphor generation with symbolism and discriminative decoding",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computa-",
            "year": 2021
        },
        {
            "authors": [
                "Paul Cook",
                "Afsaneh Fazly",
                "Suzanne Stevenson."
            ],
            "title": "The vnc-tokens dataset",
            "venue": "Proceedings of the LREC Workshop on Towards a Shared Task for Multiword Expressions (MWE 2008).",
            "year": 2008
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
            "year": 2019
        },
        {
            "authors": [
                "Erik-L\u00e2n Do Dinh",
                "Hannah Wieland",
                "Iryna Gurevych."
            ],
            "title": "Weeding out conventionalized metaphors: A corpus of novel metaphor annotations",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages",
            "year": 2018
        },
        {
            "authors": [
                "Aleksandr Drozd",
                "Anna Gladkova",
                "Satoshi Matsuoka."
            ],
            "title": "Word embeddings, analogies, and machine learning: Beyond king - man + woman = queen",
            "venue": "Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics:",
            "year": 2016
        },
        {
            "authors": [
                "Jonathan Dunn."
            ],
            "title": "Measuring metaphoricity",
            "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers).",
            "year": 2014
        },
        {
            "authors": [
                "Stefan Falkner",
                "Aaron Klein",
                "Frank Hutter"
            ],
            "title": "Bohb: Robust and efficient hyperparameter optimization at scale",
            "year": 2018
        },
        {
            "authors": [
                "Christiane Fellbaum."
            ],
            "title": "WordNet: An Electronic Lexical Database",
            "venue": "Bradford Books.",
            "year": 1998
        },
        {
            "authors": [
                "Ge Gao",
                "Eunsol Choi",
                "Yejin Choi",
                "Luke Zettlemoyer."
            ],
            "title": "Neural metaphor detection in context",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 607\u2013613, Brussels, Belgium. Association for Com-",
            "year": 2018
        },
        {
            "authors": [
                "Suchin Gururangan",
                "Swabha Swayamdipta",
                "Omer Levy",
                "Roy Schwartz",
                "Samuel Bowman",
                "Noah A. Smith."
            ],
            "title": "Annotation artifacts in natural language inference data",
            "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for",
            "year": 2018
        },
        {
            "authors": [
                "E. Dario Guti\u00e9rrez",
                "Ekaterina Shutova",
                "Tyler Marghetis",
                "Benjamin Bergen."
            ],
            "title": "Literal and metaphorical senses in compositional distributional semantic models",
            "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics",
            "year": 2016
        },
        {
            "authors": [
                "Hessel Haagsma",
                "Johan Bos",
                "Malvina Nissim."
            ],
            "title": "MAGPIE: A large corpus of potentially idiomatic expressions",
            "venue": "Proceedings of the 12th Language Resources and Evaluation Conference, pages 279\u2013287, Marseille, France. European Language Resources",
            "year": 2020
        },
        {
            "authors": [
                "Katarzyna Jankowiak."
            ],
            "title": "Normative data for novel nominal metaphors, novel similes, literal, and anomalous utterances in polish and english",
            "venue": "Journal of Psycholinguistic Research, 49(4):541\u2013569.",
            "year": 2020
        },
        {
            "authors": [
                "Albert Katz",
                "Allan Paivio",
                "Marc Marschark",
                "Jim Clark."
            ],
            "title": "Norms for 204 literary and 260 nonliterary metaphors on 10 psychological dimensions",
            "venue": "Metaphor and Symbol - METAPHOR SYMB, 3:191\u2013 214.",
            "year": 1988
        },
        {
            "authors": [
                "Ioannis Korkontzelos",
                "Torsten Zesch",
                "Fabio Massimo Zanzotto",
                "Chris Biemann."
            ],
            "title": "SemEval-2013 task 5: Evaluating phrasal semantics",
            "venue": "Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Proceedings of the Seventh Inter-",
            "year": 2013
        },
        {
            "authors": [
                "Chee Wee (Ben) Leong",
                "Beata Beigman Klebanov",
                "Chris Hamill",
                "Egon Stemle",
                "Rutuja Ubale",
                "Xianyang Chen"
            ],
            "title": "A report on the 2020 VUA and TOEFL metaphor detection shared task",
            "venue": "In Proceedings of the Second Workshop on Figurative Language",
            "year": 2020
        },
        {
            "authors": [
                "Omer Levy",
                "Steffen Remus",
                "Chris Biemann",
                "Ido Dagan"
            ],
            "title": "Do supervised distributional methods really learn lexical inference relations",
            "venue": "In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Lin-",
            "year": 2015
        },
        {
            "authors": [
                "Tal Linzen."
            ],
            "title": "Issues in evaluating semantic spaces using word analogies",
            "venue": "Proceedings of the 1st Workshop on Evaluating Vector-Space Representations for NLP, pages 13\u201318, Berlin, Germany. Association for Computational Linguistics.",
            "year": 2016
        },
        {
            "authors": [
                "Saif Mohammad",
                "Ekaterina Shutova",
                "Peter Turney."
            ],
            "title": "Metaphor as a medium for emotion: An empirical study",
            "venue": "Proceedings of the Fifth Joint Conference on Lexical and Computational Semantics, pages 23\u201333, Berlin, Germany. Association for",
            "year": 2016
        },
        {
            "authors": [
                "Michael Mohler",
                "Mary Brunson",
                "Bryan Rink",
                "Marc Tomlinson."
            ],
            "title": "Introducing the LCC metaphor datasets",
            "venue": "Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC\u201916), pages 4221\u20134227, Portoro\u017e, Slovenia.",
            "year": 2016
        },
        {
            "authors": [
                "Malvina Nissim",
                "Rik van Noord",
                "Rob van der Goot."
            ],
            "title": "Fair is better than sensational: Man is to doctor as woman is to doctor",
            "venue": "Computational Linguistics, 46(2):487\u2013497.",
            "year": 2020
        },
        {
            "authors": [
                "Natalie Parde",
                "Rodney Nielsen."
            ],
            "title": "A corpus of metaphor novelty scores for syntactically-related word pairs",
            "venue": "Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC-2018). European Language Resource",
            "year": 2018
        },
        {
            "authors": [
                "Natalie Parde",
                "Rodney Nielsen."
            ],
            "title": "Exploring the terrain of metaphor novelty: A regression-based approach for automatically scoring metaphors",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, 32(1).",
            "year": 2018
        },
        {
            "authors": [
                "Adam Poliak",
                "Jason Naradowsky",
                "Aparajita Haldar",
                "Rachel Rudinger",
                "Benjamin Van Durme."
            ],
            "title": "Hypothesis only baselines in natural language inference",
            "venue": "Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics, pages",
            "year": 2018
        },
        {
            "authors": [
                "Marek Rei",
                "Luana Bulat",
                "Douwe Kiela",
                "Ekaterina Shutova."
            ],
            "title": "Grasping the finer point: A supervised similarity network for metaphor detection",
            "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages",
            "year": 2017
        },
        {
            "authors": [
                "Ekaterina Shutova",
                "Douwe Kiela",
                "Jean Maillard."
            ],
            "title": "Black holes and white rabbits: Metaphor identification with visual features",
            "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Hu-",
            "year": 2016
        },
        {
            "authors": [
                "Vered Shwartz",
                "Yoav Goldberg",
                "Ido Dagan."
            ],
            "title": "Improving hypernymy detection with an integrated path-based and distributional method",
            "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),",
            "year": 2016
        },
        {
            "authors": [
                "Caroline Sporleder",
                "Linlin Li",
                "Philip Gorinski",
                "Xaver Koch."
            ],
            "title": "Idioms in context: The IDIX corpus",
            "venue": "Proceedings of the Seventh International Conference on Language Resources and Evaluation (LREC\u201910), Valletta, Malta. European Language Re-",
            "year": 2010
        },
        {
            "authors": [
                "Gerard Steen."
            ],
            "title": "A method for linguistic metaphor identification: from MIP to MIPVU, volume v",
            "venue": "14 of Converging evidence in language and communication research. John Benjamins Pub. Co., Amsterdam.",
            "year": 2010
        },
        {
            "authors": [
                "Harish Tayyar Madabushi",
                "Edward Gow-Smith",
                "Marcos Garcia",
                "Carolina Scarton",
                "Marco Idiart",
                "Aline Villavicencio."
            ],
            "title": "SemEval-2022 Task 2: Multilingual Idiomaticity Detection and Sentence Embedding",
            "venue": "Proceedings of the 16th International Workshop on",
            "year": 2022
        },
        {
            "authors": [
                "Yulia Tsvetkov",
                "Leonid Boytsov",
                "Anatole Gershman",
                "Eric Nyberg",
                "Chris Dyer."
            ],
            "title": "Metaphor detection with cross-lingual model transfer",
            "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Pa-",
            "year": 2014
        },
        {
            "authors": [
                "Yuancheng Tu",
                "Dan Roth."
            ],
            "title": "Sorting out the most confusing english phrasal verbs",
            "venue": "Proceedings of the First Joint Conference on Lexical and Computational Semantics.",
            "year": 2012
        },
        {
            "authors": [
                "Peter Turney",
                "Yair Neuman",
                "Dan Assaf",
                "Yohai Cohen."
            ],
            "title": "Literal and metaphorical sense identification through concrete and abstract context",
            "venue": "Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 680\u2013",
            "year": 2011
        },
        {
            "authors": [
                "Shun Wang",
                "Yucheng Li",
                "Chenghua Lin",
                "Loic Barrault",
                "Frank Guerin."
            ],
            "title": "Metaphor detection with effective context denoising",
            "venue": "Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pages 1404\u2013",
            "year": 2023
        },
        {
            "authors": [
                "Chuhan Wu",
                "Fangzhao Wu",
                "Yubo Chen",
                "Sixing Wu",
                "Zhigang Yuan",
                "Yongfeng Huang."
            ],
            "title": "Neural metaphor detecting with CNN-LSTM model",
            "venue": "Proceedings of the Workshop on Figurative Language Processing, pages 110\u2013114, New Orleans, Louisiana.",
            "year": 2018
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "The automatic identification of metaphors in corpora is an active area of research in NLP (Tsvetkov et al., 2014; Shutova et al., 2016; Rei et al., 2017; Wu et al., 2018; Gao et al., 2018; Wang et al., 2023), and consequently, previous works have proposed the construction, curation and testing of metaphor indentification datasets in various forms (Birke and Sarkar, 2006; Tsvetkov et al., 2014), also stemming from previous psycholinguistic research (Cardillo et al., 2017; Jankowiak, 2020). A common feature of metaphor identification tasks is that they are usually framed as binary classification, where a potentially metaphorical expression (PME) and a context are provided as input, and a system has to determine whether the given PME is used metaphorically or not. For instance, dark is used metaphorically in the sentence \u2018The latest developments move us closer to a dark age\u2019, but not in \u2018I like dark colors\u2019. While this setting is attractive for testing supervised systems, it also simplifies the task, introducing a real risk of not testing metaphoricity,\nbut instead spurious correlations that might lead to an artificially correct solutions.\nAnalyses of this sort have often proven critical in understanding the relationship between a proposed task and whether performance on associated datasets can be directly linked to performance on the task itself. Notable examples include the work of Levy et al. (2015), who showed that, in the task of lexical relation modeling, the unreasonably high performance of supervised systems could be attributed to lexical memorization, that is, the presence of large number of prototypical cases in the dataset (e.g. animal for the hypernymy relation) that made it simple for models to \u201cdetect the relation\u201d without having to consider both words in the pair. This issue has been identified also in word analogies (Linzen, 2016; Drozd et al., 2016; Nissim et al., 2020) and, beyond lexical semantics, in natural language inference (NLI). In NLI, given a hypothesis and a premise, a system must determine whether the premise entails, contradicts or is neutral with respect to the hypothesis. Previous works have shown that supervised models could rely on superficial factors, e.g., in the SNLI dataset (Bowman et al., 2015), hypothesis-only models are surprisingly competitive (Poliak et al., 2018; Gururangan et al., 2018), a trend also observed in medical NLI datasets (Alghanmi et al., 2021).\nIn this paper, we report experimental results which suggest that (1) language models can identify metaphorical expressions with great accuracy without even seeing the given expression; and that (2) a model only seeing the metaphorical expression with no context performs very competitively, in both cases close to the model with complete information. A crucial distinction in this phenomenon, however, is that this is only observed in datasets not sampled from a natural distribution, and that whenever such distribution is preserved, these shortcuts are not as effective. We thus propose a simple sampling procedure from a naturally-distributed corpus\nthat mitigates this issue."
        },
        {
            "heading": "2 Metaphor Identification Datasets",
            "text": "This section presents the list of existing Englishlanguage metaphor identification datasets used in our experiments. All the datasets are summarized in Table 1."
        },
        {
            "heading": "2.1 Psycholinguistic datasets",
            "text": "Datasets created for Psycholinguistics and Cognitive Science served experiments on the perception of metaphors. They are composed of sentences written or rephrased for the purpose of controlling characteristics such as length and word frequency. This line of study for the English language started with Katz et al. (1988)1, and was pursued by Cardillo et al. (2010) and Cardillo et al. (2017), who released a corpus of verbal and nominal PMEs (CARD_N and CARD_V). Jankowiak (2020) extended this work for sentences of the form A is-a B."
        },
        {
            "heading": "2.2 NLP datasets",
            "text": "Sentence context. The TroFi dataset published by Birke and Sarkar (2006) has often been used to evaluate metaphor identification NLP systems, Turney et al. (2011) being a notable example. The collection started with 50 predefined labeled target verbs. Then, sentences containing one of those verbs were extracted from the Wall Street Journal (WSJ) corpus, and subsequently labeled. Tsvetkov et al. (2014) used TroFi to train a classifier for verbal metaphor identification that integrates features from WordNet (Fellbaum, 1998). As part of their research, they released a balanced test set of 222 sentences with verbal PME, and a new corpus of adjective-noun pairs (TSV_V and TSV_AN). Only clear examples with strong inter-annotator agreements were included in the test set. Mohammad et al. (2016) studied the correlation between metaphors and emotions. This was achieved by building a new corpus of verbal metaphors from WordNet glosses of polysemous predicates, enriched with synset annotations and emotionality (MOH). Mohler et al. (2016) released the large LLC corpus of metaphors covering several part-ofspeech and longer metaphoric expressions, scored on metaphoricity and emotions scales, and enriched\n1This dataset is not included in our experiments because it only contains metaphoric instances.\nwith source domain information. The dataset construction relies partly on automatic metaphor extraction tools, with a manual validation of the output, resulting inevitably in a bias of the considered instances. Dunn (2014) created a small test set of 60 instances to evaluate a metaphoricity score model (DUNN), with the same verb appearing with different levels of metaphoricity in three sentences sets. Similarly, Chakrabarty et al. (2021) released a small corpus of verbal PME to evaluate the MERMAID model (CHAK).\nAdjective-nouns. Assaf et al. (2013) developed sets of labeled adjective-noun pairs of concreteabstract associations (NEU) and Guti\u00e9rrez et al. (2016) released a much larger dataset of frequent adjective noun pairs for a study on the compositional properties of metaphors (GUT)."
        },
        {
            "heading": "2.3 Potential Idiomatic Expressions (PIEs)",
            "text": "Idioms, such as to rock the boat, are multiword metaphoric expressions that became lexicalized in\na language. Similarly to the corpus construction methodology used for TroFi, idiomatic expressions datasets are usually built from an initial list of PIEs, which are then used to extract sentences from corpora, mostly from the British National Corpus. Several datasets focusing on different types of PIE have been released: Cook et al. (2008) for verb noun constructs (VNC), Sporleder et al. (2010) for various multiword expressions (IDIX), Tu and Roth (2012) on prepositional verb constructs (PVC), Korkontzelos et al. (2013) in two different tracks (SE2013), and Tayyar Madabushi et al. (2022) for nominal compounds (MAD). Haagsma et al. (2020) released the large MAGPIE dataset containing PIEs of various syntactic constructs, following an initial release of a smaller PIE corpus."
        },
        {
            "heading": "2.4 Sampled from annotated corpora: The case of the Amsterdam corpus",
            "text": "The VU Amsterdam Corpus (Steen, 2010, VUAC), is a collection of documents from the British National Corpus (BNC-baby), labeled following the metaphor identification MIPVU protocol. Each word of the documents was considered by the annotators and marked when metaphoric. The dataset covers over 190,000 lexical units. Because very conventional metaphors were labeled as figurative, several versions of the corpus have later been released containing metaphoricity or novelty scores. Other modifications have been made to frame the corpus into NLP system inputs for figurative language identification. For example, Do Dinh et al. (2018) enriched the data with novelty scores (DO) and Leong et al. (2020) released two versions of VUAC for a shared task of metaphor identification, with different sets of Part-of-Speech (PoS) tags considered (ST1 for verbs, ST2 for all PoS tags). The datasets are designed for classification of single tokens in the context of a sentence. Parde and Nielsen (2018a; 2018b) also worked on metaphor novelty in VUAC, releasing a version of potential metaphoric source-target pairs among the syntactic dependencies of a metaphoric word.\nOur sampling method. Framing the VUAC for binary classification requires to sample literal instances of PME from the corpus. Any token that is not labeled as metaphoric in the VUAC can be considered literal. To avoid PME sampling biases, we sample literal instances from the set of expressions that also occur as metaphors, whenever possible. When the same word sequence is not found, we\nrely on identical lemma sequences, and finally on identical PoS sequences (we referred to this VUAC sampling as VUAC_BO)."
        },
        {
            "heading": "3 Evaluation",
            "text": "The evaluation is aimed at understanding to what extent metaphor identification datasets are affected by construction or sampling biases."
        },
        {
            "heading": "3.1 Experimental setting",
            "text": "Data. For the initial experiments (Section 3.2), we rely on the original data splits of existing metaphor identification datasets (see Table 1).2 In Section 3.3 we also provide an extended analysis to assess the impact of baselines in different data splits with 5-folds cross-validation.\nModel and training. As our model for all the experiments, we rely on BERT-base (Devlin et al., 2019). Note that the goal of the experiments is not to provide the best possible model, but rather to show how a supervised model with incomplete information can attain a performance similar to the model with all the information. We use the HuggingFace transformers library and models, adding a classification layer on top of the pre-trained BERT model. For hyperparameter optimisation, we rely on the Bayesian Optimization with Hyperband (BOHB) algorithm (Falkner et al., 2018) with 50 trials, available in RayTune (Liaw et al., 2018). The hyperparameters search space is set to a batch size equal to 4, 8 or 16; the learning rate in a ranging from 5e-7 to 5e-5; the number of epochs within 1 to 12; and the random seeds taking three possible values (1, 2 and 3).\nBaselines. In order to test our hypothesis, we test two baselines with incomplete information that effectively hide the context or the information about the metaphorical expression being tested. We will use the following sentence as our running example, with dark being the PME of the sentence: The latest developments move us closer to a dark age. We test the following three configurations according to the input shown to the model:\n1. Default: The latest developments move us closer to a <PME>dark</PME> age.3\n2. Baseline 1 \u2013 Only PME: dark. 2In Appendix A, we include more details on how individual datasets were sampled and preprocessed. 3<PME> is a special token to indicate the PME position.\nDef PME Masked\nNLP Trofi 75.78 56.67 (-25.2%) 74.42 (-1.8%) TSV_AN 87.36 52.49 (-39.9%) 80.88 (-7.4%) TSV_SVO 92.33 50.27 (-45.5%) 90.98 (-1.5%)\nPIE\nSE2013_ALL 86.46 49.39 (-42.9%) 74.60 (-13.7%) SE2013_LEX 92.92 63.46 (-31.7%) 89.21 (-4.0%) MAD_FEWSHOT 94.72 89.14 (-5.9%) 71.72 (-24.3%) MAD_ONESHOT 88.21 86.24 (-2.2%) 65.65 (-25.6%) MAD_ZEROSHOT 81.54 77.56 (-4.9%) 64.44 (-21.0%) PIE 88.81 91.14 (+2.6%) 81.0 (-8.8%) MAGPIE_L 94.47 90.87 (-3.8%) 81.25 (-14.0%) MAGPIE_R 88.99 79.23 (-11.0%) 80.25 (-9.8%)\nVUAC VUAC_ST1 82.52 69.82 (-15.4%) 70.64 (-14.4%) VUAC_ST2 82.22 73.10 (-11.1%) 66.48 (-19.1%)\nEvaluation metrics. Macro-F1 is chosen as our objective evaluation metric on the validation set during hyper-parameter optimization and is the main metric used in our experiment. Our choice of Macro-F1 was because it gives a synthetic view of the performances of the models for both classes, and datasets with very different label distributions. Accuracy, precision, recall and F1 results for the metaphor class can be found in Appendix B."
        },
        {
            "heading": "3.2 Results",
            "text": "The main results are displayed in Table 2. In general, gaps in performance should only be compared within datasets, and are not comparable across datasets because of different sampling techniques. For instance, the balance between metaphorical expressions may be vastly different in different datasets, and the performance gap may also reflect this.\nAs can be observed, the results of models with incomplete information are extremely competitive, even in the case when only a PME is included. The results also show how different datasets suffer from diverse types of bias. For instance, the only PME baseline appears to be insufficient in NLP datasets, with the masked baseline being close to the default setting. In contrast, in datasets such as MAD, PIE or MAGPIE, the only PME baseline is more competitive (with relative drops often lower\nthan 5%), even surpassing the default upperbound in PIE."
        },
        {
            "heading": "3.3 Analysis: Random and Lexical Splits",
            "text": "Setting. Similarly to Shwartz et al. (2016), in this experiment we consider datasets with random and lexical splits. A random split is simply a random allocation of instances for the training and test sets. In a lexical split, however, we ensure that a target word (the PME in our case) in the test set is not included in the training set. For a better generalisation, the experiments for this analysis were repeated on five different splits using 5-fold crossvalidation.4 For each fold, 70% of the instances in the training set, 10% in the validation set and 20% in the test set.\nResults. Table 3 shows the results for the random and lexical splits. The trends observed in these splits are similar to the original splits. This now includes the datasets coming from the psycholinguistic literature, with minimal performance drops of the baseline masking the PME (lower than 5% in all cases). When the natural distribution is not used, the gap between the baselines and the model seeing all the information is in some cases small, with the performance of these baselines being nontrivial. This is not the case in all the datasets, such as MOH, which differs in its construction method from most of the other datasets.\nRandom vs lexical splits. There are cases in which the random split may mislead the model, especially when the number of examples is limited. For example, one dataset may contain only literal examples in the training set for a PME, and the model may wrongly conclude that all instances in the test set of that PME are literal, when this may not be the case. This would not happen in the lexical split. For instance, the DUNN dataset was created with three instances per PME, one literal and two metaphorical. Similarly, for CARD there are two instances per PME, one literal and one metaphorical.\nThe case of VUAC. VUAC has the particularity of following a natural distribution, and has been sampled differently by different researchers to frame it into a binary classification task. Despite its simplicity, our VUAC sampling approach\n4Given their large size (i.e., more than 10,000 test instances), the MAGPIE and VUAC-derived datasets experiments are done on a single train/valid/test split.\n(i.e., VUAC_BO) appears to be the generally robust, with over 10 points of difference between the results of the default configuration and the best baseline, both for the random and lexical splits."
        },
        {
            "heading": "4 Conclusion",
            "text": "Getting inspiration from previous studies analysing artifacts and biases from NLP datasets, we delved into the field of metaphor identification. By proposing baselines with incomplete information that hide the PME or the context, we show how the performance achieved by a supervised model is closed to the model that uses complete information. This highlights the type of bias that a model is picking up at training time, which differ from what a well-trained model would be expected to learn. Finally, we show that this problem is generally observed in datasets that are artificially constructed, as carefully sampling from datasets (VUAC_BO) stemming from a fully-annotated corpus alleviates this issue.\nLimitations\nSince this is a preliminary study on the study of biases in metaphor identification datasets, this comes with its own limitations that could be addressed in future work. For instance, we evaluate the models in a set of pre-defined splits that may include their own biases. We attempt at mitigating this by also including cross-validation settings on lexical and random splits, but this may not provide the full picture. For example, we do not study the effect of context duplication in this work (more details in the last point of Appendix A). In terms of the models evaluated, given computational limitations, we focus on a single language model. Our goal is not to achieve the best possible results, but it is likely that some of the conclusions may slightly differ for different supervised models, including other language models.\nA human evaluation in the three settings would be interesting to better understand the inherent biases of the models due to the probability of each PIE to be used metaphorically or the probability of\na context to be paired with a metaphor from dataset artifacts. This could help provide a more complete picture of the reasons behind our findings.\nEthics statement\nOur analysis sheds light on the possible deficiencies of supervised settings and the spurious correlations that supervised models can learn from if the datasets are not carefully designed. As such, researchers should be careful in the claims we can make about such models, in particularly in relation to metaphor identification as we show in this paper. In fact, that there may be other artifacts and biases not considered in this paper."
        },
        {
            "heading": "Acknowledgments",
            "text": "Jose Camacho-Collados is supported by a UKRI Future Leaders Fellowship. We thank all the authors of the datasets used in this paper for kindly sharing them with us, including those that were not currently available online."
        },
        {
            "heading": "A Dataset Preprocessing and Sampling",
            "text": "In the following we list a few individual dataset preprocessing and sampling details not included in the main paper.\n1. A few instances of the TSV test set were found in the training set, we deduplicate them in the random and lexical splits of the dataset.\n2. The original 200 instances of the TSV_AN test set are provided within a full sentence context, which we ignore in our experiments because the training set only contains adjectivenoun pairs and finetuned language models generally perform better when the test set is similar to the training set.\n3. In the lexical splits columns of Table 3, TSV_AN and TSV_AN_L2 are two lexical splits of the TSV_AN instances respectively on the adjective and the noun. The same reading applies to the GUT dataset. The results for a single random split are shown duplicated in the table, for the two rows.\n4. Some sentences of the WSJ appear several times in the TroFi dataset original version, we also deduplicate them in the random and lexical spits of the dataset.\n5. The MAGPIE has an original lexical and random split, for which the results are presented in Table 1 (MAGPIE_L and MAGPIE_R). Different random and lexical splits appear in Table 3.\n6. In Table 1, the statistics of the original MAD_FEWSHOT dataset are shown. Two other versions with less instances have been shared in Tayyar Madabushi et al. (2022): MAD_ONESHOT and MAD_ZEROSHOT (equivalent to a lexical split). The results for the three original splits appear in Table 2. Our lexical and random splits of the MAD dataset are made from the instances of the few-shot version, that contains the largest number of instances among the three.\n7. The PVC dataset contains too few distinct V-PREP instance for a relevant lexical split based on the two words, the split is made on the verb for each PME.\n8. A few instances of the IDIX and VNC datasets contain minimal meaningful contextual information in the only-PME setting because they are multiword expressions of non consecutive words. For example, the idiom to rock the boat appears in the sentence as rock the political boat. Informative context inserted between the terms of a PIE occurs very rarely in those two datasets. Therefore, while they could have an effect in the final results, we decided not to not modify them and simply extracted or tagged the expressions by selecting the string starting and finishing with the PIE.\n9. Context duplication: The CHAK dataset has the specificity of being made of triples, where nearly identical sentences appear three times. The sentences of a triple have the same context and three different substituted verbal PMEs, among which exactly one is literal and two are figurative. The MOH dataset, constructed from WordNet glosses, also contains several instances with identical context, and alternated verbal PMEs that are near synonyms of each other. Finally, multiple instances of the VUAC datasets may be constructed from the same sentence during the sampling procedure, because several words of each sentence may have been labeled as metaphoric during the original annotation process."
        },
        {
            "heading": "B Additional evaluation metrics",
            "text": "Tables 4 and 5 provide additional metrics for the random and lexical split analyses, respectively (see Section 3.3). In particular, the tables include accuracy and precision, recall and F1 on the metaphor class. Moreover, for completeness we have included the accuracy results of a naive baseline that outputs the metaphor for all instances."
        }
    ],
    "title": "Construction Artifacts in Metaphor Identification Datasets",
    "year": 2023
}