{
    "abstractText": "In this paper, we investigate the impact of objects on gender bias in image captioning systems. Our results show that only genderspecific objects have a strong gender bias (e.g. women-lipstick). In addition, we propose a visual semantic-based gender score that measures the degree of bias and can be used as a plug-in for any image captioning system. Our experiments demonstrate the utility of the gender score, since we observe that our score can measure the bias relation between a caption and its related gender; therefore, our score can be used as an additional metric to the existing Object Gender Co-Occ approach. Code and data are publicly available at https://github.com/ ahmedssabir/GenderScore.",
    "authors": [
        {
            "affiliations": [],
            "name": "Ahmed Sabir"
        },
        {
            "affiliations": [],
            "name": "Llu\u00eds Padr\u00f3"
        }
    ],
    "id": "SP:2f8e8011f2529b4c5bc2a110d6a4cbd3d1ec4d3b",
    "references": [
        {
            "authors": [
                "Peter Anderson",
                "Xiaodong He",
                "Chris Buehler",
                "Damien Teney",
                "Mark Johnson",
                "Stephen Gould",
                "Lei Zhang."
            ],
            "title": "Bottom-up and top-down attention for image captioning and visual question answering",
            "venue": "CVPR.",
            "year": 2018
        },
        {
            "authors": [
                "Sergey Blok",
                "Douglas Medin",
                "Daniel Osherson."
            ],
            "title": "Probability from similarity",
            "venue": "AAAI.",
            "year": 2003
        },
        {
            "authors": [
                "Piotr Bojanowski",
                "Edouard Grave",
                "Armand Joulin",
                "Tomas Mikolov."
            ],
            "title": "Enriching word vectors with subword information",
            "venue": "TACL.",
            "year": 2017
        },
        {
            "authors": [
                "Jaemin Cho",
                "Seunghyun Yoon",
                "Ajinkya Kale",
                "Franck Dernoncourt",
                "Trung Bui",
                "Mohit Bansal."
            ],
            "title": "Fine-grained image captioning with clip reward",
            "venue": "arXiv preprint arXiv:2205.13115.",
            "year": 2022
        },
        {
            "authors": [
                "Alexis Conneau",
                "Douwe Kiela",
                "Holger Schwenk",
                "Loic Barrault",
                "Antoine Bordes."
            ],
            "title": "Supervised learning of universal sentence representations from natural language inference data",
            "venue": "arXiv preprint arXiv:1705.02364.",
            "year": 2017
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "NAACL-HLT.",
            "year": 2019
        },
        {
            "authors": [
                "Tianyu Gao",
                "Xingcheng Yao",
                "Danqi Chen."
            ],
            "title": "SimCSE: Simg of sentence embeddings",
            "venue": "arXiv preprint arXiv:2104.08821.",
            "year": 2021
        },
        {
            "authors": [
                "Monica Gonzalez-Marquez."
            ],
            "title": "Methods in cognitive linguistics, volume 18",
            "venue": "John Benjamins Publishing.",
            "year": 2007
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun."
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "CVPR.",
            "year": 2016
        },
        {
            "authors": [
                "Lisa Anne Hendricks",
                "Kaylee Burns",
                "Kate Saenko",
                "Trevor Darrell",
                "Anna Rohrbach."
            ],
            "title": "Women also snowboard: Overcoming bias in captioning models",
            "venue": "ECCV.",
            "year": 2018
        },
        {
            "authors": [
                "Yusuke Hirota",
                "Yuta Nakashima",
                "Noa Garcia."
            ],
            "title": "Quantifying societal bias amplification in image captioning",
            "venue": "CVPR.",
            "year": 2022
        },
        {
            "authors": [
                "Jonathan Huang",
                "Vivek Rathod",
                "Chen Sun",
                "Menglong Zhu",
                "Anoop Korattikara",
                "Alireza Fathi",
                "Ian Fischer",
                "Zbigniew Wojna",
                "Yang Song",
                "Sergio Guadarrama"
            ],
            "title": "Speed/accuracy trade-offs for modern convolutional object detectors",
            "year": 2017
        },
        {
            "authors": [
                "Lun Huang",
                "Wenmin Wang",
                "Jie Chen",
                "Xiao-Yong Wei."
            ],
            "title": "Attention on attention for image captioning",
            "venue": "CVPR.",
            "year": 2019
        },
        {
            "authors": [
                "Andrej Karpathy",
                "Li Fei-Fei."
            ],
            "title": "Deep visualsemantic alignments for generating image descriptions",
            "venue": "CVPR.",
            "year": 2015
        },
        {
            "authors": [
                "Junnan Li",
                "Dongxu Li",
                "Silvio Savarese",
                "Steven Hoi."
            ],
            "title": "Blip-2: Bootstrapping language-image pretraining with frozen image encoders and large language models",
            "venue": "arXiv preprint arXiv:2301.12597.",
            "year": 2023
        },
        {
            "authors": [
                "Junnan Li",
                "Dongxu Li",
                "Caiming Xiong",
                "Steven Hoi."
            ],
            "title": "Blip: Bootstrapping language-image pretraining for unified vision-language understanding and generation",
            "venue": "arXiv preprint arXiv:2201.12086.",
            "year": 2022
        },
        {
            "authors": [
                "Xiujun Li",
                "Xi Yin",
                "Chunyuan Li",
                "Pengchuan Zhang",
                "Xiaowei Hu",
                "Lei Zhang",
                "Lijuan Wang",
                "Houdong Hu",
                "Li Dong",
                "Furu Wei"
            ],
            "title": "Oscar: Objectsemantics aligned pre-training for vision-language",
            "year": 2020
        },
        {
            "authors": [
                "Tsung-Yi Lin",
                "Michael Maire",
                "Serge Belongie",
                "James Hays",
                "Pietro Perona",
                "Deva Ramanan",
                "Piotr Doll\u00e1r",
                "C Lawrence Zitnick."
            ],
            "title": "Microsoft coco: Common objects in context",
            "venue": "ECCV.",
            "year": 2014
        },
        {
            "authors": [
                "Jiasen Lu",
                "Vedanuj Goswami",
                "Marcus Rohrbach",
                "Devi Parikh",
                "Stefan Lee."
            ],
            "title": "12-in-1: Multi-task vision and language representation learning",
            "venue": "CVPR.",
            "year": 2020
        },
        {
            "authors": [
                "Tomas Mikolov",
                "Kai Chen",
                "Greg Corrado",
                "Jeffrey Dean."
            ],
            "title": "Efficient estimation of word representations in vector space",
            "venue": "arXiv preprint arXiv:1301.3781.",
            "year": 2013
        },
        {
            "authors": [
                "Jeffrey Pennington",
                "Richard Socher",
                "Christopher D Manning."
            ],
            "title": "Glove: Global vectors for word representation",
            "venue": "EMNLP.",
            "year": 2014
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark"
            ],
            "title": "Learning transferable visual models from natural language supervision",
            "year": 2021
        },
        {
            "authors": [
                "Alec Radford",
                "Jeffrey Wu",
                "Rewon Child",
                "David Luan",
                "Dario Amodei",
                "Ilya Sutskever."
            ],
            "title": "Language models are unsupervised multitask learners",
            "venue": "OpenAI blog.",
            "year": 2019
        },
        {
            "authors": [
                "Nils Reimers",
                "Iryna Gurevych."
            ],
            "title": "Sentence-bert: Sentence embeddings using siamese bert-networks",
            "venue": "arXiv preprint arXiv:1908.10084.",
            "year": 2019
        },
        {
            "authors": [
                "Ahmed Sabir",
                "Francesc Moreno-Noguer",
                "Pranava Madhyastha",
                "Llu\u00eds Padr\u00f3."
            ],
            "title": "Belief revision based caption re-ranker with visual semantic information",
            "venue": "arXiv preprint arXiv:2209.08163.",
            "year": 2022
        },
        {
            "authors": [
                "Ahmed Sabir",
                "Francesc Moreno-Noguer",
                "Llu\u00eds Padr\u00f3."
            ],
            "title": "Visual semantic relatedness dataset for image captioning",
            "venue": "CVPRW.",
            "year": 2023
        },
        {
            "authors": [
                "Ruixiang Tang",
                "Mengnan Du",
                "Yuening Li",
                "Zirui Liu",
                "Na Zou",
                "Xia Hu."
            ],
            "title": "Mitigating gender bias in captioning systems",
            "venue": "Proceedings of the Web Conference 2021.",
            "year": 2021
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin."
            ],
            "title": "Attention is all you need",
            "venue": "NeurIPS.",
            "year": 2017
        },
        {
            "authors": [
                "Tianlu Wang",
                "Jieyu Zhao",
                "Mark Yatskar",
                "Kai-Wei Chang",
                "Vicente Ordonez."
            ],
            "title": "Balanced datasets are not enough: Estimating and mitigating gender bias in deep image representations",
            "venue": "CVPR.",
            "year": 2019
        },
        {
            "authors": [
                "Xing Wu",
                "Chaochen Gao",
                "Zijia Lin",
                "Jizhong Han",
                "Zhongyuan Wang",
                "Songlin Hu."
            ],
            "title": "Infocse: Information-aggregated contrastive learning of sentence embeddings",
            "venue": "arXiv preprint arXiv:2210.06432.",
            "year": 2022
        },
        {
            "authors": [
                "Peter Young",
                "Alice Lai",
                "Micah Hodosh",
                "Julia Hockenmaier."
            ],
            "title": "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions",
            "venue": "TACL.",
            "year": 2014
        },
        {
            "authors": [
                "Pengchuan Zhang",
                "Xiujun Li",
                "Xiaowei Hu",
                "Jianwei Yang",
                "Lei Zhang",
                "Lijuan Wang",
                "Yejin Choi",
                "Jianfeng Gao."
            ],
            "title": "Vinvl: Making visual representations matter in vision-language models",
            "venue": "CVPR.",
            "year": 2021
        },
        {
            "authors": [
                "Dora Zhao",
                "Angelina Wang",
                "Olga Russakovsky."
            ],
            "title": "Understanding and evaluating racial biases in image captioning",
            "venue": "CVPR.",
            "year": 2021
        },
        {
            "authors": [
                "Jieyu Zhao",
                "Tianlu Wang",
                "Mark Yatskar",
                "Vicente Ordonez",
                "Kai-Wei Chang."
            ],
            "title": "Men also like shopping: Reducing gender bias amplification using corpus-level constraints",
            "venue": "arXiv preprint arXiv:1707.09457.",
            "year": 2017
        },
        {
            "authors": [
                "Jieyu Zhao",
                "Yichao Zhou",
                "Zeyu Li",
                "Wei Wang",
                "Kai-Wei Chang."
            ],
            "title": "Learning gender-neutral word embeddings",
            "venue": "arXiv preprint arXiv:1809.01496.",
            "year": 2018
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Visual understanding of image captioning is an important and rapidly evolving research topic (Karpathy and Fei-Fei, 2015; Anderson et al., 2018). Recent approaches predominantly rely on Transformer (Huang et al., 2019; Cho et al., 2022) and the BERT based pre-trained paradigm (Devlin et al., 2019) to learn cross-modal representation (Li et al., 2020; Zhang et al., 2021; Li et al., 2022, 2023).\nWhile image captioning models achieved notable benchmark performance in utilizing the correlation between visual and co-occurring labels to generate an accurate image description, this often results in a gender bias that relates to a specific gender, such as confidently identifying a woman when there is a kitchen in the image. The work of (Zhao et al., 2017) tackles the problem of gender bias in visual semantic role labeling by balancing the distribution. For image captioning, Hendricks et al. (2018) consider ameliorating gender bias via a balance classifier. More recently, Hirota et al. (2022) measured racial and gender bias amplification in image captioning via a trainable classifier on an additional human-annotated existing caption dataset for the gender bias task (Zhao et al., 2021).\nTo close the full picture of gender bias in image captioning, in this work, unlike other works, we examine the problem from a visual semantic relation perspective. We therefore propose a Gender Score via human-inspired judgment named Belief Revision (Blok et al., 2003) which can be used to (1) discover bias and (2) predict gender bias without training or unbalancing the dataset. We conclude our contributions are as follows: (1) we investigate the gender object relation for image captioning at the word level (i.e. object-gender) and the sentence level with captions (i.e. gender-caption); (2) we propose a Gender Score that uses gender in relation to the visual information in the image to predict the gender bias. Our Gender Score can be used as a plug-in for any out-of-the-box image captioning system. Figure 1 shows an overview of the proposed gender bias measure for image captioning."
        },
        {
            "heading": "2 Visual Context Information",
            "text": "In this work, we investigate the relation between gender bias and the objects that are mainly used in image captioning systems, and more precisely, the widely used manually annotated image caption datasets: Flickr30K (Young et al., 2014) and COCO dataset (Lin et al., 2014). However, we focus mainly on the COCO dataset as the most used dataset in gender bias evaluation. COCO Captions is an unbalanced gender bias dataset with a 1:3 ratio bias towards men (Zhao et al., 2017; Hendricks et al., 2018; Tang et al., 2021). The dataset contains around 120K images, and each image is annotated with five different human-written captions.\nTo obtain the visual context o from each image I , we use out-of-the-box classifiers to extract the image context information o(I). Specifically, following (Sabir et al., 2023), the objects extracted from all pre-trained models are obtained by extracting the top-3 object class/category (excluding person category) from each classifier after filtering out instances with (1) the cosine distance between\nobjects (voting between classifiers of top-k) and (2) a low confidence score via a probability threshold < 0.2. We next describe each of these classifiers:\nResNet-152 (He et al., 2016): A residual deep network that is designed for ImageNet classification tasks, which relies heavily on batch normalization. CLIP (Radford et al., 2021): (Contrastive Language-Image Pre-training) is a pre-trained model with contrastive loss where the pair of imagetext needs to be distinguished from randomly selected sample pairs. CLIP uses Internet resources without human annotation on 400M pairs. Inception-ResNet FRCNN (Huang et al., 2017): is an improved variant of Faster R-CNN, with a trade-off of better accuracy and fast inference via high-level features extracted from InceptionResNet. It is a pre-trained model that is trained on COCO categories, with 80 object categories."
        },
        {
            "heading": "3 Gender Object Distance",
            "text": "The main component of our Gender Score (next section) is the semantic relation between the object and the gender. Therefore, in this section, we investigate the semantic correlation between the object and the gender in the general dataset (e.g. wiki). More specifically, we assume that all images contain a gender man, woman or gender neutral person, and we aim to measure the Cosine Distance between the object o, objects with context\ni.e. caption y from the image I and its related gender a\u2032 \u2208 {man, woman, person}. In this context, we refer to the Cosine Distance as the closest distance (i.e. semantic relatedness score) between the gender-to-object via similarity measure e.g. cosine similarity. We apply our analysis to the most widely used human-annotated datasets in image captioning tasks: Flikr30K and COCO Captions datasets.\nWe employ several commonly used pre-trained models: (1) word-level: GloVe (Pennington et al., 2014), fasttext (Bojanowski et al., 2017) and word2vec (Mikolov et al., 2013) out-of-the-box as baselines, then we utilize Gender Neutral GloVe (Zhao et al., 2018), which balances the gender bias; (2) sentence-level: Sentence-BERT (Reimers and Gurevych, 2019) tuned on Natural Language Inference (NLI) (Conneau et al., 2017), (2b) SimCSE (Gao et al., 2021) contrastive learning supervised by NLI, and an improved SimCSE version with additional Information aggregation via masked language model objective InfoCSE (Wu et al., 2022). In particular, we measure the Cosine Distance from the gender a\u2032 to the object o or caption y as shown in Figure 1 (a, b) (e.g. how close the gender vector of a\u2032 is to the object o umbrella). Table 1 shows the top most object-gender frequent count with the standard and gender-balanced GloVe, respectively. Gender Bias Ratio: we follow the work of (Zhao et al., 2017) to calculate the gender bias ratio towards men as:\nbiasto-m = s(obj, m)\ns(obj, m) + s(obj, w) (1)\nwhere m and w refer to the gender in the image, and the s is our proposed gender-to-object semantic relation bias score. In our case, we also use the score to compute the ratio to gender neutral person:\nRatioto-n = s(obj, m/w) s(obj, person)\n(2)\ndatasets. The ratio is the gender bias rate towards men/women. The results show there is a slight bias toward men. GloVe and GN-GloVe (balanced) show identical results on COCO Captions, which indicate that not all objects have a strong bias toward a specific gender. In particular, regarding non-biased objects, both models exhibit a low/similar bias ratio e.g. bicycle-gender (GloVe: m=0.31 | w=0.27, ratio=0.53) and (GN-GloVe: m=0.15 | w=0.13, ratio=0.53).\nKarpathy test split. Our score balances the bias better than direct Cosine Distance, primarily because not all objects exhibit strong gender bias. The leakage is a comparison between human-annotated caption and the classifier output that uses the gender-related object to influence the final gender prediction, such as associating women with food."
        },
        {
            "heading": "4 Gender Score",
            "text": "In this section, we describe the proposed Gender Score that estimates gender based on its semantic relation with the visual information extracted from the image. Sabir et al. (2022) proposed a caption re-ranking method that leverages visual semantic context. This approach utilized Belief Revision (Blok et al., 2003) to convert the similarity (i.e. Cosine Distance) into a probability measure. Belief Revision. To obtain likelihood bias revisions based on similarity scores (i.e. gender, object), we need three parameters: (1) Hypothesis (g): caption y with the associated gender a \u2208 {man, woman}, (2) Informativeness (c): image object information o confidence and (3) Similarities: the degree of relatedness between object and gender sim(y, o).\nGSa(y) = 1 |D| \u2211\n(y,o)\u2208D\nP (gy | co) = P(gy)\u03b1 (3)\nwhere P(gy) is the hypothesis probability of y, D is the predicted captions with the gender a, and P(co) is the probability of the evidence that causes hypothesis probability revision i.e. visual bias revision via object context o from the image I , o(I): Hypothesis: P(gy) (caption y with the gender a) Informativeness: 1\u2212 P(co) (object context o)\nSimilarities: \u03b1 = [ 1\u2212sim(y,o) 1+sim(y,o) ]1\u2212P(co) (visual bias)\nThe visual context P(co) will revise the caption with the associated gender P(gy) (i.e. gender bias) if there is a semantic relation between them sim(y, o). We discuss each component next: Hypothesis initial bias: In visual-based belief revision, one of the conditions is to start with an initial hypothesis and then revise it using visual context and a similarity score. Therefore, we initialize the caption hypothesis P(gy) with a common observation P(g\u2032y), such as a Language Model (LM) (we\nconsider this as an initial bias without visual). We employ (GPT-2) (Radford et al., 2019) with mean token probability since it achieves better results. Informativeness of bias information: As the visual context probability P(co) approaches 1 and in consequence is less informative (very frequent objects have no discriminative power since they may co-occur with any gender) 1 \u2212 P(co) approaches zero, causing \u03b1 to get closer to 1, and thus, a smaller revision of P(g\u2032y). Therefore, as we described in the Visual Context Information Section 2, we leverage a threshold and semantic filter visual context dataset from ResNet, Inception-ResNet v2 based Faster R-CNN object detector, and CLIP to extract top-k textual visual context information from the image. The extracted object is used to measure the gender-to-object bias direct relation. Relatedness between hypothesis and bias information: Likelihood revision occurs if there is a close correlation between the hypothesis and the new information. As the sim(y, o) (gender, object), gets closer to 1 (higher relatedness) \u03b1 gets closer to 0, and thus hypothesis probability is revised (i.e. gender bias) and raised closer to 1. Therefore, the initial hypothesis will be revised or backed off to 1 (no bias) based on the relatedness score. In our case, we employ Sentence-BERT to compute the Cosine Distance (Section 3) by using object o as context for the caption y with associated gender a."
        },
        {
            "heading": "5 Experiments",
            "text": "Caption model. We examine seven of the most recent Transformer state-of-the-art caption models:\nTransformer (Vaswani et al., 2017) with bottom-up top-down features (Anderson et al., 2018), AoANet (Huang et al., 2019), Vilbert (Lu et al., 2020), OSCAR (Li et al., 2020), BLIP (Li et al., 2022), Transformer with Reinforcement Learning (RL) CLIPS+CIDEr as image+text similarity Reward (TraCLIPS-Reward) (Cho et al., 2022) and Large LM2.7B based BLIP-2 (Li et al., 2023). Note that for a fair comparison with other pre-trained models, OSCAR uses a cross-entropy evaluation score rather than the RL-based CIDEr optimization score.\nData. Our gender bias score is performed on the (82783 \u00d7 5 human annotations) COCO Captions dataset. For baselines (testing), the score is used to evaluate gender-to-object bias on the standard 5K Karpathy test split images (Karpathy and Fei-Fei, 2015) (GT is the average of five human bias ratios).\nOur experiments apply visual semantics between the object or object with context i.e. caption and its related gender information to predict object-togender related bias. The proposed visual-gender scores are (1): Cosine Distance, which uses the similarity score to try to estimate the proper gender as shown in Table 2; (2) Gender Score, which carries out Belief Revision (visual bias likelihood revision) to revise the hypothesis initial bias via similarity e.g. Cosine Distance (gender, object). For our baseline, we adopt the Object Gender Co-Occ metric (Zhao et al., 2017) for the image captioning task.\nIn this work, we hypothesize that every image has a gender \u2208 {man, woman} or gender neutral (i.e. person), and our rationale is that each model suffers from Right for the Wrong Reasons (Hen-\ndricks et al., 2018) or leakage (Wang et al., 2019) (see Table 3), such as associating all kitchens with women. Therefore, we want to explore all the cases and let the proposed distance/score decide which gender (i.e. bias) is in the image based on a visual bias. In particular, inspired by the cloze probability last word completion task (Gonzalez-Marquez, 2007), we generate two identical sentences but with a different gender, and then we compute the likelihood revisions between the sentence-gender and the caption using the object probability. Table 2 shows that GloVe and GN-GloVe (balanced) have identical results on COCO Captions dataset, which indicate that not all objects have a strong bias toward a specific gender. In addition, Table 3 shows that our score balances the bias of the Cosine Distance and demonstrates, on average, that not all objects have a strong gender bias. Also, our approach detects strong specific object-to-gender bias and has a similar result to the existing Object Gender Co-Occ method on the most biased object toward men, as shown in Table 4. TraCLIPS-Reward inherits biases from RL-CLIPS and thus generates caption w/o a specific gender (e.g. person, guy, etc). Therefore, we adopt the combined CLIPS+CIDEr Rewards, which suffer less gender prediction error.\nGender Score Estimation. In this experiment, we < MASK > the gender and use the object with context to predict the gender as shown in Figure 2. The idea\nscore measures gender bias more accurately, particularly when there is a strong object to gender bias relation.\nis to measure the amplified gender-to-object bias in pre-trained models. Table 5 shows that the fill-in gender score has a more bias towards men results than object-gender pair counting. The rationale is that the model can estimate the strong gender object bias as shown in Figure 2, including the false positive and leakage cases by the classifier. Discussion. Our approach measures the amplified gender bias more accurately than the Object Gender Co-Occ (Zhao et al., 2017) in the following two scenarios: (1) where the gender is not obvious in the image and is misclassified by the caption baseline, and (2) when there is leakage by the classifier. In addition, unlike the Object Gender Co-Occ, our model balances the gender to object bias and only measures a strong object to gender bias relation as shown in Figure 2. For instance, the word \u201chitting\u201d in a generated caption (as a stand-alone without context) is associated with a bias toward men more than women, and it will influence the final genderto-caption bias score. However, our gender score balances the unwanted bias and only measures the pronounced gender to object bias relation."
        },
        {
            "heading": "6 Conclusion",
            "text": "In this work, we investigate the relation between objects and gender bias in image captioning systems. Our results show that not all objects exhibit a strong gender bias and only in special cases does an object have a strong gender bias. We also propose a Gender Score that can be used as an additional metric to the existing Object-Gender Co-Occ method.\nLimitations\nThe accuracy of our Gender Score heavily relies on the semantic relation (i.e. Cosine Distance) between gender and object, a high or low degree of similarity score between them can influence the final bias score negatively. Also, our model relies on a large pre-trained model(s), which inherently encapsulate their own latent biases that might impact the visual bias revision behavior in genderto-object bias scenarios. Specifically, in cases when there are multiple strong bias contexts (i.e. non-objects context) with high similarity scores toward a particular gender present within the caption. This can imbalance the final gender-to-object score, leading to errors in gender prediction and bias estimation. In addition, the false positive object context information extracted by the visual classifier will result in inaccurate bias estimation.\nEthical Considerations\nWe rely upon an existing range of well-known publicly available caption datasets crawled from the web and annotated by humans that assume a binary conceptualization of gender. Therefore, it is important to acknowledge that within the scope of this work, we are treating gender as strictly binary (i.e. man and woman) oversimplifies a complex and multifaceted aspect of human identity. Gender is better understood as a spectrum, with many variations beyond just two categories, and should be addressed in future work. Since all models are being trained on these datasets, we anticipate all models contain other biases (racial, cultural, etc.). For example, an observation in Table 1, when we remove gender bias, we notice the emergence of another bias, such as racial bias. For example, vectors representing Black person or women are closer together than those representing other colors, like white. Moreover, there is another form of bias that has received limited attention in the literature, the propagation of gender and racial bias via RL (e.g. RL-CLIPS). For instance, in Figure 1 the model associates gender with race, as \u201casian woman\u201d."
        }
    ],
    "title": "Women Wearing Lipstick: Measuring the Bias Between an Object and Its Related Gender",
    "year": 2023
}