{
    "abstractText": "Large language models (LLMs) have shown promise for generative and knowledgeintensive tasks including question-answering (QA) tasks. However, the practical deployment still faces challenges, notably the issue of \"hallucination\", where models generate plausible-sounding but unfaithful or nonsensical information. This issue becomes particularly critical in the medical domain due to the uncommon professional concepts and potential social risks involved. This paper analyses the phenomenon of hallucination in medical generative QA systems using widely adopted LLMs and datasets. Our investigation centers on the identification and comprehension of common problematic answers, with a specific emphasis on hallucination. To tackle this challenge, we present an interactive self-reflection methodology that incorporates knowledge acquisition and answer generation. Through this feedback process, our approach steadily enhances the factuality, consistency, and entailment of the generated answers. Consequently, we harness the interactivity and multitasking ability of LLMs and produce progressively more precise and accurate answers. Experimental results on both automatic and human evaluation demonstrate the superiority of our approach in hallucination reduction compared to baselines.1",
    "authors": [
        {
            "affiliations": [],
            "name": "Ziwei Ji"
        },
        {
            "affiliations": [],
            "name": "Tiezheng Yu"
        },
        {
            "affiliations": [],
            "name": "Yan Xu"
        },
        {
            "affiliations": [],
            "name": "Nayeon Lee"
        },
        {
            "affiliations": [],
            "name": "Etsuko Ishii"
        },
        {
            "affiliations": [],
            "name": "Pascale Fung"
        }
    ],
    "id": "SP:0711406113ef7c5ce36e894d43756a674fd2e587",
    "references": [
        {
            "authors": [
                "Asma Ben Abacha",
                "Chaitanya Shivade",
                "Dina Demner-Fushman."
            ],
            "title": "Overview of the mediqa 2019 shared task on textual inference, question entailment and question answering",
            "venue": "Proceedings of the 18th BioNLP Workshop and Shared Task, pages",
            "year": 2019
        },
        {
            "authors": [
                "Emily Alsentzer",
                "John Murphy",
                "William Boag",
                "WeiHung Weng",
                "Di Jindi",
                "Tristan Naumann",
                "Matthew McDermott."
            ],
            "title": "Publicly available clinical bert embeddings",
            "venue": "Proceedings of the 2nd Clinical Natural Language Processing Workshop, pages",
            "year": 2019
        },
        {
            "authors": [
                "Yejin Bang",
                "Samuel Cahyawijaya",
                "Nayeon Lee",
                "Wenliang Dai",
                "Dan Su",
                "Bryan Wilie",
                "Holy Lovenia",
                "Ziwei Ji",
                "Tiezheng Yu",
                "Willy Chung"
            ],
            "title": "A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity",
            "year": 2023
        },
        {
            "authors": [
                "Asma Ben Abacha",
                "Eugene Agichtein",
                "Yuval Pinter",
                "Dina Demner-Fushman."
            ],
            "title": "Overview of the medical question answering task at trec 2017 liveqa",
            "venue": "TREC 2017.",
            "year": 2017
        },
        {
            "authors": [
                "Asma Ben Abacha",
                "Dina Demner-Fushman."
            ],
            "title": "A question-entailment approach to question answering",
            "venue": "BMC Bioinform., 20(1):511:1\u2013511:23.",
            "year": 2019
        },
        {
            "authors": [
                "Asma Ben Abacha",
                "Chaitanya Shivade",
                "Dina Demner-Fushman."
            ],
            "title": "Overview of the mediqa 2019 shared task on textual inference, question entailment and question answering",
            "venue": "ACL-BioNLP 2019.",
            "year": 2019
        },
        {
            "authors": [
                "Jonathan Berant",
                "Vivek Srikumar",
                "Pei-Chun Chen",
                "Abby Vander Linden",
                "Brittany Harding",
                "Brad Huang",
                "Peter Clark",
                "Christopher D Manning."
            ],
            "title": "Modeling biological processes for reading comprehension",
            "venue": "Proceedings of the 2014 conference on empirical",
            "year": 2014
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners. Advances in neural information processing",
            "year": 2020
        },
        {
            "authors": [
                "Collin Burns",
                "Haotian Ye",
                "Dan Klein",
                "Jacob Steinhardt."
            ],
            "title": "Discovering latent knowledge in language models without supervision",
            "venue": "The Eleventh International Conference on Learning Representations.",
            "year": 2022
        },
        {
            "authors": [
                "Wei-Lin Chiang",
                "Zhuohan Li",
                "Zi Lin",
                "Ying Sheng",
                "Zhanghao Wu",
                "Hao Zhang",
                "Lianmin Zheng",
                "Siyuan Zhuang",
                "Yonghao Zhuang",
                "Joseph E. Gonzalez",
                "Ion Stoica",
                "Eric P. Xing"
            ],
            "title": "Vicuna: An opensource chatbot impressing gpt-4 with 90%* chatgpt",
            "year": 2023
        },
        {
            "authors": [
                "Bhuwan Dhingra",
                "Danish Danish",
                "Dheeraj Rajagopal."
            ],
            "title": "Simple and effective semi-supervised question answering",
            "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Lan-",
            "year": 2018
        },
        {
            "authors": [
                "Shizhe Diao",
                "Rui Pan",
                "Hanze Dong",
                "KaShun Shum",
                "Jipeng Zhang",
                "Wei Xiong",
                "Tong Zhang"
            ],
            "title": "Lmflow: An extensible toolkit for finetuning and inference of large foundation models",
            "year": 2023
        },
        {
            "authors": [
                "Jinlan Fu",
                "See-Kiong Ng",
                "Zhengbao Jiang",
                "Pengfei Liu."
            ],
            "title": "Gptscore: Evaluate as you desire",
            "venue": "arXiv preprint arXiv:2302.04166.",
            "year": 2023
        },
        {
            "authors": [
                "Kelvin Guu",
                "Kenton Lee",
                "Zora Tung",
                "Panupong Pasupat",
                "Mingwei Chang."
            ],
            "title": "Retrieval augmented language model pre-training",
            "venue": "International conference on machine learning, pages 3929\u20133938. PMLR.",
            "year": 2020
        },
        {
            "authors": [
                "Tianyu Han",
                "Lisa C Adams",
                "Jens-Michalis Papaioannou",
                "Paul Grundmann",
                "Tom Oberhauser",
                "Alexander L\u00f6ser",
                "Daniel Truhn",
                "Keno K Bressem."
            ],
            "title": "Medalpaca\u2013an open-source collection of medical conversational ai models and training data",
            "venue": "arXiv",
            "year": 2023
        },
        {
            "authors": [
                "Gautier Izacard",
                "Patrick Lewis",
                "Maria Lomeli",
                "Lucas Hosseini",
                "Fabio Petroni",
                "Timo Schick",
                "Jane Dwivedi-Yu",
                "Armand Joulin",
                "Sebastian Riedel",
                "Edouard Grave."
            ],
            "title": "Few-shot learning with retrieval augmented language models",
            "venue": "arXiv preprint",
            "year": 2022
        },
        {
            "authors": [
                "Ziwei Ji",
                "Nayeon Lee",
                "Rita Frieske",
                "Tiezheng Yu",
                "Dan Su",
                "Yan Xu",
                "Etsuko Ishii",
                "Ye Jin Bang",
                "Andrea Madotto",
                "Pascale Fung."
            ],
            "title": "Survey of hallucination in natural language generation",
            "venue": "ACM Computing Surveys, 55(12):1\u201338.",
            "year": 2023
        },
        {
            "authors": [
                "Qiao Jin",
                "Bhuwan Dhingra",
                "Zhengping Liu",
                "William Cohen",
                "Xinghua Lu."
            ],
            "title": "PubMedQA: A dataset for biomedical research question answering",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the",
            "year": 2019
        },
        {
            "authors": [
                "Saurav Kadavath",
                "Tom Conerly",
                "Amanda Askell",
                "Tom Henighan",
                "Dawn Drain",
                "Ethan Perez",
                "Nicholas Schiefer",
                "Zac Hatfield-Dodds",
                "Nova DasSarma",
                "Eli Tran-Johnson"
            ],
            "title": "Language models (mostly) know what they know",
            "year": 2022
        },
        {
            "authors": [
                "Zakaria Kaddari",
                "Youssef Mellah",
                "Jamal Berrich",
                "Toumi Bouchentouf",
                "Mohammed G. Belkasmi."
            ],
            "title": "Biomedical question answering: A survey of methods and datasets",
            "venue": "2020 Fourth International Conference On Intelligent Computing in Data Sciences",
            "year": 2020
        },
        {
            "authors": [
                "Pei Ke",
                "Hao Zhou",
                "Yankai Lin",
                "Peng Li",
                "Jie Zhou",
                "Xiaoyan Zhu",
                "Minlie Huang."
            ],
            "title": "Ctrleval: An unsupervised reference-free metric for evaluating controlled text generation",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Compu-",
            "year": 2022
        },
        {
            "authors": [
                "Klaus Krippendorff"
            ],
            "title": "Computing krippendorff\u2019s alpha-reliability",
            "year": 2011
        },
        {
            "authors": [
                "Hwanhee Lee",
                "Seunghyun Yoon",
                "Franck Dernoncourt",
                "Doo Soon Kim",
                "Trung Bui",
                "Joongbo Shin",
                "Kyomin Jung."
            ],
            "title": "KPQA: A metric for generative question answering using keyphrase weights",
            "venue": "Proceedings of the 2021 Conference of the North Amer-",
            "year": 2021
        },
        {
            "authors": [
                "Patrick Lewis",
                "Ethan Perez",
                "Aleksandra Piktus",
                "Fabio Petroni",
                "Vladimir Karpukhin",
                "Naman Goyal",
                "Heinrich K\u00fcttler",
                "Mike Lewis",
                "Wen-tau Yih",
                "Tim Rockt\u00e4schel"
            ],
            "title": "Retrieval-augmented generation for knowledge-intensive nlp tasks",
            "year": 2020
        },
        {
            "authors": [
                "Chenliang Li",
                "Bin Bi",
                "Ming Yan",
                "Wei Wang",
                "Songfang Huang."
            ],
            "title": "Addressing semantic drift in generative question answering with auxiliary extraction",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the",
            "year": 2021
        },
        {
            "authors": [
                "Jing Li",
                "Shangping Zhong",
                "Kaizhi Chen."
            ],
            "title": "Mlec-qa: A chinese multi-choice biomedical question answering dataset",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 8862\u20138874.",
            "year": 2021
        },
        {
            "authors": [
                "Chin-Yew Lin."
            ],
            "title": "Rouge: A package for automatic evaluation of summaries",
            "venue": "Text summarization branches out, pages 74\u201381.",
            "year": 2004
        },
        {
            "authors": [
                "Stephanie Lin",
                "Jacob Hilton",
                "Owain Evans."
            ],
            "title": "Truthfulqa: Measuring how models mimic human falsehoods",
            "venue": "arXiv preprint arXiv:2109.07958.",
            "year": 2021
        },
        {
            "authors": [
                "Lang Liu",
                "Junxiang Ren",
                "Yuejiao Wu",
                "Ruilin Song",
                "Zhen Cheng",
                "Sibo Wang."
            ],
            "title": "A pre-trained language model for medical question answering based on domain adaption",
            "venue": "Natural Language Processing and Chinese Computing: 11th CCF International Confer-",
            "year": 2022
        },
        {
            "authors": [
                "Potsawee Manakul",
                "Adian Liusie",
                "Mark JF Gales."
            ],
            "title": "Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models",
            "venue": "arXiv preprint arXiv:2303.08896.",
            "year": 2023
        },
        {
            "authors": [
                "Nick McKenna",
                "Tianyi Li",
                "Liang Cheng",
                "Mohammad Javad Hosseini",
                "Mark Johnson",
                "Mark Steedman."
            ],
            "title": "Sources of hallucination by large language models on inference tasks",
            "venue": "arXiv preprint arXiv:2305.14552.",
            "year": 2023
        },
        {
            "authors": [
                "Sewon Min",
                "Xinxi Lyu",
                "Ari Holtzman",
                "Mikel Artetxe",
                "Mike Lewis",
                "Hannaneh Hajishirzi",
                "Luke Zettlemoyer"
            ],
            "title": "Rethinking the role of demonstrations: What makes in-context learning work? arXiv preprint arXiv:2202.12837",
            "year": 2022
        },
        {
            "authors": [
                "Diego Moll\u00e1",
                "Mar\u00eda Elena Santiago-Mart\u00ednez",
                "Abeed Sarker",
                "C\u00e9cile Paris."
            ],
            "title": "A corpus for research in text processing for evidence based medicine",
            "venue": "Language Resources and Evaluation, 50:705\u2013727.",
            "year": 2016
        },
        {
            "authors": [
                "Timo M\u00f6ller",
                "Anthony Reina",
                "Raghavan Jayakumar",
                "Malte Pietsch."
            ],
            "title": "Covid-qa: A question answering dataset for covid-19",
            "venue": "Proceedings of the 1st Workshop on NLP for COVID-19 at ACL 2020.",
            "year": 2020
        },
        {
            "authors": [
                "Roser Morante",
                "Martin Krallinger",
                "Alfonso Valencia",
                "Walter Daelemans."
            ],
            "title": "Machine reading of biomedical texts about alzheimers disease",
            "venue": "CLEF 2012 Conference and Labs of the Evaluation ForumQuestion Answering For Machine Reading Evalua-",
            "year": 2012
        },
        {
            "authors": [
                "Jessica Morley",
                "Caio CV Machado",
                "Christopher Burr",
                "Josh Cowls",
                "Indra Joshi",
                "Mariarosaria Taddeo",
                "Luciano Floridi."
            ],
            "title": "The ethics of ai in health care: a mapping review",
            "venue": "Social Science & Medicine, 260:113172.",
            "year": 2020
        },
        {
            "authors": [
                "Reiichiro Nakano",
                "Jacob Hilton",
                "Suchir Balaji",
                "Jeff Wu",
                "Long Ouyang",
                "Christina Kim",
                "Christopher Hesse",
                "Shantanu Jain",
                "Vineet Kosaraju",
                "William Saunders"
            ],
            "title": "Webgpt: Browser-assisted questionanswering with human feedback",
            "year": 2021
        },
        {
            "authors": [
                "OpenAI."
            ],
            "title": "Gpt-4 technical report",
            "venue": "ArXiv, abs/2303.08774.",
            "year": 2023
        },
        {
            "authors": [
                "Ankit Pal",
                "Logesh Kumar Umapathi",
                "Malaikannan Sankarasubbu."
            ],
            "title": "Medmcqa: A large-scale multi-subject multi-choice dataset for medical domain question answering",
            "venue": "Conference on Health, Inference, and Learning, pages 248\u2013260. PMLR.",
            "year": 2022
        },
        {
            "authors": [
                "Anusri Pampari",
                "Preethi Raghavan",
                "Jennifer Liang",
                "Jian Peng."
            ],
            "title": "emrqa: A large corpus for question answering on electronic medical records",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2357\u20132368.",
            "year": 2018
        },
        {
            "authors": [
                "Fabio Petroni",
                "Aleksandra Piktus",
                "Angela Fan",
                "Patrick Lewis",
                "Majid Yazdani",
                "Nicola De Cao",
                "James Thorne",
                "Yacine Jernite",
                "Vladimir Karpukhin",
                "Jean Maillard"
            ],
            "title": "Kilt: a benchmark for knowledge intensive language tasks",
            "year": 2021
        },
        {
            "authors": [
                "Long N Phan",
                "James T Anibal",
                "Hieu Tran",
                "Shaurya Chanana",
                "Erol Bahadroglu",
                "Alec Peltekian",
                "Gr\u00e9goire Altan-Bonnet."
            ],
            "title": "Scifive: a text-to-text transformer model for biomedical literature",
            "venue": "arXiv preprint arXiv:2106.03598.",
            "year": 2021
        },
        {
            "authors": [
                "Patrik Puchert",
                "Poonam Poonam",
                "Christian van Onzenoodt",
                "Timo Ropinski."
            ],
            "title": "Llmmaps\u2013 a visual metaphor for stratified evaluation of large language models",
            "venue": "arXiv preprint arXiv:2304.00457.",
            "year": 2023
        },
        {
            "authors": [
                "Pranav Rajpurkar",
                "Robin Jia",
                "Percy Liang."
            ],
            "title": "Know what you don\u2019t know: Unanswerable questions for squad",
            "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers). Association for",
            "year": 2018
        },
        {
            "authors": [
                "Nils Reimers",
                "Iryna Gurevych."
            ],
            "title": "Sentence-bert: Sentence embeddings using siamese bert-networks",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language",
            "year": 2019
        },
        {
            "authors": [
                "Adam Roberts",
                "Colin Raffel",
                "Noam Shazeer"
            ],
            "title": "How much knowledge can you pack into the parameters of a language model",
            "venue": "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
            "year": 2020
        },
        {
            "authors": [
                "Max Savery",
                "Asma Ben Abacha",
                "Soumya Gayen",
                "Dina Demner-Fushman."
            ],
            "title": "Question-driven summarization of answers to consumer health questions",
            "venue": "Scientific Data, 7(1):322.",
            "year": 2020
        },
        {
            "authors": [
                "Sarvesh Soni",
                "Kirk Roberts."
            ],
            "title": "Evaluation of dataset selection for pre-training and fine-tuning transformer language models for clinical question answering",
            "venue": "Proceedings of the Twelfth Language Resources and Evaluation Conference, pages 5532\u2013",
            "year": 2020
        },
        {
            "authors": [
                "Dan Su",
                "Xiaoguang Li",
                "Jindi Zhang",
                "Lifeng Shang",
                "Xin Jiang",
                "Qun Liu",
                "Pascale Fung."
            ],
            "title": "Read before generate! faithful long form question answering with machine reading",
            "venue": "Findings of the Association for Computational Linguistics: ACL 2022, pages 744\u2013",
            "year": 2022
        },
        {
            "authors": [
                "Dan Su",
                "Mostofa Patwary",
                "Shrimai Prabhumoye",
                "Peng Xu",
                "Ryan Prenger",
                "Mohammad Shoeybi",
                "Pascale Fung",
                "Anima Anandkumar",
                "Bryan Catanzaro."
            ],
            "title": "Context generation improves open domain question answering",
            "venue": "Findings of the Association",
            "year": 2023
        },
        {
            "authors": [
                "Zhiqing Sun",
                "Xuezhi Wang",
                "Yi Tay",
                "Yiming Yang",
                "Denny Zhou."
            ],
            "title": "Recitation-augmented language models",
            "venue": "The Eleventh International Conference on Learning Representations.",
            "year": 2023
        },
        {
            "authors": [
                "Simon \u0160uster",
                "Walter Daelemans."
            ],
            "title": "Clicr: A dataset of clinical case reports for machine reading comprehension",
            "venue": "Proceedings of NAACL-HLT, pages 1551\u20131563.",
            "year": 2018
        },
        {
            "authors": [
                "Alex Tamkin",
                "Miles Brundage",
                "Jack Clark",
                "Deep Ganguli."
            ],
            "title": "Understanding the capabilities, limitations, and societal impact of large language models",
            "venue": "arXiv preprint arXiv:2102.02503.",
            "year": 2021
        },
        {
            "authors": [
                "Raphael Tang",
                "Rodrigo Nogueira",
                "Edwin Zhang",
                "Nikhil Gupta",
                "Phuong Cam",
                "Kyunghyun Cho",
                "Jimmy Lin."
            ],
            "title": "Rapidly bootstrapping a question answering dataset for covid-19",
            "venue": "arXiv preprint arXiv:2004.11339.",
            "year": 2020
        },
        {
            "authors": [
                "Hugo Touvron",
                "Thibaut Lavril",
                "Gautier Izacard",
                "Xavier Martinet",
                "Marie-Anne Lachaux",
                "Timoth\u00e9e Lacroix",
                "Baptiste Rozi\u00e8re",
                "Naman Goyal",
                "Eric Hambro",
                "Faisal Azhar"
            ],
            "title": "Llama: Open and efficient foundation language models",
            "year": 2023
        },
        {
            "authors": [
                "George Tsatsaronis",
                "Georgios Balikas",
                "Prodromos Malakasiotis",
                "Ioannis Partalas",
                "Matthias Zschunke",
                "Michael R Alvers",
                "Dirk Weissenborn",
                "Anastasia Krithara",
                "Sergios Petridis",
                "Dimitris Polychronopoulos"
            ],
            "title": "An overview of the bioasq large-scale",
            "year": 2015
        },
        {
            "authors": [
                "Cunxiang Wang",
                "Pai Liu",
                "Yue Zhang"
            ],
            "title": "Can generative pre-trained language models serve as knowledge bases for closed-book qa? In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International",
            "year": 2021
        },
        {
            "authors": [
                "Jason Wei",
                "Maarten Bosma",
                "Vincent Y Zhao",
                "Kelvin Guu",
                "Adams Wei Yu",
                "Brian Lester",
                "Nan Du",
                "Andrew M Dai",
                "Quoc V Le."
            ],
            "title": "Finetuned language models are zero-shot learners",
            "venue": "arXiv preprint arXiv:2109.01652.",
            "year": 2021
        },
        {
            "authors": [
                "Johannes Welbl",
                "Pontus Stenetorp",
                "Sebastian Riedel."
            ],
            "title": "Constructing Datasets for Multi-hop Reading Comprehension Across Documents",
            "venue": "Transactions of the Association for Computational Linguistics, 6:287\u2013 302.",
            "year": 2018
        },
        {
            "authors": [
                "Yan Xu",
                "Etsuko Ishii",
                "Samuel Cahyawijaya",
                "Zihan Liu",
                "Genta Indra Winata",
                "Andrea Madotto",
                "Dan Su",
                "Pascale Fung."
            ],
            "title": "Retrieval-free knowledgegrounded dialogue response generation with adapters",
            "venue": "Proceedings of the Second DialDoc Workshop on",
            "year": 2022
        },
        {
            "authors": [
                "Zhangyue Yin",
                "Qiushi Sun",
                "Qipeng Guo",
                "Jiawen Wu",
                "Xipeng Qiu",
                "Xuanjing Huang"
            ],
            "title": "Do large language models know what they don\u2019t know? In Findings of the Association for Computational Linguistics: ACL",
            "year": 2023
        },
        {
            "authors": [
                "Wenhao Yu",
                "Dan Iter",
                "Shuohang Wang",
                "Yichong Xu",
                "Mingxuan Ju",
                "Soumya Sanyal",
                "Chenguang Zhu",
                "Michael Zeng",
                "Meng Jiang."
            ],
            "title": "Generate rather than retrieve: Large language models are strong context generators",
            "venue": "The Eleventh Inter-",
            "year": 2023
        },
        {
            "authors": [
                "Hao Zhou",
                "Minlie Huang",
                "Yong Liu",
                "Wei Chen",
                "Xiaoyan Zhu."
            ],
            "title": "Earl: Informative knowledge-grounded conversation generation with entity-agnostic representation learning",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods",
            "year": 2021
        },
        {
            "authors": [
                "Ming Zhu",
                "Aman Ahuja",
                "Da-Cheng Juan",
                "Wei Wei",
                "Chandan K. Reddy."
            ],
            "title": "Question answering with long multiple-span answers",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3840\u20133849, Online. Association for Computa-",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Large language models (LLMs) have shown promise for generative and knowledge-intensive tasks which require world or domain knowledge (Petroni et al., 2021). One representative task is generative question-answering (GQA) which provides relevant information in response to queries (Li et al., 2021a; Su et al., 2023; Nakano et al., 2021) and can also serve to probe the capabilities of language models (Wang et al., 2021). How-\n1https://github.com/ziweiji/Self_Reflection_ Medical\never, despite their potential and recent advancements, these models face a concerning issue, \u201challucination\u201d, a phenomenon where the model generates plausible-sounding but unfaithful or nonsensical information (Ji et al., 2023). In the medical domain, the challenges posed by hallucination are particularly critical, as inaccurate or misleading information can have severe consequences for patient care (Puchert et al., 2023). Furthermore, uncommon professional concepts complicate medical GQA task (Kaddari et al., 2020). For the GQA example in Figure 1, PTEN (Phosphatase and tensin homolog) mutation is not typically associated with Noonan syndrome, so the answer sounds plausible but is hallucinated. Therefore, it is crucial to understand and mitigate these hallucinations to ensure the reliability and safety of AI-enabled medical services (Morley et al., 2020).\nParallel to retrieving external relevant knowledge (Lewis et al., 2020; Guu et al., 2020; Izacard et al., 2022), some current works (Yu et al., 2023; Wang et al., 2021; Roberts et al., 2020; Xu et al.,\n2directly generated by Alpaca-Lora. 3According to https://www.chop.edu/ conditions-diseases/noonan-syndrome and https://en.wikipedia.org/wiki/Noonan_syndrome.\n2022; Sun et al., 2023) explore leveraging the parametric knowledge in LLMs and tap into their potential for knowledge-intensive tasks. GQA works in other domains (Lin et al., 2021; Su et al., 2022) underscore the importance of addressing hallucination and improving faithfulness. However, the current understanding of the extent of hallucination in medical answers generated by LLMs remains unclear, and there is a need to explore the potential for further improvement in this aspect.\nTo fill the gap, this study investigates hallucination in the context of medical GQA systems, particularly in general LLMs like Vicuna (Chiang et al., 2023), Alpaca-LoRA (Wang, 2023), ChatGPT (OpenAI, 2023a), and medical LLMs like MedAlpaca (Han et al., 2023), and Robinmedical (Diao et al., 2023) on popular medical datasets: PubMedQA (Jin et al., 2019), MedQuAD (Ben Abacha and Demner-Fushman, 2019), MEDIQA2019 (Ben Abacha et al., 2019), LiveMedQA2017 (Ben Abacha et al., 2017), MASH-QA (Zhu et al., 2020). We evaluate the incidence of hallucinations, explore the potential cause, and propose a strategy to mitigate this issue. Through a comprehensive analysis of the recent models, we hope to increase the understanding of hallucination in these systems and provide a road map towards more reliable AI-assisted healthcare.\nCurrent research works (Yin et al., 2023; Burns et al., 2022; Rajpurkar et al., 2018; Kadavath et al., 2022; Manakul et al., 2023) highlight a gap between surface realization and inherent knowledge in NLG tasks. Models can realize they are generating something hallucinated in some way. To reduce this gap and mitigate hallucinations in medical GQA, we devise an iterative, introspective process that leverages the multi-turn interactivity and multitask ability of LLMs. Our self-reflective methodology initiates the generation of pertinent background knowledge for a given question, followed by a factuality evaluation. Upon detection of discrepancies, the model is urged to self-correct, utilizing its inherent reflective capacities to refine the knowledge. This cyclical process is repeated until a satisfactory level of factuality is achieved. In the answering stage, we employ a similar generation-score-refine strategy to ensure consistency between the generated answer and the background knowledge. Additionally, an entailment evaluation is conducted between the answer and the question. If the generated answer fails to meet the standard, the process\nreturns to the initial stage, repeating the cycle. Our method fosters a dynamic interaction between the system and its knowledge, enhancing the model\u2019s ability to provide accurate, reliable, and factually grounded responses in healthcare settings.\nThe experimental results of our method showcase its effectiveness across LLMs with varying parameters, including 7B and 175B, on all five datasets. This robust performance highlights the generalizability and scalability of our approach, further validating its efficacy in the context of medical question-answering tasks. Our method explores extracting and digging knowledge from LLMs, leveraging their full potential, and strives to approach their upper performance bound. Our method can also combine other techniques such as utilizing external knowledge and more powerful LLMs in the future to contribute to the development of robust application systems. In summary, the major contributions of this work are threefold:\n\u2022 We provide a comprehensive examination of the hallucination phenomenon in medical GQA systems, particularly those employing five LLMs across five medical GQA datasets. \u2022 We propose an innovative self-reflection method to mitigate hallucination in LLMs. The iterative feedback loop process generates, scores, and refines knowledge and answers until they reach satisfactory levels, enhancing the accuracy and reliability of the answers. \u2022 Experimental results demonstrate the effectiveness, generalizability, and scalability of our method. This investigation sheds light on the potential of LLMs as valuable tools for medical GQA tasks, showcasing their ability to provide meaningful insights without explicit training on specific datasets."
        },
        {
            "heading": "2 Related Work",
            "text": "Medical Question Answering Medical QA systems have shown significant potential for enhancing information accessibility and understanding in the healthcare field. These systems respond to various question formats, including Yes/No (Tsatsaronis et al., 2015; Jin et al., 2019), multi-choice (Pal et al., 2022; Li et al., 2021b; Welbl et al., 2018; Berant et al., 2014; Abacha et al., 2019; Morante et al., 2012), extractive (Tsatsaronis et al., 2015; Dhingra et al., 2018; \u0160uster and Daelemans, 2018; Tang et al., 2020; M\u00f6ller et al., 2020; Pampari et al., 2018), and generative (Savery et al., 2020;\nMoll\u00e1 et al., 2016; Jin et al., 2019; Ben Abacha and Demner-Fushman, 2019; Ben Abacha et al., 2019, 2017). The introduction of pre-trained language models has further enhanced the capabilities of GQA systems, enabling them to generate fluent and meaningful responses to medical queries (Soni and Roberts, 2020; Liu et al., 2022; Savery et al., 2020; Alsentzer et al., 2019; Kaddari et al., 2020).\nHallucination in Generative Question Answering Faithful GQA, which aims to generate answers strictly grounded in the source text or valid external knowledge, has gained significant research attention (Nakano et al., 2021; Su et al., 2022, 2023). The more faithful the answer is, the less hallucinated content it contains. Other terms like semantic drift, factual correctness can also reflect hallucination level (Li et al., 2021a; Su et al., 2022). Rationale-Enriched Answer Generator (REAG) (Li et al., 2021a) add an extraction task to obtain answer rationale and generate answers with high confidence. Read-before-Generate (Su et al., 2022) combines answer generation with machine reading to incorporate fine-grained, answer-related salient information. A benchmark (Lin et al., 2021) measures the truthfulness of answers generated by language models across various domains.These studies underscore the importance of reducing hallucination, a key focus of our work.\nLarge Language Models The advent of LLMs, including GPT-3 (Brown et al., 2020), ChatGPT (OpenAI, 2023a), LLaMA (Touvron et al., 2023), and GPT-4 (OpenAI, 2023b), has revolutionized natural language processing tasks, showcasing their impressive language capabilities in generating fluent, contextually relevant responses (Brown et al., 2020; OpenAI, 2023a; Touvron et al., 2023; OpenAI, 2023b). In addition, emergent abilities are revealed from these models, such as incontext learning (Min et al., 2022), zero-shot in-\nstruction (Ouyang et al., 2022; Wei et al., 2021), and chain-of-thought reasoning (Wei et al.). However, their deployment in practical applications has also surfaced challenges related to the control, bias, and reliability (Tamkin et al., 2021), where hallucination has recently become an increasingly visible issue (OpenAI, 2023a; Bang et al., 2023)."
        },
        {
            "heading": "3 Analysis of Hallucination",
            "text": "In this section, we directly ask LLMs medical questions from five datasets leveraging their zero-shot capability. We then comprehensively evaluate and analyze the generated answers, with a focus on examining the occurrence of hallucination."
        },
        {
            "heading": "3.1 Models",
            "text": "We evaluate the generated answers from five LLMs, including three general LLMs and two LLMs finetuned in the medical domain. Vicuna (Chiang et al., 2023) is trained by fine-tuning LLaMA on user-shared conversations from ShareGPT. AlpacaLoRA (Wang, 2023) employs Low-Rank Adaptation (LoRA) to replicate the results of Stanford\u2019s Alpaca model. ChatGPT (OpenAI, 2023a) interprets prompts and provides comprehensive responses using Reinforcement Learning from Human Feedback (RLHF). MedAlpaca (Han et al., 2023) is built upon the frameworks of LLaMA and fine-tuned on instruction-tuning formatted medical dialogue and QA texts. Robin-medical (Diao et al., 2023) is fine-tuned LLaMA in the medical domain using LMFlow."
        },
        {
            "heading": "3.2 Dataset",
            "text": "PubMedQA (Jin et al., 2019) is a biomedical QA dataset containing 1k expert-labeled instances which include questions derived from research article titles, abstracts as the context, long answers from abstract conclusions, and concise\nyes/no/maybe answers. MedQuAD (Ben Abacha and Demner-Fushman, 2019) comprises 47,457 QA pairs from National Institutes of Health websites and covers various medical topics including diseases, medications, and diagnostic tests. We use the medical QA dataset from MEDIQA2019 (Ben Abacha et al., 2019) challenge and consider answers with scores 3 and 4 as golden answers. LiveMedQA2017 (Ben Abacha et al., 2017) contains annotated medical QA pairs for question analysis and answering systems. MASH-QA (Zhu et al., 2020) includes 34k QA pairs from the consumer health domain designed for Multiple Answer Spans Healthcare QA. Except for PubMedQA, answer annotation in these datasets involves manual extraction and copying from authentic web content. While the answers are pertinent and verifiable, there is room for improvement in terms of contextual coherence and question linkage. Please see Appendix A for details and an example."
        },
        {
            "heading": "3.3 Evaluation Protocols",
            "text": "To evaluate the generation quality, we follow the previous work (Su et al., 2022) utilizing GQA metrics: unigram F1 and ROUGE-L (Lin, 2004). However, the widely used n-gram similarity metrics often fail to discriminate the hallucinated/incorrect answers and correlate weakly with human judgments (Lee et al., 2021; Zhou et al., 2021). We introduce Med-NLI (Medical Natural Language Inference) to assess the logical consistency/entailment of generated answers with the provided context or the reference answer. We adopt SciFive (Phan et al., 2021), a T5 model pre-trained on extensive biomedical corpora. Our evaluation occurs at two levels: Sample-level Med-NLI evaluates whether each generated answer entails (1), is neutral (0), or contradicts (-1) the context or the reference answer. Sentence-level Med-NLI determines the same but for each individual sentence within the generated response. We also use CTRLEval (Ke et al., 2022), an unsupervised, referencefree, and task-agnostic evaluation metric that assesses generation from various aspects by formulating each aspect into multiple text-infilling tasks. In our work, we specifically employ the consistency aspect of this metric."
        },
        {
            "heading": "3.4 Results and Discussion",
            "text": "Table 2 shows the experimental results on automatic metrics over the test sets from five datasets.\nError Analysis After analyzing 250 directly generated examples from the five models, we classify problematic answers into three categories: Fact Inconsistency, Query Inconsistency, and Tangentiality. Please refer to Table 1 and Figure 2 for the representative example and incidence for each category and model. We consider the first two as the hallucination problem. 1. Fact Inconsistency refers to answers that provide information that is inconsistent or in conflict with the fact. It arises when the model fails to appropriately recall relevant knowledge when responding to the question. The example answer in Table 1 incorrectly states that Noonan syndrome is not inherited while it is inherited in an autosomal dominant manner. 2. Query Inconsistency refers to answers that are unrelated to the query or nonsensical. It occurs when the model neither responds to the question nor invokes relevant knowledge appropriately. The example answer in Table 1 discusses the benefits of Vitamin but does not mention cardiac reoperations. 3. Tangentiality refers to answers that provide information related to the topic but do not directly address the question. It occurs when the model does not further process mastered knowledge, such as inductive, deductive, and logical reasoning. The example answer in Table 1 tangentially discusses uveal membrane but fails to mention the effect of c-Kit on uveal membrane.\nAddressing these challenges requires models to recall factual knowledge, contextual understanding, and reasoning abilities. Further exploration and development of these capabilities in LLMs are necessary to improve the reliability and trustworthiness of generation systems.\nThe Effect of Fine-Tuning on Medical Domain LLMs fine-tuned on medical domain texts (Han et al., 2023; Diao et al., 2023) have demonstrated\nimproved performance on certain types of questions, such as multi-choice QA, benefiting from the availability of abundant training data. However, their performance in GQA tasks suffers from issues like unrelated content, grammar issues, unwarranted templates, non-existent references, and a lack of explanatory reasoning. As in Table 2, Robin-medical obtains the lowest F1 and Rouge-L scores. For example, given the question: \u201cWho can\nget eczema?\u201d, Robin-medical generates: \u201c(A) All (B) 10% (C) 20% (D) 30%.\\n Output: A. \u201d The discrepancy between MedAplpaca and Robin-medical indicates that instruction learning is more suitable for LLMs than non-instruction tuning in our tasks. Due to Robin-medical\u2019s relatively poor generation performance, we exclude it from further experiments.\nThe Effect of Frequency Considering the impracticality of measuring frequency according to an LLM\u2019s pre-train corpus, we use Google Ngrams4 as a proxy of the text distribution in the natural world and pre-training corpora. We randomly select 100 samples generated by the general models. We exact the keywords or topics of the questions, which usually are disease names. We take average frequencies between the years 1950- 2019 (McKenna et al., 2023) of these keywords. As in Figure 3, the problematic answers have lower frequencies than fine ones. This suggests that low frequency may be a potential cause of hallucinations, which requires more exploration to prove."
        },
        {
            "heading": "4 Hallucination Mitigation Method",
            "text": ""
        },
        {
            "heading": "4.1 Methodology",
            "text": "To address the issue of hallucination, we propose an iterative self-reflection process that leverages the capabilities of LLMs in generating and refining responses. Illustrated in Figure 4, our methodology comprises three loops: Factual Knowledge Acquiring Loop, Knowledge-Consistent Answering Loop, and Question-Entailment Answering Loop.\nFactual Knowledge Acquiring Loop First, the model generates background knowledge based on\n4The website https://books.google.com/ngrams charts the frequencies of any set of search strings using a yearly count of n-grams found in printed sources.\nthe provided question. This step capitalizes on the inherent ability of LLMs to synthesize contextrelevant information, forming the foundation for the subsequent scoring and refining stages.\nThen, a factuality evaluation of the generated knowledge is conducted with a customized and reference-free scorer. The factuality scorer is designed through in-context instruction learning with the following formula:\nFs(k|D,Q) = \u2211m\nt=1 logP (kt|k<t, T (D,Q)) (1) The knowledge to be evaluated is k = {k1, k2, ..., km}. D is the few-shot demonstrations that are annotated examples and Q is the given question. T (\u00b7) is the prompt template including the definition of factuality and task description: \u201cBased on Question, please generate the factual knowledge. To do this, please consider these factors: Verifiability, Objectivity, and Reliability of Source. Note that this evaluation should be based on the best available medical knowledge.\\n\\nQuestion:...\\nKnowledge:...\u201d In-context instruction learning is verified in the aspect of relevance, fluency, informativeness, etc. on text generation tasks (Fu et al., 2023).\nIf the factuality score is lower than the threshold set in the evaluation phase, we instruct the model self-reflect and refine the knowledge with the following prompt: \u201cThe factuality score for the knowledge is XXX (less than THRESHOLD_FACTUAL), which means the knowledge is not strongly supported by empirical evidence. Please refine the\nknowledge to improve its factuality.\u201d This generate-score-refine strategy is repeated interactively until the generated knowledge reaches the satisfactory factuality level. This iterative procedure fosters dynamic and iterative interaction between the system and its generated knowledge. And ensures that the model progressively refines the produced background knowledge, adhering it to established facts.\nKnowledge-Consistent Answering Loop Once the generated knowledge attains the requisite quality, the model proceeds to generate an answer based on the provided question and the final knowledge with the template: \u201cRefer to the knowledge: \"final_knowledge\" and answer the question: XXX with one paragraph.\u201d A consistency evaluation of the generated answer is conducted with CTRLEval (introduced in \u00a7 3.3). If the generated answer\u2019s consistency score lowers the threshold, the model is prompted to introspect, self-correct, and revise the answer with \u201cThe consistency score for the knowledge is XXX (less than THRESHOLD_CONS), which means the alignment and consistency between response and knowledge are low. Please refine the response to improve its consistency.\u201d\nThis generate-score-refine strategy is repeated until the generated answer reaches the consistency level. This iterative procedure ensures that the model progressively refines the produced answer aligning with the vetted background knowledge, thus maintaining its integrity.\nQuestion-Entailment Answering Loop After the above two loops, we evaluate the generated answer\u2019s entailment via sentence-BERT embedding similariy (Reimers and Gurevych, 2019)5 to ensure the entailment and answerability. If the generated answer does not meet the satisfactory entailment level, the process returns to the initial stage of the framework, and the entire cycle is repeated, iterating through the aforementioned stages."
        },
        {
            "heading": "4.2 Experiments",
            "text": ""
        },
        {
            "heading": "4.2.1 Evaluation",
            "text": "In addition to the automatic metrics described in \u00a7 3.3, we conduct human evaluations using Amazon Mechanical Turk6 to further assess the quality of generated answers. The human evaluation for question-consistency and tangentiality is conducted at the sample level, where we ask annotators to categorize each answer as Query-Inconsistent, Tangential, or Entailed. \u201cQuery-Inconsistent\u201d means that the answer provides information unrelated to the query or is non-sensical and meaningless. \u201cTangential\u201d means that the answer provides information related to the question but doesn\u2019t directly address the question. \u201cEntailed\u201d means that the answer directly addresses the question. The human evaluation for fact-consistency is conducted at the sentence level, where we ask annotators to categorize each sentence in the answer as FactInconsistent, Fact-Consistent, or Generic. \u201cFactInconsistent\u201d means that the answer sentence con-\n5https://huggingface.co/ sentence-transformers/multi-qa-MiniLM-L6-cos-v1\n6https://www.mturk.com/\ntradicts or cannot be verified by the reference contexts or websites. \u201cFact-Consistent\u201d means that the answer sentence is supported by the given contexts or websites. \u201cGeneric\u201d means that the sentence in the answer has no statement to judge. Please see Appendix D for details."
        },
        {
            "heading": "4.2.2 Results",
            "text": "Automatic Evaluation Table 2 presents the automatic evaluation results for our self-reflection loop approach and the baselines that directly generate answers. We observe the superior performance of our method compared to the baselines, as evidenced by both classic overlap metrics and hallucination metrics across all five datasets.\nOur method demonstrates a remarkable increase in MedNLI. For example, Alpaca-Lora-7B with self-reflection loop gains around three times larger than baseline for Sample- and Sentence-level MedNLI on PubMedQA. The improvement in F1 and Rouge-L scores, which are overlap-based metrics, is relatively modest sometimes. This is primarily due to the inherent reliance of these metrics on the accuracy of the golden answers. As such, even though the generated answers exhibit high quality, they may differ from the golden answers, thus impacting the performance of these metrics.\nNotably, our method showcases its effectiveness across language models with varying parameters, including 7B and 175B, across all five datasets. This robust performance highlights the generalizability and scalability of our approach, further validating its efficacy in the context of medical question-answering tasks. Human Evaluation The results in Table 3 demonstrate that our method successfully reduces the percentage of query inconsistency, tangentiality, and fact inconsistency in both Vicuna and ChatGPT, which aligns with the findings from the automatic evaluation. The inter-annotator agreement, measured using Krippendorff\u2019s alpha (Krippendorff, 2011), indicates high agreement among the annotators, with values exceeding 0.8 for question inconsistency and tangentiality, and exceeding 0.7 for fact consistency. Please see Appendix D for detailed results."
        },
        {
            "heading": "4.3 Discussion",
            "text": ""
        },
        {
            "heading": "4.3.1 Ablation Study",
            "text": "To assess the individual contributions of specific components in our method, we conduct an ablation analysis. The results in Table 4 demonstrate the\nthe parts relevant to the question or aligning with the golden answer; underline and italicize the parts unrelated to the question or conflicting with the golden answer.\nperformance of different variations of our approach in terms of automatic hallucination metrics. Effect of Refinement To evaluate the impact of refinement, we omit the scoring and refining stages and only conduct the generation stage. We acquire background knowledge based on the question and then answer based on the knowledge. As in Table 4, the answers generated by the loop without refinement attain lower MedNLI and CtrlEval, which means the refinement is helpful for fewer hallucinations and higher consistency. Effect of Aspect Description To evaluate the impact of providing explicit aspect descriptions for improvement, we omit mentioning the specific aspect that requires refinement. Instead, we instruct the model to engage in self-reflection by using a more general instruction: \u201cPlease refine the knowledge/response.\u201d As in Table 4, the answers generated by the loop without aspect attain lower MedNLI and CtrlEval, which means the aspect description can lead to fewer hallucinations and higher consistency. Effect of Score Number To examine the influence of exact scores in the evaluation phase, we omit presenting exact values. Instead, we only describe the aspect that requires improvement in the instructions: \u201cThe knowledge is not strongly supported by empirical evidence. Please refine the knowledge to improve its factuality.\u201d and \u201cThe alignment and consistency between response and knowledge are low. Please refine the response to improve its consistency.\u201d As in Table 4, the loop without score number attains lower MedNLI and CtrlEval than the full implementation, indicating that the explicit score numbers contribute to better refinement."
        },
        {
            "heading": "4.3.2 Case Study",
            "text": "The examples in Table 5 demonstrate the effectiveness of our method in addressing fact and query inconsistency. In the first line, the directly generated answer inaccurately states that EtCO2 levels may not be a reliable indicator of arterial CO2 levels, while ours accurately indicates the positive correlation between these measures, aligning with the golden answer and fact. In the second line, the directly generated answer is in an email format and mentions irrelevant information: It greets Nadine and mentions PAG-based GABAAergic signaling of adult male rats and depression- and anxiety-like behavior. In contrast, our responses are more relevant and directly address the given question. Please see Appendix E for more examples."
        },
        {
            "heading": "5 Conclusion and Future Work",
            "text": "Hallucinations in generation tasks pose significant challenges to AI\u2019s accountability and trustworthiness. We investigate this problem thoroughly and systematically in the context of medical GQA in general and domain-specific LLMs. To address this challenge, we propose an iterative self-reflection method by adopting a generate-score-refine strategy on background knowledge and answers. Our\nmethod is empirically proven effective, generalizable, and scalable in reducing hallucinations. In future work, we will investigate underlying causes of hallucination, examine this phenomenon in other generation tasks and extend our method to address challenges associated with these tasks.\nLimitations\nWhile our methodology shows promise in mitigating hallucination, it does not entirely eliminate the possibility. The model might still generate ungrounded information, especially in complex or ambiguous scenarios. Currently, our method is still in its early stages and not yet ready for direct real-world deployment. It should be viewed as a complementary approach alongside other methods, such as retrieval, with the potential to contribute to more robust application systems in the future.\nOur study primarily focuses on English medical queries, limiting the generalizability to other languages, domains, and modalities. Further research is necessary to investigate the potential language-specific challenges, domain adaptation challenges, and multimodality fusion challenges. By addressing these aspects, we can adapt our proposed methodology to different contexts, resulting in a more comprehensive understanding and application of our approach across diverse linguistic, multi-domain, and multimodal settings.\nWhile this paper has addressed certain issues in this domain, numerous challenges remain, such as empowering LLMs with high-level ability.\nEthics Statement\nWe used publicly available or synthetic datasets for our experiments, avoiding any potential harm to individuals or groups. The data used in this study were carefully selected and processed to ensure privacy and confidentiality. No personally identifiable information is used, and all data were anonymized prior to analysis.\nIn considering the application of our research findings, we acknowledge the potential risks and ethical considerations associated with AI-assisted healthcare. The hallucination issue, in particular, can have significant implications for patient care and clinical decision-making. Our work is guided by the principle of \"do no harm\", aiming to enhance the reliability and safety of medical QA systems rather than substitute for professional medical judgment."
        },
        {
            "heading": "Acknowledgement",
            "text": "The authors would like to thank Samuel Cahyawijaya for the discussion and suggestions.\nThis work has been supported by the China NSFC Project (No. NSFC21EG14), SAAIR Project (No. Z1286), and HKJCCT21EG01 (RG192)."
        },
        {
            "heading": "A Dataset",
            "text": "PubMedQA (Jin et al., 2019) is a biomedical QA dataset consisting of 1k expert labeled, 61.2k unlabeled, and 211.3k artificially generated instances. It includes questions derived from research article titles, abstracts as the context, long answers from abstract conclusions, and concise yes/no/maybe answers. MedQuAD (Ben Abacha and Demner-Fushman, 2019) comprises 47,457 medical QA pairs derived from 12 National Institutes of Health (NIH) websites. It spans 37 question categories and covers various medical topics including diseases, medications, and diagnostic tests. MEDIQA2019 (Ben Abacha et al., 2019) challenge includes three tasks: Natural Language Inference (NLI), Recognizing Question Entailment (RQE), and QA in the medical domain. For this particular study, we focus on Task 3\u2019s dataset, which centers around medical question answering and consider the golden answers with scores 3 and 4 as the correct responses. LiveMedQA2017 (Ben Abacha et al., 2017) contains annotated medical QA pairs, aiding the development of question analysis and answering systems. The test questions span 26 types within five main categories, each question comprising one or more sub-questions, a focus, and a type. All reference answers, curated from reliable sources and vetted by medical experts, include a URL and relevant comments. A minimum of one reference answer is provided for each test question. MASH-QA (Zhu et al., 2020) is a dataset from the consumer health domain designed for Multiple Answer Spans Healthcare QA. It includes 34k QA pairs where answers may be drawn from nonsequential sections of a lengthy document.\nIn these datasets, except PubMedQA, answer annotation is undertaken through a manually involved process of extracting and copying from authentic web content. It is imperative to note that although the annotated answers are indeed pertinent and verifiable, it has not undergone refinement, thereby improvements are needed in harmonizing the contextual coherence and problem linkage. For an example from LiveMedQA2017, given question: \u201cDo Zolmitriptan 5mg tablets contain gluten?\u201d, the answer is \u201cZolmitriptan tablets are available as 2.5 mg (yellow and functionally-scored) and 5 mg (pink, not scored) film coated tablets for oral administration...\u201d\nB Implementation Details\nVicuna is an open-source foundation, equipped with an enriched dataset and a scalable, userfriendly infrastructure. It is trained by fine-tuning LLaMA (Touvron et al., 2023) on user-shared conversations derived from ShareGPT. Vicuna generates more detailed and well-structured responses compared to its predecessor, Alpaca, with a quality equivalent to ChatGPT. We use the code and checkpoint from the official library7. We run generation on two GeForce RTX 2080 GPUs with the following settings: temperature 1.0, max new tokens 512, others are default. The max number of the main loop, knowledge loop, and response loop is 3. The factuality, consistency, and entailment threshold are -1.0, -5.0, and 0.8, respectively. The demo number for the factuality scorer is 1 for PubMedQA; 3 for MedQuAD, MEDIQA2019, MASH-QA, and LiveMedQA2017.\nAlpaca-LoRA replicates the results of Stanford\u2019s Alpaca model, employing a technique known as Low-Rank Adaptation (LoRA). This instruct model, comparable in quality to GPT-3.5, is capable of operating under low resource conditions. Remarkably, even without hyperparameter tuning, Alpaca-LoRA generates outputs on par with the original Stanford Alpaca model. We use the code and checkpoint from the official library8. We run generation on two GeForce RTX 2080 GPUs with the following settings: temperature 1.0, max new tokens 512, others are default. The max number of the main loop, knowledge loop, and response loop is 3. The factuality, consistency, and entailment threshold are -1.0, -5.0, and 0.8, respectively. The demo number for the factuality scorer is 1 for PubMedQA, MASH-QA, MEDIQA2019, and MedQuAD; 2 for LiveMedQA2017.\nChatGPT is designed to interpret prompts and furnish comprehensive responses. It employs Reinforcement Learning from Human Feedback (RLHF), similar to InstructGPT (Ouyang et al., 2022), albeit with minor differences in data collection methodology. The initial model is trained through supervised fine-tuning, where human AI trainers conducted dialogues, simulating both the user and AI assistant roles. We use the official API from 9 to generate answers. The max number of\n7https://github.com/lm-sys/FastChat 8https://github.com/tloen/alpaca-lora 9https://openai.com/blog/openai-api\nthe main loop and response loop is 3. The max number of the knowledge loop is 1 for MedQuAD and 3 for others. The factuality, consistency, and entailment threshold are -1.0, -5.0, and 0.8, respectively. The demo number for the factuality scorer is 1 for PubMedQA, MASH-QA, MEDIQA2019, and MedQuAD; 3 for LiveMedQA2017.\nMedAlpaca is a large language model meticulously fine-tuned for medical dialogue and QA applications upon the frameworks of Alpaca and Alpaca-LoRA. These models have been trained using an array of medical texts, including medical flashcards, wikis, and dialogue datasets. We use the code and checkpoint from the official library10. We run generation on two GeForce RTX 2080 GPUs with the following settings: temperature 1.0, max new tokens 512, others are default. The max number of the main loop, knowledge loop, and response loop is 3. The factuality, consistency, and entailment threshold are -1.0, -5.0, and 0.8, respectively. The demo number for the factuality scorer is 1 for PubMedQA, MEDIQA2019, and LiveMedQA2017; 2 for MASH-QA, and MedQuAD.\nRobin-medical is a large language model finetuned in the medical domain via LMFlow, an extensible toolkit for finetuning and inference of large foundation models. We use the code and checkpoint from the official library11. We run generation on two GeForce RTX 2080 GPUs with the following settings: temperature 1.0, max new tokens 512, others are default."
        },
        {
            "heading": "C Factuality Scorer",
            "text": "Here are demonstrations for our factuality scorer: Question: What are the risk factors for heart disease?\\n Knowledge: Risk factors for heart disease can be categorized into modifiable and non-modifiable. Modifiable risk factors include high blood pressure, high cholesterol, smoking, unhealthy diet, physical inactivity, obesity, and excessive alcohol use. Non-modifiable risk factors include age, gender, family history, and race or ethnicity.\\n Question: How does smoking affect lung health?\\n Knowledge: Smoking damages the\n10https://github.com/kbressem/medAlpaca 11https://github.com/OptimalScale/LMFlow\nairways and small air sacs in your lungs, which can lead to a variety of lung diseases including chronic bronchitis, emphysema, and lung cancer. It also decreases your lung capacity and makes it harder for your lungs to defend against infections and clear out mucus.\\n\nQuestion: Is it safe to take aspirin every day?\\n Knowledge: For some people, taking aspirin every day can help prevent heart attacks or strokes. However, daily aspirin isn\u2019t appropriate for everyone. It can cause side effects like gastrointestinal bleeding and isn\u2019t recommended for people with certain health conditions or who take certain medications. Always consult with a healthcare professional before starting any new medication regimen.\\n"
        },
        {
            "heading": "D Human Evaluation",
            "text": "We conduct the human evaluation to assess our method\u2019s performance in GQA for the ability to reduce hallucination. In detail, we randomly select 50 question-answer pairs directly generated by Vicuna and 50 question-answer pairs generated with our proposed loop. Each sample is evaluated by three different annotators to rule out potential bias. We specify that annotators must meet the following qualifications: Their Human Intelligence Task (HIT) approval rates are greater than or equal to 95%, and the numbers of HITs approved are greater than or equal to 5000. The annotators are located in Australia, Canada, the United Kingdom, and the United States. The evaluation cost for fact inconsistency is 0.3 US dollars per sentence, and for question inconsistency and tangentiality, it is 0.15 US dollars per answer. Figure 5 and 6 are the user interfaces (UIs) on Amazon Mechanical Turk for human evaluation of fact-consistency and question-consistency and tangentiality, respectively. The instructions, questions, and examples for annotators are shown.\nTo identify the Fact Inconsistency in generated answers, we ask annotators to refer the context provided in dataset. As a supplement, we retrieve the related paragraph from WikiMedicalTerms Dataset12 via sentence-BERT embedding\n12https://huggingface.co/datasets/gamino/wiki_ medical_terms\nsimilariy13. This dataset contains over 6,000 medical terms and their Wikipedia text. In addition, we ask annotators to search for references in WikiDoc14, a medical wiki encyclopedia used by the international community of healthcare professionals.\nThe results in Table 6 demonstrate that our method successfully reduces the percentage of query inconsistency, tangentially, and fact inconsistency in both Vicuna and ChatGPT. Furthermore, we observe that ChatGPT tends to generate more general sentences compared to Vicuna. Our approach can keep generic responses at a relatively low level while improving fact consistency."
        },
        {
            "heading": "E Case Study",
            "text": "Table 7 presents some example answers generated by ChatGPT, ChatGPT with our self-reflection loop, and GPT4. While ChatGPT provides a negative answer, both ChatGPT with our self-reflection loop and GPT4 offer positive answers that align better with the golden answer. ChatGPT suggests that there is no evidence in its available literature that shape analysis can differentiate free-floating internal carotid artery thrombus from atherosclerotic plaque in CTA. In contrast, the golden answer states shape signature highlights the potential to distinguish the two. ChatGPT with our self-reflection loop acknowledges the role of shape analysis in differentiation and mentions works on machine learning algorithms and radiomic features that show promising results in classification. GPT4 also suggests shape analysis can potentially help differentiate, but its subsequent content is unrelated to shape analysis itself. These results demonstrate the effectiveness of our method and the capabilities of more powerful LLMs.\n13https://huggingface.co/ sentence-transformers/all-MiniLM-L6-v2\n14https://www.wikidoc.org/index.php/Main_Page\ncluding parts conflicting with the golden answer."
        }
    ],
    "title": "Towards Mitigating Hallucination in Large Language Models via Self-Reflection",
    "year": 2023
}