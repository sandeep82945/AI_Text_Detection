{
    "abstractText": "Toxic language, such as hate speech, can deter users from participating in online communities and enjoying popular platforms. Previous approaches to detecting toxic language and norm violations have been primarily concerned with conversations from online forums and social media, such as Reddit and Twitter. These approaches are less effective when applied to conversations on live-streaming platforms, such as Twitch and YouTube Live, as each comment is only visible for a limited time and lacks a thread structure that establishes its relationship with other comments. In this work, we share the first NLP study dedicated to detecting norm violations in conversations on live-streaming platforms. We define norm violation categories in live-stream chats and annotate 4,583 moderated comments from Twitch. We articulate several facets of live-stream data that differ from other forums, and demonstrate that existing models perform poorly in this setting. By conducting a user study, we identify the informational context humans use in live-stream moderation, and train models leveraging context to identify norm violations. Our results show that appropriate contextual information can boost moderation performance by 35%. 1",
    "authors": [
        {
            "affiliations": [],
            "name": "Jihyung Moon"
        },
        {
            "affiliations": [],
            "name": "Dong-Ho Lee"
        },
        {
            "affiliations": [],
            "name": "Hyundong Cho"
        },
        {
            "affiliations": [],
            "name": "Woojeong Jin"
        },
        {
            "affiliations": [],
            "name": "Chan Young Park"
        },
        {
            "affiliations": [],
            "name": "Minwoo Kim"
        },
        {
            "affiliations": [],
            "name": "Jonathan May"
        },
        {
            "affiliations": [],
            "name": "Jay Pujara"
        },
        {
            "affiliations": [],
            "name": "Sungjoon Park"
        }
    ],
    "id": "SP:21e605a1f31ed9bf1905e1d563d39621cfc02d0e",
    "references": [
        {
            "authors": [
                "Valerio Basile",
                "Cristina Bosco",
                "Elisabetta Fersini",
                "Debora Nozza",
                "Viviana Patti",
                "Francisco Manuel Rangel Pardo",
                "Paolo Rosso",
                "Manuela Sanguinetti"
            ],
            "title": "SemEval-2019 task 5: Multilingual detection of hate speech against immigrants and women",
            "year": 2019
        },
        {
            "authors": [
                "Luke Breitfeller",
                "Emily Ahn",
                "David Jurgens",
                "Yulia Tsvetkov."
            ],
            "title": "Finding microaggressions in the wild: A case for locating elusive phenomena in social media posts",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Pro-",
            "year": 2019
        },
        {
            "authors": [
                "Eshwar Chandrasekharan",
                "Mattia Samory",
                "Shagun Jhaver",
                "Hunter Charvat",
                "Amy Bruckman",
                "Cliff Lampe",
                "Jacob Eisenstein",
                "Eric Gilbert"
            ],
            "title": "The internet\u2019s hidden rules: An empirical study of reddit norm violations at micro, meso, and macro scales",
            "year": 2018
        },
        {
            "authors": [
                "Jithin Cheriyan",
                "Bastin Tony Roy Savarimuthu",
                "Stephen Cranefield."
            ],
            "title": "Norm violation in online communities\u2013a study of stack overflow comments",
            "venue": "Coordination, Organizations, Institutions, Norms, and Ethics for Governance of Multi-Agent Systems",
            "year": 2017
        },
        {
            "authors": [
                "Srayan Datta",
                "Eytan Adar."
            ],
            "title": "Extracting intercommunity conflicts in reddit",
            "venue": "Proceedings of the international AAAI conference on Web and Social Media, volume 13, pages 146\u2013157.",
            "year": 2019
        },
        {
            "authors": [
                "Aida Mostafazadeh Davani",
                "Mark D\u00edaz",
                "Vinodkumar Prabhakaran."
            ],
            "title": "Dealing with Disagreements: Looking Beyond the Majority Vote in Subjective Annotations",
            "venue": "Transactions of the Association for Computational Linguistics, 10:92\u2013110.",
            "year": 2022
        },
        {
            "authors": [
                "Thomas Davidson",
                "Dana Warmsley",
                "Michael Macy",
                "Ingmar Weber."
            ],
            "title": "Automated hate speech detection and the problem of offensive language",
            "venue": "Proceedings of the international AAAI conference on web and social media, volume 11, pages 512\u2013515.",
            "year": 2017
        },
        {
            "authors": [
                "Mai ElSherief",
                "Caleb Ziems",
                "David Muchlinski",
                "Vaishnavi Anupindi",
                "Jordyn Seybolt",
                "Munmun De Choudhury",
                "Diyi Yang."
            ],
            "title": "Latent hatred: A benchmark for understanding implicit hate speech",
            "venue": "Proceedings of the 2021 Conference on Empirical Meth-",
            "year": 2021
        },
        {
            "authors": [
                "Casey Fiesler",
                "Joshua McCann",
                "Kyle Frye",
                "Jed R Brubaker"
            ],
            "title": "Reddit rules! characterizing an ecosystem of governance",
            "venue": "In Twelfth International AAAI Conference on Web and Social Media",
            "year": 2018
        },
        {
            "authors": [
                "Antigoni Maria Founta",
                "Constantinos Djouvas",
                "Despoina Chatzakou",
                "Ilias Leontiadis",
                "Jeremy Blackburn",
                "Gianluca Stringhini",
                "Athena Vakali",
                "Michael Sirivianos",
                "Nicolas Kourtellis"
            ],
            "title": "Large scale crowdsourcing and characterization of twitter abusive",
            "year": 2018
        },
        {
            "authors": [
                "Hongyu Gong",
                "Alberto Valido",
                "Katherine M Ingram",
                "Giulia Fanti",
                "Suma Bhat",
                "Dorothy L Espelage."
            ],
            "title": "Abusive language detection in heterogeneous contexts: Dataset collection and the role of supervised attention",
            "venue": "Proceedings of the AAAI Con-",
            "year": 2021
        },
        {
            "authors": [
                "Xiaochuang Han",
                "Yulia Tsvetkov."
            ],
            "title": "Fortifying toxic speech detectors against veiled toxicity",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 7732\u20137739, Online. Association for Computa-",
            "year": 2020
        },
        {
            "authors": [
                "Thomas Hartvigsen",
                "Saadia Gabriel",
                "Hamid Palangi",
                "Maarten Sap",
                "Dipankar Ray",
                "Ece Kamar."
            ],
            "title": "ToxiGen: A large-scale machine-generated dataset for adversarial and implicit hate speech detection",
            "venue": "Proceedings of the 60th Annual Meeting of the",
            "year": 2022
        },
        {
            "authors": [
                "Tianxing He",
                "Jun Liu",
                "Kyunghyun Cho",
                "Myle Ott",
                "Bing Liu",
                "James Glass",
                "Fuchun Peng."
            ],
            "title": "Analyzing the forgetting problem in pretrain-finetuning of opendomain dialogue response models",
            "venue": "Proceedings of the 16th Conference of the European Chapter of",
            "year": 2021
        },
        {
            "authors": [
                "David Jurgens",
                "Libby Hemphill",
                "Eshwar Chandrasekharan."
            ],
            "title": "A just and comprehensive strategy for using NLP to address online abuse",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3658\u2013",
            "year": 2019
        },
        {
            "authors": [
                "Brendan Kennedy",
                "Mohammad Atari",
                "Aida Mostafazadeh Davani",
                "Leigh Yeh",
                "Ali Omrani",
                "Yehsong Kim",
                "Kris Coombs",
                "Shreya Havaldar",
                "Gwenyth Portillo-Wightman",
                "Elaine Gonzalez"
            ],
            "title": "The gab hate corpus: A",
            "year": 2018
        },
        {
            "authors": [
                "Brendan Kennedy",
                "Xisen Jin",
                "Aida Mostafazadeh Davani",
                "Morteza Dehghani",
                "Xiang Ren."
            ],
            "title": "Contextualizing hate speech classifiers with post-hoc explanation",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,",
            "year": 2020
        },
        {
            "authors": [
                "Srijan Kumar",
                "William L Hamilton",
                "Jure Leskovec",
                "Dan Jurafsky."
            ],
            "title": "Community interaction and conflict on the web",
            "venue": "Proceedings of the 2018 world wide web conference, pages 933\u2013943.",
            "year": 2018
        },
        {
            "authors": [
                "Dong-Ho Lee",
                "Akshen Kadakia",
                "Brihi Joshi",
                "Aaron Chan",
                "Ziyi Liu",
                "Kiran Narahari",
                "Takashi Shibuya",
                "Ryosuke Mitani",
                "Toshiyuki Sekiya",
                "Jay Pujara"
            ],
            "title": "Xmd: An end-to-end framework for interactive explanation-based debugging of nlp models",
            "year": 2022
        },
        {
            "authors": [
                "Alyssa Lees",
                "Daniel Borkan",
                "Ian Kivlichan",
                "Jorge Nario",
                "Tesh Goyal."
            ],
            "title": "Capturing covertly toxic speech via crowdsourcing",
            "venue": "Proceedings of the First Workshop on Bridging Human\u2013Computer Interaction and Natural Language Processing, pages",
            "year": 2021
        },
        {
            "authors": [
                "Alyssa Lees",
                "Vinh Q Tran",
                "Yi Tay",
                "Jeffrey Sorensen",
                "Jai Gupta",
                "Donald Metzler",
                "Lucy Vasserman."
            ],
            "title": "A new generation of perspective api: Efficient multilingual character-level transformers",
            "venue": "arXiv preprint arXiv:2202.11176.",
            "year": 2022
        },
        {
            "authors": [
                "Han Li",
                "Robert E Kraut",
                "Haiyi Zhu."
            ],
            "title": "Technical features of asynchronous and synchronous community platforms and their effects on community cohesion: A comparative study of forum-based and chatbased online mental health communities",
            "venue": "Journal",
            "year": 2021
        },
        {
            "authors": [
                "Todor Markov",
                "Chong Zhang",
                "Sandhini Agarwal",
                "Tyna Eloundou",
                "Teddy Lee",
                "Steven Adler",
                "Angela Jiang",
                "Lilian Weng."
            ],
            "title": "A holistic approach to undesired content detection in the real world",
            "venue": "arXiv preprint arXiv:2208.03274.",
            "year": 2022
        },
        {
            "authors": [
                "Binny Mathew",
                "Punyajoy Saha",
                "Seid Muhie Yimam",
                "Chris Biemann",
                "Pawan Goyal",
                "Animesh Mukherjee."
            ],
            "title": "Hatexplain: A benchmark dataset for explainable hate speech detection",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, vol-",
            "year": 2021
        },
        {
            "authors": [
                "Stefano Menini",
                "Alessio Palmero Aprosio",
                "Sara Tonelli."
            ],
            "title": "Abuse is contextual, what about nlp? the role of context in abusive language annotation and detection",
            "venue": "arXiv preprint arXiv:2103.14916.",
            "year": 2021
        },
        {
            "authors": [
                "Courtney Miller",
                "Sophie Cohen",
                "Daniel Klug",
                "Bogdan Vasilescu",
                "Christian K\u00e4stner."
            ],
            "title": "did you miss my comment or what?\u201d understanding toxicity in open source discussions",
            "venue": "In 44th International Conference on Software Engineering (ICSE\u201922).",
            "year": 2022
        },
        {
            "authors": [
                "Nedjma Ousidhoum",
                "Zizheng Lin",
                "Hongming Zhang",
                "Yangqiu Song",
                "Dit-Yan Yeung."
            ],
            "title": "Multilingual and multi-aspect hate speech analysis",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the",
            "year": 2019
        },
        {
            "authors": [
                "Chan Young Park",
                "Julia Mendelsohn",
                "Karthik Radhakrishnan",
                "Kinjal Jain",
                "Tushar Kanakagiri",
                "David Jurgens",
                "Yulia Tsvetkov."
            ],
            "title": "Detecting community sensitive norm violations in online conversations",
            "venue": "Findings of the Association for Computational",
            "year": 2021
        },
        {
            "authors": [
                "jani",
                "Sasank Chilamkurthy",
                "Benoit Steiner",
                "Lu Fang",
                "Junjie Bai",
                "Soumith Chintala"
            ],
            "title": "Pytorch: An imperative style, high-performance deep learning library",
            "venue": "In Advances in Neural Information Processing Systems",
            "year": 2019
        },
        {
            "authors": [
                "John Pavlopoulos",
                "Jeffrey Sorensen",
                "Lucas Dixon",
                "Nithum Thain",
                "Ion Androutsopoulos"
            ],
            "title": "Toxicity detection: Does context really matter",
            "venue": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,",
            "year": 2020
        },
        {
            "authors": [
                "Patricia Rossini."
            ],
            "title": "Beyond incivility: Understanding patterns of uncivil and intolerant discourse in online political talk",
            "venue": "Communication Research, 49(3):399\u2013425.",
            "year": 2022
        },
        {
            "authors": [
                "Chinnadhurai Sankar",
                "Sandeep Subramanian",
                "Chris Pal",
                "Sarath Chandar",
                "Yoshua Bengio."
            ],
            "title": "Do neural dialog systems use the conversation history effectively? an empirical study",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Com-",
            "year": 2019
        },
        {
            "authors": [
                "Maarten Sap",
                "Dallas Card",
                "Saadia Gabriel",
                "Yejin Choi",
                "Noah A. Smith."
            ],
            "title": "The risk of racial bias in hate speech detection",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1668\u20131678, Florence, Italy. Asso-",
            "year": 2019
        },
        {
            "authors": [
                "Maarten Sap",
                "Saadia Gabriel",
                "Lianhui Qin",
                "Dan Jurafsky",
                "Noah A. Smith",
                "Yejin Choi."
            ],
            "title": "Social bias frames: Reasoning about social and power implications of language",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational",
            "year": 2020
        },
        {
            "authors": [
                "Maarten Sap",
                "Swabha Swayamdipta",
                "Laura Vianna",
                "Xuhui Zhou",
                "Yejin Choi",
                "Noah A. Smith."
            ],
            "title": "Annotators with attitudes: How annotator beliefs and identities bias toxic language detection",
            "venue": "Proceedings of the 2022 Conference of the North Amer-",
            "year": 2022
        },
        {
            "authors": [
                "Leandro Silva",
                "Mainack Mondal",
                "Denzil Correa",
                "Fabr\u00edcio Benevenuto",
                "Ingmar Weber."
            ],
            "title": "Analyzing the targets of hate in online social media",
            "venue": "Tenth international AAAI conference on web and social media.",
            "year": 2016
        },
        {
            "authors": [
                "Rohit Sridhar",
                "Diyi Yang."
            ],
            "title": "Explaining toxic text via knowledge enhanced text generation",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
            "year": 2022
        },
        {
            "authors": [
                "Jherez Taylor",
                "Melvyn Peignon",
                "Yi-Shin Chen."
            ],
            "title": "Surfacing contextual hate speech words within social media",
            "venue": "arXiv preprint arXiv:1711.10093.",
            "year": 2017
        },
        {
            "authors": [
                "William Warner",
                "Julia Hirschberg."
            ],
            "title": "Detecting hate speech on the world wide web",
            "venue": "Proceedings of the Second Workshop on Language in Social Media, pages 19\u201326, Montr\u00e9al, Canada. Association for Computational Linguistics.",
            "year": 2012
        },
        {
            "authors": [
                "Zeerak Waseem",
                "Dirk Hovy."
            ],
            "title": "Hateful symbols or hateful people? predictive features for hate speech detection on Twitter",
            "venue": "Proceedings of the NAACL Student Research Workshop, pages 88\u201393, San Diego, California. Association for Computational Linguis-",
            "year": 2016
        },
        {
            "authors": [
                "Thomas Wolf",
                "Lysandre Debut",
                "Victor Sanh",
                "Julien Chaumond",
                "Clement Delangue",
                "Anthony Moi",
                "Pierric Cistac",
                "Tim Rault",
                "R\u00e9mi Louf",
                "Morgan Funtowicz"
            ],
            "title": "Huggingface\u2019s transformers: State-ofthe-art natural language processing",
            "year": 2019
        },
        {
            "authors": [
                "Alexandros Xenos",
                "John Pavlopoulos",
                "Ion Androutsopoulos."
            ],
            "title": "Context sensitivity estimation in toxicity detection",
            "venue": "Proceedings of the 5th Workshop on Online Abuse and Harms (WOAH 2021), pages 140\u2013145, Online. Association for Computa-",
            "year": 2021
        },
        {
            "authors": [
                "He"
            ],
            "title": "2021). Moreover, input as the sequential order of chats presented in the contextaware model (Pavlopoulos et al., 2020)",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Interactive live streaming services such as Twitch 2 and YouTube Live 3 have emerged as one of the most popular and widely-used social platforms. Unfortunately, streamers on these platforms struggle with an increasing volume of toxic comments and norm-violating behavior.4 While there has been extensive research on mitigating similar problems for online conversations across various platforms such as Twitter (Waseem and Hovy, 2016; Davidson\n\u2217Authors contributed equally. 1https://github.com/softly-ai/live-NormVio 2https://www.twitch.tv/ 3https://www.youtube.com/ 4https://safety.twitch.tv/s/article/Community-Guidelines\net al., 2017; Founta et al., 2018; Basile et al., 2019; ElSherief et al., 2021), Reddit (Datta and Adar, 2019; Kumar et al., 2018; Park et al., 2021), Stackoverflow (Cheriyan et al., 2017) and Github (Miller et al., 2022), efforts that extend them to live streaming platforms have been absent. In this paper, we study unique characteristics of comments in livestreaming services and develop new datasets and models for appropriately using contextual information to automatically moderate toxic content and norm violations.\nConversations in online communities studied in previous work are asynchronous: utterances are grouped into threads that structurally establish conversational context, allowing users to respond to prior utterances without time constraints. The lack of time constraints allows users to formulate longer and better thought-out responses and more easily reference prior context.\nOn the other hand, conversations on live streaming platforms are synchronous, i.e. in real-time, as utterances are presented in temporal order without a thread-like structure. Context is mostly established by consecutive utterances (Li et al., 2021). The transient nature of live-stream utterances en-\ncourages fast responses, and encourages producing multiple short comments that may be more prone to typos (70% of comments are made up of < 4 words). Figure 1 shows an illustration of the contrasting temporal and length patterns between the asynchronous and synchronous platforms.\nOwing to these different characteristics, we find that previous approaches for detecting norm violations are ineffective for live-streaming platforms. To address this limitation, we present the first NLP study of detecting norm violations in live-stream chats. We first establish norms of interest by collecting 329 rules from Twitch streamers\u2019 channels and define 15 different fine-grained norm categories through an iterative coding process. Next, we collect 4,583 moderated chats and their corresponding context from Twitch live streams and annotate them with these norm categories (\u00a72.1- \u00a72.3). With our data, we explore the following research questions: (1) How are norm violations in live-stream chats, i.e. synchronous conversations, different from those in previous social media datasets, i.e. asynchronous conversations?; (2) Are existing norm violation or toxicity detection models robust to the distributional shift between the asynchronous and synchronous platforms? (\u00a73.1, \u00a73.3); and (3) Which features (e.g., context and domain knowledge) are important for detecting norm violation in synchronous conversations? (\u00a73.2)\nFrom our explorations, we discover that (1) livestream chats have unique characteristics and norm violating behavior that diverges from those in previous toxicity and norm-violation literature; (2) existing models for moderation perform poorly on detecting norm violations in live-stream chats; and (3) additional information, such as chat and video context, are crucial features for identifying norm violations in live-stream chats. We show that incorporating such information increases inter-annotator agreement for categorizing moderated content and that selecting temporally proximal chat context is crucial for enhancing the performance of norm violation detection models in live-stream chats."
        },
        {
            "heading": "2 NormVio-RT",
            "text": "To investigate norm-violations in live-stream chat, we first collect Norm Violations in Real-Time Conversations (NormVio-RT), which contains 4,583 norm-violating comments on Twitch that were moderated by channel moderators.5 An overview of our\n5Please contact the authors for the anonymized study data.\ndata collection procedure is illustrated in Figure 2. We first select 200 top Twitch streamers and collect moderated comments from their streamed sessions (\u00a72.1). To understand why these chats are moderated, we collect chat rules from these streamers and aggregate them to define coarse and fine-grained norm categories (\u00a72.2). We design a three-step annotation process to determine the impact of the chat history, video context, and external knowledge on labeling decisions (\u00a72.3). Lastly, we present analysis of the collected data (\u00a72.4)."
        },
        {
            "heading": "2.1 Data Collection",
            "text": "We collected data using the Twitch API and IRC6 from the streamers with videos that are available for download among the top 200 Twitch streamers as of June 20227. We specifically looked for comments that triggered a moderation event during a live stream (e.g. user ban, user timeout), and collected the moderated comment and the corresponding video and chat logs up to two minutes prior to the moderation event. Logs of moderated events from August 22, 2022 to September 3, 2022 were collected. We excluded comments that were moderated within less than 1 second of being posted, as they are likely to have been moderated by bots rather than humans."
        },
        {
            "heading": "2.2 Norm Categorization",
            "text": "Twitch streamers can set their own rules for their channels, and these channel-specific rules are essential for understanding why comments were moderated. We first collect 329 rules from the top\n6https://github.com/TwitchIO/TwitchIO 7https://twitchtracker.com/channels/viewership/english\n200 Twitch streamers\u2019 channels. Next, following Fiesler et al. (2018), we take an iterative coding process such that the authors of this paper individually code for rule types with certain categories, come together to determine differences and then repeat the coding process individually. With this process, we aggregated similar rules into 15 different finegrained level norm categories (e.g., controversial topics, begging) and cluster multiple fine-grained categories into 8 different coarse-grained norm categories (e.g., off-topic). To better understand the targets of offensive comments in the HIB (Harassment, Intimidation, Bullying) class, we added an additional dimension to consider whether the target is the broadcaster (streamer), participants in the channel (e.g., moderators and viewers), or someone not directly involved in the broadcast. We asked annotators to assign \u201cIncivility\u201d to cases where annotators do not believe that a specific pre-defined rule type has been violated although moderated. Examples of \u201cIncivility\u201d are provided in Appendix A.4. Table 1 shows the resulting norm categories and corresponding fine-grained norms with examples."
        },
        {
            "heading": "2.3 Violated Norm Type Annotation",
            "text": "We recruited three annotators who are fluent in English and spend at least 10 hours a week on live streaming platforms to ensure that annotators understood live streaming content and conventions. Their fluency was verified through several rounds of pilot annotation work. Internal auditors continuously conducted intermittent audits to ensure that annotators fully understood the guidelines.\nAnnotators were asked to annotate each mod-\nLastly, to examine how much external knowledge matters in understanding comments on live streaming platforms, we asked annotators to (1) indicate whether external knowledge is necessary to understand why a comment triggered a moderation event and if so (2) describe what that knowledge is. We focus on two types of external knowledge: platform- and streamer-specific. Platform-specific knowledge includes the implicit meaning of partic-\nular emojis, emotes, and slang that are commonly used on Twitch. Streamer-specific knowledge involves the streamer\u2019s personal background and previous streaming sessions. As shown in Table 2, we provide templates for each type that annotators can easily fill out (More details in Appendix A.3)."
        },
        {
            "heading": "2.4 Data Statistics and Analysis",
            "text": "General Observations We identified three characteristics that distinguish real-time live-streaming chat from other domains. First, the majority of comments are very short; 70% of comments are made up of < 4 words. Additionally, they are often very noisy due to the real-time nature of communication, which leads to a high number of typos, abbreviations, acronyms, and slang in the comments. Lastly, some comments use unusual visual devices such as ASCII art and \u201call caps\u201d, to make them more noticeable. This is because each comment is visible only for a short time in popular streams (on average, there are around 316 chats per minute for the streamers in our data). The chat window in live streaming platforms can only display a limited number of comments, so viewers are incentivized to use visual devices to draw the streamer\u2019s attention in these fast-paced conditions.\nFalse positives in data. We find that the \u201cIncivility\u201d case contains many false positives, as they include cases that seem to have been moderated for no particular reason. We asked annotators to put all miscellaneous things into the \u201cIncivility\u201d category, and also to mark as \u201cIncivility\u201d if they\ncould not identify any reason for the moderation. We found that many cases are not identifiable, as shown in Table 3. It is natural that many cases are non-identifiable in stage 1, as annotators are only given the moderated comment and no context. However, the 7.45% non-identifiable cases that remain even after stage 3 could be false positives, or they could be cases where the moderation event occurred more than two minutes after a problematic comment was made.\nContext improves inter-annotator agreement. Interestingly, providing context helps mitigate annotator bias, as shown by the increase in interannotator agreement from stage 1 to stages 2 and 3 in Table 4. Here, the exact match determines whether all three annotators have exactly the same rules; partial match determines whether there is at least one intersection rule between three annotators; and majority vote chooses the rule types that were selected by at least two people. Also, non-identifiable and disagreement cases drop significantly when the contexts are given as shown in Table 3. Similarly for determining rule types, context also helps annotators identify targets for HIB and reduces inconsistencies between annota-\ntors. Our observation emphasizes the importance of context in synchronous communication and differs from previous findings that context-sensitive toxic content is rare in asynchronous communication (Pavlopoulos et al., 2020; Xenos et al., 2021). Analysis details are in Appendix A.2.\nExternal knowledge helps annotations. To investigate the impact of external knowledge on annotators\u2019 labeling decisions, we compare annotations made with and without external knowledge provided. For examples with knowledge statements, we expect to see differences in annotation if external knowledge is necessary to comprehend why they were moderated. Statistics show that 296 examples (6.6%) require knowledge, with 183 examples requiring streamer knowledge and 187 examples requiring platform knowledge. Note that there are some examples require both. Details of statistics and examples are presented in Appendix A.3.\nNorm Category Distribution Table 3 shows the norm category distribution of streamers\u2019 rules and the moderated comments. While the categories are not directly comparable to the ones defined in NormVio for Reddit (Park et al., 2021), we identified a few similar patterns. First, in both domains, Harassment and Incivility (i.e., Discrimination, HIB, Incivility) take up a significant portion of the entire set of norm violations. Also, the two domains show a similar pattern where rules for Off-Topic, Inappropriate Contents, and Privacy exist but are relatively less enforced in practice. However, we also found that the two domains differ in various ways. For example, Spam and Meta-Rules cover significantly higher portions of both rules and moderated comments on Twitch than on Reddit. On the other hand, there are fewer rules about content on Twitch, which implies that streamers are less concerned about the content of the comments than Reddit community moderators. As our data shows that norm-violating comments on live chats exhibit distinctive rules and patterns, it suggests that the existing norm violation detection systems may not perform well without domain adaptation to account for these distributional differences. We examine this hypothesis empirically in the following section and suggest appropriate modeling adjustments to better detect toxicity for real-time comments."
        },
        {
            "heading": "3 Toxicity Detection in Live-stream Chat",
            "text": "In this section, we first check whether norm violation and toxicity detection models are robust\nto the distributional shift from asynchronous conversations to synchronous conversations and vice versa, and identify how important the context or domain knowledge are for detecting toxicity and norm violation in synchronous conversations."
        },
        {
            "heading": "3.1 Performance of Existing Frameworks.",
            "text": "To examine the difference in toxicity detection between asynchronous and synchronous communication, we investigate whether existing toxicity detection models are effective for synchronous communication. We evaluate the performance of four existing tools on NormVio-RT: Google\u2019s Perspective API (Lees et al., 2022)8, OpenAI content filter9, OpenAI moderation (Markov et al., 2022)10, and a RoBERTa-large model fine-tuned on machine-generated toxicity dataset called ToxiGen (Hartvigsen et al., 2022). We only use examples from the discrimination and HIB categories in NormVio-RT, as they are most similar to the label space that the existing models are trained for (e.g., hateful content, sexual content, violence, self-harm, and harassment). Categories are determined based on the stage 1 consolidated labels, as we do not provide any context to the model. Additionally, we select an equal number of random chats from the collected stream to construct negative examples. To ensure the quality of negative examples, we only select chats that are not within two minutes prior to any moderation event as they are less likely to contain norm violating chats. We also only select chats from users who have never been moderated in our data. To obtain the predictions from the models, we check whether toxicity score is greater than or equal to 0.5 for Perspective API, and for OpenAI, check the value of the \u201cflagged\u201d field which indicates whether OpenAI\u2019s content policy is violated. We use binary classification outputs for ToxiGen.\nTable 5 shows the results obtained from 2,102 examples with 1,051 examples each for toxic and non-\n8https://perspectiveapi.com/ 9https://beta.openai.com/docs/models/content-filter\n10https://beta.openai.com/docs/api-reference/moderations\ntoxic messages. The results illustrate that while existing models do not frequently produce false positives (high recall), they perform poorly in detecting toxic messages found in synchronous chats, with a detection rate of only around 55% at best (low precision)."
        },
        {
            "heading": "3.2 Norm Classification in NormVio-RT.",
            "text": "To understand the model\u2019s ability to detect norm violations and how additional information can affect detection, we train binary classification models for each category with different types of context including conversation history, broadcast category, and rule description following Park et al. (2021).\nExperimental Setup. For each coarse-level category, we train a RoBERTa-base model with a binary cross entropy loss to determine whether the message is violating the certain norm or not. Following Park et al. (2021), we perform an 80-10-10 train/dev/test random split of moderated messages and add the same number of unmoderated messages in the same split. Next, for each binary classification, we consider the target category label as 1 and others as 0 and construct a balanced training data set. Appendix B (See Table 12). Here, the labels are based on stage 3.\nTo examine how context affects model performance, we experiment with four model variants with different input context: (1) Single user context is only the chat logs of the moderated user that took place up to two minutes before the moderation event; (2) Multi-user context (event) is N messages that directly precede the moderation event, regardless of whether it belongs to the moderated user; (3) Multi-user context (utterance) is N messages that directly precedes the single utterance, which is the moderated user\u2019s last message before the moderation event (i.e., chat 3 in Figure 3).; (4) Multi-user context (first) is the first N messages of the collected two-minute chat logs. The intuition for this selection is that the moderation event may have taken place much earlier than\nthe moderation event. In all the Multi-user contexts, we use N = 5; (5) Broadcast category is the category that streamers have chosen for their broadcast. It usually is the title of a game or set to \u201cjust chatting\u201d; and (6) Rule text is a representative rule example shown in Table 1. The rule text is only used for training examples because it is not possible to know which rule was violated for unseen examples and we use randomly selected rule text for unmoderated negative examples in training examples. All contexts are appended to the input text (single utterance) with a special token ([SEP]) added between the input text and the context. Chat logs for multi-user context and single-user context are placed sequentially with spaces between chats. Training details and data statistics are presented in Appendix B.\nExperimental Results. Table 6 presents performance of norm classification for coarse-level norm categories. \u201cAll\u201d refers to binary moderation detection, whether the message is moderated or not, and not the specific norm type. First, we can see that additional context improves the performance of \u201cAll,\u201d but context does not consistently improve the performance of category-specific norm classifiers. For example, context reduces performance for categories where the issues are usually limited to the utterance itself (e.g., discrimination and privacy). In contrast, categories that rely on the relationships between utterances, such as HIB and incivility, show improved performance with con-\ntext. Secondly, multi-user context performs quite well compared to the other contexts, indicating that a more global context that includes utterances from other users helps determine the toxicity of target utterances. Lastly, the strong performance of Multiuser context (first) suggests that earlier messages in the two-minute window are more important, meaning that the temporal distance between the moderation event and the actual offending utterance may be substantial in many cases. Thus, our results encourage future efforts on developing a more sophisticated approach for context selection.\nAvailability of Context. To compare human decisions with those of our models, we conduct experiments varying the context available to annotators and models. For example, we expect models trained with only single utterances to perform best when using stage 1 (utterance only) labels as ground-truth labels since humans are also not given any context at stage 1. Indeed, in Figure 4, using the stage 1 labels as the ground truth labels yields the best performance for a model trained without any context, while using the stage 2 (context) labels as the ground truth labels shows the best performance for a model trained with previous chat history. Since our experiments only handle text inputs, it is not surprising that using stage 3 (video) labels as ground-truth labels yields worse performance than using stage 2 labels. However, interestingly, the gap is not large, which indicates that gains from a multi-modal model that incorporates information from the video may be small and that single modality (text-only) models can be sufficient for the majority of moderation instances.\nContext Size. To understand how the amount of available context affects moderation performance, we compare the multi-user context configurations with various number of messages from one to 25. Figure 5 demonstrates that 15 to 20 messages prior\nto the moderated user\u2019s message helps with moderation performance the most (See utterance and first). However, increasing the number of messages that directly precede the moderation event actually lowers moderation performance (See event). It may be that most of this context serves as noise."
        },
        {
            "heading": "3.3 Distribution Shift in Norm Classification.",
            "text": "Existing tools often focus on identifying harmful speech, but NormVio (Park et al., 2021) also considers a wider range of norm-violating comments on Reddit, similar to NormVio-RT but in a different domain. We compare NormVio and NormVioRT by evaluating the performance of a model finetuned on NormVio with NormVio-RT, and vice versa, to examine the impact of distribution shift between these domains. We choose six coarse-level categories that overlap between the two, as shown in Table 7. To measure with-context performance, we use the previous comment history for Reddit and multi-user context (utterance) for Twitch to simulate the most similar setup in both domains. Overall, experimental results show a pronounced distribution shift between Reddit (asynchronous) and Twitch (synchronous). Interestingly, models trained on Twitch are able to generalize better than models trained on Reddit despite having 6x less training data. Specifically, models trained using the out-of-domain Twitch+context data perform comparably on the Reddit test set to those trained using in-domain Reddit+context data."
        },
        {
            "heading": "4 Related Work",
            "text": "Toxicity Detection Most toxic language data consists of explicit hate speech consisting of hate lexicons (Waseem and Hovy, 2016; Davidson et al., 2017; Founta et al., 2018; Basile et al., 2019), group identifiers (Warner and Hirschberg, 2012; Kennedy et al., 2020), and hateful phrase (Silva\net al., 2016) in asynchronous communication (e.g., Twitter, Reddit). However, models trained on such data may have spurious correlations that result in many false positives (e.g., group identifiers) (Sap et al., 2019; Kennedy et al., 2020; Hartvigsen et al., 2022; Lee et al., 2022). To reduce such bias, implicit hate speech, toxic language use without any explicit hateful words or phrases, has been explored (Kennedy et al., 2018; ElSherief et al., 2021; Hartvigsen et al., 2022).\nBeyond Binary Toxicity Detection Treating toxicity detection as a binary task may not be enough to understand nuanced intents and people\u2019s reactions to toxic language use (Jurgens et al., 2019; Rossini, 2022). To holistically analyze toxicity, recent works take a more fine-grained and multidimensional approach: (1) Explainability explains why a particular chat is toxic with highlighted rationales (Mathew et al., 2021), free-text annotations of implied stereotype (Sap et al., 2020; ElSherief et al., 2021; Sridhar and Yang, 2022), or pre-defined violation norms (Chandrasekharan et al., 2018; Park et al., 2021). These explanations can be used not only to improve the performance of the toxicity detection model, but also to train models that generate explanations; (2) Target identification finds the targets of toxic speech, such as whether the target is an individual or a group, or the name of the group (e.g., race, religion, gender) (Ousidhoum et al., 2019; Mathew et al., 2021); (3) Context sensitivity determines toxicity by leveraging context, such as previous tweets (Menini et al., 2021), comments (Pavlopoulos et al., 2020; Xenos et al., 2021) or previous sentences and phrases within the comments (Gong et al., 2021). They show that context can alter labeling decisions by annotators, but that it does not largely impact model performance (Pavlopoulos et al., 2020; Xenos et al., 2021; Menini et al., 2021); (4) implication understands veiled toxicity that are implied in codewords and emojis (Taylor et al., 2017; Lees et al., 2021), and\nmicroaggressions that subtly expresses a prejudice attitude toward certain groups (Breitfeller et al., 2019; Han and Tsvetkov, 2020); and (5) Subjectivity measures annotation bias (Sap et al., 2022) and manage annotator subjectivity involved in labeling various types of toxicity, which arises from differences in social and cultural backgrounds (Davani et al., 2022). In this paper, we analyze the toxicity of synchronous conversations in terms of the aforementioned dimensions by identifying explanation of toxicity as a form of norm categories (explainability), finding targets of HIB words (target identification), leveraging context for both annotation and modeling (context sensitivity), asking annotators for implied knowledge statement (implication), and examining how human decisions align with machine decisions under different amounts of information (subjectivity)."
        },
        {
            "heading": "5 Conclusion",
            "text": "In this paper, we analyzed messages flagged by human moderators on Twitch to understand the nature of norm violations in live-stream chats, a previously overlooked domain. We annotated 4,583 moderated chats from live streams with their norm violation category and contrasted them with those from asynchronous platforms. We shed light on the unique characteristics of live-stream chats and showed that models trained with existing data sets perform poorly in detecting toxic messages in our data, which motivates the development of specialized approaches for the synchronous setting. Our experiments established that selecting relevant context is an important feature for detecting norm violations in the synchronous domain. we hope our work will help develop tools that enable human moderators to efficiently moderate problematic comments in real-time synchronous settings and make the user-experience in these communities more pleasant."
        },
        {
            "heading": "6 Limitations",
            "text": "Our data, analysis, and findings have certain limitations. Our research is restricted to the English language and the Twitch platform, although the methods used to detect rule violations in live-stream chat and collect data can be adapted to other languages. Additionally, we recognize that our annotators were recruited from one country, which may result in a lack of diversity in perspectives and potential societal biases. Furthermore, we established a 2-minute context window for each moderated comment within the moderation event, but this may not capture all relevant context.\nAdditionally, the small size of our humanannotated data may limit the generalizability of our findings to other situations. We recognize that our data set may not represent all instances of rule violations in real-world scenarios. This may be due to the biases of the moderators in choosing which users or comments to moderate or prioritizing certain types of violations over others. Also, the randomly sampled data we annotated may not be representative of the entire population and the imbalance of rule violation classes in our data set may not contain enough samples of rare categories to make definitive conclusions.\nOur experimental results indicate that models trained to detect norm violation using our data are far from perfect and may produce errors. When such models are used in real world applications, this can result in overlooking potentially problematic comments or incorrectly flagging nonproblematic comments. Therefore, we recommend using AI-based tools to assist human moderators rather than trying to fully replace them. Practitioners should also be aware that there may be users with malicious intent who try to bypass moderation by making their comments appear innocent. By employing moderation models, malicious users may be better able to craft toxic messages undetectable by existing models. As mentioned above, having a final step of human review or verification of the model output will be beneficial. Additionally, it may be necessary to continuously update the model and limit public access to it."
        },
        {
            "heading": "7 Ethical Considerations",
            "text": "We took several steps to ensure that our data collection was ethical and legal. We set the hourly rate of compensation for workers at $16.15, which was well above the country\u2019s minimum wage at the time ($7.4). To ensure the safety and well-being of\nour workers, we maintained open communication channels, allowing them to voice any question, concerns, or feedback about the data annotation. This also helped to improve the quality of the collected data as we promptly addressed issues reported by workers throughout the process. We also give each annotation instance enough time so that we do not pressure annotators (40 days for 4,583 instances). We did not collect any personal information from annotators and we did not conduct any experiments with human subjects.\nWe confirm that we collected and used chats, also referred to as user content, in accordance with Twitch\u2019s Terms of Service and do not publicly release the data as it may be in violation of laws against unauthorized distribution of user content. However, we intend to make the platform-specific knowledge statements we compiled available to support future research on real-time chat in the livestreaming domain. During the collection process, we used the official Twitch API to monitor and retrieve chats.\nLastly, we want to emphasize that careful consideration must be given to user privacy when using moderation events to study norm violations. While users may be aware that their comments can be viewed by others in the chat room, researchers must also understand that users have the right to request not to be included in the data and establish a mechanism for users to contact researchers to have their data removed, and refrain from publicly releasing the data and instead share it on a need-to-know basis to control who has access to the data."
        },
        {
            "heading": "8 Acknowledgement",
            "text": "We would like to thank to Yeeun Shin (SoftlyAI) for managing the data annotation process and Kyumin Park (SoftlyAI) for the development of the web annotation framework. Datumo, known as SELECTSTAR in South Korea, provided a crowdsourcing platform for the annotation of the data.\nThis work was funded in part by the Defense Advanced Research Projects Agency (DARPA) under Contract No. N660011924033, Contract No. HR00112290106, and Contract No. HR00112290025, and with support from the Keston Exploratory Research Award. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of DARPA, ARO or the U.S. Government."
        },
        {
            "heading": "A Annotation Details",
            "text": "We engage in active discussions with annotators and provide detailed feedback after multiple rounds of pilot study to ensure the data quality.\nA.1 Annotation UI To make it easy for annotators to annotate with various types of contexts, we create an annotation tool. The annotation tool has three options and the user can select each option for each step annotation. Figure 6 shows the UI for step 1 which shows only the user\u2019s last chat (bad utterance) before the moderation event. Figure 7 shows chat logs up to two minutes ago based on the moderation events on multi user context panel. To make it easier for annotators to find previous chats from moderated users, we create single user context panel to only display chat logs of the moderated user in multi user context. Figure 8 shows both chat logs and video context. The video context shows 1-minute clipped video around the moderation event.\nA.2 Annotation Consolidation To determine the final label for each moderated event, we aggregate the labels of annotators using a majority vote with heuristic rules. Each annotator ai identifies a list of violated rules L = {l1, l2, \u00b7 \u00b7 \u00b7 , lk} for a moderated event e at each stage k = {1, 2, 3}. Here, we don\u2019t consider the target for HIB. We first evaluate the percentage agreement to measure inter-annotator agreement in each stage by exact match and partial match. The exact match determines whether all three annotators have exactly the same rules (La1 = La2 = La3) and partial match determines whether there is at least one intersection rule between three annotators ((La1 \u2229 La2 \u2229 La3) > 0). Table 4 shows the inter-annotator agreement percentage. We find that 98% of agreements from exact match are single label cases (i.e., 98% of exact matches have only one label) and many disagreements are resolved using the partial match method. 92% disagreements that persist even with the partial match method are the case where one or two annotators marking a comment as violating the \u201cIncivility\u201d rule while the others do not. Finally, to determine the gold label using the annotations from the three annotators, we apply a majority vote approach, choosing the rule types that were selected by at least two people. We discard approximately 3% of events that cannot be consolidated because all three annotators provided different labels.\nTarget Agreement for HIB For cases consolidated as HIB with the majority vote, we further analyze the inter-annotator agreement of target labels among annotators who have marked them as HIB. In cases where the annotator was unable to identify the target, we asked them to mark the target as \u201cnon-identifiable\u201d. Table 8 shows that most HIB words (92.05%) are directed at someone in the broadcast such as the broadcaster or viewers.\nA.3 Knowledge Statement. Template. Table 2 shows templates for the knowledge statement. For platform knowledge, annotators should fill out the span and what span means, and choose whether the span is emoji or text. For streamer knowledge, annotators should fill out the name of streamer and his/her personal background that may need to decide the label.\nData Statistics. Table 9 shows data statistics of knowledge statement for each coarse-level norm category. Statistics show that 296 examples (6.6%) require knowledge, with 183 examples requiring streamer knowledge and 187 examples requiring platform knowledge. Note that there are some examples require both. Statistics demonstrate that HIB and incivility require domain knowledge the most to understand the meaning behind them.\nKnowledge statement examples. For each coarse-level norm category, we present its examples (See Table 10).\nA.4 Examples of Incivility.\nTable 11 presents examples of chat moderation by streamers where the underlying reason for moderation is not apparent. The cases highlight potentially uncomfortable situations that streamers may encounter."
        },
        {
            "heading": "B Experimental Setup Details",
            "text": "Each fine-tuned experiment uses 1 NVIDIA RTX A5000 GPU and uses FP16. We implement models using PyTorch (Paszke et al., 2019) and Huggingface Transformers (Wolf et al., 2019). We use the Adam optimizer with a maximum sequence length of 256 and a batch size of 4. We set 100 epochs and validate the performance every 100 steps. The stopping criteria is set to 10. For each data, we searched for the best learning rate for our model out of [1e-5, 2e-5, 5e-5, 1e-4, 3e-4]. Then, we report the average score of 3 runs by different random seeds (42, 2023, 5555). Each run takes 10 to 30 minutes. To determine the data distribution ratio between positives and negatives in the training data, we searched for the best distribution out of [1:1, 1:2, 1:5, Original] by random negative sampling. As shown in Table 12, we found that the evenly distribution (1:1) shows the most stable performance with the lowest standard deviation under with and without context. Data statistics for both Twitch and Reddit (Park et al., 2021) are presented in Table 13-14. Note that we report the number of data statistics after sampling the same number of negative samples as positive samples."
        },
        {
            "heading": "C Ablation Study",
            "text": "C.1 Context Arrangement To understand how the context arrangement in the input affects the performance, we conduct experiments with multiple variants of context arrangement on moderation detection (See Table 15). First, the results show that randomly shuffled context consistently harm the performance. It indicates that context order matters, in contrast to the findings in dialog system study results (Sankar et al., 2019; He et al., 2021). Moreover, input as the sequential order of chats presented in the contextaware model (Pavlopoulos et al., 2020), or adding more contexts (e.g., broadcast category, rule text) degrade the performance. This indicates that the target text should always be placed first, and some contexts may not be helpful."
        }
    ],
    "title": "Analyzing Norm Violations in Live-Stream Chat",
    "year": 2023
}