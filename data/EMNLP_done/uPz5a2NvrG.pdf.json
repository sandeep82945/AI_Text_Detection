{
    "abstractText": "The automatic generation of medical reports plays a crucial role in clinical automation. In contrast to natural images, radiological images exhibit a high degree of similarity, while medical data are prone to data bias and complex noise, posing challenges for existing methods in capturing nuanced visual information. To address these challenges, we introduce a novel normal-abnormal semantic decoupling network that utilizes abnormal pattern memory. Different from directly optimizing the network using medical reports, we optimize visual extraction through the extraction of abnormal semantics from the reports. Moreover, we independently learn normal semantics based on abnormal semantics, ensuring that the optimization of the visual network remains unaffected by normal semantics learning. Then, we divided the words in the report into four parts: normal/abnormal sentences and normal/abnormal semantics, optimizing the network with distinct weights for each partition. The two semantic components, along with visual information, are seamlessly integrated to facilitate the generation of precise and coherent reports. This approach mitigates the impact of noisy normal semantics and reports. Moreover, we develop a novel encoder for abnormal pattern memory, which improves the network\u2019s ability to detect anomalies by capturing and embedding the abnormal patterns of images in the visual encoder. This approach demonstrates excellent performance on the benchmark MIMIC-CXR, surpassing the current state-of-the-art methods.1.",
    "authors": [
        {
            "affiliations": [],
            "name": "Guosheng Zhao"
        },
        {
            "affiliations": [],
            "name": "Yan Yan"
        },
        {
            "affiliations": [],
            "name": "Zijian Zhao"
        }
    ],
    "id": "SP:6231fd4c956f4936bd37b2a8f5b3122c4214671b",
    "references": [
        {
            "authors": [
                "Peter Anderson",
                "Xiaodong He",
                "Chris Buehler",
                "Damien Teney",
                "Mark Johnson",
                "Stephen Gould",
                "Lei Zhang."
            ],
            "title": "Bottom-up and top-down attention for image captioning and visual question answering",
            "venue": "Proceedings of the IEEE conference on computer vision",
            "year": 2018
        },
        {
            "authors": [
                "Satanjeev Banerjee",
                "Alon Lavie."
            ],
            "title": "Meteor: An automatic metric for mt evaluation with improved correlation with human judgments",
            "venue": "Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summariza-",
            "year": 2005
        },
        {
            "authors": [
                "Nori",
                "Javier Alvarez-Valle"
            ],
            "title": "Making the most of text semantics to improve biomedical vision\u2013language processing",
            "venue": "In Computer Vision\u2013 ECCV 2022: 17th European Conference,",
            "year": 2022
        },
        {
            "authors": [
                "Long Chen",
                "Zhihong Jiang",
                "Jun Xiao",
                "Wei Liu."
            ],
            "title": "Human-like controllable image captioning with verb-specific semantic roles",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16846\u201316856.",
            "year": 2021
        },
        {
            "authors": [
                "Qi Chen",
                "Chaorui Deng",
                "Qi Wu."
            ],
            "title": "Learning distinct and representative modes for image captioning",
            "venue": "Advances in Neural Information Processing Systems, volume 35, pages 9472\u20139485. Curran Associates, Inc.",
            "year": 2022
        },
        {
            "authors": [
                "Zhihong Chen",
                "Yaling Shen",
                "Yan Song",
                "Xiang Wan."
            ],
            "title": "Cross-modal memory networks for radiology report generation",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference",
            "year": 2021
        },
        {
            "authors": [
                "Zhihong Chen",
                "Yan Song",
                "Tsung-Hui Chang",
                "Xiang Wan."
            ],
            "title": "Generating radiology reports via memory-driven transformer",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1439\u20131449,",
            "year": 2020
        },
        {
            "authors": [
                "Marcella Cornia",
                "Matteo Stefanini",
                "Lorenzo Baraldi",
                "Rita Cucchiara."
            ],
            "title": "Meshed-memory transformer for image captioning",
            "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10578\u201310587.",
            "year": 2020
        },
        {
            "authors": [
                "Dina Demner-Fushman",
                "Marc D Kohli",
                "Marc B Rosenman",
                "Sonya E Shooshan",
                "Laritza Rodriguez",
                "Sameer Antani",
                "George R Thoma",
                "Clement J McDonald."
            ],
            "title": "Preparing a collection of radiology examinations for distribution and retrieval",
            "venue": "Journal",
            "year": 2016
        },
        {
            "authors": [
                "Zhiyuan Fang",
                "Jianfeng Wang",
                "Xiaowei Hu",
                "Lin Liang",
                "Zhe Gan",
                "Lijuan Wang",
                "Yezhou Yang",
                "Zicheng Liu."
            ],
            "title": "Injecting semantic concepts into endto-end image captioning",
            "venue": "Proceedings of the IEEE/CVF conference on computer vision and pat-",
            "year": 2022
        },
        {
            "authors": [
                "MD Zakir Hossain",
                "Ferdous Sohel",
                "Mohd Fairuz Shiratuddin",
                "Hamid Laga."
            ],
            "title": "A comprehensive survey of deep learning for image captioning",
            "venue": "ACM Computing Surveys (CsUR), 51(6):1\u201336.",
            "year": 2019
        },
        {
            "authors": [
                "Jie Hu",
                "Li Shen",
                "Gang Sun."
            ],
            "title": "Squeeze-andexcitation networks",
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition, pages 7132\u20137141.",
            "year": 2018
        },
        {
            "authors": [
                "Jeremy Irvin",
                "Pranav Rajpurkar",
                "Michael Ko",
                "Yifan Yu",
                "Silviana Ciurea-Ilcus",
                "Chris Chute",
                "Henrik Marklund",
                "Behzad Haghgoo",
                "Robyn Ball",
                "Katie Shpanskaya"
            ],
            "title": "Chexpert: A large chest radiograph dataset with uncertainty labels and expert comparison",
            "year": 2019
        },
        {
            "authors": [
                "Saahil Jain",
                "Ashwin Agrawal",
                "Adriel Saporta",
                "Steven QH Truong",
                "Du Nguyen Duong",
                "Tan Bui",
                "Pierre Chambon",
                "Yuhao Zhang",
                "Matthew P Lungren",
                "Andrew Y Ng"
            ],
            "title": "Radgraph: Extracting clinical entities and relations from radiology reports",
            "year": 2021
        },
        {
            "authors": [
                "Jaehwan Jeong",
                "Katherine Tian",
                "Andrew Li",
                "Sina Hartung",
                "Subathra Adithan",
                "Fardad Behzadi",
                "Juan Calle",
                "David Osayande",
                "Michael Pohlen",
                "Pranav Rajpurkar"
            ],
            "title": "Multimodal image-text matching improves retrieval-based chest x-ray report",
            "year": 2023
        },
        {
            "authors": [
                "Baoyu Jing",
                "Zeya Wang",
                "Eric Xing."
            ],
            "title": "Show, describe and conclude: On exploiting the structure information of chest X-ray reports",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6570\u20136580, Florence,",
            "year": 2019
        },
        {
            "authors": [
                "Baoyu Jing",
                "Pengtao Xie",
                "Eric Xing."
            ],
            "title": "On the automatic generation of medical imaging reports",
            "venue": "arXiv preprint arXiv:1711.08195.",
            "year": 2017
        },
        {
            "authors": [
                "Alistair EW Johnson",
                "Tom J Pollard",
                "Nathaniel R Greenbaum",
                "Matthew P Lungren",
                "Chih-ying Deng",
                "Yifan Peng",
                "Zhiyong Lu",
                "Roger G Mark",
                "Seth J Berkowitz",
                "Steven Horng"
            ],
            "title": "Mimic-cxr-jpg, a large publicly available database of labeled chest radiographs",
            "year": 2019
        },
        {
            "authors": [
                "Alex Kendall",
                "Yarin Gal",
                "Roberto Cipolla."
            ],
            "title": "Multi-task learning using uncertainty to weigh losses for scene geometry and semantics",
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition, pages 7482\u20137491.",
            "year": 2018
        },
        {
            "authors": [
                "Jin-Hwa Kim",
                "Kyoung-Woon On",
                "Woosang Lim",
                "Jeonghee Kim",
                "Jung-Woo Ha",
                "Byoung-Tak Zhang."
            ],
            "title": "Hadamard product for low-rank bilinear pooling",
            "venue": "arXiv preprint arXiv:1610.04325.",
            "year": 2016
        },
        {
            "authors": [
                "Ming Kong",
                "Zhengxing Huang",
                "Kun Kuang",
                "Qiang Zhu",
                "Fei Wu."
            ],
            "title": "Transq: Transformer-based semantic query for medical report generation",
            "venue": "Medical Image Computing and Computer Assisted Intervention\u2013MICCAI 2022: 25th International Con-",
            "year": 2022
        },
        {
            "authors": [
                "Harold W Kuhn."
            ],
            "title": "The hungarian method for the assignment problem",
            "venue": "Naval research logistics quarterly, 2(1-2):83\u201397.",
            "year": 1955
        },
        {
            "authors": [
                "Yehao Li",
                "Yingwei Pan",
                "Jingwen Chen",
                "Ting Yao",
                "Tao Mei."
            ],
            "title": "X-modaler: A versatile and highperformance codebase for cross-modal analytics",
            "venue": "In",
            "year": 2021
        },
        {
            "authors": [
                "Yehao Li",
                "Yingwei Pan",
                "Ting Yao",
                "Tao Mei."
            ],
            "title": "Comprehending and ordering semantics for image captioning",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 17990\u201317999.",
            "year": 2022
        },
        {
            "authors": [
                "Lukas Liebel",
                "Marco K\u00f6rner."
            ],
            "title": "Auxiliary tasks in multi-task learning",
            "venue": "arXiv preprint arXiv:1805.06334.",
            "year": 2018
        },
        {
            "authors": [
                "Chin-Yew Lin."
            ],
            "title": "Rouge: A package for automatic evaluation of summaries",
            "venue": "Text summarization branches out, pages 74\u201381.",
            "year": 2004
        },
        {
            "authors": [
                "Tsung-Yu Lin",
                "Aruni RoyChowdhury",
                "Subhransu Maji."
            ],
            "title": "Bilinear cnn models for fine-grained visual recognition",
            "venue": "2015 IEEE International Conference on Computer Vision (ICCV), pages 1449\u20131457.",
            "year": 2015
        },
        {
            "authors": [
                "Fenglin Liu",
                "Xian Wu",
                "Shen Ge",
                "Wei Fan",
                "Yuexian Zou."
            ],
            "title": "Exploring and distilling posterior and prior knowledge for radiology report generation",
            "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 13753\u2013",
            "year": 2021
        },
        {
            "authors": [
                "Shilong Liu",
                "Lei Zhang",
                "Xiao Yang",
                "Hang Su",
                "Jun Zhu."
            ],
            "title": "Query2label: A simple transformer way to multi-label classification",
            "venue": "arXiv preprint arXiv:2107.10834.",
            "year": 2021
        },
        {
            "authors": [
                "Jiasen Lu",
                "Caiming Xiong",
                "Devi Parikh",
                "Richard Socher."
            ],
            "title": "Knowing when to look: Adaptive attention via a visual sentinel for image captioning",
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition, pages 375\u2013383.",
            "year": 2017
        },
        {
            "authors": [
                "Luke Melas-Kyriazi",
                "Alexander M Rush",
                "George Han."
            ],
            "title": "Training for diversity in image paragraph captioning",
            "venue": "proceedings of the 2018 conference on empirical methods in natural language processing, pages 757\u2013761.",
            "year": 2018
        },
        {
            "authors": [
                "Ivona Najdenkoska",
                "Xiantong Zhen",
                "Marcel Worring",
                "Ling Shao."
            ],
            "title": "Variational topic inference for chest x-ray report generation",
            "venue": "Medical Image Computing and Computer Assisted Intervention\u2013MICCAI 2021: 24th International Conference, Strasbourg,",
            "year": 2021
        },
        {
            "authors": [
                "Farhad Nooralahzadeh",
                "Nicolas Perez Gonzalez",
                "Thomas Frauenfelder",
                "Koji Fujimoto",
                "Michael Krauthammer."
            ],
            "title": "Progressive transformer-based generation of radiology reports",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP",
            "year": 2021
        },
        {
            "authors": [
                "Yingwei Pan",
                "Ting Yao",
                "Yehao Li",
                "Tao Mei."
            ],
            "title": "X-linear attention networks for image captioning",
            "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10971\u2013 10980.",
            "year": 2020
        },
        {
            "authors": [
                "Kishore Papineni",
                "Salim Roukos",
                "Todd Ward",
                "WeiJing Zhu."
            ],
            "title": "Bleu: a method for automatic evaluation of machine translation",
            "venue": "Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 311\u2013318.",
            "year": 2002
        },
        {
            "authors": [
                "Han Qin",
                "Yan Song."
            ],
            "title": "Reinforced cross-modal alignment for radiology report generation",
            "venue": "Findings of the Association for Computational Linguistics: ACL 2022, pages 448\u2013458.",
            "year": 2022
        },
        {
            "authors": [
                "Tal Ridnik",
                "Emanuel Ben-Baruch",
                "Nadav Zamir",
                "Asaf Noy",
                "Itamar Friedman",
                "Matan Protter",
                "Lihi Zelnik-Manor."
            ],
            "title": "Asymmetric loss for multilabel classification",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision, pages",
            "year": 2021
        },
        {
            "authors": [
                "Aaron Van Den Oord",
                "Oriol Vinyals"
            ],
            "title": "Neural discrete representation learning",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin."
            ],
            "title": "Attention is all you need",
            "venue": "Advances in neural information processing systems, 30.",
            "year": 2017
        },
        {
            "authors": [
                "Oriol Vinyals",
                "Alexander Toshev",
                "Samy Bengio",
                "Dumitru Erhan."
            ],
            "title": "Show and tell: A neural image caption generator",
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3156\u20133164.",
            "year": 2015
        },
        {
            "authors": [
                "Jun Wang",
                "Abhir Bhalerao",
                "Yulan He."
            ],
            "title": "Crossmodal prototype driven network for radiology report generation",
            "venue": "Computer Vision\u2013ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23\u2013 27, 2022, Proceedings, Part XXXV, pages 563\u2013579.",
            "year": 2022
        },
        {
            "authors": [
                "Lin Wang",
                "Munan Ning",
                "Donghuan Lu",
                "Dong Wei",
                "Yefeng Zheng",
                "Jie Chen."
            ],
            "title": "An inclusive task-aware framework for radiology report generation",
            "venue": "Medical Image Computing and Computer Assisted Intervention\u2013MICCAI 2022: 25th In-",
            "year": 2022
        },
        {
            "authors": [
                "Zhanyu Wang",
                "Mingkang Tang",
                "Lei Wang",
                "Xiu Li",
                "Luping Zhou."
            ],
            "title": "A medical semantic-assisted transformer for radiographic report generation",
            "venue": "Medical Image Computing and Computer Assisted Intervention \u2013 MICCAI 2022: 25th International",
            "year": 2022
        },
        {
            "authors": [
                "Guanghui Xu",
                "Shuaicheng Niu",
                "Mingkui Tan",
                "Yucheng Luo",
                "Qing Du",
                "Qi Wu."
            ],
            "title": "Towards accurate text-based image captioning with content diversity exploration",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Shuxin Yang",
                "Xian Wu",
                "Shen Ge",
                "S. Kevin Zhou",
                "Li Xiao."
            ],
            "title": "Knowledge matters: Chest radiology report generation with general and specific knowledge",
            "venue": "Medical Image Analysis, 80:102510.",
            "year": 2022
        },
        {
            "authors": [
                "Changchang Yin",
                "Buyue Qian",
                "Jishang Wei",
                "Xiaoyu Li",
                "Xianli Zhang",
                "Yang Li",
                "Qinghua Zheng."
            ],
            "title": "Automatic generation of medical imaging diagnostic report with hierarchical recurrent neural network",
            "venue": "2019 IEEE international conference on data mining",
            "year": 2019
        },
        {
            "authors": [
                "Feiyang Yu",
                "Mark Endo",
                "Rayan Krishnan",
                "Ian Pan",
                "Andy Tsai",
                "Eduardo Pontes Reis",
                "Henrique Min Ho Lee",
                "Zahra Shakeri Hossein Abad",
                "Andrew Y Ng"
            ],
            "title": "Evaluating progress in automatic chest x-ray",
            "year": 2023
        },
        {
            "authors": [
                "Ke Yu",
                "Shantanu Ghosh",
                "Zhexiong Liu",
                "Christopher Deible",
                "Kayhan Batmanghelich."
            ],
            "title": "Anatomyguided weakly-supervised abnormality localization in chest x-rays",
            "venue": "Medical Image Computing and Computer Assisted Intervention \u2013 MICCAI 2022,",
            "year": 2022
        },
        {
            "authors": [
                "Jianbo Yuan",
                "Haofu Liao",
                "Rui Luo",
                "Jiebo Luo."
            ],
            "title": "Automatic radiology report generation based on multi-view image fusion and medical concept enrichment",
            "venue": "Medical Image Computing and Computer Assisted Intervention\u2013MICCAI 2019: 22nd In-",
            "year": 2019
        },
        {
            "authors": [
                "Yixiao Zhang",
                "Xiaosong Wang",
                "Ziyue Xu",
                "Qihang Yu",
                "Alan Yuille",
                "Daguang Xu."
            ],
            "title": "When radiology report generation meets knowledge graph",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 12910\u201312917.",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Report writing is a critical responsibility for radiologists. Automatic report generation is clinically significant as it alleviates the workload of physicians. Recently, substantial advancements have\n1Our code is available at https://github.com/kzzjk/ NADM\n*Corresponding authors.\nbeen achieved in this field (Wang et al., 2022c,a,b; Kong et al., 2022; Qin and Song, 2022). Nonetheless, generating radiological reports remains a formidable undertaking. However, several aspects require further exploration, such as: 1) the presence of pronounced visual and textual data biases. Within medical data, normal images predominate the section containing descriptions of normal utterances in the reports. 2) Unlike natural images, radiological images frequently exhibit high similarity, posing challenges in extracting fine-grained visual details. The presence of noise in reports impacts network optimization. The descriptions of similar semantics may vary among different doctors. For instance, in Figure 1a, \"cardiac\" and \"mediastinal\" are used, whereas in Figure 1b, \"cardiomediastinal\" is employed. Moreover, it is plausible for the syntax of the same description to exhibit significant variation. Some normal descriptions may not be mentioned due to physicians\u2019 practices. For instance, in Figure 1b, \"consolidation\" and \"effusion\" are referenced, whereas in Figure 1a, they are disregarded. These sources of noise significantly impact network optimization, yet only limited research has been dedicated to addressing this issue.\nTo address the aforementioned limitations, we propose a semantic decoupling network based on abnormal pattern memory for generating reports. In medical reports, the fundamental semantic structure comprises three components: anatomy, observation, and judgment. To mitigate the impact of noise in the reports, we optimize the visual extractor using abnormal semantics. This is based on the observation that abnormal semantics are relatively consistent, despite the occasional omission of normal descriptions and variations in similar semantic descriptions across reports from different doctors. We utilize RadGraph (Jain et al., 2021) to extract the semantics, focusing solely on the core semantic components while disregarding others. We employ a visual encoder based on higher-order feature interaction attention, enhancing the perception of fine-grained features through a bilinear pool (Kim et al., 2016), which has been shown to be effective in fine-grained classification (Lin et al., 2015). Furthermore, drawing inspiration from discrete variational autoencoder (Van Den Oord et al., 2017), we store the abnormal modalities of images and incorporate higher-order feature interaction processes to augment anomaly perception. During the report generation phase, we fuse the two modalities, semantic and visual, to jointly guide the report generation process. In the training phase, we employ convolution for image encoding and grid feature generation. Moreover, we utilize a variational selfencoder to capture anomaly modality memory and introduce two semantic branches atop it to predict both abnormal and normal semantics of the image during the visual attention phase. We argue that different words in the report possess varying levels of importance, with abnormal semantics being intuitively more significant than normal semantics. However, previous approaches (Jing et al., 2017; Yuan et al., 2019; Yin et al., 2019; Najdenkoska et al., 2021; Chen et al., 2021b; Wang et al., 2022c) tend to treat all report words equally. Therefore, we divide the reported words into four categories: normal/abnormal sentences and normal/abnormal semantics. Subsequently, we automatically learn distinct weighting parameters for optimization. In conclusion, our contributions are outlined as follows:\n(1) We introduce a network for report generation that leverages anomaly semantic extraction. This approach focuses on optimizing the visual extraction network solely using anomaly semantics,\neffectively mitigating the impact of noise and data bias present in reports.\n(2) We develop a visual encoder based on anomaly pattern memory, which enhances anomaly perception by explicitly memorizing abnormal patterns and incorporating them during higher-order interaction in the visual processing phase.\n(3) Our approach shows promising performance on MIMIC-CXR over multiple state-of-the-art methods."
        },
        {
            "heading": "2 Related Work",
            "text": "Image captioning involves generating relevant textual descriptions or topics for a given image. Early approaches utilized templates or retrieval-based methods for caption generation (Hossain et al., 2019). In recent years, encoder-decoder frameworks, primarily based on transformer architectures (Vaswani et al., 2017), have been widely employed to generate individual descriptive sentences from images (Cornia et al., 2020; Anderson et al., 2018; Chen et al., 2021a; Xu et al., 2021). Notably, anchor-frame-based methods for title generation have demonstrated accurate text description generation (Xu et al., 2021). However, a significant portion of the available data lacks annotations. To address this limitation, (Fang et al., 2022) proposed the use of semantic extraction to enrich information and improve caption generation. Furthermore, while most caption generation tasks focus on shorter descriptive discourse, existing approaches for very long utterances often overlook issues related to data bias (Melas-Kyriazi et al., 2018). The image captioning task has a more explicit objective, but the presence of complex semantic contexts, judgmental utterances, and noise problems in medical reports introduces additional challenges.\nMedical report generation, which falls under the category of image captioning, predominantly adopts an encoder-decoder framework. Various attention mechanisms and hierarchical LSTM models have been proposed to generate radiology reports (Jing et al., 2017; Yuan et al., 2019; Yin et al., 2019). Another approach involves constructing graphs based on medical knowledge and utilizing graph convolutional neural networks to enhance feature extraction (Zhang et al., 2020). To address the issue of modal bias, (Najdenkoska et al., 2021) introduced a set of latent variables as topics to guide sentence generation by aligning image and language patterns in the latent space. Modal\nalignment was further improved through the introduction of a cross-modal memory network by (Chen et al., 2021b), and later refined by (Wang et al., 2022a) with the proposition of a cross-modal prototype-driven network. Additionally, (Liu et al., 2021a) presented a framework that leverages both a priori and a posteriori data to enhance generative reporting. (Wang et al., 2022c) utilized semantic extraction to improve generation, they overlooked the problems of noise and data bias prevalent in the reports. In (Jing et al., 2019), the data bias problem is considered, where the noise of normal statements still affects the optimization of the model despite the separate generation of normal/abnormal statements."
        },
        {
            "heading": "3 The Proposed Method",
            "text": "The proposed architecture for the memory of anomaly patterns is the semantic decoupling network, consisting of three critical components: a visual extractor, a semantic extractor, and a decoder, where the visual extractor consists of an image encoder and a visual encoder. The overall structure of the network is illustrated in Figure 2. The visual extractor is responsible for converting images into detailed features with the goal of detecting anomalies and generating precise visual representations. The semantic extractor is divided into two parts: abnormal semantic extractor and normal semantic extraction. The purpose of abnormal semantic extractor is to identify semantics related to aberrations from the preceding visual representation and to capture normal semantics while considering the abnormal semantics. The decoder is responsible for processing the visual and semantic characteristics, and guides the report generation by deep fusion."
        },
        {
            "heading": "3.1 Image Encoder",
            "text": "To extract visual information from the images, we extracted features using the ResNet50 pre-trained by BioViL (Boecking et al., 2022). Given an image I , grid features are obtained by convolution Vl = ResNet50(I), and combined with global features Vg with position encoding Epos to compose the visual information Vc.\nVc = Concat (Vg, Vl) + Epos (1)"
        },
        {
            "heading": "3.2 Abnormal Mode Memory",
            "text": "To prevent anomalies from being overwhelmed, we are inspired by variational autoencoder (Van\nDen Oord et al., 2017), and introduce a discrete potential space called the memory codebook \u2126, where each entry in \u2126 corresponds to a potential embedding of an image pattern, and N denotes the total number of memorized patterns as a hyperparameter. In the case of an abnormal image, the features extracted by visual convolution search for a match within \u2126 to find the corresponding location of the memory pattern. While the nearest neighbor approach is a simple way to find a match, it poses the risk of pattern collapse (Chen et al., 2022). Thus, we adopt a dichotomous matching approach and apply the Hungarian algorithm (Kuhn, 1955) to resolve this issue. When memorizing, we look for a unique pattern in the memory space corresponding to each image encoding. The Hungarian algorithm is used to find a unique pattern match in the memory matrix by treating the image features with each pattern of the memory matrix as an ensemble matching problem. Specifically, we construct a bipartite graph using all local grid features Vg and pattern embeddings in \u2126, ensuring that the sizes of Vg and codebook k are matched using \u2205, and then determine the arrangement of the k elements \u03c4 \u2208 SN with the lowest allocation cost.\n\u03c4\u0302 = argmin \u03c4\u2208SN N\u2211 i \u2225\u2225V ig \u2212 \u2126\u03c4(i)\u2225\u22252 (2)"
        },
        {
            "heading": "3.3 Abnormal Pattern Enhancement Multi-Head Attention",
            "text": "The general self-attention approach only leverages the interaction of first-order features, which presents certain limitations. To achieve more detailed feature extraction, we adopt the bilinear attention module designed in accordance with (Pan et al., 2020). In the implementation, global features are queries Q, regional features are keys K, and values V . To incorporate abnormal patterns into higher-order interactions, we extend the key and value sets with mnemonics to encode and collect abnormal patterns from visual convolution. Our keys and values can be defined as K = [K,\u2126] and V = [V,\u2126] , where [, ] denotes splicing and \u2126 is all abnormal embeddings of the memory. Then a low-rank bilinear pooling is performed to obtain the joint bilinear query-key Bk and queryvalue BV by Bk = \u03c3 (WkK) \u2299 \u03c3 ( WkqQ ) and\nBV = \u03c3 (WV V)\u2299\u03c3 ( WVq Q ) ,Wv,W V q ,Wk and WKq are learnable parameters. \u03c3 denotes ReLU unit, and \u2299 represents Hadamard Product. We then compute the attention on the space and channels by\nprojecting each bilinear query key representation into the corresponding attention weights through two embedding layers, and then normalize using the softmax layer to introduce the spatial bilinear attention B\n\u2032 k = \u03c3 ( WkBBK ) ,then use another lin-\near layer to map B \u2032 k from Dc dimension to 1 dimension to obtain the spatial-wise attention weight \u03b1s.Meanwhile, we perform a squeeze-excitation operation (Hu et al., 2018) to generating channel attention \u03b2c = Sigmoid(Wc)B\u0304,Wc are learnable parameters and B\u0304 is an average pooling of B\n\u2032 k.The\nbasic layers that make up the visual encoder are as follows:\nAPE(Vc) = \u03b2c \u2299 \u03b1sBv (3)\nVif = AddLN (FFN (AddLN (APE (Vc)))) (4) Where V if , V i f represents the i-th layer of visual feature extraction. FFN denotes a fully connected feed-forward network, and AddLN represents the composition of a residual connection and a normalization layer."
        },
        {
            "heading": "3.4 Semantic Extractor",
            "text": "To extract semantic information, we treat it as a multi-label classification process, and employ RadGraph (Jain et al., 2021) to extract pseudo-medical concepts as ground-truth for this task. The transformer attention block is used to process the intermediate features V mf generated by the image encoder. To facilitate multi-label perception (MLP) network prediction, the [CLS] token is added to the image features, and its corresponding output represents the concept token. The vocabulary of concept tokens is the same as that used for the headings. Importantly, we predict concepts at the token level rather than the label level, to enable the multimodal decoding module to directly employ the top-K tokens for guidance reporting. The extraction process of anomaly semantics involves the following:\nAtt(Q,K, V ) = Softmax ( QKT\u221a\nd\n) V (5)\nMHA = [Att1, Att2...]W o (6) fm = MHA ( V mf , V m f , V m f ) (7)\nSa = AddLN (FFN (AddLN (fm))) (8)\nSKa = Emb(MLP (S 1 a)) (9)\nWhere Sa is the generated intermediate semantic prediction, S1a denotes the output corresponding to the classification flag [CLS], MLP represents the final classification layer, Emb represents the embedding layer, Att represents the attention layer, and SKa represents the top-k abnormal semantic concepts encoded at the end.\nAfter obtaining the abnormal semantics, we consider the acquisition of normal semantics as a conditional probability problem under the condition of abnormal semantics. Due to the complexity of the normal semantics, it does not serve as a basis for optimal visual extractor, so its update is only related to the normal semantic extractor. We add a [slot],representing the prediction of the normal semantics. The extractor process is as follows:\nfn = AddLN(MHA ( SKa , S K a , S K a ) ) (10)\nf \u2032 n = MHA(fn, V m f , V m f ) (11)\nSn = AddLN ( FFN ( AddLN ( f \u2032 n ))) (12)\nSKn = Emb(MLP (S 1 n)) (13)\nSf = Concat(S K a , S K n ) (14)\nWhere Sn is the generated intermediate semantic prediction, S1n denotes the output corresponding to the classification flag [slot] , SKn represents the top-k normal semantic concepts encoded at the end and Sf represents the final semantic output."
        },
        {
            "heading": "3.5 Decoder",
            "text": "We adopt the decoder structure introduced in (Li et al., 2022), which leverages the rich visual tokens Vf obtained from the visual encoder and the semantic tokens Sf extracted from the semantic extractor. The decoder combines both visual and semantic information to provide guidance for generating accurate and coherent sentences. Formally, let R = {r0, r1, ..., rt} denote report of t words generated. The decoder takes the report R as input and learns to predict the next word automatically and regressively, conditional on the visual Vf and semantic tokens Sf . We implemented the decoder as an Nd-stacked transformer block. It consists of a masked multiheaded attention layer Mask-MHA for modeling the overall textual context h \u2032 t of the previously generated word ht, and two crossed multiheaded attention layers that cross the visual and semantic tokens to trigger the generation of the cue\nhvt , respectively. subsequently, the previous contextual, semantic, and visual information is fused and encoded using Sigmoid to obtain the gating g.Taking the state at time t as an example, the equation is as follows:\nh \u2032 t = Mask-MHA(ht, ht, ht) (15)\nhvt = MHA(ht, Vf , Vf ) +MHA(ht, Sf , Sf ) (16)\ng = Sigmoid(Wg[h v t , h \u2032 t]) (17)\nht+1 = AddLN(gh \u2032 t + (1\u2212 g)hvt ) (18)"
        },
        {
            "heading": "3.6 Objective Function",
            "text": "Memory loss: the anomaly memory codebook serves the purpose of storing various anomaly information and interacting with image information. It is important to note that the learning process of the codebook should not affect the image coding. We want to use orthogonality to make the codebook learn as much information as possible about different modes. We use Sg to represent the gradient cutoff and E to denote the unit matrix. The loss function is defined as follows:\nLm = \u2225\u2225sg [V ic ]\u2212 q (V ic )\u2225\u222522 + \u2225\u2225\u2225\u2126\u22a4\u2126\u2212 E\u2225\u2225\u22252 (19) Semantic loss: in the semantic extraction task, our goal is to extract normal and abnormal semantics, which entails a multi-label classification process. However, the semantic distribution in medical reports is often heavily imbalanced, which can pose a challenge for standard multi-label classification approaches. To address this issue, we employ an asymmetric loss (Fang et al., 2022; Ridnik et al., 2021; Liu et al., 2021b; Ridnik et al., 2021), which has shown good performance in handling unbalanced problems.\nLs = asym ( P\u0303a,ya ) + asym ( P\u0303n,yn ) (20)\nP\u0303a, P\u0303n denotes the predicted probability distribution corresponding to the abnormal/normal semantics, respectively, and ya, yn denotes its corresponding label.\nReports loss: For the generated reports, we use minimizing the negative log-likelihood of the given image features, semantic features to train the model parameters. Although our positive anomaly semantic decoupling avoids the effect of noise on feature extraction, the effect of noise is still unavoidable in\nthe decoding phase of report generation. Thus, measuring the importance of different words with different weights to mitigate the effect of noisy normal descriptions makes the network focus more on critical information. We use RadGraph to classify the vocabulary in the report into normal/abnormal sentences, and normal/abnormal semantics. Inspired by the multitasking approach (Kendall et al., 2018; Liebel and K\u00f6rner, 2018), the importance of words is measured using uncertainty. Following (Liebel and K\u00f6rner, 2018), the four weights are learned automatically.\nL\u2032r = T\u2211 i=1 logP\u03b8 (Rt | R<t,Vf ,Sf ) (21)\nLr = 4\u2211\nm=0\n1\n\u03c32m Lmr + log(1 + \u03c3m) (22)\nRt denotes the report information at time t, Vf denotes the image information, Sf denotes the semantic information,\u03c3m denotes the different uncertainty parameters. Lmr denotes category m reports loss. The final losses are as follows:\nL = Lm + Ls + Lr (23)"
        },
        {
            "heading": "4 Experiment Settings",
            "text": ""
        },
        {
            "heading": "4.1 Datasets",
            "text": "We conducted numerical experiments on MIMICCXR2 (Johnson et al., 2019). MIMIC-CXR is the largest radiology dataset to date, including 473,057\n2https://physionet.org/content/mimic-cxr/2.0. 0/\nchest x-ray images. In our experiments, for a fair comparison, we used the official segmentation of MIMIC-CXR after the work (Chen et al., 2020) and excluded all sample reports that did not contain a description of medical observations. We only focus on the finding part of the medical report."
        },
        {
            "heading": "4.2 Evaluation Metrics",
            "text": "We used the widely used BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), and ROUGE-L (Lin, 2004), computed by the standard evaluation toolkit 3. In particular, BLEU and METEOR are proposed for machine translation evaluation. ROUGE-L was designed to measure the quality of the summary. For report generation, the predictive accuracy of the disease should also be considered. Therefore, we used clinical efficiency (CE) metrics to express the performance of our model. We used CheXpert (Irvin et al., 2019)4 to\n3https://github.com/tylin/coco-caption 4https://github.com/MIT-LCP/mimic-cxr/tree/\nmaster/txt/chexpert\nextract disease labels from real reports and model predictions to calculate precision, recall, and F1 scores.\nAn important note: the implementation details, dataset splits, preprocessing steps, and additional experiments are described in the supplementary materials."
        },
        {
            "heading": "5 Results and Analysis",
            "text": ""
        },
        {
            "heading": "5.1 Comparison with Previous Studies",
            "text": "To further validate the effectiveness of our method, we compare our proposed method with conventional image captioning works, e.g. S&T (Vinyals et al., 2015), AdaAtt (Lu et al., 2017), TopDown (Anderson et al., 2018), and the ones proposed for the medical domain, e.g. R2Gen (Chen et al., 2020), PPKED (Liu et al., 2021a), M2TR (Nooralahzadeh et al., 2021), R2GenCMN (Chen et al., 2021b), XProNet (Wang et al., 2022a), GSKET (Yang et al., 2022), R2GenRL (Qin and Song, 2022) and MSAT(Wang et al., 2022c).The results on S&T (Vinyals et al., 2015), AdaAtt (Lu et al., 2017), TopDown (Anderson et al., 2018) from (Chen et al., 2020), and the rest of the results were cited from the original paper. Table 1 shows the NLG metrics and Table 2 shows the CE metrics. As can be seen, thanks to our good framework of positive anomaly separation and excellent feature extraction ability. We achieve a large improvement\nin both metrics and obtain SOTA results. This proves the superiority of our method."
        },
        {
            "heading": "5.2 Ablation Study",
            "text": "In this section, we performed ablation experiments on the MIMIC dataset to investigate the contribution of each component of our method. Table 3 shows the experimental results. Bio denotes the initialization of the ResNet network weights using BioViL (Boecking et al., 2022). DE denotes the truncated reports loss on the visually extracted gradients. SE denotes the semantic extractor structure with normal and abnormal separation. APE denotes the abnormal memory extraction structure. WE denotes the report loss weight adjustment.We investigated the following variants:\nw/o Bio indicates that the model for ResNet50 randomly initializes the weight parameters and leaves the rest of the structure unchanged. w/o SE indicates that instead of using decoupled extraction semantics, a single semantic extractor branch is taken and normal and abnormal semantics are predicted simultaneously. w/o APE indicates that instead of using the structure of APE-MHA, the visual encoder is replaced with the structure of selfattention MHA. w/o WE indicates that instead of weight the report loss, but treat all words in the report equally.\nContribution of each component. We conducted an analysis to evaluate the importance of different components in our proposed method. We found that all key components play a critical role in achieving high performance. Removing any of these components results in significant performance degradation. Specifically, if we do not initialize our visual extractor with BioVil, we observe a reduction in performance. This underscores the importance of BioVil in providing prior information by aligning images with reports, thereby reducing the modal differences. This also suggests that some alignment aspects could potentially be\nintegrated with our method to further boost performance. Additionally, we found that the decoupling structure and APE-MHA are crucial components, and their absence results in the largest performance degradation. This highlights the significance of decoupled learning, which mitigates the negative impact of the uncertainty noise contained in normal semantics on predicting abnormal semantics. Furthermore, our proposed abnormal modal memory structure enhances the perception of fine-grained images and improves model performance. We also observed that WE enhances the report generation process by optimizing the grouping of words in the report and assigning different weights to them. This allows the decoder to focus more on the key semantics, resulting in better quality reports. Overall, our analysis demonstrates the effectiveness and importance of each component in our proposed method.\nImpact of memory size. To analyze the effect of memory capacity on the model during anomaly modality extraction, we trained the model using different memory capacities. Since the memory modality is matched one-to-one with the local features of the image, using a multiple of the image information size of 49 we conducted the experiments and the results are shown in Figure 3. It can be seen that as the memory capacity increases, the model performance rises. This is due to the fact that a larger capacity allows more image information to be stored and may capture more anomalies, which can be used as a prior for subsequent bilinear extraction. However, when greater than a certain threshold, the memory of abnormal modes is sufficient, and more capacity instead memorizes unimportant secondary information introducing noise and causing negative effects."
        },
        {
            "heading": "5.3 Qualitative analysis",
            "text": "To evaluate the performance of our model, we performed a qualitative analysis on the MIMIC-CXR\ndataset. As shown in the Figure 4, we visualized the attention graph produced by the model for the images, as well as for the top 15 predicted semantics. Abnormal semantics related to the report were highlighted in red, while normal semantics related to the report were highlighted in blue. Our analysis revealed that the model pays close attention to abnormal regions in the images, and successfully predicts abnormal phenomena such as heart enlargement. In the prediction of normal semantics, the model gives a rich set of normal semantic candidates, containing all normal mentions in real reports, such as consolidation, pleural effusion, and osseous abnormality. This shows that our model can produce relatively accurate and fluent reports. These results suggest that our model is capable of generating relatively accurate and fluent reports.In addition, we visualize the final learned report weights such that \u03bb = 1\n\u03c32 , then \u03bb corresponding to abnormal\nnormal semantics, abnormal normal sentences is [1.502,1.196,0.993, 0.691]. From the weights, we can see that the abnormal cases have higher weights and the semantics have higher weights than the normal sentences. This indicates that the main body of the report is the anomaly semantics, which contains more information. It is more important to pay attention to the anomaly semantics when the report is generated."
        },
        {
            "heading": "6 Conclusions",
            "text": "We propose a new framework for extracting normal and abnormal semantics separately. By decoupling the semantics, the noise that may exist in the normal semantics is avoided. And we propose the structure of anomaly modality enhancement to enhance the extraction of abnormal features. We classify the reported words into different parts to automatically learn the weights to measure the importance. This eventually makes better semantic and visual features jointly know to generate process accurate reports.\nLimitations\nWhile our approach achieves good results, it is not without limitations. The main limitation lies in treating semantic extraction as a multi-label classification process. In practice, medical reports contain many labeling concepts, including some rare diseases that are seldom mentioned and difficult for the network to identify. And some specific descriptions such as \"acute obliquely oriented lucency through the right 12th posterior rib\" cannot be generated.Additionally, while noise in the report does not affect visual feature extraction, optimizing the report using different weights may only partially alleviate the effects of noise on the decoder, without addressing the root cause.In the future, it may be necessary to start at the data level and standardize reporting up front. Reduce the interference of uncertainty. For rare diseases, additional medical knowledge may need to be introduced into the model. Improve identification of rare diseases through external knowledge enhancement. So at this stage, it can still only play a supporting role in the medical system.\nEthics Statement\nThe MIMIC-CXR datasets employed in our study underwent a meticulous de-identification process. Our utilization of the MIMIC-CXR dataset aligns with the PhysioNet Health Data license 1.5.0 7. Importantly, the distributed MIMIC-CXR dataset underwent thorough processing to remove all instances of protected health information (PHI). This approach underscores our commitment to adhering to ethical standards."
        },
        {
            "heading": "A Appendix",
            "text": "A.1 Dataset and pre-processing We used the MIMIC-CXR dataset consisting of 377,110 chest x-ray images. To facilitate comparison, we followed the preprocessing methods of other previous work (Chen et al., 2020). We only focus on the finding part of the medical report, exclude invalid data in the data, and use lowercase for all reports. Words that appear less than 10 times are ignored and replaced with <unk>. Only the first 100 words are retained for each report. The final number of training/validation/testing sets obtained based on official data division: 269235/2112/3851. In the training process, we scaled the image data to a uniform size of (256,256) and used the following data enhancements: random image cropping to (224,224) and affine transformation, translation up to \u00b12% of image height/width and rotation up to \u00b110\u00b0.\nTo enable semantic tagging, we utilize the RadGraph (Jain et al., 2021) method for extracting clinical entities and their relationships from radiology reports. Specifically, we focus on extracting anatomical entities and observation entities from the reports. To perform normal/abnormal semantic classification, we employ a keyword detection method. In particular, we define a set of normal keywords based on the reference (Yu et al., 2022). These extracted semantics are then used as multi-label classification labels for the semantic extraction task. Purely keyword-based detection sometimes produces misclassifications due to keywords. For example, \u2019enlarged heart size is stable since\u2019 may be misclassified as normal. To address these challenges, we have leveraged ChatGPT to enhance our categorization process. Specifically, the formidable semantic understanding capabilities of ChatGPT assist us in discerning between normal and abnormal cases.We use the GPT-3.5-turbo model for a two-stage assessment. In the first stage, we determine whether the sentence is normal or abnormal. In the second stage, for sentences identified as abnormal, we use the semantic keywords extracted by RadGraph to assess whether the specific semantics are normal or abnormal. In the first stage,prompt: \"You are a specialized radiologist. Evaluate the following medical descriptions. Note: any deviation from the imaging presentation of a normal person is considered abnormal. No need to explain; answer directly: normal or abnormal. Description: XX.\" In the second stage, prompt: \"You are a specialized radiologist. Determine whether the description of a given keyword is abnormal in a medical description. Note: any deviation from the imaging presentation of a normal person is considered abnormal. No need to explain, answer directly: normal or abnormal. Description: XX. Keyword: XX.\" It is important to note that our semantics share the same word list as the original report, and do not require any additional tokenization. Furthermore, the sentences where normal semantics are found are categorized as normal sentences, while those containing abnormal semantics are labeled as abnormal sentences. Overall, we divide the words in the report into four parts: normal/abnormal sentences, and normal/abnormal semantics. A.2 Implementation Details To ensure consistency with the experimental setup of previous work, we used one image as input for MIMIC-CXR. The encoder and decoder modules in our framework consist of three basic layers. We set the 8 attention heads, 512 dimensions for hidden states and initialize it randomly respectively. During the report generation process, we set the beam size to 3. The codebook size N is 98, while the value of K used in the semantic generation process is 25. Semantic branch starts from the first layer APE-MHA. To optimize the model, we employ the AdamW optimizer, with a model learning rate set to 5\u00d710\u22126. We implement our model using Pytorch and X-modaler (Li et al., 2021). We used 2080ti at MIMIC-CXR for 30 epoch of training for a total of 26 GPU hours. A.3 Additional Experiments A.3.1 Semantic extraction method We evaluated the effectiveness of our proposed method for semantic extraction, and the results are summarized in Table 4. In the table, SE represents the decoupled extraction of semantics, where Abnormal and Normal indicate abnormal and normal semantics, respectively. Specifically, a denotes the\nextraction of abnormal semantics only, b denotes the extraction of both normal and abnormal semantics but without decoupling, and c represents our proposed decoupling method. Comparing a and b, we observe that both types of semantics have a positive impact on the final report generation. However, when comparing b and c, we find that the decoupled and separated extraction approach is more effective, with a 0.9% improvement in BLUE4. This underscores the presence of noise in normal semantics, which can adversely affect the visual extractor and compromise reporting accuracy. On the other hand, our proposed decoupling method avoids the effects of noise and achieves better reporting results.\nA.3.2 Memory method We compared our proposed memory approach with the memory modeling approach proposed in method (Cornia et al., 2020) to highlight the role of abnormal memory. For the experiments, we kept parameters such as memory capacity constant. The results show that the memory approach in method (Cornia et al., 2020) did not perform well. We believe that this is due to data bias, as method (Cornia et al., 2020) introduces a memory matrix in the self-attentive phase, which enables it to automatically memorize features in the training data and form prior knowledge. However, since data bias may have caused it to memorize more repetitions of normal cases, it achieved suboptimal results.\nIn contrast, our approach provides more useful information by explicitly memorizing abnormali-\nties. Specifically, we memorize only the images where anomalies exist, and the model is encouraged to memorize different features by leveraging oneto-one matching and mutual exclusion loss. Figure 5 shows the similarity matrix of the memorized codebooks in our method, and it can be observed that the patterns memorized in the codebooks only have a high similarity to themselves. This illustrates the diversity of our method\u2019s memory. Overall, our abnormal memory approach yields better results than method A, which underscores its effectiveness in addressing data bias and improving the performance of medical report generation models.\nA.3.3 Noise impact experiment To explore the effect of noise in report and normal semantics, we designed additional experiments with Normal_De denoting the gradient of truncated normal semantic branches on visual extraction and Report_De denoting the gradient of truncated report decoding on visual extraction. a denotes that report noise with normal semantic noise can affect visual extraction, b denotes that report noise only affects, and c denotes that normal semantic noise only influence. d indicates our final model, i.e., truncated with all noise gradients. The results in Table 6 show that both noises have a negative impact and cause a decrease in the metrics. This also shows that our framework of normal anomaly semantic decoupling can avoid the effect of this complex noise to some extent.\nA.3.4 More detailed evaluation to comprehensively demonstrate the performance of our model, we employed the RadGraph-F1 (Yu et al., 2023) test model and provided a detailed breakdown of disease-specific F1 scores and proportions. Table 9 represents the RadGraphF1 scores, where R2Gen (Chen et al., 2020) and R2GenCMN (Chen et al., 2021b) are obtained by calculations using the officially provided model weights and X-REM (Jeong et al., 2023) are from the original paper. Our metrics achieve the highest scores. Table 8 represents the F1 scores for the 14 diseases. The weight of this disease occupying all the MIMIC-CXR data is labeled after each category. R2Gen (Chen et al., 2020) and R2GenCMN (Chen et al., 2021b) obtained by calculations using the officially provided model weights. It can be seen that the best scores are achieved in most categories. And the percentage of anomalies is small in terms of category share. Detection failures can be due\nto different reasons, such as too little training data (e.g., \"fracture\", \"Lesion\") or too difficult to learn (e.g., \"pneumothorax\", which is is also difficult for clinicians to determine).\nA.3.5 IU-Xray experiment The Indiana University chest x-ray collection (IUXray5) (Demner-Fushman et al., 2016) is a public radiology examination dataset and a common dataset used in medical report generation tasks.\n5https://openi.nlm.nih.gov/\nThe dataset includes 7,470 x-ray images and the corresponding 3,955 radiology reports. However, the IU-Xray dataset does not have a standard dataset segmentation, resulting in some of the previous methods not performing comparably in terms of metrics. To facilitate comparison with previous work, we used the same preprocessing and segmentation as in (Chen et al., 2020). We also compare only the methods that use the same segmentation. The training/test/val setting for the entire dataset was 7:1:2.Our data processing steps were similar to those of MIMIC-CXR. Due to the small number of IU-Xray data, we excluded words with less than 3 occurrences and focused only on the first 60 words.\nwe compare our proposed method with conventional image captioning works, e.g. S&T (Vinyals et al., 2015), AdaAtt (Lu et al., 2017), and the ones proposed for the medical domain, e.g. R2Gen (Chen et al., 2020), PPKED (Liu et al., 2021a), M2TR (Nooralahzadeh et al., 2021), R2GenCMN (Chen et al., 2021b) GSKET (Yang et al., 2022), R2GenRL (Qin and Song, 2022) and MSAT(Wang et al., 2022c). The results on S&T (Vinyals et al., 2015), AdaAtt (Lu et al., 2017) from (Chen et al., 2020), and the rest of the results were cited from the original paper.As we can see from the results in Table 7, our method can still achieve good results on the IU-Xray dataset, which indicates the generality of our method.\nA.4 Hallucination samples\nIn order to more clearly exemplify the error and hallucination samples, we have listed a detailed comparison in Figure 6. It can be seen that our report (a) observes atelectasis but omits fractures. This may be mainly due to the fact that the fractures category is underrepresented in the data. Secondly we are not able to report specific descriptions like l1 and t12. Although we report degenerative changes, they are not specific to the right shoulder. For some of the normal descriptions of (b), we don\u2019t match well with the real report, we don\u2019t report no acute bony abnormalities. This has to do with the normal noise in the report, which is mentioned in some doctors\u2019 reports and not in others. In (c) we report patchy opacities, due to lung atelectasis, but in reality it is basilar lung opacities. and we report pleural effusion on the left side, but in the actual report it is present on the left side as well as the right side. It is possible that the model discriminated the pleural effusion but was inadequate for orientation. This\nmay be due to the lack of more detailed labeling in the report.\nA.5 More Example Visualizations\nimage GTours"
        }
    ],
    "title": "Normal-Abnormal Decoupling Memory for Medical Report Generation",
    "year": 2023
}