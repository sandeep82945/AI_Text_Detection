{
    "abstractText": "Document-level Relation Extraction (DocRE), which aims to extract relations from a long context, is a critical challenge in achieving finegrained structural comprehension and generating interpretable document representations. Inspired by recent advances in in-context learning capabilities emergent from large language models (LLMs), such as ChatGPT, we aim to design an automated annotation method for DocRE with minimum human effort. Unfortunately, vanilla in-context learning is infeasible for document-level Relation Extraction (RE) due to the plenty of predefined fine-grained relation types and the uncontrolled generations of LLMs. To tackle this issue, we propose a method integrating a Large Language Model (LLM) and a natural language inference (NLI) module to generate relation triples, thereby augmenting document-level relation datasets. We demonstrate the effectiveness of our approach by introducing an enhanced dataset known as DocGNRE, which excels in re-annotating numerous long-tail relation types. We are confident that our method holds the potential for broader applications in domain-specific relation type definitions and offers tangible benefits in advancing generalized language semantic comprehension.",
    "authors": [
        {
            "affiliations": [],
            "name": "Junpeng Li"
        },
        {
            "affiliations": [],
            "name": "Zixia Jia"
        },
        {
            "affiliations": [],
            "name": "Zilong Zheng"
        }
    ],
    "id": "SP:b3dc6d447e71453e2805f6b889690fb6ab475e64",
    "references": [
        {
            "authors": [
                "Anastasia Chan."
            ],
            "title": "Gpt-3 and instructgpt: technological dystopianism, utopianism, and \u201ccontextual\u201d perspectives in ai ethics and industry",
            "venue": "AI and Ethics, 3(1):53\u201364.",
            "year": 2023
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "arXiv preprint arXiv:1810.04805.",
            "year": 2018
        },
        {
            "authors": [
                "Alexander Dunn",
                "John Dagdelen",
                "Nicholas Walker",
                "Sanghoon Lee",
                "Andrew S Rosen",
                "Gerbrand Ceder",
                "Kristin Persson",
                "Anubhav Jain."
            ],
            "title": "Structured information extraction from complex scientific text with fine-tuned large language models",
            "venue": "arXiv",
            "year": 2022
        },
        {
            "authors": [
                "Luciano Floridi",
                "Massimo Chiriatti."
            ],
            "title": "Gpt-3: Its nature, scope, limits, and consequences",
            "venue": "Minds and Machines, 30:681\u2013694.",
            "year": 2020
        },
        {
            "authors": [
                "Fabrizio Gilardi",
                "Meysam Alizadeh",
                "Ma\u00ebl Kubli."
            ],
            "title": "Chatgpt outperforms crowd-workers for textannotation tasks",
            "venue": "arXiv preprint arXiv:2303.15056.",
            "year": 2023
        },
        {
            "authors": [
                "Bernal Jim\u00e9nez Guti\u00e9rrez",
                "Nikolas McNeal",
                "Clay Washington",
                "You Chen",
                "Lang Li",
                "Huan Sun",
                "Yu Su."
            ],
            "title": "Thinking about gpt-3 in-context learning for biomedical ie? think again",
            "venue": "arXiv preprint arXiv:2203.08410.",
            "year": 2022
        },
        {
            "authors": [
                "Or Honovich",
                "Roee Aharoni",
                "Jonathan Herzig",
                "Hagai Taitelbaum",
                "Doron Kukliansy",
                "Vered Cohen",
                "Thomas Scialom",
                "Idan Szpektor",
                "Avinatan Hassidim",
                "Yossi Matias."
            ],
            "title": "TRUE: Re-evaluating factual consistency evaluation",
            "venue": "Proceedings of the Second",
            "year": 2022
        },
        {
            "authors": [
                "Quzhe Huang",
                "Shibo Hao",
                "Yuan Ye",
                "Shengqi Zhu",
                "Yansong Feng",
                "Dongyan Zhao."
            ],
            "title": "Does recommend-revise produce reliable annotations? an analysis on missing instances in docred",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for",
            "year": 2022
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Roberta: A robustly optimized bert pretraining approach",
            "venue": "arXiv preprint arXiv:1907.11692.",
            "year": 2019
        },
        {
            "authors": [
                "Zhengliang Liu",
                "Xiaowei Yu",
                "Lu Zhang",
                "Zihao Wu",
                "Chao Cao",
                "Haixing Dai",
                "Lin Zhao",
                "Wei Liu",
                "Dinggang Shen",
                "Quanzheng Li"
            ],
            "title": "Deid-gpt: Zero-shot medical text de-identification by gpt-4",
            "venue": "arXiv preprint arXiv:2303.11032",
            "year": 2023
        },
        {
            "authors": [
                "Youmi Ma",
                "An Wang",
                "Naoaki Okazaki."
            ],
            "title": "DREEAM: Guiding attention with evidence for improving document-level relation extraction",
            "venue": "Proceedings of the 17th Conference of the European Chapter of the Association for Computational Lin-",
            "year": 2023
        },
        {
            "authors": [
                "Bill MacCartney."
            ],
            "title": "Natural language inference",
            "venue": "Stanford University.",
            "year": 2009
        },
        {
            "authors": [
                "Guoshun Nan",
                "Zhijiang Guo",
                "Ivan Sekuli\u0107",
                "Wei Lu."
            ],
            "title": "Reasoning with latent structure refinement for document-level relation extraction",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1546\u20131557.",
            "year": 2020
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J Liu."
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "The Journal of Machine Learning Research,",
            "year": 2020
        },
        {
            "authors": [
                "Qingyu Tan",
                "Ruidan He",
                "Lidong Bing",
                "Hwee Tou Ng."
            ],
            "title": "Document-level relation extraction with adaptive focal loss and knowledge distillation",
            "venue": "Findings of the Association for Computational Linguistics: ACL 2022, pages 1672\u20131681, Dublin, Ire-",
            "year": 2022
        },
        {
            "authors": [
                "Qingyu Tan",
                "Lu Xu",
                "Lidong Bing",
                "Hwee Tou Ng",
                "Sharifah Mahani Aljunied."
            ],
            "title": "Revisiting docred \u2013 addressing the false negative problem in relation extraction",
            "venue": "Annual Conference on Empirical Methods in Natural Language Processing (EMNLP).",
            "year": 2022
        },
        {
            "authors": [
                "Petter T\u00f6rnberg."
            ],
            "title": "Chatgpt-4 outperforms experts and crowd workers in annotating political twitter messages with zero-shot learning",
            "venue": "arXiv preprint arXiv:2304.06588.",
            "year": 2023
        },
        {
            "authors": [
                "Somin Wadhwa",
                "Silvio Amir",
                "Byron C Wallace."
            ],
            "title": "Revisiting relation extraction in the era of large language models",
            "venue": "arXiv preprint arXiv:2305.05003.",
            "year": 2023
        },
        {
            "authors": [
                "Zhen Wan",
                "Fei Cheng",
                "Zhuoyuan Mao",
                "Qianying Liu",
                "Haiyue Song",
                "Jiwei Li",
                "Sadao Kurohashi."
            ],
            "title": "Gpt-re: In-context learning for relation extraction using large language models",
            "venue": "arXiv preprint arXiv:2305.02105.",
            "year": 2023
        },
        {
            "authors": [
                "Difeng Wang",
                "Wei Hu",
                "Ermei Cao",
                "Weijian Sun."
            ],
            "title": "Global-to-local neural networks for documentlevel relation extraction",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 3711\u20133721.",
            "year": 2020
        },
        {
            "authors": [
                "Shuhe Wang",
                "Xiaofei Sun",
                "Xiaoya Li",
                "Rongbin Ouyang",
                "Fei Wu",
                "Tianwei Zhang",
                "Jiwei Li",
                "Guoyin Wang."
            ],
            "title": "Gpt-ner: Named entity recognition via large language models",
            "venue": "arXiv preprint arXiv:2304.10428.",
            "year": 2023
        },
        {
            "authors": [
                "Xin Xu",
                "Yuqi Zhu",
                "Xiaohan Wang",
                "Ningyu Zhang"
            ],
            "title": "How to unleash the power of large language models for few-shot relation extraction? arXiv preprint arXiv:2305.01555",
            "year": 2023
        },
        {
            "authors": [
                "Yuan Yao",
                "Deming Ye",
                "Peng Li",
                "Xu Han",
                "Yankai Lin",
                "Zhenghao Liu",
                "Zhiyuan Liu",
                "Lixin Huang",
                "Jie Zhou",
                "Maosong Sun."
            ],
            "title": "DocRED: A large-scale document-level relation extraction dataset",
            "venue": "Proceedings of the 57th Annual Meeting of the Associa-",
            "year": 2019
        },
        {
            "authors": [
                "Ningyu Zhang",
                "Xiang Chen",
                "Xin Xie",
                "Shumin Deng",
                "Chuanqi Tan",
                "Mosha Chen",
                "Fei Huang",
                "Luo Si",
                "Huajun Chen."
            ],
            "title": "Document-level relation extraction as semantic segmentation",
            "venue": "arXiv preprint arXiv:2106.03618.",
            "year": 2021
        },
        {
            "authors": [
                "Wenxuan Zhou",
                "Kevin Huang",
                "Tengyu Ma",
                "Jing Huang."
            ],
            "title": "Document-level relation extraction with adaptive thresholding and localized context pooling",
            "venue": "Proceedings of the AAAI conference on artificial intelligence, volume 35, pages 14612\u201314620.",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Document-level Relation Extraction (DocRE) is a task that focuses on extracting fine-grained relations between entity pairs within a lengthy context (Yao et al., 2019; Nan et al., 2020; Wang et al., 2020; Zhou et al., 2021; Zhang et al., 2021; Ma et al., 2023). The abundance of entity pairs in a document, coupled with a vast array of fine-grained relation types, makes DocRE inherently more challenging than sentence-level RE. The challenge is observed not only in model learning but also in human annotations.\n\u2217Equal contributions. Authors ordered alphabetically. Correspondence to Zilong Zheng <zlzheng@bigai.ai>.\nThe original document-level RE dataset DocRED (Yao et al., 2019) has been recognized for its false negative issue and subsequently re-annotated to address this concern by supplementing a significant number of relation triples. Notably, two representative works, Huang et al. (2022) and Tan et al. (2022b), have contributed to this re-annotation process. Huang et al. (2022) undertook manual annotation from scratch, employing two expert annotators to annotate 96 documents. On the other hand, Tan et al. (2022b) utilized pre-trained RE models in conjunction with manual revision to construct Re-DocRED. Despite their contributions to supplementing relations for DocRED, both methods have certain limitations. First, achieving complete manual annotation is challenging: each document within this dataset contains an average of 19.5 entities, requiring consideration of approximately 37,000 candidate triples (including 97 relation types, including NULL). Second, the supplementary annotations are derived from the existing data distribution: Tan et al. (2022b) first pre-trained a RE model with distantly supervised data from DocRED, then utilized this model to predict triple candidates. Such a process may introduce model bias, potentially resulting in the exclusion of sparse relations that exist beyond the scope of the existing\ndata distribution. Figure 1 illustrates the counts of some relation types across various datasets. It is evident that the supplementary annotations in ReDocRED exhibit a distribution similar to that of the DocRED test set.\nIn light of the limitations inherent in prior annotation techniques and the imperative to enhance the completeness of DocRE datasets, we propose a novel approach aimed at augmenting the ReDocRED dataset through the utilization of the powerful generalization capabilities of Large Language Models (LLMs). By leveraging our method, as shown in Figure 1, our revised test set, named DocGNRE, exhibits the advantage of re-annotating a greater number of long-tail types, such as P276 and P551.\nRecent studies have utilized GPT (Floridi and Chiriatti, 2020; Chan, 2023) for various structural prediction tasks, such as named entity prediction and relation extraction (Dunn et al., 2022; Guti\u00e9rrez et al., 2022; Wang et al., 2023; Liu et al., 2023; Xu et al., 2023), as well as text classification labeling (Gilardi et al., 2023; T\u00f6rnberg, 2023). Notably, researchers such as Wan et al. (2023); Wadhwa et al. (2023) have demonstrated the effectiveness of in-context learning by incorporating prompts containing suitable example demonstrations for RE tasks. However, it is worth noting that without explicit instructions, GPT may generate uncontrolled relations that do not align with predefined types. Therefore, recent methods only work in sentencelevel RE and especially highlight one distinguished challenge for LLM in-context learning: unable to fitting detailed instructions for long-context documents (Wadhwa et al., 2023).\nTo align GPT-generated relations and predefined relation types, we first combine Natural Language Inference (NLI) models (MacCartney, 2009) with GPT to solve zero-shot DocRE. The results show that although GPT generations only hit partial ground truth of Re-DocRED, it detects substantial external valid relation triples (details in Sec. 3.1). Therefore, we design a pipeline framework to further complement the test set of Re-DocRED and automatically generate distant training annotations by combining GPT and NLI modules. To verify that we supplement many relation triples beyond the scope of the original data distribution, we test previous models in our DocGNRE test set. Additionally, we train the state-of-the-art (SOTA) model using our distant training dataset.\nOur contributions can be summarized as follows: We conduct a quantitative analysis to evaluate\nthe performance of GPT in zero-shot documentlevel RE. We propose a novel framework that integrates an NLI module and an LLM to automatically generate distant relation triples, supplementing existing DocRE datasets. We create an enhanced version of the ReDocRED test set, named DocGNRE, with minimal human intervention, ensuring high quality. Additionally, we augment the Re-DocRED training set by supplementing it with distant relation triples automatically generated by our framework1, referring to Tabel 1."
        },
        {
            "heading": "2 LLM Enhanced Automatic Data Generation",
            "text": "Our approach consists of two main procedures: constructing prompts for LLM to generate relations triples as proposals and employing Natural Language Inference (NLI) models to align generated relations with predefined relation types. Fig. 2 shows the whole framework. In the first procedure, we observed that even though we imposed restrictions on the entity and relation lists in the prompts, LLMs (both GPT-3.5 and GPT-4) still generated triples that fell outside of our intended constraints. Furthermore, we found that the generated relations expressed by contextual words in the document were more accurate than those with the restricted relation types. Based on these insights and to fully leverage the potential of LLMs, hereby generating more accurate relation triples, we removed the restrictions of specific relation types for LLMs. Instead, we subsequently utilize an NLI module to map the generated relations to the predefined relation types in the Re-DocRED dataset."
        },
        {
            "heading": "2.1 GPT Results as Proposals",
            "text": "We select GPT-3.5 (gpt-3.5-turbo) as our LLM module, considering a balance between cost and performance. Given that the original DocRED dataset provides an entity list for each document, we constrain the responses of GPT to utilize only the entities present in the provided list.\nPrompt Construction As shown in Figure 2, the prompt consists of a generation demonstration and a specific context followed by a corresponding\n1Our dataset is publicly available at https://github. com/bigai-nlco/DocGNRE.\nentity list. We notice that as the generated content by LLMs became longer, the accuracy decreased. To mitigate this, we set \u201cat least 20 triples\u201d in the initial prompt 2. To generate more additional triples, we employ an iterative approach by feeding the previous GPT answers as input while instructing GPT to \u201cPlease keep generating 20 more triples using only the given entities from the entity list\u201d. However, despite providing the entity list in the prompt, we observed that undesired triples with incorrect entity pairs still occurred. To address this, we implemented a filtering process to remove these undesired triples. Consequently, all the remaining triples are treated as proposals and later aligned using the NLI module."
        },
        {
            "heading": "2.2 NLI Module as an Annotator",
            "text": "In this procedure, our goal is to map the relations generated by GPT to predefined types. To achieve this, a reasonable approach is to align the semantic meaning of relations. Therefore, we employ a NLI model, which has demonstrated effectiveness in assessing factual consistency (Honovich et al., 2022). The NLI model takes two sentences as input, typically referred to as the premise and the hypothesis. It assigns a score to each term, indi-\n2We choose the \u201c20\u201d number because it is a trade-off between ensuring accuracy and the quantity of generated relations. We find that the first 20 or so relations generated by GPT-3.5 (gpt-3.5-turbo) exhibit a relatively promising level of quality. The \u201cat least 20\u201d could be replaced by \u201cno more than\u201d and \u201ca maximum of\u201d. We discovered that GPT-3.5 generates comparable numbers and quality of triples using all of these expressions.\ncating whether it signifies entailment, neutrality, or contradiction. If the term \u201centailment\u201d receives the highest score, the model concludes that the two sentences are factually consistent.\nPremise and Hypothesis Construction In our framework, we take each GPT-generated triple as the premise and replace the relation in such triple with a specific predefined relation type as the hypothesis. Remember that our purpose is to map each GPT-generated relation proposal to a predefined relation type. Hence, we should enumerate the hypothesis constructed by each specific type in the predefined set to calculate the entailment scores with the corresponding premise and choose the ONE with the highest score. Moreover, we observe that the GPT-generated relation may correspond to an inverse predefined relation type. For example, if the predefined relation set contains the \u201cemployee\u201d type rather than \u201cemployer\u201d, the GPTgenerated triple \u201c<David Lean, worked for, London Films>\u201d will correspond to \u201c<London Films, employee, David Lean>\u201d rather than \u201c<David Lean, employee, London Films>\u201d. Therefore, for each generated relation proposal as a premise, we construct 96\u22172 = 192 possible hypotheses, where 96 is the size of the predefined relation set without NULL, and double means we change the subject and object entities for each predefined type. Specifically, take a triple < e1, rgpt, e2 > generated from GPT as an example premise, given the predefined relation set {r1, r2, ...}, we construct candidate hypotheses {< e1, r1, e2 >,< e2, r1, e1 >,< e1, r2, e2 >,<\ne2, r2, e1 >, ...}. Because the NLI model is pretrained with natural language sentences, we convert the triples to natural sentences. Most of the GPT-generated relations are themselves in natural language, so each triple\u2019s subject entity, relation, and object entity are directly concatenated to get a natural sentence. The predefined relation types typically are abstractive. To make the hypothesis precisely convey the meaning of each relation type, we integrate the description of each relation type with subject and object entities. Hypothesis for each relation type can be found in Appendix C.\nEntail Scores from NLI Model We use the T5-based NLI model3 in this paper for its powerful generalizability. T5-XXL (Raffel et al., 2020) is a generative model, which identifies \"Entailment\" and \"No entailment\" by generating two sequences in the inference stage. We leverage the probabilities of such two sequences omitting the start and end tokens to calculate the entailment scores used for sorting predefined relation types. Details can be found in Appendix A.\nPost Processing To ensure the high quality of newly produced relation triples, we ultimately retain those hypothesis triples that should satisfy all the following principles: The entity types of subject and object entities\nsatisfy the type constraints of the relation. Get the highest entailment scores. Get the entailment scores of more than 0.6.\nNote that some of the GPT-generated relations may be exactly those in the predefined set of relation types. We do not need to map these generated triples via our NLI module and just add them into the final selected triples set.\n3https://huggingface.co/google/t5_xxl_true_ nli_mixture\nThrough above procedures, we process each document of the Re-DocRED train set to produce additional distant relation triples. For the Re-DocRED test set, after acquiring distant relation triples, each distant triple will be conducted through human verification. Two annotators are asked to answer whether the relation triples can be inferred according to the provided documents. A third annotator will resolve the conflicting annotations. Specifically, we use Mechanical Turk for human annotations. In order to ensure that annotators possessed a significant level of qualification, prospective annotators were required to meet the following criteria: \u2022 \u201cHIT Approval Rate(%) for all Requesters\u2019 HITs\u201d > 95. \u2022 \u201cNumber of HITs Approved\u201d > 1000. \u2022 \u201cLocation\u201d is one of {United States, Canada,\nGreat Britain, Australia, Singapore, Ireland, New Zealand}. The first two indicators are calculated by Mechanical Turk according to one\u2019s historical performance and the last one aims to promise English proficiency of the annotators. Finally, the acceptance rate of the NLI-selected relations in the test set is 71.3%. We provide a more accurate and complete test set with the addition of 2078 triples than ReDocRED. Detailed statistics of our datasets can be found in Table 1."
        },
        {
            "heading": "3 Experiments",
            "text": ""
        },
        {
            "heading": "3.1 Zero-shot Document-level RE",
            "text": "Our framework can obviously be used to predict document-level relations directly. Therefore, in the first experiment, we explore the GPT performance on the zero-shot document-level RE. Table 2 shows the results. As far as we know, we are the first to report these results on document-level RE.\nWe have three observations based on Table 2: i) Pure GPT-3.5 (without our NLI module) only hits rare ground truth. As aforementioned, GPT generates most relations expressed by natural language, which do not exactly match with the ground truth, even though some of these relations represent the same meaning as ground truth. So the exactmatch F1 scores are unsatisfactory; ii) NLI module can improve pure GPT performance. With NLI module mapping GPT answers to predefined types, GPT-3.5 (gpt-3.5-turbo) predicts a small portion of ground truth triples that are manually annotated (5.77 recall in the Re-DocRED test set). The reason may be that we ask it to generate multiple relations\nat once by one prompt rather than enumerate all entity pairs to ask for relations one by one (which is too costly and time-consuming to execute for plenty of entities on document-level RE). But from human verification, the accuracy of NLI-selected triples has been proven relatively high (72.71 precision in our supplementary test set DocGNRE), which illustrates that most triples predicted by our framework are the supplementary of Re-DocRED; iii) Relation descriptions can guide NLI to output expected relations. With relation descriptions to construct hypothesises, the performance is further improved (25.31 vs. 16.2) in our DocGNRE."
        },
        {
            "heading": "3.2 Training with Distant Triples",
            "text": "We test the SOTA document-level RE model (Ma et al., 2023) on our DocGNRE and retrain it with our distant training set. All experiment settings are the same as Ma et al. (2023) except for training data in the +GPT setting. Table 3 shows the results. We can find that i) the recall of previous models on our DocGNRE drops, which demonstrates the\ndifficult prediction on our supplementary test relation triples when the model is only trained with the training set of DocRED or ReDocRED; ii) The recall scores on all the test sets are improved with directly supervised training on our training set (which exhibits the capability to predict additional ground truth instances), even though our distantly supervised data is somewhat noisy. Designing more advanced methods to leverage our distant training set is taken in future work.\nIn addition, we conducted experiments using two other DocRE models, ATLOP (Zhou et al., 2021) and KD-DocRE (Tan et al., 2022a), by leveraging their officially provided code. Experimental results of ATLOP and KD-DocRE show a similar tendency to DREEAM. Detailed results are in Appendix B."
        },
        {
            "heading": "4 Conclusion",
            "text": "LLMs face challenges in extracting fine-grained relations within lengthy contexts. To address this limitation, we present a novel framework that integrates an NLI module in this work. With our framework, we improve the performance of GPT in zero-shot document-level RE. Above all, our framework enhances the automatic data generation capability with minimum human effort. We supplement the existing DocRE dataset, providing a complete test set DocGNRE and a distant training set. Given the inherent presence of false negative instances in numerous RE datasets, particularly those constructed through a recommend-revise scheme or distant supervision, we believe our framework possesses a broad utility that extends to a wider array of datasets.\nLimitations\nThe limited generated length of LLMs causes the limitation of our methods. There is a specific upper limit on the number of relation triples that can be generated for each document. Therefore, our framework is an excellent data supplement method rather than a perfect zero-shot predictor."
        },
        {
            "heading": "Acknowledgements",
            "text": "This work is supported in part by the National Key R&D Program of China (2021ZD0150200) and the National Natural Science Foundation of China (62376031)."
        },
        {
            "heading": "A NLI Score",
            "text": "We choose the T5-based NLI model released by Google in this paper. As mentioned earlier, T5XXL is a generative model. The model identifies \u201cEntailment\u201d and \u201cNo entailment\u201d by generating two sequences in the inference stage. During inference, the sequence \u201c<pad>_0</s>\u201d identifies \u201cNo entailment\u201d, and the sequence \u201c<pad>1</s><pad>\u201d identifies \u201cEntailment\u201d. Because both the first tokens are \u201c<pad>\u201d, and when the first three tokens have been determined, the prediction of the last token has a high probability, so we do not consider the first and last token when calculating the NLI score. To obtain the NLI score, we obtain the logits of four subsequences (\"_0\", \"_</s>\", \"10\", \"1</s>\") and perform a softmax operation to obtain the corresponding probabilities. The score of \"_0\" sequence corresponds to the score of \u201cNo entailment\u201d. The score of \u201c1</s>\u201d sequence corresponds to the score of \u201cEntailment\u201d. To further distinguish the NLI score among the constructed triples, we subtract the score of \u201cNo entailment\u201d from the score of \u201cEntailment\u201d to fuse the two scores as the final scores."
        },
        {
            "heading": "B Model Results",
            "text": "Experimental results of ATLOP and KD-DocRE are shown in Table 4 and Table 5 respectively."
        },
        {
            "heading": "C Hypothesis Construction of Relation",
            "text": "We provide hypothesis construction of predefined relations in DocRED and Re-DocRED with Wikidata Id and Name in Table 6.\nWikidata ID Name Hypothesis Construction\nP6 head of government The head of the executive power of the governmental body sub. is obj. P17 country The sovereign state of this item sub. is obj. P19 place of birth The birth location of the person, animal or fictional character sub. is obj. P20 place of death The death location of the person, animal or fictional character sub. is obj. P22 father The father of sub. is obj. P25 mother The mother of sub. is obj. P26 spouse The spouse of sub. is obj. P27 country of citizen-\nship obj. is a country that recognizes sub. as its citizen\nP30 continent obj. is the continent of which sub. is a part P31 instance of obj. is that class of which sub. is a particular example and member. (sub. typically an individual member with proper name label) P35 head of state obj. is the official with the highest formal authority in the country/state sub. P36 capital obj. is the primary city of the country/state sub. P37 official language obj. is the language designated as official by sub. P39 position held sub. currently or formerly holds the position or public office obj. P40 child sub. has obj. as their offspring son or daughter P50 author The main creator(s) of the written work sub. is(are) obj. P54 member of sports\nteam The sports team or club that sub. represents or formerly represented is obj.\nP57 director The director of this film, TV-series, stageplay or video game is obj. P58 screenwriter The author(s) of the screenplay or script for this work sub. is(are) obj. P69 educated at The educational institution attended by sub. is obj. P86 composer The person(s) who wrote the music sub. is(are) obj. P102 member of political\nparty The political party of which this politician sub. is or has been a member is obj.\nP108 employer The person or organization for which sub. works or worked is obj. P112 founded by The founder or co-founder of this organization, religion or place sub. is obj. P118 league The league in which the team or player sub. plays or has played in is obj. P123 publisher The organization or person responsible for publishing books, periodicals, games or software sub. is obj. P127 owned by The owner of sub. is obj. P131 located in the ad-\nministrative territorial entity\nsub. is located on the territory of the following administrative entity obj.\nP136 genre The creative work sub.\u2019s genre is obj. P137 operator The person or organization that operates the equipment, facility, or service sub. is obj. P140 religion The religion of a person, organization or religious building, or associated with sub. is obj. P150 contains administra-\ntive territorial entity The direct subdivisions of an administrative territorial entity sub. has obj.\nWikidata ID Name Hypothesis Construction\nP155 follows The immediately prior item in some series of which sub. is part is obj. P156 followed by The immediately following item in some series of which sub. is part is obj. P159 headquarters location The specific location where sub.\u2019s headquarters is or has been situated is obj. P161 cast member The actor performing live sub. for a camera or audience has obj. P162 producer The producer(s) of this film or music work sub. is(are) obj. P166 award received The award or recognition received by a person, organization or creative work sub. is obj. P170 creator The maker of a creative work sub. is obj. P171 parent taxon The closest parent taxon of the taxon sub. is obj. P172 ethnic group sub.\u2019s ethnicity is obj. P175 performer The performer involved in the performance or the recoding of the work sub. is obj. P176 manufacturer The manufacturer or producer of the product sub. is obj. P178 developer The organization or person that developed sub. is obj. P179 series The series which contains sub. is obj. P190 sister city sub. and obj. are twin towns, sister cities, twinned municipalities P194 legislative body The legislative body governing sub. is obj. P205 basin country The country that have drainage to/from or border the body of water sub. has obj. P206 located in or next to\nbody of water sub. is located in or next to body of water obj.\nP241 military branch The branch to which the military unit, award, office, or person sub. belongs is obj. P264 record label The brand and trademark associated with the marketing of subject music recordings and music videos sub. is obj. P272 production company The company that produced this film, audio or performing arts work sub. is obj. P276 location The location of the item, physical object or event sub. is within is obj. P279 subclass of All instances of sub. are instances of obj. P355 subsidiary The subsidiary of a company or organization sub. has obj. P361 part of obj. has part or parts sub. P364 original language of\nwork The language in which the film or a performance work sub. was originally created is obj.\nP400 platform The platform for which the work sub. has been developed or released / specific platform version fo the software sub. developed is obj. P403 mouth of the watercourse The body of water to which the watercourse sub. drains is obj. P449 original network The network(s) the radio or television show sub. was originally aired on has obj. P463 member of The organization or club to which sub. belongs is obj. P488 chairperson The presiding member of the organization, group or body sub. is obj. P495 country of origin The country of origin of the creative work sub. is obj. P527 has part sub. has part or parts obj. P551 residence The place where the person sub. is, or has been, resident is obj. P569 date of birth The date on which sub. was born is obj. P570 date of death The date on which sub. died is obj."
        }
    ],
    "title": "Semi-automatic Data Enhancement for Document-Level Relation Extraction with Distant Supervision from Large Language Models",
    "year": 2023
}