{
    "abstractText": "We investigate the problem of generating instructions to guide humans to navigate in simulated residential environments. A major issue with current models is hallucination: they generate references to actions or objects that are inconsistent with what a human follower would perform or encounter along the described path. We develop a model that detects these hallucinated references by adopting a model pretrained on a large corpus of image-text pairs, and fine-tuning it with a contrastive loss that separates correct instructions from instructions containing synthesized hallucinations. Our final model outperforms several baselines, including using word probability estimated by the instruction-generation model, and supervised models based on LSTM and Transformer.",
    "authors": [],
    "id": "SP:54d1893a3a48d1284d394f0d395ce64f87a1b67c",
    "references": [
        {
            "authors": [
                "Peter Anderson",
                "Xiaodong He",
                "Chris Buehler",
                "Damien Teney",
                "Mark Johnson",
                "Stephen Gould",
                "Lei Zhang."
            ],
            "title": "Bottom-up and top-down attention for image captioning and visual question answering",
            "venue": "Proceedings of the IEEE conference on computer vision",
            "year": 2018
        },
        {
            "authors": [
                "Peter Anderson",
                "Qi Wu",
                "Damien Teney",
                "Jake Bruce",
                "Mark Johnson",
                "Niko S\u00fcnderhauf",
                "Ian Reid",
                "Stephen Gould",
                "Anton Van Den Hengel"
            ],
            "title": "2018b. Visionand-language navigation: Interpreting visuallygrounded navigation instructions in real environ",
            "year": 2018
        },
        {
            "authors": [
                "Ali Furkan Biten",
                "Llu\u00eds G\u00f3mez",
                "Dimosthenis Karatzas."
            ],
            "title": "Let there be a clock on the beach: Reducing object hallucination in image captioning",
            "venue": "Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 1381\u20131390.",
            "year": 2022
        },
        {
            "authors": [
                "Donna Byron",
                "Alexander Koller",
                "Kristina Striegnitz",
                "Justine Cassell",
                "Robert Dale",
                "Johanna D Moore",
                "Jon Oberlander"
            ],
            "title": "Report on the first nlg challenge on generating instructions in virtual environments (give)",
            "year": 2010
        },
        {
            "authors": [
                "Angel Chang",
                "Angela Dai",
                "Thomas Funkhouser",
                "Maciej Halber",
                "Matthias Niessner",
                "Manolis Savva",
                "Shuran Song",
                "Andy Zeng",
                "Yinda Zhang."
            ],
            "title": "Matterport3D: Learning from RGB-D data in indoor environments",
            "venue": "International Conference on 3D Vision",
            "year": 2017
        },
        {
            "authors": [
                "Sihao Chen",
                "Fan Zhang",
                "Kazoo Sone",
                "Dan Roth."
            ],
            "title": "Improving faithfulness in abstractive summarization with contrast candidate generation and selection",
            "venue": "arXiv preprint arXiv:2104.09061.",
            "year": 2021
        },
        {
            "authors": [
                "Wenliang Dai",
                "Zihan Liu",
                "Ziwei Ji",
                "Dan Su",
                "Pascale Fung."
            ],
            "title": "Plausible may not be faithful: Probing object hallucination in vision-language pre-training",
            "venue": "arXiv preprint arXiv:2210.07688.",
            "year": 2022
        },
        {
            "authors": [
                "David Dale",
                "Elena Voita",
                "Lo\u00efc Barrault",
                "Marta R Costa-juss\u00e0."
            ],
            "title": "Detecting and mitigating hallucinations in machine translation: Model internal workings alone do well, sentence similarity even better",
            "venue": "arXiv preprint arXiv:2212.08597.",
            "year": 2022
        },
        {
            "authors": [
                "Esin Durmus",
                "He He",
                "Mona Diab."
            ],
            "title": "Feqa: A question answering evaluation framework for faithfulness assessment in abstractive summarization",
            "venue": "arXiv preprint arXiv:2005.03754.",
            "year": 2020
        },
        {
            "authors": [
                "Tobias Falke",
                "Leonardo FR Ribeiro",
                "Prasetya Ajie Utama",
                "Ido Dagan",
                "Iryna Gurevych."
            ],
            "title": "Ranking generated summaries by correctness: An interesting but challenging application for natural language inference",
            "venue": "Proceedings of the 57th Annual Meet-",
            "year": 2019
        },
        {
            "authors": [
                "Daniel Fried",
                "Jacob Andreas",
                "Dan Klein."
            ],
            "title": "Unified pragmatic models for generating and following instructions",
            "venue": "arXiv preprint arXiv:1711.04987.",
            "year": 2017
        },
        {
            "authors": [
                "Daniel Fried",
                "Ronghang Hu",
                "Volkan Cirik",
                "Anna Rohrbach",
                "Jacob Andreas",
                "Louis-Philippe Morency",
                "Taylor Berg-Kirkpatrick",
                "Kate Saenko",
                "Dan Klein",
                "Trevor Darrell."
            ],
            "title": "Speaker-follower models for vision-and-language navigation",
            "venue": "Advances in",
            "year": 2018
        },
        {
            "authors": [
                "Xiaofeng Gao",
                "Qiaozi Gao",
                "Ran Gong",
                "Kaixiang Lin",
                "Govind Thattai",
                "Gaurav S Sukhatme."
            ],
            "title": "Dialfred: Dialogue-enabled agents for embodied instruction following",
            "venue": "IEEE Robotics and Automation Letters, 7(4):10049\u201310056.",
            "year": 2022
        },
        {
            "authors": [
                "Robert Goeddel",
                "Edwin Olson."
            ],
            "title": "Dart: A particle-based method for generating easy-to-follow directions",
            "venue": "2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 1213\u2013 1219. IEEE.",
            "year": 2012
        },
        {
            "authors": [
                "Sachin Goyal",
                "Ananya Kumar",
                "Sankalp Garg",
                "Zico Kolter",
                "Aditi Raghunathan."
            ],
            "title": "Finetune like you pretrain: Improved finetuning of zero-shot vision models",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Nuno M Guerreiro",
                "Elena Voita",
                "Andr\u00e9 FT Martins."
            ],
            "title": "Looking for a needle in a haystack: A comprehensive study of hallucinations in neural machine translation",
            "venue": "arXiv preprint arXiv:2208.05309.",
            "year": 2022
        },
        {
            "authors": [
                "Pierre-Louis Guhur",
                "Makarand Tapaswi",
                "Shizhe Chen",
                "Ivan Laptev",
                "Cordelia Schmid."
            ],
            "title": "Airbert: Indomain pretraining for vision-and-language navigation",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1634\u20131643.",
            "year": 2021
        },
        {
            "authors": [
                "Beliz Gunel",
                "Jingfei Du",
                "Alexis Conneau",
                "Ves Stoyanov."
            ],
            "title": "Supervised contrastive learning for pretrained language model fine-tuning",
            "venue": "Proceedings of the International Conference on Learning Representations.",
            "year": 2021
        },
        {
            "authors": [
                "Anisha Gunjal",
                "Jihan Yin",
                "Erhan Bas."
            ],
            "title": "Detecting and preventing hallucinations in large vision language models",
            "venue": "arXiv preprint arXiv:2308.06394.",
            "year": 2023
        },
        {
            "authors": [
                "Keji He",
                "Yan Huang",
                "Qi Wu",
                "Jianhua Yang",
                "Dong An",
                "Shuanglin Sima",
                "Liang Wang."
            ],
            "title": "Landmarkrxr: Solving vision-and-language navigation with fine-grained alignment supervision",
            "venue": "Advances in Neural Information Processing Systems, 34:652\u2013663.",
            "year": 2021
        },
        {
            "authors": [
                "Yicong Hong",
                "Qi Wu",
                "Yuankai Qi",
                "Cristian RodriguezOpazo",
                "Stephen Gould."
            ],
            "title": "Vln bert: A recurrent vision-and-language bert for navigation",
            "venue": "Proceedings of the IEEE/CVF conference on Computer Vision and Pattern Recognition, pages 1643\u20131653.",
            "year": 2021
        },
        {
            "authors": [
                "Matthew Honnibal",
                "Ines Montani."
            ],
            "title": "spaCy 2: Natural language understanding with Bloom embeddings, convolutional neural networks and incremental parsing",
            "venue": "To appear.",
            "year": 2017
        },
        {
            "authors": [
                "Haoshuo Huang",
                "Vihan Jain",
                "Harsh Mehta",
                "Jason Baldridge",
                "Eugene Ie."
            ],
            "title": "Multi-modal discriminative model for vision-and-language navigation",
            "venue": "arXiv preprint arXiv:1905.13358.",
            "year": 2019
        },
        {
            "authors": [
                "Ziwei Ji",
                "Nayeon Lee",
                "Rita Frieske",
                "Tiezheng Yu",
                "Dan Su",
                "Yan Xu",
                "Etsuko Ishii",
                "Ye Jin Bang",
                "Andrea Madotto",
                "Pascale Fung."
            ],
            "title": "Survey of hallucination in natural language generation",
            "venue": "ACM Computing Surveys, 55(12):1\u201338.",
            "year": 2023
        },
        {
            "authors": [
                "Aishwarya Kamath",
                "Peter Anderson",
                "Su Wang",
                "Jing Yu Koh",
                "Alexander Ku",
                "Austin Waters",
                "Yinfei Yang",
                "Jason Baldridge",
                "Zarana Parekh."
            ],
            "title": "A new path: Scaling vision-and-language navigation with synthetic instructions and imitation learning",
            "venue": "arXiv",
            "year": 2022
        },
        {
            "authors": [
                "Alexander Koller",
                "Kristina Striegnitz",
                "Andrew Gargett",
                "Donna Byron",
                "Justine Cassell",
                "Robert Dale",
                "Johanna D Moore",
                "Jon Oberlander."
            ],
            "title": "Report on the second nlg challenge on generating instructions in virtual environments (give-2)",
            "venue": "Proceed-",
            "year": 2010
        },
        {
            "authors": [
                "Wojciech Kry\u015bci\u0144ski",
                "Bryan McCann",
                "Caiming Xiong",
                "Richard Socher."
            ],
            "title": "Evaluating the factual consistency of abstractive text summarization",
            "venue": "arXiv preprint arXiv:1910.12840.",
            "year": 2019
        },
        {
            "authors": [
                "Katherine Lee",
                "Orhan Firat",
                "Ashish Agarwal",
                "Clara Fannjiang",
                "David Sussillo."
            ],
            "title": "Hallucinations in neural machine translation",
            "venue": "Interpretability and Robustness in Audio, Speech, and Language Workshop (NeurIPS).",
            "year": 2018
        },
        {
            "authors": [
                "Yifan Li",
                "Yifan Du",
                "Kun Zhou",
                "Jinpeng Wang",
                "Wayne Xin Zhao",
                "Ji-Rong Wen."
            ],
            "title": "Evaluating object hallucination in large vision-language models",
            "venue": "arXiv preprint arXiv:2305.10355.",
            "year": 2023
        },
        {
            "authors": [
                "Tianyu Liu",
                "Yizhe Zhang",
                "Chris Brockett",
                "Yi Mao",
                "Zhifang Sui",
                "Weizhu Chen",
                "Bill Dolan."
            ],
            "title": "A token-level reference-free hallucination detection benchmark for free-form text generation",
            "venue": "arXiv preprint arXiv:2104.08704.",
            "year": 2021
        },
        {
            "authors": [
                "Jiasen Lu",
                "Dhruv Batra",
                "Devi Parikh",
                "Stefan Lee."
            ],
            "title": "Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks",
            "venue": "Advances in neural information processing systems, 32.",
            "year": 2019
        },
        {
            "authors": [
                "Marianna Martindale",
                "Marine Carpuat",
                "Kevin Duh",
                "Paul McNamee."
            ],
            "title": "Identifying fluently inadequate output in neural and statistical machine translation",
            "venue": "Proceedings of Machine Translation Summit XVII: Research Track, pages 233\u2013243.",
            "year": 2019
        },
        {
            "authors": [
                "Joshua Maynez",
                "Shashi Narayan",
                "Bernd Bohnet",
                "Ryan McDonald."
            ],
            "title": "On faithfulness and factuality in abstractive summarization",
            "venue": "arXiv preprint arXiv:2005.00661.",
            "year": 2020
        },
        {
            "authors": [
                "Mathias M\u00fcller",
                "Annette Rios",
                "Rico Sennrich."
            ],
            "title": "Domain robustness in neural machine translation",
            "venue": "arXiv preprint arXiv:1911.03109.",
            "year": 2019
        },
        {
            "authors": [
                "Feng Nie",
                "Jin-Ge Yao",
                "Jinpeng Wang",
                "Rong Pan",
                "Chin-Yew Lin."
            ],
            "title": "A simple recipe towards reducing hallucination in neural surface realisation",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2673\u2013",
            "year": 2019
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark"
            ],
            "title": "Learning transferable visual models from natural language supervision",
            "year": 2021
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J Liu."
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "The Journal of Machine Learning Research,",
            "year": 2020
        },
        {
            "authors": [
                "Vikas Raunak",
                "Arul Menezes",
                "Marcin JunczysDowmunt."
            ],
            "title": "The curious case of hallucinations in neural machine translation",
            "venue": "arXiv preprint arXiv:2104.06683.",
            "year": 2021
        },
        {
            "authors": [
                "Vipula Rawte",
                "Amit Sheth",
                "Amitava Das."
            ],
            "title": "A survey of hallucination in large foundation models",
            "venue": "arXiv preprint arXiv:2309.05922.",
            "year": 2023
        },
        {
            "authors": [
                "Cl\u00e9ment Rebuffel",
                "Marco Roberti",
                "Laure Soulier",
                "Geoffrey Scoutheeten",
                "Rossella Cancelliere",
                "Patrick Gallinari."
            ],
            "title": "Controlling hallucinations at word level in data-to-text generation",
            "venue": "Data Mining and Knowledge Discovery, pages 1\u201337.",
            "year": 2022
        },
        {
            "authors": [
                "Anna Rohrbach",
                "Lisa Anne Hendricks",
                "Kaylee Burns",
                "Trevor Darrell",
                "Kate Saenko."
            ],
            "title": "Object hallucination in image captioning",
            "venue": "arXiv preprint arXiv:1809.02156.",
            "year": 2018
        },
        {
            "authors": [
                "Sheng Shen",
                "Liunian Harold Li",
                "Hao Tan",
                "Mohit Bansal",
                "Anna Rohrbach",
                "Kai-Wei Chang",
                "Zhewei Yao",
                "Kurt Keutzer"
            ],
            "title": "How much can clip benefit vision-and-language tasks? arXiv preprint arXiv:2107.06383",
            "year": 2021
        },
        {
            "authors": [
                "Kristina Striegnitz",
                "Alexandre AJ Denis",
                "Andrew Gargett",
                "Konstantina Garoufi",
                "Alexander Koller",
                "Mari\u00ebt Theune"
            ],
            "title": "Report on the second second challenge on generating instructions in virtual environments (give-2.5)",
            "year": 2011
        },
        {
            "authors": [
                "Liam van der Poel",
                "Ryan Cotterell",
                "Clara Meister."
            ],
            "title": "Mutual information alleviates hallucinations in abstractive summarization",
            "venue": "arXiv preprint arXiv:2210.13210.",
            "year": 2022
        },
        {
            "authors": [
                "Chaojun Wang",
                "Rico Sennrich."
            ],
            "title": "On exposure bias, hallucination and domain shift in neural machine translation",
            "venue": "arXiv preprint arXiv:2005.03642.",
            "year": 2020
        },
        {
            "authors": [
                "Su Wang",
                "Ceslee Montgomery",
                "Jordi Orbay",
                "Vighnesh Birodkar",
                "Aleksandra Faust",
                "Izzeddin Gur",
                "Natasha Jaques",
                "Austin Waters",
                "Jason Baldridge",
                "Peter Anderson."
            ],
            "title": "Less is more: Generating grounded navigation instructions from landmarks",
            "venue": "Proceed-",
            "year": 2022
        },
        {
            "authors": [
                "Sam Wiseman",
                "Stuart M Shieber",
                "Alexander M Rush."
            ],
            "title": "Challenges in data-to-document generation",
            "venue": "arXiv preprint arXiv:1707.08052.",
            "year": 2017
        },
        {
            "authors": [
                "Yijun Xiao",
                "William Yang Wang."
            ],
            "title": "On hallucination and predictive uncertainty in conditional language generation",
            "venue": "arXiv preprint arXiv:2103.15025.",
            "year": 2021
        },
        {
            "authors": [
                "Weijia Xu",
                "Sweta Agrawal",
                "Eleftheria Briakou",
                "Marianna J Martindale",
                "Marine Carpuat."
            ],
            "title": "Understanding and detecting hallucinations in neural machine translation via model introspection",
            "venue": "arXiv preprint arXiv:2301.07779.",
            "year": 2023
        },
        {
            "authors": [
                "Jianguo Zhang",
                "Trung Bui",
                "Seunghyun Yoon",
                "Xiang Chen",
                "Zhiwei Liu",
                "Congying Xia",
                "Quan Hung Tran",
                "Walter Chang",
                "Philip Yu."
            ],
            "title": "Few-shot intent detection via contrastive pre-training and fine-tuning",
            "venue": "Proceedings of Empirical Methods in Natural Lan-",
            "year": 2021
        },
        {
            "authors": [
                "Lingjun Zhao",
                "Khanh Nguyen",
                "Hal Daum\u00e9 III."
            ],
            "title": "Define, evaluate, and improve task-oriented cognitive capabilities for instruction generation models",
            "venue": "Findings of ACL.",
            "year": 2023
        },
        {
            "authors": [
                "Ming Zhao",
                "Peter Anderson",
                "Vihan Jain",
                "Su Wang",
                "Alexander Ku",
                "Jason Baldridge",
                "Eugene Ie."
            ],
            "title": "On the evaluation of vision-and-language navigation instructions",
            "venue": "arXiv preprint arXiv:2101.10504.",
            "year": 2021
        },
        {
            "authors": [
                "Chunting Zhou",
                "Graham Neubig",
                "Jiatao Gu",
                "Mona Diab",
                "Paco Guzman",
                "Luke Zettlemoyer",
                "Marjan Ghazvininejad."
            ],
            "title": "Detecting hallucinated content in conditional neural sequence generation",
            "venue": "arXiv preprint arXiv:2011.02593.",
            "year": 2020
        },
        {
            "authors": [
                "Zhao"
            ],
            "title": "2023), we convert each panoramic observation ot into a collection of 36 images that represent the first-person views obtained from 36 gaze directions. We feed these into a pre-trained vision model (Radford",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Performance of neural-network-based models on generating navigation instructions is substantially inferior to that of humans (Zhao et al., 2023). These models often hallucinate, generating references to objects or actions that do not exist or are impossible to execute in the environment. Similar behavior has been observed in language models in other domains of text generation (Raunak et al., 2021; Ji et al., 2023; Xiao and Wang, 2021; Lee et al., 2018; Guerreiro et al., 2022; Rawte et al., 2023).\nInstructions containing hallucinations can confuse or misdirect humans, leading to frustration and sometimes even catastrophic mistakes. Detecting hallucinations is therefore essential to improve instruction generation models and inform risk to human users. Nevertheless, ground-truth wordlevel hallucination labels are typically not readily available in this domain. Meanwhile, hiring crowdworkers to annotate instructions can be very costly (Anderson et al., 2018b; He et al., 2021; Wang et al., 2022; Gao et al., 2022).\nWe propose a data-efficient weakly supervised approach to hallucination detection. Our approach\nreduces the necessary supervision in two ways. First, we leverage a pre-trained vision-language model (Guhur et al., 2021) that has learned transferable representations of path-instruction pairs through self-supervised learning. Second, we introduce data-augmentation strategies to create synthetic data with \u201cfree\u201d hallucination labels. We fine-tune the pre-trained model with the synthesized data using a contrastive learning objective to learn representations that separate positive examples (hallucinations) from negative examples (non-hallucinations). Our model outperforms various baselines in terms of F-1 scores on human-annotated evaluation data, beating an LSTM- and a Transformer-based models by 6.2 and 10.0 points, respectively. Ablation studies demonstrate the effectiveness of the proposed self-supervised pre-training and contrastive fine-tuning approach. We release the code, models, and data at https://lingjunzhao. github.io/hallucination_detection.html."
        },
        {
            "heading": "2 Related Work",
            "text": "Hallucination detection. Neural sequence to sequence models are prone to generate hallucinations, where the outputs are inconsistent with the inputs or the environments (M\u00fcller et al., 2019; Maynez et al., 2020; Wiseman et al., 2017; Martindale et al., 2019; Durmus et al., 2020; Ji et al., 2023). Recent work largely focuses on text-only domains (Wang and Sennrich, 2020; Zhou et al., 2020; Chen et al., 2021; Dale et al., 2022; Xu et al., 2023; Nie et al., 2019; Falke et al., 2019; Krys\u0301cin\u0301ski et al., 2019; Rebuffel et al., 2022; Liu et al., 2021; van der Poel et al., 2022) and image captioning (Rohrbach et al., 2018; Dai et al., 2022; Biten et al., 2022; Li et al., 2023; Gunjal et al., 2023). To the best of our knowledge, our work is the first study of hallucination in grounded instruction generation.\nGrounded Instruction Generation. Instruction generation has been commonly studied in navigation settings (Anderson et al., 1991; Byron et al., 2010; Koller et al., 2010; Striegnitz et al., 2011; Goeddel and Olson, 2012; Fried et al., 2017, 2018; Wang et al., 2022; Kamath et al., 2022). Recent work by Zhao et al. (2023) reveals a significant gap between the performance of models and humans. Our work constructs a model that can be useful for evaluating and enhancing instruction-generation models. Huang et al. (2019) and Zhao et al. (2021) train LSTM-based discriminative models with contrastive learning to score instructions. We follow a similar approach but focus on identifying word-level hallucinations, and effectively leverage a large pre-trained Transformer model."
        },
        {
            "heading": "3 Problem Setting",
            "text": "Grounded instruction generation. Our task takes place in an environment, where a speaker model S(u | r) composes an instruction u to communicate an imaginary trajectory r to a follower so that the latter can generate the same trajectory in the environment. An instruction is a sequence of words ui, whereas a trajectory is a sequence of observations ot and actions at. We employ the Matterport3D simulator for experiments (Anderson et al., 2018b) which embeds a follower in a 3D model of a real-world residential building. The observation ot of the follower comprises of an RGB image representing the panoramic view at a location in a building, and orientation features encoding the follower\u2019s gaze direction. Each action at moves the follower to a new location close to where it is standing and changes its observation.\nSpeaker model. We follow Zhao et al. (2023) to train a T5-based (Raffel et al., 2020) speaker model. This model encodes a trajectory into a sequence of hidden vectors and applies multi-headed attention on those vectors to generate an instruction autoregressively. It is trained on the Room-to-Room (R2R) dataset provided by the Matterport3D simulator. Detail about the model is provided in \u00a7A.1.\nHallucination in grounded instruction. Instructions generated by our speaker model often contain words that are inconsistent with the input trajectory. We refer to those words as hallucinations. Similar to prior work (Zhou et al., 2020), we observe two types of hallucinations:\n\u2022 Intrinsic hallucination is a word that needs to\nbe replaced because it inaccurately describes an observation or action. For example, an instruction says \u201cWalk past the reception desk and out the door on the right,\u201d but in the described trajectory, the door is on the left;\n\u2022 Extrinsic hallucination is a word that needs to be removed because it has no correspondence in the input trajectory. Our model typically exhibits this type of hallucination by repeatedly generating the same sentence, e.g., \u201cWalk out of the office. Walk into the hallway and turn left. Walk into the hallway and turn left.\u201d\nWe formulate hallucination detection as binary classification: given an input x = (r,u, i) consisting of a trajectory r, an instruction u, and an index i \u2208 {1, \u00b7 \u00b7 \u00b7 , |u|}, decide whether the word ui is a hallucination, i.e. whether it should be replaced or removed to make u consistent with r.\nCandidate selection. For each instruction, we identify a set of candidate words for classification, which are (a) directional words like left, right, etc. (see \u00a7A.2 for a full list) as well as (b) nouns identified by the SpaCy part-of-speech tagger (Honnibal and Montani, 2017)."
        },
        {
            "heading": "4 Hallucination Detection Model",
            "text": ""
        },
        {
            "heading": "4.1 Architecture",
            "text": "We learn a classifier C(y = 1 | x = (r,u, i)) to decide whether a word ui is hallucinated. Our model is based on the Airbert model (Guhur et al., 2021), which inherits the ViLBERT architecture (Lu et al., 2019). An overview of the model is given in Figure 1. It implements two Transformers: one encodes the instruction u and the other encodes the trajectory r. We wrap the word to be classified ui between a pair of special tokens ([BH] and [EH]). Let hlang be the output of the language-encoding Transformer, and hvision be that of the vision-encoding Transformer. The model computes a score function s(x) = s(r,u, i) = w\u22a4(hlang \u2299 hvision), where w is a learnable vector, and \u2299 denotes element-wise multiplication. More details about the model are given in \u00a7A.1."
        },
        {
            "heading": "4.2 Learning approach",
            "text": "Self-supervised pre-training. Instead of learning from scratch, we fine-tune a pre-trained checkpoint of the Airbert model. The checkpoint was first trained on a large collection of 1.4M images\nand 0.7M captions collected from AirBnB. It was subsequently adapted for a trajectory-instruction compatibility estimation task using the Room-toRoom dataset. The objective in each phase combines BERT-style pre-training (mask and pair prediction) with contrastive learning. We refer the readers to the original paper for an elaborate description of the pre-training phase.\nContrastive fine-tuning. We assume a dataset of contrastive pairs (x+,x\u2212). The positive and negative examples of a pair have the same trajectory r and word index i, but differ in the instruction u. The classified word in x\u2212 is a hallucination, whereas that in x+ is not. For each pair, we compute the model scores s(x+) and s(x\u2212), and construct the softmax distribution p\u0302 = Softmax(s) where s = (s(x+), s(x\u2212)). We then train the model to recognize the positive example by minimizing the cross entropy between p\u0302 and p\u22c6 = (1, 0). This objective effectively forces the representation of the trajectory to be similar to that of the positive instruction and dissimilar to that of the negative instruction. At inference time, we define the hallucination detection classifier as C(x) = 1\u2212 \u03c3(s(x)), where \u03c3 is the sigmoid function."
        },
        {
            "heading": "4.3 Synthesizing data creation",
            "text": "Even for fine-tuning, acquiring human-labeled data can be prohibitively expensive. For evaluation, we manually annotated a small sample of labels (\u00a75). The annotation process was laborious, with an average time of 30 minutes required to annotate just 10 instructions. Based on our calculations, with a compensation of 15 USD per hour, it would cost approximately 9,000 USD to hire crowd workers to annotate all instances (\u223c12,000) in the R2R train-\ning set. Thus, we propose a more cost-effective methodology for generating training data.\nSynthetic negative examples. We start with a training example (u+, r) in the Room-to-Room training set and modify the human-written instruction u+ to create instructions with hallucinations. We first extract the candidate words in the instruction (\u00a73). To create an intrinsic hallucination, we choose a candidate word and apply the following procedure:\n\u2022 If the word is a direction, we replace it with an alternative direction. E.g., \u201cWalk downup one flight of stairs and stop on the landing.\u201d; \u2022 If it is a room, we substitute it with another room randomly selected from a pre-composed list. E.g., \u201cExit the bedroom balcony via the farthest left. Walk toward the couch. Stop there.\u201d; \u2022 Otherwise, we swap it for another word in the instruction that is neither a direction nor a room. E.g., \u201cExit the bedroom using the door step on the left then go straight until you get to the stairs and wait on the second step door.\u201d\nUsing this procedure, we first generate an intrinsic hallucination in u+ to synthesize u\u2212. Then, with a probability of 0.5, we synthesize another intrinsic hallucination in each of u+ and u\u2212. This step makes the training instructions more similar to the test-time inputs, which may contain multiple intrinsic hallucinations as they are generated by imperfect speaker models.\nTo create an instruction with extrinsic hallucinations, we append a sentence, taken from u+ or another instruction, to the end of a random sentence in u+. For example: \u201cWalk out of the office. Walk into the hallway and turn left. Walk\ninto the hallway and turn left.\u201d. Every word in the added sentence is considered an extrinsic hallucination. We do not create additional intrinsic hallucinations in the instruction.\nAlleviating input-distribution shift. Model trained only on human-written instruction may perform poorly on model-generated instructions. Therefore, we also include \u201chigh-quality\u201d modelgenerated instructions on the R2R training set as positive examples and apply the same strategies to generate negative examples. The quality of an instruction is measured by the success rate of an ensemble of VLN\u27f3 BERT instruction-following agents (Hong et al., 2021) in recreating the described trajectory. We consider a model-generated instruction to be of high quality if at least 80% of the ensemble agents can successfully reach the final location in the described trajectory."
        },
        {
            "heading": "5 Experiments",
            "text": "Data. Following the procedure described in \u00a74.3, we generate a training set of 325,346 contrastive pairs. For evaluation, we use the same 75 evaluation trajectories in (Zhao et al., 2023) to form the test set. We randomly select another set of 20 trajectories in the R2R validation seen set for development. The environments in which the evaluation trajectories are generated are a subset of the train-\ning environments. We use the speaker model to generate instructions from these trajectories. The first two authors then manually annotate word-level hallucinations, creating 209 development examples and 632 test examples. The final labels are decided by mutual agreement. We choose the decision threshold of a model to maximize its F-1 score on the development set.\nBaselines. (i) random classifier assigns a label chosen uniformly at random, (ii) speaker model probability defines the hallucination probability C(x) = 1 \u2212 S(ui | r;u<i) where x = (r,u, i), S is the speaker model (\u00a7 3), and u<i is the instruction generated up to step i\u2212 1 for the input r; (iii) LSTM and (iv) T5 are binary classifiers learned under a standard maximum-likelihood objective. They implement an encoder-decoder architecture based on LSTM and Transformer, respectively, and are trained using the same synthetic dataset as our proposed model. These models are initialized with random parameters. The detailed implementations and hyperparameters of all models are given in \u00a7A.1.\nMain results (Table 1). The speaker-modelprobability is a remarkably strong baseline, despite not trained for hallucination detection. Its performance is on par with that of T5, which is the same model but trained specifically for hallucination detection. The LSTM-based model outperforms the T5-based models. Scaling up the size of the T5 model improves the recall score by 10 points. Our proposed model (fine-tuned Airbert) beats all baselines by wide margins in terms of F-1 score for hallucination labels, (+10.0 versus T5-base, +6.2 versus LSTM). It excels in precision compared to the baselines. We also include results on the development set in \u00a7A.3.\nModel highlight: Walk up the steps and turn right . Walk up the steps and turn right \u2026 Gold highlight: Walk up the steps and turn right . Walk up the steps and turn right \u2026\n(a) Success on detecting extrinsic hallucination: the second sentence should be removed entirely; the model marks all the candidate words in the sentence.\nModel highlight: \u2026 Walk past the bed and exit the bedroom \u2026 Gold highlight: \u2026 Walk past the bed and exit the bedroom \u2026\n(b) Success on detecting intrinsic hallucination: the correct direction is to go to the left side of the bedroom, not exiting it.\nModel highlight: Walk past the couch and stop in front of the TV Gold highlight: Walk past the couch and stop in front of the TV\n(c) Model misidentifies the stopping location due to lacking depth information: the TV in the far left corner looks to be close to the true stopping location.\nModel highlight: Walk down the hallway and stop in the first doorway on your left Gold highlight: Walk down the hallway and stop in the first doorway on your left\n(d) Ambiguous direction: a slight left turn that appears like a straight walk in this viewpoint.\nFigure 3: Some successful and failure cases of the fine-tuned Airbert model. The blue arrow indicates the described path, and the green represents the next location.\nAblation studies (Figure 2). Our results confirm that self-supervised pre-training and contrastive fine-tuning are requisite to the performance of our model. Without pre-training, our model is just as bad as the LSTM-based model. We also compare fine-tuning via contrastive learning with fine-tuning via a maximum-likelihood learning. In the latter approach, the model simply takes as input an example (r,u, i) and learns to directly predict the true label. The approach underperforms contrastive learning by 4.9 F-1 points. Our finding aligns with previous work (Gunel et al., 2021; Zhang et al., 2021; Goyal et al., 2023), suggesting that contrastive learning is effective not only as a representation learning objective, but also as a classification objective.\nError and Qualitative Analysis. In Table 2, we break down the performance of our model by word type. Our model struggles with detecting room and object hallucinations, indicating that its understanding of visually grounded words is lacking. Especially, it has relatively low recall on object hallucinations, potentially due to lack of diversity of this word type in the training data. Figure 3 shows a few successful and failure examples of our model."
        },
        {
            "heading": "6 Conclusion",
            "text": "This work is an early attempt to address the hallucination issue in grounded instruction generation. We have shown that techniques like self-supervised\npre-training on multimodal data and contrastive fine-tuning on synthetic data are promising scalable approaches. We hope that these directions can be further developed in future work.\nLimitations\nDespite the effectiveness of the data generation method, this approach requires substantial domainspecific knowledge. Our method, particularly to generate directional hallucinations, is based on heuristics and does not take into account the actual environment. Another limitation is the small size of the evaluation datasets due to the expensive cost of annotation."
        },
        {
            "heading": "Acknowledgments",
            "text": "We thank the CLIP Laboratory at Maryland and our reviewers for providing helpful feedback to improve the manuscript."
        },
        {
            "heading": "A Appendices",
            "text": "A.1 Models\nSpeaker. The speaker model takes as input a trajectory and computes a distribution over instructions. To encode a trajectory r, following prior work (Shen et al., 2021; Zhao et al., 2023), we convert each panoramic observation ot into a collection of 36 images that represent the first-person views obtained from 36 gaze directions. We feed these into a pre-trained vision model (Radford et al., 2021) to obtain a set of view vectors {oit}36i=1. For each action at, which corresponds to an adjacent location, let k \u2208 {1, \u00b7 \u00b7 \u00b7 , 36} be the direction towards that location and let \u03b8 = (\u03b8hor, \u03b8ver) be the horizontal and vertical angles of that direction. We represent at by concatenating the visual features okt with the directional features [cos\u03b8, sin\u03b8]. The sequence of observation and action representations is fed into a Transformer encoder to produce a sequence of hidden vectors. A transformer decoder then applies multi-headed attention to those vectors and generates an instruction u auto-regressively.\nAirbert model. The input of the classifier is a trajectory r and an instruction u. The instruction has the following format:[ [CLS], u1, . . . , [BH], ui, [EH], . . . , u|u|, [SEP]\n] where the word to be classified ui is enclosed by special tokens [BH] and [EH], and the [CLS] and [SEP] tokens mark the beginning and the end. Each token are replaced by a sum of a token embedding and a positional embedding. We pass this sequence of embeddings into a Transformer.\nFor the trajectory, the model extracts from a panoramic view ot a set of image regions {o(j)t }Kj=1 and represents the sequence of observations as:\n[ [IMG],o (1) 1 , . . . ,o (K) 1 , [IMG],o (1) 2 , . . . ,o (K) 2 ,\n\u00b7 \u00b7 \u00b7 , [IMG],o(1)T , . . . ,o (K) T ]\nwhere [IMG] is the embedding of an observationseparating token. Each image region o(j)t is converted into a visual embedding, which is an addition of three embeddings: visual embedding (computed by a Faster R-CNN model (Anderson et al., 2018a)), directional embedding, and region-index embedding. We feed the sequence of visual embeddings into a second Transformer.\nLet hlang be the output at the position of the [CLS] token of the language-encoding Transformer, and hvision be the output at the position of the first [IMG] token of the vision-encoding Transformer. The score function s(x) is defined as:\ns(x) = s(r,u, i) = w\u22a4(hlang \u2299 hvision)\nwhere w is a learnable vector, and \u2299 denotes element-wise multiplication.\nT5. This model is the same as the speaker model. However, instead of generating an instruction, it computes a score s(x) like the Airbert model. The input x is also a tuple (r,u, i). The instruction u has the same format as in the case of the Airbert model, with the word to be classified surrounded by two special tokens. Let {hj}|u|j=1 be the sequence of hidden vectors obtain after decoding the input instruction. We compute the mean vector h = 1 |u| \u2211|u| j=1 hj . The score is computed as s(x) = w\u22a4h, where w is a learnable vector.\nLSTM. This model is similar to the T5 model except that the encoder and decoder are LSTMs.\nHyperparamters and Computation. The hyperparameters and computation cost of all models are listed in Table 3.\nA.2 Word replacement\nWe compiled a list of direction words and divided them into groups (Table 4). When constructing a negative example, if a word is selected, a replacement is randomly selected among the remaining words in the same group.\nOur compiled list of rooms to generate synthetic examples are: { \u201claundry room\u201d, \u201cmudroom\u201d, \u201cfamily room\u201d, \u201cbalcony\u201d, \u201cutility room\u201d, \u201ctool room\u201d, \u201centryway\u201d, \u201cfoyer\u201d, \u201clobby\u201d, \u201clibrary\u201d, \u201cbathroom\u201d, \u201cbar\u201d, \u201cspa\u201d, \u201csauna\u201d, \u201cliving room\u201d, \u201cother room\u201d, \u201cstaircase\u201d, \u201cgarage\u201d, \u201challway\u201d, \u201coffice\u201d, \u201cclassroom\u201d, \u201coutdoor areas\u201d, \u201cmeeting room\u201d, \u201cconference room\u201d, \u201cdining room\u201d, \u201clounge\u201d, \u201cbedroom\u201d, \u201cporch\u201d, \u201cterrace\u201d, \u201cdeck\u201d, \u201cdriveway\u201d, \u201ckitchen\u201d, \u201ctoilet\u201d, \u201cworkout room\u201d, \u201cexercise room\u201d, \u201cgym\u201d, \u201ctv room\u201d, \u201crecreation room \u201d, \u201cgame room\u201d, \u201ccloset\u201d, \u201cjunk\u201d, \u201cstudy\u201d, \u201cguest room\u201d, \u201cmusic room\u201d, \u201chome theater\u201d, \u201csunroom\u201d, \u201cconservatory\u201d, \u201cplayroom\u201d, \u201cpantry\u201d,\n\u201cstorage room\u201d, \u201cattic\u201d, \u201cbasement\u201d, \u201cgallery\u201d, \u201cgreenhouse\u201d, \u201cyoga studio\u201d, \u201cmeditation room\u201d, \u201cstairs\u201d, \u201cstaircase\u201d, \u201cfloor\u201d }. These are based on room labels in the Matterport3D dataset (Chang et al., 2017) and suggestions of GPT-4 (OpenAI, 2023).\nA.3 Results on development set (Table 5)"
        }
    ],
    "title": "Hallucination Detection for Grounded Instruction Generation",
    "year": 2023
}