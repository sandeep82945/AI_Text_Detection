{
    "abstractText": "Large language models (LLMs) enable zeroshot approaches in open-domain question answering (ODQA), yet with limited advancements as the reader is compared to the retriever. This study aims at the feasibility of a zero-shot reader that addresses the challenges of computational cost and the need for labeled data. We find that LLMs are distracted due to irrelevant documents in the retrieved set and the overconfidence of the generated answers when they are exploited as zero-shot readers. To tackle these problems, we mitigate the impact of such documents via Distraction-aware Answer Selection (DAS) with a negation-based instruction and score adjustment for proper answer selection. Experimental results show that our approach successfully handles distraction across diverse scenarios, enhancing the performance of zeroshot readers. Furthermore, unlike supervised readers struggling with unseen data, zero-shot readers demonstrate outstanding transferability without any training.",
    "authors": [
        {
            "affiliations": [],
            "name": "Sukmin Cho"
        },
        {
            "affiliations": [],
            "name": "Jeong yeon Seo"
        },
        {
            "affiliations": [],
            "name": "Soyeong Jeong"
        },
        {
            "affiliations": [],
            "name": "Jong C. Park"
        }
    ],
    "id": "SP:4cad20d6082260c9ed9142a837b6dc6056de682d",
    "references": [
        {
            "authors": [
                "Jonathan Berant",
                "Andrew Chou",
                "Roy Frostig",
                "Percy Liang."
            ],
            "title": "Semantic parsing on Freebase from question-answer pairs",
            "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1533\u20131544, Seattle, Wash-",
            "year": 2013
        },
        {
            "authors": [
                "Danqi Chen",
                "Adam Fisch",
                "Jason Weston",
                "Antoine Bordes."
            ],
            "title": "Reading Wikipedia to answer opendomain questions",
            "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1870\u20131879,",
            "year": 2017
        },
        {
            "authors": [
                "Sukmin Cho",
                "Soyeong Jeong",
                "Jeongyeon Seo",
                "Jong C Park."
            ],
            "title": "Discrete prompt optimization via constrained generation for zero-shot re-ranker",
            "venue": "arXiv preprint arXiv:2305.13729.",
            "year": 2023
        },
        {
            "authors": [
                "Yung-Sung Chuang",
                "Wei Fang",
                "Shang-Wen Li",
                "Wen-tau Yih",
                "James Glass."
            ],
            "title": "Expand, rerank, and retrieve: Query reranking for open-domain question answering",
            "venue": "arXiv preprint arXiv:2305.17080.",
            "year": 2023
        },
        {
            "authors": [
                "Hyung Won Chung",
                "Le Hou",
                "Shayne Longpre",
                "Barret Zoph",
                "Yi Tay",
                "William Fedus",
                "Eric Li",
                "Xuezhi Wang",
                "Mostafa Dehghani",
                "Siddhartha Brahma"
            ],
            "title": "Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416",
            "year": 2022
        },
        {
            "authors": [
                "Srinivasan Iyer",
                "Xi Victoria Lin",
                "Ramakanth Pasunuru",
                "Todor Mihaylov",
                "D\u00e1niel Simig",
                "Ping Yu",
                "Kurt Shuster",
                "Tianlu Wang",
                "Qing Liu",
                "Punit Singh Koura"
            ],
            "title": "Opt-iml: Scaling language model instruction meta learning through the lens of generalization",
            "year": 2022
        },
        {
            "authors": [
                "Gautier Izacard",
                "Edouard Grave."
            ],
            "title": "Leveraging passage retrieval with generative models for open domain question answering",
            "venue": "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume,",
            "year": 2021
        },
        {
            "authors": [
                "Robin Jia",
                "Percy Liang."
            ],
            "title": "Adversarial examples for evaluating reading comprehension systems",
            "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2021\u20132031, Copenhagen, Denmark. Association for",
            "year": 2017
        },
        {
            "authors": [
                "Mandar Joshi",
                "Eunsol Choi",
                "Daniel Weld",
                "Luke Zettlemoyer."
            ],
            "title": "TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension",
            "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Vol-",
            "year": 2017
        },
        {
            "authors": [
                "Jia-Huei Ju",
                "Jheng-Hong Yang",
                "Chuan-Ju Wang"
            ],
            "title": "Text-to-text multi-view learning for passage",
            "year": 2021
        },
        {
            "authors": [
                "Vladimir Karpukhin",
                "Barlas Oguz",
                "Sewon Min",
                "Patrick Lewis",
                "Ledell Wu",
                "Sergey Edunov",
                "Danqi Chen",
                "Wen-tau Yih."
            ],
            "title": "Dense passage retrieval for opendomain question answering",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural",
            "year": 2020
        },
        {
            "authors": [
                "Jungo Kasai",
                "Keisuke Sakaguchi",
                "Yoichi Takahashi",
                "Ronan Le Bras",
                "Akari Asai",
                "Xinyan Yu",
                "Dragomir R. Radev",
                "Noah A. Smith",
                "Yejin Choi",
                "Kentaro Inui"
            ],
            "title": "Realtime QA: what\u2019s the answer right now? CoRR, abs/2207.13332",
            "year": 2022
        },
        {
            "authors": [
                "Daniel Khashabi",
                "Tushar Khot",
                "Ashish Sabharwal",
                "Dan Roth."
            ],
            "title": "Learning what is essential in questions",
            "venue": "Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017), pages 80\u201389, Vancouver, Canada. Association",
            "year": 2017
        },
        {
            "authors": [
                "Uszkoreit",
                "Quoc Le",
                "Slav Petrov."
            ],
            "title": "Natural questions: A benchmark for question answering research",
            "venue": "Transactions of the Association for Computational Linguistics, 7:452\u2013466.",
            "year": 2019
        },
        {
            "authors": [
                "Kenton Lee",
                "Ming-Wei Chang",
                "Kristina Toutanova."
            ],
            "title": "Latent retrieval for weakly supervised open domain question answering",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6086\u20136096, Florence, Italy.",
            "year": 2019
        },
        {
            "authors": [
                "Yoav Levine",
                "Ori Ram",
                "Daniel Jannai",
                "Barak Lenz",
                "Shai Shalev-Shwartz",
                "Amnon Shashua",
                "Kevin LeytonBrown",
                "Yoav Shoham."
            ],
            "title": "Huge frozen language models as readers for open-domain question answering",
            "venue": "ICML 2022 Workshop on Knowledge",
            "year": 2022
        },
        {
            "authors": [
                "Daliang Li",
                "Ankit Singh Rawat",
                "Manzil Zaheer",
                "Xin Wang",
                "Michal Lukasik",
                "Andreas Veit",
                "Felix Yu",
                "Sanjiv Kumar."
            ],
            "title": "Large language models with controllable working memory",
            "venue": "arXiv preprint arXiv:2211.05110.",
            "year": 2022
        },
        {
            "authors": [
                "Linqing Liu",
                "Patrick Lewis",
                "Sebastian Riedel",
                "Pontus Stenetorp."
            ],
            "title": "Challenges in generalization in open domain question answering",
            "venue": "Findings of the Association for Computational Linguistics: NAACL 2022, pages 2014\u20132029, Seattle, United States. Asso-",
            "year": 2022
        },
        {
            "authors": [
                "McAuley"
            ],
            "title": "Learning to attend on essential",
            "year": 2019
        },
        {
            "authors": [
                "Wei Li",
                "Peter J. Liu"
            ],
            "title": "Exploring the limits",
            "year": 2020
        },
        {
            "authors": [
                "Denny Zhou"
            ],
            "title": "Large language models can be",
            "year": 2023
        },
        {
            "authors": [
                "Susan Zhang",
                "Stephen Roller",
                "Naman Goyal",
                "Mikel Artetxe",
                "Moya Chen",
                "Shuohui Chen",
                "Christopher Dewan",
                "Mona Diab",
                "Xian Li",
                "Xi Victoria Lin"
            ],
            "title": "Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068",
            "year": 2022
        },
        {
            "authors": [
                "Nogueira dos Santos et al",
                "Ju"
            ],
            "title": "2021). Recently, LLMs serve as zero-shot re-rankers with outstanding performance gain by computing the measure (Sachan et al., 2022",
            "year": 2023
        },
        {
            "authors": [
                "Sachan"
            ],
            "title": "For a fair comparison against the supervised readers, DPR (Karpukhin et al., 2020) and FiD (Izacard and Grave, 2021) 2, we use the test set of each dataset",
            "year": 2022
        },
        {
            "authors": [
                "Iyer"
            ],
            "title": "If you don\u2019t know the answer, return unanswerable\". When transmitting a query, a document, and an instruction to LLMs, we use the input template following the setting from Chung et al",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Open domain question answering (ODQA) is a task for answering questions with the evidence documents fetched from a large corpus (Voorhees and Tice, 2000). A retrieve-read framework has achieved remarkable performance in ODQA by fine-tuning the language models with labeled datasets (Lee et al., 2019; Karpukhin et al., 2020; Izacard and Grave, 2021). The emergence of large language models (LLMs) has enabled the exploration of zero-shot approaches in this framework, with less emphasis on the reader component (Sachan et al., 2022; Chuang et al., 2023; Levine et al., 2022).\nUtilizing an LLM as a reader provides an advantage in the generalization ability with the rich world knowledge, unlike conventional small-sized supervised readers (Karpukhin et al., 2020; Izacard\n\u2217 Corresponding author\nand Grave, 2021). While the supervised readers show remarkable performance on ODQA, they are hampered by two weaknesses: the high computational cost involved in training and the necessity of annotated query-document datasets. These limitations impede the transferability of readers to diverse tasks and domains. To solve this, we aim to validate the feasibility of using an LLM as a reader, leveraging its inherent advantages while reducing the aforementioned limitations.\nHowever, the performance of an LLM in various tasks is easily distracted by irrelevant documents (Li et al., 2022; Shi et al., 2023), underscoring the importance of resolving these challenges in ODQA. The tendency of an LLM to generate incorrect answers becomes apparent when reading retrieved sets that include irrelevant documents. These documents, while related to the query, may lack the necessary information to provide an answer, leading to the occurrence of hallucination. This emphasizes the need for proper handling of such documents to fully harness the potential of an LLM, thereby achieving reliable performance as a reader. This paper addresses the requisite of hallucination mitigation to validate the possibility of an LLM as a zero-shot reader.\nIn this paper, we propose Distraction-aware Answer Selection (DAS), handling the challenges posed by irrelevant documents and overconfident scores as shown in Figure 1. First, we provide\nmodels with an \"unanswerable\" instruction, allowing them to abstain from answering. Then, we adjust the answer scores by reflecting the query generation score as the relevance between the given query-document pairs. These approaches reduce the impact of irrelevant documents and improve the selection of the correct answer from the relevant document.\nWe evaluate our proposed method on representative ODQA benchmarks with two publicly open LLMs, FLAN-T5 (Chuang et al., 2023) and OPTIML-MAX (Iyer et al., 2022). This results in substantial performance improvements achieved by ours compared to a na\u00efve LLM across all scenarios. Note that ours effectively alleviates the hallucination induced by irrelevant documents by enhancing the robustness against the number of documents that are read. Furthermore, an LLM with our method exhibits excellent transferability compared to the supervised reader, offering the untapped potential of an LLM as a zero-shot reader.\nOur contributions in this paper are threefold:\n\u2022 We tackle the distraction incurred by irrelevant documents and overconfident scores when exploiting an LLM as a zero-shot reader in ODQA tasks.\n\u2022 We introduce Distraction-aware Answer Selection (DAS) for a zero-shot reader, with the unanswerable instruction and the score adjustment eliciting its deductive ability.\n\u2022 We empirically verify the efficacy of our proposed approach in effectively mitigating hallucination and unlocking the feasibility of zeroshot readers with a generalization ability."
        },
        {
            "heading": "2 Related Work",
            "text": "Zero-shot Approach in ODQA The advent of an LLM has shown the potential that it can be used in two stages without parameter updates. For the retrieval stage, an LLM is exploited as a re-ranker via query generation or document permutation (Sachan et al., 2022; Cho et al., 2023; Sun et al., 2023) or expanded query to diverse pseudo queries for improving the performance of supervised retrievers (Liu et al., 2022; Yu et al., 2023; Chuang et al., 2023). For the reader stage, Levine et al. (2022) attempted to utilize an LLM as a zero-shot reader, addressing the irrelevant documents through a re-ranker. In this study, we focus on a fully zero-shot reader without an additional module.\nDistraction from Noisy Input Recent work addresses the negative impact of noisy inputs when exploiting an LLM in diverse tasks. LLMs are easily distracted by the noisy input having incorrect or irrelevant information on machine reading comprehension tasks (Li et al., 2022; Su et al., 2022; Shi et al., 2023). However, the ODQA task increases the complexity, where large-scale document sets appear within unrelated documents. Given the impact of distracting sentences in QA (Khashabi et al., 2017; Jia and Liang, 2017; Ni et al., 2019), our approach aims to alleviate them."
        },
        {
            "heading": "3 Method",
            "text": ""
        },
        {
            "heading": "3.1 Preliminaries",
            "text": "To adopt the LLM into the reader, we define a two-step answering pipeline consisting of answer candidate generation and final answer selection.\nAnswer Candidate Generation The LLM M generates answer candidate ai based on the given query q, the evidence document di in retrieved documents D and the reading comprehension instruction \u03c1rc via greedy decoding. This process results in an answer candidate set S = {(ai, di)}ki=1.\nFinal Answer Selection We select the final document-answer pair p\u2217 = (a\u2217, d\u2217) from an answer candidate set S based on the generation probability PM (ai|q, di, \u03c1rc) as the answer score. The document-answer pair with the highest probability is chosen as the most likely correct answer."
        },
        {
            "heading": "3.2 Problem Definition",
            "text": "We address selecting the incorrect answer as the final one as caused by distraction from the irrelevant documents dN . The irrelevant documents present a challenge as they cannot be used to infer the correct answer, misleading the LLM to generate incorrect but plausible answers aN . The presence of such answers aN in the answer set A can result in obstacles during the final answer selection.\nAnother challenge arises from the overconfident scores, making it difficult to discern the incorrect answers aN from the documents dN . The LLM, being an auto-regressive model, tends to produce text sequences with high probabilities when using greedy decoding. Consequently, it becomes hard to accurately determine the correct answer a\u2217 based on the generation probabilities, especially when it also includes incorrect answers like aN ."
        },
        {
            "heading": "3.3 Distraction-aware Answer Selection",
            "text": "We present simple yet effective Distraction-aware Answer Selection (DAS) for a zero-shot reader. We aim to reduce the negative impact of irrelevant documents in a two-step answering pipeline. Initially, we offer an option to refuse responses to irrelevant documents via an unanswerable instruction. To improve the final answer selection, we incorporate the relevance of the query-document pair into the scoring process.\nDocument Selection (D.S.) We utilize the unanswerable instruction to enhance the deduction capability by giving the option not to respond. We exclude responses that belong to the unanswerable response set U as follows:\nS\u2032 = {(ai, di)|ai /\u2208 U, (ai, di) \u2208 S} (1)\nWe construct an unanswerable response set U = {\"Unanswerable\", \"Answer not in context\"}. The answers in U are judged unanswerable as if the reader rejects to respond to the irrelevant documents.\nAnswer Selection (A.S.) Then, we adjust the answer score by multiplying the query generation score in consideration for the query-document rele-\nvance. This is formulated as follows:\n(a\u2217, d\u2217) = argmax (a\u2032i,d \u2032 i)\u2208S \u2032 PM (a \u2032 i|q, d\u2032i, \u03c1rc) \u00b7 PM (q|d\u2032i, \u03c1qg)\n(2)\nwhere \u03c1qg denotes the query generation instruction. The query generation score from the given document is computed as:\nlogP (q|d) = 1|q| \u2211 t logP (qt|q<t, d) (3)"
        },
        {
            "heading": "4 Experimental Setup",
            "text": "Dataset We experiment on Natural Question (NQ) (Kwiatkowski et al., 2019), TriviaQA (TQA) (Joshi et al., 2017), WebQuestions (WebQ) (Berant et al., 2013) and SQuAD (Rajpurkar et al., 2016) (SQD). 1 For annotated evidence documents for query, the development sets of each dataset are used.\nRetriever We employ the representative sparse retriever, BM25 (Robertson and Zaragoza, 2009), and the dense one, DPR (Karpukhin et al., 2020).\n1Following the settings from Karpukhin et al. (2020), the English Wikipedia dump from Dec 20, 2018, is used.\nLanguage Model We select two publicly open LLMs: 1) FLAN-T5 (Chung et al., 2022) is the family of T5 (Raffel et al., 2020) with instruction tuning; 2) OPT-IML (Iyer et al., 2022) is the finetuned version of OPT (Zhang et al., 2022) by instruction meta learning. We exploit FLAN-T5-XL containing 3B parameters and OPT-IML-MAX1.3B in our main experiments.\nMetrics In our evaluation, we employ the exact match (EM) accuracy metric to assess whether the reader generates the same answer as the annotated answer, after applying normalization techniques such as punctuation removal. We adhere to the same normalization process utilized in previous works (Chen et al., 2017; Lee et al., 2019).\nImplementation Details The reading comprehension instruction is \"Read the following context and answer the question\". We add \"If you don\u2019t know the answer, return unanswerable\" for the unanswerable instruction, as mentioned in Sanh et al. (2022). Also, we compute the query generation score, following settings from Sachan et al. (2022). More details are in Appendix B."
        },
        {
            "heading": "5 Result",
            "text": ""
        },
        {
            "heading": "5.1 Main Result",
            "text": "Table 1 demonstrates the significant performance improvements achieved by DAS regardless of retrievers, LLMs, and datasets. Our method achieves an increase in EM of 64% on average against the default, with a remarkable improvement of 231%.\nAs the size of the retrieved set increases the likelihood of including relevant documents, the reader should be robust to irrelevant documents. Nevertheless, the presence of disctration becomes apparent as indicated by the performance decline without DAS, as shown in Table 1 and Figure 2, when processing more documents. This challenge is addressed by mitigating the negative impact of irrelevant documents. Our approach achieves an average enhancement of 17% in EM when reading 100 documents compared to 20. This shows the robustness of our approach in handling the problem stemming from the irrelevant documents.\nAlso, we find that when reading 100 documents, the use of documents collected through BM25 has a more positive impact on the performance of the reader compared to documents from DPR. This finding is noteworthy, especially considering that DPR generally performs better in retriever tasks. When employing a zero-shot reader, it cannot be definitively concluded that improved performance of the retriever will necessarily lead to enhanced reader performance. More details are in Appendix C.\nComparison against Supervised Reader We directly compare with the supervised readers on the aforementioned datasets and an additional held-out dataset, RealTimeQA (RQA) (Kasai et al., 2022). As shown in Table 2, the zero-shot reader with ours shows robust performance compared to supervised readers, DPR (Karpukhin et al., 2020) and FiD (Izacard and Grave, 2021), which perform poorly on unseen data such as SQuAD and RQA. We highlight their potential as a valuable alternative that avoids the limitations and costs associated with supervised readers."
        },
        {
            "heading": "5.2 Analysis",
            "text": "Our analysis is conducted on NQ with the top 100 documents retrieved by DPR with FLAN-T5-XL. Detailed analysis are in Appendix D.\nImpact of Model Size We conduct experiments to assess the impact of model size on performance. As shown in Figure 3, the results demonstrate that even with smaller models, ours maximizes the performance of an LLM as a zero-shot reader. This indicates that our approach enables LLMs to function effectively as zero-shot readers, even without the need for extensively large parameter sizes.\nAnswer Candidate Set We examine the effects of applying DAS on the answer candidate set S as presented in Table 3. Our findings highlight a remarkable shift in the distribution of answers, with changes of 16.43%p and 5.84%p observed in each reader. Substantial increases in the ratio of correct answers demonstrate that ours effectively mitigates the inclusion of incorrect answers from irrelevant documents.\nFinal Answer Pair Figure 4 illustrates an analysis of the distribution of the final answer pair p\u2217. The results provide evidence that ours successfully selects documents that are relevant to the given query and enable the extraction of a higher number of correct answers from the relevant documents. Additionally, ours shows a reduction of approximately 5% in the rate of incorrect answers generated from irrelevant documents."
        },
        {
            "heading": "6 Conclusion",
            "text": "In this paper, we propose Distraction-aware Answer Selection (DAS) to address the irrelevant documents in the retrieved set when an LLM is used as a zero-shot reader. To validate its capability, we define hallucination caused by irrelevant documents and overconfident answer scores in ODQA setting. Ours aims to mitigate the impact of these aspects by incorporating unanswerable instruction and adjusting answer scores for better answer selection. Experimental results demonstrate the effectiveness of our proposal in handling hallucination across various scenarios, thereby improving the performance of ODQA benchmarks. Our approach, utilizing an LLM, showcases strong generalization capabilities across diverse datasets, distinguishing it from supervised readers and highlighting the potential of a zero-shot reader."
        },
        {
            "heading": "Limitations",
            "text": "Our methodology utilizes a two-step pipeline to enhance the performance of an LLM as a zero-shot reader, addressing hallucination issues and leveraging its functionality. While ours fully elicit the inherent ability of the zero-shot reader from LLM, its effectiveness is dependent on the capabilities and characteristics of the LLM. For example, the prompt sensitivity of an LLM is one of the important aspects to consider, as different prompts may lead to varying results. Also, the performance of an LLM is size-dependent. Although our experiments have yielded consistent results in numerous cases, further investigation is required to evaluate our approach with larger LLMs. Despite these limitations, the zero-shot approach holds great promise in terms of cost-effectiveness and leveraging abundant world knowledge. As future advancements in LLMs are anticipated, we expect even greater improvements in performance over the state-of-the-art supervised readers."
        },
        {
            "heading": "Ethics Statement",
            "text": "We acknowledge the possibility of bias or offensive answer sets in utilizing an LLM as a zeroshot reader. Since this paper primarily focuses on the mitigating impact of irrelevant documents in ODQA without parametric updates, addressing the issue of bias and offensive language within an LLM is beyond the scope of our paper. We are aware that ongoing research and efforts are being made by researchers to address these concerns and improve the ethical aspects of LLMs. It is expected that future advancements and research in the field will contribute to addressing these biases and ensuring an ethical use of LLMs."
        },
        {
            "heading": "Acknowledgements",
            "text": "This work was supported by an Institute for Information and communications Technology Promotion (IITP) grant funded by the Korea government (No. 2018-0-00582, Prediction and augmentation of the credibility distribution via linguistic analysis and automated evidence document collection). This work was also supported by the Artificial intelligence industrial convergence cluster development project funded by the Ministry of Science and ICT (MSIT, Korea) & Gwangju Metropolitan City."
        },
        {
            "heading": "A Related Work",
            "text": "We describe the related work on unanswerable instruction and query generation score in our proposed method, Distraction-aware Answer Selection (DAS)."
        },
        {
            "heading": "A.1 Unanswerable Instruction",
            "text": "The unanswerable queries were introduced to ensure the effective discernment of query-document relevance (Rajpurkar et al., 2018). This approach was incorporated in the pre-training of LLMs when models cannot find the answer within the provided document (Wei et al., 2022; Sanh et al., 2022; Iyer et al., 2022). We revisit these approaches in a zeroshot setting to confirm the feasibility of the unanswerable instruction for filtering out irrelevant documents in the retrieved set."
        },
        {
            "heading": "A.2 Document Ranking with Query Generation Score",
            "text": "The query generation score is a widely used measure of query-document relevance when ranking the documents (Nogueira dos Santos et al., 2020; Ju et al., 2021). Recently, LLMs serve as zero-shot re-rankers with outstanding performance gain by computing the measure (Sachan et al., 2022; Cho et al., 2023). To this end, we highlight the capacity of LLMs to ascertain the relevance between the query-document pair when exploiting them as a zero-shot reader."
        },
        {
            "heading": "B Experimental Setup",
            "text": ""
        },
        {
            "heading": "B.1 Dataset",
            "text": "In our main experiments, we utilize a development set of four representative ODQA datasets for employing annotated evidence documents to analyze the impact of query-document relevance. We apply a filtering process to exclude some data that do not contain evidence documents. For a fair comparison against the supervised readers, DPR (Karpukhin et al., 2020) and FiD (Izacard and Grave, 2021) 2, we use the test set of each dataset which has already been preprocessed in Sachan et al. (2022) 3.\nNatural Question (NQ) (Kwiatkowski et al., 2019) was specifically crafted for ODQA tasks. It comprises queries from Google search engines and\n2We evaluate FiD with the model checkpoints from their publicly opened repository.\n3https://github.com/DevSinghSachan/ unsupervised-passage-reranking\nthe answers extracted from Wikipedia documents. In our experiment, a development set and a test set of NQ contain 6,515 and 3,610 queries, respectively.\nTriviaQA (TQA) (Joshi et al., 2017) was for reading comprehension dataset consisting of question-answer-envidence triplets. The queries are fetched from the quiz websites and the corresponding evidence documents are collected from the Wikipedia documents via the Bing search engine. In our experiment, a development set and a test set of TQA contain 6,760 and 11,313 queries, respectively.\nWebQuestions (WebQ) (Berant et al., 2013) collected the queries from Google Suggest API and its answer from the entities in Freebase. The evidence documents were defined as the highestranked documents from BM25 having the answer (Lee et al., 2019). We use a development set of WebQ consisting of 361 questions.\nSQuAD (SQD) (Rajpurkar et al., 2016) was based on manually annotated queries from Wikipedia documents. While SQuAD wasn\u2019t designed for ODQA tasks, it was widely used for evaluating reader performance. A development set and a test of SQuAD contain 8,886 and 10,570 queries, respectively.\nB.2 Instruction & Template\nAs LLMs are sensitive to instruction and templates when adopting the downstream tasks without parameter updates, we carefully select via iterative validation. The reading comprehension instruction is \"Read the following context and answer the question\" and the unanswerable instruction is \"Read the following context and answer the question. If you don\u2019t know the answer, return unanswerable\". When transmitting a query, a document, and an instruction to LLMs, we use the input template following the setting from Chung et al. (2022) and Iyer et al. (2022). The input templates are \"{I}\\n\\nContext: {D}\\nQuestion: {Q}\" for FLAN-T5 and \"{I}\\n\\nContext: {D}\\nQuestion: {Q}\\nAnswer: \" for OPT-IML-MAX where I,D and Q denotes an instruction, a document, and a question, respectively."
        },
        {
            "heading": "B.3 Environment",
            "text": "We conduct all experiments on A100 80GB GPUs. We use BEIR (Thakur et al., 2021) framework4 for the retriever, BM25 and DPR. We employ FLANT5 and OPT-IML-MAX with 3B and 1.3B parameters publicly open on the Huggingface model hub5 (Wolf et al., 2020)."
        },
        {
            "heading": "C Detailed Results",
            "text": "We provide more comprehensive results in terms of both top-10 and top-50 documents, as illustrated in Table 4 and Figure 6. In the absence of our proposed methodology, there is a noticeable decline in performance as the number of documents read increases. However, when employing DAS, we observe a reduction in the impact of hard-negative documents within the document set, resulting in an enhanced reader capability. DAS effectively mitigates the adverse effects of such documents and maximizes the overall performance of a reader.\nIn an ablation study, Figure 6 showcases the influence of document selection (D.S.) and answer selection (A.S.) within our proposed method. Both\n4http://beir.ai/ 5https://huggingface.co/models\nselections contribute positively to enhancing the performance of LLM. However, in the case of OPTIML-MAX, the impact of document selection is found to be insignificant. This observation suggests that OPT-IML-MAX, despite its ability to distinguish irrelevant documents based on instructions, falls short compared to FLAN-T5 in effectively addressing the hallucination."
        },
        {
            "heading": "D Analysis",
            "text": ""
        },
        {
            "heading": "D.1 Aanlaysis of Unanswerables",
            "text": "As shown in Table 5, we conduct an analysis of the model\u2019s responses to documents, including those that are excluded from the answer candidate set S during the document selection process. While our method successfully reduced the number of responses from irrelevant documents, we observed a slight decrease in relevant documents. However, the primary focus of our methodology is on increasing the portion of correct answers by minimizing the number of incorrect answers originating from irrelevant documents. This aspect is key to our approach and contributes to the overall improvement of reader performance."
        },
        {
            "heading": "D.2 Analysis of Overconfident Score",
            "text": "We conducted a verification to determine whether the answer score was indeed overconfident. As depicted in Figure 5, when DAS is not utilized, the incorrect answer exhibits a remarkably high generation probability, making it indistinguishable from the correct answer. However, upon implementing DAS, the scores are normalized, resulting in a discernible distribution disparity between correct and incorrect answers."
        },
        {
            "heading": "D.3 Case Study",
            "text": "We present two curated examples in Table 6 to illustrate the effectiveness of our proposed approach in mitigating hallucination compared to na\u00efve LLMs.\nIn these examples, the na\u00efve LLMs erroneously provide the answer \"Straits of Mackinac\" in unrelated contexts to \"Lake Michigan-Huron\" when given the query about \"The Great Lakes\". However, by employing our method, the correct answers are extracted from the relevant documents. This highlights the ability of our approach to alleviate hallucination and facilitate the accurate selection of appropriate answers based on contextual information.\nAdditionally, we showcase two error cases in Table 6. In these cases, the reader generates the correct answer based on the relevant document, but our approach produces plausible alternative answers. For instance, in response to the question \"What is the deepest depth in the oceans?\", the reader correctly identifies \"Challenger Deep\" based on another relevant document not included in annotated evidence set. While this answer is technically incorrect according to EM evaluation, it is difficult to perceive it as entirely incorrect when assessed qualitatively."
        },
        {
            "heading": "1020 50 100",
            "text": ""
        },
        {
            "heading": "1020 50 100",
            "text": ""
        },
        {
            "heading": "1020 50 100",
            "text": ""
        },
        {
            "heading": "1020 50 100",
            "text": ""
        }
    ],
    "title": "Improving Zero-shot Reader by Reducing Distractions from Irrelevant Documents in Open-Domain Question Answering",
    "year": 2023
}