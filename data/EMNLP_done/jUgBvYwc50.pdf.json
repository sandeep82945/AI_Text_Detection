{
    "abstractText": "Language models (LMs) that jointly generate end-task answers as well as free-text rationales are known as self-rationalization models. Recent works demonstrate great performance gain for self-rationalization by few-shot prompting LMs with rationale-augmented exemplars. However, the ability to benefit from explanations only emerges with large-scale LMs, which have poor accessibility. In this work, we explore the less-studied setting of leveraging explanations for small LMs to improve fewshot self-rationalization. We first revisit the relationship between rationales and answers. Inspired by the implicit mental process of how human beings assess explanations, we present a novel approach, Zero-shot Augmentation of Rationale-Answer pairs (ZARA), to automatically construct pseudo-parallel data for selftraining by reducing the problem of plausibility judgement to natural language inference. Experimental results show ZARA achieves SOTA performance on the FEB benchmark, for both the task accuracy and the explanation metric. In addition, we conduct human and quantitative evaluation validating ZARA\u2019s ability to automatically identify plausible and accurate rationale-answer pairs.1",
    "authors": [
        {
            "affiliations": [],
            "name": "Wei-Lin Chen"
        },
        {
            "affiliations": [],
            "name": "An-Zi Yen"
        },
        {
            "affiliations": [],
            "name": "Cheng-Kuang Wu"
        },
        {
            "affiliations": [],
            "name": "Hen-Hsen Huang"
        },
        {
            "affiliations": [],
            "name": "Hsin-Hsi Chen"
        },
        {
            "affiliations": [],
            "name": "Yang Ming Chiao"
        }
    ],
    "id": "SP:732c09006c00085eb3fa49e82f7b6babab38196f",
    "references": [
        {
            "authors": [
                "Shourya Aggarwal",
                "Divyanshu Mandowara",
                "Vishwajeet Agrawal",
                "Dinesh Khandelwal",
                "Parag Singla",
                "Dinesh Garg."
            ],
            "title": "Explanations for CommonsenseQA: New Dataset and Models",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for",
            "year": 2021
        },
        {
            "authors": [
                "Jonathan Bragg",
                "Arman Cohan",
                "Kyle Lo",
                "Iz Beltagy."
            ],
            "title": "Flex: Unifying evaluation for few-shot nlp",
            "venue": "Advances in Neural Information Processing Systems, 34:15787\u201315800.",
            "year": 2021
        },
        {
            "authors": [
                "Oana-Maria Camburu",
                "Tim Rockt\u00e4schel",
                "Thomas Lukasiewicz",
                "Phil Blunsom."
            ],
            "title": "e-snli: Natural language inference with natural language explanations",
            "venue": "Advances in Neural Information Processing Systems, 31.",
            "year": 2018
        },
        {
            "authors": [
                "Jifan Chen",
                "Eunsol Choi",
                "Greg Durrett."
            ],
            "title": "Can NLI models verify QA systems\u2019 predictions? In Findings of the Association for Computational Linguistics: EMNLP 2021, pages 3841\u20133854, Punta Cana, Dominican Republic",
            "venue": "Association for Compu-",
            "year": 2021
        },
        {
            "authors": [
                "Wei-Lin Chen",
                "An-Zi Yen",
                "Hen-Hsen Huang",
                "HsinHsi Chen."
            ],
            "title": "Learning to generate explanation from e-hospital services for medical suggestion",
            "venue": "Proceedings of the 29th International Conference on Computational Linguistics, pages 2946\u20132951,",
            "year": 2022
        },
        {
            "authors": [
                "Nouha Dziri",
                "Hannah Rashkin",
                "Tal Linzen",
                "David Reitter."
            ],
            "title": "Evaluating attribution in dialogue systems: The BEGIN benchmark",
            "venue": "Transactions of the Association for Computational Linguistics, 10:1066\u2013 1083.",
            "year": 2022
        },
        {
            "authors": [
                "Tobias Falke",
                "Leonardo F.R. Ribeiro",
                "Prasetya Ajie Utama",
                "Ido Dagan",
                "Iryna Gurevych."
            ],
            "title": "Ranking generated summaries by correctness: An interesting but challenging application for natural language inference",
            "venue": "Proceedings of the 57th Annual Meet-",
            "year": 2019
        },
        {
            "authors": [
                "Tianyu Gao",
                "Adam Fisch",
                "Danqi Chen."
            ],
            "title": "Making pre-trained language models better few-shot learners",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natu-",
            "year": 2021
        },
        {
            "authors": [
                "Ariel Gera",
                "Alon Halfon",
                "Eyal Shnarch",
                "Yotam Perlitz",
                "Liat Ein-Dor",
                "Noam Slonim."
            ],
            "title": "Zero-shot text classification with self-training",
            "venue": "arXiv preprint arXiv:2210.17541.",
            "year": 2022
        },
        {
            "authors": [
                "Fr\u00e9deric Godin",
                "Kris Demuynck",
                "Joni Dambre",
                "Wesley De Neve",
                "Thomas Demeester"
            ],
            "title": "Explaining character-aware neural networks for wordlevel prediction: Do they discover linguistic rules",
            "venue": "In Proceedings of the 2018 Conference on Empiri-",
            "year": 2018
        },
        {
            "authors": [
                "Pengcheng He",
                "Xiaodong Liu",
                "Jianfeng Gao",
                "Weizhu Chen."
            ],
            "title": "Deberta: Decoding-enhanced bert with disentangled attention",
            "venue": "International Conference on Learning Representations.",
            "year": 2020
        },
        {
            "authors": [
                "Namgyu Ho",
                "Laura Schmid",
                "Se-Young Yun."
            ],
            "title": "Large language models are reasoning teachers",
            "venue": "arXiv preprint arXiv:2212.10071.",
            "year": 2022
        },
        {
            "authors": [
                "Or Honovich",
                "Leshem Choshen",
                "Roee Aharoni",
                "Ella Neeman",
                "Idan Szpektor",
                "Omri Abend."
            ],
            "title": "q2: Evaluating factual consistency in knowledgegrounded dialogues via question generation and question answering",
            "venue": "Proceedings of the 2021 Confer-",
            "year": 2021
        },
        {
            "authors": [
                "Cheng-Yu Hsieh",
                "Chun-Liang Li",
                "Chih-Kuan Yeh",
                "Hootan Nakhost",
                "Yasuhisa Fujii",
                "Alexander Ratner",
                "Ranjay Krishna",
                "Chen-Yu Lee",
                "Tomas Pfister"
            ],
            "title": "Distilling step-by-step! outperforming larger language models with less training data and smaller",
            "year": 2023
        },
        {
            "authors": [
                "Jiaxin Huang",
                "Shixiang Shane Gu",
                "Le Hou",
                "Yuexin Wu",
                "Xuezhi Wang",
                "Hongkun Yu",
                "Jiawei Han."
            ],
            "title": "Large language models can self-improve",
            "venue": "arXiv preprint arXiv:2210.11610.",
            "year": 2022
        },
        {
            "authors": [
                "Alon Jacovi",
                "Yoav Goldberg."
            ],
            "title": "Aligning faithful interpretations with their social attribution",
            "venue": "Transactions of the Association for Computational Linguistics, 9:294\u2013310.",
            "year": 2021
        },
        {
            "authors": [
                "Sarthak Jain",
                "Sarah Wiegreffe",
                "Yuval Pinter",
                "Byron C. Wallace."
            ],
            "title": "Learning to faithfully rationalize by construction",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4459\u20134473, Online. Association",
            "year": 2020
        },
        {
            "authors": [
                "Maxime Kayser",
                "Oana-Maria Camburu",
                "Leonard Salewski",
                "Cornelius Emde",
                "Virginie Do",
                "Zeynep Akata",
                "Thomas Lukasiewicz."
            ],
            "title": "e-vil: A dataset and benchmark for natural language explanations in vision-language tasks",
            "venue": "Proceedings of the",
            "year": 2021
        },
        {
            "authors": [
                "Daniel Khashabi",
                "Sewon Min",
                "Tushar Khot",
                "Ashish Sabharwal",
                "Oyvind Tafjord",
                "Peter Clark",
                "Hannaneh Hajishirzi."
            ],
            "title": "UNIFIEDQA: Crossing format boundaries with a single QA system",
            "venue": "Findings of the Association for Computational Linguistics:",
            "year": 2020
        },
        {
            "authors": [
                "James Kirkpatrick",
                "Razvan Pascanu",
                "Neil Rabinowitz",
                "Joel Veness",
                "Guillaume Desjardins",
                "Andrei A Rusu",
                "Kieran Milan",
                "John Quan",
                "Tiago Ramalho",
                "Agnieszka Grabska-Barwinska"
            ],
            "title": "Overcoming catastrophic forgetting in neural networks",
            "year": 2017
        },
        {
            "authors": [
                "Wojciech Kryscinski",
                "Bryan McCann",
                "Caiming Xiong",
                "Richard Socher."
            ],
            "title": "Evaluating the factual consistency of abstractive text summarization",
            "venue": "In",
            "year": 2020
        },
        {
            "authors": [
                "Philippe Laban",
                "Tobias Schnabel",
                "Paul N. Bennett",
                "Marti A. Hearst."
            ],
            "title": "SummaC: Re-visiting NLIbased models for inconsistency detection in summarization",
            "venue": "Transactions of the Association for Computational Linguistics, 10:163\u2013177.",
            "year": 2022
        },
        {
            "authors": [
                "Andrew K Lampinen",
                "Ishita Dasgupta",
                "Stephanie CY Chan",
                "Kory Matthewson",
                "Michael Henry Tessler",
                "Antonia Creswell",
                "James L McClelland",
                "Jane X Wang",
                "Felix Hill"
            ],
            "title": "Can language models learn from explanations in context",
            "year": 2022
        },
        {
            "authors": [
                "Tao Lei",
                "Regina Barzilay",
                "Tommi Jaakkola."
            ],
            "title": "Rationalizing neural predictions",
            "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 107\u2013117, Austin, Texas. Association for Computational Linguistics.",
            "year": 2016
        },
        {
            "authors": [
                "Mike Lewis",
                "Yinhan Liu",
                "Naman Goyal",
                "Marjan Ghazvininejad",
                "Abdelrahman Mohamed",
                "Omer Levy",
                "Veselin Stoyanov",
                "Luke Zettlemoyer"
            ],
            "title": "BART: Denoising sequence-to-sequence pre-training for natural language",
            "year": 2020
        },
        {
            "authors": [
                "Jiwei Li",
                "Xinlei Chen",
                "Eduard Hovy",
                "Dan Jurafsky."
            ],
            "title": "Visualizing and understanding neural models in NLP",
            "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language",
            "year": 2016
        },
        {
            "authors": [
                "Qing Li",
                "Qingyi Tao",
                "Shafiq Joty",
                "Jianfei Cai",
                "Jiebo Luo."
            ],
            "title": "Vqa-e: Explaining, elaborating, and enhancing your answers for visual questions",
            "venue": "Proceedings of the European Conference on Computer Vision (ECCV), pages 552\u2013567.",
            "year": 2018
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Roberta: A robustly optimized bert pretraining approach",
            "venue": "arXiv preprint arXiv:1907.11692.",
            "year": 2019
        },
        {
            "authors": [
                "Ana Marasovic",
                "Iz Beltagy",
                "Doug Downey",
                "Matthew Peters."
            ],
            "title": "Few-shot self-rationalization with natural language prompts",
            "venue": "Findings of the Association for Computational Linguistics: NAACL 2022, pages 410\u2013424, Seattle, United States. Association",
            "year": 2022
        },
        {
            "authors": [
                "Ana Marasovi\u0107",
                "Chandra Bhagavatula",
                "Jae sung Park",
                "Ronan Le Bras",
                "Noah A. Smith",
                "Yejin Choi"
            ],
            "title": "Natural language rationales with full-stack visual reasoning: From pixels to semantic frames",
            "year": 2020
        },
        {
            "authors": [
                "Quinn McNemar."
            ],
            "title": "Note on the sampling error of the difference between correlated proportions or percentages",
            "venue": "Psychometrika, 12(2):153\u2013157.",
            "year": 1947
        },
        {
            "authors": [
                "Takeru Miyato",
                "Shin-ichi Maeda",
                "Masanori Koyama",
                "Shin Ishii."
            ],
            "title": "Virtual adversarial training: a regularization method for supervised and semisupervised learning",
            "venue": "IEEE transactions on pattern analysis and machine intelligence, 41(8):1979\u20131993.",
            "year": 2018
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J Liu"
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "J. Mach. Learn. Res.,",
            "year": 2020
        },
        {
            "authors": [
                "Nazneen Fatema Rajani",
                "Bryan McCann",
                "Caiming Xiong",
                "Richard Socher."
            ],
            "title": "Explain yourself! leveraging language models for commonsense reasoning",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,",
            "year": 2019
        },
        {
            "authors": [
                "Maarten Sap",
                "Saadia Gabriel",
                "Lianhui Qin",
                "Dan Jurafsky",
                "Noah A. Smith",
                "Yejin Choi."
            ],
            "title": "Social bias frames: Reasoning about social and power implications of language",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational",
            "year": 2020
        },
        {
            "authors": [
                "Henry Scudder."
            ],
            "title": "Probability of error of some adaptive pattern-recognition machines",
            "venue": "IEEE Transactions on Information Theory, 11(3):363\u2013371.",
            "year": 1965
        },
        {
            "authors": [
                "Noam Slonim",
                "Elad Yom-Tov",
                "Koby Crammer."
            ],
            "title": "Active online classification via information maximization",
            "venue": "Twenty-Second International Joint Conference on Artificial Intelligence.",
            "year": 2011
        },
        {
            "authors": [
                "Cunxiang Wang",
                "Shuailong Liang",
                "Yue Zhang",
                "Xiaonan Li",
                "Tian Gao."
            ],
            "title": "Does it make sense? and why? a pilot study for sense making and explanation",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages",
            "year": 2019
        },
        {
            "authors": [
                "Peifeng Wang",
                "Aaron Chan",
                "Filip Ilievski",
                "Muhao Chen",
                "Xiang Ren."
            ],
            "title": "Pinto: Faithful language reasoning using prompt-generated rationales",
            "venue": "arXiv preprint arXiv:2211.01562.",
            "year": 2022
        },
        {
            "authors": [
                "Xuezhi Wang",
                "Jason Wei",
                "Dale Schuurmans",
                "Quoc Le",
                "Ed Chi",
                "Denny Zhou."
            ],
            "title": "Self-consistency improves chain of thought reasoning in language models",
            "venue": "arXiv preprint arXiv:2203.11171.",
            "year": 2022
        },
        {
            "authors": [
                "Kyunghyun Cho"
            ],
            "title": "Dialogue natural language",
            "year": 2019
        },
        {
            "authors": [
                "Quoc Le"
            ],
            "title": "Unsupervised data augmenta",
            "year": 2020
        }
    ],
    "sections": [
        {
            "text": "Language models (LMs) that jointly generate end-task answers as well as free-text rationales are known as self-rationalization models. Recent works demonstrate great performance gain for self-rationalization by few-shot prompting LMs with rationale-augmented exemplars. However, the ability to benefit from explanations only emerges with large-scale LMs, which have poor accessibility. In this work, we explore the less-studied setting of leveraging explanations for small LMs to improve fewshot self-rationalization. We first revisit the relationship between rationales and answers. Inspired by the implicit mental process of how human beings assess explanations, we present a novel approach, Zero-shot Augmentation of Rationale-Answer pairs (ZARA), to automatically construct pseudo-parallel data for selftraining by reducing the problem of plausibility judgement to natural language inference. Experimental results show ZARA achieves SOTA performance on the FEB benchmark, for both the task accuracy and the explanation metric. In addition, we conduct human and quantitative evaluation validating ZARA\u2019s ability to automatically identify plausible and accurate rationale-answer pairs.1"
        },
        {
            "heading": "1 Introduction",
            "text": "Driven by the concerns of whether the decisions made by the artificial intelligence models are trustworthy, providing free-text, natural language explanations (NLEs) has drawn substantial attention in the research community (Camburu et al., 2018; Li et al., 2018; Rajani et al., 2019; Aggarwal et al., 2021; Chen et al., 2022). Comparing with popular explanation techniques within the input scope, e.g., attributing feature importance scores to tokens (Li et al., 2016; Godin et al., 2018) or extracting fragments of text highlights (Lei et al., 2016; Jain\n1https://github.com/ntunlplab/ZARA\net al., 2020), free-text explanation2 is more expressive, inherently apt for human comprehension and brings richer information in addition to input context (Camburu et al., 2018; Wiegreffe et al., 2021). Yet, the construction of NLE datasets is expensive and challenging due to quality control issues such as inconsistency and under-specification (Wiegreffe and Marasovic, 2021). The development of interpretable NLP systems which can provide NLEs in few-shot is necessitated.\nRecent works (Wei et al., 2022; Wang et al., 2022b; Lampinen et al., 2022) achieve few-shot self-rationalization, i.e., jointly generating freetext explanations and end-task labels, by extending the usage of NLEs to compose chain-of-thought (CoT) input-rationale-output demonstrations for prompt-based learning. Comparing with standard\n2We use the term \u201cfree-text explanation\u201d and \u201cnatural language explanation\u201d interchangeably; and the term \u201cexplnantion\u201d can also refer to the rationale generated by the model.\nprompting (i.e., without rationales), prompting with rationale-augmented exemplars triggers LM\u2019s complex reasoning ability, significantly boosting the end-task performance. However, the main drawback is that only excessively large LMs (generally 100B-plus) demonstrate this ability to leverage explanations, which sharply emerges when scaling model size sufficiently (Wei et al., 2022; Lampinen et al., 2022).\nIn this work, we explore the less-studied setting of improving few-shot self-rationalization only relying on affordable, small LMs (200M\u223c2.7B). We adopt self-training (Scudder, 1965), a simple yet effective methodology that is not practical for large LMs in most real-world scenarios. We first investigate the relationship between the generated explanations and end-task predictions, and find plausible explanations are usually paired with correct label predictions. Namely, plausibility is a strong indicator for answer correctness. Motivated by this finding, we propose Zero-shot Augmentation of Rational-Answer pairs (ZARA) for self-training.\nSpecifically, we reduce the problem of assessing rationale plausibility to the task of natural language inference (NLI), and propose a zero-shot plausibility approximator towards automatic assessment of the generated rationales, without requiring any ground-truth labels or golden explanations. The approximator can be viewed as an agent for plausibility judgement. As illustrated in Figure 1, to determine the plausibility of the rationale, humans implicitly ask themselves whether they can draw conclusions to the predicted answer by understanding the task, the input question, and the supported rationale with their logic and reasoning. To approximate such process explicitly, the approximator leverages the ability of textual entailment to yield a probability score indicating the explanation plausibility. Connecting to the self-training paradigm, we first train a self-rationalization model by fewshot prompt-based learning with natural language prompts, and leverage the approximator to collect pseudo-parallel data, i.e, unlabeled inputs paired with high-confident rationale-answer pairs, for creating an augmented training set which is then used to learn an improved self-rationalization model.\nWith various small-size LMs, experiments show our approach notably improves the FEB benchmark3 (Marasovic et al., 2022)\u2014a recently pro-\n3https://github.com/allenai/feb (Licensed under the Apache License 2.0.)\nposed standardized few-shot self-rationalization benchmark\u2014with 3.4%\u223c 5.1% and 3.0%\u223c 5.8% for task accuracy and the associated explanation metric, respectively. Additionally, we validate the approximator\u2019s ability with both human and quantitative evaluations. The results suggest our approximator can effectively select plausible explanations that lead to higher accuracy for end-task predictions. In summary, our main contributions are three-fold:\n1. We show how to leverage explanations for small LMs by an in-depth analysis of the relationship between rationales and task labels.\n2. We propose ZARA, a novel approach for small LMs to improve self-rationalization with self-training.\n3. Our NLI-based approximator sheds light on the potential of automatic evaluation for explanation plausibility and post-hoc verification for label accuracy."
        },
        {
            "heading": "2 Background and Motivation",
            "text": "Given a trained self-rationalization model f\u03b8(\u00b7) and an input sequence x, we denote a prediction f\u03b8(x) = (r\u0302, a\u0302), where r\u0302 is the generated free-text rationale and a\u0302 is the predicted answer, typically a classification label. Note that r\u0302 and a\u0302 are parsed from the output sequence of f\u03b8(x). Evaluation of a self-rationalization model requires assessing both a\u0302 for the end-task performance and r\u0302 for the quality of the explanation. With the lack of an ideal and unified automatic metric, the current gold standard for determining the quality of r\u0302 is to conduct a human evaluation to check its plausibility (Marasovic\u0301 et al., 2020; Kayser et al., 2021; Wiegreffe et al., 2022; Marasovic et al., 2022). An ideal r\u0302 is considered to be plausible if it is able to justify a\u0302, that is, providing a logical and reasonable explanation supporting the model\u2019s prediction.\nHowever, if r\u0302 is deemed plausible by humans, it does not mean a\u0302 is correct. As the example in Table 1, commonsense would know \u201cbed\" is likely the answer, yet the generated explanation for the corresponding prediction \u201ccouch\" is still plausible. Plausibility illustrates the degree of convincement towards the model\u2019s prediction, regardless of whether the model is actually making an accurate prediction or not (Jacovi and Goldberg, 2021).4\n4For a confounder-free setting, prior works (Kayser et al.,\nNaturally, generating plausible explanations that can justify the wrong answers should be much harder comparing to justifying the correct answers. Since such r\u0302 usually demonstrates a slight pivot from commonsense yet still introduces a sound reason to support the inaccurate a\u0302. We hypothesize this\u2014plausible explanation towards inaccurate endtask prediction\u2014is not the circumstance in most cases of (a\u0302, r\u0302). In other words, if r\u0302 is considered to be plausible, it is likely that a\u0302 is a correct prediction. Hence, the first research question arises: RQ1: \u201cTo what extent do plausible explanations imply correct label predictions?\" And if we could verify RQ1, the follow-up question would be RQ2: \u201cIs it possible to automatically identify plausible r\u0302 and utilize (r\u0302, a\u0302) for further model improvement?\"\nIn the following of our work, we answer RQ1 by inspecting the interrelationship between the plausibility of r\u0302 and the correctness of a\u0302 (Section 4), where we show evidence supporting the linkage to RQ2. Ergo, we propose ZARA coupled with selftraining to accomplish RQ2 (Section 5), improving few-shot self-rationalization models."
        },
        {
            "heading": "3 Datasets and Tasks",
            "text": "We adopt FEB (Marasovic et al., 2022), a newly proposed few-shot self-rationalization benchmark,\n2021; Marasovic et al., 2022) only evaluates r\u0302 with a\u0302 = a, i.e, explanation for the correctly predicted answer. This may overestimate the quality of explanations (Wiegreffe et al., 2022).\nas the dataset for experiments throughout this work. FEB consists of four sub-tasks from existing English-language explainable datasets with free-text explanations: (1) Nonsensical sentence selection (COMVE; Wang et al., 2019). Given two sentences, select the sentence that is less likely to make sense. (2) Offensiveness classification (SBIC; Sap et al., 2020). Classify a given post as offensive or not. (3) Natural language inference (E-SNLI; Camburu et al., 2018). Classify the relationship between two sequences as entailment, neutral, or contradiction. (4) Multiple-choice commonsense QA (ECQA; Aggarwal et al., 2021). Given a question, select the correct answer from five choices.\nThe goal for each sub-task is the same, namely, to predict a label for the underling classification task and generate a free-text explanation supporting the model\u2019s decision. Each sub-task has 60 episodes, and each episode is a train-test split with 48 training examples and 350 evaluation examples. This design of no extra validation data encompasses the FLEX principles (Bragg et al., 2021) for performing robust few-shot NLP evaluation to avoid per-episode hyper-parameter tuning, which could inflate the evaluation results considerably as shown in previous work (Gao et al., 2021). Hence, a single set of hyper-parameter is used across all episodes."
        },
        {
            "heading": "4 Correlation between Plausibility and Correctness",
            "text": "As described in Section 2, following we attempt to answer RQ1 by measuring the correlation between the plausibility of r\u0302 and the correctness of a\u0302. We conduct human studies on results from a self-rationalization model (without self-training) using the FEB dataset. We adopt prompt-based finetuning with natural language prompt on a sequenceto-sequence language model to perform few-shot self-rationalization.\nFor each episode of the sub-task, we train a self-\nrationalization model with the training set and generate rationale-answer pairs on the test set. We then gather all predictions from the 60 episodes and randomly select 350 examples for human studies. We present the description of the task, the input instance x and the rationale-answer pair (r\u0302, a\u0302) for the annotators, and ask them to judge the plausibility of r\u0302, i.e., whether it can justify a\u0302. Following prior works (Marasovic\u0301 et al., 2020; Marasovic et al., 2022), the annotator determines the plausibility by assigning labels from {\u201cno\", \u201cweak no\", \u201cweak yes\", \u201cyes\"}. We then map labels to plausibility scores {1, 2, 3, 4} and instances with average scores above 2.5 are deemed plausible. We provide inter-annotator agreement details in Appendix C.\nThe results are shown in Figure 2. We can observe that for all sub-tasks, when the explanations\nare judged as plausible, they are much more likely paired with correctly predicted answers in constrast to implausible ones. This verifies our hypothesis (discussed in Section 2) and shows plausibility to be a strong signal for correct label predictions. Our results also align with the prior work (Wiegreffe et al., 2021), where they find self-rationalization models demonstrate high label-rationale association against robustness testing. In conclusion, identifying (r\u0302, a\u0302) pairs that have plausible r\u0302 spurs great potential for boosting model performance, and connects us to RQ2."
        },
        {
            "heading": "5 Zero-Shot Augmentation of Rationale-Answer Pairs",
            "text": "As shown in Section 4, plausible explanations imply that the corresponding task predictions are more\nlikely to be correct. Following we present ZARA\u2014 the approach towards automatically judging the plausibility of generated explanations, and leverages the high confident rationale-answer pairs to boost model performance via self-training."
        },
        {
            "heading": "5.1 Reduce plausibility judgement to NLI",
            "text": "Given a rationale-answer pair (r\u0302, a\u0302) output by a self-rationalization model, a human evaluates whether r\u0302 is plausible by understanding the input context and the task objective, and applying reasoning ability to determine if r\u0302 justifies a\u0302. Specifically, humans implicitly form propositions from the input context and rationale by understanding the problem (the task). Then do inference, i.e., apply logic and reasoning to draw conclusions, in their mind to decide if the propositions support the predicted answer. This mental process of assessing plausibility resembles determining the relationship between a premise and a hypothesis. Driven by this formulation, we reduce the problem of judging the plausibility of explanations to the task of natural language inference (NLI), and construct a zeroshot approximator, which leverages existing NLI models to automatically approximate the human judgement of plausibility.\nNLI Mapping. The formulation as NLI requires the mapping of (x, r\u0302, a\u0302) \u2192 (p, h), where x, p, and h are the input instance, premise, and hypothesis, respectively. We manually create the mappings for each FEB sub-task as shown in Table 2. Constructing such mappings can be easily achieved with minimal effort 5 compared with human evaluation on all r\u0302. Consider the COMVE example in Table 2, the goal is to select the nonsensical sentence from two sentences. As we can see \u201ci drove my computer to the gas station.\" is nonsensical, and the rationale\n5One can simply design the mapping by observing the training data.\njustifies it by stating \u201cyou can\u2019t drive a computer.\", which explains why the answer is nonsensical by providing information refuting the answer sentence, resulting in a contradiction relationship between the two. Hence, the approximator can estimate the degree of plausibility by referring to the score of the contradiction class.\nThe approximator. For developing the approximator, we ensemble three state-of-the-art pre-trained NLI models by averaging their output scores for the decision of NLI class. Specifically, we adopt RoBERTa (Liu et al., 2019), DeBERTa (He et al., 2020), and BART (Lewis et al., 2020), trained on the MultiNLI corpus (Williams et al., 2018), one of the largest available NLI dataset. The approximator is zero-shot, i.e., all three models are used off-the-shelf (See Appendix A for details) without any fine-tuning on our dataset, accommodating the few-shot, data scarcity setting."
        },
        {
            "heading": "5.2 Self-training",
            "text": "In the self-training paradigm, a trained model augments its own training set by constructing pseudo-parallel data with predictions on unlabeled instances, where the most confident predictions are collected as new training examples and used to re-train an improved model. For applying self-training, most works focus on classification tasks (Miyato et al., 2018; Xie et al., 2020; Gera et al., 2022) with common strategies based on operations of confidence scores such as probability values to select new examples. E.g., finding predictions that are far from the decision boundary (Slonim et al., 2011).\nHowever, the adoption of self-training for selfrationalization differs from typical classification tasks in two aspects: (1) Compared with fixed classification labels, the target space of neural sequence generation is much more complex. (2) The selec-\ntion requires considering both the task label a\u0302 and the rationale r\u0302 with their relationship. By a proxy model, i.e, the approximator, we could reduce the target dimensions to fixed class labels to address the former. For the latter, we could resolve it by only considering the plausibility of r\u0302 since plausible r\u0302 likely implies correct a\u0302 as shown in Section 4. Following we introduce our self-training paradigm of ZARA\u2014a train-judge-train procedure. See Figure 3 for illustration.\nGiven an episode E consisting of a training split Dtrain and a test split Dtest, where an example in E is an input-rationale-answer tuple (x, r, a). We first train a LM M0 on Dtrain for self-rationalization by prompt-based fine-tuning with natural language prompts. The trained model is denoted as M1.\nNext, we perform inference with M1 on unlabeled instances x \u2208 Dunlabled, where Dunlabled is a non-overlapping set randomly sampled from other episodes with size |Dunlabled| = |Dtest|. For each prediction, the input x and the generated rationaleanswer pair (r\u0302, a\u0302) are mapped to the NLI format, i.e., (x, r\u0302, a\u0302) \u2192 (p, h), and passed to the zero-shot plausibility approximator.6 The approximator automatically judges the plausibility of r\u0302, where the most confident predictions are selected by a plausibility threshold \u03b1, i.e., a probability score (See Appendix B for details). This process does not require any ground truth label or golden rationale.\nThe collected high-confident (x, r\u0302, a\u0302) predictions become new instances to augment Dtrain. Also, we ensure the added instances are balanced for classification tasks by downsampling majority classes. We then re-train M0 on the augmented training split to obtain our final self-rationalization model M2, and evaluate on Dtest."
        },
        {
            "heading": "6 Experiments",
            "text": "In this section, we discuss the experimental setup and present the results of our proposed method, ZARA, for improving few-shot self-rationalization via self-training. We also perform human and quantitative evaluations to validate the automatic plausibility assessment for our approximator."
        },
        {
            "heading": "6.1 Model",
            "text": "For comparison purposes, we follow FEB and use UNIFIEDQA (Khashabi et al., 2020), a T5 (Raffel et al., 2020) variant trained on a multi-task\n6Depending on the mapping design, some sub-tasks do not require input content x to form the premise and hypothesis.\nmixture of QA datasets, as our self-rationalization model for all experiments. The model performs few-shot learning via fine-tuning with natural language prompts. We experiment with three model sizes: UNIFIEDQA-base (200M), UNIFIEDQA-large (770M), and UNIFIEDQA-3B (2.7B). The results presented in Section 4 are conducted with UNIFIEDQA-3B. More details of the experimental setups and configurations are provided in Appendix A."
        },
        {
            "heading": "6.2 Main results",
            "text": "The evaluation metrics of FEB are accuracy and BERTscore (Zhang et al., 2019) for end-task labels and explanations, respectively.7 For each sub-task, we train 60 models (one per episode) and report the mean and standard error of accuracy/BERTscore in Table 3. We also provide statistics on the number of instances added for augmentation in Appendix D. To the best of our knowledge, we present the first results on the newly introduced FEB benchmark (besides their original approach in the paper).\nWe experiment with three model sizes: base, large and 3B. In ZARA, both training stages adopt models of the same size; the original FEB baseline only involves training one model (one stage). As observed in Table 3, our method substantially outperforms the FEB baseline for all datasets. In general, COMVE, SBIC and E-SNLI demonstrate relatively consistent improvements across model size. The only anomoly is for ECQA. We hypothesize the under-parameterized models (base and large) suffer forgetting from continuous learning with the augmented data (Kirkpatrick et al., 2017), since ECQA may require commonsense knowledge which is not presented in the FEB training data but is encoded in models\u2019 parameters originally. However, for the 3B model\u2014which is still significantly smaller than most large-scale LMs\u2014great performance gain with ZARA is exhibited."
        },
        {
            "heading": "6.3 Approximator evaluation",
            "text": "Plausibility evaluation We conduct human evaluation to validate our approximator. Specifically, the human evaluation can be considered as a metaevaluation for evaluating the approximator\u2019s ability to evaluate explanations, i.e., its ability to assess\n7BERTscore is one of the most correlated automatic NLG metrics with human judgement of plausibility for free-text explanation, as shown by Kayser et al. (2021).\nplausibility. To recap, the approximator\u2019s output probability of the corresponding NLI class (based on the mapping design in Table 2) represents an estimation of plausibility degree, i.e., a pseudoplausibility score. We use the same batch of annotated data from Section 4. That is, 350 randomly selected examples generated by the stage-one model with human judgement of plausibility value {1, 2, 3, 4} mapped from {\u201cno\", \u201cweak no\", \u201cweak yes\", \u201cyes\"} and averaged across annotators.\nThe results are presented in Figure 4. We group the instances into four bins, each containing 25% of data according to the percentile ranking of their pseudo-plausibility score. In general, the median performance of human plausibility judgement increases with higher percentile groups, especially for the COMVE and SBIC sub-tasks. Interestingly, due to the nature of NLI model of the approximator, its output (i.e., pseudo-plausibility scores) may be effected by spurious surface features learned only for NLI tasks (transferred from the MultiNLI dataset), giving rise to the larger interquartile range of the top percentile group in E-SNLI. Overall, the results show our approximator is capable of reflecting human plausibility judgement.\nCorrectness evaluation As stated in Section 4, plausible rationales likely indicate correct answer predictions. We further evaluate our approximator regarding this property by checking the end-task answer accuracy of the data subset selected for augmentation from stage-one model\u2019s prediction pool. We consider three selection strategies: (1) ZARA, i.e., our proposed method, which selects confident (high-scoring) predictions; (2) Random, the data subset is selected randomly from prediction pool; (3) Lowest, in contrast to ZARA, we select a subset from the data with lowest-ranking pseudoplausibility scores.\nFor each episode, the number of augmented\ninstances for (2) and (3) are determined by (1), i.e., we randomly select n instances or select n bottom-ranking instances, where n is the number of instances for augmentation using ZARA. The results are shown in Figure 5. We can observe ZARA consistently outperforms Random and Lowest with substantial margins under different model sizes across all four datasets, and Lowest demonstrates the poorest accuracy. This suggest our approximator is able to verify label predictions post-hoc, i.e., the high/low pseudo-plausibility score suggests the prediction is accurate/inaccurate. In conclusion, the overall evaluation results suggest our approximator can effectively extract rationale-answer pairs which are more plausible and accurate."
        },
        {
            "heading": "7 Related Work",
            "text": ""
        },
        {
            "heading": "7.1 Few-shot self-rationalization",
            "text": "To provide NLEs under low supervision, Marasovic et al. (2022) propose the FEB benchmark and establish the first results by exploring natural language prompt-based fine-tuning. Wiegreffe et al. (2022) focus on improving NLEs with an overgenerate-and-filter pipeline: prompting GPT-3 with gold labels to generate explanation candidates which are then filtered by a model trained with human annotations. Recent works (Wei et al., 2022; Wang et al., 2022b; Huang et al., 2022) leverage rationale-augmented chain-of-thought (CoT) inputs to prompt frozen large-scale LMs in few-shot. Concurrent works (Wang et al., 2022a; Ho et al., 2022; Hsieh et al., 2023) propose pipeline frameworks to distill knowledge by prompting a large \u201cteacher\u201d LM to generate diverse reasoning rationales which are then used to fine-tuning a small \u201cstudent\u201d LM. In comparison, ZARA directly optimizes small LMs on downstream tasks, without access to any large LMs.\nA previous work that shares a conceptual simi-\nlarity to ours is STaR (Zelikman et al., 2022). Give an initial training set consisting of a large amount of labels and a few seed rationales, STaR iteratively fine-tunes a GPT-J model to build an augmented training set to bootstrap itself. The fundamental difference between ZARA and STaR is that STaR needs ground-truth labels to select and generate rationales for augmentation, whereas, ZARA augments rationale-label pairs in zero-shot, without any requirements of ground-truth labels or golden rationales. Another related work by Ye and Durrett (2022) leverages NLEs to boost end-task predictions post-hoc, via training a calibrator. In comparison, we directly improve self-rationalization and our approximator does not require any further training. Moreover, all LMs used by Ye and Durrett (2022) are 175B."
        },
        {
            "heading": "7.2 Leveraging NLI for downstream tasks",
            "text": "The framework of NLI has been expanded to benefit many NLP tasks. Welleck et al. (2019) develop a dataset to improve dialogue models by framing the dialogue consistency problem as NLI. Honovich et al. (2021); Dziri et al. (2022) use NLI to design automatic metrics evaluating factuality of knowledge-grounded dialogue systems. Falke et al. (2019); Kryscinski et al. (2020); Laban et al. (2022) use NLI models to detect factual errors in abstractive summarization tasks. For question answering, Chen et al. (2021) propose a framework to verify QA systems\u2019 predictions with NLI by training models to generate premise-hypothesis pairs from QA instances. Driven by human reasoning, Yin et al. (2019) approach text classification in zero-shot by formulating it as an entailment problem\u2014given the input text (premise), humans mentally construct hypotheses \u201cthe text is about [label choice]\u201d to determine the answer\u2014and adopt out-of-the-box NLI models for predictions."
        },
        {
            "heading": "8 Conclusion",
            "text": "In this work, we first show evidences that plausible explanations imply correct label predictions, and leverage a novel NLI approximator to automatically identify plausible rationales paired with correct answers from unlabeled results.s By collecting such rationale-answer pairs with self-training, we effectively improve the performance of few-shot self-rationalization for small LMs. Moreover, we demonstrate the potential for automatic evaluation of free-text explanations. In light of this, we believe developing a supervised approximator with a unified NLI mapping schema across tasks to be a promising avenue for future works.\nLimitations\nThe success of the approximator relies on the quality of the NLI mapping. Though we showcase great improvement across four different tasks, if the complexity of a task makes the mapping construction non-trivial, the created mapping might not be able to accurately reflect human plausibility judgement of the generated rationales, and the benefit of selftraining could not be guaranteed. Namely, the approximator may identify noisy instances that would instead hurt model performance."
        },
        {
            "heading": "Acknowledgements",
            "text": "We thank the reviewers for their insightful comments. This research was partially supported by National Science and Technology Council, Taiwan, under grants MOST 110-2221-E-002-128-MY3 and NSTC 111-2634-F-002-023-, and Ministry of Education (MOE) in Taiwan, under grants NTU112L900901."
        },
        {
            "heading": "B Plausibility threshold",
            "text": "To estimate a threshold that is sufficient but not overly strict, we compute the average number of training set instances (which are required to be plausible) per episode with probability scores above different threshold values, as shown in Figure 6. The dotted line represents the segment with the smallest slope, indicating increasing the threshold results in the largest data lost. The starting, i.e., smaller, x-value of the dotted line is chosen as our plausibility threshold. Thus, 0.9 for COMVE, E-SNLI and ECQA, and 0.8 for SBIC."
        },
        {
            "heading": "C Human annotation details",
            "text": "We invite three annotators8 to conduct human evaluation and compute inter-annotator agreements by Randolph\u2019s \u03ba on 100 overlapping annotation examples. We record \u03ba of 0.60, 0.56, 0.36, and 0.49 for COMVE, SBIC, E-SNLI, and ECQA, respectively. The low (0.36) to moderate (0.60, 0.56, 0.49) agreements align with prior works\u2019 observations on evaluating plausibility of free-text explanation, reflecting the task subjectivity (Wiegreffe et al.,\n8The annotators include two graduate students and one Ph.D. student. As our tasks do not require specific domain expertise, the payment is determined by the minimum wage.\n2022) and could require more fine-grained analysis in the future (Marasovic et al., 2022)."
        },
        {
            "heading": "D Data augmentation details",
            "text": "Table 4 reports the average number of additional test set instances added per episode for stage-two training. For COMVE, SBIC, and E-SNLI, about one-third of the test data are selected, with only minor differences against model sizes. On the other hand, ECQA shows a notable increment on the 3B model, yet significant lower addition number in general comparing to the other three sub-tasks, which may attribute to the nature of difficulty for commonsense question answering."
        }
    ],
    "title": "ZARA: Improving Few-Shot Self-Rationalization for Small Language Models",
    "year": 2023
}