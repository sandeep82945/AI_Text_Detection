{
    "abstractText": "Learning logic rules for knowledge graph reasoning is essential as such rules provide interpretable explanations for reasoning and can be generalized to different domains. However, existing methods often face challenges such as searching in a vast search space (e.g., enumeration of relational paths or multiplication of highdimensional matrices) and inefficient optimization (e.g., techniques based on reinforcement learning or EM algorithm). To address these limitations, this paper proposes a novel framework called LATENTLOGIC to efficiently mine logic rules by controllable generation in the latent space. Specifically, to map the discrete relational paths into the latent space, we leverage a pre-trained VAE and employ a discriminator to establish an energy-based distribution. Additionally, we incorporate a sampler based on ordinary differential equations, enabling the efficient generation of logic rules in our approach. Extensive experiments on benchmark datasets demonstrate the effectiveness and efficiency of our proposed method.",
    "authors": [
        {
            "affiliations": [],
            "name": "Junnan Liu"
        },
        {
            "affiliations": [],
            "name": "Qianren Mao"
        },
        {
            "affiliations": [],
            "name": "Chenghua Lin"
        },
        {
            "affiliations": [],
            "name": "Yangqiu Song"
        },
        {
            "affiliations": [],
            "name": "Jianxin Li"
        }
    ],
    "id": "SP:9a35a5dd569bcb8b1a86ec5234bc465b5c881d6d",
    "references": [
        {
            "authors": [
                "Ivana Balazevic",
                "Carl Allen",
                "Timothy M. Hospedales."
            ],
            "title": "Tucker: Tensor factorization for knowledge graph completion",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International",
            "year": 2019
        },
        {
            "authors": [
                "Antoine Bordes",
                "Nicolas Usunier",
                "Alberto Garc\u00edaDur\u00e1n",
                "Jason Weston",
                "Oksana Yakhnenko."
            ],
            "title": "Translating embeddings for modeling multirelational data",
            "venue": "Advances in Neural Information Processing Systems 26: 27th Annual Conference on",
            "year": 2013
        },
        {
            "authors": [
                "Ricky T.Q. Chen",
                "Brandon Amos",
                "Maximilian Nickel."
            ],
            "title": "Learning neural event functions for ordinary differential equations",
            "venue": "9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenRe-",
            "year": 2021
        },
        {
            "authors": [
                "Tian Qi Chen",
                "Yulia Rubanova",
                "Jesse Bettencourt",
                "David Duvenaud."
            ],
            "title": "Neural ordinary differential equations",
            "venue": "Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS",
            "year": 2018
        },
        {
            "authors": [
                "Kewei Cheng",
                "Jiahao Liu",
                "Wei Wang",
                "Yizhou Sun."
            ],
            "title": "Rlogic: Recursive logical rule learning from knowledge graphs",
            "venue": "KDD \u201922: The 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, Washington, DC, USA, August 14 - 18,",
            "year": 2022
        },
        {
            "authors": [
                "Rajarshi Das",
                "Shehzaad Dhuliawala",
                "Manzil Zaheer",
                "Luke Vilnis",
                "Ishan Durugkar",
                "Akshay Krishnamurthy",
                "Alex Smola",
                "Andrew McCallum."
            ],
            "title": "Go for a walk and arrive at the answer: Reasoning over paths in knowledge bases using reinforcement learning",
            "venue": "In",
            "year": 2018
        },
        {
            "authors": [
                "Andrew McCallum"
            ],
            "title": "Chains of reasoning over",
            "year": 2017
        },
        {
            "authors": [
                "Sebastian Riedel"
            ],
            "title": "Convolutional 2d knowl",
            "year": 2018
        },
        {
            "authors": [
                "Ni Lao",
                "Tom M. Mitchell",
                "William W. Cohen."
            ],
            "title": "Random walk inference and learning in A large scale knowledge base",
            "venue": "Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, EMNLP 2011, 27-31 July 2011, John",
            "year": 2011
        },
        {
            "authors": [
                "Yann LeCun",
                "Sumit Chopra",
                "Raia Hadsell",
                "M Ranzato",
                "Fujie Huang."
            ],
            "title": "A tutorial on energy-based learning",
            "venue": "Predicting structured data, 1(0).",
            "year": 2006
        },
        {
            "authors": [
                "Sangkeun Lee",
                "Sungchan Park",
                "Minsuk Kahng",
                "Sang-goo Lee."
            ],
            "title": "Pathrank: Ranking nodes on a heterogeneous graph for flexible hybrid recommender systems",
            "venue": "Expert Syst. Appl., 40(2):684\u2013697.",
            "year": 2013
        },
        {
            "authors": [
                "Ruizhe Li",
                "Xiao Li",
                "Guanyi Chen",
                "Chenghua Lin."
            ],
            "title": "Improving variational autoencoder for text modelling with timestep-wise regularisation",
            "venue": "Proceedings of the 28th International Conference on Computational Linguistics, pages 2381\u20132397.",
            "year": 2020
        },
        {
            "authors": [
                "Yizhi Li",
                "Wei Fan",
                "Chao Liu",
                "Chenghua Lin",
                "Jiang Qian."
            ],
            "title": "TranSHER: Translating knowledge graph embedding with hyper-ellipsoidal restriction",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages",
            "year": 2022
        },
        {
            "authors": [
                "Xi Victoria Lin",
                "Richard Socher",
                "Caiming Xiong."
            ],
            "title": "Multi-hop knowledge graph reasoning with reward shaping",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31 - November 4,",
            "year": 2018
        },
        {
            "authors": [
                "Xiao Liu",
                "Shiyu Zhao",
                "Kai Su",
                "Yukuo Cen",
                "Jiezhong Qiu",
                "Mengdi Zhang",
                "Wei Wu",
                "Yuxiao Dong",
                "Jie Tang."
            ],
            "title": "Mask and reason: Pre-training knowledge graph transformers for complex logical queries",
            "venue": "KDD \u201922: The 28th ACM SIGKDD Conference on",
            "year": 2022
        },
        {
            "authors": [
                "Shengyao Lu",
                "Bang Liu",
                "Keith G. Mills",
                "Shangling Jui",
                "Di Niu."
            ],
            "title": "R5: rule discovery with reinforced and recurrent relational reasoning",
            "venue": "The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. Open-",
            "year": 2022
        },
        {
            "authors": [
                "Denis Lukovnikov",
                "Asja Fischer",
                "Jens Lehmann",
                "S\u00f6ren Auer."
            ],
            "title": "Neural network-based question answering over knowledge graphs on word and character level",
            "venue": "Proceedings of the 26th International Conference on World Wide Web, WWW 2017, Perth,",
            "year": 2017
        },
        {
            "authors": [
                "Weili Nie",
                "Arash Vahdat",
                "Anima Anandkumar."
            ],
            "title": "Controllable and compositional generation with latent-space energy-based models",
            "venue": "Advances",
            "year": 2021
        },
        {
            "authors": [
                "Meng Qu",
                "Junkun Chen",
                "Louis-Pascal A.C. Xhonneux",
                "Yoshua Bengio",
                "Jian Tang."
            ],
            "title": "Rnnlogic: Learning logic rules for reasoning on knowledge graphs",
            "venue": "9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria,",
            "year": 2021
        },
        {
            "authors": [
                "Matthew Richardson",
                "Pedro M. Domingos."
            ],
            "title": "Markov logic networks",
            "venue": "Mach. Learn., 62(1-2):107\u2013 136.",
            "year": 2006
        },
        {
            "authors": [
                "Tim Rockt\u00e4schel",
                "Sebastian Riedel."
            ],
            "title": "End-toend differentiable proving",
            "venue": "Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pages",
            "year": 2017
        },
        {
            "authors": [
                "Ali Sadeghian",
                "Mohammadreza Armandpour",
                "Patrick Ding",
                "Daisy Zhe Wang."
            ],
            "title": "DRUM: end-toend differentiable rule mining on knowledge graphs",
            "venue": "Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information",
            "year": 2019
        },
        {
            "authors": [
                "Eric Salvat",
                "Marie-Laure Mugnier."
            ],
            "title": "Sound and complete forward and backward chainingd of graph rules",
            "venue": "Conceptual Structures: Knowledge Representation as Interlingua, 4th International Conference on Conceptual Structures, ICCS \u201996, Sydney,",
            "year": 1996
        },
        {
            "authors": [
                "Yelong Shen",
                "Jianshu Chen",
                "Po-Sen Huang",
                "Yuqing Guo",
                "Jianfeng Gao."
            ],
            "title": "M-walk: Learning to walk over graphs using monte carlo tree search",
            "venue": "Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Pro-",
            "year": 2018
        },
        {
            "authors": [
                "Yang Song",
                "Jascha Sohl-Dickstein",
                "Diederik P. Kingma",
                "Abhishek Kumar",
                "Stefano Ermon",
                "Ben Poole."
            ],
            "title": "Score-based generative modeling through stochastic differential equations",
            "venue": "9th International Conference on Learning Representations, ICLR 2021,",
            "year": 2021
        },
        {
            "authors": [
                "Frank Ludwig Spitzer."
            ],
            "title": "Principles of Random Walk",
            "venue": "Principles of Random Walk.",
            "year": 1975
        },
        {
            "authors": [
                "Zhiqing Sun",
                "Zhi-Hong Deng",
                "Jian-Yun Nie",
                "Jian Tang."
            ],
            "title": "Rotate: Knowledge graph embedding by relational rotation in complex space",
            "venue": "7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "Chen Tang",
                "Hongbo Zhang",
                "Tyler Loakman",
                "Chenghua Lin",
                "Frank Guerin."
            ],
            "title": "Enhancing dialogue generation via dynamic graph knowledge aggregation",
            "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Vol-",
            "year": 2023
        },
        {
            "authors": [
                "Chen Tang",
                "Hongbo Zhang",
                "Tyler Loakman",
                "Chenghua Lin",
                "Frank Guerin."
            ],
            "title": "Terminology-aware medical dialogue generation",
            "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 1\u20135. IEEE.",
            "year": 2023
        },
        {
            "authors": [
                "Komal K. Teru",
                "William L. Hamilton."
            ],
            "title": "Inductive relation prediction on knowledge graphs",
            "venue": "CoRR, abs/1911.06962.",
            "year": 2019
        },
        {
            "authors": [
                "Kristina Toutanova",
                "Danqi Chen."
            ],
            "title": "Observed versus latent features for knowledge base and text inference",
            "venue": "Proceedings of the 3rd Workshop on Continuous Vector Space Models and their Compositionality, CVSC 2015, Beijing, China, July 26-31,",
            "year": 2015
        },
        {
            "authors": [
                "Hongwei Wang",
                "Fuzheng Zhang",
                "Jialin Wang",
                "Miao Zhao",
                "Wenjie Li",
                "Xing Xie",
                "Minyi Guo."
            ],
            "title": "Ripplenet: Propagating user preferences on the knowledge graph for recommender systems",
            "venue": "Proceedings of the 27th ACM International Conference",
            "year": 2018
        },
        {
            "authors": [
                "William Yang Wang",
                "Kathryn Mazaitis",
                "William W. Cohen."
            ],
            "title": "Programming with personalized pagerank: a locally groundable first-order probabilistic logic",
            "venue": "22nd ACM International Conference on Information and Knowledge Management, CIKM\u201913,",
            "year": 2013
        },
        {
            "authors": [
                "William Yang Wang",
                "Kathryn Mazaitis",
                "William W. Cohen."
            ],
            "title": "Proppr: Efficient first-order probabilistic logic programming for structure discovery, parameter learning, and scalable inference",
            "venue": "Statistical Relational Artificial Intelligence, Papers from the",
            "year": 2014
        },
        {
            "authors": [
                "William Yang Wang",
                "Kathryn Mazaitis",
                "William W. Cohen."
            ],
            "title": "Structure learning via parameter learning",
            "venue": "Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management, CIKM 2014, Shang-",
            "year": 2014
        },
        {
            "authors": [
                "Chenyan Xiong",
                "Russell Power",
                "Jamie Callan."
            ],
            "title": "Explicit semantic ranking for academic search via knowledge graph embedding",
            "venue": "Proceedings of the 26th International Conference on World Wide Web, WWW 2017, Perth, Australia, April 3-7,",
            "year": 2017
        },
        {
            "authors": [
                "Wenhan Xiong",
                "Thien Hoang",
                "William Yang Wang."
            ],
            "title": "Deeppath: A reinforcement learning method for knowledge graph reasoning",
            "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, EMNLP 2017, Copen-",
            "year": 2017
        },
        {
            "authors": [
                "Bishan Yang",
                "Wen-tau Yih",
                "Xiaodong He",
                "Jianfeng Gao",
                "Li Deng."
            ],
            "title": "Embedding entities and relations for learning and inference in knowledge bases",
            "venue": "3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9,",
            "year": 2015
        },
        {
            "authors": [
                "Fan Yang",
                "Zhilin Yang",
                "William W. Cohen."
            ],
            "title": "Differentiable learning of logical rules for knowledge base reasoning",
            "venue": "Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December",
            "year": 2017
        },
        {
            "authors": [
                "Yuyu Zhang",
                "Xinshi Chen",
                "Yuan Yang",
                "Arun Ramamurthy",
                "Bo Li",
                "Yuan Qi",
                "Le Song."
            ],
            "title": "Efficient probabilistic logic reasoning with graph neural networks",
            "venue": "8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia,",
            "year": 2020
        },
        {
            "authors": [
                "Zhengyan Zhang",
                "Xu Han",
                "Zhiyuan Liu",
                "Xin Jiang",
                "Maosong Sun",
                "Qun Liu."
            ],
            "title": "ERNIE: enhanced language representation with informative entities",
            "venue": "Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Flo-",
            "year": 2019
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Knowledge graphs usually contain collections of real-world facts encoded in triplets with entity and relation information, and find broad applications in across multiple domains (Lukovnikov et al., 2017; Xiong et al., 2017a; Wang et al., 2018; Zhang et al., 2019; Huang et al., 2019; Tang et al., 2023a,b). Despite some knowledge graphs holding hundreds of millions of triples, they still suffer from incompleteness, whereby many valid triples are missing since it is impractical to identify them all manually. Therefore, a fundamental and essential task in knowledge graphs is to utilize existing facts to predict the missing ones.\nRecent studies have focused on learning logic rules from knowledge graphs and utilizing these learned rules to predict absent facts. An example\n\u2217 Jianxin Li is the corresponding author.\nof such a rule is \u2200X,Y, Z nationality(X,Y )\u2190 classmate(X,Z) \u2227 nationality(Z, Y ), indicating that if Z is the classmate of X and has a nationality of Y , then X is likely to have a nationality of Y . This rule can be applied to deduce the nationalities of new individuals. Compared to other methods such as knowledge graph embedding approaches (Bordes et al., 2013; Sun et al., 2019; Li et al., 2022), the rule-based method (Zhang et al., 2020) is more interpretable and can be applied to inductive scenarios (Teru and Hamilton, 2019).\nMost rule-based methods involve enumerating relational paths as candidate rules, followed by assigning weights to each rule to indicate their quality (Lao and Cohen, 2010; Richardson and Domingos, 2006; Yang et al., 2017; Sadeghian et al., 2019). When the scale of the knowledge graph expands, these methods face the challenge of exponentially growing search space. To overcome this problem, RNNLogic (Qu et al., 2021) introduces a rule generator and a reasoning predictor to separate rule generation from rule weight learning. However, the optimization process based on the Expectation-Maximization (EM) algorithm tends to have slow convergence, leading to extended training periods. Another line of research utilizes reinforcement learning (RL) to search for logic rules by making sequential decisions (Xiong et al., 2017b; Lin et al., 2018; Das et al., 2018; Lu et al., 2022). Nevertheless, RL-based methods often encounter challenges such as large action spaces and sparse rewards during training. As a result, efficiently mining high-quality logic rules for knowledge graph reasoning remains a challenging task.\nIn this paper, we propose a novel framework named LATENTLOGIC, which overcomes the aforementioned challenges. Our approach bypasses the enumeration of relational paths by employing controllable sampling in latent space. Furthermore, each component of LATENTLOGIC is trained in an end-to-end fashion, avoiding the indirect and\ninefficient optimization procedure. Concretely, we utilize a VAE-based autoencoder (Kingma and Welling, 2014; Li et al., 2020) to map discrete relational paths into a low-dimensional latent space. We employ a discriminator to measure the semantic coherence between the latent vector and the rule head, thereby creating an energy-based distribution in the latent space. To obtain latent vector samples that correspond to the desired rule head, we employ a sampler based on ordinary differential equations (Song et al., 2021; Nie et al., 2021). The latent vectors are used to generate rule bodies for the given rule head using the VAE generator.\nExtensive experiments demonstrated the computational efficiency of LATENTLOGIC in comparison to previous rule learning methods (Yang et al., 2017; Sadeghian et al., 2019; Qu et al., 2021), indicating that LATENTLOGIC exhibits enhanced scalability for mining logic rules on largerscale knowledge graphs. Furthermore, through experiments on two commonly used benchmark datasets, FB15k-237 (Toutanova and Chen, 2015) and WN18RR (Dettmers et al., 2018), we observed that LATENTLOGIC successfully generates highquality logic rules for knowledge graph reasoning and evidently outperform the salient baseline methods."
        },
        {
            "heading": "2 Framework",
            "text": "Problem Definition First, we introduce some definitions and notations. A knowledge graph G = (V, T ,R) is usually defined by a triple set T = {(h, r, t)} \u2286 V \u00d7 R \u00d7 V , where V denotes an entity set, and R represents a relation set. In this paper, we aim to learn logic rules in the conjunctive form \u2200{Xi}li=0 : r(X0, Xl) \u2190 r1(X0, X1) \u2227 \u00b7 \u00b7 \u00b7 \u2227 rl(Xl\u22121, Xl) from the given knowledge graph. A logic rule, which can be abbreviated as r \u2190 r1 \u2227 \u00b7 \u00b7 \u00b7 \u2227 rl, consists of a rule head, denoted as r, and a rule body (relational path), represented as r1 \u2227 \u00b7 \u00b7 \u00b7 \u2227 rl.\nFramework Overview Our approach converts rule learning problems to controllable generation problems by developing a generative model, denoted as p\u03b8(P|s), to generate rules given a specific rule head s. Here, P represents the rule body, i.e., relational path, P = (r1, r2, . . . rl). The main innovation behind our approach is that we substitute the enumeration of the relational path with the sampling of latent vectors, significantly reducing training overhead. Additionally, to accomplish our\naim, we incorporate a relational path autoencoder and a rule discriminator that operates in the latent space. As shown in Fig. 1, we firstly utilize a VAEbased Relational Path Autoencoder, which consists of an encoder E and a decoder D, to compress relational paths into a low-dimensional latent space. To perform controllable generation of rule bodies, we need to construct a joint distribution of latent vectors and rule heads for sampling. Therefore, we introduce a Rule Discriminator on the latent space to form the joint distribution represented by the energy-based model. This joint distribution allows us to obtain latent vectors associated with the desired rule head by using a sampler based on an ordinary differential equation. Finally, we can decode the latent vectors into rule bodies using the VAE decoder, allowing us to generate rules given certain rule heads."
        },
        {
            "heading": "2.1 Relational Path Autoencoder",
            "text": "Note that each rule body r1\u2227\u00b7 \u00b7 \u00b7\u2227rl can be considered a sequence of relations [r1, . . . , rl]. Such sequences of relations can be effectively modeled by sequence neural networks (Das et al., 2017; Kotnis et al., 2021; Liu et al., 2022), and thus we introduce RNN (Hochreiter and Schmidhuber, 1997) to parameterize the relational path autoencoder. Specifically, we map relational path P into latent vector z using an RNN-based encoder q(z|P), and an RNN-based decoder p(P|z, q) that maps z into the relational path q is a unified query embedding for all inputs. Our decoder does not use an autoregressive approach. Instead, it takes the unified query embedding q and positional embedding as input, simultaneously generating relational paths. We optimize the encoder and decoder parameters for each input relational path P with the objective:\nLVAE(P) =\u2212 Eq(z|P)[log p(P|z, q)] + KL (q(z|P)||N (0, I)) ,\n(1)\nwhere KL(\u00b7||\u00b7) is the Kullack-Leibler divergence that pushes q to be close to the prior N (0, I)."
        },
        {
            "heading": "2.2 Rule Discriminator",
            "text": "Now we aim to model the joint distribution p(z, s), where z denotes the latent vector of a relational path and s represents a desired rule head. This joint distribution can be represented as p(z, s) = pprior(z)p(s|z), where pprior(z) is the prior distribution, i.e., standard Gaussian distribution, and p(s|z) is conditional distribution on s given z. Inspired by Nie et al. (2021), we define p(s|z) as an\nenergy-based model (EBM) (LeCun et al., 2006) as follows:\np(s|z) \u221d e\u2212E\u03b8(s|z), E\u03b8(s|z) =\u2212 f(g(z))[s] + const,\n(2)\nwhere const = log \u2211\ns\u2032 exp(f(g(z))[s \u2032]) is a nor-\nmalization term corresponding to all rule heads, g(\u00b7) is the fixed VAE decoder that decodes z into P , and f indicates a neural network that takes g(z) and s as input and produces a score that measures how well s is carried in z. The function f can be any advanced model if it measures the consistency between relational paths and the rule heads (e.g., other rule mining models like neural logical programming). We adopt a simple network as suggested by Das et al. (2017), which encodes the relational path using the recurrent neural network, computing the score using the similarity between path representation and relation representation. For each training sample (rh, (r0, . . . , ri)). Suppose y\u0302 denotes the output logits of the rule discriminator; the objective can be expressed as:\nLDiscriminator = \u2212 \u2211 S |R|\u2211 r yrhr log y\u0302r, (3)\nwhere S represents all the training samples and yrh is a one-hot vector that only the rh-th position is 1."
        },
        {
            "heading": "2.3 Rule Generation",
            "text": "Model Training We train both the relational path autoencoder and the rule discriminator on the given knowledge graph G. For the relational path autoencoder, we adopt a random walk (Spitzer, 1975)- based procedure to efficiently sample relational\npaths to obtain training data. Each subsequent node is generated using the following distribution:\np(xi|xi\u22121) =  1 |N (xi\u22121)| , (xi, \u00b7, xi\u22121) \u2208 T ,\n0, otherwise, (4)\nwhere N (xi) denotes all the neighborhoods of entity xi. Then we optimize the auto-encoder by minimizing the Eq. 1. For the rule discriminator, we employ a similar sampling strategy. Each time we sample the next entity xi, we include relation rh, which directly connects xo and xi, to create a training sample (rh, (r0, . . . , ri)). Suppose y\u0302 denotes the output logits of the rule discriminator, the objective can be expressed as follows:\nLDiscriminator = \u2212 \u2211 S |R|\u2211 r yrhr log y\u0302r, (5)\nwhere S represents all the training samples and yrh is a one-hot vector that only the rh-th position is 1.\nLatent Vector Sampling Given the joint distribution p(z, s), we would like to draw samples z conditioned on the target rule head s, which are then fed to the VAE decoder to obtain the desired rule bodies. According to Song et al. (2021), sampling from an EBM can be achieved by solving a specific ordinary differential equation (ODE). In our work, the ODE in the latent space can be expressed as: dz = 12\u03b2(t)E\u03b8(s|g(z))dt, with negative time increments from T to 0. To generate latent vectors based on the given rule head s, we first draw z(T ) from N (0, I), and then apply a neural ODE solver1 (Chen et al., 2018, 2021) to\n1https://github.com/rtqichen/torchdiffeq\nobtain z = z(0).\nRule Generation We draw n samples {zi(T )}ni=0 fromN (0, I) for each rule head s and obtain n latent vectors {zi(0)}ni=0. Then, we feed {zi(0)}ni=0 to pretrained VAE decoder to generate corresponding rule bodies. Finally, we calculate the confidence scores of the generated rules using the aforementioned rule discriminator."
        },
        {
            "heading": "3 Experimental Setup",
            "text": "Datasets We conduct experiments on two widely used knowledge graph reasoning benchmark datasets: FB15k-237 (Toutanova and Chen, 2015) and WN18RR (Dettmers et al., 2018). WN18RR comprises of 40,943 entities and 11 relations, with 86k/3k/3k instances set aside for training/validation/testing correspondingly. FB15k237 comprises 14,541 entities and 237 relations and has 272k/17k/20k instances reserved for training/validation/testing, respectively."
        },
        {
            "heading": "4 Experimental Results",
            "text": "Baselines We compared LATENTLOGIC with two taxonomies of methods, including: Knowledge graph embedding methods: TransE (Bordes et al., 2013), DistMult (Yang et al., 2015), TuckER (Balazevic et al., 2019), and RotatE (Sun et al., 2019). Rule-based methods: PathRank (Lee et al., 2013), NeuralLP (Yang et al., 2017), DRUM (Sadeghian et al., 2019), RNNLogic (Qu et al., 2021), and RLogic (Cheng et al., 2022).\nEvaluation Metrics We adopt forward chaining (Salvat and Mugnier, 1996) for inferring miss-\ning facts from logical rules. For every test triplet (h, r, t), two queries are created: (h, r, ?) and (?, r, t), using t and h as answers, respectively. To maintain consistency with previous studies, Mean Rank (MR), Mean Reciprocal Rank (MRR), and hit@k are selected as evaluation metrics under the filtered setting (Sun et al., 2019). Furthermore, to mitigate the effects of random sampling, we evaluate the model performance on five different random seeds, and report the average performance.\nOverall Performance As shown in Tab. 1, we present the experimental results on the FB15k237 and WN18RR datasets. Firstly, we compare LATENTLOGIC with rule-based methods and observe that LATENTLOGIC outperforms the statistical learning method PathRank, neural differentiable methods NeuralLP and DRUM, as well as recent methods RNNLogic and RLogic. In particular, we obtain 2.56% and 2.59% relative increase in MRR and Hits@10 on FB15k-237 against the state-of-the-art rule-based method RLogic (Cheng et al., 2022). Similarly, LATENTLOGIC achieves 1.69% and 2.98% increase in MRR and Hits@10 on the WN18RR dataset against RLogic. Then, we also compare LATENTLOGIC against salient knowledge graph embedding-based methods and find that LATENTLOGIC yields comparable performance to embedding-based methods, especially on WN18RR, where it outperforms selected embedding-based baselines.\nQuality of Learned Rules We assess the quality of the logic rules learned from different models in this part. Following the settings of Qu et al. (2021),\nwe generate n logic rules with the highest quality score for each query relation and then use them for training a predictor for knowledge graph reasoning. As mentioned before, the quality of each rule learned by LATENTLOGIC can be calculated using the rule discriminator. Results for different n values are reported in Fig. 2. Our observations suggest that LATENTLOGIC outperforms the compared methods remarkably. Moreover, even with a limited number of rules considered per relation, LATENTLOGIC still achieves competitive results.\nCase Study To demonstrate that LATENTLOGIC is capable of generating helpful and varied rules for knowledge graph reasoning, we present some of the logic rules that LATENTLOGIC produced on the FB15k-237 dataset in Tab. 2. It can be observed that the logic rules generated by LATENTLOGIC are semantically meaningful and diverse."
        },
        {
            "heading": "5 Related Work",
            "text": "Over the past years, learning logical rules over knowledge graphs has been an active research area. Most traditional methods enumerate relational paths between query entities and answer entities as candidate logic rules, and further learn a scalar weight for each rule to assess the quality. Some representative works include Markov Logic Network (MLN) (Kok and Domingos, 2005; Richardson and Domingos, 2006), path ranking (Lao and Cohen, 2010; Lao et al., 2011) and probabilistic personalized page rank (ProPPR) algorithms (Wang et al.,\n2013, 2014a,b). Then some methods extend the idea by simultaneously learning logic rules and the weights in a differentiable way. For example, some works (Rockt\u00e4schel and Riedel, 2017; Yang et al., 2017; Sadeghian et al., 2019) try to use neural logic programming to model rule-based reasoning and to learn high-quality logical rules. Another kind of rule-learning method is based on reinforcement learning. The basic idea is to train a path-finding agent, which is used to search for reasoning paths over knowledge graphs to answer queries, and then extract logic rules from reasoning paths. Some representative works include DeepPath (Xiong et al., 2017b), MINERVA (Das et al., 2018), M-Walk (Shen et al., 2018) and R5 (Lu et al., 2022). Recently, RNNLogic (Qu et al., 2021) solves this problem by training a generator and a predictor alternately. RLgoic (Cheng et al., 2022) learns rules in a recursive manner."
        },
        {
            "heading": "6 Conclusion",
            "text": "Learning logic rules for knowledge graph reasoning is crucial, as they offer interpretable explanations for the process of reasoning. We propose a novel framework for learning logic rules in latent space. Concretely, we introduce a relational path autoencoder to map paths into latent space and a rule discriminator to access the consistency between rule bodies and rule heads. With the sampler based on ODE, LATENTLOGIC can generate logic rules efficiently. Experimental results showed that LATENTLOGIC outperforms strong baseline methods and is efficient for training."
        },
        {
            "heading": "Acknowledgements",
            "text": "We thank the anonymous reviewers for their insightful feedback. This work is supported by the National Natural Science Foundation of China (No.U20B2053). We thank the Beijing Advanced Innovation Center for Big Data and Brain Computing for its computational resources.\nLimitations\nIn this paper, we introduce a generative framework for rule mining on knowledge graphs. We believe this approach still has much room for improvement: 1) We have not used the more complicated model architecture of the rule discriminator. 2) The proposed sampling strategy may lead to some same relational paths or out-of-domain relational paths. For future work, finding a way to incorporate a more advanced rule discriminator and prevent generating invalid relational paths is worth exploring."
        },
        {
            "heading": "7 Appendix",
            "text": ""
        },
        {
            "heading": "7.1 Dataset Statistics",
            "text": "In this part, we provide a brief introduction and statistics of FB15k-237 and WN18RR, which are used in this paper."
        },
        {
            "heading": "7.2 Implementation Details",
            "text": "We implement LATENTLOGIC over Pytorch2. We use the Adam optimizer to train both the relational path autoencoder and the rule discriminator. A grid search is performed to determine the best hyperparameters based on the performance on the validation sets. We employ the dopri5 neural ODE solver, with (10\u22123, 10\u22123) tolerances, and set T = 1, \u03b2min = 0.1, and \u03b2max = 20. The maximum length of relational paths is set to 3 in FB15k-237 and 2 in WN18RR. All experiments are executed on a single Nvidia Tesla V100 GPU."
        },
        {
            "heading": "7.3 Details of Forward Chain",
            "text": "We provide the details of how to use forward chain to infer missing facts given learned logic rules. Specifically, Given a query (h, r, t), let A be the set of candidate answers that can be discovered by any learned rule using forward chaining. For each candidate answer a \u2208 A, the score of triple (h, r, a) can be calculated as \u2211 rule \u2211 path \u03d5(rule), where \u03d5 is the confidence score of logic rule. Then we can rank candidate answers by the scores.\n2https://pytorch.org/"
        },
        {
            "heading": "7.4 Supplementary Experiment",
            "text": "Evaluation of Training Efficiency To showcase the efficiency of LATENTLOGIC, we compare the training time of different models in Tab. 4. For a fair comparison, we utilize a single GPU (Tesla V100) to train each model and implement the hyperparameters recommended by the original papers. Our observations are as follows: (i) LATENTLOGIC outperforms the baselines in efficiency while still achieving competitive performance. (ii) NeuralLP and DRUM do not perform well due to their involvement in large matrix multiplication. (iii) RNNLogic is also less efficient because of the EMbased optimization procedure.\nSensitivity of Randomness Since our approach relies on random sampling techniques, we would like to conduct experiments to examine the sensitivity of LATENTLOGIC to randomness. In Tab.5, we report the mean and standard deviations of the results under different random seeds. We can notice that the randomness sampling has minimal impact on the performance of LATENTLOGIC."
        }
    ],
    "title": "LATENTLOGIC: Learning Logic Rules in Latent Space over Knowledge Graphs",
    "year": 2023
}