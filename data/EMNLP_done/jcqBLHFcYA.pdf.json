{
    "abstractText": "Theory of Mind (ToM) is a critical component of intelligence but its assessment remains the subject of heated debates. Prior research applied human ToM assessments to natural language processing models using either humancreated standardized tests or rule-based templates. However, these methods primarily focus on simplistic reasoning and require further validation. Here, we leverage dynamic epistemic logic to isolate a particular component of ToM and to generate controlled problems. We also introduce new verbalization techniques to express these problems in English natural language. Our findings indicate that some language model scaling (from 70M to 6B and 350M to 174B) does not consistently yield results better than random chance. While GPT-4 demonstrates superior epistemic reasoning capabilities, there is still room for improvement. Our code and datasets are publicly available1",
    "authors": [
        {
            "affiliations": [],
            "name": "Damien Sileo"
        },
        {
            "affiliations": [],
            "name": "Antoine Lernould"
        }
    ],
    "id": "SP:f7cd7752b6ea6be998986f41753c9ca1eef55eb8",
    "references": [
        {
            "authors": [
                "Cristian-Paul Bara",
                "Sky CH-Wang",
                "Joyce Chai."
            ],
            "title": "MindCraft: Theory of mind modeling for situated dialogue in collaborative tasks",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 1112\u20131125, Online",
            "year": 2021
        },
        {
            "authors": [
                "Simon Baron-Cohen",
                "Alan M Leslie",
                "Uta Frith"
            ],
            "title": "Does the autistic child have a \u201ctheory of mind\u201d? Cognition, 21(1):37\u201346",
            "year": 1985
        },
        {
            "authors": [
                "Johan Benthem",
                "Jan van Eijck",
                "Malvin Gattinger",
                "Kaile Su."
            ],
            "title": "Symbolic model checking for dynamic epistemic logic \u2014 s5 and beyond",
            "venue": "Journal of Logic and Computation, 28:367\u2013402.",
            "year": 2018
        },
        {
            "authors": [
                "Stella Biderman",
                "Hailey Schoelkopf",
                "Quentin Anthony",
                "Herbie Bradley",
                "Kyle O\u2019Brien",
                "Eric Hallahan",
                "Mohammad Aflah Khan",
                "Shivanshu Purohit",
                "USVSN Sai Prashanth",
                "Edward Raff"
            ],
            "title": "Pythia: A suite for analyzing large language models across training",
            "year": 2023
        },
        {
            "authors": [
                "Thomas Bolander."
            ],
            "title": "Seeing is believing: Formalising false-belief tasks in dynamic epistemic logic",
            "venue": "Jaakko Hintikka on Knowledge and GameTheoretical Semantics, pages 207\u2013236.",
            "year": 2018
        },
        {
            "authors": [
                "Peter Clark",
                "Oyvind Tafjord",
                "Kyle Richardson."
            ],
            "title": "Transformers as soft reasoners over language",
            "venue": "Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI-20, pages 3882\u20133890. International Joint Conferences on Arti-",
            "year": 2020
        },
        {
            "authors": [
                "Michael Cohen."
            ],
            "title": "Exploring roberta\u2019s theory of mind through textual entailment",
            "venue": "philarchive.",
            "year": 2021
        },
        {
            "authors": [
                "F. de Waal."
            ],
            "title": "Are We Smart Enough to Know How Smart Animals Are? W",
            "venue": "W. Norton.",
            "year": 2016
        },
        {
            "authors": [
                "Lasse Dissing",
                "Thomas Bolander."
            ],
            "title": "Implementing theory of mind on a robot using dynamic epistemic logic",
            "venue": "Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI-20, pages 1615\u20131621. International",
            "year": 2020
        },
        {
            "authors": [
                "Suchin Gururangan",
                "Swabha Swayamdipta",
                "Omer Levy",
                "Roy Schwartz",
                "Samuel Bowman",
                "Noah A. Smith."
            ],
            "title": "Annotation artifacts in natural language inference data",
            "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for",
            "year": 2018
        },
        {
            "authors": [
                "Kyle Hamilton",
                "Aparna Nayak",
                "Bojan Bo\u017ei\u0107",
                "Luca Longo."
            ],
            "title": "Is neuro-symbolic ai meeting its promises in natural language processing? a structured review",
            "venue": "Semantic Web, pages 1\u201342.",
            "year": 2022
        },
        {
            "authors": [
                "Pengcheng He",
                "Jianfeng Gao",
                "Weizhu Chen."
            ],
            "title": "Debertav3: Improving deberta using electra-style pretraining with gradient-disentangled embedding sharing",
            "venue": "arXiv preprint arXiv:2111.09543.",
            "year": 2021
        },
        {
            "authors": [
                "Chadi Helwe",
                "Chlo\u00e9 Clavel",
                "Fabian Suchanek."
            ],
            "title": "Logitorch: A pytorch-based library for logical reasoning on natural language",
            "venue": "The 2022 Conference on",
            "year": 2022
        },
        {
            "authors": [
                "Michal Kosinski."
            ],
            "title": "Theory of mind may have spontaneously emerged in large language models",
            "venue": "arXiv preprint arXiv:2302.02083.",
            "year": 2023
        },
        {
            "authors": [
                "Matthew Le",
                "Y-Lan Boureau",
                "Maximilian Nickel."
            ],
            "title": "Revisiting the evaluation of theory of mind through question answering",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International",
            "year": 2019
        },
        {
            "authors": [
                "Clarence Irving Lewis",
                "Cooper Harold Langford",
                "P Lamprecht."
            ],
            "title": "Symbolic logic, volume 170",
            "venue": "Dover publications New York.",
            "year": 1959
        },
        {
            "authors": [
                "Xiaomeng Ma",
                "Lingyu Gao",
                "Qihui Xu."
            ],
            "title": "Tomchallenges: A principle-guided dataset and diverse evaluation tasks for exploring theory of mind",
            "venue": "arXiv preprint arXiv:2305.15068.",
            "year": 2023
        },
        {
            "authors": [
                "Shima Rahimi Moghaddam",
                "Christopher J Honey."
            ],
            "title": "Boosting theory-of-mind performance in large language models via prompting",
            "venue": "arXiv preprint arXiv:2304.11490.",
            "year": 2023
        },
        {
            "authors": [
                "Ester Navarro",
                "Sara Anne Goring",
                "Andrew R.A. Conway."
            ],
            "title": "The relationship between theory of mind and intelligence: A formative g approach",
            "venue": "Journal of Intelligence, 9.",
            "year": 2020
        },
        {
            "authors": [
                "Aida Nematzadeh",
                "Kaylee Burns",
                "Erin Grant",
                "Alison Gopnik",
                "Tom Griffiths."
            ],
            "title": "Evaluating theory of mind in question answering",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2392\u20132400, Brussels,",
            "year": 2018
        },
        {
            "authors": [
                "Rebecca Qian",
                "Candace Ross",
                "Jude Fernandes",
                "Eric Smith",
                "Douwe Kiela",
                "Adina Williams."
            ],
            "title": "Perturbation augmentation for fairer nlp",
            "venue": "arXiv preprint arXiv:2205.12586.",
            "year": 2022
        },
        {
            "authors": [
                "Kyle Richardson",
                "Ashish Sabharwal."
            ],
            "title": "Pushing the limits of rule reasoning in transformers through natural language satisfiability",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 11209\u201311219.",
            "year": 2022
        },
        {
            "authors": [
                "Maarten Sap",
                "Hannah Rashkin",
                "Derek Chen",
                "Ronan Le Bras",
                "Yejin Choi."
            ],
            "title": "Social IQa: Commonsense reasoning about social interactions",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the",
            "year": 2019
        },
        {
            "authors": [
                "Natalie Shapira",
                "Mosh Levy",
                "Seyed Hossein Alavi",
                "Xuhui Zhou",
                "Yejin Choi",
                "Yoav Goldberg",
                "Maarten Sap",
                "Vered Shwartz."
            ],
            "title": "Clever hans or neural theory of mind? stress testing social reasoning in large language models",
            "venue": "arXiv preprint",
            "year": 2023
        },
        {
            "authors": [
                "Natalie Shapira",
                "Guy Zwirn",
                "Yoav Goldberg."
            ],
            "title": "How well do large language models perform on faux pas tests? In Findings of the Association for Computational Linguistics: ACL 2023, pages 10438\u201310451, Toronto, Canada",
            "venue": "Association for Computational Lin-",
            "year": 2023
        },
        {
            "authors": [
                "Damien Sileo",
                "Marie-Francine Moens."
            ],
            "title": "Probing neural language models for understanding of words of estimative probability",
            "venue": "arXiv preprint arXiv:2211.03358.",
            "year": 2022
        },
        {
            "authors": [
                "Aarohi Srivastava",
                "Abhinav Rastogi",
                "Abhishek Rao",
                "Abu Awal Md Shoeb",
                "Abubakar Abid",
                "Adam Fisch",
                "Adam R Brown",
                "Adam Santoro",
                "Aditya Gupta",
                "Adri\u00e0 Garriga-Alonso"
            ],
            "title": "Beyond the imitation game: Quantifying and extrapolating",
            "year": 2022
        },
        {
            "authors": [
                "Jidong Tian",
                "Yitian Li",
                "Wenqing Chen",
                "Liqiang Xiao",
                "Hao He",
                "Yaohui Jin."
            ],
            "title": "Diagnosing the firstorder logical reasoning ability through LogicNLI",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages",
            "year": 2021
        },
        {
            "authors": [
                "Tomer Ullman."
            ],
            "title": "Large language models fail on trivial alterations to theory-of-mind tasks",
            "venue": "arXiv preprint arXiv:2302.08399.",
            "year": 2023
        },
        {
            "authors": [
                "Iris Van De Pol",
                "Iris Van Rooij",
                "Jakub Szymanik."
            ],
            "title": "Parameterized complexity of theory of mind reasoning in dynamic epistemic logic",
            "venue": "Journal of Logic, Language and Information, 27:255\u2013294.",
            "year": 2018
        },
        {
            "authors": [
                "Hans Van Ditmarsch",
                "Willem Labuschagne."
            ],
            "title": "My beliefs about your beliefs: a case study in theory of mind and epistemic logic",
            "venue": "Synthese, 155:191\u2013209.",
            "year": 2007
        },
        {
            "authors": [
                "Jan van Eijck."
            ],
            "title": "Dynamic epistemic logics",
            "venue": "Johan van Benthem on logic and information dynamics, pages 175\u2013202.",
            "year": 2014
        },
        {
            "authors": [
                "Jason Weston",
                "Antoine Bordes",
                "Sumit Chopra",
                "Tom\u00e1s Mikolov."
            ],
            "title": "Towards ai-complete question answering: A set of prerequisite toy tasks",
            "venue": "4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016,",
            "year": 2016
        },
        {
            "authors": [
                "Honghua Zhang",
                "Liunian Harold Li",
                "Tao Meng",
                "KaiWei Chang",
                "Guy Van den Broeck."
            ],
            "title": "On the paradox of learning to reason from data",
            "venue": "IJCAI 23, pages 3365\u20133373. International Joint Conferences on Artificial Intelligence Organization. Main Track.",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Theory of Mind (ToM) is the cognitive ability to attribute mental states, such as beliefs, desires, and intentions, to oneself and others, allowing individuals to understand and predict behavior based on these inferred mental states. It is an important requirement for general text understanding or artificial intelligence (Navarro et al., 2020), but claims about ToM are prone to bias from human expectations (de Waal, 2016). Kosinski (2023) recently sparked debate by showing that scaling large language models (LLMs) improves performance at standardized tests designed to measure ToM. However, these tests were widely discussed in academic research and might have leaked into the training corpora of LLM. Earlier work generated synthetic examples instead, extending the bAbi (Weston et al., 2016) framework. Nematzadeh et al. (2018) proposed a dataset of fixed templates based on the\n1[code:GitHub][data:HF-datasets]\nSally-Anne problem (Baron-Cohen et al., 1985): Sally puts a marble in a box while Anne is with her. Sally leaves for a moment and Mary puts the marble in a basket. Where will Sally look for the marble? [ANSWER=BOX] Le et al. (2019) deem these problems simplistic and extend them to track second-order beliefs (e.g. the belief of Sally about Anne\u2019s beliefs).\nIn our study, we generate dynamic epistemic logic (DEL) problems and develop verbalizations to transform them into natural language inference problems. DEL is a branch of modal logic that can model an individual\u2019s knowledge about particular facts or about other agents\u2019 knowledge. DEL also enables reasoning about the impact of consecutive public announcements:\nAlice and Bob have mud on their head. Their father says that at least one of them is muddy. He asks Alice and Bob if they are muddy. Do Alice and Bob know that they are muddy? [ANSWER=NO] They answer that they don\u2019t know. Do Alice and Bob now know that they are muddy? [ANSWER=YES] Bob would have answered YES to the first question if Alice was not muddy, so after Bob\u2019s first answer, Alice can know that she is muddy.2 DEL can formalize certain ToM problems, making it a valuable perspective for ToM assessment. The problems we create can require tracking multiple agents\u2019 beliefs and reasoning about higher-order beliefs3. Our dataset encompasses numerous variations of the Muddy Children and Drinking Logicians problems (van Eijck, 2014). This controlled test bench offers new appreciations of language model scaling and presents the first dataset with a complexity that can challenge supervised learning models. The dataset and the scripts to generate is publicly available1.\n2The same holds if we switch Bob and Alice. 3For example, Anne\u2019s belief about Sally\u2019s belief about\nAnne\u2019s belief about Mary\u2019s belief."
        },
        {
            "heading": "2 Related Work",
            "text": "Logical Reasoning in Natural Language Processing Logic shares profound connections with NLP. Early systems were built around logic, and more recent approaches incorporate logical reasoning into neural networks (Hamilton et al., 2022; Helwe et al., 2022). Another line of research closer to ours investigates the logical capabilities of NLP models using textual datasets and labels generated with logical reasoning tools. RuleTaker (Clark et al., 2020) explores this area with propositional logic, while LogicNLI addresses first-order logic (Tian et al., 2021). Richardson and Sabharwal (2022) examine the satisfiability problem in natural language. Sileo and Moens (2022) targets probabilistic logic. Our study is the first to focus on modal logic, specifically epistemic logic, in natural language.\nTheory of Mind in NLP To measure ToM capabilities of NLP models, Nematzadeh et al. (2018) created examples using Sally-Ann templates, and Le et al. (2019) added complexity to the data by incorporating second-order knowledge. Both studies framed their examples as question-answering tasks. Kosinski (2023) employed handcrafted tests to evaluate language models\u2019 next-word prediction capabilities. Ullman (2023) showed LLM brittleness to interventions on these datasets and Ma et al. (2023) consolidated the prior datasets into a principled evaluation suite. The Social-IQA dataset (Sap et al., 2019) covers a broad spectrum of social commonsense, encompassing aspects of theory of mind and challenges like comprehending desires and emotions. Cohen (2021) investigated whether natural language inference models captured veridicality with epistemic verbs like know and think, using handcrafted patterns. This task was incorporated into the BIG-Bench framework (Srivastava et al., 2022) as the epistemic-reasoning task, but it targets only one shallow aspect of epistemic reasoning. Bara et al. (2021) used a Minecraft dataset for real-time belief deduction in collaborative tasks. Shapira et al. (2023b) highlighted LLM struggles in faux pas tests. Shapira et al. (2023a) conducted stress tests on LLMs\u2019 social reasoning capabilities.\nEpistemic Logic and ToM Bolander (2018) showed that the Sally-Ann problem could be modeled with epistemic logic. Van Ditmarsch and Labuschagne (2007) examined more general connections between DEL and ToM, while Dissing and Bolander (2020) demonstrated DEL\u2019s applicability\nin robotics. Van De Pol et al. (2018) explored the plausibility of epistemic logic for ToM by investigating its theoretical computational tractability."
        },
        {
            "heading": "3 Dynamic Epistemic Logic Problem Generation and Verbalization",
            "text": ""
        },
        {
            "heading": "3.1 Problem definition",
            "text": "Our objective is to simultaneously create dynamic epistemic logic problems and their corresponding natural language representations, with a (PREMISE, HYPOTHESIS, LABEL) format.\nAn epistemic logic problem can be decomposed into the following components:\nAgents: A set of N individuals, each assigned a different arbitrary name.\nPredicates: A set of Boolean predicates. Here, we use N predicates, one corresponding to each agent (e.g., Alice has mud on her head).\nObservabilities: The description of each agent\u2019s initial knowledge of the predicate values. We represent observabilities with a boolean matrix O of size N\u00d7N , where Oi,j=1 means that agent i initially knows whether predicate j is true.\nAnnouncements: A list of expressions (predicates or agent knowledge about predicates) that are shared to all agents. Announcements are made sequentially, and each new announcement can change what the agents know, even if it is the same announcement is repeated twice.\nHypothesis: An expression that may contain predicates and knowledge of agents about particular expressions after the announcements, given the agents, observabilities, and announcements grouped into a premise."
        },
        {
            "heading": "3.2 Setups: connecting predicate and observabilities",
            "text": "The concrete choice of predicates dictates the structure of observabilities. For example, the predicate \"Alice has mud on her head\" is observable by agents other than Alice, but \"Alice has mud on her hand\" could be observable by everyone. We group predicates and observabilities into what we call setups to generate textual descriptions. We define the following setups:\nForehead-mud setup PREDICATEi: <AGENTi>\u2019s forehead is muddy. O : ONES(N)\u2212 IDENTITY(N)"
        },
        {
            "heading": "70M 160M410M 1B 1.4B 2.8B 6.9B",
            "text": ""
        },
        {
            "heading": "350M 1.3B 6.7B 174B",
            "text": "Forehead-mud-mirror setup PREDICATEi: <AGENTi>\u2019s forehead is muddy. O : ONES(N) OBSERVATION: There is a mirror in the room. Thirst setup PREDICATEi: <AGENTi>\u2019s is thirsty. O : IDENTITY(N) Explicit setup PREDICATEi: <AGENTi> picked a red card. O : RANDBOOL(N,N),E(sum(O))=N OBSERVATION: Each person draws a card, face unrevealed (red or black). < <AGENTj> card is revealed to <AGENTi>. for all i, j where Oi,j=1>"
        },
        {
            "heading": "3.3 Problem verbalization",
            "text": "We then construct a problem for a given setup with the following natural language template:\n[Premise] There are <N> persons. Everyone is visible to others. <OBSERVATION> It is publicly announced that someone <PREDICATE> <[0\u2212N ] ANNOUNCEMENTS>\n[Hypothesis] <[1\u2212K]th ORDER BELIEF>\n[0 \u2212 N ] denotes uniform sampling from 0 to N . We restrict announcements to first-order beliefs. A first-order belief has the following structure: <AGENT> (can know whether | can know that | cannot know that | cannot know whether)\n(<PREDICATE>|<NEGATED-PREDICATE>), e.g. Alice cannot know whether Bob is not muddy. We use can to acknowledge that an agent could theoretically infer something but fail to see it. A Kth order belief is a first-order belief about a (K\u22121)th order belief. We consider everyone, not everyone, and nobody as possible subjects for the setup predicates. Subjects are uniformly sampled among these quantifiers and the list of individual agents. We transform abstract problem representations into natural language and code that can be fed to a model checker to determine whether a hypothesis is entailed by the premise. We use the SMCDEL model checker (Benthem et al., 2018), an announcement logic based on the S5 (Lewis et al., 1959) modal logic. This implementation is the most cited publicly available epistemic logic as of April 2023. We discard examples where the premise contains a contradiction4. To generate diverse and genderbalanced random English surnames, we use CensusName5 (Qian et al., 2022)."
        },
        {
            "heading": "4 Experiments",
            "text": ""
        },
        {
            "heading": "4.1 Problem generation parameters",
            "text": "We randomly sample N\u2208{2, 3, 4} agents, as we observed that problems were sufficiently challeng-\n4We identify contradictions by examining whether an unused predicate is entailed or not by the premise.\n5https://pypi.org/project/censusname/\ning with only three agents, and we use K=2 for the same reason. We use knowledge predicate negations 80% of the time to encourage richer inferences (as the fact that an agent does not know something conveys information to others) in announcements and 50% of the time otherwise."
        },
        {
            "heading": "4.2 Controlling for example difficulty",
            "text": "Shortcuts, like hypothesis only bias (Gururangan et al., 2018; Zhang et al., 2023), can lead to the answer without correct reasoning. To control for shortcuts, we trained a relatively shallow supervised model (deberta-small (He et al., 2021), 6 layers, 44M backbone parameters) on a training set combining all setups (ensuring that there was no duplicate and no example that was also in the test set). We used 11.2k training examples for 3 epochs and a learning rate of 3e-5 and 3.73k test and validation examples. Overall validation accuracy was 83%. We also experimented with simpler lexical baselines like TF-IDF which did not capture negations well enough. We assumed that examples correctly predicted by deberta-small with high confidence contained shortcut cues. We used these deberta-small predictions and confidence as additional metadata. We found that the evaluated language models already failed on easy examples. So we used a random subset of the validation and test subsets for our experiments, but our dataset can be filtered by difficulty using the provided confidence level and the discrepancy between deberta-small prediction and ground truth.\nWe limit the number of agents to 3 and deduplicate then undersample the problems to generate 400 test cases with a perfect balance of True/False labels per setup. We refer to the resulting dataset as MindGames."
        },
        {
            "heading": "4.3 Scaling experiments",
            "text": "We conduct zero-shot experiments and few-shots with a range of language models. We use standard prompting to follow Kosinski (2023) setup. We use the lm-eval-harness software (Gao et al., 2021) to measure whether a language model perplexity favors the correct reasoning in a multiplechoice setting, with a natural language inference prompt from Brown et al. (2020): <PREMISE> Question: <HYPOTHESIS> True or False ?\" with two possible continuation choices, True and False. We evaluate two families of language models:\nHuman evaluation We present 50 test samples per setup to two NLP researchers only instructed to perform entailment detection. Inter-annotator agreement is 0.89, and average accuracy is 94%6.\nPythia language models We select the Pythia (Biderman et al., 2023) language models for our open-source scaling experiments. We use the checkpoints trained on the deduplicated corpus (deduped) with checkpoint sizes of 70M, 160M, 410M, 1B, 1.4B, 2.8B, and 6.9B.\nOpenAI API We evaluate the OpenAI GPT-3 (Brown et al., 2020) models, specifically the ada, babbage, curie, and davinci checkpoints, through the public API. We assume that their model sizes are respectively 350M, 1.3B, 6.7B, and 174B and we use the default temperature.\nFigure 1 displays the results for various Pythia model sizes. We observe that scaling improves 5- shot7 reasoning, but it has no impact on zero-shot reasoning. In contrast to the emergence results reported by Kosinski (2023), Figure 2 does not show a clear scaling trend for GPT-3 models on MindGames data, which suggests that the emergent behavior they observed was not due to robust epistemic logic capabilities."
        },
        {
            "heading": "4.4 Qualitative analysis with ChatGPT",
            "text": "We also run brief qualitative analyses with GPT3.5 and GPT-4 (OpenAI, 2023), as of May 2023. On 20 randomly sampled problems, we found that GPT3 was 60% correct and GPT-4 70% correct. We present a brief qualitative analysis of the respective models.\nAs shown in Appendix A, GPT3.5 tends to answer that there is not enough information and to perform correct inferences only when it requires very shallow reasoning. GPT-4 can solve this particular example. However, some problems are still challenging, as shown in Figure 3. GPT-4 rarely answers that there is not enough information and its reasoning looks has the surface form of epistemic reasoning, but occasionally contains glaring mistakes."
        },
        {
            "heading": "5 Conclusion",
            "text": "We developed a novel dataset aimed at evaluating epistemic logic reasoning, addressing a particular\n6Most errors arose from failing to distinguish between know whether and know that.\n7Increasing number of examples did not improve validation accuracy.\naspect of ToM. Our results reveal that this task continues to pose challenges for contemporary largescale language models. When future models can solve MindGames for 2-3 agents, the difficulty can be easily scaled up with more agents. Future studies could better explore human performance on our dataset, taking into account factors such as age and educational background. Additionally, further investigation can examine the impact of fine-tuning on other downstream tasks and assess how well Transformer circuits model Kripke structures that represent modal logic problems."
        },
        {
            "heading": "6 Limitations",
            "text": "Theory of mind is a complex subject, and our study takes a deliberately specific angle, leaving multiple open problems:\nLanguage Our work is centered on English, the method could be adapted to other languages using a subject-verb-object structure. Besides, we restricted our study to templates that do not cover the full variety of the English language.\nPrompt structure and models scaling We focused on zero-shot and few-shot prompting, which were sufficient to (Kosinski, 2023), however,\nMoghaddam and Honey (2023) recently showed that more advanced prompting schemes made significant differences. In addition, we did not explore the full range of Pythia models due to computational limitations.\nTask complexity, annotators variation The task we proposed is relatively complex, and raises questions about the profiles of annotators that would match the results of a symbolic reasoner. The framework of DEL itself can also provide insights on theory of mind, as a DEL solver perfectly solves this task, even though we could feel uncomfortable attributing ToM to the solver. We might argue that failing on simple DEL examples disproves ToM, but proving failure is difficult, as mentioned in the previous paragraph."
        },
        {
            "heading": "7 Ethical considerations",
            "text": "This work involves human annotations. However, we used procedurally generated data, ensuring no confidential or harmful content. Besides, annotations were carried out during the researchers\u2019 working hours. For these reasons, our Institutional Review Board has determined that it was exempted from formal review according to internal guidelines."
        },
        {
            "heading": "A Qualitative example with GPT3.5",
            "text": ""
        }
    ],
    "title": "MindGames: Targeting Theory of Mind in Large Language Models with Dynamic Epistemic Modal Logic",
    "year": 2023
}