{
    "abstractText": "A trustworthy real-world prediction system should produce well-calibrated confidence scores; that is, its confidence in an answer should be indicative of the likelihood that the answer is correct, enabling deferral to an expert in cases of low-confidence predictions. Recent studies have shown that unsupervised pretraining produces large language models (LMs) whose conditional probabilities are remarkably well-calibrated. However, the most widelyused LMs are fine-tuned with reinforcement learning from human feedback (RLHF-LMs), and some studies have suggested that RLHFLMs produce conditional probabilities that are very poorly calibrated. In light of this perceived weakness, we conduct a broad evaluation of methods for extracting confidence scores from RLHF-LMs. For RLHF-LMs such as ChatGPT, GPT-4, and Claude, we find that verbalized confidences emitted as output tokens are typically better-calibrated than the model\u2019s conditional probabilities on the TriviaQA, SciQ, and TruthfulQA benchmarks, often reducing the expected calibration error by a relative 50%.",
    "authors": [
        {
            "affiliations": [],
            "name": "Katherine Tian"
        },
        {
            "affiliations": [],
            "name": "Eric Mitchell"
        },
        {
            "affiliations": [],
            "name": "Allan Zhou"
        },
        {
            "affiliations": [],
            "name": "Archit Sharma"
        },
        {
            "affiliations": [],
            "name": "Rafael Rafailov"
        },
        {
            "affiliations": [],
            "name": "Huaxiu Yao"
        },
        {
            "affiliations": [],
            "name": "Chelsea Finn"
        },
        {
            "affiliations": [],
            "name": "Christopher D. Manning"
        }
    ],
    "id": "SP:d813e6ef0db44a50f0ad430e744c73a87f11c1d2",
    "references": [
        {
            "authors": [
                "Nicholas Joseph",
                "Sam McCandlish",
                "Tom Brown",
                "Jared Kaplan"
            ],
            "title": "Constitutional AI: Harmlessness from ai feedback",
            "year": 2022
        },
        {
            "authors": [
                "Glenn W. Brier."
            ],
            "title": "Verification of Forecasts Expressed in Terms of Probability",
            "venue": "Monthly Weather Review, 78(1):1\u20133.",
            "year": 1950
        },
        {
            "authors": [
                "Paul F Christiano",
                "Jan Leike",
                "Tom Brown",
                "Miljan Martic",
                "Shane Legg",
                "Dario Amodei."
            ],
            "title": "Deep reinforcement learning from human preferences",
            "venue": "Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc.",
            "year": 2017
        },
        {
            "authors": [
                "Wade Fagen-Ulmschneider."
            ],
            "title": "Perception of probability words",
            "venue": "Ms., UIUC, 05-24-2023.",
            "year": 2023
        },
        {
            "authors": [
                "Yonatan Geifman",
                "Ran El-Yaniv."
            ],
            "title": "Selective classification for deep neural networks",
            "venue": "Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS\u201917, page 4885\u20134894, Red Hook, NY, USA. Curran Associates",
            "year": 2017
        },
        {
            "authors": [
                "Chuan Guo",
                "Geoff Pleiss",
                "Yu Sun",
                "Kilian Q. Weinberger."
            ],
            "title": "On calibration of modern neural networks",
            "venue": "Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pages 1321\u2013",
            "year": 2017
        },
        {
            "authors": [
                "Mandar Joshi",
                "Eunsol Choi",
                "Daniel Weld",
                "Luke Zettlemoyer."
            ],
            "title": "TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension",
            "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Vol-",
            "year": 2017
        },
        {
            "authors": [
                "Ben Mann",
                "Sam McCandlish",
                "Chris Olah",
                "Jared Kaplan."
            ],
            "title": "Language models (mostly) know what they know",
            "venue": "Arxiv arxiv:2207.05221.",
            "year": 2022
        },
        {
            "authors": [
                "Lorenz Kuhn",
                "Yarin Gal",
                "Sebastian Farquhar."
            ],
            "title": "Semantic uncertainty: Linguistic invariances for uncertainty estimation in natural language generation",
            "venue": "The Eleventh International Conference on Learning Representations.",
            "year": 2023
        },
        {
            "authors": [
                "Stephanie Lin",
                "Jacob Hilton",
                "Owain Evans."
            ],
            "title": "Teaching models to express their uncertainty in words",
            "venue": "Transactions on Machine Learning Research.",
            "year": 2022
        },
        {
            "authors": [
                "Stephanie Lin",
                "Jacob Hilton",
                "Owain Evans"
            ],
            "title": "2022b. TruthfulQA: Measuring how models mimic human",
            "year": 2022
        },
        {
            "authors": [
                "Charles Lord",
                "Mark Lepper",
                "Elizabeth Preston."
            ],
            "title": "Considering the opposite: A corrective strategy for social judgment",
            "venue": "Journal of personality and social psychology, 47:1231\u201343.",
            "year": 1985
        },
        {
            "authors": [
                "Sabrina J. Mielke",
                "Arthur Szlam",
                "Emily Dinan",
                "YLan Boureau."
            ],
            "title": "Reducing conversational agents\u2019 overconfidence through linguistic calibration",
            "venue": "Transactions of the Association for Computational Linguistics, 10:857\u2013872.",
            "year": 2022
        },
        {
            "authors": [
                "Thomas Mussweiler",
                "Fritz Strack",
                "Tim Pfeiffer."
            ],
            "title": "Overcoming the inevitable anchoring effect: Considering the opposite compensates for selective accessibility",
            "venue": "Personality and Social Psychology Bulletin, 26(9):1142\u20131150.",
            "year": 2000
        },
        {
            "authors": [
                "Paul F Christiano",
                "Jan Leike",
                "Ryan Lowe."
            ],
            "title": "Training language models to follow instructions with human feedback",
            "venue": "Advances in Neural Information Processing Systems, volume 35, pages 27730\u201327744. Curran Associates, Inc.",
            "year": 2022
        },
        {
            "authors": [
                "Yaniv Ovadia",
                "Emily Fertig",
                "Jie Ren",
                "Zachary Nado",
                "D. Sculley",
                "Sebastian Nowozin",
                "Joshua V. Dillon",
                "Balaji Lakshminarayanan",
                "Jasper Snoek."
            ],
            "title": "Can you trust your model\u2019s uncertainty? evaluating predictive uncertainty under dataset shift",
            "venue": "Pro-",
            "year": 2019
        },
        {
            "authors": [
                "Seo Yeon Park",
                "Cornelia Caragea."
            ],
            "title": "On the calibration of pre-trained language models using mixup guided by area under the margin and saliency",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1:",
            "year": 2022
        },
        {
            "authors": [
                "Adam Roberts",
                "Colin Raffel",
                "Noam Shazeer"
            ],
            "title": "How much knowledge can you pack into the parameters of a language model",
            "venue": "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
            "year": 2020
        },
        {
            "authors": [
                "Avijit Thawani",
                "Jay Pujara",
                "Filip Ilievski",
                "Pedro Szekely."
            ],
            "title": "Representing numbers in NLP: a survey and a vision",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human",
            "year": 2021
        },
        {
            "authors": [
                "Johannes Welbl",
                "Nelson F. Liu",
                "Matt Gardner."
            ],
            "title": "Crowdsourcing multiple choice science questions",
            "venue": "ArXiv, abs/1707.06209.",
            "year": 2017
        },
        {
            "authors": [
                "Yuxin Xiao",
                "Paul Pu Liang",
                "Umang Bhatt",
                "Willie Neiswanger",
                "Ruslan Salakhutdinov",
                "LouisPhilippe Morency."
            ],
            "title": "Uncertainty quantification with pre-trained language models: A large-scale empirical analysis",
            "venue": "Findings of the Association for",
            "year": 2022
        },
        {
            "authors": [
                "Kaitlyn Zhou",
                "Dan Jurafsky",
                "Tatsunori Hashimoto"
            ],
            "title": "Navigating the grey area: Expressions of overconfidence and uncertainty in language models",
            "year": 2023
        },
        {
            "authors": [
                "Daniel M. Ziegler",
                "Nisan Stiennon",
                "Jeffrey Wu",
                "Tom B. Brown",
                "Alec Radford",
                "Dario Amodei",
                "Paul Christiano",
                "Geoffrey Irving"
            ],
            "title": "Fine-tuning language models from human preferences",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Real-world prediction systems invariably make errors. However, some mitigation of these errors is possible if the system produces well-calibrated1 confidence estimates. In this case, the system\u2019s least confident predictions correspond to those that are most likely to be incorrect, potentially allowing these predictions to be skipped or overridden by a human. In the context of language models, one consequence of poor calibration may be hallucination, where a language model confidently asserts incorrect facts or reasoning. While the ability of very large LMs to absorb and synthesize knowledge about the outside world has gained significant\n\u2217Equal contribution. 1i.e., the confidence in a prediction accurately reflects the\nprobability that the prediction is correct (Guo et al., 2017).\nattention (Brown et al., 2020; Roberts et al., 2020; Bubeck et al., 2023), relatively little attention has been given to their well-calibratedness (Kadavath et al., 2022). Further, most existing analyses of the calibratedness of LLMs focus on models trained with maximum likelihood, while in practice, the most widely-used LLMs (such as ChatGPT) are fine-tuned using methods such as reinforcement learning from human feedback (Christiano et al., 2017). Some findings suggest that RLHF-LMs may sacrifice well-calibrated predictions for the sake of closer adherence to user instructions in dialogue (Kadavath et al., 2022; OpenAI, 2023), as the reinforcement learning objective encourages the model to allocate probability mass to the most preferred answer(s), rather than matching the relative frequency of possible answers.\nThis paper evaluates several methods for extracting confidences about model predictions from\nRLHF-LMs. Due to concerns that RLHF may cause systematic overconfidence in the model\u2019s probabilities (Figure 2), as well as the general unavailability of per-token log-probabilities in widely used RLHF-LMs, we pay particular attention to prompts that elicit verbalized probabilities, i.e., the model expresses its confidence in token-space, as either numerical probabilities or another linguistic expression of uncertainty. We find that, surprisingly, popular RLHF-LMs are able to directly verbalize confidence scores that are better-calibrated than the model\u2019s conditional probabilities (estimated via sampling), without any fine-tuning to learn verbalization. To further improve calibration, we take inspiration from research in human psychology showing that overconfidence can be mitigated by considering alternative answers before responding (Lord et al., 1985; Mussweiler et al., 2000). We show that prompting a model to produce several answer choices before giving its confidence scores significantly improves calibration of verbalized probabilities. Combined with temperature scaling (Guo et al., 2017), this approach generally provides better calibration than model probabilities for ChatGPT2, GPT-43, and Claude 24 across three datasets, often reducing expected calibration error (ECE) by over 50%.\nRelated Work. Several studies have examined the calibration of large LMs (Lin et al., 2022a; Park and Caragea, 2022; Kadavath et al., 2022; Xiao et al., 2022; Kuhn et al., 2023), finding that combining large pre-trained LMs with temperature scaling (Guo et al., 2017) produces very well-\n2gpt-3.5-turbo, accessed in June 2023. 3https://cdn.openai.com/papers/gpt-4-system-card.pdf 4https://www-files.anthropic.com/production/images/Model-\nCard-Claude-2.pdf\ncalibrated predictions (Kadavath et al., 2022; Xiao et al., 2022; Kuhn et al., 2023). Other work focuses on the tendency of language and dialogue models to use linguistic expressions of uncertainty in a well-calibrated manner (Zhou et al., 2023; Mielke et al., 2022). However, existing studies focus on LMs trained purely with unsupervised learning (although Kadavath et al. (2022) briefly examine RLHF-LMs), while widely used models in practice are fine-tuned with instruction-tuning or RLHF (Christiano et al., 2017). RLHF has been shown to effectively leverage annotations of human preferences to control sentiment (Ziegler et al., 2020), improve summarization or instruction-following quality (Stiennon et al., 2022; Ouyang et al., 2022), and inject behavioral priors of harmlessness (Bai et al., 2022b,a). However, recent work has raised the question of whether or not RLHF harms calibration (OpenAI, 2023). Our work is the first to show that verbalized probabilities are often bettercalibrated than the model\u2019s conditional probabilities for RLHF-LMs such as ChatGPT, GPT-4, and Claude, and Llama-2-70B-Chat."
        },
        {
            "heading": "2 Evaluating Calibration in RLHF-LMs",
            "text": "To study the calibration of RLHF-LMs, we conduct experiments with gpt-3.5-turbo (ChatGPT), gpt-4 (GPT-4), claude-1 (Claude 1), claude-2 (Claude 2), and Llama-2-70b-chat (Llama-270B-Chat).\nMetrics. We measure calibration with multiple metrics. To measure ECE (expected calibration error; Guo et al. (2017)), we bin model predictions by their confidence and measure the average accuracy of predictions in each confidence bin. The ECE is defined as the average (squared) error between the average accuracy and confidence within each bin, where each error is weighted by the fraction of samples falling within the bin. We report raw ECE as well as ECE with temperature scaling (ECE-t). Temperature scaling fits a single temperature value \u03b2 to the model\u2019s confidences to minimize negative log likelihood (NLL) on the data, giving scaled probability p\u0303i of class i as p\u0303i \u221d p\u03b2i . See Figure 1 for a depiction of ECE binning. Although ECE is a standard and interpretable measure of calibration error, it completely fails to capture the confidences\u2019 discriminative power.5 We therefore also report\n5For binary classification, a system that guesses randomly and outputs 50% confidence each time has perfect ECE."
        },
        {
            "heading": "Method ECE \u2193 ECE-t \u2193 BS-t \u2193 AUC \u2191 ECE \u2193 ECE-t \u2193 BS-t \u2193 AUC \u2191 ECE \u2193 ECE-t \u2193 BS-t \u2193 AUC \u2191",
            "text": ""
        },
        {
            "heading": "Method ECE \u2193 ECE-t \u2193 BS-t \u2193 AUC \u2191 ECE \u2193 ECE-t \u2193 BS-t \u2193 AUC \u2191 ECE \u2193 ECE-t \u2193 BS-t \u2193 AUC \u2191",
            "text": "Brier Score (BS; Brier (1950)) on temperaturescaled confidences (BS-t), a proper scoring rule (Ovadia et al., 2019) that is the mean squared error between the confidences and the correctness labels. Finally, we assess calibration using a metric from the selective classification literature (Geifman and El-Yaniv, 2017), specifically, the area under the curve of selective accuracy and coverage (AUC).\nDatasets. Our experiments use three questionanswering datasets assessing factual knowledge. TriviaQA (Joshi et al., 2017) contains 650k question-answer pairs gathered by trivia enthusiasts; SciQ (Welbl et al., 2017) contains approximately 14k crowdsourced science exam questionanswer pairs; TruthfulQA (Lin et al., 2022b) contains 817 questions designed to test language models\u2019 tendency to mimic human falsehoods. We sample 1000 questions from the validation split of TriviaQA (rc.web.nocontext) and SciQ and all 817 questions from the validation split of TruthfulQA (generation) for our experiments.\nEvaluation protocol. For each dataset, we generate a response and corresponding confidence from each method on each of the evaluation questions. Because calibration essentially quantifies the relationship between model confidence and correctness, computing correctness is crucial to accurate measurements of calibration. However, we find doing so to be a challenge, especially in datasets where only a single ground-truth answer (but not aliases or semantically equivalent rephrases) is provided. To avoid excessive false negatives in our correctness computation as a result of exact-match evaluation, we use either GPT-4 or GPT-3.5 to evaluate whether a response is essentially equivalent to the ground truth answer; see Appendix C for the complete equivalence-checking procedure.\nMethods. We compare a wide variety of methods for extracting confidence estimates from LLMs. For a comprehensive list of the prompts used for each method, see Appendix Table 6.\nFirst, we consider two methods that leverage the true conditional distribution of the model to gener-"
        },
        {
            "heading": "Method ECE \u2193 ECE-t \u2193 BS-t \u2193 AUC \u2191 ECE \u2193 ECE-t \u2193 BS-t \u2193 AUC \u2191 ECE \u2193 ECE-t \u2193 BS-t \u2193 AUC \u2191",
            "text": ""
        },
        {
            "heading": "Method ECE \u2193 ECE-t \u2193 BS-t \u2193 AUC \u2191 ECE \u2193 ECE-t \u2193 BS-t \u2193 AUC \u2191 ECE \u2193 ECE-t \u2193 BS-t \u2193 AUC \u2191",
            "text": "ate confidence scores. The simplest is Label prob., which uses the conditional probability distribution p(y|x) of the model given a question x, which we estimate using n = 10 samples, since many RLHFLMs are closed-source and do not offer per-token probabilities.67 We return the most common answer, using the LLM-based equivalence function to determine when two lexically different answers are semantically equivalent. In a variation of the method described by Kadavath et al. (2022) (again, we use samples since model probabilities are not available), \u2018Is True\u2019 prob. samples a single answer y\u0302 from the model given a question x, and the probability it is true is estimated by the probability the model assigns to \u2018True\u2019 when asked if the given answer is true (where once again the probabilities are estimated via samples), i.e., p(True|x, y\u0302).\nNext, we consider methods that extract confidence scores through verbalization (Lin et al., 2022a), i.e., where the model expresses its confidence in token space, either with numerical probabilities or linguistic expressions of likelihood.8 First, Verb. 1S top-k prompts the model to produce k guesses and a probability that each is correct all in a single response (i.e., \u20181 stage\u2019). We take the highest-probability prediction and its as-\n6We evaluated gpt-3.5-turbo on all three datasets using n = 20 samples, but the calibration did not meaningfully improve, so we always use n = 10 to reduce API costs.\n7For each closed LM, we use its default sampling parameters (top-p 1.0 for GPT-* and top-p 0.7 for Claude). For Llama-2, we use temperature 1.0 and top-p 1.0.\n8However, note that none of the methods described finetune the model to perform better on verbalization.\nsociated probability as the model\u2019s output and confidence. Verb. 2S top-k similarly uses numerical probabilities, except the model is first asked to provide only its answers, and afterwards, in a second round of dialogue, asked to assign probabilities of correctness to each answer (i.e., \u20182 stages\u2019). Verb. 2S CoT uses a chain-of-thought prompt before giving a single answer, and in a second round of dialogue, the model is prompted to assign a probability to that answer (with the chain of thought present in the model\u2019s context). Ling. 1S-human uses linguistic likelihood expressions, rather than numerical probabilities, to express uncertainty. The model is prompted to assign confidences to its guesses by choosing from a set of linguistic expressions of uncertainty: {Almost certain, Likely, . . . , Almost no chance}. Each linguistic likelihood expression is mapped to a probability using responses from a human survey on social media with 123 respondents (FagenUlmschneider, 2023). Ling. 1S-opt. uses a held out set of calibration questions and answers to compute the average accuracy for each likelihood expression, using these \u2018optimized\u2019 values instead. Expressions that are not used for at least 1N of questions, where N is the number of calibration questions, simply use the human probability."
        },
        {
            "heading": "3 Results",
            "text": "Tables 1\u20135 show the results of evaluating various methods for extracting confidence from RLHFLMs on gpt-3.5-turbo, gpt-4, claude-1,"
        },
        {
            "heading": "Method ECE \u2193 ECE-t \u2193 BS-t \u2193 AUC \u2191 ECE \u2193 ECE-t \u2193 BS-t \u2193 AUC \u2191 ECE \u2193 ECE-t \u2193 BS-t \u2193 AUC \u2191",
            "text": "claude-2, and Llama-2-70b-chat, respectively. We distill several key conclusions from these experiments. 1. Large RLHF-LMs can often directly verbalize better-calibrated confidences (either a numerical confidence probability or an expression such as \u2018highly likely\u2019) than the models\u2019 conditional probabilities. 2. Among the methods for verbalizing probabilities directly, we observe that generating and evaluating multiple hypotheses improves calibration (see Figure 1), similarly to humans (Lord et al., 1985), and corroborating a similar finding in LMs (Kadavath et al., 2022). 3. Language models can express their uncertainty with numerical probabilities as well or better than with words, which is surprising in light of longstanding difficulties in representing numbers in language models (Thawani et al., 2021). 4. Chainof-thought prompting does not improve verbalized calibration (see Appendix Figure 5 for additional CoT results). 5. The calibration of both Claude models\u2019 conditional probabilities roughly falls between gpt-3.5-turbo and gpt-4; however, while Claude 1 is much weaker at verbalizing its confidence, Claude 2 is generally a bit stronger than gpt-3.5-turbo at verbalizing. The verbal calibration of the open source model Llama-2-70b-chat is generally weaker than that of closed source models but still demonstrates improvement over its conditional probabilities by some metrics, and does so most clearly on TruthfulQA."
        },
        {
            "heading": "4 Discussion",
            "text": "In summary, we study the calibration of widely used RLHF-LMs. We first replicate the finding for GPT-4 (OpenAI, 2023) that RLHF can worsen the calibration of a model\u2019s conditional probabilities using the open-source Llama-2-70B base and chat models (Figure 2). To mitigate this regression and ease extraction of calibrated confidence scores for models for which log probabilities are not available, we propose and study new methods that can\nelicit calibrated confidences from RLHF-LMs by prompting the model to verbalize its confidence in token space. We find verbalized probabilities are better-calibrated than conditional probabilities across several closed models, with mixed results for Llama-2-70B-Chat.\nOur results raise several questions for future work. Most notably, the difference between GPT-*, Claude-*, and Llama-2\u2019s ability to verbalize confidence is significant. What factors are important for learning this skill? Additionally, the 1-stage and 2-stage verbalized numerical confidence prompts sometimes differ drastically in the calibration of their confidences. How can we reduce sensitivity of a model\u2019s calibration to the prompt? Going beyond question-answering, can we leverage good calibration in short-answer settings to improve the reliability of long-form generations, perhaps by breaking down long-form generation into a sequence of short questions? Finally, to what extent does a language model\u2019s calibration depend on the domain; do our conclusions in the context of factual recall hold in the context of reasoning or arithmetic? Answering these questions provides one path toward building more trustworthy and useful language systems.\nLimitations. While our work demonstrates a promising new approach to generating calibrated confidences through verbalization, there are limitations that could be addressed in future work. First, our experiments are focused on factual recalloriented problems, and the extent to which our observations would hold for reasoning-heavy settings is an interesting open question. Additionally, the lack of technical details available for many state-ofthe-art closed RLHF-LMs may limit our ability to understand what factors enable a model to verbalize well-calibrated confidences and differences in this ability across different models. Finally, our study is limited to short-form question-answering; future work should extend this analysis to longer-form generation settings.\nAcknowledgements. CF and CDM are CIFAR Fellows. EM gratefully acknowledges funding from a Knight-Hennessy Graduate Fellowship. AZ is supported by the NSF graduate research fellowship program. This research was supported in part by Juniper Networks, Apple, and ONR grant N0001420-1-2675. The authors thank Yoonho Lee and Noah Goodman for helpful feedback on calibration metrics and experiment design."
        },
        {
            "heading": "A Additional Results",
            "text": "Here, we include the likelihood expression usage distribution for gpt-3.5 and gpt-4 in Figures 3 and 4, respectively. gpt-3.5 is systematically less confident for TruthfulQA. The contrast between model confidence for TriviaQA and SciQ compared with TruthfulQA is even more stark for gpt-4.\nWe also provide additional calibration results for chain-of-thought methods. We compare a onestage verbalized CoT prompt (Verb. 1S CoT), a two-stage verbalized CoT prompt (Verb. 2S CoT), and a two-stage verbalized method that uses CoT just before eliciting the numerical confidence (Verb. 2S Cot Prob) instead of before the guess, as shown for gpt-3.5 on Trivia QA, SciQ, and Truthful QA in Figure 5. We find that CoT does not noticeably improve calibration across any setting or dataset."
        },
        {
            "heading": "B Fitting Procedure for Temperature and Probabilities for Linguistic Expressions",
            "text": "To fit the temperature that is used to compute ECEt and BS-t we split our total data into 5 folds. For\neach fold, we use it once to fit a temperature and evaluate metrics on the remaining folds. We find that fitting the temperature on 20% of the data yields relatively stable temperatures across folds. We report the average temperature-scaled ECE and BS as ECE-t and BS-t.\nTo compute ECE and AUC for Ling. 1S-opt., we similarly split our total data into 5 folds, using 4 folds to fit the probabilities behind each linguistic expression of confidence, then evaluating on the remaining fold. To compute ECE-t and BS-t for Ling. 1S-opt, we hold out one of the 5 folds to fit temperature. We use 3 folds to fit probabilities for linguistic expressions, compute the temperature based on these probabilities on the temperature set, and evaluate metrics on the last fold. We then average metrics across all 20 rotations of folds."
        },
        {
            "heading": "C Prompt Templates",
            "text": "The prompt template for each sampling method is provided in Table 6. The question is substituted for the variable ${THE_QUESTION} in each prompt. To evaluate answer correctness, we use gpt-3.5-turbo for SciQ and TruthfulQA and gpt-4 for TriviaQA due to gpt-3.5-turbo\u2019s"
        },
        {
            "heading": "Method Template",
            "text": "high disagreement with a human evaluator on TriviaQA. Using the ground truth answer as ${GOLD_ANSWER} and the model-generated answer as ${PRED_ANSWER}, we use the following prompt template: Are the following two answers to my\nquestion Q semantically equivalent?\\n\\nQ: ${THE_QUESTION}\\nA1: ${GOLD_ANSWER}\\nA2: ${PRED_ANSWER}\\n\\nPlease answer with a single word, either \u201cYes.\" or \u201cNo.\", and explain your reasoning."
        }
    ],
    "title": "Just Ask for Calibration: Strategies for Eliciting Calibrated Confidence Scores from Language Models Fine-Tuned with Human Feedback",
    "year": 2023
}