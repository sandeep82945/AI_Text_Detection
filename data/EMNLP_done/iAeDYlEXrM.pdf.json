{
    "abstractText": "Large-scale pre-training is widely used in recent document understanding tasks. During deployment, one may expect that models should trigger a conservative fallback policy when encountering out-of-distribution (OOD) samples, which highlights the importance of OOD detection. However, most existing OOD detection methods focus on single-modal inputs such as images or texts. While documents are multimodal in nature, it is underexplored if and how multi-modal information in documents can be exploited for OOD detection. In this work, we first provide a systematic and in-depth analysis on OOD detection for document understanding models. We study the effects of model modality, pre-training, and fine-tuning across various types of OOD inputs. In particular, we find that spatial information is critical for document OOD detection. To better exploit spatial information, we propose a spatial-aware adapter, which serves as a parameter-efficient add-on module to adapt transformer-based language models to the document domain. Extensive experiments show that adding the spatial-aware adapter significantly improves the OOD detection performance compared to directly using the language model and achieves superior performance compared to competitive baselines.",
    "authors": [
        {
            "affiliations": [],
            "name": "Jiuxiang Gu"
        },
        {
            "affiliations": [],
            "name": "Yifei Ming"
        },
        {
            "affiliations": [],
            "name": "Yi Zhou"
        },
        {
            "affiliations": [],
            "name": "Jason Kuen"
        },
        {
            "affiliations": [],
            "name": "Vlad I. Morariu"
        },
        {
            "affiliations": [],
            "name": "Handong Zhao"
        },
        {
            "affiliations": [],
            "name": "Ruiyi Zhang"
        },
        {
            "affiliations": [],
            "name": "Nikolaos Barmpalios"
        },
        {
            "affiliations": [],
            "name": "Anqi Liu"
        },
        {
            "affiliations": [],
            "name": "Yixuan Li"
        },
        {
            "affiliations": [],
            "name": "Tong Sun"
        },
        {
            "affiliations": [],
            "name": "Ani Nenkova"
        }
    ],
    "id": "SP:4020a06c9f6c06be06aa3afce6e44bcfb3d5b5c8",
    "references": [
        {
            "authors": [
                "David Alvarez-Melis",
                "Nicolo Fusi."
            ],
            "title": "Geometric dataset distances via optimal transport",
            "venue": "NeurIPS.",
            "year": 2020
        },
        {
            "authors": [
                "Srikar Appalaraju",
                "Bhavan Jasani",
                "Bhargava Urala Kota",
                "Yusheng Xie",
                "R Manmatha."
            ],
            "title": "Docformer: End-to-end transformer for document understanding",
            "venue": "ICCV.",
            "year": 2021
        },
        {
            "authors": [
                "Udit Arora",
                "William Huang",
                "He He."
            ],
            "title": "Types of out-of-distribution texts and how to detect them",
            "venue": "EMNLP.",
            "year": 2021
        },
        {
            "authors": [
                "Iz Beltagy",
                "Matthew E Peters",
                "Arman Cohan."
            ],
            "title": "Longformer: The long-document transformer",
            "venue": "arXiv preprint arXiv:2004.05150.",
            "year": 2020
        },
        {
            "authors": [
                "Abhijit Bendale",
                "Terrance E Boult."
            ],
            "title": "Towards open set deep networks",
            "venue": "CVPR.",
            "year": 2016
        },
        {
            "authors": [
                "Julian Bitterwolf",
                "Maximilian Mueller",
                "Matthias Hein."
            ],
            "title": "In or out? fixing imagenet out-ofdistribution detection evaluation",
            "venue": "ICML.",
            "year": 2023
        },
        {
            "authors": [
                "Lei Cui",
                "Yiheng Xu",
                "Tengchao Lv",
                "Furu Wei."
            ],
            "title": "Document ai: Benchmarks, models and applications",
            "venue": "arXiv preprint arXiv:2111.08609.",
            "year": 2021
        },
        {
            "authors": [
                "Jia Deng",
                "Wei Dong",
                "Richard Socher",
                "Li-Jia Li",
                "Kai Li",
                "Li Fei-Fei."
            ],
            "title": "Imagenet: A large-scale hierarchical image database",
            "venue": "CVPR.",
            "year": 2009
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "NAACL.",
            "year": 2018
        },
        {
            "authors": [
                "Xuefeng Du",
                "Zhaoning Wang",
                "Mu Cai",
                "Yixuan Li."
            ],
            "title": "Vos: Learning what you don\u2019t know by virtual outlier synthesis",
            "venue": "ICLR.",
            "year": 2022
        },
        {
            "authors": [
                "Sepideh Esmaeilpour",
                "Bing Liu",
                "Eric Robertson",
                "Lei Shu."
            ],
            "title": "Zero-shot open set detection by extending clip",
            "venue": "AAAI.",
            "year": 2022
        },
        {
            "authors": [
                "Stanislav Fort",
                "Jie Ren",
                "Balaji Lakshminarayanan."
            ],
            "title": "Exploring the limits of out-of-distribution detection",
            "venue": "NeurIPS.",
            "year": 2021
        },
        {
            "authors": [
                "ZongYuan Ge",
                "Sergey Demyanov",
                "Zetao Chen",
                "Rahil Garnavi."
            ],
            "title": "Generative openmax for multi-class open set classification",
            "venue": "arXiv preprint arXiv:1707.07418.",
            "year": 2017
        },
        {
            "authors": [
                "Jiuxiang Gu",
                "Jason Kuen",
                "Vlad I Morariu",
                "Handong Zhao",
                "Rajiv Jain",
                "Nikolaos Barmpalios",
                "Ani Nenkova",
                "Tong Sun."
            ],
            "title": "Unified pretraining framework for document understanding",
            "venue": "NeurIPS.",
            "year": 2021
        },
        {
            "authors": [
                "Zhangxuan Gu",
                "Changhua Meng",
                "Ke Wang",
                "Jun Lan",
                "Weiqiang Wang",
                "Ming Gu",
                "Liqing Zhang."
            ],
            "title": "Xylayoutlm: Towards layout-aware multimodal networks for visually-rich document understanding",
            "venue": "CVPR.",
            "year": 2022
        },
        {
            "authors": [
                "Adam W Harley",
                "Alex Ufkes",
                "Konstantinos G Derpanis."
            ],
            "title": "Evaluation of deep convolutional nets for document image classification and retrieval",
            "venue": "ICDAR.",
            "year": 2015
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun."
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "IEEE Conf. Comput. Vis. Pattern Recog., pages 770\u2013778.",
            "year": 2016
        },
        {
            "authors": [
                "Dan Hendrycks",
                "Steven Basart",
                "Mantas Mazeika",
                "Mohammadreza Mostajabi",
                "Jacob Steinhardt",
                "Dawn Song."
            ],
            "title": "Scaling out-of-distribution detection for real-world settings",
            "venue": "ICML.",
            "year": 2022
        },
        {
            "authors": [
                "Dan Hendrycks",
                "Kevin Gimpel."
            ],
            "title": "A baseline for detecting misclassified and out-of-distribution examples in neural networks",
            "venue": "ICLR.",
            "year": 2017
        },
        {
            "authors": [
                "Teakgyu Hong",
                "Donghyun Kim",
                "Mingi Ji",
                "Wonseok Hwang",
                "Daehyun Nam",
                "Sungrae Park."
            ],
            "title": "Bros: A pre-trained language model focusing on text and layout for better key information extraction from documents",
            "venue": "AAAI.",
            "year": 2022
        },
        {
            "authors": [
                "Yen-Chang Hsu",
                "Yilin Shen",
                "Hongxia Jin",
                "Zsolt Kira."
            ],
            "title": "Generalized odin: Detecting out-ofdistribution image without learning from out-ofdistribution data",
            "venue": "CVPR.",
            "year": 2020
        },
        {
            "authors": [
                "Rui Huang",
                "Andrew Geng",
                "Yixuan Li."
            ],
            "title": "On the importance of gradients for detecting distributional shifts in the wild",
            "venue": "NeurIPS.",
            "year": 2021
        },
        {
            "authors": [
                "Rui Huang",
                "Yixuan Li."
            ],
            "title": "Mos: Towards scaling out-of-distribution detection for large semantic space",
            "venue": "CVPR.",
            "year": 2021
        },
        {
            "authors": [
                "Yupan Huang",
                "Tengchao Lv",
                "Lei Cui",
                "Yutong Lu",
                "Furu Wei."
            ],
            "title": "Layoutlmv3: Pre-training for document ai with unified text and image masking",
            "venue": "ACMMM.",
            "year": 2022
        },
        {
            "authors": [
                "Guillaume Jaume",
                "Hazim Kemal Ekenel",
                "JeanPhilippe Thiran."
            ],
            "title": "Funsd: A dataset for form understanding in noisy scanned documents",
            "venue": "ICDAR Workshop.",
            "year": 2019
        },
        {
            "authors": [
                "Di Jin",
                "Shuyang Gao",
                "Seokhwan Kim",
                "Yang Liu",
                "Dilek Hakkani-Tur."
            ],
            "title": "Towards textual out-ofdomain detection without in-domain labels",
            "venue": "TASLP.",
            "year": 2022
        },
        {
            "authors": [
                "Geewook Kim",
                "Teakgyu Hong",
                "Moonbin Yim",
                "Jinyoung Park",
                "Jinyeong Yim",
                "Wonseok Hwang",
                "Sangdoo Yun",
                "Dongyoon Han",
                "Seunghyun Park"
            ],
            "title": "Donut: Document understanding transformer without ocr",
            "year": 2022
        },
        {
            "authors": [
                "Diederik P Kingma",
                "Jimmy Ba."
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "ICLR.",
            "year": 2014
        },
        {
            "authors": [
                "Stefan Larson",
                "Gordon Lim",
                "Yutong Ai",
                "David Kuang",
                "Kevin Leach."
            ],
            "title": "Evaluating out-ofdistribution performance on document image classifiers",
            "venue": "NeurIPS.",
            "year": 2022
        },
        {
            "authors": [
                "Kimin Lee",
                "Kibok Lee",
                "Honglak Lee",
                "Jinwoo Shin."
            ],
            "title": "A simple unified framework for detecting outof-distribution samples and adversarial attacks",
            "venue": "NeurIPS.",
            "year": 2018
        },
        {
            "authors": [
                "D. Lewis",
                "G. Agam",
                "S. Argamon",
                "O. Frieder",
                "D. Grossman",
                "J. Heard."
            ],
            "title": "Building a test collection for complex document information processing",
            "venue": "SIGIR.",
            "year": 2006
        },
        {
            "authors": [
                "Gen Li",
                "Nan Duan",
                "Yuejian Fang",
                "Daxin Jiang",
                "Ming Zhou."
            ],
            "title": "Unicoder-vl: A universal encoder for vision and language by cross-modal pre-training",
            "venue": "AAAI.",
            "year": 2020
        },
        {
            "authors": [
                "Junlong Li",
                "Yiheng Xu",
                "Tengchao Lv",
                "Lei Cui",
                "Cha Zhang",
                "Furu Wei."
            ],
            "title": "Dit: Self-supervised pretraining for document image transformer",
            "venue": "ACM MM.",
            "year": 2022
        },
        {
            "authors": [
                "Peizhao Li",
                "Jiuxiang Gu",
                "Jason Kuen",
                "Vlad Morariu",
                "Handong Zhao",
                "Rajiv Jain",
                "Varun Manjunatha",
                "Hongfu Liu."
            ],
            "title": "Selfdoc: Self-supervised document representation learning",
            "venue": "CVPR.",
            "year": 2021
        },
        {
            "authors": [
                "Xiaoya Li",
                "Jiwei Li",
                "Xiaofei Sun",
                "Chun Fan",
                "Tianwei Zhang",
                "Fei Wu",
                "Yuxian Meng",
                "Jun Zhang."
            ],
            "title": "kfolden: k-fold ensemble for out-of-distribution detection",
            "venue": "EMNLP.",
            "year": 2021
        },
        {
            "authors": [
                "Shiyu Liang",
                "Yixuan Li",
                "Rayadurgam Srikant."
            ],
            "title": "Enhancing the reliability of out-of-distribution image detection in neural networks",
            "venue": "ICLR.",
            "year": 2018
        },
        {
            "authors": [
                "Weitang Liu",
                "Xiaoyun Wang",
                "John Owens",
                "Yixuan Li."
            ],
            "title": "Energy-based out-of-distribution detection",
            "venue": "NeurIPS.",
            "year": 2020
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Roberta: A robustly optimized bert pretraining approach",
            "venue": "arXiv preprint arXiv:1907.11692.",
            "year": 2019
        },
        {
            "authors": [
                "Jiasen Lu",
                "Dhruv Batra",
                "Devi Parikh",
                "Stefan Lee."
            ],
            "title": "Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks",
            "venue": "NeurIPS.",
            "year": 2019
        },
        {
            "authors": [
                "Yifei Ming",
                "Ziyang Cai",
                "Jiuxiang Gu",
                "Yiyou Sun",
                "Wei Li",
                "Yixuan Li."
            ],
            "title": "Delving into out-ofdistribution detection with vision-language representations",
            "venue": "NeurIPS.",
            "year": 2022
        },
        {
            "authors": [
                "Yifei Ming",
                "Ying Fan",
                "Yixuan Li."
            ],
            "title": "Poem: Out-of-distribution detection with posterior sampling",
            "venue": "ICML. PMLR.",
            "year": 2022
        },
        {
            "authors": [
                "Yifei Ming",
                "Yixuan Li"
            ],
            "title": "How does finetuning impact out-of-distribution detection for visionlanguage models? IJCV",
            "year": 2023
        },
        {
            "authors": [
                "Yifei Ming",
                "Yiyou Sun",
                "Ousmane Dia",
                "Yixuan Li"
            ],
            "title": "How to exploit hyperspherical embeddings for out-of-distribution detection? In ICLR",
            "year": 2023
        },
        {
            "authors": [
                "Yifei Ming",
                "Hang Yin",
                "Yixuan Li."
            ],
            "title": "On the impact of spurious correlation for out-of-distribution detection",
            "venue": "AAAI.",
            "year": 2022
        },
        {
            "authors": [
                "Ajoy Mondal",
                "Peter Lipps",
                "CV Jawahar."
            ],
            "title": "Iiitar-13k: a new dataset for graphical object detection in documents",
            "venue": "International Workshop on Document Analysis Systems.",
            "year": 2020
        },
        {
            "authors": [
                "Eric Nalisnick",
                "Akihiro Matsukawa",
                "Yee Whye Teh",
                "Dilan Gorur",
                "Balaji Lakshminarayanan"
            ],
            "title": "Do deep generative models know what they don\u2019t know? In ICLR",
            "year": 2019
        },
        {
            "authors": [
                "Poojan Oza",
                "Vishal M Patel."
            ],
            "title": "C2ae: Class conditioned auto-encoder for open-set recognition",
            "venue": "CVPR.",
            "year": 2019
        },
        {
            "authors": [
                "Seunghyun Park",
                "Seung Shin",
                "Bado Lee",
                "Junyeop Lee",
                "Jaeheung Surh",
                "Minjoon Seo",
                "Hwalsuk Lee."
            ],
            "title": "Cord: A consolidated receipt dataset for post-ocr parsing",
            "venue": "NeurIPS Workshop.",
            "year": 2019
        },
        {
            "authors": [
                "Alexander Podolskiy",
                "Dmitry Lipin",
                "Andrey Bout",
                "Ekaterina Artemova",
                "Irina Piontkovskaya."
            ],
            "title": "Revisiting mahalanobis distance for transformer-based out-of-domain detection",
            "venue": "AAAI.",
            "year": 2021
        },
        {
            "authors": [
                "Yu-Ting Qiang",
                "Yan-Wei Fu",
                "Xiao Yu",
                "Yan-Wen Guo",
                "Zhi-Hua Zhou",
                "Leonid Sigal."
            ],
            "title": "Learning to generate posters of scientific papers by probabilistic graphical models",
            "venue": "JCST.",
            "year": 2019
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark"
            ],
            "title": "Learning transferable visual models from natural language supervision",
            "venue": "In ICML",
            "year": 2021
        },
        {
            "authors": [
                "Jie Ren",
                "Jiaming Luo",
                "Yao Zhao",
                "Kundan Krishna",
                "Mohammad Saleh",
                "Balaji Lakshminarayanan",
                "Peter J Liu."
            ],
            "title": "Out-of-distribution detection and selective generation for conditional language models",
            "venue": "ICLR.",
            "year": 2023
        },
        {
            "authors": [
                "Madeline C Schiappa",
                "Yogesh S Rawat",
                "Shruti Vyas",
                "Vibhav Vineet",
                "Hamid Palangi."
            ],
            "title": "Multimodal robustness analysis against language and visual perturbations",
            "venue": "NeurIPS.",
            "year": 2022
        },
        {
            "authors": [
                "Vikash Sehwag",
                "Mung Chiang",
                "Prateek Mittal."
            ],
            "title": "Ssd: A unified framework for self-supervised outlier detection",
            "venue": "ICLR.",
            "year": 2021
        },
        {
            "authors": [
                "Yilin Shen",
                "Yen-Chang Hsu",
                "Avik Ray",
                "Hongxia Jin."
            ],
            "title": "Enhancing the generalization for intent classification and out-of-domain detection in SLU",
            "venue": "ACL-IJCNLP.",
            "year": 2021
        },
        {
            "authors": [
                "Ray Smith."
            ],
            "title": "An overview of the tesseract ocr engine",
            "venue": "ICDAR.",
            "year": 2007
        },
        {
            "authors": [
                "Weijie Su",
                "Xizhou Zhu",
                "Yue Cao",
                "Bin Li",
                "Lewei Lu",
                "Furu Wei",
                "Jifeng Dai."
            ],
            "title": "Vl-bert: Pre-training of generic visual-linguistic representations",
            "venue": "ICLR.",
            "year": 2019
        },
        {
            "authors": [
                "Yiyou Sun",
                "Chuan Guo",
                "Yixuan Li."
            ],
            "title": "React: Out-of-distribution detection with rectified activations",
            "venue": "NeurIPS.",
            "year": 2021
        },
        {
            "authors": [
                "Yiyou Sun",
                "Yifei Ming",
                "Xiaojin Zhu",
                "Yixuan Li."
            ],
            "title": "Out-of-distribution detection with deep nearest neighbors",
            "venue": "ICML.",
            "year": 2022
        },
        {
            "authors": [
                "Jihoon Tack",
                "Sangwoo Mo",
                "Jongheon Jeong",
                "Jinwoo Shin."
            ],
            "title": "Csi: Novelty detection via contrastive learning on distributionally shifted instances",
            "venue": "NeurIPS.",
            "year": 2020
        },
        {
            "authors": [
                "Zineng Tang",
                "Ziyi Yang",
                "Guoxin Wang",
                "Yuwei Fang",
                "Yang Liu",
                "Chenguang Zhu",
                "Michael Zeng",
                "Cha Zhang",
                "Mohit Bansal."
            ],
            "title": "Unifying vision, text, and layout for universal document processing",
            "venue": "CVPR.",
            "year": 2023
        },
        {
            "authors": [
                "Thirumalaisamy P Velavan",
                "Christian G Meyer."
            ],
            "title": "The covid-19 epidemic",
            "venue": "Tropical medicine & international health, 25(3):278.",
            "year": 2020
        },
        {
            "authors": [
                "Wenjin Wang",
                "Zhengjie Huang",
                "Bin Luo",
                "Qianglong Chen",
                "Qiming Peng",
                "Yinxu Pan",
                "Weichong Yin",
                "Shikun Feng",
                "Yu Sun",
                "Dianhai Yu"
            ],
            "title": "2022a. mmlayout: Multi-grained multimodal transformer for document understanding",
            "venue": "In ACMMM",
            "year": 2022
        },
        {
            "authors": [
                "Zilong Wang",
                "Jiuxiang Gu",
                "Chris Tensmeyer",
                "Nikolaos Barmpalios",
                "Ani Nenkova",
                "Tong Sun",
                "Jingbo Shang",
                "Vlad I Morariu."
            ],
            "title": "Mgdoc: Pre-training with multi-granular hierarchy for document image understanding",
            "venue": "EMNLP.",
            "year": 2022
        },
        {
            "authors": [
                "Thomas Wolf",
                "Lysandre Debut",
                "Victor Sanh",
                "Julien Chaumond",
                "Clement Delangue",
                "Anthony Moi",
                "Pierric Cistac",
                "Tim Rault",
                "R\u00e9mi Louf",
                "Morgan Funtowicz"
            ],
            "title": "Huggingface\u2019s transformers: State-ofthe-art natural language processing",
            "year": 2019
        },
        {
            "authors": [
                "Yuxin Wu",
                "Alexander Kirillov",
                "Francisco Massa",
                "Wan-Yen Lo",
                "Ross Girshick."
            ],
            "title": "Detectron2",
            "venue": "https://github.com/facebookresearch/ detectron2.",
            "year": 2019
        },
        {
            "authors": [
                "Zhisheng Xiao",
                "Qing Yan",
                "Yali Amit."
            ],
            "title": "Likelihood regret: An out-of-distribution detection score for variational auto-encoder",
            "venue": "NeurIPS.",
            "year": 2020
        },
        {
            "authors": [
                "Keyang Xu",
                "Tongzheng Ren",
                "Shikun Zhang",
                "Yihao Feng",
                "Caiming Xiong."
            ],
            "title": "Unsupervised out-of-domain detection via pre-trained transformers",
            "venue": "ACL.",
            "year": 2021
        },
        {
            "authors": [
                "Yang Xu",
                "Yiheng Xu",
                "Tengchao Lv",
                "Lei Cui",
                "Furu Wei",
                "Guoxin Wang",
                "Yijuan Lu",
                "Dinei Florencio",
                "Cha Zhang",
                "Wanxiang Che"
            ],
            "title": "2021b. Layoutlmv2: Multi-modal pre-training for visually-rich document understanding",
            "year": 2021
        },
        {
            "authors": [
                "Yiheng Xu",
                "Minghao Li",
                "Lei Cui",
                "Shaohan Huang",
                "Furu Wei",
                "Ming Zhou."
            ],
            "title": "Layoutlm: Pre-training of text and layout for document image understanding",
            "venue": "SIGKDD.",
            "year": 2020
        },
        {
            "authors": [
                "Xu Zhong",
                "Jianbin Tang",
                "Antonio Jimeno Yepes."
            ],
            "title": "Publaynet: largest dataset ever for document layout analysis",
            "venue": "ICDAR.",
            "year": 2019
        },
        {
            "authors": [
                "Wenxuan Zhou",
                "Fangyu Liu",
                "Muhao Chen."
            ],
            "title": "Contrastive out-of-distribution detection for pretrained transformers",
            "venue": "EMNLP.",
            "year": 2021
        },
        {
            "authors": [
                "Yunhua Zhou",
                "Peiju Liu",
                "Xipeng Qiu."
            ],
            "title": "KNNcontrastive learning for out-of-domain intent classification",
            "venue": "ACL.",
            "year": 2022
        },
        {
            "authors": [
                "PubLayNet (Zhong"
            ],
            "title": "2019), which contains 336K/11K training/validation images with 6 categories (text, title, list, fig., and table). The original IIIT-AR-13K (Mondal et al., 2020) contains (table, fig., natural image, logo, and signature)",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "The recent success of large-scale pre-training has propelled the widespread deployment of deep learning models in the document domain, where model predictions are used to help humans make decisions in various applications such as tax form processing and medical reports analysis. However, models are typically pre-trained on data collected from the web but deployed in an environment with distributional shifts (Cui et al., 2021). For instance, the outbreak of COVID-19 has led to continually\n\u2217 Equal contribution \u2020 Work done during the internship at Adobe Research\nchanging data distributions in machine-assisted medical document analysis systems (Velavan and Meyer, 2020). This motivates the need for reliable document understanding models against outof-distribution (OOD) inputs.\nThe goal of OOD detection is to categorize indistribution (ID) samples into one of the known categories and detect inputs that do not belong to any known classes at test time (Bendale and Boult, 2016). A plethora of OOD detection methods has been proposed for single-modal (image or text) inputs (Ge et al., 2017; Nalisnick et al., 2019; Oza and Patel, 2019; Tack et al., 2020; Hsu et al., 2020; Arora et al., 2021; Zhou et al., 2021; Xiao et al., 2020; Xu et al., 2021a; Li et al., 2021b; Shen et al., 2021; Jin et al., 2022; Zhou et al., 2022; Ming et al., 2022b,c; Podolskiy et al., 2021; Ren et al., 2023). Recent works (Fort et al., 2021; Esmaeilpour et al., 2022; Ming et al., 2022a; Ming and Li, 2023; Bitterwolf et al., 2023) also demonstrate promising OOD detection performance based on large-scale models pre-trained on text-image pairs, as pre-training enables models to learn powerful and transferable feature representations (Radford et al., 2021). However, it remains largely unexplored if existing findings in the OOD detection literature for images or texts can be naturally extended to the document\ndomain. Multiple unique challenges exist for document\nOOD detection. Unlike natural images, texts, or image-text pairs, no captions can describe a document and images in documents rarely contain natural objects. Moreover, the spatial relationship of text blocks further differentiates multimodal learning in documents from multimodal learning in the vision-language domain (Lu et al., 2019; Li et al., 2020). In addition, while recent pre-training methods have demonstrated remarkable performance in downstream document understanding tasks (Xu et al., 2020, 2021b; Li et al., 2021a; Gu et al., 2022; Hong et al., 2022; Huang et al., 2022; Li et al., 2022; Wang et al., 2022a), existing pre-training datasets for documents are limited and lack diversity. This is in sharp contrast to common pretraining datasets for natural images. It remains underexplored whether existing OOD detection methods are reliable in the document domain and how pre-training impacts OOD reliability.\nIn this work, we first present a comprehensive study to better understand OOD detection in the document domain through the following questions: (1) What is the role of document pre-training? How do pre-training datasets and tasks affect OOD detection performance? (2) Are existing OOD detection methods developed for natural images and texts transferrable to documents? (3) How does modality (textual, visual, and especially spatial information) affect OOD performance? In particular, we find that spatial information is critical for improving OOD reliability. Moreover, we propose a new spatial-aware adapter, a small learned module that can be inserted within a pre-trained language model such as RoBERTa (Liu et al., 2019). Our module is computationally efficient and significantly improves both ID classification and OOD detection performance (Sec. 5.2). Our contributions are summarized as follows:\n\u2022 We provide an extensive and in-depth study to investigate the impacts of pre-training, fine-tuning, model-modality, and OOD scoring functions on a broad spectrum of document OOD detection tasks. Our codebase will be open-sourced to facilitate future research.\n\u2022 We present unique insights on document OOD detection. For example, we observe that distancebased OOD scores are consistently advantageous over logit-based scores, which is underexplored\nin the recent OOD detection literature on visionlanguage pre-trained models.\n\u2022 We further propose a spatial-aware adapter module for transformer-based language models, facilitating easy adaptation of pre-trained language models to the document domain. Extensive experiments confirm the effectiveness of our module across diverse types of OOD data."
        },
        {
            "heading": "2 Preliminaries and Related Works",
            "text": ""
        },
        {
            "heading": "2.1 Document Models and Pre-Training",
            "text": "Large-scale pre-trained models gradually gain popularity in the document domain due to their success in producing generic representations from largescale unlabeled corpora in vision and natural language processing (NLP) tasks (Devlin et al., 2018; Lu et al., 2019; Su et al., 2019; Schiappa et al., 2022). As documents contain both visual and textual information distributed spatially in semantic regions, document-specific models and pre-training objectives are often necessary, which are distinct from vision or language domains.\nWe summarize common model structures for document pre-training in Fig. 2a. Specifically, LayoutLM (Xu et al., 2020) takes a sequence of Optical Character Recognition (OCR) (Smith, 2007) words and word bounding boxes as inputs. It extends BERT to learn contextualized word representations for document images through multitask learning. LayoutLMv2 (Xu et al., 2021b) improves on the prior work with new pre-training tasks to model the interaction among texts, layouts, and images. DocFormer (Appalaraju et al., 2021) adopts a CNN model to extract image grid features, fusing the spatial information as an inductive bias for the self-attention module. LayoutLMv3 (Huang et al., 2022) further enhances visual and spatial characteristics with masked image modeling and word-patch alignment tasks. Another line of work focuses on various granularities of documents, such as regionlevel text/image blocks. Examples of such models include SelfDoc (Li et al., 2021a), UDoc (Gu et al., 2021), and MGDoc (Wang et al., 2022b), which are pre-trained with a cross-modal encoder to capture the relationship between visual and textual features. These models incorporate spatial information by fusing position embeddings at the output layer of their encoders, instead of the input layer. Additionally, OCR-free models (Kim et al., 2022; Tang et al., 2023) tackle document understanding as a se-\nquence generation problem, unifying multiple tasks through an image-to-sequence generation network.\nWhile these pre-trained models demonstrate promising performance on downstream applications, their robustness to different types of OOD data, the influence of pre-training and fine-tuning, and the value of different modalities (e.g. spatial, textual, and visual) for document OOD detection remain largely unexplored."
        },
        {
            "heading": "2.2 Out-of-Distribution Detection",
            "text": "OOD detection has been extensively studied for open-world multi-class classification with natural image and text inputs, where the goal is to derive an OOD score that separates OOD from ID samples. A plethora of methods are proposed for deep neural networks, where the OOD scoring function is typically derived based on logits (without softmax scaling) (Hendrycks et al., 2022), softmax outputs (Liang et al., 2018; Hsu et al., 2020; Huang and Li, 2021; Sun et al., 2021), gradients (Huang et al., 2021), and feature embeddings (Tack et al., 2020; Fort et al., 2021; Ming et al., 2023). Despite their impressive performance on natural images and texts, it is underexplored if the results are transferrable to the document domain. A recent work (Larson et al., 2022) studied OOD detection for documents but only explored a limited number of models and OOD detection methods. The impacts of pre-training, fine-tuning, and spatial information remain unknown. In this work, we aim to provide a comprehensive and finer-grained analysis to shed light on the key factors for OOD robustness in the document domain.\nNotations. Following prior works on OOD detection with large-scale pre-trained models (Ming et al., 2022a; Ming and Li, 2023), the task of OOD detection is defined with respect to the downstream dataset, instead of the pre-training data which is often hard to characterize. In document classification, we use X in and Y in = {1, . . . ,K} to denote the input and label space, respectively. Let Din = {(xini , yini )}Ni=1 be the ID dataset, where x \u2208 X in, and yin \u2208 Y in. Let Dout = {(xouti , youti )}Mi=1 denote an OOD test set where yout \u2208 Yout, and Yout \u2229 Y in = \u2205. We express the neural network model f := g \u25e6 h as a composition of a feature extractor h : X \u2192 Rd and a classifier g : Rd \u2192 RK , which maps the feature embedding of an input to K real-valued numbers known as logits. During inference time, given an input x, OOD detection\ncan be formulated as:\nG\u03b3(x;h, g) = { ID S(x;h, g) \u2265 \u03b3 OOD S(x;h, g) < \u03b3 ,\nwhere S(\u00b7) is a scoring function that measures OOD uncertainty. In practice, the threshold q\u03b3 is often chosen so that a high fraction of ID data (e.g., 95%) is above the threshold.\nOOD detection scores. We focus on two major categories of computationally efficient OOD detection methods1: logit-based methods derive OOD scores from the logit layer of the model, while distance-based methods directly leverage feature embeddings, as shown in Fig. 1. We describe a few popular methods for each category as follows.\n\u2022 Logit-based: Maximum Softmax Probability (MSP) score (Hendrycks and Gimpel, 2017) SMSP = maxi\u2208[K] e fi(x)/ \u2211K j=1 e fj(x) natu-\nrally arises as a classic baseline as models often output lower softmax probabilities for OOD data; Energy score (Liu et al., 2020): SEnergy = log \u2211 i\u2208[K] e\nfi(x) utilizes the Helmholtz free energy of the data and theoretically aligns with the logarithm of the ID density; the simple MaxLogit score (Hendrycks et al., 2022): SMaxlogit = maxi\u2208[K] fi(x) has demonstrated promising performance on large-scale natural image datasets. We select the above scores due to their simplicity and computational efficiency. In addition, recent studies demonstrate that such simple scores are particularly effective with large-scale pre-trained models in vision (Fort et al., 2021) and visionlanguage domains (Ming et al., 2022a; Bitterwolf et al., 2023). We complement previous studies and investigate their effectiveness for documents.\n\u2022 Distance-based: Distance-based methods directly leverage feature embeddings z = h(x) based on the idea that OOD inputs are relatively far away from ID clusters in the feature space, compared to ID inputs. Distance-based methods can be characterized as parametric and non-parametric. Parametric methods such as Mahalanobis score (Lee et al., 2018; Sehwag et al., 2021) assume ID embeddings follow classconditional Gaussian distributions and use the Mahalanobis distance as the distance metric. On the other hand, non-parametric methods such as KNN+ (Sun et al., 2022) use cosine similarity as the distance metric.\n1We also investigate gradient-based methods such as GradNorm (Huang et al., 2021) in Appendix C.\nEvaluation metrics. To evaluate OOD detection performance, we adopt the following commonly used metrics: the Area Under the Receiver Operating Characteristic (AUROC), False Positive Rate at 95% Recall (FPR95), and the multi-class classification accuracy (ID Acc)."
        },
        {
            "heading": "3 Experimental Setup",
            "text": "Models. Fig. 2a summarizes common structures for document pre-training and classification models2. While documents typically come in the form of images (Harley et al., 2015), an OCR system can be used to extract words and their coordinates from the input image. Therefore, models can use singlemodal or multi-modal information. We categorize these models according to the input modalities into the following groups: (1) models using only visual features, (2) models using solely textual features, (3) models incorporating both visual and textual features, and (4) models integrating additional spatial (especially layout) information. Further details can be found in Appendix A.\n\u2022 Vision-only: Document classification can be viewed as a standard image classification problem. We consider ResNet-50 (He et al., 2016) and ViT (Fort et al., 2021) as exemplar document image classification models. We adopt two common pre-training settings: (1) only pre-trained on ImageNet (Deng et al., 2009) and (2) further pre-trained on IIT-CDIP (Lewis et al., 2006) with masked image modeling (MIM)3. After pretraining, we append a classifier for fine-tuning.\n2Apart from document classification, in the Appendix B, we also investigate OOD detection for two entity-level tasks: document entity recognition and document object detection.\n3Note that the document classification dataset we used in\n\u2022 Text-only: Alternatively, we can view document classification as text classification since documents often contain text blocks. To this end, we use RoBERTa (Liu et al., 2019) and Longformer (Beltagy et al., 2020) as the backbones. RoBERTa can handle up to 512 input tokens while Longformer can handle up to 4,096 input tokens. We pre-train the language models with masked language modeling (MLM) on IIT-CDIP extracted text corpus.\n\u2022 Text+Layout: Layout information plays a crucial role in the document domain, as shown in Fig. 3. To investigate the effect of layout information, we adopt LayoutLM as the backbone. We will show that spatial-aware models demonstrate promising OOD detection performance. However, such specialized models can be computationally expensive. Therefore, we propose a new spatial-aware adapter, a small learned module that can be inserted within a pre-trained language model such as RoBERTa and transforms it into a spatial-aware model, which is computationally efficient and competitive for both ID classification and OOD detection (Sec. 5.2).\n\u2022 Vision+Text+Layout: For comprehensiveness, we consider LayoutLMv3 and UDoc, which are large and computationally intensive. Both models are pre-trained on the full IIT-CDIP for fairness. These models utilize different input granularities and modalities, including textual, visual, and spatial information for document tasks.\nthis paper, RVL-CDIP (Harley et al., 2015), is a subset of IIT-CDIP. Hence, unless otherwise specified, the IIT-CDIP pre-training data used in this paper excludes RVL-CDIP.\nConstructing ID and OOD datasets. We construct ID datasets from RVL-CDIP (Harley et al., 2015), where 12 out of 16 classes are selected as ID classes. Dataset details are in Appendix A. We consider two OOD scenarios: in-domain and outdomain, based on the content (e.g., words, background) and layout characteristics.\n\u2022 In-domain OOD: To determine the OOD categories, we analyzed the performance of recent document classification models on the RVLCDIP test set. Fig. 2b shows the per-category test accuracy of various models. Naturally, for the classes the models perform poorly on, we may expect the models to detect such inputs as OOD instead of assigning a specific ID class with low confidence. We observe that the 4 categories (letter, form, scientific report, and presentation) result in the worst performance across most of the models with different modalities. We use these as OOD categories and construct the OOD datasets accordingly. The ID dataset is constructed from the remaining 12 categories, which we refer to as in-domain OOD datasets, as they are also sourced from RVL-CDIP.\n\u2022 Out-domain OOD: In the open-world setting, test inputs can have significantly different color schemes and layouts compared to ID samples. To mimic such scenarios, we use two public datasets as out-domain OOD test sets: NJUFudan Paper-Poster Dataset (Qiang et al., 2019) and CORD (Park et al., 2019). NJU-Fudan PaperPoster Dataset contains scientific posters in digital PDF format4. CORD is a receipt understanding dataset with significantly different inputs compared to RVL-CDIP. As shown in Fig. 3, receipt images can be challenging and require models to handle not only textual but also visual and spatial information.\nWe further support our domain selection using OTDD (Alvarez-Melis and Fusi, 2020), a flexible geometric method for comparing probability distributions, which enables us to compare any two datasets regardless of their label sets. We observe a clear gap between in-domain and out-domain data, which aligns with our data selection. Further details can be found in Appendix A.1.\n4Extracted using https://github.com/pymupdf/PyMuPDF"
        },
        {
            "heading": "4 Analyzing OOD Reliability for Documents",
            "text": ""
        },
        {
            "heading": "4.1 OOD Detection Without Fine-Tuning",
            "text": "5Note that we do not show 0% in Fig. 4c since we pre-train LayoutLM from scratch.\n0% 10% 20% 40% 100% + Percentage of IIT-CDIP data\n45.0\n47.5\n50.0\n52.5\n55.0\n57.5\n60.0\n62.5\nAU R\nO C\n(% )\nIn-domain OOD detection\nViTBase(RVL-CDIP) RoBERTaBase (RVL-CDIP)\n0% 10% 20% 40% 100% + Percentage of IIT-CDIP data\n75\n80\n85\n90\n95\n100 Out-domain OOD detection\nViTBase(Sci. Poster) ViTBase (Recipt) RoBERTaBase(Sci. Poster) RoBERTaBase (Recipt)\n(a) Pre-train on IIT-CDIP.\nLetter (1.70M)\n15% Form (1.08M)\n10%\nEmail (0.62M)\n6%\nHandwritten (0.49M)\n4%\nAdvertisement (0.47M)\n4%\nSci. report (1.30M)\n12%\nSci. publication (0.45M)\n4%\nSpecification (0.74M)\n7%\nFile folder (0.31M)\n3%\nNews article (0.34M)\n3% Budget (0.48M)\n4% Invoice (0.13M)\n1%\nPresentation (0.78M)7%\nQuestionnaire (0.92M)\n8%\nResume (0.10M)\n1%\nMemo (1.13M)\n10%\n(b) Analysis of IIT-CDIP.\n10% 20% 40% 100% + Percentage of IIT-CDIP data\n62.5\n65.0\n67.5\n70.0\n72.5\n75.0\n77.5\n80.0\nAU R\nO C\n(% )\nIn-domain OOD detection\nLayoutLMv1Base(RVL-CDIP) RoBERTaBase (RVL-CDIP)\n10% 20% 40% 100% + Percentage of IIT-CDIP data\n75\n80\n85\n90\n95\n100 Out-domain OOD detection\nLayoutLMv1Base(Sci. Poster) LayoutLMv1Base (Recipt) RoBERTaBase(Sci. Poster) RoBERTaBase (Recipt)\n(c) Pre-train on IIT-CDIP\u2212.\nFigure 4: The impact of pre-training data on zero-shot OOD detection performance. IIT-CDIP\u2212 denotes the filtered pre-training data after removing the \u201cOOD\" categories.\nfor boosting the OOD reliability in the document domain. Motivated by the above observations, we dive deeper and analyze spatial-aware models next.\nWhile pre-trained models exhibit the capability to differentiate data from various domains as a result of being trained on a diverse range of data. We observe that achieving more precise separation for in-domain OOD inputs remains difficult. Given this observation, we further analyze the impacts of fine-tuning for OOD detection with fixed pretraining datasets in the next section. By combining pre-trained models with a simple classifier and finetuning on RVL-CDIP (ID), we find that fine-tuning is advantageous in enhancing the OOD detection performance for both types of OOD samples."
        },
        {
            "heading": "4.2 The Impact of Fine-Tuning on Document OOD Detection",
            "text": "Recent document models are often pre-trained on a large-scale dataset and adapted to the target task via fine-tuning. To better understand the role of fine-tuning, we explore the following questions: 1) How does fine-tuning impact OOD reliability for in-domain and out-domain OOD inputs? 2) How does model modality impact the performance?\nWe consider a wide range of models pretrained on pure-text/image data (e.g., ImageNet and Wikipedia) described in Appendix A.3. During fine-tuning, we combine pre-trained models with a simple classifier and fine-tune on RVL-CDIP (ID). For models before and after fine-tuning, we extract the final feature embeddings and use a distancebased method KNN+ (Sun et al., 2022) for OOD detection. The results are shown in Fig. 6. We observe the following trends. First, fine-tuning largely improves OOD detection performance for both in-domain and out-domain OOD data. The same trend holds broadly across models with different modalities. Second, the improvement of fine-tuning is less significant for out-domain OOD data. For example, on Receipt (out-domain OOD), the AUROC for pre-trained ViT model is 97.13, whereas fine-tuning only improves by 0.79%. This suggests that pre-trained models do have the potential to separate data from different domains due to the diversity of data used for pre-training, while it remains hard for pre-trained models to perform finer-grained separation for in-domain OOD inputs. Therefore, fine-tuning is beneficial for improving OOD detection performance for both types of OOD\nRes Ne t50 Sw inBa se ViTB ase\nRoB ER\nTaB ase\nLon gfo\nrme rBas\ne 40\n50\n60\n70\n80\n90\n100\nAU R\nO C\n(% )\n48.58 52.15 50.44\n62.54\n55.08\n84.96\n91.68 89.4 88.02 87.15\nIn-domain OOD (RVL-CDIP, Average)\nPre-trained Fine-tuned\nRes Ne t50 Sw inBa se ViTB ase\nRoB ER\nTaB ase\nLon gfo\nrme rBas\ne\n85.0\n93.26 90.27\n95.99\n88.69\n99.12 96.73 97.96 96.52 96.01\nOut-domain OOD (Scientific Poster)\nPre-trained Fine-tuned\nRes Ne t50 Sw inBa se ViTB ase\nRoB ER\nTaB ase\nLon gfo\nrme rBas\ne\n97.0 99.35\n97.13\n72.99\n64.97\n98.98 98.3 97.92 92.47\n86.31\nOut-domain OOD (Receipt)\nPre-trained Fine-tuned\nFigure 6: OOD detection performance for pre-trained models w. and w.o. fine-tuning. We use a distance-based method KNN+ as the OOD scoring function. Fine-tuning significantly improves performance for both in and out-domain OOD data.\nsamples. To further validate our conclusion, we consider two additional in-domain OOD settings for our analysis: (1) selecting the classes the model performs well on, as in-domain OOD categories; (2) randomly selecting classes as OOD categories (Appendix A.2). We find that fine-tuning improves OOD detection for both settings, further verifying our observations.\nNext, we take a closer look at the impact of model modality on out-domain OOD detection. As shown in Fig. 6 (mid and right), both vision and text-based models demonstrate strong reliability against scientific posters (OOD). However, visionbased models display stronger performance than text-based models for Receipts (OOD). This can be explained by the fact that ViT was first pre-trained on ImageNet while scientific posters and receipts contain diverse visual information such as colors and edges for vision models to utilize (see Fig. 3). On the other hand, although fine-tuning text-based models largely improves the detection performance compared to pre-trained counterparts, utilizing only textual information can be inherently limited for out-domain OOD detection."
        },
        {
            "heading": "5 The Importance of Spatial-Awareness",
            "text": "In previous sections, we mainly focus on mainstream text-based and vision-based models for inand out-domain OOD detection. Next, we consider\nmodels tailored to document processing, which we refer to as spatial-aware models, such as LayoutLMv3 and UDoc. Given fine-tuned models, we compare the performance of logit-based and distance-based OOD scores.\nE ncoder\nC lassifier\nC lassifier E ncoder\nE ncoder"
        },
        {
            "heading": "5.1 Analysis of Spatial-Aware Models",
            "text": "We summarize key comparisons in Fig. 5, where we use MSP and Energy as exemplar logit-based scores and KNN+ as the distance-based score. Full results are in Appendix C. We can see that the simple KNN-based score (KNN+) consistently outperforms logit-based scores for both in-domain and\nout-domain OOD data across different models with different modalities. This is in contrast with recent works that investigate large-scale pre-trained models in the vision-language domain, where logitbased scores demonstrate strong OOD detection performance (Fort et al., 2021). As documents are distinct from natural image-text pairs, observations in the vision-language domain do not seamlessly translate to the document domain. Moreover, spatial-aware models demonstrate stronger OOD detection performance for both in and out-domain OOD. For example, with the best scoring function (KNN+), LayoutLMv3 improves the average AUROC by 7.09% for out-domain OOD and 7.54% for in-domain OOD data compared to RoBERTa. This further highlights the value of spatial information for improving OOD robustness for documents.\nDespite the impressive improvements brought by spatial-aware models, acquiring a large-scale pretraining dataset that includes spatial information remains challenging. In contrast, there is a growing abundance of pre-trained language models that are based on textual data. This motivates us to explore the possibility of leveraging these pre-trained language models by training an adapter on a small dataset containing document-specific information. By adopting this approach, we can effectively utilize existing models while minimizing the time and cost required for training."
        },
        {
            "heading": "5.2 Towards Effective Spatial-Aware Adapter",
            "text": "During our investigation into the effects of model modality, pre-training, and fine-tuning on various types of OOD inputs, we find that spatial/layout information plays a critical role in the document domain. However, existing pre-training models such as LayoutLM series, SelfDoc, and UDoc do not fully leverage the benefits of well-pre-trained language models. This raises the question of whether a large-scale language model, such as RoBERTa, can be adapted to detect OOD documents effectively. In this section, we demonstrate that incorporating an adapter module that accounts for spatial information with transformer-based pre-trained models can achieve strong performance with minimal changes to the code. To the best of our knowledge, this is the first study to apply the adapter idea to documents.\nSpatial-aware adapter. Given a pre-trained language model such as RoBERTa, we propose an adapter that utilizes spatial information. We consider two potential designs: 1) the adapter is ap-\nRoBERTaBase Spatial-RoBERTaBase 60\n65\n70\n75\n80\n85\n90\n95\n100\nAU R\nO C\n(% )\n63.52\n77.66\n87.04\n96.24\nIn-domain OOD (Avg)\nPre-trained Fine-tuned\nRoBERTaBase Spatial-RoBERTaBase 60\n65\n70\n75\n80\n85\n90\n95\n100\nAU R\nO C\n(% )\n92.73 89.4890.24\n95.19\nOut-domain OOD (Avg)\nPre-trained Fine-tuned\nRo BE\nRT a-b\nas e\nLo ng\nfor me\nr-b as\ne\nRo BE\nRT a-b\nas e (\n+) Re sN et5 0\nLo ng\nfor me\nr-b as\ne ( +)\nViT -ba\nse ViT -B as e ( +) Sw inba se\nSp ati\nalRo\nBE RT\na-b as\ne\nLa yo\nut LM\n-ba se UD oc\nSp ati\nalRo\nBE RT\na-l ar\nge\nLa yo\nut LM\nv3 -ba\nse\n86\n88\n90\n92\n94\n96 98 Pe rc en ta ge\nID Acc. and In-domain (RVL-CDIP, Avg.) OOD AUROC\nAccuracy (ID) AUROC (RVL-CDIP)\nRo BE\nRT a-b\nas e\nLo ng\nfor me\nr-b as\ne\nRo BE\nRT a-b\nas e (\n+) Re sN et5 0\nLo ng\nfor me\nr-b as\ne ( +)\nViT -ba\nse ViT -B as e ( +) Sw inba se\nSp ati\nalRo\nBE RT\na-b as\ne\nLa yo\nut LM\n-ba se UD oc\nSp ati\nalRo\nBE RT\na-l ar\nge\nLa yo\nut LM\nv3 -ba\nse\nPe rc\nen ta\nge\nID Acc. and Out-domain (Sci. Poster) OOD AUROC\nAccuracy (ID) AUROC (Sci. Poster)\nRo BE\nRT a-b\nas e\nLo ng\nfor me\nr-b as\ne\nRo BE\nRT a-b\nas e (\n+) Re sN et5 0\nLo ng\nfor me\nr-b as\ne ( +)\nViT -ba\nse ViT -B as e ( +) Sw inba se\nSp ati\nalRo\nBE RT\na-b as\ne\nLa yo\nut LM\n-ba se UD oc\nSp ati\nalRo\nBE RT\na-l ar\nge\nLa yo\nut LM\nv3 -ba\nse\nPe rc\nen ta\nge\nID Acc. and Out-domain (Receipt) OOD AUROC\nAccuracy (ID) AUROC (Recipt)\nFigure 9: Correlation between ID accuracy and OOD detection performance. For most models, ID accuracy is positively correlated with OOD detection performance. Language models with spatial-aware adapters (highlighted in blue) achieve significantly higher ID accuracy and stronger OOD robustness (in AUROC) compared to language models without adapters. Here, (+) represents further pre-training on the IIT-CDIP dataset.\npre-trained encoder. Our experiments demonstrate that introducing spatial-aware adapters during pretraining yields better results than only adding position embeddings during fine-tuning. For additional details6, please refer to Appendix C. In the following, we focus on analyzing Spatial-RoBERTa (pre) and comparing both ID and OOD performance with that of the pure-text pre-trained RoBERTa."
        },
        {
            "heading": "Spatial-RoBERTa significantly outperforms",
            "text": "RoBERTa. To verify the effectiveness of SpatialRoBERTa, we compare the OOD detection performance of pre-trained and fine-tuned models. The results are shown in Fig. 8, where OOD performance is based on KNN+ (K=10). Full results can be seen in Table 6. Spatial-RoBERTa significantly improves the OOD detection performance, especially after fine-tuning. For example, compared to RoBERTa (base), Spatial-RoBERTa (base) improves AUROC significantly by 4.24% averaged over four in-domain OOD datasets. This further confirms the importance of spatial information for OOD detection in the document domain."
        },
        {
            "heading": "Spatial-RoBERTa is competitive for both ID",
            "text": "classification and OOD detection. Beyond OOD detection performance, we also examine the multi-class ID classification accuracy and plot the two metrics for all models with different modalities in Fig. 9. We can clearly observe a positive correlation between ID accuracy and OOD detection performance (measured by AUROC) for both in-domain and out-domain OOD data. Moreover, spatial-aware models display superior ID accuracy and OOD robustness compared to text-only and\n6Spatial-RoBERTaBase (pre) incorporates position information during both pre-training and fine-tuning, while SpatialRoBERTaBase (post) only inserts the adapter into the output layer for fine-tuning.\nvision-only models. Overall, Spatial-RoBERTa greatly improves upon RoBERTa and matches the performance of models with more complex and specialized architectures such as LayoutLM. Specifically, Spatial-RoBERTaLarge achieves 97.37 ID accuracy, which is even higher than LayoutLM (97.28) and UDoc (97.36).\nTo summarize, our spatial-aware adapter effectively adapts pre-trained transformer-based text models to the document domain, improving both ID and OOD performance. In addition, by freezing the original word embeddings during pre-training, the models (Spatial-RoBERTaBase and SpatialRoBERTaLarge) are parameter-efficient and thus reduce the training cost."
        },
        {
            "heading": "6 Conclusions",
            "text": "In this work, we provide a comprehensive and indepth study on the impacts of pre-training, finetuning, model-modality, and OOD scores on a broad variety of document OOD detection tasks. We present novel insights on document OOD detection, which are under-explored or in contrast with OOD detection works based on vision-language models. In particular, we highlight that spatial information is critical for OOD detection in documents. We further propose a spatial-aware adapter as an add-on module to transformer-based models. Our module adapts pre-trained language models to the document domain. Extensive experiments on a broad range of datasets verify the effectiveness of our design. We hope our work will inspire future research toward improving OOD robustness for reliable document understanding."
        },
        {
            "heading": "7 Limitations",
            "text": "In this work, our main focus is on OOD detection for document understanding, with a specific emphasis on the context of document classification. As OOD detection based on document pre-trained models remains largely underexplored, we believe establishing an in-depth and extensive study of OOD detection for document classification would be a valuable stepping stone towards more complex tasks. Apart from document classification, in the Appendix B, we also investigate OOD detection for two entity-level tasks: document entity recognition and document object detection. We leave a more comprehensive treatment for future works."
        },
        {
            "heading": "A Dataset and Model Details",
            "text": ""
        },
        {
            "heading": "A.1 Datasets",
            "text": "The full RVL-CDIP dataset consists of 320K/40K/40K training/validation/testing images under 16 categories. We select 12 of them as the ID (In-domain) data. We employ the Google OCR engine7 to extract the text and layout information, which provides tokens, text blocks and the corresponding bounding boxes."
        },
        {
            "heading": "A.2 Quantifying OOD Dataset Construction",
            "text": "The distance between datasets can be measured via Optimal Transport Dataset Distance (OTDD)8. We visualize the OTDD distance between ID and the OOD (both in-domain and out-domain) data in Fig. 10a, where we highlight the in-domain OOD data in blue and the out-domain OOD data in green. Specifically, we randomly sample 1000 images from each dataset and calculate the average distance between pairs of datasets. We can see a significant gap between the OTDD of in-domain OOD data and out-domain OOD data. To make the analysis more thorough, we consider two additional in-domain OOD settings: (1) select the classes the model performs well as OOD data; (2) randomly select classes as OOD data. The results are shown in Fig. 10b and Fig. 10c. We can see that the distance between ID and in-domain OOD is similar to the original scheme (Fig. 10a). This suggests that most in-domain OOD categories are not far from ID data.\nWhile this paper represents an initial endeavor, we hope that our work will serve as a stepping stone towards constructing more comprehensive and diverse OOD benchmarks in the document domain, akin to those available in the NLP and natural image domain."
        },
        {
            "heading": "A.3 Models and Training Details",
            "text": "All models reported in Fig. 2b, except UDoc, are initialized with pre-trained weights from Huggingface9 and fine-tuned on the full RVL-CDIP training set. During fine-tuning, we train these models on RVL-CDIP with the cross-entropy loss. The models were optimized with Adam optimizer (Kingma and Ba, 2014) for 30 epochs with a batch size of 50 and a learning rate of 2\u00d7 10\u22125 on 8 A100 GPUs.\n7https://cloud.google.com/vision/docs/ocr 8https://github.com/microsoft/otdd 9https://huggingface.co/models\nThe following are the hyperparameters of the models used in our paper:"
        },
        {
            "heading": "Text-only:",
            "text": "\u2022 BERT and RoBERTa: We adopt RoBERTaBase (12 layers) and BERTBase (12 layers) as backbones and set the maximum sequence length to 512. For RoBERTa, the classifier consists of two linear layers followed by a tanh activation function.\n\u2022 LongformerBase: We also employ LongformerBase (12 layers) as the backbone and set the maximum sequence length to 4,096."
        },
        {
            "heading": "Vision-only:",
            "text": "\u2022 ResNet50: We adopt ResNet50 pre-trained on ImageNet-1k as the backbone. We fine-tune the model at a resolution of 224\u00d7224.\n\u2022 ViT: We consider ViTBase (vit-base-patch16224, pre-trained on ImageNet-21k) as the backbon and fine-tune at a resolution of 224\u00d7224.\n\u2022 SwinB: We also use the Swin Transformer (swin-base-patch4-window7-224-in22k, pretrained on ImageNet-21k) as the backbone and fine-tune the model at a resolution of 224\u00d7224."
        },
        {
            "heading": "Text+Layout:",
            "text": "\u2022 LayoutLMv1: This model employs the LayoutLM (layoutlm-base-uncased, 12 layers, pre-trained on IIT-CDIP) as the backbone. We set the maximum sequence length to 512.\n\u2022 Spatial-RoBERTaBase (Pre): This model combines our spatial-aware adapter to the pretrained RoBERTaBase model. The adapter is applied to the word embedding layer. We freeze the pre-trained word embeddings and optimize the spatial-aware adapter and transformers.\n\u2022 Spatial-RoBERTaBase (Post): Instead of inserting the spatial-aware adapter in the input layer, this model integrates the spatial-aware adapter at the output layer of the transformer.\n1"
        },
        {
            "heading": "RVL-CDIP (OOD, Presentation) Sci. Poster (OOD)",
            "text": "2\nE ne\nrg y\n(a) ResNet-50 (b) RoBERTa-base (c) LayouLMv3-base\nM SP\nK N\nN\n(k =1\n0)\nE ne\nrg y\nM SP\nK N\nN\n(k =1\n0)\nE ne\nrg y\nM SP\nK N\nN\n(k =1\n0)\nFigure 12: MSP, Energy, KNN, and Maha score histogram distributions of ID (blue) and OOD (green) inputs derived from fine-tuned ResNet-50, RoBERTa, and LayoutLMv3. The KNN scores calculated from both vision and language models naturally form smooth distributions. In contrast, MSP and Maha scores for both in- and out-of-distribution data concentrate on high values. Overall our experiments show that using feature space makes the scores more distinguishable between and out-of-distributions and, as a result, enables more effective OOD detection.\nObject Detection Faster RCNN, etc.\nVisual Encoder\nLayout+Vision\nBBoxes\nEntity Recognition Layout+Vision+Language\nVisual Encoder\nWords+BBoxes\nTextual Encoder BBoxes\nLayout+Language\nWords+BBoxes\nDetection HeadMultimodal Encoder\nRecognition Head\nProposal Generation Visual Encoder\nRecognition Head\nTextual Encoder\nRecognition Head"
        },
        {
            "heading": "Vision+Text+Layout:",
            "text": "\u2022 LaytouLMv3: We use LayoutLMv3 (layoutlmv3-base, 12 layers, pre-trained on IIT-CDIP) as the backbone.\n\u2022 UDoc: We use a slight variant of UDoc with the only difference in the sentence encoder, where we adopt a smaller version of the pretrained sentence encoder (all-MiniLM-L6-v2, 6 layers) instead of the larger sentence encoder (bert-base-nli-mean-tokens, 12 layers)."
        },
        {
            "heading": "B Beyond Document Classification",
            "text": "Document Object Detection. For document object detection, we use PubLayNet as the ID dataset and construct the OOD dataset from IIIT-AR-13K. Unlike PubLayNet, where the documents are scientific articles, IIIT-AR-13K is a dataset for graphical object detection in business documents (e.g., annual reports), thus there exists an obvious domain gap. We select natural images as the OOD entity and filter images that contain the OOD entity. Two object detection models are considered in this paper: (1) Vanilla Faster-RCNN with ResNet50 visual backbone, and (2) Faster-RCNN with VOS (Du et al., 2022), a recent unknown-aware learning framework to improve OOD detection performance for natural images. Following the original paper, we use 1,000 samples for each ID class to estimate the class-conditional Gaussian statistics. The models are trained for 180k iterations with a base learning rate of 0.01 and a batch size of 8 using the Detectron2 framework (Wu et al., 2019). The performance of the models is measured using the mean average precision (MAP) @ intersection over union (IOU) [0.50:0.95] of bounding boxes.\nDocument Entity Recognition. For entity recognition, we construct ID and OOD datasets from FUNSD. Each semantic entity includes a list of words, a label, and a bounding box. The standard label set for this dataset contains four categories: question, answer, header, and other. In this paper, we select entities labeled as other or header as OOD data, and the entities belonging to the other three categories as ID. Instead of treating entity recognition as a named-entity recognition problem, we follow UDoc and solve this problem at the semantic region level. We replace the sentence encoder in UDoc with a smaller sentence encoder (all-MiniLM-L6-v210) from Huggingface (Wolf et al., 2019). We also have the following model variants to verify the effectiveness of the combination of modalities: textual-only, visual-only, textual+spatial, visual+spatial, and visual+textual+spatial.\nWe provide details on datasets and models as follows."
        },
        {
            "heading": "B.1 Datasets",
            "text": "The original FUNSD (Jaume et al., 2019) dataset contains 149 training and 50 testing images. For document entity recognition, we treat entities with the category other/header as OOD entities. After\n10https://huggingface.co/sentence-transformers"
        },
        {
            "heading": "B.2 Models",
            "text": ""
        },
        {
            "heading": "Vision/Vision+Layout:",
            "text": "\u2022 ResNet-50: This model is composed of the ResNet-50 from pre-trained UDoc. It adopts the RoI pooling followed by a classifier to extract the entity features.\n\u2022 ResNet-50+Position: This model also adapts UDoc\u2019s pre-trained ResNet-50 for further improvement. It makes the RoI features spatially aware by adding position embeddings, which are mapped from the bounding boxes via a linear mapping layer."
        },
        {
            "heading": "Text/Text+Layout:",
            "text": "\u2022 Sentence BERT: This model adopts the language branch of UDoc and appends the classifier to the output of the sentence encoder.\n4\n\u2022 Sentence BERT+Position: This model is close to the above model but adds position embeddings to the sentence embeddings."
        },
        {
            "heading": "Vision+Text+Layout:",
            "text": "\u2022 ResNet-50+sentence BERT: This model follows the same framework as UDoc, but replaces the sentence encoder in their original design with a more miniature sentence encoder (all-MiniLM-L6-v2).\n\u2022 SwinT+Sentence BERT: This model replaces the ResNet-50 visual backbone with a pre-trained tiny Swin Transformer (swintiny-patch4-window7-224) adopted from the Huggingface.\nAll the models are fine-tuned with the cross-entropy loss for 100 epochs, using a learning rate of 10\u22125\nand a batch size of 8 on an A100 GPU."
        },
        {
            "heading": "B.3 Summary of Observations",
            "text": "We provide a summary of observations here and hope to inspire future works on a thorough investigation of OOD detection for entity-level tasks. To identify entity types, models should not only understand the words but also utilize spatial and visual information.\nFor document entity recognition, the comparison of distance-based and logit-based OOD detection methods with different models are shown in Fig. 14a. More details are shown in Table 2. We see that models can better predict the entity type and also achieve better OOD robustness with the help of spatial information. Considering the weak language dependency between entities, it is not surprising that vision-based models achieve better performance than text-based models. In particular, UDoc with ResNet-50 achieves the best performance on two OOD test sets, illustrating that visual information plays a major role in increasing the discrimination of entities with similar semantics. For document object detection, we summarize our findings in Fig. 14b and describe them in more\ndetail in Table 1. We can see that the OOD detection performance is further improved by introducing document images from IIIT-AR-13K with the same ID annotations as training data.\nTo provide more intuitions, in Fig. 15, we visualize the document entity recognition OOD detection results. In Fig. 16, we visualize the prediction on sample OOD images, using object detection models trained without VOS (top) and with VOS (bottom), respectively. We can see that vanilla Faster RCNN trained on PubLayNet produces false positives when applied to the OOD document images from IIIT-AR-13K. Table 1 shows that introducing the unknown-aware learning method optimized for both ID and OOD can reduce the FPR95 while preserving the mAP on the ID data. This experiment indicates that incorporating uncertainty estimation into the entity detection training procedure can improve the reliability of the document object detection system."
        },
        {
            "heading": "C Detailed Experimental Results",
            "text": "\u2022 Table 2 corresponds to the results shown in Fig. 15 and Fig. 14a.\n\u2022 Table 1 corresponds to the results shown in Fig. 16 and Fig. 14b.\n\u2022 Table 3 and Table 7 correspond to the results shown in Fig. 4a.\n\u2022 Table 4 and Table 5 correspond to the results shown in Fig. 4c.\n\u2022 Table 6 corresponds to the results shown in Fig. 8 and Fig. 9.\n\u2022 Table 9 and Table 8 correspond to the results shown in Fig. 6 and Fig. 9.\n\u2022 Table 10 and Table 11 correspond to the analysis for Sec. 4 and Sec. 4.2.\n\u2022 Table 12 corresponds to the results shown in Fig. 9.\n5"
        }
    ],
    "title": "A Critical Analysis of Document Out-of-Distribution Detection",
    "year": 2023
}