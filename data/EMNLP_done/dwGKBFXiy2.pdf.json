{
    "abstractText": "Structured pruning methods have proven effective in reducing the model size and accelerating inference speed in various network architectures such as Transformers. Despite the versatility of encoder-decoder models in numerous NLP tasks, the structured pruning methods on such models are relatively less explored compared to encoder-only models. In this study, we investigate the behavior of the structured pruning of the encoder-decoder models in the decoupled pruning perspective of the encoder and decoder component, respectively. Our findings highlight two insights: (1) the number of decoder layers is the dominant factor of inference speed, and (2) low sparsity in the pruned encoder network enhances generation quality. Motivated by these findings, we propose a simple and effective framework, NASH, that narrows the encoder and shortens the decoder networks of encoder-decoder models. Extensive experiments on diverse generation and inference tasks validate the effectiveness of our method in both speedup and output quality.",
    "authors": [
        {
            "affiliations": [],
            "name": "Jongwoo Ko"
        },
        {
            "affiliations": [],
            "name": "Seungjoon Park"
        },
        {
            "affiliations": [],
            "name": "Yujin Kim"
        },
        {
            "affiliations": [],
            "name": "Sumyeong Ahn"
        },
        {
            "affiliations": [],
            "name": "Du-Seong Chang"
        },
        {
            "affiliations": [],
            "name": "Euijai Ahn"
        },
        {
            "affiliations": [],
            "name": "Se-Young Yun"
        }
    ],
    "id": "SP:d3f0834c42d3d03531a108616967faefb3ed8d3a",
    "references": [
        {
            "authors": [
                "Md Zahangir Alom",
                "Adam T Moody",
                "Naoya Maruyama",
                "Brian C Van Essen",
                "Tarek M Taha."
            ],
            "title": "Effective quantization approaches for recurrent neural networks",
            "venue": "2018 international joint conference on neural networks (IJCNN), pages 1\u20138. IEEE.",
            "year": 2018
        },
        {
            "authors": [
                "Satanjeev Banerjee",
                "Alon Lavie."
            ],
            "title": "METEOR: An automatic metric for MT evaluation with improved correlation with human judgments",
            "venue": "Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Transla-",
            "year": 2005
        },
        {
            "authors": [
                "Tianlong Chen",
                "Jonathan Frankle",
                "Shiyu Chang",
                "Sijia Liu",
                "Yang Zhang",
                "Zhangyang Wang",
                "Michael Carbin."
            ],
            "title": "The lottery ticket hypothesis for pretrained bert networks",
            "venue": "Advances in neural information processing systems, 33:15834\u201315846.",
            "year": 2020
        },
        {
            "authors": [
                "Wei-Lin Chiang",
                "Zhuohan Li",
                "Zi Lin",
                "Ying Sheng",
                "Zhanghao Wu",
                "Hao Zhang",
                "Lianmin Zheng",
                "Siyuan Zhuang",
                "Yonghao Zhuang",
                "Joseph E. Gonzalez",
                "Ion Stoica",
                "Eric P. Xing"
            ],
            "title": "Vicuna: An opensource chatbot impressing gpt-4 with 90%* chatgpt",
            "year": 2023
        },
        {
            "authors": [
                "Hyung Won Chung",
                "Le Hou",
                "Shayne Longpre",
                "Barret Zoph",
                "Yi Tay",
                "William Fedus",
                "Eric Li",
                "Xuezhi Wang",
                "Mostafa Dehghani",
                "Siddhartha Brahma"
            ],
            "title": "Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416",
            "year": 2022
        },
        {
            "authors": [
                "Jason Wei"
            ],
            "title": "2022b. Scaling instruction-finetuned language models",
            "year": 2022
        },
        {
            "authors": [
                "Mike Conover",
                "Matt Hayes",
                "Ankit Mathur",
                "Jianwei Xie",
                "Jun Wan",
                "Sam Shah",
                "Ali Ghodsi",
                "Patrick Wendell",
                "Matei Zaharia",
                "Reynold Xin"
            ],
            "title": "Free dolly: Introducing the world\u2019s first truly open instructiontuned llm",
            "year": 2023
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
            "year": 2019
        },
        {
            "authors": [
                "Chenhe Dong",
                "Guangrun Wang",
                "Hang Xu",
                "Jiefeng Peng",
                "Xiaozhe Ren",
                "Xiaodan Liang."
            ],
            "title": "EfficientBERT: Progressively searching multilayer perceptron via warm-up knowledge distillation",
            "venue": "Findings of the Association for Computational Linguis-",
            "year": 2021
        },
        {
            "authors": [
                "Jonathan Frankle",
                "Michael Carbin."
            ],
            "title": "The lottery ticket hypothesis: Finding sparse, trainable neural networks",
            "venue": "arXiv preprint arXiv:1803.03635.",
            "year": 2018
        },
        {
            "authors": [
                "Elias Frantar",
                "Dan Alistarh"
            ],
            "title": "Sparsegpt: Massive language models can be accurately pruned in one-shot",
            "year": 2023
        },
        {
            "authors": [
                "Xinyang Geng",
                "Arnav Gudibande",
                "Hao Liu",
                "Eric Wallace",
                "Pieter Abbeel",
                "Sergey Levine",
                "Dawn Song."
            ],
            "title": "Koala: A dialogue model for academic research",
            "venue": "Blog post.",
            "year": 2023
        },
        {
            "authors": [
                "Bogdan Gliwa",
                "Iwona Mochol",
                "Maciej Biesek",
                "Aleksander Wawer."
            ],
            "title": "SAMSum corpus: A humanannotated dialogue dataset for abstractive summarization",
            "venue": "Proceedings of the 2nd Workshop on New Frontiers in Summarization, pages 70\u201379, Hong",
            "year": 2019
        },
        {
            "authors": [
                "Yuxian Gu",
                "Li Dong",
                "Furu Wei",
                "Minlie Huang."
            ],
            "title": "Knowledge distillation of large language models",
            "venue": "arXiv preprint arXiv:2306.08543.",
            "year": 2023
        },
        {
            "authors": [
                "Daya Guo",
                "Shuai Lu",
                "Nan Duan",
                "Yanlin Wang",
                "Ming Zhou",
                "Jian Yin."
            ],
            "title": "UniXcoder: Unified crossmodal pre-training for code representation",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long",
            "year": 2022
        },
        {
            "authors": [
                "Manish Gupta",
                "Puneet Agrawal."
            ],
            "title": "Compression of deep learning models for text: A survey",
            "venue": "ACM Trans. Knowl. Discov. Data, 16:61:1\u201361:55.",
            "year": 2020
        },
        {
            "authors": [
                "Song Han",
                "Xingyu Liu",
                "Huizi Mao",
                "Jing Pu",
                "Ardavan Pedram",
                "Mark A. Horowitz",
                "William J. Dally."
            ],
            "title": "Eie: Efficient inference engine on compressed deep neural network",
            "venue": "SIGARCH Comput. Archit. News, 44(3):243\u2013254.",
            "year": 2016
        },
        {
            "authors": [
                "Pengcheng He",
                "Jianfeng Gao",
                "Weizhu Chen."
            ],
            "title": "Newmodel: Improving deBERTa using ELECTRAstyle pre-training with gradient-disentangled embedding sharing",
            "venue": "International Conference on Learning Representations.",
            "year": 2023
        },
        {
            "authors": [
                "Qinyao He",
                "He Wen",
                "Shuchang Zhou",
                "Yuxin Wu",
                "Cong Yao",
                "Xinyu Zhou",
                "Yuheng Zou."
            ],
            "title": "Effective quantization methods for recurrent neural networks",
            "venue": "arXiv preprint arXiv:1611.10176.",
            "year": 2016
        },
        {
            "authors": [
                "Lu Hou",
                "Zhiqi Huang",
                "Lifeng Shang",
                "Xin Jiang",
                "Xiao Chen",
                "Qun Liu."
            ],
            "title": "Dynabert: Dynamic bert with adaptive width and depth",
            "venue": "Advances in Neural Information Processing Systems, 33:9782\u20139793.",
            "year": 2020
        },
        {
            "authors": [
                "Xiaoqi Jiao",
                "Yichun Yin",
                "Lifeng Shang",
                "Xin Jiang",
                "Xiao Chen",
                "Linlin Li",
                "Fang Wang",
                "Qun Liu."
            ],
            "title": "Tinybert: Distilling bert for natural language understanding",
            "venue": "arXiv preprint arXiv:1909.10351.",
            "year": 2019
        },
        {
            "authors": [
                "Xiaoqi Jiao",
                "Yichun Yin",
                "Lifeng Shang",
                "Xin Jiang",
                "Xiao Chen",
                "Linlin Li",
                "Fang Wang",
                "Qun Liu."
            ],
            "title": "TinyBERT: Distilling BERT for natural language understanding",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4163\u2013",
            "year": 2020
        },
        {
            "authors": [
                "Jungo Kasai",
                "Nikolaos Pappas",
                "Hao Peng",
                "James Cross",
                "Noah A Smith."
            ],
            "title": "Deep encoder, shallow decoder: Reevaluating non-autoregressive machine translation",
            "venue": "arXiv preprint arXiv:2006.10369.",
            "year": 2020
        },
        {
            "authors": [
                "Eldar Kurtic",
                "Elias Frantar",
                "Dan Alistarh"
            ],
            "title": "Ziplm: Hardware-aware structured pruning of language models",
            "year": 2023
        },
        {
            "authors": [
                "Woosuk Kwon",
                "Sehoon Kim",
                "Michael W. Mahoney",
                "Joseph Hassoun",
                "Kurt Keutzer",
                "Amir Gholami"
            ],
            "title": "A fast post-training pruning framework for transformers",
            "year": 2022
        },
        {
            "authors": [
                "Fran\u00e7ois Lagunas",
                "Ella Charlaix",
                "Victor Sanh",
                "Alexander Rush."
            ],
            "title": "Block pruning for faster transformers",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 10619\u201310629, Online and Punta Cana,",
            "year": 2021
        },
        {
            "authors": [
                "Mike Lewis",
                "Yinhan Liu",
                "Naman Goyal",
                "Marjan Ghazvininejad",
                "Abdelrahman Mohamed",
                "Omer Levy",
                "Veselin Stoyanov",
                "Luke Zettlemoyer"
            ],
            "title": "BART: Denoising sequence-to-sequence pre-training for natural language",
            "year": 2020
        },
        {
            "authors": [
                "Chin-Yew Lin."
            ],
            "title": "ROUGE: A package for automatic evaluation of summaries",
            "venue": "Text Summarization Branches Out, pages 74\u201381, Barcelona, Spain. Association for Computational Linguistics.",
            "year": 2004
        },
        {
            "authors": [
                "Christos Louizos",
                "Max Welling",
                "Diederik P. Kingma"
            ],
            "title": "Learning sparse neural networks through l0 regularization",
            "year": 2018
        },
        {
            "authors": [
                "Shashi Narayan",
                "Shay B. Cohen",
                "Mirella Lapata."
            ],
            "title": "Don\u2019t give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Lan-",
            "year": 2018
        },
        {
            "authors": [
                "Long Ouyang",
                "Jeffrey Wu",
                "Xu Jiang",
                "Diogo Almeida",
                "Carroll Wainwright",
                "Pamela Mishkin",
                "Chong Zhang",
                "Sandhini Agarwal",
                "Katarina Slama",
                "Alex Ray"
            ],
            "title": "Training language models to follow instructions with human",
            "year": 2022
        },
        {
            "authors": [
                "Adam Paszke",
                "Sam Gross",
                "Francisco Massa",
                "Adam Lerer",
                "James Bradbury",
                "Gregory Chanan",
                "Trevor Killeen",
                "Zeming Lin",
                "Natalia Gimelshein",
                "Luca Antiga"
            ],
            "title": "Pytorch: An imperative style, high-performance deep learning library",
            "year": 2019
        },
        {
            "authors": [
                "Sai Prasanna",
                "Anna Rogers",
                "Anna Rumshisky."
            ],
            "title": "When bert plays the lottery, all tickets are winning",
            "venue": "arXiv preprint arXiv:2005.00561.",
            "year": 2020
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J. Liu"
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "year": 2020
        },
        {
            "authors": [
                "Victor Sanh",
                "Lysandre Debut",
                "Julien Chaumond",
                "Thomas Wolf."
            ],
            "title": "Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter",
            "venue": "arXiv preprint arXiv:1910.01108.",
            "year": 2019
        },
        {
            "authors": [
                "Victor Sanh",
                "Thomas Wolf",
                "Alexander M. Rush"
            ],
            "title": "Movement pruning: Adaptive sparsity by finetuning",
            "year": 2020
        },
        {
            "authors": [
                "Michael Santacroce",
                "Zixin Wen",
                "Yelong Shen",
                "Yuanzhi Li"
            ],
            "title": "What matters in the structured pruning of generative language models? arXiv preprint arXiv:2302.03773",
            "year": 2023
        },
        {
            "authors": [
                "Abigail See",
                "Peter J. Liu",
                "Christopher D. Manning."
            ],
            "title": "Get to the point: Summarization with pointergenerator networks",
            "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1073\u2013",
            "year": 2017
        },
        {
            "authors": [
                "Sheng Shen",
                "Zhen Dong",
                "Jiayu Ye",
                "Linjian Ma",
                "Zhewei Yao",
                "Amir Gholami",
                "Michael W Mahoney",
                "Kurt Keutzer."
            ],
            "title": "Q-bert: Hessian based ultra low precision quantization of bert",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, vol-",
            "year": 2020
        },
        {
            "authors": [
                "Sam Shleifer",
                "Alexander M Rush."
            ],
            "title": "Pretrained summarization distillation",
            "venue": "arXiv preprint arXiv:2010.13002.",
            "year": 2020
        },
        {
            "authors": [
                "Siqi Sun",
                "Yu Cheng",
                "Zhe Gan",
                "Jingjing Liu."
            ],
            "title": "Patient knowledge distillation for bert model compression",
            "venue": "arXiv preprint arXiv:1908.09355.",
            "year": 2019
        },
        {
            "authors": [
                "Zhiqing Sun",
                "Hongkun Yu",
                "Xiaodan Song",
                "Renjie Liu",
                "Yiming Yang",
                "Denny Zhou."
            ],
            "title": "Mobilebert: a compact task-agnostic bert for resource-limited devices",
            "venue": "arXiv preprint arXiv:2004.02984.",
            "year": 2020
        },
        {
            "authors": [
                "Chaofan Tao",
                "Lu Hou",
                "Haoli Bai",
                "Jiansheng Wei",
                "Xin Jiang",
                "Qun Liu",
                "Ping Luo",
                "Ngai Wong."
            ],
            "title": "Structured pruning for efficient generative pre-trained language models",
            "venue": "Findings of the Association for Computational Linguistics: ACL 2023, pages 10880\u2013",
            "year": 2023
        },
        {
            "authors": [
                "Yi Tay",
                "Mostafa Dehghani",
                "Jinfeng Rao",
                "William Fedus",
                "Samira Abnar",
                "Hyung Won Chung",
                "Sharan Narang",
                "Dani Yogatama",
                "Ashish Vaswani",
                "Donald Metzler."
            ],
            "title": "Scale efficiently: Insights from pretraining and fine-tuning transformers",
            "venue": "arXiv preprint",
            "year": 2021
        },
        {
            "authors": [
                "Yi Tay",
                "Mostafa Dehghani",
                "Vinh Q. Tran",
                "Xavier Garcia",
                "Jason Wei",
                "Xuezhi Wang",
                "Hyung Won Chung",
                "Dara Bahri",
                "Tal Schuster",
                "Steven Zheng",
                "Denny Zhou",
                "Neil Houlsby",
                "Donald Metzler."
            ],
            "title": "UL2: Unifying language learning paradigms",
            "venue": "The Eleventh Inter-",
            "year": 2023
        },
        {
            "authors": [
                "Hugo Touvron",
                "Thibaut Lavril",
                "Gautier Izacard",
                "Xavier Martinet",
                "Marie-Anne Lachaux",
                "Timoth\u00e9e Lacroix",
                "Baptiste Rozi\u00e8re",
                "Naman Goyal",
                "Eric Hambro",
                "Faisal Azhar"
            ],
            "title": "Llama: Open and efficient foundation language models",
            "year": 2023
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin."
            ],
            "title": "Attention is all you need",
            "venue": "Advances in neural information processing systems, 30.",
            "year": 2017
        },
        {
            "authors": [
                "Alex Wang",
                "Yada Pruksachatkun",
                "Nikita Nangia",
                "Amanpreet Singh",
                "Julian Michael",
                "Felix Hill",
                "Omer Levy",
                "Samuel Bowman."
            ],
            "title": "Superglue: A stickier benchmark for general-purpose language understanding systems",
            "venue": "Advances in neural information pro-",
            "year": 2019
        },
        {
            "authors": [
                "Alex Wang",
                "Amanpreet Singh",
                "Julian Michael",
                "Felix Hill",
                "Omer Levy",
                "Samuel Bowman."
            ],
            "title": "GLUE: A multi-task benchmark and analysis platform for natural language understanding",
            "venue": "Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing",
            "year": 2018
        },
        {
            "authors": [
                "Wenhui Wang",
                "Hangbo Bao",
                "Shaohan Huang",
                "Li Dong",
                "Furu Wei."
            ],
            "title": "Minilmv2: Multi-head self-attention relation distillation for compressing pretrained transformers",
            "venue": "arXiv preprint arXiv:2012.15828.",
            "year": 2020
        },
        {
            "authors": [
                "Wenhui Wang",
                "Furu Wei",
                "Li Dong",
                "Hangbo Bao",
                "Nan Yang",
                "Ming Zhou."
            ],
            "title": "Minilm: Deep selfattention distillation for task-agnostic compression of pre-trained transformers",
            "venue": "Advances in Neural Information Processing Systems, 33:5776\u20135788.",
            "year": 2020
        },
        {
            "authors": [
                "Hajishirzi."
            ],
            "title": "Self-instruct: Aligning language models with self-generated instructions",
            "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 13484\u201313508, Toronto, Canada. Association",
            "year": 2023
        },
        {
            "authors": [
                "Yue Wang",
                "Hung Le",
                "Akhilesh Deepak Gotmare",
                "Nghi DQ Bui",
                "Junnan Li",
                "Steven CH Hoi."
            ],
            "title": "Codet5+: Open code large language models for code understanding and generation",
            "venue": "arXiv preprint arXiv:2305.07922.",
            "year": 2023
        },
        {
            "authors": [
                "Ziheng Wang",
                "Jeremy Wohlwend",
                "Tao Lei."
            ],
            "title": "Structured pruning of large language models",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6151\u20136162, Online. Association for Computa-",
            "year": 2020
        },
        {
            "authors": [
                "Scao",
                "Sylvain Gugger",
                "Mariama Drame",
                "Quentin Lhoest",
                "Alexander M. Rush."
            ],
            "title": "Transformers: State-of-the-art natural language processing",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System",
            "year": 2020
        },
        {
            "authors": [
                "Mengzhou Xia",
                "Zexuan Zhong",
                "Danqi Chen."
            ],
            "title": "Structured pruning learns compact and accurate models",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1513\u20131528, Dublin, Ireland.",
            "year": 2022
        },
        {
            "authors": [
                "Wenhan Xiong",
                "Jiawei Wu",
                "Hong Wang",
                "Vivek Kulkarni",
                "Mo Yu",
                "Shiyu Chang",
                "Xiaoxiao Guo",
                "William Yang Wang."
            ],
            "title": "TWEETQA: A social media focused question answering dataset",
            "venue": "Proceedings of the 57th Annual Meeting of the Asso-",
            "year": 2019
        },
        {
            "authors": [
                "Ziqing Yang",
                "Yiming Cui",
                "Zhigang Chen."
            ],
            "title": "Textpruner: A model pruning toolkit for pre-trained language models",
            "venue": "arXiv preprint arXiv:2203.15996.",
            "year": 2022
        },
        {
            "authors": [
                "Zhewei Yao",
                "Reza Yazdani Aminabadi",
                "Minjia Zhang",
                "Xiaoxia Wu",
                "Conglong Li",
                "Yuxiong He."
            ],
            "title": "Zeroquant: Efficient and affordable post-training quantization for large-scale transformers",
            "venue": "Advances in Neural Information Processing Systems, 35:27168\u2013",
            "year": 2022
        },
        {
            "authors": [
                "Ofir Zafrir",
                "Guy Boudoukh",
                "Peter Izsak",
                "Moshe Wasserblat."
            ],
            "title": "Q8bert: Quantized 8bit bert",
            "venue": "2019 Fifth Workshop on Energy Efficient Machine Learning and Cognitive Computing-NeurIPS Edition (EMC2-NIPS), pages 36\u201339. IEEE.",
            "year": 2019
        },
        {
            "authors": [
                "Tianyi Zhang",
                "Felix Wu",
                "Arzoo Katiyar",
                "Kilian Q Weinberger",
                "Yoav Artzi."
            ],
            "title": "Revisiting few-sample {bert} fine-tuning",
            "venue": "International Conference on Learning Representations.",
            "year": 2021
        },
        {
            "authors": [
                "Wang"
            ],
            "title": "2020c) observes that optimizing with those relaxed regularizations makes models converge to differentsize subnetworks depending on a learning rate and",
            "year": 2020
        },
        {
            "authors": [
                "Tay"
            ],
            "title": "In our evaluation, we examine the performance of our method on two tasks, namely TweetQA and SAMSum, using 2, 4, 6, and 8 decoder layers",
            "venue": "NASH",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "In recent years, pre-trained language models (LMs) have demonstrated their effectiveness in various downstream tasks, such as natural language understanding (NLU) and natural language generation (NLG). Especially, there have been three main types of research, e.g.,encoder-only LMs (Devlin et al., 2019; He et al., 2023), decoder-only LMs (Touvron et al., 2023; OpenAI, 2023), and encoder-decoder LMs (Lewis et al., 2020; Raffel et al., 2020; Chung et al., 2022b; Tay et al., 2023), which aim for their specific expertise. Among these various types of LMs, we will focus on the widely studied and utilized encoder-decoder LMs due to their flexibility in application across a range of tasks (Guo et al., 2022; Wang et al., 2023b).\n\u2217denotes equal contribution. \u2020 denotes equal advising. \u2021 denotes working done at KAIST AI. Correspondence to Jongwoo Ko <jongwoo.ko@kaist.ac.kr> .\nOn the other perspective of LM researches rather than performances, efficiency of LMs (e.g.,computational and memory cost) have been intensively studied because of their huge computational requirements. This research direction is called model compression. Among the various model compression techniques (Jiao et al., 2020; Yao et al., 2022), pruning (Frankle and Carbin, 2018; Sanh et al., 2020; Wang et al., 2020c; Xia et al., 2022) is a promising method that aims to remove redundant weights from networks, resulting in improved efficiency by saving storage capacity and enhancing inference speed. Between structured pruning and unstructured pruning approaches, structured pruning is typically preferred in practice due to its relative ease of deployment on various types of hardware platforms compared to unstructured pruning (Han et al., 2016; Gupta and Agrawal, 2020).\nTherefore, we focus on the structured pruning method specifically tailored for encoder-decoder LMs. Despite the remarkable advancements in encoder-decoder models, little attention has been given to structured pruning methods for encoderdecoder LMs. This can be attributed to the inherent\ndifferences in the components that enhance pruning efficiency between encoder and decoder networks. Consequently, traditional structured pruning methods that rely on encoder-only models may not effectively optimize encoder-decoder models. For instance, CoFi (Xia et al., 2022), one of the SoTA encoder-only pruning methods, demonstrates a maximum speedup improvement of 1.53\u00d7 on the CNN/DailyMail (See et al., 2017) dataset, with a ROUGE-L drop of 7.36%. This gain is considerably lower compared to the original result achieved on the encoder-only model applied to the worst QQP case in the GLUE (Wang et al., 2018), where the speedup reaches 11.0\u00d7 with an accuracy drop of 1.20%. Thus, it becomes crucial to investigate structured pruning methods that are specifically tailored for the encoder and decoder networks.\nTo this end, in this paper, we pose the following question: How can we design a structured pruning method that effectively accelerates the encoderdecoder model while maintaining its performance? To the best of our knowledge, this study represents the first attempt to address this question. In order to accomplish this, we conduct systematic studies to examine the impact of structured pruning on the encoder and decoder networks, respectively.\nContribution. In this study, we propose an algorithm, NASH, which is strongly motivated by two findings derived from our preliminary experiments. (1) The number of decoder layers is the primary factor for inference speedup. (2) The sparsity of the encoder network is a key factor affecting the output quality of encoder-decoder LMs.\nBased on these findings, we propose an algorithm, illustrated in Figure 1, that consists of two parts: the encoder network, which enhances output quality by gradually reducing the width of each layer, and the decoder network, which achieves faster inference speed by uniformly selecting layers to reduce depth.\nWe empirically evaluate the performance of NASH on various NLG datasets including standard fine-tuning on a single task (Gliwa et al., 2019; Xiong et al., 2019), multi-task learning scenarios, and recent instruction-tuning datasets (Conover et al., 2023; Wang et al., 2023a). Notably, in our experiments using T5-base, NASH achieves a speedup of 2.5-4.2\u00d7 while preserving 95% of the output quality. Our experimental results show that NASH can be a unified framework which is regardless of task difficulty and model type."
        },
        {
            "heading": "2 Preliminary",
            "text": "Transformers. We focus on the Transformer network (Vaswani et al., 2017), which consists of the encoder and decoder architecture. The encoder architecture is composed of L blocks, and each block consists of a multi-head attention (MHA) layer and a feed-forward (FFN) layer. An MHA layer in the i-th Transformer layer with Nh heads outputs:\nMHA(i,j)(Q,K,V) = Att(QW (i,j) Q ,KW (i,j) K ,VW (i,j) V ),\nMHA(i)(Q,K,V) = Nh\u2211 j=1 MHA(i,j)(Q,K,V)W (i,j) O ,\nwhere Att represents a dot product attention head, and Q, K, and V are the input sequences for query, key, and value, respectively. In self-attention layers, all of the keys, values, and queries come from the outputs of the previous layer. On the other hand, in cross-attention layers, the queries come from the previous decoder layer, while the memory keys and values come from the output of the encoder. It is important to note that the j-th head is parameterized by W(i,j)Q ,W (i,j) K ,W (i,j) V , and W(i,j)O \u2208 Rd\u00d7dh , which represent the query, key, value, and output matrices, respectively. Here, d and dh denote the hidden state dimension and attention head dimension, respectively.\nThe output of the MHA layer, denoted as X, is then fed into the FFN layer in the i-th Transformer layer:\nFFN(i)(X) = GELU(XW1)W2.\nHere, the two fully-connected layers are parameterized by W1 \u2208 Rd\u00d7df and W2 \u2208 Rdf\u00d7d, with df representing the dimension of the FFN layer.\nStructured Pruning. Structured pruning gradually removes unnecessary parameters from a model, targeting width-related components (e.g.,MHA heads, FFN hidden units) and depth-related elements (e.g.,Transformer layers) during training. Recent advancements have demonstrated significant speedups with minimal output quality reduction. For example, block pruning (Lagunas et al., 2021) and CoFi (Xia et al., 2022) have enhanced flexibility, optimization, and enabled simultaneous pruning at multiple levels.\nPruning the components of the i-th layer related\nto MHA can be formulated as follows:\nMHA(i)(Q,K,V) = z (i) MHA \u00b7 Nh\u2211 j=1 z (i,j) head \u00b7\nAtt(QW(i,j)Q ,KW (i,j) K ,VW (i,j) V )W (i,j) O ,\nwhere z(i)MHA and z (i,j) head \u2208 {0, 1} and to mask MHA layer and individual head of MHA. The FFN layer, which is another major component of the Transformer network, is also known to be over-parameterized (Dong et al., 2021). Strategies for pruning include pruning an entire FFN layer and pruning intermediate dimensions at a more granular width level. This can be achieved by introducing mask variables, z(i)FFN and z (i) int \u2208 {0, 1}df , with the following formulation:\nFFN(i)(X) = z (i) FFN\u00b7GELU(XW1)\u00b7diag(z (i) int )\u00b7W2\nVarious techniques have been employed to learn these mask variables used in structured pruning. For example, Wang et al. (2020c) and Xia et al. (2022) utilized L0 regularization to eliminate redundant parameters. On the other hand, Lagunas et al. (2021) adopted the movement score introduced by Sanh et al. (2020) as a measurement for their pruning approach."
        },
        {
            "heading": "3 Experimental Motivations",
            "text": "In this section, we separately investigate the behavior of the encoder and decoder when depth pruning is applied or not, using CoFi-T5, the modified version of CoFi (Xia et al., 2022) tailored for T5 (Raffel et al., 2020). Particularly, in Figure 2, we study the results of four cases: encoder with depth-pruning (\u25b3), encoder without depth-pruning (\u20dd), decoder with depth-pruning (\u25b3), and decoder without depth-pruning (\u20dd). From these four types\nof cases, we aim to address the following questions: (1) Does depth pruning exhibit different phenomena in each case? (2) What is the key factor for accelerating inference speed while preserving sufficient output quality? We provide detailed answers to each question by training T5-Base with target sparsities of {60%, 70%, 80%, 90%, 95%} for the decoder cases, and {20%, 40%, 60%, 70%, 80%, 90%, 95%} for the encoder cases. 1\nBefore delving into the detailed answers, we briefly address the first question: the impact of depth pruning when applied to the encoder and decoder, respectively. As depicted in Figure 2, depth pruning exhibits a significant influence on the decoder (as indicated by \u25b3 and \u20dd), while the encoder shows negligible effects (as observed in \u25b3 and \u20dd). Consequently, the appropriate utilization of depth pruning becomes crucial. In the following paragraphs, we outline our key findings related to the second question to establish an effective structured pruning mechanism for encoder-decoder LMs.\nFinding 3.1. The number of layers in the decoder network is the dominant factor affecting the inference speed, while the decoder width does not have a significant impact.\nWe evaluate the findings regarding the decoder network from two perspectives: (1) the effectiveness of layer-wise pruning and (2) the ineffectiveness of width pruning. Firstly, as demonstrated in the second and fourth plots of Figure 2, the decoder exhibits a significant speedup with minor degradation (when comparing \u25b3 and \u20dd), whereas the encoder shows no such effect (when comparing \u25b3 and \u20dd). This indicates that layer-wise pruning plays a dominant role in pruning the decoder. On the other hand, when comparing the model size\n1In the case of a high level of target sparsity, we observed that CoFi-T5 is unable to achieve the desired sparsity. A detailed explanation of this phenomenon is in the Appendix A.\nand speedup (as shown in the first and second plots of Figure 2 with \u20dd), width pruning reduces the model size but leads to both performance degradation and negligible speedup. This suggests that width pruning is not effective for the decoder.\nTo further investigate Finding 3.1, we investigate the inference speed of Transformer layers, with a specific focus on understanding why width pruning is ineffective. This analysis involves two key observations: (1) finding the metric that synergizes with width pruning, and (2) identifying the component that predominantly consumes computational resources. According to Figure 3, width pruning can have a significant impact on the computational cost as the sequence length increases. However, due to the inherent nature of the autoregressive decoding process, the decoder network is constrained to a sequence length of 1. As a result, width pruning cannot effectively improve the speed of the decoder network. Furthermore, as illustrated in Figure 4, Layer Normalization (LN) and dropout (DO) collectively contribute approximately 20-25% to the overall inference time. Consequently, the time allocated to these fixed operations remains constant, leading to diminished efficiency in terms of inference speed. In conclusion, width pruning is not an appropriate approach for optimizing the decoder.\nFinding 3.2. From the perspective of encoder pruning, while achieving high-level sparsity may not be desirable, attaining low-level sparsity not only slightly accelerates inference speed but also enhances performance.\nBy comparing the \u20dd points and \u22c6 in the second and fourth plots of Figure 2, we observe that encoder pruning yields a slight speedup along with improved performance. However, when the encoder network is heavily pruned, it experiences sig-\nnificant performance degradation. These findings emphasize the significance of considering pruning in both the decoder and encoder networks. Furthermore, they provide insights into the necessity of employing distinct pruning strategies for these two networks, considering their unique characteristics.\nComparison with Prior Observations. Our key findings provide valuable insights: the appropriate strategy for encoder-decoder models involves using a small number of layers for the decoder and minimal pruning for the encoder networks. Importantly, our observations offer a more generalized understanding compared to previous works (Kasai et al., 2020; Tay et al., 2021). Unlike prior studies that manually determined model configurations for specific tasks such as machine translation (Kasai et al., 2020) or NLU (Tay et al., 2021), our conclusions are derived automatically through gradual structured pruning and have been validated across both NLG and NLU tasks. Furthermore, while the DeepNarrow strategy proposed by Tay et al. (2021) demonstrates effectiveness in NLU tasks with short output sequences, it exhibits computational inefficiency when applied to NLG tasks. Similarly, the contribution of processing time for encoder networks varies, necessitating the use of a narrower encoder architecture contrary to the approach proposed by Kasai et al. (2020)."
        },
        {
            "heading": "4 Narrow Encoder and Shallow Decoder",
            "text": "Based on the findings presented in Section 3, we propose a structured pruning framework called NASH (Narrow encoder and Shallow decoder) that is specifically optimized for encoder-decoder LMs. Our approach focuses on enhancing inference speed by utilizing uniform layer selection in the decoder network, deviating from the gradual pruning technique commonly employed in encoder-only models. Additionally, we improve generation per-\nformance by applying gradual L0 regularization pruning specifically to the encoder network, inducing low sparsity instead of solely prioritizing inference speed improvement."
        },
        {
            "heading": "4.1 Shallow Decoder: Uniform Layer Selection for Decoder Networks",
            "text": "For a given number of selected layers ds, we can generate a sub-network of the decoder network with a set of selected layers as follows:\nLs = {\u230a L\u2212 1 ds \u2212 1 \u230b \u00b7 \u2113+ 1 \u2223\u2223\u2223\u2223\u2113 \u2208 {0, . . . , ds \u2212 1}} We match the hidden states of the sub-networks to those of unpruned decoder networks:\nLdech = \u2211\n\u2113\u2208{1,...,ds}\nMSE(H\u2113dec,s,H\n\u230a L\u22121 ds\u22121 \u230b \u00b7\u2113+1 dec,t ).\nWhile uniformly selecting layers work well on various domains such as knowledge distillation (Jiao et al., 2019; Shleifer and Rush, 2020) or structured pruning of encoder-only model (Hou et al., 2020), our work first proposes using uniform layer selection of decoder network for structured pruning of encoder-decoder LMs.\nThe key philosophy of our proposed module is twofold: (1) As shown in Finding 3.1, the number of layers in the decoder network is the main factor affecting inference speedup. (2) Uniform selection is proven to be an effective approach for selecting layers (Hou et al., 2020). To verify this second statement, we compare various candidates, including uniform selection, selection with lower layers, selection with higher layers, and the L0 regularization-based approach (Louizos et al., 2018). Through our empirical evaluation, we confirm that uniform selection is the best approach among these candidates (see Section 5.3 and Table 3 for details). Based on this philosophy, we construct shallow decoder pruning by selecting the layers using uniform selection."
        },
        {
            "heading": "4.2 Narrow Encoder: Gradual",
            "text": "L0-regularization with Low Sparsity\nAmong various structured pruning methods (Hou et al., 2020; Lagunas et al., 2021), we utilize the L0 regularization-based pruning method, which has shown the state-of-the-art performances in encoderonly language models (Wang et al., 2020c; Xia et al., 2022). The application of L0 regularization\nin practice is achieved by enforcing an equality constraint between the target sparsity and the current sparsity:\nR = \u03bb1(s\u0302\u2212 t) + \u03bb2(s\u0302\u2212 t)2,\nwhere \u03bb1, \u03bb2, s\u0302, and t denote the learnable Lagrange multipliers, current sparsity, and target sparsity, respectively. The detailed derivation of R is described in Appendix B. The current sparsity, s\u0302, is calculated as follows:\ns\u0302 = 4\nM \u00b7 dh \u00b7 L\u2211 i Nh\u2211 j z (i,j) head + 2 M \u00b7 L\u2211 i df\u2211 j z (i,j) int ,\nwhere M , L, Nh, and df indicate the number of model parameters, encoder layers, attention heads, and feed-forward layer dimensions, respectively.\nWe only conduct the pruning of individual attention heads and intermediate layer dimensions by introducing variables z(i)head and zint.\nMHA(Q,K,V) = Nh\u2211 i=1 z (i) headMHAi(Q,K,V)W (i) O ,\nFFN(X) = GELU(XW1) \u00b7 diag(zint) \u00b7W2.\nWe further use hidden states distillation by matching the hidden states of pruned and unpruned networks at the same layers as follows:\nLench = \u2211\n\u2113\u2208{1,...,L}\nMSE(H\u2113enc,s,H \u2113 enc,t).\nAs we demonstrated in Finding 3.2, structured pruning with low sparsity enables output quality enhancement rather than inference speedup gain. Motivated by this finding, unlike previous methods (Wang et al., 2020c; Xia et al., 2022) that mainly use L0 regularization to achieve high inference speedup, we use such L0 regularization to accomplish improvement of output quality.\n4.3 Training Loss Function of NASH\nWe combine hidden states distillation with prediction-layer distillation by using Kullback\u2013Leibler divergence (KLD) function.\nLpred = KLD (f(\u00b7)\u2225g(\u00b7)) ,\nwhere the f(\u00b7) and (\u00b7) are softmax outputs for the sub-network of pruned model and unpruned model,\nrespectively. Then, the total training objective for a pruned model is\nL = Lpred + \u03bbencLench + \u03bbdecLdech +R,\nwhere \u03bbenc and \u03bbdec are coefficients for controlling the contribution of hidden state distillation for the encoder and decoder network, respectively."
        },
        {
            "heading": "5 Experiments",
            "text": ""
        },
        {
            "heading": "5.1 Experimental Setup",
            "text": "Dataset. We evaluate our proposed method on various tasks using the versatility of encoderdecoder LMs. For abstractive question answering, we conduct experiments on TweetQA (Xiong et al.,\n2019). For the text summarization task, we experiment on XSum (Narayan et al., 2018), SAMSum (Gliwa et al., 2019), and CNN/DailyMail (See et al., 2017). We evaluate the output quality using METEOR (Banerjee and Lavie, 2005) for abstractive question answering and ROUGE (Lin, 2004) for the summarization tasks. We conduct experiments on multi-task scenario that consists of SAMSum, TweetQA, and five tasks from GLUE (Wang et al., 2018) and SuperGLUE (Wang et al., 2019) benchmarks. The detailed explanations for the datasets used are described in Appendix C.\nImplementation. First, we fine-tune a model and perform uniform layer selection on the decoder network of the fine-tuned model to generate a subnetwork model. Subsequently, we further train the sub-network model with the pruning objective, utilizing a scheduler to gradually increase the sparsity until reaching the desired target value. In our experiments, we calculate sparsity by dividing the number of pruned parameters (excluding the embedding) by the size of the full model. Following the approach of Xia et al. (2022), we continue fine-tuning the pruned model until convergence. We set the target sparsity of the encoder networks as 30% for all experiments. The reported results are based on\nthe validation sets of all datasets. Additionally, our models are implemented using Huggingface (Wolf et al., 2020) library."
        },
        {
            "heading": "5.2 Main Results",
            "text": "Standard Fine-tuning. Table 1 and Figure 5 summarize that the proposed method outperforms CoFi-T5, 6-layer T5-Small,(Raffel et al., 2020), and 4-layer T5-mini,(Tay et al., 2021) in terms of output quality and inference speedup. Our results consistently demonstrate the superiority of NASH with three decoder layers over the baselines in both performance metrics, although there is a trade-off between output quality and inference speedup with our method. Moreover, our method is particularly effective in improving inference speed, especially for tasks involving longer sequence outputs, such as SAMSum or CNN/DailyMail. However, CoFiT5 falls short in achieving comparable speedups to T5-Small while maintaining the same output quality. Figure 6 illustrates the pruned model structure trained on SAMSum, following the method described in Table 1 to provide a deeper understanding of the results. Despite CoFi-T5 removing more modules in the encoder network compared to NASH, it cannot remove as many layers in the decoder layer as NASH does.\nMulti-task Learning. We conducted an analysis to observe how performance trend change with varying numbers of tasks. As depicted in Figure 7, both fine-tuned T5-Base and our algo-\nrithm (NASH) exhibit fluctuations in accuracy according to the number of tasks. However, it is worth noting that our proposed method demonstrates significantly less variation in generation performance when compared to T5-Small. This robustness in multi-task learning is consistently observed across all datasets. Overall, NASH consistently outperforms T5-Small in terms of both generation performance and speedup, similar to standard fine-tuning. Through the results shown in complex scenarios, we verify that the generation performance of encoder-decoder models is robust to the number of decoder layers.\nInstruction Tuning. To verify versatility of proposed NASH, we consider the instruction-tuning scenario with the databricks-dolly-15k (Conover et al., 2023) dataset. By following Gu et al. (2023), we split the total dataset into 14k train samples and 1k evaluation samples. We evaluate the trained model (with 14k of train samples) on three instruction-following datasets to check the generalizability of the model across the different datasets: (1) 1k dolly evaluation; (2) Self-Instruct (Wang et al., 2023a); (3) Vicuna evaluation (Chiang et al., 2023). Similar to previous instances of task-specific fine-tuning and multi-task scenarios, our algorithm with three decoder layers consistently outperforms T5-Small across various metrics, including GPT4 evaluation, ROUGE-L, and speedup, as shown in Table 2. These results suggest that our proposed method is well-suited for developing general-\npurpose language models, a usage pattern widely adopted in recent large language models (Chung et al., 2022a; Tay et al., 2023)."
        },
        {
            "heading": "5.3 Ablation Studies",
            "text": "Different Layer Selection. To validate the effectiveness of uniform layer selection in shrinking the decoder network, we investigate other layer selection methods in a two-layer selection problem. We compare four selection methods: lower selection (first 2 layers), higher selection (last 2 layers), L0 regularization-based automatic selection (Louizos et al., 2018; Xia et al., 2022), and our uniform selection. The results presented in Table 3 demonstrate that uniform selection consistently outperforms the other manual selection methods. The performance margin becomes more pronounced in NLG tasks. Notably, we observe that automatic selection fails to achieve the target sparsity consistently across all tasks, except for BoolQ. This instability in automatic selection aligns with our preliminary findings discussed in Appendix A.\nDifferent Pruning on Encoder. To evaluate the effectiveness of our pruning strategy on the encoder network, we investigate the following strategies at different levels of sparsity: (1) without encoder pruning, (2) uniform layer selection (similar to the decoder network), and (3) the proposed L0 regularization approach. We prune the encoder network of the T5-Base, which has four decoder layers se-\nlected uniformly. The results presented in Table 4 clearly demonstrate that our chosen approach, with low sparsity, outperforms both the unpruned baseline and the uniform layer selection. We also observe that the advantage of this approach is only noticeable at low sparsity, as evidenced by the comparison between 30% and 60% sparsity.\nNASH on Different LMs. We also conducted a deeper model experiment using T5-Smallefficient,(Tay et al., 2021), which is a variant of T5-Small with up to four times more layers while maintaining the same configuration. This experiment aimed to determine the effectiveness of our method regardless of the model architecture. The results presented in Table 5 consistently demonstrate that NASH improves inference speed without significantly compromising the quality of generated outputs, regardless of the depth of the decoder networks. It is noteworthy that the acceleration compared to the original model increases as the number of decoder layers increases. Furthermore, NASH exhibits faster inference and higher output quality compared to CoFi-T5, which is consistent with the results presented in Table 1.\nComparison with Tao et al. (2023). We applied NASH to BART-Base (Lewis et al., 2020) using the CNN/DailyMail dataset, conducting a direct comparison with SIMPLE (Tao et al., 2023). SIMPLE introduced a structured pruning method for generative LMs, which is relevant to our work. Notably, NASH exhibits higher ROUGE-L scores than SIMPLE when both models are at 27% sparsity. Additionally, despite having larger parameters, NASH\noutperforms SIMPLE with 50% sparsity in terms of speedup. Our approach achieves more than three times the speedup, while SIMPLE reaches a maximum of 1.5 times on the GPU."
        },
        {
            "heading": "6 Related Works",
            "text": "Language Model Compression. With the advancement of NLP, LMs have grown in size, making it difficult to deploy them on edge devices and resulting in slower inference speed. As a result, there has been active research on language model compression which has three main approaches: quantization, knowledge distillation, pruning. Quantization (He et al., 2016; Alom et al., 2018; Zafrir et al., 2019; Shen et al., 2020; Yao et al., 2022) minimizes the storage requirements for weight values by reducing the number of bits needed to represent them. Knowledge distillation (Sanh et al., 2019; Jiao et al., 2019; Sun et al., 2019, 2020; Wang et al., 2020b,a) transfers the knowledge of a large-scale teacher model with high performance to a smaller-scale student model, enabling the student model to replicate the behavior of the teacher model. Pruning (Chen et al., 2020; Sanh et al., 2020; Kwon et al., 2022; Frantar and Alistarh, 2023) reduces the size of a model by removing unnecessary parts of large networks such as neurons, weights, or layers.\nPruning. Pruning can be categorized into two parts: (1) unstructured pruning and (2) structured pruning. In unstructured pruning (Chen et al., 2020; Prasanna et al., 2020), weights, which are connections between neurons, are removed from the network based on various criteria. However, this line of methods produces sparse weight matrices, requiring specific hardware support. On the other hand, structured pruning (Xia et al., 2022; Kwon et al., 2022; Kurtic et al., 2023), prunes away structures such as neurons, weight matrix blocks, or layers. Most previous works on structured pruning have focused on encoder-based models (Xia et al., 2022; Kwon et al., 2022; Kurtic et al., 2023), which remove attention heads, columns, and rows of weight matrices using different importance score metrics, including magnitudes or Hessians of weight matrices, and L0 loss. However, structured pruning on generative models has been significantly underinvestigated, with only a few available works (Lagunas et al., 2021; Yang et al., 2022; Santacroce et al., 2023). Lagunas et al. (2021) extended movement pruning (Sanh et al., 2020) into structured prun-\ning, but their method can only achieve up to 1.4\u00d7 speedup for encoder-decoder based BART (Lewis et al., 2020). Yang et al. (2022) released an opensource toolkit that combines structured pruning and vocabulary pruning for various pre-trained language models, but only vocabulary pruning is applicable to T5 and BART."
        },
        {
            "heading": "7 Conclusion",
            "text": "We propose NASH to address the lack of exploration in structured pruning of encoder-decoder LMs. To design a structured pruning method suitable for encoder-decoder models, we first examine the behavior of pruned models with different strategies, focusing on inference speed and generation performance. Our findings reveal that (1) the number of decoder network layers is the key factor in accelerating inference speed and (2) low sparsity pruning on the encoder network can enhance model performance. Based on these insights, we develop NASH, which constructs a narrow encoder and a shallow decoder network for encoder-decoder LMs through gradual L0 regularization pruning and uniform layer selection, respectively. We demonstrate the superiority of NASH in terms of speedup and output quality across various tasks. We strongly believe this work lays a strong foundation for further investigation into effective pruning approaches for encoder-decoder LM.\nLimitations\nAlthough we were unable to conduct research on unstructured pruning due to device limitations, collaboration with devices could facilitate performance enhancements. Furthermore, owing to the motivating analysis and algorithm construction of this paper, i.e., analysis of separate encoder and decoder networks, further exploration of a cooptimized method is necessary, and there is potential for improvement in this aspect."
        },
        {
            "heading": "Acknowledgment",
            "text": "This work was supported by the \u201cResearch on model compression algorithm for Large-scale Language Models\u201d project funded by KT (KT award B220002586, 90%) and Institute of Information & communications Technology Planning & Evaluation (IITP) grant funded by Korea government (MSIT) [No. 2019-0-00075, Artificial Intelligence Graduate School Program (KAIST), 10%]."
        },
        {
            "heading": "B Pruning with L0 Regularization",
            "text": "In this section, we give the details of the pruning with L0 regularization. Structured pruning through L0 regularization (Louizos et al., 2018) is proposed to construct the sparse neural network efficiently. In addition, this scheme using L0 regularization is widely applied to prune LMs (Xia et al., 2022; Wang et al., 2020c). With a given LM f(\u00b7; \u03b8) that is parameterized by \u03b8 = {\u03b8j}nj=1, we can define binary mask variable z = {zj}nj=1. Note that \u03b8j and zj denote a unit of weights (e.g., weights of attention heads, or column of an MLP layer) and mask variable corresponding to \u03b8j , respectively.\nIn this formulation, a pruned LM is written as f(\u00b7; \u03b8\u0303), where \u03b8\u0303 = \u03b8 \u2299 z and the pruning is a problem to find optimal mask variables and weights. In the L0 regularization-based pruning, these masks and weights are learned based on the following objective function:\nminEz\n[ 1\nD D\u2211 i=1 L(f(xi; \u03b8\u0303), yi) + \u03bb||\u03b8\u0303||0 ] In the objective function above, every mask variable zj is chosen based on the prior distribution q(zj |\u03c0j) = Bernoulli(\u03c0j). Considering the number of binary masks, n, the possible choices of z can be represented as 2n. The discrete feature of the mask and this tremendous amount of choices make mask optimization practically intractable.\nLouizos et al. (2018) mitigate this issue with a re-parameterization trick, enabling z to be differentiable and updates with the model parameter \u03b8. In detail, the masks z are defined as continuous\nvariables determined by min(1,max(0, s)), where continuous random variable s is sampled from the range of [0, 1]. Note that it is equivalent to sample u from the uniform distribution, U(0, 1) and calculate s as follows:\ns = sigmoid(logu\u2212 log(1\u2212 u) + \u03b1), s\u0304 = s\u00d7 (r \u2212 l) + l, z = min(1,max(0, s\u0304))\nwhere l and r are constant values that satisfy l < 0 and r > 0, and \u03b1 is a learnable parameter. From this formulation, the learning objective can be rewritten as:\nminEu\u223cU(0,1)\n[ 1\nD D\u2211 i=1 L(p(xi; \u03b8\u0303), yi) + \u03bb||\u03b8\u0303||0\n]\nThis process obtains z ={zj}nj=1 where every zj is in the range of [0, 1]. However, Wang et al. (2020c) observes that optimizing with those relaxed regularizations makes models converge to differentsize subnetworks depending on a learning rate and pruning schedule. To mitigate this problem, they suggest using a Lagrangian relaxation instead of the L0 regularizer \u03bb\u2225\u03b8\u0303\u22250 as follows:\nR = \u03bb1(s\u0302\u2212 t) + \u03bb2(s\u0302\u2212 t)2\nwhere \u03bb1 and \u03bb2 are learnable Lagrange multipliers. s\u0302 represents the current sparsity calculated by the masks z, while t represents the target sparsity. Motivated by these works, we also utilize the relaxed regularization term, R, for gradually structured pruning on the encoder network."
        },
        {
            "heading": "C Description of Datasets",
            "text": "Natural Language Generation Tasks. Since we study the structured pruning for encoder-decoder models, a sort of generative model, we conduct comprehensive experiments on the NLG tasks. The NLG datasets used in our study encompass\ntwo tasks: summarization and abstract question answering. We employed the XSum (Narayan et al., 2018), SAMSum (Gliwa et al., 2019), and CNN/DailyMail (See et al., 2017) datasets to assess the summarization capability of our proposed method. These datasets are widely used in evaluating the effectiveness of summarization techniques. Regarding abstractive question answering, we employed the TweetQA (Xiong et al., 2019) dataset to evaluate our method.\n\u2022 XSUM (Summarization): XSUM (Narayan et al., 2018) comprises articles sourced from BBC, along with corresponding single sentence summaries. More specifically, each article begins with an introductory sentence that serves as a summary. These summaries are professionally written and are usually provided by the article\u2019s author.\n\u2022 SAMSum (Summarization): SAMSum (Gliwa et al., 2019) consists of 16K messenger-like conversations annotated with a summary for providing a concise overview of the conversation\u2019s content in third person. The conversations encompass a variety of styles and registers, ranging from informal to semi-formal and formal. Additionally, they may include slang words, emoticons, and typographical errors.\n\u2022 CNN/DailyMail (Summarization): CNN / DailyMail (See et al., 2017) consists of over 300K English news articles that were originally designed for machine-reading and comprehension as well as abstractive question answering, but it now also supports extractive and abstractive summarization. In this work, we utilize the 3.0.0 version.\n\u2022 TweetQA (Abstract Question Answering): TweetQA (Xiong et al., 2019) is the first largescale dataset for question answering (QA) over\nNatural Language Understanding Tasks. We apply GLUE (Wang et al., 2018) and SuperGLUE benchmarks (Wang et al., 2019) for evaluating on NLU tasks. While these benchmarks consist of classification datasets, we generate the phrase related to the label instead of the class index. The detailed descriptions and labels of each task are described in Table 7.\n\u2022 GLUE: GLUE (Wang et al., 2018) is a collection of datasets for evaluating the performance\nof models across a diverse set of existing NLU tasks. By including tasks with limited training data, GLUE is designed to favor and encourage models that share general linguistic knowledge across tasks. In this work, we employ six tasks in GLUE benchmarks: CoLA, MRPC, STS-B, RTE, SST-2, and QNLI.\n\u2022 SuperGLUE: SuperGLUE (Wang et al., 2019) is a new benchmark styled after GLUE with a new set of more difficult NLU tasks. It incorporates improved resources to address the fact that performance on GLUE has surpassed the level of non-expert humans, thereby indicating the limited potential for further research progress. In this work, we adopt some of SuperGLUE tasks: CB, BoolQ, WiC, and COPA.\nInstruction-Tuning Tasks. We apply databricks-dolly-15k (Conover et al., 2023), Self-Instruct (Wang et al., 2023a), and Vicuna (Chiang et al., 2023) for evaluating on instruction-tuning tasks.\n\u2022 databricks-dolly-15k: databricks-dolly-15k (Conover et al., 2023) is an open-source dataset of instructionfollowing records generated by thousands of Databricks employees in several of the behavioral categories outlined in the InstructGPT (Ouyang et al., 2022), including brainstorming, classification, closed QA, generation, information extraction, open QA, and summarization.\n\u2022 Self-Instrcut: The authors of SelfInstruct (Wang et al., 2023a) have introduced a dataset comprising 52,000 instructions matched with 82,000 instance inputs and outputs. This dataset serves as a resource for fine-tuning language models to improve their adherence to instructions. Additionally, they\u2019ve provided 252 expert-created tasks and instructions designed for user-centric applications, which are used in the human evaluation section of their research. Furthermore, the Self-Instruct dataset includes 50,000 examples from the P3 and Super Natural Instructions datasets for the purpose of facilitating comparisons with existing public datasets.\n\u2022 Vicuna: Vicuna (Chiang et al., 2023) utilized approximately 70,000 multi-round conversations between users and ChatGPT collected\nfrom the ShareGPT website (Geng et al., 2023), which allows sharing of ChatGPT dialogues, as a dataset for fine-tuning. In this work, we utilize 80 challenging questions used in the Vicuna evaluation."
        },
        {
            "heading": "D Hyperparameters",
            "text": "In this section, we describe the hyperparameter setup of experiments. We report the hyperparameters that we utilized in Table 8 and 9 for NLG and NLU tasks, respectively. We use different hyperparameter sets for small NLG datasets (TweetQA, SAMSum) and large NLG (XSum, CNN/DailyMail) datasets. Similarly, for NLU tasks, we use different hyperparameters depending on the dataset size. For the small-size NLU datasets (CB, COPA) that the number of samples is smaller than 1,000, we use the hyperparameters (small) described in Table 9. We especially train our model for 150 epochs because the data size is too small to learn the weights for the L0 regularization. We use the hyperparameters (middle) described in Table 9 for the middlesize NLU datasets (MRPC, RTE, STS-B, CoLA, WIC, BOOLQ) that the number of samples is more than 1, 000 but less than 10, 000. We use the hyperparameters (high) described in Table 9 for the large-size NLU datasets (MNLI, QQP, QNLI, SST2) that the NLU datasets whose size is larger than 10, 000.\nE Instruction-tuning Details\nIn the evaluation process of GPT-4, feedback is solicited by instructing the model to compare its generated responses with the authentic, reference answers and assign a numerical score ranging from 1 to 10 to each response. Drawing upon the methodology outlined by Gu et al. (2023), we calculate the ratio between the cumulative scores assigned to the model\u2019s responses and those of the ground truth answers. Further details regarding the specific prompt employed for this evaluation are presented in Figure 9."
        },
        {
            "heading": "F Speedup Evaluation Metric",
            "text": "To measure the inference speed, we conducted inference predictions for each dataset and examined configuration using the PyTorch (Paszke et al., 2019) compiled function. This was done on a single server equipped with a NVIDIA GeForce RTX\n3090 GPU and an AMD EPYC 7502 32-Core Processor CPU. For each inference prediction, we utilized a batch size of 32. Additionally, we generated output sequences using a beam size of 4. The time taken for the measurements included all decoding steps until completion."
        },
        {
            "heading": "G Additional Experiments",
            "text": "G.1 Comparison with Efficient-T5\nWe compare our algorithm, NASH, with models designed to have a shallow decoder depth originally from the pre-training stage, as proposed by Tay et al. (2021). In our evaluation, we examine the performance of our method on two tasks, namely TweetQA and SAMSum, using 2, 4, 6, and 8 decoder layers. As shown in Table 10, NASH demonstrates superior performance in most cases. This result is noteworthy as our method can construct a small yet effective model without requiring any costs to make the small pre-trained language model.\nG.2 Results on NLU Tasks\nWe also compare NASH to the baseline methods on the GLUE and SuperGLUE benchmarks, which are focused on NLU tasks. Since these tasks involve relatively longer input sequences and shorter\noutput sequences compared to NLG tasks, our proposed method exhibited less effectiveness. However, NASH still demonstrates superiority over the baselines, as depicted in Figure 10. It is important to note that the performance of our method remains robust across different compression rates. We also provide detailed performance results of our proposed method for the full GLUE and SuperGLUE benchmarks in the Appendix G.2 in Table 11. The results demonstrate the effectiveness of our proposed NASH method in achieving high output quality while significantly improving inference speed. The superior performance of NASH across both GLUE and SuperGLUE benchmarks highlights its potential as an efficient acceleration method for NLU tasks as well as for NLG tasks."
        }
    ],
    "title": "NASH: A Simple Unified Framework of Structured Pruning for Accelerating Encoder-Decoder Language Models",
    "year": 2023
}