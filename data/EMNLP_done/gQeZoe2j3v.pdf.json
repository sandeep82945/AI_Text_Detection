{
    "abstractText": "Video Question Answering (VideoQA) aims to answer questions about the visual content of a video. Current methods mainly focus on improving joint representations of video and text. However, these methods pay little attention to the fine-grained semantic interaction between video and text. In this paper, we propose Mulan: a Multi-Level Alignment Model for Video Question Answering, which establishes alignment between visual and textual modalities at the object-level, frame-level, and video-level. Specifically, for object-level alignment, we propose a mask-guided visual feature encoding method and a visual-guided text description method to learn fine-grained spatial information. For frame-level alignment, we introduce the use of visual features from individual frames, combined with a caption generator, to learn overall spatial information within the scene. For video-level alignment, we propose an expandable ordinal prompt for textual descriptions, combined with visual features, to learn temporal information. Experimental results show that our method outperforms the state-of-the-art methods, even when utilizing the smallest amount of extra visual-language pre-training data and a reduced number of trainable parameters. Our code is publicly available at https://github.com/fuyu1998/Mulan.",
    "authors": [
        {
            "affiliations": [],
            "name": "Yu Fu"
        },
        {
            "affiliations": [],
            "name": "Cong Cao"
        },
        {
            "affiliations": [],
            "name": "Yuling Yang"
        },
        {
            "affiliations": [],
            "name": "Yuhai Lu"
        },
        {
            "affiliations": [],
            "name": "Fangfang Yuan"
        },
        {
            "affiliations": [],
            "name": "Dakui Wang"
        },
        {
            "affiliations": [],
            "name": "Yanbing Liu"
        }
    ],
    "id": "SP:3438adc84f491f0a10b8bde51fb5fd4492aed311",
    "references": [
        {
            "authors": [
                "Marianne Monteiro",
                "Jacob L Menick",
                "Sebastian Borgeaud",
                "Andy Brock",
                "Aida Nematzadeh",
                "Sahand Sharifzadeh",
                "Miko\u0142 aj Bi\u0144kowski",
                "Ricardo Barreira",
                "Oriol Vinyals",
                "Andrew Zisserman",
                "Kar\u00e9n Simonyan"
            ],
            "title": "Flamingo: a visual language model",
            "year": 2022
        },
        {
            "authors": [
                "Stanislaw Antol",
                "Aishwarya Agrawal",
                "Jiasen Lu",
                "Margaret Mitchell",
                "Dhruv Batra",
                "C. Lawrence Zitnick",
                "Devi Parikh."
            ],
            "title": "VQA: Visual question answering",
            "venue": "International Conference on Computer Vision (ICCV).",
            "year": 2015
        },
        {
            "authors": [
                "Max Bain",
                "Arsha Nagrani",
                "G\u00fcl Varol",
                "Andrew Zisserman."
            ],
            "title": "Frozen in Time: A joint video and image encoder for end-to-end retrieval",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 1728\u20131738.",
            "year": 2021
        },
        {
            "authors": [
                "Zhaowei Cai",
                "Nuno Vasconcelos."
            ],
            "title": "Cascade R-CNN: High quality object detection and instance segmentation",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 43(5):1483\u20131498.",
            "year": 2021
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
            "year": 2019
        },
        {
            "authors": [
                "Constantin Eichenberg",
                "Sidney Black",
                "Samuel Weinbach",
                "Letitia Parcalabescu",
                "Anette Frank."
            ],
            "title": "MAGMA \u2013 multimodal augmentation of generative models through adapter-based finetuning",
            "venue": "Findings of the Association for Computational Linguistics:",
            "year": 2022
        },
        {
            "authors": [
                "Han Fang",
                "Pengfei Xiong",
                "Luhui Xu",
                "Yu Chen."
            ],
            "title": "Clip2video: Mastering video-text retrieval via image clip",
            "venue": "arXiv preprint arXiv:2106.11097.",
            "year": 2021
        },
        {
            "authors": [
                "Christoph Feichtenhofer",
                "Haoqi Fan",
                "Jitendra Malik",
                "Kaiming He."
            ],
            "title": "Slowfast networks for video recognition",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV).",
            "year": 2019
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun."
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR).",
            "year": 2016
        },
        {
            "authors": [
                "Pengcheng He",
                "Xiaodong Liu",
                "Jianfeng Gao",
                "Weizhu Chen."
            ],
            "title": "Deberta: Decoding-enhanced bert with disentangled attention",
            "venue": "International Conference on Learning Representations.",
            "year": 2021
        },
        {
            "authors": [
                "Matthew Honnibal",
                "Mark Johnson."
            ],
            "title": "An improved non-monotonic transition system for dependency parsing",
            "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1373\u20131378, Lisbon, Portugal. As-",
            "year": 2015
        },
        {
            "authors": [
                "Neil Houlsby",
                "Andrei Giurgiu",
                "Stanislaw Jastrzebski",
                "Bruna Morrone",
                "Quentin De Laroussilhe",
                "Andrea Gesmundo",
                "Mona Attariyan",
                "Sylvain Gelly."
            ],
            "title": "Parameter-efficient transfer learning for NLP",
            "venue": "Proceedings of the 36th International Conference",
            "year": 2019
        },
        {
            "authors": [
                "Edward J Hu",
                "yelong shen",
                "Phillip Wallis",
                "Zeyuan AllenZhu",
                "Yuanzhi Li",
                "Shean Wang",
                "Lu Wang",
                "Weizhu Chen"
            ],
            "title": "LoRA: Low-rank adaptation of large language models",
            "venue": "In International Conference on Learning Representations",
            "year": 2022
        },
        {
            "authors": [
                "Yunseok Jang",
                "Yale Song",
                "Youngjae Yu",
                "Youngjin Kim",
                "Gunhee Kim."
            ],
            "title": "TGIF-QA: Toward spatiotemporal reasoning in visual question answering",
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR).",
            "year": 2017
        },
        {
            "authors": [
                "Chao Jia",
                "Yinfei Yang",
                "Ye Xia",
                "Yi-Ting Chen",
                "Zarana Parekh",
                "Hieu Pham",
                "Quoc Le",
                "Yun-Hsuan Sung",
                "Zhen Li",
                "Tom Duerig."
            ],
            "title": "Scaling up visual and vision-language representation learning with noisy text supervision",
            "venue": "Proceedings of the 38th Inter-",
            "year": 2021
        },
        {
            "authors": [
                "Diederik P. Kingma",
                "Jimmy Ba."
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings.",
            "year": 2015
        },
        {
            "authors": [
                "Alina Kuznetsova",
                "Hassan Rom",
                "Neil Alldrin",
                "Jasper Uijlings",
                "Ivan Krasin",
                "Jordi Pont-Tuset",
                "Shahab Kamali",
                "Stefan Popov",
                "Matteo Malloci",
                "Alexander Kolesnikov",
                "Tom Duerig",
                "Vittorio Ferrari."
            ],
            "title": "The Open Images Dataset V4",
            "venue": "International Journal of Com-",
            "year": 2020
        },
        {
            "authors": [
                "Jie Lei",
                "Linjie Li",
                "Luowei Zhou",
                "Zhe Gan",
                "Tamara L. Berg",
                "Mohit Bansal",
                "Jingjing Liu."
            ],
            "title": "Less Is More: Clipbert for video-and-language learning via sparse sampling",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recog-",
            "year": 2021
        },
        {
            "authors": [
                "Jie Lei",
                "Licheng Yu",
                "Mohit Bansal",
                "Tamara Berg."
            ],
            "title": "TVQA: Localized, compositional video question answering",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1369\u20131379, Brussels, Belgium.",
            "year": 2018
        },
        {
            "authors": [
                "Dongxu Li",
                "Junnan Li",
                "Hongdong Li",
                "Juan Carlos Niebles",
                "Steven C.H. Hoi."
            ],
            "title": "Align and Prompt: Video-and-language pre-training with entity prompts",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "year": 2022
        },
        {
            "authors": [
                "Junnan Li",
                "Dongxu Li",
                "Silvio Savarese",
                "Steven Hoi."
            ],
            "title": "BLIP-2: Bootstrapping language-image pretraining with frozen image encoders and large language models",
            "venue": "arXiv preprint arXiv:2301.12597.",
            "year": 2023
        },
        {
            "authors": [
                "Junnan Li",
                "Dongxu Li",
                "Caiming Xiong",
                "Steven Hoi"
            ],
            "title": "2022b. BLIP: Bootstrapping language-image pretraining for unified vision-language understanding",
            "year": 2022
        },
        {
            "authors": [
                "Linjie Li",
                "Yen-Chun Chen",
                "Yu Cheng",
                "Zhe Gan",
                "Licheng Yu",
                "Jingjing Liu."
            ],
            "title": "HERO: Hierarchical encoder for Video+Language omni-representation pretraining",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing",
            "year": 2020
        },
        {
            "authors": [
                "Linjie Li",
                "Zhe Gan",
                "Kevin Lin",
                "Chung-Ching Lin",
                "Zicheng Liu",
                "Ce Liu",
                "Lijuan Wang."
            ],
            "title": "LAVENDER: Unifying video-language understanding as masked language modeling",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and",
            "year": 2023
        },
        {
            "authors": [
                "Manling Li",
                "Ruochen Xu",
                "Shuohang Wang",
                "Luowei Zhou",
                "Xudong Lin",
                "Chenguang Zhu",
                "Michael Zeng",
                "Heng Ji",
                "Shih-Fu Chang."
            ],
            "title": "CLIP-Event: Connecting text and images with event structures",
            "venue": "Proceedings of the IEEE/CVF Conference on Com-",
            "year": 2022
        },
        {
            "authors": [
                "Xudong Lin",
                "Fabio Petroni",
                "Gedas Bertasius",
                "Marcus Rohrbach",
                "Shih-Fu Chang",
                "Lorenzo Torresani."
            ],
            "title": "Learning to recognize procedural activities with distant supervision",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pat-",
            "year": 2022
        },
        {
            "authors": [
                "Xudong Lin",
                "Simran Tiwari",
                "Shiyuan Huang",
                "Manling Li",
                "Mike Zheng Shou",
                "Heng Ji",
                "Shih-Fu Chang."
            ],
            "title": "Towards fast adaptation of pretrained contrastive models for multi-channel video-language retrieval",
            "venue": "Proceedings of the IEEE/CVF Conference",
            "year": 2023
        },
        {
            "authors": [
                "Huaishao Luo",
                "Lei Ji",
                "Ming Zhong",
                "Yang Chen",
                "Wen Lei",
                "Nan Duan",
                "Tianrui Li."
            ],
            "title": "CLIP4Clip: An empirical study of clip for end to end video clip retrieval and captioning",
            "venue": "Neurocomputing, 508:293\u2013 304.",
            "year": 2022
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark",
                "Gretchen Krueger",
                "Ilya Sutskever"
            ],
            "title": "Learning transferable visual models from natural language",
            "year": 2021
        },
        {
            "authors": [
                "Jianfeng Wang",
                "Zhengyuan Yang",
                "Xiaowei Hu",
                "Linjie Li",
                "Kevin Lin",
                "Zhe Gan",
                "Zicheng Liu",
                "Ce Liu",
                "Lijuan Wang."
            ],
            "title": "GIT: A generative image-to-text transformer for vision and language",
            "venue": "Transactions on Machine Learning Research.",
            "year": 2022
        },
        {
            "authors": [
                "Jinpeng Wang",
                "Yixiao Ge",
                "Guanyu Cai",
                "Rui Yan",
                "Xudong Lin",
                "Ying Shan",
                "Xiaohu Qie",
                "Mike Zheng Shou."
            ],
            "title": "Object-aware videolanguage pre-training for retrieval",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and",
            "year": 2022
        },
        {
            "authors": [
                "Jinpeng Wang",
                "Yixiao Ge",
                "Rui Yan",
                "Yuying Ge",
                "Kevin Qinghong Lin",
                "Satoshi Tsutsui",
                "Xudong Lin",
                "Guanyu Cai",
                "Jianping Wu",
                "Ying Shan",
                "Xiaohu Qie",
                "Mike Zheng Shou."
            ],
            "title": "All in One: Exploring unified video-language pre-training",
            "venue": "Proceedings",
            "year": 2023
        },
        {
            "authors": [
                "Wenhai Wang",
                "Jifeng Dai",
                "Zhe Chen",
                "Zhenhang Huang",
                "Zhiqi Li",
                "Xizhou Zhu",
                "Xiaowei Hu",
                "Tong Lu",
                "Lewei Lu",
                "Hongsheng Li"
            ],
            "title": "2022c. InternImage: Exploring large-scale vision foundation models with deformable convolutions",
            "year": 2022
        },
        {
            "authors": [
                "Xudong Wang",
                "Rohit Girdhar",
                "Stella X Yu",
                "Ishan Misra."
            ],
            "title": "Cut and learn for unsupervised object detection and instance segmentation",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3124\u20133134.",
            "year": 2023
        },
        {
            "authors": [
                "Yi Wang",
                "Kunchang Li",
                "Yizhuo Li",
                "Yinan He",
                "Bingkun Huang",
                "Zhiyu Zhao",
                "Hongjie Zhang",
                "Jilan Xu",
                "Yi Liu",
                "Zun Wang",
                "Sen Xing",
                "Guo Chen",
                "Junting Pan",
                "Jiashuo Yu",
                "Yali Wang",
                "Limin Wang",
                "Yu Qiao"
            ],
            "title": "InternVideo: General video foundation",
            "year": 2022
        },
        {
            "authors": [
                "Zhenhailong Wang",
                "Manling Li",
                "Ruochen Xu",
                "Luowei Zhou",
                "Jie Lei",
                "Xudong Lin",
                "Shuohang Wang",
                "Ziyi Yang",
                "Chenguang Zhu",
                "Derek Hoiem",
                "Shih-Fu Chang",
                "Mohit Bansal",
                "Heng Ji"
            ],
            "title": "Language models with image descriptors are strong few-shot",
            "year": 2022
        },
        {
            "authors": [
                "Dejing Xu",
                "Zhou Zhao",
                "Jun Xiao",
                "Fei Wu",
                "Hanwang Zhang",
                "Xiangnan He",
                "Yueting Zhuang."
            ],
            "title": "Video question answering via gradually refined attention over appearance and motion",
            "venue": "Proceedings of the 25th ACM International Conference on Multime-",
            "year": 2017
        },
        {
            "authors": [
                "Haiyang Xu",
                "Qinghao Ye",
                "Ming Yan",
                "Yaya Shi",
                "Jiabo Ye",
                "Yuanhong Xu",
                "Chenliang Li",
                "Bin Bi",
                "Qi Qian",
                "Wei Wang"
            ],
            "title": "mplug-2: A modularized multimodal foundation model across text, image and video. arXiv preprint arXiv:2302.00402",
            "year": 2023
        },
        {
            "authors": [
                "Shen Yan",
                "Tao Zhu",
                "Zirui Wang",
                "Yuan Cao",
                "Mi Zhang",
                "Soham Ghosh",
                "Yonghui Wu",
                "Jiahui Yu."
            ],
            "title": "Video-Text Modeling with Zero-Shot Transfer from Contrastive Captioners",
            "venue": "arXiv preprint arXiv:2212.04979.",
            "year": 2022
        },
        {
            "authors": [
                "Antoine Yang",
                "Antoine Miech",
                "Josef Sivic",
                "Ivan Laptev",
                "Cordelia Schmid."
            ],
            "title": "Just Ask: Learning to answer questions from millions of narrated videos",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 1686\u20131697.",
            "year": 2021
        },
        {
            "authors": [
                "Antoine Yang",
                "Antoine Miech",
                "Josef Sivic",
                "Ivan Laptev",
                "Cordelia Schmid."
            ],
            "title": "Zero-shot video question answering via frozen bidirectional language models",
            "venue": "Advances in Neural Information Processing Systems.",
            "year": 2022
        },
        {
            "authors": [
                "Zhengyuan Yang",
                "Zhe Gan",
                "Jianfeng Wang",
                "Xiaowei Hu",
                "Yumao Lu",
                "Zicheng Liu",
                "Lijuan Wang."
            ],
            "title": "An empirical study of GPT-3 for few-shot knowledgebased VQA",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, 36(3):3081\u20133089.",
            "year": 2022
        },
        {
            "authors": [
                "Qinghao Ye",
                "Guohai Xu",
                "Ming Yan",
                "Haiyang Xu",
                "Qi Qian",
                "Ji Zhang",
                "Fei Huang."
            ],
            "title": "HiTeA: Hierarchical temporal-aware video-language pretraining",
            "venue": "arXiv preprint arXiv:2212.14546.",
            "year": 2022
        },
        {
            "authors": [
                "Chun-Hsiao Yeh",
                "Bryan Russell",
                "Josef Sivic",
                "Fabian Caba Heilbron",
                "Simon Jenni."
            ],
            "title": "Meta-personalizing vision-language models to find named instances in video",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and",
            "year": 2023
        },
        {
            "authors": [
                "Rowan Zellers",
                "Ximing Lu",
                "Jack Hessel",
                "Youngjae Yu",
                "Jae Sung Park",
                "Jize Cao",
                "Ali Farhadi",
                "Yejin Choi."
            ],
            "title": "MERLOT: Multimodal neural script knowledge models",
            "venue": "Advances in Neural Information Processing Systems, volume 34, pages 23634\u201323651.",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Video Question Answering (VideoQA) is a task that focuses on answering questions related to the visual content of a video. It serves as a representative task that showcases the fusion of visual and linguistic modalities. It demands the ability to comprehend and integrate data from both modalities to learn complex spatiotemporal information.\nGiven the videos are a continuous sequence of images, the mainstream models (Li et al., 2020, 2022c; Yang et al., 2021, 2022a; Zellers et al., 2021;\n\u2217Corresponding authors."
        },
        {
            "heading": "Video-Level Alignment",
            "text": ""
        },
        {
            "heading": "Frame-Level Alignment",
            "text": ""
        },
        {
            "heading": "Object-Level Alignment",
            "text": ""
        },
        {
            "heading": "Video-Level Alignment",
            "text": "Xu et al., 2023) commonly adopt a two-step approach to establish a joint representation of video and text. First, an image-language model (He et al., 2016; Radford et al., 2021) is utilized to extract visual and textual features from the video frames. Then, alignment between the visual and textual modalities is established based on the extracted visual and textual features. However, existing methods align video and text globally to learn semantic correlations, disregarding the fine-grained interaction between local salient information in the video and important textual descriptions. Furthermore,\nthe majority of subtitles in the dataset are brief, resulting in a lack of detailed textual descriptions for significant video content, which further undermines the effectiveness of semantic alignment. Therefore, relying solely on global alignment is insufficient to address the semantic gap issue.\nTo address the above issues, we propose Mulan: a Multi-Level Alignment Model for Video Question Answering. Specifically, we establish alignment between visual and textual modalities at the video-level, frame-level, and object-level, as shown in Figure 1. Our method can be divided into three stages:\n(1) Stage one involves the process of multi-level visual features generation. The image encoding of all sampled frames is regarded as video-level visual features. The image encoding of an individual frame is regarded as frame-level visual features. For object-level visual features, we have introduced a mask-guided object-level visual feature encoding method.\n(2) Stage two involves the process of multi-level textual description generation. The original description of the video is regarded as video-level textual description. For frame-level textual descriptions, we have introduced the use of a caption generator. For object-level textual descriptions, we have proposed a vision-guided generative approach.\n(3) Stage three involves the process of multilevel alignment and training. Specifically, we initially convert visual features into inputs for a language model and design prompts to concurrently establish alignment at the video-level, frame-level, and object-level. Finally, we employ a masked language modeling objective for training the language model, enabling it to acclimatize to visual input and establish multi-level alignment.\nOur contributions can be summarised as follows: (1) We introduce a multi-level visual-language alignment method for video question answering. To the best of our knowledge, this is the first work in the field of video question answering that explores multi-level visual-language alignment.\n(2) We give a clear and unified spatiotemporal information learning framework. At the object-level and the frame-level, the method can learn spatial information, while acquiring temporal information at video-level.\n(3) The experimental results illustrate that our method surpasses state-of-the-art baselines in video question answering tasks, even when using a mini-\nmal amount of visual-language extra pre-training data (Figure 2), and a reduced number of trainable parameters. Moreover, as the size of the pretraining dataset increases, the performance of our method can be further improved."
        },
        {
            "heading": "2 Related work",
            "text": ""
        },
        {
            "heading": "2.1 Pre-Trained Image-Language Models",
            "text": "In the realm of visual and language fusion methodologies, exploratory methods, as exemplified by the multi-stream approach(Lei et al., 2018), entail the utilization of a unified encoder to merge various information streams. This method facilitates the utilization of visual and language information at diverse levels of granularity and offers straightforward scalability. However, information of the same granularity is merged into distinct streams before alignment, resulting in a relatively poor semantic consistency among these streams.\nIn recent years, contrastive learning has been substantiated as an efficacious approach for acquiring cross-modal joint representations. The imagelanguage pre-trained model, based on contrastive learning, aims to establish a mapping between global image features and global text features in a shared space (Radford et al., 2021; Li et al., 2022c; Lin et al., 2022; Jia et al., 2021; Wang et al., 2022b). These model considers aligned image-text pairs as positive samples and unaligned pairs as negative samples to optimize visual-language alignment.\nDue to the requirement of a substantial amount of training data for video-language models to perform well, recent research has focused on transferring image-language models to video-text tasks (Yang et al., 2022a; Li et al., 2023b; Yeh et al., 2023). This is because there is a strong correlation\nbetween images and videos. Sparse sampling offers an effective approach to video representation (Li et al., 2022a; Lei et al., 2021), contrasting with the use of 3D dense features (Feichtenhofer et al., 2019). It facilitates the more efficient utilization of pre-trained image-language models within the realm of video processing tasks (Wang et al., 2022e; Fang et al., 2021; Luo et al., 2022).\nOur method continues along this line of methodology by ingeniously constructing a bridge from image-language to video-language through meticulous design, thereby facilitating the transfer of knowledge."
        },
        {
            "heading": "2.2 Language Models for Visual-Language Alignment",
            "text": "Some works focus on converting visual information entirely into symbolized textual information (Yang et al., 2022b; Wang et al., 2022e; Lin et al., 2023). Some other works use raw textual descriptions of videos, adopt the approach of freezing the weights of pre-trained language models, and integrate visual information to address tasks that involve both visual and language processing (Alayrac et al., 2022; Eichenberg et al., 2022; Yang et al., 2022a). However, the first category of methods disregards visual information, particularly fine-grained visual details. Conversely, the second category of methods lacks detailed textual descriptions and only achieves alignment at the video level.\nIn contrast, our method considers multi-level visual information and goes beyond by constructing semantic representations with multi-level visual-language alignment. In addition, we use lightweight Adapter layers (Houlsby et al., 2019; Hu et al., 2022) and frozen the language model. Our model can be easily applied to different types of language models, thereby facilitating practical application and deployment."
        },
        {
            "heading": "3 Method",
            "text": ""
        },
        {
            "heading": "3.1 Overall Framework",
            "text": "Our method can be divided into three stages, as illustrated in Figure 3: (1) multi-level visual feature generation, (2) multi-level textual description generation, (3) multi-level alignment and training.\nFirstly, for a given sampled frame, an image encoder is applied to extract global frame visual features. Then, a mask generator is utilized to create masks for the objects within the frame, and the same image encoder is applied to obtain local frame\nvisual features. Global frame visual features provide a comprehensive representation of the overall spatial information, while local frame visual features accurately capture specific object details such as position and attributes. It is worth noting that in this process, a mask generator is employed instead of an object detector. This choice is motivated by the fact that the output bounding boxes from an object detector are often not specific enough and can also struggle to accurately recognize objects in an open-vocabulary setting. Furthermore, due to the fact that both local frame visual features and global frame visual features are derived via a unified image encoder, it allows for the coexistence of local frame visual features and global frame visual features within the same semantic space.\nNext, for global frame visual features, frame captions are generated using the image-language model BLIP (Li et al., 2022b). From these frame captions, the part-of-speech tagging tool provided by spaCy (Honnibal and Johnson, 2015) is employed to extract noun phrases, thus establishing a dynamic vocabulary tailored to the frame. The dynamic vocabulary terms are subsequently amalgamated with predefined static vocabulary, thereby creating the vocabulary input for the object filter. The object filter is then utilized to extract global and local frame objects from the respective global and local frame visual features. It is worth noting that in this process, due to the differences in the information contained in global and local frame visual features, the extracted global and local frame objects may also differ.\nFinally, the global and local frame visual features, in conjunction with global and local frame object descriptions and global frame captions, serve as the input to a language model equipped with the adapter. Within the information fusion process, alignment between frame-level and object-level has been established. Simultaneously, temporal information is integrated by implementing an expandable ordinal prompt approach, thereby enabling the establishment of video-level alignment."
        },
        {
            "heading": "3.2 Multi-Level Visual Features Generation",
            "text": "To capture multi-level visual information, it is crucial to understand both the relationships between objects within video frames, which are represented as global frame visual features, and the local semantic information of the objects, which are represented as local frame visual features.\nGlobal Frame Visual Features. The video is represented as a sequence of frames f = {fi}T1 obtained through uniform and sparse sampling, where T is the total number of sampled frames. Each frame fi is individually encoded using the image encoder \u03d5CLIP to generate global frame visual features vG:\nvGi = \u03d5CLIP(fi) \u2208 RDv (1)\nvG = {vGi }T1 \u2208 RT\u00d7Dv (2)\nwhere Dv is the dimension of the visual features. As vG represents the visual features of the original frames, it contains the overall spatial information of those frames. Therefore, we consider vG to represent frame-level visual information. During the experimental process, the image encoder is frozen, meaning that its weights are not updated.\nLocal Frame Visual Features. Our goal is to establish multi-level visual-language alignment.\nGiven the effectiveness of contrastive learningbased image-language models in learning shared representations of images and text in a common space, we employ CLIP (Radford et al., 2021), a contrastive learning-based model, as the image encoder. However, due to its focus on capturing global information of images, CLIP is not wellsuited for directly capturing local details. To address this issue, we leverage the existing unsupervised mask generator CutLER (Wang et al., 2023b) to guide the image encoder CLIP in generating local frame visual features.\nTo obtain local frame visual features, we first utilize the mask generator CutLER \u03d5CutLER to generate a set of image masks mi for each frame fi. We refer to the collection of mask sets for each frame as m:\nmi = \u03d5CutLER(fi) (3)\nm = {mi}T1 (4)\nThen, we apply cropping and masking operations to the images and feed them into the image encoder \u03d5CLIP to obtain the local frame visual features vL:\nvLi = \u03d5CLIP(Tcrop(fi \u2299mi)) \u2208 RNi\u00d7Dv (5)\nvL = {vLi }T1 \u2208 R( \u2211T i=1 Ni)\u00d7Dv (6)\nwhere Tcrop(\u00b7) denotes the operations of cropping and masking, and \u2299 is the Hadamard product operation. We assume that the number of masks for the i-th frame is Ni.\nDue to the removal of object-irrelevant regions in vL, it focuses solely on the target objects themselves. Therefore, we consider vL to represent object-level visual information. During the experimental process, the mask generator is also frozen."
        },
        {
            "heading": "3.3 Multi-Level Textual Description Generation",
            "text": "To capture multi-level textual information, it is crucial to utilize both the text that describes the relationships between objects, represented by the global frame captions, and the detailed information about the objects, represented by the descriptions of global and local frame objects.\nGlobal Frame Captions. To establish textual connections among objects in the frame and obtain an overall description of the frame\u2019s information, we utilize the image-language model BLIP (Li et al., 2022b) to generate global frame captions c = {ci}T1 . We consider c to represent frame-level textual information.\nGlobal and Local Frame Object Descriptions. Global frame captions alone may not capture the fine-grained details of objects. Therefore, constructing textual descriptions only at the frame level is insufficient. To address this issue, we further utilize global and local frame visual features to construct global and local object descriptions.\nFirstly, we construct a predefined static vocabulary V S , which includes a set of candidate object names and attributes. However, the predefined static vocabulary cannot cover all objects and attributes. To address this issue, we utilize spaCy to extract noun phrases from the global frame captions c, creating a dynamic vocabulary V D. The final vocabulary V is defined as follows:\nV = V S \u222a V D (7) Next, we utilize the text encoder \u03d5CLIP-text in the CLIP model, which is based on contrastive\nlearning, to compute the text features r for the final vocabulary V :\nr = {\u03d5CLIP-text(Vi)}L1 (8)\nwhere L is the size of the final vocabulary V . Finally, we determine object descriptions for each visual feature by evaluating the cosine similarity between the visual features vG or vL and text features r. For the global frame visual features vG, we generate a total of MG global object descriptions for each visual feature, denoted as tG. Similarly, for the local frame visual features vL, we generate a total of ML local object descriptions for each visual feature, denoted as tL.\nDue to the different focuses of global frame visual features vG and local frame visual features vL, there are also differences in the global object descriptions tG and local object descriptions tL. We consider tG and tL to represent object-level textual information."
        },
        {
            "heading": "3.4 Multi-Level Alignment and Training",
            "text": "Multi-Level Alignment Prompting. To achieve Visual-Language alignment, we integrate the obtained object-level and frame-level visual features with their corresponding textual descriptions. This integration allows us to align and fuse the visual and textual information at both the object-level and frame-level. Then, we integrate expandable temporal markers to facilitate visual-language alignment at the video-level.\nAs illustrated in Figure 3, we have devised the following prompt:\n\u201cQuestion: <Question>? Answer: [MASK].\nCaption: <Caption>. Global: <Global Objects>.\nLocal: <Local Objects>. Subtitles: <additional\ndescription>.\u201d\nFor the alignment at the object-level, we consider \u201ca visual feature and its corresponding object description\u201d as the alignment unit. We iteratively combine these units to align the global and local frame visual features (vG or vL) with their respective object descriptions (tG or tL). To leverage the inductive bias of language proximity, we sort the object descriptions in descending order based on their cosine similarity between text features and visual features. Specifically, for each global frame visual feature \u201c[GLOBAL]\u201d and local frame visual feature \u201c[LOCAL]\u201d, we construct the following prompt:\n\u201c[GLOBAL] <Description 1>, ...\u201d\n\u201c[LOCAL] <Description 1>, ...\u201d\nFor the alignment at the frame-level, we consider all object-level information grouping by frame and global frame captions \u201cFirst, <Caption 1>. Second, <Caption 2>. ...\u201d as the alignment unit.\nFor the alignment at the video-level, we consider the frame-level information and additional descriptions as the alignment unit. At the videolevel, temporal information is indeed crucial for video understanding. Therefore, we incorporate expandable ordinal markers into our model, such as \u201cFirst,\u201d \u201cSecond,\u201d and so on.\nLanguage Model with Adapter. We use the same Adapter layer as in the FrozenBiLM (Yang et al., 2022a) model. Each visual feature is passed through a linear projection layer and incorporated into the language model as an individual prompt. The global and local visual features (vG and vL) are linearly mapped using the projection matrices (PG and PL) to obtain the global and local feature prompts (uG and uL):\nuG = {PG(vGi )}T1 (9)\nuL = {PL(vLi )} \u2211T j=1 Nj 1 (10)\nFor autoencoder language models like DeBERTa (He et al., 2021), we employ a frozen MLM classifier head, denoted as m\u03b8, to predict answers from the vocabulary set A, which is constructed from the answers appearing in the training set.\nMultimodal Training. During training, we only update the parameters of the visual-to-text projection modules P and the adapter module. To ensure consistency with language models, We use the visually-conditioned Masked Language Modeling (MLM) objective, where some tokens {xm} are randomly masked, and the model has to predict these masked tokens based on the other tokens and the visual input. Formally, we minimize the following loss:\nL\u00b5(x, y) = \u2212 1\nM \u2211 m log p\u00b5(x\u0303, y) xm m , (11)\nwhere x\u0303 is the corrupted text sequence, y is the sequence of visual input, p\u00b5(x\u0303, y)xmm is the probability for the (masked) m-th token in x\u0303 to be xm, and M is the number of masks in the sequence x\u0303. In detail, we adopt the same configuration as BERT (Devlin et al., 2019) for our method."
        },
        {
            "heading": "4 Experiments",
            "text": ""
        },
        {
            "heading": "4.1 Experimental Setup and Datasets",
            "text": "General Setup. To ensure reproducibility, we utilize the same fixed seed for all experiments. Each experiment was conducted three times, and the average results were reported as the final outcome. For all other methods, we report the experimental results as documented in the paper.\nPre-Trained Models. We utilize the CLIP ViT-L/14 model (Radford et al., 2021) as the image encoder \u03d5CLIP with an input image resolution of 336 \u00d7 336. We utilize the Cutler (Wang et al., 2023b) model as the mask generator \u03d5CutLER, which leverages the Cascade Mask R-CNN (Cai and Vasconcelos, 2021) as the detector for generating masks. We utilize the BLIP model (Li et al., 2022b) with ViT-L (Dosovitskiy et al., 2021) as the caption generator. We utilize the DeBERTa-V2XLarge model (He et al., 2021) as the language model.\nNote that in practical applications, the caption generator usually incorporates an image encoder (Li et al., 2022b, 2023a; Wang et al., 2022c), which facilitates the integration of the image encoder and caption generator to streamline the process. In our experiments, we intentionally separated the image encoder and subtitle generator to ensure a fairer comparison with other CLIP-based methods.\nDatasets. We pre-trained our model using the WebVid-2M (Bain et al., 2021) dataset. We conducted evaluations on the iVQA (Yang et al., 2021), MSRVTT-QA (Xu et al., 2017), MSVD-QA (Xu et al., 2017), and TGIF-FrameQA (Jang et al., 2017) datasets to assess the performance of our approach. Detailed information about the dataset is provided in the Appendix A.\nFurthermore, given that pre-trained models are trained with additional datasets before being used, our method also incorporates these additional datasets. Our method predominantly integrates 400 million image-text pairs from the image encoder CLIP, along with 14 million image-text pairs from BLIP(Li et al., 2022b), and an additional 1.3 million image-text pairs from CutLER(Wang et al., 2023b). Therefore, we incorporate a total of 415.3 million image-text pairs through all pre-trained models.\nImplementation Details. Detailed implementation information is provided in the Appendix B."
        },
        {
            "heading": "4.2 Comparison with State-of-the-art",
            "text": "In this section, we conduct a comprehensive evaluation of our method by comparing it with stateof-the-art methods in both the fully-supervised and zero-shot settings.\nFully-Supervised Benchmarks. Table 1 presents the results of our method compared to the state-of-the-art fully-supervised methods. Our method achieves state-of-the-art performance on the iVQA, MSRVTT-QA and MSVD-QA datasets, while obtaining competitive results on the TGIFFrameQA dataset. Note that we have utilized the smallest extra pre-training dataset compared to all methods. In particular, our method outperforms FrozenBiLM (Yang et al., 2022a), which also utilizes a language model with Adapter. This can be attributed to the effective transfer of the zero-shot capability of the mask generator, image encoder, and caption generator in our method, which reduces\nthe dependency on additional pre-training data.\nZero-Shot Benchmarks. Table 2 presents the results of our method compared to the state-ofthe-art zero-shot methods. The few-shot results of our method can be found in Appendix C. Our method achieves state-of-the-art performance on the TGIF-Frame dataset, while obtaining competitive results on the iVQA dataset. Similarly, we have utilized the smallest extra pre-training dataset and a reduced number of trained parameters. Similar to a fully-supervised setting, our method demonstrates a relative advantage over FrozenBiLM (Yang et al., 2022a) in the zero-shot setting. In zero-shot tasks, the size of the pre-training dataset plays a more crucial role compared to fully-supervised tasks. However, even with the smallest extra pre-training dataset, our method still achieves state-of-the-art or competitive performance. This can be attributed to the effective transfer of the zero-shot capability\nof the mask generator, image encoder, and caption generator in our method, which reduces the dependency on additional pre-training data.\nThe influence of the size of the pre-training dataset is shown in Figure 4, and detailed data can be found in Appendix C.2. For all datasets, our method exhibits enhanced performance as the size of the pre-training dataset expands. This indicates that our method still has significant potential for improvement.\nDataset-Specific Analysis. For the TGIFFrameQA dataset, 51% of the questions focus on concrete object-related information such as color and quantity, while 49% of the questions address abstract information, such as actions. Our method performs well in capturing concrete object-related information in the zero-shot setting, resulting in a strong performance in that scenario. However, in the fully-supervised setting, our method faces challenges in further improving its capability to extract concrete information, and it exhibits relatively low ability in capturing abstract information\ncompared to concrete information. As a result, our method achieves suboptimal performance in the fully-supervised setting.\nFor the iVQA dataset, in the zero-shot setting, our method demonstrates comparable performance to Flamingo-9B, which has 1.8 billion trained parameters. In practice, it is commonly observed that larger trained parameter sizes tend to lead to improved generalization performance. Therefore, although our method achieved suboptimal performance compared to Flamingo, which has 10 billion trained parameters, it is still a significant accomplishment considering that our trained parameters constitute only 0.3% of Flamingo and 1.7% of Flamingo-9B."
        },
        {
            "heading": "4.3 The Influence of Multi-Level Alignment",
            "text": "In this section, we conduct an ablation study to analyze the influence of multi-level alignment and explore the complementary and substitutive effects among the three levels of alignment. All experiments were conducted with the identical pretraining configuration, as elucidated in appendix B. To minimize the influence of fine-tuning and conserve computational resources, all ablation experiments will be conducted and reported under the zero-shot setting.\nThe Complementary Effect. Table 3 presents the results of the ablation studies on frame-level and object-level alignment, with the utilization of video-level alignment. This experiment aims to uncover the complementary effects of frame-level and object-level alignment on video-level alignment.\nFor the MSRVTT-QA and MSVD-QA datasets, where the primary question types involve global descriptions of the videos, and for the iVQA dataset,\nwhich focuses on scene and coarse-grained object information, frame-level alignment plays a dominant role as it encompasses a significant majority of the spatiotemporal information relevant to the questions. In contrast, for the TGIF-FrameQA dataset, which primarily focuses on object information, object-level alignment assumes the leading role as it helps capture fine-grained spatial information of the objects.\nOverall, the comparison between row 4 and the other rows in Table 3 supports the complementary effect of object-level and frame-level alignment, and further confirms that utilizing three levels of alignment simultaneously is the optimal method.\nThe Alternative Effect. Table 4 presents the results of the ablation studies on frame-level and object-level alignment, without the utilization of video-level alignment. This experiment aims to uncover the alternative effects of frame-level and object-level alignment on video-level alignment.\nOverall, the comparison between row 3 and rows 1 to 2 in Table 4 provides evidence that the combined effect of frame-level and object-level alignment is superior to their individual effects. Additionally, the comparison between row 4 and rows 1 to 3 confirms the importance of video-level alignment, suggesting that object-level and frame-level alignment have less substitutive effects on videolevel alignment.\nExperimental Summary. Considering all the conclusions, it can be inferred that the three levels of alignment exhibit complementary effects. Additionally, we investigate the influence of misalignment between the visual and textual modalities in Appendix D. For the influence of temporal information, please refer to Appendix E. Moreover, we conducted experiments to examine the influence of accumulated errors in the Appendix F."
        },
        {
            "heading": "5 Conclusions",
            "text": "In this paper, we introduce Mulan: A Multi-Level Alignment Model for Video Question Answering, which enhances temporal and spatial information learning through multi-level alignment. Additionally, we conduct experiments on public VideoQA datasets and achieve state-of-the-art results. Finally, we provide ablation studies to demonstrate the complementary effects of multi-level alignment and the effectiveness of our method."
        },
        {
            "heading": "Limitations",
            "text": "In this work, we analyze the limitations as follows: (1) Influence of maximum sequence length: Our method is influenced by the maximum sequence length of the language model. Although, as discussed in Appendix E, there is a diminishing marginal utility of increasing the number of sampled frames, and expanding sequence lengths is a crucial direction in the development of large language models. However, reducing the temporal and spatial redundant information between different levels can still be beneficial.\n(2) Abstraction ability: Our method provides less assistance in answering questions that focus on abstract information, while it offers greater assistance in answering questions that focus on concrete information. To enhance the abstraction ability of the model, we believe that deeper interactions between different levels and modalities are beneficial."
        },
        {
            "heading": "Ethics Statement",
            "text": "This work does not involve any direct ethical concerns. We are dedicated to advancing the field of video question answering. All experiments were conducted on open datasets, and the findings and conclusions of this paper are accurately and objectively reported."
        },
        {
            "heading": "A Datasets",
            "text": "WebVid-2M (Bain et al., 2021) consists of 2.5 million videos and their corresponding text pairs, collected from the Shutterstock website. The dataset does not include audio, and the video captions are obtained from pre-existing alternative textual descriptions.\niVQA (Yang et al., 2021) is an open-end VideoQA benchmark that focuses on objects, scenes, and people in instructional videos. It consists of 10k video clips and 10k QA pairs, split into 6k/2k/2k for training/validation/testing.\nMSVD-QA (Xu et al., 2017) is an open-end VideoQA benchmark that consists of 2k video clips and 51k QA pairs, split into 32k/6k/13k for training/validation/testing. The QA pairs in MSVD-QA are automatically generated from video descriptions.\nMSRVTT-QA (Xu et al., 2017) is an open-end VideoQA benchmark that consists of 10k video clips and 243k QA pairs, split into 158k/12k/73k\nfor training/validation/testing. Similar to MSVDQA, the QA pairs in MSRVTT-QA are also automatically generated from video descriptions.\nTGIF-FrameQA (Jang et al., 2017) is an openend VideoQA benchmark based on GIF videos. In the TGIF-FrameQA dataset, the majority of videos have short durations, typically less than 5 seconds. These videos are primarily classified into four categories: objects, quantities, colors, and positions. It consists of 46k gifs and 53k QA pairs, split into 39k/13k for training/testing.\nB Implementation Details\nHyperparameter Settings. For all experiments, we employ uniform sampling to select 8 frames (T = 8) from each video. For pre-training experiments, we solely utilize video-level textual descriptions to avoid the model gathering information from other frame-level and object-level textual descriptions, which could potentially impact performance. For evaluation and fine-tuning experiments, we impose certain constraints to accommodate the token limitations of the language model. Specifically, we limit the generation of a maximum of 8 objects per frame. Additionally, the maximum number of global frame object descriptions per frame is set to 8, and the maximum number of local frame object descriptions per object is set to 2. For pre-training experiments, we use a sequence length of 128, and for evaluation and fine-tuning experiments, we use a sequence length of 512. The visual feature dimension of the pre-trained image encoder ViT-L/14 (Dosovitskiy et al., 2021) is 768, while the hidden dimension of the pre-trained language model DeBERTa-V2-XLarge (He et al., 2021) is 1536.\nTraining. We conducted all experiments on 8 NVIDIA GeForce RTX 3090 GPUs. The pretraining on the WebVid-2M (Bain et al., 2021) dataset took approximately 10 hours, with a total of 2 epochs. For the pre-training experiments, we set the learning rate to 3\u00d7 10\u22125 and trained for 2 epochs with a batch size of 16. For the fine-tuning experiments, we set the learning rate to 5\u00d7 10\u22125 and trained for 40 epochs with a batch size of 4. For all training experiments, we employ the Adam optimizer (Kingma and Ba, 2015) with \u03b2 = (0.9, 0.95) and no weight decay.\nStatic Vocabulary Setting. We constructed the static vocabulary based on the class names from the OpenImage v7 dataset (Kuznetsova et al., 2020),\nwhich consists of 21k noun phrases. For these noun phrases, we calculate the cosine similarity between each pair, and if the cosine similarity between two words exceeds 0.95, we remove the word with the smaller frequency. In addition, we manually removed less informative words from the vocabulary, such as \u201cvideo,\u201d \u201cpicture,\u201d \u201cphoto,\u201d and so on."
        },
        {
            "heading": "C Results in Low-Data Regime",
            "text": "To examine the influence of the low-data regime on the performance of our method, we conducted experiments during both the pre-training and finetuning stages."
        },
        {
            "heading": "C.1 Few-Shot Setting",
            "text": "We employed few-shot learning as an experimental setup to evaluate the performance of our method in the low-data regime during the fine-tuning stage. The experimental results under the few-shot setting are presented in Table 5. The experimental results reveal a substantial performance improvement when utilizing only 1% and 10% of the training data compared to the zero-shot setting."
        },
        {
            "heading": "C.2 Size of the Pre-Training Dataset",
            "text": "We manipulated the size of the pre-training dataset and performed zero-shot testing to evaluate the performance of our method in the low-data regime during the fine-tuning stage. The detailed experimental results are presented in Table 6. The experimental results confirm the substantial impact of the pre-training dataset size on the model\u2019s performance. The performance of our method improves as the size of the pre-training dataset increases."
        },
        {
            "heading": "D The Influence of Misalignment",
            "text": "To examine the influence of modality misalignment, we conducted experiments that involved the separation of visual and textual modalities. The experimental results are presented in Table 7. In this experiment, we conducted pre-training using all modalities and evaluated the performance by separately considering the visual and textual modalities in the zero-shot setting.\nFrom the perspective of the individual effects of the visual and textual modalities, the textual modality plays a more prominent role compared to the visual modality. This is primarily because we leverage the language model for multimodal fusion. In terms of question types, the textual modality demonstrates significant assistance, especially for questions of the \u201cnumber\u201d, \u201cwhere\u201d, and \u201cwhen\u201d types. These question types often require a higher level of abstraction, and the textual modality provides a more abstract representation compared to the visual modality. Conversely, the visual modality proves to be particularly advantageous for \u201cwho\u201d type of questions, as this type of question requires the model to distinguish between visual entities present in the scene. The visual modality provides more detailed information about different visual entities, enabling the model to better address \u201cwho\u201d type questions. Overall, the experimental results across all datasets consistently demonstrate that the synergistic alignment between the visual and textual modalities can effectively improve the performance."
        },
        {
            "heading": "E The Influence of Temporal Information",
            "text": "The temporal nature of videos is the fundamental distinction between videos and images. In this sec-\ntion, we examine the influence of temporal information in videos. Specifically, we will conduct experiments to investigate the influence of different sampling frame numbers and our proposed expandable ordinal prompt approach. When two frames are sampled, our method shows a notable improvement in performance as it becomes capable of capturing the temporal information present in the video. As the number of sampled frames increases, the performance of our method continues to improve, albeit with diminishing returns."
        },
        {
            "heading": "E.1 The Influence of Sampling Frame Numbers",
            "text": "Table 8 presents the influence of sampling frame numbers. When only a single frame is sampled, the video question answering system degrades to an image question answering system, resulting in the poorest performance due to the inability to capture the temporal information of the video. As the\nnumber of sampled frames increases, the performance of our method continues to improve, albeit with diminishing returns. In conclusion, we have confirmed the importance of the temporal nature of videos."
        },
        {
            "heading": "E.2 The Influence of the Expandable Ordinal Prompt Approach",
            "text": "Table 9 presents the influence of the expandable ordinal prompt approach. The experimental results demonstrate that the expandable ordinal prompt approach enhances the performance of nearly all question types across all datasets when compared to not utilizing the temporal prompt method. The experimental results confirm that our proposed expandable ordinal prompt approach facilitates the language model\u2019s acquisition of temporal information in videos."
        },
        {
            "heading": "F The Influence of Accumulated Errors",
            "text": "From a structural standpoint, our method conforms to a pipeline structure. At an equivalent scale, a pipeline structure, when compared to an end-to-end structure, offers increased flexibility. However, the pipeline structure method tends to result in a higher incidence of cascading errors.\nTo examine the influence of cascading errors, We designed experiments in which we introduced simulated errors by randomly substituting language information and adding random noise to visual information. Specifically, concerning the textual information of a sample, we randomly substitute sentences or words with sentences or words from another sample. Regarding the visual information of a sample, we randomly added Gaussian noise to the pixel images of frames or objects.\nTable 1 presents the results on the MSVD-QA dataset, which emphasizes the overall video information. Table 2 presents the results on the TGIFFrameQA dataset, which focuses on fine-grained details. Overall, the cascading errors generated by frame-level features surpass those of object-level features due to the dependence of object-level features on frame-level features. Similarly, the cascading errors arising from the visual modality outpace those from the textual modality, primarily attributed to the demanding visual capabilities required in video question answering tasks and the dependence of text description generation on visual features. Furthermore, as the ratio of random noise increases, the performance undergoes an accelerated decline rather than a linear descent. When the random noise ratio reaches 75%, our method still manages to maintain a relatively comparable performance. From a dataset perspective, the TGIFFrameQA dataset exhibits heightened sensitivity to\nthe cascading errors arising from object-level information in comparison to the MSVD-QA dataset. This differentiation stems from the fact that the TGIF-FrameQA dataset is designed to focus on intricate details, whereas the MSVD-QA dataset emphasizes the holistic video information."
        }
    ],
    "title": "Mulan: A Multi-Level Alignment Model for Video Question Answering",
    "year": 2023
}