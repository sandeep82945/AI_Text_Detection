{
    "abstractText": "Current approaches to Argument Mining (AM) tend to take a holistic or black-box view of the overall pipeline. This paper, in contrast, aims to provide a solution to achieve increased performance based on current components instead of independent all-new solutions. To that end, it presents the Deployment of Recombination and Ensemble methods for Argument Miners (DREAM) framework that allows for the (automated) combination of AM components. Using ensemble methods, DREAM combines sets of AM systems to improve accuracy for the four tasks in the AM pipeline. Furthermore, it leverages recombination by using different argument miners elements throughout the pipeline. Experiments with five systems previously included in a benchmark show that the systems combined with DREAM can outperform the previous best single systems in terms of accuracy measured by an AM benchmark.",
    "authors": [
        {
            "affiliations": [],
            "name": "Florian Ruosch"
        },
        {
            "affiliations": [],
            "name": "Cristina Sarasua"
        }
    ],
    "id": "SP:28b99977494d867473a583f831563e655b612339",
    "references": [
        {
            "authors": [
                "Khalid Al-Khatib",
                "Tirthankar Ghosal",
                "Yufang Hou",
                "Anita de Waard",
                "Dayne Freitag."
            ],
            "title": "Argument Mining for Scholarly Document Processing: Taking Stock and Looking Ahead",
            "venue": "Proceedings of the Second Workshop on Scholarly Document Pro-",
            "year": 2021
        },
        {
            "authors": [
                "Iz Beltagy",
                "Kyle Lo",
                "Arman Cohan."
            ],
            "title": "SciBERT: A pretrained language model for scientific text",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Lan-",
            "year": 2019
        },
        {
            "authors": [
                "Yoav Benjamini",
                "Yosef Hochberg."
            ],
            "title": "Controlling the false discovery rate: a practical and powerful approach to multiple testing",
            "venue": "Journal of the Royal statistical society: series B (Methodological), 57(1):289\u2013300.",
            "year": 1995
        },
        {
            "authors": [
                "Leo Breiman."
            ],
            "title": "Bagging predictors",
            "venue": "Machine learning, 24:123\u2013140.",
            "year": 1996
        },
        {
            "authors": [
                "Katarzyna Budzynska",
                "Serena Villata."
            ],
            "title": "Argument Mining",
            "venue": "IEEE Intelligent Informatics Bulletin, 17(1):1\u20136.",
            "year": 2015
        },
        {
            "authors": [
                "Artem Chernodub",
                "Oleksiy Oliynyk",
                "Philipp Heidenreich",
                "Alexander Bondarenko",
                "Matthias Hagen",
                "Chris Biemann",
                "Alexander Panchenko."
            ],
            "title": "TARGER: Neural Argument Mining at Your Fingertips",
            "venue": "Proceedings of the 57th Annual Meeting of the",
            "year": 2019
        },
        {
            "authors": [
                "Michael Collins",
                "Nigel Duffy."
            ],
            "title": "New ranking algorithms for parsing and tagging: Kernels over discrete structures, and the voted perceptron",
            "venue": "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 263\u2013270.",
            "year": 2002
        },
        {
            "authors": [
                "Beatriz Fisas",
                "Francesco Ronzano",
                "Horacio Saggion."
            ],
            "title": "A multi-layered annotated corpus of scientific papers",
            "venue": "Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC\u201916), pages 3081\u20133088.",
            "year": 2016
        },
        {
            "authors": [
                "Chris Fournier."
            ],
            "title": "Evaluating text segmentation using boundary edit distance",
            "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1702\u20131712.",
            "year": 2013
        },
        {
            "authors": [
                "William H Greene."
            ],
            "title": "Econometric analysis",
            "venue": "Pearson Education India.",
            "year": 2003
        },
        {
            "authors": [
                "Steffen Herbold."
            ],
            "title": "Autorank: A python package for automated ranking of classifiers",
            "venue": "Journal of Open Source Software, 5(48):2173.",
            "year": 2020
        },
        {
            "authors": [
                "Anne Lauscher",
                "Goran Glava\u0161",
                "Kai Eckert."
            ],
            "title": "ArguminSci: A Tool for Analyzing Argumentation and Rhetorical Aspects in Scientific Writing",
            "venue": "Proceedings of the 5th Workshop on Argument Mining, pages 22\u201328.",
            "year": 2018
        },
        {
            "authors": [
                "Anne Lauscher",
                "Goran Glava\u0161",
                "Simone Paolo Ponzetto."
            ],
            "title": "An Argument-Annotated Corpus of Scientific Publications",
            "venue": "Proceedings of the 5th Workshop on Argument Mining, pages 40\u201346.",
            "year": 2018
        },
        {
            "authors": [
                "John Lawrence",
                "Chris Reed."
            ],
            "title": "Combining Argument Mining Techniques",
            "venue": "Proceedings of the 2nd Workshop on Argumentation Mining, pages 127\u2013136.",
            "year": 2015
        },
        {
            "authors": [
                "John Lawrence",
                "Chris Reed."
            ],
            "title": "Argument Mining: A Survey",
            "venue": "Computational Linguistics, (August):1\u201355.",
            "year": 2019
        },
        {
            "authors": [
                "Marco Lippi",
                "Paolo Torroni."
            ],
            "title": "Argumentation Mining: State of the Art and Emerging Trends",
            "venue": "ACM Transactions on Internet Technology, 16(2):1\u201325.",
            "year": 2016
        },
        {
            "authors": [
                "Marco Lippi",
                "Paolo Torroni."
            ],
            "title": "MARGOT: A web server for argumentation mining",
            "venue": "Expert Systems with Applications, 65:292\u2013303.",
            "year": 2016
        },
        {
            "authors": [
                "Nick Littlestone",
                "Manfred K Warmuth."
            ],
            "title": "The weighted majority algorithm",
            "venue": "Information and computation, 108(2):212\u2013261.",
            "year": 1994
        },
        {
            "authors": [
                "Tobias Mayer",
                "Elena Cabrio",
                "Serena Villata."
            ],
            "title": "Transformer-based argument mining for healthcare applications",
            "venue": "Frontiers in Artificial Intelligence and Applications, volume 325, pages 2108\u20132115.",
            "year": 2020
        },
        {
            "authors": [
                "David Opitz",
                "Richard Maclin."
            ],
            "title": "Popular ensemble methods: An empirical study",
            "venue": "Journal of artificial intelligence research, 11:169\u2013198.",
            "year": 1999
        },
        {
            "authors": [
                "Nils Reimers",
                "Benjamin Schiller",
                "Tilman Beck",
                "Johannes Daxenberger",
                "Christian Stab",
                "Iryna Gurevych."
            ],
            "title": "Classification and Clustering of Arguments with Contextualized Word Embeddings",
            "venue": "Proceedings of the 57th Annual Meeting of the",
            "year": 2019
        },
        {
            "authors": [
                "Leonardo F.R. Ribeiro",
                "Yue Zhang",
                "Claire Gardent",
                "Iryna Gurevych."
            ],
            "title": "Modeling Global and Local Node Contexts for Text Generation from Knowledge Graphs",
            "venue": "Transactions of the Association for Computational Linguistics, 8:589\u2013604.",
            "year": 2020
        },
        {
            "authors": [
                "Florian Ruosch",
                "Cristina Sarasua",
                "Abraham Bernstein."
            ],
            "title": "BAM: Benchmarking Argument Mining on Scientific Documents",
            "venue": "Proceedings of the Workshop on Scientific Document Understanding colocated with 36th AAAI Conference on Artificial In-",
            "year": 2022
        },
        {
            "authors": [
                "Isabel Segura-Bedmar",
                "Paloma Mart\u00ednez Fern\u00e1ndez",
                "Mar\u00eda Herrero Zazo."
            ],
            "title": "Semeval-2013 task 9: Extraction of drug-drug interactions from biomedical texts (ddiextraction 2013)",
            "venue": "Association for Computational Linguistics.",
            "year": 2013
        },
        {
            "authors": [
                "Dietrich Trautmann",
                "Johannes Daxenberger",
                "Christian Stab",
                "Hinrich Sch\u00fctze",
                "Iryna Gurevych."
            ],
            "title": "Fine-Grained Argument Unit Recognition and Classification",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, pages 9048\u20139056.",
            "year": 2020
        },
        {
            "authors": [
                "Douglas Walton",
                "Christopher Reed",
                "Fabrizio Macagno."
            ],
            "title": "Argumentation schemes",
            "venue": "Cambridge University Press.",
            "year": 2008
        },
        {
            "authors": [
                "Simon Wells"
            ],
            "title": "Argument Mining: Was Ist Das",
            "venue": "Proceedings of the 14th International Workshop on Computational Models of Natural Argument (CMNA14),",
            "year": 2014
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "A well-known and open challenge in Argument Mining (AM) is that approaches do not generalize well across domains (Lippi and Torroni, 2016a). Thus, a single system will not be able to solve the task of extracting arguments from publications across multiple research fields. Therefore, we investigate the use of ensemble methods (Opitz and Maclin, 1999) to find combinations that are expected to help alleviate the issue.\nFurthermore, the overview of AM approaches presented by Lawrence and Reed (2019) shows that papers typically introduce novel techniques or use methods for AM that have demonstrated success in other applications. As new systems tend to take the holistic view of an end-to-end pipeline (Lawrence and Reed, 2019), it has become evident that novel approaches rarely investigate improvements of intermediate steps. By introducing a system with ensemble methods and combinations, we aim to improve smaller aspects of the pipeline.\nMoreover, Lawrence and Reed (2019) also take the same line by advocating for a unifying framework to enable the harmonization of all AM tasks, including the format of data and results. Such a unification would be necessary to combine many systems and facilitate the integration of additional ones. Likewise, not every task receives the same amount of attention, with approaches for identifying argumentative relations being sparse (AlKhatib et al., 2021). Thus, they cover a smaller range of domains or do not work well across them. By using recombination, we hypothesize to increase coverage and find yet untapped potential.\nTo this end, we formulate the following research question:\nRQ. How can we leverage (re-)combinations of Argument Mining systems to improve accuracy?\nThus, we build DREAM, a system that allows for the Deployment of Recombination and Ensemble methods for Argument Miners. For this endeavor, we base our approach and the evaluation on BAM (Ruosch et al., 2022), our benchmark for Argument Mining. We reuse the performance data of five AM systems when evaluated by BAM as well as its implementation for our purposes. Accordingly, we restrict the systems for the initial combinations to these five argument miners and adhere to the definition of the four tasks in the AM pipeline (Lippi and Torroni, 2016a): sentence classification, boundary detection, component identification, and relation prediction. Using these tools, we try to outperform the current best accuracy for every task of the AM pipeline by combining systems with the following ensemble methods: voting, stacking, and bagging. Finally, we split the AM systems into \u201cmodules\u201d according to the AM pipeline, allowing their recombination to increase accuracy.\nWe present two main contributions in this paper. First and foremost, we build the DREAM framework to combine AM systems using ensemble\nmethods and recombinations. Second, we show the value of such combinations, as they outperform some of the state-of-the-art systems used in the AM benchmark.\nThe remainder of this paper is structured as follows: Section 2 presents background and the related work, and Section 3 introduces our methodology. In the ensuing Section 4, we describe our experiments and their results before we evaluate them in Section 5. Then, Section 6 discusses limitations and future work. Finally, we draw conclusions in Section 7."
        },
        {
            "heading": "2 Background",
            "text": "In this section, we lay the foundations by describing Argument Mining and presenting specific related work."
        },
        {
            "heading": "2.1 Argument Mining",
            "text": "The field of AM is wide-ranging and has different interpretations (Wells, 2014) of what its tasks consist of. We focus on the information extraction approach (Budzynska and Villata, 2015; Lippi and Torroni, 2016a): the automated analysis of arguments in natural language text. To this endavor, we consider the AM pipeline as described by Lippi and Torroni (2016a), depicted in Figure 1. The input text is processed in four stages: argumentative sentence detection, argument component boundary detection, argument component detection, and argument structure prediction.\nIn the first step, sentences are classified as argumentative if they contain parts of an argument and as non-argumentative otherwise. Next, the boundaries of the argument components are identified by segmenting the argumentative sentences. Then, these argument components are classified according to the representation of the arguments defined beforehand. Finally, the structure (i.e., relations) of the previously identified components is predicted to form an argument graph. The annotated text (in any format) is the output of the AM pipeline."
        },
        {
            "heading": "2.2 Specific Related Work",
            "text": "Combining approaches in AM has barely received any attention in previous literature. The only exception is the work of Lawrence and Reed (2015), where the authors implement and combine three different AM techniques. They are evaluated with\nrespect to identified connections between propositions and use a fixed set of 36 pairs.\nFirst, the presence of discourse indicators: words such as \u201cbecause\u201d and \u201chowever\u201d, indicating support- and conflict-relations, respectively, between adjacent statements. These words provide a good signal (precision of 1.00), but the technique fails to capture most relations (recall of 0.08) due to their low number of occurrences in texts. Furthermore, they can not be used to find relations for non-adjacent propositions.\nThe second technique is based on changes in the topic for consecutive propositions, which is assumed to relate to the argumentative structure in the text. The similarity of adjacent propositions is calculated using the synsets of WordNet1, resulting in a number on a scale from 0 to 1. A preset threshold then determines whether the topic remains the same, and, that being the case, it is deduced that the propositions are connected. This approach achieves a precision of 0.70 and a recall of 0.54, respectively.\nThe third method uses argumentation schemes (Walton et al., 2008): \u201ccommon patterns of human reasoning.\u201d They avoid the need for having the components and the structure of arguments already annotated by instead focusing on features of the parts of the present scheme. With a list of propositions from the text and a Na\u00efve Bayes classifier, they can determine the particular scheme and, therefore, detect information about the structure of the argumentation. This results in a precision of 0.82 and a recall of 0.69.\nFinally, the techniques are combined to exploit their respective fortes. The presence of discourse indicators is used to infer connections among propositions in the first step. Subsequently, components are related after having determined argumentation scheme instances. Lastly, previously unconnected units are integrated based on topic similarity. Combining the methods results in an improved performance with precision and recall, increasing to 0.91 and 0.77, respectively.\nIn contrast to the approach described above, we aim to provide combinations on a larger scale and a pipeline for a unifying framework that allows for integrating additional components. We aim to investigate if and how combinations (of parts of) different AM system can be used to improve overall performance. Finally, our approach also differs\n1http://wordnet.princeton.edu\nin that we do not look to combine techniques or features but rather out-of-the-box argument miners. This facilitates the integration of additional systems."
        },
        {
            "heading": "3 Methodology",
            "text": "In this section, we first lay out our evaluation hypotheses and then describe the concept of the DREAM (re-)combination framework."
        },
        {
            "heading": "3.1 Hypotheses",
            "text": "We base both the evaluation and the conception on the systems, data, and results of BAM, our benchmark for Argument Mining (Ruosch et al., 2022). We utilize the five systems included in BAM (i.e., AURC, TARGER, TRABAM, ArguminSci, and MARGOT) and evaluate the recombinations using the provided implementation of the benchmark. That means the AM pipeline is split into four tasks (Lippi and Torroni, 2016a): sentence classification, boundary detection, component identification, and relation prediction. These tasks are evaluated with their respective metrics from BAM (i.e., micro F1, the boundary similarity measure defined by Fournier (2013), and F1-score).\nTo evaluate the implemented recombination system, we formulate the following hypotheses derived from the research question and describe our approach to assess their acceptance or rejection.\nH1. For some tasks in the AM pipeline, ensembles of systems exist for which accuracy will be higher than for the most accurate single system.\nThis hypothesis encapsulates two different but entangled problems: finding the optimal set of systems to combine and testing whether they are more accurate than the current top system. Thus, we split it into two sub-hypotheses, which are the requisites for accepting Hypothesis H1.\nH1.1. There exists an ensemble of systems for every task that is more accurate than any other ensemble of systems (excluding single systems).\nSince we already restrict the space of systems and combinations that we need to explore by limiting ourselves to the systems in BAM, we can test all combinations of size n, where 1 < n \u2264 5, because we require combinations of at least two and can combine at most all five systems. Thus, it becomes a matter of running all possible systems and combining them using ensemble methods We accept the hypothesis if we find one or more ensembles of systems that exceed all others in terms of accuracy as measured by BAM for all the tasks. It is important to note that these ensembles might differ for individual tasks.\nH1.2. For some tasks in the AM pipeline, the most accurate ensemble of systems will be more accurate than the most accurate single system for this task.\nFor the second sub-hypothesis, we can compare the previously discovered combinations with the most accurate single system and compare their numbers for all the tasks. That means doing a pair-wise comparison four times, namely once for every task, and checking whether the combinations outperform the single systems. Again, we accept the hypothesis if we can confirm this for at least some of the four tasks.\nH2. For some tasks in the AM pipeline, the accuracy for subsequent tasks will be higher if intermediate data is used that has been produced by the system with the highest accuracy for the preceding task instead of its own intermediate data.\nSubsequently, we investigate how to improve in single tasks and how the intermediate results influence the ensuing tasks of the pipeline. Thus, we\nhypothesize that data of higher accuracy compared to the ground truth will also result in an increase in a system\u2019s accuracy as opposed to its own intermediate results. Again, we try out all possible pairs to answer this hypothesis. Any two systems can be combined by using one\u2019s output as the other one\u2019s input, provided that the former\u2019s accuracy was higher than the latter\u2019s at the preceding task. Considering the five systems and four tasks in the benchmark, we have to try out a maximum of 60 pairwise combinations, every system acting as \u201cinput provider\u201d and \u201cinput taker\u201d but never at the same time. We compare the new highest accuracy for every task and system to the previous results and accept this hypothesis if the new numbers are higher than the old ones.\nH3. For some tasks in the AM pipeline, the accuracy will be higher if we use an ensemble of systems and intermediate results as input produced by the most accurate system (ensemble) for the preceding task.\nThe final hypothesis brings all possible combinations together. We not only allow combining systems for tasks but also to \u201cmix-and-match\u201d for the intermediate results in the hope of improving the accuracy of the whole pipeline. We employ the best combinations from Hypothesis H1 and combine them with the insights from Hypothesis H2. We compare the newly obtained accuracies to the previous best per the benchmark and accept the hypothesis if we outperform the top single system for every task."
        },
        {
            "heading": "3.2 The DREAM Framework",
            "text": "The basic idea behind the approach to combining multiple AM systems is simple: Employ a multitude of systems such that they can combine their strengths and, at the same time, balance out their weaknesses. Our framework, DREAM, is intended for the (automated) recombination of multiple AM systems according to predefined parameters. Following the aforementioned AM pipeline by Lippi and Torroni (2016a), we first identify argumentative sentences, then we identify the boundaries of the components and classify them (usually as either claim or premise). Finally, we predict the relations between the argumentative components (such as supports or attacks).\nNot every argument miner adheres to this pipeline, which results in some of the argument\nminers lacking the capabilities to solve one or more of these tasks. Furthermore, Lawrence and Reed (2019) point out that current systems tend to take a holistic view of the end-to-end pipeline. This is further emphasized by the fact that black-box models, such as neural networks and, more specifically, transformers, become increasingly prevalent. While they carry the advantage of improved performance, they prevent a look into their inner workings and modularization of their features.\nThus, we have access solely to the final outputs of argument miners for our framework. However, as we showed in BAM (Ruosch et al., 2022), we can reconstruct the intermediate results necessary for evaluating the tasks mentioned above of the AM pipeline. We can use these reconstructed intermediate results for the recombination effort, with the added benefit of not needing to re-train or re-run any of the systems (i.e., we only perform post-hoc combinations).\nDREAM reads the output files from the argument miners and calculates the combinations according to the specified parameters. There are several different options when combining this data: the list of employed systems, the method to calculate the combination, and the targeted task.\nFigure 2 visualizes the ways we combine systems. Figure 2a corresponds to what is described in Hypothesis H1: using ensemble methods to combine multiple systems for a single task. This is what we call Vertical Integration. Meanwhile, Figure 2b illustrates Hypothesis H2: using different systems throughout the AM pipeline (recombination). This is referred to as Horizontal Integration. Tying these two together, we get the Combined Integration, where we allow sets of systems to be used for the intermediate results fed forward in the pipeline to either other combinations or single systems."
        },
        {
            "heading": "3.2.1 Vertical Integration",
            "text": "Vertical Integration gets its name because we choose systems from the \u201ccolumn\u201d of options as illustrated in Figure 2a. The number of systems used for the combination can vary from a minimum of two to all available systems. The list of used systems can be either specified or the recombination framework can try (all) possible combinations (including power sets). This is how we approach auto-experimentation for recombination.\nAs for the method to calculate the combination of results, we follow the well-established ensemble\nS System 1 System 2 System 3 System 4 System 5\nB C R\n(a) Using ensemble methods per task.\nS System 1 System 2 System 3 System 4 System 5\nB C R\n(b) Recombining systems over the whole pipeline.\nFigure 2: The two types of combinations employed in the framework.\nmethods (Opitz and Maclin, 1999). As simplest method, we employ a hard voting scheme (Littlestone and Warmuth, 1994), where systems can optionally be assigned a weight. It is important to note that systems may all receive a uniform weight of one or be assigned arbitrary scores (e.g., benchmark results). Then, we calculate the score of the available answers for a given item based on the systems\u2019 output and weights. Next, we use the ensemble stacking method, which trains a metaclassifier on the predictions the trained argument miners produced. We employ multinomial logistic regression (Greene, 2003) as the stacking model. Our third ensemble method is bootstrap aggregating (bagging) (Breiman, 1996). Using bootstrapped sets for training, we expect to strengthen the ensemble of classifiers."
        },
        {
            "heading": "3.2.2 Horizontal Integration",
            "text": "Next, we allow combining systems across the borders of individual tasks (i.e., columns) by using intermediate results and feeding them to other systems. This results in what we call Horizontal Integration, since we allow the combination of different \u201crows\u201d, as depicted in Figure 2b. The output for all the tasks in the AM pipeline depends on the input fed into the corresponding module. Although, these modules may not be explicitly constructed as such and may have to be inferred due to the holistic view of AM systems (Lawrence and Reed, 2019). Still, we can generally describe the data processing in the AM pipeline. In the first step, the raw text supplied to the pipeline is split into sentences, which in turn are classified as either argumentative or non-argumentative, depending on the presence of argumentative components in them. Thus, the output of the sentence detection depends on its input because it will process (and output) no more and no less than the text it has been supplied with. Subsequently, the boundary detection will find the delineations of components in only the\nargumentative sentences since, by definition, only they may contain argumentative components. The same holds for the component identification: it will only identify components whose boundaries have been detected. Lastly, the relation prediction relies on the previously identified components to find the triples (subject and object are from the set of argumentative components) that constitute its output. Thus, we can see that every subsequent step in the AM pipeline depends on its predecessor\u2019s output."
        },
        {
            "heading": "3.2.3 Combined Integration",
            "text": "Finally, we will also allow for the Vertically Integrated ensemble learners to be used as the intermediate result creators and, thus, bring it together with Horizontal Integration to the Combined Integration. Since we hypothesize that both individual Integrations increase accuracy, we hypothesized their combination exhibits an even higher performance. Thus, we make an effort to find sequences of combined AM systems that further improve the accuracy of the tasks in the pipeline."
        },
        {
            "heading": "4 Experiments",
            "text": "In this section, we discuss conducted experiments. First, we describe the setup used. Then, we explain the implementation of the experiments and the subsequent evaluation."
        },
        {
            "heading": "4.1 Setup",
            "text": "We rely on the systems and data used by BAM (Ruosch et al., 2022), the results of which are shown in Table 1. Thus, we consider five different AM systems that have been benchmarked using the SciArg data set (Lauscher et al., 2018b). It represents the only available collection of full argument annotated scientific papers in English and builds on the Dr. Inventor data set (Fisas et al., 2016). The corpus consists of publications from the field of computer graphics and contains a total of 10,780\nsentences that have been annotated with argumentative components (background and own claims as well as data) and relations (contradicts, supports, semantically same, and part of ).\nBAM uses individual evaluation measures for each of the pipeline tasks. For the argumentative sentence classification, it employs the microF1 (van Rijsbergen, 1979) score to avoid the skewing effect of a possible label imbalance. For the boundary detection task, BAM uses the boundary similarity measure as proposed by Fournier (2013), which compares the identified boundaries for two segmentations for the same text. The argumentative component identification is evaluated by using the F1 as implemented for the task of Named Entity Recognition (Segura-Bedmar et al., 2013). Finally, relation prediction is treated as the classification of triples (subject, predicate, object) into retrieved or missed and thus, BAM employs the F1score. Therefore, we obtain four individual scores between 0 and 1, one per task in the AM pipeline, where bigger signifies better.\nWe use the same five systems that have already been evaluated in the initial showcase of BAM. The first three were trained on the benchmark data set, while the last two were already pre-trained by the authors of the systems. AURC (Trautmann et al., 2020) treats AM as a sequence tagging problem and employs the BiLSTM model of Reimers et al. (2019) to identify argumentative spans in texts. TARGER (Chernodub et al., 2019) also uses a BiLSTM in conjunction with a CNN-CRF and pre-computed word embeddings to label tokens from free text input as belonging to either claims or premises. TRABAM (Mayer et al., 2020) relies on pre-trained transformers such as SciBERT (Beltagy et al., 2019) in combination with neural networks. TRABAM is the sole system in the benchmark that solves all the pipeline tasks, tagging argumentative components and predicting relations between them. ArguminSci (Lauscher et al., 2018a) was trained on the data set that is also incorporated in\nthe benchmark. It consists of a range of different tools to analyze rhetorical aspects, but we only use the argument component identification functionality. This module uses a BiLSTM to tag tokens as one of three argumentative component types akin to the annotations in the corpus: background claim, own claim, or data. Finally, MARGOT (Lippi and Torroni, 2016b) detects claims and evidences by analyzing the sentence structures and uses a subset tree kernel (Collins and Duffy, 2002) to compare their constituency parse trees.\nData and code involved in the execution and subsequent evaluation are available in the project\u2019s repository.2"
        },
        {
            "heading": "4.2 Vertical Integration",
            "text": "The best results for each task using the Vertical Integration are presented in Table 2.3 For context, we also report the runner-up and the worst result, as indicated in the Result column, and provide the mean, median, and variance for each task. We round all results to three decimal places for readability, except where necessary to indicate differences. Each task of the AM pipeline is represented by a row, in which the accuracy (as measured by BAM), the used ensemble method, and the systems involved are indicated (the order matters therein as the first system serves as the primary to which all other annotations are aligned to). Notably, the relation prediction score R is absent since only one system performed it in BAM, and thus, there is no opportunity to apply an ensemble method. Also, because no system explicitly disentangles the AM pipeline into individual tasks, we perform the combination on the final output and not on task-specific annotations, akin to the way it is handled in BAM. We tried every possible combination of all system lists and ensemble methods to obtain the results and list the best, second best, and worst here.\n2https://gitlab.ifi.uzh.ch/DDIS-Public/DREAM 3The full results are omitted for brevity and are available\nin the online repository.\nStacking the systems TARGER, AURC, MARGOT, and TRABAM using logistic regression is the most accurate ensemble for sentence classification with S = 0.8419. Bagging with TARGER, ArguminSci, MARGOT, and TRABAM achieves a score of B = 0.4972 for boundary detection, which is the highest among the ensembles. Combining the two systems TRABAM and TARGER using the hard voting scheme results in C = 0.673 as the best score for component identification.\nThe main insight gained from these results is that no ensemble method outperforms the others. Rather, each of the three techniques achieves the highest score for one task."
        },
        {
            "heading": "4.3 Horizontal Integration",
            "text": "Table 3 shows the complete results for the Horizontal Integration. We used the most accurate system from BAM, TRABAM, as listed in the \u201cFrom\u201d-column to indicate where the intermediate results originated from. These were combined with the output of the individual systems (in the \u201cTo\u201d-column) in the respective rows by using them as the template for the subsequent annotations.\nTARGER combined with TRABAM scores the highest for both the boundary detection B = 0.494 and component identification C = 0.630. Again, due to the lack of a system to combine TRABAM with, the results for the relation prediction R are\nomitted. The sentence classification is not considered for the Vertical Integration as its input is the initial text, which is not considered an intermediate result since it is the same for every system."
        },
        {
            "heading": "4.4 Combined Integration",
            "text": "In Table 4, we show the results of the Combined Integration. We list the results achieved with the previously identified most accurate single system or ensemble (from the Vertical Integration) and their score for each AM pipeline task. For each row in the table, the output has been combined with the output of the preceding row, according to the Horizontal Integration. This results in the Combined Integration.\nThe ensemble of TARGER, AURC, MARGOT, and TRABAM stacked using logistic regression is the most accurate for sentence classification with S = 0.842. The single system TRABAM achieves the highest boundary detection score with B = 0.483. Combining TRABAM and TARGER into an ensemble using voting results in C = 0.673 as the best score for component identification. Finally, TRABAM scores R = 0.019 for the relation prediction. Interestingly, ensembles are only better than single systems in two out of the three AM pipeline tasks (relation prediction does not have an alternative to TRABAM)."
        },
        {
            "heading": "5 Hypotheses Evaluation",
            "text": "In this section, we evaluate the hypotheses individually. The results from BAM in Table 1 serve as the baseline, more specifically, the best-performing system nicknamed TRABAM in with the boldfaced numbers. It achieved the following scores for the AM tasks, where each of them is on a scale from zero to one, and higher means better: sen-\ntence classification S = 0.832, boundary detection B = 0.506, component identification C = 0.662, and relation prediction R = 0.021. Unlike the result reported in BAM, we use TRABAM\u2019s intermediate results as input for the last step, decreasing the accuracy (from R = 0.318 when using the ground truth components). We compare the newly obtained scores to these numbers to evaluate the hypotheses. The statistical significance testing results and the correction for multiple comparisons can be found in Appendix A."
        },
        {
            "heading": "5.1 Hypothesis H1",
            "text": "This evaluation is based on the outcome of the Subhypotheses H1.1 and H1.2. Thus, we assess these two before giving the verdict on H1.\nH1.1 Before collating previous and new results, we look at the isolated findings from applying the ensemble methods. We hypothesized that there would be a set of systems that is the most accurate compared to any other combination. We can confirm this hypothesis by looking at the results produced in the experiments by using the ensemble methods. Due to the lack of a second system to combine TARGER with, no ensembles can be built to improve the relation prediction score R; thus, it is omitted.\nTable 2 shows the results for each task. From it, we can see that the highest scores are unique numbers. This leads us to accept Sub-hypothesis H1.1.\nH1.2 This hypothesis compares the results from the benchmark and the Vertical Integration by opposing the best results from Table 1 and Table 2. For the sentence classification, the ensemble of TARGER, AURC, MARGOT, and TRABAM combined by stacking them (with logistic regression) slightly outperforms the previously most accurate single system TRABAM: S = 0.842 and S = 0.832, respectively. Statistical testing, however, reveals that the difference is not significant (cf. Appendix A). For the component identification where the two systems TRABAM and\nTARGER were combined using the voting method (C = 0.673), they beat the previous best achieved by TRABAM (C = 0.662), with the difference being statistically significant. This is in contrast to the boundary detection, where the best ensemble result does not reach the most accurate single system: bagging TARGER, ArguminSci, MARGOT, and TRABAM scored B = 0.497, while TRABAM held the most accurate result in B = 0.506. As explained in the previous hypothesis, the relation prediction is omitted.\nSince we found one of three ensembles to outperform single systems with a statistical significance, this leads us to accept Sub-hypothesis H1.2. Moreover, this indicates a correlation between the systems\u2019 errors since they do not seem to balance out their weaknesses in all cases. An exhaustive error analysis would be necessary to reveal more detailed insights.\nH1 We based the acceptance of Hypothesis H1 on accepting both its corresponding subhypotheses, which we did as explained above. This means that we also accept Hypothesis H1."
        },
        {
            "heading": "5.2 Hypothesis H2",
            "text": "Table 3 shows the results of using the annotations produced by TRABAM (i.e., the most accurate system in the benchmark) as the input to subsequent steps for the other systems. The boldfaced numbers indicate improvements over the initial results with the system\u2019s own data. We can see that, except when combining TRABAM with TARGER for the component classification, we consistently outperform the benchmark results, and the differences are all statistically significant. Akin to the previous hypotheses, R cannot be improved as we do not have another system to feed TRABAM\u2019s intermediate results into, or vice versa. Therefore, we also omit the relation prediction from evaluating this hypothesis. Since we could show that using more accurate intermediate results can improve the subsequent step of the AM pipeline, we accept Hypothesis H2."
        },
        {
            "heading": "5.3 Hypothesis H3",
            "text": "This hypothesis merges the Vertical and the Horizontal Integration into the Combined Integration to improve the accuracy for all tasks in the pipeline by also allowing intermediate data produced by ensemble methods. The results are shown in Table 4 with the boldfaced numbers indicating the tasks for which a new highest accuracy was achieved: S = 0.842 and C = 0.673 outperform the previous best single systems from BAM, but only the latter being statistically significantly different. This is in contrast to B = 0.483 and R = 0.019, where the former did not perform better, and the latter even lowered the score. Still, we have evidence that Combined Integration can be used to improve at least some tasks in the AM pipeline. Thus, we accept Hypothesis H3."
        },
        {
            "heading": "6 Limitations and Future Work",
            "text": "The major limitation of this work is that we implement post-hoc combinations. The reasons for not re-training the systems are two-fold. First, out of practicality to facilitate the addition of new AM systems and existing ensembles. Second, to set the scope of this research as opposed to works that look to explicitly fuse models such as neural networks by entangling the final classification layer such as described in Ribeiro et al. (2020). The latter opens up the future work of applying these techniques to the current five AM systems and mixing their latent representations, as opposed to only their outputs.\nAnother limitation is that all the included systems take a holistic view of the AM pipeline, and none is explicitly split into the four modules we infer for the ensemble methods. Given the success of (re-)combinations of components in other domains, this paper can, hence, be seen as a call to action to systematically explore the effectiveness of functional components of the AM pipeline and share these for re-use by others. Indeed, more broadly, the limited availability of AM systems and benchmark datasets hampered our ability to systematically compare a larger design space of system (component) combinations and limits the generalizability of our findings to other domains/datasets.\nThe plans for future efforts in this direction include two main points. As the next step, we aim to conduct an error analysis and explore the influences of the systems on the results. This will help identify the strengths and weaknesses of the individual systems and may provide insights about current\nAM systems\u2019 common weaknesses. Also, the new analysis can incorporate the spatial and temporal costs of the recombinations, which was omitted in this paper. In the future, should the number of argument mining systems considerably increase, the framework could be extended to include a predictor to choose the sets and sequences of argument miners for a given document that lead to an optimal accuracy improvement. This would involve developing a cost function."
        },
        {
            "heading": "7 Conclusions",
            "text": "This paper presented DREAM, a framework for the Deployment of Recombination and Ensemble methods for Argument Miners. Our work focuses on improving accuracy in Argument Mining (AM) and addresses the need for incremental improvements as opposed to current approaches, which tend to provide all-new solutions (Lawrence and Reed, 2019). With the DREAM framework, we implemented a flexible and automated approach for (re)combining AM systems. It offers the Vertical Integration (using ensemble methods for a single task), the Horizontal Integration (using different systems throughout the pipeline), and, finally, the Combined Integration (allowing sets of systems for the intermediate data).\nOur findings confirmed the hypotheses formulated in this work. We showed that ensemble methods (Opitz and Maclin, 1999) could be used to improve accuracy for specific tasks in the AM pipeline. Furthermore, we demonstrated that recombination by using intermediate data from the most accurate system could lead to higher accuracy in the subsequent task. Finally, we highlighted the potential of deploying ensemble methods and recombination for AM. We hope this work will contribute to the further improvement of state-ofthe-art and better generalizing AM systems across domains, a prevalent and well-acknowledged problem (Lippi and Torroni, 2016a)."
        },
        {
            "heading": "Acknowledgments",
            "text": "This research was partially funded by the Swiss National Science Foundation (SNSF) under Project \u201cCrowdAlytics\u201d (Grant Number 184994). The authors would also like to thank Luca Rossetto for his inputs and the anonymous reviewers for their constructive feedback."
        },
        {
            "heading": "A Appendix",
            "text": "A.1 Statistical Significance Testing\nWe report the p-values where we claim to outperform a previously best result (indicated in bold in the original tables) in Table 5, Table 6, and Table 7. Bold numbers indicate statistically significant differences (p < 0.05), while * denotes small, ** medium, and *** large effect size. The testing for the statistical significance of the results was conducted with Autorank (Herbold, 2020). The detailed reports on the conducted tests for statistical significance, including all procedures and assumptions testing, are shown below.\nS for TARGER, AURC, MARGOT, and TRABAM (Stacking) vs. TRABAM The statistical analysis was conducted for 2 populations with 12 paired samples. The family-wise significance level of the tests is alpha=0.050. We failed to reject the null hypothesis that the population is normal for all populations (minimal observed p-value=0.328). Therefore, we assume that all populations are normal. No check for homogeneity was required because we only have two populations. Because we have only two populations and both populations are normal, we use the t-test to determine differences between the mean values of the populations and report the mean value (M) and the standard deviation (SD) for each population. We failed to reject the null hypothesis (p=0.180) of the paired t-test that the mean values of the populations Ctrabam-test (M=0.834+-0.041, SD=0.054) and SBS-targer+aurc+margot+trabam (M=0.838+-0.039, SD=0.052) are are equal. Therefore, we assume that there is no statistically significant difference between the mean values of the populations.\nC for TRABAM and TARGER (Voting) vs. TRABAM The statistical analysis was conducted for 2 populations with 12 paired samples. The family-wise significance level of the tests is alpha=0.050. We rejected the null hypothesis that the population is normal for the population C-Vtrabam+targer (p=0.024). Therefore, we assume that not all populations are normal. No check for homogeneity was required because we only have two populations. Because we have only two populations and one of them is not normal, we use Wilcoxon\u2019s signed rank test to determine the differences in the central tendency and report the median (MD) and the median absolute deviation (MAD) for each population. We reject the null hypothesis (p=0.021) of Wilcoxon\u2019s signed rank test that population C-trabam-test (MD=0.671+-0.062, MAD=0.038) is not greater than population C-Vtrabam+targer (MD=0.691+-0.048, MAD=0.017). Therefore, we assume that the median of C-Vtrabam+targer is significantly larger than the median value of C-trabam-test with a small effect size (gamma=-0.461).\nB for TRABAM and AURC (Recombination) vs. AURC The statistical analysis was conducted for 2 populations with 12 paired samples. The family-wise significance level of the tests is alpha=0.050. We failed to reject the null hypothesis that the population is normal for all populations (minimal observed p-value=0.052). Therefore, we assume that all populations are normal. No check for homogeneity was required because we only have two populations. Because we have only two populations and both populations are normal, we use the t-test to determine differences between the mean values of the populations and report the mean value (M) and the standard deviation (SD) for each population. We reject the null hypothesis (p=0.000) of the paired t-test that the mean values of the populations B-aurc-test (M=0.028+-0.009, SD=0.012) and SB-R-(trabam)+(aurc) (M=0.488+0.036, SD=0.047) are equal. Therefore, we assume that the mean value of SB-R-(trabam)+(aurc) is significantly larger than the mean value of B-aurc-test with a large effect size (d=-13.252).\nB for TRABAM and TARGER (Recombination) vs. TARGER The statistical analysis was conducted for 2 populations with 12 paired samples. The family-wise significance level of the tests is alpha=0.050. We failed to reject the null hypoth-\nesis that the population is normal for all populations (minimal observed p-value=0.189). Therefore, we assume that all populations are normal. No check for homogeneity was required because we only have two populations. Because we have only two populations and both populations are normal, we use the t-test to determine differences between the mean values of the populations and report the mean value (M) and the standard deviation (SD) for each population. We reject the null hypothesis (p=0.018) of the paired t-test that the mean values of the populations C-targer-test (M=0.485+-0.047, SD=0.063) and C-R-(trabam)+(targer) (M=0.504+0.042, SD=0.056) are equal. Therefore, we assume that the mean value of C-R-(trabam)+(targer) is significantly larger than the mean value of C-targertest with a small effect size (d=-0.313).\nB for TRABAM and ArguminSci (Recombination) vs. ArguminSci The statistical analysis was conducted for 2 populations with 12 paired samples. The family-wise significance level of the tests is alpha=0.050. We failed to reject the null hypothesis that the population is normal for all populations (minimal observed p-value=0.120). Therefore, we assume that all populations are normal. No check for homogeneity was required because we only have two populations. Because we have only two populations and both populations are normal, we use the t-test to determine differences between the mean values of the populations and report the mean value (M) and the standard deviation (SD) for each population. We reject the null hypothesis (p=0.000) of the paired t-test that the mean values of the populations C-arguminsci-test (M=0.102+0.013, SD=0.018) and C-R-(trabam)+(arguminsci) (M=0.287+-0.035, SD=0.047) are equal. There-\nfore, we assume that the mean value of C-R(trabam)+(arguminsci) is significantly larger than the mean value of C-arguminsci-test with a large effect size (d=-5.227).\nC for TRABAM and ArguminSci (Recombination) vs. ArguminSci The statistical analysis was conducted for 2 populations with 12 paired samples. The family-wise significance level of the tests is alpha=0.050. We failed to reject the null hypothesis that the population is normal for all populations (minimal observed p-value=0.681). Therefore, we assume that all populations are normal. No check for homogeneity was required because we only have two populations. Because we have only two populations and both populations are normal, we use the t-test to determine differences between the mean values of the populations and report the mean value (M) and the standard deviation (SD) for each population. We reject the null hypothesis (p=0.000) of the paired t-test that the mean values of the populations C-arguminsci-test (M=0.093+0.016, SD=0.021) and C-R-(trabam)+(arguminsci) (M=0.344+-0.040, SD=0.054) are equal. Therefore, we assume that the mean value of C-R(trabam)+(arguminsci) is significantly larger than the mean value of C-arguminsci-test with a large effect size (d=-6.140).\nB for MARGOT and TRABAM (Recombination) vs. MARGOT The statistical analysis was conducted for 2 populations with 12 paired samples. The family-wise significance level of the tests is alpha=0.050. We failed to reject the null hypothesis that the population is normal for all populations (minimal observed p-value=0.133). Therefore, we assume that all populations are normal. No check for homogeneity was required because we\nonly have two populations. Because we have only two populations and both populations are normal, we use the t-test to determine differences between the mean values of the populations and report the mean value (M) and the standard deviation (SD) for each population. We reject the null hypothesis (p=0.000) of the paired t-test that the mean values of the populations C-margot-test (M=0.098+0.014, SD=0.019) and C-R-(trabam)+(margot) (M=0.171+-0.020, SD=0.026) are equal. Therefore, we assume that the mean value of C-R(trabam)+(margot) is significantly larger than the mean value of C-margot-test with a large effect size (d=-3.210).\nC for MARGOT and TRABAM (Recombination) vs. MARGOT The statistical analysis was conducted for 2 populations with 12 paired samples. The family-wise significance level of the tests is alpha=0.050. We failed to reject the null hypothesis that the population is normal for all populations (minimal observed p-value=0.347). Therefore, we assume that all populations are normal. No check for homogeneity was required because we only have two populations. Because we have only two populations and both populations are normal, we use the t-test to determine differences between the mean values of the populations and report the mean value (M) and the standard deviation (SD) for each population. We reject the null hypothesis (p=0.000) of the paired t-test that the mean values of the populations C-margot-test (M=0.135+0.031, SD=0.042) and C-R-(trabam)+(margot)\n(M=0.164+-0.029, SD=0.039) are equal. Therefore, we assume that the mean value of C-R(trabam)+(margot) is significantly larger than the mean value of C-margot-test with a medium effect size (d=-0.743).\nS for TARGER, AURC, MARGOT, and TRABAM (Stacking) vs. TRABAM The statistical analysis was conducted for 2 populations with 12 paired samples. The family-wise significance level of the tests is alpha=0.050. We failed to reject the null hypothesis that the population is normal for all populations (minimal observed p-value=0.328). Therefore, we assume that all populations are normal. No check for homogeneity was required because we only have two populations. Because we have only two populations and both populations are normal, we use the t-test to determine differences between the mean values of the populations and report the mean value (M) and the standard deviation (SD) for each population. We failed to reject the null hypothesis (p=0.180) of the paired t-test that the mean values of the populations Ctrabam-test (M=0.834+-0.041, SD=0.054) and SBS-targer+aurc+margot+trabam (M=0.838+-0.039, SD=0.052) are are equal. Therefore, we assume that there is no statistically significant difference between the mean values of the populations.\nC for (TRABAM and TARGER (Voting)) and TRABAM (Recombination) vs. TRABAM The statistical analysis was conducted for 2 populations with 12 paired samples. The family-wise significance level of the tests is alpha=0.050. We\nrejected the null hypothesis that the population is normal for the population C-R-(trabam)+(C-Vtrabam+targer) (p=0.024). Therefore, we assume that not all populations are normal. No check for homogeneity was required because we only have two populations. Because we have only two populations and one of them is not normal, we use Wilcoxon\u2019s signed rank test to determine the differences in the central tendency and report the median (MD) and the median absolute deviation (MAD) for each population. We reject the null hypothesis (p=0.021) of Wilcoxon\u2019s signed rank test that population C-trabam-test (MD=0.671+0.062, MAD=0.038) is not greater than population C-R-(trabam)+(C-V-trabam+targer) (MD=0.691+0.048, MAD=0.017). Therefore, we assume that the median of C-R-(trabam)+(C-V-trabam+targer) is significantly larger than the median value of C-trabam-test with a small effect size (gamma=0.461).\nA.2 Multiple Comparisons Problem With the results from the tests above, we correct for the multiple comparisons problem by using the Benjamini-Hochberg procedure (Benjamini and Hochberg, 1995) with a critical value of \u03b1 = 0.05. Table 8 shows the details of the calculations. From it, we see that all the differences are still statistically significant, even after correcting for the multiple comparisons problem."
        }
    ],
    "title": "DREAM: Deployment of Recombination and Ensembles in Argument Mining",
    "year": 2023
}