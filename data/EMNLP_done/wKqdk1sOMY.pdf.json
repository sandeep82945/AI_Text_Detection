{
    "abstractText": "To extend the scope of coding queries to more realistic settings, we propose ODEX, the first Open-Domain EXecution-based natural language (NL) to Python code generation dataset. ODEX has 945 NL-Code pairs spanning 79 diverse libraries, along with 1,707 humanwritten test cases for execution. Our NL-Code pairs are harvested from StackOverflow forums to encourage natural and practical coding queries. Moreover, ODEX supports four natural languages as intents, in English, Spanish, Japanese, and Russian. ODEX unveils intriguing behavioral differences among topperforming code language models (LM). While CODEX achieves better overall results, CODEGEN improves effectively via scaling \u2013 CODEGEN 6.1B performs comparably with CODEX 12B. Both models show substantial gaps between open and closed domains, but CODEGEN gaps tend to decrease with model size while CODEX gaps increase. We release ODEX to facilitate research into open-domain problems for the code generation community.1",
    "authors": [
        {
            "affiliations": [],
            "name": "Zhiruo Wang"
        },
        {
            "affiliations": [],
            "name": "Shuyan Zhou"
        },
        {
            "affiliations": [],
            "name": "Daniel Fried"
        },
        {
            "affiliations": [],
            "name": "Graham Neubig"
        }
    ],
    "id": "SP:1959fe5a0f28a5c4ff31bc65e59fc67e9d825436",
    "references": [
        {
            "authors": [
                "Rajas Agashe",
                "Srinivasan Iyer",
                "Luke Zettlemoyer."
            ],
            "title": "Juice: A large scale distantly supervised dataset for open domain context-based code generation",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing",
            "year": 2019
        },
        {
            "authors": [
                "Loubna Ben Allal",
                "Raymond Li",
                "Denis Kocetkov",
                "Chenghao Mou",
                "Christopher Akiki",
                "Carlos Munoz Ferrandis",
                "Niklas Muennighoff",
                "Mayank Mishra",
                "Alex Gu",
                "Manan Dey"
            ],
            "title": "Santacoder: don\u2019t reach for the stars",
            "year": 2023
        },
        {
            "authors": [
                "Jacob Austin",
                "Augustus Odena",
                "Maxwell Nye",
                "Maarten Bosma",
                "Henryk Michalewski",
                "David Dohan",
                "Ellen Jiang",
                "Carrie Cai",
                "Michael Terry",
                "Quoc Le",
                "Charles Sutton"
            ],
            "title": "Program synthesis with large language models",
            "year": 2021
        },
        {
            "authors": [
                "Satanjeev Banerjee",
                "Alon Lavie."
            ],
            "title": "METEOR: An automatic metric for MT evaluation with improved correlation with human judgments",
            "venue": "Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Transla-",
            "year": 2005
        },
        {
            "authors": [
                "Shubham Chandel",
                "Colin B. Clement",
                "Guillermo Serrato",
                "Neel Sundaresan"
            ],
            "title": "Training and evaluating a jupyter notebook data science assistant",
            "year": 2022
        },
        {
            "authors": [
                "Bei Chen",
                "Fengji Zhang",
                "Anh Nguyen",
                "Daoguang Zan",
                "Zeqi Lin",
                "Jian-Guang Lou",
                "Weizhu Chen."
            ],
            "title": "Codet: Code generation with generated tests",
            "venue": "International Conference on Learning Representations.",
            "year": 2023
        },
        {
            "authors": [
                "Morikawa",
                "Alec Radford",
                "Matthew Knight",
                "Miles Brundage",
                "Mira Murati",
                "Katie Mayer",
                "Peter Welinder",
                "Bob McGrew",
                "Dario Amodei",
                "Sam McCandlish",
                "Ilya Sutskever",
                "Wojciech Zaremba"
            ],
            "title": "Evaluating large language models trained on code",
            "year": 2021
        },
        {
            "authors": [
                "Naihao Deng",
                "Shuaichen Chang",
                "Peng Shi",
                "Tao Yu",
                "Rui Zhang"
            ],
            "title": "Prefix-to-sql: Text-to-sql generation from incomplete user questions",
            "year": 2021
        },
        {
            "authors": [
                "Li Dong",
                "Mirella Lapata."
            ],
            "title": "Language to logical form with neural attention",
            "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 33\u201343, Berlin, Germany. Association for Computa-",
            "year": 2016
        },
        {
            "authors": [
                "Rotem Dror",
                "Gili Baumer",
                "Segev Shlomov",
                "Roi Reichart."
            ],
            "title": "The hitchhiker\u2019s guide to testing statistical significance in natural language processing",
            "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume",
            "year": 2018
        },
        {
            "authors": [
                "Mikhail Evtikhiev",
                "Egor Bogomolov",
                "Yaroslav Sokolov",
                "Timofey Bryksin"
            ],
            "title": "Out of the bleu: how should we assess quality of the code generation models",
            "venue": "Journal of Systems and Software,",
            "year": 2023
        },
        {
            "authors": [
                "Leo Gao",
                "Stella Biderman",
                "Sid Black",
                "Laurence Golding",
                "Travis Hoppe",
                "Charles Foster",
                "Jason Phang",
                "Horace He",
                "Anish Thite",
                "Noa Nabeshima",
                "Shawn Presser",
                "Connor Leahy"
            ],
            "title": "The pile: An 800gb dataset of diverse text for language modeling",
            "year": 2020
        },
        {
            "authors": [
                "Patrick Haluptzok",
                "Matthew Bowers",
                "Adam Tauman Kalai"
            ],
            "title": "Language models can teach themselves to program better",
            "year": 2023
        },
        {
            "authors": [
                "Dan Hendrycks",
                "Steven Basart",
                "Saurav Kadavath",
                "Mantas Mazeika",
                "Akul Arora",
                "Ethan Guo",
                "Collin Burns",
                "Samir Puranik",
                "Horace He",
                "Dawn Song",
                "Jacob Steinhardt"
            ],
            "title": "Measuring coding challenge competence with apps",
            "year": 2021
        },
        {
            "authors": [
                "Ari Holtzman",
                "Jan Buys",
                "Li Du",
                "Maxwell Forbes",
                "Yejin Choi"
            ],
            "title": "The curious case of neural text degeneration",
            "year": 2020
        },
        {
            "authors": [
                "Junjie Huang",
                "Chenglong Wang",
                "Jipeng Zhang",
                "Cong Yan",
                "Haotian Cui",
                "Jeevana Priya Inala",
                "Colin Clement",
                "Nan Duan."
            ],
            "title": "Execution-based evaluation for data science code generation models",
            "venue": "Proceedings of the Fourth Workshop on Data Science with Human-",
            "year": 2022
        },
        {
            "authors": [
                "Srinivasan Iyer",
                "Ioannis Konstas",
                "Alvin Cheung",
                "Luke Zettlemoyer."
            ],
            "title": "Mapping language to code in programmatic context",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1643\u20131652, Brussels, Bel-",
            "year": 2018
        },
        {
            "authors": [
                "Yuhang Lai",
                "Chengxi Li",
                "Yiming Wang",
                "Tianyi Zhang",
                "Ruiqi Zhong",
                "Luke Zettlemoyer",
                "Scott Wen tau Yih",
                "Daniel Fried",
                "Sida Wang",
                "Tao Yu"
            ],
            "title": "Ds-1000: A natural and reliable benchmark for data science code generation",
            "year": 2022
        },
        {
            "authors": [
                "Yujia Li",
                "David Choi",
                "Junyoung Chung",
                "Nate Kushman",
                "Julian Schrittwieser",
                "R\u00e9mi Leblond",
                "Tom Eccles",
                "James Keeling",
                "Felix Gimeno",
                "Agustin Dal Lago"
            ],
            "title": "Competition-level code generation with alphacode",
            "year": 2022
        },
        {
            "authors": [
                "Chin-Yew Lin."
            ],
            "title": "ROUGE: A package for automatic evaluation of summaries",
            "venue": "Text Summarization Branches Out, pages 74\u201381, Barcelona, Spain. Association for Computational Linguistics.",
            "year": 2004
        },
        {
            "authors": [
                "Chin-Yew Lin",
                "Franz Josef Och."
            ],
            "title": "Automatic evaluation of machine translation quality using longest common subsequence and skip-bigram statistics",
            "venue": "Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL-",
            "year": 2004
        },
        {
            "authors": [
                "Stephan Lukasczyk",
                "Gordon Fraser."
            ],
            "title": "Pynguin: Automated unit test generation for python",
            "venue": "International Conference on Software Engineering: Companion Proceedings.",
            "year": 2022
        },
        {
            "authors": [
                "Erik Nijkamp",
                "Bo Pang",
                "Hiroaki Hayashi",
                "Lifu Tu",
                "Huan Wang",
                "Yingbo Zhou",
                "Silvio Savarese",
                "Caiming Xiong."
            ],
            "title": "Codegen: An open large language model for code with multi-turn program synthesis",
            "venue": "International Conference on Learning Representa-",
            "year": 2023
        },
        {
            "authors": [
                "Kishore Papineni",
                "Salim Roukos",
                "Todd Ward",
                "WeiJing Zhu."
            ],
            "title": "Bleu: a method for automatic evaluation of machine translation",
            "venue": "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311\u2013318, Philadelphia,",
            "year": 2002
        },
        {
            "authors": [
                "Hammond Pearce",
                "Baleegh Ahmad",
                "Benjamin Tan",
                "Brendan Dolan-Gavitt",
                "Ramesh Karri"
            ],
            "title": "Asleep at the keyboard? assessing the security of github copilot\u2019s code contributions",
            "year": 2021
        },
        {
            "authors": [
                "Maja Popovi\u0107."
            ],
            "title": "chrF: character n-gram F-score for automatic MT evaluation",
            "venue": "Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 392\u2013395, Lisbon, Portugal. Association for Computational Linguistics.",
            "year": 2015
        },
        {
            "authors": [
                "Shuo Ren",
                "Daya Guo",
                "Shuai Lu",
                "Long Zhou",
                "Shujie Liu",
                "Duyu Tang",
                "Neel Sundaresan",
                "Ming Zhou",
                "Ambrosio Blanco",
                "Shuai Ma"
            ],
            "title": "Codebleu: a method for automatic evaluation of code synthesis",
            "year": 2020
        },
        {
            "authors": [
                "Michele Tufano",
                "Dawn Drain",
                "Alexey Svyatkovskiy",
                "Shao Kun Deng",
                "Neel Sundaresan"
            ],
            "title": "Unit test case generation with transformers and focal context",
            "year": 2021
        },
        {
            "authors": [
                "Morteza Verdi",
                "Ashkan Sami",
                "Jafar Akhondali",
                "Foutse Khomh",
                "Gias Uddin",
                "Alireza Karami Motlagh."
            ],
            "title": "An empirical study of c++ vulnerabilities in crowd-sourced code examples",
            "venue": "IEEE Transactions on Software Engineering.",
            "year": 2020
        },
        {
            "authors": [
                "Eric Wallace",
                "Tony Zhao",
                "Shi Feng",
                "Sameer Singh."
            ],
            "title": "Concealed data poisoning attacks on NLP models",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech-",
            "year": 2021
        },
        {
            "authors": [
                "Zhiruo Wang",
                "Grace Cuenca",
                "Shuyan Zhou",
                "Frank F. Xu",
                "Graham Neubig."
            ],
            "title": "MCoNaLa: A benchmark for code generation from multiple natural languages",
            "venue": "Findings of the Association for Computational Linguistics: EACL 2023, pages 265\u2013273,",
            "year": 2023
        },
        {
            "authors": [
                "Pengcheng Yin",
                "Bowen Deng",
                "Edgar Chen",
                "Bogdan Vasilescu",
                "Graham Neubig."
            ],
            "title": "Learning to mine aligned code and natural language pairs from stack overflow",
            "venue": "International Conference on Mining Software Repositories, pages 476\u2013486. IEEE.",
            "year": 2018
        },
        {
            "authors": [
                "Pengcheng Yin",
                "Graham Neubig."
            ],
            "title": "TRANX: A transition-based neural abstract syntax parser for semantic parsing and code generation",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstra-",
            "year": 2018
        },
        {
            "authors": [
                "Victor Zhong",
                "Caiming Xiong",
                "Richard Socher"
            ],
            "title": "Seq2sql: Generating structured queries from natural language using reinforcement learning",
            "year": 2017
        },
        {
            "authors": [
                "Hendrycks"
            ],
            "title": "2021) or specific libraries of interest (Lai et al., 2022",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Evaluations of NL-to-code generation systems, especially for general-purpose programming languages such as Python, have put an increasing emphasis on methods that execute code to verify the results. The predominant approach for creating such test sets is to manually write test cases for canonical code solutions (Chen et al., 2021; Austin et al., 2021; Lai et al., 2022; Huang et al., 2022). The correctness of model predictions is then evaluated by seeing if generated code passes the test cases (Chen et al., 2021). Compared to execution-free metrics such as text match against reference solutions, execution-based methods more rigorously assess the functional correctness of code (Hendrycks et al., 2021; Chen et al., 2021).\n1https://anonymous.4open.science/r/odex-emnlp\nHowever, most resources with execution support only apply to closed-domain code, that only use Python built-in functions (Chen et al., 2021; Hendrycks et al., 2021; Austin et al., 2021; Li et al., 2022; Haluptzok et al., 2023) or specific libraries in data science domains (Lai et al., 2022; Huang et al., 2022). This focus on closed-domain problems diverges substantially from natural open-domain program usage covering a diverse range of libraries and functionalities (Yin et al., 2018; Agashe et al., 2019; Wang et al., 2023). To enable executionbased evaluation for coding queries using libraries, we present ODEX, an Open-Domain EXecutionbased dataset (\u00a72). We build ODEX by creating 1,707 test cases for 945 NL-Code pairs from the CoNaLa (Yin et al., 2018) and MCoNaLa (Wang et al., 2023) datasets, both stemming from StackOverflow2 with broad practical coding queries.\nWe analyze and highlight three aspects of ODEX (\u00a73). First, ODEX has broad domain coverage of 79 libraries, with 53.4% of the problems employing at least one library. Second, ODEX contains queries in four different languages, with 439, 90, 164, and 252 samples in English, Spanish, Japanese, and Russian, as shown in Figure 1. Third, ODEX addresses three unique challenges in open-domain code execution: irreproducible runs (Figure 1 a ), randomized outputs (Figure 1 b ), and specialized equivalence checks (Figure 2).\nWe evaluate two state-of-the-art code LLM families, CODEX and CODEGEN, on ODEX (\u00a75). Our study shows that larger model sizes and augmented training data improve execution accuracy. Meanwhile, we observe satisfactory multilingual capabilities, despite that neither model was specifically designed for multilingual usage. However, we find that models face greater yet varied challenges with open-domain queries compared to closed-domain queries (\u00a75). Specifically, CODEX achieves higher\n2https://stackoverflow.com\nqueries is often more challenging: a requires simulated execution due to the difficulty of reproduction; b is verified through approximate equivalence. Prior work focuses more on basic assertions, as in c and d .\noverall results, while CODEGEN presents better parameter efficiency and more balanced open-closed domain performance as model size scales up. By comparing execution-based metric with a series of execution-free metrics (\u00a76), we further confirm the advantage of execution on allowing alternative solutions, but also show the potential of lexical metrics to identify simple bug fixes.\nODEX jointly facilitates practical open-domain code generation and execution-based evaluation. It serves as a comprehensive data benchmark for NL-to-code systems, supporting diverse NL contexts, library usage, and evaluation methods. By addressing the unique challenges of test creation and execution, we hope to lay a foundation for evaluating open-domain code via execution."
        },
        {
            "heading": "2 The ODEX Dataset",
            "text": "In this section, we describe our four-step process of constructing the ODEX dataset. We first collect resources of natural, open-domain coding queries (\u00a72.1). Next, we establish the annotation standard and procedures for test case creation (\u00a72.2). We then describe the annotator hiring and working processes (\u00a72.3). Finally, we conduct checks to ensure data quality (\u00a72.4)."
        },
        {
            "heading": "2.1 Resource Collection",
            "text": "We take two NL-to-code datasets, CoNaLa (Yin et al., 2018) and MCoNaLa (Wang et al., 2023), as sources for ODEX. We refer to them together as (M)CoNaLa. Their NL-Code pairs are collected from StackOverflow, which contains abundant coding queries that (1) naturally reflect practical program usage, and (2) cover diverse domains as measured by libraries used. These properties align well with our main focus on open-domain queries. (M)CoNaLa further proofs and clarifies its NL intents using human annotators to ensure data quality."
        },
        {
            "heading": "2.2 Annotation Standard and Procedures",
            "text": "Given each source NL-Code pair, our main annotation task is to write test cases to check code execution correctness, as illustrated by the four steps in Figure 2. A qualified test case should verify the main functionality of the canonical code solution. In the case where annotators do not understand the language of the intent, we use translation tools such as the Google Translate API.3\nStep 1: Wrapping Snippets into Functions Code solutions in (M)CoNaLa are often short snippets (e.g., x = np.zeros(5)) to ensure more pre-\n3https://translate.google.com\ncise matches with NL intents, but to be executable they often need additional context such as variable assignments. We therefore wrap code into standalone functions by specifying input and output arguments as contexts. For example, Step 1 in Figure 2 identifies variable a as an input argument.\nStep 2: Specifying Library Prerequisites Due to the open-domain coverage of (M)CoNaLa, some code snippets require extra library imports to execute correctly. Accordingly, our second step is to specify the prerequisite libraries for code solutions.\nStep 3: Test Case Annotation Next, we write test cases that contain three parts: (1) input: passing values to input arguments, (2) output: stating expected execution outputs, and (3) assertion: checking if execution results match the expected outputs.\nHowever, test case creation for open-domain code faces three challenges. First, safe and reproducible execution can be hard to achieve. As in Figure 1 a , it is impractical to send an HTTP request when evaluating this sample. Instead, we use mock to simulate the output (a success response status code 200). Second, some codes entail randomness (e.g., random.randint(3,5)) and have no definite value. We instead make bounding assertions, e.g., checking that all elements are integers within the range of [3,5]. Third, standard equivalence checks by == may be invalid, since library-specific objects often require specialized equality checks. For example, checking the equivalence of two NumPy arrays a and b uses np.array_equal(a,b), while a == b would cause execution errors.\nStep 4: Self Verification In the last step, we perform self-verification to efficiently ensure the annotation quality. We execute the canonical code solution on each newly created test case. Unless the\ntest case enables a successful pass of the solution, it should not be taken as a valid annotation."
        },
        {
            "heading": "2.3 Annotator Hiring and Task Fulfillment",
            "text": "As our data involves diverse functionalities from multiple libraries, our annotation task holds a relatively high standard for annotators. A qualified annotator should be proficient in Python and common libraries, and in writing workable test cases.\nWe chose to hire undergraduate students who have strong computer science backgrounds in Python. Of the 20 applicants who applied, we first conducted a resume screening to filter candidates with sufficient programming experience. Next, we gave each candidate an annotation test with five randomly selected NL-Code pairs. Since the test mirrors the official annotation process, we provided clear instructions about each step (as in \u00a72.2) and code scripts for self-verification. Candidates were asked to finish their tests in three calendar days. Based on their test performance, we hired four candidates to officially participate in this job."
        },
        {
            "heading": "2.4 Quality Check",
            "text": "We put great effort into ensuring data quality throughout the annotation process. To assist annotators in more efficiently and accurately writing workable test cases, we require them to execute each written test case using the verification code that we provided, and explicitly report whether the canonical code solution can successfully pass all the annotated test cases that they created.\nAfter the annotation, the authors performed posthoc verification to check if each test case reads reasonably and executes correctly. In our final rounds of automatic quality checks, we confirm that the pass rate for all canonical code solutions over their annotated test cases is 100%.\nWe collect a total of 945 samples with NLs in four languages, including 439 samples in English, 90 in Spanish, 164 in Japanese, and 252 in Russian."
        },
        {
            "heading": "3 Dataset Analysis",
            "text": "We analyze ODEX from three aspects: domain diversity (\u00a73.1), sample complexity (\u00a73.2), and execution support (\u00a73.3)."
        },
        {
            "heading": "3.1 Diversity",
            "text": "One unique property of ODEX is its broad domain coverage. We categorize codes that entail library usage (both built-in and third-party) as being in the\nopen domain and those with none in the closed domain. Different libraries often serve specific functions and have unique capabilities. For instance, the datetime library is designed to handle date/time operations, while other libraries focus on various other fields such as data analysis or web requests. Therefore, in this work, we view the diversity in libraries as a representation of distinct domains.\nTable 1 reports domain statistics and Figure 3 shows the library distribution. ODEX covers a diverse set of 79 libraries, which varies per language. Most samples, 53.4%, use at least one library.\n0% 25% 50% 75%\nComparison to Existing Datasets We compare ODEX with eight other code generation datasets that support test case execution: HumanEval (Chen et al., 2021), MBPP (Austin et al., 2021), APPS (Hendrycks et al., 2021), MTPB (Nijkamp et al., 2023), P3 (Haluptzok et al., 2023), DSP (Chandel et al., 2022), DS-1000 (Lai et al., 2022), and ExeDS (Huang et al., 2022).\nFrom their distributions in Figure 4, six out of eight datasets focus on the closed domain and most\nexamples use zero libraries. Such examples deviate from realistic programs, which often use APIs of different libraries. DS-1000 and Exe-DS feature some open-domain problems, but their library usage is more homogeneous with a particular focus on data science domains. Moreover, DS-1000 restricts to code using libraries but only has seven libraries. In contrast, ODEX is more \u201ccolorful\u201d; it covers significantly more open-domain libraries, as well as frequent queries in the closed domain.\nComparison to Natural Distribution To provide a reference on natural domain distribution, we approximate real-world usage by counting GitHub Python files that use each library. As shown in Figure 5, ODEX presents a better alignment with the practical scenario concerning the open domains \u2013 it features more diverse domains and preserves the long-tailed pattern in practical scenarios.\nThe full lists of libraries and their frequencies about ODEX, the eight comparison datasets, and the approximated natural setting are in \u00a7A.1.\n0% 25% 50% 75%\nnone pandas numpy re os collections matplotlib datetime urllib sys random io json 67 more\n25% 50% 75%\nnone pandas numpy re os collections matplotlib datetime urllib sys random io json 67 more\nFigure 5: Approximated natural distribution based on GitHub Python files in the open domain."
        },
        {
            "heading": "3.2 Complexity",
            "text": "To measure dataset complexity, we first calculate the lengths of NL intents and code snippets. We tokenize NL intents with the spaCy4 tokenizers in respective languages; we follow Yin and Neubig (2018) to tokenize code. For code, we also parse the AST tree using the Python standard ast library,5 and count the number of input and output variables to quantify the complexity of execution contexts.\nIn Table 2, we see that code in the Spanish set is longer on average than other languages. For both the input and output sides, code in the English set has fewer variables, suggesting potentially simpler\n4https://spacy.io/ 5https://docs.python.org/3/library/ast.html\nexecution environments, which could stem from relative simplicity of SO queries asked in English."
        },
        {
            "heading": "3.3 Execution Support",
            "text": "We systematically compare code generation datasets that concern execution or open-domain code in Table 3. ODEX is the first dataset that supports execution-based evaluation for open-domain code. While ODEX does not have the largest number of test cases, we discuss in \u00a77 how these test cases can still reliably measure code correctness."
        },
        {
            "heading": "4 Experiment Setup",
            "text": "Code LLMs have achieved strong results on multiple code generation tasks, yet their open-domain proficiency is understudied due to the limited domain settings of past datasets. To examine model capabilities in the open domain, we evaluate two top-performing model families, CODEX and CODEGEN, on ODEX. We perform evaluations using a prompting setting, without finetuning any model.\nWe introduce the baseline models, the prompt settings, and lay out the metrics for evaluation.\nThe CODEX Family At the time of this work, CODEX had three publicly available models. CODECUSHMAN-001 (C1) is a 12B CODEX model in Chen et al. (2021). CODE-DAVINCI-001/002 (D1, D2) are two 175B GPT-3 models.6\nThe CODEGEN Family CODEGEN (Nijkamp et al., 2023) models are auto-regressive models trained on a combination of NL and code corpora, differing in model sizes (350M, 2.7B, 6.1B, 16.1B) and training data. Models are progressively trained\n6https://beta.openai.com/docs/ model-index-for-researchers\non THEPILE (Gao et al., 2020), BIGQUERY,7 and BIGPYTHON datasets are denoted as NL, MULTI, and MONO. The most powerful CODEGEN-16.1BMONO, performs similarly to CODE-CUSHMAN001 on the HumanEval and MTPB datasets.\nPrompt Design For fair comparison, we use the same prompt for both model families. While prompting with few-shot in-context examples may improve, our experiments do not always find this helpful for both models. Therefore, we report zeroshot results as baselines and leave few-shot results to \u00a77. Creating zero-shot prompts only requires content from the test sample. Following Chen et al. (2021), we construct prompts by concatenating function context and a docstring. A docstring includes the NL intent and optional unit tests (compared in \u00a77). Figure 6 shows an example prompt.\nEvaluation Metrics We follow Chen et al. (2021) and measure the execution accuracy using the pass@k metric, by computing the fraction of problems having at least one correct prediction within k samples. We also compare it with a series of execution-free metrics later in \u00a75.\nImplementation Details We follow Chen et al. (2021) and use nucleus sampling (Holtzman et al., 2020) with top-p set to 0.95 and temperature set to 0.8. We set outputs to a maximum of 512 tokens.\n7https://cloud.google.com/bigquery"
        },
        {
            "heading": "5 Experiment Results",
            "text": "We first present the overall performance of two model families on ODEX (\u00a75.1). Next, given the unique challenges of open-domain code, we study the variances between open- and closed-domain problems (\u00a75.2), and in individual domains (\u00a75.3)."
        },
        {
            "heading": "5.1 Baseline Performance",
            "text": "CODEX Results As in Table 4, aligning to existing works and our intuition, larger DAVINCI 175B models outperform the smaller CUSHMAN 12B model, and the 002 version improves over 001. This trend holds for all languages and all sampling sizes. Somewhat surprisingly, all models attain decent results on non-English problems, even though CODEX is not designed for multilingual use. This high accuracy on non-English problems suggests the multilingual potential of CODEX models.\nCODEGEN Results We report results of MONO models in Table 4 given their superior performance over NL and MULTI variants (Nijkamp et al., 2023). The pass rate increases as CODEGEN grows from 350M to 2.7B, and continues to increase in nonEnglish languages when further scaling to 6.1B. CODEGEN exhibits multilingual capacity, as its results on non-English subsets are close to that on English, and consistently increase during scaling.\nAlthough CODEX and CODEGEN have comparable performance on existing datasets such as HumanEval, ODEX effectively unveils the efficacy of CODEGEN on open-domain coding queries even with many fewer parameters, i.e., CODEGEN 6.1B yields similar pass@1 to the 176B CODEX DAVINCI-001 model, although not necessarily so when k increases. More detailed results (pass@k at 1 \u2264 k \u2264 10) for both models are in \u00a7B."
        },
        {
            "heading": "5.2 Open Domain versus Closed Domain",
            "text": "CODEX Results Figure 7 (left) shows pass@1 on open-domain and closed-domain. All CODEX models score much lower in open than in closed domain. Such large gaps hold across all languages, ranging from 4.34 in Spanish to 38.57 in Japanese on the best DAVINCI-002 model. Model upgrades (C1 \u2192 D1 \u2192 D2) do not always reduce the gaps. Gaps slightly shrink in Spanish, but increase in English and Japanese. While D2 performs the best, it also exhibits the most severe gaps. These findings suggest that common practices to improve LLMs may not address the complexities inherent in opendomain coding problems. It is hence imperative that more advanced strategies are employed.\nCODEGEN Results As shown in Figure 7 (right), CODEGEN also has substantial gaps between open and closed domains, however, smaller than CODEX gaps across all languages, by on average 6.0% points. As model size increases from 2.7B to 6.1B, the gaps reduce by about 6.3 points in English and 1.7 points in Spanish. This is in contrast to CODEX, which when scaling up to DAVINCI-002, these gaps continue to increase by 4.9 points on average, indicating that scaling up CODEGEN more effectively catches up on open-domain performance."
        },
        {
            "heading": "5.3 Domain Variance",
            "text": "We now dive deeper into the results within individual domains. We focus on the CODE-DAVINCI-002 model as it has the best performance across all models. In Figure 8, we plot accuracy with respect to the domain frequency, as approximated in \u00a73.1.\nExecution accuracy is not low on all open domains. For example, CODE-DAVINCI-002 achieves 50% pass@1 for several common libraries such as random and math. But high domain frequency does not ensure model proficiency. For example,\n35 50 65 80 p@1 p@2 p@3 p@4 p@5 p@6 p@7 p@8 p@9 p@10\nen / 1 en / n es / 1 es / n ja / 1 ja / n ru / 1 ru / n\n35\n50 65 80\np@1 p@2 p@3 p@4 p@5 p@6 p@7 p@8 p@9 p@10\nen / id en / const en / intent es / id es / const es / intent ja / id ja / const ja / intent ru / id ru / const ru / intent\non libraries with complex functionalities such as matplotlib and tensorflow, pass@1 can go below 10%. See \u00a7C for more domain-wise results."
        },
        {
            "heading": "6 Comparing to Execution-Free Metrics",
            "text": "In this section, we study the alignment between execution-based evaluation and five execution-free metrics, identifying advantages for both types.\nModel Ranking Using Different Metrics We evaluate models using five execution-free metrics using lexical, syntax, and semantic matches: BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), METEOR (Banerjee and Lavie, 2005), ChrF (Popovic\u0301, 2015), and CodeBLEU (Ren et al., 2020). Refer to \u00a7D.1 for more descriptions.\nWe analyze using CODEX, given its better per-\nformance. As shown in Figure 9, model rankings by execution-free metrics do not precisely correlate with their rankings by execution accuracy. Even when the rankings align, their differences are largely not proportional. Comparing the metrics, ChrF and METEOR have smaller inter-model variances, while BLEU and ROUGE change more and correlate better with pass rates. Notably, CodeBLEU is low in most settings and might not be suitable for evaluating code in snippet-style.\nMetric Correlation We next evaluate whether execution-free metrics might be used to discriminate between passed and failed samples. We take BLEU as an example since it shows similar ranking patterns to execution. Figure 10 shows negligible variances in BLEU scores of passed and failed groups. The other four metrics exhibit similar patterns, as could be found in \u00a7D.3."
        },
        {
            "heading": "7 What Affects Model Performance?",
            "text": "Besides differences in model configurations, we study three factors that might affect performance.\nNumber of In-Context Examples Models might benefit from example NL-Code pairs. We thus explore to few-shot prefixing N \u2208 {1, 2, 3} inputoutput pairs in prompts. In Figure 11 (left), for CUSHMAN-001 and DAVINCI-001, few-shot ex-\namples yield a clear improvement over the zeroshot setting; but for the strongest DAVINCI-002, it brings minimal gains in English. See similar results in other languages in \u00a7E.1.\nNumber of Test Cases in the Docstring Including test cases in inputs adds execution hints of the expected functionality of the solution, and hence may improve execution accuracy. We test this hypothesis by experimenting with prompts that have varying numbers of test cases. Besides the default setting with zero tests, we compare adding one random test case and all annotated test cases.\nFigure 11 (right) shows that injecting as few as one exemplar test case significantly improves the execution accuracy, yet adding more cases has little bonus. This potentially implies the sufficiency of one test case to show the main functionality.\nNumber of Evaluation Test Cases Execution results could be more reliable if using more test cases for evaluation. However, there is a trade-off between evaluation effectiveness and annotation efficiency, due to the high cost of human effort. To study this tradeoff, we observe how results change with respect to the number of tests. Compared to using all cases in default, we also try using one randomly selected case. For simplicity, we do not include any test cases in prompts.\nAs shown in Figure 12, evaluating over one random test largely preserves the accuracy of using all tests, indicating that one case is sufficient to test the main functionality for most queries. Check \u00a7E for analysis on other factors such as function naming."
        },
        {
            "heading": "8 Related Work",
            "text": "Open Domain Code Generation Programs often use APIs from different Python libraries. Some datasets preserve natural coverage from interactive Jupyter Notebooks (Agashe et al., 2019) or StackOverflow posts (Yin et al., 2018; Wang et al., 2023),\nbut face challenges in enabling execution (Lai et al., 2022; Chandel et al., 2022). Our ODEX dataset addresses execution for open-domain code."
        },
        {
            "heading": "Coding Queries vs. Programming Challenges",
            "text": "Some works stem from coding contest websites (Hendrycks et al., 2021; Li et al., 2022), but GitHub Jupyter Notebooks (Agashe et al., 2019; Huang et al., 2022) and StackOverflow (SO) (Yin et al., 2018; Wang et al., 2023; Lai et al., 2022) provide more natural and practical coding queries. We preserve this naturalness and incorporate various NL settings to assist programmers worldwide.\nExecution-based Evaluation Evaluation by execution has long been used for SQL (Zhong et al., 2017) or logical forms (Dong and Lapata, 2016). Many datasets have begun to support Python execution via test cases, however focus on built-in functions (Chen et al., 2021; Austin et al., 2021; Hendrycks et al., 2021) or specific domains (Lai et al., 2022; Huang et al., 2022). Our test cases, in contrast, cover diverse libraries in the open domain."
        },
        {
            "heading": "9 Conclusion",
            "text": "We present ODEX, an open-domain code generation dataset supporting execution-based evaluation via human-written test cases. ODEX not only supports execution-based evaluation of code using test cases, but also extends the task to the open domain, covering 79 diverse Python libraries and four natural languages (English, Spanish, Japanese, and Russian). Comparing two state-of-the-art code generation models, CODEX and CODEGEN, our dataset effectively unveils their varied behaviors between program domains and language contexts. ODEX serves as a comprehensive NL-to-code benchmark given its open-domain coverage, multi-natural language queries, and multi-metric support. When bringing code execution to open domain scenarios, our explorations also reveal emerging challenges in test creation and reliable execution, which we hope that our dataset will enable future work to tackle."
        },
        {
            "heading": "Limitations",
            "text": "ODEX aims to serve as a comprehensive testbed, by enabling execution-based evaluation of code in the open domain, with flexible intent inputs in four natural languages. However, we should hold continuous awareness of execution security, multilingual support, and evaluation reliability.\nFirst, execution supports in ODEX enables more rigorous evaluations than other execution-free methods. However, due to the increased complexity of open-domain codes, more inspections are required for execution safety, either for code solutions or test cases. We should always keep alert to avoid concealing malicious code (Wallace et al., 2021) or generating code with security vulnerabilities (Verdi et al., 2020; Pearce et al., 2021).\nSecond, in addition to English inputs, ODEX also includes intents specified in three other languages. Still, its language coverage is bounded by the available forums in StackOverflow. We hope our initiative can highlight the multilingual nature of program developers, encourage the emergence of similar data resources in other languages, and continuously promote AI programming assistance in languages worldwide.\nThird, as ODEX covers wide-ranging code queries in the open domain, it is more suitable for less resource-demanding scenarios such as downstream evaluation or few-shot learning. Although ODEX is larger than many previous datasets with human-written test cases, it is still limited due to the intense human effort required by the curation process. Regarding this, we encourage users of the dataset to conduct significance testing (Dror et al., 2018) and report more substantial model improvements."
        },
        {
            "heading": "Ethics Statement",
            "text": "Our work has received IRB approval and is licensed under a Creative Commons Attribution-ShareAlike (CC BY-SA) 4.0 International License. The resulting ODEX dataset is built to serve as a benchmark for open-domain code generation, to further facilitate technological advances in AI programming assistance, meanwhile supporting multiple languages to encourage its universal accessibility.\nWe strive to ensure high data quality and optimize annotation efficiency. We build the ODEX dataset with natural and practical StackOverflow resources and hire annotators with qualified programming proficiency. We provide our annotators with\nclearly documented instructions, flexible annotation interfaces (Google Sheets, Jupyter Notebooks), and self-verification tools. We (authors) conduct pilot annotation to confirm the clarity of annotation standards and feasibility of the annotation task. We conduct posthoc examinations on the annotation results, both manually and automatically, to obtain assured data quality (100% pass rate).\nWe respect the contribution and privacy of our annotators. We offer competitive remuneration for their annotation job and treat each one of them fairly. All annotators possess the right to withdraw at any time. We secure that all their personal information is removed before public release.\nWe conduct systematic analysis from multiple perspectives in the paper, in an attempt to foster public awareness on generating and evaluating programs in the open domain, both in encouraging more advances in this direction, and raising more concerns about the robustness and security of such unique coding problems."
        },
        {
            "heading": "A ODEX Dataset",
            "text": ""
        },
        {
            "heading": "A.1 Library Distribution Statistics",
            "text": "Aside from the illustrations in \u00a7 3.1, we list out the detailed statistics of libraries in ODEX, the eight comparison datasets, and the approximated natural distribution.\nODEX Domain Statistics Table 5 lists the number and percentage of occurrences for each library in the ODEX dataset."
        },
        {
            "heading": "ODEX Library Count Frequency Library Count Frequency",
            "text": "Domain Statistics of Comparison Datasets Table 6 lists the library frequency of eight comparison dataset mentioned in \u00a7 3: HumanEval, MBPP, APPS, MTPB, P3, DSP, DS-1000, and Exe-DS."
        },
        {
            "heading": "Approximated Natural Domain Distribution",
            "text": "To approximate the natural distribution of libraries in the open domain, we count the number of Python files on GitHub that imports the library of interest. Following the GitHub search syntax,8 we use the query import ${library_name} to search files that import a certain library, and use NOT import to count files not using any libraries. Their frequencies are shown in Table 7."
        },
        {
            "heading": "Approximated Natural Distribution",
            "text": ""
        },
        {
            "heading": "A.2 More Annotation Details",
            "text": "Along with the NL-Code pair, we also provide IDs of the source StackOverflow post, using which annotators can trace back to the original post webpage\n8https://docs.github.com/en/search-github/ searching-on-github/searching-code\nand get a better understanding of the question. If any errors or under-specification are spotted in the given NL or code, we ask the annotators to correct it by making the minimal change possible.\nAligning with how programmers import a library, we require the expressions be written in three forms: (1) import ${LIBRARY}, (2) import ${LIBRARY} as ${ABBR}, or (3) from ${LIBRARY} import ${FUNCTION}, where the ${LIBRARY} can also be sub-classes such as matplotlib.pyplot.\nWe encourage the annotators to use the language identical to the given NL intent when creating the test cases, especially if the code involves stringrelated operations (e.g., writing regular expressions in Japanese). We encourage the annotators to write reasonably more and diverse test cases, by varying the values or types of variables.\nPlease find the full instruction9 and examples10\nfor annotation in our code repository."
        },
        {
            "heading": "B Baseline Results",
            "text": "According to the baseline results in \u00a7 5.1, we provide more detailed evaluation results, on the execution pass rate ranging from the top-1 to top-10 model predictions. Table 8 and Table 9 show the zero-shot execution accuracy of CODEX and CODEGEN models, respectively.\n9https://anonymous.4open.science/r/odex/data/\ninstruction.md 10https://anonymous.4open.science/r/odex/data/sample_ annotation.ipynb"
        },
        {
            "heading": "C Domain-Wise Execution Results",
            "text": "We list out detailed results for experiments in \u00a75."
        },
        {
            "heading": "C.1 Open Domain Versus Closed Domain",
            "text": "Table 10 and Table 11 shows the execution accuracy for CODEX and CODEGEN on open-domain and closed-domain problems, respectively.\nNL Split Pass Rate\n@1 @2 @3 @4 @5 @6 @7 @8 @9 @10\nCODE-CUSHMAN-001\nen - 31.91 44.67 51.81 56.54 59.95 62.56 64.61 66.28 67.65 68.79\nopen 24.39 35.82 43.08 48.22 52.04 54.97 57.27 59.10 60.57 61.74 close 40.19 54.42 61.41 65.69 68.66 70.90 72.70 74.18 75.45 76.56 es - 31.89 43.33 49.23 53.01 55.72 57.81 59.52 60.96 62.22 63.33\nopen 27.71 38.98 45.12 49.14 52.06 54.34 56.20 57.78 59.17 60.42 close 36.67 48.31 53.93 57.44 59.91 61.79 63.31 64.60 65.71 66.67 ja - 25.67 36.69 42.66 46.49 49.27 51.44 53.23 54.76 56.10 57.32\nopen 21.24 30.29 35.16 38.34 40.71 42.61 44.20 45.55 46.73 47.79 close 35.49 50.89 59.28 64.56 68.23 71.01 73.25 75.16 76.86 78.43 ru - 31.91 44.67 51.81 56.54 59.95 62.56 64.61 66.28 67.65 68.79\nopen 25.96 36.80 42.57 46.22 48.79 50.76 52.38 53.76 55.00 56.14 close 51.59 67.26 74.47 78.61 81.37 83.37 84.87 86.04 86.96 87.68\nCODE-DAVINCI-001\nen - 33.62 46.65 53.27 57.34 60.18 62.31 64.00 65.37 66.49 67.43\nopen 26.91 39.25 45.97 50.25 53.33 55.70 57.62 59.21 60.57 61.74 close 41.00 54.79 61.32 65.14 67.71 69.59 71.02 72.14 73.01 73.68 es - 36.89 49.46 55.44 58.96 61.37 63.22 64.78 66.20 67.56 68.89\nopen 31.67 44.63 51.11 54.78 57.07 58.63 59.81 60.79 61.67 62.50 close 42.86 54.97 60.40 63.73 66.28 68.46 70.46 72.38 74.29 76.19 ja - 31.04 42.11 47.83 51.54 54.26 56.39 58.11 59.53 60.67 61.59\nopen 23.72 32.72 37.88 41.48 44.21 46.36 48.08 49.46 50.53 51.33 close 47.25 62.92 69.89 73.85 76.54 78.62 80.34 81.83 83.14 84.31 ru - 43.21 57.53 63.93 67.58 70.03 71.85 73.29 74.51 75.60 76.59\nopen 28.86 41.01 47.05 50.77 53.47 55.65 57.53 59.22 60.79 62.28 close 55.07 71.18 77.87 81.47 83.71 85.22 86.32 87.15 87.83 88.41\nCODE-DAVINCI-002"
        },
        {
            "heading": "C.2 Domain-wise Execution Accuracy",
            "text": "As introduced in \u00a7 5.3, we take CODE-DAVINCI002, and report its execution accuracy on each domain in Table 12."
        },
        {
            "heading": "Library Count Pass@1 Library Count Pass@1",
            "text": ""
        },
        {
            "heading": "C.3 Qualitative Error Analysis",
            "text": "To provide more intuitive explanations of the domain divergence aforementioned, we conduct error analysis over 60 randomly selected examples from ODEX dataset (15 for each language). By examining the error patterns from these examples, we aim to answer: what are the common error types on open- and closed-domain problems? What are the main differences between them?\nSimilar to the previous section, we take the CODE-DAVINCI-002 since it scores the best and presents clear domain gaps, which might give more intuitive variances between domains.\nClosed-Domain Errors Of the 60 random samples we analyzed, 31 are closed-domain problems, and CODEX predicts erroneous code solutions for 22 of them. We identify four main types of errors\nfrom these samples: (1) 11 cases (50.0%) use the Python built-in functions incorrectly, mostly about strings manipulations and number calculations; (2) 7 cases (31.8%) failed at complex functions, which usually require multi-step implementations; (3) 4 cases (18.2%) received empty predictions, potentially because they involve unfamiliar topics to the model; (4) 2 cases (9.1%) imports extra library or add redundant implementations.\nNote that the number of error cases in these four categories does not add up to 22. Since we analyze all of the error predictions among the model top-10 predictions, one case could present multiple error types in its different predictions.\nOpen-Domain Errors Of the other 29 problems belonging to the open domain, 26 of them have erroneous predictions. Errors in the open domain exhibit more diversity than in the closed domain. The major error enclosing 16 cases (61.5%) is the failure to use the prerequisite libraries, or missing part of them when multiple libraries are involved. The next major type is using incorrect functions, which happens in 9 cases (34.6%). Similarly to the closed-domain errors, 5 cases (19.2%) have error usage of correct functions, 4 cases (15.4%) struggle with complex multi-step implementations, and 3 cases (11.5%) face empty predictions.\nOD and CD problems share some error categories such as function misuse and complex operations. Nonetheless, open-domain problems introduce extra challenges: correct selection and usage of libraries and functions in the wild."
        },
        {
            "heading": "D Evaluation Metrics",
            "text": "We describe each of the non-execution metrics (\u00a7 D.1) as introduced in \u00a7 6, report model performance with each (\u00a7 D.2), and visualize their correlations with the execution accuracy (\u00a7 D.3)."
        },
        {
            "heading": "D.1 Metric Description",
            "text": "BLEU BLEU (Papineni et al., 2002) is a lexicalbased evaluation metric, which calculates the ngram overlap between text prediction and (multiple) references. Most default calculation processes calculate up to 4-grams and adopt the smoothing function introduced in Lin and Och (2004).\nROUGE ROUGE (Lin, 2004) is another more recall-oriented lexical-based evaluation metric. It was originally designed for measuring text summarization, mainly by counting the number of\noverlapping units (n-gram, word sequences, and word pairs) between prediction and references. Among the multiple variants proposed (ROUGE-N, ROUGE-L, ROUGE-W, and ROUGE-S), we use the most common ROUGE-L in our experiments.\nMETEOR METEOR (Banerjee and Lavie, 2005) is a unigram-based metric originally intended for machine translation. It builds on a generalized unigram concept by involving unigram precision, unigram recall, and word order measures.\nChrF ChrF (Popovic\u0301, 2015) targets lexical match on the character level, by calculating the characterlevel n-gram F-score between predictions and references. ChrF is also originally proposed for the machine translation task, but later adopted for some code evaluation works (Evtikhiev et al., 2023).\nCodeBLEU CodeBLEU (Ren et al., 2020) is specifically designed for code evaluation, by jointly considering the surface-form match, syntax similarly, and semantic data flows."
        },
        {
            "heading": "D.2 Evaluating with Non-execution Metrics",
            "text": "Table 13 and Table 14 shows the scores of CODEX and CODEGEN using non-execution metrics.\nD.3 Visualizing Metric Correlations Following the discussion in \u00a7 6, we visualize the non-execution metric metrics between samples that pass and fail during execution time. All experiments use CODE-DAVINCI-002 predictions for evaluation. Figure 13, Figure 14, Figure 15, Figure 16 illustrates the histogram between passed/failed samples using ROUGE, METEOR, ChrF, and CodeBLEU metrics, respectively."
        },
        {
            "heading": "D.4 Why is Execution Better?",
            "text": "To give more intuitive reasons for the advantages of execution, we randomly sample 15 cases from each language subset and identified two major benefits: it tolerates alternative solutions and allows execution results as outputs.\nAlternative Code Implementation Probably the greatest advantage of execution is it only requires correct execution results, without limitations on alternative methods, as in Figure 17.\nDirectly Generating Execution Results Another interesting category is directly generating the code execution results instead of the implementation steps. This often happens to simple coding queries such as basic string manipulation, where predicting the results might cost the model similar efforts to getting the programmatic solutions.\nIn Figure 18, instead of the string decoding program, the model directly outputs the result string \u201cJLK\u201d. While this is somewhat unexpected under\nthe NL-to-Code task, execution effectively handles such cases and would judge them as correct."
        },
        {
            "heading": "D.5 Potential Benefit of Lexical-based Metrics",
            "text": "Lexical-based metrics, although relatively ineffective for functional correctness, still are potentially helpful for debugging and interpretation. They are effective in small errors of two types: (1) a single function misuse and (2) slight variance in complex strings. The high lexical match in such cases indicates less effort for fixing (Deng et al., 2021).\nFunction Misuse Some code predictions are correct except for a single place where a wrong function is used, or an argument is misplaced.\nFor example, in Figure 19, the code imports the library and copies all strings correctly. But it uses the wrong function match instead of the correct findall. Although the execution fails, the code is similar to the solution. Given the sign of a high BLEU score of 92.5, we could readily spot such similarities and fix them with simple edits.\nString Difference Another frequent error concerns string copying, where the code calls the correct functions but copies the string differently.\nThe example in Figure 20 gets a 100.0 BLEU score, but the string inside actually misses a single whitespace, which the BLEU tokenization would discard. Such code also resembles the solution and could be easily fixed by even rule-based methods."
        },
        {
            "heading": "E Ablation Studies",
            "text": "This section provides the results tables according to each ablation study section in \u00a7 7."
        },
        {
            "heading": "E.1 Prompting Strategy",
            "text": ""
        },
        {
            "heading": "E.1.1 Few-shot Prompting",
            "text": "Table 15, Table 16, Table 17 show the change in execution accuracy with respect to the examples in in-context learning, on the three CODEX variants"
        },
        {
            "heading": "E.1.2 Number of Input Test Cases",
            "text": "Table 18 shows the effects on execution accuracy of adding one or more test cases to prompts. Experiments use CODE-DAVINCI-002 as an example.\nFurthermore, we experiment on the subset of examples having sufficient test cases, to prevent the\nn-test setting being trivialized into the 1-test case. Concretely, we filtered all examples with at least 3 test cases and got 112, 17, 25, and 45 examples in English, Spanish, Japanese, and Russian. Pass@1 results on 0/1/n-test settings are shown in Table 19."
        },
        {
            "heading": "E.1.3 Pre-processing: Trailing Whitespaces",
            "text": "While the input construction process may introduce whitespaces at the start and the end of the text sequence, we find CODEGEN model unexpectedly sensitive to trailing whitespaces. As shown in Table 20, removing whitespaces from the prompt input increases the pass rate of all sized CODEGEN models by over 20 percent.\nWe conjecture the gain brought by whitespace stripping to be better distributional alignment with CODEGEN training data. As CODEGEN might be pre-trained on whitespace-stripped text sequences, inputs without whitespaces are potentially more aligned with them, hence resulting in better testtime performance. Meanwhile, note that the tokenization processes for text (natural language) and code (programming language) differ in whitespacestyle tokens such as \\n or \\t. These tokens would be removed by text tokenizers by default, while preserved by code tokenizers since they imply structural information in code pieces."
        },
        {
            "heading": "E.2 Number of Evaluation Test Cases",
            "text": "Table 21 shows the effect when using different numbers of test cases for execution-based evaluation."
        },
        {
            "heading": "E.2.1 Number of Evaluation Test Cases",
            "text": "We also evaluate on the subset of examples having at least 3 test cases. Table 22 shows the pass@1 results for each language."
        },
        {
            "heading": "E.3 Semantics of Function Names",
            "text": "Because code is wrapped into functions to enable execution, how functions are named may affect model predictions. By default, we name functions using the post ID (e.g., f_3844801), which expresses little semantics of queries. So we try two other methods: (1) a constant string function; and (2) summary phrases from NL intents, e.g., find_max_value.\nTo do (2), we conduct a heuristic phrase extraction. We first cut the NL intent into words by whitespace, then remove the stop words (\u2018in\u2019, \u2018of\u2019, \u2018a\u2019, \u2018to\u2019, \u2018and\u2019, \u2018for\u2019, \u2018with\u2019, \u2018that\u2019) and meaningless punctuations, lastly, concatenate the first M = 4 words with \u2018_\u2019. For example, given an intent \u201cdecode a hex string \u20194a4b4c\u2019 to UTF-8\u201d, the resulting function name would be \u201cdecode_a_hex_string\u201d. However, for languages that do not separate words with whitespace, this approach may produce less meaningful strings, hence contributing to the inferior performance as shown below.\nTo fairly compare with previous results, we do not add test cases in prompts.\nFrom Figure 21 and Table 23, using more semantically meaningful functional names barely improves over the default setting. Intuitively, summarizing names from intents adds no extra semantics, but may cost information loss at the curation step, both contributing to the performance drop."
        },
        {
            "heading": "F ODEX Results on Additional Models",
            "text": "It is possible that some source StackOverflow (SO) posts used to create ODEX examples were used in the training data of the closed-source models in our experiments. However, we have no way to remove those overlapping examples due to the lack of a detailed web index within the training data of these models. On the one hand, we modified the NL intents and code solutions to some extent \u00a72.2, which may alleviate exact matches to scraped training data and, hence reduce the influence of unqualified training data (Lai et al., 2022).\nTo further address the train-test data overlap issue in Codex and CodeGen models, we addition-\nally evaluate the SantaCoder (Allal et al., 2023) and StarCoder (Li et al., 2023) models, which have not been trained on any SO data. Table 24 shows the pass@1 of 16B StarCoder and StarCoderBase models, where both models show significant gaps between open- and closed-domain queries, thanks to the broad domain coverage of ODEX."
        },
        {
            "heading": "G Related Work",
            "text": "Open Domain Code Generation Code written in general-purpose programming languages often uses classes or functions from external libraries. A few datasets for code generation preserve this open-domain nature. The CONCODE (Iyer et al., 2018) dataset tested generation of Java class methods. Later works target Python generation given the interactive context of Jupyter Notebooks (Agashe et al., 2019) or natural language intents from StackOverflow posts (Yin et al., 2018; Wang et al., 2023). Despite their natural coverage, enabling open-domain code execution has faced great challenges given its diversity and complexity (Lai et al., 2022; Chandel et al., 2022). To address this issue, our ODEX provides test cases as code execution contexts for evaluation.\nCode Evaluation via Execution Executionbased evaluation has been long adopted for domain-specific programming languages such as SQL queries (Zhong et al., 2017) or logical forms (Dong and Lapata, 2016). This executionbased paradigm has not been introduced to generalpurpose languages until recently by the HumanEval dataset (Chen et al., 2021), where human-written test cases are provided for code execution. Many works afterward follow this approach, but focus more on closed-domain settings (Austin et al., 2021; Hendrycks et al., 2021) or specific libraries of interest (Lai et al., 2022; Huang et al., 2022). Toward broader execution environments, we provide executable test cases for as many as 79 libraries.\nCoding Queries Versus Programming Challenges Programs from different sources are organized for various purposes. Coding contest websites such as LeetCode11 and Codeforces12 have been used to build many code generation benchmarks (Hendrycks et al., 2021; Li et al., 2022). However, they randomly align with how humans program in practical scenarios. To build datasets\n11https://leetcode.com/ 12https://codeforces.com/\nwith natural and practical usage of code, many works use GitHub Jupyter Notebooks (Agashe et al., 2019; Huang et al., 2022) and StackOverflow forums (Yin et al., 2018; Wang et al., 2023; Lai et al., 2022) as a source of naturally-occurring code. We remain such naturalness by using StackOverflow posts, but uniquely from forums in various languages to also assist programmers worldwide.\nTest Case Creation While most benchmarks use Python test cases annotated by human programmers (Chen et al., 2021; Nijkamp et al., 2023; Lai et al., 2022), challenge-style datasets adopt a more direct approach by crawling from the web (Hendrycks et al., 2021; Li et al., 2022). Another thread of work attempts to generate test cases automatically based on the Python grammar (Lukasczyk and Fraser, 2022), but is largely limited to basic Python functions. Some propose to leverage the power of neural LMs (Tufano et al., 2021; Li et al., 2022), even jointly considering solution and test case generation (Chen et al., 2023). However, the quality and diversity of test cases are not robustly ensured. We hence use high-quality human-written test cases for ODEX evaluation."
        }
    ],
    "title": "Execution-Based Evaluation for Open-Domain Code Generation",
    "year": 2023
}