{
    "abstractText": "Automated summary quality assessment falls into two categories: reference-based and reference-free. Reference-based metrics, historically deemed more accurate due to the additional information provided by human-written references, are limited by their reliance on human input. In this paper, we hypothesize that the comparison methodologies used by some reference-based metrics to evaluate a system summary against its corresponding reference can be effectively adapted to assess it against its source document, thereby transforming these metrics into reference-free ones. Experimental results support this hypothesis. After being repurposed reference-freely, the zero-shot BERTScore using the pretrained DeBERTalarge-MNLI model of <0.5B parameters consistently outperforms its original referencebased version across various aspects on the SummEval and Newsroom datasets. It also excels in comparison to most existing referencefree metrics and closely competes with zeroshot summary evaluators based on GPT-3.5.",
    "authors": [
        {
            "affiliations": [],
            "name": "Forrest Sheng Bao"
        },
        {
            "affiliations": [],
            "name": "Ruixuan Tu"
        },
        {
            "affiliations": [],
            "name": "Ge Luo"
        },
        {
            "affiliations": [],
            "name": "Yinfei Yang"
        },
        {
            "affiliations": [],
            "name": "Hebi Li"
        },
        {
            "affiliations": [],
            "name": "Minghui Qiu"
        },
        {
            "affiliations": [],
            "name": "Youbiao He"
        },
        {
            "affiliations": [],
            "name": "Cen Chen"
        }
    ],
    "id": "SP:900926bd5bc88e45a8ba8a60b87043c2285227b5",
    "references": [
        {
            "authors": [
                "Satanjeev Banerjee",
                "Alon Lavie."
            ],
            "title": "METEOR: An automatic metric for MT evaluation with improved correlation with human judgments",
            "venue": "Proceedings of the ACL workshop on intrinsic and extrinsic evaluation measures for machine translation",
            "year": 2005
        },
        {
            "authors": [
                "Forrest Sheng Bao",
                "Ge Luo",
                "Hebi Li",
                "Minghui Qiu",
                "Yinfei Yang",
                "Youbiao He",
                "Cen Chen."
            ],
            "title": "SueNes: A weakly supervised approach to evaluating singledocument summarization via negative sampling",
            "venue": "Proceedings of the 2022 Conference of the North",
            "year": 2022
        },
        {
            "authors": [
                "Alexander R. Fabbri",
                "Wojciech Kry\u015bci\u0144ski",
                "Bryan McCann",
                "Caiming Xiong",
                "Richard Socher",
                "Dragomir Radev."
            ],
            "title": "SummEval: Re-evaluating Summarization Evaluation",
            "venue": "Transactions of the Association for Computational Linguistics, 9:391\u2013409.",
            "year": 2021
        },
        {
            "authors": [
                "Mingqi Gao",
                "Jie Ruan",
                "Renliang Sun",
                "Xunjian Yin",
                "Shiping Yang",
                "Xiaojun Wan"
            ],
            "title": "Human-like summarization evaluation with ChatGPT",
            "year": 2023
        },
        {
            "authors": [
                "Yang Gao",
                "Wei Zhao",
                "Steffen Eger"
            ],
            "title": "SUPERT: Towards new frontiers in unsupervised evaluation",
            "year": 2020
        },
        {
            "authors": [
                "Max Grusky",
                "Mor Naaman",
                "Yoav Artzi"
            ],
            "title": "Newsroom: A dataset of 1.3 million summaries with diverse extractive strategies",
            "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association",
            "year": 2018
        },
        {
            "authors": [
                "Philippe Laban",
                "Tobias Schnabel",
                "Paul N. Bennett",
                "Marti A. Hearst."
            ],
            "title": "SummaC: Re-Visiting NLIbased Models for Inconsistency Detection in Summarization",
            "venue": "Transactions of the Association for Computational Linguistics, 10:163\u2013177.",
            "year": 2022
        },
        {
            "authors": [
                "Chin-Yew Lin."
            ],
            "title": "ROUGE: A package for automatic evaluation of summaries",
            "venue": "Text Summarization Branches Out, pages 74\u201381, Barcelona, Spain. Association for Computational Linguistics.",
            "year": 2004
        },
        {
            "authors": [
                "Yang Liu",
                "Dan Iter",
                "Yichong Xu",
                "Shuohang Wang",
                "Ruochen Xu",
                "Chenguang Zhu"
            ],
            "title": "G-Eval: NLG evaluation using GPT-4 with better human alignment",
            "year": 2023
        },
        {
            "authors": [
                "Yizhu Liu",
                "Qi Jia",
                "Kenny Zhu."
            ],
            "title": "Reference-free summarization evaluation via semantic correlation and compression ratio",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human",
            "year": 2022
        },
        {
            "authors": [
                "Ananya Mukherjee",
                "Manish Shrivastava."
            ],
            "title": "REUSE: REference-free UnSupervised quality estimation metric",
            "venue": "Proceedings of the Seventh Conference on Machine Translation (WMT), pages 564\u2013568, Abu Dhabi, United Arab Emirates (Hybrid). Associa-",
            "year": 2022
        },
        {
            "authors": [
                "Ani Nenkova",
                "Rebecca Passonneau",
                "Kathleen McKeown."
            ],
            "title": "The pyramid method: Incorporating human content selection variation in summarization evaluation",
            "venue": "ACM Transactions on Speech and Language Processing (TSLP), 4(2):4\u2013es.",
            "year": 2007
        },
        {
            "authors": [
                "NIST."
            ],
            "title": "TAC2010 guided summarization competition",
            "venue": "https://tac.nist.gov/2010/ Summarization/Guided-Summ.2010. guidelines.html. Accessed: 2021-08-16.",
            "year": 2010
        },
        {
            "authors": [
                "Maxime Peyrard",
                "Teresa Botschen",
                "Iryna Gurevych."
            ],
            "title": "Learning to score system summaries for better content selection evaluation",
            "venue": "Proceedings of the Workshop on New Frontiers in Summarization, pages 74\u201384, Copenhagen, Denmark. Association for",
            "year": 2017
        },
        {
            "authors": [
                "Thomas Scialom",
                "Sylvain Lamprier",
                "Benjamin Piwowarski",
                "Jacopo Staiano"
            ],
            "title": "Answers unite! unsupervised metrics for reinforced summarization",
            "year": 2019
        },
        {
            "authors": [
                "Thibault Sellam",
                "Dipanjan Das",
                "Ankur Parikh."
            ],
            "title": "BLEURT: Learning robust metrics for text generation",
            "venue": "pages 7881\u20137892, Online. Association for Computational Linguistics.",
            "year": 2020
        },
        {
            "authors": [
                "Oleg Vasilyev",
                "Vedant Dharnidharka",
                "John Bohannon."
            ],
            "title": "Fill in the BLANC: Human-free quality estimation of document summaries",
            "venue": "Proceedings of the First Workshop on Evaluation and Comparison of NLP Systems, pages 11\u201320, Online. Association",
            "year": 2020
        },
        {
            "authors": [
                "Jiaan Wang",
                "Yunlong Liang",
                "Fandong Meng",
                "Zengkui Sun",
                "Haoxiang Shi",
                "Zhixu Li",
                "Jinan Xu",
                "Jianfeng Qu",
                "Jie Zhou"
            ],
            "title": "Is ChatGPT a good NLG evaluator? a preliminary study",
            "year": 2023
        },
        {
            "authors": [
                "Tianyi Zhang",
                "Varsha Kishore",
                "Felix Wu",
                "Kilian Q. Weinberger",
                "Yoav Artzi"
            ],
            "title": "BERTScore: Evaluating text generation with BERT",
            "venue": "In International Conference on Learning Representations",
            "year": 2020
        },
        {
            "authors": [
                "Wei Zhao",
                "Maxime Peyrard",
                "Fei Liu",
                "Yang Gao",
                "Christian M. Meyer",
                "Steffen Eger."
            ],
            "title": "MoverScore: Text generation evaluating with contextualized embeddings and earth mover distance",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in",
            "year": 2019
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Summarization is an important natural language generation (NLG) task. A problem that goes hand in hand with it is summary evaluation, which quantifies the quality of a summarizer or a system summary it generates. The traditional approach to automated\u2020 summary quality assessment is reference-based, such as ROUGE (Lin, 2004), BERTScore (Zhang* et al., 2020) and MoverScore (Zhao et al., 2019), which assesses a system summary against one or a plurality of humanwritten reference summaries.\n\u2020The ground truth is still human evaluation.\nRequiring highly educated human labor, reference summaries are very costly to obtain. Therefore, many reference-free metrics have emerged recently (Scialom et al., 2019; Vasilyev et al., 2020; Bao et al., 2022), which directly compute a score between a system summary and its source document. However, the performance of referencefree metrics has historically lagged behind that of reference-based metrics because a human-written reference summary serves as a fluent and comprehensive representation of the key facts in the input document and thus gives reference-based metrics an advantage.\nRecently, large language models (LLMs) have shown promise in building reference-free summary quality metrics. Metrics based on LLMs like GPT3.5/4 (Liu et al., 2023; Wang et al., 2023; Gao et al., 2023) have outperformed both reference-free and reference-based baselines. However, LLMs are computationally expensive, and the closed nature of GPT-3+ restricts their usage with legal and reproducibility\u2021 limitations. A more viable solution that uses much more cost-effective language models is highly expected.\nTo build an accurate but efficient metric, we revisit the reference-based metrics and hypothesize that they can be repurposed into referencefree metrics by directly comparing a summary with its source document. After being repurposed, BERTScore outperforms not only its original reference-based version, but also most existing reference-free metrics across the SummEval, Newsroom, and TAC2010 datasets on both semantic and linguistic aspects. Notably, the repurposed BERTScore achieves superior or comparable per-\n\u2021https://hackingsemantics.xyz/2023/ closed-baselines/\nformance to GPT-3.5-based summarization evaluators. It is worth noting that these results are achieved using foundation models with significantly fewer parameters (<0.5B) compared to GPT3.5\u2019s extensive 175 billion parameters.\nWe hope this paper can inspire more work into zero-shot summarization or NLG evaluation using cost-effective (e.g., <1B parameters) LMs. Our source code is at https://github.com/ SigmaWe/DocAsRef. In summary, the key findings of this paper include:\n1. The proposed reference-free repurposing does improve performances for Transformer-based metrics including BERTScore and BLEURT.\n2. The repurposed BERTScore can significantly outperform all non-GPT-3.5 baselines using underlying LMs of the similar capacity.\n3. With LMs hundreds of times smaller, the repurposed BERTScore can further match the performance of those based on GPT-3.5 in most of the cases."
        },
        {
            "heading": "2 Approach",
            "text": ""
        },
        {
            "heading": "2.1 Background: Ref-based and ref-free summary evaluation metrics",
            "text": "A system summary is generated from a source document by a summarizer, which is usually embodied by a neural network model today. A corresponding reference is generated from the same document by a human. Metrics for summary evaluation fall into two categories: the reference-based (short as ref-based) ones which are functions comparing a candidate summary and a human-written reference summary: f(system summary, reference), and reference-free (short as ref-free) ones which are functions that evaluate a candidate summary based solely on the input document:\nf(system summary, document). Ref-based metrics, such as ROUGE (Lin, 2004), BERTScore (Zhang* et al., 2020), BLEURT (Sellam et al., 2020), and MoverScore (Zhao et al., 2019), historically have an advantage over ref-free ones, such as Blanc (Vasilyev et al., 2020), SummQA (Scialom et al., 2019), SDC* (Liu et al., 2022), and SueNes (Bao et al., 2022), because the human-written reference summary serves as a fluent and comprehensive representation of the key facts in the input document. Recent GPT-based summary metrics (Gao et al., 2023; Wang et al., 2023; Liu et al., 2023) are all ref-free in nature."
        },
        {
            "heading": "2.2 Repurposing ref-based to ref-free",
            "text": "The idea of repurposing ref-based metrics for reffree evaluation involves leveraging the mechanism employed by these metrics to compare two texts. Although ref-based metrics were originally designed to compare a system summary against a reference summary, we hypothesize that they can still be effective in directly comparing the system summary with the document.\nTo repurpose a ref-based metric f into a ref-free one, we simply feed the document in lieu of the reference when using f . While the idea of using the document as the reference is not new, the specific approach proposed here, which is straightforward and direct, has not been previously explored. Embracing the principle that simplicity is beautiful in science, we decide to give it a try.\nRemarkably, our simple strategy has yielded good results. Three representative ref-based metrics gain their performances after being repurposed (Table 1). One of them, BERTScore employing generically trained LMs such as RoBERTa-large has a performance very close to the performances of metrics based on GPT-3.5, which utilizes hundreds of times more parameters (Tables 2 & 3). This outcome highlights the effectiveness of repurposing ref-based metrics for ref-free evaluation."
        },
        {
            "heading": "2.3 Variants of BERTScore",
            "text": "The promising initial results encouraged us to explore modifications to the ref-based metrics for enhanced performances. ROUGE and BLEURT have limited room for tweaking because ROUGE-1 and ROUGE-2 have been the best among its variants in the past two decades and BLEURT is already finetuned explicitly for summary evaluation. Hence, we focus on refining BERTScore.\nThe first tweak we applied onto BERTScore is to try different small-scale, pretrained language models (LMs). We conducted experiments with three LMs: RoBERTa, DeBERTa, and BART, both their base versions (around 110M parameters) and large versions (around 400M parameters). Additionally, we explored the variants of these LMs that have been officially fine-tuned on the MNLI dataset. Our hypothesis is that an LM fine-tuned for the MNLI task may be better suited for computing text similarity than generic LMs.\nThe second tweak we explored is expanding BERTScore to the sentence level by calculating the similarity between sentences instead of tokens. Various similarity measures and sentence weight-\ning schemes were proposed (Appendix B). Unfortunately, they rarely perform better than the original token-level BERTScore."
        },
        {
            "heading": "3 Experiments",
            "text": ""
        },
        {
            "heading": "3.1 Settings",
            "text": "Because of their exceptional performances and impacts, four ref-based metrics are picked as candidate metrics to be repurposed: ROUGE (Lin, 2004), BERTScore (Zhang* et al., 2020), BLEURT (Sellam et al., 2020), and MoverScore (Zhao et al., 2019). ROUGE is the classic metric used in summarization. The rest three are widely used as baselines in the field in recent years.\nSeven ref-free baselines\u00a7 are included in our study. Four of them use underlying foundation LMs of fewer than 1B parameters: SummaQA (Scialom et al., 2019), BLANC (Vasilyev et al., 2020), SUPERT (Gao et al., 2020), and SueNes (Bao et al., 2022). The rest three (Liu et al., 2023; Gao et al., 2023; Wang et al., 2023) of them are based on GPT-3.5, which has 175B parameters.\nThree multi-facet summarization evaluation datasets with human ratings are used as the test datasets: SummEval (Fabbri et al., 2021), Newsroom (Grusky et al., 2018) and TAC2010 (NIST, 2010). SummEval and Newsroom are for singledocument summarization while TAC2010 is for multi-document summarization. SummEval covers four aspects: CONsistency, RELevance, COHerence, and FLUency. Newsroom covers four aspects: INFormativeness, RELevance, COHerence, and FLUency. TAC2010 reports three scores: Pyramid (Nenkova et al., 2007), linguistic, and overall scores. For TAC2010, only Set A of TAC2010 is used in this paper because Set B \u201cupdate summarization\u201d does not fit the problem formulation in \u00a7 2.1. Measuring how well a summary covers key pieces of information in the source document, RELevance or Pyramid score is generally considered the most important aspect of a summary. CONsistency a raising concern recently due to the hallucination issue. Details for the datasets and their aspects can be found from their respective papers.\nUnderlying language models (LMs). The LMs used in repurposed BERTScore variants are discussed in \u00a7 2.3. The default LM is RoBERTalarge. All ref-free baselines involving finetuning:\n\u00a7We did not run the experiments on baselines but simply copied the numbers from their original papers to here. For the three GPT3.5-based baselines, we pick their best results from their papers.\nBLANC, SummaQA, and SueNes, share the common initial checkpoint, BERT-base. MoverScore and BLUERT use RoBERTa-large and BLUERT20 as the LMs.\nBERTScore is a pairwise comparison metric. Depending on the axis along which max pooling is done, each BERTScore variant yields three scores: P (Precision), R (recall), and F (F1). The experiments are carried out on individual RTX 3090 24GB GPUs. For more details, see Appendix A."
        },
        {
            "heading": "3.2 Results",
            "text": "Following the trend in recent summary evaluation studies (Peyrard et al., 2017), we report the results at the summary level. Spearman\u2019s correlation coefficients between metrics\u2019 predictions and humanrated ground truth are the performance measure. For space sake, we present selected results here with extended results available in the appendices."
        },
        {
            "heading": "3.2.1 Is repurposing useful? Before vs. after",
            "text": "The answer is yes! Despite that ref-based metrics historically perform better than ref-free metrics, Table 1 shows that the three modern metrics, MoverScore, BERTScore, and BLEURT, gain their performances after being repurposed, on nearly all aspects of all datasets. The lexicon-based ROUGE1/2/L also improves its performance on some aspects or datasets after being repurposed.\nAfter being repurposed (top of half of Table 1), BERTScore outperforms all other metrics across datasets, with only a couple of exceptions. It outperforms MoverScore and BLEURT significantly. While BERTScore underperforms ROUGE on SummEval before repurposing, it turns the tide after.\nThe ref-free metrics used in their original designated way perform extremely bad on the Newsroom dataset (bottom half of Table 1 and additional evidence in Appendix D). This is due to that in Newsroom, a reference summary can be as short as one sentence. Here, the reliance to reference summaries becomes a weakness of ref-based summary quality metrics. In this case, the original document may be better than the reference summary to compare with for judging the summary quality."
        },
        {
            "heading": "3.2.2 Repurposed BERTScore vs. ref-free baselines",
            "text": "BERTScore is the most tweakable (\u00a7 2.3) and bestperforming (\u00a7 3.2.1) metric. So we further study how it compares with the ref-free baselines. As mentioned in \u00a7 2.3, to study its robustness, different\nunderlying LMs are used with BERTScore. Due to space limit, here we only report the results using RoBERTa-large and DeBERTa-large which give the best performance.\nThe results on SummEval are given in Table 2. Repurposed BERTScore outperforms all non-GPT baselines by a significant margin. Additionally, it performs comparably to GPT3.5-based baselines on the RELevance and COHerence aspects. It is superior than one of the two GPT-3.5-based approaches on the CONsistency aspect. It should be noted that SummEval is challenging due to its coverage of 23 modern summarizers, many of which exhibit highly similar behavior.\nTable 3 reports the results on the Newsroom dataset. The Newsroom dataset poses a significant challenge for new metrics since the baselines already perform very well on this dataset, likely because it evaluates only seven systems with dis-\ntinct performances. Despite the challenges, repurposed BERTScore outperforms all baselines except SueNes, which is finetued using data explicitly augmented for the summary evaluation task, on all aspects.\nBecause the non-GPT baselines, BLANC, SummaQA, and SueNes, use BERT-base as the underlying LM, for a fair comparison, we include BERTScore\u2019s results using RoBERTa/DeBERTa/BART-base in Appendix D. Even when they use LMs of the same size, BERTScore still outperforms them.\nTable 4 shows the results on the TAC2010 dataset where BERTScore outperforms baselines on all aspects except linguistics. As a multidocument summarization dataset, TAC2010 provides 10 source documents d1, \u00b7 \u00b7 \u00b7 , d10 for generating a system summary s. We use the formula\u2211\ni\u2208[1..10] f(di, s) to approximate the score of a\nsummary s given a single-document summarization metric f ."
        },
        {
            "heading": "3.3 What makes BERTScore powerful",
            "text": "While the result of this paper may sound surprising because the method is very simple, it is totally explainable. Comparing a summary with a document is theoretically more challenging than comparing it with a reference, because information is more sparse in a document than in a reference. This might be the reason that strong NLG evaluation metrics are historically reference-based. However, BERTScore exhibits exceptional performance after being repurprosed from ref-based to ref-free. We attribute this to both the contextual embedding of the underlying LMs and the maxpooling step of BERTScore.\nThe Transformers have the ability to identify important information in a context: by showing strong attentions to the important tokens as learned in pretraining. In other words, encoder-only Transformers used in BERTScore can identify important tokens and function as implicit summarizers. Extraneous information in a summary causes the summary\u2019s context to diverge from that of the original document, resulting in a reduction of semantic similarity, even when comparing the same token in the summary to its \u2018counterpart in the document. The maxpooling step of BERTScore further focuses on the alignment of the most semantically proximate token pairs between the document and the summary. Because the document and the summary are independently embedded in BERTScore, only when important information in the document and the summary align, the BERTScore can be high. On a related note, BERTScore alone is found very\neffectively in measuring factual inconsistency in summaries (Laban et al., 2022).\nThe IDF part of BERTScore may not play an important role because the attention mechanism already factors in what IDF does. A stopword or a boilerplate word has a weak attention to other tokens. In BERTScore\u2019s original paper (Zhang* et al., 2020), IDF makes very marginal impact on all except one datasets/tasks. Table 5 shows our ablation study on the impact of IDF. IDF makes a very small impact and in many cases, it even decreases the performance.\nThe repurposed BERTScore shows relatively robust performance with respect to the choice of the underlying LMs. For example, on Newroom, BERTScore\u2019s worst performing variant in every aspect still outperforms the ChatGPT-based. The only aspect on which BERTScore is not stable is the COHerence aspect of SummEval."
        },
        {
            "heading": "4 Conclusion",
            "text": "In this paper, we explore repurposing summary evaluation metrics that were originally designed or trained for reference-based use as reference-free metrics. The motivation was to reuse their power in comparing texts. Comprehensive experiments on multiple datasets show that four representative metrics generally perform better after the repurposing. The best among them, BERTScore, is further studied with different configurations. The repurposed BERTScore using 0.5B-parameter LMs can outperform all non-GPT baselines significantly and even most of the times those based on GPT3.5."
        },
        {
            "heading": "Acknowledgments",
            "text": "This research is partially supported by National Science Founation (NSF) grant CNS-1817089. The authors would also like to thank reviewers who have given precious feedback on improving this\nwork. Forrest Bao also wants to dedicate this paper to the people of Ukraine who have been courageously fighting for freedom since February 24, 2022.\nLimitations\nThe test sets are all from the news domain which is the only domain that human evaluation to system summaries has been done. This is a limit beyond our control.\nUnfortunately, our attempt (Appendix B) to expand BERTScore from token-level to sentencelevel fails. Moreover, unlike token-level BERTScore, which remains stable across different LM choices, sentence-level BERTScore is highly sensitive to the selection of LMs. Extended results can be found in the appendices.\nBERTScore can have a variant, which is at chuck-level. This idea was proposed in REUSE for machine translation (Mukherjee and Shrivastava, 2022). Since we have tried token-level and sentence-level BERTScore, trying chuck-level BERTScore in summarization can be part of the future work."
        },
        {
            "heading": "A More Experimental Information",
            "text": "More details on underline language models, BLANC and SueNes, the two training-based reference-free baselines use BERT-base while the training-based reference-based BLEURT uses BLEURT-20\u00b6, a 32-layer Transformer model. SUPERT uses BERT-Large-based SentenceTransformer while SummaQA uses a BERT-Large-based QA model. Please understand the tremendous amount of effort and time needed to re-train or rebenchmark all metrics using the same underlying pre-trained language model.\nExperimental time: The experiment can be done really quickly. For SummEval, 5 mins for\n\u00b6Per the BLEURT authors, BLEURT-20 is the strongest, released BLEURT model https://github.com/google-research/ bleurt/blob/master/checkpoints.md# the-recommended-checkpoint-bleurt-20\neach token-level metric, 30 mins for each sentencelevel BERTScore-variant. For Newsroom, the numbers are 8 minutes and 45 minutes, respectively.\nSoftware packages: We used HuggingFace\u2019s evaluate library for the metrics and sentence-transformer library for cosine similarity computation. The evaluate library automatically downloads and plugs models into the metrics. We also used numpy and scipy for general computation. For MoverScore, we used its official implementation from its Github Repo https://github. com/AIPHES/emnlp19-moverscore. The v1 code has deprecated dependencies. So we used the v2 version."
        },
        {
            "heading": "B Expanding BERTScore to the sentence level",
            "text": "We experimented with two approaches to measure sentence similarity: cosine/dot-product similarity and using text reasoning probabilities. Let us elaborate on the latter. Models trained for natural language inference (NLI) tasks typically output three probabilities representing the relationships between two input sentences: \u201centailing\u201d (E), \u201cneutral\u201d (N), and \u201ccontradictory\u201d (C). In other words, given a pair of sentences x and y, we obtain: [E,N,C] = NLI(x, y). We experimented with three options: 1 \u2212 N , E \u2212 C, and E, and selected E \u2212 C due to its intuitive appeal and empirical evidence of its effectiveness.\nThe original BERTScore uses IDF to weight tokens. To weight sentences, we employ a PageRankstyle approach below. First, we decide the importance of document sentences [x1, x2, . . . ]. A sentence is considered important if it can relate to many other sentences. Hence, the importance of a document sentence xi can be estimated as wi = g(sim(xi, x1), sim(xi, x2), . . . ) where sim(\u00b7) is a sentence-level similarity measure and g can be sum or entropy. In the simplest case, we have wi = \u2211 i \u0338=j,j\u2208N+ sim(xi, xj). Second, we let the document sentences \u201cvote\u201d on the importance of summary sentences. The importance of a summary sentence is determined by the sum of its similarities to all document sentences, weighted by the importance (voting power) of document sentences. Thus, the importance of the j-th sentence yj in the summary is vj = \u2211 i,j\u2208N+ wisim(xi, yj).\nUnfortunately, as you can see in \u00a7 D, the sentence-level tweaks do not yield better results except on the consistency aspect. Since there are too\nmany sentence-level BERTScore variants, they are referred to in this A-B-C nomenclature, where A is the similarity measure which is Cosine if cosine similarity and MNLI if using entailment confidence from an MNLI-finetuned model, B is the underlying LM, and C, optional, is the sentence weighting method g."
        },
        {
            "heading": "C Our idea in code",
            "text": "We hope this code can help explain what we mean by \u201crepurposing\u201d and also how to directly use the conclusion of this paper.\n1import evaluate # HuggingFace\u2019s 2import functools # Python\u2019s standard 3 4bertscore = evaluate.load(\"bertscore\") 5bertscore_deberta_large_mnli = functools.partial( 6bertscore.compute, 7lang=\"en\", 8use_fast_tokenizer=True, 9model_type=\"microsoft/deberta-large-mnli\" 10) 11scores = bertscore_deberta_large_mnli( 12predictions = [\"this is a summary\"], 13# references = [\"this is a reference\"] # old way 14references = [\"this is the DOC\"] # DocAsRef 15)[0][\u2019recall\u2019]\nAt line 13, conventional approaches plug in human-written references. But in our proposed idea (line 14), just plug in the source documents, and you will get the best reference-free summary quality assessor.\nIt\u2019s easy-to-implement, zero-shot, and referencefree."
        },
        {
            "heading": "D More comprehensive results",
            "text": "Please refer to Table 6 and Table 7."
        },
        {
            "heading": "E Leadword heuristic",
            "text": "It is common that important information is clustered at the beginning of a document. Hence, Leadword is a simple but effective method to extract important information. SUPERT build pseudo-references by extracting salient sentences and found that Leadword is better than any other simple extractive approach. So we also experiment with limiting the BERTScore-style pairwise comparison to top-k sentences in the input document. We use top-k slightly different from its common use in text generation. Here k means a ratio rather than an absolute number because the length of the input document varies a lot.\nIs Leadword heuristic useful? In this study, no repurposed metrics benefit from the Leadword heuristic, unlike the result reported in SUPERT (Gao et al., 2020). Nearly every metric loses\nperformance after using the Leadword heuristic. The shorter the lead is, the more performance drop. Investigating the reason is part of our future work."
        }
    ],
    "title": "DocAsRef: An Empirical Study on Repurposing Reference-based Summary Quality Metrics as Reference-free Metrics",
    "year": 2023
}