{
    "abstractText": "Over recent decades, significant advancements in cross-modal retrieval are mainly driven by breakthroughs in visual and linguistic modeling. However, a recent study shows that multi-modal data representations tend to cluster within a limited convex cone (as representation degeneration problem), which hinders retrieval performance due to the inseparability of these representations. In our study, we first empirically validate the presence of the representation degeneration problem across multiple cross-modal benchmarks and methods. Next, to address it, we introduce a novel method, called INVGC, a post-processing technique inspired by graph convolution and average pooling. Specifically, INVGC defines the graph topology within the datasets and then applies graph convolution in a subtractive manner. This method effectively separates representations by increasing the distances between data points. To improve the efficiency and effectiveness of INVGC, we propose an advanced graph topology, LOCALADJ, which only aims to increase the distances between each data point and its nearest neighbors. To understand why INVGC works, we present a detailed theoretical analysis, proving that the lower bound of recall will be improved after deploying INVGC. Extensive empirical results show that INVGC and INVGC w/LOCALADJ significantly mitigate the representation degeneration problem, thereby enhancing retrieval performance. Our code is available at link.",
    "authors": [
        {
            "affiliations": [],
            "name": "Xiangru Jian"
        },
        {
            "affiliations": [],
            "name": "Yimu Wang"
        }
    ],
    "id": "SP:92338f98d3e4ee385079e496cf474f5265d40255",
    "references": [
        {
            "authors": [
                "Aseem Baranwal",
                "Kimon Fountoulakis",
                "Aukosh Jagannath."
            ],
            "title": "Effects of graph convolutions in multi-layer networks",
            "venue": "The Eleventh International Conference on Learning Representations.",
            "year": 2023
        },
        {
            "authors": [
                "Richard Bellman."
            ],
            "title": "Adaptive Control Processes A Guided Tour (Reprint from 1961), volume 2045 of Princeton Legacy Library",
            "venue": "Princeton University Press.",
            "year": 2015
        },
        {
            "authors": [
                "Simion-Vlad Bogolin",
                "Ioana Croitoru",
                "Hailin Jin",
                "Yang Liu",
                "Samuel Albanie."
            ],
            "title": "Cross modal retrieval with querybank normalisation",
            "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA,",
            "year": 2022
        },
        {
            "authors": [
                "Y-Lan Boureau",
                "Jean Ponce",
                "Yann LeCun"
            ],
            "title": "A theoretical analysis of feature pooling in visual",
            "year": 2010
        },
        {
            "authors": [
                "Joan Bruna",
                "Wojciech Zaremba",
                "Arthur Szlam",
                "Yann LeCun"
            ],
            "title": "Spectral networks and locally connected networks on graphs",
            "year": 2014
        },
        {
            "authors": [
                "David Chen",
                "William Dolan."
            ],
            "title": "Collecting highly parallel data for paraphrase evaluation",
            "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 190\u2013200, Portland,",
            "year": 2011
        },
        {
            "authors": [
                "Shizhe Chen",
                "Yida Zhao",
                "Qin Jin",
                "Qi Wu."
            ],
            "title": "Fine-grained video-text retrieval with hierarchical graph reasoning",
            "venue": "2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020, pages",
            "year": 2020
        },
        {
            "authors": [
                "Xing Cheng",
                "Hezheng Lin",
                "Xiangyu Wu",
                "Fan Yang",
                "Dong Shen."
            ],
            "title": "Improving video-text retrieval by multi-stream corpus alignment and dual softmax loss",
            "venue": "CoRR, abs/2109.04290.",
            "year": 2021
        },
        {
            "authors": [
                "Ioana Croitoru",
                "Simion-Vlad Bogolin",
                "Marius Leordeanu",
                "Hailin Jin",
                "Andrew Zisserman",
                "Samuel Albanie",
                "Yang Liu."
            ],
            "title": "Teachtext: Crossmodal generalized distillation for text-video retrieval",
            "venue": "2021 IEEE/CVF International Conference on",
            "year": 2021
        },
        {
            "authors": [
                "Victor H. de la Pena",
                "S.J. Montgomery-Smith."
            ],
            "title": "Decoupling Inequalities for the Tail Probabilities of Multivariate U -Statistics",
            "venue": "The Annals of Probability, 23(2):806 \u2013 816.",
            "year": 1995
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association",
            "year": 2019
        },
        {
            "authors": [
                "Konstantinos Drossos",
                "Samuel Lipping",
                "Tuomas Virtanen."
            ],
            "title": "Clotho: an Audio Captioning Dataset",
            "venue": "ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 736\u2013740. ISSN:",
            "year": 2020
        },
        {
            "authors": [
                "Bernard Ghanem Fabian Caba Heilbron",
                "Victor Escorcia",
                "Juan Carlos Niebles."
            ],
            "title": "Activitynet: A large-scale video benchmark for human activity understanding",
            "venue": "Proceedings of the IEEE Conference",
            "year": 2015
        },
        {
            "authors": [
                "Valentin Gabeur",
                "Chen Sun",
                "Karteek Alahari",
                "Cordelia Schmid."
            ],
            "title": "Multi-modal transformer for video retrieval",
            "venue": "Computer Vision - ECCV 2020 - 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part IV, volume 12349 of",
            "year": 2020
        },
        {
            "authors": [
                "Zhe Gan",
                "Yen-Chun Chen",
                "Linjie Li",
                "Chen Zhu",
                "Yu Cheng",
                "Jingjing Liu."
            ],
            "title": "Large-scale adversarial training for vision-and-language representation learning",
            "venue": "Advances in Neural Information Processing Systems 33: Annual Conference on",
            "year": 2020
        },
        {
            "authors": [
                "Jun Gao",
                "Di He",
                "Xu Tan",
                "Tao Qin",
                "Liwei Wang",
                "TieYan Liu."
            ],
            "title": "Representation degeneration problem in training natural language generation models",
            "venue": "7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA,",
            "year": 2019
        },
        {
            "authors": [
                "Zijian Gao",
                "Jingyu Liu",
                "Sheng Chen",
                "Dedan Chang",
                "Hao Zhang",
                "Jinwei Yuan."
            ],
            "title": "CLIP2TV: an empirical study on transformer-based methods for video-text retrieval",
            "venue": "CoRR, abs/2111.05610.",
            "year": 2021
        },
        {
            "authors": [
                "Justin Gilmer",
                "Samuel S. Schoenholz",
                "Patrick F. Riley",
                "Oriol Vinyals",
                "George E. Dahl."
            ],
            "title": "Neural message passing for quantum chemistry",
            "venue": "Proceedings of the 34th International Conference on Machine Learning - Volume 70, ICML\u201917, page",
            "year": 2017
        },
        {
            "authors": [
                "Yunchao Gong",
                "Svetlana Lazebnik",
                "Albert Gordo",
                "Florent Perronnin."
            ],
            "title": "Iterative quantization: A procrustean approach to learning binary codes for large-scale image retrieval",
            "venue": "IEEE Trans. Pattern Anal. Mach. Intell., 35(12):2916\u20132929.",
            "year": 2013
        },
        {
            "authors": [
                "Ian Goodfellow",
                "Yoshua Bengio",
                "Aaron Courville."
            ],
            "title": "Deep Learning",
            "venue": "MIT Press. http://www. deeplearningbook.org.",
            "year": 2016
        },
        {
            "authors": [
                "Satya Krishna Gorti",
                "No\u00ebl Vouitsis",
                "Junwei Ma",
                "Keyvan Golestan",
                "Maksims Volkovs",
                "Animesh Garg",
                "Guangwei Yu."
            ],
            "title": "X-pool: Cross-modal language-video attention for text-video retrieval",
            "venue": "IEEE/CVF Conference on Computer Vision and",
            "year": 2022
        },
        {
            "authors": [
                "William L. Hamilton."
            ],
            "title": "Graph representation learning",
            "venue": "Synthesis Lectures on Artificial Intelligence and Machine Learning, 14(3):1\u2013159.",
            "year": 2020
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun."
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016, pages 770\u2013778. IEEE",
            "year": 2016
        },
        {
            "authors": [
                "Lisa Anne Hendricks",
                "Oliver Wang",
                "Eli Shechtman",
                "Josef Sivic",
                "Trevor Darrell",
                "Bryan Russell."
            ],
            "title": "Localizing Moments in Video with Natural Language",
            "venue": "2017 IEEE International Conference on Computer Vision (ICCV), pages 5804\u20135813, Venice.",
            "year": 2017
        },
        {
            "authors": [
                "Zhenyu Huang",
                "Guocheng Niu",
                "Xiao Liu",
                "Wenbiao Ding",
                "Xinyan Xiao",
                "Hua Wu",
                "Xi Peng."
            ],
            "title": "Learning with noisy correspondence for cross-modal matching",
            "venue": "Advances in Neural Information Processing Systems, volume 34, pages 29406\u201329419.",
            "year": 2021
        },
        {
            "authors": [
                "Eamonn Keogh",
                "Abdullah Mueen."
            ],
            "title": "Curse of Dimensionality, pages 314\u2013315",
            "venue": "Springer US, Boston, MA.",
            "year": 2017
        },
        {
            "authors": [
                "Chris Dongjoo Kim",
                "Byeongchang Kim",
                "Hyunmin Lee",
                "Gunhee Kim."
            ],
            "title": "AudioCaps: Generating captions for audios in the wild",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics:",
            "year": 2019
        },
        {
            "authors": [
                "Wonhark Park",
                "Dongkwan Lee",
                "Nojun Kwak",
                "Yujin Wang",
                "Yimu Wang",
                "Tiancheng Gu",
                "Xingchang Lv",
                "Mingmao Sun"
            ],
            "title": "Nice: Cvpr 2023 challenge on zero-shot image captioning",
            "year": 2023
        },
        {
            "authors": [
                "Thomas N. Kipf",
                "Max Welling."
            ],
            "title": "Semisupervised classification with graph convolutional networks",
            "venue": "International Conference on Learning Representations.",
            "year": 2017
        },
        {
            "authors": [
                "A. Sophia Koepke",
                "Andreea-Maria Oncescu",
                "Joao Henriques",
                "Zeynep Akata",
                "Samuel Albanie."
            ],
            "title": "Audio Retrieval with Natural Language Queries: A Benchmark Study",
            "venue": "IEEE Transactions on Multimedia, pages 1\u20131. Conference Name: IEEE",
            "year": 2022
        },
        {
            "authors": [
                "Jie Lei",
                "Linjie Li",
                "Luowei Zhou",
                "Zhe Gan",
                "Tamara L. Berg",
                "Mohit Bansal",
                "Jingjing Liu"
            ],
            "title": "Less is more: Clipbert for video-and-language learning",
            "year": 2021
        },
        {
            "authors": [
                "Xiujun Li",
                "Xi Yin",
                "Chunyuan Li",
                "Pengchuan Zhang",
                "Xiaowei Hu",
                "Lei Zhang",
                "Lijuan Wang",
                "Houdong Hu",
                "Li Dong",
                "Furu Wei",
                "Yejin Choi",
                "Jianfeng Gao."
            ],
            "title": "Oscar: Object-semantics aligned pretraining for vision-language tasks",
            "venue": "Computer",
            "year": 2020
        },
        {
            "authors": [
                "Weixin Liang",
                "Yuhui Zhang",
                "Yongchan Kwon",
                "Serena Yeung",
                "James Zou."
            ],
            "title": "Mind the gap: Understanding the modality gap in multi-modal contrastive representation learning",
            "venue": "Advances in neural information processing systems.",
            "year": 2022
        },
        {
            "authors": [
                "Tsung-Yi Lin",
                "Michael Maire",
                "Serge J. Belongie",
                "James Hays",
                "Pietro Perona",
                "Deva Ramanan",
                "Piotr Doll\u00e1r",
                "C. Lawrence Zitnick."
            ],
            "title": "Microsoft COCO: common objects in context",
            "venue": "Computer Vision ECCV 2014 - 13th European Conference, Zurich,",
            "year": 2014
        },
        {
            "authors": [
                "Venice Erin Liong",
                "Jiwen Lu",
                "Yap-Peng Tan",
                "Jie Zhou."
            ],
            "title": "Cross-modal deep variational hashing",
            "venue": "IEEE International Conference on Computer Vision, ICCV 2017, Venice, Italy, October 22-29, 2017, pages 4097\u20134105. IEEE Computer Society.",
            "year": 2017
        },
        {
            "authors": [
                "Yang Liu",
                "Samuel Albanie",
                "Arsha Nagrani",
                "Andrew Zisserman."
            ],
            "title": "Use what you have: Video retrieval using representations from collaborative experts",
            "venue": "30th British Machine Vision Conference 2019, BMVC 2019, Cardiff, UK, September 9-12,",
            "year": 2019
        },
        {
            "authors": [
                "Huaishao Luo",
                "Lei Ji",
                "Ming Zhong",
                "Yang Chen",
                "Wen Lei",
                "Nan Duan",
                "Tianrui Li."
            ],
            "title": "Clip4clip: An empirical study of CLIP for end to end video clip retrieval and captioning",
            "venue": "Neurocomputing, 508:293\u2013 304.",
            "year": 2022
        },
        {
            "authors": [
                "Yiwei Ma",
                "Guohai Xu",
                "Xiaoshuai Sun",
                "Ming Yan",
                "Ji Zhang",
                "Rongrong Ji."
            ],
            "title": "X-CLIP: endto-end multi-grained contrastive learning for videotext retrieval",
            "venue": "MM \u201922: The 30th ACM International Conference on Multimedia, Lisboa,",
            "year": 2022
        },
        {
            "authors": [
                "Daniele Micciancio",
                "Panagiotis Voulgaris."
            ],
            "title": "Faster exponential time algorithms for the shortest vector problem",
            "venue": "Proceedings of the Twenty-First Annual ACM-SIAM Symposium on Discrete Algorithms, SODA \u201910, page 1468\u20131480,",
            "year": 2010
        },
        {
            "authors": [
                "Petar Velickovic",
                "Guillem Cucurull",
                "Arantxa Casanova",
                "Adriana Romero",
                "Pietro Lio",
                "Yoshua Bengio."
            ],
            "title": "Graph attention networks",
            "venue": "International Conference on Learning Representations.",
            "year": 2018
        },
        {
            "authors": [
                "Haoran Wang",
                "Di Xu",
                "Dongliang He",
                "Fu Li",
                "Zhong Ji",
                "Jungong Han",
                "Errui Ding."
            ],
            "title": "Boosting video-text retrieval with explicit highlevel semantics",
            "venue": "MM \u201922: The 30th ACM International Conference on Multimedia, Lisboa,",
            "year": 2022
        },
        {
            "authors": [
                "Xiaohan Wang",
                "Linchao Zhu",
                "Zhedong Zheng",
                "Mingliang Xu",
                "Yi Yang."
            ],
            "title": "Align and tell: Boosting text-video retrieval with local alignment and fine-grained supervision",
            "venue": "IEEE Transactions on Multimedia, pages 1\u201311.",
            "year": 2022
        },
        {
            "authors": [
                "Yimu Wang",
                "Xiangru Jian",
                "Bo Xue"
            ],
            "title": "Balance act: Mitigating hubness in cross-modal retrieval with query and gallery banks",
            "year": 2023
        },
        {
            "authors": [
                "Yimu Wang",
                "Shiyin Lu",
                "Lijun Zhang."
            ],
            "title": "Searching privately by imperceptible lying: A novel private hashing method with differential privacy",
            "venue": "Proceedings of the 28th ACM International Conference on Multimedia, page 2700\u20132709.",
            "year": 2020
        },
        {
            "authors": [
                "Yimu Wang",
                "Peng Shi."
            ],
            "title": "Video-Text Retrieval by Supervised Multi-Space Multi-Grained Alignment",
            "venue": "ArXiv:2302.09473 [cs].",
            "year": 2023
        },
        {
            "authors": [
                "Yimu Wang",
                "Xiu-Shen Wei",
                "Bo Xue",
                "Lijun Zhang."
            ],
            "title": "Piecewise hashing: A deep hashing method for large-scale fine-grained search",
            "venue": "Pattern Recognition and Computer Vision - Third Chinese Conference, PRCV 2020, Nanjing, China, October",
            "year": 2020
        },
        {
            "authors": [
                "Yimu Wang",
                "Bo Xue",
                "Quan Cheng",
                "Yuhui Chen",
                "Lijun Zhang."
            ],
            "title": "Deep unified cross-modality hashing by pairwise data alignment",
            "venue": "Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21, pages 1129\u20131135.",
            "year": 2021
        },
        {
            "authors": [
                "Jun Xu",
                "Tao Mei",
                "Ting Yao",
                "Yong Rui."
            ],
            "title": "MSRVTT: A large video description dataset for bridging video and language",
            "venue": "2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016, pages",
            "year": 2016
        },
        {
            "authors": [
                "Mouxing Yang",
                "Zhenyu Huang",
                "Peng Hu",
                "Taihao Li",
                "Jiancheng Lv",
                "Xi Peng."
            ],
            "title": "Learning with twin noisy labels for visible-infrared person reidentification",
            "venue": "2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2022
        },
        {
            "authors": [
                "Qiying Yu",
                "Yang Liu",
                "Yimu Wang",
                "Ke Xu",
                "Jingjing Liu."
            ],
            "title": "Multimodal federated learning via contrastive representation ensemble",
            "venue": "In",
            "year": 2023
        },
        {
            "authors": [
                "Sangwon Yu",
                "Jongyoon Song",
                "Heeseung Kim",
                "Seongmin Lee",
                "Woo-Jong Ryu",
                "Sungroh Yoon."
            ],
            "title": "Rare tokens degenerate all tokens: Improving neural text generation via adaptive gradient gating for rare token embeddings",
            "venue": "Proceedings",
            "year": 2022
        },
        {
            "authors": [
                "Zhong Zhang",
                "Chongming Gao",
                "Cong Xu",
                "Rui Miao",
                "Qinli Yang",
                "Junming Shao."
            ],
            "title": "Revisiting representation degeneration problem in language modeling",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020, pages",
            "year": 2020
        },
        {
            "authors": [
                "Shuai Zhao",
                "Linchao Zhu",
                "Xiaohan Wang",
                "Yi Yang."
            ],
            "title": "Centerclip: Token clustering for efficient text-video retrieval",
            "venue": "Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR",
            "year": 2022
        },
        {
            "authors": [
                "Zhun Zhong",
                "Liang Zheng",
                "Donglin Cao",
                "Shaozi Li."
            ],
            "title": "Re-ranking person re-identification with k-reciprocal encoding",
            "venue": "2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017, pages",
            "year": 2017
        },
        {
            "authors": [
                "He"
            ],
            "title": "2016), numerous methods based on deep neural networks have been proposed for image-text retrieval (Radford et al., 2021), video-text retrieval (Luo et al., 2022), and audio-text retrieval (Oncescu et",
            "year": 2021
        },
        {
            "authors": [
                "Miech"
            ],
            "title": "2021), query expansion (Chen et al., 2020), vector compression schemes based on binary codes (Su et al., 2019; Liong et al., 2017) and quantization (Gong et al., 2013) that help address the curse of dimensionality (Keogh",
            "year": 2013
        },
        {
            "authors": [
                "Ma et al",
                "Park"
            ],
            "title": "2022), using both the official (full) split and the 1k-A split. The full split includes 2,990 videos for testing and 497 for validation, whereas the 1k-A split has 1,000 videos for testing and around",
            "year": 2022
        },
        {
            "authors": [
                "Niebles"
            ],
            "title": "2015): Contains 20k videos and approximately 100K descriptive sentences",
            "year": 2015
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Cross-modal retrieval (CMR) (Wang et al., 2021; Yu et al., 2023; Kim et al., 2023), which aims to enable flexible retrieval across different modalities, e.g., images, videos, audio, and text, has attracted significant research interest in the last few decades. The goal of CMR is to learn a pair of encoders that\n\u2217Corresponding author.\nmap data from different modalities into a common space where they can be directly compared. For example, in text-to-video retrieval, the objective is to rank gallery videos based on the features of the query text. Recently, inspired by the success in self-supervised learning (Radford et al., 2021), significant progress has been made in CMR, including image-text retrieval (Radford et al., 2021; Li et al., 2020; Wang et al., 2020a), video-text retrieval (Chen et al., 2020; Cheng et al., 2021; Gao et al., 2021; Lei et al., 2021; Ma et al., 2022; Park et al., 2022; Wang et al., 2022a,b; Zhao et al., 2022;\nWang and Shi, 2023; Wang et al., 2023), and audiotext retrieval (Oncescu et al., 2021), with satisfactory retrieval performances.\nHowever, Liang et al. (2022) demonstrate that the current common learning paradigm of CMR leads to the representation degeneration problem, which concentrates all the representations in a small (convex) cone (Gao et al., 2019; Zhang et al., 2020) in image-text retrieval and the cosine similarity between any two points is positive, as shown in Figure 1a. Consequently, retrieval performance will be significantly affected (Radovanovic et al., 2010; Gao et al., 2019). Due to the limitation of space, detailed related works are presented in Appendix A.\nIn this paper, to step forward in addressing representation degeneration problem and further improve the retrieval performance, we first empirically test whether it is prevalent across various cross-modal retrieval models and datasets, including video, image, text, and audio. We found that the representations of MSCOCO generated by CLIP are gathered in a very narrow cone in the embedding space, proving the existence of representation degeneration problem, as shown in Figure 1a. This case does not stand alone since similar observations are observed across several cross-modal retrieval models and datasets as shown in Appendix C.3.\nNext, to model how severe the representation degeneration problem is and the relationship between this problem and the retrieval performance, drawing inspiration from previous work (Gao et al., 2019; Zhang et al., 2020; Yu et al., 2022; Liang et al., 2022; Yang et al., 2022; Huang et al., 2021), we define it in CMR as the average similarity between each point and its nearest neighbor in the gallery set, as shown in Equation (2). We observe that the scores are very high across different datasets and methods. They are able to model this problem as a high score always leads to more concentrated data distribution as shown in Appendix C.3.\nWhile CMR has suffered from this problem, on the other side, the graph convolution (Kipf and Welling, 2017) and average pooling (Boureau et al., 2010), which are widely employed in graph neural networks (Gilmer et al., 2017; Kipf and Welling, 2017; Velickovic et al., 2018) and deep neural networks (He et al., 2016; Yu et al., 2023), respectively, are designed to move the representations closer to each other if they are semantically similar (Baran-\nwal et al., 2023). It might lead to the emergence of representation degeneration problem.\nDrawing inspiration from the graph convolution and average pooling, we propose a novel method, INVGC, which separates representations by performing the graph convolution inversely to separate representations with a bigger margin as shown in Figure 1a. Specifically, different from the vanilla graph convolution, considering one modality, INVGC separates representations by subtracting the representation of the neighboring nodes from each node, instead of aggregating them as,\nxi \u2032 = xi \u2212 r \u2211 j \u0338=i Sijxj,\u2200i , (1)\nwhere xi\u2032 and xi are the updated and the original representations, Sij is the similarity between the i-th and j-th data, and r is a predefined hyperparameter. As shown in Figure 1, INVGC better scatter representations and alleviates representation degeneration problem. Moreover, the histogram of similarity between any two points is more balanced with INVGC, as shown in Figure 1. To boost the effectiveness and efficiency of INVGC, we propose an advanced adjacency matrix LOCALADJ that directs INVGC w/LOCALADJ to focus on the nearest neighbors of each data point instead of considering all the data points.\nTo evaluate the effectiveness of INVGC and INVGC w/LOCALADJ, we conducted experiments on eight cross-modal benchmarks (Xu et al., 2016; Chen and Dolan, 2011; Fabian Caba Heilbron and Niebles, 2015; Hendricks et al., 2017; Lin et al., 2014; Plummer et al., 2017; Kim et al., 2019; Drossos et al., 2020). Experimental results show that INVGC alleviates representation degeneration problem across different datasets and methods and improves retrieval performance as a by-product.\nIn summary, our contributions are as follows1:\n\u2022 We are the first to formalize the definition of representation degeneration in cross-modal retrieval and perform a theoretical analysis of the relationship between representation degeneration problem and retrieval performance.\n\u2022 Inspired by the graph convolution, we propose the first post-processing method in crossmodal retrieval, namely INVGC, to alleviate representation degeneration problem without any training process or additional data.\n1The code is released at link.\n\u2022 We design an adjacency matrix, called LOCALADJ, for the graph convolution, which leverages only the nearest neighbors of each data point instead of all the data. INVGC with LOCALADJ, namely INVGC w/LOCALADJ. It is shown to be more effective and efficient.\n\u2022 Extensive experiments show that INVGC and INVGC w/LOCALADJ alleviate representation degeneration problem and improve retrieval performance as a by-product."
        },
        {
            "heading": "2 Preliminaries",
            "text": ""
        },
        {
            "heading": "2.1 Task Definition",
            "text": "In this paper, we focus on the representation degeneration problem in cross-modal retrieval (Wang et al., 2020b). Two modalities are denoted as X and Y . X is the query modality, while Y is the gallery modality. The (test) gallery, denoted G = {g1, . . . ,gNg}, contains all the representations of the (test) gallery data, where Ng is the size. The query set is Q = {q1, . . . ,qNq}, where Nq is the number of queries. Usually, in crossmodal retrieval, the gallery data does not overlap with the training data. Additionally, as INVGC requires training (or validation) data to address the representation degeneration problem, we define the set of representations of training (or validation) query and gallery data as Q\u0302 = {q\u03021, . . . , q\u0302NQ\u0302} and G\u0302 = {g\u03021, . . . , g\u0302NG\u0302}, respectively, where NQ\u0302 and NG\u0302 are the size of the training (or validation) query and gallery set, respectively. The similarity between two embeddings a and b is defined as sa,b = sim(a,b), where sim(\u00b7, \u00b7) could be some measure of distance."
        },
        {
            "heading": "2.2 Representation Degeneration in Cross-modal Retrieval",
            "text": "Taking inspiration from Liang et al. (2022), we define the representation degeneration problem as the average similarity of pairs that are closest to each other,\n\u2206deg = 1\nm \u2211 x sim(x,y) , (2)\nwhere y is the closest data point to x while m is the number of data points. A higher score, as shown in Appendix C.3, indicates a more severe representation degeneration issue, with each data point being closer to its nearest neighbors in the set.\nTo understand the relationship between the degeneration score (Equation (2)) and the retrieval performance, we present the following theorems.\nTheorem 1. Let x1 be any point in G, x2 be the nearest neighbor of x1 and n be the dimension of the representation. Given a query point q that is semantically similar to x1 and sampled from Q, which follows an independent and identical uniform distribution in Rn, the probability of a query point q to successfully retrieve x1, denoted P(x1, b), is bounded by,\nn 2 \u00b7 bn > P(x1, b) > 1 4 \u00b7 bn+1 ,\nwhere b = ||x1 \u2212 x2||2/2. Corollary 2. The ratio between the probability of successful retrieval of any two different neighborhood radii, namely b1 and b2, is\nP(x1, b1) P(x1, b2) = O\n( n \u00b7 ( b1 b2 )n) .\nDue to space limitation, the proofs are deferred to Appendix D.1.\nRemark 1. These theorems show that a high similarity of the nearest neighbor, i.e., smaller b, leads to an exponentially lower probability for successful retrieval. Therefore, a higher \u2206deg score leads to bad retrieval performance."
        },
        {
            "heading": "3 INVGC",
            "text": "To alleviate the representation degeneration problem and further boost the retrieval performance, we design a post-processing method, called INVGC, which does not need any additional training.\nOur idea is generally based on the mathematical principles laid out in Equation (1) and Figure 1, where the inverse version of graph convolution is able to decrease the similarity between data points and their neighbors. For the sake of clear representation, we use the cosine similarity as the similarity metric in this section following Luo et al. (2022), as it is the most common practice in cross-modal retrieval2. The formal definition of INVGC is presented in Section 3.3."
        },
        {
            "heading": "3.1 Mathematical Intuition",
            "text": "INVGC originates from graph convolution, which is widely employed in graph neural networks (Kipf\n2INVGC can be easily migrated to other similarity metrics adopted by different retrieval methods (Croitoru et al., 2021; Liu et al., 2019).\nand Welling, 2017; Gilmer et al., 2017; Velickovic et al., 2018).\nSpecifically, graph convolution will concentrate all the embeddings of similar nodes which might lead to the concentration of similarity (de la Pena and Montgomery-Smith, 1995) and the data degeneration problem (Baranwal et al., 2023). On the other side, a similar operation, average pooling, has been employed in computer vision (He et al., 2016; Wang and Shi, 2023). Average pooling will aggregate the features that are location-based similar3.\nAs a result, graph convolution and average pooling concentrate the representation of all the similar nodes and force them to become very similar to each other, potentially leading to representation degeneration problem. This observation inspires us to pose the following research question:\nCan the issue of representation degeneration problem in cross-modal retrieval be alleviated by conducting inverse graph convolution (INVGC)?\nTo answer this question, we first give an inverse variant of graph convolution as follows,\nxi \u2032 = xi \u2212 \u2211 j Aijxj , (3)\nwhere A is the adjacency matrix for the data in the gallery set. Since the adjacency matrix is not available, to encourage separating the nearest neighbor in terms of the similarity score of each node, we choose Aij = sim(i, j) := Sij with detailed discussion in Section 3.2. Therefore, based on the inverse graph convolution, we propose the basic setting of INVGC as shown in Equation (1), which only considers one modality. We notice that it is able to alleviate the representation degeneration problem as shown in Figure 1.\nNote that the ultimate goal of INVGC is to reduce \u2206deg score of the distribution of the representation of the gallery instead of merely the gallery set G, which can be regarded only as a sampled subset of the distribution with very limited size in practice. Therefore, the best approximation of the distribution is the training (or validation) gallery set G\u0302 since it is the largest one we can obtain4.\nSimilarly, the distribution of the query set Q\u0302 is theoretically expected to be similar to that of G\u0302 as\n3The details of graph convolution and average pooling discussed in this study are deferred to the Appendices B.1 and B.2, respectively.\n4In practice, the test queries are invisible to each other as the queries do not come at the same time. So the size of the query set Ng is equal to 1.\na basic assumption in machine learning (Bogolin et al., 2022). A detailed explanation of the claims is included in Appendix B.3. Moreover, as CMR needs to contrast the data points from both modalities, we utilize the (train or validation) gallery set G\u0302 and the (train or validation) query set Q\u0302 to better estimate the hidden distribution as shown in Equation (4),\nxi \u2032 = xi \u2212 rg \u2211 xj\u2208G\u0302 Sgijxj \u2212 rq \u2211 xj\u2208Q\u0302 Sqijxj . (4)\nwhere rg and rq are two hyperparameters, Sg \u2208 RNg\u00d7NG\u0302 and Sq \u2208 RNq\u00d7NQ\u0302 is the adjacency matrices between every pair of embedding from G and G\u0302 and that from Q and Q\u0302, respectively.\nTo the best of our knowledge, INVGC is the first to utilize the (inverse) graph convolution for separating the representation of data. Instead of the commonly used capability of aggregation and message passing, we introduce an inverse variant of convolution that separates data representations compared to the vanilla graph convolution."
        },
        {
            "heading": "3.2 Constructing Adjacency matrix",
            "text": "Next, we need to establish the graph structure, i.e., build the adjacency matrix Sg and Sq, since there is no natural graph structure in the dataset. The simplest idea will be that the edge weight between i and j equals 1, i.e., Sij = 1, if the cosine similarity between xi and xj, i.e., sim(xi,xj), is larger than 0 (or some thresholds). INVGC with this form of the adjacency matrix is an inverse variant of average pooling. It serves as a baseline in this study, denoted as AVGPOOL.\nHowever, this scheme is not capable to reflect the degree of the similarity between xi and xj since it cannot precisely depict the relation between different data points.\nAs the magnitude of Sij directly controls how far will xi go in the opposite direction of xj, a greedy design will be using the similarity score between them as the edge weight, i.e., Sij = sim(xi,xj).\nTherefore, we can calculate the adjacency matrix Sg, which contains the similarity score between every pair of embedding from G and G\u0302, respectively. Specifically, the (i, j)-entry of Sg follows,\nSgi,j = sim(gi, g\u0302j) . (5)\nSimilarly, the matrix Sq containing the similarity score between every pair of embedding from G and\nQ\u0302 is calculated as follows,\nSqi,j = sim(gi, q\u0302j) . (6)\nNow, with the well-defined Sq and Sg, we can finally perform INVGC to alleviate representation degeneration problem.\nAs shown in Theorem 1, given any data point, the similarity of the nearest neighbor in the representation space is critical to retrieval performance. Inspired by this, when performing the inverse convolution on a node, we force INVGC to pay attention to those most similar nodes to it. This can be achieved by assigning edge weight only to the nodes having the top k percent of the largest similarity scores relative to the given node. Specifically, each entry of Sg and Sq in this case is calculated as follows,\nSg(i, j) = { sim(gi, g\u0302j) , if sim(gi, g\u0302j) \u2265 Pi(G\u0302, k) 0 , else\n(7)\nSq(i, j) = { sim(qi, q\u0302j) , if sim(qi, q\u0302j) \u2265 Pi(Q\u0302, k) 0 , else\n(8)\nwhere Pi(G\u0302, k) is the value of k-percentage largest similarity between node i and all the nodes in G\u0302. The same approach applies to Pi(Q\u0302, k) as well. We denote this refined adjacency matrix as LOCALADJ since it focuses on the local neighborhoods of each node."
        },
        {
            "heading": "3.3 Formal Definition of INVGC",
            "text": "With the well-formed adjacency matrices , we formally define INVGC to obtain the updated embedding of G, denoted as G\u2032, in a matrix form as,\nG\u2032 = 1\n2\n[ norm(G\u2212 rgSgG\u0302) + norm(G\u2212 rqSqQ\u0302) ] ,\n(9)\nwhere norm(\u00b7) normalizes a matrix with respect to rows, which is employed for uniformly distributing the intensity of convolution when cosine similarity is used and should be removed when adopting other similarity metrics and the adjacency matrices Sg and Sq can be calculated as Equations (5) to (7). Note that, compared to Equation (4), we separate the convolution on G\u0302 and Q\u0302 to pursue more robust results, for avoiding the distribution shifts between G\u0302 and Q\u0302 due to the imperfection of the representation method.\nIn summary, INVGC, as shown in Equation (9), is a brand new type of graph operation performed on the specifically designed graph structures (adjacency matrix) of data points in the cross-modal dataset. It helps alleviate the representation degeneration problem by separating data points located close to each other in the representation space.\nAfter obtaining G\u2032, the similarity between the ith gallery points g\u2032 and a query point q is calculated as sim(q,g\u2032)."
        },
        {
            "heading": "4 Experiments",
            "text": "We conduct a series of experiments to demonstrate that INVGC can efficiently alleviate the representation degeneration problem in a post-hoc manner. We compare INVGC and INVGC w/LOCALADJ with the baseline performance produced by the representation model adopted. We also introduce the inverse version of average pooling, namely AVGPOOL, as another baseline. A series of ablation studies indicate that INVGC addresses the representation degeneration problem by reducing the similarity between points in the gallery set and is not sensitive to hyperparameters and the amount of training data."
        },
        {
            "heading": "4.1 Experimental and implementation settings",
            "text": "The implementation of INVGC exactly follows Equation (9) with the adjacency matrix defined in Equations (5) and (6) and a separate pair of tuned hyperparameters rg and rq for each retrieval model of each dataset. To balance the edge weight in Sq and Sg and make sure the scale of weights is stable, they subtract their average edge weights, respectively.\nThe only difference between INVGC w/LOCALADJ and INVGC is the adjacency matrix applied. Instead of the ones from Equations (5) and (6), we apply Sq and Sg defined in Equations (7) and (8). We choose the value of k to be 1 (i.e., top 1% nearest neighbors) throughout the study while the proposed method is very robust to the value of the k. There is a large range of reasonable values that can boost the performance, proved by the ablation study in Section 4.3.\nAVGPOOL is the simplest variant of inverse graph convolution with binary adjacency matrix where the edge weight between a pair of nodes is 1 if they are neighbors, or 0 otherwise. Then, following the approach in LOCALADJ, we update the embeddings using only the top p percent of near-\nest neighbors. To provide a more comprehensive benchmark, we pick four values of p from 10% to 100%. Note that we also independently tune rg and rq for AVGPOOL to make sure the evaluation is fair. The comparison with AVGPOOL validates not only the effectiveness of INVGC but also the importance of the similarity-based adjacency matrix. The detailed setting is included in Appendix C.4."
        },
        {
            "heading": "4.2 Datasets and evaluation metrics",
            "text": "While we mainly focus our experiments on standard benchmarks for text-image retrieval, i.e., MSCOCO (Lin et al., 2014) and Flickr30k (Plummer et al., 2017), we also explore the generalization of INVGC by selecting four text-video retrieval benchmarks (MSR-VTT (Xu et al., 2016), MSVD (Chen and Dolan, 2011), Didemo (Hendricks et al., 2017), and ActivityNet (Fabian Caba Heilbron and Niebles, 2015)) and two textaudio retrieval benchmark (AudioCaps (Kim et al., 2019) and CLOTHO (Drossos et al., 2020)). The details of the benchmarks are deferred to Appendix C.1.\nTo evaluate the retrieval performance of INVGC, we use recall at Rank K (R@K, higher is better), median rank (MdR, lower is better), and mean rank (MnR, lower is better) as retrieval metrics, which are widely used in previous retrieval works (Luo et al., 2022; Ma et al., 2022; Radford et al., 2021)."
        },
        {
            "heading": "4.3 INVGC and INVGC w/LOCALADJ",
            "text": "In this section, to answer a series of questions relating to INVGC and INVGC w/LOCALADJ, we investigate its performance on MSCOCO with CLIP given different settings. Due to space limitations, discussions of the sensitivity of INVGC w/LOCALADJ to k and the complexity of both methods are included in the Appendix.\nRQ1: Is the data degeneration problem alleviated? This problem can be firstly explained by the change of the similarity measure within the test gallery set G. As presented in Table 1, we collect the mean similarity of the gallery set of both tasks for three scenarios, the overall mean (MeanSim), the mean between the nearest neighbor(MeanSim@1), and the mean between nearest 10 neighbors (MeanSim@10). Note that MeanSim@1 is strictly equivalent to \u2206deg(G). It is very clear that INVGC and INVGC w/LOCALADJ do reduce the similarity on all accounts, especially MeanSim@1, indicating a targeted ability to alleviate the data degeneration issue.\nBesides, given the assumption that the distribution of the test query set Q is close to that of G, the similarity score of the gallery set to the query set for both tasks is also worth exploring. Since any point in G should theoretically share an identical representation with its corresponding query in Q and we try to maximize the similarity between them, we\nexclude this similarity score between them. Consequently, we want the similarity to be as small as possible since it reflects the margin of the retrieval task, as a gallery point is supposed to be as far from the irrelevant queries as possible to have a more robust result. We adopt the same metrics as Table 1, and the results are presented in Table 2. Again, we observe a comprehensive decrease in the similarity score, especially between the nearest neighbors. Note that, compared to INVGC, INVGC w/LOCALADJ can better address the representation degeneration problem, validating that the design of LOCALADJ does help alleviate the problem by focusing more on the local information. Not for MSCOCO alone, we witness exactly similar results across all the datasets and methods, whose detail is included in Continuation on RQ1 Appendix C.7.\nRQ2: Is INVGC (or INVGC w/LOCALADJ) sensitive to the hyperparameter rg and rq? To answer the question, we evaluate the R@1 metrics of INVGC with a very large range of hyperparameters respective to the optimal choice adopted. We defer the analysis of INVGC w/LOCALADJ to Appendix C.7. For each task, we fix one of rg or rq\nand tune the other to show the change on the R@1 metrics. The results of the MSCOCO dataset are shown in Figure 2. Although subject to some variation, INVGC constantly outperforms the baseline, which is presented as the red dashed line. This indicates that the proposed method can consistently improve performance with a very large range of parameters.\nRQ3: How much data is needed for both proposed methods? Since we use the training query and gallery set as a sampled subset from the hidden distribution of representation, it is quite intuitive to ask if the performance of INVGC or INVGC w/LOCALADJ is sensitive to the size of this sampled subset. Therefore, we uniformly sample different ratios of data from both the training query and gallery set at the same time and evaluate the performance of both methods with the identical hyperparameters, as presented in Figure 3. From the results, we conclude that both proposed methods perform stably and robustly regardless of the size of the training data.\nRQ4: Is INVGC w/LOCALADJ sensitive to the hyperparameter k? To address the question, we evaluate the R@1 metrics of INVGC w/LOCALADJ with a very large range of possible k values (even in logarithmic scale) compared to the optimal choice adopted(i.e.,k = 1). For each task, we fix everything except the k value.The results of MSCOCO dataset are shown in Figure 4. Compared with the baseline, it is safe to claim that the proposed INVGC w/LOCALADJ is very robust to the choice of k."
        },
        {
            "heading": "4.4 Quantative results",
            "text": "In this section, we present the quantitative results of four cross-modal retrieval benchmarks. Across eight different benchmarks, INVGC and INVGC w/LOCALADJ significantly improve upon the baselines, demonstrating the superiority of our proposed methods.\nText-Image Retrieval. Results are presented in Tables 3 and 4. We observe that one of our methods achieves the best performance on R@1, R@5, and R@10 by a large margin. When evaluated on the CLIP method, INVGC w/LOCALADJ outperforms\nthe baselines on both the MSCOCO and Flickr30k datasets, improving R@1 and R@5 by at least 2% compared to all baselines.\nText-Video Retrieval. Results are presented in Tables 5 to 7. We can also conclude that one of our methods achieves the best performance on R@1,\nR@5, and R@10. Specifically, on the ActivityNet dataset, INVGC w/LOCALADJ shows excellent performance with both CLIP4CLIP and X-CLIP methods, significantly outperforming all the baselines on R@1 and R@5 roughly by 2%.\nText-Audio Retrieval. Results are presented in Tables 8 and 9. On the CLOTHO dataset, INVGC exhibits significantly better performance compared to all the baselines while INVGC w/LOCALADJ achieves the best results on the AudioCaps dataset.\nIn summary, our experiments demonstrate that employing INVGC consistently improves retrieval performance across different datasets and retrieval tasks. The models with INVGC demonstrate better accuracy and ranking in retrieving relevant videos, images, and textual descriptions based on given queries. The complete results of retrieval performance can be found in Appendix C.6."
        },
        {
            "heading": "5 Conclusion",
            "text": "This paper addressed representation degeneration problem in cross-modal retrieval, which led to a decrease in retrieval performance. The representation degeneration problem was validated across multiple benchmarks and methods. To alleviate this issue, we proposed a novel method called INVGC, inspired by graph convolution and average pooling. The method established a graph topology structure within the datasets and applied graph convolution in an inverse form with subtraction over the neighborhood. Additionally, we designed the adjacency matrix, LOCALADJ, that only leveraged the nearest neighbors of each data point rather than the entire dataset, resulting in a more effective and efficient method, INVGC w/LOCALADJ. Both INVGC and INVGC w/LOCALADJ were validated through theoretical analysis and demonstrated their ability to separate representations. Finally, extensive experi-\nments on various cross-modal benchmarks showed that both of our methods successfully alleviated the problem of representation degeneration and, as a result, improved retrieval performance."
        },
        {
            "heading": "Limitations",
            "text": "First, although INVGC has been validated through theoretical analysis and demonstrated its efficacy in separating representations, its performance may vary across different datasets and modalities. The effectiveness of our method might be influenced by variations in dataset characteristics, such as data distribution, scale, and complexity. Further investigation and experimentation on a wider range of datasets are needed to fully understand the generalizability of INVGC and its performance under diverse conditions.\nSecond, while our method shows promising results in alleviating the representation degeneration problem, it is worth noting that cross-modal retrieval tasks can still pose challenges due to inherent differences in modalities. Variations in feature spaces, data modalities, and semantic gaps between modalities may limit the overall retrieval performance. Future research efforts should focus on exploring complementary techniques, such as multimodal fusion, attention mechanisms, or domain adaptation, to further enhance the retrieval accuracy and alleviate representation degeneration problem.\nIn the future, it would be interesting to explore the performance of INVGC on diverse datasets, the challenges associated with cross-modal differences, and better definitions or metrics for measuring representation degeneration problem."
        },
        {
            "heading": "A Related Work",
            "text": "We review prior work in cross-modal retrieval and representation degeneration, which are the two most related areas to our work.\nCross-modal Retrival. The goal of cross-modal retrieval is to learn a common representation space, where the similarity between samples from different modalities can be directly measured. Recently, inspired by the success of deep learning (Devlin et al., 2019; He et al., 2016), numerous methods based on deep neural networks have been proposed for image-text retrieval (Radford et al., 2021), video-text retrieval (Luo et al., 2022), and audio-text retrieval (Oncescu et al., 2021). Further, to learn a better representation space, visionlanguage pretraining (Gan et al., 2020; Li et al., 2020; Singh et al., 2022) on large-scale unlabeled cross-modal data has been widely employed and have shown promising performance. Motivated by this, recent works have attempted to pretrain or fine-tune cross-modal retrieval models, e.g., imagetext retrieval (Radford et al., 2021; Li et al., 2020), video-text retrieval (Chen et al., 2020; Cheng et al., 2021; Gao et al., 2021; Gorti et al., 2022; Lei et al., 2021; Ma et al., 2022; Park et al., 2022; Wang et al., 2022a,b; Zhao et al., 2022), and audio-text retrieval (Oncescu et al., 2021) in an end-to-end manner.\nIn contrast to the methods that focus on improving the representation learning ability, another line of cross-modal retrieval research has focused on improving the effectiveness of retrieval, including k-d trees (Bellman, 2015), re-ranking (Zhong et al., 2017; Miech et al., 2021), query expansion (Chen et al., 2020), vector compression schemes based on binary codes (Su et al., 2019; Liong et al., 2017) and quantization (Gong et al., 2013) that help address the curse of dimensionality (Keogh and Mueen, 2017). However, a recent study (Liang et al., 2022) shows that the representation degeneration problem has significantly affected the performance of multi-modal learning. To investigate the influence of representation degeneration problem in the cross-modal retrieval, we show that representation degeneration problem widely exists in different datasets and models.\nRepresentation degeneration. The representation degeneration problem was first introduced in the natural language processing (NLP) area (Gao et al., 2019). It was found that when training a model for natural language generation tasks\nthrough likelihood maximization with the weighttying trick, especially with large training datasets, many of the learned word embeddings tend to degenerate and be distributed into a narrow cone. This limitation largely reduces the representation power of word embeddings. The representation degeneration problem leads to an increase in the overall similarity between token embeddings, which has a negative effect on the performance of the models. It was noted that Laplacian regularization can address this problem better than cosine regularization through theoretical proof and empirical experiments (Zhang et al., 2020). Subsequent work highlighted (Yu et al., 2022) that the training dynamics of the token embeddings focus on rare token embedding which leads to the degeneration problem for all tokens. To this end, they use adaptive gradient gating which gates the specific part of the gradient for rare token embeddings and thus better alleviates the data degeneration problem.\nThough representation degeneration has been explored in NLP, it remains unexplored in multimodal learning for a long time. A recent work (Liang et al., 2022) shows that the representation generated by a common deep neural network is restricted to a narrow cone and consequently, with two modality-dependent encoders, the representations from the two modalities are clearly apart during the whole training procedure. Further, they also show that varying the modality gap distance has a significant impact on improving the model\u2019s downstream zero-shot classification performance and fairness.\nTo step forward towards better representation learning in cross-modal retrieval, different from the previous methods in NLP which focus on addressing this problem in the training procedure, we propose a novel method, namely INVGC, which proposes to avoid representation degeneration in a post-processing manner. Inspired by the representation aggregation induced by graph convolution (Keogh and Mueen, 2017; Baranwal et al., 2023), we utilize the graph convolution in an opposite way to separate the data points that share similar representation. As the first method in solving the representation degeneration problem in crossmodal retrieval, INVGC does not require retraining the model or any other time-consuming operation. INVGC achieves better retrieval performance with a larger margin between different representations compared to the baselines and does this faster."
        },
        {
            "heading": "B Elaborations on Methodologies",
            "text": ""
        },
        {
            "heading": "B.1 Graph Convolution",
            "text": "Graph convolution is a mathematical operation that transforms the features of nodes in a graph based on their local neighborhoods. The objective is to learn a function of signals/features on a graph, which takes into account the graph structure and the node features. It can be regarded as a generalization of convolutions to non-Euclidean data (Bruna et al., 2014). The operation is first introduced and popularized in the work of Graph Convolution Networks (Kipf and Welling, 2017), which is considered one of the most seminal papers in the area of graph learning.\nThe main idea behind a graph convolution operation is to generate a new representation for each node that captures the local neighborhood information around it. This is usually achieved by aggregating feature information from a node\u2019s direct neighbors, sometimes including the node itself. Formally, it can be defined as a special case of a Message Passing Network (MPNN), in which vector messages are exchanged between nodes and updated using neural networks (Gilmer et al., 2017). The basic operation of MPNN can be expressed as (Hamilton, 2020)\nx (k+1) i =UPDATE (k) ( x (k) i ,\nAGGREGATE(k) ({\nx (k) j , \u2200j \u2208 N (i) }) =UPDATE(k) ( x (k) i ,m (k) N (i) ) ,\nwhere UPDATE and AGGREGATE are arbitrary differentiable functions (i.e., neural networks) and x(k)i is the embedding(representation) of node i at k-th iteration. mN (i) is the \"message\" that is aggregated from i \u2019s graph neighborhood N (i).\nIn this study, we adopt a simple message passing operator that performs non-trainable propagation since we want to propose a post-processing method with any training. The adopt operator is actually the backbone of multiple GNN studies, which can be expressed as (GraphConv),\nx\u2032i = \u2295\nj\u2208N (i)\neji \u00b7 xj ,\nwhere \u2295\ndefines a custom aggregation scheme. x\u2032i is updated representation of node i and eji is edge weight between node i and j.\nFor the sake of simplicity and explainability, we concretize the above operation only with simple addition and self-loop, as follows,\nxi \u2032 = xi + \u2211 j Aijxj . (10)\nNote that the only difference with Equation (10) and inverse convolution Equation (3) we apply in the study is that addiction is replaced with subtraction, leading to the name \u2019inverse\u2019."
        },
        {
            "heading": "B.2 Average Pooling in CNN",
            "text": "Average pooling is one type of pooling layers that conducts dimensionality reduction, reducing the number of parameters in the input (Goodfellow et al., 2016). Similar to the convolutional layer, the pooling operation sweeps a filter across the entire input, but the difference is that this filter does not have any weights. Instead, the kernel applies an aggregation function to the values within the receptive field. Specifically, the adopted average pooling calculates the average value within the receptive field of the filter as it moves across the input. Here in this study, the receptive field is subject to the size of the neighborhood of each data point."
        },
        {
            "heading": "B.3 Distribution of Data Representation",
            "text": "While we keep using the discrete gallery and query set like in Equation (3), they can be regarded as sampled results from the hidden continuous distribution dependent on the intrinsic properties of the corresponding dataset and the representation learning method applied. Therefore, ideally, two gallery sets sampled from the same dataset will probably have different \u2206deg scores even with the same representation learning method due to the variance introduced by the sampling process. That is to say, performing the same inverse convolution operation on the two sampled gallery sets might have quite different effects, especially when the sample size is small. Also, since the proposed method in this study is a post-processing approach without any training, we need to control the magnitude of the convolution with the help of some hyperparameters. The reason for doing this is to control the change in the representation of the gallery data that has already been aligned with the embedding space of query data by the representation learning model. Given the sampled gallery set is small, this means a very large variance in the value of the best hyperparameters as well.\nUnfortunately, it is quite common in practice that we only have a small sampled gallery set. Usually, when cross-modal retrieval is carried out, we constantly cut down the size of the gallery set with the help of some pre-ranking or indexing techniques. This process can be somehow regarded as sampling a set from the distribution that is empirically represented by the whole gallery set. Also, during the evaluation of any method on various datasets, the size of the test or evaluation gallery set is typically much smaller compared to the training set. Both cases make the result of the proposed methods subjected to potentially large variance.\nHowever, it would be more promising that our method is generally stable and robust to any size of the gallery set. The ideal case would be that we can perform the inverse convolution similar to Equation (1) but based on the continuous distribution, where P(xj) is data point xj to be sampled from the hidden distribution, as follows,\nxi \u2032 = xi \u2212 r \u222b xj Sij P(xj)xj . (11)\nHowever, it is impossible to have exact access to this hidden distribution in practice. The best approximation is the training (or validation) gallery set G\u0302 since it is the largest one we can obtain. Therefore, we can perform the inverse convolution on G\u0302. Note that the distribution of the query set Q\u0302 should theoretically be similar to that of G\u0302 as this a basic assumption in machine learning (Bogolin et al., 2022). Therefore, it is possible to combine the (train or validation) gallery set G\u0302 and the (train or validation) query set Q\u0302 to be the even better estimation of the hidden distribution. Thus, we go on to refine INVGC as in Equation (4),\nIn general, with reasonable and general assumptions, we strike the importance of utilizing the data of both the modality from the training set when we want to capture a more accurate and stable distribution of data representation. The idea is not bound to the proposed methods and can be adopted by any future work on the post-processing of cross-modal retrieval tasks."
        },
        {
            "heading": "C Experiments",
            "text": ""
        },
        {
            "heading": "C.1 Datasets Details",
            "text": "The experiments are conducted on eight crossmodal benchmarks, which include four videotext retrieval benchmarks (MSR-VTT (Xu et al.,\n2016), MSVD (Chen and Dolan, 2011), ActivityNet (Fabian Caba Heilbron and Niebles, 2015), and DiDemo (Hendricks et al., 2017)), two imagetext retrieval benchmarks (MSCOCO (Lin et al., 2014) and Flickr30k (Plummer et al., 2017)), as well as two audio-text retrieval benchmarks (AudioCaps (Kim et al., 2019) and CLOTHO (Drossos et al., 2020)). The details of the datasets are presented below:\n\u2022 MSR-VTT (Xu et al., 2016): Comprises approximately 10k videos, each accompanied by 20 captions. For text-video retrieval, we follow the protocol set by previous works (Liu et al., 2019; Croitoru et al., 2021; Luo et al., 2022; Ma et al., 2022; Park et al., 2022), using both the official (full) split and the 1k-A split. The full split includes 2,990 videos for testing and 497 for validation, whereas the 1k-A split has 1,000 videos for testing and around 9,000 for training.\n\u2022 MSVD (Chen and Dolan, 2011): Contains 1,970 videos and about 80k captions. The standard split used in prior works (Liu et al., 2019; Croitoru et al., 2021; Luo et al., 2022; Park et al., 2022) is adopted for reporting results, which includes 1,200 videos for training, 100 for validation, and 670 for testing.\n\u2022 ActivityNet (Fabian Caba Heilbron and Niebles, 2015): Contains 20k videos and approximately 100K descriptive sentences. These videos are extracted from YouTube. We employ a paragraph video retrieval setup as defined in prior works (Liu et al., 2019; Croitoru et al., 2021; Luo et al., 2022; Park et al., 2022). We report results on the val1 split. The training split includes 10,009 videos, with 4,917 videos allocated for testing.\n\u2022 DiDemo (Hendricks et al., 2017): Includes over 10,000 personal videos, each lasting between 25-30 seconds, along with over 40,000 localized text descriptions. The videos are divided into training (8,395), validation (1,065), and testing (1,004) sets.\n\u2022 MSCOCO (Lin et al., 2014): Consists of 123k images, each accompanied by 5 captions. The 5k split is used for evaluation.\n\u2022 Flickr30k (Plummer et al., 2017): This dataset contains 31,000 images collected from"
        },
        {
            "heading": "C.2 Experiment Details",
            "text": "The public codes and weights of all the tasks in this study are summarized in Table 10. Note that there is no available trained model for CLIP and X-CLIP. Therefore, we train both models on a single A100 GPU with the hyperparameters recommended by the original studies."
        },
        {
            "heading": "C.3 Prevalence of Representation Degeneration Problem across Datasets and Methods",
            "text": "To show that the representation degeneration problem prevails in all datasets, methods, and tasks, we perform the same analysis in Figure 1a. We uniformly sample a subset of the gallery set of both retrieval tasks (i.e., text to other modality or other modality to text, other modalities can be video, image, or audio depending on the dataset), and perform PCA upon it to reduce the dimension of the representations down to 2, which is the first two principal dimensions. The results are presented in Figure 5. Note that \u2206deg (i.e., the degree of representation degeneration problem) included in each figure is the one for the complete gallery set instead of the sampled set used in the figure.\nFor this qualitative but very intuitive study, We firmly validate again that almost all the data repre-\nsentations gathered in a very narrow cone in the embedding space for basically all the datasets, methods, and tasks. Also, though subject to the difference between datasets and methods, we can witness that a more convex-shaped distribution usually generally leads to a larger \u2206deg.\nThe results imply the universality of the representation degeneration problem. More quantitative results can be found in RQ1 in Section 4.3 and in the Continuation on RQ1 section (Appendix C.7)."
        },
        {
            "heading": "C.4 Detail Setting of AVGPOOL",
            "text": "Following Equation (9), the adjacency matrices Sgpool and S q pool of AVGPOOL can be presented as,\nSgpool(i, j) = { 1 , sim(gi, g\u0302j) \u2265 Pi(G\u0302, p) 0 , else\nand also,\nSqpool(i, j) = { 1 , sim(gi, q\u0302j) \u2265 Pi(Q\u0302, p) 0 , else ,\nwhere Pi(G\u0302, p) and Pi(Q\u0302, p) are the value of ppercentage largest similarity between node i and all the nodes in set G\u0302 and Q\u0302, respectively. We include four values of p, namely [10, 25, 50, 100], to obtain a solid benchmark. All four benchmarks for the same task share identical hyperparameters rg and rq. Note that the worst scenario of tuning for AVGPOOL is when there is no hyperparameter that can enable AVGPOOL to beat the baseline performance of the original retrieval model. But it doesn\u2019t happen during the experiment since we can always locate a pair of hyperparameters with a little effort with which AVGPOOL with at least one of four selected p values can outperform the baseline on at least one of the evaluation metrics mentioned in Section 4.2. This indicates the effectiveness of the idea of inverse convolution."
        },
        {
            "heading": "C.5 Computational Complexity",
            "text": "In the section, we discuss the computation complexity of the proposed INVGC and INVGC w/LOCALADJ. As noted Section 2.1, Ng is the size of gallery data, and NQ\u0302 and NG\u0302 are the size of the training (or validation) query and gallery set, respectively. We also assume Nq is the size of the query.\nBoth methods need to precompute two adjacency matrices between the gallery set and both sets of training (or validation) data before performing the convolution, which costs O(Ng(NG\u0302 + NQ\u0302)) as\nmatrix multiplication. Then, the matrix multiplication of inverse convolution operation also incurs a cost of O(Ng(NG\u0302 +NQ\u0302)) for INVGC and O(k\u22121Ng(NG\u0302 +NQ\u0302)) for INVGC w/LOCALADJ, for the pruned matrix LOCALADJ. Note that if we choose a small k for LOCALADJ like 1% in the empirical study, the convolution step can be much faster in practice (though the complexity doesn\u2019t change due to the pre-computation step).\nAfter performing the proposed methods, we need another O(NqNg) time to calculate the similarity between the query and gallery set when performing the inference, given that we don\u2019t consider any advanced trick to perform argmax.\nIn sum, both INVGC and INVGC w/LOCALADJ incur computational cost of O(Ng(NG\u0302 +NQ\u0302)) for inverse graph convolution (before inference) and O(NqNg) for inference."
        },
        {
            "heading": "C.6 More Quantitative Results",
            "text": "The results on retrieval performance for all the datasets with various methods are presented in Tables 11 to 18. The results indicate that both proposed methods achieve significantly better retrieval performance on all the tasks compared to the original baseline and AVGPOOL benchmarks.\nText-Image and Image-Text Retrieval. Results are presented in Tables 11 and 12.\nOn the MSCOCO (5k split) dataset, the INVGC w/LOCALADJ method outperforms other methods in both text-to-image retrieval and image-to-text Retrieval for both CLIP and Oscar models. Specifically, for the CLIP model, INVGC w/LOCALADJ achieves the best R@1 and R@5 in text-to-image retrieval and the best R@1, R@5, R@10, and MnR scores in image-to-text retrieval. Similarly, for the Oscar model, INVGC w/LOCALADJ also delivers the best results, with the best R@1 and R@5 in\ntext-to-image retrieval. In the Flickr30k dataset, the INVGC w/LOCALADJ method generally shows the highest performance in both text-to-image retrieval and image-to-text retrieval across both CLIP and Oscar models.\nText-Video and Video-Text Retrieval. Results are presented in Tables 13 to 16.\nFrom the table for MSR-VTT (full split), we can observe that INVGC w/LOCALADJ demonstrated the best performance in both text-to-video and video-to-text retrieval on R@1, R@5, and R@10 with both methods. The results for MSR-VTT (1k\nsplit) show that INVGC w/LOCALADJ generally achieves the best performance in text-to-video retrieval while INVGC presents superior results in video-to-text retrieval.\nFor the ActivityNet dataset, INVGC w/LOCALADJexhibits superior performance in text-to-video and video-to-text retrieval on R@1, R@5, and R@10 with all four methods.\nFor the MSVD dataset, the best-performing method for text-to-video retrieval is INVGC w/LOCALADJ since it has the best performance on R@1, R@5, and R@10 with all methods except X-CLIP. Meanwhile, INVGC method has the best\nresults for video-to-text retrieval with CLIP4CLIP, CLIP2Video, and X-CLIP.\nOn the DiDeMo dataset, INVGC w/LOCALADJ with both methods achieve the best performance in the text-to-video and video-to-text retrieval on R@1 and R@5.\nText-Audio and Audio-Text Retrieval. Results are presented in Tables 17 and 18. On the CLOTHO dataset, the AR-CE method enhanced with the INVGC and INVGC w/LOCALADJ techniques outperforms the earlier techniques, MoEE and MMT, and all other benchmarks. Specifically, either INVGC or INVGC w/LOCALADJ achieves the best results in R@1, R@5, and R@10 in both text-to-audio and audio-to-text retrieval tasks.\nIn the case of the AudioCaps dataset, INVGC w/LOCALADJ method performs best across multiple metrics in both text-to-audio and audio-to-text retrieval tasks."
        },
        {
            "heading": "C.7 More Ablation Study",
            "text": "Continuation on RQ1: Is the data degeneration problem alleviated?\nTo strengthen the conclusion that both INVGC and INVGC w/LOCALADJhave strong capability to alleviate representation degeneration problem, we conduct a comprehensive experiment on all the datasets and methods as an extension to Table 1 and Table 2. We have Tables 19 to 26 that presents the mean similarity within the test gallery set of both tasks for three scenarios, the overall mean (MeanSim), the mean between the nearest neighbor(MeanSim@1), and the mean between nearest 10 neighbors (MeanSim@10). And we have Tables 27 to 34 include the similarity score from the test gallery set to the test query set with the same evaluation metrics.\nIt is quite obvious that both INVGC and INVGC w/LOCALADJ help decrease the similarity score on all metrics, especially on MeanSim@1(i.e., \u2206deg(G)). Also, both methods have better per-\nformance on the retrieval task from text to other modalities, which is the more important task in practice compared to the other direction.\nBased on the results, we can safely draw the conclusion that both proposed methods are able to\nsignificantly alleviate the representation degeneration problem by significantly reducing \u2206deg(G) score.\nContinuation on RQ2: Is INVGC w/LOCALADJ sensitive to the hyperparameter\nrg and rq? To address the question, like analysis on INVGC, we assess the R@1 metrics of our proposed method under a wide range of hyperparameters compared to the optimal choice adopted. For each task, one of rg or rq is fixed and the other is tuned to observe its impact on the R@1 metrics. The results\nobtained from the MSCOCO dataset are depicted in Figure 6. Despite some observed variations, our method consistently outperforms the baseline, represented by the red dashed line. This suggests that the proposed method can continuously enhance performance even when parameters are varied over a broad range."
        },
        {
            "heading": "D Proofs",
            "text": ""
        },
        {
            "heading": "D.1 Proof of Theorem 1",
            "text": "To prove the theorem, we start with some preliminary definitions. We denote the n-dimensional unit sphere as,\nSn\u22121 := {x \u2208 Rn : \u2225x\u22252 = 1}\nand n-dimensional ball,\nThen, n-dimensional ball representing the neighborhood of x with radius a can be denoted as Bn,x,a := {y \u2208 Rn : ||y \u2212 x||2 \u2264 a}. We go on to denote spherical caps Cn,x,a as\nCn,x,b := Sn\u22121 \u2229 Bn,x,b\nNote that the radius a here is not the neighborhood coverage b, where they have a relation as a2 = 2 \u221a a2 \u2212 b2 by simple triangle calculation. Since the basis of CR,x,a is an n\u2212 1 dimensional hypersphere of radius b and b is the key factor we\nare interested in, we denote the spherical caps using b as,\nCapn,x,b := Cn,x,a\nSince the cosine similarity applied is indepen-\ndent of the norm of each point in G and Q, we can assume without loss of generality that all the points in G and Q are on the unit sphere, i.e. \u2200x \u2208 G \u222aQ, x \u2208 Sn\u22121. Since query data follows an independent and identical uniform distribution in Rn, therefore it still follows an independent and identical uniform distribution on Sn\u22121.\nNote that the probability of x1 is correctly retrieved is lower bounded by the probability that the corresponding query point q \u2208 Capn,x1,b, for q must be the nearest neighbor of x1.\nGiven uniform distribution, the probability is A(Capn,x1,b)\nA(Sn\u22121) , where the A is the operator for area of hypersurface.\nGenerally, bounding volume is easier than the surface area, so we have the following Lemma Lemma 3 to help establish the relationship between the two.\nLemma 3. Therefore, the relationship between the surface area A(Capn,x,b) and volumn V(Capn,x,b) 5 of n-dimensional sphere cap is,\nA(Capn,x,b) = nV(Capn,x,b) (12)\nProof. We have the relationship between the surface area A(Sn\u22121) and volume V (Sn\u22121) 6 of ndimensional unit hypersphere as,\nA(Sn\u22121) = d\ndr V(Sn\u22121) = nV(Sn\u22121) (13)\nTherefore, the same relationship can be adapted to sphere cap as well.\nWith Lemma 3, we only need to bound the volume V(Capn,x,b), which is given by the following Theorem 4.\nTheorem 4. Given n-dimensional unit sphere Sn\u22121 and spherical caps Capn,x,b on it, we have,\n1 2 \u00b7 bn >\nV ( Capn,x,b ) V (Sn\u22121) > 1 4n \u00b7 bn+1.\nProof. Lower bound: Follow the proof of Lemma 4.1 in (Micciancio and Voulgaris, 2010). The basis\n5Every time we mention the hypervolume of a sphere V(Capn,x,b) where Capn,x,b = Cn,x,a = Sn\u22121 \u2229 Bn,x,a, we actually always refer to that of V(Bn,1 \u2229 Bn,x,a). We keep using expressions like V(Capn,x,b) to prevent possible confusion when dealing with the area and volume at the same time\n6When mentioning the hypervolume of a sphere Sn\u22121, we actually always refer to that of Bn,1.\nof CapR,x,b is an n\u2212 1 dimensional hypersphere of radius r = b, denoted as Bn\u22121,b. Therefore Capn,x,b includes a cone C1 with basis as Bn\u22121,b and height h = 1\u2212 \u221a 1\u2212 b2. Then, a cylinder C2 with the same basis but height 2 \u00b7 b includes Bn,b. Note that we have h > b2/2 by Taylor expansion. Based on all the facts, we have:\nV ( Capn,x,b ) > V (C1) = V (Bn\u22121,b) h\nn\n= V (C2) h\n2bn\n> V (Bn,b) h\n2bn\n> V (Bn,b) b\n4n\nTherefore,\nb 4n \u00b7 V ( Capn,x,b ) V (Sn\u22121) > b 4n \u00b7 V (Bn,b) V (Bn,1)\n= b 4n \u00b7 ( b 1 )n = 1\n4n \u00b7 bn+1\nUpper bound: Based on proof of Lower bound, we notice that half of Bn,b includes Capn,x,b, we have\nV ( Capn,x,b ) <\nV (Bn,b) 2\nTherefore, V ( Capn,x,b ) V (Sn\u22121) < 1 2 \u00b7 V (Bn,b) V (Bn,1)\n= 1\n2 \u00b7 bn\nFinally, we finish the proof using Lemma 3."
        },
        {
            "heading": "D.2 Proof of Corollary 2",
            "text": "Proof. Using the result from Theorem 1, we have the following inequalities for the probability of successful retrieval of b1 and b2,\nP(x, b1) < n\n2 \u00b7 bn1 ,\nAnd,\nP(x, b2) > 1\n4 \u00b7 bn+12 ,\nTherefore, we have,\nP(x1, b1) P(x1, b2) < 2n b2 \u00b7 ( b1 b2 )n .\nNote that 2/b2 is constant with respect to n, so we finish the proof."
        }
    ],
    "title": "INVGC: Robust Cross-Modal Retrieval by Inverse Graph Convolution",
    "year": 2023
}