{
    "abstractText": "Inverted file structure is a common technique for accelerating dense retrieval. It clusters documents based on their embeddings; during searching, it probes nearby clusters w.r.t. an input query and only evaluates documents within them by subsequent codecs, thus avoiding the expensive cost of exhaustive traversal. However, the clustering is always lossy, which results in the miss of relevant documents in the probed clusters and hence degrades retrieval quality. In contrast, lexical matching, such as overlaps of salient terms, tends to be strong feature for identifying relevant documents. In this work, we present the Hybrid Inverted Index (HI), where the embedding clusters and salient terms work collaboratively to accelerate dense retrieval. To make best of both effectiveness and efficiency, we devise a cluster selector and a term selector, to construct compact inverted lists and efficiently searching through them. Moreover, we leverage simple unsupervised algorithms as well as end-to-end knowledge distillation to learn these two modules, with the latter further boosting the effectiveness. Based on comprehensive experiments on popular retrieval benchmarks, we verify that clusters and terms indeed complement each other, enabling HI to achieve lossless retrieval quality with competitive efficiency across various index settings. Our code and checkpoint are publicly available at https://github.com/ namespace-Pt/Adon/tree/HI2.",
    "authors": [
        {
            "affiliations": [],
            "name": "Peitian Zhang"
        },
        {
            "affiliations": [],
            "name": "Zheng Liu"
        },
        {
            "affiliations": [],
            "name": "Shitao Xiao"
        },
        {
            "affiliations": [],
            "name": "Zhicheng Dou"
        },
        {
            "affiliations": [],
            "name": "Jing Yao"
        }
    ],
    "id": "SP:8ab3c4fed0fad7dd038d8010d432bec7df0808de",
    "references": [
        {
            "authors": [
                "Dmitry Baranchuk",
                "Artem Babenko",
                "Yury Malkov."
            ],
            "title": "Revisiting the inverted indices for billion-scale approximate nearest neighbors",
            "venue": "Computer Vision - ECCV 2018 - 15th European Conference, Munich, Germany, September 8-14, 2018, Proceedings, Part",
            "year": 2018
        },
        {
            "authors": [
                "Jon Louis Bentley."
            ],
            "title": "Multidimensional binary search trees used for associative searching",
            "venue": "Commun. ACM, 18(9):509\u2013517.",
            "year": 1975
        },
        {
            "authors": [
                "Andrei Z. Broder",
                "David Carmel",
                "Michael Herscovici",
                "Aya Soffer",
                "Jason Y. Zien."
            ],
            "title": "Efficient query evaluation using a two-level retrieval process",
            "venue": "Proceedings of the 2003 ACM CIKM International Conference on Information and Knowledge Manage-",
            "year": 2003
        },
        {
            "authors": [
                "Qi Chen",
                "Bing Zhao",
                "Haidong Wang",
                "Mingqin Li",
                "Chuanjie Liu",
                "Zengzhong Li",
                "Mao Yang",
                "Jingdong Wang."
            ],
            "title": "Spann: Highly-efficient billionscale approximate nearest neighbor search",
            "venue": "35th Conference on Neural Information Processing Sys-",
            "year": 2021
        },
        {
            "authors": [
                "David R. Cheriton"
            ],
            "title": "From doc2query to doctttttquery",
            "year": 2019
        },
        {
            "authors": [
                "Zhuyun Dai",
                "Jamie Callan."
            ],
            "title": "Context-aware sentence/passage term importance estimation for first stage retrieval",
            "venue": "CoRR, abs/1910.10687.",
            "year": 2019
        },
        {
            "authors": [
                "Mayur Datar",
                "Nicole Immorlica",
                "Piotr Indyk",
                "Vahab S. Mirrokni."
            ],
            "title": "Locality-sensitive hashing scheme based on p-stable distributions",
            "venue": "Proceedings of the 20th ACM Symposium on Computational Geometry, Brooklyn, New York, USA, June 8-11,",
            "year": 2004
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
            "year": 2019
        },
        {
            "authors": [
                "Wei Dong",
                "Moses Charikar",
                "Kai Li."
            ],
            "title": "Efficient k-nearest neighbor graph construction for generic similarity measures",
            "venue": "Proceedings of the 20th International Conference on World Wide Web, WWW 2011, Hyderabad, India, March 28 - April 1, 2011,",
            "year": 2011
        },
        {
            "authors": [
                "Thibault Formal",
                "Carlos Lassance",
                "Benjamin Piwowarski",
                "St\u00e9phane Clinchant."
            ],
            "title": "SPLADE v2: Sparse lexical and expansion model for information retrieval",
            "venue": "CoRR, abs/2109.10086.",
            "year": 2021
        },
        {
            "authors": [
                "Luyu Gao",
                "Jamie Callan."
            ],
            "title": "Unsupervised corpus aware language model pre-training for dense passage retrieval",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin,",
            "year": 2022
        },
        {
            "authors": [
                "Luyu Gao",
                "Zhuyun Dai",
                "Zhen Fan",
                "Jamie Callan."
            ],
            "title": "Complementing lexical retrieval with semantic residual embedding",
            "venue": "CoRR, abs/2004.13969.",
            "year": 2020
        },
        {
            "authors": [
                "Tiezheng Ge",
                "Kaiming He",
                "Qifa Ke",
                "Jian Sun."
            ],
            "title": "Optimized product quantization",
            "venue": "IEEE Trans. Pattern Anal. Mach. Intell., 36(4):744\u2013755.",
            "year": 2014
        },
        {
            "authors": [
                "Herv\u00e9 J\u00e9gou",
                "Matthijs Douze",
                "Cordelia Schmid."
            ],
            "title": "Product quantization for nearest neighbor search",
            "venue": "IEEE Trans. Pattern Anal. Mach. Intell., 33(1):117\u2013128.",
            "year": 2011
        },
        {
            "authors": [
                "Herv\u00e9 J\u00e9gou",
                "Romain Tavenard",
                "Matthijs Douze",
                "Laurent Amsaleg."
            ],
            "title": "Searching in one billion vectors: Re-rank with source coding",
            "venue": "Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing, ICASSP 2011, May",
            "year": 2011
        },
        {
            "authors": [
                "Jeff Johnson",
                "Matthijs Douze",
                "Herv\u00e9 J\u00e9gou."
            ],
            "title": "Billion-scale similarity search with gpus",
            "venue": "IEEE Transactions on Big Data, 7(3):535\u2013547.",
            "year": 2019
        },
        {
            "authors": [
                "Vladimir Karpukhin",
                "Barlas Oguz",
                "Sewon Min",
                "Patrick S.H. Lewis",
                "Ledell Wu",
                "Sergey Edunov",
                "Danqi Chen",
                "Wen-tau Yih."
            ],
            "title": "Dense passage retrieval for open-domain question answering",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Nat-",
            "year": 2020
        },
        {
            "authors": [
                "Saar Kuzi",
                "Mingyang Zhang",
                "Cheng Li",
                "Michael Bendersky",
                "Marc Najork."
            ],
            "title": "Leveraging semantic and lexical matching to improve the recall of document retrieval systems: A hybrid approach",
            "venue": "CoRR, abs/2010.01195.",
            "year": 2020
        },
        {
            "authors": [
                "Uszkoreit",
                "Quoc Le",
                "Slav Petrov."
            ],
            "title": "Natural questions: a benchmark for question answering research",
            "venue": "Trans. Assoc. Comput. Linguistics, 7:452\u2013 466.",
            "year": 2019
        },
        {
            "authors": [
                "Jimmy Lin",
                "Xueguang Ma."
            ],
            "title": "A few brief notes on deepimpact, coil, and a conceptual framework for information retrieval techniques",
            "venue": "CoRR, abs/2106.14807.",
            "year": 2021
        },
        {
            "authors": [
                "Jimmy Lin",
                "Xueguang Ma",
                "Sheng-Chieh Lin",
                "JhengHong Yang",
                "Ronak Pradeep",
                "Rodrigo Nogueira."
            ],
            "title": "Pyserini: A Python toolkit for reproducible information retrieval research with sparse and dense representations",
            "venue": "Proceedings of the 44th Annual",
            "year": 2021
        },
        {
            "authors": [
                "Yuxiang Lu",
                "Yiding Liu",
                "Jiaxiang Liu",
                "Yunsheng Shi",
                "Zhengjie Huang",
                "Shikun Feng",
                "Yu Sun",
                "Hao Tian",
                "Hua Wu",
                "Shuaiqiang Wang",
                "Dawei Yin",
                "Haifeng Wang"
            ],
            "title": "Ernie-search: Bridging cross-encoder with dual-encoder via self on-the-fly distillation",
            "year": 2022
        },
        {
            "authors": [
                "Xueguang Ma",
                "Kai Sun",
                "Ronak Pradeep",
                "Jimmy Lin."
            ],
            "title": "A replication study of dense passage retriever",
            "venue": "CoRR, abs/2104.05740.",
            "year": 2021
        },
        {
            "authors": [
                "Yu A Malkov",
                "Dmitry A Yashunin."
            ],
            "title": "Efficient and robust approximate nearest neighbor search using hierarchical navigable small world graphs",
            "venue": "IEEE transactions on pattern analysis and machine intelligence, 42(4):824\u2013836.",
            "year": 2018
        },
        {
            "authors": [
                "Antonio Mallia",
                "Joel Mackenzie",
                "Torsten Suel",
                "Nicola Tonellotto."
            ],
            "title": "Faster learned sparse retrieval with guided traversal",
            "venue": "CoRR, abs/2204.11314.",
            "year": 2022
        },
        {
            "authors": [
                "Marius Muja",
                "David G. Lowe."
            ],
            "title": "Scalable nearest neighbor algorithms for high dimensional data",
            "venue": "IEEE Trans. Pattern Anal. Mach. Intell., 36(11):2227\u2013 2240.",
            "year": 2014
        },
        {
            "authors": [
                "Linh Thai Nguyen."
            ],
            "title": "Static index pruning for information retrieval systems: A posting-based approach",
            "venue": "Proceedings of the 7th Workshop on Large-Scale Distributed Systems for Information Retrieval, colocated with ACM SIGIR 2009, LSDS-IR@SIGIR",
            "year": 2009
        },
        {
            "authors": [
                "Tri Nguyen",
                "Mir Rosenberg",
                "Xia Song",
                "Jianfeng Gao",
                "Saurabh Tiwary",
                "Rangan Majumder",
                "Li Deng."
            ],
            "title": "MS MARCO: A human generated machine reading comprehension dataset",
            "venue": "Proceedings of the Workshop on Cognitive Computation: Integrat-",
            "year": 2016
        },
        {
            "authors": [
                "Yingqi Qu",
                "Yuchen Ding",
                "Jing Liu",
                "Kai Liu",
                "Ruiyang Ren",
                "Wayne Xin Zhao",
                "Daxiang Dong",
                "Hua Wu",
                "Haifeng Wang."
            ],
            "title": "Rocketqa: An optimized training approach to dense passage retrieval for opendomain question answering",
            "venue": "Proceedings of the",
            "year": 2021
        },
        {
            "authors": [
                "Stephen E. Robertson",
                "Hugo Zaragoza."
            ],
            "title": "The probabilistic relevance framework: BM25 and beyond",
            "venue": "Found. Trends Inf. Retr., 3(4):333\u2013389.",
            "year": 2009
        },
        {
            "authors": [
                "Tao Shen",
                "Xiubo Geng",
                "Chongyang Tao",
                "Can Xu",
                "Kai Zhang",
                "Daxin Jiang."
            ],
            "title": "Unifier: A unified retriever for large-scale retrieval",
            "venue": "CoRR, abs/2205.11194.",
            "year": 2022
        },
        {
            "authors": [
                "A\u00e4ron van den Oord",
                "Oriol Vinyals",
                "Koray Kavukcuoglu."
            ],
            "title": "Neural discrete representation learning",
            "venue": "Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9,",
            "year": 2017
        },
        {
            "authors": [
                "Jing Wang",
                "Jingdong Wang",
                "Gang Zeng",
                "Zhuowen Tu",
                "Rui Gan",
                "Shipeng Li."
            ],
            "title": "Scalable k-nn graph construction for visual descriptors",
            "venue": "2012 IEEE Conference on Computer Vision and Pattern Recognition, Providence, RI, USA, June 16-21, 2012, pages",
            "year": 2012
        },
        {
            "authors": [
                "Jingdong Wang",
                "Naiyan Wang",
                "You Jia",
                "Jian Li",
                "Gang Zeng",
                "Hongbin Zha",
                "Xian-Sheng Hua."
            ],
            "title": "Trinary-projection trees for approximate nearest neighbor search",
            "venue": "IEEE Trans. Pattern Anal. Mach. Intell., 36(2):388\u2013403.",
            "year": 2014
        },
        {
            "authors": [
                "Jingdong Wang",
                "Ting Zhang",
                "Jingkuan Song",
                "Nicu Sebe",
                "Heng Tao Shen."
            ],
            "title": "A survey on learning to hash",
            "venue": "IEEE Trans. Pattern Anal. Mach. Intell., 40(4):769\u2013790.",
            "year": 2018
        },
        {
            "authors": [
                "Liang Wang",
                "Nan Yang",
                "Xiaolong Huang",
                "Binxing Jiao",
                "Linjun Yang",
                "Daxin Jiang",
                "Rangan Majumder",
                "Furu Wei."
            ],
            "title": "Simlm: Pre-training with representation bottleneck for dense passage retrieval",
            "venue": "CoRR, abs/2207.02578.",
            "year": 2022
        },
        {
            "authors": [
                "Yair Weiss",
                "Antonio Torralba",
                "Rob Fergus."
            ],
            "title": "Spectral hashing",
            "venue": "Advances in Neural Information Processing Systems, volume 21. Curran Associates, Inc.",
            "year": 2008
        },
        {
            "authors": [
                "Shitao Xiao",
                "Zheng Liu",
                "Weihao Han",
                "Jianjin Zhang",
                "Defu Lian",
                "Yeyun Gong",
                "Qi Chen",
                "Fan Yang",
                "Hao Sun",
                "Yingxia Shao",
                "Xing Xie."
            ],
            "title": "Distillvq: Learning retrieval oriented vector quantization by distilling knowledge from dense embeddings",
            "venue": "In",
            "year": 2022
        },
        {
            "authors": [
                "Shitao Xiao",
                "Zheng Liu",
                "Yingxia Shao",
                "Zhao Cao."
            ],
            "title": "Retromae: Pre-training retrieval-oriented language models via masked auto-encoder",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu",
            "year": 2022
        },
        {
            "authors": [
                "Shitao Xiao",
                "Zheng Liu",
                "Yingxia Shao",
                "Defu Lian",
                "Xing Xie."
            ],
            "title": "Matching-oriented embedding quantization for ad-hoc retrieval",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event",
            "year": 2021
        },
        {
            "authors": [
                "Lee Xiong",
                "Chenyan Xiong",
                "Ye Li",
                "Kwok-Fung Tang",
                "Jialin Liu",
                "Paul N. Bennett",
                "Junaid Ahmed",
                "Arnold Overwijk."
            ],
            "title": "Approximate nearest neighbor negative contrastive learning for dense text retrieval",
            "venue": "9th International Conference on Learning",
            "year": 2021
        },
        {
            "authors": [
                "Jingtao Zhan",
                "Jiaxin Mao",
                "Yiqun Liu",
                "Jiafeng Guo",
                "Min Zhang",
                "Shaoping Ma."
            ],
            "title": "Jointly optimizing query encoder and product quantization to improve retrieval performance",
            "venue": "CIKM \u201921: The 30th ACM International Conference on Information and",
            "year": 2021
        },
        {
            "authors": [
                "Jingtao Zhan",
                "Jiaxin Mao",
                "Yiqun Liu",
                "Jiafeng Guo",
                "Min Zhang",
                "Shaoping Ma."
            ],
            "title": "Optimizing dense retrieval model training with hard negatives",
            "venue": "SIGIR \u201921: The 44th International ACM SIGIR Conference on Research and Development in Information",
            "year": 2021
        },
        {
            "authors": [
                "Hang Zhang",
                "Yeyun Gong",
                "Yelong Shen",
                "Jiancheng Lv",
                "Nan Duan",
                "Weizhu Chen."
            ],
            "title": "Adversarial retriever-ranker for dense text retrieval",
            "venue": "The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "Kai Zhang",
                "Chongyang Tao",
                "Tao Shen",
                "Can Xu",
                "Xiubo Geng",
                "Binxing Jiao",
                "Daxin Jiang."
            ],
            "title": "LED: lexicon-enlightened dense retriever for large-scale retrieval",
            "venue": "Proceedings of the ACM Web Conference 2023, WWW 2023, Austin, TX, USA, 30 April 2023 -",
            "year": 2023
        },
        {
            "authors": [
                "Wayne Xin Zhao",
                "Jing Liu",
                "Ruiyang Ren",
                "Ji-Rong Wen."
            ],
            "title": "Dense text retrieval based on pretrained language models: A survey",
            "venue": "CoRR, abs/2211.14876.",
            "year": 2022
        },
        {
            "authors": [
                "Yutao Zhu",
                "Huaying Yuan",
                "Shuting Wang",
                "Jiongnan Liu",
                "Wenhan Liu",
                "Chenlong Deng",
                "Zhicheng Dou",
                "Ji-Rong Wen"
            ],
            "title": "Large language models for information retrieval: A survey",
            "year": 2023
        },
        {
            "authors": [
                "Justin Zobel",
                "Alistair Moffat",
                "Kotagiri Ramamohanarao."
            ],
            "title": "Inverted files versus signature files for text indexing",
            "venue": "ACM Trans. Database Syst., 23(4):453\u2013490.",
            "year": 1998
        },
        {
            "authors": [
                "Lixin Zou",
                "Shengqiang Zhang",
                "Hengyi Cai",
                "Dehong Ma",
                "Suqi Cheng",
                "Shuaiqiang Wang",
                "Daiting Shi",
                "Zhicong Cheng",
                "Dawei Yin."
            ],
            "title": "Pre-trained language model based ranking in baidu search",
            "venue": "KDD \u201921: The 27th ACM SIGKDD Conference on Knowledge",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Recently, dense retrieval has become the de-facto paradigm for high-quality first-stage text retrieval, serving as a fundamental component in various information access applications such as search engines (Zou et al., 2021), recommender systems (Zhao et al., 2022), and question answering systems (Karpukhin et al., 2020). Specifically, dense retrievers encode queries and documents into\n\u2020Corresponding Author.\ntheir latent embeddings in the semantic space using bi-encoders, and retrieve relevant documents based on embedding similarity. In practice, they rely on Approximate Nearest Neighbor (ANN) indexes to avoid expensive traversal of all document embeddings for each input query, a.k.a. the brute force search (Johnson et al., 2019).\nThere are numerous ANN options, e.g. the hashing based ones (Datar et al., 2004; Wang et al., 2018), the tree based ones (Bentley, 1975; Wang et al., 2014), the graph based ones (Wang et al., 2012; Malkov and Yashunin, 2018), and the vector quantization (VQ) based ones (J\u00e9gou et al., 2011a,b). Among all these alternatives, the VQ based indexes, exemplified by IVF-PQ, are particularly praised for their high running efficiency in terms of both query latency and space consumption, wherein the inverted file structure (IVF) is an indispensable component (J\u00e9gou et al., 2011a).\nIVF partitions all document embeddings into disjoint clusters by KMeans. During searching, it finds nearby clusters to an input query and evaluates documents within these clusters by subsequent codecs (e.g. PQ). By increasing the number of clusters to scan, one may expect higher retrieval quality since the relevant document is more likely to be included, yet with higher query latency since there are more documents to evaluate (J\u00e9gou et al., 2011a). On top of the basic idea, recent studies improve the accuracy of IVF by grouping the cluster embeddings and skipping the least promising groups (Baranchuk et al., 2018), creating duplicated records for boundary embeddings (Chen et al., 2021), and end-to-end learning the cluster assignments by knowledge distillation (Xiao et al., 2022a). Despite their improvements, IVF still exhibits limited retrieval quality, especially when high efficiency is needed. This is because the clustering is too lossy to include relevant documents in a few close clusters to the query. What\u2019s worse, it is not cost-effective to probe more clusters, which\nsacrifices much more efficiency for minor effectiveness improvements. To better illustrate the above points, we take a concrete example.\nExample 1 In Figure 1, we showcase the recalllatency trade-off derived from changing the number of clusters to visit in the basic IVF and the distilled IVF (the best IVF method so far (Xiao et al., 2022a)). We use the \u201cFlat\u201d codec that reranks the candidate documents in brute force. As such, any retrieval loss against brute force search (denoted as \u201cFlat\u201d) is due to the failure of IVF.\nTwo critical facts can be observed. First, despite improvements from end-to-end distillation, both IVF methods suffer from much poorer retrieval quality at low query latency. With 20ms latency, IVF-Flat and DistillIVF-Flat achieve recall of 0.758 and 0.862, both of which lag far behind 0.927 from brute force search. Second, probing more clusters marginally improves recall but significantly increases latency. To promote recall from 0.899 to 0.909, DistillIVF-Flat doubles the query latency (from 90ms to around 200ms). Consequently, there is plenty of room to optimize IVF to achieve lossless retrieval quality with high efficiency.\nIn contrast to cluster proximity, extensive research has demonstrated that lexical matching, e.g. overlaps of salient terms between queries and documents, tend to be strong features for identifying relevant documents (Robertson and Zaragoza, 2009; Lin and Ma, 2021; Formal et al., 2021). Moreover, complementary effect has been observed from combining lexical and semantic matching in hybrid retrieval systems (Kuzi et al., 2020; Gao et al., 2020; Shen et al., 2022; Zhang et al., 2023).\nIn this work, we explore the potential of unifying embedding clusters and salient terms in a Hybrid Inverted Index (HI2) for the acceleration of dense retrieval. Specifically, each document reference\nis indexed in inverted lists of two types of entries: embedding clusters and salient terms. When searching, the input query is dispatched to both types of inverted lists. Documents within them are merged and evaluated by subsequent codecs.\nFor effectiveness, HI2 needs to include relevant documents in the dispatched inverted lists; For efficiency, HI2 requires these inverted lists to be small enough to avoid significant overhead during posthoc evaluation. Both of them call for constructing compact inverted lists and efficiently searching through them. To this end, we devise a cluster selector and a term selector, which accurately and efficiently pick out only a few clusters and terms for indexing and searching, respectively.\nAs for the implementation of the cluster and term selector, we show simple unsupervised algorithms, i.e. KMeans and BM25, work surprisingly well, whereby HI2 already substantially outperforms previous IVF methods with competitive efficiency. Moreover, we propose to leverage neural networks for realization and end-to-end learning by a knowledge distillation objective. This approach further boosts the retrieval quality, enabling HI2 to remarkably and consistently surpass other ANN indexes on popular retrieval benchmarks, i.e. MSMARCO (Nguyen et al., 2016) and Natual Questions (Kwiatkowski et al., 2019).\nOur contributions are summarized as follows:\n\u2022 We propose the Hybrid Inverted Index, which combines embedding clusters and salient terms for accelerating dense retrieval.\n\u2022 We devise tailored techniques, i.e. the cluster selector, the term selector, and the joint optimization, to guarantee the effectiveness and efficiency of HI2.\n\u2022 We evaluate HI2 with extensive experiments and verify its robust advantage across implementation variations, indexing/searching configurations, and embedding models."
        },
        {
            "heading": "2 Related Works",
            "text": ". \u2022 Dense Retrieval. In the last four years, the rapid development of pre-trained language models, e.g. BERT (Devlin et al., 2019), has significantly pushed forward the progress of dense retrieval, making it increasingly popular for highquality first-stage retrieval (Zhao et al., 2022; Zhu et al., 2023). Dense retrievers encode queries and\ndocuments into dense vectors (i.e. embeddings) in the same latent space, where the semantic relevance is measured by embedding similarity. Recent studies further enhance their retrieval quality by retrieval-oriented pre-training (Wang et al., 2022; Xiao et al., 2022b; Gao and Callan, 2022), delicate negative sampling (Xiong et al., 2021; Qu et al., 2021; Zhan et al., 2021b), and knowledge distillation from more powerful rankers (Zhang et al., 2022; Lu et al., 2022; Qu et al., 2021).\n\u2022 ANNs Indexes. In practice, relevant documents usually need to be retrieved from a massive collection. Consequently, dense retrieval must rely on Approximate Nearest Neighbor (ANN) indexes to avoid the expensive brute force search. The ANNs indexes can be realized via different strategies: 1) the hashing based ones (Datar et al., 2004; Weiss et al., 2008; Wang et al., 2018); 2) the tree based ones (Bentley, 1975; Wang et al., 2014; Muja and Lowe, 2014); 3) the graph based ones (Dong et al., 2011; Wang et al., 2012; Malkov and Yashunin, 2018); 4) the vector quantization (VQ) based ones (Ge et al., 2014; J\u00e9gou et al., 2011a,b; Baranchuk et al., 2018). Among these options, the VQ based indexes are particularly preferred for massive-scale retrieval owing to their high efficiency in terms of both query latency and space consumption (Johnson et al., 2019).\n\u2022 VQ Index Optimization. Despite the competitive efficiency, VQ-based indexes are prone to limited retrieval quality when low latency is desired. In recent years, continuous efforts have been dedicated to alleviating this problem, which can be categorized into two threads. One thread is to design advanced heuristics for clustering and evaluation. For example, (J\u00e9gou et al., 2011b) and (Baranchuk et al., 2018) add another refinement stage over the quantized embeddings and skip less promising clusters according to tailored heuristics. (Chen et al., 2021) create duplicated reference for boundary embeddings to improve recall with high efficiency. The other research thread optimizes the VQ index towards retrieval quality with cross-entropy loss instead of minimizing the reconstruction loss. For example, (Zhan et al., 2021a) and (Xiao et al., 2021) jointly learns the query encoder and the product quantizer by contrastive learning. (Xiao et al., 2022a) further improves the accuracy by leveraging knowledge distillation for joint optimization. However, all these methods stick to conventional IVF to organize the search space, which is subop-\ntimal as shown in Example 1. In this work, our proposed Hybrid Inverted Index support efficient identification of relevant documents through both semantic and lexical matching. Note that our work is orthogonal to those about efficient inverted index access (Broder et al., 2003; Mallia et al., 2022) and hence can be combined for further acceleration. \u2022 Hybrid Retrieval. Recently, there have been emergent recipes for the union of semantic (dense) and lexical (sparse) features. Some of them are direct ensembles of dense and sparse retrievers (Ma et al., 2021; Kuzi et al., 2020); Others use enhanced optimization objectives, e.g. adversarial hard negatives and distillation, to jointly learn from semantic/lexical features (Gao et al., 2020; Shen et al., 2022; Zhang et al., 2023). However, they all rely on separate sparse and dense indexes to work, and interpolate scores from the two indexes. Different from them, HI2 combines semantic and lexical features at index level, and estimates scores universally by specific codecs. Meanwhile, HI2 may benefit from enhanced optimization methods in these methods, which we leave for future work."
        },
        {
            "heading": "3 Preliminaries",
            "text": ""
        },
        {
            "heading": "3.1 Dense Retrieval",
            "text": "Given a collection of documents D = {Di}|D|i=1, dense retrieval aims to retrieve the top R relevant documents from D in response to an input query Q. Specifically, each document D \u2208 D and query Q is encoded into its embedding eD, eQ \u2208 Rh, by a document encoder and a query encoder, respectively. Next, the relevance is measured by the inner product between them, whereby the top R ranked documents are returned.\nresult = top-R D {\u27e8eQ, eD\u27e9 | D \u2208 D} , (1)\nwhere \u27e8\u00b7, \u00b7\u27e9 denotes inner product. In reality, it is impractical to evaluate every document (computing \u27e8eQ, eD\u27e9) for each input query (i.e. the brute force search), which results in exceedingly high latency and resource consumption. Instead, ANN indexes are used to avoid exhaustively scanning all documents and accelerate the relevance measurement by approximation."
        },
        {
            "heading": "3.2 Inverted File Structure and Product Quantization",
            "text": "Among all alternative ANN indexes, the Vector Quantization (VQ) based ones are particularly popular for massive-scale retrieval. They consist of\ntwo basic modules: the inverted file structure (IVF) and the product quantization (PQ).\nTo avoid exhaustive search, IVF partitions all documents into disjoint clusters C = {Ci}Li=1 by KMeans, where each cluster is associated with an embedding eCi \u2208 Rh. For the query Q, documents within the closest KC clusters are evaluated by the subsequent codec (PQ by default):\nresult = top-R D {PQ(Q,D) | D \u2208 A(Q)} , A(Q) = \u22c3 top-KC\nCi\n{\u27e8eQ, eCi\u27e9 | Ci \u2208 C} . (2)\nTo accelerate relevance estimation, PQ compresses the document embedding into discrete integer codes according to a codebook v \u2208 Rm\u00d7k\u00d7h/m. It splits eD into m fragments {ejD}mj=1, then quantizes each fragment to the closest codeword in v:\ne\u0302jD = vj,\u03b8j , \u03b8j = argmin i ||ejD \u2212 vj,i|| 2 2. (3)\nTherefore, only the global codebook v and the codeword assignment \u03b8\u2217 need to be stored, which is much smaller than the full-precision embedding. Finally, the relevance is evaluated by:\nPQ(Q,D) = m\u2211 j=1 \u27e8ejQ, e\u0302 j D\u27e9, (4)\nwhere ejQ is the query embedding fragment. Since the inner product between ejQ and any codeword vj,\u2217 can be cached once computed, the relevance estimation approximated by PQ is much faster.\nBy increasing the number of clusters to scan (KC), higher retrieval quality can be achieved because the relevant document is more likely to be included in A(Q). Yet, the latency is increased at the same time as more documents need to be evaluated. Conventional IVF falls short in including the relevant document given a small KC , meanwhile, it needs to sacrifice a lot of efficiency for minor retrieval quality improvement. In this work, we propose an alternative to alleviate these problems."
        },
        {
            "heading": "4 Hybrid Inverted Index",
            "text": "The framework of the Hybrid Inverted Index (HI2) is shown in Figure 2. HI2 organizes the search space with two types of inverted lists: embedding clusters (C = {Ci}Li=1) and salient terms (T = {Tv | v \u2208 V} where V is the term vocabulary). Each document reference is stored in the inverted\nlists of 1 cluster and KT1 terms. When searching, the input query Q is dispatched to the inverted lists of KC clusters and KT2 terms. Documents within them are merged and evaluated by PQ. Formally,\nresult = top-R D {PQ(Q,D) | D \u2208 A(Q)} ,\nA(Q) = AC(Q) \u222a AT (Q). (5)\nTo determine which clusters/terms to use for indexing the document and dispatching the query, HI2 employs two modules: a cluster selector and a term selector. They can be implemented with simple unsupervised algorithms (resulting in HI2unsup) and neural networks (resulting in HI2sup).\n1 This flexible implementation scheme injects high practicability into HI2. In the following, we elaborate on the two modules (\u00a74.1 and \u00a74.2) and the supervised optimization for their neural network implementation (\u00a74.3)."
        },
        {
            "heading": "4.1 Cluster Selector",
            "text": "This module selects 1 cluster for indexing the document and KC clusters for dispatching the query to. Specifically, it associates each cluster Ci with an embedding eCi \u2208 Rh. Then scores each cluster with the inner product between the document embedding eD or the query embedding eQ: \u27e8e\u2217, eCi\u27e9. The document is indexed to the cluster with the highest score. When searching, the query is dispatched to the top KC clusters:\nAC(Q) = \u22c3 top-KC\nCi\n{\u27e8eQ, eCi\u27e9 | Ci \u2208 C} .\n(6) For HI2unsup, the cluster embeddings {eCi}Li=1 are produced by KMeans over all document embeddings. For HI2sup, they are initialized with KMeans and optimized on-the-fly by the objective in \u00a74.3."
        },
        {
            "heading": "4.2 Term Selector",
            "text": "This module selects KT1 terms for indexing the document and KT2 terms for dispatching the query. There are two concerns for designing the term selector: 1) the selected terms must be representative w.r.t. the input, hence the lexical matching between the query and the document can be effectively captured; 2) the term selection for the query must be efficient enough to avoid excess online overhead.\nTherefore, for the document D, the term selector first tokenizes it to {di}|D|i=1 where di \u2208 V , then it\n1We use HI2 to denote both HI2unsup and HI2sup henceforth.\nestimates the score of each unique term v \u2208 V in D with BM25 (HI2unsup) or BERT (HI 2 sup). Formally,\nsv =  (\u03b1+1)\u00d7IDF(v)\u00d7TF(v,D) TF(v,D)+\u03b1\u00d7(1\u2212\u03b2+ \u03b2|D| avgdl ) \u2203di = v \u2227 HI2unsup, max di=v (f(BERT(D)[i])) \u2203di = v \u2227 HI2sup,\n0 \u2204di = v. (7)\n\u03b1, \u03b2 are hyper parameters, avgdl is the average document length, f(\u00b7) is two-layer MLP of Rh \u2192 R1 with ReLU activation, and BERT denotes encoding by BERT model (Devlin et al., 2019). As such, the top KT1 scored terms are used for indexing the document. Besides, the average score of each term across all documents (s\u0304v) is stored.\nThe query Q is tokenized to {qi}|Q|i=1 likewise, while it is not processed with any complex computations to save online cost. For short queries, all its constituting terms are selected; for long queries, the terms with top KT2 average score are selected:\nAT (Q) = {\u22c3 {Tqi | qi \u2208 Q} |Q| \u2264 KT2 ,\u22c3 {Tqi | qi \u2208 Q\u2032} otherwise.\nQ\u2032 = top-KT2 qi {s\u0304qi | qi \u2208 Q} (8)"
        },
        {
            "heading": "4.3 Joint Optimization",
            "text": "HI2sup involves learning cluster embeddings in the cluster selector, the MLP, and BERT in the term selector. We propose a knowledge distillation objective for jointly training these parameters towards retrieval quality. Concretely, we sample a subset of documents for the query (D \u2286 D), then employ\na powerful teacher \u0398 to produce accurate estimations of their relevance. Finally, we enforce the cluster selector and the term selector to produce similar estimations by KL divergence:\nLDistill(Q) =KL(\u0398(Q,D) \u2225 CS(Q,D)) +KL(\u0398(Q,D) \u2225 TS(Q,D)). (9)\nFollowing (Xiao et al., 2022a), we simply choose off-the-shelf embeddings as teachers. Denote softmax operator as sm, the teacher estimations are:\n\u0398(Q,D) = sm{\u27e8eQ, eD\u27e9 | D \u2208 D}. (10)\nThe cluster selector estimates relevance by query-cluster embedding similarity:\nCS(Q,D) = sm{\u27e8eQ, eC\u03d5(D)\u27e9 | D \u2208 D}, (11)\nwhere \u03d5(D) is the cluster index of the document. The term selector estimates relevance by termscore vector similarity:\nTS(Q,D) = sm{\u27e8sQ, sD\u27e9 | D \u2208 D}, (12)\nwhere sQ and sD are the score vector over the vocabulary derived from Eq 7. Note that here both queries and documents are processed the same way.\nAdditionally, since the document cluster assignment \u03d5(D) is fixed, we add a commitment loss to the final loss to keep the document embedding close to its associated cluster, which is a common practice for learning quantization (van den Oord\net al., 2017): LCommit(Q) = \u2211 D\u2208D log exp(\u27e8eD, eC\u03d5(D)\u27e9)\u2211 C\u2032\u2208C exp(\u27e8eD, eC\u2032\u27e9) ,\nL = \u2211 Q LDistill(Q) + LCommit(Q). (13)"
        },
        {
            "heading": "5 Experiments",
            "text": "In this section, we first introduce our experimental settings, then carry out extensive experiments to investigate the following research questions (RQ): RQ1: How are the effectiveness and efficiency of HI2 compared with other baselines methods? RQ2: Do clusters and terms complement each other for identifying relevant documents? RQ3: How is the robustness of HI2 across different embedding models?"
        },
        {
            "heading": "5.1 Experimental Settings",
            "text": "\u2022 Datasets. We use two popular benchmark datasets. 1) MS MARCO (Nguyen et al., 2016). We use the passage track, including 502,939 training queries and 6,980 evaluation queries (dev set); the corpus size is 8,841,823. 2) Natural Questions (Kwiatkowski et al., 2019). We follow the split of DPR (Karpukhin et al., 2020), resulting in 79,168 training queries and 3,610 testing queries. The corpus size is 21,015,324. \u2022 Metrics. For evaluating retrieval quality, we leverage MRR@K (M@K) and Recall@K (R@K). For evaluating retrieval efficiency, we compute the average query latency (QL) and the overall index size (IS). Our evaluations are based on the same batch size, thread number, and toolkit (Faiss (Johnson et al., 2019) for ANNs and Pyserini (Lin et al., 2021) for sparse models). Note that the latency of HI2 is on par with that of IVF-OPQ given the same number of candidates to evaluate, because the term selector dispatches the query with simple heuristics that introduces very little overhead. \u2022 Baselines. We compare with three types of retrieval models: 1) Sparse retrievers, including BM25 (Robertson and Zaragoza, 2009), DocT5 (Cheriton, 2019), DeepCT (Dai and Callan, 2019), UniCOIL (Lin and Ma, 2021), and DistilSPLADE (Formal et al., 2021); 2) Dense retrievers with brute force search (denoted as Flat), including DPR (Karpukhin et al., 2020), ANCE (Xiong et al., 2021), CoCondenser (Gao and Callan, 2022), AR2 (Zhang et al., 2022), and RetroMAE (Xiao\net al., 2022b). 3) Dense retrievers with ANN indexes (denoted as ANNs), including IVF-PQ (J\u00e9gou et al., 2011a), IVF-OPQ (Ge et al., 2014), IVF-JPQ (Zhan et al., 2021a), Distill-VQ (Xiao et al., 2022a), and HNSW (Malkov and Yashunin, 2018) (we set the edges to 32 and efSearch to 500). We use RetroMAE and AR2 as the embedding model on MSMARCO and NQ, respectively, due to their superior performance with brute force search. More details are in the Appendix. \u2022 Implementation Details. For all methods involving clustering, we set the number of clusters L to 10000 and the number of probing clusters when searching to 100 (except HI2). For all methods involving PQ, we set the number of fragments m to 96, the number of sub-clusters k to 256. For HI2, we use the BERT\u2019s vocabulary (Devlin et al., 2019) as the term vocabulary V , resulting in 30522 unique terms in total. KT2 is always set to 32 for both HI2unsup and HI 2 sup.\nFor HI2unsup, we use KMeans over all document embeddings to produce cluster embeddings {eCi}Li=1, BM25 to produce term scores sv with \u03b1 = 0.82, \u03b2 = 0.68, and OPQ (Ge et al., 2014) as the evaluation codec, all of which are unsupervised algorithms. KC is set to 25, KT1 is set to 15. For HI2sup, we initialize cluster embeddings with KMeans and optimize them afterward. Note the cluster assignment \u03d5(D) is fixed once initialized. We use bert-base-uncased for the term selector. The passage is tokenized to 128 tokens before encoding. We employ the distilled OPQ (Xiao et al., 2022a) as the evaluation codec. KC is set to 30, KT1 is set to 3. More details are in the appendix. For reproducibility, we release our source code and model checkpoints at https://anonymous.4open.science/r/HI2/."
        },
        {
            "heading": "5.2 Main Analysis (RQ1)",
            "text": "We report the overall evaluation results in Table 1. On the one hand, our hybrid inverted index demonstrate superlative effectiveness over baseline ANN indexes. Specifically, HI2unsup, which solely relies on unsupervised algorithms, improves the Recall@100 of IVF-OPQ (the basic unsupervised VQ index) by 14%, and improves that of Distill-VQ (the strongest supervised VQ index in literature) by 8%. It even triumphs the powerful HNSW index by 3 absolute points in Recall@100, which is a more valuable metric for first-stage retrieval than MRR. Moreover, the neural network imple-\nmentation and the end-to-end knowledge distillation further unleash its potential, as HI2sup further amplify the margins over ANN baselines. Remarkably, HI2sup achieves on par retrieval quality with its brute-force-search teacher (RetroMAE on MSMARCO and AR2 on NQ), and surpasses a lot of well-established sparse and dense retrievers.\nOn the other hand, the efficiency of our hybrid inverted index is also satisfactory. Its query latency is the second lowest on both datasets, accelerating the brute force search (Flat) by hundreds of times and only slightly falling behind that of HNSW. Notably, the latency is even lower than VQ-based indexes, because HI2 needs to evaluate fewer candidate documents. Besides, HI2 possesses a moderate index size, which is bigger than VQ baselines since more document references need to be stored, while much smaller than Flat or HNSW since it does not need to store full-precision embeddings.\nAs such, we have showcased the outstanding effectiveness and efficiency of HI2 under one specific setting. Next, we are interested in the effectivenessefficiency trade-off of HI2 and ANN baselines (we exclude IVF-PQ and IVF-JPQ, the former is too weak and the latter is similar to IVF-OPQ). Specifically, for VQ indexes, we change the number of clusters to visit; for HNSW, we change the number of neighbors to visit; for HI2, we change the number of terms to index (KT1 ) and the number of clusters to dispatch (KC). Since the index size is static, we measure recall@100 as effectiveness and average query latency as efficiency. The resulted trade-off curves are reported in Figure 3.\nFrom the figure, HI2unsup performs on par with the powerful HNSW across various index settings, as their recall are almost identical given the same latency. Both of them significantly outperform VQ baselines. Besides, HI2sup brings substantial improvement over HI2unsup and HNSW, achieving higher recall with lower latency. Meanwhile, it efficiently approaches the brute-force-search effectiveness. In contrast, VQ baselines need to largely increase the latency to marginally improve the recall, yet lagging far behind brute force search.\nBased on the above analysis, we answer RQ1: HI2 achieves lossless retrieval quality against brute force search, with low query latency and small index size, significantly and consistently outperforming baseline ANN indexes and retaining the advantages across indexing/searching configurations."
        },
        {
            "heading": "5.3 Ablation Analysis (RQ2)",
            "text": "To answer RQ2, we study the individual contribution from embedding clusters and salient terms. Specifically, we disable the inverted lists corresponding to terms and clusters, respectively, denoted as w.o. Term and w.o. Clus. Other configurations are kept the same. We plot their recall-latency trade-off curves in Figure 4.\nTwo critical facts can be observed. First, salient terms tend to be better features for organizing the search space than embedding clusters, as the w.o. Clus variants significantly and consistently outperform w.o. Term ones. Thus, our claim that embedding clusters alone falls short in effective identification of relevant documents is well justified. Second,\nsalient terms and embedding clusters indeed complement each other, as HI2unsup and HI 2 sup beats their \u201chomogeneous\u201d variants in terms of both effectiveness and efficiency. Therefore, we answer RQ2: Embedding clusters and salient terms complement each other for more effective and efficient identification of relevant documents."
        },
        {
            "heading": "5.4 Robustness Analysis (RQ3)",
            "text": "In Figure 3, we have shown the robust advantage of HI2 across different index configurations. For practical usage, it is important to evaluate the robustness of HI2 given different embedding models.\nIn Table 2, we report the performance of HI2 and selected strong baselines with RetroMAE and AR2 as the embedding model. We can notice that HI2sup always achieves the best recall among all ANN indexes, which is very close to that of brute force search. Besides, HI2unsup performs on par with\nthe strong HNSW index, which uses full-precision embeddings for evaluation. As for efficiency, we observe HI2 resides in a sweet spot with the second lowest query latency and relatively small index size, which is substantially smaller than Flat and HNSW but slightly bigger than VQ baselines.\nAdditionally, the performance of ANN baselines is unstable given different embedding models: higher retrieval quality with brute-force searching does not result in higher retrieval quality with ANN acceleration. For example, the recall of AR2 Flat is inferior to that of RetroMAE Flat on MS MARCO. However, this trend reverses when ANN baselines are applied, i.e. AR2 IVF-OPQ is better than RetroMAE IVF-OPQ. By comparison, the performance of HI2 is stable: higher brute-force-search effectiveness corresponds to higher effectiveness of HI2 regardless of the embedding model. In summary, we answer RQ3: HI2 enjoys high robustness and stability across different embedding models, consistently surpassing strong ANN baselines with competitive efficiency and aligning well with the brute force search."
        },
        {
            "heading": "6 Conclusion",
            "text": "In this work, we propose the hybrid inverted index, which reformulates conventional IVF by unifying both embedding clusters and salient terms to accelerate dense retrieval. We devise tailored techniques for cluster selection, term selection, and joint optimization. With comprehensive experiments, we verify the effectiveness and efficiency\nof HI2, which consistently outperforms strong ANN baselines across implementation variations, indexing/searching configurations, and embedding models. Moreover, we demonstrate that embedding clusters and salient terms are complementary to each other for identifying relevant documents, which may inspire further research towards the combination of semantic and lexical features."
        },
        {
            "heading": "Acknowledgement",
            "text": "This work was supported by the National Natural Science Foundation of China No. 62272467 and Public Computing Cloud, Renmin University of China. The work was partially done at the Engineering Research Center of Next-Generation Intelligent Search and Recommendation, MOE.\nLimitations\nDespite the satisfactory performance of the hybrid inverted index, it has more hyper parameters than conventional IVF hence may require more effort to tune them for ideal performance. Moreover, the searching of clusters and terms is currently independent; whilst we believe it is promising to design a more flexible mechanism to control the searching behaviors. For example, only the term-side inverted lists will be searched for some queries."
        },
        {
            "heading": "A Baseline Details",
            "text": "\u2022 Sparse Retrievers. These methods represent documents and queries with sparse vectors over the vocabulary, then estimate relevance with overlapping entries. The retrieval operation is accelerated with the efficient inverted index (Zobel et al., 1998). BM25 (Robertson and Zaragoza, 2009), the most basic sparse retriever. DocT5 (Cheriton, 2019), extending BM25 by appending pseudoqueries to documents. DeepCT (Dai and Callan, 2019), learning contextualized term weights using BERT to replace the TF-IDF in BM25. UniCOIL (Lin and Ma, 2021), learning contextualized term weights using BERT with contrastive learning. DistilSPLADE (Formal et al., 2021), learning sparse term-weight vectors over vocabulary with knowledge distillation. \u2022 Dense Retrievers with Brute Force Search (Flat). These methods encode documents and\nqueries into dense embeddings, then estimate relevance with embedding similarity. For each input query, all document embeddings are evaluated. DPR (Karpukhin et al., 2020), the most basic dense retriever. ANCE (Xiong et al., 2021), enhancing DPR with hard negatives mined from the previous model snapshot. CoCondenser (Gao and Callan, 2022), retrieval-oriented pretraining the encoder model to compress more information in the embedding. AR2 (Zhang et al., 2022), adversarially train the encoder and a ranker with knowledge distillation. RetroMAE (Xiao et al., 2022b), retrievaloriented pretraining the encoder model with a shallow decoder and the representation bottleneck. \u2022 Dense Retrievers with Approximate Nearest Neighbor Indexes (ANNs). These methods leverage ANN indexes to accelerate dense retrieval. IVFPQ (J\u00e9gou et al., 2011a), the basic VQ index. IVFOPQ (Ge et al., 2014), unsupervisedly learning a transformation orthogonal matrix for PQ to achieve higher accuracy. IVF-JPQ (Zhan et al., 2021a), optimizing the PQ codebook with contrastive learning towards retrieval quality. Distill-VQ (Xiao et al., 2022a), optimizing the IVF centroids and PQ codebook with knowledge distillation from any off-theshelf embeddings. HNSW (Malkov and Yashunin, 2018), a powerful graph-based ANN index that is widely used in modern search engines (elasticsearch, 2015).\nB Implementation Details\nFor all methods involving clustering, we set the number of clusters L to 10000 and the number of probing clusters when searching to 100 (except HI2). For all methods involving PQ, we set the number of fragments m to 96, the number of subclusters k to 256, which results in 32 times smaller size than the full-precision one. For HI2, we use the BERT\u2019s vocabulary (Devlin et al., 2019) as the term vocabulary V , resulting in 30522 unique terms in total. KT2 is always set to 32 for both HI 2 unsup and HI2sup. For HI2unsup, we use KMeans over all document embeddings to produce cluster embeddings {eCi}Li=1, BM25 to produce term scores sv with \u03b1 = 0.82, \u03b2 = 0.68, and OPQ (Ge et al., 2014) as the evaluation codec, all of which are unsupervised algorithms. KC is set to 25, KT1 is set to 15. For HI2sup, we initialize cluster embeddings with KMeans and optimize them afterwards. Note the cluster assignment \u03d5(D) is fixed once initialized.\nWe use bert-base-uncased for the term selector. The passage is tokenized to 128 tokens before encoding. We employ the distilled OPQ (Xiao et al., 2022a) as the evaluation codec. KC is set to 30, KT1 is set to 3. For training HI2sup, we use the annotated ground truth document D+, 7 hard negatives sampled from BM25 top 200 results, and in-batch negatives to form D.\nIn practice, we find the terms selected by HI2sup results in much \u201cdenser\u201d inverted lists than HI2unsup. In other words, some terms may be frequently selected from multiple passages, translating to their super big inverted lists. This is especially the case for Natual Questions. Therefore, on NQ, we prune the super big inverted lists to a moderate size inspired by static index pruning technique (Nguyen, 2009). Concretely, after indexing all documents, we count the size of each term-side inverted list, then take the one at the \u03b3-th percentile (\u03b3 defaults to 0.996) as the threshold, whereby inverted lists bigger than the threshold are identified as \u201csuper big\u201d. Next, we ascendingly order document references based on their individual score to the specific term of each super big inverted list. We prune the references from the head until the size of the inverted list equals the threshold."
        },
        {
            "heading": "C Codec Analysis",
            "text": "Apart from the default PQ, HI2 can be combined with other codecs. In Table 3, we compare PQ with the most powerful yet most expensive Flat codec. It can be observed that HI2unsup and HI 2 sup both benefit from the more powerful codec. This indicates that HI2 can return high-quality candidates universally applicable for different codecs. It also reveals that the current PQ codec is still lossy. However, there is no free lunch: the powerful Flat codec comes with higher latency and higher index size, which is unfavorable for the index efficiency. In summary, we again verify the practicality of HI2, one may flexibly balance between higher effectiveness and higher efficiency."
        }
    ],
    "title": "Hybrid Inverted Index Is a Robust Accelerator for Dense Retrieval",
    "year": 2023
}