{
    "abstractText": "One of the main methods for computational interpretation of a text is mapping it into a vector in some embedding space. Such vectors can then be used for a variety of textual processing tasks. Recently, most embedding spaces are a product of training large language models (LLMs). One major drawback of this type of representation is their incomprehensibility to humans. Understanding the embedding space is crucial for several important needs, including the need to debug the embedding method and compare it to alternatives, and the need to detect biases hidden in the model. In this paper, we present a novel method of understanding embeddings by transforming a latent embedding space into a comprehensible conceptual space. We present an algorithm for deriving a conceptual space with dynamic ondemand granularity. We devise a new evaluation method, using either human rater or LLMbased raters, to show that the conceptualized vectors indeed represent the semantics of the original latent ones. We show the use of our method for various tasks, including comparing the semantics of alternative models and tracing the layers of the LLM. The code is available online1.",
    "authors": [
        {
            "affiliations": [],
            "name": "Adi Simhi"
        },
        {
            "affiliations": [],
            "name": "Shaul Markovitch"
        }
    ],
    "id": "SP:d09869e035d89185a76aa4f5aa1995bf82cf6072",
    "references": [
        {
            "authors": [
                "Ning An",
                "Meng Chen",
                "Li Lian",
                "Peng Li",
                "Kai Zhang",
                "Xiaohui Yu",
                "Yilong Yin"
            ],
            "title": "Enabling the interpretability of pretrained venue representations",
            "year": 2022
        },
        {
            "authors": [
                "Sanjeev Arora",
                "Yuanzhi Li",
                "Yingyu Liang",
                "Tengyu Ma",
                "Andrej Risteski."
            ],
            "title": "Linear algebraic structure of word senses, with applications to polysemy",
            "venue": "Trans. Assoc. Comput. Linguistics, 6:483\u2013495.",
            "year": 2018
        },
        {
            "authors": [
                "Anthony Bau",
                "Yonatan Belinkov",
                "Hassan Sajjad",
                "Nadir Durrani",
                "Fahim Dalvi",
                "James R. Glass."
            ],
            "title": "Identifying and controlling important neurons in neural machine translation",
            "venue": "7th International Conference on Learning Representations, ICLR 2019, New",
            "year": 2019
        },
        {
            "authors": [
                "G\u00e1bor Berend."
            ],
            "title": "Sparsity makes sense: Word sense disambiguation using sparse contextualized word representations",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 8498\u20138508.",
            "year": 2020
        },
        {
            "authors": [
                "Zied Bouraoui",
                "V\u00edctor Guti\u00e9rrez-Basulto",
                "Steven Schockaert."
            ],
            "title": "Integrating ontologies and vector space embeddings using conceptual spaces (invited paper)",
            "venue": "International Research School in Artificial Intelligence in Bergen, AIB 2022, June 7-11, 2022,",
            "year": 2022
        },
        {
            "authors": [
                "Nadia Burkart",
                "Marco F. Huber."
            ],
            "title": "A survey on the explainability of supervised machine learning",
            "venue": "J. Artif. Intell. Res., 70:245\u2013317.",
            "year": 2021
        },
        {
            "authors": [
                "Kevin Clark",
                "Urvashi Khandelwal",
                "Omer Levy",
                "Christopher D. Manning."
            ],
            "title": "What does BERT look at? an analysis of bert\u2019s attention",
            "venue": "Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP,",
            "year": 2019
        },
        {
            "authors": [
                "Jacob Cohen."
            ],
            "title": "A coefficient of agreement for nominal scales",
            "venue": "Educational and psychological measurement, 20(1):37\u201346.",
            "year": 1960
        },
        {
            "authors": [
                "Fahim Dalvi",
                "Nadir Durrani",
                "Hassan Sajjad",
                "Yonatan Belinkov",
                "Anthony Bau",
                "James R. Glass."
            ],
            "title": "What is one grain of sand in the desert? analyzing individual neurons in deep NLP models",
            "venue": "The Thirty-Third AAAI Conference on Artificial Intelli-",
            "year": 2019
        },
        {
            "authors": [
                "Guy Dar",
                "Mor Geva",
                "Ankit Gupta",
                "Jonathan Berant."
            ],
            "title": "Analyzing transformers in embedding space",
            "venue": "arXiv preprint arXiv:2209.02535.",
            "year": 2022
        },
        {
            "authors": [
                "Philipp Dufter",
                "Hinrich Sch\u00fctze."
            ],
            "title": "Analytical methods for interpretable ultradense word embeddings",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Nat-",
            "year": 2019
        },
        {
            "authors": [
                "Javid Ebrahimi",
                "Anyi Rao",
                "Daniel Lowd",
                "Dejing Dou."
            ],
            "title": "Hotflip: White-box adversarial examples for text classification",
            "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018, Melbourne, Australia, July",
            "year": 2018
        },
        {
            "authors": [
                "Liat Ein-Dor",
                "Yosi Mass",
                "Alon Halfon",
                "Elad Venezian",
                "Ilya Shnayderman",
                "Ranit Aharonov",
                "Noam Slonim."
            ],
            "title": "Learning thematic similarity metric from article sections using triplet networks",
            "venue": "Proceedings of the 56th Annual Meeting of the Associa-",
            "year": 2018
        },
        {
            "authors": [
                "Manaal Faruqui",
                "Jesse Dodge",
                "Sujay Kumar Jauhar",
                "Chris Dyer",
                "Eduard H. Hovy",
                "Noah A. Smith."
            ],
            "title": "Retrofitting word vectors to semantic lexicons",
            "venue": "NAACL HLT 2015, The 2015 Conference of the North American Chapter of the Association for",
            "year": 2015
        },
        {
            "authors": [
                "Tam\u00e1s Ficsor",
                "G\u00e1bor Berend."
            ],
            "title": "Changing the basis of contextual representations with explicit semantics",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural",
            "year": 2021
        },
        {
            "authors": [
                "Derek Greene",
                "Padraig Cunningham."
            ],
            "title": "Practical solutions to the problem of diagonal dominance in kernel document clustering",
            "venue": "Machine Learning, Proceedings of the Twenty-Third International Conference (ICML 2006), Pittsburgh, Pennsylvania,",
            "year": 2006
        },
        {
            "authors": [
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Roberta: A robustly optimized bert pretraining approach",
            "venue": "arXiv preprint arXiv:1907.11692.",
            "year": 2019
        },
        {
            "authors": [
                "Scott M. Lundberg",
                "Su-In Lee."
            ],
            "title": "A unified approach to interpreting model predictions",
            "venue": "Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach,",
            "year": 2017
        },
        {
            "authors": [
                "Scott M. Lundberg",
                "Su-In Lee."
            ],
            "title": "A unified approach to interpreting model predictions",
            "venue": "Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach,",
            "year": 2017
        },
        {
            "authors": [
                "Andreas Madsen",
                "Siva Reddy",
                "Sarath Chandar."
            ],
            "title": "Post-hoc interpretability for neural nlp: A survey",
            "venue": "ACM Computing Surveys, 55(8):1\u201342.",
            "year": 2022
        },
        {
            "authors": [
                "Binny Mathew",
                "Sandipan Sikdar",
                "Florian Lemmerich",
                "Markus Strohmaier."
            ],
            "title": "The POLAR framework: Polar opposites enable interpretability of pretrained word embeddings",
            "venue": "WWW \u201920: The Web Conference 2020, Taipei, Taiwan, April 20-24, 2020,",
            "year": 2020
        },
        {
            "authors": [
                "Piero Molino",
                "Yang Wang",
                "Jiawei Zhang."
            ],
            "title": "Parallax: Visualizing and understanding the semantics of embedding spaces via algebraic formulae",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: System Demon-",
            "year": 2019
        },
        {
            "authors": [
                "Brian Murphy",
                "Partha Pratim Talukdar",
                "Tom M. Mitchell."
            ],
            "title": "Learning effective and interpretable semantic models using non-negative sparse embedding",
            "venue": "COLING 2012, 24th International Conference on Computational Linguistics, Proceedings of",
            "year": 2012
        },
        {
            "authors": [
                "Jianmo Ni",
                "Gustavo Hernandez \u00c1brego",
                "Noah Constant",
                "Ji Ma",
                "Keith B. Hall",
                "Daniel Cer",
                "Yinfei Yang."
            ],
            "title": "Sentence-t5: Scalable sentence encoders from pre-trained text-to-text models",
            "venue": "Findings of the Association for Computational Linguistics: ACL 2022,",
            "year": 2022
        },
        {
            "authors": [
                "Sungjoon Park",
                "JinYeong Bak",
                "Alice Oh."
            ],
            "title": "Rotated word vector representations and their interpretability",
            "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, EMNLP 2017, Copenhagen, Denmark, Septem-",
            "year": 2017
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J. Liu."
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "J. Mach. Learn. Res., 21:140:1\u2013140:67.",
            "year": 2020
        },
        {
            "authors": [
                "Nazneen Fatema Rajani",
                "Ben Krause",
                "Wengpeng Yin",
                "Tong Niu",
                "Richard Socher",
                "Caiming Xiong."
            ],
            "title": "Explaining and improving model behavior with k nearest neighbor representations",
            "venue": "arXiv preprint arXiv:2010.09030.",
            "year": 2020
        },
        {
            "authors": [
                "Nils Reimers",
                "Iryna Gurevych."
            ],
            "title": "Sentence-bert: Sentence embeddings using siamese bert-networks",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural",
            "year": 2019
        },
        {
            "authors": [
                "Marco Tulio Ribeiro",
                "Sameer Singh",
                "Carlos Guestrin."
            ],
            "title": "Model-agnostic interpretability of machine learning",
            "venue": "arXiv preprint arXiv:1606.05386.",
            "year": 2016
        },
        {
            "authors": [
                "Marco T\u00falio Ribeiro",
                "Sameer Singh",
                "Carlos Guestrin."
            ],
            "title": "why should I trust you?\": Explaining the predictions of any classifier",
            "venue": "Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Min-",
            "year": 2016
        },
        {
            "authors": [
                "Marco T\u00falio Ribeiro",
                "Sameer Singh",
                "Carlos Guestrin."
            ],
            "title": "Anchors: High-precision modelagnostic explanations",
            "venue": "Proceedings of the ThirtySecond AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Arti-",
            "year": 2018
        },
        {
            "authors": [
                "Alexis Ross",
                "Ana Marasovic",
                "Matthew E. Peters."
            ],
            "title": "Explaining NLP models via minimal contrastive editing (mice)",
            "venue": "Findings of the Association for Computational Linguistics: ACL/IJCNLP 2021, Online Event, August 1-6, 2021, volume",
            "year": 2021
        },
        {
            "authors": [
                "Sascha Rothe",
                "Hinrich Sch\u00fctze."
            ],
            "title": "Word embedding calculus in meaningful ultradense subspaces",
            "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12, 2016, Berlin, Germany, Volume 2: Short",
            "year": 2016
        },
        {
            "authors": [
                "L\u00fctfi Kerem Senel",
                "Furkan Sahinu\u00e7",
                "Veysel Y\u00fccesoy",
                "Hinrich Sch\u00fctze",
                "Tolga \u00c7ukur",
                "Aykut Ko\u00e7."
            ],
            "title": "Learning interpretable word embeddings via bidirectional alignment of dimensions with semantic concepts",
            "venue": "Inf. Process. Manag., 59(3):102925.",
            "year": 2022
        },
        {
            "authors": [
                "L\u00fctfi Kerem \u015eenel",
                "Ihsan Utlu",
                "Furkan \u015eahinu\u00e7",
                "Haldun M Ozaktas",
                "Aykut Ko\u00e7."
            ],
            "title": "Imparting interpretability to word embeddings while preserving semantic structure",
            "venue": "Natural Language Engineering, 27(6):721\u2013746.",
            "year": 2021
        },
        {
            "authors": [
                "Lutfi Kerem Senel",
                "Ihsan Utlu",
                "Veysel Y\u00fccesoy",
                "Aykut Ko\u00e7",
                "Tolga \u00c7ukur."
            ],
            "title": "Semantic structure and interpretability of word embeddings",
            "venue": "IEEE ACM Trans. Audio Speech Lang. Process., 26(10):1769\u2013 1779.",
            "year": 2018
        },
        {
            "authors": [
                "Pia Sommerauer",
                "Antske Fokkens."
            ],
            "title": "Firearms and tigers are dangerous, kitchen knives and zebras are not: Testing whether word embeddings can tell",
            "venue": "Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Net-",
            "year": 2018
        },
        {
            "authors": [
                "Anant Subramanian",
                "Danish Pruthi",
                "Harsh Jhamtani",
                "Taylor Berg-Kirkpatrick",
                "Eduard H. Hovy."
            ],
            "title": "SPINE: sparse interpretable neural embeddings",
            "venue": "Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th inno-",
            "year": 2018
        },
        {
            "authors": [
                "Ian Tenney",
                "Dipanjan Das",
                "Ellie Pavlick."
            ],
            "title": "BERT rediscovers the classical NLP pipeline",
            "venue": "Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long",
            "year": 2019
        },
        {
            "authors": [
                "Jesse Vig",
                "Sebastian Gehrmann",
                "Yonatan Belinkov",
                "Sharon Qian",
                "Daniel Nevo",
                "Yaron Singer",
                "Stuart M. Shieber."
            ],
            "title": "Investigating gender bias in language models using causal mediation analysis",
            "venue": "Advances in Neural Information Processing Sys-",
            "year": 2020
        },
        {
            "authors": [
                "T Wu",
                "M Tulio Ribeiro",
                "J Heer",
                "D Weld."
            ],
            "title": "Polyjuice: Generating counterfactuals for explaining, evaluating, and improving models",
            "venue": "Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th Interna-",
            "year": 2021
        },
        {
            "authors": [
                "Chih-Kuan Yeh",
                "Been Kim",
                "Sercan \u00d6mer Arik",
                "ChunLiang Li",
                "Tomas Pfister",
                "Pradeep Ravikumar."
            ],
            "title": "On completeness-aware concept-based explanations in deep neural networks",
            "venue": "Advances in Neural Information Processing Systems 33: Annual",
            "year": 2020
        },
        {
            "authors": [
                "Xiang Zhang",
                "Junbo Jake Zhao",
                "Yann LeCun."
            ],
            "title": "Character-level convolutional networks for text classification",
            "venue": "Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12,",
            "year": 2015
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Recently, there has been significant progress in Natural Language Processing thanks to the development of Large Language Models (LLMs). These models are based on deep neural networks and are trained on extensive volumes of textual data (Devlin et al., 2019; Raffel et al., 2020; Liu et al., 2019).\nWhile these powerful models show excellent performance on a variety of tasks, they suffer from a significant drawback. Their complex structure hinders our ability to understand their reasoning pro-\n1https://github.com/adiSimhi/Interpreting-EmbeddingSpaces-by-Conceptualization\ncess. This limitation becomes crucial in several important scenarios, including the need to explain the decisions made by a system that employs the model, the necessity to debug the model and compare it with alternatives, and the requirement to identify any hidden biases within the model (Burkart and Huber, 2021; Ribeiro et al., 2016b; Madsen et al., 2022).\nCurrent LLMs process text by projecting it into an internal embedding space. By understanding this space, we can therefore gain an understanding of the model. Such understanding, however, is challenging as the dimensions of the embedding space are usually not human-understandable.\nThe importance of interpretability has been recognized by many researchers. Several works present methods for explaining the decision of a system that uses the embedding (mainly classifiers) (e.g. Ribeiro et al., 2016a; Lundberg and Lee, 2017a). Some works (Senel et al., 2022; Faruqui et al., 2015) perform training or retraining for generating a new model that is interpretable, thus detouring the problem of understanding the original one. Another line of work assumes the availability of an embedding matrix and uses it to find orthogonal transformations, such that the new dimensions will be more understandable (Dufter and Sch\u00fctze, 2019; Park et al., 2017). Probing methods utilize classification techniques to identify the meaning associated with individual dimensions of the original embedding space (Clark et al., 2019; Dalvi et al., 2019).\nIn this work, we present a novel methodology for interpretability of LLMs by conceptualizing the original embedding space. Our algorithm enables the mapping of any vector in the original latent space to a vector in a human-understandable conceptual space. Importantly, our approach does not assume that latent dimension corresponds to an explicit and easily-interpretable concept.\nOur method can be used in various ways:\n1. Given an input text and its latent vector, our algorithm allows understanding of the semantics of it according to the model.\n2. It can help us to gain an understanding of the model, including its strengths and weaknesses, by probing it with texts in subjects that are of interest to us. This understanding can be used for debugging a given model or for comparing alternative models.\n3. Given a decision system based on the LLM, our algorithm can help to understand the decision and to explain it using the conceptual representation. This can also be useful in detecting biased decisions.\nOur contributions are:\n1. We present a model-agnostic method for interpreting embeddings, which works with any model without the need for additional training. Our approach only requires a black box that takes a text fragment as input and produces a vector as output.\n2. We present a novel algorithm that, given an ontology, can generate a conceptual embedding space for any desired size and can be selectively refined to specialize in specific subjects.\n3. We introduce a new method for evaluating algorithms for embedding interpretation using either a human or an LLM-based evaluator."
        },
        {
            "heading": "2 The Conceptualization Algorithm",
            "text": "Let T be a space of textual objects (sentences, for example). Let L = L1 \u00d7 . . . \u00d7 Lk be a latent embedding space of k dimensions. Let f : T \u2192 L be a function that maps a text object to a vector in the latent space. Typically, f will be an LLM or LLM-based.\nOur method requires two components: A set of concepts C = c1, . . . , cn defining a conceptual space C = c1 \u00d7 . . .\u00d7 cn, and a mapping function \u03c4 : C \u2192 T that returns a textual representation for each concept in C.\nIn the pre-processing stage, we map each concept c \u2208 C to a vector in L by applying f on \u03c4(c), the textual representation of c. We thus define n vectors in L, c\u03021, . . . , c\u0302n such that c\u0302i \u2261 f(\u03c4(ci)).\nGiven a vector l \u2208 L (that typically represents some input text), we measure its similarity to each\nvector c\u0302i (that represents the concept ci) using any given similarity measure sim. The algorithm then outputs a vector in the conceptual space, using the similarities as the dimensions.\nWe have thus defined a meta-algorithm CES (Conceptualizing Embedding Spaces) that, for any given embedding method f , a set of concepts C and a mapping function \u03c4 from concepts to text, takes a vector in the latent space L and returns a vector in the conceptual space C:\nCESf,C,\u03c4 (l) = \u27e8sim(l, c\u03021), . . . , sim(l, c\u0302n)\u27e9T\nA graphical representation of the process is depicted in Figure 1.\nIf we use cosine similarity as sim, and use a normalised f function, we can implement CES as matrix multiplication, which can accelerate our computation. First, observe that, under these restrictions, cosine similarity is equivalent to the dot product between vectors. Let U = u1, . . . , uk be the standard basis in k dimensions as a base of L. We can look at the projection of U in the C space, by using function \u03d5 such that \u03d5(ui) = \u2329 \u03d5(u1i ), . . . , \u03d5(u n i ) \u232aT where \u03d5(uji ) = cosine(ui, cj) = ui \u00b7 c\u0302j . We can now create a n\u00d7k matrix M = \u27e8\u03d5(u1), . . . , \u03d5(uk)\u27e9. Using this matrix, we get CESf,C,\u03c4 (l) = M \u00b7 l."
        },
        {
            "heading": "2.1 Generating Conceptual Spaces",
            "text": "To allow a conceptual representation in various levels of abstraction, we have devised a method that, given a hierarchical ontology, generates a conceptual space of desired granularity.\nFor the experiments described in this paper, we chose Wikipedia category-directed graph as our ontology, as it provides a constantly- updated, wide and deep coverage of our knowledge, but any other knowledge graph can be used instead. Since the edges in the Wikipedia graph are not labeled, we performed an additional step of assigning a score to each edge, based on its similarity to its siblings, which we named siblings score (see Appendix A).\nA major strength of the hierarchical representation of concepts is its multiple levels of abstraction. For our purpose, that means that we can request a concept space with a given level of granularity. Given a concept graph G, we can define d(c), the depth of each concept (node) as the length of the shortest path from the root. We designate by Ci = {c \u2208 C|d(c) = i} as the set of all concepts with a depth of exactly i. For example, MATHEMATICS and HEALTH are concepts from C1, and\nMATHEMATICAL TOOLS and PUBLIC HEALTH are their direct children and are concepts from C2."
        },
        {
            "heading": "2.2 Selectively Refined Conceptual Spaces",
            "text": "One problem with fix-depth conceptual spaces is the large growth in the number of nodes with the increase in depth. For example, in our implementation, |C1| = 37, |C2| = 706 and |C3| = 3467. Another problem arises in domain-specific tasks, where high-granularity concepts are needed in specific subjects but not in others. Lastly, it is often difficult to know ahead of time what is the required granularity for the given task.\nWe have therefore developed an algorithm that, given a contextual text T \u2032 \u2286 T of input texts and the desired concept-space size, generates a concept space of that size with granularity tailored to T \u2032. The main idea is to refine categories that are strongly associated with T \u2032, thus enlarging the distances between the textual objects, allowing for more refined reasoning. We use the symbol C\u2217 to indicate a concept space that is created this way.\nThe algorithm (1) starts with C1 as its initial concept space. It then iterates until the desired size is achieved. At each iteration, the contextual text T \u2032 is embedded into the current space using CES. The concept with the largest weight after the projection to CES is then selected for expansion. The intuition is that this concept represents a main topic of the text, and will therefore benefit the most from a more refined representation. The algorithm selects its best p% children for some p, judged by their siblings score, and adds them to the current conceptual space. In addition, the algorithm utilizes a flag removeP to decide whether to remove the expanded concept. We observed that retaining the parent can often improve the quality of the model interpretation.\nIf the embedding is used for a classification task,\nAlgorithm 1 Selective Refinement Input:T \u2032, size, removeP Output:C\u2217\nC \u2190 C1 while |C| < size do\nemb\u2190 AV Gt\u2208T \u2032 ( CESf,C,\u03c4 (f(t)) ) c\u0302\u2190 concept in C with max weight in emb best \u2190 p% of children(c\u0302) with highest siblings score C \u2190 C \u222a best if removeP is True then C \u2190 C \\ c\u0302\nend if end while return C\nwe can utilize the labels of the training examples alongside their text. We assign to each concept the set of examples for which it is the top concept. The entropy of this set is then combined linearly with the text-based weight described above to determine its final value. As before, the node with the maximal value is chosen for expansion. The underlying intuition is that concepts representing texts from different classes require refinement to allow a better separation."
        },
        {
            "heading": "2.3 Mapping Concepts to Text",
            "text": "The function \u03c4 maps concepts to text. When the concepts in the ontology have meaningful names, such as in the case of Wikipedia categories, we can just use \u03c4 that maps into these names. We have also devised a more complex function, \u03c4\u0302 , that maps a concept to a concatenation of the concept name with the names of its children2. Given a concept c with name tc and children names tc1 and tc2 , \u03c4\u0302(c) = \u201dtc such as tc1 and tc2\u201d. This\n2We take the two best children (the highest siblings score)\napproach has two advantages: It exploits the elaborated knowledge embedded in the ontology for potentially more accurate mapping, and it produces full sentences, which may be a better fit for f that was trained on sentences."
        },
        {
            "heading": "3 Empirical Evaluation",
            "text": "It is not easy to evaluate an algorithm whose task is to create an understandable representation that matches the original incomprehensible embedding. We performed a series of experiments, including a human study, that show that our method indeed achieved its desired goal. For all the experiments, we have used RoBERTa sentence embedding model3 (Reimers and Gurevych, 2019; Liu et al., 2019) as our f , unless otherwise specified. All models used in this work were applied with their default parameters. Whenever the concept space C\u2217 was used, we set size = 768 to match the size used by SRoBERTa, but we observed that using much smaller values yielded almost as good results. The default value for removeP is false. For \u03c4 , the function that maps concepts to text, we have just used the text of the concept name (with a length of 4.25 words on average in G). 4"
        },
        {
            "heading": "3.1 Qualitative Evaluation",
            "text": "We first show several examples of conceptual representations created by CES to get some insight into the way that our method works. We have applied SRoBERTa to 3 sentences from 3 different recent CNN articles to get 3 latent embedding vectors. We have used the first 10 sentences of each article as the contextual text T \u2032 for generating C\u2217.\nTable 1 shows the conceptual embeddings generated by CES. We show only the 3 top concepts with their associated depth. Observe that the conceptual vectors are understandable and intuitively capture the semantics of the input texts. Note that the representations shown are not based on some new embedding method, but reflect SRoBERTa\u2019s understanding of the input text. In Appendix E, we study, using the same examples, the effect of the concept-space granularity on the conceptual representation, using a fixed-depth concept space instead of C\u2217. Lastly, in Appendix F, we study, using the\n3Model all-distilroberta-v1 from Hugging Face. For simplicity we refer to it as SRoBERTa\n4The total runtime for the experiments described here was 24 hours on 8 cores of Intel Xeon Gold 5220R CPU 2.20GHz. The graph creation from the full Wikipedia dump of 2020 took several days with a maximal memory allocation of 100GB.\nsame examples, the difference in the representation of two additional models (SBERT and ST5)."
        },
        {
            "heading": "3.2 Evaluation on Classification Tasks",
            "text": "To show that our representation matches the original one generated by the LLM, we first show that learning using the original embedding dimensions as features and learning using the conceptual features yield similar classifiers. Most works try to show such similarity by comparing accuracy results. This method, however, is prone to errors. Two classifiers might give us an accuracy of 80%, while agreeing only on 60% of the cases. Instead, we use a method that is used for rater agreement, reporting two numbers: the raw agreement and Cohen\u2019s kappa coefficient (Cohen, 1960).\nWe use the following datasets (all in English): AG News 5, Ohsumed and R8 6, Yahoo (Zhang et al., 2015), BBC News (Greene and Cunningham, 2006), DBpedia 14 (Zhang et al., 2015) and 20Newsgroup 7. We use only topical classification datasets, as the concept space we use does not include the necessary concepts needed for tasks like sentiment analysis. If a dataset has more than 10,000 examples, we randomly sample 10,000. The results are averaged over 10 folds. We use a random forest (RF) learning algorithm with 100 trees and a maximum depth of 5. The conceptual space used by CES is C\u2217, using the training set as the contextual text T \u2032.\nTable 2 shows the agreement between a random forest classifier trained on the LLM embedding and a classifier trained on the conceptual embedding generated by CES. For reference, we also show the agreement between the LLM-based classifier and a random classifier. We report raw agreement and kappa coefficient (with standard deviations). We can see that all the values are relatively high, indicating high agreement between the LLM embedding and CES\u2019s embedding. Note that Kappa can range from -1 to +1 with 0 indicating random chance. For the sake of completeness, we also report the accuracies of the two classifiers which are proved to be quite similar.\nWe repeated the experiment using a KNN classifier (n=5) with cosine similarity. The results are shown in Table 3. We can see much higher agree-\n5Available online:http://groups.di.unipi.it/\u223cgulli/AG_cor pus _of_news_articles.html\n6Available online:https://www.kaggle.com/weipengfei/ ohr8r52 used for Ohusmed and R8 datasets\n7taken from sklearn datasets python library\nment between the LLM-based and CES-based classifiers.\nWe tested the sensitivity of our algorithm to the values of the removeP = True and \u03c4\u0302 parameters. The results are shown in Appendices B and C. We can see that both parameters have little effect on performance.\nAppendix D includes additional positive results on the triplets dataset (Ein-Dor et al., 2018)."
        },
        {
            "heading": "3.3 Evaluating Understandability",
            "text": "While these results look promising, they may not be sufficient to indicate that CES indeed reflects the semantics of the text according to the LLM. Consider the following hypothetical algorithm. Let D be the size of the LLM embedding space. The algorithm selects D random English words and assigns each to an arbitrary dimension. This hypothetical algorithm satisfies two requirements: Using it for the classification tasks will always be in 100% agreement with the original (as we merely renamed the features), and its generated representation will be understandable by humans, as we use words in natural language. However, it is clear that it does not convey to humans any knowledge regarding the LLM representation. In the next subsections, we describe a novel experimental design with humans and with other models. This design aims to validate our assertion that CES produces comprehensible representations that genuinely capture the semantics of the LLM embedding."
        },
        {
            "heading": "3.3.1 Evaluation By Humans",
            "text": "We have designed a human experiment with the goal of testing the human understandability of the latent representation by observing only its conceptual mapping. The experiment tests the agreement, given a set of test examples, between two raters:\n1. A classifier that was trained on a training set using the LLM embeddings.\n2. A human rater that does not have access to the training set and does not have access to the test text. The only data presented to the human is the top 3 concepts of the CES representation of the LLM embedding. 3 graduate students were used for rating.\nWe claim that if there is a high agreement between the two, then the conceptual representation indeed reflects the meaning of the LLM embedding.\nTo allow classification by the human raters, out of the 7 datasets described in the previous subsection, we chose the 4 that have meaningful names for the classes. To make the classification task less complex for the raters, we randomly sampled two classes from each dataset, thus creating a binary classification problem. For each binary dataset, we set aside 20% of the examples for training a classifier based on the LLM embedding, using the same method and parameters as in the previous subsection. The resulting classifier was then applied to the remaining 80% of the dataset.\nOut of this test set, we sample 10 examples on which the LLM-based classifier was right and 10 on which it was wrong8. This is the test set that is presented to the human raters. Each test case is represented by the 3 top concepts of the CES embedding, after applying feature selection on the full embedding to choose the top 20% concepts. As before, the conceptual space is C\u2217 with size = 768 and with the training set used as contextual text T \u2032. The instruction to the human raters was: \"A document belongs to one of two classes. The document is described by the following 3 key phrases (topics): 1, 2, and 3. To which of the two classes do you think the document belongs to?\". The final human classification of a test example was computed by the majority voting of 3 raters. For the LLM-based classification, we used two learning algorithms. The first is Random Forest (RF) with the\n8except for the Ohsumed dataset where only 7 wrong answers were found"
        },
        {
            "heading": "R8 0.23 0.87 \u00b1 0.01 0.76 \u00b1 0.02 79.9 79.8 -0.1",
            "text": ""
        },
        {
            "heading": "R8 0.23 0.92 \u00b1 0.01 0.87 \u00b1 0.02 89.4 93.6 4.2",
            "text": "same parameters as in Section 3.2. The second is Nearest-Centroid Classifier (NC) which computes the centroid of each class and returns the one closest to the test case.\nTable 4 shows the raw agreement between the LLM-based and the human classification, for the two learning algorithms. Kappa coefficient was not computed as the test set is too small. The results are encouraging as they show quite a high agreement. Note that the learning algorithm had access to the full training set, while the human could see the conceptual representation of only the test case. Indeed, we can see that the agreement with the less sophisticated NC classifier is higher on average than the agreement with the RF classifier."
        },
        {
            "heading": "3.3.2 Evaluation by Other Models",
            "text": "We repeated the experiments of the last subsection, with the same test sets, but instead of using human raters, we used a LLM rater. The LLM rater receives the top 3 concepts, just like the human raters, and makes a decision by computing cosine similarity between its embedding of each class name to its embedding of the textual representation of the 3 concepts. The 3 LLMs used for rating are SBERT (Reimers and Gurevych, 2019) 9, ST5 (Ni et al., 2022) 10 and SRoBERTa. Note that the two uses of SRoBERTa are quite different. The one used for the original classification is based on a training set and a learning algorithm, while the model used for rating just computes similarity between the class name and the 3 concepts.\nAn alternative approach to ours is to assign a meaning to each dimension of the latent space. We denote this approach by Dimension Meaning Assignment (DMA). We have designed two competitors that represent the DMA approach.\nThe first one, termed DMAwords, is based on a\n9Model bert-base-nli-mean-tokens from Hugging Face 10Model sentence-t5-large from Hugging Face\nvocabulary of 10K frequent words 11. We represent each word by our LLM, yielding 10K vectors of size 768. We now map each dimension to the word with the highest weight for it. We make sure that the mapping is unique. The second one, which we call DMAconcepts, is built in the same way, using, instead of words, the concepts in C3. Lastly, DMAC \u2217 is added as an ablation experiment where the transformation part of our method is turned off. Table 5 shows the results expressed in raw agreement. We can see that CES method performs better than the alternatives (except for two test cases).\nThe previous two subsections (3.3.1 and 3.3.2) have presented evidence supporting our fundamental claim that the conceptual representation generated by CES accurately captures the semantic content of the input text based on the LLM model."
        },
        {
            "heading": "4 CES Application",
            "text": ""
        },
        {
            "heading": "4.1 Using CES for Comparing Models",
            "text": "One major feature of our methodology is that it allows us to gain an understanding of the semantics of trained models. This allows us when considering alternative models, to compare their semantics, to understand the differences between their views of the world, and compare their potential knowledge gaps. We demonstrate this by comparing the views of three LLMs, SBERT, ST5, and SRoBERTa on two example texts, by observing their conceptual representations in C3 generated by CES.\nTable 6 shows the top 3 concepts of the vector generated by CES for the 3 LLMs given the text \"FC Barcelona\". We can see that while SRoBERTa and ST5 give high weight to the sport aspect of the input text, SBERT does not.\nTo validate this observation, we compare, for each of the 3 models, the cosine similarity in the latent space between \"FC Barcelona\" and the sportrelated phrase \"Miami Dolphin\", to its similarity to the city-related phrase \"Politics in Spain\".\nThe results support our observation. SBERT embedding is more similar to the city aspect embedding while the two others are more similar to the sports text embedding.\nIn Table 7, for the input \"Manhattan Project\", we can see that ST5 gives high weight to the military project while SBERT gives high weight to concepts related to New York and to theater. SRoBERTa recognizes both aspects.\n11https://www.mit.edu/ ecprice/wordlist.10000"
        },
        {
            "heading": "4.2 Using CES for LLM Tracing",
            "text": "Another application of CES is analyzing the layers of the LLM, in a similar way to the Logit lens method (nostalgebraist, 2020). This can be very useful for debugging the model. We show here an example of tracing the changes of the embedding through the layers of BERT and GPT212.\nWe create a representation for each layer by calculating the average of the token embeddings within that layer13. We then use CES to map these vectors to the conceptual space. We then trace the relative weight of each concept throughout the layers to gain an understanding of the modeling process.\nIn this case study, we analyze the text \"Government\" using C3 conceptual space. We follow 6 concepts: the 3 top ones for the initial layer and the 3 top ones for the final layer. Figure 2 shows the changes in the weights of the concepts throughout the modeling process. The y axis shows the ranking of each concept.\nThe figure offers a clear visualization of the changes in the relative weights of these concepts across the different layers. Notably, in Figure 2a, the concepts TRANSPORT, MEDICINE, and CORRUPTION, which had low rankings in the initial layer, have significantly ascended to become the top concepts in the final layer. A similar transition using different concepts is found in Figure 2b."
        },
        {
            "heading": "5 Related Work",
            "text": "The problem of interpretability has received significant attention in recent years. A large body of research (Ribeiro et al., 2016a; Lundberg and Lee, 2017b; Yeh et al., 2020; Rajani et al., 2020; Ribeiro et al., 2018; Ebrahimi et al., 2018; Ross et al., 2021; Wu et al., 2021) is devoted to generate an explanation for the decision of the model (mostly classification). Many methods utilize nearby examples or counterfactuals to provide users with reasoning behind the decision.\nSeveral works set a goal, like ours, of understanding the model itself, rather than its decisions. Most of these works attempt to assign some meaning to the dimensions, either of the original latent space or of a different space that the original one is transformed to.\n12Model bert-base-uncased and gpt2 from Hugging Face, including the input initial embedding.\n13Other methods, such as using the last token, can be easily incorporated.\nOne relatively early approach tries to find orthogonal or close to orthogonal transformations of the original embedding matrix (Dufter and Sch\u00fctze, 2019; Park et al., 2017; Rothe and Sch\u00fctze, 2016) such that a set of words with high weight in a given dimension are related and thus hopefully represent some significant concept. The advantage of these orthogonal methods is that they do not lose information due to the orthogonality. Several of these works (Arora et al., 2018; Murphy et al., 2012; Subramanian et al., 2018; Ficsor and Berend, 2021; Berend, 2020) transform the original embedding to a sparse one to improve the interpretability of each dimension. One limitation of these methods is their reliance on an embedding/dictionary matrix.\nSenel et al. (2018) assigns a specific concept to\neach dimension. Note that our work is different as it does not assume that each latent dimension corresponds to a human-understandable concept.\nRecent methods (Dar et al., 2022; nostalgebraist, 2020) assume access to the model\u2019s internals, particularly the un-embedding matrix, to map a latent vector to the token space.\nOther works (Yun et al., 2021; Molino et al., 2019) created new tools to help interpret the model. Yun et al. (2021) uses dictionary learning to view the transformer model as a linear superposition of transformer factors. Molino et al. (2019) introduces a tool for doing simple operations such as PCA and t-SNE on embedding.\nProbing methods try to interpret the model by studying its internal components. Vig et al. (2020)\nmake changes to the input to find out what parts of the model (specific attention heads) a bias comes from. Tenney et al. (2019) use probing on BERT model to find the role of each layer in the text interpretation process. Bau et al. (2019) and Dalvi et al. (2019) show how linguistic properties are distributed in the model and in specific neurons. Clark et al. (2019) create an attention-based probing classifier to find out what information is captured by each attention head of BERT. Lastly, Sommerauer and Fokkens (2018) use supervised classifiers to extract semantic features.\nSome works (Mathew et al., 2020; An et al., 2022; Bouraoui et al., 2022; Faruqui et al., 2015; Senel et al., 2022; S\u0327enel et al., 2021) tackle the problem by training or retraining to create a new interpretable model. Unlike those methods, our approach focuses on understanding the original models while preserving their performance, rather than using interpretable models as substitutes."
        },
        {
            "heading": "6 Conclusion",
            "text": "In this work, we introduce a novel approach to LLM interpretation that maps the latent embedding space into a space of concepts that are wellunderstood by humans and provide good coverage of the human knowledge. We also present a method for generating such a conceptual space with an ondemand level of granularity.\nWe evaluate our method by an extensive set of experiments including a novel method for evaluating the correspondence of the conceptual embedding to the meaning of the original embedding both by humans and by other models. Finally, we showed applications of our method for comparing models, analyzing the layers of the model, and debugging."
        },
        {
            "heading": "7 Limitations",
            "text": "There are several limitations to the work presented here:\n1. For the tracing application (Section 4.2), we used a rather limited (but common) approach of averaging the embedding vectors of each token.\n2. Most of our experiments were performed using only the SRoBERTa model.\n3. We did not include experiments using CES for explanation and for debugging. Such application will be performed in future work\n4. Our evaluation was done using only Wikipedia category graph as an ontology. Using alternative knowledge graphs can be of interest."
        },
        {
            "heading": "8 Ethics Statement",
            "text": "The primary objective of our method is to facilitate a deeper comprehension of the embedding space. Our model serves as a tool to enhance understanding of the underlying model. By utilizing the model, offensive mappings in the concept space of CES can be revealed. However, it is important to note that our model is strictly intended for the purpose of assisting in understanding and debugging problems in LLMs."
        },
        {
            "heading": "C Testing the effect of the \u03c4 function",
            "text": "One of the major components of our method is the \u03c4 function that maps a concept into a text object that is then converted to a latent vector. For the experiments described in this work, we have used \u03c4 that just outputs the concept names. In this section, we repeat the classification tests with \u03c4\u0302 (see Section 2.3). Table 9 shows the results. We can see that the differences are insignificant."
        },
        {
            "heading": "D Evaluation on a Similarity Task",
            "text": "In this section, we use the conceptual representation in the context of an algorithm that estimates semantic similarity between sentences by measuring cosine similarity between their embeddings. Specifically, we evaluate the agreement between using the latent embedding generated by SRoBERTa and using the conceptual embedding generated by CES with C3 (We cannot use C\u2217 since we do not have any contextual text to be used as T \u2032).\nThe dataset used is the triplet test that was generated from Wikipedia articles (Ein-Dor et al., 2018). Each test consists of three sentences, all from the same Wikipedia article. Two sentences are from the same section and the third is from a different section. A sentence is labeled as more similar to the one from the same section than to the one from the other section. We used a subset of 1000 triplets randomly sampled from the full dataset.\nThe results are shown in Table 10. We can see that CES embedding and SRoBERTa embedding have a high raw agreement and Kappa coefficient, larger than their agreement with the true label."
        },
        {
            "heading": "E A Qualitative Evaluation using Fixed-Depth Concept Spaces",
            "text": "We ran the same qualitative evaluation, as shown in Section 3.1, on the sentences taken from CNN. Instead of using the concept space C\u2217, we used fixed-depth spaces, C1, C2, and C3. Our goal is to study the effect of the granularity of the concept space on the way the latent vectors are represented.\nThe top five concepts of each input sentence for each concept space are presented in Tables 11, 12 and 13. For comparison, we also include the top concepts of the C\u2217 concept space. We can see the refinement of the top concepts as the depth grows. Using C1, the conceptual representation gives a very general and non-specific account of the text\u2019s meaning. Using the more refined C2 and C3\nconcept spaces, we can gain a deeper understanding of the input text. We can also notice that C\u2217 has an advantage over the fixed-depth alternatives as it can use more refined concepts when needed without compromising the size."
        },
        {
            "heading": "F A Qualitative Evaluation using Different Models",
            "text": "We ran the same qualitative evaluation, as shown in Section 3.1, on the sentences taken from CNN on all three models: SBERT, ST5, and SRoBERTa. Our goal is to study the difference between the models in a qualitative test.\nThe top five concepts of each input sentence for each model are presented in Tables 14, 15 and 16. It seems that all of the models \"understood\" the texts similarly. In Table 16 we can see a difference between SBERT and the other models. It seems that SBERT gave more weight to the word bias while the other models gave more weight to the word AI from the input sentence."
        }
    ],
    "title": "Interpreting Embedding Spaces by Conceptualization",
    "year": 2023
}