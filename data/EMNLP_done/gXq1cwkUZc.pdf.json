{
    "abstractText": "Large Language Models (LLMs) play powerful, black-box readers in the retrieve-thenread pipeline, making remarkable progress in knowledge-intensive tasks. This work introduces a new framework, Rewrite-RetrieveRead instead of the previous retrieve-then-read for the retrieval-augmented LLMs from the perspective of the query rewriting. Unlike prior studies focusing on adapting either the retriever or the reader, our approach pays attention to the adaptation of the search query itself, for there is inevitably a gap between the input text and the needed knowledge in retrieval. We first prompt an LLM to generate the query, then use a web search engine to retrieve contexts. Furthermore, to better align the query to the frozen modules, we propose a trainable scheme for our pipeline. A small language model is adopted as a trainable rewriter to cater to the black-box LLM reader. The rewriter is trained using the feedback of the LLM reader by reinforcement learning. Evaluation is conducted on downstream tasks, open-domain QA and multiple-choice QA. Experiments results show consistent performance improvement, indicating that our framework is proven effective and scalable, and brings a new framework for retrieval-augmented LLM 1.",
    "authors": [
        {
            "affiliations": [],
            "name": "Xinbei Ma"
        },
        {
            "affiliations": [],
            "name": "Yeyun Gong"
        },
        {
            "affiliations": [],
            "name": "Pengcheng He"
        },
        {
            "affiliations": [],
            "name": "Hai Zhao"
        },
        {
            "affiliations": [],
            "name": "Nan Duan"
        }
    ],
    "id": "SP:582914063d677d89a5ea1e31a2fd65d91a6b5e35",
    "references": [
        {
            "authors": [
                "Yejin Bang",
                "Samuel Cahyawijaya",
                "Nayeon Lee",
                "Wenliang Dai",
                "Dan Su",
                "Bryan Wilie",
                "Holy Lovenia",
                "Ziwei Ji",
                "Tiezheng Yu",
                "Willy Chung",
                "Quyet V. Do",
                "Yan Xu",
                "Pascale Fung"
            ],
            "title": "A multitask, multilingual, multimodal evaluation of chatgpt on",
            "year": 2023
        },
        {
            "authors": [
                "Parishad BehnamGhader",
                "Santiago Miret",
                "Siva Reddy."
            ],
            "title": "Can retriever-augmented language models reason? the blame game between the retriever and the language model",
            "venue": "arXiv preprint arXiv:2212.09146.",
            "year": 2022
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners. Advances in neural information processing",
            "year": 2020
        },
        {
            "authors": [
                "Danqi Chen",
                "Adam Fisch",
                "Jason Weston",
                "Antoine Bordes."
            ],
            "title": "Reading Wikipedia to answer opendomain questions",
            "venue": "Association for Computational Linguistics (ACL).",
            "year": 2017
        },
        {
            "authors": [
                "Morikawa",
                "Alec Radford",
                "Matthew Knight",
                "Miles Brundage",
                "Mira Murati",
                "Katie Mayer",
                "Peter Welinder",
                "Bob McGrew",
                "Dario Amodei",
                "Sam McCandlish",
                "Ilya Sutskever",
                "Wojciech Zaremba."
            ],
            "title": "Evaluating large language models trained on code",
            "venue": "CoRR,",
            "year": 2021
        },
        {
            "authors": [
                "Wei-Lin Chiang",
                "Zhuohan Li",
                "Zi Lin",
                "Ying Sheng",
                "Zhanghao Wu",
                "Hao Zhang",
                "Lianmin Zheng",
                "Siyuan Zhuang",
                "Yonghao Zhuang",
                "Joseph E. Gonzalez",
                "Ion Stoica",
                "Eric P. Xing"
            ],
            "title": "Vicuna: An opensource chatbot impressing gpt-4 with 90%* chatgpt",
            "year": 2023
        },
        {
            "authors": [
                "Aakanksha Chowdhery",
                "Sharan Narang",
                "Jacob Devlin",
                "Maarten Bosma",
                "Gaurav Mishra",
                "Adam Roberts",
                "Paul Barham",
                "Hyung Won Chung",
                "Charles Sutton",
                "Sebastian Gehrmann"
            ],
            "title": "Palm: Scaling language modeling with pathways",
            "year": 2022
        },
        {
            "authors": [
                "Kelvin Guu",
                "Kenton Lee",
                "Zora Tung",
                "Panupong Pasupat",
                "Mingwei Chang."
            ],
            "title": "Retrieval augmented language model pre-training",
            "venue": "International conference on machine learning, pages 3929\u20133938. PMLR.",
            "year": 2020
        },
        {
            "authors": [
                "Dan Hendrycks",
                "Collin Burns",
                "Steven Basart",
                "Andy Zou",
                "Mantas Mazeika",
                "Dawn Song",
                "Jacob Steinhardt."
            ],
            "title": "Measuring massive multitask language understanding",
            "venue": "Proceedings of the International Conference on Learning Representations (ICLR).",
            "year": 2021
        },
        {
            "authors": [
                "Namgyu Ho",
                "Laura Schmid",
                "Se-Young Yun."
            ],
            "title": "Large language models are reasoning teachers",
            "venue": "arXiv preprint arXiv:2212.10071.",
            "year": 2022
        },
        {
            "authors": [
                "Cheng-Yu Hsieh",
                "Chun-Liang Li",
                "Chih-Kuan Yeh",
                "Hootan Nakhost",
                "Yasuhisa Fujii",
                "Alexander J. Ratner",
                "Ranjay Krishna",
                "Chen-Yu Lee",
                "Tomas Pfister"
            ],
            "title": "Distilling step-by-step! outperforming larger language models with less training data and smaller",
            "year": 2023
        },
        {
            "authors": [
                "Gautier Izacard",
                "Patrick Lewis",
                "Maria Lomeli",
                "Lucas Hosseini",
                "Fabio Petroni",
                "Timo Schick",
                "Jane DwivediYu",
                "Armand Joulin",
                "Sebastian Riedel",
                "Edouard Grave"
            ],
            "title": "Few-shot Learning with Retrieval Augmented Language Models",
            "year": 2022
        },
        {
            "authors": [
                "Joel Jang",
                "Seonghyeon Ye",
                "Changho Lee",
                "Sohee Yang",
                "Joongbo Shin",
                "Janghoon Han",
                "Gyeonghun Kim",
                "Minjoon Seo"
            ],
            "title": "Temporalwiki: A lifelong benchmark for training and evaluating ever-evolving language models",
            "year": 2022
        },
        {
            "authors": [
                "Zhengbao Jiang",
                "Luyu Gao",
                "Jun Araki",
                "Haibo Ding",
                "Zhiruo Wang",
                "Jamie Callan",
                "Graham Neubig."
            ],
            "title": "Retrieval as attention: End-to-end learning of retrieval and reading within a single transformer",
            "venue": "Conference on Empirical Methods in Natural Lan-",
            "year": 2022
        },
        {
            "authors": [
                "Vladimir Karpukhin",
                "Barlas Oguz",
                "Sewon Min",
                "Patrick Lewis",
                "Ledell Wu",
                "Sergey Edunov",
                "Danqi Chen",
                "Wen-tau Yih."
            ],
            "title": "Dense passage retrieval for opendomain question answering",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural",
            "year": 2020
        },
        {
            "authors": [
                "Omar Khattab",
                "Keshav Santhanam",
                "Xiang Lisa Li",
                "David Hall",
                "Percy Liang",
                "Christopher Potts",
                "Matei Zaharia."
            ],
            "title": "Demonstrate-searchpredict: Composing retrieval and language models for knowledge-intensive NLP",
            "venue": "arXiv preprint",
            "year": 2022
        },
        {
            "authors": [
                "Mojtaba Komeili",
                "Kurt Shuster",
                "Jason Weston."
            ],
            "title": "Internet-augmented dialogue generation",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 8460\u20138478, Dublin, Ireland. Association",
            "year": 2022
        },
        {
            "authors": [
                "Tom Kwiatkowski",
                "Jennimaria Palomaki",
                "Olivia Redfield",
                "Michael Collins",
                "Ankur Parikh",
                "Chris Alberti",
                "Danielle Epstein",
                "Illia Polosukhin",
                "Jacob Devlin",
                "Kenton Lee"
            ],
            "title": "Natural questions: a benchmark for question answering research",
            "year": 2019
        },
        {
            "authors": [
                "Angeliki Lazaridou",
                "Elena Gribovskaya",
                "Wojciech Stokowiec",
                "Nikolai Grigorev."
            ],
            "title": "Internetaugmented language models through few-shot prompting for open-domain question answering",
            "venue": "arXiv preprint arXiv:2203.05115.",
            "year": 2022
        },
        {
            "authors": [
                "Haejun Lee",
                "Akhil Kedia",
                "Jongwon Lee",
                "Ashwin Paranjape",
                "Christopher Manning",
                "Kyoung-Gu Woo."
            ],
            "title": "You only need one model for open-domain question answering",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language",
            "year": 2022
        },
        {
            "authors": [
                "Mike Lewis",
                "Yinhan Liu",
                "Naman Goyal",
                "Marjan Ghazvininejad",
                "Abdelrahman Mohamed",
                "Omer Levy",
                "Veselin Stoyanov",
                "Luke Zettlemoyer"
            ],
            "title": "2020a. BART: denoising sequence-to-sequence pre-training for natural language",
            "year": 2020
        },
        {
            "authors": [
                "Patrick Lewis",
                "Ethan Perez",
                "Aleksandra Piktus",
                "Fabio Petroni",
                "Vladimir Karpukhin",
                "Naman Goyal",
                "Heinrich K\u00fcttler",
                "Mike Lewis",
                "Wen-tau Yih",
                "Tim Rockt\u00e4schel"
            ],
            "title": "2020b. Retrieval-augmented generation for knowledge-intensive nlp tasks",
            "year": 2020
        },
        {
            "authors": [
                "Zekun Li",
                "Baolin Peng",
                "Pengcheng He",
                "Michel Galley",
                "Jianfeng Gao",
                "Xifeng Yan."
            ],
            "title": "Guiding large language models via directional stimulus prompting",
            "venue": "arXiv preprint arXiv:2302.11520.",
            "year": 2023
        },
        {
            "authors": [
                "Nelson F Liu",
                "Kevin Lin",
                "John Hewitt",
                "Ashwin Paranjape",
                "Michele Bevilacqua",
                "Fabio Petroni",
                "Percy Liang."
            ],
            "title": "Lost in the middle: How language models use long contexts",
            "venue": "arXiv preprint arXiv:2307.03172.",
            "year": 2023
        },
        {
            "authors": [
                "Kelvin Luu",
                "Daniel Khashabi",
                "Suchin Gururangan",
                "Karishma Mandyam",
                "Noah A. Smith."
            ],
            "title": "Time waits for no one! analysis and challenges of temporal misalignment",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the",
            "year": 2022
        },
        {
            "authors": [
                "Alex Mallen",
                "Akari Asai",
                "Victor Zhong",
                "Rajarshi Das",
                "Hannaneh Hajishirzi",
                "Daniel Khashabi."
            ],
            "title": "When not to trust language models: Investigating effectiveness and limitations of parametric and nonparametric memories",
            "venue": "arXiv preprint.",
            "year": 2022
        },
        {
            "authors": [
                "Jacob Menick",
                "Maja Trebacz",
                "Vladimir Mikulik",
                "John Aslanides",
                "Francis Song",
                "Martin Chadwick",
                "Mia Glaese",
                "Susannah Young",
                "Lucy CampbellGillingham",
                "Geoffrey Irving"
            ],
            "title": "Teaching language models to support answers with verified",
            "year": 2022
        },
        {
            "authors": [
                "Sewon Min",
                "Xinxi Lyu",
                "Ari Holtzman",
                "Mikel Artetxe",
                "Mike Lewis",
                "Hannaneh Hajishirzi",
                "Luke Zettlemoyer"
            ],
            "title": "Rethinking the role of demonstrations: What makes in-context learning work",
            "venue": "In Proceedings of the 2022 Conference on Empirical Methods",
            "year": 2022
        },
        {
            "authors": [
                "Sewon Min",
                "Julian Michael",
                "Hannaneh Hajishirzi",
                "Luke Zettlemoyer."
            ],
            "title": "AmbigQA: Answering ambiguous open-domain questions",
            "venue": "EMNLP.",
            "year": 2020
        },
        {
            "authors": [
                "Long Ouyang",
                "Jeffrey Wu",
                "Xu Jiang",
                "Diogo Almeida",
                "Carroll Wainwright",
                "Pamela Mishkin",
                "Chong Zhang",
                "Sandhini Agarwal",
                "Katarina Slama",
                "Alex Ray"
            ],
            "title": "Training language models to follow instructions with human",
            "year": 2022
        },
        {
            "authors": [
                "Ofir Press",
                "Muru Zhang",
                "Sewon Min",
                "Ludwig Schmidt",
                "Noah A Smith",
                "Mike Lewis."
            ],
            "title": "Measuring and narrowing the compositionality gap in language models",
            "venue": "arXiv preprint arXiv:2210.03350.",
            "year": 2022
        },
        {
            "authors": [
                "Yujia Qin",
                "Shihao Liang",
                "Yining Ye",
                "Kunlun Zhu",
                "Lan Yan",
                "Yaxi Lu",
                "Yankai Lin",
                "Xin Cong",
                "Xiangru Tang",
                "Bill Qian"
            ],
            "title": "Toolllm: Facilitating large language models to master 16000+ real-world apis",
            "venue": "ArXiv preprint,",
            "year": 2023
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J. Liu."
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "Journal of Machine Learning Research,",
            "year": 2020
        },
        {
            "authors": [
                "Rajkumar Ramamurthy",
                "Prithviraj Ammanabrolu",
                "Kiant\u00e9 Brantley",
                "Jack Hessel",
                "Rafet Sifa",
                "Christian Bauckhage",
                "Hannaneh Hajishirzi",
                "Yejin Choi"
            ],
            "title": "Is reinforcement learning (not) for natural language processing?",
            "year": 2022
        },
        {
            "authors": [
                "Paul R\u00f6ttger",
                "Janet Pierrehumbert."
            ],
            "title": "Temporal adaptation of BERT and performance on downstream document classification: Insights from social media",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2021, pages 2400\u20132412, Punta",
            "year": 2021
        },
        {
            "authors": [
                "Devendra Singh Sachan",
                "Siva Reddy",
                "William L. Hamilton",
                "Chris Dyer",
                "Dani Yogatama."
            ],
            "title": "End-toend training of multi-document reader and retriever for open-domain question answering",
            "venue": "Advances in Neural Information Processing Systems 34: An-",
            "year": 2021
        },
        {
            "authors": [
                "Timo Schick",
                "Jane Dwivedi-Yu",
                "Roberto Dess\u00ec",
                "Roberta Raileanu",
                "Maria Lomeli",
                "Luke Zettlemoyer",
                "Nicola Cancedda",
                "Thomas Scialom."
            ],
            "title": "Toolformer: Language models can teach themselves to use tools",
            "venue": "arXiv preprint arXiv:2302.04761.",
            "year": 2023
        },
        {
            "authors": [
                "John Schulman",
                "Philipp Moritz",
                "Sergey Levine",
                "Michael Jordan",
                "Pieter Abbeel."
            ],
            "title": "High-dimensional continuous control using generalized advantage estimation",
            "venue": "arXiv preprint arXiv:1506.02438.",
            "year": 2015
        },
        {
            "authors": [
                "John Schulman",
                "Filip Wolski",
                "Prafulla Dhariwal",
                "Alec Radford",
                "Oleg Klimov."
            ],
            "title": "Proximal policy optimization algorithms",
            "venue": "arXiv preprint arXiv:1707.06347.",
            "year": 2017
        },
        {
            "authors": [
                "Yongliang Shen",
                "Kaitao Song",
                "Xu Tan",
                "Dongsheng Li",
                "Weiming Lu",
                "Yueting Zhuang."
            ],
            "title": "Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface",
            "venue": "arXiv preprint arXiv:2303.17580.",
            "year": 2023
        },
        {
            "authors": [
                "Weijia Shi",
                "Sewon Min",
                "Michihiro Yasunaga",
                "Minjoon Seo",
                "Rich James",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Wen-tau Yih."
            ],
            "title": "Replug: Retrievalaugmented black-box language models",
            "venue": "arXiv preprint arXiv:2301.12652.",
            "year": 2023
        },
        {
            "authors": [
                "Kurt Shuster",
                "Mojtaba Komeili",
                "Leonard Adolphs",
                "Stephen Roller",
                "Arthur Szlam",
                "Jason Weston."
            ],
            "title": "Language models that seek for knowledge: Modular search & generation for dialogue and prompt completion",
            "venue": "Findings of the Association",
            "year": 2022
        },
        {
            "authors": [
                "Hongjin Su",
                "Jungo Kasai",
                "Chen Henry Wu",
                "Weijia Shi",
                "Tianlu Wang",
                "Jiayi Xin",
                "Rui Zhang",
                "Mari Ostendorf",
                "Luke Zettlemoyer",
                "Noah A Smith"
            ],
            "title": "Selective annotation makes language models better fewshot learners. arXiv preprint arXiv:2209.01975",
            "year": 2022
        },
        {
            "authors": [
                "Xuezhi Wang",
                "Jason Wei",
                "Dale Schuurmans",
                "Quoc V. Le",
                "Ed H. Chi",
                "Denny Zhou."
            ],
            "title": "Selfconsistency improves chain of thought reasoning in language models",
            "venue": "CoRR, abs/2203.11171.",
            "year": 2022
        },
        {
            "authors": [
                "Jason Wei",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Maarten Bosma",
                "Brian Ichter",
                "Fei Xia",
                "Ed H. Chi",
                "Quoc V. Le",
                "Denny Zhou."
            ],
            "title": "Chain-of-thought prompting elicits reasoning in large language models",
            "venue": "NeurIPS.",
            "year": 2022
        },
        {
            "authors": [
                "Zhilin Yang",
                "Peng Qi",
                "Saizheng Zhang",
                "Yoshua Bengio",
                "William Cohen",
                "Ruslan Salakhutdinov",
                "Christopher D. Manning"
            ],
            "title": "HotpotQA: A dataset",
            "year": 2018
        },
        {
            "authors": [
                "Shunyu Yao",
                "Jeffrey Zhao",
                "Dian Yu",
                "Nan Du",
                "Izhak Shafran",
                "Karthik Narasimhan",
                "Yuan Cao."
            ],
            "title": "ReAct: Synergizing reasoning and acting in language models",
            "venue": "International Conference on Learning Representations (ICLR).",
            "year": 2023
        },
        {
            "authors": [
                "Wenhao Yu",
                "Dan Iter",
                "Shuohang Wang",
                "Yichong Xu",
                "Mingxuan Ju",
                "Soumya Sanyal",
                "Chenguang Zhu",
                "Michael Zeng",
                "Meng Jiang."
            ],
            "title": "Generate rather than retrieve: Large language models are strong context generators",
            "venue": "International Confer-",
            "year": 2023
        },
        {
            "authors": [
                "Tianjun Zhang",
                "Xuezhi Wang",
                "Denny Zhou",
                "Dale Schuurmans",
                "Joseph E Gonzalez."
            ],
            "title": "Tempera: Test-time prompt editing via reinforcement learning",
            "venue": "The Eleventh International Conference on Learning Representations.",
            "year": 2023
        },
        {
            "authors": [
                "Zhuosheng Zhang",
                "Aston Zhang",
                "Mu Li",
                "Alex Smola."
            ],
            "title": "Automatic chain of thought prompting in large language models",
            "venue": "The Eleventh International Conference on Learning Representations (ICLR 2023).",
            "year": 2023
        },
        {
            "authors": [
                "Daniel M Ziegler",
                "Nisan Stiennon",
                "Jeffrey Wu",
                "Tom B Brown",
                "Alec Radford",
                "Dario Amodei",
                "Paul Christiano",
                "Geoffrey Irving."
            ],
            "title": "Fine-tuning language models from human preferences",
            "venue": "arXiv preprint arXiv:1909.08593.",
            "year": 2019
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Large Language Models (LLMs) have shown remarkable abilities for human language processing and extraordinary scalability and adaptability in few- or zero-shot settings.(Ouyang et al., 2022; Brown et al., 2020; Chowdhery et al., 2022). However, the training process depends on large-scale high-quality corpora but without the perception\n\u2217 Work done during an internship at 3Microsoft Research Asia. # Equal contribution. \u2020Corresponding author.\nThis paper was partially supported by Joint Research Project of Yangtze River Delta Science and Technology Innovation Community (No. 2022CSJGG1400).\n1https://github.com/xbmxb/RAG-query-rewriting\nof the real world. Thus, LLMs still have to face the issue of hallucination (Yao et al., 2023; Bang et al., 2023) and temporal misalignment (R\u00f6ttger and Pierrehumbert, 2021; Luu et al., 2022; Jang et al., 2022). This affects the reliability of LLMs and hinders wider practical application, because the consistency between the LLM responses with the real world needs further validation. Existing work has proved that incorporating external knowledge (i.e., non-parametric knowledge) with internal knowledge (i.e., parametric knowledge) can effectively alleviate hallucination, especially for knowledge-intensive tasks. In fact, retrievalaugmented LLMs have been shown so effective that they have been regarded as a standard solution to alleviate the factuality drawbacks in naive LLM generations. Retrieval augmentation is applied to select relative passages as external contexts for the language model, which is retrieve-then-read framework (Lewis et al., 2020b; Karpukhin et al., 2020; Izacard et al., 2022). Take the open-domain Question-Answering task (open-domain QA) as an example, a retriever first searches for related documents for a question. Then the LLM receives the question and the documents, then predicts an answer.\nAs most LLMs are only accessible through inference APIs, they play the part of black-box frozen readers in the pipeline. This makes previous retrieval augmentation methods that require complete access (Lewis et al., 2020b; Guu et al., 2020; Izacard et al., 2022) no longer feasible. Recent studies on retrieval-augmented language models lean more on the LLM-oriented adaptation. An idea is to train a dense retrieval model to cater to the frozen language model (Shi et al., 2023). By using feedback from the LLM as a training objective, the retrieval model is tuned for better LLM input contexts. Another research line focuses on the design of interactions between the retriever and the reader (Yao et al., 2023; Khattab et al., 2022), where both the\nretriever and the reader are usually frozen. The idea is to trigger the emergent ability through carefully crafted prompts or a sophisticated prompt pipeline. Multiple interactions with external knowledge allow the LLM to approach the correct answer step by step.\nHowever, there are still problems remaining to be solved. Existing approaches overlook the adaptation of the query, i.e., the input of the retrievethen-read pipeline. The retrieval query is either original from datasets or directly determined by the black-box generation, thus is always fixed. However, there is inevitably a gap between the input text and the knowledge that is really needed to query. This limits performance and places a burden on retrieval capability enhancement and prompt engineering.\nIn consideration of this issue, this paper proposes Rewrite-Retrieve-Read, a new framework for retrieval augmentation, which can be further tuned for adapting to LLMs. In front of the retriever, a step of rewriting the input is added, filling the gap between the given input and retrieval need, as is shown in Figure 1. We adopt the off-the-shelf tool, an internet search engine, as the retriever, which avoids the maintenance of the search index and can access up-to-date knowledge (Lazaridou et al., 2022). Different from previous studies (Khattab et al., 2022; Yao et al., 2023) that require the memory of multiple interaction rounds between the retriever and the LLM for each sample, the motivation of our rewriting step is to clarify the retrieval need from the input text.\nWe also propose a trainable scheme for our rewrite-retrieve-read framework (Figure 1 (c)). The black-box retriever and the reader form a frozen system. To further smooth the steps of our pipeline, we apply a small, trainable language model to perform the rewriting step, denoted as the rewriter. The rewriter is trained by reinforcement learning using the LLM performance as a reward, learning to adapt the retrieval query to improve the reader on downstream tasks.\nOur proposed methods are evaluated on knowledge-intensive downstream tasks including open-domain QA (HotpoQA (Yang et al., 2018), AmbigNQ (Min et al., 2020), PopQA (Mallen et al., 2022)) and multiple choice QA (MMLU (Hendrycks et al., 2021)). The experiments are implemented on T5-large (Raffel et al., 2020) as the rewriter, ChatGPT (Ouyang et al., 2022) and\nVicuna-13B (Chiang et al., 2023) as the LLM reader. The results show that query rewriting consistently improves the retrieve-augmented LLM performance. The results also indicate that the smaller language model can be competent for query rewriting.\nTo sum up, our proposed novel retrievalaugmentation method, rewrite-retrieve-read is the first framework where the input text is adapted for the frozen retriever and LLM reader. We introduce a tuneable scheme with a small, trainable model, achieving performance gains with less resource consumption."
        },
        {
            "heading": "2 Related Work",
            "text": ""
        },
        {
            "heading": "2.1 Retrieval Augmentation",
            "text": "Language models require external knowledge to alleviate the factuality drawbacks. Retrieval augmentation has been regarded as the standard effective solution. With a retrieval module, related passages are provided to the language model as the context of the original input. Thus factual information like common sense or real-time news helps with output prediction through contextualized reading comprehension.\nEarlier studies use sparse retriever (Chen et al., 2017) or dense retriever (Karpukhin et al., 2020) in front of a pre-trained language model (PrLM). The neural retriever and reader are both PrLMs of trainable size like BERT (Devlin et al., 2019) or BART (Lewis et al., 2020a). Hence, the whole retrieve-then-reader framework is a tuneable endto-end system, where the retrieved contexts can be regarded as the intermediate results (Karpukhin et al., 2020; Lewis et al., 2020b). Approaches to smooth the two-step framework are proposed to optimize the retrieval and the reading comprehension (Sachan et al., 2021; Lee et al., 2022; Jiang et al., 2022). More recently, retrieval remains a powerful enhancement as the size of models and data scales rapidly (Mallen et al., 2022; Shi et al., 2023; Brown et al., 2020). On the other hand, retrieval enhancement can compensate for the shortfall in parameter size, compared to large-scale language models. For example, by jointly training the retriever and the reader, Atlas (Izacard et al., 2022) shows few-shot performance on par with 540B PalM (Chowdhery et al., 2022) but be of 50\u00d7 smaller size. The Internet as a knowledge base More related to our work, the search engine can assume the role of the retriever and use the Internet as the source of\nexternal knowledge. Komeili et al. (2022) use an internet search for relevant information based on the dialogue history to perform dialogue response generation. SeeKeR (Shuster et al., 2022) use a single Transformer to iteratively perform search query generation, then knowledge extraction for dialogue generation and sentence completion. For large-scale models, web search still shows effective for knowledge augmentation (Lazaridou et al., 2022), fact-checking (Menick et al., 2022), and LLM agent enhancement (Yao et al., 2023)."
        },
        {
            "heading": "2.2 Cooperation with Black-box LLMs",
            "text": "Large Language Models, such as ChatGPT (Ouyang et al., 2022), Codex (Chen et al., 2021), PaLM (Chowdhery et al., 2022), emerge impressive natural language processing ability as well as remarkable scalability. This leads to a tendency to embrace LLMs on a wide range of NLP tasks. However, LLMs are only accessible as a black box in most cases, which is because (i) Some like ChatGPT are not open-source and kept private; (ii) The large parameter scale requires computational resources that are not always affordable to users. This constraint means nothing is available except input and output texts.\nExisting studies have proved that LLMs\u2019 abilities can be better leveraged by carefully designed interaction methods. GenRead (Yu et al., 2023) prompts an LLM to generate context instead of deploying a retriever, showing that LLMs can retrieve internal knowledge by prompting. ReAct\n(Yao et al., 2023) and Self-Ask (Press et al., 2022) combines the Chain-of-Thought (CoT) (Wei et al., 2022; Wang et al., 2022) and inter-actions with web APIs. Only relying on prompt construction, ReAct provides novel baselines for interactive tasks. Demonstrate\u2013Search\u2013Predict (DSP) (Khattab et al., 2022) defines a sophisticated pipeline between an LLM and a retriever. Unlike ReAct, DSP integrates prompts for demonstration bootstrap besides multihop breakdown and retrieval.\nDespite the promising performance in the zero or few-shot setting, the behavior of LLMs sometimes needs adjustments. A feasible approach is to append trainable small models in front of or after the LLM. The small models, as a part of the parameters of the system, can be fine-tuned for optimization. RePlug (Shi et al., 2023) is proposed to fine-tune a dense retriever for the frozen LLM in the retrievethen-read pipeline. The retriever is trained under the LLM\u2019s supervision to retrieve documents that are suitable for the LLM. With the same purpose, Directional Stimulus Prompting (Li et al., 2023) deploys a small model to provide the LLM with stimulus (e.g., keywords for summarization, or dialogue actions for response generation), which is updated according to the LLM reward.\nDifferent from the inspiring work mentioned above, our proposed pipeline contains a query rewriting step in front of the retrieve-then-read module. We further propose a trainable scheme with a small rewriting model, which is a novel enhancement for retrieval-augmented LLM by re-\nconstructing the search query."
        },
        {
            "heading": "3 Methodology",
            "text": "We present Rewrite-Retrieve-Read, a pipeline that improves the retrieval-augmented LLM from the perspective of query rewriting. Figure 1 shows an overview. This section first introduces the pipeline framework in section 3.1, then the trainable scheme in section 3.2.\n3.1 Rewrite-Retrieve-Read A task with retrieval augmentation can be denoted as follows. Given a dataset of a knowledgeintensive task (e.g., open-domain QA), D = {(x, y)i}, i = 0, 1, 2, . . . , N , x (e.g., a question) is the input to the pipeline, y is the expected output (e.g., the correct answer). Our pipeline consists of three steps. (i) Query rewrite: generate a query x\u0303 for required knowledge based on the original input x. (ii) Retrieve: search for related context, doc. (iii) Read: comprehend the input along with contexts [doc, x] and predict the output y\u0302.\nA straightforward but effective method is to ask an LLM to rewrite queries to search for information that is potentially needed. We use a few-shot prompt to encourage the LLM to think, and the output can be none, one or more queries to search."
        },
        {
            "heading": "3.2 Trainable Scheme",
            "text": "Besides, total reliance on a frozen LLM has shown some drawbacks. Reasoning errors or invalid search hinders the performance (Yao et al., 2023; BehnamGhader et al., 2022). On the other hand, retrieved knowledge may sometimes mislead and compromise the language model (Mallen et al., 2022). To better align to the frozen modules, it is feasible to add a trainable model and adapt it by taking the LLM reader feedback as a reward.\nBased on our framework, we further propose to utilize a trainable small language model to take over the rewriting step, as is shown in the right part of Figure 1. The trainable model is initialized with the pre-trained T5-large (770M) (Raffel et al., 2020), denoted as trainable rewriter, G\u03b8. The rewriter is first trained on pseudo data to warm up (\u00a73.2.1), then continually trained by reinforcement learning (\u00a73.2.2)."
        },
        {
            "heading": "3.2.1 Rewriter Warm-up",
            "text": "The task, query rewriting, is quite different from the pre-training objective of sequence-to-sequence generative models like T5. First, we construct a\npseudo dataset for the query rewriting task. Inspired by recent distillation methods (Hsieh et al., 2023; Ho et al., 2022), we prompt the LLM to rewrite the original questions x in the training set and collect the generated queries x\u0303 as pseudo labels. The collected samples are then filtered: Those that get correct predictions from the LLM reader are selected into the warm-up dataset, denoted as DTrain = {(x, x\u0303)|y\u0302 = y}. The rewriter G\u03b8 is finetuned on DTrain with the standard log-likelihood as the training objective, denoted as\nLwarm = \u2212 \u2211 t logp\u03b8( \u02c6\u0303xt | x\u0303<t, x ). (1)\nThe rewriter model after warm-up shows modest performance, which depends on the pseudo data quality and rewriter capability. Highly relying on the human-written prompt line, x\u0303 can be suboptimal. The relatively small scale of the rewriter size is also a limitation of the performance after the warm-up. Then we turn to reinforcement learning to align the rewriter to the following retriever and LLM reader."
        },
        {
            "heading": "3.2.2 Reinforcement Learning",
            "text": "To further fine-tune the rewriter to cater to the LLM reader, we adopt a policy gradient reinforcement learning framework. Task Formulation In the context of reinforcement learning, the rewriter optimization is formulated as a Markov Decision Process 5-tuple \u27e8S,A, P,R, \u03b3\u27e9. (i) The state space S is a finite set limited by the vocabulary and the sequence length. (ii) The action space A is equals to the vocabulary. (iii) The transition probability P is determined by the policy network, which is the rewriter model G\u03b8. (iv) The reward function R gives a reward value that depends on the current state. The policy gradient is derived from rewards, used as the training objective. (v) \u03b3 denotes the discount factor. More specifically, the rewriter G\u03b8 after the warm-up is the initial policy model \u03c00. At each step t, the action at is to generate the next token \u02c6\u0303xt based on the observation of the present state, st = [x, \u02c6\u0303x<t]. When the generation is stopped by the End-Of-Sentence token, one episode is ended. After finishing the retrieval and reading, a reward is computed by evaluating the final output, i.e., a score for the LLM reader prediction. Policy Optimization We adopt Proximal Policy Optimization (PPO) (Schulman et al., 2017), following (Ramamurthy et al., 2022). Maximization\nof the expectation of the reward R is formulated as\nmax \u03b8 E\u02c6\u0303x\u223cp\u03b8(\u00b7|x)[R(x, \u02c6\u0303x)],\nmax \u03b8\nE(st,at)\u223c\u03c0\u03b8\u2032 [min{kt,\u03b8A \u03b8\u2032 (st, at) ; clip (kt,\u03b8, 1\u2212 \u03b5, 1 + \u03b5)A\u03b8 \u2032 (st, at)}],\nkt,\u03b8 = p\u03b8 (at | st) p\u03b8\u2032 (at | st) ,\n(2)\nwhere \u03b8\u2032 is the temporarily fixed policy for sampling and \u03b8 is updated. A denotes the advantage function, which is formulated based on the estimation of value network V\u03d5. The value network V\u03d5 is initialized from the policy network \u03c00. The formulation follows Generalized Advantage Estimation (GAE) (Schulman et al., 2015).\n\u03b4t = R (st, at) + V\u03d5 (st+1)\u2212 V\u03d5 (st) ,\nA\u0302\u03b8t (st, at) = \u221e\u2211 t\u2032=0 \u03bbt \u2032 \u03b4t+t\u2032 ,\n(3)\nwhere \u03bb is the bias-variance trade-off parameter. The reward function R reflects the quality of the generated queries, which needs to be consistent with the final evaluation of the task. \u02c6\u0303x is fed to the retriever and the reader for a final prediction y\u0302. A part of the reward function is the measures of y\u0302 compared to the golden label y (e.g., exact match and F1 of the predicted answers), denoted as Rlm. Besides, a KL-divergence regularization is added to prevent the model from deviating too far from the initialization (Ramamurthy et al., 2022; Ziegler et al., 2019).\nR (st, at) = Rlm(\u02c6\u0303x, y)\u2212 \u03b2KL (\u03c0\u03b8\u2225\u03c00) . (4)\nThe final loss function is composed of policy loss and value loss.\nL\u03b8 = \u2212 1 |S|T \u2211 \u03c4\u2208S T\u2211 t=0 min(kt,\u03b8A \u03b8\u2032 , clipA\u03b8 \u2032 ),\nL\u03d5 = 1 |S|T \u2211 \u03c4\u2208S T\u2211 t=0 (V\u03d5 (st)\u2212Rt)2 ,\nLppo = L\u03b8 + \u03bbvL\u03d5. (5)\nHere, S denotes the sampled set, and T is for step numbers."
        },
        {
            "heading": "4 Implementation",
            "text": "Rewriter For the frozen pipeline in \u00a73.1, we prompt an LLM to rewrite the query with few-shot\nin-context learning (Brown et al., 2020; Min et al., 2022). Our prompt follows the formulation of [instruction, demonstrations, input], where the input is x. The instruction is straightforward and demonstrations are 1-3 random examples from training sets and are kept constant across all runs, mainly for the task-specific output format illustration, i.e., a short phrase as an answer for HotpotQA, and an option as an answer for MMLU. For the training scheme in \u00a73.2, we fine-tuning a T5 as the rewriter. Retriever We use the Bing search engine as the retriever. It requires no candidate index construction like a dense retriever, nor candidates like a textbook. But it allows for a wide knowledge scope and up-to-time factuality. With Bing API, the retrieval is performed in two approaches. (i) For all retrieved web pages, we concatenate the snippets that are related sentences selected by Bing. This method is similar to using a search engine in a browser, input a query and press Enter, then collect the texts shown on the search result page. (ii) For retrieved web pages, we request the URLs and parser to get all the texts. This is similar to clicking on items on the search result page. Then we use BM25 to keep those with higher relevance scores with the query, reducing the document length. Reader The reader is a frozen LLM, where we adopt ChatGPT (gpt-3.5-turbo) and Vicuna-13B. It performs reading comprehension and prediction with few-shot in-context learning. In our prompt, following the brief instruction and the demonstrations, the input is x or [doc, \u02c6\u0303x] with retrieval augmentation.\nIt has been proved that both the phrasing of prompt lines (Zhang et al., 2023a) and the selection of demonstrations show effects on the in-context learning performance (Su et al., 2022; Zhang et al., 2023b). As it is not the focus of this work, we pay no more attention to prompt editing."
        },
        {
            "heading": "5 Experiments",
            "text": ""
        },
        {
            "heading": "5.1 Task Settings",
            "text": ""
        },
        {
            "heading": "5.1.1 Open-domain QA",
            "text": "Three open-domain QA datasets are used for evaluation. (i) HotPotQA (Yang et al., 2018) consists of complex questions that require multi-hop reasoning. We evaluate the full test set. (ii) AmbigNQ (Min et al., 2020) provides a disambiguated version of Natural Questions (NQ) (Kwiatkowski et al., 2019). For ambiguous questions in NQ, minimal constraints are added to break it into several similar\nbut specific questions. The first 1000 samples are evaluated in the test set. (iii) PopQA (Mallen et al., 2022) includes long-tail distributions as it contains more low-popularity knowledge than other popular QA tasks. We split the dataset into 13k for training and 714 for testing.\nOpen-domain QA benchmarks are sets of question-answer pairs denoted as {(q, a)i}. We use ChatGPT for both the reader and the frozen rewriter. The evaluation metrics are Exact Match (EM ) and F1 scores. For the reward function in RL, we use an indicator to reward if the retrieved content hits the answer and penalize if misses the answer, denoted as Hit. The total reward is a weighted sum of EM, F1, and Hit.\nHit = { 1 a in doc, \u22121 else\nRlm = EM + \u03bbfF1 + \u03bbhHit.\n(6)"
        },
        {
            "heading": "5.1.2 Multiple-choice QA",
            "text": "For multiple-choice QA, our evaluation is conducted on Massive Multi-task Language Understanding (MMLU) (Hendrycks et al., 2021), an exam question dataset including 4 categories: Humanities, STEM, Social Sciences, and Other. Each category is split into 80% for the training set and 20% for the test set.\nMultiple-choice QA can be formulated as {(q\u2032, a)i}, where q\u2032 = [q, c0, c1, c2, c3]. c denotes the options, generally there are four for each question. The retrieved documents that are included in the officially provided contaminated lists are ignored. The questions with options are rewritten into search queries. The answer is one option. EM is reported as metrics and used for the reward.\nRlm = EM. (7)\nWe use ChatGPT as a frozen rewriter and the reader.\nWe also use Vicuna-13B as the reader for evaluation due to the rate limit issue of ChatGPT. More information on datasets and training setup are presented in the appendix."
        },
        {
            "heading": "5.2 Baselines",
            "text": "The following settings are implemented to evaluate and support our methods. (i) Direct: The standard in-context learning without any augmentations. (ii) Retrieve-then-read: The standard retrieval-augmented method. Retrieved documents are concatenated with the question. (iii) LLM as a frozen rewriter: As is introduced in \u00a73.1, we prompt a frozen LLM to reason and generate queries by few-shot in-context learning. (iv) Trainable rewriter: Applying the fine-tuned rewriter, the output queries are used by the retriever and the reader. Table 1 presents prompt line forms. Please note that the prompts for prediction are kept the same for each task."
        },
        {
            "heading": "5.3 Results",
            "text": "Experimental results on open-domain QA are reported in Table 2. For the three datasets, query rewriting consistently brings performance gain with both a frozen rewriter and a trainable rewriter. On AmbigNQ and PopQA, the standard retrieval augments the reader, indicating useful external knowledge is retrieved. On HotpotQA, the standard retrieval hurts the reader. This shows that using complex questions as queries cannot compensate for the parametric knowledge, but bring noises instead (Mallen et al., 2022). This suggests that multi-hop questions are not suitable queries for the web search engine. The scores increase by adding the rewriting step. On PopQA, our trainable rewriter surpasses standard retrieval while being inferior to the LLM rewriter. This indicates that the\ndistillation of query rewriting is sub-optimal. The scores on multiple-choice QA are presented in Table 3. With ChatGPT as a reader, it can be observed that query rewriting improves the scores in most of the settings, except for the social sciences category. With Vicuna as a reader, our method achieves more gains on the four categories compared to ChatGPT. This agrees with the intuition that a more powerful reader has more parametric memories, thus more difficult to compensate with external knowledge."
        },
        {
            "heading": "6 Analysis",
            "text": ""
        },
        {
            "heading": "6.1 Training Process",
            "text": "The training process includes two stages, warm-up and reinforcement learning. This section shows the validation scores of the three open-domain QA datasets for further analysis. Figure 2 presents the metric scores through training iterations in the process of reinforcement learning. As the rewriting models have been warmed up on the pseudo data before RL, scores at \u201c0 iteration\u201d denote the ability acquired from the warm-up training.\nIt can be observed that the curves show upward trends with some fluctuations on all the datasets. (i) For multi-hop questions in HotpotQA, the standard retrieval is relatively weaker. Complex questions can be not specific search queries and show a larger gap from rewritten queries, i.e., the green and red lines. (ii) On AmbigNQ and PopQA, our method surpasses the baselines after several iterations (3 or 4). This indicates that the RL training stage can compensate for the insufficiency of the distillation on the pseudo data during warm-up training. (iii) In particular, on PopQA, the trainable rewriter remains inferior to the LLM rewriter. This can be explained as the dataset is constructed for adaptive retrieval (Mallen et al., 2022), which only uses retrieval where it helps to avoid harmful redundant retrieval. Thus, \u201cNone\u201d is a possible query that means no retrieval. This causes more complexity and uncertainty. LLM rewriter knows better when the retrieval is needed for itself as a reader, although the rewriting step is not concatenated as\nthe input context of the reader. We calculate the performance of query \u201cNone\u201d. The questions that can be correctly answered without retrieval (i.e., the \u201cDirect\u201d method) are those samples that need no more context. Comparing this retrieval-free set with those that are rewritten to be\u201cNone\u201d query, the F1 score of the LLM rewriter is 71.9% and the T5 rewriter score is 67.1%. If we consider the questions that can be correctly answered without retrieval but go wrong with retrieval as the retrieval-free set, the F1 scores are 78.7% for LLM rewriter and 77.4% for T5."
        },
        {
            "heading": "6.2 Retrieval Result",
            "text": "Our proposed method is a pipeline framework, instead of an end-to-end system. The query rewriting first affects the retrieved context, then the context makes a difference to the output of the reader. Hence, QA metrics are indirect measurements. We take a closer look at the retrieved context and the reader capability through the retrieval metric, hit ratio. After text normalization, the hit rate is computed to measure whether the retrieved context contains the correct answers.\nTable 4 shows the scores on AmbigNQ. The scores in the second line are computed on a selection of the samples whose retrieved contexts hit correct answers (under the standard retrieve-thenread setting). The scores show the approximate upper bound ability of the reader with retrieval augmentation, abbreviated as the \u201cupper bound\u201d score. The effectiveness of retrieval is proved compared to the no retrieval setting (the first line). For each retrieval method, two settings are presented: (i) collecting Bing snippets, (ii) selecting from URLs by BM25. The metrics show that content selection with BM25 recalls better documents than snippets,\n2Our trainable rewriter is adapted to the retriever using BM25 during RL training. Using the output queries of the test set after training, the snippet hit rate is 73.4%.\nwhile query rewriting makes progress on both settings. We also observed that the improvement in the hit rate of the retriever is more significant than the improvement in the reader. This is consistent with the findings in related search (Mallen et al., 2022; Liu et al., 2023)."
        },
        {
            "heading": "6.3 Case Study",
            "text": "To intuitively show how the query rewriting makes a difference in the retrieved contexts and prediction performance, we present examples in Figure 3 to compare the original questions and the queries. In example 1, the original question asks for a film that the youngest daughter of Lady Mary-Gaye Curzon co-stars with two certain actors. Both query 1 and query 2 put the keyword film forward, closely following the youngest daughter of Lady Mary-Gaye Curzon. With both, the actress Charlotte Calthorpe and her movie information can be retrieved and the answer is included. The second is an example where the query from the LLM rewriter failed but\nthe query from T5 gets the correct answer. The number 2000 is misunderstood in query 1, while query 2 keeps 200 movie together, avoiding meaningless retrieval. Example 3 is for multiple choice. The query simplifies the background and enhances the keyword community planner. The retrieve contexts are mainly about Introduction to Community Planning where the answer environment appears several times."
        },
        {
            "heading": "7 Conclusion",
            "text": "This paper introduces the Rewrite-Retrieve-Read pipeline, where a query rewriting step is added for the retrieval-augmented LLM. This approach is applicable for adopting a frozen large language model as the reader and a real-time web search engine as the retriever. Further, we propose to apply a tuneable small language model the rewriter, which can be trained to cater to the frozen retriever and reader. The training implementation consists of two stages, warm-up and reinforcement learning. Evaluation and analyses on open-domain QA and multiple-choice QA show the effectiveness of query rewriting. Our work proposes a novel retrieval-augmented black-box LLM framework, proves that the retrieval augmentation can be enhanced from the aspect of query rewriting, and provides a new method for integrating trainable modules into black-box LLMs.\nLimitations\nWe acknowledge the limitations of this work. (i) There is still a trade-off between generalization and specialization among downstream tasks. Adding a training process, the scalability to direct transfer is compromised, compared to few-shot in-context learning. (ii) The research line of LLM agent has shown impressive performance but relies on multiple calls to the LLM for each sample (Khattab et al., 2022; Yao et al., 2023), where the LLM plays as an agent to flexibly call the retriever multiple times, reads the context in earlier hops, and generates follow-up questions. Different from these studies, our motivation is to enhance the oneturn retriever-then-read framework with a trainable query rewriter. (iii) Using a web search engine as the retriever also leads to some limitations. Neural dense retrievers that are based on professional, filtered knowledge bases may potentially achieve better and controllable retrieval. More discussion is included in the appendix."
        },
        {
            "heading": "A Warm-up Dataset",
            "text": "For the warm-up training of the tuneable rewriter, we construct a pseudo dataset for the query rewriting task. For benchmarks that provide official training and test splits (HotpotQA and AmbigNQ), we use the whole training set. For those that have no official splits (PopQA and MMLU), we randomly split the full dataset. In detail, PopQA contains 16 types of questions, thus split into 13k for training and 714 for testing following stratified sampling. For MMLU, each of the 4 categories is randomly split into 80% for the training set and 20% for the test set. Then the training sets of each benchmark are used to derive the pseudo dataset for the query rewriting, i.e., DTrain = {(x, x\u0303)|y\u0302 = y}. We present the statistics of the splits and warm-up dataset in Table 5."
        },
        {
            "heading": "B Setup Details",
            "text": "For warm-up, we train the T5-large with 3e-5 learning rate, {16, 20} batch size, for {6,8,12} epochs. For reinforcement learning, we set the sampling\nsteps to 5120, 10 threads, 512 steps for each. After sampling, the policy network is trained for {2,3,4} epochs, with learning rate as 2e-6 and batch size as {8,16}. \u03bbf and \u03bbh are 1.0. \u03b2 in Eq. 4 is dynamically adapted according to Ramamurthy et al. (2022); Ziegler et al. (2019),\net = clip\n( KL (\u03c0\u2225\u03c00)\u2212KLtarget\nKLtarget ,\u22120.2, 0.2\n) ,\n\u03b2t+1 = \u03b2t (1 + K\u03b2et) ,\nwhere KLtarget is set to 0.2, K\u03b2 is set to 0.1. \u03b20 is initialized to be 0.001. The generation strategy follows the 4-beam search and returns the one sequence. In the implementation of the BM25based retriever, the textboxes from searched URLs are parsed from HTML code. We compute BM25 scores between the paragraph from each textbox and the query following the scikit-learn package, then keep those with higher scores until the reserved context reaches a max length. In reinforcement learning, the results of AmbigNQ are with the BM25 method, while others use snippets as context."
        },
        {
            "heading": "C Web Search: Tool Use",
            "text": "Our proposed pipeline integrates an externally built web search engine as the retriever module. We present more discussion on the advantages and disadvantages here.\nThe usage of external tools expands the ability boundary of language models, compensating for the parametric knowledge, and grounding the capabilities of language models to interact with environments (Qin et al., 2023; Schick et al., 2023). Recent studies show a trend to leverage plug-andplay tools like search engines to enhance language agents (Lazaridou et al., 2022; Menick et al., 2022; Shuster et al., 2022; Shen et al., 2023). Search engine APIs are well-developed retrievers, saving efforts to build and maintain another retriever, like a Contriever. Accessible to the whole Internet, the web search retrieves from a wide-range, up-to-date\nknowledge base. The temporal misalignment problem on a fixed candidate database can be alleviated.\nOn the other hand, web search APIs are commercial products requiring subscriptions. Also, the vast amount of knowledge on the web can be difficult to control. The retrieved context from the Internet can be occasionally inconsistent, redundant, and toxic, which hinders the LLM reader.\nBeyond retrieval augmentation, in a general scope, other tools called by LLMs, like code interpreters, online models, and expert applications, are all similar to search engines, without trainable parameters to optimize. There could be a gap between the LM and these tools. This paper proposes an idea to align them through a trainable small model."
        }
    ],
    "title": "Query Rewriting for Retrieval-Augmented Large Language Models",
    "year": 2023
}