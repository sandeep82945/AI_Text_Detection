{
    "abstractText": "Text summarization models are evaluated in terms of accuracy and quality using various measures such as ROUGE, BLEU, METEOR, BERTScore, PYRAMID, readability, and several recently proposed ones. The central objective of all accuracy measures is to evaluate the model\u2019s ability to capture saliency accurately. Since saliency is subjective w.r.t the readers\u2019 preferences, there cannot be a fit-all summary for a given document. This means that in many use cases, summarization models need to be personalized w.r.t user profiles. However, to our knowledge, there is no measure to evaluate the degree of personalization of a summarization model. In this paper, we first establish that existing accuracy measures cannot evaluate the degree of personalization of any summarization model and then propose a novel measure, called EGISES, for automatically computing the same. Using the PENS dataset released by Microsoft Research, we analyze the degree of personalization of ten different state-of-the-art summarization models (both extractive and abstractive), five of which are explicitly trained for personalized summarization, and the remaining are appropriated to exhibit personalization. We conclude by proposing a generalized accuracy measure, called P -Accuracy, for designing accuracy measures that should also take personalization into account and demonstrate the robustness and reliability of the measure through meta-evaluation.",
    "authors": [
        {
            "affiliations": [],
            "name": "Rahul Vansh"
        },
        {
            "affiliations": [],
            "name": "Sourish Dasgupta"
        },
        {
            "affiliations": [],
            "name": "Tanmoy Chakraborty"
        }
    ],
    "id": "SP:dd5688c0dd6992633e5505a20b6de7e4ef946ba1",
    "references": [
        {
            "authors": [
                "Xiang Ao",
                "Xiting Wang",
                "Ling Luo",
                "Ying Qiao",
                "Qing He",
                "Xing Xie."
            ],
            "title": "PENS: A dataset and generic framework for personalized news headline generation",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and",
            "year": 2021
        },
        {
            "authors": [
                "Satanjeev Banerjee",
                "Alon Lavie."
            ],
            "title": "METEOR: An automatic metric for MT evaluation with improved correlation with human judgments",
            "venue": "Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Transla-",
            "year": 2005
        },
        {
            "authors": [
                "Manik Bhandari",
                "Pranav Gour",
                "Atabak Ashfaq",
                "Pengfei Liu",
                "Graham Neubig."
            ],
            "title": "Re-evaluating evaluation in text summarization",
            "venue": "arXiv preprint arXiv:2010.07100.",
            "year": 2020
        },
        {
            "authors": [
                "Eirini Chatzikoumi."
            ],
            "title": "How to evaluate machine translation: A review of automated and human metrics",
            "venue": "Natural Language Engineering, 26(2):137\u2013161.",
            "year": 2020
        },
        {
            "authors": [
                "Pierre Jean A Colombo",
                "Chlo\u00e9 Clavel",
                "Pablo Piantanida."
            ],
            "title": "Infolm: A new metric to evaluate summarization & data2text generation",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 10554\u201310562.",
            "year": 2022
        },
        {
            "authors": [
                "Daniel Deutsch",
                "Rotem Dror",
                "Dan Roth."
            ],
            "title": "A Statistical Analysis of Summarization Evaluation Metrics Using Resampling Methods",
            "venue": "Transactions of the Association for Computational Linguistics, 9:1132\u20131146.",
            "year": 2021
        },
        {
            "authors": [
                "Abdul Ghafoor Etemad",
                "Ali Imam Abidi",
                "Megha Chhabra."
            ],
            "title": "Fine-tuned t5 for abstractive summarization",
            "venue": "International Journal of Performability Engineering, 17(10).",
            "year": 2021
        },
        {
            "authors": [
                "Alexander R Fabbri",
                "Wojciech Kry\u015bci\u0144ski",
                "Bryan McCann",
                "Caiming Xiong",
                "Richard Socher",
                "Dragomir Radev."
            ],
            "title": "Summeval: Re-evaluating summarization evaluation",
            "venue": "Transactions of the Association for Computational Linguistics, 9:391\u2013409.",
            "year": 2021
        },
        {
            "authors": [
                "Yang Gao",
                "Wei Zhao",
                "Steffen Eger."
            ],
            "title": "Supert: Towards new frontiers in unsupervised evaluation metrics for multi-document summarization",
            "venue": "arXiv preprint arXiv:2005.03724.",
            "year": 2020
        },
        {
            "authors": [
                "Yanjun Gao",
                "Chen Sun",
                "Rebecca J Passonneau."
            ],
            "title": "Automated pyramid summarization evaluation",
            "venue": "Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL).",
            "year": 2019
        },
        {
            "authors": [
                "Samira Ghodratnama",
                "Amin Beheshti",
                "Mehrdad Zakershahrak",
                "Fariborz Sobhanmanesh."
            ],
            "title": "Extractive document summarization based on dynamic feature space mapping",
            "venue": "IEEE Access, 8:139084\u2013 139095.",
            "year": 2020
        },
        {
            "authors": [
                "Samira Ghodratnama",
                "Mehrdad Zakershahrak",
                "Fariborz Sobhanmanesh."
            ],
            "title": "Adaptive summaries: A personalized concept-based summarization approach by learning from users\u2019 feedback",
            "venue": "CoRR, abs/2012.13387.",
            "year": 2020
        },
        {
            "authors": [
                "Yvette Graham."
            ],
            "title": "Re-evaluating automatic summarization with BLEU and 192 shades of ROUGE",
            "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 128\u2013 137, Lisbon, Portugal. Association for Computational",
            "year": 2015
        },
        {
            "authors": [
                "Aniko Hannak",
                "Piotr Sapiezynski",
                "Arash Molavi Kakhki",
                "Balachander Krishnamurthy",
                "David Lazer",
                "Alan Mislove",
                "Christo Wilson."
            ],
            "title": "Measuring personalization of web search",
            "venue": "Proceedings of the 22nd international conference on",
            "year": 2013
        },
        {
            "authors": [
                "Raghav Jain",
                "Vaibhav Mavi",
                "Anubhav Jangra",
                "Sriparna Saha."
            ],
            "title": "Widar-weighted input document augmented rouge",
            "venue": "Advances in Information Retrieval: 44th European Conference on IR Research, ECIR 2022, Stavanger, Norway, April 10\u201314, 2022,",
            "year": 2022
        },
        {
            "authors": [
                "Eric Langford",
                "Neil Schwertman",
                "Margaret Owens"
            ],
            "title": "Is the property of being positively correlated transitive",
            "venue": "The American Statistician,",
            "year": 2001
        },
        {
            "authors": [
                "Mike Lewis",
                "Yinhan Liu",
                "Naman Goyal",
                "Marjan Ghazvininejad",
                "Abdelrahman Mohamed",
                "Omer Levy",
                "Veselin Stoyanov",
                "Luke Zettlemoyer"
            ],
            "title": "BART: Denoising sequence-to-sequence pre-training for natural language",
            "year": 2020
        },
        {
            "authors": [
                "Chin-Yew Lin."
            ],
            "title": "ROUGE: A package for automatic evaluation of summaries",
            "venue": "Text Summarization Branches Out, pages 74\u201381, Barcelona, Spain. Association for Computational Linguistics.",
            "year": 2004
        },
        {
            "authors": [
                "Chin-Yew Lin",
                "Franz Josef Och."
            ],
            "title": "Automatic evaluation of machine translation quality using longest common subsequence and skip-bigram statistics",
            "venue": "Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL-",
            "year": 2004
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov"
            ],
            "title": "Roberta: A robustly optimized bert pretraining approach",
            "year": 2019
        },
        {
            "authors": [
                "Yixin Liu",
                "Pengfei Liu."
            ],
            "title": "Simcls: A simple framework for contrastive learning of abstractive summarization",
            "venue": "arXiv preprint arXiv:2106.01890.",
            "year": 2021
        },
        {
            "authors": [
                "Yixin Liu",
                "Pengfei Liu",
                "Dragomir Radev",
                "Graham Neubig."
            ],
            "title": "BRIO: Bringing order to abstractive summarization",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2890\u20132903,",
            "year": 2022
        },
        {
            "authors": [
                "Nicolaas Matthijs",
                "Filip Radlinski."
            ],
            "title": "Personalizing web search using long term browsing history",
            "venue": "Proceedings of the fourth ACM international conference on Web search and data mining, pages 25\u201334.",
            "year": 2011
        },
        {
            "authors": [
                "M.L. Men\u00e9ndez",
                "J.A. Pardo",
                "L. Pardo",
                "M.C. Pardo."
            ],
            "title": "The jensen-shannon divergence",
            "venue": "Journal of the Franklin Institute, 334(2):307\u2013318.",
            "year": 1997
        },
        {
            "authors": [
                "Ani Nenkova",
                "Rebecca J Passonneau."
            ],
            "title": "Evaluating content selection in summarization: The pyramid method",
            "venue": "Proceedings of the human language technology conference of the north american chapter of the association for computational linguistics: Hlt-",
            "year": 2004
        },
        {
            "authors": [
                "Shumpei Okura",
                "Yukihiro Tagami",
                "Shingo Ono",
                "Akira Tajima."
            ],
            "title": "Embedding-based news recommendation for millions of users",
            "venue": "Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD \u201917,",
            "year": 2017
        },
        {
            "authors": [
                "Mikhail Orzhenovskii."
            ],
            "title": "T5-long-extract at fns2021 shared task",
            "venue": "Proceedings of the 3rd Financial Narrative Processing Workshop, pages 67\u201369.",
            "year": 2021
        },
        {
            "authors": [
                "Kishore Papineni",
                "Salim Roukos",
                "Todd Ward",
                "WeiJing Zhu."
            ],
            "title": "Bleu: a method for automatic evaluation of machine translation",
            "venue": "Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 311\u2013318.",
            "year": 2002
        },
        {
            "authors": [
                "Maxime Peyrard."
            ],
            "title": "Studying summarization evaluation metrics in the appropriate scoring range",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5093\u2013 5100, Florence, Italy. Association for Computational",
            "year": 2019
        },
        {
            "authors": [
                "Weizhen Qi",
                "Yu Yan",
                "Yeyun Gong",
                "Dayiheng Liu",
                "Nan Duan",
                "Jiusheng Chen",
                "Ruofei Zhang",
                "Ming Zhou."
            ],
            "title": "Prophetnet: Predicting future n-gram for sequence-to-sequence pre-training",
            "venue": "arXiv preprint arXiv:2001.04063.",
            "year": 2020
        },
        {
            "authors": [
                "GS Ramesh",
                "Vamsi Manyam",
                "Vijoosh Mandula",
                "Pavan Myana",
                "Sathvika Macha",
                "Suprith Reddy."
            ],
            "title": "Abstractive text summarization using t5 architecture",
            "venue": "Proceedings of Second International Conference on Advances in Computer Engineering and Commu-",
            "year": 2022
        },
        {
            "authors": [
                "T Tawmo",
                "Mrinmoi Bohra",
                "Pankaj Dadure",
                "Partha Pakray"
            ],
            "title": "Comparative analysis of t5 model for abstractive text summarization on different datasets",
            "venue": "In Proceedings of the International Conference on Innovative Computing Communication",
            "year": 2022
        },
        {
            "authors": [
                "Johnny Wei",
                "Robin Jia."
            ],
            "title": "The statistical advantage of automatic NLG metrics at the system level",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Lan-",
            "year": 2021
        },
        {
            "authors": [
                "Chuhan Wu",
                "Fangzhao Wu",
                "Mingxiao An",
                "Jianqiang Huang",
                "Yongfeng Huang",
                "Xing Xie"
            ],
            "title": "Neural news recommendation with attentive multi-view learning",
            "year": 2019
        },
        {
            "authors": [
                "Chuhan Wu",
                "Fangzhao Wu",
                "Suyu Ge",
                "Tao Qi",
                "Yongfeng Huang",
                "Xing Xie."
            ],
            "title": "Neural news recommendation with multi-head self-attention",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th In-",
            "year": 2019
        },
        {
            "authors": [
                "Manzil Zaheer",
                "Guru Guruganesh",
                "Kumar Avinava Dubey",
                "Joshua Ainslie",
                "Chris Alberti",
                "Santiago Ontanon",
                "Philip Pham",
                "Anirudh Ravula",
                "Qifan Wang",
                "Li Yang",
                "Amr Ahmed."
            ],
            "title": "Big bird: Transformers for longer sequences",
            "venue": "Advances in Neural",
            "year": 2020
        },
        {
            "authors": [
                "Tianyi Zhang",
                "Varsha Kishore",
                "Felix Wu",
                "Kilian Q Weinberger",
                "Yoav Artzi."
            ],
            "title": "Bertscore: Evaluating text generation with bert",
            "venue": "arXiv preprint arXiv:1904.09675.",
            "year": 2019
        }
    ],
    "sections": [
        {
            "text": "Text summarization models are evaluated in terms of accuracy and quality using various measures such as ROUGE, BLEU, METEOR, BERTScore, PYRAMID, readability, and several recently proposed ones. The central objective of all accuracy measures is to evaluate the model\u2019s ability to capture saliency accurately. Since saliency is subjective w.r.t the readers\u2019 preferences, there cannot be a fit-all summary for a given document. This means that in many use cases, summarization models need to be personalized w.r.t user profiles. However, to our knowledge, there is no measure to evaluate the degree of personalization of a summarization model. In this paper, we first establish that existing accuracy measures cannot evaluate the degree of personalization of any summarization model and then propose a novel measure, called EGISES, for automatically computing the same. Using the PENS dataset released by Microsoft Research, we analyze the degree of personalization of ten different state-of-the-art summarization models (both extractive and abstractive), five of which are explicitly trained for personalized summarization, and the remaining are appropriated to exhibit personalization. We conclude by proposing a generalized accuracy measure, called P -Accuracy, for designing accuracy measures that should also take personalization into account and demonstrate the robustness and reliability of the measure through meta-evaluation."
        },
        {
            "heading": "1 Introduction",
            "text": "The growing availability of large-scale text data has led to an increasing demand for automated text summarization systems that aim to compress a lengthy document into a short paragraph, which includes salient information about the document. Evaluation of such summarization systems (and the underlying models) is performed either in terms of their accuracy or quality. To date, several accuracy\n*All authors have equal contributions.\nmeasures have been proposed such as the widely adopted ROUGE variants (e.g., ROUGE-n/L/SU4 etc.) (Lin, 2004), BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), BERTScore (Zhang et al., 2019), PYRAMID (Nenkova and Passonneau, 2004; Gao et al., 2019) and the more recently proposed ones such as SUPERT (Gao et al., 2020), WIDAR (Jain et al., 2022), and InfoLM (Colombo et al., 2022). At the same time, several meta-evaluation studies on the reliability of these measures based on the human judgment correlation have been proposed (Graham, 2015; Chatzikoumi, 2020; Bhandari et al., 2020; Deutsch et al., 2021; Fabbri et al., 2021; Peyrard, 2019; Wei and Jia, 2021).\nWithin the context of all these developments on novel metrics and meta-evaluation showing varied results on different datasets and summarization systems, the central idea remains the same for all accuracy measures, i.e., they all need to measure the ability of summarization models to capture saliency accurately. However, saliency is subjective to a reader\u2019s prior reading history and evolving preferences (aka attention drift). In other words, in many use cases, models have to consider user reading patterns and then generate personalized summaries instead of a fit-all generic summary. This calls for summarizers to be personalized. Although, relatively speaking, personalized summarization needs much research attention, there have been a few noteworthy studies in this direction (Ghodratnama et al., 2020b; Ao et al., 2021). The evaluation of these models has been mostly based on the accuracy measures (e.g., ROUGE-L). However, in this paper, we establish that we cannot measure the degree of personalization of summarization models using accuracy. We theoretically prove that personalization is an independent characteristic compared to accuracy, and it is possible that while a model performs fairly reasonably w.r.t any accuracy measure, it may have a poor degree of personalization.\nWe, thereby, propose a novel measure, called the effective Degree of Insensitivity w.r.t Subjectivity (EGISES) that conversely measures how insensitive a model is if there is a significant difference in the preferences (or expected summaries) of two readers for the same original document, where an insensitive model would not have much change in the generated summaries for the two readers. To the best of our knowledge, this notion of degree of personalization and its corresponding measure would be the first of its kind. We evaluate EGISES on ten stateof-the-art summarization models (both abstractive and extractive), including the best-performing ones proposed by (Ao et al., 2021) using the PENS test dataset (Ao et al., 2021), and show that the leaderboard so generated does not have any consistent correlation with those generated by standard accuracy measures, thereby empirically showing that accuracy is not enough.\nA key step for any new accuracy metric is to establish its robustness and reliability. The standard method to do so is human-judgment-based meta-evaluation using correlation statistics such as Pearson\u2019s Coefficient, Spearman Rank Correlation, and Kendall \u03c4 Rank Correlation. Since direct methods are practically infeasible (we will discuss that in Section 6.1) we adopt an indirect metaevaluation method that establishes a high humanjudgment vs. EGISES correlation. We then propose a generic accuracy measure based on EGISES, called P -Accuracy, for calculating the realistic accuracy of models that need to exhibit personalization, and have empirically shown that the measure is stable w.r.t its invariance to the original accuracy leaderboard."
        },
        {
            "heading": "2 Preliminaries",
            "text": ""
        },
        {
            "heading": "2.1 Degree of Personalization",
            "text": "The degree of personalization of a summarization model is the quantitative measure of the extent to which a model can generate summaries that align with the reader\u2019s subjective appreciation of saliency. While we provide a formal definition in Section 2.3, informally, if two readers\u2019 subjective agreement of saliency of a given document is low, then the model should generate personalized summaries for the readers such that they should not have high overlap, and vice-versa, without affecting the required accuracy. Although the general intuition might be that accuracy measures should be sufficient to measure this subjectivity, we demonstrate\nthat measuring personalization is completely different from measuring how close a model\u2019s generated summary is to that of the expected summary (which accuracy measures do). To elucidate this counter-intuitive aspect with an example, imagine Alice and Bob are interested in news related to the Russia-Ukraine war, but Bob is more focused on news related to the war fronts, while Alice is interested in news related to civilian distress. Now, for a given news article, a model may be able to generate a fairly accurate summary for Bob where the core narrative is about the battles, while at the same time, it is also able to get a good accuracy for Alice because it happens to have sufficient peripheral content (i.e., off-topic mentions) about, say, civilian displacement. In this situation, Alice has to filter out content that is not salient to her so as to get to her interest. This becomes even more problematic in the case when models also generate headlines. In our example, Alice will not get to see the summary headline aligned with her interest and, therefore, may completely skip the summary. Therefore, the degree of personalization provides an insight into how engaging a model can be in terms of better user experience and not just a clinical computation of the overlap of model-generated summaries to that of their gold references. In the following section, we will provide a mathematical sketch-of-proof that will clearly demonstrate that accuracy measures are not capable of measuring the degree of personalization of a model."
        },
        {
            "heading": "2.2 Accuracy is Not Enough - Proof Sketch",
            "text": "In this section, we provide a sketch-of-proof that the accuracy measures are insufficient to capture the degree of personalization of summarization models. For a given document di, two readerprofiles (i.e., expected personalized summaries or reference summaries) uij and uik, and the corresponding summaries suij and suik generated by a model M\u03b8,u, let\u2019s choose an arbitrary distance metric \u03c3 defined on the metric space M where d, u, s are defined1. In other words, \u03c3(s, u) is the abstraction over any arbitrary accuracy measure. In a similar way, the notion of degree-of-personalization can be captured by the ratio \u03c3(uij ,uik)\u03c3(suij ,suik )\n. We term this ratio as the deviation.\nTheorem 1. The deviation of a model M\u03b8,u on the metric space M can be changed without any\n1\u03c3 satisfies positivity, reflexive, maximality, symmetry, and the triangle inequality.\nchange in \u03c3(s, u).\nProof. Let d, u, s be triangulated as per Figure 1. Keeping d, u fixed, we can perform an arbitrary rotation operation (rot(\u2022, u, \u03b1); \u03b1: angle of rotation) on suij and suik s.t. rot(\u2022, u, \u03b1) is a closure operator in M . Now, \u2203(p, q) \u2208M , s.t.\nmax (p,q) {\u03c3(rot(suij , uij , \u03b1p), rot(suik , uik, \u03b1q))} >\nmin (p,q) {\u03c3(rot(suij , uij , \u03b1p), rot(suik , uik, \u03b1q))}\nIn other words, a total ordering of deviations exists. Also, for any arbitrary \u03b1, \u03c3(u, rot(s, u, \u03b1)) = \u03c3(s, u) by the property of the rotation operator. Since we have kept u fixed, the deviation, therefore, can be varied by changing \u03b1 (and thereby \u03c3(suij , suik)) from a minimum to a maximum.\nThe proof shows that two models may have the same accuracy but different degree-ofpersonalization. Our findings support the same (Section 5.3). Therefore, accuracy measures are not adequate for measuring the degree-ofpersonalization. We term this proof framework as the triangulation framework and will continue to use this in our further discussions."
        },
        {
            "heading": "2.3 Measuring Insensitivity to Subjectivity",
            "text": "One way of looking at the degree of personalization is how insensitive a summarization model is in detecting the divergence in the subjectivity of readers\u2019 interests and preferences. If the model is highly insensitive, it means it practically generates almost the same summary irrespective of the divergence in the readers\u2019 subjective expectations. Hence, in such a case, the degree of personalization will be low and may lead to low engagement. In this section, we provide a formal and generic framework for designing measures for the degree of personalization in terms of insensitivity-to-subjectivity.\nDefinition 1. Personalized Summarization Model. Given document d, and user profile u, a summarization model M\u03b8,u is said to be personalized iff M\u03b8,u 7\u2192 s\u0302u, where s\u0302u is the best estimated summary of d considering user profile u, \u03b8 being model parameters.\nDefinition 2. Weak Insensitivity-to-Subjectivity. Given two mutually indistiguishable reader profiles, ui and uj , a summarization model M\u03b8,u is (weakly) Insensitive-to-Subjectivity iff \u2200(ui, uj),\u220b fUdist(ui, u\u2217j ) \u2264 \u03c4Umax: fSsim(M\u03b8,u(d, ui),M\u03b8,u(d, uj)) > \u03c4 S min, where fUdist indicates user profile distance function, f S sim is a summary similarity function, \u03c4Umax is the maximum limit for two different user profiles to be mutually indistinguishable, and \u03c4Smin is the minimum limit for two generated summary w.r.t two different users to be mutually distinguishable. * : fUdist(ui, ui) = 0 ; f U dist(ui, uj) \u2208 [0, 1].\nDefinition 3. Strong Insensitivity-to-Subjectivity. Given two different reader profiles, ui and uj , a summarization model M\u03b8,u is (strongly) Insensitive-to-Subjectivity iff the model satisfies the condition of weak insensitivity and also \u2200(ui, uj),\u220b fUdist(ui, uj) > \u03c4Umax: fSsim(M\u03b8,u(d, ui),M\u03b8,u(d, uj)) < \u03c4 S min.\n3 EGISES: Measure for Insensitivity"
        },
        {
            "heading": "3.1 Triangulation Space Representation",
            "text": "To calculate how insensitive (or inversely, sensitive) a model is to the change in the readers\u2019 profiles when the same document has to be summarized for two different readers, we need to have a well-defined notion of deviation w.r.t an algebraic space where document set D, the model-generated summary set S, and readers\u2019 subjective expectation set U can be represented. In this paper, we make the bag-of-word assumption of d \u2208 D, s \u2208 S, and u \u2208 U, and represent them on the probability space for a specific document dj on the lines of the triangulation framework (see Section 2.2) as (\u2126dj ,Fdj ,P):\n\u2126dj = {wi \u2208 {dj |U|\u22c3 k=1 (ujk \u222a sujk)}|wi \u2208 V} (1)\nFdj = 2 \u2126dj ;P : \u2126dj 7\u2192 [0, 1]; \u2211 wi\u2208\u2126dj p(wi) = 1\n(2)\nEq. 1 indicates that any dj is a probability distribution over unique word-phrases wi from the sample space \u2126dj where wis are lexicons in a vocabulary V (i.e., dj \u2208 Fdj ). Similarly, we represent u on the same space (i.e., for a given dj , uj \u2208 Fdj ) where we consider uj to be the expected summary of the reader for dj (in the evaluation setup, it will be the gold-reference summary). suj is also defined on the same space, and the extractive version can be considered as a subset of dj (i.e., for a given (dj , ujk), sujk \u2208 Fdj ). We calculate the distributions as follows:\np(wi|dj) = count(wi \u2208 dj)\nNdj (3)\nr(wi|sujk) =\ncount(wi\u2208sujk ) Nsujk\np(wi|dj) = p(wi|sujk) p(wi|dj)\n{r(\u2022) indicates the ratio of wi}\n(4)\np\u0302(wi \u2208 sujk) = r(wi|sujk)\u2211\nwl\u2208\u2126 r(wl|sujk)\n{Estimated probability distribution of summary} (5)\nContinuing with the triangulation framework, the distance between a document d, the user profiles ui and uj , and the corresponding userspecific summaries sui and suj generated by a model M\u03b8,u is calculated using Jenson-Shannon divergence (JSD) (Men\u00e9ndez et al., 1997)2 \u2013 a symmetrized version of the Kullback\u2013Leibler divergence, which measures the similarity between two distributions. For calculating the divergence for the abstractive version of suj where we might encounter out-of-vocabulary (OOV) word-phrases (i.e., p(wi|dj) = 0 in Eq. 4, we propose a RoBERTa (Liu et al., 2019) embedding-based smoothing method in Section 3.3."
        },
        {
            "heading": "3.2 Deviation of a Summarization Model",
            "text": "Now that we have defined the triangulation space on which deviation can be computed, we propose a measure to calculate summary-level insensitivity of a model w.r.t subjectivity, called Deviation-ofSummarization Model (Dev(suij |(di, uij))) given reader-profiles (i.e., expected summaries or goldreferences) and the document to be summarized.\n2For two distributions P and Q: JSD(P\u2225Q) = 1\n2 [DKL(P\u2225M) +DKL(Q\u2225M)]\nM = (P+Q) 2 ; where DKL is the KL divergence.\nDefinition 4. Summary-level Deviation. Given a document di and a reader-profile uij , the summary-level deviation of a model M\u03b8,u (Dev(suij |(di, uij))) is defined as the proportional divergence 3 between the summary suij of di that has been generated by M\u03b8,u for uij from other reader-profile specific summaries of di generated by M\u03b8,u w.r.t a corresponding divergence of uij from other the reader-profiles (see Figure 2).\nWe formulate Dev(suij |(di, uij)) as follows:\nDev(suij |(di, uij)) = 1\n|U| |U|\u2211 k=1 min(Xijk, Yijk) max(Xijk, Yijk)\n(6)\nXijk = exp(w(uij |uik))\n|U|\u2211 l=1 exp(w(uij |uil)) \u00b7 JSD(uij ||uik)\n(7)\nYijk = exp(w(suij |suik))\n|U|\u2211 l=1 exp(w(suij |suil)) \u00b7 JSD(suij ||suik)\n(8)\nw(uij |uik) = JSD(uij ||uik) JSD(uij ||di)\n(9)\nw(suij |suik) = JSD(suij ||suik) JSD(suij ||di)\n(10)\nHere w(uij |uik) and w(suij |suik) measure the relative divergence of uij and the corresponding profile-specific summary suij from the document di. A lower value of Dev(suij |(di, uij)) indicates that while reader-profiles are different, the generated summary suij is very similar to other readerspecific summaries (or vice versa 4), and hence, is not personalized at the summary-level.\n3In this paper, we have chosen Jenson-Shannon Divergence but both the triangulation space and the distance measure can be chosen to be something else as well.\n4This ensures the strong condition of insensitivity."
        },
        {
            "heading": "3.3 Handling OOV",
            "text": "In Section 3.1, we discussed that smoothing is desirable for abstractive summaries. This is because there may be word-phrases in an abstractive summary, say suij , that are not present in the original document di (i.e., Out-of-Vocabulary (OOV) wordphrases). This can seriously affect the divergence computations. To solve this, we propose a smoothing algorithm using contextual embeddings generated by RoBERTa (Liu et al., 2019). The central idea of the smoothing algorithm (see Appendix A for details) is to predict whether one or more of the OOV word-phrases present in sujk could be alternatives or augmentation to the word-phrases in the original document dj . In the case of OOVs, we check how much wi \u2208 dj and wOOVi \u2208 sujk are related by applying cosine similarity on their contextual word embeddings. A bias is added to capture the possibility of wOOVi to be an unrelated addition in the summary. If the bias is higher than the closest match of wOOVi in dj , it indicates that w OOV i is not an alternative/augmentation usage, and therefore, no smoothing is required. Else, smoothing is applied by taking the odds of p(wOOVi |sujk) to p(wOOVi \u2208 dj)."
        },
        {
            "heading": "3.4 Effective Degree of Insensitivity",
            "text": "In Section 3.2, we defined the summary-level insensitivity of a model M\u03b8,u. To determine the degree of insensitivity at a system level, we propose EGISES (Effective deGree-of-InSEnsitivity w.r.t Subjectivity). We formulate EGISES as follows:\nEGISES = 1\u2212 1 |D||U| |D|\u2211 i=1 |U|\u2211 j=1 Dev(suij |(di, uij))\n(11) A high value of EGISES indicates that the model implies that the model is insensitive to the reader\u2019s subjective preferences. As a result, a model with a high EGISES score would not be a good selection for use cases where we need personalized summaries."
        },
        {
            "heading": "4 Personalization of Existing Models",
            "text": "One of the key objectives of this paper is to analyze the degree of personalization of different state-ofthe-art summarization models using EGISES. For this purpose, we chose ten different models, including abstractive and extractive summarizers. In this\nsection, we provide the framework for evaluating these models."
        },
        {
            "heading": "4.1 Evaluation Dataset",
            "text": "We use the test data in the PENS dataset5 released by Microsoft Research to evaluate the models. It consists of news headlines together with news articles. The headlines could be considered extreme summaries of the corresponding news articles. The test set was created in two phases. In the first phase of data collection, 103 native English speakers were asked to browse through 1,000 news headlines and mark at least 50 pieces they were interested in. The headlines were randomly selected and arranged according to their first exposure time (this ensures the dataset has the reader\u2019s reading sequence captured as well). In the second phase, participants were asked to write their preferred headlines (i.e., gold references, and in our case, these become the reader-profile set U) for 200 different articles without knowing the original news title. These news articles were excluded from the first stage and were redundantly assigned to ensure that, on average, each news article has four gold-reference summaries (this makes this dataset suitable for testing insensitivity-to-subjectivity). The participants\u2019 click behaviors and more than 20,000 goalreference personalized headlines of news articles were also collected, regarded as the expected summaries (Ao et al., 2021)."
        },
        {
            "heading": "4.2 Models Studied",
            "text": "We studied ten off-the-shelf summarization models. We include five models from the PENS framework (Ao et al., 2021): PENS-NRMS Injection-Type 1 (or T1) and Injection-Type 2 (or T2), PENSNAML T1, PENS-EBNR T1 & T2. We also include five more state-of-the-art models \u2013 BRIO (Liu et al., 2022), SimCLS (Liu and Liu, 2021), BigBird-Pegasus (Zaheer et al., 2020), ProphetNet (Qi et al., 2020), and T5-base (Orzhenovskii, 2021). Appendix B provides a brief description of each model. We select these models since they have been reported to be in the top 5 over the last four years on the CNN/Daily Mail news dataset, which is similar in content to the PENS dataset used for our evaluation.\n5https://github.com/LLluoling/PENS-Personalized-NewsHeadline-Generation"
        },
        {
            "heading": "4.3 Compared Accuracy Measures",
            "text": "To compare and correlate the leaderboard generated by EGISES with standard accuracy measures that have been reported to have sufficiently fair human-judgment correlation (and therefore, can be trusted), we select two ROUGE variants (RGL (Lin and Och, 2004) and RG-SU4 (Lin, 2004)), BLEU-1 (Papineni et al., 2002), and METEOR (Banerjee and Lavie, 2005) (for a summary, see Appendix C.1). We use three standard correlation measures \u2013 Pearson\u2019s Correlation Coefficient (r), Spearman\u2019s \u03c1 Coefficient, and Kendall\u2019s \u03c4 Coefficient (see Appendix C.2).\n5 Model Performance w.r.t EGISES"
        },
        {
            "heading": "5.1 Experiment Design",
            "text": "There are three objectives regarding the state-ofthe-art model evaluation: (i) leaderboard generation w.r.t EGISES, (ii) establishing that EGISES is a stable measure, and (iii) comparing the leaderboard with that of accuracy leaderboards. For the first two objectives, we conduct our experiments on ten random sample sets drawn from 100%, 80%, 60%, 40%, and 20% of the PENS test dataset. This is done to understand the bias and variance in the calculated EGISES scores, thereby measuring the stability of the measure. For the third objective, we use the correlation measures introduced in Section 4.3. For the PENS framework models, evaluation is direct since the models were designed to take user behavioral input in the way it is provided in the PENS dataset. However, for the other models, we need to appropriate the evaluation setup since they are not explicitly designed to be personalized. For these models, we add the gold reference summary of every reader as a title to the document that the models had to summarize, thereby creating multiple versions of the document corresponding to each reader. We then expect the model to take the injected titles as cues during summary generation, thereby inducing a personalization behavior. This injection also serves as a good baseline model.\n5.2 Model Analysis & EGISES Stability Our first observation (see Table 1) is that there is significant scope for improvement for personalized summarization considering the base models \u2013 the best PENS model is PENS-NAML (T1), which ranks sixth with an EGISES score of 0.8991 as compared to the best model (BigBird-Pegasus) that scores 0.4286. The induced personalization\nin the generic models clearly helps them to significantly outperform the PENS models that were specifically designed for personalization. This also shows their ability to utilize the cue injection. At the same time, it shows that EGISES as a measure is clearly able to capture their ability to detect the injected cue and, hence, discriminate these models from the rest. Finally, we see no change of rank as we randomize our sample selection and average over ten draws for each set. The bias and variance fluctuation across sample size are very low, thereby showing the stability of EGISES."
        },
        {
            "heading": "5.3 Accuracy is Not Enough: Leaderboard",
            "text": "Correlation Inconsistency\nTable 2 shows that leaderboard correlation between that of EGISES and the other four accuracy measures w.r.t the three chosen correlation measures is inconsistent and inconclusive6. This supports our hypothesis that real-world datasets (such as PENS) have all possibilities of triangulations. Therefore, the theoretical proof that personalization and accuracy are unrelated (see Section 2.2) is also empirically established.\n6 Reliability of EGISES"
        },
        {
            "heading": "6.1 Meta Evaluation: Experiment Design",
            "text": "As a part of standard meta-evaluation of EGISES, collecting human-judgments for personalization can be practically infeasible. This is because to assess whether a model is insensitive or not, a human evaluator needs to go through a 3-step process: (i) give a judgment on the divergence between the expected summaries of the readers, (ii) a judgment on the divergence between the corresponding (personalized) summaries generated by any model, and then (iii) judge whether that divergence proportionally varies with expected summary divergences of the readers. This significantly differs from having the human judge provide a quality score on the model-generated summaries (HJ) and, therefore, requires significantly more resources and time to create such a dataset. One way to resolve this bottleneck is to utilize the (conditioned) transitivity property of Pearson\u2019s correlation (Langford et al., 2001) to establish a sufficiently high r(HJ, EGISES). Given that r(HJ,Acc) is high (> 0.7) in most standard datasets such as CNN/DM and TAC-2008 (for RG-L) (Bhandari et al., 2020), and DUC-2001/2002 (for RG-L/SU-4) (Lin, 2004),\n6Appendix 6 contains accuracy scores and ranking.\none can therefore conclude high r(HJ, EGISES) if the condition below holds:\nr(HJ,Acc)2 + r(DevAcc, DevEGISES) 2 > 1\n=\u21d2 r(HJ, EGISES) > 0 (12)\nr(DevAcc, DevEGISES) (calculated as per Figure 3) denotes the agreement in the degree-ofpersonalization ranking if we replace the proposed divergence measure (DevEGISES) introduced in Section 3.2 with that of standard accuracy measures that are used to measure divergence, but between model-generated summaries and referencesummaries. Therefore, according to Eq. 12, r(DevAcc, DevEGISES) > 0.3 would be sufficient to establish required r(HJ, EGISES).\n6.2 Meta Evaluation: EGISES Reliability\nWe generate a degree-of-personalization leaderboard based on deviation calculation using all the selected standard accuracy measures (i.e., RGL/SU-4, BLEU, METEOR). We can observe from Table 3 that in all cases, the Pearson correlation is > 0.4, with RG-L having a very high correlation of 0.8954. This result helps us to conclude that we can claim sufficiently high r(HJ, EGISES)."
        },
        {
            "heading": "7 Personalized Accuracy",
            "text": "In this paper, we primarily focus on proving theoretically and establishing empirically that personalization is a separate attribute from accuracy (and therefore, the leaderboards of both do not correlate). Having said that, in this section, we show\nthat even when the primary objective of an evaluator is to measure the accuracy of a model, one should also take into account the ability of personalization of the model to get a realistic accuracy judgment. We call such an accuracy measure as P \u2212 Acc. However, we would like to emphasize that the key objective of P \u2212Acc is not to improve accuracy ranking but rather to achieve a more reliable accuracy value (or score) so that we can benchmark our personalized SOTA summarization models better and understand the scope of further improvement. Therefore, it is rather desirable that P \u2212 Acc should have a high correlation with corresponding Acc. Considering this objective, this section aims to establish that EGISEScan be a good plugin for P \u2212Acc."
        },
        {
            "heading": "7.1 Formulation",
            "text": "To design P -Acc w.r.t a given accuracy measure Acc and score ScoreAcc(M\u03b8,u), we can think of the model (M\u03b8,u) performance as a vector in\nR2: [ ScoreAcc(M\u03b8,u) EGISES(M\u03b8,u) ] . Now, we can generalize ScoreAcc(M\u03b8,u) as a multiplication of the score with its associated unit-scale (UnitAcc). In standard cases, UnitAcc = 1. We propose that to understand the accuracy of personalized summarization models, the model\u2019s original accuracy score should be penalized. In other words, UnitAcc should be penalized in proportion to the EGISES score as follows,\nUnitP -Acc = 1\u2212 [\u03b1 \u00b7 ( fsig(\u03b2 \u00b7 EGISES(M\u03b8,u))\nScoreAcc(M\u03b8,u) )]\n(13) Here, \u03b2 \u2208 (0, 1] is the personalization-coefficient that controls how much importance one can expect to give to the personalization dimension of the vector (i.e., the squashing of the corresponding personalization basis vector)7. \u03b1 \u2208 [0, 1] is the compensation-coefficient and regulates the final penalty that needs to be applied. fsig is a sigmoid function to prevent the original accuracy score from getting overly dampened. As a result, we can now formulate P -Acc as,\nP -Acc(M\u03b8,u) = ScoreAcc(M\u03b8,u) \u00b7 UnitP -Acc (14)\nWe take \u03b2 = 1 to understand the extreme case effect of EGISES on the original accuracy leaderboard and \u03b1 = 0.5 so as not to over-penalize a fairly accurate model for poor personalization. It is up to the evaluator to decide how much personalization is to be emphasized (controlled by \u03b2 value) during accuracy evaluation and what percentage of the overall penalty (due to lack of personalization) (controlled by \u03b1) should be injected into the original accuracy score value."
        },
        {
            "heading": "7.2 Meta Evaluation: P-Acc Leaderboard Correlation",
            "text": "To analyze how stable P -Acc is when compared to changes in the corresponding accuracy measure,\n7\u03b2 = 0 would result in UnitP -Acc = UnitAcc.\nwe perform a correlation analysis and find that the Kendall \u03c4 coefficient is highly positive (lowest of 0.8182 for RG-L and maximum of 0.9878 for METEOR) (see Table 4 for details). This means incorporating personalization into the accuracy measures will not adversely affect a model\u2019s expected accuracy. However, P -Acc can give us a more realistic understanding of how accurate a personalized summarization model is."
        },
        {
            "heading": "8 Related Work",
            "text": "Personalization in summarization models can be broadly categorized into two types \u2013 iterative human feedback based models (IHF models), and user-profile based models. In IHF-based models, the reader keeps giving feedback on the summary iteratively till the reader is satisfied with the modelgenerated summary. Ghodratnama et al. (2020b) proposed a personalized summarization approach in which Exdos (Ghodratnama et al., 2020a) is used as a base model for extractive summarization model to rank sentences of news body, and then concepts are extracted and shown to readers for their feedback. The system uses this feedback to iteratively generate a summary that includes the most important concepts till no further negative feedback. However, the evaluation metric used was ROUGE variants as accuracy metrics, and as established in this paper, cannot be used for measuring personalization. They also measured the change in ROUGE as the iterations increased, but that still is not a measure of subjective deviation. On the other hand, user-profile-based models such as those that were designed using the PENS framework (Ao et al., 2021), which we studied extensively, need significant improvement in personalization. These models, too, were evaluated using ROUGE variants w.r.t accuracy. In the area of personalized recommendation and search, we can find Jaccard Index-based measures, order edit distance-based measures (Hannak et al., 2013), and the popular nDCG (normalized Discounted Cumulative Gain)-\nbased measures (Matthijs and Radlinski, 2011). However, these are not directly applicable to text summarization and are also extrinsic, relying on human feedback (clicks, likes, etc.), and therefore, cannot be automatic intrinsic measures."
        },
        {
            "heading": "9 Conclusion",
            "text": "In this paper, we establish theoretically and empirically that accuracy measures are unsuitable to evaluate the degree-of-personalization of summarization models, specifically when saliency is fairly subjective. We introduce a new measure, called EGISES, and show that EGISES is both stable w.r.t bias and variance and reliable w.r.t humanjudgment correlation when we analyzed the degreeof-personalization of ten summarization models. As an extension, EGISES needs to be appropriated for capturing the sensitivity of models to the timevariant evolving interests of readers.\nLimitations\nAs discussed in Section 6.1, creating a dataset for direct meta-evaluation of EGISES is crucial to have a quantitative understanding of the correlation between human judgment and the proposed measure. At present, we have only been able to establish that there is a sufficiently high correlation. We are currently in the process of opening up an online survey specifically for the creation of this dataset. Also, currently, we are not in a position to conclude what kind of metric space EGISES should be defined (in this paper, we took a probability space) and what distance metric for measuring deviation would yield the best human-judgment correlation (we used Jenson-Shannon Divergence and tried to understand how that correlated if replaced by standard accuracy measures but as deviation measures). Finally, we do not have conclusive results of the effect of different contextual embeddings from other SOTA LLMs (specifically GPT-X models) on the overall leaderboard and human-judgment correlation. Thorough ablation studies are required.\nEthics Statement\nWe believe there needs to be a formal framework within which both theoretical and empirical analysis of the personalization capabilities of contemporary large language models (LLMs) can be evaluated. Personalization is not only an important aspect of \"intelligence\", but it also elucidates\nthe LLMs\u2019 capabilities of discerning what is subjectively valued and, more importantly, what is not. We would like to highly encourage fellow researchers to do a serious review of this work and build on the proposed EGISES framework. We would also like to declare that we used the PENS dataset prepared and released by Microsoft Research and did not involve any human subject for any part of the evaluation or meta-evaluation."
        },
        {
            "heading": "Acknowledgements",
            "text": "This work could not have been completed in time without all the accuracy results generated by Isha Motiyani (202218015@daiict.ac.in), Nidhi Somaiya (202218060@daiict.ac.in), and Radhika Singhal (202218062@daiict.ac.in) who are research assistants at KDM Lab, DA-IICT, India."
        },
        {
            "heading": "A Out-of-Vocabulary Smoothing",
            "text": "While generating the distribution of the summary, there can be wOOVi \u2208 sujk , that are present in the summary but not in the document itself, in that case rather assigning 0 probability assuming that the word is not part of the document, we proposed an algorithm to handle such cases. We proposed Out-of-Vocabulary Smoothing algorithm that calculates the probability of having an alternative or augmentation of the wOOVi word in the document. To explain the algorithm with an example, consider a toy document: \"The red cat is on the red tall table.\" which after preprocessing would be the bow: {red, cat, red, tall, table}. Let\u2019s say that the reader\u2019s expected summary (gold-reference) is: \"The cat is on table\" whose bow is: {cat, table}. Now, given the document and the reader\u2019s profile an abstractive model generates a summary \"The red cat is on the desk\", desk being the wOOVi , and whose bow is: {red, cat, desk}. As per table 5, we can see that desk can be considered as an alternative to table, and therefore gets a smoothened probability mass of 0.3805 by doing a sum of all softmax (\u03c3) similarity score of words in the document. Intuitively, softmax similarly score acts as the probability of that word being alternative or augmentation of the wOOVi \u2208 sujk . It\u2019s possible that the model generates a completely new word, i.e., unrelated to any of the words in the document, which means the probability of that word in the document should be 0. In our algorithm, bias gets the highest similarity score among all scores in that case because bias is calculated from the highest similarity score of words in the document, and the highest similarity word in the document itself gets a low similarity score."
        },
        {
            "heading": "B Models Details",
            "text": "We briefly introduce the SOTA summarization models that were analyzed to understand their degreeof-personalization below:\n1. PENS-NRMS Injection-Type 1: The PENS framework (Ao et al., 2021) takes user embedding as input along with the news article to generate a personalized summary for that user. To generate user embedding NRMS\n7In our implementation, we used the all-distilroberta-v1 model from Huggingface. This model maps sentences paragraphs to a 768-dimensional dense vector space and has 6 layers, 768 dimensions and 12 heads, and 82M parameters.\nAlgorithm 1 Out-of-Vocabulary Smoothing Input: dj , sujk Output: OOV {Estimated probabilities of OOV word-phrases as alternatives/augmentation to document phrases} procedure GETEMBEDDING(dj , RoBERTa)\nfor {wOOVi \u2208 sujk |wOOVi /\u2208 dj} do ewOOVi \u2190GetEmbedding(w OOV i , RoBERTa)\nmaxsim[i]\u2190 max l {fcos(ewOOVi , {ewl |wl \u2208 dj})}\nbias[i]\u2190 1\u2212 \u221a\nscoresim[i]\nif bias[i] > scoresim[i] then OOV [i] \u2190 0 {wOOVi is not an alterna-\ntive/augmentation usage (i.e. probability = 0, or no smoothing required)}\nelse sumsim[i]\u2190 \u2211 l \u03c3(fcos(ewOOVi , {ewl |wl \u2208 dj}))\n{\u03c3 : Softmax(\u2022)}\nOOV [i] \u2190 count(wOOVi )/Nsujk\nsumsim[i] {Nsujk : total num-\nber of word-phrases in sujk .} return OOV\n(Neural News Recommendation with MultiHead Self-Attention) (Wu et al., 2019b) is used. It includes a news encoder that utilizes multi-head self-attentions to understand news titles. The user encoder learns user representations based on their browsing history and uses multi-head self-attention to capture connections between news articles. Additive attention is added to learning the news and user representations more effectively by selecting important words and articles. Here, InjectionType 1 indicates that NRMS user embedding is injected into PENS by initializing the de-\ncoder\u2019s hidden state of the headline generator, which will influence the summary generation.\n2. PENS-NRMS Injection-Type 2: To generate a personalized summary, NRMS user embedding is injected into attention values (Injection-Type 2) of PENS that helps to personalize attentive values of words in the news body.\n3. PENS-NAML Injection-Type 1: NAML (Neural News Recommendation with Attentive Multi-View Learning) (Wu et al., 2019a) incorporates a news encoder that utilizes a multi-view (i.e., titles, bodies, and topic categories) attention model to generate comprehensive news representations. The user encoder is designed to learn user representations based on their interactions with browsed news. It also allows the selection of highly informative news during the user representation learning process. This user embedding is injected into the PENS model using Type-1 for personalization.\n4. PENS-EBNR Injection-Type 1: EBNR (Embedding-based News Recommendation for Millions of Users) (Okura et al., 2017) proposes a method for user representations by using an RNN model that takes browsing histories as input sequences. This user embedding is injected using Type 1 into the PENS model for personalization.\n5. PENS-EBNR Injection-Type 2: This personalized model injects EBNR user embedding into PENS using type-2.\n6. BRIO: Instead of a traditional MLE-based training approach, BRIO (Liu et al., 2022) assumes a non-deterministic training paradigm that assigns probability mass to different candidate summaries according to their quality, thereby helping it to better distinguish\nbetween high-quality and low-quality summaries.\n7. SimCLS: SimCLS (A Simple Framework for Contrastive Learning of Abstractive Summarization) (Liu and Liu, 2021) uses a twostage training procedure. In the first stage, a Seq2Seq model (BART (Lewis et al., 2020)) is trained to generate candidate summaries with MLE loss. Next, the evaluation model, initiated with RoBERTa is trained to rank the generated candidates with contrastive learning.\n8. BigBird-Pegasus: BigBird (Zaheer et al., 2020) is an extension of Transformer based models designed specifically for processing longer sequences. It utilizes sparse attention, global attention, and random attention mechanisms to approximate full attention. This enables BigBird to handle longer contexts more efficiently and, therefore, can be suitable for summarization.\n9. ProphetNet: ProphetNet (Qi et al., 2020) is a sequence-to-sequence pre-trained model that employs n-gram prediction using the n-stream self-attention mechanism. ProphetNet optimizes n-step ahead prediction by simultaneously predicting the next n tokens based on previous context tokens, thus preventing overfitting on local correlations.\n10. T5: T5 (Text-To-Text Transfer Transformer) is based on the Transformer-based EncoderDecoder architecture that operates on the principle of the unified text-to-text task for any NLP problem, including summarization. Some recent analysis on the performance of T5 on summarization task can be found in (Tawmo et al., 2022; Ramesh et al., 2022; Etemad et al., 2021)."
        },
        {
            "heading": "C Accuracy and Performance",
            "text": "C.1 Accuracy Measures Compared 1. RG-L: ROUGE-L (Recall-Oriented Under-\nstudy for Gisting Evaluation) (Lin and Och, 2004) calculates the longest common subsequence between the generated summary and the reference summary and then measures the precision, recall, and F1 score based on this comparison.\n2. RG-SU4: In addition to capturing unigram, bigram, and trigram matches, ROUGESU4 (Lin, 2004) also considers skip-bigram matches, which allow for gaps of certain words between the matched n-grams, thereby also considering non-contiguous ngram matches.\n3. BLEU: BLEU (Bilingual Evaluation Understudy) (Papineni et al., 2002) is a popular evaluation metric that measures the precision of n-gram matches between the modelgenerated summaries and the reference summaries. BLEU computes a modified precision score for various n-gram lengths and then combines them using a geometric mean.\n4. METEOR: METEOR (Metric for Evaluation of Translation with Explicit ORdering) (Banerjee and Lavie, 2005) matches unigrams based on surface forms, stemmed forms, and meanings and then calculates score using a combination of precision, recall, and the orderalignment of the matched words w.r.t reference summary.\nC.2 Correlation Measures\n1. Pearson\u2019s Correlation Coefficient (r):\nr = \u2211n i=1(xi \u2212 x)(yi \u2212 y)\u221a\u2211n\ni=1(xi \u2212 x)2 \u2211n i=1(yi \u2212 y)2\nwhere x, y are the means of the variables xi and yi ; n = the number of samples.\n2. Spearman\u2019s \u03c1 Coefficient:\n\u03c1 = 1\u2212 6 \u2211\nd2i n(n2 \u2212 1)\nwhere d = the pairwise distances of the ranks of the variables xi and yi ; n = the number of samples.\n3. Kendall\u2019s \u03c4 Coefficient:\n\u03c4 = c\u2212 d c+ d = S( n 2 ) = 2S n(n\u2212 1)\nwhere, c = the number of concordant pairs; d = the number of discordant pairs.\nC.3 Accuracy Leaderboard In table 6, each column contains the ranking of the summarization model as per that specified measure. The table is sorted by the ranking of EGISESscore. The ranking of a model is inconsistent across different measures due to that the correlation between EGISESand the other four accuracy measures is low."
        }
    ],
    "title": "Accuracy is Not Enough: Evaluating Personalization in Summarizers",
    "year": 2023
}