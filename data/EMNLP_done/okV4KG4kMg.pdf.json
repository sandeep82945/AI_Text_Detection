{
    "abstractText": "As short-form funny videos on social networks are gaining popularity, it becomes demanding for AI models to understand them for better communication with humans. Unfortunately, previous video humor datasets target specific domains such as speeches or sitcoms, and mostly focus on verbal cues. We curate a usergenerated dataset of 10K multimodal funny videos from YouTube, called ExFunTube. Using a video filtering pipeline with GPT-3.5, we verify both verbal and visual elements contributing to humor. After filtering, we annotate each video with timestamps and text explanations for funny moments. Our ExFunTube is unique over existing datasets in that our videos cover a wide range of domains with various types of humor that necessitate a multimodal understanding of the content. Also, we develop a zero-shot video-to-text prompting to maximize video humor understanding of large language models (LLMs). With three different evaluation methods using automatic scores, rationale quality experiments, and human evaluations, we show that our prompting significantly improves LLMs\u2019 ability for humor explanation.",
    "authors": [
        {
            "affiliations": [],
            "name": "Dayoon Ko"
        },
        {
            "affiliations": [],
            "name": "Sangho Lee"
        },
        {
            "affiliations": [],
            "name": "Gunhee Kim"
        }
    ],
    "id": "SP:e5a1353580928ba3acec4e1371207d21d3346785",
    "references": [
        {
            "authors": [
                "Jean-Baptiste Alayrac",
                "Jeff Donahue",
                "Pauline Luc",
                "Antoine Miech",
                "Iain Barr",
                "Yana Hasson",
                "Karel Lenc",
                "Arthur Mensch",
                "Katherine Millican",
                "Malcolm Reynolds"
            ],
            "title": "Flamingo: a Visual Language Model for Few-Shot Learning",
            "year": 2022
        },
        {
            "authors": [
                "Issa Annamoradnejad",
                "Gohar Zoghi."
            ],
            "title": "ColBERT: Using BERT Sentence Embedding for Humor Detection",
            "venue": "arXiv preprint arXiv:2004.12765, 1(3).",
            "year": 2020
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language Models Are Few-Shot Learners",
            "year": 2020
        },
        {
            "authors": [
                "Moniek Buijzen",
                "Patti M Valkenburg."
            ],
            "title": "Developing a Typology of Humor in Audiovisual Media",
            "venue": "Media psychology, 6(2):147\u2013167.",
            "year": 2004
        },
        {
            "authors": [
                "Oana-Maria Camburu",
                "Tim Rockt\u00e4schel",
                "Thomas Lukasiewicz",
                "Phil Blunsom."
            ],
            "title": "e-SNLI: Natural Language Inference with Natural Language Explanations",
            "venue": "Advances in Neural Information Processing Systems, volume 31.",
            "year": 2018
        },
        {
            "authors": [
                "Santiago Castro",
                "Devamanyu Hazarika",
                "Ver\u00f3nica P\u00e9rezRosas",
                "Roger Zimmermann",
                "Rada Mihalcea",
                "Soujanya Poria."
            ],
            "title": "Towards Multimodal Sarcasm Detection (An _Obviously_ Perfect Paper)",
            "venue": "Proceedings of the 57th Annual Meeting of the Associa-",
            "year": 2019
        },
        {
            "authors": [
                "Tuhin Chakrabarty",
                "Arkadiy Saakyan",
                "Debanjan Ghosh",
                "Smaranda Muresan."
            ],
            "title": "Flute: Figurative Language Understanding through Textual Explanations",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing.",
            "year": 2022
        },
        {
            "authors": [
                "Arjun Chandrasekaran",
                "Ashwin K Vijayakumar",
                "Stanislaw Antol",
                "Mohit Bansal",
                "Dhruv Batra",
                "C Lawrence Zitnick",
                "Devi Parikh."
            ],
            "title": "We Are Humor Beings: Understanding and Predicting Visual Humor",
            "venue": "Proceedings of the IEEE Conference on Computer",
            "year": 2016
        },
        {
            "authors": [
                "Diaz",
                "Orhan Firat",
                "Michele Catasta",
                "Jason Wei",
                "Kathy Meier-Hellstern",
                "Douglas Eck",
                "Jeff Dean",
                "Slav Petrov",
                "Noah Fiedel"
            ],
            "title": "PaLM: Scaling Language Modeling with Pathways",
            "year": 2022
        },
        {
            "authors": [
                "Poorav Desai",
                "Tanmoy Chakraborty",
                "Md Shad Akhtar."
            ],
            "title": "Nice Perfume",
            "venue": "How Long Did You Marinate in It? Multimodal Sarcasm Explanation. In Proceedings of the AAAI Conference on Artificial Intelligence.",
            "year": 2022
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
            "venue": "Proceedings of Human Language Technologies: The Annual Conference of the North",
            "year": 2019
        },
        {
            "authors": [
                "Olga Golovneva",
                "Moya Chen",
                "Spencer Poff",
                "Martin Corredor",
                "Luke Zettlemoyer",
                "Maryam Fazel-Zarandi",
                "Asli Celikyilmaz."
            ],
            "title": "ROSCOE: A Suite of Metrics for Scoring Step-by-Step Reasoning",
            "venue": "arXiv preprint arXiv:2212.07919.",
            "year": 2022
        },
        {
            "authors": [
                "Md Kamrul Hasan",
                "Wasifur Rahman",
                "AmirAli Bagher Zadeh",
                "Jianyuan Zhong",
                "Md Iftekhar Tanveer",
                "LouisPhilippe Morency",
                "Mohammed Ehsan Hoque."
            ],
            "title": "UR-FUNNY: A Multimodal Language Dataset for Understanding Humor",
            "venue": "Proceedings of the",
            "year": 2019
        },
        {
            "authors": [
                "William Hazlitt."
            ],
            "title": "Lectures on the English Comic Writers",
            "venue": "28. Wiley and Putnam.",
            "year": 1845
        },
        {
            "authors": [
                "Jack Hessel",
                "Ana Marasovi\u0107",
                "Jena D Hwang",
                "Lillian Lee",
                "Jeff Da",
                "Rowan Zellers",
                "Robert Mankoff",
                "Yejin Choi."
            ],
            "title": "Do Androids Laugh at Electric Sheep? Humor \"Understanding\" Benchmarks from The New Yorker Caption Contest",
            "venue": "arXiv preprint",
            "year": 2022
        },
        {
            "authors": [
                "James McCoy Jones."
            ],
            "title": "Cognitive Factors in the Appreciation of Humor: A Theoretical and Experimental Analysis",
            "venue": "Yale University.",
            "year": 1970
        },
        {
            "authors": [
                "Immanuel Kant."
            ],
            "title": "Kritik der Urteilskraft und Schriften zur Naturphilosophie, volume 5",
            "venue": "InselVerlag.",
            "year": 1786
        },
        {
            "authors": [
                "Shivani Kumar",
                "Atharva Kulkarni",
                "Md Shad Akhtar",
                "Tanmoy Chakraborty."
            ],
            "title": "When did you become so smart, oh wise one?! Sarcasm Explanation in Multi-modal Multi-party Dialogues",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for",
            "year": 2022
        },
        {
            "authors": [
                "Mike Lewis",
                "Yinhan Liu",
                "Naman Goyal",
                "Marjan Ghazvininejad",
                "Abdelrahman Mohamed",
                "Omer Levy",
                "Veselin Stoyanov",
                "Luke Zettlemoyer"
            ],
            "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation",
            "year": 2020
        },
        {
            "authors": [
                "Junnan Li",
                "Dongxu Li",
                "Silvio Savarese",
                "Steven Hoi."
            ],
            "title": "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models",
            "venue": "arXiv preprint arXiv:2301.12597.",
            "year": 2023
        },
        {
            "authors": [
                "KunChang Li",
                "Yinan He",
                "Yi Wang",
                "Yizhuo Li",
                "Wenhai Wang",
                "Ping Luo",
                "Yali Wang",
                "Limin Wang",
                "Yu Qiao"
            ],
            "title": "Videochat: Chat-centric video understanding",
            "year": 2023
        },
        {
            "authors": [
                "Alisa Liu",
                "Swabha Swayamdipta",
                "Noah A Smith",
                "Yejin Choi."
            ],
            "title": "WANLI: Worker and Ai Collaboration for Natural Language Inference Dataset Creation",
            "venue": "arXiv preprint arXiv:2201.05955.",
            "year": 2022
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter."
            ],
            "title": "Decoupled Weight Decay Regularization",
            "venue": "International Conference on Learning Representations.",
            "year": 2019
        },
        {
            "authors": [
                "Rod A Martin",
                "Thomas Ford."
            ],
            "title": "The Psychology of Humor: An Integrative Approach",
            "venue": "Academic press.",
            "year": 2018
        },
        {
            "authors": [
                "Tristan Miller",
                "Christian F Hempelmann",
                "Iryna Gurevych."
            ],
            "title": "Semeval-2017 Task 7: Detection and Interpretation of English Puns",
            "venue": "Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017).",
            "year": 2017
        },
        {
            "authors": [
                "WonJun Moon",
                "Sangeek Hyun",
                "SangUk Park",
                "Dongchan Park",
                "Jae-Pil Heo."
            ],
            "title": "Query-Dependent Video Representation for Moment Retrieval and Highlight Detection",
            "venue": "arXiv preprint arXiv:2303.13874.",
            "year": 2023
        },
        {
            "authors": [
                "G\u00f6ran Nerhardt."
            ],
            "title": "Humor and Inclination to Laugh: Emotional Reactions to Stimuli of Different Divergence from a Range of Expectancy",
            "venue": "Scandinavian Journal of Psychology, 11(1):185\u2013195.",
            "year": 1970
        },
        {
            "authors": [
                "OpenAI."
            ],
            "title": "ChatGPT",
            "venue": "Generated with GPT-3 technology.",
            "year": 2023
        },
        {
            "authors": [
                "Long Ouyang",
                "Jeffrey Wu",
                "Xu Jiang",
                "Diogo Almeida",
                "Carroll Wainwright",
                "Pamela Mishkin",
                "Chong Zhang",
                "Sandhini Agarwal",
                "Katarina Slama",
                "Alex Ray"
            ],
            "title": "Training language models to follow instructions with human",
            "year": 2022
        },
        {
            "authors": [
                "Badri N Patro",
                "Mayank Lunayach",
                "Deepankar Srivastava",
                "Hunar Singh",
                "Vinay P Namboodiri"
            ],
            "title": "Multimodal Humor Dataset: Predicting Laughter Tracks for Sitcoms",
            "venue": "In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision",
            "year": 2021
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark"
            ],
            "title": "Learning Transferable Visual Models from Natural Language Supervision",
            "year": 2021
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Tao Xu",
                "Greg Brockman",
                "Christine McLeavey",
                "Ilya Sutskever."
            ],
            "title": "Robust Speech Recognition via Large-scale Weak Supervision",
            "venue": "arXiv preprint arXiv:2212.04356.",
            "year": 2022
        },
        {
            "authors": [
                "Alec Radford",
                "Jeffrey Wu",
                "Rewon Child",
                "David Luan",
                "Dario Amodei",
                "Ilya Sutskever"
            ],
            "title": "Language Models are Unsupervised Multitask Learners",
            "venue": "OpenAI blog,",
            "year": 2019
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J Liu."
            ],
            "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
            "venue": "Journal of Machine Learning Research,",
            "year": 2020
        },
        {
            "authors": [
                "Nils Reimers",
                "Iryna Gurevych."
            ],
            "title": "SentenceBERT: Sentence Embeddings using Siamese BERTNetworks",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural",
            "year": 2019
        },
        {
            "authors": [
                "Florian Schmid",
                "Khaled Koutini",
                "Gerhard Widmer."
            ],
            "title": "Efficient Large-scale Audio Tagging via Transformer-to-CNN Knowledge Distillation",
            "venue": "arXiv preprint arXiv:2211.04772.",
            "year": 2022
        },
        {
            "authors": [
                "Thomas R Shultz."
            ],
            "title": "The role of incongruity and resolution in children\u2019s appreciation of cartoon humor",
            "venue": "Journal of Experimental Child Psychology, 13(3):456\u2013 477.",
            "year": 1972
        },
        {
            "authors": [
                "Jerry Suls."
            ],
            "title": "Cognitive Processes in Humor Appreciation",
            "venue": "Handbook of Humor Research: Volume 1: Basic Issues, pages 39\u201357.",
            "year": 1983
        },
        {
            "authors": [
                "Jerry M Suls."
            ],
            "title": "A Two-Stage Model for the Appreciation of Jokes and Cartoons: An InformationProcessing Analysis",
            "venue": "The psychology of humor: Theoretical perspectives and empirical issues, 1:81\u2013100.",
            "year": 1972
        },
        {
            "authors": [
                "Jiao Sun",
                "Anjali Narayan-Chen",
                "Shereen Oraby",
                "Alessandra Cervone",
                "Tagyoung Chung",
                "Jing Huang",
                "Yang Liu",
                "Nanyun Peng."
            ],
            "title": "ExPUNations: Augmenting Puns with Keywords and Explanations",
            "venue": "arXiv preprint arXiv:2210.13513.",
            "year": 2022
        },
        {
            "authors": [
                "Yoad Tewel",
                "Yoav Shalev",
                "Roy Nadler",
                "Idan Schwartz",
                "Lior Wolf."
            ],
            "title": "Zero-Shot Video Captioning with Evolving Pseudo-Tokens",
            "venue": "arXiv preprint arXiv:2207.11100.",
            "year": 2022
        },
        {
            "authors": [
                "Romal Thoppilan",
                "Daniel De Freitas",
                "Jamie Hall",
                "Noam Shazeer",
                "Apoorv Kulshreshtha",
                "Heng-Tze Cheng",
                "Alicia Jin",
                "Taylor Bos",
                "Leslie Baker",
                "Yu Du"
            ],
            "title": "LaMDA: Language Models for Dialog Applications. arXiv preprint arXiv:2201.08239",
            "year": 2022
        },
        {
            "authors": [
                "Anthony Meng Huat Tiong",
                "Junnan Li",
                "Boyang Li",
                "Silvio Savarese",
                "Steven CH Hoi."
            ],
            "title": "Plugand-Play VQA: Zero-shot VQA by Conjoining Large Pretrained Models with Zero Training",
            "venue": "arXiv preprint arXiv:2210.08773.",
            "year": 2022
        },
        {
            "authors": [
                "Yi Wang",
                "Kunchang Li",
                "Yizhuo Li",
                "Yinan He",
                "Bingkun Huang",
                "Zhiyu Zhao",
                "Hongjie Zhang",
                "Jilan Xu",
                "Yi Liu",
                "Zun Wang"
            ],
            "title": "2022a. InternVideo: General Video Foundation Models via Generative and Discriminative Learning",
            "venue": "arXiv preprint arXiv:2212.03191",
            "year": 2022
        },
        {
            "authors": [
                "Zhenhailong Wang",
                "Manling Li",
                "Ruochen Xu",
                "Luowei Zhou",
                "Jie Lei",
                "Xudong Lin",
                "Shuohang Wang",
                "Ziyi Yang",
                "Chenguang Zhu",
                "Derek Hoiem",
                "Shih-Fu Chang",
                "Mohit Bansal",
                "Heng Ji"
            ],
            "title": "2022b. Language Models with Image Descriptors are Strong Few-Shot Video",
            "year": 2022
        },
        {
            "authors": [
                "Jason Wei",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Maarten Bosma",
                "Ed Chi",
                "Quoc Le",
                "Denny Zhou."
            ],
            "title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models",
            "venue": "arXiv preprint arXiv:2201.11903.",
            "year": 2022
        },
        {
            "authors": [
                "Sarah Wiegreffe",
                "Jack Hessel",
                "Swabha Swayamdipta",
                "Mark Riedl",
                "Yejin Choi."
            ],
            "title": "Reframing HumanAI Collaboration for Generating Free-Text Explanations",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for",
            "year": 2022
        },
        {
            "authors": [
                "Sarah Wiegreffe",
                "Ana Marasovi\u0107",
                "Noah A Smith."
            ],
            "title": "Measuring Association Between Labels and Free-Text Rationales",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing.",
            "year": 2021
        },
        {
            "authors": [
                "Zhengyuan Yang",
                "Zhe Gan",
                "Jianfeng Wang",
                "Xiaowei Hu",
                "Yumao Lu",
                "Zicheng Liu",
                "Lijuan Wang."
            ],
            "title": "An Empirical Study of GPT-3 for Few-Shot KnowledgeBased VQA",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, pages 3081\u20133089.",
            "year": 2022
        },
        {
            "authors": [
                "Andy Zeng",
                "Maria Attarian",
                "Brian Ichter",
                "Krzysztof Choromanski",
                "Adrian Wong",
                "Stefan Welker",
                "Federico Tombari",
                "Aveek Purohit",
                "Michael Ryoo",
                "Vikas Sindhwani",
                "Johnny Lee",
                "Vincent Vanhoucke",
                "Pete Florence"
            ],
            "title": "Socratic Models: Composing Zero",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Today, a huge number of short-form funny videos are popularly circulated on social media platforms. Although humor often triggers instant laughter, understanding humor is not a straightforward process. Numerous studies (Hazlitt, 1845; Kant, 1786; Nerhardt, 1970; Jones, 1970; Shultz, 1972; Suls, 1972, 1983) have explored the cognitive process of humor appreciation. For instance, Hazlitt (1845) and Kant (1786) propose the incongruity theory, asserting that incongruity provokes laughter. Nerhardt (1970) further develops the idea by defining the discrepancy between expectation and content, such as punchlines or cartoons. Suls (1972) suggests the incongruity-resolution theory, positing that humor arises only when the incongruity is resolved by\nHey Luke, sit. Luke, dandelion.\n00:15\nTimestamps & Explanations of the funny moments 2s ~ 4s It\u2019s funny because the white dog is shown a dandelion and then the dog\neats the dandelion unexpectedly.\n8s ~ 10s It\u2019s funny because the black and white dog is shown a dandelion and does the same thing as the dog and eats the dandelion. Also, the man\u2019s intention was simply to give the dog a flower, not for the dog to eat it. 17s ~ 20s The dog turns and notices the dandelion, then goes over and eats the dandelion from the man\u2019s hand. It\u2019s funny because of the man\u2019s exaggerated reaction.\n00:00 00:03 00:07 00:011 00:18 00:21 Gus, look dandelion. AH-HA-HA-HA!\nAHHH! Luke, look what you did to the dandelion.\nAYE! MY DANDELION!\nLook what he did to the dandelion. I was trying to give him a flower.\nFigure 1: An example from the ExFunTube dataset. We curate funny short-form videos in various domains through a filtering pipeline that verifies both verbal and visual elements contributing to humor. Each video is annotated with timestamps and explanations for funny moments. In this example, three funny moments are identified.\nretrieving information from the joke, cartoon, or the perceiver\u2019s own knowledge. Since a sufficient understanding of the context is required to perceive and further resolve the incongruity, understanding humor can be challenging. Nevertheless, if AI models can understand humor, they could interact more effectively with humans by providing empathetic responses based on users\u2019 sense of humor. Furthermore, if the models understand short-form funny videos, they can recommend videos based on users\u2019 preferences or even generate witty titles based on video contexts.\nSeveral studies (Hasan et al., 2019; Castro et al., 2019; Patro et al., 2021; Kumar et al., 2022) have collected humorous video datasets to investigate whether models can understand if a video is funny or not. However, the datasets have been gathered from a limited domain, such as speeches or sitcoms. For example, Hasan et al. (2019) collect videos from TED, where there is a single speaker, and visual cues are restricted to gestures or facial ex-\nDataset Modality Type #DataPoints Data Config Exp Task\nExPUN T Pun 2K {Pun, Keywords, Up to 5 scores & explanations} \u2713 Pun Exp\nAVH / FOR I AbstractScene 3K / 15K {A funny image, An unfunny image, 10 funniness ratings} / {A counterpart (object replaced) image} - Image Humor Scoring & Altering\npressions. Castro et al. (2019) build the MUStARD dataset from four sitcoms, mainly from \"Friends\" and \"Big Bang Theory,\" and Patro et al. (2021) collect the MHD dataset from the sitcom \"Big Bang Theory.\" However, in sitcoms, the fixed actors follow a predetermined script on a constructed set, and the punchline plays a crucial role, so the visual elements may have less contribution to humor. Moreover, the aforementioned video datasets only have binary labels indicating whether the content is humorous or not. As binary classification may not evaluate whether a model truly understands the humor in a video, Kumar et al. (2022) collect WITS with annotated text explanations. However, this dataset is limited to sarcasm, a specific form of humor, and focuses on sarcasm explanation in dialogue. It highlights a need for a humor explanation dataset that considers visual elements more and covers general humor.\nTo this end, we curate ExFunTube, a dataset of funny, short-form videos with explanations. These videos are collected from user-generated YouTube videos, which are shared on the \"r/youtubehaiku\" subreddit. In this subreddit, users upload shortform funny videos, typically up to 30 seconds long. We develop a video filtering pipeline with GPT-3.5 (Ouyang et al., 2022), designed to exclude the videos with minimal visual impact on humor. Then, we annotate the collected videos with timestamps and text explanations of funny moments, as exemplified in Figure 1.\nRecent LLMs show great performance for ex-\nplaining humor present in text to some extent (Chowdhery et al., 2022). Inspired by the recent research on multimodal-informed prompting (Zeng et al., 2022), we convert video content into text, leveraging various zero-shot models on diverse modalities of the video. We provide LLMs with the text prompt as a linguistic summary of video content. Specifically, we consider two modalities of the video content: visual and audio. From the visual modality, we obtain dense video descriptions. From the audio modality, we acquire speech transcripts and sound labels. Finally, we chronologically integrate them into a text prompt that can maximize LLMs\u2019 ability for humor explanation.\nSince evaluating a model\u2019s ability to explain humor is challenging, we report our results in three different ways: model-based automatic scores, rationale quality metrics with the moment localization task, and human evaluation. First, we report modelbased metrics instead of those using word overlap. Second, we conduct a rationale quality experiment, which assesses the quality of explanations from the accuracy of predicting gold labels (Wiegreffe et al., 2021). Finally, we carry out human evaluations with sampled test examples. Through these three different results, our prompting approach considerably improves the humor explanation performance of three important LLMs, including one zero-shot GPT-3.5 and two finetuned T5 (Raffel et al., 2020) and BART (Lewis et al., 2020).\nTo summarize, our key contributions are:\n1. We curate ExFunTube, a dataset consisting\nof 10,136 user-generated, funny short-form videos. Each video is annotated with timestamps and explanations of funny moments. As compared in Table 1, our ExFunTube is unique over existing datasets in that our videos cover a wide range of domains with various types of humor that necessitate a multimodal understanding of the content.\n2. We design a zero-shot video-to-text prompting that converts video content into text to maximize LLMs\u2019 ability to explain video humor.\n3. With three different evaluation methods of model-based lexical scores, rationale quality scores, and human evaluations, we verify that our prompting improves LLMs\u2019 performance on humor explanation."
        },
        {
            "heading": "2 Related work",
            "text": "Humor Understanding. It has been a longstanding question whether AI models can understand humor in text, images, or videos. Early studies focused on classifying whether text (Annamoradnejad and Zoghi, 2020), images (Chandrasekaran et al., 2016), or videos (Hasan et al., 2019; Castro et al., 2019; Patro et al., 2021) are humorous or not. Some studies, such as Chandrasekaran et al. (2016), also rate the degree to which abstract scenes are perceived as humorous. However, binary classifications or ratings do not fully evaluate whether a model understands humor in detail. Recent humor studies have shifted towards having models explain humor. Sun et al. (2022) augment the SemEval 2017 Task 7 (Miller et al., 2017) with funniness ratings and explanations. Hessel et al. (2022) augment the New Yorker cartoon captions with explanations. Desai et al. (2022) propose a dataset of explanations for sarcastic captions, and Kumar et al. (2022) collect sarcastic videos from a sitcom with explanations.\nNatural Language Explanation. As tasks of interest become increasingly complex, predicting labels may not be enough to evaluate the models\u2019 true understanding. Thus, some works make models explain their decisions as an alternative. For instance, FLUTE (Chakrabarty et al., 2022) augments e-SNLI (Camburu et al., 2018) to curate figurative texts with labels for natural language inference (NLI) tasks and evaluate model-generated explanations. To evaluate model explanations, they utilize a rationale quality metric suggested by Wiegreffe\net al. (2021). As word-overlap scores may be insufficient for the evaluation of explanation, Wiegreffe et al. (2021) propose a rationale quality metric that calculates the difference of prediction scores for gold labels when rationales are provided or not: Acc (IR \u2192 O) \u2212 Acc (I \u2192 O), where I, R, and O denote input, rationale, and gold label, respectively. In addition, Sun et al. (2022) evaluate explanations by comparing the accuracy of joke classification with and without explanations: Acc (IE \u2192 O) \u2212 Acc (I \u2192 O) where E denotes explanation. We introduce a moment localization task to compute the rationale quality score of the video explanation.\nModular Vision-Language Learning. As pretrained models become larger and are trained with extensive datasets, various multimodal comprehension tasks have been tackled by composing these pretrained models. One approach is to transform visual information into discrete text words (Zeng et al., 2022; Yang et al., 2022; Wang et al., 2022b). Zeng et al. (2022) propose a modular framework that leverages LLM to construct the input text for the subsequent model based on the output of multimodal models in the previous stage. They demonstrate performance improvements in image captioning and visual question answering (VQA) tasks. Another approach connects pretrained models through continuous feature embeddings (Patro et al., 2021; Alayrac et al., 2022; Tiong et al., 2022). Li et al. (2023a) pretrain additional lightweight modules that bridge the frozen image encoder and LLMs to eliminate the modality gap between the two frozen pretrained models. Tewel et al. (2022) connect the frozen image encoder with the frozen language decoder and evolve additional pseudo tokens during inference time to perform the video captioning task. Recently, there have been efforts to integrate these two different approaches. Li et al. (2023b) introduce VideoChat, a chat-centric video understanding system consisting of two modules: VideoChat-Text and VideoChat-Embed. The former generates text descriptions from the video and the latter encodes the video as embeddings. These text descriptions and embeddings are combined with a received question to form a prompt, based on which the LLM generates a response.\nIn our work, we combine vision-language pretrained models with LLMs through text for two uses: (i) video filtering for collecting multimodal funny videos and (ii) video-to-text generation to provide LLMs with a prompt of video content."
        },
        {
            "heading": "3 The ExFunTube Dataset",
            "text": "The ExFunTube dataset comprises 10,136 videos, each annotated with timestamps of funny moments and corresponding explanations describing why each moment is humorous. The purpose of this dataset is to evaluate the models\u2019 ability to explain why a given video is funny as a measure of understanding video humor."
        },
        {
            "heading": "3.1 Video Collection and Filtering",
            "text": "We initially crawl all 220K videos shared on the subreddit \"r/youtubehaiku,\"1 where people share humorous short-form YouTube videos lasting up to 30 seconds. To ensure multimodal humor in videos, we design a four-step filtering pipeline that selects videos with both visual and verbal elements contributing to humor, as shown in Figure 2.\nVideo Caption and Transcript. In the first step (Figure 2 (a)), we obtain a transcript and a video caption to describe the verbal and visual elements of a video clip, respectively. We extract a video caption using a zero-shot video captioning model (Tewel et al., 2022). Since our dataset contains diverse videos such as animations and edited videos not present in previous video datasets, we choose a model that utilizes both CLIP (Radford et al., 2021) and GPT-2 (Radford et al., 2019), which are pretrained on huge Web-sourced data. We transcribe audio from the video clip using a speechto-text model Whisper (Radford et al., 2022). We remove videos with no speech or in languages other than English.\nMultimodal Humor. Our goal is to collect the videos that are funny from both verbal and visual elements, instead of funny from only one modality. Thus, as shown in Figure 2 (b), we first verify that the video is verbally funny; we do this by whether GPT-3.5 can find a funny utterance given a pair of the video caption and the transcript. If GPT-3.5 detects no funny utterances, we filter out the video. Next, as shown in Figure 2 (c), we again prompt GPT-3.5 to find a funny utterance with only a transcript (i.e., no video caption). If no funny utterance is detected, then we accept this video. The rationale is that the humor of this video is multimodal; the visual caption is required to identify the fun in the video. Otherwise, if GPT-3.5 can find a funny utterance in this case, we perform a further inspection as follows.\n1https://www.reddit.com/r/youtubehaiku/\nDifference in Explanations. In the last step (Figure 2 (d)), GPT-3.5 is prompted to generate explanations in one sentence for the two cases: when given both a video caption and a transcript and when given only a transcript. We then measure the similarity between the two explanations using the SentBERT score (Reimers and Gurevych, 2019), which embeds each sentence and calculates the cosine similarity of their embeddings. The reason for adopting the SentBERT score is that it can reflect the semantics of the entire sentence. If the score is higher than the threshold, we exclude the video since the video caption does not contribute to the humor explanation. Otherwise, the video is accepted.\nRationale of Our Pipeline. There has yet to be a method to gauge the extent and manner in which visual elements contribute to humor. In other benchmarks, the multimodality of datasets has been validated by analyzing the performance gap when visual information is either provided or not (Hasan et al., 2019; Patro et al., 2021; Kumar et al., 2022). Similarly, we collect videos that exhibit differences in the assigned task (i.e., identifying humorous utterances by GPT-3.5) with or without visual information. In the field of NLI, previous works (Liu et al., 2022; Wiegreffe et al., 2022; Chakrabarty et al., 2022) leverage the power of LLMs such as GPT-3 (Brown et al., 2020) in creating figurative language examples or explanations for them. Likewise, we use GPT-3.5 to check the difference between generated explanations. To the best of our knowledge, this is the first approach that employs explanations for curating a dataset. Thanks to the pipeline, we can collect 21K high-quality multimodal humorous videos.\nPostprocessing. To ensure that our dataset does not contain any disrespectful or harmful content towards individuals or animals, we conduct a thorough manual review of all 21K videos. We filter out the videos using the five criteria based on the safety objectives outlined by Thoppilan et al. (2022): (i) Discrimination: videos displaying discrimination based on race, gender, sexual orientation, age, or disability. (ii) Animal cruelty: videos depicting acts of animal cruelty, such as a cat falling. (iii) Dangerous goods, services, activities, or self-harm: videos featuring dangerous content like drugs, violence, or bullying. (iv) Obscenities or profanities: videos containing explicit language or sexual actions. (v) Shocking content: videos that include shocking content, such as gunshots or explosions. After the filtering, about 50% of the videos are removed, and we are left with 10,136 videos."
        },
        {
            "heading": "3.2 Data annotations",
            "text": "We crowdsource via Amazon Mechanical Turk (AMT) to annotate start and end timestamps of funny moments and provide text explanations for each moment. To participate in our dataset annotation, workers must meet the following criteria: a HIT approval rate of 99% or higher, a total of more than 10,000 approved HITs, and be located in one of the countries of AU, CA, GB, NZ, or US. We conduct a qualification test for these workers, selecting those who can effectively explain humor.\nOut of 219 workers, only 60 pass the qualification test, indicating our thorough selection.\nFor each video, we instruct one worker first to identify up to three funny moments within a video (up to 30 seconds long) and then annotate why each moment is funny. To make workers explain both humor elements and justifications, we provide a recommended format: \u201c[What is funny]. It is funny because [Why funny]\u201d. We only accept responses including both descriptions (What) and justifications (Why) and reject those that lack either. Given the difficulty of the task, we offer detailed feedback to the workers, helping them improve their performance with a high annotation standard.\nAs a result, we obtain 11,166 explanations, each paired with start and end timestamps of the moment. They consist of 44.3 words on average. Out of 10,136 videos, 9,222 contain one funny moment, 798 contain two, and 116 contain three. Most videos contain a single funny moment since videos are typically shorter than 30 seconds. However, given the varied content in each video, there can be any number of funny moments."
        },
        {
            "heading": "4 Approach",
            "text": "We explore an approach to explain video humor. Our idea is first to convert the video content into fine-grained text and then take advantage of recent powerful LLMs in a zero-shot manner. We design to extract as much information from videos into text as possible. Figure 3 shows a zero-shot videoto-text prompting that converts the video content into a text input to LLMs."
        },
        {
            "heading": "4.1 Fine-grained Text Prompts",
            "text": "Videos contain visual and audio modalities. The audio is further split into speech and sound. For each component, we initially generate text descriptions using state-of-the-art zero-shot models. Then, we arrange text descriptions in chronological order and use them as a prompt.\nVisual. In order to populate high-quality text descriptions about the visual, we first (i) segment the video, (ii) generate multiple frame captions, and (iii) retrieve the best-matching caption with the video-to-text model.\nFirst, we employ PySceneDetect2 to divide a video into a set of \ud835\udc41 segments based on visual changes. During the filtering pipeline (\u00a73.1), the speech-to-text model Whisper generates timestamps\n2https://github.com/Breakthrough/PySceneDetect\nfor each utterance. We also use them to split the segments further, resulting in more fine-grained and semantically meaningful video segments.\nNext, we extract frames at a rate of 5fps from each of the \ud835\udc41 video segments. We generate\ud835\udc3e (= 20) captions per frame using the image captioning model BLIP-2 (Li et al., 2023a) with a \"Who is doing what?\" prompt, which can enhance action detection. We then have a frame caption corpus (# Frames \u00d7 \ud835\udc3e captions) per segment. Subsequently, we use the video-to-text model InternVideo (Wang et al., 2022a) to retrieve the caption that best matches each video segment from the respective frame corpus. Finally, we obtain one caption per segment, resulting in a total of \ud835\udc41 captions, which are fine-grained descriptions of the visual component.\nSpeech. We transcribe audio with Whisper (Radford et al., 2022) as done in our video filtering pipeline. We then predict the number of speakers and assign speakers to each utterance utilizing ChatGPT (OpenAI, 2023). This speaker separation helps a deep understanding of dialogue.\nSound. We extract sound tags to provide more context. We use an audio tagging model (Schmid et al., 2022) to classify the entire audio stream. We select the top 3 predicted tags that have a higher confidence value than the threshold (0.3). We concatenate the tags and insert them at the beginning of the prompt. This can provide the model with an overall atmosphere of the video."
        },
        {
            "heading": "4.2 Prompt Configuration and LLMs",
            "text": "After extracting text from visual, speech, and sound, we configure the prompt like an example of Figure 3. The prompt starts with a predefined text \u201cPlease generate \u223c\u201d to instruct LLMs to explain as if they are watching the video. We then include sound tags enclosed in parentheses and arrange the extracted text of speech and visuals for each video segment chronologically. To distinguish between video segments, we begin each segment with \"Scene: \". Finally, we ask LLMs to generate an explanation of up to three sentences.\nLLMs. Although any LLMs can be adopted, we use three different ones: finetuned T5 (Raffel et al., 2020) and BART (Lewis et al., 2020), and zero-shot GPT-3.5 text-davinci-003."
        },
        {
            "heading": "5 Experiments",
            "text": "We experiment with different models to see how well they explain the humor in the ExFunTube videos. We evaluate the models in three different ways of model-based automatic scores, rationale quality experiments, and human evaluation."
        },
        {
            "heading": "5.1 Experimental Setup",
            "text": "Baselines. We evaluate four types of explanation models. (i) Text-only LLMs generate explanations when only a transcript is provided (i.e., no use of visual). We use T5 Large and BART Large with finetuning and GPT-3.5 as a zero-shot model. (ii)\nMAF (Kumar et al., 2022) is a multimodal end-toend model designed for video sarcasm explanation. It generates explanations by receiving features of the three components (visual, speech, and audio). We train the model on our dataset. (iii) VideoChatText (Li et al., 2023b) is a multimodal prompting framework that textualizes video information into text, including video/clip captions, objects contained in the video and a transcript. Given the prompt, GPT-3.5 generates explanations in a zeroshot manner. (iv) LLMs with our prompting generate explanations given a prompt created by our zero-shot video-to-text prompting, using the same LLMs as (i) of T5, BART, and GPT-3.5. Note that T5 and BART models are finetuned to generate explanations given generated prompts, while GPT-3.5 generates in a zero-shot manner.\nExplanation Generation. For all finetuned models on our dataset, we employ K-fold crossvalidation as follows. We divide the entire dataset of 10,136 videos into five equal-sized subsets. In each iteration, we train the model on three subsets, use one subset for validation, and test on the remaining subset. We repeat this process five times, rotating the test subset in each iteration. Finally, we obtain predicted explanations for the entire set.\nEvaluation. To compare the predicted explanation with the gold explanation for each video, we concatenate explanations for each moment into a single, unified explanation. For more details on experiments, please refer to the Appendix."
        },
        {
            "heading": "5.2 Results of Model-based Automatic Scores",
            "text": "Since the metrics based on word overlaps may fail to reflect faithfulness and plausibility as highlighted by Sun et al. (2022), we evaluate explanations using two model-based scores: SentBERT Score and ROSCOE (Golovneva et al., 2022). ROSCOE is a suite of metrics designed to evaluate the reasoning process within a chain-of-thought prompting (Wei et al., 2022). It is suitable for our explanation tasks since our goal is to uncover the reason for laughter (i.e., why is the video humorous?) Among the various scores provided by ROSCOE, we use the reasoning alignment (RA) score, which computes the contextual similarity between the hypothesis and reasoning.\nTable 2 reports the model-based automatic scores of different methods. We show not only the mean metric values but also the proportions of the test set with scores higher than various thresholds; @\ud835\udc3e represents the proportion of data points with scores equal to or greater than \ud835\udc3e .\nThe results show that, except for SentBERT @0.7, GPT-3.5 with our prompting reaches the best performance. Especially, the SentBERT and ROSCOE scores with our prompting are higher than those with text-only baselines in all cases. In addition, our method outperforms the multimodal end-toend baseline MAF and the multimodal zero-shot prompting baseline VideoChat-Text. The comparison of @\ud835\udc3e metrics shows even more significant differences, particularly for SentBERT @0.5 and\nROSCOE @0.8, where the performance margin ranges from 0.1 (BART) to 0.27 (GPT-3.5) compared to the text-only baselines. This means that using transcripts alone may not be sufficient to understand the humor in our videos."
        },
        {
            "heading": "5.3 Results of Rationale Quality Scores",
            "text": "We conduct a rationale quality experiment following Wiegreffe et al. (2021) and Sun et al. (2022). Since our dataset consists of videos, unlike theirs, we adapt the experimentation scheme by evaluating the rationale quality through a moment localization task, which aims at predicting funny moments defined by their start and end timestamps in a video given the text explanation.\nWe use QD-DETR (Moon et al., 2023) as a localizer and divide the entire dataset into 8:1:1 splits for training (8,110), validation (1,013), and testing (1,013). During the training, the localizer is learned to predict the gold timestamp given a gold explanation. At inference, we compute the rationale quality as the prediction difference of the localizer between when given a model-generated explanation and when given a gold explanation.\nLet \ud835\udc40 be a model-generated explanation, \ud835\udc3a be a gold explanation, and \ud835\udf0f be a threshold. For each test data point, we calculate the maximum IoU from the top 5 candidates given \ud835\udc40 or \ud835\udc3a, respectively denoted as IoUM or IoUG. We use the top 5 since there can be at most three funny moments in a single video and the localization predictions can overlap with each other. We compute the difference when IoU\ud835\udc40 > \ud835\udf0f. The final score \ud835\udc46 is the sum of differences for all test data:\n\ud835\udc46 = \ud835\udc5b\u2211\ufe01 \ud835\udc56=1 (IoU\ud835\udc3a\ud835\udc56 \u2212 IoU\ud835\udc40\ud835\udc56 ) \u00b7 \ud835\udfd9(IoU\ud835\udc40\ud835\udc56 > \ud835\udf0f),\nwhere \ud835\udc5b is the number of test data points, and \ud835\udfd9(\u00b7) is the indicator function.\nTable 2 shows the results when the IoU threshold \ud835\udf0f is set to 0.3 and 0.5. A lower score is better as it is closer to the gold standard. In each LLM, the performance improves when our prompting is included compared to corresponding text-only ones. In particular, our approach improves GPT-3.5 the most, with the threshold at 0.3 resulting in a score gap of 13.3, and at 0.5, a score gap of 13.2. Again, the performance of all LLMs with our prompting is better than MAF and VideoChat-Text."
        },
        {
            "heading": "5.4 Results of Human Evaluations",
            "text": "For human evaluation, we employ 10 AMT workers using the same criteria as in the dataset annotation but excluding the ones who already participated in the annotation. We randomly select 100 videos and evaluate explanations generated by all models except baselines using T5 and VideoChat-Text, which show worse automatic scores than other text-only or multimodal baselines. We obtain human evaluations with two methods: rating and comparison.\nFor the rating, workers are asked to rate each explanation according to No (0), Weak No (0.25), Neutral (0.5), Weak Yes (0.75), and Yes (1) and check any shortcomings. We ask five workers for each explanation, exclude the highest and lowest scores, and take the average. For the comparison, workers compare GPT-3.5 with our prompting to (1) Text-only GPT-3.5, (2) MAF, and (3) Gold explanations and choose the better explanation. We ask five workers for each pair of comparisons.\nThe rating results are presented on the far right of Table 2. The scores of BART and GPT-3.5 increase by about 0.1 when our prompting is included. The comparison results are presented in Figure 4. The number of votes for text-only GPT3.5 is significantly lower than that of GPT-3.5 with our prompting, indicating that visual information is valuable, and our prompting helps convey visual information effectively. In both rating and comparison, MAF shows lower performance than the text-only models despite being a multimodal model. This suggests that providing visual information as text to LLMs could be more effective than training the multimodal model end-to-end. Moreover, GPT-3.5 with our prompting, which shows the best results, still scores lower than Gold, indicating that understanding and explaining the humor in our dataset still remains unsolved."
        },
        {
            "heading": "5.5 Analyzing LLMs with Humor Taxonomy",
            "text": "We classify our dataset into a total of 20 humor categories referring to Martin and Ford (2018) and Bu\u0133zen and Valkenburg (2004), and observe the performance of baselines by the humor taxonomy. We provide ChatGPT with 20 categories along with a brief description and one example (i.e., oneshot learning) and instruct ChatGPT to classify the video based on the given explanation. Thanks to ChatGPT\u2019s powerful in-context learning capability, we effectively classify 10,136 videos based on their corresponding explanations.\nFigure 5 shows the models\u2019 performance by humor categories. Excluding the Jokes and Selfdeprecating classes, the performance increases with our prompting in all categories. In particular, the performance significantly increases in Clownish humor, Visual gags, and Slapsticks, which heavily reflect visual elements. This indicates that our zeroshot video-to-text prompting effectively conveys visual elements to the LLM."
        },
        {
            "heading": "5.6 Ablation Study",
            "text": "We compare the importance of each modality in humor explanation. Table 3 presents the results of SentBERT and ROSCOE scores when visual, speech, and sound components are not included in the prompt one by one. In GPT-3.5 with our prompting, the performance without the visual component drops as much as when the speech is removed, indicating that the visual component plays an important role in our dataset. Moreover, the performance decreases when either of the components is removed,\nwhich suggests that all three components are crucial for understanding and explaining humorous videos in our dataset. Additional ablation studies are presented in the Appendix."
        },
        {
            "heading": "6 Conclusion",
            "text": "We introduced ExFunTube, a dataset consisting of 10,136 user-generated videos annotated with timestamps and explanations of funny moments. Our dataset aims to assess how well AI models understand and explain video humor. We devised a zero-shot video-to-text prompting to make existing LLMs better explain the video content. With three different evaluation methods, we demonstrated that the humor in our dataset is multimodal, and our prompting maximized LLMs\u2019 ability to generate explanations.\nHowever, as the performance still falls short of human levels, our dataset remains sufficiently challenging and calls for future research. Furthermore, we can consider the training of the model using user feedback for personalized humor understanding.\nLimitations\nSince the copyright remains with the original owners of our dataset videos, we will only distribute URLs instead of videos.\nOur method relies on the performance of existing state-of-the-art models, as we used them in a zeroshot composition. Also, our approach composes models through text, so it could also be explorable to use an adaptor-based method for prompt tuning during inference.\nWe measured the videos by dividing them into three modalities, but we did not consider the temporal information of sound. As timing can play a role in humor, analyzing the sound in accordance with the timeline could be helpful.\nLastly, humor is subjective, which means that our collected explanations may be subjective, too.\nEthics Statement\nWe put much effort into ensuring that our dataset contains no inappropriate videos that may raise ethical issues. Based on the safety rules of Thoppilan et al. (2022), authors manually viewed each video entirely from start to end and filtered the video if there was any content that corresponded to the filtering criteria presented in the dataset postprocessing. Although we carefully reviewed all the videos, there could still be some videos that are not comfortable for someone. If such inappropriate videos are found, we will remove them in the future. Also, since we only recruit workers in AU, CA, GB, NZ, and US as mentioned in the Appendix, the cultural and geographic biases may influence humor explanations."
        },
        {
            "heading": "Acknowledgments",
            "text": "We sincerely thank Jaekyeom Kim, Jaewoo Ahn, Soochan Lee, Wonkwang Lee, Yeda Song, and Jaehyeon Son for their valuable comments. We would also like to thank AMT workers for their commitment to building the ExFunTube dataset. This work was supported by the SNU-Global Excellence Research Center establishment project, Basic Science Research Program through the National Research Foundation of Korea (NRF) funded by the Ministry of Education (RS-2023-00274280), Institute of Information & communications Technology Planning & Evaluation (IITP) grant funded by the Korea government (MSIT) (No. 2022-0-00156, Fundamental research on continual meta-learning\nfor quality enhancement of casual videos and their 3D metaverse transformation), and Institute of Information & communications Technology Planning & Evaluation (IITP) grant funded by the Korea government (MSIT) (No. 2021-0-01343, Artificial Intelligence Graduate School Program (Seoul National University)). Gunhee Kim is the corresponding author."
        },
        {
            "heading": "A Experimental Details",
            "text": "Video Filtering Pipeline. In the video filtering pipeline, we utilize a zero-shot video captioning model from Tewel et al. (2022), a speech-to-text model Whisper (Radford et al., 2022), and GPT-3.5 (Ouyang et al., 2022). For the video captioning model, we optimize pseudo tokens for 25 iterations at inference time to guide the pretrained GPT-2 (Radford et al., 2019) with the CLIP ViT-L/14 image encoder (Radford et al., 2021). We use AdamW optimizer (Loshchilov and Hutter, 2019) with a learning rate of 0.008 and an L2 weight decay of 0.003. For Whisper, we use the large-v2 model. For GPT-3.5, we use text-davinci-003 and set the temperature to 0 for funny utterance detection and 0.3 for explanation generation.\nVideo-to-Text Prompting. During the prompting stage, we use BLIP-2 (Li et al., 2023a), InternVideo (Wang et al., 2022a), Whisper, ChatGPT (OpenAI, 2023), and an audio-tagging model from Schmid et al. (2022). We use the coco-pretrained BLIP-2 model with nucleus sampling. For InternVideo, we use CLIP ViT-L/14 as the image encoder. We set the temperature to 0.3 for ChatGPT, and we use the mn40_as model for audio tagging.\nExplanation Generation. To generate explanations with baseline models, we finetune T5 (Raffel et al., 2020) and BART (Lewis et al., 2020) with a batch size of 4 for 5 epochs. We use the AdamW optimizer with a learning rate of 2e-5 and an L2 weight decay of 0.01. Additionally, we train MAF (Kumar et al., 2022), a multimodal end-to-end model with an adaptor added to BART, with a batch size of 4 for 20 epochs. We use the AdamW optimizer with an L2 weight decay of 1e-4, and the learning rate is set to 5e-8 for BART parameters and 5e-7 for the remaining parameters. We use BART Large for all models.\nRationale Quality Experiments. For the rationale quality experiments with moment localization, we train QD-DETR (Moon et al., 2023) with a batch size of 128 for 200 epochs. We use the AdamW optimizer with a learning rate of 1e-5 and an L2 weight decay of 1e-4. We optimize with the moment retrieval loss consisting of the L1 loss, the cross-entropy loss and the generalized IoU loss. We use the loss balancing terms of 10, 1 and 2 for each of them, respectively. We do not use the saliency loss. We use the bert-base-uncased model (Devlin et al., 2019) as the text encoder with the max query length set to 400 and CLIP ViT-L/14 as the video\nencoder. We sample video frames at a rate of 1 fps. Except for the aforementioned hyperparameters, we use the default values for all models."
        },
        {
            "heading": "B Additional Ablation Study",
            "text": "We conduct ablation experiments on BART and T5 with our prompting as well, and the results are as shown in Table 4. Similar to the results of GPT-3.5 with our prompting, using all modalities achieves the best performance, and there is a certain degree of performance decrease when the visual component is removed."
        },
        {
            "heading": "C Crowdsourcing Details",
            "text": "We use three different user interfaces of Amazon Mechanical Turk (AMT) for (i) annotating the timestamps and explanations of funny moments, and the human evaluation of (ii) rating and (iii) comparison, as shown in Figures 6-8, respectively. We guarantee AMT workers receive fair wages of approximately $18 per hour. Additionally, we allocate about $2 as compensation for each data point and grant additional wages to workers contributing extended time and effort."
        },
        {
            "heading": "D Case Study",
            "text": "Figures 9-12 show representative videos accepted or excluded by our video filtering pipeline. Figures 13-18 provide several examples to demonstrate humor explanations that our baseline models actually generate. We color-code relevant (blue) and irrelevant (red) information contained in generated explanations. LLMs with our prompting, especially GPT-3.5, correctly explain the funny moments in\nFigures 13-16 while text-only LLMs and MAF fail to. All the models fail to explain humorous moments in Figures 17-18.\nT5 A man is sitting in a chair and he says \"look what you did to the dandelion\" and then he screams. It is funny because it is unexpected and because he is screaming at the same time.\nBART The video is funny because it presents a close up capture of a dandelion that has been knocked over by a man that is attempting to give it a flower. Furthermore, this scene is comical because the man featured seems to be comically flailing his arms and legs in such a way that makes it appear as if he is trying to dance to the beat of the song playing in the background. The video is also funny when the man says, \"I was just trying to give him a flower,\" because it is clear that he is not in the mood to dance, but instead is merely trying to get the attention of his companion.\nGPT3.5 The video is funny because of the contrast between the speaker's expectation of Luke's reaction to the dandelion and his actual reaction. The speaker's shock and dismay at Luke's destruction of the dandelion is comically exaggerated, and the speaker's surprise is punctuated by the loud \"Aye!\" when the dandelion is destroyed. The speaker's over-the-top reaction to the destruction of the dandelion is what makes the video funny.\nHey Luke, sit. Luke, dandelion. AHHH! Luke, look what you did to the dandelion. Gus, look dandelion. AH-HAHA-HA! Look what he did to the dandelion. I was trying to give him a flower. AYE! MY DANDELION!"
        }
    ],
    "title": "Can Language Models Laugh at YouTube Short-form Videos?",
    "year": 2023
}