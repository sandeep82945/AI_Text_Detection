{
    "abstractText": "Transformer-based models have achieved great success on sentence pair modeling tasks, such as answer selection and natural language inference (NLI). These models generally perform cross-attention over input pairs, leading to prohibitive computational costs. Recent studies propose dual-encoder and late interaction architectures for faster computation. However, the balance between the expressive of crossattention and computation speedup still needs better coordinated. To this end, this paper introduces a novel paradigm MixEncoder for efficient sentence pair modeling. MixEncoder involves a lightweight cross-attention mechanism. It avoids the repeated encoding of the same query for different candidates, thus allowing modeling the query-candidate interaction in parallel. Extensive experiments conducted on four tasks demonstrate that our MixEncoder can speed up sentence pairing by over 113x while achieving comparable performance as the more expensive cross-attention models. The source code is available at https: //github.com/ysngki/MixEncoder.",
    "authors": [
        {
            "affiliations": [],
            "name": "Yuanhang Yang"
        },
        {
            "affiliations": [],
            "name": "Shiyi Qi"
        },
        {
            "affiliations": [],
            "name": "Chuanyi Liu"
        },
        {
            "affiliations": [],
            "name": "Qifan Wang"
        },
        {
            "affiliations": [],
            "name": "Cuiyun Gao"
        },
        {
            "affiliations": [],
            "name": "Zenglin Xu"
        }
    ],
    "id": "SP:4d5212b7cd95a9ad69223a33a753a435bfbd7878",
    "references": [
        {
            "authors": [
                "Qingqing Cao",
                "Harsh Trivedi",
                "Aruna Balasubramanian",
                "Niranjan Balasubramanian."
            ],
            "title": "DeFormer: Decomposing pre-trained transformers for faster question answering",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational",
            "year": 2020
        },
        {
            "authors": [
                "Jiecao Chen",
                "Liu Yang",
                "Karthik Raman",
                "Michael Bendersky",
                "Jung-Jung Yeh",
                "Yun Zhou",
                "Marc Najork",
                "Danyang Cai",
                "Ehsan Emadzadeh."
            ],
            "title": "DiPair: Fast and accurate distillation for trillion-scale text matching and pair modeling",
            "venue": "Findings of the Asso-",
            "year": 2020
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
            "year": 2019
        },
        {
            "authors": [
                "Luyu Gao",
                "Zhuyun Dai",
                "Jamie Callan."
            ],
            "title": "Modularized transfomer-based ranking framework",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 4180\u20134190, Online. Association for Computa-",
            "year": 2020
        },
        {
            "authors": [
                "Zhe Hu",
                "Zuohui Fu",
                "Yu Yin",
                "Gerard de Melo."
            ],
            "title": "Context-aware interaction network for question matching",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3846\u20133853, Online and Punta Cana,",
            "year": 2021
        },
        {
            "authors": [
                "Samuel Humeau",
                "Kurt Shuster",
                "Marie-Anne Lachaux",
                "Jason Weston."
            ],
            "title": "Poly-encoders: Architectures and pre-training strategies for fast and accurate multi-sentence scoring",
            "venue": "International Conference on Learning Representations.",
            "year": 2020
        },
        {
            "authors": [
                "Vladimir Karpukhin",
                "Barlas Oguz",
                "Sewon Min",
                "Patrick Lewis",
                "Ledell Wu",
                "Sergey Edunov",
                "Danqi Chen",
                "Wen-tau Yih."
            ],
            "title": "Dense passage retrieval for opendomain question answering",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural",
            "year": 2020
        },
        {
            "authors": [
                "Omar Khattab",
                "Matei Zaharia."
            ],
            "title": "Colbert: Efficient and effective passage search via contextualized late interaction over bert",
            "venue": "Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval,",
            "year": 2020
        },
        {
            "authors": [
                "Dan Li",
                "Yang Yang",
                "Hongyin Tang",
                "Jiahao Liu",
                "Qifan Wang",
                "Jingang Wang",
                "Tong Xu",
                "Wei Wu",
                "Enhong Chen."
            ],
            "title": "VIRT: Improving representation-based text matching via virtual interaction",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in",
            "year": 2022
        },
        {
            "authors": [
                "Ryan Lowe",
                "Nissan Pow",
                "Iulian Serban",
                "Joelle Pineau."
            ],
            "title": "The Ubuntu dialogue corpus: A large dataset for research in unstructured multi-turn dialogue systems",
            "venue": "Proceedings of the 16th Annual Meeting of the Special Interest Group on Discourse",
            "year": 2015
        },
        {
            "authors": [
                "Yi Luan",
                "Jacob Eisenstein",
                "Kristina Toutanova",
                "Michael Collins."
            ],
            "title": "Sparse, dense, and attentional representations for text retrieval",
            "venue": "Transactions of the Association for Computational Linguistics, 9:329\u2013 345.",
            "year": 2021
        },
        {
            "authors": [
                "Sean MacAvaney",
                "Franco Maria Nardini",
                "Raffaele Perego",
                "Nicola Tonellotto",
                "Nazli Goharian",
                "Ophir Frieder."
            ],
            "title": "Efficient document re-ranking for transformers by precomputing term representations",
            "venue": "Proceedings of the 43rd International ACM SI-",
            "year": 2020
        },
        {
            "authors": [
                "Ping Nie",
                "Yuyu Zhang",
                "Xiubo Geng",
                "Arun Ramamurthy",
                "Le Song",
                "Daxin Jiang."
            ],
            "title": "Dc-bert: Decoupling question and document for efficient contextual encoding",
            "venue": "Proceedings of the 43rd International ACM SIGIR Conference on Research and Develop-",
            "year": 2020
        },
        {
            "authors": [
                "Rodrigo Nogueira",
                "Kyunghyun Cho"
            ],
            "title": "Passage re-ranking with bert",
            "year": 2020
        },
        {
            "authors": [
                "Yingqi Qu",
                "Yuchen Ding",
                "Jing Liu",
                "Kai Liu",
                "Ruiyang Ren",
                "Wayne Xin Zhao",
                "Daxiang Dong",
                "Hua Wu",
                "Haifeng Wang."
            ],
            "title": "RocketQA: An optimized training approach to dense passage retrieval for opendomain question answering",
            "venue": "Proceedings of the",
            "year": 2021
        },
        {
            "authors": [
                "Nils Reimers",
                "Iryna Gurevych."
            ],
            "title": "SentenceBERT: Sentence embeddings using Siamese BERTnetworks",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natu-",
            "year": 2019
        },
        {
            "authors": [
                "Kaiser",
                "Illia Polosukhin"
            ],
            "title": "Attention is all",
            "year": 2017
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Sentence pair modeling, such as natural language inference, question answering, and information retrieval, is an essential task in natural language processing (Nogueira and Cho, 2020; Qu et al., 2021; Zhao et al., 2021). These tasks can be depicted as a procedure of scoring the candidates given a query. Recently, Transformer-based models (Vaswani et al., 2017; Devlin et al., 2019) have shown promising performance on sentence pair modeling tasks due to the expressiveness of the pre-trained cross-encoder. As shown in Figure 1(a), the cross-encoder takes a pair of query and candidate as input and calculates the interaction between them at each layer by the input-wide self-attention mechanism. Despite the effective text representation power, the cross-encoder leads to exhaustive\ncomputation costs, especially when the number of candidates is very large ( e.g., the interaction will be calculated N times if there are N candidates). This computation cost, therefore, restricts the use of these cross-encoder models in many real-world applications (Chen et al., 2020).\nTo tackle this issue, we propose a lightweight cross-attention mechanism, called MixEncoder, that speeds up the inference while maintaining the expressiveness of cross-attention. Specifically, the proposed MixEncoder accelerates the crossattention by performing attention only from candidates to the query, involving few tokens and only at a few layers. This lightweight cross-attention avoids repetitive query encoding, supporting the processing of multiple candidates in parallel and thus reducing computation costs. Additionally, MixEncoder allows to pre-compute the candidates into several dense context embeddings and to store them offline to accelerate the inference further.\nWe evaluate MixEncoder for sentence pair modeling on four benchmark datasets related to tasks of natural language inference, dialogue, and information retrieval. The results demonstrate that MixEncoder better balances the effectiveness and efficiency. For example, MixEncoder achieves a substantial speedup of more than 113x over the crossencoder and provides competitive performance."
        },
        {
            "heading": "2 Background",
            "text": "Extensive studies, including dual-encoder (Reimers and Gurevych, 2019) and late interaction models (MacAvaney et al., 2020; Gao et al., 2020; Chen et al., 2020; Khattab and Zaharia, 2020), have been proposed to accelerate the transformer inference on sentence pair modeling tasks.\nAs shown in Figure 1, dual-encoders process the query and candidates separately, allowing precomputing the candidates to accelerate online inference, resulting in fast inference speed. However, this speedup is built upon sacrificing the expres-\nsiveness of cross-attention (Luan et al., 2021; Hu et al., 2021; Zhang et al., 2021). Alternatively, late-interaction models adjust dual-encoders by appending an interaction component, such as a stack of Transformer layers (Cao et al., 2020; Nie et al., 2020), for modeling the interaction between the query and the cached candidates. These approaches still suffer from the high costs of the interaction component (Chen et al., 2020)."
        },
        {
            "heading": "3 Method",
            "text": "In this section, we introduce the details of the proposed MixEncoder, which simplifies crossattention by enabling pre-computation, reducing the times of query encoding, and reducing the number of involved tokens and layers."
        },
        {
            "heading": "3.1 Candidate Pre-computation",
            "text": "Given a candidate that is a sequence of tokens Ti = [t1, \u00b7 \u00b7 \u00b7 , tl], we experiment with two strategies to encode these tokens into k context embeddings in advance, where k \u226a l: (1) prepending k special tokens {Si}ki=1 to Ti before feeding Ti into the Transformer encoder (Vaswani et al., 2017; Devlin et al., 2019), and using the output at these special tokens as context embeddings (S-strategy); (2) maintaining k context codes (Humeau et al.,\n2020) to extract global features from output of the encoder by attention mechanism (C-strategy). The default configuration is S-strategy as it provides slightly better performance. The pre-computed context embeddings E \u2208 RN\u00d7k\u00d7d are cached for online inference, where N is the number of candidates."
        },
        {
            "heading": "3.1.1 Query Encoding",
            "text": "Since the cross-encoder performs N times of query encoding, which contributes to the inefficiency, a straightforward way to accelerate the inference is to reduce the encoding times of the query. Here we encode the query without taking its candidates into account, thus requiring the encoding only once.\nTo preserve the expressiveness of the crossattention, the simplified cross-attention is performed at several interaction layers. As shown in Figure 2, the context embeddings Ej\u22121 of candidates are allowed to attend over the intermediate token embeddings of the query, thus obtaining context-aware representations Ej and Hj for the query and its candidates.\nConcretely, at each interaction layer, the key and value matrices of the query are utilized by candidates in two ways. (1) Producing contextualized representations for the candidates:\nEj = Attn(Q\u2032, [K \u2032;K], [V \u2032;V ]), (1)\nwhere Q\u2032, K \u2032, V \u2032 are derived from the Ej\u22121 with a linear transformation. Ej is supposed to contain semantics from both the query and candidates. (2) Compressing the semantics of the query into a vector for each candidate:\nHj = Gate(Attn(Q\u2217,K, V ), Hj\u22121), (2)\nwhere Q\u2217 \u2208 RN\u00d7d is derived from Ej\u22121 by a pooling operation, H \u2208 RN\u00d7d stands for the candidateaware query states and H0 is initialized as a zero matrix."
        },
        {
            "heading": "3.2 Prediction",
            "text": "Let H and E denote the query states and the candidate context embeddings generated by the last interaction layer, respectively. For the i-th candidate, its representation is the mean of the i-th row of E, denoted as ei. The representation of the query with respect to this candidate is the i-th row of H , denoted as hi. The cosine similarity between ei and hi is used as the semantic similarity. Additionally, we can pass ei and hi to a classifier for classification tasks."
        },
        {
            "heading": "3.3 Time Complexity",
            "text": "Table 1 presents the time complexity of the DualBERT, Cross-BERT, and our proposed MixEncoder. We can observe that MixEncoder supports offline pre-computation to reduce the online time complexity. During the online inference, the query encoding cost term (dq2 + d2q) of MixEncoder does not increase with the number of candidates since it conducts query encoding only once. Moreover, MixEncoder\u2019s query-candidate term N(k + q + d)dk can be reduced by setting k as a small value, which can further speed up the inference."
        },
        {
            "heading": "4 Experiments",
            "text": "Datasets. We evaluate MixEncoder on three paired-input tasks over four datasets, including MNLI (Williams et al., 2018) for natural language inference, MS MARCO passage reranking (Bajaj et al., 2018) for information retrieval, and DSTC7 (Yoshino et al., 2019), Ubuntu V2 (Lowe et al., 2015) for utterance selection for dialogue.\nBaselines. (1) Cross-BERT is the original BERT (Devlin et al., 2019). (2) Dual-BERT (Sentence-BERT) is proposed by Reimers et al. (Reimers and Gurevych, 2019). (3) Deformer (Cao et al., 2020) is a decomposed Transformer that utilizes lower layers to encode sentences separately and then uses upper layers to encode text pairs together. (4) Poly-Encoder (Humeau et al., 2020) encodes the query and its candidates separately and performs a light-weight late interaction. (5) ColBERT (Khattab and Zaharia, 2020) is a late interaction model which adopts the MaxSim operation to obtain relevance scores. This operation prohibits the utilization of ColBERT on classification tasks. (6) VIRT (Li et al., 2022) performs the cross-attention at the last layer and utilizes knowledge distillation during training.\nTraining Details. While training models on MNLI, we use the labels provided in the dataset. While\ntraining models on the other three datasets, we use in-batch negatives (Karpukhin et al., 2020; Qu et al., 2021). Detailed settings are provided in A.1."
        },
        {
            "heading": "5 Results",
            "text": "Table 2 shows the experimental results of baselines and three variants of MixEncoder. We measure the inference time of all the baseline models for queries with 1000 candidates and report the speedup."
        },
        {
            "heading": "5.1 Performance Comparison",
            "text": "Variants of MixEncoder. To study the effect of the number of interaction layers and that of the number of context embeddings per candidate, we consider three variants, denoted as MixEncoder-a, -b, and -c, respectively. Specifically, MixEncoder-a and -b set k as 1. The former performs interaction at the last layer and the latter performs interaction at the last three layers. MixEncoder-c is similar to MixEncoder-b but with k = 2.\nDual-BERT and Cross-BERT. The performance of the dual-BERT and cross-BERT are reported in the first two rows of Table 2. We can observe that MixEncoder consistently outperforms the DualBERT. The variants with more interaction layers or more context embeddings generally yield more improvement. For example, on DSTC7, MixEncodera and MixEncoder-b achieve an improvement by 0.7% (absolute) and 1.6% over the Dual-BERT, respectively. Moreover, MixEncoder-a provides comparable performance to the Cross-BERT on both Ubuntu and DSTC7. MixEncoder-b can even outperform the Cross-BERT on DSTC7 (+0.6), since MixEncoder can benefit from a large batch size (Humeau et al., 2020). However, the effectiveness of the MixEncoder on MS MARCO is slight.\nWe can find that the difference in the inference time between the Dual-BERT and MixEncoder is minimal, while Cross-BERT is 2 orders of magnitude slower than these models.\nTable 2: Performance of Dual-BERT, Cross-BERT and three variants of MixEncoder on four datasets.\nModel MNLI Ubuntu DSTC7 MS MARCO Speedup Space\nAccuracy R1@10 MRR R1@100 MRR R1@1000 MRR(dev) Times GB Cross-BERT 83.70.1 83.10.7 89.40.5 66.80.6 75.20.4 23.3 36.0 1.0x - Dual-BERT 75.20.1 81.60.2 88.50.1 65.81.0 74.20.7 20.3 32.2 132x 0.3 PolyEncoder-64 76.80.1 82.30.5 88.90.4 66.41.5 74.80.9 20.3 32.3 130x 0.3 PolyEncoder-360 77.30.2 81.80.2 88.60.1 65.70.6 74.00.3 20.5 32.4 127x 0.3\nColBERT \u00d7 82.90.3 89.30.2 67.20.7 74.80.4 22.8 35.4 35.2x 8.6 VIRT 78.30.3 83.10.2 89.40.2 66.50.7 74.90.2 21.5 32.3 33.3x 52.7\nDeformer 82.00.1 83.20.4 89.50.2 66.31.0 75.30.6 23.0 35.7 1.9x 52.7 MixEncoder-a 77.50.4 83.10.1 89.40.1 66.90.5 74.90.2 20.4 32.0 113x 0.3 MixEncoder-b 77.80.2 83.20.0 89.50.1 68.20.8 75.80.5 20.7 32.5 89.6x 0.3 MixEncoder-c 78.40.4 83.30.1 89.50.0 66.70.4 74.80.3 20.0 31.9 84.8x 0.6\nLate Interaction Models. From Table 2, we have the following observations. First, among all the late interaction models, Deformer that adopts a stack of Transformer layers as the late interaction component consistently shows the best performance on all the datasets. This demonstrates the effectiveness of cross-attention. In exchange, Deformer shows limited speedup (1.9x). Compared to the ColBERT and Poly-Encoder, MixEncoder outperforms them on the datasets except for MS MARCO. Although ColBERT consumes more computation than MixEncoder, it shows worse performance than MixEncoder on DSTC7 and Ubuntu. This demonstrates that the lightweight cross-attention can achieve a better trade-off between efficiency and effectiveness. However, on MS MARCO, MixEncoder and poly-encoder lag behind the ColBERT by a large margin. We conjecture that MixEncoder falls short of handling term-level matching. We will elaborate on it in section A.4."
        },
        {
            "heading": "5.2 Ablation Study",
            "text": "Representations. We conduct ablation studies to quantify the impact of two key components (E and H) utilized in MixEncoder. The results are shown in Table 3. All components contribute to a gain in performance. It demonstrates that the simplified cross-attention can produce effective representations for both the query and its candidates.\nInteraction layers. Figure 3(a) shows the results when MixEncoder performs interaction at Transformer layers upper than x. Increasing interaction layers cannot continuously improve the ranking quality. On both Ubuntu and DSTC7, the performance of MixEncoder achieves a peak with the last three layers utilized for interaction. More experiments are reported in section A.6.\nContext embeddings. We study the effect of the number of candidate embeddings and the precomputation strategies with the last layer to perform the simplified cross-attention. From Figure 3(b), it is observed that the S-strategy generally outperforms the C-strategy, and a larger k can lead to a better performance for the S-strategy.\nTable 4 shows the average time per example for different models. It is shown that MixEncoder consumes more time as k increases. Nevertheless, the difference in timing between Dual-BERT and MixEncoder is rather minimal, whereas Cross-BERT\nis significantly slower by two orders of magnitude."
        },
        {
            "heading": "6 Conclusion",
            "text": "In this paper, we propose MixEncoder to balance the trade-off between performance and efficiency. It involves a lightweight cross-attention mechanism that allows us to encode the query once and process all the candidates in parallel. Experimental results demonstrate that MixEncoder can speed up sentence pairing by over 113x while achieving comparable performance as the more expensive cross-attention models."
        },
        {
            "heading": "7 Acknowledgements",
            "text": "This work was partially supported by the National Key Research and Development Program of China (No. 2018AAA0100204), a key program of fundamental research from Shenzhen Science and Technology Innovation Commission (No. JCYJ20200109113403826), the Major Key Project of PCL (No. 2022ZD0115301), and an Open Research Project of Zhejiang Lab (NO.2022RC0AB04).\nLimitations\nAlthough MixEncoder has been demonstrated to be effective in cross-attention computation, we recognize that MixEncoder does not perform well on MS MARCO. It indicates that our MixEncoder falls short of detecting token overlapping since it loses token-level features by pre-encode candidates into several context embeddings. Moreover, MixEncoder is not evaluated on a large-scale evaluation dataset, such as an end-to-end retrieval task, which requires the model to retrieve top-k candidates from millions of candidates (Qu et al., 2021; Khattab and Zaharia, 2020)."
        },
        {
            "heading": "A More Details",
            "text": "A.1 Training Details\nFor Cross-BERT and Deformer, which require exhaustive computation, we set the batch size as 16 due to the limitation of computation resources. For other models, we set the batch size as 64. All the models use BERT (based, uncased) with 12 layers and fine-tune it for up to 50 epochs with a learning rate of 1e-5 and linear scheduling. All experiments are conducted on a server with 4 Nvidia Tesla A100 GPUs, which have 40 GB graphic memory.\nA.2 Datasets\nThe statistics of datasets are detailed in Table 5. We use accuracy to evaluate the classification performance on MNLI. For other datasets, MRR and recall are used as evaluation metrics.\nA.3 In-batch Negative Training\nWe change the batch size and show the results in\nFigure 4. It can be observed that increasing batch size contributes to better performance. Moreover, we have the observation that models may fail to diverge with small batch sizes. Due to the limitation of computation resources, we set the batch size as 64 for our training.\nA.4 Error Analysis\nIn this section, we take a sample from MS MARCO to analyze our errors. We observe that MixEncoder falls short of detecting token overlapping. Given the query \"foods and supplements to lower blood sugar\", MixEncoder fails to pay attention to the keyword \u201csupplements,\" which appears in both the query and the positive candidate. We conjecture that this drawback is due to the pre-computation that represents each candidate into k context embeddings. It loses the token-level features of the candidates. On the contrary, ColBERT caches all the token embeddings of the candidates and estimates relevance scores based on token-level similarity.\n16 32 48 64 Batch Size\n88.5\n89.0\n89.5\nUb un\ntu\n1 2 3 10 b) Model with a varing number of context vectors and different strategies.\n76.8\n77.0\n77.2\n77.5\n77.8\nM NL\nI\n68.0\n70.0\n72.0\n74.0\nDS TC\n7\nUbuntu DSTC7\n74.0\n75.0\n76.0\nDS TC\n7\nMNLI C-strategy MNLI S-strategy DSTC7 C-strategy DSTC7 S-strategyFigure 4: Parameter analysis on the batch size.\nA.5 Inference Speed We conduct speed experiments to measure the online inference speed for all the baselines. Concretely, we sample 100 samples from MS MARCO. Each of the samples has roughly 1000 candidates. We measure the time for computations on the GPU and exclude time for text reprocessing and moving data to the GPU.\nA.6 Interaction Layers From Table 7, it is observed that performing crossattention at higher layers generally yields better performance. Since we use the output of the final interaction layers as the sentence embeddings, choosing low layers enables the early exit mechanism."
        }
    ],
    "title": "Once is Enough: A Lightweight Cross-Attention for Fast Sentence Pair Modeling",
    "year": 2023
}