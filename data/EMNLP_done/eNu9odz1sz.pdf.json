{
    "abstractText": "When deploying machine learning systems to the wild, it is highly desirable for them to effectively leverage prior knowledge to the unfamiliar domain while also firing alarms to anomalous inputs. In order to address these requirements, Universal Domain Adaptation (UniDA) has emerged as a novel research area in computer vision, focusing on achieving both adaptation ability and robustness (i.e., the ability to detect out-of-distribution samples). While UniDA has led significant progress in computer vision, its application on language input still needs to be explored despite its feasibility. In this paper, we propose a comprehensive benchmark for natural language that offers thorough viewpoints of the model\u2019s generalizability and robustness. Our benchmark encompasses multiple datasets with varying difficulty levels and characteristics, including temporal shifts and diverse domains. On top of our testbed, we validate existing UniDA methods from computer vision and state-of-the-art domain adaptation techniques from NLP literature, yielding valuable findings: We observe that UniDA methods originally designed for image input can be effectively transferred to the natural language domain while also underscoring the effect of adaptation difficulty in determining the model\u2019s performance.",
    "authors": [
        {
            "affiliations": [],
            "name": "Hyuhng Joon Kim"
        },
        {
            "affiliations": [],
            "name": "Hyunsoo Cho"
        },
        {
            "affiliations": [],
            "name": "Sang-Woo Lee"
        },
        {
            "affiliations": [],
            "name": "Junyeob Kim"
        },
        {
            "affiliations": [],
            "name": "Choonghyun Park"
        },
        {
            "affiliations": [],
            "name": "Sang-goo Lee"
        },
        {
            "affiliations": [],
            "name": "Kang Min Yoo"
        },
        {
            "affiliations": [],
            "name": "Taeuk Kim"
        }
    ],
    "id": "SP:191b500d017c6dde8d6e1e008db3d380171d8c9b",
    "references": [
        {
            "authors": [
                "Charu C. Aggarwal."
            ],
            "title": "An Introduction to Outlier Analysis, pages 1\u201334",
            "venue": "Springer International Publishing, Cham.",
            "year": 2017
        },
        {
            "authors": [
                "Eyal Ben-David",
                "Carmel Rabinovitz",
                "Roi Reichart."
            ],
            "title": "Perl: Pivot-based domain adaptation for pre-trained deep contextualized embedding models",
            "venue": "Transactions of the Association for Computational Linguistics, 8:504\u2013521.",
            "year": 2020
        },
        {
            "authors": [
                "John Blitzer",
                "Ryan McDonald",
                "Fernando Pereira."
            ],
            "title": "Domain adaptation with structural correspondence learning",
            "venue": "Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, pages 120\u2013128, Sydney, Australia. Asso-",
            "year": 2006
        },
        {
            "authors": [
                "Wanxing Chang",
                "Ye Shi",
                "Hoang Duong Tuan",
                "Jingya Wang."
            ],
            "title": "Unified optimal transport framework for universal domain adaptation",
            "venue": "Advances in Neural Information Processing Systems.",
            "year": 2022
        },
        {
            "authors": [
                "Derek Chen",
                "Zhou Yu."
            ],
            "title": "GOLD: Improving out-of-scope detection in dialogues using data augmentation",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 429\u2013442, Online and Punta Cana, Dominican",
            "year": 2021
        },
        {
            "authors": [
                "Liang Chen",
                "Qianjin Du",
                "Yihang Lou",
                "Jianzhong He",
                "Tao Bai",
                "Minghua Deng."
            ],
            "title": "Mutual nearest neighbor contrast and hybrid prototype selftraining for universal domain adaptation",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2022
        },
        {
            "authors": [
                "Liang Chen",
                "Yihang Lou",
                "Jianzhong He",
                "Tao Bai",
                "Minghua Deng."
            ],
            "title": "Evidential neighborhood contrastive learning for universal domain adaptation",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, 36(6):6258\u20136267.",
            "year": 2022
        },
        {
            "authors": [
                "Liang Chen",
                "Yihang Lou",
                "Jianzhong He",
                "Tao Bai",
                "Minghua Deng."
            ],
            "title": "Geometric anchor correspondence mining with uncertainty modeling for universal domain adaptation",
            "venue": "2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2022
        },
        {
            "authors": [
                "Hyunsoo Cho",
                "Choonghyun Park",
                "Jaewook Kang",
                "Kang Min Yoo",
                "Taeuk Kim",
                "Sang-goo Lee."
            ],
            "title": "Enhancing out-of-distribution detection in natural language understanding via implicit layer ensemble",
            "venue": "Findings of the Association for Computa-",
            "year": 2022
        },
        {
            "authors": [
                "Hyunsoo Cho",
                "Jinseok Seol",
                "Sang-goo Lee."
            ],
            "title": "Masked contrastive learning for anomaly detection",
            "venue": "Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21, pages 1434\u20131441. International Joint Conferences on",
            "year": 2021
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
            "year": 2019
        },
        {
            "authors": [
                "Akshay Raj Dhamija",
                "Manuel G\u00fcnther",
                "Terrance Boult."
            ],
            "title": "Reducing network agnostophobia",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2018
        },
        {
            "authors": [
                "Chunning Du",
                "Haifeng Sun",
                "Jingyu Wang",
                "Qi Qi",
                "Jianxin Liao"
            ],
            "title": "Adversarial and domain-aware bert for cross-domain sentiment analysis",
            "venue": "In Proceedings of the 58th annual meeting of the Association for Computational Linguistics,",
            "year": 2020
        },
        {
            "authors": [
                "Bo Fu",
                "Zhangjie Cao",
                "Mingsheng Long",
                "Jianmin Wang."
            ],
            "title": "Learning to detect open classes for universal domain adaptation",
            "venue": "European Conference on Computer Vision.",
            "year": 2020
        },
        {
            "authors": [
                "Yaroslav Ganin",
                "Victor Lempitsky."
            ],
            "title": "Unsupervised domain adaptation by backpropagation",
            "venue": "Proceedings of the 32nd International Conference on International Conference on Machine Learning Volume 37, ICML\u201915, page 1180\u20131189. JMLR.org.",
            "year": 2015
        },
        {
            "authors": [
                "Yaroslav Ganin",
                "E. Ustinova",
                "Hana Ajakan",
                "Pascal Germain",
                "H. Larochelle",
                "Fran\u00e7ois Laviolette",
                "Mario Marchand",
                "Victor S. Lempitsky."
            ],
            "title": "Domainadversarial training of neural networks",
            "venue": "Journal of machine learning research.",
            "year": 2016
        },
        {
            "authors": [
                "Yaroslav Ganin",
                "Evgeniya Ustinova",
                "Hana Ajakan",
                "Pascal Germain",
                "Hugo Larochelle",
                "Fran\u00e7ois Laviolette",
                "Mario Marchand",
                "Victor Lempitsky."
            ],
            "title": "Domain-adversarial training of neural networks",
            "venue": "The journal of machine learning research, 17(1):2096\u2013",
            "year": 2016
        },
        {
            "authors": [
                "Dan Hendrycks",
                "Kevin Gimpel."
            ],
            "title": "A baseline for detecting misclassified and out-of-distribution examples in neural networks",
            "venue": "International Conference on Learning Representations.",
            "year": 2017
        },
        {
            "authors": [
                "Dan Hendrycks",
                "Xiaoyuan Liu",
                "Eric Wallace",
                "Adam Dziedzic",
                "Rishabh Krishnan",
                "Dawn Song"
            ],
            "title": "Pretrained transformers improve out-of-distribution",
            "year": 2020
        },
        {
            "authors": [
                "Gupta"
            ],
            "title": "To trust or not to trust a classifier",
            "year": 2018
        },
        {
            "authors": [
                "Smith"
            ],
            "title": "The multilingual Amazon reviews",
            "year": 2020
        },
        {
            "authors": [
                "R. Venkatesh Babu"
            ],
            "title": "Subsidiary prototype",
            "year": 2022
        },
        {
            "authors": [
                "Weitang Liu",
                "Xiaoyun Wang",
                "John Owens",
                "Yixuan Li."
            ],
            "title": "Energy-based out-of-distribution detection",
            "venue": "Advances in Neural Information Processing Systems, volume 33, pages 21464\u201321475. Curran Associates, Inc.",
            "year": 2020
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter."
            ],
            "title": "Decoupled weight decay regularization",
            "venue": "International Conference on Learning Representations.",
            "year": 2019
        },
        {
            "authors": [
                "Andrei Manolache",
                "Florin Brad",
                "Elena Burceanu."
            ],
            "title": "DATE: Detecting anomalies in text via selfsupervision of transformers",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Hu-",
            "year": 2021
        },
        {
            "authors": [
                "John Miller",
                "Karl Krauth",
                "Benjamin Recht",
                "Ludwig Schmidt."
            ],
            "title": "The effect of natural distribution shift on question answering models",
            "venue": "Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning",
            "year": 2020
        },
        {
            "authors": [
                "Rishabh Misra."
            ],
            "title": "News category dataset",
            "venue": "arXiv preprint arXiv:2209.11429.",
            "year": 2022
        },
        {
            "authors": [
                "Seung Jun Moon",
                "Sangwoo Mo",
                "Kimin Lee",
                "Jaeho Lee",
                "Jinwoo Shin."
            ],
            "title": "Masker: Masked keyword regularization for reliable text classification",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, 35(15):13578\u201313586.",
            "year": 2021
        },
        {
            "authors": [
                "Sinno Jialin Pan",
                "Xiaochuan Ni",
                "Jian-Tao Sun",
                "Qiang Yang",
                "Zheng Chen."
            ],
            "title": "Cross-domain sentiment classification via spectral feature alignment",
            "venue": "Proceedings of the 19th International Conference on World Wide Web, WWW \u201910, page 751\u2013760, New",
            "year": 2010
        },
        {
            "authors": [
                "Adam Paszke",
                "Sam Gross",
                "Francisco Massa",
                "Adam Lerer",
                "James Bradbury",
                "Gregory Chanan",
                "Trevor Killeen",
                "Zeming Lin",
                "Natalia Gimelshein",
                "Luca Antiga"
            ],
            "title": "Pytorch: An imperative style, high-performance deep learning library",
            "year": 2019
        },
        {
            "authors": [
                "Marco Tulio Ribeiro",
                "Tongshuang Wu",
                "Carlos Guestrin",
                "Sameer Singh."
            ],
            "title": "Beyond accuracy: Behavioral testing of NLP models with CheckList",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4902\u2013",
            "year": 2020
        },
        {
            "authors": [
                "Kuniaki Saito",
                "Kate Saenko."
            ],
            "title": "Ovanet: One-vsall network for universal domain adaptation",
            "venue": "arXiv preprint arXiv:2104.03344.",
            "year": 2021
        },
        {
            "authors": [
                "Jihoon Tack",
                "Sangwoo Mo",
                "Jongheon Jeong",
                "Jinwoo Shin"
            ],
            "title": "Csi: Novelty detection via contrastive",
            "year": 2020
        },
        {
            "authors": [
                "Jindong Wang",
                "Xixu Hu",
                "Wenxin Hou",
                "Hao Chen",
                "Runkai Zheng",
                "Yidong Wang",
                "Linyi Yang",
                "Haojun Huang",
                "Wei Ye",
                "Xiubo Geng",
                "Binxin Jiao",
                "Yue Zhang",
                "Xing Xie"
            ],
            "title": "On the robustness of chatgpt: An adversarial and out-of-distribution perspective",
            "year": 2023
        },
        {
            "authors": [
                "Teven Le Scao",
                "Sylvain Gugger",
                "Mariama Drame",
                "Quentin Lhoest",
                "Alexander Rush."
            ],
            "title": "Transformers: State-of-the-art natural language processing",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System",
            "year": 2020
        },
        {
            "authors": [
                "Hui Wu",
                "Xiaodong Shi."
            ],
            "title": "Adversarial soft prompt tuning for cross-domain sentiment analysis",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2438\u20132447, Dublin, Ireland.",
            "year": 2022
        },
        {
            "authors": [
                "Kaichao You",
                "Mingsheng Long",
                "Zhangjie Cao",
                "Jianmin Wang",
                "Michael I. Jordan."
            ],
            "title": "Universal domain adaptation",
            "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR).",
            "year": 2019
        },
        {
            "authors": [
                "Zhiyuan Zeng",
                "Keqing He",
                "Yuanmeng Yan",
                "Zijun Liu",
                "Yanan Wu",
                "Hong Xu",
                "Huixing Jiang",
                "Weiran Xu."
            ],
            "title": "Modeling discriminative representations for out-of-domain detection with supervised contrastive learning",
            "venue": "Proceedings of the 59th Annual Meet-",
            "year": 2021
        },
        {
            "authors": [
                "Li-Ming Zhan",
                "Haowen Liang",
                "Bo Liu",
                "Lu Fan",
                "XiaoMing Wu",
                "Albert Y.S. Lam."
            ],
            "title": "Out-of-scope intent detection with self-supervision and discriminative training",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Lin-",
            "year": 2021
        },
        {
            "authors": [
                "Yinhe Zheng",
                "Guanyi Chen",
                "Minlie Huang."
            ],
            "title": "Out-of-domain detection for natural language understanding in dialog systems",
            "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing, 28:1198\u20131209.",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Deep learning models demonstrate satisfactory performance when tested on data from the training distribution. However, real-world inputs encounter novel data ceaselessly that deviate from the trained distribution, commonly known as distributional shift. When confronted with such inputs, machine learning models frequently struggle to differentiate them from regular input. Consequently, they face challenges in adapting their previously acquired knowledge to the new data distribution, resulting\n*Co-corresponding authors.\nin a significant drop in performance (Ribeiro et al., 2020; Miller et al., 2020; Hendrycks et al., 2020). The aforementioned phenomenon represents a longstanding challenge within the machine learning community, wherein even recent cutting-edge language models (OpenAI, 2023; Touvron et al., 2023; Chowdhery et al., 2022; Brown et al., 2020) do not serve as an exception to this predicament (Wang et al., 2023).\nIn response to these challenges, existing literature proposes two distinct approaches. The first approach, known as Domain Adaptation (DA) (Blitzer et al., 2006; Ganin et al., 2016b; Karouzos et al., 2021; Wu and Shi, 2022), endeavors to establish alignment between a new set of data from an unknown distribution and the model\u2019s prior knowl-\nedge distribution. The objective is to enhance the model\u2019s generalization capability and reduce performance drop springing from the distributional shift. In parallel, a distinct line of work, referred to as out-of-distribution (OOD) detection (Aggarwal, 2017; Hendrycks and Gimpel, 2017; Hendrycks et al., 2019; Cho et al., 2021), focuses on discerning inputs originating from dissimilar distributions. They opt to circumvent potential risks or disruptions arising from shifted inputs, thereby enriching system robustness and resilience.\nWhile both approaches offer unique advantages addressing specific distributional shifts, integrating their merits could substantially enhance robustness. In pursuit of this objective, a novel field called Universal Domain Adaptation (UniDA) (You et al., 2019) has emerged, aiming to harness the synergies of both OOD detection and DA when confronted with distributional shifts. UniDA leverages the best of the two worlds and offers comprehensive perspectives that integrate the merits of these two research areas. The essence of UniDA lies in measuring the uncertainty of the data from the shifted distribution precisely. Then, we can enhance the model\u2019s transferability by distinguishing portions of low-uncertainty inputs that can be adequately handled with the current model\u2019s knowledge. Simultaneously, we enrich the robustness of the model to OOD inputs by discerning the remaining samples that cannot be processed normally. However, distinguishing between these inputs and properly processing them becomes increasingly challenging without explicit supervision.\nDespite the versatility of UniDA, this topic has yet to be explored in the Natural Language Processing (NLP) literature. As a cornerstone in enhancing reliability against distributional shifts in NLP, we introduce a testbed for evaluating the model\u2019s robustness in a holistic view. First, we construct various adaptation scenarios in NLP, utilizing an array of thoughtfully selected datasets. To discern the degree to which our proposed datasets incorporate the various degree of challenges in UniDA, we define two novel metrics: Performance Drop Rate (PDR) and Distinction Difficulty Score (DDS). Using these metrics, We verify that our testbed captures a broad spectrum of distributional shifts. Finally, based on the suggested setting, we systematically compare several UniDA methods inherently designed for the task, against heuristic combinations of previous approaches for the parts\nof the problem, i.e., OOD detection and DA. Our empirical results show that UniDA methods are fully transferable in the NLP domain and can robustly respond to various degrees of shift. Moreover, we find out that the adaptation difficulty notably affects the performance of the methods. In certain circumstances, DA methods display comparable or even better performance. We release our dataset, encouraging future research on UniDA in NLP to foster the development of more resilient and domain-specific strategies.2"
        },
        {
            "heading": "2 Universal Domain Adaptation",
            "text": ""
        },
        {
            "heading": "2.1 Problem Formulation",
            "text": "Distributional shift refers to a situation where the joint distribution P estimated from the training dataset fails to adequately represent the wide range of diverse and complex test inputs. More formally, a distributional shift arises when the test input xtest originates from a distant distribution Q, which is not effectively encompassed by the current trained distribution P .\nOne of the most prevalent research areas addressing this distributional shift includes OOD detection and DA. OOD detection aims to strictly detect all inputs from Q to enhance the model\u2019s reliability. Although distribution Q demonstrates a discernibly different distribution from the distribution P , the trained model can still transfer a subset of instances from Q, overcoming the inherent discrepancy between P and Q. This particular capability serves as a fundamental motivation underlying the pursuit of DA. UniDA endeavours to integrate the merits of both fields, thereby enhancing both the generalizability and reliability of the model. Specifically, let us divide the target distribution Q into disjoint subsets H , which share the same label space with source distribution P and its complement I (Q = H \u222a I). The objective of UniDA is to enrich the robustness of the model by flexibly transferring existing knowledge to transferable samples from H while firing alarms to unknown samples from I ."
        },
        {
            "heading": "2.2 Challenges in UniDA",
            "text": "UniDA models should be capable of accurately capturing the underlying reasons behind the shift, thereby enabling the discrimination between transferable samples and unknown samples. Among the diverse categories of causes, the domain gap\n2The dataset is available at https://github.com/ heyjoonkim/universal_domain_adaptation_for_nlp\nand the category gap (You et al., 2019) emerge as pivotal factors, each exerting a substantial impact on the overall complexity of the UniDA problem. While these concepts have previously been defined in a rather vague manner, we deduced the necessity for a more explicit definition. Thus, we set forth to redefine the concepts of domain and category gap more explicitly.\nDomain gap refers to the performance drop when a model trained on P fails to correctly process transferable inputs due to the fundamental discrepancy between P and H , i.e., a domain shift. A dataset with a higher domain gap amplifies the problem\u2019s difficulty as the trained model becomes more susceptible to misaligning transferable samples.\nA category shift, characterized by the disparity in the class sets considered by P and I , causes a category gap. Category gap represents the performance drop that arises for inputs from I , which cannot be processed properly due to differing class sets between P and I , which are erroneously handled. A larger category gap makes distinguishing unknown samples from transferable samples harder, thereby worsening the robustness of the model.\nFrom the perspective of addressing the domain gap and category gap, the main goal of UniDA is to minimize both gaps simultaneously. This aims to ensure transferable samples properly align with the source domain for adequate processing, while handling unknown samples as unprocessable exceptions."
        },
        {
            "heading": "3 Testbed Design",
            "text": "The primary objective of our research is to construct a comprehensive benchmark dataset that effectively captures the viewpoint of UniDA. To accomplish our objective, we attempt to create a diverse dataset that encompasses a range of difficulty levels and characteristics, such as domains, sentiment, or temporal change. These variations are the fundamental elements that can significantly influence the overall performance.\nSpecifically, we initially select datasets from multiple practical domains and approximate the adaptation difficulty by quantifying different shifts with our newly proposed metrics. In the following subsections, we provide an in-depth explanation of our dataset along with the analysis of our benchmarks."
        },
        {
            "heading": "3.1 Quantifying Different Shifts",
            "text": "As the extent of both domain and category gaps significantly influences the overall adaptation complexity, it is essential to quantify these gaps when designing the dataset for evaluation. Unfortunately, existing literature has not devised a clear-cut and quantitative measure for assessing domain and category gaps. Therefore, we endeavoured to define measures that can aptly approximate the two types of gaps.\nPerformance Drop Rate (PDR) measures the degree of domain gap by assessing the absolute difficulty of the dataset itself and the performance drop caused by the shift from P to H . Specifically, we fine-tune bert-base-uncased on the source train set and evaluate its test set accuracy accs from the same distribution. Leveraging the same model trained on the source domain, we then measure the accuracy of the target test set acct. We measure the performance degradation caused by the distributional shift by measuring accs \u2212 acct. Since the significance of the performance drop may vary depending on the source performance, we normalize the result with the source performance and measure the proportion of the performance degradation. A more significant drop rate indicates a greater decline in performance, considering the source domain performance. Formally, PDR for a source domain s and a target domain t can be measured as follows:\nPDRs,t = 100\u00d7 accs \u2212 acct\naccs (1)\nDistinction Difficulty Score (DDS) is measured to estimate the difficulty of distinguishing between H and I , which, in other words, measures the difficulty of handling the category shift. We utilized the same model trained on the source domain and extracted the [CLS] representations of the source inputs. We estimated the source distribution, assuming the extracted representations follow the multivariate normal distribution. We then extracted [CLS] representations of target distribution inputs from the same model and measured the Mahalanobis distance between the source distribution. Using the distance, we measured the Area Under the ROC Curve (AUC) as a metric for discerning I and H . AUC values closer to 1 indicate the ease of discerning unknown inputs from the transferable inputs. Since we focus on quantifying the difficulty in distinguishing the two, we subtract the AUC from 1 to derive our final measure of interest. For\nthe source domain s, the target domain t, and AUC as AUCs,t, DDS can be measured as:\nDDSs,t = 100\u00d7 (1\u2212 AUCs,t) (2)"
        },
        {
            "heading": "3.2 Implementation of Different Shifts",
            "text": "To construct a representative testbed for UniDA, it is essential to illustrate domain and category shifts. To exhibit domain shift, we delineated domains from various perspectives. This involves explicit factors such as temporal or sentiment and implicit definitions based on the composition of the class set. Detailed formation of domains for each dataset is stipulated in Section 3.3.\nTo establish category shifts, the source and the target domain must have a set of common classes C and a set of their own private classes, C\u0304s and C\u0304t, respectively. We followed previous works (You et al., 2019; Fu et al., 2020) by sorting the class name in alphabetical order and selecting the first |C| classes as common, the subsequent |C\u0304s| as source private, and the rest as target private classes. The class splits for each dataset are stated as |C|/|C\u0304s|/|C\u0304t| in the main experiments."
        },
        {
            "heading": "3.3 Dataset Details",
            "text": "We focused on text classification tasks for our experiments. Four datasets were selected from multiple widely used classification domains in NLP, such as topic classification, sentiment analysis, and intent classification. We reformulated the datasets so that our testbed could cover diverse adaptation scenarios.\nHuffpost News Topic Classification (Huffpost) (Misra, 2022) contains Huffpost news headlines spanning from 2012 to 2022. The task is to classify news categories given the headlines. Using the temporal information additionally provided, we split the dataset year-wise from 2012 to 2017, treating each year as a distinct domain. We selected the year 2012 as the source domain, with the subsequent years assigned as the target domains, creating 5 different levels of temporal shifts.\nMultilingual Amazon Reviews Corpus (Amazon) (Keung et al., 2020) includes product reviews that are commonly used to predict star ratings based on the review, and additional product information is provided for each review. We have revised the task to predict the product information given the reviews and utilized the star ratings to define sentiment domains. Reviews with a star rating of 1 or 2 are grouped as negative sentiment, and those with\na rating of 4 or 5 are categorized as positive. We exclude 3-star reviews, considering them neutral.\nMASSIVE (FitzGerald et al., 2022) is a hierarchical dataset for intent classification. The dataset consists of 18 first-level and 60 secondlevel classes. Each domain is defined as a set of classes, including private classes exclusive to a specific domain and common classes shared across domains. We divided the common first-level class into two parts based on second-level classes to simulate domain discrepancy. The half of the divided common class represents the source domain while the other half represents the target domain. We assume that the second-level classes within the same first-level class share a common feature and thus can be adapted.\nCLINC-150 (Larson et al., 2019) is widely used for intent classification in OOD detection. The dataset consists of 150 second-level classes over 10 first level-classes and a single out-of-scope class. The domain is defined in the same way as MASSIVE."
        },
        {
            "heading": "3.4 Dataset Analysis",
            "text": "In this section, we intend to validate whether our testbed successfully demonstrates diverse adaptation difficulties, aligning with our original motivation. We assess adaptation difficulty from domain and category gap perspectives, each approximated by PDR and DDS, respectively.\nThe results of PDR and DDS are reported in Table 1. The result shows diverse PDR values ranging from 7 to 36 points, indicating various degrees of domain gap across the datasets. MASSIVE measured the most considerable domain gap, while Huffpost (2013) demonstrated the most negligible domain gap among the datasets. Additionally, our testbed covers a wide range of category gaps, indicated by the broad spectrum of DDS values. Specifically, Amazon exhibits a significantly high\nDDS value, representing an extremely challenging scenario of differentiating the unknown samples from the transferable samples.\nWe consolidate the two indicators to measure the coverage of different adaptation complexity of our proposed testbed. We visualized the datasets considering the PDR and DDS; metrics on the respective axes. Figure 2 is the visualization of the adaptation complexity for each dataset factored by PDR and DDS. We grouped the datasets into four distinct clusters based on the plotted distribution. Datasets closer to the lower-left corner, CLINC-150 and Huffpost (2013), are easy adaptation scenarios with minor domain and category gaps. Datasets plotted in the center, Huffpost (2014, 2015, 2016), presents moderate difficulty. Amazon suffers from a category gap significantly, while Huffpost (2017) and MASSIVE demonstrate a notable domain gap, yielding high adaptation complexity. The results validate that our testbed embodies a diverse range of adaptation difficulties as intended."
        },
        {
            "heading": "4 Experimental Setting",
            "text": ""
        },
        {
            "heading": "4.1 Compared Methods",
            "text": "We compare several domain adaptation methods on our proposed testbed. We selected two previous state-of-the-art closed-set Domain Adaptation (CDA) methods, UDALM (Karouzos et al., 2021)\nand AdSPT (Wu and Shi, 2022), under the assumption that all the inputs from the target domain are transferable without considering unknown classes. Two previous state-of-the-art UniDA methods were selected, OVANet (Saito and Saenko, 2021) and UniOT (Chang et al., 2022), which are fully optimized to handle UniDA scenarios in the vision domain. We also conducted experiments with additional baseline methods such as DANN (Ganin et al., 2016a), UAN (You et al., 2019), and CMU (Fu et al., 2020). However, the performance was subpar compared to the selected methods, exhibiting a similar tendency. Hence, we report the additional results in Appendix A. For the backbone of all the methods, we utilized bert-base-uncased (Devlin et al., 2019) and used the [CLS] representation as the input feature. Implementation details are stipulated in Appendix B."
        },
        {
            "heading": "4.2 Thresholding Method",
            "text": "Since CDA methods are not designed to handle unknown inputs, additional techniques are required to discern them. A straightforward yet intuitive approach to detecting unknown inputs is applying a threshold for the output of the scoring function. The scoring function reflects the appropriateness of the input based on the extracted representation. If the output of the scoring function falls below the threshold, the instance is classified as unknown. We sequentially apply thresholding after the adaptation process.3 Formally, for an input x, categorical prediction y\u0302, threshold value w, and a scoring function fscore, the final prediction is made as:\ny(x) = { argmax(y\u0302), if fscore(x) > w unknown, otherwise.\n(3)\nWe utilize Maximum Softmax Probability (MSP) as the scoring function4 (Hendrycks and Gimpel, 2017). Following the OOD detection literature, the value at the point of 95% from the sorted score values was selected as the threshold.\n3In the case of applying the thresholding first, all the inputs from the target domain would be classified as OOD. If all the target inputs were classified as OOD, the criteria for discerning transferable and unknown inputs become inherently unclear. Therefore, we have only considered scenarios where thresholding is applied after the adaptation.\n4In addition to MSP, we have applied various scoring functions such as cosine similarity and Mahalanobis distance, but using MSP achieved the best performance. Results for other thresholding methods have been included in the Appendix A.\nCLI NC\n-15 0 201 3 201 4 201 5 201 6 201 7\nMA SSI\nVE\nAm azo\nn\nDataset\n0\n20\n40\n60\nHsc\nor e\nAdSPT (CDA) UniOT (UniDA)\nFigure 3: H-score results of AdSPT and UniOT on all the datasets. The preferred method varies depending on the adaptation complexity."
        },
        {
            "heading": "4.3 Evaluation Protocol",
            "text": "The goal of UniDA is to properly process the transferable inputs and detect the unknown inputs simultaneously, consequently making both the transferable and the unknown accuracies crucial metrics. We applied H-score (Fu et al., 2020) as the primary evaluation metric to integrate both evaluation metrics. H-score is the harmonic mean between the accuracy of common class accC and unknown class accC\u0304t , where accC is the accuracy over the common class set C and accC\u0304t is the accuracy predicting the unknown class. The model with a high H-score is considered robust in the UniDA setting, indicating its proficiency in both adaptation (high accC) and OOD detection (high accC\u0304t). Formally, the H-score can be defined as :\nHscore = 2 \u00b7 accC \u00b7 accC\u0304t accC + accC\u0304t\n(4)\nAlthough the H-score serves as an effective evaluation criterion for UniDA, we also report accC and accC\u0304t to provide a comprehensive assessment. We report the averaged results and standard deviations over four runs for all experiments."
        },
        {
            "heading": "5 Experimental Results",
            "text": ""
        },
        {
            "heading": "5.1 Overview",
            "text": "We conduct evaluations based on the clusters defined in Section 3.4 and analyze how the results vary depending on the adaptation complexity. Figure 3 presents an overview of the H-score results for the best-performing method from each CDA\nHuffpost (2012 \u2192 2013) (3 / 4 / 4)\nMethod accC accC\u0304t H-score\nUDALM 52.74 \u00b12.92 58.55 \u00b16.03 55.15 \u00b12.37 AdSPT 55.05 \u00b12.11 80.66 \u00b12.99 65.38 \u00b11.00\nOVANet 65.11 \u00b10.60 24.91 \u00b16.75 35.64 \u00b16.70 UniOT 53.76 \u00b11.10 65.86 \u00b13.57 59.14 \u00b11.14\nCLINC-150 (4 / 3 / 3)\nand UniDA approach: AdSPT representing CDA and UniOT representing UniDA. Despite an outlier caused by unstable thresholding in CLINC-150, the overall trend demonstrates that AdSPT manifests comparable performance in less complex scenarios, while UniOT exhibits superior performance towards challenging scenarios. These trends align with the findings of other methods that are not depicted in the figure."
        },
        {
            "heading": "5.2 Detailed Results",
            "text": "Table 2 demonstrates the results of relatively easy adaptation scenarios. CDA methods demonstrate performance that is on par with, or even superior to, UniDA methods. The results appear counterintuitive, as CDA methods are designed without considering unknown samples. Specifically, UDALM outperforms all the UniDA methods in CLINC-150 and performs comparable or even better in Huffpost (2013). AdSPT exhibits the best performance in Huffpost (2013). However, AdSPT suffers a significant performance drop in CLINC150, as we speculate this result is due to the inherent instability of the thresholding method. The misguided threshold classifies the majority of the inputs as unknown, which leads to a very high accC\u0304t , but significantly reduces the accC . This inconsistency also leads to a high variance of accC\u0304t for all the CDA methods.\nIn the case of moderate shifts, no particular method decisively stands out, as presented by Table 3. In all cases, AdSPT and UniOT present\nthe best performance with a marginal difference, making it inconclusive to determine a superior approach. Despite the relatively subpar performance, UDALM and OVANet also exhibit similar results. Still, it is notable that CDA methods, which are not inherently optimized for UniDA settings, show comparable results.\nThe result of Amazon, in which the category gap is most prominent, is reported in Table 4. UniDA methods exhibit substantially superior performance to CDA methods. In particular, while the difference in accC is marginal, there exists a substantial gap of up to 55 points in accC\u0304t . As the category gap intensifies, we observe the decline in the performance of CDA methods, which are fundamentally limited by the inability to handle unknown inputs.\nFinally, the results of Huffpost (2017) and MASSIVE, which exhibit a high domain gap, are reported in Table 5. The result indicates that UniDA methods consistently display superior performance in most cases. However, the divergence between the approaches is relatively small compared to Amazon. UniOT demonstrates the best performance in all datasets, with OVANet\u2019s slightly lower performance. AdSPT demonstrates marginally better performance than OVANet in Huffpost (2017),\nMASSIVE (8 / 5 / 5)\nbut the gap is marginal. Even though CDA methods pose comparable performance, UniDA methods demonstrate better overall performance."
        },
        {
            "heading": "5.3 Impact of Threshold Values",
            "text": "The selection of the threshold value considerably influences the performance of CDA methods. In order to probe the impact of the threshold values on the performance, we carry out an analysis whereby different threshold values are applied to measure the performance of the methods.\nThe results are demonstrated in Figure 4. In cases of low or moderate adaptation complexity, such as CLINC-150 and Huffpost (2013, 2014, 2015, 2016), CDA methods demonstrate the potential to outperform UniDA methods when provided an appropriate threshold. However, as the adaptation complexity intensifies, such as Huffpost (2017), MASSIVE, and Amazon, UniDA methods outperform CDA methods regardless of the selected threshold. These observations align seam-\nlessly with the findings from Section 4.3 that underscore the proficiency of UniDA methods in managing challenging adaptation scenarios. Additionally, it should be noted that determining the optimal threshold is particularly challenging in the absence of supervision from the target domain. Therefore, the best performance should be considered upperbound of the CDA methods."
        },
        {
            "heading": "6 Related Work",
            "text": ""
        },
        {
            "heading": "6.1 Domain Adaptation",
            "text": "The studies in the field of DA in NLP primarily assumes a closed-set environment, which the source and the target domain share the same label space. CDA research predominantly concentrated on learning domain invariant features (Blitzer et al., 2006; Pan et al., 2010; Ben-David et al., 2020; Ganin and Lempitsky, 2015; Du et al., 2020) for effective adaptation. With the advent of pretrained language models (PLMs), CDA methods have evolved to effectively leverage the capabilities of PLMs. Techniques such as masked language modeling (Karouzos et al., 2021) or soft-prompt with adversarial training (Wu and Shi, 2022) have shown promising results. However, the closed-set assumption has a fundamental drawback as it may leave the models vulnerable when exposed to data from an unknown class.\nTo mitigate such issue, a new line of work named UniDA (You et al., 2019) was proposed which assumes no prior knowledge about the target domain.\nYou et al. (2019) quantifies sample-level transferability by using of uncertainty and domain similarity. Following the work, Fu et al. (2020) calibrates multiple uncertainty measures to handle such an issue. Saito and Saenko (2021) apply a one-vsall classifier to minimize inter-class distance and classify unknown classes. More recently, Chang et al. (2022) applied Optimal Transport and further expanded the task to discovering private classes. Other recent works focus on utilizing mutually nearest neighbor samples (Chen et al., 2022a,c) or leveraging source prototypes with target samples (Chen et al., 2022b; Kundu et al., 2022). Despite the practicality of UniDA, its application in the NLP domain has barely explored."
        },
        {
            "heading": "6.2 Out-of-Distribution Detection",
            "text": "The early exploration of OOD detection focused on training supervised detectors (Dhamija et al., 2018; Lee et al., 2018a; Jiang et al., 2018). However, since obtaining labeled OOD samples is impractical, recent OOD detection research has shifted towards unsupervised methods, such as generating pseudo-OOD data (Chen and Yu, 2021; Zheng et al., 2020), utilizing self-supervised learning (Moon et al., 2021; Manolache et al., 2021; Li et al., 2021; Zeng et al., 2021; Zhan et al., 2021; Cho et al., 2022), and measuring uncertainty through scoring functions for input instances (Hendrycks and Gimpel, 2017; Lee et al., 2018b; Liu et al., 2020; Tack et al., 2020). While these methods have shown effectiveness, OOD detection is limited in\nthat it does not offer opportunities for adaptation."
        },
        {
            "heading": "7 Conclusion and Future Work",
            "text": "In this study, we present a testbed for evaluating UniDA in the field of NLP. The testbed is designed to exhibit various levels of domain and category gaps through different datasets. Two novel metrics, PDR and DDS, were proposed which can measure the degree of domain and category gap, respectively. We assessed UniDA methods and the heuristic combination of CDA and OOD detection in our proposed testbed. Experimental results show that UniDA methods, initially designed for the vision domain, can be effectively transferred to NLP. Additionally, CDA methods, which are not fully optimized in UniDA scenario, produce comparable results in certain circumstances.\nRecent trends in NLP focus on Large Language Models (LLMs) of their significant generalization abilities. However, the robustness of LLMs from the perspective of UniDA remains uncertain. As part of our future work, we assess the performance and the capabilities of LLMs from a UniDA viewpoint.\nLimitations\nLimited coverage of the evaluated model sizes The evaluation was conducted only with models of limited size. Moreover, there is a lack of zeroshot and few-shot evaluations for large language models (LLMs) that have recently emerged with remarkable generalization capabilities. The evaluation of LLMs is currently being considered as a top priority for our future work, and based on preliminary experiments, the results were somewhat unsatisfactory compared to small models with basic tuning for classification performance. In this regard, recent research that evaluated LLMs for classification problems (such as GLUE) also reported that the performance is not yet comparable to task-specifically tuned models. Considering the limitations of LLMs in terms of their massive resource usage and the fact that tuning small models still outperforms them in a task-specific manner, the findings from this study are still considered highly valuable in the NLP community.\nLimited scope of the tasks Our proposed testbed is restricted to text classification tasks only. The majority of existing research on DA and OOD also focuses on classification. This selective task preference is primarily due to the challenge of defining\nconcepts such as domain shifts and category shifts in generative tasks. However, in light of recent advancements in the generative capabilities of models, handling distributional shifts in generative tasks is indubitably an essential problem that needs to be addressed in our future work."
        },
        {
            "heading": "Acknowledgement",
            "text": "This work was mainly supported by SNU-NAVER Hyperscale AI Center and partly supported by Institute of Information & communications Technology Planning & Evaluation (IITP) grant funded by the Korea government (MSIT) [No.2020-0-01373, Artificial Intelligence Graduate School Program (Hanyang University), NO.2021-0-02068, Artificial Intelligence Innovation Hub (Artificial Intelligence Institute, Seoul National University)] and Korea Evaluation Institute of Industrial Technology (KEIT) grant funded by the Korea government (MOTIE)."
        },
        {
            "heading": "A Full Experimental Results",
            "text": "A.1 UniDA Results\nTable 7 is the full results of UniDA methods in our proposed testbed. Baseline methods such as UAN (You et al., 2019) and CMU (Fu et al., 2020) are included in the results. We can observe that UniDA methods do not always retrain the same level of applicability in NLP. Specifically, UAN and CMU utilize a fixed threshold defined in the vision domain. While CMU remains fully compatible in the NLP domain, UAN struggles to apply effectively, as it fails to detect unknown samples.\nA.2 CDA Results\nIn this section, we demonstrate the experimental results of CDA methods with two additional scoring functions: cosine similarity and Mahalanobis distance. The threshold value was selected based on the score from the scoring functions, using the same approach as the main experiment. Also, we report the results of DANN (Ganin et al., 2016a) and source-only fine-tuning which was left out from the main experiment. In some cases, sourceonly fine-tuning outperforms other adaptation methods, which is also observed in the vision domain (You et al., 2019).\n1. Cosine Similarity (Tack et al., 2020) calculates the cosine similarity score between the test input and the train input. The score is selected as the cosine similarity between the input and the nearest neighbor. The results are reported in Table 8.\n2. Mahalanobis Distance (Lee et al., 2018b) is the distance of the test sample to each class distribution. The representation is assumed to follow the multivariate normal distributions. The distance between the nearest class distribution is used as the score. The results are demonstrated in Table 9\nAdditionally, the full experimental results of MSP thresholding are presented in Table 10.\nB Implementation Details\nFor the experiments, we adopt a 12-layer pretrained language model bert-base-uncased (Devlin et al., 2019) as the backbone of all the methods. We utilized the [CLS] representation as the input feature. AdamW optimizer (Loshchilov and Hutter, 2019) was used for all the experiments with a batch size of 32. We selected the best learning rate among 5e-4, 1e-4, 5e-5, 1e-5, and 5e-6. The learning rate for each method is reported in Table 6. The model was trained for 10 epochs with an early stopping on the accuracy of the source domain\u2019s evaluation set. All the experiments were implemented with Pytorch (Paszke et al., 2019) and Huggingface Transformers library (Wolf et al., 2020). The experiments take an hour on a single Tesla V100 GPU."
        },
        {
            "heading": "C Ablation on Different Class Splits",
            "text": "For the main experiment, we utilized class names as the criterion to implement the category gap. However, this may only show the specific scenario of the category gap. To provide a more comprehensive analysis, we also report the results when the class set is randomly split. We utilized CLINC-150 and MASSIVE dataset for the ablation study, and MSP thresholding was applied for CDA methods. We conducted three experiments, each with a different class split, and for every split, we reported\nthe average results of three different runs. Table 11 is the results of the experiments. Due to the changes in the class set to be predicted, the task difficulty varies, resulting in differences in absolute performances. However, when comparing the relative performance between different methods, we can observe that they exhibit consistent trends regardless of the class split."
        },
        {
            "heading": "D Receiver Operating Characteristic (ROC) Curve",
            "text": "To measure Distinction Difficulty Score (DDS), we calculated the AUROC and subtracted from 1. Figure 5 is the ROC curve of discerning unknown inputs from the transferable inputs for our proposed datasets. The closer the ROC curve is to the upperleft corner, it indicates that it is easier to distinguish between unknown and transferable inputs."
        }
    ],
    "title": "Universal Domain Adaptation for Robust Handling of Distributional Shifts in NLP",
    "year": 2023
}