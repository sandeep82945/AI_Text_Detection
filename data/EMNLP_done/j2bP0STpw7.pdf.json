{
    "abstractText": "We present The Vault, a dataset of high-quality code-text pairs in multiple programming languages for training large language models to understand and generate code. We present methods for thoroughly extracting samples that use both rule-based and deep learningbased methods to ensure that they contain highquality pairs of code and text, resulting in a dataset of 43 million high-quality code-text pairs. Our extensive evaluations on common coding tasks including code generation, code search and code summarization show that when fine-tuning Code Large Language Models on The Vault, such models outperform the same models trained on other datasets such as CodeSearchNet. We also provide detailed analyses of our datasets to assess the effects of various programming languages and docstrings on the performance of such models.",
    "authors": [
        {
            "affiliations": [],
            "name": "Dung Nguyen Manh"
        },
        {
            "affiliations": [],
            "name": "Nam Le Hai"
        },
        {
            "affiliations": [],
            "name": "Anh T. V. Dau"
        },
        {
            "affiliations": [],
            "name": "Anh Minh Nguyen"
        },
        {
            "affiliations": [],
            "name": "Khanh Nghiem"
        },
        {
            "affiliations": [],
            "name": "Jin Guo"
        },
        {
            "affiliations": [],
            "name": "Nghi D. Q. Bui"
        }
    ],
    "id": "SP:4915f8fcf8388e6a6ae922928fb362e3b56da4b6",
    "references": [
        {
            "authors": [
                "T. Ahmed",
                "P. Devanbu"
            ],
            "title": "Multilingual training for software engineering",
            "venue": "In Proceedings of the 44th International Conference on Software Engineering,",
            "year": 2022
        },
        {
            "authors": [
                "N.D. Bui",
                "Y. Yu",
                "L. Jiang"
            ],
            "title": "Sar: learning crosslanguage api mappings with little knowledge",
            "venue": "In Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering,",
            "year": 2019
        },
        {
            "authors": [
                "N.D. Bui",
                "Y. Yu",
                "L. Jiang"
            ],
            "title": "Infercode: Selfsupervised learning of code representations by predicting subtrees",
            "venue": "IEEE/ACM 43rd International Conference on Software Engineering (ICSE),",
            "year": 2021
        },
        {
            "authors": [
                "N.D. Bui",
                "Y. Yu",
                "L. Jiang"
            ],
            "title": "Treecaps: Tree-based capsule networks for source code processing",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2021
        },
        {
            "authors": [
                "M. Chen",
                "J. Tworek",
                "H. Jun",
                "Q. Yuan",
                "H.P. d. O. Pinto",
                "J. Kaplan",
                "H. Edwards",
                "Y. Burda",
                "N. Joseph",
                "G. Brockman"
            ],
            "title": "Evaluating large language models trained on code",
            "venue": "arXiv preprint arXiv:2107.03374,",
            "year": 2021
        },
        {
            "authors": [
                "A. Ciurumelea",
                "S. Proksch",
                "H.C. Gall"
            ],
            "title": "Suggesting comment completions for python using neural language models",
            "venue": "IEEE 27th International Conference on Software Analysis, Evolution and Reengineering (SANER),",
            "year": 2020
        },
        {
            "authors": [
                "C.B. Clement",
                "D. Drain",
                "J. Timcheck",
                "A. Svyatkovskiy",
                "N. Sundaresan"
            ],
            "title": "Pymt5: multi-mode translation of natural language and python code with transformers",
            "venue": "Proceedings of the 2020 Conference on Em-",
            "year": 2020
        },
        {
            "authors": [
                "A.T.V. Dau",
                "N.D.Q. Bui",
                "T. Nguyen-Duc",
                "H. Thanh-Tung"
            ],
            "title": "Towards using data-influence methods to detect noisy samples in source code corpora",
            "venue": "In 37th IEEE/ACM International Conference on Automated Software Engineering,",
            "year": 2022
        },
        {
            "authors": [
                "Z. Feng",
                "D. Guo",
                "D. Tang",
                "N. Duan",
                "X. Feng",
                "M. Gong",
                "L. Shou",
                "B. Qin",
                "T. Liu",
                "D. Jiang",
                "M. Zhou"
            ],
            "title": "CodeBERT: A pre-trained model for programming and natural languages",
            "venue": "In Findings of the Association for Computational Linguistics: EMNLP 2020,",
            "year": 2020
        },
        {
            "authors": [
                "L. Gao",
                "S. Biderman",
                "S. Black",
                "L. Golding",
                "T. Hoppe",
                "C. Foster",
                "J. Phang",
                "H. He",
                "A. Thite",
                "N. Nabeshima"
            ],
            "title": "The pile: An 800gb dataset of diverse text for language modeling",
            "venue": "arXiv preprint arXiv:2101.00027,",
            "year": 2020
        },
        {
            "authors": [
                "Z. Gao",
                "X. Xia",
                "J. Grundy",
                "D. Lo",
                "Y. Li"
            ],
            "title": "Generating question titles for stack overflow from mined code snippets",
            "venue": "ACM Trans. Softw. Eng. Methodol.,",
            "year": 2020
        },
        {
            "authors": [
                "M.A. Gordon",
                "K. Duh",
                "J. Kaplan"
            ],
            "title": "Data and parameter scaling laws for neural machine translation",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2021
        },
        {
            "authors": [
                "X. Hu",
                "G. Li",
                "X. Xia",
                "D. Lo",
                "Z. Jin"
            ],
            "title": "Deep code comment generation with hybrid lexical and syntactical information",
            "venue": "Empir. Softw. Eng.,",
            "year": 2020
        },
        {
            "authors": [
                "H. Husain",
                "H.-H. Wu",
                "T. Gazit",
                "M. Allamanis",
                "M. Brockschmidt"
            ],
            "title": "Codesearchnet challenge: Evaluating the state of semantic code search",
            "year": 1909
        },
        {
            "authors": [
                "S. Iyer",
                "I. Konstas",
                "A. Cheung",
                "L. Zettlemoyer"
            ],
            "title": "Mapping language to code in programmatic context",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2018
        },
        {
            "authors": [
                "S. Iyer",
                "I. Konstas",
                "A. Cheung",
                "L. Zettlemoyer"
            ],
            "title": "Mapping language to code in programmatic context",
            "venue": "In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2018
        },
        {
            "authors": [
                "A. Kanade",
                "P. Maniatis",
                "G. Balakrishnan",
                "K. Shi"
            ],
            "title": "Learning and evaluating contextual embedding of source code",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "J. Kaplan",
                "S. McCandlish",
                "T. Henighan",
                "T.B. Brown",
                "B. Chess",
                "R. Child",
                "S. Gray",
                "A. Radford",
                "J. Wu",
                "D. Amodei"
            ],
            "title": "Scaling laws for neural language models",
            "year": 2001
        },
        {
            "authors": [
                "D. Kocetkov",
                "R. Li",
                "L.B. Allal",
                "J. Li",
                "C. Mou",
                "C.M. Ferrandis",
                "Y. Jernite",
                "M. Mitchell",
                "S. Hughes",
                "T. Wolf"
            ],
            "title": "The stack: 3 tb of permissively licensed source code",
            "venue": "arXiv preprint arXiv:2211.15533,",
            "year": 2022
        },
        {
            "authors": [
                "A. Gokaslan",
                "S. Bose",
                "D. Adelani",
                "L. Phan",
                "H. Tran",
                "I. Yu",
                "S. Pai",
                "J. Chim",
                "V. Lepercq",
                "S. Ilic",
                "M. Mitchell",
                "S.A. Luccioni",
                "Y. Jernite"
            ],
            "title": "The bigscience roots corpus: A 1.6tb composite multilingual dataset, 2023",
            "year": 2023
        },
        {
            "authors": [
                "A. LeClair",
                "C. McMillan"
            ],
            "title": "Recommendations for datasets for source code summarization",
            "venue": "Association for Computational Linguistics,",
            "year": 2019
        },
        {
            "authors": [
                "C.-Y. Lin"
            ],
            "title": "Rouge: A package for automatic evaluation of summaries",
            "venue": "In Text summarization branches out,",
            "year": 2004
        },
        {
            "authors": [
                "C.-Y. Lin",
                "F.J. Och"
            ],
            "title": "Orange: a method for evaluating automatic evaluation metrics for machine translation",
            "venue": "In COLING 2004: Proceedings of the 20th International Conference on Computational Linguistics,",
            "year": 2004
        },
        {
            "authors": [
                "Y. Liu",
                "M. Ott",
                "N. Goyal",
                "J. Du",
                "M. Joshi",
                "D. Chen",
                "O. Levy",
                "M. Lewis",
                "L. Zettlemoyer",
                "V. Stoyanov"
            ],
            "title": "Roberta: a robustly optimized bert pretraining approach (2019)",
            "venue": "arXiv preprint arXiv:1907.11692,",
            "year": 1907
        },
        {
            "authors": [
                "Y. Liu",
                "M. Ott",
                "N. Goyal",
                "J. Du",
                "M. Joshi",
                "D. Chen",
                "O. Levy",
                "M. Lewis",
                "L. Zettlemoyer",
                "V. Stoyanov"
            ],
            "title": "Roberta: A robustly optimized bert pretraining approach",
            "year": 1907
        },
        {
            "authors": [
                "Z. Luo",
                "C. Xu",
                "P. Zhao",
                "Q. Sun",
                "X. Geng",
                "W. Hu",
                "C. Tao",
                "J. Ma",
                "Q. Lin",
                "D. Jiang"
            ],
            "title": "Wizardcoder: Empowering code large language models with evol-instruct",
            "venue": "arXiv preprint arXiv:2306.08568,",
            "year": 2023
        },
        {
            "authors": [
                "J. Mahmud",
                "F. Faisal",
                "R.I. Arnob",
                "A. Anastasopoulos",
                "K. Moran"
            ],
            "title": "Code to comment translation: A comparative study on model effectiveness",
            "year": 2021
        },
        {
            "authors": [
                "C. Nguyen",
                "L. Ngo",
                "T. Nguyen"
            ],
            "title": "Retrieving relevant context to align representations for cross-lingual event detection",
            "venue": "In Findings of the Association for Computational Linguistics: ACL 2023,",
            "year": 2023
        },
        {
            "authors": [
                "C. Niu",
                "C. Li",
                "V. Ng",
                "J. Ge",
                "L. Huang",
                "B. Luo"
            ],
            "title": "Sptcode: sequence-to-sequence pre-training for learning source code representations",
            "venue": "In Proceedings of the 44th International Conference on Software Engineering,",
            "year": 2022
        },
        {
            "authors": [
                "D. Peng",
                "S. Zheng",
                "Y. Li",
                "G. Ke",
                "D. He",
                "T.-Y. Liu"
            ],
            "title": "How could neural networks understand programs",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "C. Raffel",
                "N. Shazeer",
                "A. Roberts",
                "K. Lee",
                "S. Narang",
                "M. Matena",
                "Y. Zhou",
                "W. Li",
                "P.J. Liu"
            ],
            "title": "Exploring the limits of transfer learning with a unified text-totext transformer",
            "venue": "arXiv preprint arXiv:1910.10683,",
            "year": 2019
        },
        {
            "authors": [
                "B. Roziere",
                "M.-A. Lachaux",
                "L. Chanussot",
                "G. Lample"
            ],
            "title": "Unsupervised translation of programming languages",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "T.L. Scao",
                "A. Fan",
                "C. Akiki",
                "E. Pavlick",
                "S. Ili\u0107",
                "D. Hesslow",
                "R. Castagn\u00e9",
                "A.S. Luccioni",
                "F. Yvon",
                "M. Gall\u00e9"
            ],
            "title": "Bloom: A 176b-parameter openaccess multilingual language model",
            "venue": "arXiv preprint arXiv:2211.05100,",
            "year": 2022
        },
        {
            "authors": [
                "B. Shen",
                "J. Zhang",
                "T. Chen",
                "D. Zan",
                "B. Geng",
                "A. Fu",
                "M. Zeng",
                "A. Yu",
                "J. Ji",
                "J. Zhao"
            ],
            "title": "Pangu-coder2: Boosting large language models for code with ranking feedback",
            "venue": "arXiv preprint arXiv:2307.14936,",
            "year": 2023
        },
        {
            "authors": [
                "B. Sorscher",
                "R. Geirhos",
                "S. Shekhar",
                "S. Ganguli",
                "A. Morcos"
            ],
            "title": "Beyond neural scaling laws: beating power law scaling via data pruning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 1952
        },
        {
            "authors": [
                "H. To",
                "N. Bui",
                "J. Guo",
                "T. Nguyen"
            ],
            "title": "Better language models of code through self-improvement (2023)",
            "venue": "DOI: https://doi",
            "year": 2023
        },
        {
            "authors": [
                "H. Touvron",
                "T. Lavril",
                "G. Izacard",
                "X. Martinet",
                "M.-A. Lachaux",
                "T. Lacroix",
                "B. Rozi\u00e8re",
                "N. Goyal",
                "E. Hambro",
                "F. Azhar",
                "A. Rodriguez",
                "A. Joulin",
                "E. Grave",
                "G. Lample"
            ],
            "title": "Llama: Open and efficient foundation language models, 2023",
            "year": 2023
        },
        {
            "authors": [
                "L. Tunstall",
                "L. Von Werra",
                "T. Wolf"
            ],
            "title": "Natural language processing with transformers",
            "year": 2022
        },
        {
            "authors": [
                "L.N. Van",
                "N.L. Hai",
                "H. Pham",
                "K. Than"
            ],
            "title": "Auxiliary local variables for improving regularization/prior approach in continual learning",
            "venue": "In Pacific-Asia Conference on Knowledge Discovery and Data Mining,",
            "year": 2022
        },
        {
            "authors": [
                "Y. Wang",
                "W. Wang",
                "S.R. Joty",
                "S.C.H. Hoi"
            ],
            "title": "Codet5: Identifier-aware unified pre-trained encoder-decoder models for code understanding and generation",
            "venue": "Proceedings of the 2021 Conference on Em-",
            "year": 2021
        },
        {
            "authors": [
                "Y. Wang",
                "H. Le",
                "A.D. Gotmare",
                "N.D.Q. Bui",
                "J. Li",
                "S.C.H. Hoi"
            ],
            "title": "Codet5+: Open code large language models for code understanding and generation, 2023",
            "year": 2023
        },
        {
            "authors": [
                "C.S. Xia",
                "Y. Wei",
                "L. Zhang"
            ],
            "title": "Practical program repair in the era of large pre-trained language models",
            "venue": "arXiv preprint arXiv:2210.14179,",
            "year": 2022
        },
        {
            "authors": [
                "T. Zhang",
                "V. Kishore",
                "F. Wu",
                "K.Q. Weinberger",
                "Y. Artzi"
            ],
            "title": "Bertscore: Evaluating text generation with BERT",
            "year": 2020
        },
        {
            "authors": [
                "C. Zhou",
                "P. Liu",
                "P. Xu",
                "S. Iyer",
                "J. Sun",
                "Y. Mao",
                "X. Ma",
                "A. Efrat",
                "P. Yu",
                "L. Yu"
            ],
            "title": "Lima: Less is more for alignment",
            "venue": "arXiv preprint arXiv:2305.11206,",
            "year": 2023
        },
        {
            "authors": [
                "endline symbol [Hasan et al",
                "Mahmud"
            ],
            "title": "2021], our heuristic rules take a different approach. Instead of discarding such characters outright, we selectively remove the noisy elements while aiming to capture",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "The advent of deep learning and advancements in large language models (LLMs) have spurred a revolution in the field of code representation learning. These developments, supported by the growing accessibility of vast open-source code repositories, have heralded the emergence of code large language models (CodeLLMs) for code generation and understanding tasks. The sheer volume of these repositories and the rich, unprocessed raw data they contain, serve as unparalleled resources for training LLMs. Consequently, current state-ofthe-art models for coding tasks effectively utilize\n\u2217Equal contribution\nthese expansive datasets for training. However, it is important to note that these datasets, including The Stack [Kocetkov et al., 2022] and The Pile [Gao et al., 2020a], often comprise unprocessed data.\nAlternatively, there are established datasets, such as CONCODE [Iyer et al., 2018b], FunCom [LeClair et al., 2019], Deepcom [Hu et al., 2020] for code summarization tasks; APPS [Hendrycks et al., 2021] for text-to-code generation; and CodeSearchNet [Husain et al., 2019] for code search. These datasets contain carefully curated code-text pairs. Although considerably smaller in comparison to raw code datasets (e.g., 2.3M functions in CodeSearchNet [Husain et al., 2019] versus 197M files in The Stack [Kocetkov et al., 2022]), they provide high-quality code-text pairings that significantly enhance the effectiveness of model training.\nConsequently, we identify two main types of datasets used to train CodeLLMs: large yet unprocessed, and smaller yet well-structured (e.g., arranged into code-text pairs). The scaling law [Kaplan et al., 2020, Gordon et al., 2021, Sorscher et al., 2022] indicates that the volume of training data is crucial for model performance. However, other studies underscore the importance of dataset quality over quantity in training superior LLMs [Zhou et al., 2023, Sorscher et al., 2022, Dau et al., 2022, Brown et al., 2020, Khan et al., 2020]. Given these observations, we propose that an ideal dataset for training CodeLLMs should combine both elements: it should be expansive in volume and meticulously processed to ensure quality.\nIn this paper, we present The Vault dataset, detailing its creation process, the toolkit developed\nfor constructing and quality-controlling code-text pairs from raw source code, as well as an analysis of The Vault\u2019s metrics. We also share empirical results obtained from utilizing The Vault to finetune well-known foundational models. Our specific contributions include the following:\n\u2022 A dataset with approximately 43M pairs of highquality code-text pairs (over 10 times larger than CoDesc), 243M unimodal samples, and 69M pairs of line comments with context from 10 popular programming languages (Java, JavaScript, Python, Ruby, Rust, Golang, C#, C++, C, PHP), more diverse than CodeSearchNet, which has six programming languages.\n\u2022 A novel approach to use a pre-trained language model for detecting and removing noisy samples to complement traditional rule-based methods.\n\u2022 A thorough process for transforming raw source code into code-text pairs and filtering noisy samples. We have released the toolkit used in this process to the open community via a public GitHub repository1, including tools for parsing code and docstrings in different programming languages.\n\u2022 We perform extensive evaluation where we finetuned different CodeLLMs with The Vault compared to other datasets, such as CodeSearchNet on various code understanding tasks, including code generation, code summarization and code search. The results show that models finetuned on The Vault outperform those fine-tuned on CodeSearchNet (code summarization, code search) and outperform the original model by a significant margin (code generation on pass@k over HumanEval and MBPP datasets)."
        },
        {
            "heading": "2 Related works",
            "text": "Code Large Language Models for Understanding and Generation Code large language models facilitate various code understanding and code generation tasks, including but not limited to code generation [Feng et al., 2020a, Wang et al., 2023, Elnaggar et al., 2021, To et al., Luo et al., 2023, Shen et al., 2023], code completion [Feng et al., 2020a, Wang et al., 2023, Peng et al., 2021], program repair [Xia et al., 2022], program classification [Bui et al., 2021a,c,b] and code translation [Roziere\n1https://github.com/FSoft-AI4Code/ TheVault\net al., 2020, Bui et al., 2019]. A significant portion of recent research employs language models, originally developed for natural language processing, for handling code [Feng et al., 2020a, Wang et al., 2023, Guo et al., Ahmad et al., 2021b, Bui et al., 2021b, Elnaggar et al., 2021, Peng et al., 2021, Kanade et al., 2020, Chakraborty et al., 2022, Ahmed and Devanbu, 2022, Niu et al., 2022]. Such approaches largely regard code as analogous to text and adapt pretraining strategies that mirror those used for natural languages. CodeBERT [Feng et al., 2020a], for instance, modifies a Roberta model [Liu et al., 2019] to pretrain a code model on multiple programming languages. CodeT5 [Wang et al., 2021] and CodeT5+ [Wang et al., 2023] employs unique identifier information from source code to pretrain the T5 model [Raffel et al., 2019] for code in a multi-modal fashion."
        },
        {
            "heading": "Datasets for Code Representation Learning:",
            "text": "Code is commonly represented in training datasets for foundational LLMs, including the ROOTS corpus [Laurenc\u0327on et al., 2023] for training BLOOM [Scao et al., 2022] and The Pile [Gao et al., 2020a] for training LLaMA [Touvron et al., 2023]. The code data represented in these datasets are unlabeled raw source code from GitHub. There is also a family of code-only datasets for training or finetuning coding-specific LLMs, including The Stack [Kocetkov et al., 2022], a 3TB corpus of permissively licensed source code, preceded by CodeParrot with 50GB of deduplicated source code [Tunstall et al., 2022]. These massive datasets are usually used to train CodeLLMs. However, labeled data are required for training and evaluating LLMs for coding tasks involving source code and natural language descriptions. CodeXGLUE is a benchmark dataset Lu et al. [2021] for 10 coding tasks that include 14 subsets, four of which are code-text pairs. Most of the code-text pairs in CodeXGLUE come from CodeSearchNet.\nCodeSearchNet (CSN) has also been employed for pretraining LLMs, enabling supervised learning techniques to achieve state-of-the-art performance for models such as CodeT5+ [Wang et al., 2023] and UniXcoder [Guo et al., 2022]. A few codetext pair datasets set out to surpass CSN in size. CoDesc combines existing parallel datasets (CSN, DeepCom [Hu et al., 2020], CONCODE [Iyer et al., 2018a], and FunCom [LeClair et al., 2019]), and then refines the results from the superset, which yielded 4.2M Java data samples. PyMT5 [Clement\net al., 2020] is a dataset with 7.7M Python codetext. However, both of these datasets each contains code for a single programming language. Notable datasets created from Stack Overflow 2 include the necessary code-text data for generating post titles [Gao et al., 2020b, Liu et al., 2022]."
        },
        {
            "heading": "3 The Vault dataset",
            "text": ""
        },
        {
            "heading": "3.1 Overview",
            "text": "In The Vault, we leverage a subset of The Stack [Kocetkov et al., 2022], recognized as the most expansive publicly available, multilingual, permissive-licensed source code dataset weighing in at 3TB. From this large-scale dataset, The Vault transforms raw source code into a collection of high quality pairs of code and text. Our transformation pipeline is designed to efficiently extract data from source code, create text-code pairings, and remove noise, yielding three distinct output datasets, as detailed in Figure 2. We draw from a subset of The Stack, which comprises code in 10 prevalent programming languages, such as C, C#, C++, Java, JavaScript, GoLang, PHP, Python, Ruby, and Rust (out of the total 300 languages featured in The Stack). Each language-specific raw source code feeds into a custom-built tree-sitter3 parser. This parser is designed to extract functions, classes, methods, block code snippets, and their corresponding block or inline comments. The figure 1 illustrated a basic structure of a code file that contains multiple levels of code snippets. By applying a breadth-first search on the Abstract Syntax Tree (AST) of the root node, the parser is able to traverse down different node and leaf levels (class, function, and inline), result three separate datasets:\n1. The first output dataset, referred to as Dpaired, contains pairs of classes (node 1) and functions (node 3) with corresponding block comments that serve as docstrings (node 2). After the initial construction, this dataset proceeds through a pipeline that employs both rule-based filters and neural-based filters to remove noisy samples that fail to meet the criteria detailed in Section 3.2.\n2. The second output dataset, denoted as Dunimodal, consists of standalone functions and classes, not 2https://stackoverflow.com/ 3https://tree-sitter.github.io/ tree-sitter/\npaired with any docstring or comments, thereby forming a unimodal dataset.\n3. The third and final dataset, Dblock, includes pairs of arbitrary code blocks (node 4) and inline comments (node 5). To construct this set, we capture all inline comments. Each comment is paired with the preceding code block, tagged as the \u201cprevious context\u201d (node 4a), and the following code block, \u201cnext context\u201d (node 4b).\nA large number of block comments adhere to widely accepted docstring formats (Appendix A.5), encompassing neatly organized details about the name (identifier) of the associated function or class, their parameters, arguments, and return types. We channel these block comments through docstring parsers, which we have developed and made publicly available, to extract this information as metadata for each sample in our dataset. We contend that this metadata could prove beneficial for downstream tasks, prompt settings, and other applications (Figure 8). Collectively, these three datasets (Dblock, Dunimodal, and Dpaired) constitute The Vault. Note that through the evaluation process, only Dpaired is used since its contains data that is suitable for training and comparison with other datasets."
        },
        {
            "heading": "3.2 Data Cleaning Pipeline",
            "text": "From preliminary survey of the output dataset containing pairs of classes and functions with their corresponding block comments Dpaired, we observe salient patterns that would impair the training quality for code related tasks. We implemented a set of rule-based filters (Section 3.2.1) to remove irrelevant information or reformat textual data to be more descriptive of the corresponding code block. To address cases where the code-text pairs have inadequate or erroneous semantic correlation, we trained a neural-based model based on CodeBERT (Section 3.2.2) to serve as a filter. Such a filter generates a score, which is used to assess the alignment of a pair of code and text. Low-scoring samples are assumed to be unaligned and will be removed."
        },
        {
            "heading": "3.2.1 Remove Noisy Sample by Rules",
            "text": "Our data pipeline employs 13 rule-based filters to eliminate noisy patterns in the source dataset. These filters, detailed in Table 1, are categorized into three main groups: enhancing readability, promoting consistency, and preserving the intended usage of the code.\nIn terms of readability, we strip delimiters, math formulas, HTML tags, and metadata tags from the text. This ensures a cleaner and more coherent code-text pairing. For consistency, we remove elements that may cause irregularities in the dataset. This includes stripping hyperlinks and embedded code, and removing empty comments, overly short or long comments, non-English comments, autogenerated blocks, and work-in-progress comments. Lastly, to preserve the original purpose of the code, we remove comments that are questions or serve as examples or notes. This rigorous filtering process guarantees a high-quality dataset, improving the effectiveness of code-focused language models."
        },
        {
            "heading": "3.2.2 Remove Low-Quality Samples with Neural-based Classifier",
            "text": "Beyond the use of rule-based filtering methods, a crucial question arises: how do we ensure alignment between code and text? Random comments unrelated to the functionality of the code snippet can contaminate the dataset, necessitating the removal of such misaligned samples to guarantee quality. To address this issue, we constructed a classifier utilizing CodeBERT [Feng et al., 2020b], de-"
        },
        {
            "heading": "Categories Percentage (%)",
            "text": "signed to score the semantic relationship between a function or class and its corresponding docstring.\nIn our scoring model, we input code snippets and docstrings separated by a token < /s >. Approximately 12% of the already rule-filtered code-text pairs dataset was randomly selected for training.\nAs labeled data was unavailable, we generated negative samples by randomly pairing functions and docstrings within the same programming language. We then passed the representation of the < s > token to a linear layer, which produced a semantic correlation score between 0.0 and 1.0. Code-text pairs were then filtered using a binary classification gate with a threshold of 0.5.\nTo validate our model, we employed GPT 3.5 for analogous predictions. A million predictions were generated from unseen instances, from which we selected 300 per language: 200 high-confidence instances (100 consistent and 100 inconsistent code-text predictions) and 100 low-confidence instances. GPT 3.5-turbo was instructed to assign a consistency score (1-10) for each instance\u2019s codedocstring pair, serving as a benchmark for our model\u2019s predictions. For high-confidence instances, our model agreed with the GPT 3.5-turbo scores over 80% of the time. Although our model faced challenges with ambiguous samples, the Area Un-\nder the Curve (AUC) metric proved suitable due to our primary goal of excluding misalignments while preserving matched examples. An average AUC of 0.89 indicates that our approach effectively reduced dataset noise without discarding numerous informative samples. Detailed configurations and evaluation results are available in Appendix A.2.\nIn addition, we use our model to find noisy examples in the rule-based noise-remove version of CodeSearchNet in CodeXGlue. Table 3 presents some inconsistent examples found by our model for Python, Java, JavaScript, and PHP in CSN. It can be observed that detected pairs show strong inconsistency between docstring and code. For instance, the docstring of the example in Python does not give much insight into what the code does or its purpose. The code defines a method named \u2018has url\u2019 which checks if the attributes have a non-empty value; however, the docstring mentions templates which does not provide enough context to fully understand how this code relates to templates or\nits broader purpose. Besides, our model is able to identify non-English samples, which are presented in the example of PHP, that are not captured by the rule-based methods."
        },
        {
            "heading": "4 Empirical Evaluation",
            "text": "In this section, we aim to assess the quality of The Vault in comparison with other datasets, such as CSN. To substantiate this quality, we fine-tune prominent CodeLLMs on tasks that necessitate the involvement of both code and text, including code summarization, code search, and code generation. We then compare these models, which have been fine-tuned on The Vault, with those fine-tuned on CSN. The comparison is made using the same test\ndatasets and commonly employed metrics, such as MRR, smoothed BLEU [Lin and Och, 2004], and pass@k [Chen et al., 2021]."
        },
        {
            "heading": "4.1 Dataset Statistics",
            "text": "Table 2 provides the statistics of the samples for each programming language after undergoing our data-cleaning pipeline. In total, we have approximately 34M samples. The table also includes other information, like the number of tokens for code and docstrings, and the quantity of repositories.\nTable 4 offers a comparison between The Vault and other parallel datasets frequently used for pretraining and fine-tuning downstream tasks. These\ndatasets include Funcom [LeClair and McMillan, 2019], Deepcom [Hu et al., 2020], CONCODE [Iyer et al., 2018b], CSN [Husain et al., 2019], CoDesc [Hasan et al., 2021], and non-public data used for pretraining [Clement et al., 2020, Ciurumelea et al., 2020, Wang et al., 2021].\nWe split the training set into two smaller subsets: the small set and the medium set that contain 5% and 20% of the full training set, respectively. To reduce data leakage during training, we employed the MinHash LSH technique [Zhu et al., 2023] to filter training instance clusters that are close to samples in the validation and test sets of CSN, HumanEval, and MBPP. Additionally, during dataset partitioning, we prevented content from the same repository from appearing in multiple sets, thereby avoiding any potential internal data leakage. A more detailed analysis of The Vault at the class and code block levels can be found in Appendix A.4."
        },
        {
            "heading": "4.2 Experiment Setup",
            "text": "Data splitting: During the experiment phase, The Vault (Dpaired) was split into three distinct datasets: training, validating, and testing sets. To avoid data leakage, we reinforced a policy where code samples from the same repository must all be in the same set. In the splitting algorithm, we also included as a goal the preservation of the token length distribution from The Vault\u2019s dataset in each subset.\nFor richer comparisons, the training set was further branched off to two smaller sets, the small and medium training sets, sampling 5% and 20% of the full training set, respectively. Details about\nexperiment data can be found in Table 5. Note that TheVault/small has a comparable size with CSN, making it fair to assess and compare the quality of these two datasets.\nBesides, in order to validate the efficiency of our processing pipeline, we conduct a comparison between the performance of models trained on The Stack (raw data) and The Vault (processed data). Specifically, we established three function-level subsets, each approximately the size of TheVault/small (\u22481.7M code-text instances). These subsets were created by randomly sampling the raw function-level dataset extracted from The Stack, without applying any filtering (referred to as raw/TheStack). We use three different seeds to sample raw/TheStack and report the average result. All experiments are conducted using 4 NVIDIA A100 GPUs.\nCode search: We select CodeBERT [Feng et al., 2020a], RoBERTa [Liu et al., 2019] and UniXCoder [Guo et al., 2022] as the encoder for embedding source code and natural language query. We train each model for 10 epochs with a sequence max length of 512, and a learning rate of 2\u22125.\nCode summarization: CodeT5 [Wang et al., 2021] and PLBART [Ahmad et al., 2021a] are employed for the summarization task. We use the base versions and set the max input tokens to 512 and the max output tokens to 400. We train for 5 epochs with batch size of 512 and a learning rate of 2\u22124.\nCode generation: We use CodeGen 350M and 2B Multi [Nijkamp et al., 2023] to evaluate code generation. We use the same configuration as in the code summarization task."
        },
        {
            "heading": "4.3 Evaluation Results",
            "text": ""
        },
        {
            "heading": "4.3.1 Code Summarization",
            "text": "For this task, we utilize the Vault and CSN to fine-tune CodeT5 and PLBART to summarize the source code. The Vault and CSN exhibit significant differences in docstring format. The Vault retains the complete docstring format, offering comprehensive descriptions of core logic, parameters, arguments, and return types. This feature enables versatile applications in code documentation and various downstream tasks. Additionally, we save the first sentence of each complete docstring as metadata, termed as short docstring. To facilitate fair comparison between The Vault and CSN, we apply post-processing to our full docstrings and short docstrings training sets, thereby reducing format distribution disparity.\nTable 6 shows the results when comparing CodeT5 and PLBART trained on CSN and The Vault for the code summarization task, we report the best score when using full docstrings and short docstrings. We present further experimental outcomes using the Rouge-L [Lin, 2004] and BERTScore [Zhang et al., 2020] metrics in Appendix, Table 14. The results show that our pipeline has witnessed strong effectiveness compared to unprocessed data, raw/TheStack. Particularly, during training on the raw/TheStack dataset for the code summarization task, we found that the PLBART and CodeT5 generate outputs with substantial noise. These outputs are characterized by a prevalence of special tokens like \u201c//\u201d and \u201c*\u201d. This finding strongly underscores the efficacy of our filtering process in enhancing the quality of the dataset. However, the result using CSN shows superior performance on CSN\u2019s testset than using The Vault. The reason for this is our mention of the post-processing step to reduce the difference between the CSN and The Vault filtering methods, where the syntactic distribution can still exhibit nonidentical characteristics, which can affect the BLEU score. However, this gap could be reduced by using the full version of The Vault as shown in Table 14. Although the total performance gain when evaluated on the CSN test set is marginal (21.73 versus 21.24), it is worth noting that, despite the intermediary processing, CSN is a considerably smaller dataset with more consistent docstring patterns. In contrast, our dataset is substantially larger and exhibits greater diversity, thereby encouraging broader generalization. When evaluated against The Vault\u2019s test set, the model fine-tuned on CSN lags behind by over 10%.\nModel Fine-tune dataset pass@1 pass@10 pass@100\nHUMANEVAL\nCodeGen 350M - 6.67 10.61 16.84 Py/CodeSearchNet 2.76 8.76 14.72 (250K) Py/TheVault 3.74 10.57 16.26 raw/PyTheStack 6.64 15.42 24.80 Py/TheVault 8.14 18.12 30.07 CodeGen 2B - 14.51 24.67 38.56 Py/TheVault 14.00 25.74 41.72\nMBPP"
        },
        {
            "heading": "4.3.2 Code Search",
            "text": "We utilize CodeBERT, RoBERTa and UniXCoder to fine-tune both The Vault and CSN for the purpose of the code search task. We also furnish a baseline Mean Reciprocal Rank (MRR) score. MRR is a widely used metric for evaluating code search tasks, and in our case, it is trained on 10 different programming languages and assessed using the test set from CSN and The Vault. The results of this task, when fine-tuning the model on The Vault and CSN, are illustrated in Table 7. Remarkably, we attain superior results in most languages when finetund using the smallest dataset, TheVault/small, in contrast to solely fine-tuning on the CSN corpus. Surprisingly, RoBERTa, a model pretrained on natural language text, outperforms the two codepretrained models when evaluated on code search. This could imply the importance of natural language text representation over code representation in this task. Furthermore, models trained on The Vault consistently outperform all baseline models trained on raw/TheStack, underscoring both the efficiency of our processing pipeline and the dataset\u2019s ability to generalize across different architectures."
        },
        {
            "heading": "4.4 Code Generation",
            "text": "We experiment with two versions of CodeGen Multi [Nijkamp et al., 2023], which are 350M and 2B models on the HumanEval and MBPP benchmarks for code generation. The scope of our experiment was limited because the benchmarks only support Python. We use these checkpoints and continue fine-tuning them on The Vault because CodeGen Multi models are trained on the dataset with multiple languages.\nTo create Py/CodeSearchNet and Py/TheVault, we use the Python subsets of CSN and TheVault, respectively. We sampled the training Python set of\nTheVault to match the size of the Python subset in CSN with 250K samples in the first round of finetuning. Additionally, raw/PyTheStack is a subset of Python data from The Stack mirroring the size of Python data present in The Vault dataset, which helps us to demonstrate the advancements achieved in our data process pipeline.\nThe results are shown in Table 8. We can see that fine-tuning the CodeGen Multi 350M on The Vault causes the model to improve significantly in terms of pass@1, pass@10, and pass@100 on the HumanEval and MBPP benchmarks. Additionally, CodeGen 2B is used to assess The Vault on larger scale models. Similar to experiments on small models, Table 8 shows that The Vault can improve the performance of pretrained large-scale models. These results validate The Vault\u2019s ability to improve the performance of pre-existing pretrained models. In the future, we will expand our evaluation to even larger scale models and assess The Vault\u2019s impact on them."
        },
        {
            "heading": "5 Conclusion",
            "text": "In this paper, we presented The Vault, a large dataset of high-quality code-text pairs from ten programming languages, with over 43 million samples. The Vault was carefully curated to ensure that each pair meets quality standards, with detailed and informative descriptions and consistent coding styles. Our analysis uncovered a number of intriguing patterns and trends that shed light on the characteristics of programming languages and coding practices. We believe that The Vault will be a valuable resource for researchers and practitioners in this rapidly evolving field, providing a solid foundation for developing novel approaches and advancing state-of-the-art code large language models."
        },
        {
            "heading": "Limitations",
            "text": "In our approach, we employed 13 heuristic and context-specific rule-based filters, curated from manual data observations. While these filters effectively mitigated noisy patterns, their deterministic nature precluded comprehensive generalizability. To address this, we supplemented these rules with a neural-based approach as described in Section 3.2.2. However, the absence of labeled training data necessitated pseudo-random sample generation, which could compromise model soundness and potentially eliminate quality code-text pairs. Although cross-validation with GPT 3.5-turbo occasionally revealed scoring inconsistencies, we believe that human labeling and model fine-tuning could further refine the dataset.\nCompared to The Stack and The Pile, our dataset is smaller, mainly due to our rigorous quality control procedures. Moreover, creating AST parsers for each programming language is a non-trivial task, limiting our dataset to 10 popular programming languages compared to The Stack\u2019s 300. Nonetheless, our framework\u2019s codebase is publicly available, encouraging future contributions to extend our parsers and rules to additional languages.\nThe current study primarily utilized small models with less than 2 billion parameters to illustrate the value of The Vault. These models effectively demonstrated the dataset\u2019s potential, but further research with larger models would shed light on its robustness and scalability across more complex tasks. In future work, we plan to conduct experiments using large-scale language models to further assess the impact of our dataset."
        },
        {
            "heading": "A Appendix",
            "text": ""
        },
        {
            "heading": "A.1 Rule-based filters",
            "text": "While some datasets eliminate all special characters (!@#$%&*() -+=/.,\u2019\u2014 \u2018) and keep only the first sentence or the paragraph preceding the first double endline symbol [Hasan et al., 2021, Mahmud et al., 2021], our heuristic rules take a different approach. Instead of discarding such characters outright, we selectively remove the noisy elements while aiming to capture as many informative sections as possible.\nWe analyze each docstring block individually and retain the sections that meet our quality criteria. Table 9 provides comprehensive descriptions of our 13 rule-based filters, accompanied by illustrative examples. Additionally, table 10 presents the corresponding percentages of code-text pairs generated through the application of these rule-based filters."
        },
        {
            "heading": "A.2 Neural-based refinement method",
            "text": "To detect semantic inconsistency between code-text pairs, we considered fine-tuning on large foundational models such as CodeGen [Nijkamp et al., 2023], BLOOM [Scao et al., 2022] or leverage GPT 3.5-turbo APIs. However, these approaches would incur very high costs in terms of financial resources, time, and computational power. We decided to train a dedicated model to deal with this specific task and use GPT 3.5-turbo to cross-check the predictions.\nTraining: We trained our model based on CodeBERT, [Feng et al., 2020a]. The model assigns a score for semantic correspondence between code and text, before passing through binary classification into Consistent and Inconsistent categories. We randomly chose 5M samples (500K for each language in The Vault) and divided them into training, validation, and testing sets at a ratio of 3:1:1. The input to the model is the concatenation of the docstring and the code together with the < /s > token used to separate them (Figure 3). We use the representation of the < s > token and feed it into a linear layer to obtain the output logit.\nSince labeled data was unavailable, we utilized self-supervised learning. We created negative samples by randomly pairing a function with a docstring from the same programming language (Figure 3).\nCross-check: We used GPT 3.5-turbo to perform similar classifications for semantic consistency of code-text pairs. We used a prompting\ntemplate to ask GPT 3.5-turbo to score each pair of code-text on a scale of 1 to 10 for semantic correspondence with a detailed explanation and ran this prompting template on systematically selected 300 data points from each language with 100 data points in each of the following groups:\n\u2022 Consistency group: Examples that the model gives high confidence prediction to class Consistent. We select the top 100 based on the output probability for class 1.\n\u2022 Inconsistency group: Examples that the model gives high confidence prediction to class Inconsistent. We select the top 100 based on the output probability for class 0.\n\u2022 Uncertainty group: Examples that the model gives uncertain predictions. We select the lowest top 50 examples for each class.\nThe systematic sampling scheme helped us select 2994 samples in function level to be scored out of millions, reducing the cost of requesting GPT 3.5-turbo API while enabling meaningful analysis. The prompt input to GPT 3.5-turbo is as follow:"
        },
        {
            "heading": "I want you to act as an unbiased",
            "text": "docstring evaluator for code. I will give you a docstring along with a source code, and you will give me a score for the consistency between them. The score will be on a scale of 1 to 10, 10 means the docstring can effectively summarize the code while 1 means they are inconsistent. The response answers must contain the score and the explanation that follows the format in the response format.\n- Response format: Score: X Explanation: Y\n- Docstring: \"{docstring}\"\n- Code: \"{code}\"\nEmpirical Evaluation Results: Table 11 presents the performance of our model with GPT 3.5 turbo\u2019s scores as a reference, along with the scoring result for each group. In groups with high confidence, we witness a strong correlation between our model and GPT 3.5-turbo, with a high score for Consistency (7.81) and a low score for Inconsistency (3.15). A similar pattern is observed in the Uncertainty group, where the average score is close to the middle of the scale at 5.74.\nIn addition, we use GPT 3.5-turbo\u2019s scores to generate pseudo-labels and calculate accuracy and AUC for our model. We set a relative threshold of 5 to determine the labels. It can be witnessed that our model performs well in high-confidence groups but struggles in the uncertainty group. However, the accuracy is influenced by the choice of relative threshold, we consider Area Under the Curve (AUC) to measure the false positive and true positive rates. The metric shows a convincing result averaging 0.89, enabling us to effectively reduce a high amount of noise in our dataset while avoiding excluding too many informative examples. Finally, after removing noisy data using the proposed neural-based method, we notice a decrease of 1.3% in the dataset.\nWe use our model to find noisy examples in the rule-based noise-remove version of CodeSearchNet in CodeXGlue. Table 15 illustrates some examples found in 6 programming languages. It can be observed that detected pairs show strong inconsistency between docstring and code. For instance, the docstring of the first example in Python does not give much insight into what the code does or its pur-\npose. The code defines a method named \u2018has url\u2019 which checks if the attributes have a non-empty value; however, the docstring mentions templates which does not provide enough context to fully understand how this code relates to templates or its broader purpose. A similar pattern also presents in the remaining examples. An example that provides more clarity is the second example in Ruby. The docstring describes a function with a \u2018YAML filePath\u2019 parameter, but the function itself does not actually have this parameter. Besides, our model is able to identify non-English samples (the second example in PHP) that are not captured by the rule-based method."
        },
        {
            "heading": "A.3 Analysis of Function-Level Data in The Vault",
            "text": "Detailed description of function level data in The Vault can be found in Figure 4."
        },
        {
            "heading": "A.3.1 Code and Docstring Analysis",
            "text": "Token Length Distribution: When training seqto-seq LLMs, maximum input and output lengths are typically required. By understanding the distribution of sequence lengths in the corpus, we can\nchoose appropriate input and output lengths for training. This can help improve the performance of training a language model and prevent the resulting LLMs from producing outcomes too short or too long for the intended use cases [Kaplan et al., 2020, Brown et al., 2020].\nOur tokenization process utilizes the tree-sitter framework to parse source code into nodes on an abstract syntax tree; each node is considered a token. For docstring tokenization, we tokenize by word and punctuation. The code and docstring tokens length distribution for each programming language is illustrated in Figure 5. The number of tokens present in a function (average of around 100 tokens) is considerably more than the number of tokens found in the docstrings (average of 15-30 tokens) that describe it. In particular, among the\n10 programming languages, C and C++ have the highest number of tokens in a function. This can be attributed to the fact that these languages are low-level languages, which typically require more code to perform a task when compared to higherlevel languages. In the case of docstrings, their number of tokens is determined not only by the naturalness of the description in practice but also by cleaning rules outlined in Section 3.2.1. From Figure 5-Right and Table 10, it can be observed that the docstrings in Java and C are lengthy but are slightly cleaned by update-action rules, indicating that the docstrings in these two languages are typically long and more detailed in practice. Meanwhile, the number of tokens of docstrings in C# is the lowest. The cleaning rules may have played a role, as a significant proportion of the samples in C# has been updated based on Comment Delimite (16,7%) and HTML Tags (17,15%) rules.\nTable 2 depicts the overall number of distinct tokens for each programming language. As our dataset contains extensive unique tokens, we believe that model training on The Vault can effectively handle unseen tokens. Besides, we find that multiple function names are reused due to the relatively small number of unique identifiers compared to the total number of functions in the dataset. This finding implies that even for humans, naming functions might be a difficult task.\nDocstring Styles: Alongside typical docstrings that provide brief descriptions of the source code, many adhere to formatting and style conventions like Google, Jsdoc, and reST styles, among others. Our toolkit, designed to parse docstrings and extract metadata into a dictionary, supports 11 prevalent docstring styles. The styles we support and the information we aim to extract are depicted in figures 10 and 8 in Appendix A.5. This rich dataset could inspire research on advanced problems, such as controlling docstring style during generation or crafting explanations for function parameters.\nFigure 9 provides statistics on the number of docstrings following a standard style. The data suggests that styled docstrings constitute a small fraction of the overall code-text dataset. One possible explanation is that our style detection rules are stringent, excluding docstrings with even minor syntax deviations, which might result in underestimating the number of docstrings adhering to a specific format. For styled docstrings, Figure 9-bottom presents the distribution of the number\nof extracted attributes for each programming language, with most having between 1 to 5 elements. We make our docstring-style parser available to the community to facilitate easy customization and enhancement."
        },
        {
            "heading": "A.4 Analyzing for Class and Inline Comment Set",
            "text": "In Table 12, we provide a statistical analysis of the number of classes and inline comments in both the raw set and the filtered set. Since the class structure is not defined in C and Go, we do not have their information to give in this table.\nInitially, we excluded a substantial number of class samples from the raw dataset that lacked docstrings. The remaining class-docstring pairs underwent additional processing. Since the nature of classes and functions is similar, their functionalities can be meaningfully defined by pairs of a code snippet and a docstring. However, one of the problems when constructing paired data for classcomment samples is the large code snippet length of the class structure. As a result, we set the maximum number of code tokens that a class can have to 5000. On average, the code-token length of the class set is approximately 500, which is around five times longer compared to the average token length in the function set, while the number of docstringtoken lengths is similar between the two sets, as shown in Figure 6. Each pair of class-docstring is also examined via a rule-based filtering process, as described in Section 3.2.1, serving as a sample point in Dpair dataset.\nIn the Dblock analysis, we initiate the initial formation of the sub-dataset by identifying and extracting inline comments within code functions. The extracted comments undergo a series of cleaning procedures similar to those applied to the docstrings (as discussed in Section 3.2.1). After eliminating noisy samples, we proceed to establish various intervals for the number of comment tokens, aiming to determine the optimal upper and lower bounds that yield high-quality collected comments. Our observations reveal that inline comments exceeding 15 tokens typically incorporate code snippets, while comments containing fewer than 3 tokens lack substantial meaningful information. Consequently, this interval serves as a filtering criterion to generate the final version of Dblock. Figure 7 shows the distribution of code-token length and docstring-token length in Dblock set."
        },
        {
            "heading": "A.5 Docstring Styling",
            "text": "A docstring is a string literal used as a form of documentation for a module, function, class, or method definition in programming languages. It is usually placed as the first statement in the code block (which can be inside or outside the code block itself) and enclosed by a comment delimiter (e.g.,\ntriple quotes (\u201c\u2018) or a star slash (\\*)). Depending on developer comment habit or docstring style format, docstrings can form two types: one-line docstrings and multi-line (or block) docstrings. A docstring can provide a concise summary of the functionality while also providing a detailed description of the code block, including its parameters, return values, exceptions, and other relevant information (as illustrated in Figure 8)\nThe primary purpose of a docstring is to provide clear, concise, and easily accessible documentation for a code block. Docstring styles are conventions followed while writing docstrings to ensure consistency, readability, and ease of understanding throughout a codebase. This has become a standard for clean code in the industry and has developers saving tons of time when it comes to understanding or (auto-)generating documentation (using Sphinx, Doxygen, etc).\nThere are several popular docstring styles, such as Google Style, NumPy Style, reStructuredText (reST) Style for Python programmers, JavaDoc Style or Doxygen for Java users, each with its own formatting rules, structure and target programming language (docstring style examples and preferred language are listed in Figure 10). The statistic for docstring style corresponding to function level is presented in Figure 9. We believe that information inside a docstring is extremely useful and can provide numerous advantages for various applications in the fields of AI for source code, such as providing more precise and relevant search results for code search and retrieval tasks, or the performance of code analysis or refactoring can be significantly improved while the identifier of a parameter and\nits corresponding docstring information is available. Furthermore, the presence of various data types allows for the exploration of scenarios such as continual learning [Van et al., 2022, Nguyen et al., 2023, Yadav et al., 2023] and multitask learning [Zhang et al., 2023], which have been lacking investigation in the context of source code data.\nA.6 Experimental results on code summarization\nWe report Rouge-L, BERTScore, and BLEU-4 metrics on test sets of CSN and The Vault in Table 14. The results obtained from the experiments clearly indicate that models trained on our dataset consistently outperform CSN on all three evaluation metrics. This notable improvement across the metrics serves as strong evidence for the syntactic and semantic richness embedded within our dataset for code summarization. This highlights the effectiveness of our dataset in enabling models to grasp contextual information and generate high-quality summaries that accurately represent the underlying code functionality.\nA.7 Experimental results on code search In this section, we assess TheVault\u2019s versatility and adaptability by providing additional experimental results on several architectures (RoBERTa [Liu et al., 1907], UniXcoder [Guo et al., 2022], PLBART [Ahmad et al., 2021a]) for code search. Tables 13 illustrates the results for code search. As a result, models trained on The Vault consistently outperform all baseline models, underscoring both the efficiency of our pipeline and the dataset\u2019s ability to generalize across different architectures.\nLanguages Inconsistent pairs\nPython\n// Handy for templates. def has_urls(self):\nif self.isbn_uk or self.isbn_us or self.official_url or self. notes_url:\nreturn True else:\nreturn False\n// compresses the waveform horizontally; one of // \u2018\u2018\"normal\"\u2018\u2018, \u2018\u2018\"resync\"\u2018\u2018, \u2018\u2018\"resync2\"\u2018\u2018 def phase_type(self, value):\nself._params.phase_type = value self._overwrite_lock.disable()\nGo\n// InWithTags, OutWithTags, Both, BothWithTags func Predicates(from Shape, in bool) Shape {\ndir := quad.Subject if in { dir = quad.Object } return Unique{NodesFrom{\nQuads: Quads{ {Dir: dir, Values: from}, }, Dir: quad.Predicate,\n}} }\n// select Surf ro PhomtomJS func (self *DefaultRequest) GetDownloaderID() int {\nself.once.Do(self.prepare) return self.DownloaderID\n}\nJava\n// supplied callback function. public boolean rm(Pipe pipe, IMtrieHandler func, XPub pub)\n{ assert (pipe != null); assert (func != null); return rmHelper(pipe, new byte[0], 0, 0, func, pub); }\n// only for change appenders public MapContentType getMapContentType(ContainerType\ncontainerType){ JaversType keyType = getJaversType(Integer.class); JaversType valueType = getJaversType(containerType.\ngetItemType()); return new MapContentType(keyType, valueType);\n}\nLanguages Inconsistent pairs\nJavaScript\n// we do not need Buffer pollyfill for now function(str){ var ret = new Array(str.length), len = str.length; while(len--) ret[len] = str.charCodeAt(len); return Uint8Array.from(ret); }\n// WeakMap works in IE11, node 0.12 function (fn, name) { function proxiedFn() { \u2019use strict\u2019; var fields = privates.get(this); // jshint ignore:line return fn.apply(fields, arguments);\n}\nObject.defineProperty(proxiedFn, \u2019name\u2019, { value: name, configurable: true });\nreturn proxiedFn; }\nPHP\n// -> NEW public function consumerId()\n{ if (isset($this->session->data[\u2019customer_id\u2019]) === true) {\nreturn $this->session->data[\u2019customer_id\u2019]; } return null;\n}\n// disini mo ba atur akan apa mo kamana private function _parse_routes()\n{ $uri=implode(\u2019/\u2019, $this->uri->segments());\nif (isset($this->router[$uri])) { return $this->_set_request(explode(\u2019/\u2019, $this->router [$uri])); }\nforeach ($this->router as $key \u2192 $val) { $key = str_replace(\u2019:any\u2019, \u2019.+\u2019, str_replace(\u2019:num\u2019,\n\u2019[0-9]+\u2019, $key));\nif (preg_match(\u2019#\u02c6\u2019.$key.\u2019$#\u2019, $uri)) { if (strpos($val, \u2019$\u2019) !== FALSE AND strpos($key,\n\u2019(\u2019) !== FALSE) { $val = preg_replace(\u2019#\u02c6\u2019.$key.\u2019$#\u2019, $val,\n$uri); }\nreturn $this->_set_request(explode(\u2019/\u2019, $val)); }\n}\n$this->_set_request($this->uri->segments()); }"
        }
    ],
    "title": "The Vault: A Comprehensive Multilingual Dataset for Advancing Code Understanding and Generation",
    "year": 2023
}