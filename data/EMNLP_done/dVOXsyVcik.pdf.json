{
    "abstractText": "Feature attribution scores are used for explaining the prediction of a text classifier to users by highlighting a k number of tokens. In this work, we propose a way to determine the number of optimal k tokens that should be displayed from sequential properties of the attribution scores. Our approach is dynamic across sentences, method-agnostic, and deals with sentence length bias. We compare agreement between multiple methods and humans on an NLI task, using fixed k and dynamic k. We find that perturbation-based methods and Vanilla Gradient exhibit highest agreement on most method\u2013method and method\u2013human agreement metrics with a static k. Their advantage over other methods disappears with dynamic ks which mainly improve Integrated Gradient and GradientXInput. To our knowledge, this is the first evidence that sequential properties of attribution scores are informative for consolidating attribution signals for human interpretation.",
    "authors": [
        {
            "affiliations": [],
            "name": "Jonathan Kamp"
        },
        {
            "affiliations": [],
            "name": "Lisa Beinborn"
        },
        {
            "affiliations": [],
            "name": "Antske Fokkens"
        }
    ],
    "id": "SP:29831178300afafc85fc00251d9e38233ca98c91",
    "references": [
        {
            "authors": [
                "Pepa Atanasova",
                "Jakob Grue Simonsen",
                "Christina Lioma",
                "Isabelle Augenstein."
            ],
            "title": "A diagnostic study of explainability techniques for text classification",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing",
            "year": 2020
        },
        {
            "authors": [
                "Giuseppe Attanasio",
                "Debora Nozza",
                "Eliana Pastor",
                "Dirk Hovy"
            ],
            "title": "Benchmarking post-hoc interpretability approaches for transformer-based misog",
            "year": 2022
        },
        {
            "authors": [
                "Giuseppe Attanasio",
                "Eliana Pastor",
                "Chiara Di Bonaventura",
                "Debora Nozza."
            ],
            "title": "ferret: a framework for benchmarking explainers on transformers",
            "venue": "Proceedings of the 17th Conference of the European Chapter of the Association for Computational Lin-",
            "year": 2023
        },
        {
            "authors": [
                "Jasmijn Bastings",
                "Sebastian Ebert",
                "Polina Zablotskaia",
                "Anders Sandholm",
                "Katja Filippova."
            ],
            "title": "will you find these shortcuts?\u201d a protocol for evaluating the faithfulness of input salience methods for text classification",
            "venue": "Proceedings of the 2022 Confer-",
            "year": 2022
        },
        {
            "authors": [
                "Oana-Maria Camburu",
                "Eleonora Giunchiglia",
                "Jakob Foerster",
                "Thomas Lukasiewicz",
                "Phil Blunsom."
            ],
            "title": "Can i trust the explainer? verifying post-hoc explanatory methods",
            "venue": "arXiv preprint arXiv:1910.02065.",
            "year": 2019
        },
        {
            "authors": [
                "Oana-Maria Camburu",
                "Tim Rockt\u00e4schel",
                "Thomas Lukasiewicz",
                "Phil Blunsom."
            ],
            "title": "e-snli: Natural language inference with natural language explanations",
            "venue": "Advances in Neural Information Processing Systems, 31.",
            "year": 2018
        },
        {
            "authors": [
                "Shuoyang Ding",
                "Philipp Koehn."
            ],
            "title": "Evaluating saliency methods for neural language models",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
            "year": 2021
        },
        {
            "authors": [
                "Alon Jacovi",
                "Yoav Goldberg"
            ],
            "title": "Towards faithfully interpretable nlp systems: How should we define and evaluate faithfulness",
            "venue": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,",
            "year": 2020
        },
        {
            "authors": [
                "S\u00e9rgio Jesus",
                "Catarina Bel\u00e9m",
                "Vladimir Balayan",
                "Jo\u00e3o Bento",
                "Pedro Saleiro",
                "Pedro Bizarro",
                "Jo\u00e3o Gama."
            ],
            "title": "How can i choose an explainer? an applicationgrounded evaluation of post-hoc explanations",
            "venue": "Proceedings of the 2021 ACM Conference on Fair-",
            "year": 2021
        },
        {
            "authors": [
                "Satyapriya Krishna",
                "Tessa Han",
                "Alex Gu",
                "Javin Pombra",
                "Shahin Jabbari",
                "Steven Wu",
                "Himabindu Lakkaraju."
            ],
            "title": "The disagreement problem in explainable machine learning: A practitioner\u2019s perspective",
            "venue": "arXiv preprint arXiv:2202.01602.",
            "year": 2022
        },
        {
            "authors": [
                "Jierui Li",
                "Lemao Liu",
                "Huayang Li",
                "Guanlin Li",
                "Guoping Huang",
                "Shuming Shi."
            ],
            "title": "Evaluating explanation methods for neural machine translation",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 365\u2013375.",
            "year": 2020
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Roberta: A robustly optimized bert pretraining approach",
            "venue": "arXiv e-prints, pages arXiv\u20131907.",
            "year": 2019
        },
        {
            "authors": [
                "Scott M Lundberg",
                "Su-In Lee."
            ],
            "title": "A unified approach to interpreting model predictions",
            "venue": "Advances in neural information processing systems, 30.",
            "year": 2017
        },
        {
            "authors": [
                "Andreas Madsen",
                "Siva Reddy",
                "Sarath Chandar."
            ],
            "title": "Post-hoc interpretability for neural nlp: A survey",
            "venue": "ACM Computing Surveys, 55(8):1\u201342.",
            "year": 2022
        },
        {
            "authors": [
                "Michael Neely",
                "Stefan F Schouten",
                "Maurits Bleeker",
                "Ana Lucic."
            ],
            "title": "A song of (dis) agreement: Evaluating the evaluation of explainable artificial intelligence in natural language processing",
            "venue": "HHAI2022: Augmenting Human Intellect, pages 60\u201378. IOS Press.",
            "year": 2022
        },
        {
            "authors": [
                "Girish Palshikar"
            ],
            "title": "Simple algorithms for peak detection in time-series",
            "venue": "Proc. 1st Int. Conf. Advanced Data Analysis, Business Analytics and Intelligence, volume 122.",
            "year": 2009
        },
        {
            "authors": [
                "Marco Tulio Ribeiro",
                "Sameer Singh",
                "Carlos Guestrin."
            ],
            "title": " why should i trust you?\" explaining the predictions of any classifier",
            "venue": "Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining, pages 1135\u2013",
            "year": 2016
        },
        {
            "authors": [
                "Victor Sanh",
                "Lysandre Debut",
                "Julien Chaumond",
                "Thomas Wolf."
            ],
            "title": "Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter",
            "venue": "arXiv preprint arXiv:1910.01108.",
            "year": 2019
        },
        {
            "authors": [
                "Avanti Shrikumar",
                "Peyton Greenside",
                "Anshul Kundaje."
            ],
            "title": "Learning important features through propagating activation differences",
            "venue": "Proceedings of the 34th International Conference on Machine Learning - Volume 70, ICML\u201917, page 3145\u20133153.",
            "year": 2017
        },
        {
            "authors": [
                "K Simonyan",
                "A Vedaldi",
                "A Zisserman."
            ],
            "title": "Deep inside convolutional networks: visualising image classification models and saliency maps",
            "venue": "Proceedings of the International Conference on Learning Representations (ICLR). ICLR.",
            "year": 2014
        },
        {
            "authors": [
                "Mukund Sundararajan",
                "Ankur Taly",
                "Qiqi Yan."
            ],
            "title": "Axiomatic attribution for deep networks",
            "venue": "International conference on machine learning, pages 3319\u2013 3328. PMLR.",
            "year": 2017
        },
        {
            "authors": [
                "Wayne A Taylor"
            ],
            "title": "Change-point analysis: a powerful new tool for detecting changes",
            "year": 2000
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Feature attribution scores are a glimpse behind the scenes of neural models, or at least that is the promise. Various interpretability methods have been developed that can generate attribution scores to interpret the degree to which a language model\u2019s features (tokens) contributed to the predicted label. However, attribution values from different methods can vary considerably even on the same instance (Madsen et al., 2022, i.a.). Contradictory interpretations cast doubts on their usefulness and reliability in a practical setting.\nWhen comparing attribution methods, the focus commonly lies on assessing agreement with human explanations (often referred to as plausibility (Jacovi and Goldberg, 2020)) and on agreement between methods (Neely et al., 2022; Krishna et al., 2022). However, the evaluation procedures have not been standardised yet and varying influential\nfactors such as the exact task, data, and model have led to contradictory conclusions. For example, Attanasio et al. (2022) find that perturbationbased methods better agree with human preferences than gradient-based methods, while Atanasova et al. (2020) report the reverse tendency.\nA factor that may also influence results but has been largely understudied is the impact of the chosen number of k most salient tokens that are taken into consideration. Previous analyses either use a fixed value of k or determine the most salient tokens based on a fixed threshold for the attribution values. A clear drawback of using a fixed number of k is that it can result in excluding tokens with scores close to the top-k and including tokens with low scores, disregarding significant score gaps with truly important tokens. Jesus et al. (2021) set k to a fixed value of six and examine the effect of visualizing the most salient features on decisionmaking accuracy, time, and agreement for a fraud analysis task. Bastings et al. (2022) set k to the low fixed values of 1 and 2 as their analysis focuses on the identification of specific shortcut cues in a closed experimental setup. Camburu et al. (2019) determine k using a fixed threshold and identify\ntokens as salient if their attribution is > 0.1. When visualizing attributions to users, they use constant values of k (5 and 10). Absolute attribution scores tend to decrease for sentences with a larger number of tokens which is not captured by their thresholdbased approach. Krishna et al. (2022) determine k as a 1/4 fraction of the average sentence length in the data and set it to 11.\nAs token-level agreement generally increases the closer k gets to the number of tokens in a sentence (see Appendix C for a visualisation of the sentence length bias), the evaluation of attribution methods is clearly top-k dependent. Li et al. (2020) indicate that intrinsic model evaluation is generally sensitive to different values of k but do not measure its effect on agreement. Both Neely et al. (2022) and Krishna et al. (2022) find consistent disagreement between attribution methods, but do not account for the influence of k.1\nContributions In this paper, we systematically explore the role of k on the observed method\u2013 method agreement and method\u2013human agreement for extracting explanations for a natural language inference task. For this purpose, we develop the new metric agreement@k. We propose to determine k dynamically for each method and each instance based on the sequential properties of the attribution profile. Our approach is inspired by methods for event detection and detects attribution peaks in a method-agnostic fashion that circumvents the sentence-length bias by allowing different values of k for each instance. We find that:\n\u2022 agreement at the token level is sensitive to different values of k and the effect varies across attribution methods;\n\u2022 determining k with respect to attribution profiles consolidates the disagreement between attribution methods, in particular for Integrated Gradient and GradientXInput.\nWe take a novel perspective on human attribution evaluation by interpreting it as a ranking task."
        },
        {
            "heading": "2 Experimental Setup",
            "text": "We fine-tune a model on a natural language inference task and analyze the agreement between feature attribution methods.\n1All analyses are available at: https://github.com/ jbkamp/repo-Dynamic-K.\nData We use the e-SNLI dataset with the default split of 549,361 instances for training, 9,842 for development, and 9,824 for testing (Camburu et al., 2018). Each instance consists of a premise, a hypothesis, and an output label that indicates the semantic relation between the premise and the hypothesis: contradiction, entailment, neutral. Each premise is paired with three hypotheses (one for each label) to obtain balanced classes. Instances span 21 tokens on average (5\u2013113). 6,325 annotators highlighted tokens that they found most important to explain the gold label (avg. number of highlighted tokens: 4\u00b13). We used the dev and test instances which were annotated by at least three annotators (we used dev for exploration, and test for our experiments).\nBackbone Model We fine-tune DistilBERT (Sanh et al., 2019) using ten different random seeds and select the median model, i.e., the model with the least variation in attribution profiles compared to the other nine models, for further analysis.2 The model yields a performance of 0.89 F1 on both the dev and test set.\nFeature Attribution We use the Ferret package v0.4.1 (Attanasio et al., 2023) to calculate attributions using the gradient-based methods Vanilla Gradient (Simonyan et al., 2014) and Integrated Gradient (Sundararajan et al., 2017), and the perturbation-based methods Partition SHAP (Lundberg and Lee, 2017) and LIME (Ribeiro et al., 2016). For the gradient-based methods, we use both the plain gradients and the gradientXInput version for which the gradient is multiplied by the input token embeddings (Shrikumar et al., 2017). Hence, we examine a total of six methods.\nEvaluation Recent studies evaluate feature attributions as a ranking task. Atanasova et al. (2020) calculate the mean average precision (MAP) compared to human labels and Bastings et al. (2022) restrict the evaluation to the top k ranks (MAP@k) but it remains an open question how k is selected.\nAn attribution method A assigns an attribution vector a = {a1, a2, ..., an} to a sentence consisting of tokens s = {w1, w2, ..., wn} so that each ai indicates the salience of token wi for the predicted output label. We determine the topkA = {t1, t2, ..., tk} by selecting the k tokens with the highest attribution values. We propose a new metric\n2See Appendix A for implementation details on the model selection.\nfor comparing m attribution methods A1, ..., Am by calculating sentence-level agreement@k based on token relevance. Relevance for a token wi is determined by the ratio of methods that include the token in the topk. A high relevance score indicates that a token is assigned high attribution by many of the compared methods.\nRelevance r(wi) =\nm\u2211 Aj=1 [wi \u2208 topkAj ]\nm (1)\nAgreement@k(si) =\nn\u2211 wi=1 r(wi)\nn\u2211 wi=1 [r(wi) > 0]\n(2)\nThis way, tokens not included in the top-k selection of any method are assigned a relevance score of zero and are not considered in the calculation of agreement@k. This approach avoids artificially inflating the agreement@k score by having high agreement on non-relevant tokens. The agreement@k for a dataset of sentences D = {s1, s2, . . . , sd} is computed by averaging the sentence-level agreement@k scores.\nAgreement@k(D) =\nd\u2211 si=1 agreement@k(si)\nd (3)\nWe evaluate our experiments comparing mean agreement@k between the six attribution methods on the test data of e-SNLI."
        },
        {
            "heading": "3 Agreement at Fixed k",
            "text": "We compare attribution methods with each other and with human labels and analyze the role of k.\nMethod\u2013Method Agreement We identify three groups of mean agreement@k across pairs of attribution methods in Figure 2a. The agreement between Partition SHAP\u2013LIME clearly stands out (yellow line), in particular for small k. All comparisons involving Integrated Gradient or GradientXInput end up in the group with the least agreement (purple lines) and the remaining pairs obtain medium-level agreement (green lines). Agreement increases for bigger k for pairs of methods with low to medium agreement. Our results contrast previous work which identified higher pairwise agreement between gradient-based methods compared to perturbation-based methods (Krishna et al., 2022).\nMethod\u2013Human Agreement We calculate token relevance for the three human annotators as the ratio of annotators who selected the token, therefore in the range [0, .33, .67, 1]. When we compare the attribution methods to human annotations, we find that the two perturbation-based methods lead to higher agreement than the gradientbased ones (Figure 2b) and that higher values of k generally lead to better agreement. Our findings are partly in line with Attanasio et al. (2022) in that perturbation-based methods are more plausible than most gradient-based methods, and partly with Atanasova et al. (2020) for finding that Vanilla Gradient agrees more with human rationales than perturbation-based methods which in turn agree more with human than most of the other gradientbased methods. We contrast Ding and Koehn (2021) who find higher plausibility for Integrated Gradient over Vanilla Gradient. While consistency in performance across studies sheds light on the interrelatedness between methods, it is important to exercise caution when generalizing evaluation results across different models, datasets, and tasks.\n4 Dynamic Top-k Estimation\nWe have seen that the value of k has a strong influence on the agreement between methods. Perturbation-based methods are more in line with human annotations for smaller settings of k while the attribution profiles of gradient-based methods require relatively larger settings of k to obtain a similar reflection of human preferences. We propose to determine the number of k salient tokens dynamically based on the attribution profiles of the methods.\nInspired by event detection in time series (Taylor (2000), Palshikar et al. (2009), e.g.), we consider attribution profiles as sequences of token-level scores that indicate the local presence or absence of a peak. Each peak is a point in the sequence (i.e., the sentence) to which the model attributed a higher salience compared to neighboring points. We apply peak detection based on local maxima in the attribution profile to estimate a k that is dynamic across method\u2013instance combinations. The attribution profile of each individual method thus serves as the indicator of its peaks. A local maximum is defined as a point xi in a sequence such that it is greater than both its immediate left neighbor, xi\u22121, and its immediate right neighbor, xi+1. We additionally enforce the constraint that xi needs to be higher than the mean attribution of the sequence, corresponding to above-average model behavior. With our dynamic k approach, we favor relative differences in attribution values over absolute thresholds for identifying the top-k tokens. Figure 3 shows\nFigure 3: Mean agreement@k: dynamic k (bottom left) and difference (top right) compared to fixed human average k = 4. The brighter the color, the higher the agreement.\nthat when determining k dynamically, the agreement of Integrated Gradient and GradientXInput with the other methods increases. We find that dynamic k indeed varies across instances and across methods. For example, attributions by Partition SHAP leads to fewer local maxima and therefore to lower values for dynamic k (4.5\u00b11.7 on average) compared to IntegratedGradient (7.3\u00b12.6). We note that these ranges are close to human preferences for k on the NLI task (4\u00b13)."
        },
        {
            "heading": "5 Further Insights",
            "text": "We briefly discuss complementary insights to the main results, as well as two examples of dynamic k on instances from the dataset.\nAverage Human Preference Human annotations of token-level explanations were available for this task and we compared the effect of dynamic k to the average number of annotations. However, these types of annotations are expensive, which often leads to selecting a fixed k. Normally, a valid approach is choosing a fixed k that is close to the average number of annotated tokens per sentence, as the resulting ranges of dynamic k for our methods reflected. In absence of annotations, this average number is unknown.\nDynamic k highlights the implications of choosing a fixed k that deviates from the average. We observe a clear distinction between lower and higher than average for GradientXInput and for Integrated Gradient. Table 1 reports the improvement on mean agreement@k by the dynamic approach over fixed values. Generally, the lower fixed k, the more improvement we observe. For k > 5 there is no improvement.\nk = 1 2 3 4 5\nGrad X I +.10 +.08 +.05 +.02 +.01 IntGrad +.01 +.04 +.02 +.01 +.00\nPeaks as Signals We analyse two examples to better understand the dynamic k approach. Figures\n1 and 4 show the application of dynamic k versus fixed k set to 3, 5, and 7 for two different methods.\nIn Figure 1, dynamic k highlights the same top three tokens that have been selected by fixed k, namely reading, paper and argue. Tokens that are only selected by k = 5 and k = 7 are not included as they do not form peaks in the attribution profile.\nFigure 4 illustrates differences in the attribution patterns captured by fixed k and dynamic k. While fixed k selects any tokens in descending order of attribution score, dynamic k captures signals. It highlights two peaks that were not captured by fixed k (due to their lower absolute values) because they stand out relative to the surrounding tokens. Signals can be described as tokens that represent a salient part or phrase in the sentence beyond the token level (e.g. a man in a suit; a lady). By focusing on signals, dynamic k skips tokens with a relatively high score but that can be attributed to the same signal of a neighboring salient token."
        },
        {
            "heading": "6 Discussion and Conclusion",
            "text": "Attribution methods disagree on the salience of tokens. Our analyses show that the observed level of agreement is sensitive to the number of k tokens taken into consideration. We propose a dynamic k that can be directly applied to any attribution profile. In contrast to fixed k, it takes local relative differences of the attribution values into account. Our analyses with dynamic k indicate that different attribution methods capture varying degrees of attribution scope. Determining dynamic k purely on attribution profiles yields a level of plausibility that is comparable to determining the average human\npreference for k and is therefore a viable alternative in the absence of task-specific human data. Furthermore, as dynamic k is estimated for each instance separately, it can account for sentence length bias.\nOur peak detection method is an intuitive approach for determining salient tokens based solely on attribution profiles. It focuses on isolated key tokens which might not adequately capture human tendencies of chunking words into phrases. In future work, we plan to analyze how our findings generalize to other task and model conditions and want to explore alternative methods to dynamically determine k by combining the attribution with linguistic information towards better span-level visualisations (see Figure 5). This line of research needs to be closely coupled with cognitive analyses of human preferences.\nLimitations\nIn this study, we encountered certain limitations that should be taken into account when interpreting the results and when conducting subsequent research. First, due to pragmatic constraints, we focused on a single model and a selected set of attribution methods for comparison, which restricts the direct generalisation of our findings to methods outside this set. However, the proposed metric agreement@k and the concept of dynamic k can be readily applied to evaluate other methods in future research. The scarce availability of multiple human rationales at the token level, necessary for creating human aggregation scores, limited our ability to expand the scope of this research. Furthermore, it is worth noting that the aggregation scores in our study fall within the range of [0, .33, .67, 1].\nConsequently, the precision of the overlap between detected human peaks may be compromised when the number of annotators is low. The resolution of ties in these scores was resolved randomly, which introduces a potential source of improvement and variability in the results. While these limitations should be acknowledged, they do not invalidate the overall contributions of our research. They provide valuable insights into the effectiveness of the selected methods and highlight avenues for future investigations, such as incorporating additional datasets.\nEthics Statement\nIn the field of interpretability, results need to be communicated with particular caution to avoid anthropomorphizing neural models. With respect to this study, caution should be exercised when interpreting findings from attribution methods. Attribution scores cannot be blindly relied upon to precisely determine model functioning, as they can be influenced by experimental factors such as task and model performance. To avoid drawing generalised conclusions, it is advisable to employ multiple metrics when studying feature attribution."
        },
        {
            "heading": "Acknowledgements",
            "text": "Jonathan Kamp\u2019s research was funded by the Dutch National Science Organisation (NWO) through the project InDeep: Interpreting Deep Learning Models for Text and Sound (NWA.1292.19.399). Antske Fokkens was supported by the EU Horizon 2020 project InTaVia: In/Tangible European Heritage - Visual Analysis, Curation and Communication (http://intavia.eu) under grant agreement No. 101004825. Lisa Beinborn\u2019s work was funded by the Dutch National Science Organisation (NWO) through the VENI program (Vl.Veni.211C.039). We would like to thank the anonymous reviewers for their valuable contribution."
        },
        {
            "heading": "A Average Pairwise Difference (APD)",
            "text": "The Average Pairwise Difference (APD) indicates how the sets of attribution scores yielded by different combinations of classification models with attribution methods differ from one another.\nFirst, it is possible to compute the Average Difference (AD) between two matrices T1 and T2 of the same size. AD is a measure of dissimilarity and provides an indication of the overall average magnitude of differences between the matrices. In our case, T1 and T2 are two attribution matrices and are constructed by concatenating the vectors of token-wise attribution scores a = {a1, a2, ..., an} (computed for all l attribution methods A) for each instance in our dataset of size d, after 0-padding as to maximum sentence length. More precisely, let T1 and T2 be two matrices of size (d \u2217 l)\u00d7m, where n is the number of sentences in the dataset, l is the number of attribution methods and m is maximum sentence length. The AD between matrices T1 and T2 is calculated by taking the average of the element-wise absolute differences between the corresponding elements of T1 and T2.\nWe then construct two attribution matrices and calculate AD for every pair of runs from the pool of 10 models each trained with a different random seed, assigning an APD score to each model by averaging the AD scores for that model\u2019s attribution matrix in pairwise relation to the other models\u2019 attribution matrices. The model with lowest APD (in bold in Table 2) was selected for our experiments."
        },
        {
            "heading": "B Fine-tuning and Analysis",
            "text": "The input instances for fine-tuning are premises and hypotheses concatenated by a single [SEP] token. We removed 6 instances from the training set where the hypothesis was missing. The main hyperparameters for our models are the following: 15 training epochs with early stopping, training batch size of 32, learning rate set to 5e-6, weight decay set to\n0.01 and warmup steps set to 6% of the total. We found that computing the attributions for a larger model such as RoBERTa (Liu et al., 2019) takes significantly longer and aligning the attributions with human annotated text is less straightforward for tokenisation reasons. When pre-processing the human annotations, we assign a 0 score to punctuation characters as they did not receive a dedicated annotation label."
        },
        {
            "heading": "C Sentence Length Bias",
            "text": ""
        }
    ],
    "title": "Dynamic Top-k Estimation Consolidates Disagreement between Feature Attribution Methods",
    "year": 2023
}