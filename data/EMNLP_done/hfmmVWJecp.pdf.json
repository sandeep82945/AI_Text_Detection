{
    "abstractText": "When natural language phrases are combined, their meaning is often more than the sum of their parts. In the context of NLP tasks such as sentiment analysis, where the meaning of a phrase is its sentiment, that still applies. Many NLP studies on sentiment analysis, however, focus on the fact that sentiment computations are largely compositional. We, instead, set out to obtain non-compositionality ratings for phrases with respect to their sentiment. Our contributions are as follows: a) a methodology for obtaining those non-compositionality ratings, b) a resource of ratings for 259 phrases \u2013 NONCOMPSST \u2013 along with an analysis of that resource, and c) an evaluation of computational models for sentiment analysis using this new resource.",
    "authors": [
        {
            "affiliations": [],
            "name": "Verna Dankers"
        },
        {
            "affiliations": [],
            "name": "Christopher G. Lucas"
        }
    ],
    "id": "SP:ffbee6f9691cfaf9faa7a7e47145300089c55786",
    "references": [
        {
            "authors": [
                "Jeremy Claude Barnes",
                "Lilja \u00d8vrelid",
                "Erik Velldal."
            ],
            "title": "Sentiment analysis is not solved! Assessing and probing sentiment classification",
            "venue": "Association for Computational Linguistics (ACL). Annual Meeting Conference Proceedings, pages 12\u201323. Associa-",
            "year": 2019
        },
        {
            "authors": [
                "Jose Camacho-collados",
                "Kiamehr Rezaee",
                "Talayeh Riahi",
                "Asahi Ushio",
                "Daniel Loureiro",
                "Dimosthenis Antypas",
                "Joanne Boisson",
                "Luis Espinosa Anke",
                "Fangyu Liu",
                "Eugenio Mart\u00ednez C\u00e1mara"
            ],
            "title": "TweetNLP: Cutting-edge natural language",
            "year": 2022
        },
        {
            "authors": [
                "Verna Dankers",
                "Ivan Titov."
            ],
            "title": "Recursive neural networks with bottlenecks diagnose (non-) compositionality",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2022, pages 4361\u20134378.",
            "year": 2022
        },
        {
            "authors": [
                "Jochen Hartmann",
                "Mark Heitmann",
                "Christina Schamp",
                "Oded Netzer."
            ],
            "title": "The power of brand selfies",
            "venue": "Journal of Marketing Research.",
            "year": 2021
        },
        {
            "authors": [
                "Jochen Hartmann",
                "Mark Heitmann",
                "Christian Siebert",
                "Christina Schamp."
            ],
            "title": "More than a feeling: Accuracy and application of sentiment analysis",
            "venue": "International Journal of Research in Marketing, 40(1):75\u201387.",
            "year": 2023
        },
        {
            "authors": [
                "Dieuwke Hupkes",
                "Mario Giulianelli",
                "Verna Dankers",
                "Mikel Artetxe",
                "Yanai Elazar",
                "Tiago Pimentel",
                "Christos Christodoulopoulos",
                "Karim Lasri",
                "Naomi Saphra",
                "Arabella Sinclair"
            ],
            "title": "A taxonomy and review of generalization research in NLP",
            "venue": "Nature",
            "year": 2023
        },
        {
            "authors": [
                "Alyssa Hwang",
                "Christopher Hidey."
            ],
            "title": "Confirming the non-compositionality of idioms for sentiment analysis",
            "venue": "Proceedings of the Joint Workshop on Multiword Expressions and WordNet (MWE-WN 2019), pages 125\u2013129.",
            "year": 2019
        },
        {
            "authors": [
                "Svetlana Kiritchenko",
                "Saif Mohammad."
            ],
            "title": "Happy accident: A sentiment composition lexicon for opposing polarity phrases",
            "venue": "Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC\u201916), pages 1157\u2013",
            "year": 2016
        },
        {
            "authors": [
                "Svetlana Kiritchenko",
                "Saif Mohammad."
            ],
            "title": "Sentiment composition of words with opposing polarities",
            "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
            "year": 2016
        },
        {
            "authors": [
                "Svetlana Kiritchenko",
                "Saif M Mohammad."
            ],
            "title": "The effect of negators, modals, and degree adverbs on sentiment composition",
            "venue": "Proceedings of NAACLHLT, pages 43\u201352.",
            "year": 2016
        },
        {
            "authors": [
                "Dan Klein",
                "Christopher D Manning."
            ],
            "title": "Accurate unlexicalized parsing",
            "venue": "Proceedings of the 41st annual meeting of the association for computational linguistics, pages 423\u2013430.",
            "year": 2003
        },
        {
            "authors": [
                "Zolt\u00e1n K\u00f6vecses",
                "Zolt\u00e1n K\u00f6vecses."
            ],
            "title": "The concept of emotion: The container metaphor",
            "venue": "Emotion Concepts, pages 144\u2013159.",
            "year": 1990
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Roberta: A robustly optimized BERT pretraining approach",
            "venue": "arXiv preprint arXiv:1907.11692.",
            "year": 2019
        },
        {
            "authors": [
                "Daniel Loureiro",
                "Francesco Barbieri",
                "Leonardo Neves",
                "Luis Espinosa Anke",
                "Jose Camacho-Collados."
            ],
            "title": "TimeLMs: Diachronic language models from Twitter",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics:",
            "year": 2022
        },
        {
            "authors": [
                "Andrew Maas",
                "Raymond E Daly",
                "Peter T Pham",
                "Dan Huang",
                "Andrew Y Ng",
                "Christopher Potts."
            ],
            "title": "Learning word vectors for sentiment analysis",
            "venue": "Proceedings of the 49th annual meeting of the association for computational linguistics: Human language",
            "year": 2011
        },
        {
            "authors": [
                "Karo Moilanen",
                "Stephen Pulman."
            ],
            "title": "Sentiment composition",
            "venue": "Proceedings of the Recent Advances in Natural Language Processing International Conference, pages 378\u2013382. Association for Computational Linguistics.",
            "year": 2007
        },
        {
            "authors": [
                "Preslav Nakov",
                "Alan Ritter",
                "Sara Rosenthal",
                "Fabrizio Sebastiani",
                "Veselin Stoyanov."
            ],
            "title": "Semeval-2016 task 4: Sentiment analysis in twitter",
            "venue": "Proceedings of SemEval, pages 1\u201318.",
            "year": 2016
        },
        {
            "authors": [
                "Dat Quoc Nguyen",
                "Thanh Vu",
                "Anh Tuan Nguyen."
            ],
            "title": "BERTweet: A pre-trained language model for English Tweets",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 9\u201314.",
            "year": 2020
        },
        {
            "authors": [
                "Bo Pang",
                "Lillian Lee."
            ],
            "title": "Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales",
            "venue": "Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL\u201905), pages 115\u2013124.",
            "year": 2005
        },
        {
            "authors": [
                "Juan Manuel P\u00e9rez",
                "Juan Carlos Giudici",
                "Franco Luque."
            ],
            "title": "pysentimiento: A python toolkit for sentiment analysis and SocialNLP tasks",
            "venue": "arXiv preprint arXiv:2106.09462.",
            "year": 2021
        },
        {
            "authors": [
                "Livia Polanyi",
                "Annie Zaenen."
            ],
            "title": "Contextual valence shifters",
            "venue": "Computing attitude and affect in text: Theory and applications, pages 1\u201310.",
            "year": 2006
        },
        {
            "authors": [
                "Peng Qi",
                "Yuhao Zhang",
                "Yuhui Zhang",
                "Jason Bolton",
                "Christopher D Manning."
            ],
            "title": "Stanza: A python natural language processing toolkit for many human languages",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics:",
            "year": 2020
        },
        {
            "authors": [
                "Sara Rosenthal",
                "Noura Farra",
                "Preslav Nakov."
            ],
            "title": "Semeval-2017 task 4: Sentiment analysis in twitter",
            "venue": "Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017), pages 502\u2013 518.",
            "year": 2017
        },
        {
            "authors": [
                "Sara Rosenthal",
                "Preslav Nakov",
                "Svetlana Kiritchenko",
                "Saif Mohammad",
                "Alan Ritter",
                "Veselin Stoyanov."
            ],
            "title": "Semeval-2015 task 10: Sentiment analysis in twitter",
            "venue": "Proceedings of the 9th international workshop on semantic evaluation (SemEval 2015),",
            "year": 2015
        },
        {
            "authors": [
                "Richard Socher",
                "Alex Perelygin",
                "Jean Wu",
                "Jason Chuang",
                "Christopher D Manning",
                "Andrew Y Ng",
                "Christopher Potts."
            ],
            "title": "Recursive deep models for semantic compositionality over a sentiment treebank",
            "venue": "Proceedings of the 2013 conference on empiri-",
            "year": 2013
        },
        {
            "authors": [
                "Alexander Sutherland",
                "Suna Bensch",
                "Thomas Hellstr\u00f6m",
                "Sven Magg",
                "Stefan Wermter."
            ],
            "title": "Tell me why you feel that way: Processing compositional dependency for Tree-LSTM aspect sentiment triplet extraction (TASTE)",
            "venue": "Artificial Neural Networks and",
            "year": 2020
        },
        {
            "authors": [
                "Maite Taboada",
                "Julian Brooke",
                "Milan Tofiloski",
                "Kimberly Voll",
                "Manfred Stede."
            ],
            "title": "Lexicon-based methods for sentiment analysis",
            "venue": "Computational linguistics, 37(2):267\u2013307.",
            "year": 2011
        },
        {
            "authors": [
                "Bashar MA Tahayna",
                "Ramesh Kumar Ayyasamy",
                "Rehan Akbar."
            ],
            "title": "Automatic sentiment annotation of idiomatic expressions for sentiment analysis task",
            "venue": "IEEE Access, 10:122234\u2013122242.",
            "year": 2022
        },
        {
            "authors": [
                "Kai Sheng Tai",
                "Richard Socher",
                "Christopher D Manning."
            ],
            "title": "Improved semantic representations from tree-structured long short-term memory networks",
            "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th",
            "year": 2015
        },
        {
            "authors": [
                "Alex Wang",
                "Amanpreet Singh",
                "Julian Michael",
                "Felix Hill",
                "Omer Levy",
                "Samuel R Bowman."
            ],
            "title": "GLUE: A multi-task benchmark and analysis platform for natural language understanding",
            "venue": "International Conference on Learning Representations.",
            "year": 2018
        },
        {
            "authors": [
                "Zengzhi Wang",
                "Qiming Xie",
                "Zixiang Ding",
                "Yi Feng",
                "Rui Xia."
            ],
            "title": "Is ChatGPT a good sentiment analyzer? A preliminary study",
            "venue": "arXiv preprint arXiv:2304.04339.",
            "year": 2023
        },
        {
            "authors": [
                "Da Yin",
                "Tao Meng",
                "Kai-Wei Chang."
            ],
            "title": "SentiBERT: A transferable transformer-based architecture for compositional sentiment semantics",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3695\u20133706.",
            "year": 2020
        },
        {
            "authors": [
                "Wlodek Zadrozny."
            ],
            "title": "From compositional to systematic semantics",
            "venue": "Linguistics and philosophy, 17:329\u2013 342.",
            "year": 1994
        },
        {
            "authors": [
                "Xiaodan Zhu",
                "Hongyu Guo",
                "Saif Mohammad",
                "Svetlana Kiritchenko."
            ],
            "title": "An empirical study on the effect of negation words on sentiment",
            "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Pa-",
            "year": 2014
        },
        {
            "authors": [
                "Xiaodan Zhu",
                "Hongyu Guo",
                "Parinaz Sobhani."
            ],
            "title": "Neural networks for integrating compositional and non-compositional sentiment in sentiment composition",
            "venue": "Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics, pages 1\u20139.",
            "year": 2015
        },
        {
            "authors": [
                "Socher"
            ],
            "title": "The sentences were parsed with the Stanford Parser (Klein and Manning, 2003) to allow for sentiment annotations of phrases in addition to full sentences. The dataset includes sentiment labels for all nodes of those parse trees. The sentiment labels were obtained from Amazon Mechanical Turk annotators, who indicated the sentiment using a multi-stop slider bar with 25 ticks",
            "year": 2013
        },
        {
            "authors": [
                "Socher"
            ],
            "title": "The constituents do not contain a named entity, such as a movie star or the name of a film. Not all participants may know these names and the associated sentiment, hence we opt for excluding them. 4) The standard deviation of the original SST annotations lies within a range of five (on the original 25-point scale)",
            "year": 2013
        },
        {
            "authors": [
                "Hupkes"
            ],
            "title": "2023): our experiments are cognitively motivated and focus on (non-)compositional generalisation. By separating inputs based on compositionality, we create a covariate shift between train and test data. That shift has a partitioned natural source, since the stimuli are from a human-written corpus, but the split we create is curated",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "In NLP, the topics of the compositionality of language and neural models\u2019 capabilities to compute meaning compositionally have gained substantial interest in recent years. Yet, the meaning of linguistic utterances often does not adhere to strict patterns and can be surprising when looking at the individual words involved. This affects how those utterances behave in downstream tasks, such as sentiment analysis. Given a phrase or sentence, that task involves predicting the polarity as positive, negative or neutral. Sentiment largely adheres to compositional principles (Moilanen and Pulman, 2007, p.1): \u201cIf the meaning of a sentence is a function of the meanings of its parts then the global polarity of a sentence is a function of the polarities of its parts.\u201d Modelling sentiment as a compositional process is, therefore, often mentioned as a design principle for computational sentiment models (e.g. by Socher et al., 2013; Sutherland et al., 2020; Yin et al., 2020).\nNonetheless, one can think of examples where the sentiment of a phrase is unexpected given the\nsentiment of the individual parts (e.g. see Zhu et al., 2015; Hwang and Hidey, 2019; Barnes et al., 2019; Tahayna et al., 2022, for work on noncompositional sentiment). These include the case of sarcasm (\u201clife is good, you should get one\u201d), opposing sentiments (\u201cterribly fascinating\u201d), idiomatic expressions (\u201cbreak a leg\u201d) and neutral terms that, when composed, suddenly convey sentiment (\u201cyeah right\u201d). Adequately capturing sentiment computationally requires both learning compositional rules and understanding when such exceptions exist, where most contemporary sentiment models are expected to learn that via mere end-toend training on examples.\nHow can we identify whether the sentiment of a phrase is non-compositional? We design a protocol to obtain such non-compositionality judgments based on human-annotated sentiment. Our methodology (elaborated on in \u00a73) utilises phrases from the Stanford Sentiment Treebank (SST) (Socher et al., 2013) and contrasts the sentiment of a phrase with control stimuli, in which one of the two subphrases has been replaced. Phrases whose annotated sentiment deviates from what is expected based on the controls are considered less compositional, as is illustrated in Figure 1. We analyse the resulting non-compositionality ranking of phrases\n(\u00a74) and show how the constructed resource can be used to evaluate sentiment models (\u00a75). Our new resource (NONCOMPSST) can further improve the understanding of what underlies noncompositionality in sentiment analysis, and can complement existing evaluation protocols for sentiment analysis models."
        },
        {
            "heading": "2 Related work",
            "text": "Over the course of years, sentiment analysis systems went from using rule-based models and sentiment lexicons (e.g. Moilanen and Pulman, 2007; Taboada et al., 2011) to using recursive neural networks (Socher et al., 2013; Tai et al., 2015; Zhu et al., 2015), to abandoning the use of structure altogether by finetuning pretrained large language models (e.g. P\u00e9rez et al., 2021; Camacho-collados et al., 2022; Hartmann et al., 2023), and recently, to abandoning training, via zero-shot generalisation (Wang et al., 2023). Crucial to the development of these systems has been the introduction of benchmarks, such as SST (Socher et al., 2013) and SemEval\u2019s Twitter benchmarks (e.g. Rosenthal et al., 2015; Nakov et al., 2016; Rosenthal et al., 2017).\nAlthough the vast majority of related work focused on simply improving on benchmarks, there have been studies more closely related to ours, asking questions such as: How do phrases with opposing sentiment affect each other (Kiritchenko and Mohammad, 2016a,b)? What is the role of negations (Zhu et al., 2014), modals and adverbs (Kiritchenko and Mohammad, 2016c)? Do idioms have non-compositional sentiment (Hwang and Hidey, 2019)? Which linguistic phenomena are still problematic for SOTA sentiment systems (Barnes et al., 2019)? Can we incorporate compositional and non-compositional processing in one system (Zhu et al., 2015)? And can we computationally rank sentences according to their sentiment compositionality (Dankers and Titov, 2022)? Gaining a better understanding of the contexts in which sentiment functions non-compositionally and is challenging to predict is crucial for the evaluation of sentiment models in an age where sentiment benchmarks may appear saturated (Barnes et al., 2019).1\nWe position our work in this latter group of articles, of which that of Hwang and Hidey is most closely related.\n1As a concrete example, consider the widely-used binary SST sentiment analysis task contained in the GLUE benchmark (Wang et al., 2018): at the time of writing, SOTA performance for this task matches humans\u2019 performance."
        },
        {
            "heading": "3 Collecting non-compositionality ratings",
            "text": "Compositional processing of sentiment involves applying a function to the polarity of subphrases to obtain the polarity of the phrase. Turning this notion into a quantifiable metric requires us to measure the polarities and determine the composition function. Non-compositional phrases are then simply phrases whose sentiment deviates from what is expected. How do we implement this? We first select data (\u00a73.1) and then obtain sentiment labels for phrases through data annotation studies (\u00a73.2). We consider the composition function to be the default mapping from two subphrases with a specific sentiment to their combined sentiment. We obtain the default mapping by replacing subphrases with control stimuli and annotating sentiment for those modified phrases. Using those results, we can compute the non-compositionality ratings (\u00a73.3). Figure 2 summarises the full procedure."
        },
        {
            "heading": "3.1 Materials",
            "text": "We first select phrases for which to obtain the noncompositionality ratings, along with control stimuli.\nData selection We obtain our data from the SST dataset, containing 11,855 sentences from movie reviews (Pang and Lee, 2005), and sentiment annotations from Socher et al. (2013). The dataset provides sentiment labels for all full sentences and\nphrases contained in these sentences (all phrases that represent a node in the constituency parse trees of these sentences). We select candidate phrases to include in our dataset by applying the following constraints to the phrases: they consist of two subphrases that contain 3-8 tokens each, do not contain named entities and had a relatively high agreement in the original dataset. In Appendix A, we elaborate on the implementation of our constraints.\nSelection of control subphrases We assume that if a subphrase behaves compositionally, replacing it with a control should not affect the overall sentiment of the phrase. How do we select control stimuli? By taking subphrases with the same sentiment (based on SST\u2019s sentiment labels) and phrase type (e.g. NP, PP, SBAR). For each phrase \u2013 consisting of subphrases A and B \u2013 we automatically select 32 candidate control subphrases and manually narrow them down to eight (A\u2032n, B \u2032 n, where n \u2208 {1, 2, 3, 4}). During the manual annotation, we removed examples for which fewer than eight suitable control stimuli remained. Our final collection contains 500 phrases to be used in the human annotation study."
        },
        {
            "heading": "3.2 Data annotation studies",
            "text": "We collect sentiment labels in two rounds using a 7- point scale. In Study 1, we obtain the sentiment for all subphrases involved to ensure that subphrases and their controls have the same sentiment. We then discard phrases for which A and/or B do not have more than three controls each, where we restrict the controls to those whose sentiment is at most 1 point removed from the sentiment of A (for A\u2032n) or B (for B \u2032 n).\nIn Study 2, we collect sentiment labels for all subphrase combinations, namely the remaining 259 phrases and the 1554 phrases in which a control subphrase is inserted. For a phrase \u201cA B\u201d, there are six controls: three that substitute A, and three that substitute B. Those substitutions could lead to ungrammatical constructions in spite of the data selection procedure, and participants can indicate that with a checkbox. Figure 4 in Appendix B displays example questions as shown to the participants. Participants were recruited via Prolific and annotated sentiment via a Qualtrics survey. In Study 1, 57 participants annotate 93 or 94 subphrases each. In Study 2, 90 participants annotate 60 or 61 subphrase combinations each. That way, every unique phrase and subphrase receives three annotations\ntotal. The inter-annotator agreement rates obtained were 0.60 and 0.64 for Study 1 and 2, respectively, in terms of Krippendorff\u2019s \u03b1 for ordinal data. Appendix B further discusses these studies along with ethical considerations and annotation statistics."
        },
        {
            "heading": "3.3 Computing non-compositionality ratings",
            "text": "We obtain one sentiment label per phrase by averaging the annotations from Study 2. Afterwards, the non-compositionality ratings for a phrase \u201cAB\u201d are computed separately for A and B. The rating for A is the difference between the sentiment of \u201cA B\u201d and the mean sentiment of phrases \u201cA\u2032n B\u201d (n \u2208 {1, 2, 3}), and vice versa for B. Together, the two ratings express the non-compositionality of \u201cA B\u201d. We compute four variants of the ratings: ALL, ALLABS, MAX, MAXABS, ALLCLEAN. The first two include A and B separately, the second two use one rating per phrase (the largest of the two). ALLCLEAN includes A and B separately but excludes any phrases that are considered ungrammatical (109 out of 1813 phrases involved in Study 2 were flagged for that)."
        },
        {
            "heading": "4 Analysis of the ratings",
            "text": "What patterns can we identify in these ratings? We examine sentiment composition types, phrase lengths and syntactic categories of subphrases in Appendix C; only the sentiment composition type\nsubphrase A subphrase B Rating Sentiment HUMAN ROBERTA\ndisplays a clear pattern, illustrated by the ratings\u2019 distributions in Figure 3. The most compositional are the cases where the subphrases share their positive/negative sentiment, whereas combining opposites is the least compositional. Most phrases have an absolute rating within 1 point of our 7- point scale; only for 67 out of the 259 phrases, the MAXABS non-compositionality rating exceeds 1.\nWhat characterises the least compositional examples?2 The most prominent pattern is that figurative language is over-represented, which we can quantitatively illustrate by annotating all phrases as figurative and literal; the resulting MAXABS distributions differ substantially (Figure 3). In Table 1, some examples of figures of speech are the \u201cpressure cooker\u201d (a container metaphor implying a stressful situation, K\u00f6vecses and K\u00f6vecses, 1990), the \u201cnearly terminal case of the cutes\u201d (suggesting one can die of cuteness for emphasis), the \u201cserpent\u2019s smirk\u201d (metaphorically used to invoke connotations about evil) and the hyperbole of \u201ceveryone [...] is a con artist and a liar\u201d.\nOther atypical sentiment patterns that we observe require common-sense reasoning about terms that act as contextual valence shifters (Polanyi and Zaenen, 2006), e.g. to understand that \u201care long past\u201d implies something about current times, or that \u201ceating oatmeal\u201d relates to the blandness of that experience. Lastly, we also observe discourse relations between subphrases that modify the sentiment in a non-compositional manner. In \u201cfans of the animated wildlife adventure show will be in warthog heaven\u201d, the parallel between \u2018wildlife\u2019 and \u2018warthog heaven\u2019 amplifies the positive sentiment in a way that would not have happened for fans of a fashion show. Similarly,\n2We include them in Appendix C, in Table 3.\n\u201creturns with a chase to end all chases\u201d functions differently when it concerns the \u201cmaster of the chase sequence\u201d rather than anyone else. These examples illustrate sentiment compositions are often nuanced and that there is a long tail of atypical non-compositional phenomena."
        },
        {
            "heading": "5 Evaluating sentiment models",
            "text": "How can we employ the non-compositionality ratings to better understand the quality of sentiment systems? We illustrate this by recreating the ratings using state-of-the-art pretrained neural models and comparing them to the humans\u2019 ratings.\nExperimental setup To obtain the ratings from models, we adapt SST3 to use the 7-point scale and exclude the phrases of interest from the training data. Per model type, we fine-tune three model seeds that we evaluate on the SST test set using F1-score, and on NONCOMPSST using a) the correlation of the models\u2019 and humans\u2019 noncompositionality ratings (Pearson\u2019s r), and b) the F1-score of NONCOMPSST phrases, using the humans\u2019 sentiment scores as labels. To obtain models\u2019 non-compositionality ratings, we average sentiment predictions from the three model seeds and apply the same postprocessing as applied to the human-annotated data (see \u00a73.3).\nWe evaluate ROBERTA-BASE and -LARGE (Liu et al., 2019) along with variants of those models that are further trained on sentiment-laden data: TIMELM (the BASE model pretrained on tweets by Loureiro et al., 2022); the model of Camachocollados et al. (2022), which is TIMELM fine-tuned to predict tweets\u2019 sentiment; BERTTWEET (a BASE\n3The authors published the unprocessed original annotations by Amazon Mechanical Turk annotators of Socher et al. (2013); we process this data into SST-7.\nmodel pretrained on tweets by Nguyen et al., 2020); the model of P\u00e9rez et al. (2021) (BERTTWEET fine-tuned to predict tweets\u2019 sentiment); ROBERTALARGE (Hartmann et al., 2021) fine-tuned on sentiment of comments from social media posts; and finally ROBERTA-BASE fine-tuned on sentiment from IMDB movie reviews (Maas et al., 2011).4\nResults The results in Table 2 suggest that even though the systems have very similar performance on the SST test set, there are differences in terms of the NONCOMPSST ratings: ROBERTA-BASE has the lowest SST F1, but is the second-best BASE model in terms of r, only outperformed by IMDBROBERTA (i.e. the variant fine-tuned on movie reviews, the domain of SST). The ROBERTA-LARGE model outperforms both Hartmann et al.\u2019s model and the BASE models in terms of the SST F1 and NONCOMPSST correlations. Together, these observations suggest that pretraining or fine-tuning using data from a different domain can harm models\u2019 ability to capture nuanced sentiment differences required to estimate NONCOMPSST ratings. The SST performance is less sensitive to this, suggesting that our resource can provide a complementary view of sentiment systems.\nFinally, we also inspect the NONCOMPSST F1scores for all 259 phrases and the 67 phrases with the highest non-compositionality ratings according to the human annotators, for which results are included in the final two columns of Table 2. On that subset, IMDB-ROBERTA and the model of\n4See Appendix D for further details on the experimental setup used to fine-tune these models on SST. Visit our repository for the data and code.\nCamacho-collados et al. (2022) achieve the highest F1-score. These scores emphasise that for the top 67, sentiment is substantially harder to predict: non-compositional examples indeed present a larger challenge to sentiment models than compositional examples do."
        },
        {
            "heading": "6 Conclusion",
            "text": "Sentiment and compositionality go hand-in-hand: success in sentiment analysis is often attributed to models\u2019 capability to \u2018compose\u2019 sentiment. Indeed, the sentiment of a phrase is reasonably predictable from its subphrases\u2019 sentiment, but there are exceptions due to the ambiguity, contextuality and creativity of language. We made this explicit through an experimental design that determines non-compositionality ratings using humans\u2019 sentiment annotations and obtained ratings for 259 phrases (\u00a73). Even though most phrases are fairly compositional, we found intriguing exceptions (\u00a74), and have shown how the resource can be used for model evaluation (\u00a75). For future sentiment analysis approaches, we recommend a multi-faceted evaluation setup: to grasp the nuances of sentiment, one needs more than compositionality.\nLimitations\nOur work makes several limiting assumptions about compositionality in the context of sentiment:\n1. We maintain a simplistic interpretation of the composition \u2018function\u2019 but are aware that compositionality is considered vacuous by some (Zadrozny, 1994) since by using a generic\nnotion of a \u2018function\u2019, any sentiment computation can be considered compositional. We, therefore, only consider some phrases noncompositional because of the strict interpretation of that \u2018function\u2019. As a result, one might argue that whether a phrase such as \u201call the excitement of eating oatmeal\u201d is noncompositional in terms of its sentiment is debatable. We agree with that; if you represent the sentiment with a very expressive representation, every sentiment computation is compositional. Our results only apply given a very narrow interpretation of compositionality. 2. In this work, we restrict the notion of meaning compositions to the notion of sentiment compositions. Hence, there might be phrases that behave compositionally in terms of sentiment but are considered non-compositional otherwise. For instance, \u201crotten apple\u201d carries negative sentiment, both literally and figuratively, and might thus be considered compositional in terms of sentiment.\nIn addition to that, the resource we developed has technical limitations:\n1. Human annotators can provide unreliable sentiment annotations: they do not necessarily agree with one another, may lose focus while performing the task or may misunderstand the linguistic utterances they annotate. As a result, the resource inevitably contains some sentiment ratings that are inaccurate. 2. The resource we develop is small in size, which limits the robustness of the results when using the resource for experimentation. We would like to point out, however, that \u226b259 annotations were involved in obtaining the ratings for these 259 phrases. The results illustrate that, in spite of these limitations, the resource can still lead to valuable conclusions.\nFinally, the evaluation of the models in \u00a75 is somewhat limited, considering that the various models have been fine-tuned or pretrained by the mentioned authors using different experimental setups. Even though we then apply the same setup to finetune on SST, these differences need to be kept in mind when interpreting the results."
        },
        {
            "heading": "Acknowledgements",
            "text": "We thank Kenny Smith and Ivan Titov for their suggestions throughout this project and Matthias\nLindemann for his comments on a draft of this article. VD is supported by the UKRI Centre for Doctoral Training in Natural Language Processing, funded by the UKRI (grant EP/S022481/1) and the University of Edinburgh, School of Informatics and School of Philosophy, Psychology & Language Sciences."
        },
        {
            "heading": "A Materials",
            "text": "We require data for which to obtain the non-compositionality ratings and the control stimuli used to construct the default mapping (step 1 in Figure 2).\nData selection We obtain our data from the Stanford Sentiment Treebank (SST), a resource containing 11,855 sentences from movie reviews collected by Pang and Lee (2005), and annotated with sentiment labels by Socher et al. (2013). The sentences were parsed with the Stanford Parser (Klein and Manning, 2003) to allow for sentiment annotations of phrases in addition to full sentences. The dataset includes sentiment labels for all nodes of those parse trees. The sentiment labels were obtained from Amazon Mechanical Turk annotators, who indicated the sentiment using a multi-stop slider bar with 25 ticks.\nWe select potential phrases to include in our dataset, by applying the following constraints to the SST constituents. 1) They are marked as constituents by a state-of-the-art constituency parser, and have two subphrases.5 This ensures that the segmentation of the two parts is somewhat reasonable, and excludes compositions of three or more constituents. 2) The constituents\u2019 subphrases have 3-8 tokens, excluding punctuation. We exclude longer constituents, cognisant of the higher cognitive load that labelling those phrases would have for our participants. 3) The constituents do not contain a named entity, such as a movie star or the name of a film. Not all participants may know these names and the associated sentiment, hence we opt for excluding them. 4) The standard deviation of the original SST annotations lies within a range of five (on the original 25-point scale), to exclude the most ambiguous phrases. That standard deviation is computed over the three annotations per phrase obtained by Socher et al. (2013).\nSelection of control subphrases How do we select the control stimuli? They should have the same polarity as the part they are replacing. In that way, we can obtain the default mapping without altering the \u201cpolarities of the parts\u201d (Moilanen and Pulman, 2007). Each phrase from the data selection step has two subphrases. We distribute those subphrases into groups based on their SST sentiment (on an 11-point scale, from 0.0 to 1.0) and phrase type (NP, VP, PP, SBAR).\nFor each phrase \u2013 consisting of subphrases A and B \u2013 we randomly select 32 candidate control subphrases from those groups and manually narrow those 32 down to 8 control subphrases (4 for A, 4 for B). Where needed, small modifications are made to the control stimuli to create grammatical agreement between A\u2032n and B, and A and B \u2032 n. During the manual annotation, examples for which fewer than 8 suitable control stimuli remain are removed until 500 phrases remain to be used in the annotation study."
        },
        {
            "heading": "B Studies",
            "text": "The data annotation studies have been approved by the local ethics committee of the institute to which the authors belong. Prior to starting the questionnaire, the participants are presented with a Participant\n5We use stanza\u2019s tokenizer, parser and named entity recognition model for English (Qi et al., 2020), https://stanfordnlp. github.io/stanza.\nInformation Sheet that explains their data protection rights, the goal of the study and the compensation that they will receive. We set out to pay participants at a rate of \u00a311 per hour (above the minimum wage), and estimated both studies to take 12 minutes to fill out.\nProcedure Study 1 Participants annotate the data via a survey on the Qualtrics platform. We first ask participants to familiarise themselves with the sentiment labels, by showing seven subphrases from the SST dataset and requesting the participants to match them with the correct sentiment label. Afterwards, the correct answers are shown. These subphrases and their labels are taken from the SST dataset. Afterwards, the participants are shown 93 or 94 subphrases in isolation, and annotate them (the number of stimuli could not evenly be divided over participants while adhering to the desired study length of approximately 12 minutes). At the very end, the participants can provide feedback on the task. This study constitutes step 2 in Figure 2.\nProcedure Study 2 The second study has a very similar procedure. First, the participants familiarise themselves with matching subphrases with the seven labels. Afterwards, they are asked to use the same labels but now assign them to subphrase combinations. In both cases, the correct answers are shown, afterwards. We also explain that the subphrase combinations can seem odd semantically but that if the participants encounter ungrammatical subphrase combinations, they can flag that using a checkbox. Afterwards, the participants are shown 60 or 61 subphrase combinations that they then annotate. We clearly mark the segmentation of the phrase into two parts using colour. Participants receive fewer questions compared to the first study, due to the longer explanation that is included in Study 2, and due to the fact that annotating longer phrases may simply take longer. At the very end, the participants can provide feedback on the task. This study constitutes step 3 in Figure 2.\nParticipants We recruited participants via Prolific, requiring them to be located in the United Kingdom. Further selection criteria were that they have listed English as their first language and that they have a perfect Prolific approval rate over a minimum of 20 completed studies. Participants were initially compensated \u00a32.20 for both studies. The median completion time for Study 1 was 10:30 minutes. The median completion time for Study 2 was 13:16 minutes; these participants were paid a bonus to increase the mean reward per hour to \u00a311. In both studies, we excluded the results for participants whose sentiment annotations for the practice questions were, on average, more than 1 point removed from the correct labels (according to SST), or for whom the correlation between their responses and the labels of these practice questions was below 0.8 (according to Spearman\u2019s \u03c1).\nAnnotations Figure 5 includes the distributions over the sentiment labels that the participants assign. For 104 phrases, there was one annotator that indicated that the phrase was ungrammatical, and for 5 phrases, two annotators indicate that the phrase was ungrammatical. Sentiment label \u20183\u2019 represents the neutral label. In Study 1, this label is much more frequent compared to Study 2. This is in line with findings from Socher et al. (2013), who established that the longer the phrase, the more frequent the phrase is considered sentiment-laden. Across the board, the positive phrases are slightly over-represented. In Study 2, there are both natural and control stimuli, but the distributions are not substantially different.\nC Visualisation of the non-compositionality ratings\nIn Figure 6 and Figure 7 we separate the MAXABS and ABS non-compositionality ratings per sentiment composition type, per length combination, and per syntactic category combination. Only sentiment composition type is a clear dominant factor leading to higher ratings. Even though there are some longer phrase combinations with high average ratings, data from those categories is also rather scarce.\nTable 3 gives 60 examples of phrases whose MAXABS rating exceeds 1, ranked from the highest to the lowest."
        },
        {
            "heading": "D Models & model evaluation",
            "text": "Experimental setup We fine-tune the following models, all obtained from the HuggingFace model hub:\n\u2022 ROBERTA-BASE https://huggingface.co/roberta-base \u2022 ROBERTA-LARGE https://huggingface.co/roberta-large \u2022 TIMELM (Loureiro et al., 2022): https://huggingface.co/cardiffnlp/twitter-roberta-base \u2022 BERTTWEET (Nguyen et al., 2020): https://huggingface.co/vinai/bertweet-base \u2022 Camacho-collados et al. (2022): https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment-latest \u2022 P\u00e9rez et al. (2021): https://huggingface.co/finiteautomata/bertweet-base-sentiment-analysis \u2022 Hartmann et al. (2021): https://huggingface.co/j-hartmann/sentiment-roberta-large-english-3-classes \u2022 IMDB ROBERTA-BASE: https://huggingface.co/textattack/roberta-base-imdb\nThe experimental setup used to fine-tune them on SST is as follows: we use a batch size of 32, learning rate 5e\u22126, the Adam optimiser with a cosine-based warmup scheduler (warmup is 20%). We train for 5 epochs, selecting the best model based on the validation data and evaluate that on the test sets. We do not experiment with these hyperparameters extensively, as they are within the range of recommended settings and all models use the same base models (ROBERTA-BASE or ROBERTA-LARGE).\nWe train using NVIDIA A100-SXM-80GB GPUs on which training one model for one seed takes up to 30 minutes for the BASE model, and 50 minutes for the LARGE model.\nCharacterising our experiments Through our experiments, we contribute to generalisation evaluation in NLP: evaluating models\u2019 generalisation capabilities beyond standard i.i.d. testing, in particular through the evaluation on the 67 most non-compositional phrases. We characterise our experiments using the taxonomy of Hupkes et al. (2023): our experiments are cognitively motivated and focus on (non-)compositional generalisation. By separating inputs based on compositionality, we create a covariate shift between train and test data. That shift has a partitioned natural source, since the stimuli are from a human-written corpus, but the split we create is curated. In our experiments, we examine the influence of fine-tuning on SST-7 and evaluating on non-compositional examples (fine-tune train-test locus) but also examine the effect of a data shift in the pretrain-train locus, by considering models with different pretraining corpora.\nMotivation Practical Cognitive Intrinsic Fairness\n\u25a1\nGeneralisation type Compositional Structural Cross Task Cross Language Cross Domain Robustness\n\u25a1\nShift type Covariate Label Full Assumed\n\u25a1\nShift source Naturally occuring Partitioned natural Generated shift Fully generated\n\u25a1\nShift locus Train\u2013test Finetune train\u2013test Pretrain\u2013train Pretrain\u2013test\n\u25a1 \u25a1"
        }
    ],
    "title": "Non-Compositionality in Sentiment: New Data and Analyses",
    "year": 2023
}