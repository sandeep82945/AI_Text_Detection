{
    "abstractText": "Contrastive learning has been the dominant approach to train state-of-the-art sentence embeddings. Previous studies have typically learned sentence embeddings either through the use of human-annotated natural language inference (NLI) data or via large-scale unlabeled sentences in an unsupervised manner. However, even in the case of 1;unlabeled data, their acquisition presents challenges in certain domains due to various reasons. To address these issues, we present SynCSE, a contrastive learning framework that trains sentence embeddings with synthesized data. Specifically, we explore utilizing large language models to synthesize the required data samples for contrastive learning, including (1) producing positive and negative annotations given unlabeled sentences (SynCSE-partial), and (2) generating sentences along with their corresponding annotations from scratch (SynCSE-scratch). Experimental results on sentence similarity and reranking tasks indicate that both SynCSE-partial and SynCSE-scratch greatly outperform unsupervised baselines, and SynCSE-partial even achieves comparable performance to the supervised models in most settings.1",
    "authors": [
        {
            "affiliations": [],
            "name": "Junlei Zhang"
        },
        {
            "affiliations": [],
            "name": "Zhenzhong Lan"
        },
        {
            "affiliations": [],
            "name": "Junxian He"
        }
    ],
    "id": "SP:1ddbd16c2acdcb42462172fa4c30c5755a75429c",
    "references": [
        {
            "authors": [
                "Eneko Agirre",
                "Carmen Banea",
                "Claire Cardie",
                "Daniel Cer",
                "Mona Diab",
                "Aitor Gonzalez-Agirre",
                "Weiwei Guo",
                "Rada Mihalcea",
                "German Rigau",
                "Janyce Wiebe."
            ],
            "title": "SemEval-2014 task 10: Multilingual semantic textual similarity",
            "venue": "Proceedings of the 8th Interna-",
            "year": 2014
        },
        {
            "authors": [
                "Eneko Agirre",
                "Carmen Banea",
                "Daniel Cer",
                "Mona Diab",
                "Aitor Gonzalez-Agirre",
                "Rada Mihalcea",
                "German Rigau",
                "Janyce Wiebe."
            ],
            "title": "SemEval-2016 task 1: Semantic textual similarity, monolingual and cross-lingual evaluation",
            "venue": "Proceedings of the",
            "year": 2016
        },
        {
            "authors": [
                "Eneko Agirre",
                "Daniel Cer",
                "Mona Diab",
                "Aitor Gonzalez-Agirre."
            ],
            "title": "SemEval-2012 task 6: A pilot on semantic textual similarity",
            "venue": "*SEM 2012: The First Joint Conference on Lexical and Computational Semantics \u2013 Volume 1: Proceedings of the",
            "year": 2012
        },
        {
            "authors": [
                "Eneko Agirre",
                "Daniel Cer",
                "Mona Diab",
                "Aitor GonzalezAgirre",
                "Weiwei Guo."
            ],
            "title": "SEM 2013 shared task: Semantic textual similarity",
            "venue": "Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Confer-",
            "year": 2013
        },
        {
            "authors": [
                "Ken Barker",
                "Parul Awasthy",
                "Jian Ni",
                "Radu Florian."
            ],
            "title": "IBM MNLP IE at CASE 2021 task 2: NLI reranking for zero-shot text classification",
            "venue": "Proceedings of the 4th Workshop on Challenges and Applications of Automated Extraction of Socio-political",
            "year": 2021
        },
        {
            "authors": [
                "Daniel Cer",
                "Mona Diab",
                "Eneko Agirre",
                "I\u00f1igo LopezGazpio",
                "Lucia Specia."
            ],
            "title": "SemEval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation",
            "venue": "Proceedings of the 11th International Workshop on Semantic",
            "year": 2017
        },
        {
            "authors": [
                "Yiming Chen",
                "Yan Zhang",
                "Bin Wang",
                "Zuozhu Liu",
                "Haizhou Li."
            ],
            "title": "Generate, discriminate and contrast: A semi-supervised sentence representation learning framework",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language",
            "year": 2022
        },
        {
            "authors": [
                "Hyung Won Chung",
                "Le Hou",
                "Shayne Longpre",
                "Barret Zoph",
                "Yi Tay",
                "William Fedus",
                "Eric Li",
                "Xuezhi Wang",
                "Mostafa Dehghani",
                "Siddhartha Brahma"
            ],
            "title": "Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416",
            "year": 2022
        },
        {
            "authors": [
                "Arman Cohan",
                "Sergey Feldman",
                "Iz Beltagy",
                "Doug Downey",
                "Daniel Weld."
            ],
            "title": "SPECTER: Document-level representation learning using citation-informed transformers",
            "venue": "Proceedings of the 58th Annual Meeting of the Association",
            "year": 2020
        },
        {
            "authors": [
                "Tianyu Gao",
                "Xingcheng Yao",
                "Danqi Chen."
            ],
            "title": "SimCSE: Simple contrastive learning of sentence embeddings",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6894\u20136910, Online and Punta Cana, Do-",
            "year": 2021
        },
        {
            "authors": [
                "Fabrizio Gilardi",
                "Meysam Alizadeh",
                "Ma\u00ebl Kubli."
            ],
            "title": "Chatgpt outperforms crowd-workers for textannotation tasks",
            "venue": "arXiv preprint arXiv:2303.15056.",
            "year": 2023
        },
        {
            "authors": [
                "Hongliang He",
                "Junlei Zhang",
                "Zhenzhong Lan",
                "Yue Zhang."
            ],
            "title": "Instance smoothed contrastive learning for unsupervised sentence embedding",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 12863\u201312871.",
            "year": 2023
        },
        {
            "authors": [
                "Minqing Hu",
                "Bing Liu."
            ],
            "title": "Mining and summarizing customer reviews",
            "venue": "Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 168\u2013177.",
            "year": 2004
        },
        {
            "authors": [
                "Ting Jiang",
                "Jian Jiao",
                "Shaohan Huang",
                "Zihan Zhang",
                "Deqing Wang",
                "Fuzhen Zhuang",
                "Furu Wei",
                "Haizhen Huang",
                "Denvy Deng",
                "Qi Zhang."
            ],
            "title": "PromptBERT: Improving BERT sentence embeddings with prompts",
            "venue": "Proceedings of the 2022 Conference on",
            "year": 2022
        },
        {
            "authors": [
                "Yuxin Jiang",
                "Linhan Zhang",
                "Wei Wang."
            ],
            "title": "Improved universal sentence embeddings with promptbased contrastive learning and energy-based learning",
            "venue": "Findings of the Association for Computational",
            "year": 2022
        },
        {
            "authors": [
                "Ann Lee",
                "Michael Auli",
                "Marc\u2019Aurelio Ranzato"
            ],
            "title": "Discriminative reranking for neural machine translation",
            "venue": "In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natu-",
            "year": 2021
        },
        {
            "authors": [
                "Tao Lei",
                "Hrishikesh Joshi",
                "Regina Barzilay",
                "Tommi Jaakkola",
                "Kateryna Tymoshenko",
                "Alessandro Moschitti",
                "Llu\u00eds M\u00e0rquez."
            ],
            "title": "Semi-supervised question retrieval with gated convolutions",
            "venue": "Proceedings of the 2016 Conference of the North Amer-",
            "year": 2016
        },
        {
            "authors": [
                "Peerat Limkonchotiwat",
                "Wuttikorn Ponwitayarat",
                "Lalita Lowphansirikul",
                "Can Udomcharoenchaikit",
                "Ekapol Chuangsuwanich",
                "Sarana Nutanong."
            ],
            "title": "ConGen: Unsupervised control and generalization distillation for sentence representation",
            "venue": "Findings of the",
            "year": 2022
        },
        {
            "authors": [
                "Xueqing Liu",
                "Chi Wang",
                "Yue Leng",
                "ChengXiang Zhai."
            ],
            "title": "Linkso: a dataset for learning to retrieve similar question answer pairs on software development forums",
            "venue": "Proceedings of the 4th ACM SIGSOFT International Workshop on NLP for Software",
            "year": 2018
        },
        {
            "authors": [
                "Marco Marelli",
                "Stefano Menini",
                "Marco Baroni",
                "Luisa Bentivogli",
                "Raffaella Bernardi",
                "Roberto Zamparelli."
            ],
            "title": "A SICK cure for the evaluation of compositional distributional semantic models",
            "venue": "Proceedings of the Ninth International Conference",
            "year": 2014
        },
        {
            "authors": [
                "Amita Misra",
                "Brian Ecker",
                "Marilyn Walker."
            ],
            "title": "Measuring the similarity of sentential arguments in dialogue",
            "venue": "Proceedings of the 17th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 276\u2013287, Los Angeles. Association for",
            "year": 2016
        },
        {
            "authors": [
                "Niklas Muennighoff",
                "Nouamane Tazi",
                "Loic Magne",
                "Nils Reimers."
            ],
            "title": "MTEB: Massive text embedding benchmark",
            "venue": "Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pages 2014\u20132037, Dubrovnik,",
            "year": 2023
        },
        {
            "authors": [
                "Graham Neubig",
                "Zhiwei He"
            ],
            "title": "Zeno GPT Machine Translation Report",
            "year": 2023
        },
        {
            "authors": [
                "OpenAI."
            ],
            "title": "Chatgpt: Optimizing language models for dialogue",
            "venue": "OpenAI Blog.",
            "year": 2022
        },
        {
            "authors": [
                "OpenAI."
            ],
            "title": "Gpt-4 technical report",
            "venue": "arXiv.",
            "year": 2023
        },
        {
            "authors": [
                "Long Ouyang",
                "Jeffrey Wu",
                "Xu Jiang",
                "Diogo Almeida",
                "Carroll Wainwright",
                "Pamela Mishkin",
                "Chong Zhang",
                "Sandhini Agarwal",
                "Katarina Slama",
                "Alex Ray"
            ],
            "title": "Training language models to follow instructions with human",
            "year": 2022
        },
        {
            "authors": [
                "Bo Pang",
                "Lillian Lee."
            ],
            "title": "A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts",
            "venue": "Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL-04), pages 271\u2013278,",
            "year": 2004
        },
        {
            "authors": [
                "Bo Pang",
                "Lillian Lee."
            ],
            "title": "Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales",
            "venue": "Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL\u201905), pages 115\u2013124, Ann",
            "year": 2005
        },
        {
            "authors": [
                "Bryan A Plummer",
                "Liwei Wang",
                "Chris M Cervantes",
                "Juan C Caicedo",
                "Julia Hockenmaier",
                "Svetlana Lazebnik."
            ],
            "title": "Flickr30k entities: Collecting region-to-phrase correspondences for richer imageto-sentence models",
            "venue": "Proceedings of the IEEE",
            "year": 2015
        },
        {
            "authors": [
                "Timo Schick",
                "Hinrich Sch\u00fctze."
            ],
            "title": "Generating datasets with pretrained language models",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6943\u2013 6951, Online and Punta Cana, Dominican Republic.",
            "year": 2021
        },
        {
            "authors": [
                "Richard Socher",
                "Alex Perelygin",
                "Jean Wu",
                "Jason Chuang",
                "Christopher D. Manning",
                "Andrew Ng",
                "Christopher Potts."
            ],
            "title": "Recursive deep models for semantic compositionality over a sentiment treebank",
            "venue": "Proceedings of the 2013 Conference on Empiri-",
            "year": 2013
        },
        {
            "authors": [
                "Gizem So\u011fanc\u0131o\u011flu",
                "Hakime \u00d6zt\u00fcrk",
                "Arzucan \u00d6zg\u00fcr."
            ],
            "title": "Biosses: a semantic sentence similarity estimation system for the biomedical domain",
            "venue": "Bioinformatics, 33(14):i49\u2013i58.",
            "year": 2017
        },
        {
            "authors": [
                "Nandan Thakur",
                "Nils Reimers",
                "Andreas R\u00fcckl\u00e9",
                "Abhishek Srivastava",
                "Iryna Gurevych."
            ],
            "title": "Beir: A heterogeneous benchmark for zero-shot evaluation of information retrieval models",
            "venue": "Thirty-fifth Conference on Neural Information Processing Systems",
            "year": 2021
        },
        {
            "authors": [
                "Tianduo Wang",
                "Wei Lu"
            ],
            "title": "Differentiable data",
            "year": 2022
        },
        {
            "authors": [
                "Yang. 2022c"
            ],
            "title": "Improving contrastive learning",
            "year": 2022
        },
        {
            "authors": [
                "Xing Wu",
                "Chaochen Gao",
                "Liangjun Zang",
                "Jizhong Han",
                "Zhongyuan Wang",
                "Songlin Hu."
            ],
            "title": "ESimCSE: Enhanced sample building method for contrastive learning of unsupervised sentence embedding",
            "venue": "Proceedings of the 29th International Con-",
            "year": 2022
        },
        {
            "authors": [
                "Jiacheng Ye",
                "Jiahui Gao",
                "Qintong Li",
                "Hang Xu",
                "Jiangtao Feng",
                "Zhiyong Wu",
                "Tao Yu",
                "Lingpeng Kong."
            ],
            "title": "ZeroGen: Efficient zero-shot learning via dataset generation",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language",
            "year": 2022
        },
        {
            "authors": [
                "Jiacheng Ye",
                "Jiahui Gao",
                "Zhiyong Wu",
                "Jiangtao Feng",
                "Tao Yu",
                "Lingpeng Kong."
            ],
            "title": "ProGen: Progressive zero-shot dataset generation via in-context feedback",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2022, pages 3671\u2013",
            "year": 2022
        },
        {
            "authors": [
                "Jiali Zeng",
                "Yongjing Yin",
                "Yufan Jiang",
                "Shuangzhi Wu",
                "Yunbo Cao."
            ],
            "title": "Contrastive learning with prompt-derived virtual semantic prototypes for unsupervised sentence embedding",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP",
            "year": 2022
        },
        {
            "authors": [
                "Kun Zhou",
                "Beichen Zhang",
                "Xin Zhao",
                "Ji-Rong Wen."
            ],
            "title": "Debiased contrastive learning of unsupervised sentence representations",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 6120\u2013",
            "year": 2022
        },
        {
            "authors": [
                "Kun Zhou",
                "Yuanhang Zhou",
                "Wayne Xin Zhao",
                "JiRong Wen."
            ],
            "title": "Learning to perturb for contrastive learning of unsupervised sentence representations",
            "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing.",
            "year": 2023
        },
        {
            "authors": [
                "Neubig",
                "He",
                "2023). E"
            ],
            "title": "Synthetic data amount We also analyzed the impact on performance when augmenting the volume of generated data on the manually curated",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "The objective of sentence representation learning is to derive sentence embeddings that can benefit a wide range of downstream tasks, including reranking (Lee et al., 2021; Barker et al., 2021), natural language understanding (Cer et al., 2018), and retrieval (Misra et al., 2016; Thakur et al., 2021; Wang et al., 2022a). Methods built on contrastive learning, such as SimCSE (Gao et al., 2021) and PromCSE (Jiang et al., 2022b), have dominated the field due to their competitive performance (Zeng\n*Work done during Junlei\u2019s visit to HKUST. \u2020Corresponding author. 1Code and the synthesized datasets are available at\nhttps://github.com/hkust-nlp/SynCSE.\net al., 2022; Limkonchotiwat et al., 2022; Wu et al., 2022a; Wang et al., 2022c; He et al., 2023).\nContrastive learning trains sentence representations through distinguishing positive samples from negative ones. In this framework, the quality of these positive and negative annotations plays a critical role. Supervised approaches typically gather these annotations from labeled natural language inference (NLI) datasets (Jiang et al., 2022a; Limkonchotiwat et al., 2022) \u2013 however, such sources are generally unavailable for most settings, and manually creating them is cost-prohibitive. As a result, unsupervised methods that solely rely on unlabeled sentences attract significantly more attention re-\ncently (Gao et al., 2021; Zhou et al., 2022; Wu et al., 2022a) \u2013 they mostly develop methods to automatically obtain positive and negative samples to facilitate contrastive learning. A representative example is SimCSE (Gao et al., 2021), which leverages perturbed hidden states as the positive samples and in-batch sentences as negatives to perform contrastive learning. To differentiate between in-batch negatives and the annotated negatives, the latter are often termed \u201chard negatives\u201d, which have proven to be significantly advantageous in enhancing sentence embeddings (Wang et al., 2022b,c).\nDespite considerable advances in recent years, the performance of these unsupervised methods still falls short when compared to their supervised counterparts. Moreover, the unavailability of largescale unlabeled data for the targeted domain often poses additional limitations to these approaches. To overcome these challenges, we introduce SynCSE, an unsupervised contrastive framework that trains sentence embeddings with synthesized data. Concretely, we propose to prompt large language models (LLMs) such as ChatGPT (OpenAI, 2022) to synthesize the samples needed for contrastive learning. This is inspired by recent successes of prompting large language models (LLMs) to perform various tasks (Chung et al., 2022; Ouyang et al., 2022; OpenAI, 2023), especially the superior performance of LLMs over crowd-workers on text annotation (Gilardi et al., 2023). We investigate two variants of SynCSE in this work that correspond to two practical scenarios: (1) SynCSE-partial, where large-scale unlabeled sentences are available and LLMs are prompted to produce positive and hard negative annotations, and (2) SynCSE-scratch, where large-scale unlabeled sentences are not available, prompting LLMs to generate sentences and their corresponding annotations from scratch. The latter represents a particularly challenging yet practical scenario where we aim to learn sentence embeddings without any data samples.\nWe conduct comprehensive experiments on the standard Semantic Textual Similarity (STS) benchmark, along with four reranking tasks and four domain adaptation tasks. Our results demonstrate that both SynCSE-partial and SynCSE-scratch substantially outperform the unsupervised baselines in all cases \u2013 for example, SynCSE-partial and SynCSEscratch exceed the unsupervised SimCSE baseline by 5.37 and 4.18 absolute points respectively on STS. Particularly, SynCSE-partial often equals its\nsupervised counterpart on STS, marking the first instance of an unsupervised method matching supervised results on this benchmark. We release our synthesized datasets to facilitate further research to learn better sentence embeddings."
        },
        {
            "heading": "2 SynCSE",
            "text": ""
        },
        {
            "heading": "2.1 Background",
            "text": "We base our approach on the formulation of SimCSE (Gao et al., 2021), which is one of the most common and effective contrastive learning frameworks to learn sentence embeddings. Formally, we denote the unlabeled sentence as xi and its positive sample as x+i . Let hi and h + i denote the representations of xi and x+i respectively, then the unsupervised SimCSE loss is defined as:\n\u2212 log e sim(hi,h + i )/\u03c4\u2211M\nj=1 e sim(hi,h\n+ j )/\u03c4\n, (1)\nwhere M denotes the mini-batch\u2019s size, \u03c4 is a temperature hyperparameter, and sim(\u00b7, \u00b7) stands for a similarity function. Unsupervised SimCSE passes the same xi twice to the encoder to form (hi,h+i ) pairs due to random dropout, and other sentences within the same mini-batch are considered as negative samples as shown in Eq. 1. Supervised SimCSE further extends (xi, x+i ) with hard negative samples x\u2212i to constitute the triplet datasets{ xi, x + i , x \u2212 i }N i=1 and define the supervised loss:\n\u2212 log e sim(hi,h + i )/\u03c4\u2211M\nj=1(e sim(hi,h\n+ j )/\u03c4 + esim(hi,h \u2212 j )/\u03c4 )\n.\n(2) In supervised SimCSE, the (xi, x+i , x \u2212 i ) triplets are typically from annotated NLI datasets, where xi is the premise, x+i and x \u2212 i are the entailment and contradiction hypotheses. Supervised SimCSE significantly outperforms the unsupervised one due to the enhanced quality of positive and hard negative samples. However, such annotated data are typically unavailable in most settings, and manually annotating triplets (xi, x+i , x \u2212 i ) can be resource-intensive, rendering unsupervised approaches the most promising choices in practice. In this work, we focus on the supervised loss in Eq. 2, but synthesize (x+i , x \u2212 i ) given xi or even generate (xi, x + i , x \u2212 i ) triplets from scratch, aiming to approach the performance of supervised models with an unsupervised method. We describe our data synthesis process next."
        },
        {
            "heading": "2.2 Data Synthesis from ChatGPT",
            "text": "We propose to prompt ChatGPT (OpenAI, 2022) to synthesize the required data in contrastive learning, inspired by recent successes of prompting LLMs to fulfill multiple tasks (Chung et al., 2022; OpenAI, 2023). Concretely, we introduce two variants of SynCSE: (1) SynCSE-partial which synthesizes (x+i , x \u2212 i ) given xi, and (2) SynCSE-scratch which synthesizes (xi, x+i , x \u2212 i ) from scratch. SynCSEscratch is practically useful since large-scale unlabeled data are not always available in the domain of interest due to copyright restrictions, data distribution issues, or messy formats. We describe these two variants below. In general, using SynCSEscratch as an example, the complete data generation process includes two parts: (1) generating unlabeled sentences in the target domain; (2) generating\npositive/hard negative labels with prompt/example pools."
        },
        {
            "heading": "2.3 SynCSE-partial",
            "text": "Synthesizing positive and hard negative examples: We prompt ChatGPT in a few-shot setting to annotate positive and hard negative samples given a sentence xi, an illustrative example is shown in Figure 2. The structure of the prompts for generating positive and hard negative examples remains the same; the only difference lies in the prompts. In our implementation with the ChatGPT model, we have designed a few-shot prompt in a multi-turn chat format.\nExample and prompt pools: A significant challenge in creating synthetic datasets lies in enhancing the dataset\u2019s diversity. Ye et al. (2022b) suggested that merely increasing the size of the synthetic dataset might not lead to better performance, with one reason being the lack of diversity. Datasets labeled by groups of annotators can naturally help to mitigate this problem due to the variance in understanding and interpretation of prompts among different annotators. This variance results in diverse outputs, even for the same input. For example, Williams et al. (2018) utilized 387 annotators to create the MultiNLI dataset. Even with the same prompt, these annotators provided varied outputs due to their individual understanding of the prompt and their unique world knowledge, leading to a more diverse dataset. In an attempt to mimic this variation among different annotators, we employ example pools and prompt pools. Specifically, we designed four types of positive/hard negative prompts (an example of hard negative prompts are\nshowed in Table 1) and 18 few-shot exemplars for each of the prompt (generated using GPT-4). During each data generation process, we sample one prompt and five exemplars to construct a distinct input prompt. Details of these pools can be found in Appendix A."
        },
        {
            "heading": "2.4 SynCSE-scratch",
            "text": "Creating a synthetic dataset from scratch, where the necessary unlabeled sentences for annotation are absent, presents a substantial challenge. We address this problem in two stages: initially, we generate unlabeled sentences, and subsequently, we apply the procedure discussed in \u00a72.3 to annotate positive and hard negative samples of these sentences.\nTo ensure data diversity during the generation of unlabeled sentences, we employ a strategy that specifies the genres and topics when generation, combined with the utilization of example and prompt pools. This strategy is intended to minimize repetition and redundancy between the new data and the generated data so far. More specifically, as illustrated in Figure 1, given a text genre, we randomly select six topics from a pre-defined list to be included in the prompt (the list of genres and topics used in this paper can be found in Appendix B). The term \"etc.\" in the prompt ensures that the generated sentences are not strictly limited to these six topics. We adopt one-shot prompting to generate several sentences at once. As long as given different genres or topics when adding data compared to the existing data, the added data will likely have low redundancy with the existing data, thereby enhancing the overall diversity of the dataset. The examples used for generating raw sentences were produced by GPT-4."
        },
        {
            "heading": "3 Experiment",
            "text": ""
        },
        {
            "heading": "3.1 Training",
            "text": "We evaluate three different settings in the experiments, including SynCSE-partial, SynCSE-scratch, as well as a combination of SynCSE-scratch with existing annotated datasets in a supervised setting. While both SynCSE-partial and SynCSE-scratch represent unsupervised settings, in the combination setting we augment previous annotated datasets with the synthesized data produced in SynCSEscratch, to examine whether SynCSE-scratch could provide help for a supervised scenario as well.\nWe refer to the NLI dataset (MNLI+SNLI) used\nby SimCSE as SimCSE_NLI. In the creation of the SynCSE-partial dataset, for a fair comparison, we utilized the unlabeled sentences x from SimCSE_NLI, and generated positive/hard negative examples for them using the algorithm detailed in \u00a72.3. For SynCSE-scratch, we generate the same number of examples as in the SynCSE-partial case, as detailed in \u00a72.4. While our method can easily scale up the dataset, for a fair comparison, we ensure the data volume used for SynCSE-scratch and SynCSE-partial is equivalent to that of SimCSE_NLI. For the combination of the SynCSEscratch and SimCSE_NLI datasets, we simply merge these two datasets to evaluate whether our generated dataset can aid the manually annotated one.\nGiven that SimCSE serves as a general method in contrastive learning, we consistently use SimCSE as the backbone method for SynCSE. We note that SynCSE is general and could be combined with more advanced algorithms as well, such as with PromCSE (Jiang et al., 2022b) and CARDS (Wang et al., 2022c). We emphasize that, after training the models on the NLI dataset, we freeze the models and directly evaluate our embeddings on all the different tasks and setting below \u2013 we do not specifically train sentence embeddings on each setting separately. For the STS and transfer learning tasks, we use the same hyperparameters as SimCSE. Since SimCSE did not conduct reranking experiments, we directly use the default parameters of MTEB (Muennighoff et al., 2023) to evaluate embeddings on the reranking tasks."
        },
        {
            "heading": "3.2 Evaluation Settings",
            "text": "Semantic Textual Similarity Tasks: Following the procedure outlined in SimCSE, we evaluate our model, trained on the synthetic NLI dataset, across seven semantic textual similarity (STS) tasks: STS 2012-2016 (Agirre et al., 2012, 2013, 2014, 2015, 2016), the STS Benchmark (Cer et al., 2017), and SICK Relatedness (Marelli et al., 2014). It is important to note that no data from these STS tasks were used during training. Our model was trained solely on our synthetic NLI dataset. The sentence embeddings, which we evaluate on the STS tasks, are obtained from the [CLS] representation. During the training process, we average the development scores from the STS Benchmark and SICK Relatedness to form the evaluation matrix. This matrix is used to select the best models. The other hyper-\nparameters are kept consistent with those used in SimCSE.\nReranking tasks: We further evaluate the synthetic dataset on four reranking tasks: AskUbuntuDupQuestions (Lei et al., 2016), MindSmallReranking (Wu et al., 2020), SciDocsRR (Cohan et al., 2020), and StackOverflowDupQuestions (Liu et al., 2018). We directly evaluate the model, which is frozen after training on the NLI dataset, on reranking tasks, without using the training sets of reranking tasks. The resulting ranking is scored for each query and averaged across all queries. In line with the methodology of MTEB (Muennighoff et al., 2023), we utilize Mean Average Precision (MAP) as the primary metric.\nBaselines: We compare our approach with stateof-the-art sentence embedding learning methods: RankCSE (Liu et al.), L2P-CSR (Zhou et al., 2023), PCL (Wu et al., 2022a), CARDS (Wang et al., 2022c), ConPVP (Zeng et al., 2022), and PromptRoBERTa (Jiang et al., 2022a). While we base our approach on SimCSE, we emphasize that our approach is orthogonal to the baseline algorithms and our synthesized datasets may be combined with them to further boost the performance. We directly report the results from their respective papers."
        },
        {
            "heading": "3.3 Semantic Texual Similarity",
            "text": "Main results: As shown in Table 2, Both SynCSE-partial and SynCSE-scratch significantly\noutperformed all the unsupervised baselines by more than 2 absolute points. Even when compared with supervised settings, our approach achieved performance near that of manual annotation on RoBERTa-base, falling behind by only about 1 point on RoBERTa-large. It\u2019s worth noting that while the supervised SimCSE training dataset (SNLI) and STS test data share a significant overlap in domains (for instance, both STSb and SNLI extensively used Flicker30k data (Plummer et al., 2015)), the domains were not explicitly known while generating the SynCSE-scratch dataset. Interestingly, SynCSE-partial does not always beat SynCSE-scratch as demonstrated in the RoBERTa-large case, which implies the potential of SynCSE-scratch as a promising approach to learn sentence embeddings without using any real data samples. By augmenting annotated NLI data with the SynCSE-scratch synthetic dataset, our approach outperformed sup-SimCSE significantly, reaching a performance of 84.37% with RoBERta-large, suggesting that our synthetic data is complementary to human-labeled NLI datasets. \u201cPromptCSE+EH\u201d (Jiang et al., 2022b) achieves competitive performance in the supervised setups. As an orthogonal contribution, however, SynCSE may be combined with the loss function they proposed to further advance the results."
        },
        {
            "heading": "3.4 Reranking",
            "text": "Table 3 shows the results of the reranking tasks. Compared to the STS task, the domain of the reranking task data is more divergent from that of the NLI data used for training, as a result, SynCSEscratch actually outperforms SynCSE-partial significantly, which implies the advantage of SynCSE-\nscratch when in-domain unlabeled sentences are unavailable. SynCSE-scratch also surpasses other unsupervised baselines while SynCSE-partial underperforms them. Moreover, the combination of SynCSE-scratch with manually annotated datasets still facilitates further performance enhancement, substantiating that our method can aid in augmenting existing datasets."
        },
        {
            "heading": "3.5 Comparison with Other Synthetic Datasets",
            "text": "In addition to comparing with the MNLI+SNLI datasets used in SimCSE, we also compare our method with three other baselines that leverage synthetic NLI data: (1) GENSE (Chen et al., 2022) aims to automatically annotate the positive and hard negative examples with a LLM trained on existing NLI labeled dataset. We sample the same number of examples from their published dataset as those used in SynCSE; (2) The objective of DINO (Schick and Sch\u00fctze, 2021) is to generate synthetic data for sentence embeddings as well. In DINO\u2019s most effective configuration, they generate the positive or hard negative samples and assign a similarity score to them based on the prompts used. As they have not made an NLI-style dataset available, we directly report results from their paper, and (3) ZeroGen (Ye et al., 2022a) proposes an efficienty unsupervised dataset generation method. We selected those examples that have been provided in both \"entailment\" and \"not_entailment\" sentences to construct sentence pairs, totaling 46,311 pairs, as training data. To ensure a fair comparison, we randomly sampled an equal number of examples gen-\nerated by SynCSE-scratch. We compare the generated sentences of our methods with them in Table 11. From the table, we can find that our generated sentence can generate more diverse annotations. As depicted in Table 4, both SynCSE-scratch and SynCSE-partial have achieved performance on the STS task that surpasses that of DINO, GenSE. In a practical setting when generating a dataset from scratch (SynCSE-scratch), we compare our method with ZeroGen (Table 5), and the results show our method significantly outperforms the baseline."
        },
        {
            "heading": "3.6 Applying to Specialized Domains",
            "text": "SynCSE is advantageous when dealing with specialized domains where unlabeled data is unavailable. In such cases, traditional methods are not directly applicable. To evaluate SynCSE in this scenario, we conduct experiments on two another datasets focused on specialized domains \u2013 the BIOSSES (Sog\u0306anc\u0131og\u0306lu et al., 2017) dataset of a semantic textual similarity task for the biomedical domain, and the StackOverflowDupQuestions (Liu et al., 2018) dataset of a reranking task for the programming questions domain. Specifically, our experimental design is based on the assumption that we only have access to the names of the target domains (i.e., \u201cbiomedicine\u201d and \u201cStack Overflow website\u201d) without any data available. We run SynCSE-scratch in these settings. Concretely, we first generate 37k unlabeled sentences in the respective domain following the procedure described in Section \u00a72.4, then generate positive and hard negatives for these sentences, and train the models. We use the publicly available unsupervised\nSimCSE model checkpoint that was trained on the Wikipedia domain for comparison. This is because we assumed no access to unlabeled sentences in these domains, which is a practical setting. Our observations (Table 6) show that SynCSE-scratch outperforms the unsupervised SimCSE baseline significantly in both domains. This experiment further demonstrates the superiority of our method on new domains where no data is available \u2013 traditional unsupervised approaches like SimCSE tend to experience a domain transfer drop in performance in such scenarios."
        },
        {
            "heading": "3.7 Analysis",
            "text": "In this subsection, we provide an in-depth analysis of SynCSE. All results presented here are based on the RoBERTa-base model.\nTransfer tasks: Following SimCSE, we execute seven transfer learning tasks: MR (Pang and Lee, 2005), CR (Hu and Liu, 2004), SUBJ (Pang and Lee, 2004), MPQA (Wiebe et al., 2005), SST2 (Socher et al., 2013), TREC (Voorhees and Tice, 2000), and MRPC (Voorhees and Tice, 2000). These experiments are carried out with the same settings as used in SimCSE. As shown in Table 7, SynCSE-partial outperforms all unsupervised baselines.\nComparion with the naive generation process: To validate the effectiveness of our data synthesis process, we conduct an ablation experiment, where (1) we do not specify topics or genres when generating unlabeled sentences, and (2) we do not vary the prompt and exemplars but fix them the same (that are randomly selected from the pools) when generating the positive and hard negative labels.\nCounselor 1 H2 H3 H4 H5 Avg\nFraction of ethically unsafe data\n1% 0% 0% 1% 0% 0.4%\nTable 9: The result of the fraction of ethically unsafe data annotated by one psychological counselor and four postgraduate students. H\u2217 means the index of postgraduate annotators.\nOther settings are kept the same as in SynCSEscratch. We perform the ablation experiment on 22k examples. We denote the baseline without diversity control as \u201cNaive Generation\u201d and show them in the Table 8, our method outperforms the Naive Generation baseline by an average of 8.96%, demonstrating the critical role of diversity control in our data synthesis process.\nEthical considerations of the synthetic dataset: To evaluate the safety of our synthetic dataset, we ask five annotators (one of which is a psychological counselor and the other four are postgraduate students) to annotate whether the generated sentences have ethical problems. Specifically, we randomly select 100 sentences from those generated by SynCSE-scratch, and each sentence is independently evaluated by the five people for potential ethical problems. As the Table 9 suggests, only a minor portion of the data is classified as ethically unsafe, indicating that our synthetic dataset upholds a certain level of safety concerning ethical issues. This is not surprising since ChatGPT, the backend in our experiments, is already heavily aligned to avoid producing text with ethical or safety issues."
        },
        {
            "heading": "4 Related Work",
            "text": "Prior approaches for sentence embedding fall into two main categories: (1) supervised learning with labeled sentences, and (2) unsupervised sentence embedding with unlabeled sentences. Among these, works based on contrastive learning have proven to be the most effective. For unsupervised methods, SimCSE uses dropout masks to construct positive pairs for learning, while negative examples use in-batch negative examples. Some works employ data augmentation techniques on input sentences, such as word repetition (Wu et al., 2022b), case flipping (Wang et al., 2022c), or a combination of multiple data augmentation strategies to offset the bias caused by mono-augmentation (Wu et al., 2022a). PromptBERT (Jiang et al., 2022a)\nuses prompts instead of the [CLS] token to extract embeddings.\nHowever, these unsupervised methods significantly lag behind their supervised counterparts. Supervised approaches usually derive positive and hard negative samples from labeled NLI datasets (Wang and Lu, 2022; Gao et al., 2021; Jiang et al., 2022a), but these datasets are limited in quantity and domain. Additionally, annotating a new NLI dataset is costly, especially in fields that require trained annotators. Chen et al. (2022) trained a T5 (Chung et al., 2022) model capable of producing positive and hard negative samples, while Ye et al. (2022b) implemented a continuously updated model to modify prompts for generation. However, the performance of these algorithms is still constrained by the performance of generators, which need labeled NLI data for training. Differing from these methods, which necessitate training an additional model, Wang et al. (2022b) proposed a rule-based algorithm capable of generating hard negative annotations. However, its diversity is limited to the prescribed rules. Gilardi et al. (2023) used ChatGPT for dataset annotation. However, their exploration was limited to tasks with explicit answer labels such as \"RELEVANT\" or \"IRRELEVANT\". They did not attempt to annotate datasets that required diverse responses. Schick and Sch\u00fctze (2021) also propose to generate both annotations and unlabeled sentences, while they do not focus on the contrastive learning framework."
        },
        {
            "heading": "5 Discussion",
            "text": "In this work, we propose SynCSE, a novel contrastive learning framework for learning sentence embeddings with synthetic data. We prompt LLMs to synthesize unlabeled sentences and their positive and hard negative examples. Furthermore, by utilizing example and prompt pools, we can specify the genre and topic of generated sentences, thereby enhancing the quality of the synthetic dataset. Experiments on both sentence similarity and reranking tasks demonstrate the effectiveness of SynCSE. The performance of SynCSE in this study strongly suggests the potential of synthetic datasets generated by the increasingly advanced LLMs of today. We envision that, through the effective use of prompting strategies with LLMs, synthetic datasets produced by these models could potentially serve as promising alternatives to real-world data across a wide range of tasks."
        },
        {
            "heading": "A Prompt pools",
            "text": "In order to increase the diversity of input prompts, we designed a variety of prompts for generating positive samples, hard negative samples, and unlabeled data, which are adopted during generation based on certain probabilities. The specific prompts are displayed in Tables 12, 1, and 15. Given that generating image captions differs somewhat from generating other types of text, we have designed unique prompts for image captions to further enhance diversity, as illustrated in Table 16."
        },
        {
            "heading": "B Genres and Topics",
            "text": "Genres: When generating unlabeled sentences, to make the newly generated sentences as different as possible from existing data, we specify the genre and topic of the new sentences. As long as the genre and topic of the new sentences are different from existing ones, the probability of these new sentences providing more new information to the dataset becomes higher. In this paper, we use 20 different genres (Table 10) and 31 different topics (Table 2). Before generating sentences, we use GPT-4 to generate 30 examples for each genre as one-shot example sentences. When using them, we first specify a genre and fill it into the \"[the description of the genre]\" in the prompt of Table 15, then randomly choose 6 from the topic list to fill into \"[topici]\". These descriptions are adapted from the genre specifications provided by GPT-4, thus, creating new descriptions does not require a significant effort.\nTopics: We leveraged GPT-4 to generate an array of diverse topics, and 37 of these were randomly selected as the thematic grounding for our generation of unlabeled sentences. Concretely, these themes are: nature, technology, food, sports, culture, history, animals, environment, politics, finance, education, social issues, global issues, entertainment, healthcare, war, mathematical and electrical engineering, crime, relationships and emotional bonds, magic and mythical creatures, personal life stories, business strategies, fitness and mental health, global warming and conservation, various forms of art and cultural practices, teaching methodologies and learning styles, recipes and culinary techniques, ethical dilemmas and existential questions, space exploration and celestial phenomena, legal issues and courtroom drama, examination of past events and civilizations, ancient myths and legends,\nscientific theories, life stories of notable individuals, COVID-19, immigration policies, and mental health.\nGenre descriptions\n1 in-person conversations 2 letters 3 reports, speeches, letters, and press releases from public domain government websites 4 fiction 5 image descriptions 6 video descriptions 7 news from websites or newspapers 8 reviews and critiques for shopping 9 headlines of news 10 dialogue of technical or Instructional tutorial 11 informative and expository texts which pro-\nvide guidance or explanations related to a wide range of topics\n12 STEM examination questions 13 travelogue 14 historical description 15 plots involving political intrigue and maneuvering 16 formal writings that present research findings or scholarly discussion 17 speeches given by politicians, often with the\nintent of persuading or informing an audience about political topics\n18 written works such as poetry, drama, and novels 19 pieces of content shared on social media platforms 20 short image captions 21 messages paid for by a business or individual\nto promote a product, service, or event\nTable 10: The list of genre descriptions."
        },
        {
            "heading": "C Hyperparameters",
            "text": "We employed gpt-3.5-turbo-0301 for sentence generation. For the generation of unlabeled sentences, we set the temperature to 1.3, top_p to 1.0, and both presence_penalty and frequency_penalty to 0.3. The input prompts were one-shot prompts; in the example, 10 sentences were generated at once, and during the generation process, 20 sentences were generated at once. During the generation of positive sample annotations, we set the temperature to 1.0 and top_p to 0.9. In the generation of neg-\nUnlabeled Sentence Positive Label Hard Negative Label All\ncost (% per sentence)\n0.00007 0.00067 0.00076 0.0015\nTable 13: The cost analysis of our method generating sentences with gpt-3.5-turbo.\native sample annotations, we set the temperature to 1.0 and top_p to 0.95. Both positive and negative sample generations were 5-shot. Our training framework is based on SimCSE, which forcibly truncates parts of the sentence exceeding 32 words during training. To maintain a fair comparison, we filter out sentences with more than 32 words before training with the SimCSE framework after generating sentences with SynCSE-scratch."
        },
        {
            "heading": "D Cost of the synthesize data",
            "text": "We used gpt-3.5-turbo to synthesize data that is not very expensive, currently costing 0.0015 dollars per 1K tokens for input and 0.002 dollars per 1K tokens for output. Concretely, there are three parts in the data generation process: unlabeled sentences, positive labels, and hard negative labels. Since the length of each input varies, to quantify the cost, we randomly sampled 40 inputs and calculated the average cost per sentence. As detailed in Table 13, our method cost a total of around 1.5 $ for generating 1000 sentences, and the total cost of producing the 276k sentences used in our experiments of SynCSE-scratch in Table 2 is around 414 $. In the domain specialized task (Table 6), we just generate 37k sentence pairs and significantly surpass SimCSE in the target domain, and the cost is around 55 $. We would like to highlight that\nData 0% 20% 40% 60% 80% 100%\nAvg. STS 82.04 82.10 82.73 82.58 82.75 82.61\nTable 14: Performance of SimCSE_NLI when combined with varying amounts of our synthetic SynCSEscratch dataset. We report the performance on the avg STS results on the test set.\nthe rate per sentence above is much cheaper than manually labeling data; for instance, in machine translation tasks, human translation (around $0.1 per word) can be thousands of times costlier than using gpt-3.5-turbo (Neubig and He, 2023)."
        },
        {
            "heading": "E Synthetic data amount",
            "text": "We also analyzed the impact on performance when augmenting the volume of generated data on the manually curated dataset, as shown in Table 14. Since the domain of SynCSE-scratch is established upon its completion, the performance ceases to increase after a certain amount of SynCSE-scratch data is added to SimCSE. This may be due to the fact that the added data is randomly sampled, which likely already covers the domain of SynCSEscratch."
        }
    ],
    "title": "Contrastive Learning of Sentence Embeddings from Scratch",
    "year": 2023
}