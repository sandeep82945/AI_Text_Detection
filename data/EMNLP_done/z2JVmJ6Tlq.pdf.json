{
    "abstractText": "Retrofitting techniques, which inject external resources into word representations, have compensated for the weakness of distributed representations in semantic and relational knowledge between words. However, the previous methods require additional external resources and strongly depend on the lexicon. To address the issues, we propose a simple extension of extrofitting, self-supervised extrofitting: extrofitting by its own word vector distribution. Our methods improve the vanilla embeddings on all of word similarity tasks without any external resources. Moreover, the method is also effective in various languages, which implies that our method will be useful in lexicon-scarce languages. As downstream tasks, we show its benefits in dialogue state tracking and text classification tasks, reporting better and generalized results compared to other word vector specialization methods.1",
    "authors": [
        {
            "affiliations": [],
            "name": "Hwiyeol Jo"
        }
    ],
    "id": "SP:59ee64b1db9f91d1d6650e437970e6b78523705b",
    "references": [
        {
            "authors": [
                "Carl Allen",
                "Ivana Balazevic",
                "Timothy Hospedales."
            ],
            "title": "What the vec? towards probabilistically grounded embeddings",
            "venue": "Advances in Neural Information Processing Systems, pages 7465\u20137475.",
            "year": 2019
        },
        {
            "authors": [
                "Mikel Artetxe",
                "Gorka Labaka",
                "Eneko Agirre"
            ],
            "title": "A robust self-learning method for fully unsupervised",
            "year": 2018
        },
        {
            "authors": [
                "Piotr Bojanowski",
                "Edouard Grave",
                "Armand Joulin",
                "Tomas Mikolov."
            ],
            "title": "Enriching word vectors with subword information",
            "venue": "arXiv preprint arXiv:1607.04606.",
            "year": 2016
        },
        {
            "authors": [
                "Elia Bruni",
                "N Tram",
                "Marco Baroni"
            ],
            "title": "Multimodal distributional semantics",
            "venue": "The Journal of Artificial Intelligence Research,",
            "year": 2014
        },
        {
            "authors": [
                "Jose Camacho-Collados",
                "Mohammad Taher Pilehvar",
                "Nigel Collier",
                "Roberto Navigli."
            ],
            "title": "Semeval2017 task 2: Multilingual and cross-lingual semantic word similarity",
            "venue": "Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-",
            "year": 2017
        },
        {
            "authors": [
                "Daniel Cer",
                "Mona Diab",
                "Eneko Agirre",
                "Inigo LopezGazpio",
                "Lucia Specia."
            ],
            "title": "Semeval-2017 task 1: Semantic textual similarity-multilingual and cross-lingual focused evaluation",
            "venue": "arXiv preprint arXiv:1708.00055.",
            "year": 2017
        },
        {
            "authors": [
                "Ming-Wei Chang",
                "Lev-Arie Ratinov",
                "Dan Roth",
                "Vivek Srikumar."
            ],
            "title": "Importance of semantic representation: Dataless classification",
            "venue": "AAAI, volume 2, pages 830\u2013835.",
            "year": 2008
        },
        {
            "authors": [
                "Wayne W Daniel."
            ],
            "title": "Spearman rank correlation coefficient",
            "venue": "Applied nonparametric statistics, pages 358\u2013365.",
            "year": 1990
        },
        {
            "authors": [
                "Manaal Faruqui",
                "Jesse Dodge",
                "Sujay Kumar Jauhar",
                "Chris Dyer",
                "Eduard Hovy",
                "Noah A Smith."
            ],
            "title": "Retrofitting word vectors to semantic lexicons",
            "venue": "Proceedings of the 2015 Conference of the North American Chapter of the Association for Computa-",
            "year": 2015
        },
        {
            "authors": [
                "Yue Feng",
                "Yang Wang",
                "Hang Li."
            ],
            "title": "A sequenceto-sequence approach to dialogue state tracking",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language",
            "year": 2021
        },
        {
            "authors": [
                "Lev Finkelstein",
                "Evgeniy Gabrilovich",
                "Yossi Matias",
                "Ehud Rivlin",
                "Zach Solan",
                "Gadi Wolfman",
                "Eytan Ruppin."
            ],
            "title": "Placing search in context: The concept revisited",
            "venue": "Proceedings of the 10th international conference on World Wide Web, pages",
            "year": 2001
        },
        {
            "authors": [
                "Daniela Gerz",
                "Ivan Vuli\u0107",
                "Felix Hill",
                "Roi Reichart",
                "Anna Korhonen."
            ],
            "title": "Simverb-3500: A large-scale evaluation set of verb similarity",
            "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2173\u20132182.",
            "year": 2016
        },
        {
            "authors": [
                "Goran Glava\u0161",
                "Ivan Vuli\u0107."
            ],
            "title": "Explicit retrofitting of distributional word vectors",
            "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), volume 1, pages 34\u201345.",
            "year": 2018
        },
        {
            "authors": [
                "Richard HR Hahnloser",
                "Rahul Sarpeshkar",
                "Misha A Mahowald",
                "Rodney J Douglas",
                "H Sebastian Seung."
            ],
            "title": "Digital selection and analogue amplification coexist in a cortex-inspired silicon circuit",
            "venue": "Nature, 405(6789):947.",
            "year": 2000
        },
        {
            "authors": [
                "Felix Hill",
                "Roi Reichart",
                "Anna Korhonen."
            ],
            "title": "Simlex-999: Evaluating semantic models with (genuine) similarity estimation",
            "venue": "Computational Linguistics, 41(4):665\u2013695.",
            "year": 2015
        },
        {
            "authors": [
                "Hwiyeol Jo",
                "Stanley Jungkyu Choi."
            ],
            "title": "Extrofitting: Enriching word representation and its vector space with semantic lexicons",
            "venue": "arXiv preprint arXiv:1804.07946.",
            "year": 2018
        },
        {
            "authors": [
                "Aishwarya Kamath",
                "Jonas Pfeiffer",
                "Edoardo Maria Ponti",
                "Goran Glava\u0161",
                "Ivan Vuli\u0107."
            ],
            "title": "Specializing distributional vectors of all words for lexical entailment",
            "venue": "Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019),",
            "year": 2019
        },
        {
            "authors": [
                "Yoon Kim."
            ],
            "title": "Convolutional neural networks for sentence classification",
            "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1746\u20131751.",
            "year": 2014
        },
        {
            "authors": [
                "Diederik P Kingma",
                "Jimmy Ba."
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "arXiv preprint arXiv:1412.6980.",
            "year": 2014
        },
        {
            "authors": [
                "Thomas K Landauer",
                "Susan T Dumais."
            ],
            "title": "A solution to plato\u2019s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge",
            "venue": "Psychological review, 104(2):211.",
            "year": 1997
        },
        {
            "authors": [
                "Jens Lehmann",
                "Robert Isele",
                "Max Jakob",
                "Anja Jentzsch",
                "Dimitris Kontokostas",
                "Pablo N Mendes",
                "Sebastian Hellmann",
                "Mohamed Morsey",
                "Patrick Van Kleef",
                "S\u00f6ren Auer"
            ],
            "title": "Dbpedia\u2013a large-scale, multilingual knowledge base extracted from wikipedia",
            "year": 2015
        },
        {
            "authors": [
                "Omer Levy",
                "Yoav Goldberg."
            ],
            "title": "Dependencybased word embeddings",
            "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 302\u2013 308.",
            "year": 2014
        },
        {
            "authors": [
                "Minh-Thang Luong",
                "Richard Socher",
                "Christopher D Manning."
            ],
            "title": "Better word representations with recursive neural networks for morphology",
            "venue": "Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 104\u2013113.",
            "year": 2013
        },
        {
            "authors": [
                "Andrew L Maas",
                "Raymond E Daly",
                "Peter T Pham",
                "Dan Huang",
                "Andrew Y Ng",
                "Christopher Potts."
            ],
            "title": "Learning word vectors for sentiment analysis",
            "venue": "In",
            "year": 2011
        },
        {
            "authors": [
                "Steve Young"
            ],
            "title": "Semantic specialization",
            "year": 2017
        },
        {
            "authors": [
                "Goran Glava\u0161"
            ],
            "title": "LexFit: Lexical fine-tuning",
            "year": 2021
        },
        {
            "authors": [
                "Ivan Vuli\u0107",
                "Simone Paolo Ponzetto",
                "Goran Glava\u0161."
            ],
            "title": "Multilingual and cross-lingual graded lexical entailment",
            "venue": "Proceedings of the 57th Conference of the Association for Computational Linguistics, pages 4963\u20134974.",
            "year": 2019
        },
        {
            "authors": [
                "Tsung-Hsien Wen",
                "David Vandyke",
                "Nikola Mrk\u0161i\u0107",
                "Milica Gasic",
                "Lina M Rojas Barahona",
                "Pei-Hao Su",
                "Stefan Ultes",
                "Steve Young."
            ],
            "title": "A network-based end-to-end trainable task-oriented dialogue system",
            "venue": "Proceedings of the 15th Conference of the Euro-",
            "year": 2017
        },
        {
            "authors": [
                "John Wieting",
                "Mohit Bansal",
                "Kevin Gimpel",
                "Karen Livescu",
                "Dan Roth."
            ],
            "title": "From paraphrase database to compositional paraphrase model and back",
            "venue": "Transactions of the Association for Computational Linguistics, 3:345\u2013358.",
            "year": 2015
        },
        {
            "authors": [
                "Steve Young",
                "Milica Ga\u0161i\u0107",
                "Simon Keizer",
                "Fran\u00e7ois Mairesse",
                "Jost Schatzmann",
                "Blaise Thomson",
                "Kai Yu."
            ],
            "title": "The hidden information state model: A practical framework for pomdp-based spoken dialogue management",
            "venue": "Computer Speech & Language,",
            "year": 2010
        },
        {
            "authors": [
                "Xiang Zhang",
                "Junbo Zhao",
                "Yann LeCun."
            ],
            "title": "Character-level convolutional networks for text classification",
            "venue": "Advances in neural information processing systems, pages 649\u2013657.",
            "year": 2015
        },
        {
            "authors": [
                "Chang"
            ],
            "title": "said the dataset has 20 top-level categories but actually it",
            "year": 2008
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Static word vectors are still widely used in natural language tasks despite the recent trends of contextualized models. For example, in a wide study of Dialogue State Tracking (DST) (Feng et al., 2021), contextualized models performed worse than static word embeddings. It seems reasonable that the contextualized models do not perform well in such a lack of context, which implies that static embeddings are still useful.\nTo make better word vectors, we focus on retrofitting ideas (also called word vector postprocessing), which injects the semantic information from external resources by modifying the values of pretrained word vectors (Faruqui et al. (2015); Mrk\u0161ic\u0301 et al. (2016a); inter alia). The benefits of post-processing methods are that (1) the methods can reflect additional resources into the word vectors without re-training, (2) the methods can be\n1http://github.com/hwiyeoljo/SelfExtro\napplied to any kinds of pretrained word vectors, and (3) retrofitting can make word vectors specialize in a specific task.\nThe previous studies focusing on explicit retrofitting have used manually defined or learned functions to make synonyms close and antonyms distant (see the details in \u00a74). As a result, their approaches strongly rely on external resources.\nFurthermore, we agree that making synonyms close together is somewhat reasonable, even though the synonyms have different nuances in some contexts. However, other kinds of relations, such as antonyms, should be further investigated. For instance, love and hate are generally grouped as antonyms. Most of the previous studies have made the words distant from each other, but the words definitely share the meaning of emotion in their representations. We thus conjecture that the methods are not generalized well.\nIn this paper, we propose word vector enrichment based on extrofitting (Jo and Choi, 2018):\n\u2022 Self-supervised extrofitting that extends extrofitting for enriching word vectors without using external semantic lexicons. This method can resolve a limitation of post-processing approaches, which requires well-defined semantic lexicons. We highlight its usefulness in (relatively) lexicon-scarce languages. \u2022 We report the effects of word vector post-\nprocessing on several downstream tasks to show the generalization of the word vectors. Our methods consistently improve model performances in the fundamental tasks. In contrast, other post-processing methods degrade the performance."
        },
        {
            "heading": "2 Preliminary",
            "text": "Extrofitting Extrofitting (Jo and Choi, 2018) expands word embedding matrix W by concatenating\nW with rw:\nExpand(W, c) = W \u2295 rw {\nmeanw\u2208c(\u00b5w) if w \u2208 L \u00b5w otherwise\nwhere \u00b5w is the mean value of elements in word vector w. L denotes semantic lexicons, and c denotes the same class (synonym pairs). In other words, Expand makes an additional dimension per the word vector, and fill it with the same value if the pairs are synonym.\nNext, Trans calculates transform matrix given the matrix W:\nTrans(W, c) = argmaxU |UT \u2211c(\u00b5c \u2212 \u00b5)(\u00b5c \u2212 \u00b5)T U | |UT \u2211c\u2211i(xi \u2212 \u00b5c)(xi \u2212 \u00b5c)T U |\nwhere x is a word vector, c is a class. The overall average of x is \u00b5, and the class average in class i is denoted by \u00b5i. This formula finds a transform matrix U, which minimizes the variance within the same class and maximizes the variance between different classes. Each class is defined as the index of synonym pairs.\nIn the end, Extrofitting is formulated as follows:\nExtro(W, c)\n= Trans(Expand(W, c), c)T Expand(W, c)\nLatent Semantic Analysis (LSA) LSA (Landauer and Dumais, 1997) has been used to extract the relation of data through latent variables. LSA is based on Singular Value Decomposition (SVD), which decomposes a matrix as follows:\nA = US VT ,\nwhere S is a diagonal matrix with singular values, and U and V are the orthogonal eigenvectors. We can select top-k singular values to represent matrix A in k-dimensional latent space. Then U and V are re-defined as Uk \u2208 RN\u00d7k and VTk \u2208 Rk\u00d7N , respectively, with diagonal matrix S k \u2208 Rk\u00d7k. When we use LSA for topic modeling, A is defined as a term-document matrix. Then, UkS k and S kVTk are considered as term vectors and document vectors in the k-dimensional latent space, respectively."
        },
        {
            "heading": "3 Self-supervised Extrofitting",
            "text": "We consider the word embedding matrix as the term-document matrix; The x-axis of the matrix\nis vocabulary and the y-axis is (unknown) semantic dimension. Thus, as researchers have use the decomposition of the term-document matrix to get the term-vectors in LSA, the decomposition of the term-semantic matrix can be considered to represent term-vectors (US ) and semantic-vectors (S VT ). This intuition corresponds to the traditional methods in vector embedding, verified in Levy and Goldberg (2014); Allen et al. (2019).\nWith the idea, we first decompose word embeddings matrix W to make latent representations as follows: Wk = UkS kVTk As described above, we can get term-vectors, which are word representations in k-dimensional latent space, by computing UkS k. Adjusting the dimension of latent space (k in \u00a72), we calculate semantically related words using cosine similarity. For every word, we calculate its cosine similarity to all other words in the vocabulary. If the similarity exceeds a predetermined threshold, we group the words. Note that a group can contain only a single word. This process is repeated iteratively for each ungrouped word until all words are clustered.\nWe use the set of semantically related words (c\u2019) as the class (synonym pairs, c) of extrofitting instead of semantic lexicons:\nSelfExtro(W, c\u2032)\n= Trans(Expand(W, c\u2032), c\u2032)T Expand(W, c\u2032)\nTo sum up, we use the principle of vanilla extrofitting (Jo and Choi, 2018), which utilizes LDA algorithms to group in-domain instances (in this case, word vectors). Simultaneously, the algorithm pushes out-domain instances apart. Instead of using pre-defined lexicons for extrofitting, we use the idea of LSA to get semantically related words. Even if the number of extracted word pairs are small or the words are not meaningful, the process of making out-domain instances far can make better representations.\nIn the experiments, we start with pretrained GloVe (Pennington et al., 2014) (if we do not mention it explicitly) and set a threshold that determines whether a pair of words are semantically related. We use a high threshold (0.9 of cosine similarity) since type II error is rather better than type I error."
        },
        {
            "heading": "4 Related Works",
            "text": "The first successful post-processing approach was Retrofitting (Faruqui et al., 2015), which modi-\nfied word vectors by weighted averaging the word vectors with semantic lexicons. Extending from the simple idea, Counter-fitting (Mrk\u0161ic\u0301 et al., 2016a) used both synonym pairs to collect word vectors and antonym pairs to make word vectors the gene from one another. Next, Paragram embeddings (Wieting et al., 2015) used synonyms and negative sampling to collect the word vectors. Borrowing attract-terms from the Paragram embeddings and adding repel-terms, Attract-Repel (Mrk\u0161ic\u0301 et al., 2017) injected linguistic constraints into word vectors through predefined cost function with mono-/cross-lingual linguistic constraints. Explicit Retrofitting (ERCNT) (Glava\u0161 and Vulic\u0301, 2018) directly learned mapping functions of linguistic constraints with deep neural network architectures. They then used the functions to retrofit the word vectors. PostSpecialization (Vulic\u0301 et al., 2018; Ponti et al., 2018) resolved the problem of the previous models that only updates words in external lexicons. Some works have used cross-lingual resources to get further semantic information (Vulic\u0301 et al., 2019; Kamath et al., 2019).\nWhile the previous methods utilized text-level resources, vecmap (Artetxe et al., 2018) used other word embeddings as external resources.2 For fair comparisons, we input the same word vectors for source and target embeddings of the method.\nRecently, BERT-based model LexFit (Vulic\u0301 et al., 2021) was derived, but the method requires external resources."
        },
        {
            "heading": "5 Experiment 1: Word Similarity Tasks",
            "text": "Settings. Word similarity datasets consist of word pairs with human-rated similarity scores between the words and models calculate Spearman\u2019s correlation (Daniel, 1990) between the similarity\n2We selected unsupervised version of vecmap because it performs better than identical version.\nscores and the cosine similarity of the word vector pairs.\nWe use 6 datasets in English: WordSim353 (W(s) for similarity and W(r) for relation) (Finkelstein et al., 2001), RareWord (RW) (Luong et al., 2013), MEN-3k (ME) (Bruni et al., 2014), SemEval (SE) (Camacho-Collados et al., 2017), SimLex999 (SL) (Hill et al., 2015), and SimVerb-3500 (SV) (Gerz et al., 2016).\nResults. In Table 1, self-supervised extrofitting (SelfExtro) improves the performance on all the word similarity datasets when compared with the popular pretrained word vectors, GloVe (Pennington et al., 2014) and fastText (Bojanowski et al., 2016)3. The result implies the pretrained word vectors can be enriched by our method, which means it does not require any semantic lexicons to make better embeddings. In addition, SelfExtro shows better performances with the original extrofitting (+WordNet) on most of the evaluations.\nWe also investigate the extracted semantic information (see Appendix A.2). The extracted information hardly seems synonyms, but we can see that some similar words are grouped. As extrofitting affects all the word vectors, every word can benefit from the other words being enriched. In other words, although the number of extracted information is small, this simple extension makes extrofitting fully utilize its advantages.\nQualitative examples of self-supervised extrofitting are presented in Table 2. Although the representations lose similarity scores, the similar words become diverse and reasonable. Additional qualitative examples are shown in Appendix A.5.\nOur proposed SelfExtro is also potentially useful when semantic lexicon resources are scarce, such as in many non-English languages. In Ta-\n3word2vec could also be considered as a base model, but it takes too much time due to its large vocabulary size.\nble 3 and Table 4, we use pretrained fastText for WordSim datasets in 4 languages and MultiSimLex (Vulic\u0301 et al., 2020) in 8 languages, respectively. SelfExtro significantly increases the performance in all the languages."
        },
        {
            "heading": "6 Experiment 2: Dialogue State Tracking",
            "text": "Settings. Previous works (Mrk\u0161ic\u0301 et al., 2016a, 2017; Vulic\u0301 et al., 2019) showed that word vector post-processing is useful for dialogue state tracking (DST) (Young et al., 2010). Using Neural Belief Tracker (NBT) (Mrk\u0161ic\u0301 et al., 2016b), they have\nclaimed that the performance on the task is related to word vector specialization in that the model has to identify the flow of dialogue with only a few words, which seems reasonable. Thus, we use the NBT4 and check the model performances with our post-processed embeddings. Refer to the papers cited above for the details of DST.\nResults. Table 5 shows the performance of DST in Wizard-of-Oz 2.0 dataset (Wen et al., 2017). The results show that the semantic specialization (e.g., Attract-Repel) does not increase the performance. In contrast, Extro and SelfExtro show better performance than vanilla GloVe.\nWe additionally experiment with the lexicons (both synonyms and antonyms) included in the Github of Attract-Repel5. It shows only a little\n4https://github.com/nmrksic/ neural-belief-tracker\n5https://github.com/nmrksic/attract-repel\nperformance gain; we thus guess that the difference in DST performance comes from lexical resources or fine-tuning of NBT rather than specialization."
        },
        {
            "heading": "7 Experiment 3: Text Classification",
            "text": "Settings. We experiment with our methods in 2 different settings: fixed word vectors and trainable word vectors. When the word vectors are fixed, we can evaluate the usefulness of the word vectors per se. When the word vectors are trainable, we can see the improvement of the model performance in a conventional training setting. The dataset and classifier are described in Appendix A.3.\nResults. We report the performances in Table 6. The classifier initialized with SelfExtro outperforms the vanilla GloVe and performs on par with the original extrofitting. On the other hand, although the previous word vector post-processing methods specialize well on a domain-specific task, the approaches failed to be generalized; they show large performance deterioration despite the added general semantic information."
        },
        {
            "heading": "8 Discussion",
            "text": "The specialized word vectors do not warrant better performance. We conjecture that the methods trade off catastrophic forgetting against semantic specialization even though the previous methods successfully specialize their vectors into given lexicons, losing the pretrained information encoded in\nthe original GloVe. It affects the performance of the fundamental tasks, which are largely degraded. On the other hand, our method enriches the word vector in general, resulting in marginal but certain improvements.\nContextual representations provide substantial improvements in NLP tasks, but the representations lack semantic information due to their nature. Since the context and the semantics are different types of information that can enrich representations, we believe our approaches might further improve the contextual representations."
        },
        {
            "heading": "9 Conclusion",
            "text": "We develop a self-supervised retrofitting model that enriches word vectors without semantic lexicons. The method utilizes its own distribution of word vectors to get better representations. In the Exp. 1, we show that our method can improve the performance on word similarity tasks and present qualitative examples. It can be applied to other pretrained word vectors, resulting in better performances on all the word similarity datasets. SelfExtro also has potential advantages in lexicon-scarce languages. In the Exp. 2 and 3, we presented the effect of post-processing methods on downstream tasks. Our method shows marginal but certain improvements, while other post-processed word vectors largely degrade the performances, which seems the result of losing generalization."
        },
        {
            "heading": "10 Limitations",
            "text": "Usefulness compared with contextual embeddings Contextual embeddings are recently dominant in NLP tasks, whereas static word embeddings have become less frequently used. However, it does not mean static word embeddings are not useful. Although we can assume that the amount of pretraining dataset and training resources are similar, the cost of inference is much cheaper at static word embeddings. Furthermore, static word embeddings perform better when a task lacks enough context (e.g., word similarity tasks). It will be interesting future work to retrofit contextual embeddings, but it is out of scope in this paper.\nThe use of antonyms Although we believe that there are no definitely opposite meanings of the word (e.g., love and hate share the sense of emotion), previous works (see \u00a74) that utilize antonyms showed significant improvement in word similarity tasks. However, compared to the methods, self-supervised extrofitting explicitly considers synonyms only, but implicitly expects antonyms to be distant while maximizing the variances of inclass/out-of-class word vectors. Also, the process of self-supervised extrofitting makes it hard to incorporate other kinds of word relations.\nThe method is only a simple linear projection Both extrofitting and self-supervised extrofitting use linear projections in enriching vectors, following a traditional NLP method. The linear model might not be the best to reflect word relations in vector spaces, but we believe that it is a simple yet effective method, as we still calculate lots of things (e.g., distance) in a linear way."
        },
        {
            "heading": "Acknowledgement",
            "text": "The author would like to thank all the reviewers in several rounds of submission, a total of 5 years. Lastly, I am grateful to Alice Lee for her help in qualitative analysis."
        },
        {
            "heading": "A Appendix",
            "text": ""
        },
        {
            "heading": "A.1 Data Resources",
            "text": "A.1.1 Pretrained word vectors Pretrained word vectors include words composed of n-dimensional float vectors. One of the major pretrained word vectors we used is GloVe (Pennington et al., 2014) glove.42B.300d. Even though many word embedding algorithms and pretrained word vectors have been suggested, GloVe is still being used as a strong baseline on word similarity tasks (Cer et al., 2017; Camacho-Collados et al., 2017). We also use fastText (Bojanowski et al., 2016), and Paragram (Wieting et al., 2015) as resources and baseline of self-supervised extrofitting."
        },
        {
            "heading": "A.1.2 Semantic Lexicon",
            "text": "As an external semantic lexicon, we use WordNet (Miller, 1995), which consists of approximately 150,000 words and 115,000 synsets pairs. We use Faruqui et al.\u2019s WordNetall lexicon, comprised of synonyms, hypernyms, and hyponyms. Some recent works built their own or used large lexicons like BabelNet (Navigli and Ponzetto, 2012), but in order to observe the effect of post-processing algorithm rather than the power of lexicons, we use the (relatively) small lexicon.\nFor fair comparisons, we replace the previous works\u2019 lexicon with WordNetall lexicon. If the models require antonyms, we use the antonyms pairs, which are uploaded in their Github."
        },
        {
            "heading": "A.2 Example of Extracted Relation",
            "text": ""
        },
        {
            "heading": "A.3 Further Details in Experiment 3",
            "text": "Datasets. We use 5 classification datasets; DBpedia ontology (Lehmann et al., 2015), Yahoo!Answers (Chang et al., 2008)6, YelpRe-\n6https://cogcomp.seas.upenn.edu/page/ resource_view/89 Note that Chang et al. (2008) said the dataset has 20 top-level categories but actually it has 3\nviews (Zhang et al., 2015), AGNews, and IMDB (Maas et al., 2011). We utilize Yahoo!Answer dataset for 2 different tasks, which are classifying upper-level categories and classifying lower-level categories, respectively. We use all the words tokenized by space as inputs. The data information is described in Table 8.\nClassifier. We build TextCNN (Kim, 2014) rather than use a classifier based on Bag-of-Words (BoW), as Faruqui et al. did, in order to process word sequences. The classifier consists of 2 convolutional layers with the channel size of 32 and 16, respectively. We adopt the multi-channel approach, implementing 4 different sizes of kernels\u20132, 3, 4, and 5. We concatenate them after every max-pooling layer. The learned kernels go through an activation function, ReLU (Hahnloser et al., 2000), and max-pooling. We set the dimension of word embedding to 300, optimizer to Adam (Kingma and Ba, 2014) with learning rate 0.001, and use earlystopping if validation accuracy does not increase over 5 epochs."
        },
        {
            "heading": "A.4 Ablation Study in Threshold",
            "text": "When the latent dimension is 300, following the best performance:"
        },
        {
            "heading": "A.5 Qualitative Examples",
            "text": "The additional qualitative examples are presented in Table 12.\nduplicated top-level categories because of errors."
        }
    ],
    "title": "Self-supervised Post-processing Method to Enrich Pretrained Word Vectors",
    "year": 2023
}