{
    "abstractText": "There is growing interest in systems that generate captions for scientific figures. However, assessing these systems\u2019 output poses a significant challenge. Human evaluation requires academic expertise and is costly, while automatic evaluation depends on often low-quality author-written captions. This paper investigates using large language models (LLMs) as a cost-effective, reference-free method for evaluating figure captions. We first constructed SCICAP-EVAL,1 a human evaluation dataset that contains human judgments for 3,600 scientific figure captions, both original and machinemade, for 600 arXiv figures. We then prompted LLMs like GPT-4 and GPT-3 to score (1-6) each caption based on its potential to aid reader understanding, given relevant context such as figure-mentioning paragraphs. Results show that GPT-4, used as a zero-shot evaluator, outperformed all other models and even surpassed assessments made by Computer Science and Informatics undergraduates, achieving a Kendall correlation score of 0.401 with Ph.D. students\u2019 rankings.",
    "authors": [
        {
            "affiliations": [],
            "name": "Ting-Yao Hsu"
        },
        {
            "affiliations": [],
            "name": "Chieh-Yang Huang"
        },
        {
            "affiliations": [],
            "name": "Ryan Rossi"
        },
        {
            "affiliations": [],
            "name": "Sungchul Kim"
        },
        {
            "affiliations": [],
            "name": "Clyde Lee Giles"
        },
        {
            "affiliations": [],
            "name": "Ting-Hao \u2018Kenneth\u2019 Huang"
        }
    ],
    "id": "SP:72b92bfb1cc37aae9423aea2d99db02935638da2",
    "references": [
        {
            "authors": [
                "Anjana Arunkumar",
                "Lace Padilla",
                "Gi-Yeul Bae",
                "Chris Bryan"
            ],
            "title": "Image or information? examining the nature and impact of visualization",
            "year": 2023
        },
        {
            "authors": [
                "Dana Aubakirova",
                "Kim Gerdes",
                "Lufei Liu."
            ],
            "title": "Patfig: Generating short and long captions for patent figures",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2843\u2013 2849.",
            "year": 2023
        },
        {
            "authors": [
                "Iz Beltagy",
                "Kyle Lo",
                "Arman Cohan."
            ],
            "title": "SciBERT: A pretrained language model for scientific text",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Lan-",
            "year": 2019
        },
        {
            "authors": [
                "Charles Chen",
                "Ruiyi Zhang",
                "Sungchul Kim",
                "Scott Cohen",
                "Tong Yu",
                "Ryan Rossi",
                "Razvan Bunescu."
            ],
            "title": "Neural caption generation over figures",
            "venue": "Adjunct Proceedings of the 2019 ACM International Joint Conference on Pervasive and Ubiquitous Com-",
            "year": 2019
        },
        {
            "authors": [
                "Charles Chen",
                "Ruiyi Zhang",
                "Eunyee Koh",
                "Sungchul Kim",
                "Scott Cohen",
                "Ryan Rossi."
            ],
            "title": "Figure captioning with relation maps for reasoning",
            "venue": "Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 1537\u20131545.",
            "year": 2020
        },
        {
            "authors": [
                "Cheng-Han Chiang",
                "Hung-yi Lee"
            ],
            "title": "Can large language models be an alternative to human evaluations? arXiv preprint arXiv:2305.01937",
            "year": 2023
        },
        {
            "authors": [
                "Ting-Yao Hsu",
                "C Lee Giles",
                "Ting-Hao Huang."
            ],
            "title": "SciCap: Generating captions for scientific figures",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2021, pages 3258\u20133264, Punta Cana, Dominican Republic. Association for Compu-",
            "year": 2021
        },
        {
            "authors": [
                "Edward J Hu",
                "yelong shen",
                "Phillip Wallis",
                "Zeyuan AllenZhu",
                "Yuanzhi Li",
                "Shean Wang",
                "Lu Wang",
                "Weizhu Chen"
            ],
            "title": "LoRA: Low-rank adaptation of large language models",
            "venue": "In International Conference on Learning Representations",
            "year": 2022
        },
        {
            "authors": [
                "Chieh-Yang Huang",
                "Ting-Yao Hsu",
                "Ryan Rossi",
                "Ani Nenkova",
                "Sungchul Kim",
                "Gromit Yeuk-Yin Chan",
                "Eunyee Koh",
                "C Lee Giles",
                "Ting-Hao Huang"
            ],
            "title": "Summaries as captions: Generating figure captions for scientific documents with automated text",
            "year": 2023
        },
        {
            "authors": [
                "Shankar Kantharaj",
                "Rixie Tiffany Leong",
                "Xiang Lin",
                "Ahmed Masry",
                "Megh Thakkar",
                "Enamul Hoque",
                "Shafiq Joty."
            ],
            "title": "Chart-to-text: A large-scale benchmark for chart summarization",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Compu-",
            "year": 2022
        },
        {
            "authors": [
                "Minghao Li",
                "Tengchao Lv",
                "Jingye Chen",
                "Lei Cui",
                "Yijuan Lu",
                "Dinei Florencio",
                "Cha Zhang",
                "Zhoujun Li",
                "Furu Wei."
            ],
            "title": "Trocr: Transformer-based optical character recognition with pre-trained models",
            "venue": "Proceedings of the AAAI Conference on Artificial",
            "year": 2023
        },
        {
            "authors": [
                "Shengzhi Li",
                "Nima Tajbakhsh."
            ],
            "title": "Scigraphqa: A large-scale synthetic multi-turn question-answering dataset for scientific graphs",
            "venue": "arXiv preprint arXiv:2308.03349.",
            "year": 2023
        },
        {
            "authors": [
                "Thibault Sellam",
                "Dipanjan Das",
                "Ankur Parikh."
            ],
            "title": "BLEURT: Learning robust metrics for text generation",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7881\u20137892, Online. Association for Computational",
            "year": 2020
        },
        {
            "authors": [
                "Chenhui Shen",
                "Liying Cheng",
                "Yang You",
                "Lidong Bing"
            ],
            "title": "Are large language models good evaluators for abstractive summarization? arXiv preprint arXiv:2305.13091",
            "year": 2023
        },
        {
            "authors": [
                "Simeng Sun",
                "Ori Shapira",
                "Ido Dagan",
                "Ani Nenkova."
            ],
            "title": "How to compare summarizers without target length? pitfalls, solutions and re-examination of the neural summarization literature",
            "venue": "Proceedings of the Workshop on Methods for Optimizing and Evalu-",
            "year": 2019
        },
        {
            "authors": [
                "Benny Tang",
                "Angie Boggust",
                "Arvind Satyanarayan."
            ],
            "title": "Vistext: A benchmark for semantically rich chart captioning",
            "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 7268\u20137298.",
            "year": 2023
        },
        {
            "authors": [
                "Zheng Yan",
                "Iliyan Zarov",
                "Yuchen Zhang",
                "Angela Fan",
                "Melanie Kambadur",
                "Sharan Narang",
                "Aurelien Rodriguez",
                "Robert Stojnic",
                "Sergey Edunov",
                "Thomas Scialom"
            ],
            "title": "Llama 2: Open foundation and finetuned chat models",
            "year": 2023
        },
        {
            "authors": [
                "Jason Wei",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Maarten Bosma",
                "Fei Xia",
                "Ed Chi",
                "Quoc V Le",
                "Denny Zhou"
            ],
            "title": "Chain-of-thought prompting elicits reasoning in large language models",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Yunshu Wu",
                "Hayate Iso",
                "Pouya Pezeshkpour",
                "Nikita Bhutani",
                "Estevam Hruschka."
            ],
            "title": "Less is more for long document summary evaluation by llms",
            "venue": "arXiv preprint arXiv:2309.07382.",
            "year": 2023
        },
        {
            "authors": [
                "Zhishen Yang",
                "Raj Dabre",
                "Hideki Tanaka",
                "Naoaki Okazaki."
            ],
            "title": "Scicap+: A knowledge augmented dataset to study the challenges of scientific figure captioning",
            "venue": "arXiv preprint arXiv:2306.03491.",
            "year": 2023
        },
        {
            "authors": [
                "Jingqing Zhang",
                "Yao Zhao",
                "Mohammad Saleh",
                "Peter Liu."
            ],
            "title": "Pegasus: Pre-training with extracted gap-sentences for abstractive summarization",
            "venue": "International Conference on Machine Learning, pages 11328\u201311339. PMLR.",
            "year": 2020
        },
        {
            "authors": [
                "Tianyi Zhang",
                "Varsha Kishore",
                "Felix Wu",
                "Kilian Q. Weinberger",
                "Yoav Artzi"
            ],
            "title": "Bertscore: Evaluating text generation with bert",
            "venue": "In International Conference on Learning Representations",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "There has been increasing interest in automating caption generation for scientific figures in scholarly articles. The SCICAP dataset (Hsu et al., 2021), an extensive collection of scientific figures and captions from arXiv papers, encouraged more work in this area and led to several innovative approaches (Yang et al., 2023; Aubakirova et al., 2023; Tang et al., 2023; Li and Tajbakhsh, 2023). However, despite advances in caption generation for scientific figures, evaluating the results is still challenging. Human evaluation is costly as only domain experts can assess captions, making crowdsourced evaluation impractical. Meanwhile, automatic evaluation is not reliable. Huang et al. (2023) found that\n1SCICAP-EVAL is accessible at https://github.com/ Crowd-AI-Lab/SciCap-Eval.\ndomain-specific Ph.D. students did not prefer captions rated highly by automatic scores. Fortunately, recent findings in this area may provide solutions to the challenge. Figure captions were found to be similar to summaries of figure-mentioning paragraphs (Huang et al., 2023), i.e., most words in captions can be semantically traced back to these paragraphs, and captions can be effectively generated using text-summarization models. A missing piece from previous research is the use of this finding for evaluation, which we address in this paper.\nThis paper explores the cost-effective, reference-free evaluation of scientific figure captions using pre-trained large language models (LLMs). Figure 1 overviews the process. The intuition is that since a scientific figure\u2019s caption in scholarly articles often functions as a condensed summary of all figure-mentioning paragraphs (Huang et al., 2023), LLMs, when given the appropriate context, can effectively evaluate how well the caption captures the essential information from these contexts (Chiang and Lee, 2023; Shen et al., 2023; Wu et al., 2023).\nWe first introduced SCICAP-EVAL, a human evaluation dataset for scientific figure captions that contains 600 figures from arXiv papers. For ev-\nery figure, we collected six captions: one authored by the original paper authors and five machinegenerated. Two participant groups independently assessed these captions. The first group, Ph.D. students from the figure\u2019s domain, ranked the six captions for each figure. The second group, undergraduate Computer Science and Informatics students, rated each caption\u2019s helpfulness. We used Ph.D. students for their in-depth understanding but recognized their limited availability. Undergraduates were easier to recruit, but their knowledge might be insufficient. We aimed to examine if undergraduates could replace Ph.D. students in building a human evaluation dataset for scientific figure captions. Equipped with SCICAP-EVAL, we then prompted LLMs like GPT-4 and GPT-3.5 to assign a score (1- 6) to each caption, based on how helpful they could be for readers to understand the paper. The results showed GPT-4\u2019s effectiveness as a zero-shot evaluator. It achieved a 0.401 Kendall correlation with Ph.D. students\u2019 rankings, surpassing all other models and even exceeding evaluations by Computer Science and Informatics undergraduates.\nThis paper presents three contributions. Firstly, we have validated the effectiveness of using LLM to evaluate figure captions, exploring an avenue previously untouched with such techniques. Secondly, we have developed a new dataset, cultivated over a few months with an investment of over $3,300, poised to serve as a benchmark for subsequent research. We recruited twenty undergraduates, each handling 20 batches, with a completion time of 30 minutes to an hour per batch. Additionally, fifteen Ph.D. students participated, each completing four batches, each batch spanning 30 minutes to an hour. Lastly, our work resonates with the broader NLP community\u2019s drive to discover fresh applications for LLMs. In cases where human evaluation is costly and reference-based automatic evaluation is unreliable, LLMs offer an efficient alternative."
        },
        {
            "heading": "2 Related Work",
            "text": "Assessing scientific figure captions is both costly and time-consuming due to the need for specialized knowledge and understanding of the entire scholarly article. This has previously limited evaluations to a small scale, typically performed by graduate students in the field. For instance, Huang et al. (2023) only managed to employ 3 Ph.D. students with NLP backgrounds to evaluate 90 figures\u2019 captions. In a similar vein, Yang et al.\n(2023) could only utilize 2 human annotators to evaluate 100 figure captions. Automatic evaluation, on the other hand, has been more broadly used for its cost-effectiveness and ease. Metrics such as BLEU, ROUGH, BLEURT, CIDEr, BERTScore,and MoverScore are common (Kantharaj et al., 2022; Huang et al., 2023; Yang et al., 2023; Chen et al., 2019, 2020; Sellam et al., 2020; Zhang* et al., 2020), using human-written captions as references. However, these metrics are limited, including favoring longer captions (Sun et al., 2019) and relying on often poorly-written human-generated captions. For example, Huang et al. (2023) found that models preferred by humans often scored lower in automatic evaluations. These challenges in evaluating scientific figure captions motivate our pursuit of cost-effective, referencefree evaluation techniques."
        },
        {
            "heading": "3 SCICAP-EVAL Dataset",
            "text": "SCICAP (Hsu et al., 2021) is made of over 416,000 line charts (which were referred to as \u201cgraph plots\u201d in (Hsu et al., 2021) and 133,543 of them are singlepanel) extracted from arXiv papers in Computer Science (cs.*) and Machine Learning (stat.ML). In our paper, we focused on SCICAP\u2019s single-panel figures in the NLP, CV, and HCI domains. Most of these figures are 2-dimensional and annotated with axes labels, legends, and other texts. By Arunkumar et al. (2023)\u2019s definition, these figures are \u201cinformation\u201d visualization rather than \u201cimage\u201d.\nWe randomly sampled 600 figures from SCICAP, with each cs.CV, cs.CL, cs.HC domain providing 200 figures. We then obtained six distinct captions for each figure, primarily sourced from Huang et al. (2023) (approach ii to v), as follows: (i) Author-written captions; (ii) PegasusP , the captions created by the fine-tuned Pegasus model (Zhang et al., 2020), using figure-mentioning paragraphs as the input; (iii) PegasusP+O, same as PegasusP but additionally incorporating figures\u2019 OCR as input; (iv) PegasusO, same as PegasusP but solely relied on OCR as input; (v) TrOCR (Li et al., 2023), the fine-tuned vision-to-language model; (vi) Template-based approach, which filled predefined templates with identified axis names.\nWe then recruited the following two groups of participants to independently annotate the quality of these 3,600 captions. Thus, each caption was assessed twice. The entire study took a few months and cost over $3,300. We engaged 20 undergrad-\nuate participants, each responsible for 20 batches. Each batch consisted of 30 figures, and the time taken to finish each batch ranged from 30 minutes to an hour. Additionally, 15 Ph.D. students were recruited, with each student tasked to rank 4 batches. Each batch comprised 10 figures, each having 6 different captions. The time taken by Ph.D. students for each batch varied from 30 minutes to an hour. The authors\u2019 institute\u2019s Institutional Review Board (IRB) approved this study.\nHuman assessments by Ph.D. students on a sixitem ranking task. We recruited 15 Ph.D. students (who are not coauthors of this paper) specializing in CV, NLP, and HCI to rank six captions for each of the 40 figures in their respective fields. Specifically, we formed a ranking task with six items, where each Ph.D. student manually ranked 6 different captions for a figure. These rankings were based on how helpful the captions were in understanding the figures. The annotation interface is shown in Figure 2 in Appendix A. Participants were compensated at $20 per hour, with annotation times varying per individual. To measure data quality, we recruited two non-coauthor Ph.D. students to annotate a specific batch for assessing inter-annotator agreement, which yielded a Kendall of 0.427.\nHuman assessments by undergraduate students on a three-class labeling task. We recruited 20 STEM-focused undergraduate students, primarily Computer Science and Informatics majors,2 to rate the helpfulness of each caption with a \u201cYes,\u201d \u201cNo,\u201d or \u201cUnsure\u201d rating. We turned to the three-class labeling task as we realized that the six-item ranking task was too challenging for undergraduate students, which resulted in a long completion time. We understand using different annotation tasks would raise concerns of comparability between Ph.D. and undergraduate students\u2019 assessments, but this is the acceptable trade-off to meaningfully collect a broader set of undergraduate students\u2019 feedback. For each participant, we assigned five batches covering three different domains, with each batch comprising 30 unique figure captions. In addition to the helpfulness rating, students were asked to identify errors in the extracted figure or caption and label caption features. Full instructions, the user interface, and statistics of the annotation can be found in Appendix B. Compensation was $15\n2Computer Science: 5, Biochemistry: 1, Biology: 1, Statistics: 1, Information Sciences and Technology: 12.\nper batch, with annotation times ranging from 30 minutes to one hour. To ensure annotation quality, we provided each participant with a tutorial using an additional 30 figures."
        },
        {
            "heading": "4 Methods",
            "text": "In this section, we describe different approaches used for caption rating, including zero-shot, fewshot, and Chain-of-Thought prompting with LLMs, and fine-tuning a classifier using the data collected.\nZero-Shot and Few-Shot Prompting with LLMs. We included both the target caption and the figurementioning paragraphs from the paper (e.g., \u201cAs shown in Figure 4,...\u201d), and then asked the LLM to evaluate the caption\u2019s quality. For the few-shot setting, we included three randomly selected captions with the highest and lowest rankings from Ph.D. students\u2019 evaluations in the prompt. See Appendix D for the actual prompts.\nChain-of-Thought Prompting (Wei et al., 2022) (QA, QA-YN). Our intuition is that an effective figure caption distills the essential information from pertinent paragraphs. Acting on this intuition, we implemented a Chain-of-Thought approach to prompt LLMs to calculate evaluation scores. We first provided the LLMs with figure-mentioning paragraphs, asking them to generate questions that a suitable caption should answer. Following this, the LLMs were presented with a caption and asked to identify whether it could answer each question (Yes/No). The final evaluation score of the caption was then derived from the percentage of \u201cYes\u201d responses. In this paper, we explored two question types: open-ended (QA) and yes/no questions (QA-YN). See Appendix D for the prompts used.\nClassifiers learned from SCICAP-EVAL data. We fine-tuned the SciBERT classifier (Beltagy et al., 2019) with SCICAP-EVAL data to predict figure captions\u2019 helpfulness (Yes/No)."
        },
        {
            "heading": "5 Experimental Results",
            "text": "Experimental setups. We conducted experiments with four LLMs: GPT-4 (OpenAI, 2023), GPT-3.5 (Brown et al., 2020), Falcon (Almazrouei et al., 2023), and LLaMA-2 (Touvron et al., 2023), across various settings. We parsed the prediction scores from the outputs. When the LLMs failed to predict, we allocated the lowest score of 1. For the SciBERT classifier, we split the dataset into\nModel Setting/ Training Data\n(a) Correlation with PhD Students\u2019 Reversed Rank\n[Good to Bad Caption: 6, 5, ..., 2, 1]\nCorrelation with PhD Students\u2019 Reciprocal Rank\n(b) Original Order (Picking good samples) [Good to Bad Caption:\n1 1 , 1 2 , ..., 1 5 , 1 6 ]\n(c) Reversed Order (Spotting bad samples) [Good to Bad Caption:\n1 6 , 1 5 , ..., 1 2 , 1 1 ]\n\u03c1 \u03c4 rs \u03c1 \u03c1\nGPT-4\nZero-Shot .501 .401 .491 .391 -.492 Few-Shot .531 .429 .523 .425 -.513 CoT (QA) .283 .270 .315 .277 -.223 CoT (QA-YN) .357 .334 .400 .324 -.307\nGPT-3.5\nZero-Shot .465 .370 .462 .383 -.433 Few-Shot .462 .371 .462 .403 -.402 CoT (QA) .270 .276 .347 .249 -.226 CoT (QA-YN) .365 .329 .404 .317 -.327\nLLaMA 2-70B Zero-Shot .407 .342 .413 .326 -.392Few-Shot .424 .353 .430 .335 -.405\nFalcon-7B Zero-Shot .026 .048 .055 .018 -.030Few-Shot .044 .048 .058 .044 -.035\nFalcon-40B Zero-Shot .169 .137 .167 .152 -.136Few-Shot .150 .119 .143 .126 -.137\nSciBERT* Undergrad .329 .290 .329 .261 -.319PhD .372 .308 .379 .372 -.372\nHuman Undergrad .221 .195 .221 .206 -.196\nTable 1: Correlation coefficients (Pearson \u03c1, Kendall \u03c4 , and Spearman rs) of model output ratings versus Ph.D. students\u2019 assessments (N=3,159, excluding error cases), based on different rank conversions: (1) Reversed Rank, (2) Reciprocal Rank, and (3) Reversed Reciprocal Rank.*: SciBERT\u2019s performance was evaluated on the test split, comprising only 10% of the entire dataset.\n80/10/10 for train/validation/test and used the best model from the validation set for final testing. We excluded data marked as errors in undergrads\u2019 annotations, resulting in 3,159 valid samples. We fine-tuned SciBERT on undergraduate and Ph.D. annotations, respectively. Captions ranked bottom three by Ph.D. students were labeled as \u201cNo\u201d and the top three as \u201cYes\u201d. We treated \u201cNo\u201d and \u201cUnsure\u201d as \u201cNo\u201d for undergraduate labels. The model training details and decoding configuration are described in Appendix C.\nGPT-4 prompting achieved the highest correlations with Ph.D. students\u2019 assessments. Table 1(a) shows that GPT-4, prompted in either a zero-shot or few-shot manner, exhibited the strongest correlations with Ph.D. students\u2019 judgments, surpassing all other models and even undergraduate students\u2019 ratings. Meanwhile, despite Yes-No questions yielding better results than openended questions, our Chain-of-Thought approaches generally underperformed. We hypothesize that answering questions may only partly determine the helpfulness of a caption. More research is needed to develop improved workflows.\nGPT-4 is better at spotting bad examples than selecting good examples. We conducted additional analysis to determine whether LLMs as automatic caption evaluators are more effective at identifying good or bad examples. Table 1 displays the Pearson (\u03c1) correlation coefficients between each model\u2019s ratings and the reciprocal rank of Ph.D. students. The original order of reciprocal rank (b) places more weight on top selections, with higher scores indicating a model\u2019s effectiveness at identifying top examples. Conversely, the reversed order of reciprocal rank (c) prioritizes bottom selections, with higher absolute scores signifying the model\u2019s proficiency at pinpointing poor examples. Table 1 shows that GPT-4, in either zero-shot or few-shot prompting, excels at identifying low-quality over high-quality examples. This suggests its utility in cleaning training data by detecting poor samples."
        },
        {
            "heading": "6 Discussion",
            "text": "Do we need paragraphs that mention figures for this approach? To delve deeper into GPT-4\u2019s capability to evaluate captions, we conducted an ablation study to examine the influence of figurementioning paragraphs in the prompt. The results,\npresented in Table 2, indicate that GPT-4\u2019s performance drops a lot when paragraphs are not presented, highlighting the need for paragraphs.\nToward personalized caption generation. A goal of having undergraduate students to rate captions was to assess their potential to replace Ph.D. students in constructing human evaluation datasets for scientific captions, given their easier recruitment. However, their \u201chelpfulness\u201d ratings did not correlate well with those of Ph.D. students, suggesting different reader groups may perceive \u201chelpfulness\u201d differently. We further probed this by correlating caption features (annotated by undergraduates, see Appendix B) with the helpfulness assessments by Ph.D. and undergraduate students. Table 3 shows that for Ph.D. students, a caption\u2019s helpfulness hinges most on its reference to terms and text from the figure (OCR), while undergraduate students prioritize captions that state the figure\u2019s takeaway messages (Takeaway). Furthermore, a brief post-study survey revealed variations among Ph.D. students\u2019 assessment criteria. While the accuracy of information was a priority for most, some students focused on the ability of the caption to summarize the figure image, and others evaluated the volume of information conveyed within the caption. These observations suggest that future caption generation systems could focus on personalization to meet the needs of various reader groups.\nFine-tuning LLMs to predict Helpfulness. We do believe that fine-tuning LLMs presents a promising direction. However, our preliminary experiments with fine-tuning LLaMA-2 with LoRA (Hu et al., 2022) (both the 7B and 13B models) have not yielded encouraging results thus far. Pearson correlation with Ph.D. Students\u2019 Reversed Rank is 0.164 (7B) and 0.183 (13B) respectively. See Appendix C for training details. We believe it requires more exploration in the future.\nHow did the appearance and data domain of the images impact perceived utility? This work focuses on the utility of varied caption text of figures rather than the utility of the figures\u2019 images. We have chosen to focus on captions as we believe captions provide the most feasible entry point for technologies to intervene and boost the utility of figures in scholarly articles. Captions, often standalone like the caption{} label in LATEX, can be revised or replaced without altering the figure image. No raw chart data is strictly needed for improving captions. Using technology to customize or improve caption text is more achievable than altering figure images automatically, especially given the capabilities of LLMs. In the long run, we agree that the utility of the captions can be considered jointly with the figure images\u2013 as a good caption would not save a poorly designed visualization\u2013 but this is a future direction and beyond this paper\u2019s scope."
        },
        {
            "heading": "7 Conclusion and Future Work",
            "text": "In conclusion, we have shown the capacity of LLMs, specifically GPT-4, to evaluate scientific figure captions effectively. This work produced SCICAP-EVAL, a human evaluation dataset of 3,600 captions, both original and machinegenerated, for 600 arXiv figures. Using SCICAPEVAL, we confirmed GPT-4\u2019s ability to assess caption quality when supplied with context from figure-mentioning paragraphs. GPT-4 even outperformed Computer Science and Informatics undergraduates, achieving a Kendall correlation score of 0.401 against Ph.D. students\u2019 ratings. Future work will leverage this evaluation capability to cleanse noisy training data, a well-known challenge in figure caption generation (Huang et al., 2023). Furthermore, we plan to investigate the personalized captions generation to cater to individual reader requirements. Last, we will explore ways to consider caption factuality in the evaluation metrics."
        },
        {
            "heading": "Acknowledgements",
            "text": "We are grateful to the anonymous reviewers for their insightful feedback and to all the participants in our user study. We would also like to thank Tong Yu from Adobe for his feedback on the draft. This research was made possible through support from Adobe\u2019s gift funds and seed funding from Pennsylvania State University\u2019s College of Information Sciences and Technology (IST).\nLimitations\nDespite its potential, our work is subject to several limitations. Firstly, similar to the method of Huang et al. (2023), our approach requires figurementioning paragraphs to act as the context for effective LLM-based caption evaluation. However, mention identification is not always straightforward in real-world data; for example, no mentions were identified for 18.81% of figures in the original dataset. Secondly, our best results were achieved with a closed-source LLM, implying we inherit limitations such as restricted understanding of the model\u2019s training and data sources. Thirdly, our evaluation approach does not prioritize verifying the accuracy of the information within a caption, potentially leading to high ratings for inaccurate captions. Finally, as our methodology is solely textdependent, it cannot capture any figure\u2019s visual elements not mentioned in the text.\nEthics Statement\nThe proposed technology is considered low-risk as inaccurate figure caption assessments are unlikely to harm readers significantly. However, it is worth noting that our text-based approach inherently overlooks visual content, which could potentially influence the accessibility of the technology in evaluating figure captions."
        },
        {
            "heading": "A Annotating Interface for Ph.D. Students",
            "text": "Figure 2 shows the ranking task interface used by the Ph.D. students in Section 3."
        },
        {
            "heading": "B Annotating Instruction for Undergraduate Students",
            "text": "Figure 3 shows the rating task interface used by the undergraduate students in Section 3. We inquired whether the figure-caption combination contained any of the following four errors (Yes/No):\n\u2022 Image Extraction Error: The image we extracted from the PDF (shown above) has obvious errors (e.g., not containing the complete figure, containing parts that are not figures, damaged image, etc.)\n\u2022 Text Extraction Error: The text we extracted from the PDF (shown above) has obvious errors (e.g., not containing the complete caption, containing extra text that is not the caption, incorrect text recognition, etc.)\n\u2022 Not a Line Chart: This figure is not a line chart.\n\u2022 Compound Figure: This figure is a compound figure that contains multiple subfigures.\nThe resulting error counts are shown in Table 4. We also asked participants to annotate whether the caption contains any of the following six aspects (Yes/No):\n\u2022 OCR (Optical Character Recognition): Does this caption mention any words or phrases that appear in the figure? (Examples include the figure title, X or Y axis titles, legends, names of models, methods, subjects, etc.)\n\u2022 Visual: Does this caption mention any visual characteristics of the figure? (Examples include color, shape, direction, size, position, or opacity of any elements in the figure.)\n\u2022 Stats: Does this caption mention any statistics or numbers from the figure? (For example, \u201c20% of...\u201d or \u201cThe value of .. is 0.33...\u201d.)\n\u2022 Relation: Does this caption describe a relationship among two or more elements or subjects in the figure? (For example, \u201cA is lower than B,\u201d \u201cA is higher than B,\u201d or \u201cA is the lowest/highest in the figure.\u201d)\n\u2022 Takeaway: Does this caption describe the high-level takeaways, conclusions, or insights the figure tries to convey?\n\u2022 Helpfulness: Does this caption help you understand the figure?"
        },
        {
            "heading": "C Training and Decoding Details",
            "text": "We describe the model training details and the decoding configuration used in Section 5.\nTraining Details for Classification models. We fine-tune SciBERT3 checkpoint from HuggingFace for 100 epochs, using batch size = 32, learning rate = 5e-5 with a linear decay scheduler, warmup steps = 500, weight decay = 0.01. We evaluate every 100 steps, and the checkpoint with the highest f1 on validation set is kept and used to predict final result.\nFor our fine-tuning LLaMA-2, we modeled it as a text generation task. The input includes information from figure-mentioning paragraphs and target captions, and the model outputs evaluation for each aspect in a specific format (e.g., [helpfulness: yes]). The model was trained with a lora_rank = 8, learning rate = 5e-5, batch size = 16, and we trained the model for 50 epochs. We kept the last checkpoint for evaluation.\nDecoding Details for Open Large Language models. Parameter settings for Falcon-7b,4 Falcon-40b,5 and LLaMA 2-70b6 are the same: do_sample = True, top_p = 0.95, min_new_tokens = 10, max_new_tokens = 200, repetition_penalty = 1.0, temperature = 0.1, num_beams = 1. Note that all the models used in the experiment are the instruction-following model.\n3We used allenai/scibert_scivocab_uncased. 4We used tiiuae/falcon-7b-instruct. 5We used tiiuae/falcon-40b-instruct. 6We used meta-llama/Llama-2-70b-chat-hf."
        },
        {
            "heading": "D Prompts used for LLMs",
            "text": "In this section, we provide the prompt we used in Section 4. [PARAGRAPHS] and [CAPTION] are placeholders for figure-mentioning paragraphs and the target caption.\nZero-shot prompting. \u201cGiven the paragraphs and caption below, please rate the level of usefulness of the caption from 1 to 6 based on how well the caption could help readers understand the important information. 6 is the highest; 1 is the lowest. Please also explain your rating. Paragraphs: [PARAGRAPHS]. Caption: [CAPTION]\u201d\nFew-shot prompting. \u201cGiven the paragraph and caption below, please rate the level of usefulness of the caption from 1 to 6 based on how well the caption could help readers understand the important information. 6 is the highest; 1 is the lowest. Please also explain your rating. The following are 3 examples of high-quality captions: Best-1, Best-2, Best-3. The following are 3 examples of low-quality captions: Worst-1, Worst-2, Worst-3. Paragraphs: [PARAGRAPHS]. Caption: [CAPTION]\u201d\nChain-of-Thought prompting. Here are the prompts used for generating questions and obtaining answers for each question. We explore two types of questions, open-ended (QA) and yes/no questions (QA-YN), and prompts only difference including yes or no in question generation:\n\u2022 Open-ended Question Generation: \u201cThe following are paragraphs from a paper that mentioned figure-index. Based on these paragraphs, please generate at most five questions that the caption of figure-index should be able to answer. These questions quite be interesting and useful to the readers of the paper, who are mostly researchers in domain and AI.\u201d\n\u2022 Yes/No Question Generation: \u201cThe following are paragraphs from a paper that mentioned figure-index. Based on these paragraphs, please generate at most five yes or no questions that the caption of figure-index should be able to answer. These questions quite be interesting and useful to the readers of the paper, who are mostly researchers in domain and AI.\u201d\n\u2022 Answer: \u201cThe following is the caption of figure-index. Does this caption answer each\nquestion? Please answer Yes or No one by one and explain why or why not. Do not repeat the question.\u201d"
        }
    ],
    "title": "GPT-4 as an Effective Zero-Shot Evaluator for Scientific Figure Captions",
    "year": 2023
}