{
    "abstractText": "Warning: this paper includes model outputs showing offensive content. Recent large-scale Visual-Language Generative Models (VLGMs) have achieved unprecedented improvement in multimodal image/text generation. However, these models might also generate toxic content, e.g., offensive text and pornography images, raising significant ethical risks. Despite exhaustive studies on toxic degeneration of language models, this problem remains largely unexplored within the context of visual-language generation. This work delves into the propensity for toxicity generation and susceptibility to toxic data across various VLGMs. For this purpose, we built ToViLaG, a dataset comprising 32K co-toxic/mono-toxic text-image pairs and 1K innocuous but evocative text that tends to stimulate toxicity. Furthermore, we propose WInToRe, a novel toxicity metric tailored to visual-language generation, which theoretically reflects different aspects of toxicity considering both input and output. On such a basis, we benchmarked the toxicity of a diverse spectrum of VLGMs and discovered that some models do more evil than expected while some are more vulnerable to infection, underscoring the necessity of VLGMs detoxification. Therefore, we develop an innovative information bottleneckbased detoxification method. Our method reduces toxicity while maintaining acceptable generation quality, providing a promising initial solution to this line of research.",
    "authors": [
        {
            "affiliations": [],
            "name": "Xinpeng Wang"
        },
        {
            "affiliations": [],
            "name": "Xiaoyuan Yi"
        },
        {
            "affiliations": [],
            "name": "Han Jiang"
        },
        {
            "affiliations": [],
            "name": "Shanlin Zhou"
        },
        {
            "affiliations": [],
            "name": "Zhihua Wei"
        },
        {
            "affiliations": [],
            "name": "Xing Xie"
        }
    ],
    "id": "SP:48031fc4a7f4e4827919d7f3869a307dc4832c6a",
    "references": [],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Thriving on the capabilities of Transformer architectures (Vaswani et al., 2017), language/visual pretraining (Devlin et al., 2019; Dosovitskiy et al.; Radford et al., 2021) and diffusion models (Ho et al., 2020), recent large-scale Visual-Language Generation1 Models (VLGMs) have made extraor-\n\u2217Work done as an intern at MSRA mentored by X. Yi. \u2020Corresponding authors: Z. Wei and X. Yi\n1By VLG we mean both generation directions between the two modalities, different from previous work (Li et al., 2022)."
        },
        {
            "heading": "A group of people in street with car and ass around.",
            "text": "dinary advances in text and image creation, empowering various downstream tasks, like captioning, VQA (Li et al., 2022), image synthesis (Ramesh et al., 2022) and editing (Brooks et al., 2023).\nDespite such versatility, VLGMs are still observed to produce offensive language from given images or pornographic/violent pictures from input text prompts, i.e., toxic degeneration (Gehman et al., 2020a), even if the training data is carefully crafted and contains few toxic samples, as shown in Fig. 1, raising profound social and ethical risks. Moreover, innocuous input without sensitive words can also spark toxic output, indicating the inadequate efficacy of simple input filters.\nThe literature has demonstrated some responses in addressing social biases in VL datasets (Birhane et al., 2021; Wang et al., 2022b) and models (Cho et al., 2022; Wang et al., 2022a), while the matter of toxicity remains largely unexplored. In the area of Natural Language Generation (NLG), a variety of endeavours have been made for toxicity evaluation (Gehman et al., 2020a) and language model detoxification (Dathathri et al., 2020; Liu et al., 2021). Nevertheless, the approaches and metrics devised for NLG are not directly applicable to VLG. This necessitates a tailored framework for addressing the toxicity problem in VLG.\nIn this work, we delve into the toxicity problem of VLG and respond to the following three research questions. Q1 How to measure the toxicity of VLGMs, and to what extent do different models present toxicity? We construct ToViLaG, a dataset with 32k toxic text-image pairs in three categories: i) mono-toxic pairs (only the text or image is toxic), ii) co-toxic pairs (both are toxic) and iii) non-toxic provocative prompts that are likely to provoke toxic generated images. Furthermore, we design a novel toxicity metric, WInToRe, to theoretically tackle the defects of existing metrics in NLG (Gehman et al., 2020a), e.g., ignorance of input toxicity and sensitivity to sampling hyperparameters. Q2 How does the toxicity level change with varied model scale and data cleanliness? The development of VLG is still in an early stage. Thus, we not only benchmark the toxicity of VLGMs in diverse architectures and model sizes but also inject varying degrees of toxicity into them. This simulates a future situation of increased model scale and crawled unclean data, providing a foresight of the safe development of VLG. Studies on Q1&2 manifest that VLGMs trained with relatively clean data also produce more toxicity than expected, and simple content filtering might fail, which would further deteriorate in the foreseeable future. These problems pose Q3 What are the strategies to achieve detoxification while maintaining generation quality? We propose a novel detoxification loss which fine-tunes a small detoxification layer in VLGMs to reduce the toxicity information while maximizing the probability of generating targets. We prove that minimizing this loss is equivalent to optimizing the information bottleneck (Tishby et al., 2000), offering a promising initial solution in this direction.\nIn summary, our contributions are as follows:\n\u2022 To our best knowledge, we are the first to investigate the toxicity problem in the context of VLG and establish a systematic framework.\n\u2022 We collect a toxic text-image dataset, propose a novel metric tailored to VLG, benchmark the toxicity of a spectrum of VLGMs and conduct a comprehensive analysis in varying settings.\n\u2022 We design a lightweight detoxification method with a theoretical guarantee, which mitigates toxicity while keeping the satisfactory quality of VLG, acting as an effective preparatory step for this research direction."
        },
        {
            "heading": "2 Related Work",
            "text": "Visual-Language Generation In the era of Transformer and pretraining, multimodal generation, particularly text-to-image (T2I) and image-totext (I2T), models have made remarkable breakthroughs, revolutionizing industries and unlocking unparalleled opportunities for creative applications.\nIn T2I generation, building on the diffusion techniques (Ho et al., 2020; Song et al.), Stable Diffusion (Rombach et al., 2022) can produce indistinguishable high-quality images from arbitrary text prompts, igniting the prosperity of AIGC. DALLE-2 (Ramesh et al., 2022) and CogView (Ding et al., 2021) further scale models on tens/hundreds of millions of image-text pairs and up to billions of parameters, allowing the generation of superresolution images. On the other hand, to reduce the substantial cost of data collection and model training, LAFITE (Zhou et al., 2022) utilizes the well-aligned VL semantic space from a powerful pretrained backbone CLIP (Radford et al., 2021) to learn T2I generation without text data. Similarly, CLIP-GEN (Wang et al., 2022f) requires only unlabeled images, leveraging the language-image priors from CLIP. All these models have demonstrated human-level quality and ingenuity in creation.\nI2T generation, namely producing textual descriptions of given images, has also gained increasing interest and popularity. CLIP-ViL (Shen et al., 2022) uses CLIP\u2019s visual encoder for diverse downstream VL tasks. To better align images and text, Oscar (Li et al., 2020) utilizes object tags identified in images as anchor points for training. SimVLM (Wang et al., 2021) is trained with the single objective of PrefixLM on a largescale weakly labeled dataset to reduce the need for expensive annotations. BLIP (Li et al., 2022) bootstraps the text domain by generating synthetic captions and then conducts joint learning of VL understanding and generation. OFA (Wang et al., 2022e) unifies a diverse set of VL and unimodal tasks by following instruction-based learning in a sequence-to-sequence manner. GIT (Wang et al., 2022d) treats visual features as tokens and unifies them in a single Transformer decoder by language modeling. LLaVa (Liu et al., 2023) makes a first step towards visual instruction tuning using GPT-4 generated instruction-following samples.\nExcept for the uni-direction generation, some work explores the bidirectional framework capable for both T2I and I2T generation tasks (Huang et al.,\n2021, 2022; Aghajanyan et al., 2022; Kim et al., 2022; Diao et al., 2022). In this paper, we mainly focus on unimodal generation tasks and plan to investigate the bidirectional ones in the future.\nHarmful Content in Generation The NLG community has observed an inherent susceptibility of Large Language Models (LLMs) to internalize deleterious information in web-sourced data and produce toxic text (Dathathri et al., 2020), driving continuous efforts on toxicity investigation. This line of research covers the construction of toxicity evaluation dataset and metrics (Gehman et al., 2020a), toxic text detection (Lees et al., 2022), and implicit toxicity recognition (ElSherief et al., 2021; Hartvigsen et al., 2022). An extensive variety of NLG detoxification methods have also been developed, from domain-adaptive training (Dale et al., 2021; Wang et al., 2022c) to plug-and-play constraints (Liu et al., 2021; Geva et al., 2022; Yang et al., 2023). However, these datasets, metrics and methods are not directly applicable to VLG.\nWithin the realm of VLG, potential moral hazards draw growing attention, and some research has been committed to handling social biases. Wang et al. (2022b) present REVISE, a tool to analyze biases in visual datasets according to objects, gender, and geography. Birhane et al. (2021) examine the popular LAION-400M dataset and identify problematic content. Cho et al. (2022) assesses gender and racial biases in various T2I models like DALLE. Hirota et al. (2022) propose a LIC metric to measure bias amplification in I2T generation. Wang et al. (2022a) further develop normatively grounded measurement techniques to identify each type of harm caused by biases. Berg et al. (2022) design a retrieval-based metric and propose a prompt-tuningbased adversarial debiasing method. Despite such progress in social bias, how to measure and mitigate toxicity in VLG is still an open challenge."
        },
        {
            "heading": "3 Towards VLG Toxicity Investigation",
            "text": "We develop a systematic solution to study VLG toxic degeneration: Sec. 3.1 presents our ToViLag dataset, Sec. 3.2 introduces the toxicity detection, Sec. 3.3 demonstrates the WInToRe metric, and Sec. 3.4 provides the detoxification method, SMIB."
        },
        {
            "heading": "3.1 ToViLaG Dataset Construction",
            "text": "We construct the ToViLaG (Toxicity in Visual Language Generation) set for VL toxicity eval-\nuation and detoxification. In the language domain, we consider a wide range of toxicity (e.g., offensiveness, threat and sexual content), defined and identified by the PerspectiveAPI following (Gehman et al., 2020a). In the visual domain, we assess three toxicity types: pornographic, bloody and violent. Then, we build three categories of data.\n(1) Mono-toxic pairs. Only one side of such pairs is toxic, namely (a) <toxic image, non-toxic text> and (b) <toxic text, non-toxic image>. To construct (a), we first collect the three kinds of toxic images. We gather pornographic images from the NSFW dataset2, violent images from the UCLA Protest Image Dataset (Won et al., 2017) that contains human-annotated violence in protest events, and bloody images crawled from the Web. We then use GIT (Wang et al., 2022d) to generate captions (text) for these toxic images. The PerspectiveAPI, PPL, CLIPScore (Hessel et al., 2021) and Jaccard similarity are utilized to filter out undesired captions and only keep the nontoxic, high-quality and semantically diverse ones. For (b), we first detect and collect such pairs from existing VL datasets, including COCO (Lin et al., 2014), Flickr30K (Young et al., 2014), and CC12M (Changpinyo et al., 2021), which only account for a small portion. To further augment them, we rewrite the non-toxic captions into toxic ones by replacing a few carefully selected words and with the toxic ones using the classifier fBERT (Sarkar et al., 2021). A series of heuristic constraints, e.g., POS, and these filtering metrics are applied in the rewriting process to maintain the quality and semantic relevance of corresponding images.\n(2) Co-toxic pairs where both the image and text are toxic. We reuse the toxic images and generate captions for them using BLIP (Li et al., 2022) instead of GIT, as it produces much more toxic captions (see Table 3). The same filtering process\n2https://www.kaggle.com/\nis conducted to obtain toxic image-text pairs. (3) Innocuous provocative text prompts. Nontoxic prompts would also lead to toxic generated images, which might be maliciously used to propagate offensive and hate information in real scenarios. To demonstrate this case, we construct such prompts. In detail, we utilize a gradient-guided search method (Wallace et al., 2019) on Stable Diffusion. This approach iteratively replaces a few tokens of prompts to maximize the probability of generating toxic images. The obtained provocative prompts act as a kind of attack and are used to test the vulnerability of various T2I VLGMs.\nTable 1 shows the statistics of ToViLag, and Appendix A gives the detailed construction process."
        },
        {
            "heading": "3.2 Toxicity Classifier",
            "text": "To evaluate the toxicity of generated text/images, we need classifiers to identify the toxicity extent (probability) of given content. For language, we directly utilize the commonly-used PerspectiveAPI following (Gehman et al., 2020a; Liu et al., 2021). For images, we use part of the toxic images collected in Sec. 3.1, combined with non-toxic images from NSFW, to fine-tune three ViT-Huge (Dosovitskiy et al.) models for the three type of toxicity, respectively. Table 2 shows the validation results of the three classifiers, demonstrating acceptable detection performance. More details of the classifiers are provided in Appendix B.2."
        },
        {
            "heading": "3.3 WInToRe Metric for VLG Toxicity",
            "text": "Preliminaries Besides the direct toxicity probability measured by a classifier, we need a metric to assess the overall toxic degree of a given VLG model over a testing set. Expected Maximum Toxicity (EMT) and Toxicity Probability (TP) (Gehman et al., 2020b) are two popular ones used in NLG.\nDefine a given generation model as G and the testing set with N testing input (either text prompt or image input) as {xi}Ni=1. K samples {yi,k}Kk=1\nare generated for each xi. EMT is calculated as:\nEMT(G) = 1 N N\u2211 i=1 max{PT (yi,k)}Kk=1, (1)\nwhere PT (\u00b7) is the toxicity probability of yi,k predicted by classifiers introduced in Sec 3.2. EMT evaluates the worst-case generation, indicating to what extent the model is toxic. TP is calculated as:\nTP(G) = 1 N N\u2211 i=1 I([ K\u2211 k=1 I(PT (yi,k) > \u03c4)] \u0338= 0),\n(2) where I is the indicator function and \u03c4 is the probability threshold (usually 0.5). TP estimates the empirical frequency of generating toxic content.\nDespite their prevalence, these metrics face four defects, hindering the accurate reflection of VLGMs\u2019 internal toxicity. (1) Inconsistent perspectives of toxicity. EMT and TP emphasize two different perspectives of toxicity and ignore the other. One must report both, which confuses readers when they are inconsistent. (2) Neglect of the ratio of toxic samples. They neglect the absolute ratio of toxic outputs among the K ones but only consider the extreme or boundary case. (3) Sensitivity to K and \u03c4 . Different K lead to notably different TP scores (See Fig. 2). The influence of \u03c4 can be observed from Eq.(2), where \u03c4 determines the magnitude of TP. Larger \u03c4 results in smaller TP, which hurts their practicality in broader scenarios. (4) Ignorance of the toxicity of inputs. In the context of VLG, we must also assess the model\u2019s vulnerability to toxic input (e.g., swearwords) by investigating whether it would maintain, amplify or reduce the toxicity to prevent potential malicious attacks. Refer to Appendix C.1 for more analyses of defects.\nWInToRe Score To tackle the aforementioned challenges and consider finer-grained input toxicity in a unified form, we propose a novel metric called Wasserstein-based Hyperparameter Insensitive Toxicity Reflection (WInToRe):\nWInToRe(G) = 1 M M\u2211 m=1 [ 1 N N\u2211 i=1 I(PT (xi)>\u03c4m)\n\u2212 1 NK N\u2211 i=1 K\u2211 k=1 I(PT (yi,k)>\u03c4m)],\n(3)\nwhere {\u03c4m}Mm=1 is a series of toxicity probability thresholds. WInToRe is bounded in [\u22121, 1], and larger WInToRe indicates smaller internal toxicity.\nTo demonstrate the advantages of our new metric, we provide the following conclusion:\nTheorem 1 For any probability measure PT in [0, 1] and probability threshold \u03c4m \u2208 [0, 1] for all m, WInToRe possesses the following properties:\n(a) WInToRe simultaneously reflects different aspects (metrics) of toxicity, like EMT and TP.\n(b) WInToRe is insensitive to K and \u03c4 . lim\nK\u2192+\u221e TP (G)=1 while WInToRe is invariant to"
        },
        {
            "heading": "K. When M is appropriately large enough, the difference brought by different M becomes marginal and converges to 0 with M \u2192 +\u221e.",
            "text": "(c) WInToRe is sensitive to the toxicity of inputs and bounded in [\u22121, 1].\n(d) WInToRe approximately lower bounds the Wasserstein-1 distance W1(PX , PY ) while upper bounds \u03b4\u2217P (X > \u03b4)\u2212E[Y ], \u2200 \u03b4 specified in [0, 1]. X and Y are random variables representing the toxicity of input and output, respectively, and PX and PY are distributions of X and Y , respectively.\nProof. See Appendix C.2. Throughout the rest of this paper, we use WIn-\nToRe as the primary toxicity metric."
        },
        {
            "heading": "3.4 SMIB-Based Detoxification Method",
            "text": "As discussed in Sec. 1 and shown in Fig. 2, current VLGMs are more susceptible to toxicity and might do more evil than anticipated, underscoring the urgency of developing VLG detoxification methods.\nTo take the first step towards this goal, we propose a novel method called Squared-loss Mutual Information based Bottleneck (SMIB). Concretely, define z = f\u03b8(x) as a mapping function parameterized by \u03b8, which transfers the internal representation of the input x to an intermediate z to reduce the toxic information and motivates a non-toxic output y. To optimize \u03b8, we minimize a loss as follows:\nL(\u03b8) = \u2212 1 N N\u2211 i=1 log q\u03c8(yi|f\u03b8(xi))\n+\u03b2 1N \u2211N i=1[ p\u03d5(ai|f\u03b8(xi)) p\u0302(ai) \u2212 \u2211 j\u2208{0,1} p2\u03d5(a=j|f\u03b8(xi)) p\u0302(a=j) ],\n(4)\nwhere q\u03c8(y|f\u03b8(x)) is the VLG model to be detoxified parameterized by \u03c8, p\u03d5(a|f\u03b8(x)) is a classifier parameterized by \u03d5 to predict the toxicity of z=f\u03b8(x), a is the toxicity label with a binary value, (xi, yi, ai) is a labeled (input,output,toxicity label) tuple, N in total, and \u03b2 is a hyper-parameter.\nDuring the training process, the parameters of the VLG model, \u03c8, are fixed while the classifier p\u03d5(a|f\u03b8(x)) and the mapping function f\u03b8(x) are alternately optimized by standard classification loss and Eq.(4), respectively. To demonstrate why this method works well, we prove a conclusion:\nTheorem 2 When the classifier p\u03d5(a|z) is trained and the prior distribution of toxicity p\u0302(a) is estimated well enough, that is, KL[p\u0302(a)||p(a)] \u2192 0 and TV[p\u03d5(a|z)||p(a|z)]<\u03f5, minimizing Eq.(4) is equivalent to maximizing a lower bound of SMI(y,z) and minimizing an upper bound of SMI(z,a). This indicates that, by minimizing Eq.(4), we optimize the Information Bottleneck (IB) (Tishby et al., 2000) by replacing Mutual Information with Squared Loss Mutual Information (SMI) (Niu et al., 2013):\n\u03b8\u2217 = argmax \u03b8 SMI(y, f\u03b8(x)) \u2212 \u03b2SMI(a, f\u03b8(x)).\nProof. See Appendix C.3.\nFrom Theorem 2, by optimizing Eq. (4), we can reduce the correlation between toxicity and VLGMs\u2019 internal representations while improving the probability of producing targets, maintaining generation quality to some extent. Compared to previous IB methods (Alemi et al., 2016; Cheng et al., 2021), this SMI-based IB can be approximated more efficiently and stably from data. Besides, our method is transparent to backbone models. The detoxification layer f\u03b8 could be either a separate component or part of the VLGM. One could apply our method to any part of diverse VLG architectures."
        },
        {
            "heading": "4 Toxicity Analysis of VLG Models",
            "text": "As a preliminary toxicity examination, Fig. 2 illustrates the proportion of input images eliciting toxic outputs under various VLGMs. We find that these popular models yield an unexpectedly high degree of toxicity even trained with carefully-crafted and relatively clean data (see Appendix A). For instance, among 100K generated samples, BLIP produces toxic captions from up to 30% input images. This indicates that VLGMs would do more evil when deployed in diverse real-world application scenarios, emphasizing the importance of comprehensive toxicity analyses.\nTo respond to questions Q1 and Q2 posed in Sec. 1, we perform two kinds of experiments."
        },
        {
            "heading": "4.1 Toxicity Benchmarking",
            "text": "Settings We investigate and benchmark a variety of VLGMs. For image-to-text generation, we evaluate eight models, including VinVL (Zhang et al., 2021), GIT (Wang et al., 2022d), GRIT (Nguyen et al., 2022), OFA (Wang et al., 2022e), CLIPViL (Shen et al., 2022), BLIP (Li et al., 2022), BLIP2 (Li et al., 2023), and LLaVA (Liu et al., 2023). We use toxic images from three categories, 21,559 in total, as inputs for these models and sample 10 generated captions for each input. For models with different sizes, we choose the base version. For text-to-image generation, we consider six popular models, DALLE-Mage3, LAFITE (Zhou et al., 2021), Stable Diffusion (Rombach et al., 2022), OFA (Wang et al., 2022e), CLIP-GEN (Wang et al., 2022f), and CogView2 (Ding et al., 2022). We use 21,805 captions from ToViLaG as inputs, which cover toxic captions from existing datasets and the rewritten ones in Sec 3.1. Ten images are generated for each model and each caption. We report both TP and WInToRe scores. More details of evaluation settings are provided in Appendix B.\nResults Table 3 gives the evaluated toxicity levels of various image-to-text generation models. From the results, we get three interesting findings:\n1) Most I2T generation models exhibit more toxicity than our expectations. More than 10% of the input images can trigger GIT to generate toxic captions, while BLIP2OPT2.7B produces toxicity on a surprising 40% of the images. Such a high toxicity level means that a large portion of users might experience offensive content when\n3https://github.com/borisdayma/dalle-mini\nusing these models through corresponding downstream applications. 2) The toxicity level differs in models, potentially attributed to architectures and training data. Compared to BLIP, three models, OFA, VinVL, and CLIP-Vil, demonstrate quite small toxicity. These three models are trained with small, high-quality, clean datasets like COCO (Lin et al., 2014) and VQA (Antol et al., 2015). In contrast, other models utilize more (0.8 billion pairs in GIT) and noisier web-sourced data like CC12M and LAION400M (Schuhmann et al., 2021). Besides, these toxic models also leverage large-scale pretrained models for initialization, e.g., ViT (Dosovitskiy et al.), CLIP (Radford et al., 2021), OPT (Zhang et al., 2022), and LLaMA (Touvron et al., 2023), suggesting that the toxicity of pretraining should also be considered. 3) Our WInToRe metric reveals more hidden toxicity. Under TP scores, CLIP-ViLRN50 is less toxic than OFA. However, as discussed in Sec 3.3, TP ignores the number of toxic samples nor the toxicity probability, leading to underestimated toxicity, particularly when the overall level is low. Such results support the effectiveness of our new metric.\nTable 4 present the results of text-to-image gen-\neration models. We can also obtain similar conclusions. Generally, T2I models demonstrate a stable and relatively low toxicity level compared to the I2T models. We believe this is because the scales of data and parameters are still limited. Even so, for prevalent models like Stable Diffusion, such a toxicity level (e.g., 23% TP and 80% WInToRe) would cause severe enough consequences, raising the risks of being misused (Bommasani et al., 2021). Besides, we also try the provocative prompts created in Sec. 3.1 and give the results in the right part of Table 4. Taking into account the toxicity of input, some models become highly toxic. For example, CogView2 is the least toxic under toxic prompts, but it amplifies the toxicity using non-toxic (toxic probability < 0.5) inputs to the greatest extent. The most toxic CLIP-GEN instead reduces toxicity to some extent. From these results, we can also conclude: 1) TP score cannot capture the toxicity change between inputs and outputs, failing to reflect the intrinsic toxicity properties of VLGMs. 2) Non-toxic prompts could also elicit toxic generated images, indicating that simple preprocess methods, like filtering, are insufficient."
        },
        {
            "heading": "4.2 Foresight of Toxicity in Future Models",
            "text": "As mentioned in Sec. 1, the development of VLG is still in a very early stage. As we progress along the trajectory of LLMs\u2019 evolution, it\u2019s possible that these models will continue to scale up on model/data size (potentially more toxicity from the web). To foresee how the toxicity level would change then, we conducted further experiments.\nToxicity over model size. Fig. 3 presents the toxicity of different I2T models with varying model sizes. There is a discernible increase in the toxicity levels of models as their parameters increase,\nsimilar to the pattern observed in language models (Gehman et al., 2020a). The underlying rationale lies in the growing capabilities, which allow models to remember more knowledge in the training data, thereby internalizing more harmful information. This suggests that the toxicity of VLGMs could potentially escalate in the foreseeable future without appropriate intervention.\nToxicity over toxic training data. As we discussed in Sec. 4.1, VLGMs trained with larger webcrawled data are obviously more toxic (e.g., BLIP) because such data might contain more toxic information without careful cleaning. Therefore, to simulate a future situation where more unclean web data is involved, we conducted toxicity injection.\nIn detail, we inject toxicity into the training of VLGMs by fine-tuning them on some text-image pairs mixing different ratios of toxic data. We consider two scenarios. 1) Mono-toxicity injection. We gathered 100k pairs as training data with toxic ones from the previously created mono-toxic pairs. Mono-(a) and -(b) pairs in Table 1 are used for training T2I and I2T models, respectively. Non-toxic pairs are sampled from the COCO dataset. 2) Cotoxicity injection. The constructed co-toxic pairs are mixed with the non-toxic ones from COCO.\nFig. 4 depicts the results of the most popular three I2T and three T2I models. From the left part, we can see GIT and Stable Diffusion exhibit the highest level of toxicity but demonstrate some robustness toward increasing toxic data. On the other hand, GRIT, CLIP-ViL and LAFITE are relatively\nmore sensitive. Figure 4 (right part) illustrates the comparison between mono-toxic and co-toxic injections. Clearly, the co-toxic injection causes significantly higher toxicity since the model can build more explicit toxic connections between the two modalities. Only 5% co-toxic pairs lead to a WInToRe drop of Stable Diffusion from 80.1 to 77.9. When increasing the toxicity ratio beyond 10%, a more significant drop will be observed.\nThese analyses manifest that the existing VLGMs are more toxic and less safe than previously assumed. Besides, there is potential for further deterioration with increasingly larger model scales and more unclean web data. This situation strongly underscores the need and urgency for developing preemptive strategies for mitigating such risks.\nWe provide in Appendix D further details and in Appendix E more analyses, including quality evaluation on the injected models and the influence of decoding strategies on I2T generation toxicity."
        },
        {
            "heading": "5 Detoxification Experiments",
            "text": "Settings We perform detoxification experiments on I2T generation and consider three models: BLIP (Li et al., 2022), the most toxic one under our evaluation; GIT (Wang et al., 2022d) with high toxicity and insensitivity to toxicity control;\nGRIT (Nguyen et al., 2022) which is more susceptible to toxicity injection. The mapping function f\u03b8 and classifier p\u03d5 are both implemented as MultiLayer Perceptron (MLP) and appended to the visual encoder of each model. We use 5,000 non-toxic image-text pairs from COCO and 5,000 toxic ones from our co-toxic pairs for training. \u03b2 = 0.01 in Eq.(4). We use AdamW (Loshchilov and Hutter, 2019) (batch size=20) for optimization. For toxicity evaluation, we report TP and WInToRe (WTR). Besides, we also assess the generation quality using BERTScore (BS) (Zhang et al.), ROUGE (R) (Lin, 2004), and CLIPScore (CS) (Hessel et al., 2021). More setting details are listed in Appendix B.\nBaselines We compared our detoxification method SMIB with two baseline methods. The first is a word filtering method, which directly filters out the prohibited candidate tokens4 from the output distribution. The second is an output rectification method called FUDGE (Yang and Klein, 2021), which learns an attribute predictor to adjust the original probabilities of the model.\nResults The efficacy of our detoxification method on I2T models is evident in Table 5. We can see that SMIB demonstrates a more pronounced decline in toxicity compared to the other two baseline methods (-29.4 TP and +17.6 WTR on BLIPL). However, we also notice a notable quality drop across the three models in terms of R and CS. The primary cause of this degradation stems from the detoxification method\u2019s modification or removal of toxic tokens, which subsequently impacts metrics relying on n-gram matching (e.g., -7.0 ROUGE on GIT-L). However, the quality change in BERTScore is far less pronounced (a mere -1.9 on GIT-L), indicating the generation quality is still acceptable. The unusual quality improvement in GRIT mainly arises from its inferior model capacity. GRIT operates on a smaller model scale with less capacity, a 3-layer Transformer without pretraining as its text decoder, in contrast to BLIP\u2019s 12-layer one initialized from BERTbase. Besides, to ensure consistent decoding strategies across all models, we changed its default beam search to topk and top-p sampling, also hurting the performance. Given GRIT\u2019s inherently lower baseline quality, the incremental training during the detoxification optimization, especially with additional parameters\n4The bad word list is in https://github.com/LDNOOBW/List-of-Dirty-NaughtyObscene-and-Otherwise-Bad-Words.\n(mapping layer) and data (N more captions used in Eq.(4)), markedly enhances its text decoder and improves the output.\nHuman Evaluation We also conduct a human evaluation to compare the original GIT-L, GRIT, and BLIP-L with those detoxified by our SMIB in terms of two criteria, namely toxicity and generation quality. Two annotators are invited to evaluate 50 randomly sampled generations and are asked to compare the generation in a pairwise evaluation manner to label the result as win (score=1), lose (score=0), or tie (score = 1). Score=1 indicates lower toxicity / higher quality or comparable. The scores from the two annotators are averaged.\nThe evaluation results are shown in Table 6. The much higher toxicity scores demonstrate a decisive advantage of SMIB over the original generation, while the results on quality are on par with each other. This means that the n-gram matching metrics (e.g., ROUGE) are not reliable. The more flexible BERTScore, human evaluation, and the high pvalues together manifest that the generation quality\nof VLGMs detoxified by SMIB is satisfactory, with a negligible difference from that of original models.\nCase Study Fig. 5 presents generated samples from the original and detoxified GIT for more explicit demonstration. In all cases, our method eliminates the generated offensive words, e.g., \u2018naked\u2019 and \u2018f*\u2019 even though the inputs are highly toxic and preserve most semantics of the original images, like \u2018girl\u2019 and \u2018men\u2019. We provide more generated cases in Appendix F.\nThe considerable heterogeneity and high randomness in T2I model architectures (e.g., GAN, Diffusion, and Transformer) make it challenging to determine efficient mapping layers and optimal intervention strategies, requiring much more effort. Due to these complexities, we didn\u2019t include comprehensive experiments on T2I models. Nonetheless, we made an attempt to apply SMIB to the Stable Diffusion model. The detailed process and some preliminary analysis are described in the Appendix B.1. We highlight this challenge and leave it for future work."
        },
        {
            "heading": "6 Conclusion and Future Work",
            "text": "In this work, we delve into the unexplored toxic degeneration problem of VLGMs. To examine the propensity for susceptibility to toxicity across different VLGMs, we construct ToViLaG, a dataset comprising toxic text-image data, and introduce WInToRe, a novel toxicity metric devised for VLG. We benchmark the toxicity of a broad range of models and reveal that existing models might do more evil than expected. We then propose a novel detoxification method, SMIB, to reduce toxicity without significantly sacrificing generation quality. Our source code, the WInToRe script and other resources are available at https://github.com/ victorup/ToViLaG.\nIn the future, we plan to apply our SMIB method to T2I models and investigate the underlying mechanism of toxicity generation. We will also endeavour to expand our research to wider ethical risks, striving towards an ideal ethical future for VLG.\nLimitations\nThere are still several limitations of our work. We state some of them as follows: (1) Generalizability across tasks and domains. The efficacy of our SMIB methods has not been tested on textto-image generation. Besides, we didn\u2019t consider\nmore diverse VLG tasks, such as VAQ and Visual reasoning. The source (e.g., topic, style and semantics) of our ToViLaG dataset is also limited. We will keep expanding our work to broader domains and tasks. (2) Bias in toxicity detection. Despite high accuracy, our image toxicity classifiers might also express some biases like social bias or label bias since they suffer from imbalanced data. We will keep improving them and conduct debiasing and calibration in the future. (3) Generalizability across VLGMs. We didn\u2019t include all types of VLGMs, especially the extremely Large ones like Flamingo (Alayrac et al., 2022) and PaLME (Driess et al., 2023). Further research is needed to confirm whether our findings apply to these supermodels. (4) Effectiveness of the detoxification method: our detoxification method was shown to reduce toxicity in I2T models with a theoretical guarantee. It\u2019s unclear whether its effectiveness could hold for more tasks and models. (5) Impact on generation quality. Our method still leads to a somewhat reduction in the overall quality of the generated content. Rigorous evaluation and more research are needed to maintain generation quality.\nBroader Impact Statement\nOur work aims to measure and mitigate the toxic contents in VLG. It should be noted that there are still some imperfections in this work, and hence more elaborations should be involved for future work about ethical VLG. Limited coverage of various toxicity types: Our work, constrained by the datasets and resources at hand, makes certain assumptions and simplifications, focusing only on three types of toxic images. Therefore, the VLG models detoxified by our methods still hold the potential to produce toxic content. Similarly, due to the limitation of testing instances and toxicity coverage, VLG models obtained low toxicity under our WInToRe metric might still be toxic. Potential for malicious utilization of our method: Our technique aims to decrease the likelihood of generating toxic content, guided by a jointly trained toxicity classification layer. However, by inversely applying our method, that is, flipping the label of toxicity for training, there\u2019s a risk that it could be used to create more harmful content. Presence of offensive content within our paper: Despite initial warnings, the content of our paper, detailed examples and toxicity of different models, may cause discomfort among readers. To address this, we are\ncommitted to refining our presentation, incorporating clearer warnings, and employing less offensive case studies for better understanding."
        },
        {
            "heading": "Acknowledgements",
            "text": "The work is partially supported by the National Nature Science Foundation of China (No. 61976160, 61906137, 61976158, 62076184, 62076182) and the Shanghai Science and Technology Plan Project (No.21DZ1204800). We would like to express our sincere gratitude to Donghyeon Won for providing the violence datasets constructed in his work (Won et al., 2017) to us. We also thank all anonymous reviewers for their professional reviews and insightful comments to help us further improve our work."
        },
        {
            "heading": "A Details of Dataset Construction",
            "text": "Mono-Toxic Pairs (1) <toxic image, non-toxic text> For toxic images, we collect three types of them from different places. The NSFW dataset gathered from Kaggle5 consists of 33,095 images classified into \"Porn\" and \"Normal\" classes, with 24,998 for training, 4099 for validation, and 3,998\n5https://www.kaggle.com/\nfor testing. The violent dataset is collected from the UCLA Protest Image Dataset (Won et al., 2017), which evaluates the perceived level of violence in protest events. The dataset comprises 40,764 images, including 11,659 protest images identified by annotators, while the remaining images are hardnegative examples (such as crowds in stadiums). For the bloody dataset, we crawl 1,305 images from the Web. For the non-toxic text component, we employ the image captioning model GIT (Wang et al., 2022d) to generate captions for the toxic images. Subsequently, we utilize PerspectiveAPI6 to detect and retain the text with a toxic probability of less than 0.1. Additionally, we employ GPT-XL (Radford et al., 2019) to calculate the Perplexity (PPL) and filter out text with a PPL greater than 200. Furthermore, we filter out text with a CLIPScore (Hessel et al., 2021) exceeding 25.002. For mono-toxic injection of image-to-text generation, we use 10,000 toxic text and 4,349 non-toxic images to create 10k pairs.\n(2) <toxic text, non-toxic image> We begin by detecting and gathering such pairs from already existing VL datasets, including COCO (Lin et al., 2014), Flickr30K (Young et al., 2014), and CC12M (Changpinyo et al., 2021). PerspectiveAPI is utilized to detect toxicity in all the captions within these three datasets. The results of the toxicity detection are presented in Table 7. To address the limited number of pre-existing toxic text, we employ sentence rewriting techniques to generate additional toxic text. fBERT (Sarkar et al., 2021) is trained on SOLID, the largest English offensive language identification corpus, which contains over 1.4 million instances of offensive language. As a Masked Language Modeling (MLM) model, fBERT possesses the ability to predict masked words. Hence, we utilize fBERT as a toxicity generator to generate toxic words. The process involves extracting non-toxic captions from COCO with a toxic probability of less than 0.05, masking two words (a noun and a verb) in each caption, and utilizing fBERT to predict two words for each masked position using top-p sampling. After obtaining the rewritten sentences, we refine them by detecting and preserving sentences with a toxic probability greater than 0.6, a PPL lower than the sum of the mean and standard deviation, and a JACCARD similarity coefficient greater than 0.7. This results\n6https://github.com/conversationai/ perspectiveapi\nin 30k toxic sentences across different ranges of toxicity: 5k in the range of [0.5\u223c0.65), 15k in [0.65\u223c0.8), 10k in [0.8\u223c0.95]. Regarding the corresponding images, we retain the original image associated with the caption before sentence rewriting. To ensure the selection of non-toxic images, we employ three toxic classifiers (mentioned in section 3.2) and utilize CLIPScore to preserve scores greater than the mean minus standard deviation. For mono-toxic injection of text-to-image generation, we use 9,794 non-toxic text and 10,000 toxic images to create 10k pairs.\nPretraining Datasets Number of Toxic text\nCo-Toxic Pairs We also create co-toxic textimage pairs, which consist of both toxic images and toxic text. Similarly, the toxic images are obtained from the three categories mentioned earlier. Regarding the toxic text, we utilize BLIP (Li et al., 2022), which is capable of generating toxic content, to produce toxic captions for the toxic images. To refine the generated toxic captions, we preserve the captions with CLIPScore greater than 27.69, PPL less than 77.03, sentence length longer than 5, and filter out captions with a Jaccard similarity coefficient less than 0.5. For co-toxic injection, we use 9,869 toxic text and 5,142 toxic images to create 10k pairs.\nInnocuous Provocative Text Prompts Additionally, we construct innocuous provocative text prompts to implicitly attack text-to-image generation models. We employ a gradient-guided search method (Wallace et al., 2019) on Stable Diffusion to rewrite some non-toxic text. This iterative approach involves replacing a few tokens of the prompts to maximize the probability of generating toxic images. To begin, we utilize 10k non-toxic generated text from BLIP as the initial triggers. In each iteration, we randomly select three tokens in the triggers to be replaced. Finally, we preserve the best sentence with the smallest generation loss. After obtaining the rewritten triggers, we generate ten images for each trigger and use an image toxicity classifier to detect them. Ultimately, we obtain 902 triggers that can generate toxic images."
        },
        {
            "heading": "B Detailed Setting",
            "text": "B.1 VLGMs Details Image-to-Text Generation Models We evaluate eight models, including VinVL (Zhang et al., 2021), GIT (Wang et al., 2022d), GRIT (Nguyen et al., 2022), OFA (Wang et al., 2022e), CLIPViL (Shen et al., 2022), BLIP (Li et al., 2022), BLIP2 (Li et al., 2023), and LLaVA (Liu et al., 2023). VinVL incorporates the visual features generated by a new object detection model into the VL model Oscar, thereby improving the performance of various VL tasks. GIT treats visual features as tokens and unifies them in a single Transformer decoder by language modeling. GRIT effectively utilizes the grid and region visual features to generate better captions. OFA unifies a diverse set of VL and unimodal tasks by following instructionbased learning in a sequence-to-sequence manner. CLIP-ViL uses CLIP as the visual encoder for diverse downstream VL tasks. BLIP bootstraps the text domain by generating synthetic captions and then conducts joint learning of VL understanding and generation. LLaVa makes a first step towards visual instruction tuning using GPT-4 generated instruction-following samples.\nWe input 21,559 toxic images from three categories into these models, which include 8,595 from the NSFW dataset, 11,659 from the violence dataset, and 1,305 from the bloody dataset. We sample 10 generated captions for each input and use top-k=50 and top-p=0.9 as the decoding method. For mono-toxic injection, we use the set of 10k pairs consisting of 10,000 filtered toxic text and 4,349 non-toxic images to fine-tune the models. For co-toxic injection, we use the set of 10k pairs consisting of 9,869 toxic text and 5,142 toxic images to fine-tune the model. For detoxification, we uniformly employed the mapping layer f\u03b8 on visual features produced by the image encoder, effectively reducing the toxicity of the input image. Additionally, a classification MLP p\u03d5 is added to classify the toxicity of the image representation after the mapping layer f\u03b8. Taking the GIT model (Wang et al., 2022d) as an example, it utilizes the widely-used MLE loss to train the model: CE (yi, p (yi | \u03c4\u03c9(x), y<i\u22121)) , where x is the input image, and \u03c4\u03c9 is the image encoder (ViT in GIT), and y is the text token. In our approach, we applied the mapping layer f\u03b8 after the image encoder to remove the toxicity information in the image representation, leading to\np (yi | f\u03b8(\u03c4\u03c9(x)), y<i\u22121) . This method filters out toxic information from the input image, thereby reducing its toxicity. We use 5,000 non-toxic imagetext pairs from COCO and 5,000 toxic ones from our co-toxic pairs as the training data. We freeze the parameters of the VLG model and solely alternately update \u03b8 and \u03d5. The model first updates the parameters of the detoxification MLP based on the SMIB loss and then updates the parameters of the classification MLP. \u03b2 in Eq.(4) is set to 0.01. We use AdamW (Loshchilov and Hutter, 2019) (with learning rate=1e-6, batch size=20) for optimization.\nText-to-Image Generation Models we consider six popular models, DALLE-Mage7, LAFITE (Zhou et al., 2021), Stable Diffusion (Rombach et al., 2022), OFA (Wang et al., 2022e), CLIP-GEN (Wang et al., 2022f), and CogView2 (Ding et al., 2022). DALLE-Mage is the largest version of DALLE-Mini, which is a simplified version of DALLE. LAFITE utilizes the well-aligned VL semantic space from a powerful pretrained backbone. Stable Diffusion, built on diffusion techniques, can produce indistinguishable high-quality images from arbitrary text prompts. OFA unifies a diverse set of VL and unimodal tasks by following instruction-based learning in a sequence-to-sequence manner. CLIP-GEN requires only unlabeled images, leveraging the language-image priors from CLIP. CogView2 is a pretrained 6B-parameter text-to-image transformer allowing the generation of super-resolution images.\nWe use 21,805 captions from ToViLaG as inputs, which cover toxic captions from various existing datasets, including 570 from COCO, 233 from Flickr30K, 4,286 from CC12M, and the rewritten ones in Sec 3.1. Ten images are generated for each model and each caption. For mono-toxic injection, we use the set of 10k pairs consisting of 9,794 filtered non-toxic text and 10,000 toxic images to finetune the models. We follow the original settings of each model, such as training epochs and learning rates. Similarly, for co-toxic injection, we use the set of 10k pairs consisting of 9,869 toxic text and 5,142 toxic images to fine-tune the model. We use the 902 provocative prompts as input to assess the toxicity of the models and generate 10 images for each prompt. For detoxification, we attempt to apply SMIB to Stable Diffusion for experimentation. Stable Diffusion consists of an image autoencoder,\n7https://github.com/borisdayma/dalle-mini\na U-Net, and a text encoder, with the following training loss (Rombach et al., 2022):\nLLDM :=EE(y),x,\u03f5\u223cN (0,1),t[ ||\u03f5\u2212 \u03f5\u03c9 (zt, t, \u03c4\u03c9(x))| |22 ] , (5)\nwhere \u03f5\u03c9 denotes the U-Net, \u03c4\u03c9 represents the text encoder, zt indicates the denoised latent space at time t, x is the text prompt and y is the image. This loss serves as the first term in Eq.(4) to maximize SMI(y, f\u03b8(x)) and maintain generation quality, which is jointly used in our detoxification training process. We conduct experiments exploring three possible strategies for intervening and placing the mapping layer f\u03b8. (i) On the top of the text encoder (thus affecting the text representation), that is, ||\u03f5\u2212 \u03f5\u03c9 (zt, t, f\u03b8(\u03c4\u03c9(x)))| |22. This strategy resulted in the degeneration of generation quality because of the shift in text representation space by f\u03b8. (ii) On the top of the U-Net, that is, ||\u03f5\u2212 f\u03b8(\u03f5\u03c9 (zt, t, \u03c4\u03c9(x)))| |22. This method was ineffective because zt contains random noise and the limited capacity of f\u03b8 makes it unable to correctly predict the \u03f5 and hinders the convergence of the classification layer p\u03d5(a|f\u03b8(x)). (iii) Considering the entire U-Net as the mapping layer f\u03b8 (thus impacting the noisy prediction process), that is, ||\u03f5\u2212 f\u03b8 (zt, t, \u03c4\u03c9(x))| |22. This method successfully reduces the toxicity to some extent due to the capability of f\u03b8 (U-Net) to continuously learn to predict the desired noise. More concretely, we simultaneously incorporate the last strategy into the LLDM loss, corresponding to the first term in Eq.(4), the learnable f\u03b8 (U-Net) and a classification layers (p\u03d5) after the U-Net to compute the second term in Eq.(4). We experimented on a small set of 1,943 input prompts that can drive the original Stable Diffusion to generate toxic images. After detoxification, prompts capable of generating toxic images were reduced from 1,943 to 1,469. Moreover, there was a notable decrease in the average toxicity score, from 0.912 to 0.749. Such results demonstrate the efficacy of our detoxification method in text-to-image to some extent.\nB.2 Automatic Evaluation Metrics Toxicity Metrics We use TP and WInToRe mentioned in 3 as our toxicity metrics. For language toxicity detection, we utilize PerspectiveAPI. For image toxicity detection, we fine-tune three ViTHuge (Dosovitskiy et al.) models for the three types of toxicity. The statistics of the training data for\neach image toxicity classifier are shown in Table 8. The training data for NSFW and Violence are sourced from the original dataset. For non-bloody data, we reused 2,000 images from the normal image category in NSFW. We train the model with three epochs using an AdamW optimizer with a learning rate of 5e-5. The overall evaluation results of the three classifiers are shown in Table 9.\nQuality Metrics For image-to-text models, we assess the generation quality of the models using BERTScore (Zhang et al.), ROUGE (Lin, 2004), SPICE (Anderson et al., 2016), and CLIPScore (Hessel et al., 2021). BERTScore calculates the similarity between generated captions and references using sentence representation. ROUGE measures the similarity of n-gram occurrences in the generated text with those in the reference text. SPICE evaluates the semantic similarity between the generated and reference captions. CLIPScore calculates the semantic similarity between the representation of images and captions. For text-toimage models, we use the standard metrics, including Inception Score (IS) (Salimans et al., 2016), Frechet Inception Distance (FID) (Heusel et al., 2017) and CLIPScore (Hessel et al., 2021). IS leverages the Inception model to assess both the quality and diversity of generated images. FID measures the similarity between the feature representations of real images and generated images. CLIPScore is the same as mentioned above."
        },
        {
            "heading": "C Details of the WInToRe Metric and Detoxification Method",
            "text": "C.1 Challenges of Existing Metrics\nWe introduce the drawbacks of existing toxicity metrics, detail the design of WInToRe, and demonstrate how our new metric could address these problems, especially in the scenario of VLG.\nBesides the direct toxicity probability measured by a classifier, the most popular two toxicity metrics are Expected Maximum Toxicity and Toxicity Probability (Gehman et al., 2020b) often used in assessing the toxicity of models. Suppose G is a given generation model which is evaluated on N testing input {xi}Ni=1 (either text prompt or image input), and for each input, K samples {yi,k}Kk=1 are generated. Then the two metrics for model G are calculated as the following:\nExpected Maximum Toxicity (EMT):\nEMT(G) = 1 N N\u2211 i=1 max{PT (yi,k)}Kk=1, (6)\nwhere PT (\u00b7) is the toxicity probability of the generated content predicted by a classifier. For imageto-text generation, we use Perspective API8 as the classifier, while for text-to-image generation, we use the classifiers described in Appendix.X. EMT evaluates the worst-case generation, indicating to what extent the model is toxic.\nToxicity Probability (TP):\nTP(G) = 1 N N\u2211 i=1 I( K\u2211 k=1 I(PT (yi,k) > \u03c4) \u0338= 0),\n(7) where I is the indicator function and \u03c4 is the probability threshold that is usually set to 0.5. TP estimates the empirical frequency of generating toxic content, that is, the probability of generating a toxic output (PT (yi,k) > \u03c4 ) at least once over K generations for the given N inputs.\nDespite their prevalence, such two metrics face three challenges, hindering the accurate reflection of LM\u2019s internal toxicity.\n(1) Inconsistent Perspectives of Toxicity. EMT and TP emphasize two different perspectives of toxicity respectively and thus ignore the other. Merely EMT cannot reflect the frequency of toxicity. For example, a few extremely toxic outputs (high variance) may lead to large EMT but small TP. On\n8https://www.perspectiveapi.com/\nthe other side, only TP fails to indicate the degree of toxicity. For example, when PT (yi,k) is slightly higher than 0.5 for most outputs, TP would be large but EMT is around 0.5. Therefore, one must report both, which confuses readers when the two metrics show an inconsistent tendency.\n(2) Neglect of the Ratio of Toxic Samples. Both EMT and TP neglect the ratio of toxic samples among the K output but only consider the extreme or boundary case. Consider model GA that generates K \u2212 1 toxic samples among the K outputs, and model GB that generates only one toxic sample with similar PT (yi,k), then obviously GA is more toxic than GB . Therefore, it\u2019s necessary to take into another criterion, Absolute Toxicity Ratio (ATR), which measures the proportion of toxic samples among all generated outputs, as follows: Absolute Toxicity Ratio (ATR):\nATR(G) = 1 NK N\u2211 i=1 K\u2211 k=1 I(PT (yi,k) > \u03c4). (8)\n(3) Sensitivity to K and \u03c4 . From the above description, we can see TP is sensitive to the specified probability threshold \u03c4 (different \u03c4 leads to varying TP scores). Furthermore, TP is sensitive to the number of generated samples for each input, K (see Fig.2 and Appendix C.2). Such disadvantages require the results to be calculated based on the same \u03c4 and K. It\u2019s impractical in some scenarios, e.g., content moderation (smaller \u03c4 is required) or high-variance cases like unconditional generation (larger K is needed).\n(4) Ignorance of the toxicity of inputs. In the context of multi-modal generation, the toxicity of user-given input must be considered. Since the input (e.g., image for caption generation and textual prompt for image generation) could contain some toxicity (e.g., pornographic images or swearwords), we can evaluate the internal toxicity of models by investigating whether the model G would maintain, amplify, or reduce the toxicity degree of the input. A model that generates toxic output from nontoxic input (amplify) is obviously internally more toxic than the one that generates less toxic output from toxic inputs. Though Gehman et al. (2020b) roughly categorize prompts into Toxic Prompts and Non-toxic Prompts and separately report results on them, we believe that a better metric should consider finer-grained input toxicity in a unified form.\nC.2 The WInToRe Score\nTo tackle the aforementioned challenges, we propose a novel metric to evaluate the toxicity of multi-modal generation models, called Wasserstein-based Hyperparameter Insensitive Toxicity Reflection (WInToRe), as follows:\nWInToRe(G) = 1 M M\u2211 m=1 [ 1 N N\u2211 i=1 I(PT (xi)>\u03c4m)\n\u2212 1 NK N\u2211 i=1 K\u2211 k=1 I(PT (yi,k)>\u03c4m)],\n(9)\nwhere {\u03c4m}Mm=1 is a series of toxicity probability threshold. WInToRe could be either negative or positive, bounded in [\u22121, 1], and larger WInToRe indicates smaller internal toxicity of model G.\nTo demonstrate the advantages of our new metric, we provide the following conclusion:\nTheorem 3 For any probability measure PT in [0, 1] and probability threshold \u03c4m \u2208 [0, 1] for all m, WInToRe possesses the following properties:\n(a) WInToRe simultaneously reflects the three metrics, namely, EMT, TP, and ATR.\n(b) WInToRe is insensitive to K and \u03c4 . lim\nK\u2192+\u221e TP (G)=1 while WInToRe is invariant to"
        },
        {
            "heading": "K. With an appropriately large M , except for",
            "text": "the part reflecting maximum toxicity, WInToRe calculated with different M becomes marginal and converges to 0 with M \u2192 +\u221e.\n(c) WInToRe is sensitive to the toxicity of inputs and bounded in [\u22121, 1].\n(d) WInToRe approximately lower bounds the Wasserstein-1 distance W1(PX , PY ) while upper bounds \u03b4 \u2217 P (X > \u03b4) \u2212 E[Y ], where delta is an arbitrarily specified threshold in [0, 1], X and Y are random variables representing the toxicity of input and output, respectively, and PX and PY are distributions of X and Y , respectively.\nProof We prove each of the above properties in Theorem 1 one by one.\nProperty (a): Given a set of testing inputs {xi}Ni=1, the left part of Eq.(9) is constant, thus we only consider the right part now. We can set one of \u03c4m to 0.5, then we got one term among the M summation terms: 1NK \u2211N i=1 \u2211K k=1 I(PT (yi,k)>0.5), which is exactly ATR in Eq.(8). Since ATR lower bounds TP, WInToRe also reflects TP. Then we\nconsider a specific input xi, and analyze:\nlim M\u2192+\u221e\n1\nM M\u2211 m=1 I(PT (yi,k)>\u03c4m)\n= \u222b 1 0 I(\u03c4,1](PT (yi,k))dPT (yi,k)\n= \u222b 1 0 I[0,PT (yi,k)](\u03c4)d\u03c4 = PT (yi,k). (10)\nTherefore, WInToRe takes into account the actual toxicity probability of each generated sample, which naturally includes max{PT (yi,k)}Kk=1.\nProperty (b): With a given probability threshold \u03c4 (e.g., \u03c4 = 0.5), define event A as that at least one yi, k among the K samples satisfying PT (yi,k) > \u03c4 , and assume the event that PT (yi,k) is larger than \u03c4 as a stochastically independent event with probability pi,\u03c4 , then P (A) =\u2211K\nk=0\n( K\nk\n) pki,\u03c4 (1\u2212 pi,\u03c4 )K\u2212k = 1\u2212 (1\u2212 pi,\u03c4 )K .\nWe get lim K\u2192+\u221e P (A) = 1. On the contrary, for WInToRe, since the event \u2018PT (yi,k) is larger than \u03c4 \u2019 is a stochastically independent event, then\u2211K\nk=1 I(PT (yi,k)>\u03c4m) means the number of samples that satisfy PT (yi,k) > \u03c4m. Therefore, we get 1K \u2211K k=1 I(PT (yi,k) > \u03c4m) = 1 K \u2211K k=0 k \u2217(\nK\nk\n) pki,\u03c4m(1\u2212 pi,\u03c4m) K\u2212k=pi,\u03c4m , invariant to K.\nTo see the difference of WInToRe with different M , typically, we can divide the interval [0,1] into M parts equally. Without loss of generality, we consider WInToRe(G)M and WInToRe(G)M+1 with M and M + 1 equal intervals, respectively, where \u03c4m = m\u22121M for WInToRe(G) M and \u03c4 \u2032 m = m\u22121 M+1 for WInToRe(G)\nM+1. Then we investigate |WInToRe(G)M+1\u2212WInToRe(G)M |. For simplicity, we observe the i\u2212 th input xi, then the difference for a specific m lies in:\n|I(PT (xi)>\u03c4 \u2032 m)\u2212 I(PT (xi)>\u03c4m)+ I(PT (yi,k)>\u03c4m)\u2212 I(PT (yi,k)>\u03c4 \u2032 m)+\nI(PT (xi)>\u03c4 \u2032 M+1)\u2212\n1\nK K\u2211 k=1 I(PT (yi,k)>\u03c4 \u2032 M+1)|.\n(11)\nThe first term I(PT (xi)>\u03c4 \u2032 m)\u2212 I(PT (xi)>\u03c4m) is not equal to 0 only when m\u22121M+1 < PT (xi) < m\u22121 M , that is, the toxicity probability of input xi must fall in the interval [m\u22121M+1 , m\u22121 M ] for each\nm. For a given input set, such a difference can be calculated. For an unknown set, we can assume a prior distribution of PT (xi). For example, when PT (xi) \u223c U(0, 1), the average difference d1(M,M +1) = 1 M \u2211M m=1|I(PT (xi) > \u03c4 \u2032 m) \u2212 I(PT (xi)>\u03c4m)| = M\u221212M(M+1) . If we set M = 50, d1(M,M+1) \u2248 0.00096. Besides, from Eq.(10), we also know that lim\nM\u2192+\u221e 1 M\n\u2211M m=1 I(PT (yi,k)>\n\u03c4 \u2032 m) \u2212 I(PT (yi,k)> \u03c4m) = 0. Similarly, the second term in Eq.(11) could also be marginal. Then the main difference lies in the third term, which reflects the gap between maximum input toxicity and maximum output toxicity.\nProperty (c): From Eq.(9), obviously, our WInToRe score also takes the toxicity of inputs into account and distinguishes the generation model G\u2019s retention, reduction, and amplification effects on inputs toxicity. It\u2019s easy to see that the maximum of WInToRe is 1, obtained when PT (xi) > 1\u2212 1M for all i and PT (yi,k) = 0 for all i, k, indicating that model G reduces the high input toxicity to zero. On the other side, the minimum is -1, obtained when PT (xi) = 0 for all i and PT (yi,k) > 1\u2212 1M for all i, k, implying that model G always generates highly toxic output even with non-toxic inputs.\nProperty (d): The Eq.(9) is derived from the Wasserstein-1 distance. Specifically, the expression in Eq.(9) serves as an approximate lower bound for the Wasserstein-1 distance. Given our context where both input and output toxicity are defined as one-dimensional random variables, the general expression for the Wasserstein distance is given by\nWp(PX , PY ) = (\u222b 1 0 |P \u22121 X (t)\u2212 P \u22121 Y (t)|pdt )1/p . When we set p = 1, the formula becomes W1(PX , PY ) = \u222b 1 0 |P \u22121 X (t) \u2212 P \u22121 Y (t)|dt. We show WInToRe approximately lower bounds of the Wasserstein-1 distance:\nW1(PX , PY )=\u222b 1 0 |P\u22121(X \u2264 \u03c4)\u2212P\u22121(Y \u2264 \u03c4)|d\u03c4\n= E\u03c4\u223cU(0,1)|P (X > \u03c4)\u2212P (Y > \u03c4)| \u2265 E\u03c4\u223cU(0,1) [P (X > \u03c4)\u2212P (Y > \u03c4)] = E\u03c4\u223cU(0,1) {E[I(X > \u03c4)]\u2212 E[I(Y > \u03c4)]}\n\u2248 1 M M\u2211 m=1 [ 1 N N\u2211 i=1 I(PT (xi)>\u03c4m)\n\u2212 1 NK N\u2211 i=1 K\u2211 k=1 I(PT (yi,k)>\u03c4m)]. (12)\nWhen P (X > \u03c4) is always greater than or equal to P (Y > \u03c4), that is, the input is always more toxic than the output (e.g., extremely toxic input), our WInToRe approximates the Wasserstein-1 distance, which naturally reflects the extent that model G would maintain or change the toxicity.\nNow, we prove the lower bound of WInToRe. Since we know above that WInToRe \u2248 E\u03c4\u223cU(0,1) [P (X > \u03c4)\u2212P (Y > \u03c4)], consider E\u03c4\u223cU(0,1) [P (X > \u03c4)]. For a non-negative random variable, we have X = \u222b +\u221e 0 I(X > \u03c4)d\u03c4 . Take expectation\nof both sides, we get E[X] = \u222b +\u221e 0 E[I(X >\n\u03c4)]d\u03c4 = \u222b +\u221e 0 P (X > \u03c4)d\u03c4 . Since X \u2208 [0, 1], we have E\u03c4\u223cU(0,1) [P (X > \u03c4)] = EP [X]. By Markov\u2019s inequality, for any given \u03b4 \u2208 [0, 1], we conclude that WInToRe approximately upper bounds \u03b4 \u2217 P (X > \u03b4) \u2212 EP [Y ]. This bound indicates that WInToRe measures a more accurate difference than the gap between expected output toxicity and a given input toxicity threshold.\nC.3 Detoxification Method To reduce the toxicity of the generated content by VLG models, we propose a novel method called Squared-loss Mutual Information based Bottleneck (SMIB). In detail, define z = f\u03b8(x) as a mapping function parameterized by \u03b8, e.g., MLPs, which transfers the representation of the input, x, to an intermediate one, z, to reduce the toxic information in it and motivate a non-toxic output y. To learn \u03b8, we minimize the following loss:\nL(\u03b8) = \u2212 1 N1 N1\u2211 i=1 log q\u03c8(yi|f\u03b8(xi))\n+\u03b2 1N2 \u2211N2 i=1[ p\u03d5(ai|f\u03b8(xi)) p\u0302(ai) \u2212 \u2211K j=1 p2\u03d5(aj |f\u03b8(xi)) p\u0302(aj) ],\n(13)\nwhere q\u03c8(y|f\u03b8(x)) is the VLG model to be detoxified parameterized by \u03c8, p\u03d5(a|f\u03b8(x)) is a toxicity classifier that predicts the toxicity of z = f\u03b8(x), (xi, yi) is a labeled input-output pair, ai is the toxicity label of yi corresponding to xi, and \u03b2 is a hyper-parameter. During the training process, the parameters of the VLG model, \u03c8, are fixed while the classifier p\u03d5(a|f\u03b8(x)) and the mapping function f\u03b8(x) are iteratively optimized. That is, within one iteration, we first get z = f\u03b8(x) from the toxic and non-toxic pairs (xi, yi, ai), use them to train the classifier p\u03d5 and use the trained p\u03d5 to calculate the loss according to Eq.(13) and then to update \u03b8.\nTo demonstrate why this loss could work well, we provide the following conclusion:\nTheorem 4 When the classifier p\u03d5(a|z) is trained and the prior distribution of toxicity p\u0302(a) is estimated well enough, that is, KL[p\u0302(a)||p(a)] \u2192 0 and TV[p\u03d5(a|z)||p(a|z)]<\u03f5, minimizing Eq.(13) is equivalent to maximizing a lower bound of SMI(y,z) and minimizing an upper bound of SMI(z,a). This indicates that, by minimizing Eq.(13), we are optimizing the information bottleneck by replacing Mutual Inform with Squared Loss Mutual Information, \u03b8\u2217 = argmax\n\u03b8 SMI(y, f\u03b8(x))\u2212 \u03b2SMI(a, f\u03b8(x))\nProof For brevity, we omit the subscript representing parameters. Mutual Information (MI) is the Kullback\u2013Leibler (KL) divergence between the joint distribution and marginal distributions. That is, MI(xy) = KL[p(x, y)||p(x)p(y)]. KL divergence belongs to a more generalized class, fdivergence. In comparison, Squared-loss Mutual Information (SMI) (Suzuki et al., 2009) replace KL divergence with Pearson \u03c72-divergence between p(x, y) and p(x)p(y). Therefore, we have:\nSMI(x, y) = 1\n2\n\u222b\u222b p(x)p(y)( p(x, y)\np(x)p(y) \u22121)2dxdy. (14)\nWe first derive a more simplified form of SMI. Define r(x, y) = p(x,y)p(x)p(y) , then we have:\nSMI(x, y)= 1\n2\n\u222b\u222b p(x)p(y)[r2(x, y)\n+1\u22122r(x, y)]dxdy\n= 1\n2 Ep(x,y)[r(x, y)]\u2212\n1 2 . (15)\nDefine x as model input, y as the target, z as the intermediate representation obtained by z = f\u03b8(x), and a as the toxicity probability of x. According to the Information Bottleneck method (Tishby et al., 2000) with MI replaced by SMI, we learn \u03b8 by:\n\u03b8\u2217 = argmax \u03b8 SMI(y,z) \u2212 \u03b2 \u2217 SMI(z, a), (16)\nwhich maximizes the probability of generating the target y from z while removing toxicity a in z.\nWe now tackle the first term of Eq.(16). Consider Ep(x,y)[r(x, y)], we know logEp(x,y)[r(x, y)] \u2265 Ep(x,y)[log r(x, y)] = MI(x, y). From the BarberAgakov bound (Barber and Agakov, 2003), we have MI(x, y) \u2265 Ep(x,y)[log q(y|x)]+H(y), where\nH(y) is a constant and can be ignored. Thus, maximizing Ep(y,z)[log q(y|z)] is equivalent to maximizing a lower bound of SMI(y, z).\nThen, we handle the second term of Eq.(16). Since the real r(x, y) is actually unknown, the second term is intractable. Thus, we approximate it with r\u0302(x, y) = p\u0302(x,y)p(x)p\u0302(y) . Then we consider A = 2 \u2217 Ep(x,y)[r\u0302(x, y)]\u2212 Ep(x)p(y)[r\u03022(x, y)]\u2212 1. We now prove A is an upper bound of SMI(x, y) under some mild conditions. To prove this, we only need to prove A\u2212 SMI(x, y) \u2265 0, that is:\n4 \u2217 Ep(x,y)[r\u0302(x, y)]\u2212 2Ep(x)p(y)[r\u03022(x, y)] \u2212 Ep(x,y)[r(x, y)] \u2265 1. (17)\nEq.(16) can be further simplified to:\u222b\u222b 4p\u0302(x,y)p(x,y)\u22122ap\u03022(x,y)\u22121 a p2(x,y)\np(x)p\u0302(y) dxdy, (18)\nwhere a = p(y)p\u0302(y) . When we can accurately estimate the prior distribution of y, that is, KL[p(y)||p\u0302(y)] \u2192 0, then a\u2192 1, Eq.(17) becomes:\u222b\u222b\n4p\u0302(x, y)p(x, y)\u22122p\u03022(x, y)\u2212p2(x, y) p(x)p\u0302(y) dxdy\n=\n\u222b\u222b p\u03022 \u2212 [p\u0302\u2212 p]2 \u2212 2p\u0302[p\u0302\u2212 p]\np(x)p\u0302(y) dxdy, (19)\nwhere we omit (x, y) for brevity. When p\u0302(y|x) is trained well enough, that is, TV[p\u0302(x, y), p(x, y)] \u2264 12\u03f5, where TV is Total Variation, then we know |p\u0302(x, y), p(x, y)| \u2264 \u03b4 \u226a \u03f5 for \u2200x, y. Define Eq.(19) as B, then:\nlim \u03b4\u21920 B =\n\u222b\u222b p\u03022\np(x)p\u0302(y) dxdy\n= \u03c72[p\u0302(x, y)||p(x)p\u0302(y)] \u2265 0, (20)\nwhere \u03c72 is the chi-squared divergence (Nishiyama and Sason, 2020).\nTherefore, A approximately upper bounds SMI(x, y). Recall Eq. 16, we have:\n\u03b8\u2217 = argmax \u03b8 SMI(y,z) \u2212 \u03b2 \u2217 SMI(z, a)\n= argmax \u03b8 Ep(y,z)[log q(y|z)] \u2212 \u03b2 \u2217 Ep(z,a)[r\u0302(z, a)] + \u03b2 \u2217 Ep(z)p(a)[r\u03022(z, a)] = argmax\n\u03b8 Ep(y,z)[log q(y|z)]\n\u2212\u03b2 \u2217 Ep(a,z)[log p\u0302(a|z) p\u0302(a) ]\n+\u03b2 \u2217 Ep(z)[ \u222b p\u03022(a|z) p\u0302(a) da]. (21)\n. Therefore, we conclude the proof."
        },
        {
            "heading": "D Additional Experimental Results",
            "text": "The toxicity evaluation results of the pornographic, violent, and bloody for image-to-text models are shown in Tables 13,14,15, and the toxicity evaluation results of the text-to-image models can be found in Tables 10,11,12.\nWe also display the Toxicity Probability scores of toxicity injection, as shown in Figure 6."
        },
        {
            "heading": "E Further Analyses and Discussion",
            "text": "E.1 Further Analyses\nWe conduct quality evaluations on two types of models after injecting mono-toxic and co-toxic data. The quality results are shown in Tables 16, 17, 19 and 20. The quality scores of most models have increased.\nConsidering the impact of model decoding strategies on toxicity, we apply different strategies to GIT, including greedy search, beam search, TopK and Top-P sampling. The results are shown in Figure 7. Among the four methods, Top-P exhib-\nited the highest toxicity. The toxicity of the other methods increased as the hyperparameter values increased.\nWe further conduct detoxification on monoinjected GIT. We selected the highest toxicity of the injected GIT (5%). The comparison of toxicity and evaluation metrics between the original and detoxified GIT is shown in Table 18. The results reflect the positive effect of our detoxification method.\nE.2 Discussion\nThe observed decline in quality metrics in our detoxification performance across most comparison models. We conclude the reason as follows. (1) The quality degradation during detoxification is inevitable. The observed decline in generation quality is a common problem during detoxifica-\ntion. This phenomenon isn\u2019t exclusive to our work. Indeed, most studies in Natural Language Generation (NLG) detoxification have reported similarly degraded performance (Gehman et al., 2020a; Welbl et al., 2021; Wang et al., 2022c; Yang et al., 2023) (2) The degradation can be attributed to altered toxic tokens. The generation quality of our method is still acceptable. The primary cause of this degradation stems from the detoxification method\u2019s modification or removal of toxic tokens, which subsequently impacts metrics relying on ngram matching (e.g., ROUGE). The primary cause of this degradation stems from the detoxification method\u2019s modification or removal of toxic tokens, which subsequently impacts metrics relying on ngram matching (e.g., ROUGE). From Figure 21, it can be observed that some toxic tokens in both original generations and references are removed, leading to a significant drop in ROUGR (-7.0 on GITL). However, the quality change in BERTScore is far less pronounced (a mere -1.9 on GIT-L). Besides, the quality of detoxified outputs by our model is passable. The human evaluation results in Table 6 show that the perceived decline in quality was marginal, as further supported by the sampled\ncases in Figures 5, 8, and 9. In addition, the unusual quality improvement in GRIT can also be explained in Table 21. As mentioned in Sec.5, the incremental training during the detoxification optimization significantly improves the output. Figure 21 demonstrates a noticeable improvement in generation quality on GRIT after detoxification training. Conversely, other models that already produced high-quality outputs reached a saturation point. Consequently, further training combined with the removal of toxic tokens resulted in a deterioration of their generation quality.\nThe transferability of detoxification methods from NLG to VLG. We delve into the challenges of transferability from two perspectives. (1) Suitability for continuous output space. Existing mainstream NLG detoxification methods primarily operate within constrained decoding. These methods either explicitly remove toxic tokens during the decoding process (Gehman et al., 2020a; Sheng et al., 2021) or modify the output distribution over vocabulary to reduce the probabilities of the undesired tokens (Dathathri et al., 2020; Liu et al., 2021; Yang et al., 2023). Such a paradigm strug-\ngles to handle tasks with continuous outputs, like text-to-image generation. In contrast, our proposed approach is inherently compatible with both discrete and continuous output spaces. Empirical testing of our method on Stable Diffusion showed a drop in the average toxicity scores of generated images from 0.912 to 0.749, manifesting the effectiveness of our detoxification method also in text-to-image tasks. (2) Decreased efficacy due to multiple information sources. In NLG, the source information mainly originates from the context or prompt. VLG, however, handles dual information sources: the input image and the context of the output text. Constrained decoding methods lack awareness of the semantics or toxicity level of the input images. To illustrate, word filtering that directly removes toxic candidate tokens is limited by the coverage of the sensitive vocabulary. The output rectification methods (Dathathri et al., 2020; Yang and Klein, 2021) employ the Bayesian formula, p(xi|x1:i\u22121, a) \u221d p(xi|x1:i\u22121)\u2217p(a|x1:i) which reweights token probabilities by p(a|x1:i) and maintains the fluency of generated text by p(xi|x1:i\u22121), where a is the toxicity label and xi defines the i-th token to generate. This paradigm tends to overlook the congruence between the generated text and its corresponding image, leading to a significant degradation in output quality. In contrast, leveraging the information bottleneck, our method considers both source semantics (SMI(y, f\u03b8(x))) and detoxification requirement (SMI(a, f\u03b8(x))). A distinct paradigm worth mentioning is Domain Adaptation Training (Gehman et al., 2020a; Wang et al., 2022c). This approach requires extensive fine-tuning with a large number of carefully curated toxic input and non-toxic output pairs, e.g., 150K documents used in (Gehman et al., 2020a), incurring significant training costs. In contrast, our method introduces a new loss based on Theorem 2 and requires only a moderate amount of monotoxic data (10k in our experiments), offering a more efficient and effective solution for detoxifying VLG\nmodels.\nThe applicability of our proposed metric/method to unimodal generation tasks. Both our new metric and detoxification method are theoretically suitable for unimodal generation tasks. (1) Detoxification Method: the main objective (Eq.(4)) of our detoxification method is to eliminate toxic information from intermediary representations, which isn\u2019t confined to only VLG. The determinant of its application lies in how to tailor the intervention strategies for the mapping layer. To elaborate, when considering NLG tasks, multiple options exist. For example, placing the mapping layer before the output softmax layer or on the top of each self-attention component in Transformer. The challenge, then, is to determine the most appropriate point to incorporate the detoxification mapping layer. This requires further experiments and in-depth analyses. (2) Proposed Metric: The identified shortcomings of existing metrics and the properties presented in Theorem 1, aren\u2019t exclusive to VLG tasks; they are also applicable to NLG. In contrast, TP and EMP metrics fall short in VLG, mainly due to their neglect of input toxicity. In VLG scenarios, the input toxicity profoundly influences the resultant output toxicity. For instance, for Stable Diffusion, there\u2019s a clear correlation between the toxicity of input images (pornographic ones) and the output toxicity. The average output EMT is only 0.88 when the input EMT < 0.7, while 0.92 when input EMT > 0.7, which emphasizes the necessity of our design. While this input-output toxicity correlation is also in NLG (Gehman et al., 2020a), the NLG detoxification literature hasn\u2019t delved into or quantified it."
        },
        {
            "heading": "F More Generated Examples",
            "text": "More generated examples are shown in Figure 8."
        }
    ],
    "title": "ToViLaG: Your Visual-Language Generative Model is Also An Evildoer",
    "year": 2023
}