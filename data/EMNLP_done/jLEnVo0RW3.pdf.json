{
    "abstractText": "As Large Language Models (LLMs) become popular, there emerged an important trend of using multimodality to augment the LLMs\u2019 generation ability, which enables LLMs to better interact with the world. However, there lacks a unified perception of at which stage and how to incorporate different modalities. In this survey, we review methods that assist and augment generative models by retrieving multimodal knowledge, whose formats range from images, codes, tables, graphs, to audio. Such methods offer a promising solution to important concerns such as factuality, reasoning, interpretability, and robustness. By providing an in-depth review, this survey is expected to provide scholars with a deeper understanding of the methods\u2019 applications and encourage them to adapt existing techniques to the fast-growing field of LLMs.",
    "authors": [
        {
            "affiliations": [],
            "name": "Ruochen Zhao"
        },
        {
            "affiliations": [],
            "name": "Hailin Chen"
        },
        {
            "affiliations": [],
            "name": "Weishi Wang"
        },
        {
            "affiliations": [],
            "name": "Fangkai Jiao"
        },
        {
            "affiliations": [],
            "name": "Xuan Long Do"
        },
        {
            "affiliations": [],
            "name": "Chengwei Qin"
        },
        {
            "affiliations": [],
            "name": "Bosheng Ding"
        },
        {
            "affiliations": [],
            "name": "Xiaobao Guo"
        },
        {
            "affiliations": [],
            "name": "Minzhi Li"
        },
        {
            "affiliations": [],
            "name": "Xingxuan Li"
        },
        {
            "affiliations": [],
            "name": "Shafiq Joty"
        }
    ],
    "id": "SP:641e9b30babc547373d9e63f86d1331b1febe3af",
    "references": [
        {
            "authors": [
                "Juli\u00e1n N Acosta",
                "Guido J Falcone",
                "Pranav Rajpurkar",
                "Eric J Topol."
            ],
            "title": "Multimodal biomedical ai",
            "venue": "Nature Medicine, 28(9):1773\u20131784.",
            "year": 2022
        },
        {
            "authors": [
                "Dinesh Garg."
            ],
            "title": "Explanations for CommonsenseQA: New Dataset and Models",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing",
            "year": 2021
        },
        {
            "authors": [
                "Armen Aghajanyan",
                "Bernie Huang",
                "Candace Ross",
                "Vladimir Karpukhin",
                "Hu Xu",
                "Naman Goyal",
                "Dmytro Okhonko",
                "Mandar Joshi",
                "Gargi Ghosh",
                "Mike Lewis",
                "Luke Zettlemoyer."
            ],
            "title": "CM3: A causal masked multimodal model of the internet",
            "venue": "CoRR,",
            "year": 2022
        },
        {
            "authors": [
                "Renat Aksitov",
                "Chung-Ching Chang",
                "David Reitter",
                "Siamak Shakeri",
                "Yunhsuan Sung"
            ],
            "title": "Characterizing attribution and fluency tradeoffs for retrievalaugmented large language models",
            "year": 2023
        },
        {
            "authors": [
                "Jean-Baptiste Alayrac",
                "Jeff Donahue",
                "Pauline Luc",
                "Antoine Miech",
                "Iain Barr",
                "Yana Hasson",
                "Karel Lenc",
                "Arthur Mensch",
                "Katherine Millican",
                "Malcolm Reynolds"
            ],
            "title": "2022a. Flamingo: a visual language model for few-shot learning",
            "year": 2022
        },
        {
            "authors": [
                "Monteiro",
                "Jacob Menick",
                "Sebastian Borgeaud",
                "Andrew Brock",
                "Aida Nematzadeh",
                "Sahand Sharifzadeh",
                "Mikolaj Binkowski",
                "Ricardo Barreira",
                "Oriol Vinyals",
                "Andrew Zisserman",
                "Karen Simonyan"
            ],
            "title": "Flamingo: a visual language model for few-shot",
            "year": 2022
        },
        {
            "authors": [
                "Jinheon Baek",
                "Alham Fikri Aji",
                "Amir Saffari."
            ],
            "title": "Knowledge-augmented language model prompting for zero-shot knowledge graph question answering",
            "venue": "arXiv preprint arXiv:2306.04136.",
            "year": 2023
        },
        {
            "authors": [
                "Tadas Baltru\u0161aitis",
                "Chaitanya Ahuja",
                "Louis-Philippe Morency."
            ],
            "title": "Multimodal machine learning: A survey and taxonomy",
            "venue": "IEEE transactions on pattern analysis and machine intelligence, 41(2):423\u2013443.",
            "year": 2018
        },
        {
            "authors": [
                "Sebastian Borgeaud",
                "Arthur Mensch",
                "Jordan Hoffmann",
                "Trevor Cai",
                "Eliza Rutherford",
                "Katie Millican",
                "George Bm Van Den Driessche",
                "Jean-Baptiste Lespiau",
                "Bogdan Damoc",
                "Aidan Clark"
            ],
            "title": "Improving language models by retrieving from tril",
            "year": 2022
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners. Advances in neural information processing",
            "year": 2020
        },
        {
            "authors": [
                "Deng Cai",
                "Yan Wang",
                "Wei Bi",
                "Zhaopeng Tu",
                "Xiaojiang Liu",
                "Wai Lam",
                "Shuming Shi."
            ],
            "title": "Skeletonto-response: Dialogue generation guided by retrieval memory",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
            "year": 2019
        },
        {
            "authors": [
                "Deng Cai",
                "Yan Wang",
                "Wei Bi",
                "Zhaopeng Tu",
                "Xiaojiang Liu",
                "Shuming Shi."
            ],
            "title": "Retrievalguided dialogue response generation via a matchingto-generation framework",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natu-",
            "year": 2019
        },
        {
            "authors": [
                "David M Chan",
                "Shalini Ghosh",
                "Ariya Rastrow",
                "Bj\u00f6rn Hoffmeister."
            ],
            "title": "Using external off-policy speech-to-text mappings in contextual end-to-end automated speech recognition",
            "venue": "arXiv preprint arXiv:2301.02736.",
            "year": 2023
        },
        {
            "authors": [
                "Chieh-Yang Chen",
                "Pei-Hsin Wang",
                "Shih-Chieh Chang",
                "Da-Cheng Juan",
                "Wei Wei",
                "Jia-Yu Pan."
            ],
            "title": "AirConcierge: Generating task-oriented dialogue via efficient large-scale knowledge retrieval",
            "venue": "Findings of the Association for Computational Linguistics:",
            "year": 2020
        },
        {
            "authors": [
                "Mark Chen",
                "Jerry Tworek",
                "Heewoo Jun",
                "Qiming Yuan",
                "Henrique Ponde de Oliveira Pinto",
                "Jared Kaplan",
                "Harri Edwards",
                "Yuri Burda",
                "Nicholas Joseph",
                "Greg Brockman"
            ],
            "title": "Evaluating large language models trained on code",
            "year": 2021
        },
        {
            "authors": [
                "Wenhu Chen",
                "Hexiang Hu",
                "Xi Chen",
                "Pat Verga",
                "William Cohen."
            ],
            "title": "MuRAG: Multimodal retrieval-augmented generator for open question answering over images and text",
            "venue": "EMNLP, pages 5558\u20135570. ACL.",
            "year": 2022
        },
        {
            "authors": [
                "Wenhu Chen",
                "Hexiang Hu",
                "Xi Chen",
                "Pat Verga",
                "William Cohen."
            ],
            "title": "MuRAG: Multimodal retrieval-augmented generator for open question answering over images and text",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Nat-",
            "year": 2022
        },
        {
            "authors": [
                "Wenhu Chen",
                "Hexiang Hu",
                "Chitwan Saharia",
                "William W Cohen."
            ],
            "title": "Re-imagen: Retrievalaugmented text-to-image generator",
            "venue": "arXiv preprint arXiv:2209.14491.",
            "year": 2022
        },
        {
            "authors": [
                "Wenhu Chen",
                "Xueguang Ma",
                "Xinyi Wang",
                "William W Cohen."
            ],
            "title": "Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks",
            "venue": "arXiv preprint arXiv:2211.12588.",
            "year": 2022
        },
        {
            "authors": [
                "Liying Cheng",
                "Dekun Wu",
                "Lidong Bing",
                "Yan Zhang",
                "Zhanming Jie",
                "Wei Lu",
                "Luo Si."
            ],
            "title": "ENTDESC: Entity description generation by exploring knowledge graph",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language",
            "year": 2020
        },
        {
            "authors": [
                "Zhoujun Cheng",
                "Tianbao Xie",
                "Peng Shi",
                "Chengzu Li",
                "Rahul Nadkarni",
                "Yushi Hu",
                "Caiming Xiong",
                "Dragomir Radev",
                "Mari Ostendorf",
                "Luke Zettlemoyer",
                "Noah A. Smith",
                "Tao Yu."
            ],
            "title": "Binding language models in symbolic languages",
            "venue": "ICLR.",
            "year": 2023
        },
        {
            "authors": [
                "Jaemin Cho",
                "Seunghyun Yoon",
                "Ajinkya Kale",
                "Franck Dernoncourt",
                "Trung Bui",
                "Mohit Bansal."
            ],
            "title": "Fine-grained image captioning with CLIP reward",
            "venue": "Findings of the Association for Computational Linguistics: NAACL 2022, pages 517\u2013527, Seattle,",
            "year": 2022
        },
        {
            "authors": [
                "Aakanksha Chowdhery",
                "Sharan Narang",
                "Jacob Devlin",
                "Maarten Bosma",
                "Gaurav Mishra",
                "Adam Roberts",
                "Paul Barham",
                "Hyung Won Chung",
                "Charles Sutton",
                "Sebastian Gehrmann"
            ],
            "title": "Palm: Scaling language modeling with pathways",
            "year": 2022
        },
        {
            "authors": [
                "Colin B Clement",
                "Shuai Lu",
                "Xiaoyu Liu",
                "Michele Tufano",
                "Dawn Drain",
                "Nan Duan",
                "Neel Sundaresan",
                "Alexey Svyatkovskiy."
            ],
            "title": "Long-range modeling of source code files with ewash: Extended window access by syntax hierarchy",
            "venue": "arXiv preprint",
            "year": 2021
        },
        {
            "authors": [
                "Katherine Crowson",
                "Stella Biderman",
                "Daniel Kornis",
                "Dashiell Stander",
                "Eric Hallahan",
                "Louis Castricato",
                "Edward Raff."
            ],
            "title": "Vqgan-clip: Open domain image generation and editing with natural language guidance",
            "venue": "Computer Vision\u2013ECCV 2022: 17th",
            "year": 2022
        },
        {
            "authors": [
                "Seyed Omid Davoudi",
                "Majid Komeili."
            ],
            "title": "Toward faithful case-based reasoning through learning prototypes in a nearest neighbor-friendly space",
            "venue": "International Conference on Learning Representations.",
            "year": 2021
        },
        {
            "authors": [
                "Yangruibo Ding",
                "Zijian Wang",
                "Wasi Uddin Ahmad",
                "Murali Krishna Ramanathan",
                "Ramesh Nallapati",
                "Parminder Bhatia",
                "Dan Roth",
                "Bing Xiang."
            ],
            "title": "Cocomic: Code completion by jointly modeling in-file and cross-file context",
            "venue": "arXiv preprint",
            "year": 2022
        },
        {
            "authors": [
                "Zi-Yi Dou",
                "Aishwarya Kamath",
                "Zhe Gan",
                "Pengchuan Zhang",
                "Jianfeng Wang",
                "Linjie Li",
                "Zicheng Liu",
                "Ce Liu",
                "Yann LeCun",
                "Nanyun Peng"
            ],
            "title": "Coarse-to-fine vision-language pre-training with fusion in the backbone",
            "year": 2022
        },
        {
            "authors": [
                "Avishek Joey Bose"
            ],
            "title": "Neural path hunter",
            "year": 2021
        },
        {
            "authors": [
                "Lorenzo Valgimigli"
            ],
            "title": "BioReader: a retrieval",
            "year": 2022
        },
        {
            "authors": [
                "Jens Lehmann"
            ],
            "title": "Space efficient context encod",
            "year": 2021
        },
        {
            "authors": [
                "Luyu Gao",
                "Aman Madaan",
                "Shuyan Zhou",
                "Uri Alon",
                "Pengfei Liu",
                "Yiming Yang",
                "Jamie Callan",
                "Graham Neubig."
            ],
            "title": "Pal: Program-aided language models",
            "venue": "arXiv preprint arXiv:2211.10435.",
            "year": 2022
        },
        {
            "authors": [
                "Silin Gao",
                "Jena D. Hwang",
                "Saya Kanno",
                "Hiromi Wakaki",
                "Yuki Mitsufuji",
                "Antoine Bosselut."
            ],
            "title": "ComFact: A benchmark for linking contextual commonsense knowledge",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2022, pages",
            "year": 2022
        },
        {
            "authors": [
                "Tianyu Gao",
                "Xingcheng Yao",
                "Danqi Chen."
            ],
            "title": "SimCSE: Simple contrastive learning of sentence embeddings",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6894\u20136910, Online and Punta Cana, Do-",
            "year": 2021
        },
        {
            "authors": [
                "Jia-Chen Gu",
                "Zhenhua Ling",
                "Quan Liu",
                "Zhigang Chen",
                "Xiaodan Zhu."
            ],
            "title": "Filtering before iteratively referring for knowledge-grounded response selection in retrieval-based chatbots",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020,",
            "year": 2020
        },
        {
            "authors": [
                "Jiatao Gu",
                "Yong Wang",
                "Kyunghyun Cho",
                "Victor OK Li."
            ],
            "title": "Search engine guided neural machine translation",
            "venue": "AAAI, volume 32.",
            "year": 2018
        },
        {
            "authors": [
                "Yunfan Gu",
                "Yang Yuqiao",
                "Zhongyu Wei."
            ],
            "title": "Extract, transform and filling: A pipeline model for question paraphrasing based on template",
            "venue": "Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT 2019), pages 109\u2013114, Hong Kong,",
            "year": 2019
        },
        {
            "authors": [
                "Liangke Gui",
                "Borui Wang",
                "Qiuyuan Huang",
                "Alexander Hauptmann",
                "Yonatan Bisk",
                "Jianfeng Gao."
            ],
            "title": "KAT: A knowledge augmented transformer for vision-and-language",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of",
            "year": 2022
        },
        {
            "authors": [
                "Daya Guo",
                "Shuai Lu",
                "Nan Duan",
                "Yanlin Wang",
                "Ming Zhou",
                "Jian Yin."
            ],
            "title": "Unixcoder: Unified crossmodal pre-training for code representation",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long",
            "year": 2022
        },
        {
            "authors": [
                "Vishal Gupta",
                "Manoj Chinnakotla",
                "Manish Shrivastava."
            ],
            "title": "Retrieve and re-rank: A simple and effective IR approach to simple question answering over knowledge graphs",
            "venue": "Proceedings of the",
            "year": 2018
        },
        {
            "authors": [
                "Kelvin Guu",
                "Kenton Lee",
                "Zora Tung",
                "Panupong Pasupat",
                "Ming-Wei Chang"
            ],
            "title": "2020a. Realm: Retrievalaugmented language model pre-training",
            "year": 2020
        },
        {
            "authors": [
                "Kelvin Guu",
                "Kenton Lee",
                "Zora Tung",
                "Panupong Pasupat",
                "Mingwei Chang."
            ],
            "title": "Retrieval augmented language model pre-training",
            "venue": "International conference on machine learning, pages 3929\u20133938. PMLR.",
            "year": 2020
        },
        {
            "authors": [
                "Tatsunori B Hashimoto",
                "Kelvin Guu",
                "Yonatan Oren",
                "Percy S Liang."
            ],
            "title": "A retrieve-and-edit framework for predicting structured outputs",
            "venue": "NeurIPS, 31.",
            "year": 2018
        },
        {
            "authors": [
                "Shirley Anugrah Hayati",
                "Raphael Olivier",
                "Pravalika Avvaru",
                "Pengcheng Yin",
                "Anthony Tomasic",
                "Graham Neubig."
            ],
            "title": "Retrieval-based neural code generation",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2018
        },
        {
            "authors": [
                "Hangfeng He",
                "Hongming Zhang",
                "Dan Roth."
            ],
            "title": "Rethinking with retrieval: Faithful large language model inference",
            "venue": "arXiv preprint arXiv:2301.00303.",
            "year": 2022
        },
        {
            "authors": [
                "Qiuxiang He",
                "Guoping Huang",
                "Qu Cui",
                "Li Li",
                "Lemao Liu."
            ],
            "title": "Fast and accurate neural machine translation with translation memory",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International",
            "year": 2021
        },
        {
            "authors": [
                "Zihao He",
                "Weituo Hao",
                "Xuchen Song."
            ],
            "title": "Recap: Retrieval augmented music captioner",
            "venue": "arXiv preprint arXiv:2212.10901.",
            "year": 2022
        },
        {
            "authors": [
                "Tao Hu",
                "Xuyu Xiang",
                "Jiaohua Qin",
                "Yun Tan"
            ],
            "title": "2022a. Audio-text retrieval based on contrastive learning and collaborative attention mechanism",
            "year": 2022
        },
        {
            "authors": [
                "Xixin Hu",
                "Xuan Wu",
                "Yiheng Shu",
                "Yuzhong Qu."
            ],
            "title": "Logical form generation via multi-task learning for complex question answering over knowledge bases",
            "venue": "Proceedings of the 29th International Conference on Computational Linguistics, pages 1687\u2013",
            "year": 2022
        },
        {
            "authors": [
                "Ziniu Hu",
                "Ahmet Iscen",
                "Chen Sun",
                "Zirui Wang",
                "KaiWei Chang",
                "Yizhou Sun",
                "Cordelia Schmid",
                "David A Ross",
                "Alireza Fathi."
            ],
            "title": "Reveal: Retrievalaugmented visual-language pre-training with multisource multimodal knowledge memory",
            "venue": "Proceed-",
            "year": 2023
        },
        {
            "authors": [
                "Ziniu Hu",
                "Yichong Xu",
                "Wenhao Yu",
                "Shuohang Wang",
                "Ziyi Yang",
                "Chenguang Zhu",
                "Kai-Wei Chang",
                "Yizhou Sun"
            ],
            "title": "Empowering language models with knowledge graph reasoning for open-domain",
            "year": 2022
        },
        {
            "authors": [
                "Rongjie Huang",
                "Jiawei Huang",
                "Dongchao Yang",
                "Yi Ren",
                "Luping Liu",
                "Mingze Li",
                "Zhenhui Ye",
                "Jinglin Liu",
                "Xiang Yin",
                "Zhou Zhao."
            ],
            "title": "Make-an-audio: Text-to-audio generation with prompt-enhanced diffusion models",
            "venue": "arXiv preprint arXiv:2301.12661.",
            "year": 2023
        },
        {
            "authors": [
                "Shaohan Huang",
                "Li Dong",
                "Wenhui Wang",
                "Yaru Hao",
                "Saksham Singhal",
                "Shuming Ma",
                "Tengchao Lv",
                "Lei Cui",
                "Owais Khan Mohammed",
                "Qiang Liu"
            ],
            "title": "2023b. Language is not all you need: Aligning perception with language models",
            "year": 2023
        },
        {
            "authors": [
                "Gautier Izacard",
                "Edouard Grave."
            ],
            "title": "Leveraging passage retrieval with generative models for open domain question answering",
            "venue": "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume,",
            "year": 2021
        },
        {
            "authors": [
                "Gautier Izacard",
                "Patrick Lewis",
                "Maria Lomeli",
                "Lucas Hosseini",
                "Fabio Petroni",
                "Timo Schick",
                "Jane DwivediYu",
                "Armand Joulin",
                "Sebastian Riedel",
                "Edouard Grave."
            ],
            "title": "Atlas: Few-shot learning with retrieval augmented language models",
            "venue": "arXiv preprint arXiv,",
            "year": 2022
        },
        {
            "authors": [
                "Peter Jansen",
                "Dmitry Ustalov."
            ],
            "title": "TextGraphs 2019 shared task on multi-hop inference for explanation regeneration",
            "venue": "Proceedings of the Thirteenth Workshop on Graph-Based Methods for Natural Language Processing (TextGraphs-13), pages",
            "year": 2019
        },
        {
            "authors": [
                "Chao Jia",
                "Yinfei Yang",
                "Ye Xia",
                "Yi-Ting Chen",
                "Zarana Parekh",
                "Hieu Pham",
                "Quoc Le",
                "Yun-Hsuan Sung",
                "Zhen Li",
                "Tom Duerig."
            ],
            "title": "Scaling up visual and vision-language representation learning with noisy text supervision",
            "venue": "International Conference on",
            "year": 2021
        },
        {
            "authors": [
                "Jiajun Jiang",
                "Yingfei Xiong",
                "Hongyu Zhang",
                "Qing Gao",
                "Xiangqun Chen."
            ],
            "title": "Shaping program repair space with existing patches and similar code",
            "venue": "ISSTA, pages 298\u2013309. ACM.",
            "year": 2018
        },
        {
            "authors": [
                "Bowen Jin",
                "Yu Zhang",
                "Qi Zhu",
                "Jiawei Han."
            ],
            "title": "Heterformer: A transformer architecture for node representation learning on heterogeneous text-rich networks",
            "venue": "arXiv preprint arXiv:2205.10282.",
            "year": 2022
        },
        {
            "authors": [
                "Matthew Jin",
                "Syed Shahriar",
                "Michele Tufano",
                "Xin Shi",
                "Shuai Lu",
                "Neel Sundaresan",
                "Alexey Svyatkovskiy."
            ],
            "title": "Inferfix: End-to-end program repair with llms",
            "venue": "arXiv preprint arXiv:2303.07263.",
            "year": 2023
        },
        {
            "authors": [
                "Yohan Jo",
                "Haneul Yoo",
                "JinYeong Bak",
                "Alice Oh",
                "Chris Reed",
                "Eduard Hovy."
            ],
            "title": "Knowledge-enhanced evidence retrieval for counterargument generation",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2021, pages 3074\u20133094, Punta",
            "year": 2021
        },
        {
            "authors": [
                "Harshit Joshi",
                "Jos\u00e9 Cambronero",
                "Sumit Gulwani",
                "Vu Le",
                "Ivan Radicek",
                "Gust Verbruggen."
            ],
            "title": "Repair is nearly generation: Multilingual program repair with llms",
            "venue": "arXiv preprint arXiv:2208.11640.",
            "year": 2022
        },
        {
            "authors": [
                "Chen Ju",
                "Tengda Han",
                "Kunhao Zheng",
                "Ya Zhang",
                "Weidi Xie."
            ],
            "title": "Prompting visual-language models for efficient video understanding",
            "venue": "Computer Vision\u2013ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23\u201327, 2022, Proceedings, Part",
            "year": 2022
        },
        {
            "authors": [
                "Jaehun Jung",
                "Bokyung Son",
                "Sungwon Lyu."
            ],
            "title": "AttnIO: Knowledge Graph Exploration with In-andOut Attention Flow for Knowledge-Grounded Dialogue",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing",
            "year": 2020
        },
        {
            "authors": [
                "Minki Kang",
                "Jin Myung Kwak",
                "Jinheon Baek",
                "Sung Ju Hwang"
            ],
            "title": "Knowledge graph-augmented language models for knowledge-grounded dialogue generation",
            "year": 2023
        },
        {
            "authors": [
                "Vladimir Karpukhin",
                "Barlas O\u011fuz",
                "Sewon Min",
                "Patrick Lewis",
                "Ledell Wu",
                "Sergey Edunov",
                "Danqi Chen",
                "Wen-tau Yih."
            ],
            "title": "Dense passage retrieval for open-domain question answering",
            "venue": "arXiv preprint arXiv:2004.04906.",
            "year": 2020
        },
        {
            "authors": [
                "Vladimir Karpukhin",
                "Barlas Oguz",
                "Sewon Min",
                "Patrick Lewis",
                "Ledell Wu",
                "Sergey Edunov",
                "Danqi Chen",
                "Wen-tau Yih."
            ],
            "title": "Dense passage retrieval for open-domain question answering",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in",
            "year": 2020
        },
        {
            "authors": [
                "Minkyu Kim",
                "Kim Sung-Bin",
                "Tae-Hyun Oh."
            ],
            "title": "Prefix tuning for automated audio captioning",
            "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 1\u20135. IEEE.",
            "year": 2023
        },
        {
            "authors": [
                "A Sophia Koepke",
                "Andreea-Maria Oncescu",
                "Joao Henriques",
                "Zeynep Akata",
                "Samuel Albanie."
            ],
            "title": "Audio retrieval with natural language queries: A benchmark study",
            "venue": "IEEE Transactions on Multimedia.",
            "year": 2022
        },
        {
            "authors": [
                "Yuma Koizumi",
                "Yasunori Ohishi",
                "Daisuke Niizumi",
                "Daiki Takeuchi",
                "Masahiro Yasuda."
            ],
            "title": "Audio captioning using pre-trained large-scale language model guided by audio-based similar caption retrieval",
            "venue": "arXiv preprint arXiv:2012.07331.",
            "year": 2020
        },
        {
            "authors": [
                "Hung Le",
                "Nancy Chen",
                "Steven Hoi."
            ],
            "title": "Vgnmn: Video-grounded neural module networks for videogrounded dialogue systems",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Hu-",
            "year": 2022
        },
        {
            "authors": [
                "Hung Le",
                "Doyen Sahoo",
                "Nancy Chen",
                "Steven C.H. Hoi."
            ],
            "title": "BiST: Bi-directional spatio-temporal reasoning for video-grounded dialogues",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages",
            "year": 2020
        },
        {
            "authors": [
                "Jinhyuk Lee",
                "Mujeen Sung",
                "Jaewoo Kang",
                "Danqi Chen."
            ],
            "title": "Learning dense representations of phrases at scale",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference",
            "year": 2021
        },
        {
            "authors": [
                "Nyoungwoo Lee",
                "Suwon Shin",
                "Jaegul Choo",
                "Ho-Jin Choi",
                "Sung-Hyon Myaeng."
            ],
            "title": "Constructing multi-modal dialogue dataset by replacing text with semantically relevant images",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for",
            "year": 2021
        },
        {
            "authors": [
                "Jie Lei",
                "Linjie Li",
                "Luowei Zhou",
                "Zhe Gan",
                "Tamara L Berg",
                "Mohit Bansal",
                "Jingjing Liu."
            ],
            "title": "Less is more: Clipbert for video-and-language learning via sparse sampling",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recog-",
            "year": 2021
        },
        {
            "authors": [
                "Jie Lei",
                "Licheng Yu",
                "Tamara Berg",
                "Mohit Bansal."
            ],
            "title": "TVQA+: Spatio-temporal grounding for video question answering",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8211\u20138225, Online. Association",
            "year": 2020
        },
        {
            "authors": [
                "Jure Leskovec",
                "Anand Rajaraman",
                "Jeffrey D. Ullman."
            ],
            "title": "Mining of Massive Datasets, 2nd Ed",
            "venue": "Cambridge University Press.",
            "year": 2014
        },
        {
            "authors": [
                "Patrick Lewis",
                "Ethan Perez",
                "Aleksandra Piktus",
                "Fabio Petroni",
                "Vladimir Karpukhin",
                "Naman Goyal",
                "Heinrich K\u00fcttler",
                "Mike Lewis",
                "Wen-tau Yih",
                "Tim Rockt\u00e4schel"
            ],
            "title": "Retrieval-augmented generation for knowledge-intensive nlp tasks",
            "year": 2020
        },
        {
            "authors": [
                "Christy Y. Li",
                "Xiaodan Liang",
                "Zhiting Hu",
                "Eric P. Xing"
            ],
            "title": "Knowledge-driven encode, retrieve, paraphrase for medical image report",
            "year": 2019
        },
        {
            "authors": [
                "Huayang Li",
                "Yixuan Su",
                "Deng Cai",
                "Yan Wang",
                "Lemao Liu."
            ],
            "title": "A survey on retrieval-augmented text generation",
            "venue": "arXiv preprint arXiv:2202.01110.",
            "year": 2022
        },
        {
            "authors": [
                "Qi Liu",
                "Dani Yogatama",
                "Phil Blunsom."
            ],
            "title": "Relational memory-augmented language models",
            "venue": "Transactions of the Association for Computational Linguistics, 10:555\u2013572.",
            "year": 2022
        },
        {
            "authors": [
                "Shangqing Liu",
                "Yu Chen",
                "Xiaofei Xie",
                "Jing Kai Siow",
                "Yang Liu."
            ],
            "title": "Retrieval-augmented generation for code summarization via hybrid GNN",
            "venue": "ICLR.",
            "year": 2021
        },
        {
            "authors": [
                "Shih-Hung Liu",
                "Kuan-Yu Chen",
                "Berlin Chen",
                "Hsin-Min Wang",
                "Hsu-Chun Yen",
                "Wen-Lian Hsu."
            ],
            "title": "Combining relevance language modeling and clarity measure for extractive speech summarization",
            "venue": "IEEE/ACM Transactions on Audio, Speech, and Lan-",
            "year": 2015
        },
        {
            "authors": [
                "Xuliang Liu",
                "Hao Zhong."
            ],
            "title": "Mining stackoverflow for program repair",
            "venue": "SANER, pages 118\u2013129. IEEE Computer Society.",
            "year": 2018
        },
        {
            "authors": [
                "Ye Liu",
                "Semih Yavuz",
                "Rui Meng",
                "Dragomir Radev",
                "Caiming Xiong",
                "Yingbo Zhou."
            ],
            "title": "Uniparser: Unified semantic parser for question answering on knowledge base and database",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Nat-",
            "year": 2022
        },
        {
            "authors": [
                "Xiang Long",
                "Chuang Gan",
                "Gerard De Melo."
            ],
            "title": "Video captioning with multi-faceted attention",
            "venue": "Transactions of the Association for Computational Linguistics, 6:173\u2013184.",
            "year": 2018
        },
        {
            "authors": [
                "Siyu Lou",
                "Xuenan Xu",
                "Mengyue Wu",
                "Kai Yu."
            ],
            "title": "Audio-text retrieval in context",
            "venue": "ICASSP 20222022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 4793\u2013 4797. IEEE.",
            "year": 2022
        },
        {
            "authors": [
                "Pan Lu",
                "Swaroop Mishra",
                "Tony Xia",
                "Liang Qiu",
                "Kai-Wei Chang",
                "Song-Chun Zhu",
                "Oyvind Tafjord",
                "Peter Clark",
                "Ashwin Kalyan."
            ],
            "title": "Learn to explain: Multimodal reasoning via thought chains for science question answering",
            "venue": "arXiv preprint arXiv:2209.09513.",
            "year": 2022
        },
        {
            "authors": [
                "Shuai Lu",
                "Nan Duan",
                "Hojae Han",
                "Daya Guo",
                "Seung-won Hwang",
                "Alexey Svyatkovskiy."
            ],
            "title": "ReACC: A retrieval-augmented code completion framework",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume",
            "year": 2022
        },
        {
            "authors": [
                "Xing Han Lu",
                "Siva Reddy",
                "Harm de Vries."
            ],
            "title": "The StatCan dialogue dataset: Retrieving data tables through conversations with genuine intents",
            "venue": "Proceedings of the 17th Conference of the European Chapter of the Association for Computational Lin-",
            "year": 2023
        },
        {
            "authors": [
                "Qing Lyu",
                "Shreya Havaldar",
                "Adam Stein",
                "Li Zhang",
                "Delip Rao",
                "Eric Wong",
                "Marianna Apidianaki",
                "Chris Callison-Burch."
            ],
            "title": "Faithful chain-ofthought reasoning",
            "venue": "arXiv preprint arXiv:2301.13379.",
            "year": 2023
        },
        {
            "authors": [
                "Kaixin Ma",
                "Hao Cheng",
                "Xiaodong Liu",
                "Eric Nyberg",
                "Jianfeng Gao."
            ],
            "title": "Open-domain question answering via chain of reasoning over heterogeneous knowledge",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2022, pages 5360\u2013",
            "year": 2022
        },
        {
            "authors": [
                "Aman Madaan",
                "Shuyan Zhou",
                "Uri Alon",
                "Yiming Yang",
                "Graham Neubig."
            ],
            "title": "Language models of code are few-shot commonsense learners",
            "venue": "arXiv preprint arXiv:2210.07128.",
            "year": 2022
        },
        {
            "authors": [
                "Ilaria Manco",
                "Emmanouil Benetos",
                "Elio Quinton",
                "Gy\u00f6rgy Fazekas."
            ],
            "title": "Muscaps: Generating captions for music audio",
            "venue": "2021 International Joint Conference on Neural Networks (IJCNN), pages 1\u20138. IEEE.",
            "year": 2021
        },
        {
            "authors": [
                "Matias Martinez",
                "Westley Weimer",
                "Monperrus Martin."
            ],
            "title": "Do the fix ingredients already exist? an empirical inquiry into the redundancy assumptions of program repair approaches",
            "venue": "Companion Proceedings of the 36th International Conference on Software",
            "year": 2014
        },
        {
            "authors": [
                "Steve McConnell."
            ],
            "title": "Code complete",
            "venue": "Pearson Education.",
            "year": 2004
        },
        {
            "authors": [
                "Rafael Mestre",
                "Stuart Middleton",
                "Matt Ryan",
                "Masood Gheasi",
                "Timothy Norman",
                "Jiatong Zhu."
            ],
            "title": "Augmenting pre-trained language models with audio feature embedding for argumentation mining in political debates",
            "venue": "Findings of the Association for Com-",
            "year": 2023
        },
        {
            "authors": [
                "Gr\u00e9goire Mialon",
                "Roberto Dess\u00ec",
                "Maria Lomeli",
                "Christoforos Nalmpantis",
                "Ram Pasunuru",
                "Roberta Raileanu",
                "Baptiste Rozi\u00e8re",
                "Timo Schick",
                "Jane Dwivedi-Yu",
                "Asli Celikyilmaz"
            ],
            "title": "Augmented language models: a survey",
            "venue": "arXiv preprint arXiv:2302.07842",
            "year": 2023
        },
        {
            "authors": [
                "Benjamin Milde",
                "Jonas Wacker",
                "Stefan Radomski",
                "Max M\u00fchlh\u00e4user",
                "Chris Biemann."
            ],
            "title": "Ambient search: A document retrieval system for speech streams",
            "venue": "Proceedings of COLING 2016, the 26th International Conference on Computational Linguis-",
            "year": 2016
        },
        {
            "authors": [
                "Benjamin Milde",
                "Jonas Wacker",
                "Stefan Radomski",
                "Max M\u00fchlh\u00e4user",
                "Chris Biemann."
            ],
            "title": "Demonstrating ambient search: Implicit document retrieval for speech streams",
            "venue": "Proceedings of COLING 2016, the 26th International Conference on Computational",
            "year": 2016
        },
        {
            "authors": [
                "Stefanos Vrochidis",
                "Leo Wanner."
            ],
            "title": "A case study of NLG from multimedia data sources: Generating architectural landmark descriptions",
            "venue": "Proceedings of the 3rd International Workshop on Natural Language Generation from the Semantic Web",
            "year": 2020
        },
        {
            "authors": [
                "Niluthpol Chowdhury Mithun",
                "Juncheng Li",
                "Florian Metze",
                "Amit K Roy-Chowdhury."
            ],
            "title": "Learning joint embedding with multimodal cues for crossmodal video-text retrieval",
            "venue": "Proceedings of the 2018 ACM on International Conference on Multime-",
            "year": 2018
        },
        {
            "authors": [
                "Keerthiram Murugesan",
                "Mattia Atzeni",
                "Pavan Kapanipathi",
                "Kartik Talamadupula",
                "Mrinmaya Sachan",
                "Murray Campbell."
            ],
            "title": "Efficient text-based reinforcement learning by jointly leveraging state and commonsense graph representations",
            "venue": "Proceedings",
            "year": 2021
        },
        {
            "authors": [
                "Arsha Nagrani",
                "Shan Yang",
                "Anurag Arnab",
                "Aren Jansen",
                "Cordelia Schmid",
                "Chen Sun."
            ],
            "title": "Attention bottlenecks for multimodal fusion",
            "venue": "Advances in Neural Information Processing Systems, 34:14200\u201314213.",
            "year": 2021
        },
        {
            "authors": [
                "Kai Nakamura",
                "Sharon Levy",
                "Yi-Lin Tuan",
                "Wenhu Chen",
                "William Yang Wang."
            ],
            "title": "HybriDialogue: An information-seeking dialogue dataset grounded on tabular and textual data",
            "venue": "Findings of the Association for Computational Linguistics: ACL 2022, pages",
            "year": 2022
        },
        {
            "authors": [
                "Reiichiro Nakano",
                "Jacob Hilton",
                "Suchir Balaji",
                "Jeff Wu",
                "Long Ouyang",
                "Christina Kim",
                "Christopher Hesse",
                "Shantanu Jain",
                "Vineet Kosaraju",
                "William Saunders"
            ],
            "title": "Webgpt: Browser-assisted questionanswering with human feedback",
            "year": 2021
        },
        {
            "authors": [
                "Xiong",
                "Dragomir Radev",
                "Dragomir Radev."
            ],
            "title": "FeTaQA: Free-form table question answering",
            "venue": "Transactions of the Association for Computational Linguistics, 10:35\u201349.",
            "year": 2022
        },
        {
            "authors": [
                "Noor Nashid",
                "Mifta Sintaha",
                "Ali Mesbah."
            ],
            "title": "Retrieval-based prompt selection for code-related few-shot learning",
            "venue": "Proceedings of the 45th International Conference on Software Engineering (ICSE\u201923).",
            "year": 2023
        },
        {
            "authors": [
                "Roth."
            ],
            "title": "Entailment tree explanations via iterative retrieval-generation reasoner",
            "venue": "Findings of the Association for Computational Linguistics: NAACL 2022, pages 465\u2013475, Seattle, United States. Association for Computational Linguistics.",
            "year": 2022
        },
        {
            "authors": [
                "Long Ouyang",
                "Jeff Wu",
                "Xu Jiang",
                "Diogo Almeida",
                "Carroll L Wainwright",
                "Pamela Mishkin",
                "Chong Zhang",
                "Sandhini Agarwal",
                "Katarina Slama",
                "Alex Ray"
            ],
            "title": "Training language models to follow instructions with human feedback",
            "year": 2022
        },
        {
            "authors": [
                "Feifei Pan",
                "Mustafa Canim",
                "Michael Glass",
                "Alfio Gliozzo",
                "Peter Fox."
            ],
            "title": "CLTR: An end-to-end, transformer-based system for cell-level table retrieval and table question answering",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Compu-",
            "year": 2021
        },
        {
            "authors": [
                "Md Rizwan Parvez",
                "Wasi Ahmad",
                "Saikat Chakraborty",
                "Baishakhi Ray",
                "Kai-Wei Chang."
            ],
            "title": "Retrieval augmented code generation and summarization",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2021, pages 2719\u20132734, Punta",
            "year": 2021
        },
        {
            "authors": [
                "Ramakanth Pasunuru",
                "Mohit Bansal."
            ],
            "title": "Gamebased video-context dialogue",
            "venue": "arXiv preprint arXiv:1809.04560.",
            "year": 2018
        },
        {
            "authors": [
                "Arkil Patel",
                "Satwik Bhattamishra",
                "Navin Goyal"
            ],
            "title": "Are nlp models really able to solve simple math word problems? arXiv preprint arXiv:2103.07191",
            "year": 2021
        },
        {
            "authors": [
                "Hao Peng",
                "Ankur Parikh",
                "Manaal Faruqui",
                "Bhuwan Dhingra",
                "Dipanjan Das."
            ],
            "title": "Text generation with exemplar-based adaptive decoding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Lin-",
            "year": 2019
        },
        {
            "authors": [
                "Ben Poole",
                "Ajay Jain",
                "Jonathan T Barron",
                "Ben Mildenhall."
            ],
            "title": "Dreamfusion: Text-to-3d using 2d diffusion",
            "venue": "arXiv preprint arXiv:2209.14988.",
            "year": 2022
        },
        {
            "authors": [
                "Soumajit Pramanik",
                "Jesujoba Alabi",
                "Rishiraj Saha Roy",
                "Gerhard Weikum."
            ],
            "title": "Uniqorn: unified question answering over rdf knowledge graphs and natural language text",
            "venue": "arXiv preprint arXiv:2108.08614.",
            "year": 2021
        },
        {
            "authors": [
                "Yuhua Qi",
                "Xiaoguang Mao",
                "Yan Lei",
                "Ziying Dai",
                "Chengsong Wang."
            ],
            "title": "The strength of random search on automated program repair",
            "venue": "ICSE, pages 254\u2013265. ACM.",
            "year": 2014
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark",
                "Gretchen Krueger",
                "Ilya Sutskever"
            ],
            "title": "Learning transferable visual models from natural language",
            "year": 2021
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark"
            ],
            "title": "Learning transferable visual models from natural language supervision",
            "year": 2021
        },
        {
            "authors": [
                "Aditya Ramesh",
                "Mikhail Pavlov",
                "Gabriel Goh",
                "Scott Gray",
                "Chelsea Voss",
                "Alec Radford",
                "Mark Chen",
                "Ilya Sutskever."
            ],
            "title": "Zero-shot text-to-image generation",
            "venue": "ICML, volume 139 of Proceedings of Machine Learning Research, pages 8821\u20138831. PMLR.",
            "year": 2021
        },
        {
            "authors": [
                "Aditya Ramesh",
                "Mikhail Pavlov",
                "Gabriel Goh",
                "Scott Gray",
                "Chelsea Voss",
                "Alec Radford",
                "Mark Chen",
                "Ilya Sutskever."
            ],
            "title": "Zero-shot text-to-image generation",
            "venue": "International Conference on Machine Learning, pages 8821\u20138831. PMLR.",
            "year": 2021
        },
        {
            "authors": [
                "Kiran Ramnath",
                "Leda Sari",
                "Mark Hasegawa-Johnson",
                "Chang Yoo."
            ],
            "title": "Worldly wise (WoW) cross-lingual knowledge fusion for fact-based visual spoken-question answering",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of",
            "year": 2021
        },
        {
            "authors": [
                "Rita Ramos",
                "Desmond Elliott",
                "Bruno Martins."
            ],
            "title": "Retrieval-augmented image captioning",
            "venue": "Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pages 3666\u20133681, Dubrovnik, Croatia. Association",
            "year": 2023
        },
        {
            "authors": [
                "Brandon Royal",
                "Kien Hua",
                "Brenton Zhang."
            ],
            "title": "Deep composer: Deep neural hashing and retrieval approach to automatic music generation",
            "venue": "2020 IEEE International Conference on Multimedia and Expo (ICME), pages 1\u20136. IEEE.",
            "year": 2020
        },
        {
            "authors": [
                "Sara Sarto",
                "Marcella Cornia",
                "Lorenzo Baraldi",
                "Rita Cucchiara."
            ],
            "title": "Retrieval-augmented transformer for image captioning",
            "venue": "CBMI, pages 1\u20137. ACM.",
            "year": 2022
        },
        {
            "authors": [
                "Timo Schick",
                "Jane Dwivedi-Yu",
                "Roberto Dess\u00ec",
                "Roberta Raileanu",
                "Maria Lomeli",
                "Luke Zettlemoyer",
                "Nicola Cancedda",
                "Thomas Scialom."
            ],
            "title": "Toolformer: Language models can teach themselves to use tools",
            "venue": "arXiv preprint arXiv:2302.04761.",
            "year": 2023
        },
        {
            "authors": [
                "Christopher Sciavolino",
                "Zexuan Zhong",
                "Jinhyuk Lee",
                "Danqi Chen."
            ],
            "title": "Simple entity-centric questions challenge dense retrievers",
            "venue": "Proceedings of",
            "year": 2021
        },
        {
            "authors": [
                "Ramprasaath R. Selvaraju",
                "Michael Cogswell",
                "Abhishek Das",
                "Ramakrishna Vedantam",
                "Devi Parikh",
                "Dhruv Batra."
            ],
            "title": "Grad-cam: Visual explanations from deep networks via gradient-based localization",
            "venue": "ICCV, pages 618\u2013626. IEEE Computer",
            "year": 2017
        },
        {
            "authors": [
                "Lei Shen",
                "Haolan Zhan",
                "Xin Shen",
                "Yonghao Song",
                "Xiaofang Zhao."
            ],
            "title": "Text is not enough: Integrating visual impressions into open-domain dialogue generation",
            "venue": "Proceedings of the 29th ACM International Conference on Multimedia, MM \u201921, page",
            "year": 2021
        },
        {
            "authors": [
                "Ensheng Shi",
                "Yanlin Wang",
                "Wei Tao",
                "Lun Du",
                "Hongyu Zhang",
                "Shi Han",
                "Dongmei Zhang",
                "Hongbin Sun."
            ],
            "title": "RACE: Retrieval-augmented commit message generation",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Process-",
            "year": 2022
        },
        {
            "authors": [
                "Zhan Shi",
                "Hui Liu",
                "Martin Renqiang Min",
                "Christopher Malon",
                "Li Erran Li",
                "Xiaodan Zhu."
            ],
            "title": "Retrieval, analogy, and composition: A framework for compositional generalization in image captioning",
            "venue": "Findings of the Association for Computational Lin-",
            "year": 2021
        },
        {
            "authors": [
                "Yiheng Shu",
                "Zhiwei Yu",
                "Yuhan Li",
                "B\u00f6rje Karlsson",
                "Tingting Ma",
                "Yuzhong Qu",
                "Chin-Yew Lin."
            ],
            "title": "TIARA: Multi-grained retrieval for robust question answering over large knowledge base",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods",
            "year": 2022
        },
        {
            "authors": [
                "Yixuan Su",
                "Zaiqiao Meng",
                "Simon Baker",
                "Nigel Collier."
            ],
            "title": "Few-shot table-to-text generation with prototype memory",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2021, pages 910\u2013917, Punta Cana, Dominican Republic. Associa-",
            "year": 2021
        },
        {
            "authors": [
                "Chen Sun",
                "Austin Myers",
                "Carl Vondrick",
                "Kevin Murphy",
                "Cordelia Schmid."
            ],
            "title": "Videobert: A joint model for video and language representation learning",
            "venue": "Proceedings of the IEEE/CVF international conference on computer vision, pages 7464\u20137473.",
            "year": 2019
        },
        {
            "authors": [
                "Edward Sun",
                "Yufang Hou",
                "Dakuo Wang",
                "Yunfeng Zhang",
                "Nancy X.R. Wang."
            ],
            "title": "D2S: Document-to-slide generation via query-based text summarization",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Associ-",
            "year": 2021
        },
        {
            "authors": [
                "Chao-Hong Tan",
                "Jia-Chen Gu",
                "Chongyang Tao",
                "ZhenHua Ling",
                "Can Xu",
                "Huang Hu",
                "Xiubo Geng",
                "Daxin Jiang."
            ],
            "title": "TegTok: Augmenting text generation via task-specific and open-world knowledge",
            "venue": "Findings of the Association for Computational",
            "year": 2022
        },
        {
            "authors": [
                "Romal Thoppilan",
                "Daniel De Freitas",
                "Jamie Hall",
                "Noam Shazeer",
                "Apoorv Kulshreshtha",
                "Heng-Tze Cheng",
                "Alicia Jin",
                "Taylor Bos",
                "Leslie Baker",
                "Yu Du"
            ],
            "title": "Lamda: Language models for dialog applications",
            "venue": "arXiv preprint arXiv:2201.08239",
            "year": 2022
        },
        {
            "authors": [
                "Anthony Meng Huat Tiong",
                "Junnan Li",
                "Boyang Li",
                "Silvio Savarese",
                "Steven C.H. Hoi."
            ],
            "title": "Plug-andplay VQA: Zero-shot VQA by conjoining large pretrained models with zero training",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP",
            "year": 2022
        },
        {
            "authors": [
                "Harsh Trivedi",
                "Niranjan Balasubramanian",
                "Tushar Khot",
                "Ashish Sabharwal."
            ],
            "title": "Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions",
            "venue": "arXiv preprint arXiv:2212.10509.",
            "year": 2022
        },
        {
            "authors": [
                "Yao-Hung Hubert Tsai",
                "Shaojie Bai",
                "Paul Pu Liang",
                "J Zico Kolter",
                "Louis-Philippe Morency",
                "Ruslan Salakhutdinov."
            ],
            "title": "Multimodal transformer for unaligned multimodal language sequences",
            "venue": "Proceedings of the conference. Association for Computa-",
            "year": 2019
        },
        {
            "authors": [
                "Xiaohan Wang",
                "Linchao Zhu",
                "Yi Yang."
            ],
            "title": "T2VLAD: global-local sequence alignment for textvideo retrieval",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual, June 19-25, 2021, pages 5079\u20135088. Computer Vi-",
            "year": 2021
        },
        {
            "authors": [
                "Yue Wang",
                "Weishi Wang",
                "Shafiq Joty",
                "Steven C.H. Hoi."
            ],
            "title": "CodeT5: Identifier-aware unified pretrained encoder-decoder models for code understanding and generation",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Lan-",
            "year": 2021
        },
        {
            "authors": [
                "Zhenhailong Wang",
                "Manling Li",
                "Ruochen Xu",
                "Luowei Zhou",
                "Jie Lei",
                "Xudong Lin",
                "Shuohang Wang",
                "Ziyi Yang",
                "Chenguang Zhu",
                "Derek Hoiem"
            ],
            "title": "Language models with image descriptors are strong few-shot video-language learners",
            "year": 2022
        },
        {
            "authors": [
                "Jason Wei",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Maarten Bosma",
                "Ed Chi",
                "Quoc Le",
                "Denny Zhou."
            ],
            "title": "Chain of thought prompting elicits reasoning in large language models",
            "venue": "arXiv preprint arXiv:2201.11903.",
            "year": 2022
        },
        {
            "authors": [
                "Makarius Wenzel",
                "Lawrence C. Paulson",
                "Tobias Nipkow."
            ],
            "title": "The isabelle framework",
            "venue": "TPHOLs, volume 5170 of LNCS, pages 33\u201338. Springer.",
            "year": 2008
        },
        {
            "authors": [
                "Jason Weston",
                "Emily Dinan",
                "Alexander Miller."
            ],
            "title": "Retrieve and refine: Improved sequence generation models for dialogue",
            "venue": "Proceedings of the 2018 EMNLP Workshop SCAI: The 2nd International Workshop on Search-Oriented Conversational",
            "year": 2018
        },
        {
            "authors": [
                "Martin White",
                "Michele Tufano",
                "Matias Martinez",
                "Monperrus Martin",
                "Denys Poshyvanyk."
            ],
            "title": "Sorting and transforming program repair ingredients via deep learning code similarities",
            "venue": "2019 IEEE 26th International Conference on Software Analysis, Evolution",
            "year": 2019
        },
        {
            "authors": [
                "Spencer Whitehead",
                "Heng Ji",
                "Mohit Bansal",
                "Shih-Fu Chang",
                "Clare Voss."
            ],
            "title": "Incorporating background knowledge into video description generation",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages",
            "year": 2018
        },
        {
            "authors": [
                "Sixing Wu",
                "Ying Li",
                "Dawei Zhang",
                "Zhonghai Wu."
            ],
            "title": "Improving knowledge-aware dialogue response generation by using human-written prototype dialogues",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1402\u2013",
            "year": 2020
        },
        {
            "authors": [
                "Sixing Wu",
                "Ying Li",
                "Dawei Zhang",
                "Yang Zhou",
                "Zhonghai Wu."
            ],
            "title": "Diverse and informative dialogue generation with context-specific commonsense knowledge awareness",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational",
            "year": 2020
        },
        {
            "authors": [
                "Xian Wu",
                "Shuxin Yang",
                "Zhaopeng Qiu",
                "Shen Ge",
                "Yangtian Yan",
                "Xingwang Wu",
                "Yefeng Zheng",
                "S. Kevin Zhou",
                "Li Xiao."
            ],
            "title": "DeltaNet: Conditional medical report generation for COVID-19 diagnosis",
            "venue": "Proceedings of the 29th International Con-",
            "year": 2022
        },
        {
            "authors": [
                "Yu Wu",
                "Furu Wei",
                "Shaohan Huang",
                "Yunli Wang",
                "Zhoujun Li",
                "Ming Zhou."
            ],
            "title": "Response generation by context-aware prototype editing",
            "venue": "AAAI, volume 33, pages 7281\u20137288.",
            "year": 2019
        },
        {
            "authors": [
                "Yuhuai Wu",
                "Albert Qiaochu Jiang",
                "Wenda Li",
                "Markus Norman Rabe",
                "Charles E Staats",
                "Mateja Jamnik",
                "Christian Szegedy."
            ],
            "title": "Autoformalization with large language models",
            "venue": "NeurIPS.",
            "year": 2022
        },
        {
            "authors": [
                "Xin Xia",
                "Lingfeng Bao",
                "David Lo",
                "Pavneet Singh Kochhar",
                "Ahmed E. Hassan",
                "Zhenchang Xing."
            ],
            "title": "What do developers search for on the web? Empir",
            "venue": "Softw. Eng., 22(6):3149\u20133185.",
            "year": 2017
        },
        {
            "authors": [
                "Jia Xin",
                "Wang Hao",
                "Yin Dawei",
                "Wu Yunfang."
            ],
            "title": "Enhancing question generation with commonsense knowledge",
            "venue": "Proceedings of the 20th Chinese National Conference on Computational Linguistics, pages 976\u2013987, Huhhot, China. Chinese Information",
            "year": 2021
        },
        {
            "authors": [
                "Jitao Xu",
                "Josep-Maria Crego",
                "Jean Senellart."
            ],
            "title": "Boosting neural machine translation with similar translations",
            "venue": "Annual Meeting of the Association for Computational Linguistics, pages 1570\u20131579. Association for Computational Linguistics.",
            "year": 2020
        },
        {
            "authors": [
                "Ran Xu",
                "Caiming Xiong",
                "Wei Chen",
                "Jason Corso."
            ],
            "title": "Jointly modeling deep video and compositional text to bridge vision and language in a unified framework",
            "venue": "AAAI, volume 29.",
            "year": 2015
        },
        {
            "authors": [
                "Yichong Xu",
                "Chenguang Zhu",
                "Ruochen Xu",
                "Yang Liu",
                "Michael Zeng",
                "Xuedong Huang."
            ],
            "title": "Fusing context into knowledge graph for commonsense question answering",
            "venue": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021,",
            "year": 2021
        },
        {
            "authors": [
                "Fabian Yamaguchi",
                "Nico Golde",
                "Daniel Arp",
                "Konrad Rieck."
            ],
            "title": "Modeling and discovering vulnerabilities with code property graphs",
            "venue": "2014 IEEE Symposium on Security and Privacy, SP 2014, Berkeley, CA, USA, May 18-21, 2014, pages 590\u2013604.",
            "year": 2014
        },
        {
            "authors": [
                "Antoine Yang",
                "Arsha Nagrani",
                "Paul Hongsuck Seo",
                "Antoine Miech",
                "Jordi Pont-Tuset",
                "Ivan Laptev",
                "Josef Sivic",
                "Cordelia Schmid."
            ],
            "title": "Vid2seq: Largescale pretraining of a visual language model for dense video captioning",
            "venue": "CoRR, abs/2302.14115.",
            "year": 2023
        },
        {
            "authors": [
                "Xingyi Yang",
                "Muchao Ye",
                "Quanzeng You",
                "Fenglong Ma."
            ],
            "title": "Writing by memorizing: Hierarchical retrieval-based medical report generation",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th",
            "year": 2021
        },
        {
            "authors": [
                "Yue Yang",
                "Wenlin Yao",
                "Hongming Zhang",
                "Xiaoyang Wang",
                "Dong Yu",
                "Jianshu Chen."
            ],
            "title": "Z-LaVI: Zero-shot language solver fueled by visual imagination",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2022
        },
        {
            "authors": [
                "Zhengyuan Yang",
                "Zhe Gan",
                "Jianfeng Wang",
                "Xiaowei Hu",
                "Yumao Lu",
                "Zicheng Liu",
                "Lijuan Wang."
            ],
            "title": "An empirical study of gpt-3 for few-shot knowledgebased vqa",
            "venue": "AAAI, volume 36, pages 3081\u20133089.",
            "year": 2022
        },
        {
            "authors": [
                "Zhicheng Yang",
                "Jinghui Qin",
                "Jiaqi Chen",
                "Liang Lin",
                "Xiaodan Liang."
            ],
            "title": "LogicSolver: Towards interpretable math word problem solving with logical prompt-enhanced learning",
            "venue": "Findings of the",
            "year": 2022
        },
        {
            "authors": [
                "Zhuolin Yang",
                "Wei Ping",
                "Zihan Liu",
                "Vijay Korthikanti",
                "Weili Nie",
                "De-An Huang",
                "Linxi Fan",
                "Zhiding Yu",
                "Shiyi Lan",
                "Bo Li",
                "Ming-Yu Liu",
                "Yuke Zhu",
                "Mohammad Shoeybi",
                "Bryan Catanzaro",
                "Chaowei Xiao",
                "Anima Anandkumar"
            ],
            "title": "2023b. Re-vilm: Retrieval",
            "year": 2023
        },
        {
            "authors": [
                "Michihiro Yasunaga",
                "Armen Aghajanyan",
                "Weijia Shi",
                "Rich James",
                "Jure Leskovec",
                "Percy Liang",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Wen-tau Yih."
            ],
            "title": "Retrievalaugmented multimodal language modeling",
            "venue": "CoRR, abs/2211.12561.",
            "year": 2022
        },
        {
            "authors": [
                "Xi Ye",
                "Greg Durrett."
            ],
            "title": "The unreliability of explanations in few-shot prompting for textual reasoning",
            "venue": "Advances in Neural Information Processing Systems.",
            "year": 2022
        },
        {
            "authors": [
                "Yunhu Ye",
                "Binyuan Hui",
                "Min Yang",
                "Binhua Li",
                "Fei Huang",
                "Yongbin Li."
            ],
            "title": "Large language models are versatile decomposers: Decompose evidence and questions for table-based reasoning",
            "venue": "arXiv preprint arXiv:2301.13808.",
            "year": 2023
        },
        {
            "authors": [
                "Xiaojing Yu",
                "Anxiao Jiang."
            ],
            "title": "Expanding, retrieving and infilling: Diversifying cross-domain question generation with flexible templates",
            "venue": "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Lin-",
            "year": 2021
        },
        {
            "authors": [
                "Zheng Yuan",
                "Qiao Jin",
                "Chuanqi Tan",
                "Zhengyun Zhao",
                "Hongyi Yuan",
                "Fei Huang",
                "Songfang Huang."
            ],
            "title": "RAMM: retrieval-augmented biomedical visual question answering with multi-modal pre-training",
            "venue": "CoRR, abs/2303.00534.",
            "year": 2023
        },
        {
            "authors": [
                "Andy Zeng",
                "Maria Attarian",
                "Brian Ichter",
                "Krzysztof Choromanski",
                "Adrian Wong",
                "Stefan Welker",
                "Federico Tombari",
                "Aveek Purohit",
                "Michael Ryoo",
                "Vikas Sindhwani"
            ],
            "title": "Socratic models: Composing zero-shot multimodal reasoning with language",
            "year": 2022
        },
        {
            "authors": [
                "Hang Zhang",
                "Xin Li",
                "Lidong Bing."
            ],
            "title": "Videollama: An instruction-tuned audio-visual language model for video understanding",
            "venue": "arXiv preprint arXiv:2306.02858.",
            "year": 2023
        },
        {
            "authors": [
                "Jian Zhang",
                "Xu Wang",
                "Hongyu Zhang",
                "Hailong Sun",
                "Xudong Liu."
            ],
            "title": "Retrieval-based neural source code summarization",
            "venue": "Proceedings of the",
            "year": 2020
        },
        {
            "authors": [
                "Jingyi Zhang",
                "Masao Utiyama",
                "Eiichro Sumita",
                "Graham Neubig",
                "Satoshi Nakamura."
            ],
            "title": "Guiding neural machine translation with retrieved translation pieces",
            "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for",
            "year": 2018
        },
        {
            "authors": [
                "Jun Zhang",
                "Yan Yang",
                "Chencai Chen",
                "Liang He",
                "Zhou Yu."
            ],
            "title": "KERS: A knowledge-enhanced framework for recommendation dialog systems with multiple subgoals",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2021, pages",
            "year": 2021
        },
        {
            "authors": [
                "Zhuosheng Zhang",
                "Aston Zhang",
                "Mu Li",
                "Hai Zhao",
                "George Karypis",
                "Alex Smola."
            ],
            "title": "Multimodal chain-of-thought reasoning in language models",
            "venue": "arXiv preprint arXiv:2302.00923.",
            "year": 2023
        },
        {
            "authors": [
                "Jinming Zhao",
                "Gholamreza Haffari",
                "Ehsan Shareghi."
            ],
            "title": "Generating synthetic speech from SpokenVocab for speech translation",
            "venue": "Findings of the Association for Computational Linguistics: EACL 2023, pages 1975\u20131981, Dubrovnik, Croatia. Association",
            "year": 2023
        },
        {
            "authors": [
                "Ruochen Zhao",
                "Xingxuan Li",
                "Yew Ken Chia",
                "Bosheng Ding",
                "Lidong Bing"
            ],
            "title": "Can chatgpt-like generative models guarantee factual accuracy? on the mistakes of new generation search engines",
            "year": 2023
        },
        {
            "authors": [
                "Ruochen Zhao",
                "Xingxuan Li",
                "Shafiq Joty",
                "Chengwei Qin",
                "Lidong Bing"
            ],
            "title": "2023c. Verify-and-edit: A knowledge-enhanced chain-of-thought framework",
            "year": 2023
        },
        {
            "authors": [
                "Luowei Zhou",
                "Hamid Palangi",
                "Lei Zhang",
                "Houdong Hu",
                "Jason Corso",
                "Jianfeng Gao."
            ],
            "title": "Unified visionlanguage pre-training for image captioning and vqa",
            "venue": "AAAI, volume 34, pages 13041\u201313049.",
            "year": 2020
        },
        {
            "authors": [
                "Mingyang Zhou",
                "Grace Luo",
                "Anna Rohrbach",
                "Zhou Yu."
            ],
            "title": "Focus! relevant and sufficient context selection for news image captioning",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2022, pages 6078\u20136088, Abu Dhabi, United Arab",
            "year": 2022
        },
        {
            "authors": [
                "Shuyan Zhou",
                "Uri Alon",
                "Frank F Xu",
                "Zhiruo Wang",
                "Zhengbao Jiang",
                "Graham Neubig."
            ],
            "title": "Docprompting: Generating code by retrieving the docs",
            "venue": "arXiv preprint arXiv:2207.05987.",
            "year": 2022
        },
        {
            "authors": [
                "Yucheng Zhou",
                "Guodong Long."
            ],
            "title": "Style-aware contrastive learning for multi-style image captioning",
            "venue": "Findings of the Association for Computational Linguistics: EACL 2023, pages 2257\u20132267, Dubrovnik, Croatia. Association for Computational Linguistics.",
            "year": 2023
        },
        {
            "authors": [
                "Jie Zhu",
                "Junhui Li",
                "Muhua Zhu",
                "Longhua Qian",
                "Min Zhang",
                "Guodong Zhou."
            ],
            "title": "Modeling graph structure in transformer for better AMR-to-text generation",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing",
            "year": 2019
        },
        {
            "authors": [
                "Wanrong Zhu",
                "An Yan",
                "Yujie Lu",
                "Wenda Xu",
                "Xin Wang",
                "Miguel Eckstein",
                "William Yang Wang."
            ],
            "title": "Visualize before you write: Imagination-guided openended text generation",
            "venue": "Findings of the Association for Computational Linguistics: EACL 2023, pages",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Generative Artificial Intelligence (GAI) has demonstrated impressive performances in tasks such as text generation (Ouyang et al., 2022; Chowdhery et al., 2022; Brown et al., 2020) and text-to-image generation (Ramesh et al., 2021a; Poole et al., 2022). The recent advancements in Multimodal Large Language Models (MLLMs) (Driess et al., 2023; OpenAI, 2023; Huang et al., 2023b) have further improved the models\u2019 capabilities to handle multi-format information, opening up possibilities for developing general-purpose learners.\nNevertheless, generative models are not exempt from inherent limitations, including the tendency for generating hallucinations (Ye and Durrett, 2022), struggling with arithmetic tasks (Patel et al., 2021), and lacking interpretability . Consequently, a promising solution for enhancing their capabilities lies in enabling them to interact with the external world and acquire knowledge in diverse formats and modalities, thereby improving\n\u2217Now affiliated with NUS \u2020Work done while the author is on leave from NTU\nthe factuality and rationality of the generated content. Recently, there have been emerging studies focusing on retrieval-augmented approaches (Mialon et al., 2023), which aim to provide information that is more grounded and factually dependent. Among them, most (Nakano et al., 2021; Guu et al., 2020b) retrieves textual information, which matches the data format used during pre-training and offers a natural medium for interaction. However, there is more world knowledge stored in different structures and modalities, such as images and videos, which is often inaccessible, unavailable, or not describable in traditional textual corpora.\nTherefore, there arises an important research intersection that retrieves multimodal knowledge to augment generative models. It offers a promising solution to current challenges such as factuality, reasoning, interpretability, and robustness. As this field is very recent, there lacks a unified understanding of recognizing these methods as a specific group, visualizing their innate connections, connecting their methodologies, and outlining their applications.\nTherefore, we survey recent advancements in multimodal retrieval-augmented generation (RAG). Specifically, we discuss current research by grouping them into different modalities, including image, code, structured knowledge, audio, and video. For each modality, we systematically search the ACL Anthology and Google Scholar with relevant keywords and perform manual filtering to determine their relevance to the survey. As a result, we collect 146 papers for detailed analysis. In Appendix A.1, we include search details, statistics, and a trend analysis figure, which shows that multimodal RAG papers have indeed developed very fastly since the emergence of large-scale generalpurpose models. Within each modality, we discuss relevant papers by grouping them under different applications. By providing an in-depth survey, we hope to help researchers recognize the importance\nof incorporating knowledge in different formats and encourage adaptation and advancements on existing techniques to the fast-growing field of LLMs.\nIn summary, our contributions are as follows: \u2022 We establish retrieval augmented generation with\nmulti-modality as an important group of methods that emerges with the recent advances in LLMs.\n\u2022 For common modalities, we provide an in-depth review of research papers by contextualizing their innate connections and shared challenges.\n\u2022 We provide an informative analysis of future directions, which could contain promising solutions to many current challenges."
        },
        {
            "heading": "2 Definitions and Background",
            "text": "To better understand the state and advancements that inspired multimodal retrieval augmentation, we first define and discuss the background of two key concepts: multimodal learning and retrievalaugmented generation (RAG)."
        },
        {
            "heading": "2.1 Multimodal Learning",
            "text": "Multimodal learning refers to learning a unified representation of data from different modalities. It aims at extracting complementary information to facilitate compositional tasks (Baltru\u0161aitis et al., 2018; Gao et al., 2020). In this survey, we include all modalities whose formats are different from natural language, including image, code, structured knowledge (e.g. tables, knowledge graphs), audio, and video.\nMultimodal generative models have a wide range of applications, such as text-image generation, creative writing generation, and multilingual translation. For instance, the image recognition task can benefit from analyzing images and videos in conjunction with textual descriptions (Ju et al., 2022; Alayrac et al., 2022a; Jia et al., 2021; Radford et al., 2021b). Conversely, incorporating visual information also aids language understanding and generation (Zhou et al., 2020; Lei et al., 2021; ?). Moreover, they have the potential to significantly improve machine learning systems across various domains by enabling models to learn from and integrate multiple sources of information (Tsai et al., 2019; Acosta et al., 2022; Nagrani et al., 2021). Additionally, there has been growing interest in developing generative models that can output multiple modalities of data (Ramesh et al., 2021b; Crowson et al., 2022; Lin and Byrne, 2022a; Chen et al., 2022a). However, there remain challenges for multimodal generative models, such as gaining access\nto a large amount of multimodal data and designing a network that produces semantically meaningful outputs."
        },
        {
            "heading": "2.2 Retrieval-Augmented Generation (RAG)",
            "text": "RAG typically consists of two phases: retrieving contextually relevant information, and guiding the generation process using the retrieved knowledge.\nRecently, RAG has gained popularity in Natural Language Processing (NLP) due to the rise of general-purpose Large Language Models (LLMs) (Chowdhery et al., 2022; OpenAI, 2023), which have boosted performances in a wide range of NLP tasks. However, there are two primary challenges: Firstly, because generative models rely on the inner knowledge (weights), they result in a high amount of hallucinations (Zhao et al., 2023b). Secondly, due to their large parameter sizes and the high costs of updating, the traditional pretraining and finetuning approaches have become infeasible. As a solution, RAG methods (Gu et al., 2018; Weston et al., 2018; Cai et al., 2019b; Lewis et al., 2020) offer a promising solution for LLMs to effectively interact with the external world.\nRAG is applied to a wide range of downstream NLP tasks, including machine translation (Gu et al., 2018; Zhang et al., 2018; Xu et al., 2020; He et al., 2021), dialogue generation (Weston et al., 2018; Wu et al., 2019; Cai et al., 2019a), abstractive summarization (Peng et al., 2019), and knowledgeintensive generation (Lewis et al., 2020; Izacard and Grave, 2021). Among them, most methods focus on retrieving textual information. For example, Guu et al. (2020b); Lewis et al. (2020); Borgeaud et al. (2022); Izacard et al. (2022) jointly train a retrieval system with an encoder or sequence-tosequence LM, achieving comparable performance to larger LMs that use significantly more parameters. Recent research also proposes combining a retriever with chain-of-thought (CoT) prompting for reasoning to augment language models (He et al., 2022a; Trivedi et al., 2022; Zhao et al., 2023c)."
        },
        {
            "heading": "3 Multimodal Retrieval-Augmented Generation",
            "text": "For each modality, there are different retrieval and synthesis procedures, targeted tasks, and challenges. Therefore, we discuss relevant methods by grouping them in terms of modality, including image, code, structured knowledge, audio, and video."
        },
        {
            "heading": "3.1 Image",
            "text": "Recent advances on pretrained models shed light on general image-text multi-modal models (Ramesh et al., 2021a; Alayrac et al., 2022b; Aghajanyan et al., 2022; Yu et al., 2022; Dou et al., 2022; Li et al., 2023a). However, these models require huge computational resources for pretraining and large amounts of model parameters \u2014 as they need to memorize vast world knowledge. More critically, they cannot efficiently deal with new or out-ofdomain knowledge. To this end, multiple retrievalaugmented methods have been proposed to better incorporate external knowledge from images and text documents. In general text generation tasks, image retrieval can also improve generation quality by expanding text generation contexts with more \u201cimagination\u201d. Visual question answering (VQA) To tackle open-domain VQA, RA-VQA (Lin and Byrne, 2022b) jointly trains the document retriever and answer generation module by approximately marginalizing predictions over retrieved documents. It first uses existing tools of object detection, image captioning, and optical character recognition (OCR) to convert target images to textual data. Then, it performs dense passage retrieval (DPR) (Karpukhin et al., 2020a) to fetch text documents relevant to the target image in the database. Finally, each retrieved document is concatenated with the initial question to generate the final prediction, similar to RAG (Lewis et al., 2020). Besides external documents, PICa (Yang et al., 2022b) and KAT (Gui et al., 2022) also consider LLMs as implicit knowledge bases and extract relevant implicit information from GPT-3. Plug-and-Play (Tiong et al., 2022) retrieves relevant image patches by using GradCAM (Selvaraju et al., 2017) to localize relevant parts based on the initial question. It then performs image captioning on retrieved patches to acquire augmented context. Beyond text-only augmented context, MuRAG (Chen et al., 2022b) retrieves both text and image data and incorporates images as visual tokens. RAMM (Yuan et al., 2023) retrieves similar biomedical images and captions and encodes them through different networks. Image captioning To generate multi-style captions, Zhou and Long (2023) uses a style-aware visual encoder to retrieve image contents before generating captions. Beyond simply encoding visual information, Cho et al. (2022) further uses the multimodal similarity between image-text pairs as a reward function to train a more fine-grained captioning model. Beyond retrieving image elements, Sarto et al. (2022); Shi et al. (2021); Ramos et al. (2023); Yang et al. (2023b) retrieves relevant captions to the inputs. Zhou et al. (2022a) tackles news image captioning by retrieving visually grounded entities in news articles. Visually grounded dialogue This task (Lee et al., 2021b) requires retrieving visual information to produce relevant dialog responses. Fan et al. (2021) augments generative models with KNN-based Information Fetching (KIF) modules that retrieve images and wiki knowledge. Liang et al. (2021) retrieves a correlated image to the dialog from an image index to ground the response generator. Shen et al. (2021) trains a word-image mapping model to retrieve response visual impressions and then uses both textual and visual information for response generation. Text generation For general text generation tasks, image retrieval can also help expand contexts. Yang et al. (2022a) augments a text model\u2019s \u201cimagination \u201d by retrieving existing images and synthesizing newly generated images. As a result, fueling language models with imagination can improve performances in many downstream natural language tasks. Similarly, Zhu et al. (2023) compares \u201cimagination\u201d augmentation with synthetic and retrieved images and argues that machine-generated images could provide better guidance due to better consideration of the contexts. Moreover, Fang and Feng (2022) shows that machine translation can be significantly improved by retrieving visual information at the phrase level, especially when the textual context is limited. Image RAG can also help low-resource tasks such as medical report generation (Wu et al., 2022a) and architectural description generation (Mille et al., 2020). Beyond retrieving images before generating text, Re-Imagen (Chen et al., 2022c) leverages a multimodal knowledge base to retrieve image-text pairs to facilitate image generation. RA-CM3 (Yasunaga et al., 2022) can generate mixtures of images and text. It shows that retrieval-augmented image generation performs much better on knowledge-intensive generation tasks and opens up new capabilities such as multimodal in-context learning."
        },
        {
            "heading": "3.2 Code",
            "text": "Software developers attempt to search for relevant information to improve their productivity from large amounts of available resources such as expla-\nnations for unknown terminologies, reusable code patches, and solutions to common programming bugs (Xia et al., 2017). Inspired by the progress of deep learning in NLP, a general retrievalaugmented generation paradigm has benefited a wide range of code intelligent tasks, including code completion (Lu et al., 2022b), code generation (Zhou et al., 2022b), and automatic program repair (APR) (Nashid et al., 2023). However, these approaches often treat programming languages and natural languages as equivalent sequences of tokens and ignore the rich semantics inherent to source code. To address these limitations, recent research work has focused on improving code generalization performance via multimodal learning, which incorporates additional modalities such as code comments, identifier tags, and abstract syntax trees (AST) into code pretrained models (Wang et al., 2021b; Guo et al., 2022; Li et al., 2022d). To this end, multimodal retrieval-augmented generation approach has demonstrated its feasibility in a variety of code-specific tasks.\nText-to-Code Generation Numerous research studies have investigated the utilization of relevant codes and associated documents to benefit code generation models. A prominent example is REDCODER (Parvez et al., 2021), which retrieves the top-ranked code snippets or summaries from an existing codebase, and aggregates them with source code sequences to enhance the generation or summarization capabilities. As another such approach, DocPrompting (Zhou et al., 2022b) uses a set of relevant documentation as in-context prompts to generate corresponding code via retrieval. In addition to these lexical modalities, Hayati et al. (2018) proposes a syntax-based code generation approach to reference existing subtree from the AST as templates to direct code generation explicitly.\nCode-to-Text Generation Retrieval-based code summarization methods are studied extensively. For example, RACE (Shi et al., 2022) leverages relevant code differences and their associated commit messages to enhance commit message generation. Besides, RACE calculates the semantic similarity between source code differences and retrieved ones to weigh the importance of different input modalities. Rencos (Zhang et al., 2020) retrieves two similar code snippets based on the aspects of syntacticlevel similarity and semantic-level similarity of a given query code. These similar contexts are then incorporated into the summarization model during\nthe decoding phase. This idea is further explored by Liu et al. (2021), where retrieved code-summary pairs are used to augment the original code property graph (Yamaguchi et al., 2014) of source code via local attention mechanisms. To capture the global semantics for better code structural learning, a global structure-aware self-attention mechanism (Zhu et al., 2019) is further employed.\nCode Completion Recent advances in retrievalbased code completion tasks (McConnell, 2004) have gained increasing attention. Notably, Hashimoto et al. (2018) adapts the retrieve-and-edit framework to improve the model\u2019s performance in code auto-completion tasks. To address practical code completion scenarios, ReACC (Lu et al., 2022b) takes both lexical and semantic information of the unfinished code snippet into account, utilizing a hybrid technique to combine a lexical-based sparse retriever and a semantic-based dense retriever. First, the hybrid retriever searches for a relevant code from the codebase based on the given incomplete code. Then, the unfinished code is concatenated with the retrieval, and an auto-regressive code completion generator will generate the completed code based on them. In order to address project relations, CoCoMIC (Ding et al., 2022) decomposes a code file into four components: files, global variables, classes, and functions. It constructs an in-file context graph based on the hierarchical relations among all associated code components, forming a project-level context graph by considering both in-file and cross-file dependencies. Given an incomplete program, CoCoMIC retrieves the most relevant cross-file entities from its project-level context graph and jointly learns the incomplete program and the retrieved cross-file context for code completion.\nAutomatic Program Repair (APR) Inspired by the nature that a remarkable portion of commits is comprised of existing code commits (Martinez et al., 2014), APR is typically treated as a search problem by traversing the search space of repair ingredients to identify a correct fix (Qi et al., 2014), based on a redundancy assumption (White et al., 2019) that the target fix can often be reconstructed in the search space. Recent studies have shown that mining relevant bug-fix patterns from existing search space (Jiang et al., 2018) and external repair templates from StackOverflow (Liu and Zhong, 2018) can significantly benefit APR\nmodels. Joshi et al. (2022) intuitively ranks a collection of bug-fix pairs based on the similarity of error messages to develop few-shot prompts. They incorporate compiler error messages into a large programming language model Codex (Chen et al., 2021) for multilingual APR. CEDAR (Nashid et al., 2023) further extends this idea to retrieval-based prompts design using relevant code demonstrations, comprising more modalities such as unit test, error type, and error information. Additionally, Jin et al. (2023) leverage a static analyzer Infer to extract error type, error location, and syntax hierarchies (Clement et al., 2021) to prioritize the focal context. Then, they retrieve semantically-similar fixes from an existing bug-fix codebase and concatenate the retrieved fixes and focal context to form the instruction prompts for program repair.\nReasoning over Codes as Intermediate Steps While large language models (LLMs) have recently demonstrated their impressive capability to perform reasoning tasks, they are still prone to logical and arithmetic errors (Gao et al., 2022a; Chen et al., 2022d; Madaan et al., 2022). To mitigate this issue, emerging research papers have focused on using LLMs of code (e.g., Codex (Chen et al., 2021)) to generate the code commands for solving logical and arithmetic tasks and calling external interpreters to execute the commands to obtain the results. Notably, Gao et al. (2022a) proposes to generate Python programs as intermediate reasoning steps and offload the solution step to a Python interpreter. Additionally, Chen et al. (2022d) explore generating chain-of-thought (CoT) (Wei et al., 2022) for not only text but also programming language statements as reasoning steps to solve the problem. During the inference phase, answers are obtained via an external interpreter. Similarly, Lyu et al. (2023) propose Faithful CoT that first translates the natural language query to a symbolic reasoning chain and then solves the reasoning chain by calling external executors to derive the answer. Another example is Ye et al. (2023), which utilizes LLMs to decompose table-based reasoning tasks into subtasks, decouples logic and numerical computations in each step through SQL queries by Codex, and calls SQL interpreters to solve them (a process called \"parsing-execution-filling\").\nLLMs of code are also known as good-structured commonsense reasoners, and even better-structured reasoners than LLMs (Madaan et al., 2022). As a result, prior studies have also investigated the\nidea of transforming structured commonsense generation tasks into code generation problems and employing LLMs of code as the solvers. One such work is CoCoGen (Madaan et al., 2022) which converts each training sample (consisting of textual input and the output structure) into a Tree class in Python. The LLMs of code then perform few-shot reasoning over the textual input to generate Python code, and the Python code is then converted back to the original structure for evaluation. Besides, the success of LLMs of code such as Codex in synthesizing computer code also makes them suitable for generating formal codes. Motivated by this, Wu et al. (2022b) propose to prove mathematical theorems by adopting Codex to generate formalized theorems from natural language mathematics for the interactive theorem prover Isabelle (Wenzel et al., 2008)."
        },
        {
            "heading": "3.3 Structured Knowledge",
            "text": "An open challenge in generative models is hallucination, where the model is likely to output false information. Thus, A potential solution is to ground generation with retrieved structured knowledge, such as knowledge graphs, tables, and databases. Question Answering (QA) A natural setting to use knowledge is QA. To augment Knowledge Base (KB) QA by extracting the most relevant knowledge, Hu et al. (2022b) uses dense retrieval and Liu et al. (2022b) uses a cross-encoder ranker. Shu et al. (2022) employs multi-grained retrieval to extract relevant KB context and uses constrained decoding to control the output space. In table QA, Nan et al. (2022) proposes a dataset that requires retrieving relevant tables for answer generation. Pan et al. (2021) then proposes a method that uses a transformer-based system to retrieve the most relevant tables and locate the correct cells. Moreover, to improve Video QA, Hu et al. (2023) retrieves from Knowledge Graph (KG) encodings stored in the memory. The most prominent RAG usage remains in open-domain QA, where multiple datasets are proposed (Lin et al., 2023; Ramnath et al., 2021). For retrieval, Ma et al. (2022) verbalizes the KG and then uses dense passage retrieval. Fan et al. (2019); Gupta et al. (2018) encodes KG information into dense representations. Pramanik et al. (2021); Jin et al. (2022) builds graph embeddings to retrieve question-relevant evidence. Xu et al. (2021); Baek et al. (2023) use semantic similarity and text-matching methods. Synthesis can occur at different stages. At the input stage, Xu\net al. (2021); Baek et al. (2023) feed in the retrieved contexts as additional inputs or prompts to the PLM. (Ma et al., 2022; Fan et al., 2019) adapt the generator to accept the context representations as inputs. At model inference stage, an interesting work is Hu et al. (2022c), which inserts an interaction layer into PLMs to guide an external KG reasoning module.\nGeneral text generation External knowledge retrieval can improve general text generation to be more factually grounded. Liu et al. (2022a) presents a memory-augmented approach to condition an autoregressive language model on a knowledge graph (KG). During inference, Tan et al. (2022) selects knowledge entries through dense retrieval and then injects them into the input encoding and output decoding stages in pretrained language models (PLMs). For domain-specific text generation, Frisoni et al. (2022); Yang et al. (2021); Li et al. (2019) retrieve medical report chunks or report templates to augment input prompts. Then, they use self-devised decoders or graph transformers to generate grounded reports. To improve interpretability, RAG could be used to select facts as interpretable reasoning paths (Aggarwal et al., 2021; Jansen and Ustalov, 2019). Moreover, RAG is especially useful for low-resource generation tasks, such as question generation (Yu and Jiang, 2021; Xin et al., 2021; Gu et al., 2019), documentto-slide (Sun et al., 2021), table-to-text (Su et al., 2021), counterargument generation (Jo et al., 2021), entity description generation (Cheng et al., 2020) and text-based games (Murugesan et al., 2021).\nRecent research has attempted to reduce hallucinations in LLMs by leveraging external structured knowledge. For example, during fine-tuning, LaMDA (Thoppilan et al., 2022) learns to consult external knowledge sources before responding to the user, including an information retrieval system that can retrieve knowledge triplets and web URLs. Some papers treat the generative model (often large language models) as black-box and retrieve structured information without fine-tuning. For example, BINDER (Cheng et al., 2023) uses in-context learning to output designed API calls that retrieve question-relevant columns from tables.\nReasoning with knowledge By selecting knowledge, reasoning tasks can be solved in a more grounded and interpretable way. To generate an entailment tree explanation for a given hypothesis, Neves Ribeiro et al. (2022) retrieves from tex-\ntual premises iteratively and combines them with generation. Yang et al. (2022c) proposes a math reasoner that first retrieves highly-correlated algebraic knowledge and then passes them as prompts to improve the semantic representations for the generation task. With the recent advances in LLMs, He et al. (2022a); Li et al. (2023b) retrieve from KG and KB, such as Wikidata, based on reasoning steps obtained from the chain-of-thought (CoT) prompting (Wei et al., 2022). Knowledge-grounded dialogue Dialogue generation based on relevant tables and knowledge bases has been a practical research application (Wu et al., 2020b; Li et al., 2022b; Nakamura et al., 2022; Gao et al., 2022b; Lu et al., 2023). To tackle the challenge, Li et al. (2022c) and Galetzka et al. (2021) retrieve relevant knowledge, process it into a dense representation and incorporate it into dialogue generation. On top of dense representations, Gu et al. (2020) and Jung et al. (2020) leverage attention mechanisms to flexibly adjust which knowledge to depend on during generation. Some methods (Zhang et al., 2021; Dziri et al., 2021; Chen et al., 2020) first generate subgoals or responses and then use them to retrieve relevant knowledge. The retrieved knowledge then helps amend previous responses. Besides knowledge, Cai et al. (2019b) and Wu et al. (2020a) improve dialogue response generation by retrieving templates or prototype dialogues to augment inputs. Recently, Kang et al. (2023) retrieves relevant subgraphs from KGs, and then utilizes contrastive learning to ensure that the generated texts have high similarity to the subgraphs.\nBy retrieving from relevant sources, RAG not only improves factuality but also provides the grounding contexts while generating, thus addressing interpretability and robustness concerns. With the potential to handle more information types with recent advances in LLMs (OpenAI, 2023), RAG with structured knowledge could be further enhanced. There are still challenges to be addressed. For example, there could be new designs for better retrieval systems that could promote efficient interactions suitable for diverse knowledge bases. Synthesizing this information correctly is also an open challenge, where it is hard to decide which parts need augmenting in the textual outputs."
        },
        {
            "heading": "3.4 Audio",
            "text": "Audio RAG is useful in incorporating audio information in specific audio-language tasks, such as\nmusic captioning, music and text generation, and speech recognition. Moreover, using audio RAG for audio data augmentation has also been proven useful in mitigating the lack of audio-text training data. It could be a promising future direction (Li et al., 2022a).\nText-audio data augmentation For text-audio tasks, one of the most important challenges is the lack of training data on audio-text pairs. Therefore, retrieving audio and textual cues can alleviate the data scarcity problem and improve performance. In audio captioning, which aims at translating the input audio into its description, Koizumi et al. (2020) retrieves guidance captions similar to the input audio from the training set. Then, the retrieved guidance captions are fed into a PLM to help generate new captions, which improves generation performance. To augment scarce speech translation (ST) data, Zhao et al. (2023a) proposes SpokenVocab, a technique to convert machine translation (MT) data to synthetic ST data. To form synthetic speech, SpokenVocab retrieves and stitches audio snippets, corresponding to words in an MT sentence. Experiments show that stitched audio snippets can improve translation quality. Kim et al. (2023) leverages a PLM to tackle the data scarcity issue. It retrieves features from the input audio, maps them to continuous vectors using mapping networks, and uses vectors as prefixes for prefix tuning the PLM. With the additional information from retrieved audio, it outperforms previous methods. In text-to-audio generation, Huang et al. (2023a) applies audio-text retrieval to get pseudo text prompts, which enhance audio generation in data-scarce scenarios. To augment the argumentation mining (AM) task in political debates, Mestre et al. (2023) integrates audio features into PLMs, which improves performance when data is scarce.\nMusic captioning Music captioning is the task of generating a text description or lyrics given the music audio. And RAG is explored to learn better audio-lyric alignment. Manco et al. (2021) proposes the first music audio captioning model, MusCaps. Firstly, a pretrained multimodal encoder obtains audio representations that retrieve musical features in the input. As the pretraining bridges the gap between the audio modality and textual understanding, the method improves task performance. He et al. (2022b) learns an audio-lyric alignment through contrastive learning, which results in a higher-quality generation of captions for music.\nMusic generation Royal et al. (2020) uses deep neural hashing to retrieve music building blocks and then performs generation by using the current music segment to retrieve the next. In automatic speech recognition (ASR), Chan et al. (2023) uses a k-nearest neighbor (KNN) approach to retrieve external knowledge related to the audio and text embeddings. The retrieved knowledge significantly reduces domain adaptation time for ASR.\nThe audio modality is closely intertwined with other modalities, such as video. Therefore, recent advancements in using audio features for text-video retrieval (Falcon et al., 2022; Mithun et al., 2018) can benefit RAG tasks involving other modalities. Moreover, although audio-text retrieval has been a long-standing task (Liu et al., 2015; Milde et al., 2016a,b), exploring recently discovered techniques (Hu et al., 2022a; Lou et al., 2022; Koepke et al., 2022) could lead to further improvements."
        },
        {
            "heading": "3.5 Video",
            "text": "Retrieving video snippets for generation is used primarily in two tasks: video-grounded dialogue and video captioning. Recently, augmenting LLMs with video retrieval also demonstrates good performances, especially in few-shot settings. Video-grounded dialogue Given video contexts, the model learns to engage in a relevant dialogue. Pasunuru and Bansal (2018) introduces a video-context, many-speaker dialogue dataset, which challenges researchers to develop visuallygrounded dialogue models that generate relevant responses from live videos. Similarly, Lei et al. (2020) proposes TVQA+, a dataset that requires retrieving relevant video moments to answer textual questions about videos. Then, it proposes a unified framework that encodes video segments into representations, uses an attention mechanism to locate relevant information, and produces textual answers. To better perform visually-grounded dialogue tasks, Le et al. (2020) retrieves visual cues from prior user queries. The cues are then used as contextual information to construct relevant responses. On video QA, it substantially outperforms prior approaches. Recently, Le et al. (2022) extracts visual cues from the video to augment video-grounded dialogues. The video retrieval is performed with neural module networks, which are instantiated with entities and actions in previous dialogues. Video captioning Sharing a similar motivation to RAG, Long et al. (2018) first proposes to use attention layers to automatically select the most\nsalient visual or semantic features and use them to augment caption generation. As a result, it stably outperforms previous methods. (Whitehead et al., 2018) then develops a retrieval-based approach for video description generation. For news videos, it retrieves topically related news documents and then generates a description using a knowledge-aware video description network. LLM augmentation Wang et al. (2022) attempts to augment an LLM to generalize to various videoto-text tasks from a few examples. As the LLMs cannot accept video inputs, it first translates video contents into attributes using image-language models and then prompts the retrieved content to instruct the LLM. It demonstrates good few-shot performances on a wide range of video-language tasks.\nCurrently, the video-text research bottleneck mainly lies in the representation gap between different modalities. Research has been attempting to learn a better mapping between video-text via joint learning (Xu et al., 2015; Sun et al., 2019). Recent studies on dense video representation learning can also be useful for future video RAG. Besides, some papers (Yang et al., 2023a; Wang et al., 2021a) try to introduce fine-grained interaction between different modalities to learn better aligned representations. Zeng et al. (2022) encourages multiple pretrained models in different modalities to exchange information with each other in a zeroshot manner. Most recently, Zhang et al. (2023a) trains Video-Llama to better align pretrained video and audio encoders with LLM\u2019s embedding space."
        },
        {
            "heading": "4 Future Directions",
            "text": "With the development of multi-modal LLMs, retrieving multimodal information to augment text generation will be a promising direction to better ground textual generation in real-world contexts, contributing towards building a model that is fully aware and can better interact with the world. Specifically, we describe some potential directions that can be of benefit to the community."
        },
        {
            "heading": "4.1 Retrieval Augmented Multimodal Reasoning",
            "text": "One potential application of multimodal RAG is multimodal reasoning. Lu et al. (2022a) first introduces ScienceQA, a large-scale multimodal science question dataset annotated with lectures and explanations. Then, Zhang et al. (2023b) proposes Multimodal Chain-of-Thought (Multimodal-CoT) which incorporates language and vision modalities\ninto a two-stage (rationale generation and answer inference) framework, surpassing GPT-3.5 by a large margin with a much smaller fine-tuned model. Similar to Zhang et al. (2023b), kosmos-1 (Huang et al., 2023b) breaks down multimodal reasoning into two steps. It first generates intermediate content as the rationale based on visual information and then uses the generated rationale to induce the result. However, both methods may have difficulties understanding certain types of images (e.g., maps), which could be mitigated by retrieving informative image-text pairs."
        },
        {
            "heading": "4.2 Building a Multimodal Knowledge Index",
            "text": "In order to facilitate multimodal RAG, one of the most fundamental aspects is building a multimodal knowledge index. The goal is twofold: Firstly, dense representations should support low storage, dynamic updating of the knowledge base, and accurate search. Secondly, it could enable faster search speed with the help of local sensitive hashing (Leskovec et al., 2014), which combats scaling and robustness concerns when the knowledge base is scaled up extremely.\nCurrently, the dense representations for text snippets are widely studied for documents (Karpukhin et al., 2020b; Gao and Callan, 2021; Gao et al., 2021), entities (Sciavolino et al., 2021; Lee et al., 2021a), and images (Radford et al., 2021a). Besides, there are studies optimizing dense representations in an end-to-end manner (Lewis et al., 2020). Nevertheless, few papers (Chen et al., 2022a) have explored building a multimodal index at the same time for downstream generation tasks. How to map a multimodal knowledge index into a unified space remains a long-term challenge."
        },
        {
            "heading": "4.3 Pretraining with Multimodal Retrieval",
            "text": "To better align the abilities to handle different modalities in a pre-trained model, future work could be built on employing retrieval-based approaches during pre-training. Currently, some methods fine-tune the pre-trained generative model to learn to retrieve from different modalities. For example, LaMDA (Thoppilan et al., 2022) calls an external toolset for fine-tuning, including an information retrieval system. Similarly, during finetuning, Toolformer (Schick et al., 2023) augments models with API calls to tools including a QA system and a Wikipedia search engine.\nWhen similar retrieval abilities are leveraged during pretraining, the generative models can interact\nwith retrieval tools much better. Then, instead of relying solely on internal weights, they could effectively use an external base to output more grounded information, provide relevant contexts to users, and update their information accordingly. Such pretraining techniques would also greatly improve robustness for out-of-domain tasks. As an example, Guu et al. (2020a) augments pretraining with an external knowledge retriever, which outperforms previous methods.\nTo incorporate retrieval with pretraining, there remains the challenge of developing appropriate datasets labeled with retrieval API calls. To tackle this challenge, LaMDA (Thoppilan et al., 2022) uses labels developed by human annotators, which could be expensive to collect. Toolformer (Schick et al., 2023) uses a sampling and filtering approach for automatic labeling, which is inexpensive but could induce bias. A potential solution is to use a neuro-symbolic approach (Davoudi and Komeili, 2021), which uses prototype learning and deepKNN to find nearest neighbors during training."
        },
        {
            "heading": "5 Conclusions",
            "text": "This survey reviews research that augments generative models by retrieving multi-modal information. Specifically, we categorize the current domain into enhancing with different modalities, including image, code, structured knowledge, speech, and video. With the emergence of large multi-modal models, we believe that this survey could serve as a comprehensive overview of an emerging and promising field. Moreover, we hope it could encourage future research in the domain, including retrieval-augmented multimodal reasoning, building a multi-modal knowledge index, and combining retrieval with pretraining."
        },
        {
            "heading": "6 Limitations",
            "text": "RAG also has some limitations. For example, there exists an attribution-fluency tradeoff (Aksitov et al., 2023) where the output quality is affected due to the added constraints of the retrieved knowledge."
        },
        {
            "heading": "A Appendix",
            "text": "A.1 Search Criteria and Results\nFor searching the ACL anthology articles, we use a keyword search over titles and abstracts. We strictly enforce the keyword \u201cretriev\u201d. Then, we enforce either \u201cgenerat\u201d or \u201cground\u201d to appear. For each modality, we then add modality-specific keywords: \u201cimage\u201d for the image modality, \u201ccode\u201d for the code modality, any one from \u201cstructured knowledge/table/database/knowledge graph\u201d for the structured knowledge modality, any one from \u201caudio/speech\u201d for the audio modality, and \u201cvideo\u201d for the video modality.\nFor searching on Google Scholar, we add the keyword \u201clanguage models\u201d to select more NLPrelated articles. We then perform manual filtering on the top 3 pages of returned results.\nThe number of retrieved and analyzed research papers can be found in Table 1.\nA trend analysis of how the number of papers change across time is shown in Figure 1 We could observe that the domain of multimodal retrievalaugmented generation has indeed developed a lot\nrecently, with peaks reached around end of 2022. The observation is consistent with our hypothesis that multimodal RAG is especially important and helpful in the age of large-scale general-purpose models."
        }
    ],
    "title": "Retrieving Multimodal Information for Augmented Generation: A Survey",
    "year": 2023
}