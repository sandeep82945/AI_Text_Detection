{
    "abstractText": "Automatic summarization of legal case judgments is a practically important problem that has attracted substantial research efforts in many countries. In the context of the Indian judiciary, there is an additional complexity \u2013 Indian legal case judgments are mostly written in complex English, but a significant portion of India\u2019s population lacks command of the English language. Hence, it is crucial to summarize the legal documents in Indian languages to ensure equitable access to justice. While prior research primarily focuses on summarizing legal case judgments in their source languages, this study presents a pioneering effort toward cross-lingual summarization of English legal documents into Hindi, the most frequently spoken Indian language. We construct the first high-quality legal corpus comprising of 3,122 case judgments from prominent Indian courts in English, along with their summaries in both English and Hindi, drafted by legal practitioners. We benchmark the performance of several diverse summarization approaches on our corpus and demonstrate the need for further research in cross-lingual summarization in the legal domain.",
    "authors": [
        {
            "affiliations": [],
            "name": "Debtanu Datta"
        },
        {
            "affiliations": [],
            "name": "Shubham Soni"
        },
        {
            "affiliations": [],
            "name": "Rajdeep Mukherjee"
        },
        {
            "affiliations": [],
            "name": "Saptarshi Ghosh"
        }
    ],
    "id": "SP:dfc7d48a04a09a4ddf036b6bae3cbd8b64078019",
    "references": [
        {
            "authors": [
                "Dennis Aumiller",
                "Ashish Chouhan",
                "Michael Gertz."
            ],
            "title": "EUR-lex-sum: A multi- and cross-lingual dataset for long-form summarization in the legal domain",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Process-",
            "year": 2022
        },
        {
            "authors": [
                "Iz Beltagy",
                "Matthew E. Peters",
                "Arman Cohan."
            ],
            "title": "Longformer: The long-document transformer",
            "venue": "ArXiv, abs/2004.05150.",
            "year": 2020
        },
        {
            "authors": [
                "Abhik Bhattacharjee",
                "Tahmid Hasan",
                "Wasi Uddin Ahmad",
                "Yuan-Fang Li",
                "Yong-Bin Kang",
                "Rifat Shahriyar."
            ],
            "title": "CrossSum: Beyond English-centric cross-lingual summarization for 1,500+ language pairs",
            "venue": "Proceedings of the 61st Annual Meeting of",
            "year": 2023
        },
        {
            "authors": [
                "Paheli Bhattacharya",
                "Kaustubh Hiware",
                "Subham Rajgaria",
                "Nilay Pochhi",
                "Kripabandhu Ghosh",
                "Saptarshi Ghosh."
            ],
            "title": "A comparative study of summarization algorithms applied to legal case judgments",
            "venue": "Advances in Information Retrieval, pages 413\u2013428,",
            "year": 2019
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
            "year": 2019
        },
        {
            "authors": [
                "G\u00fcnes Erkan",
                "Dragomir R. Radev."
            ],
            "title": "Lexrank: Graph-based lexical centrality as salience in text summarization",
            "venue": "J. Artif. Int. Res., 22(1):457\u2013479.",
            "year": 2004
        },
        {
            "authors": [
                "Fangxiaoyu Feng",
                "Yinfei Yang",
                "Daniel Cer",
                "Naveen Arivazhagan",
                "Wei Wang."
            ],
            "title": "Language-agnostic BERT sentence embedding",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages",
            "year": 2022
        },
        {
            "authors": [
                "Max Grusky",
                "Mor Naaman",
                "Yoav Artzi"
            ],
            "title": "Newsroom: A dataset of 1.3 million summaries with diverse extractive strategies",
            "venue": "In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics:",
            "year": 2018
        },
        {
            "authors": [
                "Mandy Guo",
                "Joshua Ainslie",
                "David Uthus",
                "Santiago Ontanon",
                "Jianmo Ni",
                "Yun-Hsuan Sung",
                "Yinfei Yang."
            ],
            "title": "LongT5: Efficient text-to-text transformer for long sequences",
            "venue": "Findings of the Association for Computational Linguistics: NAACL 2022, pages 724\u2013",
            "year": 2022
        },
        {
            "authors": [
                "Tahmid Hasan",
                "Abhik Bhattacharjee",
                "Md. Saiful Islam",
                "Kazi Mubasshir",
                "Yuan-Fang Li",
                "Yong-Bin Kang",
                "M. Sohel Rahman",
                "Rifat Shahriyar."
            ],
            "title": "XLsum: Large-scale multilingual abstractive summarization for 44 languages",
            "venue": "Findings of the Association",
            "year": 2021
        },
        {
            "authors": [
                "Faisal Ladhak",
                "Esin Durmus",
                "Claire Cardie",
                "Kathleen McKeown."
            ],
            "title": "WikiLingua: A new benchmark dataset for cross-lingual abstractive summarization",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4034\u20134048,",
            "year": 2020
        },
        {
            "authors": [
                "Chin-Yew Lin."
            ],
            "title": "ROUGE: A package for automatic evaluation of summaries",
            "venue": "Text Summarization Branches Out, pages 74\u201381, Barcelona, Spain. Association for Computational Linguistics.",
            "year": 2004
        },
        {
            "authors": [
                "Yang Liu",
                "Mirella Lapata."
            ],
            "title": "Text summarization with pretrained encoders",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing",
            "year": 2019
        },
        {
            "authors": [
                "Ramesh Nallapati",
                "Feifei Zhai",
                "Bowen Zhou."
            ],
            "title": "Summarunner: A recurrent neural network based sequence model for extractive summarization of documents",
            "venue": "Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence, AAAI\u201917, page",
            "year": 2017
        },
        {
            "authors": [
                "Ani Nenkova",
                "Sameer Maskey",
                "Yang Liu."
            ],
            "title": "Automatic summarization",
            "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts, page 3, Portland, Oregon. Association for Computational Linguistics.",
            "year": 2011
        },
        {
            "authors": [
                "Kishore Papineni",
                "Salim Roukos",
                "Todd Ward",
                "WeiJing Zhu."
            ],
            "title": "Bleu: a method for automatic evaluation of machine translation",
            "venue": "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311\u2013318, Philadelphia,",
            "year": 2002
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J. Liu."
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "CoRR, abs/1910.10683.",
            "year": 2019
        },
        {
            "authors": [
                "Vivek Raghavan",
                "Anoop Kunchukuttan",
                "Pratyush Kumar",
                "Mitesh Shantadevi Khapra."
            ],
            "title": "Samanantar: The largest publicly available parallel corpora collection for 11 Indic languages",
            "venue": "Transactions of the Association for Computational Linguistics, 10:145\u2013",
            "year": 2022
        },
        {
            "authors": [
                "Nils Reimers",
                "Iryna Gurevych."
            ],
            "title": "Making monolingual sentence embeddings multilingual using knowledge distillation",
            "venue": "Proceedings of the",
            "year": 2020
        },
        {
            "authors": [
                "Thomas Scialom",
                "Paul-Alexis Dray",
                "Sylvain Lamprier",
                "Benjamin Piwowarski",
                "Jacopo Staiano."
            ],
            "title": "MLSUM: The multilingual summarization corpus",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
            "year": 2020
        },
        {
            "authors": [
                "Abhay Shukla",
                "Paheli Bhattacharya",
                "Soham Poddar",
                "Rajdeep Mukherjee",
                "Kripabandhu Ghosh",
                "Pawan Goyal",
                "Saptarshi Ghosh."
            ],
            "title": "Legal case document summarization: Extractive and abstractive methods and their evaluation",
            "venue": "Proceedings of the 2nd Con-",
            "year": 2022
        },
        {
            "authors": [
                "Sajad Sotudeh",
                "Arman Cohan",
                "Nazli Goharian."
            ],
            "title": "On generating extended summaries of long documents",
            "venue": "CoRR, abs/2012.14136.",
            "year": 2020
        },
        {
            "authors": [
                "Y. Tang",
                "C. Tran",
                "Xian Li",
                "Peng-Jen Chen",
                "Naman Goyal",
                "Vishrav Chaudhary",
                "Jiatao Gu",
                "Angela Fan."
            ],
            "title": "Multilingual translation with extensible multilingual pretraining and finetuning",
            "venue": "ArXiv, abs/2008.00401.",
            "year": 2020
        },
        {
            "authors": [
                "Vedanuj Goswami",
                "Francisco Guzm\u2019an",
                "Philipp Koehn",
                "Alexandre Mourachko",
                "Christophe Ropers",
                "Safiyyah Saleem",
                "Holger Schwenk",
                "Jeff Wang"
            ],
            "title": "No language left behind: Scaling human-centered machine",
            "year": 2022
        },
        {
            "authors": [
                "J\u00f6rg Tiedemann",
                "Santhosh Thottingal."
            ],
            "title": "OPUSMT \u2013 building open translation services for the world",
            "venue": "Proceedings of the 22nd Annual Conference of the European Association for Machine Translation, pages 479\u2013480, Lisboa, Portugal. European Associa-",
            "year": 2020
        },
        {
            "authors": [
                "Ashok Urlana",
                "Sahil Manoj Bhatt",
                "Nirmal Surange",
                "Manish Shrivastava."
            ],
            "title": "Indian language summarization using pretrained sequence-to-sequence models",
            "venue": "Fire.",
            "year": 2023
        },
        {
            "authors": [
                "Daniel Varab",
                "Natalie Schluter"
            ],
            "title": "MassiveSumm: a very large-scale, very multilingual",
            "year": 2021
        },
        {
            "authors": [
                "Jiaan Wang",
                "Fandong Meng",
                "Ziyao Lu",
                "Duo Zheng",
                "Zhixu Li",
                "Jianfeng Qu",
                "Jie Zhou."
            ],
            "title": "ClidSum: A benchmark dataset for cross-lingual dialogue summarization",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Process-",
            "year": 2022
        },
        {
            "authors": [
                "Linting Xue",
                "Noah Constant",
                "Adam Roberts",
                "Mihir Kale",
                "Rami Al-Rfou",
                "Aditya Siddhant",
                "Aditya Barua",
                "Colin Raffel."
            ],
            "title": "mT5: A massively multilingual pre-trained text-to-text transformer",
            "venue": "Proceedings of the 2021 Conference of the North American Chap-",
            "year": 2021
        },
        {
            "authors": [
                "Jen-Yuan Yeh",
                "Hao-Ren Ke",
                "Wei-Pang Yang",
                "IHeng Meng."
            ],
            "title": "Text summarization using a trainable summarizer and latent semantic analysis",
            "venue": "Information Processing & Management, 41(1):75\u201395. An Asian Digital Libraries Perspective.",
            "year": 2005
        },
        {
            "authors": [
                "Jingqing Zhang",
                "Yao Zhao",
                "Mohammad Saleh",
                "Peter J. Liu."
            ],
            "title": "Pegasus: Pre-training with extracted gap-sentences for abstractive summarization",
            "venue": "ArXiv, abs/1912.08777.",
            "year": 2019
        },
        {
            "authors": [
                "Tianyi Zhang",
                "Varsha Kishore",
                "Felix Wu",
                "Kilian Q. Weinberger",
                "Yoav Artzi."
            ],
            "title": "Bertscore: Evaluating text generation with bert",
            "venue": "International Conference on Learning Representations.",
            "year": 2020
        },
        {
            "authors": [
                "Bhattacharjee"
            ],
            "title": "Appendix A Details of Related work Datasets for summarization in Indian languages: Some recent work on cross-lingual summarization",
            "year": 2023
        },
        {
            "authors": [
                "French",
                "German",
                "Spanish",
                "Russian",
                "Turkish. Recently",
                "Wang"
            ],
            "title": "2022) released a benchmark dataset called CLIDSUM for Cross-Lingual Dialogue Summarization",
            "year": 2022
        },
        {
            "authors": [
                "summarization",
                "Aumiller"
            ],
            "title": "2022) recently introduced Eur-Lex, a multi- and cross-lingual corpus of legal texts and human-written summaries from the European Union (EU)",
            "year": 2022
        }
    ],
    "sections": [
        {
            "text": "Automatic summarization of legal case judgments is a practically important problem that has attracted substantial research efforts in many countries. In the context of the Indian judiciary, there is an additional complexity \u2013 Indian legal case judgments are mostly written in complex English, but a significant portion of India\u2019s population lacks command of the English language. Hence, it is crucial to summarize the legal documents in Indian languages to ensure equitable access to justice. While prior research primarily focuses on summarizing legal case judgments in their source languages, this study presents a pioneering effort toward cross-lingual summarization of English legal documents into Hindi, the most frequently spoken Indian language. We construct the first high-quality legal corpus comprising of 3,122 case judgments from prominent Indian courts in English, along with their summaries in both English and Hindi, drafted by legal practitioners. We benchmark the performance of several diverse summarization approaches on our corpus and demonstrate the need for further research in cross-lingual summarization in the legal domain."
        },
        {
            "heading": "1 Introduction",
            "text": "Legal judgment summarization is an important and challenging task, especially considering the lengthy and complex nature of case judgments (Shukla et al., 2022; Bhattacharya et al., 2019; Aumiller et al., 2022). In the context of the Indian judiciary, there is an additional requirement \u2013 Indian legal case judgments are mostly written in complex English due to historical reasons, but a significant portion of India\u2019s population lacks a strong command of the English language. Hence, it is important to summarize case judgements in Indian languages to ensure equitable access to justice.\n\u2217Equal contribution by the first two authors\nThere exist a few Indian legal case judgment summarization datasets, e.g., the datasets developed by Shukla et al. (2022), but all of them contain English case documents and summaries only. In this work, we introduce MILDSum (Multilingual Indian Legal Document Summarization), a dataset consisting of 3,122 case judgments from multiple High Courts and the Supreme Court of India in English, along with the summaries in both English and Hindi, drafted by legal practitioners.\nThere have been previous efforts in compiling document-summary pairs in the legal domain, such as the Eur-Lex (Aumiller et al., 2022) dataset containing 1,500 document-summary pairs per language, for several European languages. However, to our knowledge, there has not been any similar effort in the Indian legal domain. Thus, MILDSum is the first dataset to enable cross-lingual summarization of Indian case judgments.\nTo construct MILDSum, we utilize the website LiveLaw (https://www.livelaw.in/), a popular site among Indian Law practitioners, that publishes articles in both English and Hindi summarizing the important case judgments pronounced by the Supreme Court and High Courts of India. According to the site, these articles are written by qualified Law practitioners; hence, they can be considered as high-quality summaries of the case judgments. We carefully extract the case judgments (in English) from this website, along with the English and Hindi articles summarizing each judgment. A major challenge that we had to address in this process is to link the English article and the Hindi article corresponding to the same judgment. The MILDSum dataset is available at https://github.com/Law-AI/MILDSum.\nWe also benchmark a wide variety of summarization models on the MILDSum dataset. We consider two broad approaches \u2013 (1) a summarize-thentranslate pipeline, where a summarization model is used over a case judgment (in English) to generate\nan English summary, and then the English summary is translated into Hindi, and (2) a direct crosslingual summarization approach, where a state-ofthe-art cross-lingual summarization model (Bhattacharjee et al., 2023) is finetuned over our dataset to directly generate Hindi summaries from the English documents. We observe that the summarizethen-translate approach performs better (e.g., best ROUGE-2 score of 32.27 for English summaries and 24.87 for Hindi summaries on average) than the cross-lingual summarization approach (best ROUGE-2 score of 21.76 for the Hindi summaries), in spite of reduction in summary quality due to the translation stage. Thus, our experiments demonstrate the need for better cross-lingual summarization models for the legal domain. To this end, we believe that the MILDSum dataset can be very useful in training and evaluating cross-lingual summarization models, for making legal judgments accessible to the common masses in India.\nAlso note that, apart from training summarizing models, the MILDSum dataset can also be used to train / evaluate Machine Translation (MT) models in the legal domain, given the paired English and Hindi summaries. MT in the Indian legal domain is another challenging problem that has not been explored much till date."
        },
        {
            "heading": "2 Related Works",
            "text": "There has been limited prior work on crosslingual summarization in Indian languages. A popular large-scale multi-lingual summarization dataset MassiveSumm was introduced by Varab and Schluter (2021) which includes some Indian languages as well. The ILSUM dataset (Urlana et al., 2023) encompasses Hindi and Gujarati, in addition to English. Bhattacharjee et al. (2023) recently introduced a large-scale cross-lingual summarization dataset CrossSum, while Ladhak et al. (2020) released WikiLingua as a benchmark dataset for cross-lingual summarization. However, none of these datasets have specifically addressed crosslingual summarization within the legal domain.\nThere exist some multi/cross-lingual summarization datasets in the legal domain for non-Indian languages, such as Eur-Lex (Aumiller et al., 2022) and CLIDSUM (Wang et al., 2022). In particular, Eur-Lex is a multi- and cross-lingual corpus of legal texts originating from the European Union (EU); it covers all 24 official languages of the EU. But to our knowledge, no such effort has been made in\nthe Indian legal domain, and this work takes the first step to bridge this gap.\nWe refer the readers to Appendix A for more details about related work."
        },
        {
            "heading": "3 The MILDSum Dataset",
            "text": "In this work, we develop MILDSum (Multilingual Indian Legal Document Summarization), a collection of 3,122 Indian court judgments in English along with their summaries in both English and Hindi, drafted by legal practitioners. Statistics of the dataset are given in Table 1. This section describes the data sources and methods applied to create MILDSum, as well as studies some important properties of the dataset."
        },
        {
            "heading": "3.1 Data Sources",
            "text": "The data for this study was primarily collected from LiveLaw, a reputed website known for its reliable coverage of court judgments in India. The site maintains one version in English1 and another in Hindi2. Both versions of LiveLaw provide articles summarizing recent case judgments pronounced in the Indian Supreme Court and High Courts. According to the site, the articles (which are considered as the summaries) are written by qualified Law practitioners, which gives us confidence about the quality of the collected data."
        },
        {
            "heading": "3.2 Data Collection and Alignment",
            "text": "We observed that the articles in the English and Hindi websites of LiveLaw do not link to the corresponding articles in the other website. Hence the primary challenge is to pair the English and Hindi articles that are summaries of the same court judgment. We now describe how we address this challenge.\nWe observed that most of the Hindi article pages contain some parts of the titles of the corresponding English articles, if not the whole, in the form of metadata. We leveraged this information to find the matching English article for a particular Hindi article. To this end, we used the Bing Search API with a specialized query \u2013 site: https://www.livelaw.in/ {English title metadata} \u2013 to get the search results from https://www.livelaw.in/ (the English site), and\n1https://www.livelaw.in/ 2https://hindi.livelaw.in/\nthen we considered the first result returned by the Bing Search API.\nTo ensure the correctness of the matching (between the Hindi and English articles), we computed the Jaccard similarity between the metadata title (obtained from the Hindi article) and the title on the first page fetched by the Bing Search API (the potentially matching English article). We then enforced these 3 conditions for accepting this match as a data-point \u2013 (i) If Jaccard similarity is greater than 0.8, then we accept the match, (ii) If Jaccard similarity is in between 0.5 and 0.8, then we go for manual checking (whether the two articles indeed summarize the same case judgment), and (iii) If Jaccard similarity is lower than 0.5, then we discard this data-point. Additionally, we require that at least one of the articles must contain a link to the original judgment (usually a PDF).\nIf the conditions stated above are satisfied, then we include the English judgment, the corresponding English article, and the corresponding Hindi article, as a data-point in MILDSum. Dataset cleaning: The case judgments we obtained are in the form of PDF documents, while the English and Hindi articles are HTML pages. We employed various standard tools / Python libraries to extract the text from the downloaded HTML and PDF files. Particularly for the PDF documents, we utilized the pdftotext tool3 that converts PDFs to plain text while preserving text alignment. More details about the text extraction process and pre-processing of the documents are given in Appendix B.1."
        },
        {
            "heading": "3.3 Quality of the dataset",
            "text": "As described earlier, the MILDSum dataset was constructed by matching English and Hindi articles summarizing the same judgment. Concerns may arise regarding the quality of this matching, as they were not cross-referenced on the websites. To address this concern, we took a random sample of 300 data-points and manually checked whether the English and Hindi summaries corresponded to the said judgment. Only one data-point was found erroneous, where the English and Hindi summaries corresponded to different judgments. Hence, we conclude that a very large majority (> 99%) of the data-points in MILDSum are correct.\n3https://github.com/jalan/pdftotext"
        },
        {
            "heading": "3.4 Dataset statistics and analysis",
            "text": "MILDSum comprises a total of 3,122 case documents and their summaries in both English and Hindi. The average document length is around 4.7K tokens, the average English summary length is around 724 tokens, and the average Hindi summary length is around 695 tokens. The Compression Ratio in MILDSum \u2013 the ratio between the length of the English summaries to that of the full documents \u2013 is 1:6. The document length distributions of judgments and summaries are reported in Appendix B.2.\nWe observed that the LiveLaw articles (which are the reference summaries in MILDSum) are a mixture of extractive and abstractive summaries \u2013 they comprise of both verbatim quotations from the original case judgments (extractive parts) as well as paragraphs written in a simplified and condensed manner (abstractive parts). To quantify this extract/abstract-iveness of the summaries, we computed Extractive Fragment Coverage and Extractive Fragment Density as defined by Grusky et al. (2018). We get a high value of 0.90 for Coverage, which measures the percentage of words in the summary that are part of an extractive fragment from the document. We also observe a high value of 24.42 for Density, which quantifies how well the word sequence of the summary can be described as a series of extractions. These high values are expected as the summaries in our dataset frequently contain sentences directly quoted from the original judgments."
        },
        {
            "heading": "4 Experiments and Results",
            "text": "We divided the MILDSum dataset into 3 parts in a 70:15:15 ratio \u2013 train set (2185 data points), validation set (469 data points), and test set (468 data points). Only the test split is used to benchmark the performance of several state-of-the-art summarization models. We consider two broad approaches \u2013 (i) a summarize-then-translate pipeline approach, and (ii) a direct cross-lingual summarization approach \u2013 that are detailed next."
        },
        {
            "heading": "4.1 Summarize-then-Translate approach",
            "text": "In this pipeline approach, we first use a summarization model to generate an English summary from a given English case judgment, and then we translate the English summary to Hindi. In the first stage, we compare the performances of the following summarization models.\n\u2022 Unsupervised Extractive methods: We use LexRank (Erkan and Radev, 2004) \u2013 a graphbased method that uses eigenvector centrality to score and summarize the document sentences; LSA-based summarizer (Yeh et al., 2005) that uses Singular Value Decomposition to project the singular matrix from a higher dimensional plane to a lower dimensional plane to rank the important sentences in the document; and Luhnsummarizer (Nenkova et al., 2011) which is a frequency-based method that uses TF-IDF vectors to rank the sentences in a document.4 \u2022 Supervised Extractive models: These models treat summarization as a binary classification task (whether to include a sentence in the summary), where sentence representations are learned using a hierarchical encoder. We use SummaRuNNer5 (Nallapati et al., 2017) that uses two-layer bi-directional GRU-RNN. The first layer learns contextualized word representations which are then average-pooled to obtain sentence representations from the input document. Second, we use BERTSumExt6 (Liu and Lapata, 2019) which takes pre-trained BERT (Devlin et al., 2019) as the sentence encoder and an additional Transformer as the document encoder.7 Details of training these models on the training split of MILDSum are given in Appendix C.1.\n\u2022 Pretrained Abstractive models: We use LegalPegasus8, a fine-tuned version of Pegasus (Zhang et al., 2019) model, that is specifically designed for summarization in the legal domain by finetuning over the sec-litigation-releases dataset consisting of 2,700 US litigation releases & complaints. We also use two Long Document Summarizers. First, we use LongT5 (Guo et al., 2022), an enhanced version of the T5 (Raffel et al., 2019) model with Local and Transient Global attention mechanism than can handle long inputs up to 16,384 tokens. Next, we use LED (Longformer Encoder Decoder) (Beltagy et al., 2020), a Longformer variant with an attention\n4We used the implementations of these unsupervised methods from https://github.com/miso-belica/sumy.\n5https://github.com/hpzhao/SummaRuNNer 6https://github.com/nlpyang/PreSumm 7In the original BERTSumExt, there is a post-processing step called Trigram Blocking that excludes a candidate sentence if it has a significant amount of trigram overlap with the already generated summary to minimize redundancy in the summary. But, we observed that this step leads to too short summaries, as also observed by Sotudeh et al. (2020). Hence, we ignore this step in our work.\n8https://huggingface.co/nsi319/legal-pegasus\nmechanism that scales linearly with sequence length, enabling the model to perform seq-to-seq tasks over long documents containing thousands of tokens.\nTranslation of Generated summaries: In the second stage of this pipeline approach, all the generated English summaries are translated into Hindi using the Machine Translation (MT) model IndicTrans (Ramesh et al., 2022) which has been specifically trained for translation between English and Indian languages. We compared several MT models over a sample of our dataset, and observed IndicTrans to perform the best. More details on the translation experiments and the choice of IndicTrans are given in Appendix C.2."
        },
        {
            "heading": "4.2 Direct Cross-Lingual Summarization",
            "text": "In this approach, we directly generate a Hindi summary for a given English document (without needing any translation). To this end, we use the state-of-the-art cross-lingual summarization model CrossSum-mT5 (Bhattacharjee et al., 2023) which is a fine-tuned version of mT5 (Xue et al., 2021). We specifically use the version that is fine-tuned over all cross-lingual pairs of the CrossSum dataset, where target summary is in Hindi.9 In other words, this model is meant for summarizing text written in any language to Hindi."
        },
        {
            "heading": "4.3 Experimental setup",
            "text": "Chunking of long documents: Abstractive models like Legal-Pegasus and CrossSum-mT5 have an input capacity of 1024 tokens and 512 tokens respectively; hence a case judgement often cannot be input fully into such a model. So, to deal with the lengthy legal documents, we chunked the documents into m small chunks, where the size of each chunk is the maximum number of tokens (say, n) that the model is designed to accept without truncating (e.g., n = 512 for CrossSum-mT5). Then, we asked the model to provide a summary of k tokens for each chunk, and append the chunk-wise summaries in the same order in which the chunks appear in the document, such that the combined summary (of total m \u2217 k tokens) is almost equal in length to the reference summary.\nLong document summarizers such as LED and LongT5 have the input capacity of 16,384 tokens; hence, these models successfully handled almost\n9https://huggingface.co/csebuetnlp/mT5_m2o_hi ndi_crossSum\nall (\u223c96%) the case judgments in MILDSum without truncation or chunking. The few documents that longer were truncated at 16,384 tokens. Fine-tuning of abstractive models: We finetune the abstractive models Legal-Pegasus, LongT5, LED, and CrossSum-mT5 using the train and validation split of MILDSum. The method for generating finetuning data is explained in Appendix C.3. Hyperparameters: For all models, we have used the default hyperparameters, since we wanted to benchmark their off-the-shelf performances over our dataset. We used a default seed value of 42 to initialize the model parameters before finetuning/training, to ensure that the same results can be reproduced with the same settings.10 Evaluation metrics: To evaluate the quality of the summaries generated by the models, we considered these standard metrics \u2013 ROUGE (Lin, 2004), and BERTScore (Zhang et al., 2020). Specifically, we reported the F1 scores of ROUGE-2, ROUGE-L, and BERTScore. More details on how the metrics are computed are reported in Appendix C.4."
        },
        {
            "heading": "4.4 Main Results",
            "text": "Table 2 reports the performance of all methods stated above, where all metric values are averaged over the test split of MILDSum. We see that the extractive method SummaRunner achieved the highest ROUGE scores, closely followed by LegalPegasus-finetuned. This is possibly because the reference summaries in MILDSum have a mix of extractive and abstractive features, and they frequently quote exact lines from the original judg-\n10The experiments were carried out on a system having a NVIDIA V100 16GB GPU. The total GPU time needed for all fine-tuning experiments was approximately 48 hours.\nments (as stated earlier in Section 3.4). However, the abstractive long document summarizer methods (LongT5-finetuned and LED-finetuned) perform slightly better than extractive methods in terms of BERTScore.\nThe direct cross-lingual summarizer, CrossSumm, performed poorly when used off-the-shelf (without fine-tuning). But, CrossSumm-finetuned (fine-tuned over the train split of our MILDSum corpus) outperforms the off-the-shelf CrossSumm by a significant margin. This clearly shows the significance of our MILDSum corpus in direct crosslingual summarization.\nOverall, based on our experiments, the \u2018Summarize-then-Translate pipeline approach\u2019 performs better than the \u2018Direct Cross-Lingual Summarization\u2019 over MILDSum. Note that, in the pipeline approach, the translation stage usually introduces some additional errors, as shown by the consistently lower scores for Hindi summaries than the corresponding English summaries. In spite of the additional translation errors, the Summarizethen-Translate approach achieves higher scores over the MILDSum dataset. This result shows the need for improved cross-lingual summarization models for the legal domain in future."
        },
        {
            "heading": "5 Conclusion",
            "text": "This study develops the first multi- and crosslingual summarization dataset for Indian languages in the legal domain (available at https://gith ub.com/Law-AI/MILDSum). We also benchmark a variety of summarization models on our dataset. Our findings emphasize the need for better crosslingual summarization models in the Indian legal domain.\nLimitations\nOne limitation of the MILDSum corpus is that it is developed entirely from one source (the LiveLaw site). However, sources that provide cross-lingual summaries in the Indian legal domain (i.e., English judgments and corresponding summaries in some Indian language) are very rare. Also note that our dataset covers a wide range of cases from diverse courts such as the Supreme Court of India, and High Courts in Delhi, Bombay, Calcutta, etc. Also, the different articles (summaries) were written by different Law practitioners according to the Livelaw website. Therefore, we believe that the dataset is quite diverse with respect to topics as well as in writers of the gold standard summaries.\nAlso, a good way of evaluating model-generated legal summaries is through domain experts. While an expert evaluation has not been carried out on the MILDSum dataset till date, we plan to conduct such evaluation of the summaries generated by different methods as future work.\nAnother limitation of the dataset is the lack of summaries in Indian languages other than Hindi. A practically important future work is to enhance the dataset with summaries in other Indian languages.\nEthics Statement\nAs stated earlier, the dataset used in this work is constructed from the LiveLaw website, using content that is publicly available on the Web. According to the terms and conditions outlined on this website, content such as judgments, orders, laws, regulations, or articles can be copied and downloaded for personal and non-commercial use. We believe our use of this data adheres to these terms, as we are utilizing the content (mainly the judgments and the articles from which the summaries are obtained) for non-commercial research purposes. Note that the MILDSum dataset created in this work is meant to be used only for non-commercial purposes such as academic research."
        },
        {
            "heading": "A Details of Related work",
            "text": "Datasets for summarization in Indian languages: Some recent work on cross-lingual summarization has been done by Bhattacharjee et al. (2023) who developed a large-scale cross-lingual summarization dataset CrossSum comprising 1.7M articlesummary samples in more than 1.5K languagepairs. Another dataset WikiLingua was provided by Ladhak et al. (2020), which comprises of WikiHow articles and their summaries in multiple languages, including a substantial collection of over 9K document-summary pairs in Hindi. However, note that the summaries in this dataset are relatively short, with an average length of only 39 tokens. This characteristic implies that the WikiLingua dataset is not ideal for training legal summarization systems, since legal summarization often necessitates longer summaries that encompass intricate legal concepts.\nNotably, none of the datasets stated above are focused on cross-lingual summarization in the legal domain. This work is the first to develop a dataset for cross-lingual summarization of legal text in Indian languages. Datasets for cross-lingual summarization in nonIndian languages: In the case of non-Indian languages, Scialom et al. (2020) proposed a dataset called MLSUM, which is compiled from online news sources. It comprises more than 1.5M pairs of articles and summaries in 5 languages: French, German, Spanish, Russian, and Turkish. Recently, Wang et al. (2022) released a benchmark dataset called CLIDSUM for Cross-Lingual Dialogue Summarization.\nIn the context of legal text summarization, Aumiller et al. (2022) recently introduced Eur-Lex, a multi- and cross-lingual corpus of legal texts and human-written summaries from the European Union (EU). This dataset covers all 24 official languages of the EU. To our knowledge, no such effort has been made in the Indian legal domain; we are taking the first step in bridging this gap."
        },
        {
            "heading": "B Additional details about the MILDSum dataset",
            "text": "B.1 Data pre-processing and cleaning We used the pdf2text tool for extracting text from PDFs (the original case judgements). We conducted a quality check to ensure the reliability of\nthe pdf2text tool. We randomly selected 5 pages from various documents and then manually compared the text present in the PDF with the text extracted using the pdf2text tool. We find that the tool\u2019s output closely matches the content present in the PDFs, with only minor instances of punctuation errors. Also, these judgment PDFs were of high quality, contributing to the tool\u2019s accurate performance.\nWe also observed the need for data cleaning / pre-processing while developing the MILDSum corpus. Case judgements usually contain some metadata or additional text at the beginning, which should not be part of the input for summarization. Figure 1 shows an example of the first page of a court case judgment, where the unnecessary text portions are highlighted in pink. Such text portions should be discarded and not be part of the input to a summarization model.\nTo clean this unnecessary part, we developed an algorithm that removes lines with fewer non-space characters than the average for a document, since the said lines mostly contain very few words. The algorithm proceeds as follows: compute each document\u2019s average number of non-space characters per line. Then, iterate through each line, deleting those lines with character counts below the average, unless the previous line was included. This ensures that the last line of each paragraph is retained. Using this algorithm, we discard the metadata and retain the content of the main order/judgment from the judgment PDFs.\nB.2 Document length distributions of judgments and summaries in MILDSum\nThis section describes the document length distributions (with respect to the number of words contained in the documents) of the case judgments and summaries in MILDSum. A document length distribution graph visually represents the number of documents in a dataset that contain a certain number of words. Each point on the graph represents a bin of document lengths (in terms of number of words) and the number of documents in that bin. The shape of the graph can vary depending on the dataset\u2019s characteristics.\nFigure 2 shows the distribution of lengths of the case judgments in the MILDSum dataset. Similarly, Figure 3 and Figure 4 show the document length distributions of the English and Hindi summaries respectively. For our MILDSum corpus, the dis-\ntribution for court judgements (Figure 2) seems to follow an Inverse-Chi-Squared distribution, while for both English and Hindi summaries, the distributions seem to follow bell-shaped positively skewed distribution curves."
        },
        {
            "heading": "C More details about the experiments",
            "text": "C.1 Training supervised extractive summarization models\nThe supervised extractive summarization models SummaRuNNer and BERTSumExt have been trained over the training split of the MILDSum dataset. These supervised methods require labeled data for training, where every sentence in the document must be labeled as 1 if this sentence is suitable for inclusion in the summary, and labeled as 0 otherwise. Hence, we convert the reference English summaries of MILDSum to purely extractive form to train these methods. To this end, we adopt the technique mentioned by Nallapati et al. (2017). Briefly, we assign label 1 (suitable for inclusion in the extractive summary) to those sentences from the full document that greedily maximize the ROUGE2 (Lin, 2004) overlap with the human-written reference summary. The rest of the sentences in the full document are assigned label 0.\nC.2 Selection of translation model for translating English summaries to Hindi\nFor the second stage of the pipeline approach (Summarize-then-Translate), we tried out several state-of-the-art Machine Translation (MT) systems for English-to-Hindi translation, such as Google Cloud Translator11, Microsoft Azure Translator12, IndicTrans13 (Ramesh et al., 2022), mBART-5014 (Tang et al., 2020), NLLB15 (team et al., 2022), and OPUS16 (Tiedemann and Thottingal, 2020). To compare among these MT models, we randomly selected 100 data-points from MILDSum, and then used these MT models for Englishto-Hindi translation of the reference English summaries. We used the standard MT evaluation metric BLEU (Papineni et al., 2002) for comparing the quality of the translations, by matching a machinetranslated Hindi summary with the reference Hindi summary for the same case judgment.\nAmong these MT systems, IndicTrans exhibited the best performance with a BLEU score of 50.34 (averaged over the 100 data-points randomly selected for this evaluation). The scores for other models are as follows \u2013 MicrosoftTrans: 47.10, GoogleTrans: 43.26, NLLB: 42.56, mBART-50: 33.60, and OPUS: 10.65. Also, IndicTrans was found to be the best for English-to-Hindi translation in the prior work (Ramesh et al., 2022). For these reasons, we used IndicTrans for translation of the English summaries to Hindi.\nC.3 Generating fine-tuning data for abstractive summarization models\nThis section describes how we created fine-tuning data for the abstractive models Legal-Pegasus and CrossSum-mT5. Fine-tuning of these models requires data in the form of document chunks and the corresponding summaries of the chunks. For this experiment, we fixed the chunk length to 512 tokens. As we are chunking a case judgment, we need to chunk the corresponding reference summary as well to match the context of the judgment\u2019s\n11https://cloud.google.com/translate/docs/samp les/translate-v3-translate-text\n12https://azure.microsoft.com/en-us/products/c ognitive-services/translator\n13https://github.com/AI4Bharat/indicTrans 14https://huggingface.co/facebook/mbart-large -50-one-to-many-mmt 15https://huggingface.co/facebook/nllb-200-3.3\nB 16https://huggingface.co/Helsinki-NLP/opus-m t-en-mul\nchunks. In order to generate parallel pairs (document chunk and corresponding summary chunk), we followed the method of Reimers and Gurevych (2020) i.e., bitext mining. We generate embeddings using LaBSE (Feng et al., 2022). Then, we use the following scoring function to compute a score for each pair of embeddings; we set the threshold to 1 for including a given pair in the fine-tuning dataset.\nscore(x, y) = margin { cos(x, y),\n\u2211 z\u03f5NNk(x) cos(x, z) 2k + \u2211 z\u03f5NNk(y) cos(y, z) 2k } Where x and y are the embeddings of the judgment chunk and summary chunk respectively. NNk(x) denotes the k nearest neighbors of x. We have used ratio-based margin as the margin function, i.e., margin(a, b) = a/b.\nC.4 Evaluation metrics for summarization performance\nThe following standard metrics were used to evaluate the quality of the model-generated summaries, by comparing with the reference summaries.\n\u2022 ROUGE (Lin, 2004) stands for Recall-Oriented Understudy for Gisting Evaluation. ROUGE2 measures the textual overlap (bi-grams) between the model-generated summaries and the reference summaries. ROUGE-L measures the longest matching sequence of words using the Longest Common Subsequence (LCS).\nTo calculate multilingual ROUGE scores (English and Hindi in this work), we used the multilingual_rouge_scoring library17 (provided by Hasan et al. (2021)) which uses the OpenNMT tokenizer18.\n\u2022 BERTScore (Zhang et al., 2020) uses BERT to compute the similarity scores between the tokenlevel embeddings of the model-generated and reference summaries. It is known to correlate better with human judgments as it considers the semantic part.\nTo calculate multilingual BERTScore, we used the Huggingface evaluate library19 that incorporates the official BERTScore project20.\n17https://github.com/csebuetnlp/xl-sum 18https://opennmt.net/ 19https://huggingface.co/spaces/evaluate-metri\nc/bertscore 20https://github.com/Tiiiger/bert_score"
        }
    ],
    "title": "MILDSum: A Novel Benchmark Dataset for Multilingual Summarization of Indian Legal Case Judgments",
    "year": 2023
}