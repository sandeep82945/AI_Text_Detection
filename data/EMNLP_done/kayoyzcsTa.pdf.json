{
    "abstractText": "Claim span identification (CSI) is an important step in fact-checking pipelines, aiming to identify text segments that contain a checkworthy claim or assertion in a social media post. Despite its importance to journalists and human fact-checkers, it remains a severely understudied problem, and the scarce research on this topic so far has only focused on English. Here we aim to bridge this gap by creating a novel dataset, X-CLAIM, consisting of 7K realworld claims collected from numerous social media platforms in five Indian languages and English. We report strong baselines with stateof-the-art encoder-only language models (e.g., XLM-R) and we demonstrate the benefits of training on multiple languages over alternative cross-lingual transfer methods such as zeroshot transfer, or training on translated data, from a high-resource language such as English. We evaluate generative large language models from the GPT series using prompting methods on the X-CLAIM dataset and we find that they underperform the smaller encoder-only language models for low-resource languages.1",
    "authors": [
        {
            "affiliations": [],
            "name": "Shubham Mittal\u03b1"
        },
        {
            "affiliations": [],
            "name": "Megha Sundriyal\u03b1"
        },
        {
            "affiliations": [],
            "name": "Preslav Nakov\u03b1"
        },
        {
            "affiliations": [],
            "name": "Mohammed Bin Zayed"
        }
    ],
    "id": "SP:db3952d8b4103226993f5fd30f92d04006171710",
    "references": [
        {
            "authors": [
                "Hunt Allcott",
                "Matthew Gentzkow."
            ],
            "title": "Social media and fake news in the 2016 election",
            "venue": "Journal of economic perspectives, 31(2):211\u2013236.",
            "year": 2017
        },
        {
            "authors": [
                "Satanjeev Banerjee",
                "Alon Lavie."
            ],
            "title": "METEOR: An automatic metric for MT evaluation with improved correlation with human judgments",
            "venue": "Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Transla-",
            "year": 2005
        },
        {
            "authors": [
                "Alberto Barr\u00f3n-Cede\u00f1o",
                "Firoj Alam",
                "Tommaso Caselli",
                "Giovanni Da San Martino",
                "Tamer Elsayed",
                "Andrea Galassi",
                "Fatima Haouari",
                "Federico Ruggeri",
                "Julia Maria Stru\u00df",
                "Rabindra Nath Nandi"
            ],
            "title": "The clef-2023 checkthat! lab: Checkworthiness",
            "year": 2023
        },
        {
            "authors": [
                "Marco T Bastos",
                "Dan Mercea."
            ],
            "title": "The brexit botnet and user-generated hyperpartisan news",
            "venue": "Social science computer review, 37(1):38\u201354.",
            "year": 2019
        },
        {
            "authors": [
                "Tuhin Chakrabarty",
                "Christopher Hidey",
                "Kathy McKeown."
            ],
            "title": "IMHO fine-tuning improves claim detection",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech-",
            "year": 2019
        },
        {
            "authors": [
                "Alexis Conneau",
                "Kartikay Khandelwal",
                "Naman Goyal",
                "Vishrav Chaudhary",
                "Guillaume Wenzek",
                "Francisco Guzman",
                "Edouard Grave",
                "Myle Ott",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Unsupervised cross-lingual representation learning at scale",
            "venue": "In",
            "year": 2020
        },
        {
            "authors": [
                "Giovanni Da San Martino",
                "Seunghak Yu",
                "Alberto Barr\u00f3n-Cede\u00f1o",
                "Rostislav Petrov",
                "Preslav Nakov."
            ],
            "title": "Fine-grained analysis of propaganda in news article",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Pro-",
            "year": 2019
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association",
            "year": 2019
        },
        {
            "authors": [
                "Hernawan Dewatana",
                "Siti Ummu Adillah."
            ],
            "title": "The effectiveness of criminal eradication on hoax information and fake news",
            "venue": "Law Development Journal, 3(3):513\u2013520.",
            "year": 2021
        },
        {
            "authors": [
                "Daxiang Dong",
                "Hua Wu",
                "Wei He",
                "Dianhai Yu",
                "Haifeng Wang."
            ],
            "title": "Multi-task learning for multiple language translation",
            "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint",
            "year": 2015
        },
        {
            "authors": [
                "Zi-Yi Dou",
                "Graham Neubig."
            ],
            "title": "Word alignment by fine-tuning embeddings on parallel corpora",
            "venue": "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 2112\u20132128.",
            "year": 2021
        },
        {
            "authors": [
                "Tamer Elsayed",
                "Preslav Nakov",
                "Alberto Barr\u00f3nCedeno",
                "Maram Hasanain",
                "Reem Suwaileh",
                "Giovanni Da San Martino",
                "Pepa Atanasova."
            ],
            "title": "Overview of the clef-2019 checkthat! lab: automatic identification and verification of claims",
            "venue": "Ex-",
            "year": 2019
        },
        {
            "authors": [
                "Revanth Gangi Reddy",
                "Sai Chetan Chinthakindi",
                "Yi R. Fung",
                "Kevin Small",
                "Heng Ji."
            ],
            "title": "A zero-shot claim detection framework using question answering",
            "venue": "Proceedings of the 29th International Conference on Computational Linguistics, pages 6927\u2013",
            "year": 2022
        },
        {
            "authors": [
                "Revanth Gangi Reddy",
                "Sai Chetan Chinthakindi",
                "Zhenhailong Wang",
                "Yi Fung",
                "Kathryn Conger",
                "Ahmed ELsayed",
                "Martha Palmer",
                "Preslav Nakov",
                "Eduard Hovy",
                "Kevin Small",
                "Heng Ji"
            ],
            "title": "NewsClaims: A new benchmark for claim detection",
            "year": 2022
        },
        {
            "authors": [
                "Ashim Gupta",
                "Vivek Srikumar."
            ],
            "title": "X-fact: A new benchmark dataset for multilingual fact checking",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natu-",
            "year": 2021
        },
        {
            "authors": [
                "Shreya Gupta",
                "Parantak Singh",
                "Megha Sundriyal",
                "Md. Shad Akhtar",
                "Tanmoy Chakraborty."
            ],
            "title": "LESA: Linguistic encapsulation and semantic amalgamation based generalised claim detection from online content",
            "venue": "Proceedings of the 16th Conference",
            "year": 2021
        },
        {
            "authors": [
                "Pengcheng He",
                "Jianfeng Gao",
                "Weizhu Chen."
            ],
            "title": "DeBERTav3: Improving deBERTa using ELECTRA-style pre-training with gradientdisentangled embedding sharing",
            "venue": "The Eleventh International Conference on Learning Representa-",
            "year": 2023
        },
        {
            "authors": [
                "Israa Jaradat",
                "Pepa Gencheva",
                "Alberto Barr\u00f3n-Cede\u00f1o",
                "Llu\u00eds M\u00e0rquez",
                "Preslav Nakov."
            ],
            "title": "ClaimRank: Detecting check-worthy claims in Arabic and English",
            "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for",
            "year": 2018
        },
        {
            "authors": [
                "Diederik P. Kingma",
                "Jimmy Ba."
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings.",
            "year": 2015
        },
        {
            "authors": [
                "Keshav Kolluru",
                "Muqeeth Mohammed",
                "Shubham Mittal",
                "Soumen Chakrabarti",
                "Mausam ."
            ],
            "title": "Alignment-augmented consistent translation for multilingual open information extraction",
            "venue": "Proceedings of the 60th Annual Meeting of the Association",
            "year": 2022
        },
        {
            "authors": [
                "Ran Levy",
                "Yonatan Bilu",
                "Daniel Hershcovich",
                "Ehud Aharoni",
                "Noam Slonim."
            ],
            "title": "Context dependent claim detection",
            "venue": "Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 1489\u2013",
            "year": 2014
        },
        {
            "authors": [
                "Ran Levy",
                "Shai Gretz",
                "Benjamin Sznajder",
                "Shay Hummel",
                "Ranit Aharonov",
                "Noam Slonim."
            ],
            "title": "Unsupervised corpus\u2013wide claim detection",
            "venue": "Proceedings of the 4th Workshop on Argument Mining, pages 79\u201384, Copenhagen, Denmark. Association",
            "year": 2017
        },
        {
            "authors": [
                "Chin-Yew Lin."
            ],
            "title": "ROUGE: A package for automatic evaluation of summaries",
            "venue": "Text Summarization Branches Out, pages 74\u201381, Barcelona, Spain. Association for Computational Linguistics.",
            "year": 2004
        },
        {
            "authors": [
                "Marco Lippi",
                "Paolo Torroni."
            ],
            "title": "Contextindependent claim detection for argumentmining",
            "venue": "Twenty-Fourth International Joint Conference on Artificial Intelligence, pages 185\u2013191.",
            "year": 2015
        },
        {
            "authors": [
                "Yang Liu",
                "Mirella Lapata."
            ],
            "title": "Text summarization with pretrained encoders",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing",
            "year": 2019
        },
        {
            "authors": [
                "Jing Ma",
                "Wei Gao",
                "Shafiq Joty",
                "Kam-Fai Wong."
            ],
            "title": "Sentence-level evidence embedding for claim verification with hierarchical attention networks",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2561\u2013",
            "year": 2019
        },
        {
            "authors": [
                "ShubhamMittal",
                "Keshav Kolluru",
                "Soumen Chakrabarti",
                "andMausam"
            ],
            "title": "mOKB6: AMultilingual Open Knowledge Base Completion Benchmark",
            "venue": "In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics",
            "year": 2023
        },
        {
            "authors": [
                "Shubham Mittal",
                "Preslav Nakov."
            ],
            "title": "IITD at WANLP 2022 shared task: Multilingual multigranularity network for propaganda detection",
            "venue": "Proceedings of the The Seventh Arabic Natural Language Processing Workshop (WANLP), pages 529\u2013",
            "year": 2022
        },
        {
            "authors": [
                "Preslav Nakov",
                "Alberto Barr\u00f3n-Cede\u00f1o",
                "Giovanni da San Martino",
                "Firoj Alam",
                "Julia Maria Stru\u00df",
                "Thomas Mandl",
                "Rub\u00e9n M\u00edguez",
                "Tommaso Caselli",
                "Mucahid Kutlu",
                "Wajdi Zaghouani"
            ],
            "title": "Overview of the clef\u20132022 checkthat! lab on fighting",
            "year": 2022
        },
        {
            "authors": [
                "Preslav Nakov",
                "Alberto Barr\u00f3n-Cedeno",
                "Tamer Elsayed",
                "Reem Suwaileh",
                "Llu\u00eds M\u00e0rquez",
                "Wajdi Zaghouani",
                "Pepa Atanasova",
                "Spas Kyuchukov",
                "Giovanni Da San Martino"
            ],
            "title": "Overview of the clef-2018 checkthat! lab on automatic identification",
            "year": 2018
        },
        {
            "authors": [
                "Preslav Nakov",
                "David Corney",
                "Maram Hasanain",
                "Firoj Alam",
                "Tamer Elsayed",
                "Alberto Barr\u00f3nCede\u00f1o",
                "Paolo Papotti",
                "Shaden Shaar",
                "Giovanni Da San Martino."
            ],
            "title": "Automated fact-checking for assisting human fact-checkers",
            "venue": "Proceedings",
            "year": 2021
        },
        {
            "authors": [
                "PengQi",
                "Yuhao Zhang",
                "Yuhui Zhang",
                "JasonBolton",
                "Christopher D. Manning."
            ],
            "title": "Stanza: A Python natural language processing toolkit for many human languages",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics:",
            "year": 2020
        },
        {
            "authors": [
                "Shaden Shaar",
                "Nikola Georgiev",
                "Firoj Alam",
                "Giovanni Da San Martino",
                "Aisha Mohamed",
                "Preslav Nakov."
            ],
            "title": "Assisting the human fact-checkers: Detecting all previously fact-checked claims in a document",
            "venue": "Findings of the Association for Computa-",
            "year": 2022
        },
        {
            "authors": [
                "Shaden Shaar",
                "Alex Nikolov",
                "Nikolay Babulkov",
                "Firoj Alam",
                "Alberto Barr\u00f3n-Cedeno",
                "Tamer Elsayed",
                "Maram Hasanain",
                "Reem Suwaileh",
                "Fatima Haouari",
                "Giovanni Da San Martino"
            ],
            "title": "english: Automatic identification",
            "venue": "Overview of checkthat!",
            "year": 2020
        },
        {
            "authors": [
                "Amir Soleimani",
                "Christof Monz",
                "Marcel Worring."
            ],
            "title": "Bert for evidence retrieval and claim verification",
            "venue": "Advances in Information Retrieval, pages 359\u2013366, Cham. Springer International Publishing.",
            "year": 2020
        },
        {
            "authors": [
                "Megha Sundriyal",
                "Atharva Kulkarni",
                "Vaibhav Pulastya",
                "Md. Shad Akhtar",
                "Tanmoy Chakraborty."
            ],
            "title": "Empowering the fact-checkers! automatic identification of claim spans on Twitter",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Nat-",
            "year": 2022
        },
        {
            "authors": [
                "Megha Sundriyal",
                "Ganeshan Malhotra",
                "Md Shad Akhtar",
                "Shubhashis Sengupta",
                "Andrew Fano",
                "Tanmoy Chakraborty."
            ],
            "title": "Document retrieval and claim verification to mitigate COVID-19 misinformation",
            "venue": "Proceedings of the Workshop on Com-",
            "year": 2022
        },
        {
            "authors": [
                "Megha Sundriyal",
                "Parantak Singh",
                "Md. Shad Akhtar",
                "Shubhashis Sengupta",
                "Tanmoy Chakraborty."
            ],
            "title": "Desyr: Definition and syntactic representation based claim detection on the web",
            "venue": "Proceedings of the 30th ACM International Conference on",
            "year": 2021
        },
        {
            "authors": [
                "Stephen E Toulmin."
            ],
            "title": "The uses of argument",
            "venue": "Cambridge university press.",
            "year": 2003
        },
        {
            "authors": [
                "Sander van Der Linden",
                "Jon Roozenbeek",
                "Josh Compton."
            ],
            "title": "Inoculating against fake news about covid-19",
            "venue": "Frontiers in psychology, 11:566790.",
            "year": 2020
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141 ukasz Kaiser",
                "Illia Polosukhin."
            ],
            "title": "Attention is all you need",
            "venue": "Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc.",
            "year": 2017
        },
        {
            "authors": [
                "Andreas Vlachos",
                "Sebastian Riedel."
            ],
            "title": "Fact checking: Task definition and dataset construction",
            "venue": "Proceedings of the ACL 2014 Workshop on Language Technologies and Computational Social Science, pages 18\u201322, Baltimore, MD, USA. Associa-",
            "year": 2014
        },
        {
            "authors": [
                "Herman Wasserman",
                "Dani Madrid-Morales."
            ],
            "title": "An exploratory study of \u201cfake news\u201d and media trust in kenya, nigeria and south africa",
            "venue": "African Journalism Studies, 40(1):107\u2013123.",
            "year": 2019
        },
        {
            "authors": [
                "Dustin Wright",
                "Isabelle Augenstein."
            ],
            "title": "Claim check-worthiness detection as positive unlabelled learning",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 476\u2013488, Online. Association for Computational Linguistics.",
            "year": 2020
        },
        {
            "authors": [
                "Shijie Wu",
                "Mark Dredze."
            ],
            "title": "Beto, bentz, becas: The surprising cross-lingual effectiveness of BERT",
            "venue": "Proceedings of the 2019 Conference on",
            "year": 2019
        },
        {
            "authors": [
                "Amelie W\u00fchrl",
                "Roman Klinger."
            ],
            "title": "Claim detection in biomedical Twitter posts",
            "venue": "Proceedings of the 20th Workshop on Biomedical Language Processing, pages 131\u2013142, Online. Association for Computational Linguistics.",
            "year": 2021
        },
        {
            "authors": [
                "Mahsa Yarmohammadi",
                "Shijie Wu",
                "Marc Marone",
                "Haoran Xu",
                "Seth Ebner",
                "Guanghui Qin",
                "Yunmo Chen",
                "Jialiang Guo",
                "Craig Harman",
                "Kenton Murray",
                "Aaron Steven White",
                "Mark Dredze",
                "Benjamin Van Durme"
            ],
            "title": "Everything is all it takes: A",
            "year": 2021
        },
        {
            "authors": [
                "Tianyi Zhang",
                "Varsha Kishore",
                "Felix Wu",
                "Kilian Q. Weinberger",
                "YoavArtzi."
            ],
            "title": "Bertscore: Evaluating text generation with bert",
            "venue": "International Conference on Learning Representations.",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Social media platforms have become a prominent hub for connecting people worldwide. Along with the myriad benefits of this connectivity, e.g., the ability to share information instantaneously with a large audience, the spread of inaccurate and misleading information has emerged as a major problem (Allcott and Gentzkow, 2017). Misinformation spread via social media has far-reaching consequences, including the potential to sow chaos, to foster hatred, to manipulate public opinion, and to disturb societal stability (Wasserman and MadridMorales, 2019; Dewatana and Adillah, 2021).\n\u2020Major part of this work was done during a research internship at MBZUAI.\n1We release our X-CLAIM dataset and code at https://github.com/mbzuai-nlp/x-claim\nClaims play an integral role in propagating fake news and misinformation, serving as the building blocks upon which these deceptive narratives are formed. In their Argumentation Theory, Toulmin (2003) described a claim as \u201ca statement that asserts something as true or valid, often without providing sufficient evidence for verification.\u201d Such intentional or unintentional claims quickly gain traction over social media platforms, resulting in rapid dissemination of misinformation as was seen during recent events such as the COVID19 pandemic (van Der Linden et al., 2020) and Brexit (Bastos and Mercea, 2019). To mitigate the detrimental impact of false claims, numerous factchecking initiatives, such as PolitiFact and Snopes, dedicate substantial efforts to fact-checking claims made by public figures, organizations, and social media users. However, due to the time-intensive nature of this process, many misleading claims dodge verification and remain unaddressed. To address this, computational linguistic approaches have been developed that can assist human factcheckers (Vlachos and Riedel, 2014; Nakov et al., 2018; Shaar et al., 2020; Gupta and Srikumar, 2021; Nakov et al., 2021a; Shaar et al., 2022).\nRecently, Sundriyal et al. (2022a) introduced the task of claim span identification (CSI), where the goal is to identify textual segments that contain claims or assertions made within the social media posts. The CSI task serves as a precursor to various downstream tasks such as claim verification and check-worthiness estimation.\nWhile efforts have been made in combating misinformation in different languages (Jaradat et al., 2018; Barr\u00f3n-Cede\u00f1o et al., 2023), research in identifying the claim spans has so far been limited to English. Previously, Sundriyal et al. (2022a) have manually extracted COVID-19 claim spans from Twitter in English. However, the landscape of fraudulent claims goes beyond COVID-19 and Twitter. In this work, we aim to bridge these gaps by studying the task of multilingual claim span identification (mCSI) across numerous social media platforms and multiple languages. To the best of our knowledge, this is the first attempt towards identifying the claim spans in a language different from English.\nWe design the first data curation pipeline for the task of mCSI, which, unlike Sundriyal et al. (2022a), does not require manual annotation to create the training data. We collect data from various fact-checking sites and we automatically annotate the claim spans within the post. Using the pipeline, we create a novel dataset, named X-CLAIM, containing 7K real-world claims from numerous social media platforms in six languages: English, Hindi, Punjabi, Tamil, Telugu, and Bengali. Figure 1 showcases a few examples from our dataset.\nWe report strong baselines for the mCSI task with state-of-the-art multilingual models. We find that joint training across languages improves the model performance when compared to alternative cross-lingual transfer methods like zero-shot transfer, or training on translated data, from a highresource language like English. In this work, we make the following contributions:\n\u2022 We introduce the first automated data annotation and curation pipeline for the mCSI task.\n\u2022 We create a novel dataset, named X-CLAIM, for the mCSI task in six languages.\n\u2022 We experiment with multiple state-of-the-art encoder-only language models and the generative large language models to achieve high performance on the proposed task."
        },
        {
            "heading": "2 Related Work",
            "text": "Efforts to combat misinformation and fake news have focused on claims in various sources. The existing body of work in this area can be broadly categorized into the following major groups: claim detection (Chakrabarty et al., 2019; Gupta et al., 2021; W\u00fchrl and Klinger, 2021; Gangi Reddy et al., 2022a,b), claim check-worthiness (Jaradat et al., 2018; Wright and Augenstein, 2020), claim span identification (Sundriyal et al., 2022a), and claim verification (Ma et al., 2019; Soleimani et al., 2020). Being the precursor of several other downstream tasks, claim detection has garnered significant attention. Various methods have been proposed to tackle claim detection, aiming to identify statements that may contain claims (Lippi and Torroni, 2015; Levy et al., 2017; Gangi Reddy et al., 2022b). In response to the escalating issue of false claims on social media, there has been a surge in the development of claim detection systems specifically designed to handle text from social media platforms (Chakrabarty et al., 2019; Gupta et al., 2021; Sundriyal et al., 2021). Recently, Sundriyal et al. (2022a) introduced the task of claim span identification where the system needs to label the claim-containing textual segments from social media posts, making claim detection systems more explainable through this task.\nWhile most existing methods to combat fake news are primarily tailored for English (Levy et al., 2014; Lippi and Torroni, 2015; Sundriyal et al., 2021, 2022b), in recent times, there has been a surge in interest regarding the advancement of factchecking techniques for various other languages. ClaimRank (Jaradat et al., 2018) introduced an online system to identify sentences containing checkworthy claims in Arabic and English. The CheckThat! Lab has organized several multilingual claim tasks over the past five years, progressively expanding language support and garnering an increasing number of submissions (Nakov et al., 2018; Elsayed et al., 2019; Shaar et al., 2020; Nakov et al., 2021b, 2022). In their latest edition, Barr\u00f3nCede\u00f1o et al. (2023) featured factuality tasks in seven languages: English, German, Arabic, Italian, Spanish, Dutch, and Turkish. Gupta and Srikumar (2021) introduced X-FACT, a comprehensive multilingual dataset for factual verification of realworld claims in 25 languages. Unlike that work, here we focus on extracting the claim from a social media post, rather than fact-checking a claim.\nThe task of claim span identification remains unexplored due to the lack of datasets in other languages. Sundriyal et al. (2022a) developed a dataset of 7.5K manually annotated claim spans in tweets, named CURT; all the tweets and claim spans in that dataset are in English. Additionally, while there has been interest in claims in other languages, there is a notable lack of progress on Indian languages. Here, we aim to bridge this gap."
        },
        {
            "heading": "3 Dataset",
            "text": "We follow a two-step pipeline to develop our dataset: (i) data collection and (ii) automated annotation. We present a high-level overview of our proposed data creation methodology in Figure 2. Below, we explain these steps in detail."
        },
        {
            "heading": "3.1 Data Collection",
            "text": "We observe in various fact-checking websites that professional fact-checkers, while investigating a given social media post or news article, first find the claimmade in the post, which we call a normalized claim, and then they verify whether that claim is true, misleading, or false. This is the motivation for the CSI task as a precursor to fact-checking as it is a step in the fact-checking process as performed by humans. Thus, we leverage the efforts of factcheckers and we collect data from numerous factchecking websites that are recognized by the International Fact-Checking Network (IFCN).2 We aim to create a dataset comprising claims made in social media and in multiple languages, with a focus on Indian languages. We scrape data from factchecked posts in six languages: English, Hindi, Punjabi, Tamil, Telugu, and Bengali.\n2https://www.poynter.org/ifcn/\nWe highlight that we deal with low-resource languages since we found only a couple of factchecking websites that analyze social media posts in languages other than English. For each website, we scrape all the fact-checked posts3 with the help of a web scraping API.4\nThen, we collect the text of the social media post text and the normalized claim from the web page of each fact-checked post with the help of regular expressions based on the structure of the factchecking website. Finally, we use various filtering rules to remove posts that are about videos, Instagram reels, or when their text is too short or excessively long. These rules help us to collect only the social media posts with a text modality. We provide more details about the process of data collection in Appendix A."
        },
        {
            "heading": "3.2 Automated Annotation",
            "text": "We label the claim-containing a textual segment within the social media post using the humanwritten normalized claim as a guidance from the previous step. The normalized claim can be relied on to be extremely trustworthy since it was manually written by professional fact-checkers. However, it does not have to be literally spelled out as part of the social media post. Having this normalized claim gives us a good guidance about where to look for the claim span, and we try to do this mapping automatically.\nAs shown in the bottom row in Figure 2, this step includes two substeps: sentence selection and conversion of the normalized claim to the claim span. Both substeps use modules that support multiple languages and do not require human intervention.\n3The data was scraped in May 2023. 4https://www.octoparse.com/\nFirst, we look for the most relevant sentence that encapsulates the claimmade in the post. We do this by computing a similarity score between the normalized claim and each of the post\u2019s sentences, and we select the sentence with the maximum score.\nSecond, using awesome-align (Dou andNeubig, 2021), we find the word tokens in the post sentence that align with the word tokens in the normalized claim. We then obtain the claim span as the sequence of word tokens, starting with the first alignedword token and endingwith the last aligned word token in the sentence.\nWe use Stanza (Qi et al., 2020) to perform sentence segmentation for English, Hindi, Tamil, and Telugu. For Punjabi and Bengali, we consider the complete post text as a single sentence since we did not find any publicly available sentence segmentation tools for these languages. While using awesome-align in conversion from the normalized claim to the claim span, we used the official repository of Dou and Neubig (2021). Recent works (Yarmohammadi et al., 2021; Kolluru et al., 2022) have used word-alignment to produce silver labels in the target language (like Hindi) using gold labels available in the source language (like English). Mittal et al. (2023) used word alignments from awesome-align, and then considered the longest contiguous sequence of aligned tokens in the translated text as the final projected gold labels. Taking the longest contiguous sequence is suitable for tasks where the target text, the gold labels, or both, are relatively short. However, in our mCSI task, the normalized claims and the post texts are quite long (see Table 1). Thus, we took the sequence of words from the first to the last aligned word. We found that this yielded better performance than taking the longest contiguous sequence of aligned words in the social media post.\nNote that we empirically chose the most appropriate sentence similarity measure for sentence selection, after trying a variety of similarity measures. Tasks such as machine translation (Dong et al., 2015) and text summarization (Liu and Lapata, 2019) require evaluation measures that take paraphrasing and synonyms into account while comparing the model\u2019s generated text to the gold reference text. We leverage these evaluation measures for sentence similarity. To evaluate the commonly used measures such as ROUGE (Lin, 2004), METEOR (Banerjee and Lavie, 2005) and BERTScore, we manually annotated the claim spans for 300 randomly sampled posts in the six languages. Then, we evaluated the automatically annotated claim spans when using different similarity measures against the manually annotated claim spans. The results are shown in Table 2: we can see that BERTScore-Recall yields consistently better performance for finding the annotated spans. For Punjabi and Bengali, we only used awesomealign due to the lack of a sentence segmentation module and we observed high-quality F1 scores of 81.23% and 78.6%, respectively.\nOverall, our two-step data creation methodology yields a robust, scalable, and high-quality automatically annotated data for our multilingual claim span identification task."
        },
        {
            "heading": "3.3 Evaluation Sets and Dataset Analysis",
            "text": "We created the evaluation sets with the help of linguistic experts in the six languages. We provided them with nearly 100 samples from the curated data in each language (400 in English) along with detailed annotation guidelines for the CSI task from Sundriyal et al. (2022a). We asked them to annotate the claim spans in the social media posts under the guidance of claims authored by professional fact-checkers. We created training and development splits in a ratio of 80:20 on the remaining curated data. For Telugu and Bengali, we only formed test sets as there were less examples available for these languages. Table 1 shows statistics about the dataset and the splits, and Figure 1 shows a few examples from our X-CLAIM dataset.\nTable 1 further reports the length of the post text and the claim span. As the claim spans are generally concise and do not contain extra neighboring words, we observe that the claim spans are nearly half of the text of the post for all languages."
        },
        {
            "heading": "4 Experiments",
            "text": "Evaluation Measures: Following Sundriyal et al. (2022a), we address mCSI as a sequence tagging task. For evaluation, we use three measures, computed at the span level (Da San Martino et al., 2019): Precision (P), Recall (R), and F1-score.\nModels: We use state-of-the-art transformerbased (Vaswani et al., 2017) multilingual pretrained encoder-only language models such as mBERT (Devlin et al., 2019), mDeBERTa (He et al., 2023), and XLM-RoBERTa (XLM-R) (Conneau et al., 2020). We encode each post\u2019s token with IO (Inside-Outside) tags to mark the claim spans. Other encodings such as BIO, BEO and BEIO performed worse (see Appendix C for detailed comparison of encodings). More details about the training are given in Appendix B."
        },
        {
            "heading": "5 Results",
            "text": "We carry out an exhaustive empirical investigation to answer the following research questions:\nR1. Does the model benefit from joint training with multiple languages? (Section 5.1)\nR2. Do we need training data in low-resource languages when we have abundant data in highresource languages?5 (Section 5.2)\n5We consider English to be a high-resource language.\nR3. Can large language models (LLMs) such as GPT-4 identify the claims made in multilingual social media? (Section 5.3)\nR4. How does the automatically annotated XCLAIMdataset compare to priormanually annotated datasets like CURT? (Section 5.4)"
        },
        {
            "heading": "5.1 Training on Multilingual Social Media",
            "text": "We train and compare two kinds of models: MONOLINGUAL andMULTILINGUALmodels. In a MONOLINGUAL setup, we train one model for each language using the available training data in XCLAIM dataset, whereas in a MULTILINGUAL setup, we train a single model on the training data for all languages combined. We note that there is no MONOLINGUAL model for Telugu and Bengali due to the lack of training data for these languages. However, we evaluate the MULTILINGUAL model on them as that model was trained in multiple languages. The performance of these models with different pretrained encoders is shown in Table 3.\nWe can see that the MULTILINGUAL models outperform the MONOLINGUAL models by 1.15% precision and 0.93% F1, averaged over all languages (except for Telugu and Bengali). Even though the recall gets hurt by 0.45%, the improvement in F1 suggests that the model does benefit from joint training. We posit that the drop in recall and the gain in precision indicate that the model has become more careful when identifying the claims."
        },
        {
            "heading": "5.2 Cross-lingual Transfer from English",
            "text": "We use the English training data in two experimental settings and we compare them to MULTILINGUAL models. In the first setting, we leverage the strong cross-lingual transfer capabilities of pretrained multilingual models (Wu and Dredze, 2019). We take MONOLINGUAL models for English and test them on the remaining five languages. In this setting, we have zero-shot transfer from monolingual-English models. In the second setting, which we call translate-train models, we translate the English training data to the target language and we train a model only on the translated data. To perform translation of social media posts, we use Google translate,6 and we project the claim spans (in English), or the token labels, on the translated post using our automated annotation pipeline (see Section 3.2 for detail).\n6https://translate.google.com/\nBoth the zero-shot transfer and the translatetrain models are almost consistently worse than the MULTILINGUAL models (in terms of F1) for all five languages. The translate-train models show a drop of 1.19% F1, whereas zero-shot transfer models are 2.13% F1 behind MULTILINGUAL. This offers strong evidence that the training data in lowresource languages helps over the training data in a high-resource language.\nInterestingly, we notice that zero-shot transfer models are consistently worse than translate-train ones when using mBERT and mDeBERTa, for all five languages. For instance, with mBERT, zeroshot transfer models are worse by 2.92% F1. However, with XLM-R, zero-shot transfer models are better than translate-train models by 1.15% precision and 0.64% F1. We believe that this is because XLM-R has stronger cross-lingual transfer capabilities, stemming from its larger pretraining data compared to mBERT and mDeBERTa."
        },
        {
            "heading": "5.3 Evaluating the GPT Series LLMs",
            "text": "We experiment with several large language models (LLMs):7 text-davinci-003 (T-DV3), gpt-3.5turbo (GPT-3.5) and gpt-4-0314 (GPT-4) on the mCSI task using the OpenAI API.8 We prompted each LLM with each social post from the test sets in our X-CLAIM dataset and we asked the LLM to respond with the claim span.\n7https://platform.openai.com/docs/models 8https://platform.openai.com/docs/api-reference\nThe generated response may contain words that are either not present in the post or are synonyms of words from the posts. Thus, we treated the response like a normalized claim (Section 3.2) and we passed it through our automated annotation step (Section 3.2) to create the corresponding claim span. We evaluated the predicted claim spans with respect to the gold claim spans. More details about this setup are given in Appendix D.\nZero-shot Prompting. We experiment with four prompts that use no examples: IDENTIFY, EXTRACT, SPAN, and LANGUAGE. The exact prompt structure is given in Figure 5 in the Appendix. Table 4 shows their performance when used with different LLMs on our X-CLAIM dataset.\nWe noticed that the LLMs mostly responded in English even when asked to analyze a post in another language. One reason could be that the prompts do not explicitly specify the language the LLM should respond in. Since our automated annotation step is language-agnostic, the corresponding claim span is in the target language. To overcome this, we asked the LLM to respond in the target language with the LANGUAGE prompt. Interestingly, and unlike GPT-3.5 and GPT-4, the performance of T-DV3 with LANGUAGE prompt significantly dropped by 12-37% F1 (averaged over all languages except English) when compared to the other three prompts. This suggests that T-DV3 is weaker in a multilingual setup.\nWe further find that GPT-4 is nearly always better than GPT-3.5 by an average of 4.23% precision and 1.5% F1 over the four prompts. GPT-3.5 consistently outperformed T-DV3 by an average of 35.96% recall and 27.63% F1, but it lags behind by 0.5% in terms of precision.\nIn-Context Learning. Here, we give the model a few labeled examples as part of the prompt as shown in Figure 6 of the Appendix. Since GPT-4 outperformed the other two LLMs and showed the best performance with LANGUAGE (Table 4), we experimented with in-context learning with GPT4 and LANGUAGE prompt. For Telugu and Bengali, we use examples from translated data (Section 5.2) due to the lack of training data in these languages. The results are shown in Table 5.\nWe see that in-context learning consistently improves F1 score over the zero-shot prompting in all six languages. With more examples shown, the performance increased in English, Hindi and Punjabi at the cost of more computation time. We find that 10-shot in-context learning improved the performance by an average of 2.78% F1 for the six languages in comparison to zero-shot prompting.\nComparing mDeBERTa and GPT-4. We compared the best-performing fine-tuned encoder-only language model to the best-performing generative LLM. The MULTILINGUAL mDeBERTa model and GPT-4 yielded the best results for most languages as reported in Table 3, Table 4, and Table 5.\nIn the case of GPT-4, the best setting uses the LANGUAGE prompt with 10-shot in-context learning for the six languages. Figure 3 compares the two models in terms of F1 scores; we further offer comparison in terms of precision and recall in Table 11 of the Appendix.\nWe can see in Figure 3 that MULTILINGUAL mDeBERTa outperforms GPT-4 by 2.07% F1, averaged over the six languages. GPT-4 shows competitive performance with mDeBERTa in English, Hindi and Punjabi. On the remaining three languages, mDeBERTa outperforms GPT-4 by a large margin of 2-7% F1. This suggests that LLMs show strong performance on high-resource languages like English, but still lag behind smaller fine-tuned LMs on low-resource languages such as Bengali."
        },
        {
            "heading": "5.4 Comparing X-CLAIM and CURT",
            "text": "We trained mDeBERTa on the CURT dataset (Sundriyal et al., 2022a), containing tweets in English, and we compared it to the English MONOLINGUAL model (trained with mDeBERTa on English data in X-CLAIM) on the test sets for the six languages in the X-CLAIM dataset. We show the F1 scores for bothmodels in Figure 4 andwe report the precision and the recall scores in Table 12 in the Appendix.\nThe mDeBERTa model fine-tuned on the XCLAIM English data performs competitively in English with the CURT trained model and shows 3.52% F1 average gain over the remaining five languages. Note that CURT is manually annotated and is twice larger than the English part of the XCLAIM dataset. This offers empirical evidence of better model generalization when training on the X-CLAIM dataset compared to the CURT dataset."
        },
        {
            "heading": "6 Error Analysis",
            "text": "In this section, we qualitatively analyze the errors made by the best-performing MULTILINGUAL mDeBERTa model. To provide insights on how LLMs can be improved for this task, we also discuss the errors made by GPT-4 in its bestperforming setting of 10-shot in-context learning.\nWe analyzed the predictions on the test examples in English and Hindi, and we report the kinds of errors made by the two language models in Table 6. Below, we discuss the results of the analysis.\nEnglish. In the first post in Table 6, both models deviate from the gold claim span. GPT-4 model correctly identifies the presence of the claim but inadvertently veers away from the central checkworthy assertion and focuses on the secondary claim. On the other hand, the mDeBERTa model includes information about moisture and bacteria in the mask, but contains several grammatical errors and lacks clarity. In particular, the phrase \u2018every day day legionnaires disease\u2019 is confusing and doesn\u2019t convey a clear message.\nBoth models provide similar claim spans for the second social media post, capturing the central assertion accurately. However, mDeBERTa contains the extra words \u2018pregnancy your\u2019 at the beginning that are not present in the gold span. These extra words introduce confusion and do not accurately represent the claim made in the social media post.\nHindi. Claim span identification in other languages is more complicated than in English due to the lack of proper guidelines pertaining to their linguistic characteristics. In the first example, GPT-4 almost accurately predicted the span, missing the word \u2018\u0936\u094d\u0930\u0940\u092e\u0924\u0940\u2019 in the beginning. While mDeBERTa predicted both the claim and the premise, defying the very purpose of the task, which is to extract precise claim phrases from the post.\nIn the second post, both models performed well overall. However, we observe a similar issue as for English: the inclusion of additional phrases alongside the claim spans, which can potentially detract from the clarity and precision of the claim. This indicates that these models struggle to make precise decisions about claim boundaries.\nWe can conclude that for both languages, the models can identify the claim but might propose wider boundaries, including extra words."
        },
        {
            "heading": "7 Conclusion and Future Work",
            "text": "We proposed a novel automated data annotation methodology for multilingual claim span identification. Using it, we created and released a new dataset called X-CLAIM, which consists of realworld claim spans, and social media posts containing them, collected from numerous social media platforms in six languages: English, Hindi, Punjabi, Tamil, Telugu, and Bengali. Using state-ofthe-art multilingual models, we established strong baselines based on encoder-only and generative language models. Our experiments demonstrated the benefits of multilingual training when compared to other cross-lingual transfer methods such as zero-shot transfer, or training on the translated data, from a high-resource language like English.\nWe observed lower performance for GPT-style generative LLMs when compared to smaller finetuned encoder-only language models and we discussed their error analysis in the spirit of improving the LLMs on this task.\nOur work opens many important research questions: (1) How to obtain real-world claims without relying on fact-checkers analysis? (2) How to improve the understanding of LLMs about claims and social media in low-resource languages? (3) How to automatically curate multiple check-worthy claims made in the post? (4) How to improve the evaluation metric for the mCSI task? and (5) How to expand the CSI task to other lowresource languages? We plan to address these research questions in future work.\nLimitations Our X-CLAIM dataset for the mCSI task is limited to six languages. We do not know how well the developed systems will perform in languages that are not considered in this work. Moreover, the proposed dataset handles only the primary claim in the given social media post and ignores any other potentially check-worthy claims that the post might contain. In practice, the post may contain multiple check-worthy claims.\nEthics Broader Impact: Our models and data will help fact-checkers filter out extraneous information, thus saving them significant amounts of time and resources.\nData: We place the utmost importance on user privacy. As a result, we have no intention of disclosing any information about the users. The data we curated is solely for research purposes, ensuring that user confidentiality and privacy are protected.\nEnvironmental Impact: It is critical to acknowledge the environmental consequences of training large language models. In our case, we mitigate this concern to some extent by focusing primarily on fine-tuning pretrained models rather than training them from scratch."
        },
        {
            "heading": "A Data Collection",
            "text": "Various fact-checking websites analyze social media posts, news articles, and other information sources that may spread misleading information. We confine our data collection to those websites that meet the following requirements. First, the website should have fact-checked numerous social media posts, at least 100, so that we can have a reasonable-sized dataset. Second, it should have investigated posts containing text. We find that many social media posts investigated by factcheckers have their claim encapsulated in another modality, such as image or video, than text. The fact-checkers manually find the claims made in the posts, which we call as normalized claim. Our last requirement is that the fact-checking website should provide the normalized claim on the webpage of the fact-checked post.\nWe find that there are only a couple of factchecking websites that have investigated social media posts in low-resource languages and that meet the requirements discussed above. The website names, along with the number of fact-checked posts scraped from them, are reported in Table 7. For English, we collect data from ThipMedia,9 FullFact,10 Snopes,11 PolitiFact,12 Factly,13 and Vishvasnews.14 We use Vishvasnews for the remaining languages along with Aajtak15 for Hindi alone. We find that there are relatively fewer posts in Telugu and Bengali than in other languages, highlighting the difficulty in creating data for these extremely low-resource languages.\nWe recognize the structure of the webpage for each fact-checking website and write rules (e.g., regular expressions) to collect the post text and the normalized claim. Once the post text and the normalized claim are collected, we pass the pair through various noise removal filters so that the noisy instances (like the ones that do not meet our requirements but dodged the previous steps) are removed from the data. These include removing\n9https://www.thip.media/ 10https://fullfact.org/ 11https://www.snopes.com/ 12https://www.politifact.com/ 13https://factly.in/ 14https://www.vishvasnews.com/ 15https://www.aajtak.in/\nLanguage # Posts English 17,337 Hindi 2,378 Punjabi 1,262 Tamil 319 Telugu 261 Bengali 167\nTable 7: Number of fact-checked social media posts collected from numerous fact-checking initiatives in six languages with the help of web-scraping API.\nwhen the post text or the claim contains words like video, \u092b\u094b\u091f\u094b, reel, etc. We find that this rule is almost always correct. Further, we remove the data points when the length of the post or claim is less than 3words, omitting the erroneously scraped text, or more than 700 words, more like news articles. These filtering steps remove only 2.5% of the total data collected, averaged across six languages."
        },
        {
            "heading": "B Model Training Details",
            "text": "We train our models using the Adam optimizer (Kingma and Ba, 2015) with weight decay of 0, \u03b21 = 0.9 and \u03b22 = 0.999. All experiments are carried out on a single A100 (40 GB) GPU.We use and adapt the code of Mittal and Nakov (2022) for our task. The models are trained with three different random seeds andwe report themedian of three evaluation runs since we observed a high variation of scores across the runs.\nWe do hyperparameter tuning for the learning rate and the batch size over the English data and use the same hyperparameters over the data of the remaining five languages. Driven by the motivation that the base transformer model is pretrained on a large corpus of text and requires less training, we use a smaller learning rate of 1e-5 for it, but a slightly bigger learning rate of 3e-4 for the token-classifier network. We use a batch size of 32 for training mBERT and mDeBERTa whereas a smaller batch size of 16 for the larger model, XLM-R. The maximum sequence length for the three encoder-only language models is set to 512 to avoid initializing and training new positional embeddings. We use early stopping with a patience of 7 epochs to find the best model checkpoint as per the best F1 score over the development set.\nThe development set is set differently in various training methodologies. For MONOLINGUAL models, we use the development data in the target language, whereas, for MULTILINGUAL models, we combine and utilize the development sets of all languages. The translate-train models use the development data in the target language when available (Hindi, Punjabi, and Tamil) and use the translated English development set for Telugu and Bengali.\nWe provide the number of trainable parameters of the pretrained encoder-only language models in Table 8. For training on English data, XLM-R consumes nearly 1 hour of GPU runtime whereas mBERT and mDeBERTa take nearly 0.5 hours."
        },
        {
            "heading": "C Modelling Details",
            "text": "The encoder-only language models are trained in the frame of sequence tagging task where the model needs to predict the correct label for each token in the post text. A randomly initialized feedforward neural network is placed on top of the pretrained encoder as a token-classifier network. It takes as input the contextualized token embeddings (output by the pretrained encoder) and results in the probability distribution over the label space. The cardinality of the label space depends on how the tokens are encoded.\nWe experiment with token-level encoding schemes: IO, BIO, BEO and BEIO. We train four XLM-R models, one with each encoding, on the English training data in X-CLAIM dataset and compare their performance on the English test set in X-CLAIM. The scores are reported in Table 9: IO encoding shows the best F1 performance among different encoding schemes."
        },
        {
            "heading": "D Prompting the Large Language Models (LLMs) in GPT series",
            "text": "We use OpenAI API and evaluate text-davinci003 (T-DV3), gpt-3.5-turbo (GPT-3.5) and gpt4-0314 (GPT-4) models on multilingual claim span identification task through prompting. The prompts used in zero-shot prompting are provided in Figure 5. The decoding temperature is set to 0 and we use the default maximum response length. All the GPT series LLMs were prompted from Oct 16, 2023 to Oct 22, 2023."
        }
    ],
    "title": "Lost in Translation, Found in Spans: Identifying Claims in Multilingual Social Media",
    "year": 2023
}