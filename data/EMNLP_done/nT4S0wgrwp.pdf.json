{
    "abstractText": "Given a document in a source language, crosslingual summarization (CLS) aims at generating a concise summary in a different target language. Unlike monolingual summarization (MS), naturally occurring source-language documents paired with target-language summaries are rare. To collect large-scale CLS data, existing datasets typically involve translation in their creation. However, the translated text is distinguished from the text originally written in that language, i.e., translationese. In this paper, we first confirm that different approaches of constructing CLS datasets will lead to different degrees of translationese. Then we systematically investigate how translationese affects CLS model evaluation and performance when it appears in source documents or target summaries. In detail, we find that (1) the translationese in documents or summaries of test sets might lead to the discrepancy between human judgment and automatic evaluation; (2) the translationese in training sets would harm model performance in real-world applications; (3) though machine-translated documents involve translationese, they are very useful for building CLS systems on low-resource languages under specific training strategies. Lastly, we give suggestions for future CLS research including dataset and model developments. We hope that our work could let researchers notice the phenomenon of translationese in CLS and take it into account in the future.",
    "authors": [
        {
            "affiliations": [],
            "name": "Jiaan Wang"
        },
        {
            "affiliations": [],
            "name": "Fandong Meng"
        },
        {
            "affiliations": [],
            "name": "Yunlong Liang"
        },
        {
            "affiliations": [],
            "name": "Tingyi Zhang"
        },
        {
            "affiliations": [],
            "name": "Jiarong Xu"
        },
        {
            "affiliations": [],
            "name": "Zhixu Li"
        },
        {
            "affiliations": [],
            "name": "Jie Zhou"
        }
    ],
    "id": "SP:f81a0fc6eeb0a3ea115e064be137b3ec5c89f9c3",
    "references": [
        {
            "authors": [
                "Mikel Artetxe",
                "Gorka Labaka",
                "Eneko Agirre."
            ],
            "title": "Translation artifacts in cross-lingual transfer learning",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 7674\u20137684, Online. Association for Computa-",
            "year": 2020
        },
        {
            "authors": [
                "Yu Bai",
                "Yang Gao",
                "Heyan Huang."
            ],
            "title": "Crosslingual abstractive summarization with limited parallel resources",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference",
            "year": 2021
        },
        {
            "authors": [
                "Mona Baker",
                "Gill Francis",
                "Elena Tognini-Bonelli."
            ],
            "title": "Corpus Linguistics and Translation Studies: Implications and Applications, chapter 2",
            "venue": "John Benjamins Publishing Company, Netherlands.",
            "year": 1993
        },
        {
            "authors": [
                "Yoshua Bengio",
                "J\u00e9r\u00f4me Louradour",
                "Ronan Collobert",
                "Jason Weston."
            ],
            "title": "Curriculum learning",
            "venue": "Proceedings of the 26th Annual International Conference on Machine Learning, ICML 2009, Montreal, Quebec, Canada, June 14-18, 2009, volume 382 of",
            "year": 2009
        },
        {
            "authors": [
                "Yuri Bizzoni",
                "Tom S Juzek",
                "Cristina Espa\u00f1a-Bonet",
                "Koel Dutta Chowdhury",
                "Josef van Genabith",
                "Elke Teich."
            ],
            "title": "How human is machine translationese? comparing human and machine translations of text and speech",
            "venue": "Proceedings of the 17th International",
            "year": 2020
        },
        {
            "authors": [
                "Chris Callison-Burch",
                "Cameron Fordyce",
                "Philipp Koehn",
                "Christof Monz",
                "Josh Schroeder."
            ],
            "title": "meta-) evaluation of machine translation",
            "venue": "Proceedings of the Second Workshop on Statistical Machine Translation, pages 136\u2013158, Prague, Czech Republic. Asso-",
            "year": 2007
        },
        {
            "authors": [
                "Yue Cao",
                "Hui Liu",
                "Xiaojun Wan."
            ],
            "title": "Jointly learning to align and summarize for neural crosslingual summarization",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6220\u20136231, Online. Association",
            "year": 2020
        },
        {
            "authors": [
                "Isaac Caswell",
                "Ciprian Chelba",
                "David Grangier."
            ],
            "title": "Tagged back-translation",
            "venue": "Proceedings of the Fourth Conference on Machine Translation (Volume",
            "year": 2019
        },
        {
            "authors": [
                "Yulong Chen",
                "Ming Zhong",
                "Xuefeng Bai",
                "Naihao Deng",
                "Jing Li",
                "Xianchao Zhu",
                "Yue Zhang."
            ],
            "title": "The cross-lingual conversation summarization challenge",
            "venue": "ArXiv, abs/2205.00379.",
            "year": 2022
        },
        {
            "authors": [
                "Michael Denkowski",
                "Alon Lavie."
            ],
            "title": "Choosing the right evaluation for machine translation: an examination of annotator and automatic metric performance on human judgment tasks",
            "venue": "Proceedings of the 9th Conference of the Association for Machine",
            "year": 2010
        },
        {
            "authors": [
                "Sergey Edunov",
                "Myle Ott",
                "Marc\u2019Aurelio Ranzato",
                "Michael Auli"
            ],
            "title": "On the evaluation of machine translation systems trained with back-translation",
            "venue": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,",
            "year": 2020
        },
        {
            "authors": [
                "Michael Farrell."
            ],
            "title": "Machine translation markers in post-edited machine translation output",
            "venue": "Proceedings of the 40th Conference Translating and the Computer, pages 50\u201359.",
            "year": 2018
        },
        {
            "authors": [
                "Xiachong Feng",
                "Xiaocheng Feng",
                "Bing Qin."
            ],
            "title": "MSAMSum: Towards benchmarking multi-lingual dialogue summarization",
            "venue": "Proceedings of the Second DialDoc Workshop on Document-grounded Dialogue and Conversational Question Answering, pages",
            "year": 2022
        },
        {
            "authors": [
                "Joseph L Fleiss."
            ],
            "title": "Measuring nominal scale agreement among many raters",
            "venue": "Psychological bulletin, 76(5):378.",
            "year": 1971
        },
        {
            "authors": [
                "Martin Gellerstam."
            ],
            "title": "Translationese in swedish novels translated from english",
            "venue": "Lars Wollin and Hans Lindquist, editors, Translation Studies in Scandinavia, page 88\u201395. CWK Gleerup.",
            "year": 1986
        },
        {
            "authors": [
                "Martin Gellerstam."
            ],
            "title": "Translations as a source for cross-linguistic studies",
            "venue": "Lund Studies in English, 88:53\u201362.",
            "year": 1996
        },
        {
            "authors": [
                "Bogdan Gliwa",
                "Iwona Mochol",
                "Maciej Biesek",
                "Aleksander Wawer."
            ],
            "title": "SAMSum corpus: A humanannotated dialogue dataset for abstractive summarization",
            "venue": "Proceedings of the 2nd Workshop on New Frontiers in Summarization, pages 70\u201379, Hong",
            "year": 2019
        },
        {
            "authors": [
                "Yvette Graham",
                "Barry Haddow",
                "Philipp Koehn."
            ],
            "title": "Statistical power and translationese in machine translation evaluation",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 72\u201381, Online.",
            "year": 2020
        },
        {
            "authors": [
                "Spence Green",
                "Jeffrey Heer",
                "Christopher D Manning."
            ],
            "title": "The efficacy of human post-editing for language translation",
            "venue": "Proceedings of the SIGCHI conference on human factors in computing systems, pages 439\u2013448.",
            "year": 2013
        },
        {
            "authors": [
                "Tahmid Hasan",
                "Abhik Bhattacharjee",
                "Wasi Uddin Ahmad",
                "Yuan-Fang Li",
                "Yong-Bin Kang",
                "Rifat Shahriyar."
            ],
            "title": "Crosssum: Beyond englishcentric cross-lingual abstractive text summarization for 1500+ language pairs",
            "venue": "ArXiv preprint,",
            "year": 2021
        },
        {
            "authors": [
                "Tahmid Hasan",
                "Abhik Bhattacharjee",
                "Md. Saiful Islam",
                "Kazi Mubasshir",
                "Yuan-Fang Li",
                "Yong-Bin Kang",
                "M. Sohel Rahman",
                "Rifat Shahriyar."
            ],
            "title": "XLsum: Large-scale multilingual abstractive summarization for 44 languages",
            "venue": "Findings of the Association",
            "year": 2021
        },
        {
            "authors": [
                "Faisal Ladhak",
                "Esin Durmus",
                "Claire Cardie",
                "Kathleen McKeown."
            ],
            "title": "WikiLingua: A new benchmark dataset for cross-lingual abstractive summarization",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4034\u20134048,",
            "year": 2020
        },
        {
            "authors": [
                "Gennadi Lembersky",
                "Noam Ordan",
                "Shuly Wintner."
            ],
            "title": "Adapting translation models to translationese improves SMT",
            "venue": "Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 255\u2013265, Avignon,",
            "year": 2012
        },
        {
            "authors": [
                "Anton Leuski",
                "Chin-Yew Lin",
                "Liang Zhou",
                "Ulrich Germann",
                "Franz Josef Och",
                "Eduard H. Hovy."
            ],
            "title": "Cross-lingual c*st*rd: English access to hindi information",
            "venue": "ACM Trans. Asian Lang. Inf. Process., 2:245\u2013269.",
            "year": 2003
        },
        {
            "authors": [
                "Qian Li",
                "Shu Guo",
                "Cheng Ji",
                "Xutan Peng",
                "Shiyao Cui",
                "Jianxin Li",
                "Lihong Wang."
            ],
            "title": "Dual-gated fusion with prefix-tuning for multi-modal relation extraction",
            "venue": "Findings of the Association for Computational Linguistics: ACL 2023, Toronto, Canada,",
            "year": 2023
        },
        {
            "authors": [
                "Qian Li",
                "Shu Guo",
                "Yangyifei Luo",
                "Cheng Ji",
                "Lihong Wang",
                "Jiawei Sheng",
                "Jianxin Li."
            ],
            "title": "Attribute-consistent knowledge graph representation learning for multi-modal entity alignment",
            "venue": "Proceedings of the ACM Web Conference 2023, WWW",
            "year": 2023
        },
        {
            "authors": [
                "Yunlong Liang",
                "Fandong Meng",
                "Jiaan Wang",
                "Jinan Xu",
                "Yufeng Chen",
                "Jie Zhou."
            ],
            "title": "D2tv: Dual knowledge distillation and target-oriented vision modeling for many-to-many multimodal summarization",
            "venue": "arXiv preprint arXiv:2305.12767.",
            "year": 2023
        },
        {
            "authors": [
                "Yunlong Liang",
                "Fandong Meng",
                "Jinan Xu",
                "Jiaan Wang",
                "Yufeng Chen",
                "Jie Zhou."
            ],
            "title": "Summaryoriented vision modeling for multimodal abstractive summarization",
            "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Lin-",
            "year": 2023
        },
        {
            "authors": [
                "Yunlong Liang",
                "Fandong Meng",
                "Chulun Zhou",
                "Jinan Xu",
                "Yufeng Chen",
                "Jinsong Su",
                "Jie Zhou."
            ],
            "title": "A variational hierarchical model for neural cross-lingual summarization",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Lin-",
            "year": 2022
        },
        {
            "authors": [
                "Dublin",
                "Ireland"
            ],
            "title": "Association for Computational Linguistics",
            "venue": "Long Papers),",
            "year": 2099
        },
        {
            "authors": [
                "Chin-Yew Lin."
            ],
            "title": "ROUGE: A package for automatic evaluation of summaries",
            "venue": "Text Summarization Branches Out, pages 74\u201381, Barcelona, Spain. Association for Computational Linguistics.",
            "year": 2004
        },
        {
            "authors": [
                "Benjamin Marie",
                "Raphael Rubino",
                "Atsushi Fujita."
            ],
            "title": "Tagged back-translation revisited: Why does it really work? In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5990\u20135997, Online",
            "venue": "Association for",
            "year": 2020
        },
        {
            "authors": [
                "Ramesh Nallapati",
                "Bowen Zhou",
                "Cicero dos Santos",
                "\u00c7a\u011flar Gul\u00e7ehre",
                "Bing Xiang."
            ],
            "title": "Abstractive text summarization using sequence-to-sequence RNNs and beyond",
            "venue": "Proceedings of The 20th SIGNLL Conference on Computational Natural Lan-",
            "year": 2016
        },
        {
            "authors": [
                "Thong Thanh Nguyen",
                "Anh Tuan Luu."
            ],
            "title": "Improving neural cross-lingual abstractive summarization via employing optimal transport distance for knowledge distillation",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, 36(10):11103\u2013",
            "year": 2022
        },
        {
            "authors": [
                "Jessica Ouyang",
                "Boya Song",
                "Kathy McKeown."
            ],
            "title": "A robust abstractive system for cross-lingual summarization",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Lan-",
            "year": 2019
        },
        {
            "authors": [
                "Laura Perez-Beltrachini",
                "Mirella Lapata."
            ],
            "title": "Models and datasets for cross-lingual summarisation",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 9408\u20139423, Online and Punta Cana, Dominican Re-",
            "year": 2021
        },
        {
            "authors": [
                "Federica Scarpa."
            ],
            "title": "Corpus-based quality assessment of specialist translation: A study using parallel and comparable corpora in english and italian",
            "venue": "Insights into specialized translation\u2013linguistics insights.",
            "year": 2006
        },
        {
            "authors": [
                "Larry Selinker."
            ],
            "title": "Interlanguage",
            "venue": "International Review of Applied Linguistics in Language Teaching (IRAL), 10(1-4):209\u2013232.",
            "year": 1972
        },
        {
            "authors": [
                "Yuqing Tang",
                "Chau Tran",
                "Xian Li",
                "Peng-Jen Chen",
                "Naman Goyal",
                "Vishrav Chaudhary",
                "Jiatao Gu",
                "Angela Fan."
            ],
            "title": "Multilingual translation from denoising pre-training",
            "venue": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021,",
            "year": 2021
        },
        {
            "authors": [
                "Mildred C Templin."
            ],
            "title": "Certain language skills in children: Their development and interrelationships, volume 10",
            "venue": "JSTOR.",
            "year": 1957
        },
        {
            "authors": [
                "Antonio Toral."
            ],
            "title": "Post-editese: an exacerbated translationese",
            "venue": "Proceedings of Machine Translation Summit XVII: Research Track, pages 273\u2013281, Dublin, Ireland. European Association for Machine Translation.",
            "year": 2019
        },
        {
            "authors": [
                "Gideon Toury."
            ],
            "title": "Descriptive translation studies: And beyond",
            "venue": "Descriptive Translation Studies, pages 1\u2013366.",
            "year": 2012
        },
        {
            "authors": [
                "Vered Volansky",
                "Noam Ordan",
                "Shuly Wintner."
            ],
            "title": "On the features of translationese",
            "venue": "Digital Scholarship in the Humanities, 30(1):98\u2013118.",
            "year": 2013
        },
        {
            "authors": [
                "Xiaojun Wan."
            ],
            "title": "Using bilingual information for cross-language document summarization",
            "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 1546\u20131555, Portland, Ore-",
            "year": 2011
        },
        {
            "authors": [
                "Xiaojun Wan",
                "Huiying Li",
                "Jianguo Xiao."
            ],
            "title": "Cross-language document summarization based on machine translation quality prediction",
            "venue": "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 917\u2013926, Uppsala,",
            "year": 2010
        },
        {
            "authors": [
                "Jiaan Wang",
                "Yunlong Liang",
                "Fandong Meng",
                "Beiqi Zou",
                "Zhixu Li",
                "Jianfeng Qu",
                "Jie Zhou"
            ],
            "title": "2023a. Zeroshot cross-lingual summarization via large language models",
            "year": 2023
        },
        {
            "authors": [
                "Jiaan Wang",
                "Fandong Meng",
                "Ziyao Lu",
                "Duo Zheng",
                "Zhixu Li",
                "Jianfeng Qu",
                "Jie Zhou."
            ],
            "title": "ClidSum: A benchmark dataset for cross-lingual dialogue summarization",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Pro-",
            "year": 2022
        },
        {
            "authors": [
                "Jiaan Wang",
                "Fandong Meng",
                "Duo Zheng",
                "Yunlong Liang",
                "Zhixu Li",
                "Jianfeng Qu",
                "Jie Zhou."
            ],
            "title": "A Survey on Cross-Lingual Summarization",
            "venue": "Transactions of the Association for Computational Linguistics, 10:1304\u20131323.",
            "year": 2022
        },
        {
            "authors": [
                "Jiaan Wang",
                "Fandong Meng",
                "Duo Zheng",
                "Yunlong Liang",
                "Zhixu Li",
                "Jianfeng Qu",
                "Jie Zhou."
            ],
            "title": "Towards unifying multi-lingual and cross-lingual summarization",
            "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Lin-",
            "year": 2023
        },
        {
            "authors": [
                "Ruochen Xu",
                "Chenguang Zhu",
                "Yu Shi",
                "Michael Zeng",
                "Xuedong Huang."
            ],
            "title": "Mixed-lingual pretraining for cross-lingual summarization",
            "venue": "Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Lin-",
            "year": 2020
        },
        {
            "authors": [
                "Jin-ge Yao",
                "Xiaojun Wan",
                "Jianguo Xiao."
            ],
            "title": "Phrase-based compressive cross-language summarization",
            "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 118\u2013127, Lisbon, Portugal. Association for",
            "year": 2015
        },
        {
            "authors": [
                "Sicheng Yu",
                "Qianru Sun",
                "Hao Zhang",
                "Jing Jiang."
            ],
            "title": "Translate-train embracing translationese artifacts",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 362\u2013370, Dublin, Ire-",
            "year": 2022
        },
        {
            "authors": [
                "Kaizhong Zhang",
                "Dennis Shasha."
            ],
            "title": "Simple fast algorithms for the editing distance between trees and related problems",
            "venue": "SIAM Journal on Computing, 18(6):1245\u20131262.",
            "year": 1989
        },
        {
            "authors": [
                "Mike Zhang",
                "Antonio Toral."
            ],
            "title": "The effect of translationese in machine translation test sets",
            "venue": "Proceedings of the Fourth Conference on Machine Translation (Volume 1: Research Papers), pages 73\u2013 81, Florence, Italy. Association for Computational",
            "year": 2019
        },
        {
            "authors": [
                "Tianyi Zhang",
                "Varsha Kishore",
                "Felix Wu",
                "Kilian Q. Weinberger",
                "Yoav Artzi."
            ],
            "title": "Bertscore: Evaluating text generation with BERT",
            "venue": "8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenRe-",
            "year": 2020
        },
        {
            "authors": [
                "Chenguang Zhu",
                "Yang Liu",
                "Jie Mei",
                "Michael Zeng."
            ],
            "title": "MediaSum: A large-scale media interview dataset for dialogue summarization",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics:",
            "year": 2021
        },
        {
            "authors": [
                "Junnan Zhu",
                "Haoran Li",
                "Tianshang Liu",
                "Yu Zhou",
                "Jiajun Zhang",
                "Chengqing Zong."
            ],
            "title": "MSMO: Multimodal summarization with multimodal output",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages",
            "year": 2018
        },
        {
            "authors": [
                "Junnan Zhu",
                "Qian Wang",
                "Yining Wang",
                "Yu Zhou",
                "Jiajun Zhang",
                "Shaonan Wang",
                "Chengqing Zong."
            ],
            "title": "NCLS: Neural cross-lingual summarization",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the",
            "year": 2019
        },
        {
            "authors": [
                "Junnan Zhu",
                "Yu Zhou",
                "Jiajun Zhang",
                "Chengqing Zong."
            ],
            "title": "Attend, translate and summarize: An efficient method for neural cross-lingual summarization",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Cross-lingual summarization (CLS) aims to generate a summary in a target language from a given document in a different source language. Under the globalization background, this task helps people grasp the gist of foreign documents efficiently, and attracts wide research attention from the computational linguistics community (Leuski et al., 2003;\n\u2217Work was done when Jiaan Wang was interning at Pattern Recognition Center, WeChat AI, Tencent Inc, China.\n\u2020Corresponding author.\nWan et al., 2010; Yao et al., 2015; Zhu et al., 2019; Ouyang et al., 2019; Ladhak et al., 2020; PerezBeltrachini and Lapata, 2021; Liang et al., 2022).\nAs pointed by previous literature (Ladhak et al., 2020; Perez-Beltrachini and Lapata, 2021; Wang et al., 2022b), one of the key challenges lies in CLS is data scarcity. In detail, naturally occurring documents in a source language paired with the corresponding summaries in a target language are rare (Perez-Beltrachini and Lapata, 2021), making it difficult to collect large-scale and high-quality CLS datasets. For example, it is costly and laborintensive to employ bilingual annotators to create target-language summaries for the given sourcelanguage documents (Chen et al., 2022). Generally, to alleviate data scarcity while controlling costs, the source documents or the target summaries in existing large-scale CLS datasets (Zhu et al., 2019; Ladhak et al., 2020; Perez-Beltrachini and Lapata, 2021; Bai et al., 2021; Wang et al., 2022a; Feng et al., 2022) are (automatically or manually) translated from other languages rather than the text originally written in that language (Section 2.1).\nDistinguished from the text originally written in one language, translated text1 in the same language might involve artifacts which refer to \u201ctranslationese\u201d (Gellerstam, 1986). These artifacts include the usage of simpler, more standardized and more explicit words and grammars (Baker et al., 1993; Scarpa, 2006) as well as the lexical and word order choices that are influenced by the source language (Gellerstam, 1996; Toury, 2012). It has been observed that the translationese in data can mislead model training as its special stylistic is away from native usage (Selinker, 1972; Volansky et al., 2013; Bizzoni et al., 2020; Yu et al., 2022). Nevertheless, translationese is neglected by previous CLS work, leading to unknown impacts and potential risks.\nGrounding the truth that current large-scale CLS\n1 \u201cTranslated text\u201d is equal to \u201ctranslations\u201d, and we alternatively use these two terms in this paper.\ndatasets are typically collected via human or machine translation, in this paper, we investigate the effects of translationese when the translations appear in target summaries (Section 3) or source documents (Section 4), respectively. We first confirm that the different translation methods (i.e., human translation or machine translation) will lead to different degrees of translationese. In detail, for CLS datasets whose source documents (or target summaries) are human-translated texts, we collect their corresponding machine-translated documents (or summaries). The collected documents (or summaries) contain the same semantics as the original ones, but suffer from different translation methods. Then, we utilize automatic metrics from various aspects to quantify translationese, and show the different degrees of translationese between the original and collected documents (or summaries).\nSecond, we investigate how translationese affects CLS model evaluation and performance. To this end, we train and evaluate CLS models with the original and the collected data, respectively, and analyze the model performances via both automatic and human evaluation. We find that (1) the translationese in documents or summaries of test sets might lead to the discrepancy between human judgment and automatic evaluation (i.e. ROUGE and BERTScore). Thus, the test sets of CLS datasets should carefully control their translationese, and avoid directly adopting machine-translated documents or summaries. (2) The translationese in training sets would harm model performance in realworld applications where the translationese should be avoided. For example, a CLS model trained with machine-translated documents or summaries shows limited ability to generate informative and fluent summaries. (3) Though it is sub-optimal to train a CLS model only using machine-translated documents as source documents, they are very useful for building CLS systems on low-resource languages under specific training strategies. Lastly, since the translationese affects model evaluation and performance, we give suggestions for future CLS data and model developments especially on low-resource languages.\nContributions. (1) To our knowledge, we are the first to investigate the influence of translationese on CLS. We confirm that the different translation methods in creating CLS datasets will lead to different degrees of translationese. (2) We conduct systematic experiments to show the effects of the\ntranslationese in source documents and target summaries, respectively. (3) Based on our findings, we discuss and give suggestions for future research."
        },
        {
            "heading": "2 Background",
            "text": ""
        },
        {
            "heading": "2.1 Translations in CLS Datasets",
            "text": "To provide a deeper understanding of the translations in CLS datasets, we comprehensively review previous datasets, and introduce the fountain of their documents and summaries, respectively.\nZhu et al. (2019) utilize a machine translation (MT) service to translate the summaries of two English monolingual summarization (MS) datasets (i.e., CNN/Dailymail (Nallapati et al., 2016) and MSMO (Zhu et al., 2018)) to Chinese. The translated Chinese summaries together with the original English documents form En2ZhSum dataset. Later, Zh2EnSum (Zhu et al., 2019) and En2DeSum (Bai et al., 2021) are also constructed in this manner. The source documents of these CLS datasets are originally written in those languages (named natural text), while the target summaries are automatically translated from other languages (named MT text). Feng et al. (2022) utilize Google MT service2 to translate both the documents and summaries of an English MS dataset (SAMSum (Gliwa et al., 2019)) into other five languages. The translated data together with the original data forms MSAMSum dataset. Thus, MSAMSum contains six source languages as well as six target languages, and only the English documents and summaries are natural text, while others are MT text. Since the translations provided by MT services might contain flaws, the above datasets further use round-trip translation strategy (\u00a7 2.2) to filter out low-quality samples.\nIn addition to MT text, human-translated text (HT text) is also adopted in current CLS datasets. WikiLingua (Ladhak et al., 2020) collects documentsummary pairs in 18 languages (including English) from WikiHow3. In this dataset, only the English documents/summaries are natural text, while all those in other languages are translated from the corresponding English versions by WikiHow\u2019s human writers (Ladhak et al., 2020). XSAMSum and XMediaSum (Wang et al., 2022a) are constructed through manually translating the summaries of SAMSum (Gliwa et al., 2019) and MediaSum (Zhu et al., 2021), respectively. Thus, the source documents and target summaries are natural text and\n2https://cloud.google.com/translate 3https://www.wikihow.com/\nHT text, respectively.\nXWikis (Perez-Beltrachini and Lapata, 2021) collects document-summary pairs in 4 languages (i.e., English, French, German and Czech) from Wikipedia. Each document-summary pair is extracted from a Wikipedia page. To align the parallel pages (which are relevant to the same topic but in different languages), Wikipedia provides Interlanguage links. When creating a new Wikipedia page, it is more convenient for Wikipedians to create by translating from its parallel pages (if has) than editing from scratch, leading to a large number of HT text in XWikis.4 Thus, XWikis is formed with both natural text and HT text. Note that though the translations commonly appear in XWikis, we cannot distinguish which documents/summaries are natural text or HT text. This is because we are not provided with the translation relations among the parallel contents. For example, some documents in XWikis might be translated from their parallel documents, while others might be natural text serving as origins to create their parallel documents.\nTable 1 summarizes the fountain of the source documents and target summaries in current datasets. We can conclude that when performing CLS from a source language to a different target language, translated text (MT and HT text) is extremely common in these datasets and appears more commonly in target summaries than source documents.\n4https://en.wikipedia.org/wiki/Wikipedia: Translation"
        },
        {
            "heading": "2.2 Round-Trip Translation",
            "text": "The round-trip translation (RTT) strategy is used to filter out low-quality CLS samples built by MT services. In detail, for a given text t that needs to be translated, this strategy first translates t into the target language t\u0302, and then translates the result t\u0302 back to the original language t\u2032 based on MT services. Next, t\u0302 is considered a high-quality translation if the ROUGE scores (Lin, 2004) between t and t\u2032 exceed a pre-defined threshold. Accordingly, the CLS samples will be discarded if the translations in them are not high-quality."
        },
        {
            "heading": "2.3 Translationese Metrics",
            "text": "To quantify translationese, we follow Toral (2019) and adopt automatic metrics from three aspects, i.e., simplification, normalization and interference. Simplification. Compared with natural text, translations tend to be simpler like using a lower number of unique words (Farrell, 2018) or content words (i.e., nouns, verbs, adjectives and adverbs) (Scarpa, 2006). The following metrics are adopted: \u2022 Type-Token Ratio (TTR) is used to evaluate lexi-\ncal diversity (Templin, 1957) calculated by dividing the number of types (i.e., unique tokens) by the total number of tokens in the text. \u2022 Vocabulary Size (VS) calculates the total number of different words in the text. \u2022 Lexical Density (LD) measures the information lies in the text by calculating the ratio between the number of its content words and its total number of words (Toral, 2019).\nNormalization. The lexical choices in translated text tend to be normalized (Baker et al., 1993). We use entropy to measure this characteristic: \u2022 Entropy of distinct n-grams (Ent-n) in the text. \u2022 Entropy of content words (Ent-cw) in the text. Interference. The structure of translated text tends to be similar to its source text (Gellerstam, 1996). \u2022 Syntactic Variation (SV) is calculated by the nor-\nmalized tree edit distance (Zhang and Shasha, 1989) between the constituency parse trees of the translated text and the source text.5 \u2022 Part-of-Speech Variation (PSV) is computed by the normalized edit distance between the part-ofspeech sequences of the translated text and the source text. It is worth noting that, ideally, the fewer / lowerlevel translationese in the translations, the higher 5We remove tokens from the constituency parse trees to let the metric focus on syntax rather than lexical.\nall the above metrics will be."
        },
        {
            "heading": "3 Translationese in Target Summaries",
            "text": "In this section, we investigate how translationese affects CLS evaluation and training when it appears in the target summaries. For CLS datasets whose source documents are natural text while target summaries are HT text, we collect another summaries (in MT text) for them via Google MT. In this manner, one document will pair with two summaries (containing the same semantics, but one is HT text and the other is MT text). The translationese in these two types of summaries could be quantified. Subsequently, we can use the summaries in HT text and MT text as references, respectively, to train CLS models and analyze the influence of translationese on model performance."
        },
        {
            "heading": "3.1 Experimental Setup",
            "text": "Datasets Selection.\nFirst, we should choose CLS datasets with source documents in natural text and target summaries in HT text. Under the consideration of the diversity of languages, scales and domains, we decide to choose XSAMSum (En\u21d2Zh) and WikiLingua (En\u21d2Ru/Ar/Cs).6 Summaries Collection. The original Chinese (Zh) summaries in XSAMSum, as well as the Russian (Ru), Arabic (Ar) and Czech (Cs) summaries in WikiLingua are HT text. Besides, XSAMSum and WikiLingua also provide the corresponding English summaries in natural text. Therefore, in addition to these original target summaries (in HT text), we can automatically translate the English summaries to the target languages to collect another summaries (in MT text) based on Google MT service.\nRTT strategy (c.f., Section 2.2) is further adopted to remove the low-quality translated summaries. As a result, the number of the translated summaries is less than that of original summaries. To ensure the comparability in subsequent experiments, we also discard the original summaries if the corresponding translated ones are removed. Lastly, the remaining original and translated summaries together with source documents form the final data we used.\nThanks to MSAMSum (Feng et al., 2022) which has already translated the English summaries of SAMSum to Chinese via Google MT service, thus,\n6Since a CLS dataset might contain multiple source and target languages, we use \u201cX\u21d2Y\u201d to indicate the source language and target language are X and Y, respectively. Language nomenclature is based on ISO 639-1 codes.\nwe directly utilize their released summaries7 as the translated summaries of XSAMSum. Data Splitting. After preprocessing, WikiLingua (En\u21d2Ru, En\u21d2Ar and En\u21d2Cs) contain 34,273, 20,751 and 5,686 samples, respectively. We split them into 30,273/2,000/2,000, 16,751/2,000/2,000 and 4,686/500/500 w.r.t training/validation/test sets. For XSAMSum, since the summaries in MT text are provided by Feng et al. (2022), we also follow their splitting, i.e., 5307/302/320. Implementation Details. Following recent CLS work (Feng et al., 2022; Wang et al., 2022a), we use mBART-50 (Tang et al., 2021) as the CLS model. The implementation details of model training and testing are given in Appendix B."
        },
        {
            "heading": "3.2 Translationese Analysis",
            "text": "We analyze the translationese in the target summaries of the preprocessed datasets. As shown in Table 2, the scores (measured by the metrics described in Section 2.3) in HT summaries are generally higher than those in MT summaries, indicating the HT summaries contain more diverse words and meaningful semantics, and their sentence structures are less influenced by the source text (i.e., English summaries). Thus, the degree of translationese in HT summaries is less than that in MT summaries, which also verifies that different methods of collecting target-language summaries might lead to different degrees of translationese."
        },
        {
            "heading": "3.3 Translationese\u2019s Impact on Evaluation",
            "text": "For each dataset, we train two models with the same input documents but different target summaries. Specifically, one uses HT summaries as references (denoted as mBART-HT), while the other\n7https://github.com/xcfcode/MSAMSum\nuses MT summaries (denoted as mBART-MT). Table 3 gives the experimental results in terms of ROUGE-1/2/L (R1/R2/R-L) (Lin, 2004) and BERTScore (B-S) (Zhang et al., 2020). Note that there are two ground-truth summaries (HT and MT) in the test sets. Thus, for model performance on each dataset, we report two results using HT and MT summaries as references to evaluate CLS models, respectively. It is apparent to find that when using MT summaries as references, mBART-MT performs better than mBART-HT, but when using HT summaries as references, mBART-MT works worse. This is because the model would perform better when the distribution of the training data and the test data are more consistent. Though straightforward, this finding indicates that if a CLS model achieves higher automatic scores on the test set whose summaries are MT text, it does not mean that the model could perform better in real applications where the translationese should be avoided.\nTo confirm the above point, we further conduct human evaluation on the output summaries of mBART-HT and mBART-MT. Specifically, we randomly select 100 samples from the test set of XSAMSum, and employ five graduate students as evaluators to score the generated summaries of mBART-HT and mBART-MT, and the ground-truth HT summaries in terms of informativeness, fluency and overall quality with a 3-point scale. During scoring, the evaluators are not provided with the source of each summary. More details about human evaluation are given in Appendix C. Table 4 shows the result of human evaluation. The Fleiss\u2019 Kappa scores (Fleiss, 1971) of informativeness, fluency and overall are 0.46, 0.37 and 0.52, respectively, indicating a good inter-agreement among our evaluators. mBART-HT outperforms mBARTMT in all metrics, and thus the human judgment is in line with the automatic metrics when adopting HT summaries (rather than MT summaries) as references. Based on this finding, we argue that when building CLS datasets, the translationese in target summaries of test sets should be carefully controlled."
        },
        {
            "heading": "3.4 Translationese\u2019s Impact on Training",
            "text": "Compared with HT summaries, when using MT summaries as references to train a CLS model, it is easier for the model to learn the mapping from the source documents to the simpler and more standardized summaries. In this manner, the generated sum-\nmaries tend to have a good lexical overlap with the MT references since both the translationese texts contain normalized lexical usages. However, such summaries may not satisfy people in the real scene (c.f., our human evaluation in Table 4). Thus, the translationese in target summaries during training has a negative impact on CLS model performance.\nFurthermore, we find that mBART-HT has the following inconsistent phenomenon: the generated summaries of mBART-HT achieve a higher similarity with HT references than MT references on the WikiLingua (En\u21d2Ru, Ar and Cs) datasets (e.g., 24.6 vs. 23.9, 23.6 vs. 23.0 and 16.5 vs. 15.4 R1, respectively), but are more similar to MT references on XSAMSum (e.g., 40.2 vs. 39.1 R1). We conjecture this inconsistent performance is caused by the trade-off between the following factors: (i) mBART-HT is trained with the HT references rather than the MT references, and (ii) both the generated summaries and MT references are translationese texts containing normalized lexical usages. Factor (i) tends to steer the generated summaries closer to the HT references, while factor (ii) makes them closer to the MT references. When the CLS model has fully learned the mapping from the source documents to the HT summaries during training, factor (i) will dominate the generated sum-\nmaries closer to the HT references, otherwise, the translationese in the generated summaries will lead them closer to the MT references. Therefore, the difficulty of CLS training data would lead to the inconsistent performance of mBART-HT. The verification of our conjecture is given in Appendix A."
        },
        {
            "heading": "4 Translationese in Source Documents",
            "text": "In this section, we explore how translationese affects CLS evaluation and training when it appears in the source documents. For CLS datasets whose source documents are HT text while target summaries are natural text, we collect another documents (in MT text) for them via Google MT service. In this way, one summary will correspond to two documents (containing the same semantics, but one is HT text and the other is MT text). Next, we can use the documents in HT text and MT text as source documents, respectively, to train CLS models and analyze the influence of translationese."
        },
        {
            "heading": "4.1 Experimental Setup",
            "text": "Datasets Selection. The CLS datasets used in this section should contain source documents in HT text and target summaries in natural text. Here, we choose WikiLingua (Ar/Ru/Fr\u21d2En). Documents Collection. To collect MT documents, we translate the original English documents of WikiLingua to Arabic (Ar), Russian (Ru) and French (Fr) via Google MT service. Similar to Section 3.1, RTT strategy is also adopted to control the quality. Data Splitting. After preprocessing, WikiLingua (Ar/Ru/Fr\u21d2En) contain 25,195/36,503/60,088 samples. We split them into 21,195/2,000/2,000, 30,503/3,000/3,000 and 54,088/3,000/3,000 w.r.t training/validation/test sets, respectively."
        },
        {
            "heading": "4.2 Translationese Analysis",
            "text": "We analyze the translationese in the preprocessed documents. Table 5 shows that most scores of HT documents are higher than those of MT documents, indicating a lower degree of translationese in HT documents. Thus, the different methods to collect the source documents might also result in different degrees of translationese."
        },
        {
            "heading": "4.3 Translationese\u2019s Impact on Evaluation",
            "text": "For each direction in the WikiLingua dataset, we train two mBART models with the same output summaries but different input documents. In detail, one uses HT documents as inputs (denoted as mBART-iHT), while the other uses MT documents (denoted as mBART-iMT).\nTable 6 lists the experimental results in terms of ROUGE-1/2/L (R-1/2/L) and BERTScore (BS). Note that there are two types of input documents (HT and MT) in the test sets. For each model, we report two results using HT and MT documents as inputs to generate summaries, respectively. Compared with using HT documents as inputs, both mBART-iHT and mBART-iMT achieve higher automatic scores when using MT documents as inputs. For example, mBART-iHT achieves 32.7 and 33.7 R1, using HT documents and MT documents as inputs in WikiLingua (Ru\u21d2En), respectively. The counterparts of mBART-iMT are 32.4\nand 34.8 R1. In addition to the above automatic evaluation, we conduct human evaluation on these four types (mBART-iHT/iMT with HT/MT documents as inputs) of the generated summaries. In detail, we randomly select 100 samples from the test set of WikiLingua (Ar\u21d2En). Five graduate students are asked as evaluators to assess the generated summaries in a similar way to Section 3.3. For evaluators, the parallel documents in their mother tongue are also displayed to facilitate evaluation. As shown in Table 7, though using MT documents leads to better results in terms of automatic metrics, human evaluators prefer the summaries generated using HT documents as inputs. Thus, automatic metrics like ROUGE and BERTScore cannot capture human preferences if input documents are machine translated. Besides, translationese in source documents should also be controlled in the test sets."
        },
        {
            "heading": "4.4 Translationese\u2019s Impact on Training",
            "text": "When using HT documents as inputs, mBART-iHT outperforms mBART-iMT in both automatic and human evaluation (Table 6 and Table 7). Thus, the translationese in source documents during training has a negative impact on CLS model performance. However, different from the translationese in summaries, the translationese in documents do not affect the training objectives. Consequently, we wonder if it is possible to train a CLS model with both MT and HT documents and further improve the model performance. In this manner, MT documents are also utilized to build CLS models, benefiting the research on low-resource languages. We attempt the following strategies: (1) mBART-CL heuristically adopts a curriculum learning (Bengio et al., 2009) strategy to train a mBART model from \u27e8MT document, summary\u27e9 samples to \u27e8HT document, summary\u27e9 samples in each training epoch. (2) mBART-TT adopts the tagged training strategy (Caswell et al., 2019; Marie et al., 2020) to train a mBART model. The strategy has been studied in machine translation to improve the MT performance on low-resource source languages. In detail, the source inputs with high-level translationese (i.e., MT documents in our scenario) are prepended with a special token [TT]. For other inputs with low-level translationese (i.e., HT documents), they remain unchanged. Therefore, the special token explicitly tells the model these two types of inputs.\nAs shown in Table 6, both mBART-CL and mBART-TT outperform mBART-iHT in all three di-\nrections (according to the conclusion of our human evaluation, we only use HT documents as inputs to evaluate mBART-CL and mBART-TT). Besides, mBART-TT outperforms mBART-CL, confirming the superiority of tagged training in CLS. To give a deeper analysis of the usefulness of MT documents, we use a part of (10%, 30%, 50% and 70%, respectively) HT documents (paired with summaries) and all MT documents (paired with summaries) to jointly train mBART-TT model. Besides, we use the same part of HT documents to train mBART-iHT model for comparisons. Table 8 gives the experimental results. With the help of MT documents, mBART-TT only uses 50% of HT documents to achieve competitive results with mBARTiHT. Note that compared with HT documents, MT documents are much easier to obtain, thus the strategy is friendly to low-resource source languages."
        },
        {
            "heading": "5 Discussion and Suggestions",
            "text": "Based on the above investigation and findings, we conclude this work by presenting concrete suggestions to both the dataset and model developments. Controlling translationese in test sets. As we discussed in Section 3.3 and Section 4.3, the translationese in source documents or target summaries would lead to the inconsistency between automatic evaluation and human judgment. In addition, one should avoid directly adopting machinetranslated documents or summaries in the test sets of CLS datasets. To make the machine-translated documents or summaries suitable for evaluating model performance, some post-processing strategies should be conducted to reduce translationese. Prior work (Zhu et al., 2019) adopts post-editing strategy to manually correct the machine-translated summaries in their test sets. Though post-editing increases productivity and decreases errors compared to translation from scratch (Green et al., 2013), Toral (2019) finds that post-editing machine translation also has special stylistic which is different from native usage, i.e., post-editese. Thus, the post-\nediting strategy cannot reduce translationese. Future studies can explore other strategies to control translationese in CLS test sets. Building mixed-quality or semi-supervised CLS datasets. Since high-quality CLS pairs are difficult to collect (especially for low-resource languages), it is costly to ensure the quality of all samples in a large-scale CLS dataset. Future work could collect mix-quality CLS datasets that involve both high-quality and low-quality samples. In this manner, the collected datasets could encourage model development in more directions (e.g., the curriculum learning and tagged training strategies we discussed in Section 4.4) while controlling the cost. In addition, grounding the truth that monolingual summarization samples are much easier than CLS samples to collect under the same resources (Hasan et al., 2021b,a), semi-supervised datasets, which involve CLS samples and in-domain monolingual summarization samples, are also a good choice. Designing translationese-aware CLS models. It is inevitable to face translationese when training CLS models. The degree of translationese might be different in the document or summary of each training sample when faced with one of the following scenarios: (1) training multi-domain CLS models based on multiple CLS datasets; (2) training CLS models in low-resource languages based on mixed-quality datasets. In this situation, it is necessary to build translationese-aware CLS models. The tagged training strategy (Caswell et al., 2019) is a simple solution which only considers two-granularity translationese in source documents. It is more general to model the three-granularity translationese (i.e., natural text, HT text and MT text) in documents as well as summaries. Future work could attempt to (1) explicitly model the dif-\nferent degrees of translationese (such as prepending special tokens), or (2) implicitly let the CLS model be aware of different degrees of translationese (e.g., designing auxiliary tasks in multi-task learning)."
        },
        {
            "heading": "6 Related Work",
            "text": "Cross-Lingual Summarization. Cross-lingual summarization (CLS) aims to summarize sourcelanguage documents into a different target language. Due to data scarcity, early work typically focuses on pipeline methods (Leuski et al., 2003; Wan et al., 2010; Wan, 2011; Yao et al., 2015), i.e., translation and then summarization or summarization and then translation. Recently, many large-scale CLS datasets are proposed one after another. According to an extensive survey on CLS (Wang et al., 2022b), they can be divided into synthetic datasets and multi-lingual website datasets. Synthetic datasets (Zhu et al., 2019; Bai et al., 2021; Feng et al., 2022; Wang et al., 2022a) are constructed by translating monolingual summarization (MS) datasets. Multi-lingual website datasets (Ladhak et al., 2020; Perez-Beltrachini and Lapata, 2021) are collected from online resources. Based on these large-scale datasets, many researchers explore various ways to build CLS systems, including multi-task learning strategies (Cao et al., 2020; Liang et al., 2022), knowledge distillation methods (Nguyen and Luu, 2022; Liang et al., 2023a), resource-enhanced frameworks (Zhu et al., 2020) and pre-training techniques (Xu et al., 2020; Wang et al., 2022a, 2023b; Liang et al., 2023b). More recently, Wang et al. (2023a) explore zeroshot CLS by prompting large language models. Different from them, we are the first to investigate the influence of translationese on CLS. Translationese. Translated texts are known to\nhave special features which refer to \u201ctranslationese\u201d (Gellerstam, 1986). The phenomenon of translationese has been widely studied in machine translation (MT). Some researchers explore the influence of translationese on MT evaluation (Lembersky et al., 2012; Zhang and Toral, 2019; Graham et al., 2020; Edunov et al., 2020). To control the effect of translationese on MT models, tagged training (Caswell et al., 2019; Marie et al., 2020) is proposed to explicitly tell MT models if the given data is translated texts. Besides, Artetxe et al. (2020) and Yu et al. (2022) mitigate the effect of translationese in cross-lingual transfer learning."
        },
        {
            "heading": "7 Conclusion",
            "text": "In this paper, we investigate the influence of translationese on CLS. We design systematic experiments to investigate how translationese affects CLS model evaluation and performance when translationese appears in source documents or target summaries. Based on our findings, we also give suggestions for future dataset and model developments.\nEthical Considerations\nIn this paper, we use mBART-50 (Tang et al., 2021) as the CLS model in experiments. During finetuning, the adopted CLS samples mainly come from WikiLingua (Ladhak et al., 2020), XSAMSum (Wang et al., 2022a) and MSAMSum (Feng et al., 2022). Some CLS samples might contain translationese and flawed translations (provided by Google Translation). Therefore, the trained models might involve the same biases and toxic behaviors exhibited by these datasets and Google Translation.\nLimitations\nWhile we show the influence of translationese on CLS, there are some limitations worth considering in future work: (1) We do not analyze the effects of translationese when the translations appear in both source documents and target summaries; (2) Our experiments cover English, Chinese, Russian, Arabic, Czech and French, and future work could extend our method to more languages and give more comprehensive analyses w.r.t different language families."
        },
        {
            "heading": "Acknowledgements",
            "text": "This work is supported by the National Natural Science Foundation of China (No.62072323,\nU21A20488, 62206056), Shanghai Science and Technology Innovation Action Plan (No. 22511104700), and Key Projects of Industrial Foresight and Key Core Technology Research and Development in Suzhou (SYC2022009)."
        },
        {
            "heading": "A The Inconsistent Performance of mBART-HT",
            "text": "According to our conjecture, XSAMSum (En\u21d2Zh) should be more difficult than WikiLingua (En\u21d2Ru/Ar/Cs) for CLS models to perform. Consequently, factor (i) dominates in WikiLingua, while factor (ii) dominates in XSAMSum, leading to the inconsistent performance. To convince that, we illustrate the difficulty of each CLS dataset from the following aspects: (1) Scale calculates the number of CLS samples in each dataset. Generally, the more samples used to train a CLS model, the easier it is for the model to learn CLS. (2) Coverage measures the overlap rate between documents and summaries, which is defined as the average proportion of the copied bigram in summaries for each dataset.8 The higher coverage of a dataset, the less\n8Since the documents and summaries in CLS datasets are in different languages, the coverage is calculated based on\nsearch space for models to learn the mapping from source documents to target summaries. As shown in Table 9, XSAMSum is more difficult than other datasets based on the overall consideration from both aspects. To provide a deeper explanation of our conjecture, we further evenly split the test set of WikiLingua (En\u21d2Cs) into hard and simple subsets according to the coverage of each sample. The coverage of samples in the hard subset is less that in the simple subset. As shown in Table 10, the generated summaries of mBART-HT are closer to HT references on the simple subset, but more similar to MT references on the hard subset, demonstrating that the difficulty of CLS training data would lead to the inconsistent performance of mBART-HT.\nB Implementation Details\nThe implementation of mBART-50 (Tang et al., 2021) (610M parameters) used in our experiments is provided by the Huggingface Transformers.9 We fine-tune the model on NVIDIA Tesla V100 GPUs (32G) and set the learning rate to 5e-6, the warmup steps to 500, the epochs to 10, and the batch size is 4. The maximum number of tokens for input sequences is 1024. In the test process, beam size is set to 5. All experimental results listed in this paper are the average of 3 runs.\nTo calculate ROUGE scores, we employ the multilingual ROUGE toolkit10 that considers segmentation and stemming algorithms for various languages. To calculate BERTScore, we use the bertscore toolkit11."
        },
        {
            "heading": "C Human Evaluation",
            "text": "We tell our evaluators a brief guideline about three metrics: (1) Informativeness measures how informative the summary is. (2) Fluency measures how fluent, and grammatical the summary is. Is a summary well-written and grammatically correct? (3) Overall measures the overall quality of each generated summary. It can be judged under the consideration of informativeness, fluency, relevance, consistency and so on. Then, all evaluators are required to give each summary a score selected from \u201c1\u201d, \u201c2\u201d and \u201c3\u201d for each metric. When making the\nthe source-language summaries (i.e., English summaries in WikiLingua and XSAMSum) and source documents.\n9https://huggingface.co/facebook/ mbart-large-50-many-to-many-mmt\n10https://github.com/csebuetnlp/xl-sum/tree/ master/multilingual_rouge_scoring\n11https://github.com/Tiiiger/bert_score\njudgments, all summaries of a given document are provided for our evaluators simultaneously to let them make comparisons among different models (note that all evaluators do not know every summary is generated by which model, and the appearance order of summaries is shuffled). We do not provide detailed breakdown for each score in each metric due to the following reasons: (1) We encourage each evaluator to follow their actual feelings to make judgments since everyone in the real applications might have different criteria (for each metric). For example, someone might be sensitive to fluency while others might for informativeness. Thus, we want to make our human evaluation more in line with this real-world scenario. (2) It is hard and even unrealistic to construct a perfect quantitative human evaluation principle. (3) This evaluation method is commonly used in machine translation evaluation (Callison-Burch et al., 2007; Denkowski and Lavie, 2010), cross-lingual summarization evaluation (Zhu et al., 2020; Cao et al., 2020; Liang et al., 2022) and other tasks (Li et al., 2023b,a)."
        }
    ],
    "title": "Understanding Translationese in Cross-Lingual Summarization",
    "year": 2023
}