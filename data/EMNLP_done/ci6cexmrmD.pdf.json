{
    "abstractText": "Spatial reasoning in text plays a crucial role in various real-world applications. Existing approaches for spatial reasoning typically infer spatial relations from pure text, which overlook the gap between natural language and symbolic structures. Graph neural networks (GNNs) have showcased exceptional proficiency in inducing and aggregating symbolic structures. However, classical GNNs face challenges in handling multi-hop spatial reasoning due to the over-smoothing issue, i.e., the performance decreases substantially as the number of graph layers increases. To cope with these challenges, we propose a novel Depth-Wise Graph Neural Network (DepWiGNN). Specifically, we design a novel node memory scheme and aggregate the information over the depth dimension instead of the breadth dimension of the graph, which empowers the ability to collect long dependencies without stacking multiple layers. Experimental results on two challenging multihop spatial reasoning datasets show that DepWiGNN outperforms existing spatial reasoning methods. The comparisons with the other three GNNs further demonstrate its superiority in capturing long dependency in the graph.",
    "authors": [
        {
            "affiliations": [],
            "name": "Shuaiyi Li"
        },
        {
            "affiliations": [],
            "name": "Yang Deng"
        },
        {
            "affiliations": [],
            "name": "Wai Lam"
        },
        {
            "affiliations": [],
            "name": "Hong Kong"
        }
    ],
    "id": "SP:8b029145e59f18547d6e51c5764430be13e42f7d",
    "references": [
        {
            "authors": [
                "Yejin Bang",
                "Samuel Cahyawijaya",
                "Nayeon Lee",
                "Wenliang Dai",
                "Dan Su",
                "Bryan Wilie",
                "Holy Lovenia",
                "Ziwei Ji",
                "Tiezheng Yu",
                "Willy Chung",
                "Quyet V. Do",
                "Yan Xu",
                "Pascale Fung"
            ],
            "title": "A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hal",
            "year": 2023
        },
        {
            "authors": [
                "Yu Cao",
                "Meng Fang",
                "Dacheng Tao."
            ],
            "title": "BAG: bi-directional attention entity graph convolutional network for multi-hop reasoning question answering",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computa-",
            "year": 2019
        },
        {
            "authors": [
                "Howard Chen",
                "Alane Suhr",
                "Dipendra Misra",
                "Noah Snavely",
                "Yoav Artzi."
            ],
            "title": "TOUCHDOWN: natural language navigation and spatial reasoning in visual street environments",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition, CVPR",
            "year": 2019
        },
        {
            "authors": [
                "Kezhen Chen",
                "Qiuyuan Huang",
                "Hamid Palangi",
                "Paul Smolensky",
                "Kenneth D. Forbus",
                "Jianfeng Gao."
            ],
            "title": "Mapping natural-language problems to formal-language solutions using structured neural representations",
            "venue": "Proceedings of the 37th Inter-",
            "year": 2020
        },
        {
            "authors": [
                "Qi",
                "Wei Chu."
            ],
            "title": "Question directed graph attention network for numerical reasoning over text",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020, pages 6759\u2013",
            "year": 2020
        },
        {
            "authors": [
                "Ming Chen",
                "Zhewei Wei",
                "Zengfeng Huang",
                "Bolin Ding",
                "Yaliang Li."
            ],
            "title": "Simple and deep graph convolutional networks",
            "venue": "Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of",
            "year": 2020
        },
        {
            "authors": [
                "Surabhi Datta",
                "Yuqi Si",
                "Laritza Rodriguez",
                "Sonya E. Shooshan",
                "Dina Demner-Fushman",
                "Kirk Roberts"
            ],
            "title": "Understanding spatial language in radiology: Representation framework, annotation, and spatial relation extraction from chest x-ray reports",
            "year": 2020
        },
        {
            "authors": [
                "Mostafa Dehghani",
                "Stephan Gouws",
                "Oriol Vinyals",
                "Jakob Uszkoreit",
                "Lukasz Kaiser."
            ],
            "title": "Universal transformers",
            "venue": "7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net.",
            "year": 2019
        },
        {
            "authors": [
                "Yang Deng",
                "Shuaiyi Li",
                "Wai Lam."
            ],
            "title": "Learning to ask clarification questions with spatial reasoning",
            "venue": "Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2023, pages 2113\u20132117.",
            "year": 2023
        },
        {
            "authors": [
                "Yang Deng",
                "Yuexiang Xie",
                "Yaliang Li",
                "Min Yang",
                "Wai Lam",
                "Ying Shen."
            ],
            "title": "Contextualized knowledge-aware attentive neural network: Enhancing answer selection with knowledge",
            "venue": "ACM Trans. Inf. Syst., 40(1):2:1\u20132:33.",
            "year": 2022
        },
        {
            "authors": [
                "Yang Deng",
                "Wenxuan Zhang",
                "Weiwen Xu",
                "Ying Shen",
                "Wai Lam."
            ],
            "title": "Nonfactoid question answering as query-focused summarization with graphenhanced multihop inference",
            "venue": "IEEE Transactions on Neural Networks and Learning Systems.",
            "year": 2023
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
            "year": 2019
        },
        {
            "authors": [
                "Yuwei Fang",
                "Siqi Sun",
                "Zhe Gan",
                "Rohit Pillai",
                "Shuohang Wang",
                "Jingjing Liu."
            ],
            "title": "Hierarchical graph network for multi-hop question answering",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP",
            "year": 2020
        },
        {
            "authors": [
                "William L. Hamilton",
                "Zhitao Ying",
                "Jure Leskovec."
            ],
            "title": "Inductive representation learning on large graphs",
            "venue": "Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9,",
            "year": 2017
        },
        {
            "authors": [
                "Yu-Jung Heo",
                "Eun-Sol Kim",
                "Woo Suk Choi",
                "Byoung-Tak Zhang."
            ],
            "title": "Hypergraph transformer: Weakly-supervised multi-hop reasoning for knowledge-based visual question answering",
            "venue": "Proceedings of the 60th Annual Meeting of the Associa-",
            "year": 2022
        },
        {
            "authors": [
                "Sepp Hochreiter",
                "J\u00fcrgen Schmidhuber."
            ],
            "title": "Long short-term memory",
            "venue": "Neural Comput., 9(8):1735\u2013 1780.",
            "year": 1997
        },
        {
            "authors": [
                "Giwon Hong",
                "Jeonghwan Kim",
                "Junmo Kang",
                "SungHyon Myaeng."
            ],
            "title": "Graph-induced transformers for efficient multi-hop question answering",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022,",
            "year": 2022
        },
        {
            "authors": [
                "Qiuyuan Huang",
                "Paul Smolensky",
                "Xiaodong He",
                "Li Deng",
                "Dapeng Oliver Wu."
            ],
            "title": "Tensor product generation networks for deep NLP modeling",
            "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computa-",
            "year": 2018
        },
        {
            "authors": [
                "Rui Huang",
                "Ping Li."
            ],
            "title": "Hub-hub connections matter: Improving edge dropout to relieve oversmoothing in graph neural networks",
            "venue": "Knowl. Based Syst., 270:110556.",
            "year": 2023
        },
        {
            "authors": [
                "Yongjie Huang",
                "Meng Yang."
            ],
            "title": "Breadth first reasoning graph for multi-hop question answering",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
            "year": 2021
        },
        {
            "authors": [
                "Diederik P. Kingma",
                "Jimmy Ba."
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings.",
            "year": 2015
        },
        {
            "authors": [
                "Thomas N. Kipf",
                "Max Welling."
            ],
            "title": "Semisupervised classification with graph convolutional networks",
            "venue": "5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings.",
            "year": 2017
        },
        {
            "authors": [
                "Yeskendir Koishekenov."
            ],
            "title": "Reducing oversmoothing in graph neural networks using relational embeddings",
            "venue": "CoRR, abs/2301.02924.",
            "year": 2023
        },
        {
            "authors": [
                "Parisa Kordjamshidi",
                "Martijn van Otterlo",
                "MarieFrancine Moens."
            ],
            "title": "Spatial role labeling: Task definition and annotation scheme",
            "venue": "Proceedings of the International Conference on Language Resources and Evaluation, LREC 2010, 17-23 May 2010, Val-",
            "year": 2010
        },
        {
            "authors": [
                "Zhenzhong Lan",
                "Mingda Chen",
                "Sebastian Goodman",
                "Kevin Gimpel",
                "Piyush Sharma",
                "Radu Soricut."
            ],
            "title": "ALBERT: A lite BERT for self-supervised learning of language representations",
            "venue": "8th International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "Hung Le",
                "Truyen Tran",
                "Svetha Venkatesh."
            ],
            "title": "Self-attentive associative memory",
            "venue": "Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning",
            "year": 2020
        },
        {
            "authors": [
                "Qimai Li",
                "Zhichao Han",
                "Xiao-Ming Wu."
            ],
            "title": "Deeper insights into graph convolutional networks for semi-supervised learning",
            "venue": "Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications",
            "year": 2018
        },
        {
            "authors": [
                "Yang Liu",
                "Chuan Zhou",
                "Shirui Pan",
                "Jia Wu",
                "Zhao Li",
                "Hongyang Chen",
                "Peng Zhang."
            ],
            "title": "Curvdrop: A ricci curvature based approach to prevent graph neural networks from over-smoothing and oversquashing",
            "venue": "Proceedings of the ACM Web Confer-",
            "year": 2023
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Roberta: A robustly optimized BERT pretraining approach",
            "venue": "CoRR, abs/1907.11692.",
            "year": 2019
        },
        {
            "authors": [
                "Qian Luo",
                "Yunfei Li",
                "Yi Wu."
            ],
            "title": "Grounding object relations in language-conditioned robotic manipulation with semantic-spatial reasoning",
            "venue": "CoRR, abs/2303.17919.",
            "year": 2023
        },
        {
            "authors": [
                "Wouter Massa",
                "Parisa Kordjamshidi",
                "Thomas Provoost",
                "Marie-Francine Moens."
            ],
            "title": "Machine reading of biological texts - bacteria-biotope extraction",
            "venue": "BIOINFORMATICS 2015 - Proceedings of the International Conference on Bioinformatics Models,",
            "year": 2015
        },
        {
            "authors": [
                "Yimeng Min",
                "Frederik Wenkel",
                "Guy Wolf."
            ],
            "title": "Scattering GCN: overcoming oversmoothness in graph convolutional networks",
            "venue": "Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020,",
            "year": 2020
        },
        {
            "authors": [
                "Roshanak Mirzaee",
                "Hossein Rajaby Faghihi",
                "Qiang Ning",
                "Parisa Kordjamshidi."
            ],
            "title": "SPARTQA: A textual question answering benchmark for spatial reasoning",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association",
            "year": 2021
        },
        {
            "authors": [
                "Roshanak Mirzaee",
                "Parisa Kordjamshidi."
            ],
            "title": "Transfer learning with synthetic corpora for spatial role labeling and reasoning",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi,",
            "year": 2022
        },
        {
            "authors": [
                "Christopher Morris",
                "Martin Ritzert",
                "Matthias Fey",
                "William L. Hamilton",
                "Jan Eric Lenssen",
                "Gaurav Rattan",
                "Martin Grohe."
            ],
            "title": "Weisfeiler and leman go neural: Higher-order graph neural networks",
            "venue": "The Thirty-Third AAAI Conference on Artificial Intel-",
            "year": 2019
        },
        {
            "authors": [
                "Rasmus Berg Palm",
                "Ulrich Paquet",
                "Ole Winther."
            ],
            "title": "Recurrent relational networks",
            "venue": "Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018,",
            "year": 2018
        },
        {
            "authors": [
                "Lin Qiu",
                "Yunxuan Xiao",
                "Yanru Qu",
                "Hao Zhou",
                "Lei Li",
                "Weinan Zhang",
                "Yong Yu."
            ],
            "title": "Dynamically fused graph network for multi-hop reasoning",
            "venue": "Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence,",
            "year": 2019
        },
        {
            "authors": [
                "Adam Santoro",
                "David Raposo",
                "David G.T. Barrett",
                "Mateusz Malinowski",
                "Razvan Pascanu",
                "Peter W. Battaglia",
                "Tim Lillicrap."
            ],
            "title": "A simple neural network module for relational reasoning",
            "venue": "Advances in Neural Information Processing Systems 30:",
            "year": 2017
        },
        {
            "authors": [
                "Imanol Schlag",
                "Tsendsuren Munkhdalai",
                "J\u00fcrgen Schmidhuber."
            ],
            "title": "Learning associative inference using fast weight memory",
            "venue": "9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net.",
            "year": 2021
        },
        {
            "authors": [
                "Imanol Schlag",
                "J\u00fcrgen Schmidhuber."
            ],
            "title": "Learning to reason with third order tensor products",
            "venue": "Advances in Neural Information Processing Systems",
            "year": 2018
        },
        {
            "authors": [
                "Imanol Schlag",
                "J\u00fcrgen Schmidhuber."
            ],
            "title": "Learning to reason with third order tensor products",
            "venue": "Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8,",
            "year": 2018
        },
        {
            "authors": [
                "Zhengxiang Shi",
                "Qiang Zhang",
                "Aldo Lipani."
            ],
            "title": "Stepgame: A new benchmark for robust multi-hop spatial reasoning in texts",
            "venue": "Thirty-Sixth AAAI Conference on Artificial Intelligence, AAAI 2022, pages 11321\u201311329.",
            "year": 2022
        },
        {
            "authors": [
                "Paul Smolensky."
            ],
            "title": "Tensor product variable binding and the representation of symbolic structures in connectionist systems",
            "venue": "Artif. Intell., 46(1-2):159\u2013216.",
            "year": 1990
        },
        {
            "authors": [
                "Yunchong Song",
                "Chenghu Zhou",
                "Xinbing Wang",
                "Zhouhan Lin."
            ],
            "title": "Ordered GNN: ordering message passing to deal with heterophily and oversmoothing",
            "venue": "CoRR, abs/2302.01524.",
            "year": 2023
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N. Gomez",
                "Lukasz Kaiser",
                "Illia Polosukhin."
            ],
            "title": "Attention is all you need",
            "venue": "Advances in Neural Information Processing Systems 30: Annual Conference on Neural",
            "year": 2017
        },
        {
            "authors": [
                "Petar Velickovic",
                "Guillem Cucurull",
                "Arantxa Casanova",
                "Adriana Romero",
                "Pietro Li\u00f2",
                "Yoshua Bengio."
            ],
            "title": "Graph attention networks",
            "venue": "CoRR, abs/1710.10903.",
            "year": 2017
        },
        {
            "authors": [
                "Sagar Gubbi Venkatesh",
                "Anirban Biswas",
                "Raviteja Upadrashta",
                "Vikram Srinivasan",
                "Partha Talukdar",
                "Bharadwaj Amrutur."
            ],
            "title": "Spatial reasoning from natural language instructions for robot manipulation",
            "venue": "IEEE International Conference on Robotics and",
            "year": 2021
        },
        {
            "authors": [
                "Guangtao Wang",
                "Rex Ying",
                "Jing Huang",
                "Jure Leskovec."
            ],
            "title": "Multi-hop attention graph neural networks",
            "venue": "Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI 2021, pages 3089\u20133096. ijcai.org.",
            "year": 2021
        },
        {
            "authors": [
                "Jason Weston",
                "Antoine Bordes",
                "Sumit Chopra",
                "Tom\u00e1s Mikolov."
            ],
            "title": "Towards ai-complete question answering: A set of prerequisite toy tasks",
            "venue": "4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016,",
            "year": 2016
        },
        {
            "authors": [
                "Gongce Wu",
                "Shukuan Lin",
                "Xiaoxue Shao",
                "Peng Zhang",
                "Jianzhong Qiao."
            ],
            "title": "QPGCN: graph convolutional network with a quadratic polynomial filter for overcoming over-smoothing",
            "venue": "Appl. Intell., 53(6):7216\u20137231.",
            "year": 2023
        },
        {
            "authors": [
                "Weiwen Xu",
                "Yang Deng",
                "Huihui Zhang",
                "Deng Cai",
                "Wai Lam."
            ],
            "title": "Exploiting reasoning chains for multi-hop science question answering",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2021, pages 1143\u20131156.",
            "year": 2021
        },
        {
            "authors": [
                "Weiwen Xu",
                "Huihui Zhang",
                "Deng Cai",
                "Wai Lam."
            ],
            "title": "Dynamic semantic graph construction and reasoning for explainable multi-hop science question answering",
            "venue": "Findings of the Association for Computational Linguistics: ACL/IJCNLP 2021, Online",
            "year": 2021
        },
        {
            "authors": [
                "Rui Yang",
                "Wenrui Dai",
                "Chenglin Li",
                "Junni Zou",
                "Hongkai Xiong."
            ],
            "title": "Tackling over-smoothing in graph convolutional networks with em-based joint topology optimization and node classification",
            "venue": "IEEE Trans. Signal Inf. Process. over Networks, 9:123\u2013139.",
            "year": 2023
        },
        {
            "authors": [
                "Yue Zhang",
                "Quan Guo",
                "Parisa Kordjamshidi."
            ],
            "title": "Towards navigation by reasoning over spatial configurations",
            "venue": "CoRR, abs/2105.06839.",
            "year": 2021
        },
        {
            "authors": [
                "Yue Zhang",
                "Parisa Kordjamshidi."
            ],
            "title": "Explicit object relation alignment for vision and language navigation",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop, ACL 2022, Dublin, Ire-",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Spatial reasoning in text is crucial and indispensable in many areas, e.g., medical domain (Datta et al., 2020; Massa et al., 2015), navigations (Zhang et al., 2021; Zhang and Kordjamshidi, 2022; Chen et al., 2019) and robotics (Luo et al., 2023; Venkatesh et al., 2021). It has been demonstrated to be a challenging problem for both modern pretrained language models (PLMs) (Mirzaee et al., 2021; Deng et al., 2023a) and large language models (LLMs) like ChatGPT (Bang et al., 2023). However, early textual spatial reasoning datasets, e.g.,\n\u2217This work is substantially supported by a grant from the Research Grant Council of the Hong Kong Special Administrative Region, China (Project Code: 14200719)\n\u2020 Corresponding author.\nbAbI (Weston et al., 2016), suffer from the issue of over-simplicity and, therefore, is not qualified for revealing real textual spatial reasoning scenario. Recently, researchers propose several new benchmarks (Shi et al., 2022; Mirzaee and Kordjamshidi, 2022; Mirzaee et al., 2021) with an increased level of complexity, which involve more required reasoning steps, enhanced variety of spatial relation expression, and more. As shown in Figure 1, 4 steps of reasoning are required to answer the question, and spatial relation descriptions and categories are diverse.\nTo tackle the problem of multi-hop spatial reasoning, Shi et al. (2022) propose a recurrent memory network based on Tensor Product Representation (TPR) (Schlag and Schmidhuber, 2018a), which mimics the step-by-step reasoning by iteratively updating and removing episodes from memory. Specifically, TPR encodes symbolic knowledge hidden in natural language into distributed vector space to be used for deductive reasoning. Despite the effectiveness of applying TPR memory, the performance of this model is overwhelmed by modern PLMs (Mirzaee and Kordjamshidi, 2022). Moreover, these works typically overlook the gap between natural language and symbolic relations.\nGraph Neural Neural Networks (GNNs) have been considerably used in multi-hop reasoning (Xu et al., 2021b; Chen et al., 2020b; Qiu et al., 2019).\nThese methods often treat a single graph convolutional layer of node information propagation (node to its immediate neighbors) in GNN as one step of reasoning and expand it to multi-hop reasoning by stacking multiple layers. However, increasing the number of graph convolutional layers in deep neural structures can have a detrimental effect on model performance (Li et al., 2018). This phenomenon, known as the over-smoothing problem, occurs because each layer of graph convolutions causes adjacent nodes to become more similar to each other. This paradox poses a challenge for multi-hop reasoning: although multiple layers are needed to capture multi-hop dependencies, implementing them can fail to capture these dependencies due to the over-smoothing problem. Furthermore, many chain-finding problems, e.g., multihop spatial reasoning, only require specific depth path information to solve a single question and do not demand full breadth aggregation for all neighbors (Figure 1). Nevertheless, existing methods (Palm et al., 2018; Xu et al., 2021b; Chen et al., 2020b; Deng et al., 2022) for solving this kind of problem usually lie in the propagation conducted by iterations of breadth aggregation, which brings superfluous and irrelevant information that may distract the model from the key information.\nIn light of these challenges, we propose a novel graph-based method, named Depth-Wise Graph Neural Network (DepWiGNN), which operates over the depth instead of the breadth dimension of the graph to tackle the multi-hop spatial reasoning problem. It introduces a novel node memory implementation that only stores depth path information between nodes by applying the TPR technique. Specifically, it first initializes the node memory by filling the atomic information (spatial relation) between each pair of directly connected nodes, and then collects the relation between two indirectly connected nodes via depth-wisely retrieving and aggregating all atomic spatial relations reserved in the memory of each node in the path. The collected long-dependency information is further stored in the source node memory in the path and can be retrieved freely if the target node is given. Unlike typical existing GNNs (Morris et al., 2019; Velickovic et al., 2017; Hamilton et al., 2017; Kipf and Welling, 2017), DepWiGNN does not need to be stacked to gain long relationships between two distant nodes and, hence, is immune to the over-smoothing problem. Moreover, instead of\naimlessly performing breadth aggregation on all immediate neighbors, it selectively prioritizes the key path information.\nExperiments on two challenging multi-hop spatial reasoning datasets show that DepWiGNN not only outperforms existing spatial reasoning methods, but also enhances the spatial reasoning capability of PLMs. The comparisons with three GNNs verify that DepWiGNN surpasses classical graph convolutional layers in capturing long dependencies by a noticeable margin without harming the performance of short dependency collection.\nOverall, our contributions are threefold:\n\u2022 We propose a novel graph-based method, DepWiGNN, to perform propagation over the depth dimension of a graph, which can capture long dependency without the need of stacking layers and avoid the issue of over-smoothing.\n\u2022 We implement a novel node memory scheme, which takes advantage of TPR mechanism, enabling convenient memory updating and retrieval operations through simple arithmetic operations instead of neural layers.\n\u2022 DepWiGNN excels in multi-hop spatial reasoning tasks, surpassing existing methods in experimental evaluations on two challenging datasets. Besides, comparisons with three other GNNs highlight its superior ability to capture long dependencies within the graph. Our code will be released via https://github.com/Syon-Li/ DepWiGNN."
        },
        {
            "heading": "2 Related works",
            "text": "Spatial Reasoning In Text has experienced a thriving development in recent years, supported by several benchmark datasets. Weston et al. (2016) proposes the bAbI project, which contains 20 QA tasks, including one focusing on textual spatial reasoning. However, some issues exist in bAbI, such as data leakage, overly short reasoning steps, and monotony of spatial relation categories and descriptions, which makes it fails to reflect the intricacy of spatial reasoning in natural language (Shi et al., 2022). Targeting these shortages, StepGame (Shi et al., 2022) expands the spatial relation types, the diversity of relation descriptions, and the required reasoning steps. Besides, SPARTQA (Mirzaee et al., 2021) augments the number of question types from one to four: find relation (FR), find blocks (FB), choose objects (CO), and yes/no (YN), while\nSPARTUN (Mirzaee and Kordjamshidi, 2022) only has two question types (FR, YN) built with broader coverage of spatial relation types.\nTensor Product Representation (TPR) (Schlag and Schmidhuber, 2018a) is a mechanism to encode symbolic knowledge into a vector space, which can be applied to various natural language reasoning tasks (Huang et al., 2018; Chen et al., 2020a). For example, Schlag and Schmidhuber (2018a) perform reasoning by deconstructing the language into combinatorial symbolic representations and binding them using Third-order TPR, which can be further combined with RNN to improve the model capability of making sequential inference (Schlag et al., 2021). Shi et al. (2022) used a paragraph-level, TPR memory-augmented way to implement complex multi-hop spatial reasoning. However, existing methods typically apply TPR to pure text, which neglects the gap between natural language and symbolic structures.\nGraph Neural Networks (GNNs) (Morris et al., 2019; Velickovic et al., 2017; Hamilton et al., 2017; Kipf and Welling, 2017) have been demonstrated to be effective in inducing and aggregating symbolic structures on other multi-hop question answering problems (Cao et al., 2019; Fang et al., 2020; Huang and Yang, 2021; Heo et al., 2022; Xu et al., 2021a; Deng et al., 2023b). In practice, the required number of graph layers grows with the multi-hop dependency between two distant nodes (Wang et al., 2021; Hong et al., 2022), which inevitably encounters the problem of oversmoothing (Li et al., 2018). Some researchers have conducted studies on relieving this problem (Wu et al., 2023; Min et al., 2020; Huang and Li, 2023; Yang et al., 2023; Liu et al., 2023; Koishekenov, 2023; Song et al., 2023). However, these methods are all breadth-aggregation-based, i.e., they only posed adjustments on breadth aggregation like improving the aggregation filters, scattering the aggregation target using the probabilistic tool, etc., but never jumping out it. In this work, we investigate a depth-wise aggregation approach that captures long-range dependencies across any distance without the need to increase the model depth."
        },
        {
            "heading": "3 Method",
            "text": ""
        },
        {
            "heading": "3.1 Problem Definition",
            "text": "Following previous studies on spatial reasoning in text (Mirzaee et al., 2021; Shi et al., 2022; Mirzaee\nand Kordjamshidi, 2022), we define the problem as follows: Given a story description S consisting of multiple sentences, the system aims to answer a question Q based on the story S by selecting a correct answer from the fixed set of given candidate answers regarding the spatial relations."
        },
        {
            "heading": "3.2 The DepWiNet",
            "text": "As presented in Figure 2, the overall framework named DepWiNet consists of three modules: the entity representation extraction module, the DepWiGNN reasoning module, and the prediction module. The entity representation extraction module provides comprehensive entity embedding that is used in the reasoning module. A graph with recognized entities as nodes is constructed after obtaining entity representations, which later will be fed into DepWiGNN. The DepWiGNN reasons over the constructed graph and updates the node embedding correspondingly. The final prediction module adds the entity embedding from DepWiGNN to the embeddings from the first extraction module and applies a single step of attention (Vaswani et al., 2017) to generate the final result.\nEntity Representation Extraction Module We leverage PLMs to extract entity representations1. The model takes the concatenation of the story S and the question Q as the input and output embeddings of each token. The output embeddings are further projected using a single linear layer.\nS\u0302, Q\u0302 = PLM(S,Q) (1)\nS\u0302\u03b1 = S\u0302W\u03b1 T Q\u0302\u03b1 = Q\u0302W\u03b1 T (2)\nwhere PLM(\u00b7) denotes the PLM-based encoder, e.g., BERT (Devlin et al., 2019). S\u0302\u03b1 \u2208 RrS\u00d7dh , Q\u0302\u03b1 \u2208 RrQ\u00d7dh denotes the list of projected tokens of size dh in the story and question, and W\u03b1 \u2208 Rdh\u00d7dh is the shared projection matrix. The entity representation is just the mean pooling of all token embeddings belonging to that entity.\nGraph Construction The entities are first recognized from the input by employing rule-based entity recognition. Specifically, in StepGame (Shi et al., 2022), the entities are represented by a single capitalized letter, so we only need to locate all single capitalized letters. For SPARTUN\n1Entity in this paper refers to the spatial roles defined in(Kordjamshidi et al., 2010).\nand ReSQ, we use nltk RegexpParser2 with selfdesigned grammars3 to recognize entities. Entities and their embeddings are treated as nodes of the graph, and an edge between two entities exists if and only if the two entities co-occur in the same sentence. We also add an edge feature for each edge.\nEij = { 0 if i == j, e[CLS] ortherwise.\n(3)\nIf two entities are the same (self-loop), the edge feature is just a zero tensor with a size dh; otherwise, it is the sequence\u2019s last layer hidden state of the [CLS] token. The motivation behind treating [CLS] token as an edge feature is that it can help the model better understand the atomic relation (k=1) between two nodes (entities) so as to facilitate later depth aggregation. A straightforward justification can be found in Table 1, where all three PLMs achieve very high accuracy at k=1 cases by using the [CLS] token, which demonstrates that the [CLS] token favors the understanding of the atomic relation.\nDepWiGNN Module The DepWiGNN module (middle part in Figure 2) is responsible for multihop reasoning and can collect arbitrarily distant dependency without layer stacking.\nV\u0302 = DepWiGNN(G;V,E) (4) 2https://www.nltk.org/api/nltk.chunk.regexp.html 3r\"\"\"\"NP:<ADJ|NOUN>+<NOUN|NUM>+\"\"\" and\nr\"\"\"\"NP:<ADJ|NOUN>+<VERB>+<NOUN|NUM>\"\"\"\nwhere V \u2208 R|V |\u00d7dh and E \u2208 R|E|\u00d7dh are the nodes and edges of the graph. It comprises three components: node memory initialization, long dependency collection, and spatial relation retrieval. Details of these three components will be discussed in Section 3.3.\nPrediction Module The node embedding updated in DepWiGNN is correspondingly added to entity embedding extracted from PLM.\nZ\u0302i = { Zi + V\u0302idx(i) if i-th token \u2208 V\u0302 Zi ortherwise.\n(5)\nwhere Zi = [S\u0302; Q\u0302] is the ith token embedding from PLM and V\u0302 denotes the updated entity representation set. idx(i) is the index of i-th token in the graph nodes.\nThen, the sequence of token embeddings in the question and story are extracted separately to perform attention mechanism (Vaswani et al., 2017) with the query to be the sequence of question tokens embeddings Z\u0302Q\u0302, key and value to be the sequence of story token embeddings Z\u0302S\u0302.\nR = rQ\u2211 i=0 (softmax( Z\u0302Q\u0302Z\u0302 T S\u0302\u221a dh )Z\u0302S\u0302)i (6)\nC = FFN(layernm(R)) (7)\nwhere layernm means layer normalization and C is the final logits. The result embeddings are summed over the first dimension, layernormed, and"
        },
        {
            "heading": "1. Node initialization 2. Long dependency collection 3. Spatial relation retrieval",
            "text": "fed into a 3-layer feedforward neural network to acquire the final result. The overall framework is trained in an end-to-end manner to minimize the cross-entropy loss between the predicted candidate answer probabilities and the ground-truth answer."
        },
        {
            "heading": "3.3 Depth-wise Graph Neural Network",
            "text": "As illustrated in Figure 3, we introduce the proposed graph neural network, called DepWiGNN, i.e., the operation V\u0302 = DepWiGNN(G;V,E). Unlike existing GNNs (e.g. (Morris et al., 2019; Velickovic et al., 2017; Hamilton et al., 2017)), which counts the one-dimension node embedding itself as its memory to achieve the function of information reservation, updating, and retrieval, DepWiGNN employs a novel two-dimension node memory implementation approach that takes the advantage of TPR mechanism allowing the updating and retrieval operations of the memory to be conveniently realized by simple arithmetic operations like plus, minus and outer product. This essentially determines that the information propagation between any pair of nodes with any distance in the graph can be accomplished without the demand to iteratively apply neural layers.\nNode Memory Initialization At this stage, the node memories of all nodes are initialized with the relations to their immediate neighbors. In the multihop spatial reasoning cases, the relations will be the atomic spatial orientation relations (which only needs one hop) of the destination node relative to the source node. For example, \"X is to the left of K and is on the same horizontal plane.\" We follow the TPR mechanism (Smolensky, 1990), which uses outer product operation to bind roles and fillers4. In this work, the calculated spatial vectors are considered to be the fillers. They will be bound with corresponding node embeddings and stored in the two-dimensional node memory. Explicitly, we first\n4The preliminary of TPR is presented in Appendix A.\nacquire spatial orientation filler fij \u2208 Rdh by using a feedforward network, the input to FFN is concatenation in the form [Vi, Eij , Vj ]. Vi represents i-th node embedding.\nfij = FFN([Vi, Eij , Vj ]) (8) Mi = \u2211\nVj\u2208N(Vi) fij \u2297 Vj (9)\nThe filler is bound together with the corresponding destination node Vj using the outer product. The initial node memory Mi for node Vi is just the summation of all outer product results of the fillers and corresponding neighbors (left part in Figure 3).\nLong Dependency Collection We discuss how the model collects long dependencies in this section. Since the atomic relations are bound by corresponding destinations and have already been contained in the node memories, we can easily unbind all the atomic relation fillers in a path using the corresponding atomic destination node embedding (middle part in Figure 3). For each pair of indirectly connected nodes, we first find the shortest path between them using breadth-first search (BFS). Then, all the existing atomic relation fillers along the path are unbound using the embedding of each node in the path (Eq.10).\nf\u0302pi(pi+1) = MpiV T pi+1 (10)\nF\u0302 = Aggregator(F) (11)\nfp0pn = layernm(FFN(F\u0302) + F\u0302) (12)\nMp0 = Mp0 + fp0pn \u2297 V Tpn (13)\nwhere pi denotes the i-th element in the path and F = [f\u0302p0p1 ; ....; f\u0302pn\u22121pn ] is the retrived filler set along the path. The collected relation fillers are aggregated using a selected depth aggregator like LSTM (Hochreiter and Schmidhuber, 1997), etc, and passed to a feedforward neural network to reason the relation filler between the source and destination node in the path (Eq.12). The result spatial filler is then bound with the target node embedding and added to the source node memory (Eq.13). In this way, each node memory will finally contain the spatial orientation information to every other connected node in the graph.\nSpatial Relation Retrieval After the collection process is completed, every node memory contains spatial relation information to all other connected nodes in the graph. Therefore, we can conveniently retrieve the spatial information from a source node\nto a target node by unbinding the spatial filler from the source node memory (right part in Figure 3) using a self-determined key. The key can be the target node embedding itself if the target node can be easily recognized from the question, or some computationally extracted representation from the sequence of question token embeddings if the target node is hard to discern. We use the key to unbind the spatial relation from all nodes\u2019 memory and pass the concatenation of it with source and target node embeddings to a multilayer perceptron to get the updated node embeddings.\nf\u0302i[key] = MiV T [key] (14)\nV\u0302i = FFN([Vi, layernm(fi[key]), V[key]]) (15)\nThe updated node embeddings are then passed to the prediction module to get the final result."
        },
        {
            "heading": "4 Experiments",
            "text": ""
        },
        {
            "heading": "4.1 Experimental Setups",
            "text": "Datasets & Evaluation Metrics We investigated our model on StepGame (Shi et al., 2022) and ReSQ (Mirzaee and Kordjamshidi, 2022) datasets, which were recently published for multi-hop spatial reasoning. StepGame is a synthetic textual QA dataset that has a number of relations (k) ranging from 1 to 10. In particular, we follow the experimental procedure in the original paper (Shi et al., 2022), which utilized the TrainVersion of the dataset containing 10,000/1,000 training/validation clean samples for each k value from 1 to 5 and 10,000 noisy test examples for k value from 1 to 10. The test set contains three kinds of distraction noise: disconnected, irrelevant, and supporting. ReSQ is a crowdsourcing benchmark that includes only Yes/No questions with 1008, 333, and 610 examples for training, validating, and testing respectively. Since it is human-generated, it can reflect the natural complexity of real-world spatial description. Following the setup in (Shi et al., 2022; Mirzaee and Kordjamshidi, 2022), we report the accuracy on corresponding test sets for all experiments.\nBaselines For StepGame, we select all traditional reasoning models used in the original paper (Shi et al., 2022) and three PLMs, i.e., BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), and ALBERT (Lan et al., 2020) as our baselines. For ReSQ, we also follow the experiment setting described in (Mirzaee and Kordjamshidi, 2022),\nwhich used BERT with or without further synthetic supervision as baselines.\nImplementation Details For all experiments, we use the base version of corresponding PLMs, which has 768 embedding dimensions. The model was trained in an end-to-end manner using Adam optimizer (Kingma and Ba, 2015). The training was stopped if, up to 3 epochs, there is no improvement greater than 1e-3 on the cross-entropy loss for the validation set. We also applied a Pytorch training scheduler that reduces the learning rate with a factor of 0.1 if the improvement of cross-entropy loss on the validation set is lower than 1e-3 for 2 epochs. In terms of the determination of the key in the Spatial Relation Retrieval part, we used the target node embedding for StepGame since it can be easily recognized, and we employed a single linear layer to extract the key representation from the sum-aggregated question token embeddings for ReSQ. In the StepGame experiment, we fine-tune the model on the training set and test it on the test set. For ReSQ, we follow the procedure in (Mirzaee and Kordjamshidi, 2022) to test the model on ReSQ with or without further supervision from SPARTUN (Mirzaee and Kordjamshidi, 2022). Unless specified, all the experiments use LSTM (Hochreiter and Schmidhuber, 1997) as depth aggregator by default. The detailed hyperparameter settings are given in the Appendix B."
        },
        {
            "heading": "4.2 Overall Performance",
            "text": "Table 1 and 2 report the experiment results on StepGame and ReSQ respectively. As shown in Table 1, PLMs overwhelmingly outperform all the traditional reasoning models and the proposed DepWiNet overtakes the PLMs by a large margin, especially for the cases with greater k where the multi-hop reasoning capability plays a more important role. This aligns with the characteristics of our model architecture that the aggregation focuses on the depth dimension, which effectively avoids the problem of over-smoothing and the mixture of redundant information from the breadth aggregation. Despite the model being only trained on clean distraction-noise-absent samples with k from 1 to 5, it achieves impressive performance on the distraction-noise-present test data with k value from 6 to 10, demonstrating the more advanced generalization capability of our model. Moreover, our method also entails an improvement for the examples with lower k values for BERT and AL-\nBERT and an ignorable decrease for RoBERTa, which attests to the innocuity of our model to the few-hop reasoning tasks.\nExperimental results in Table 2 show that DepWiNet reaches a new SOTA result on ReSQ for both cases where extra supervision from SPARTUN is present and absent. Excitingly, the performance of our model without extra supervision even overwhelms the performance of the BERT with extra supervision from SPARTQA-AUTO (Mirzaee et al., 2021), StepGame, and SPARTUN-S and approaches closely to the SPARTUN supervision case. All these phenomenons signify that our model has the potential to better tackle the natural intricacy of real-world spatial expressions."
        },
        {
            "heading": "4.3 Ablation study",
            "text": "We conduct ablation studies on the impact of the three components of DepWiGNN and different depth aggregators, as presented in Table 3.\nImpact of DepWiNet components The model performance experiences a drastic decrease, particularly for the mean score of k between 6 and 10, after the Long Dependency Collection (LDC) has been removed, verifying that this component serves a crucial role in the model. Note that the mean score for k(6-10) even drops below the ALBERT baseline (Table 1). This is reasonable as the LDC is directly responsible for collecting long dependencies. We further defunctionalized the Node Memory Initialization (NMI) and then Spatial Relation Retrieval (SRR) by setting the initial fillers (Eq.8) and key representation (Eq.14) to a random vector separately. Compared with the case where only LDC was removed, both of them lead to a further decrease in small and large k values.\nImpact of Different Aggregators The results show that the mean and max pooling depth aggregators fail to understand the spatial rule as achieved by LSTM. This may be caused by the relatively less expressiveness and generalization caliber of mean and max pooling operation."
        },
        {
            "heading": "4.4 Comparisons with Different GNNs",
            "text": "To certify our model\u2019s capacity of collecting long dependencies as well as its immunity to the oversmoothing problem, we contrast it with four graph neural networks, namely, GCN (Kipf and Welling, 2017), GraphConv (Morris et al., 2019), GAT (Velickovic et al., 2017) and GCNII (Chen et al., 2020c). We consider the cases with the number of layers varying from 1 to 5 and select the best performance to compare. The plot of the accuracy metric is reported in Figure 4 and the corresponding best\ncases are in Table 4. It is worth noting that almost all the baseline GNNs cause a reduction in the original PLM performance (Table 4). The reason for this may partially come from the breadth aggregation, which aggregates the neighbors round and round and leads to indistinguishability among entity embeddings such that the PLM reasoning process has been disturbed. The majority of baseline GNNs suffer from an apparent performance drop when increasing the number of layers (Figure 4), while our model consistently performs better and is not affected by the number of layers at all since it does not use breadth aggregation. Therefore, our model has immunity to over-smoothing problems.\nIn both small and large k cases, our model outperforms the best performance of all four GNNs (including GCNII, which is specifically designed for over-smoothing issues) by a large margin (Table 4), which serves as evidence of the superiority of our model in long dependency collection."
        },
        {
            "heading": "4.5 Case study",
            "text": "In this section, we present case studies to intuitively show how DepWiGNN mitigates the three kinds of distracting noise introduced in StepGame, namely, disconnected, irrelevant, and supporting.\n\u2022 The disconnected noise is the set of entities and relations that forms a new independent chain in the graph (Figure 5(a)). The node memories constructed in DepWiGNN contain spatial information about the nodes if and only if that node stays in the same connected component; otherwise, it\nhas no information about the node as there is no path between them. Hence, in this case, for the questioned source node P, its memory has no information for the disconnected noise T and D.\n\u2022 The irrelevant noise branches the correct reasoning chain out with new entities and relations but results in no alternative reasoning path (Figure 5(b)). Hence the irrelevant noise entities will not be included in the reasoning path between the source and destination, which means that it will not affect the destination spatial filler stored in the source node memory. In this case, when the key representation (embedding of entity E) is used to unbind the spatial filler from node memory of the source node P, it obtains the filler fPE = FFN(Aggregator([fPX ; fXQ; fQE ])), which is intact to the irrelevant entity Y and relation fxy or fyx.\n\u2022 The supporting noise adds new entities and relations to the original reasoning chain that provides an alternative reasoning path (Figure 5(c)). DepWiGNN is naturally exempted from this noise for two reasons: first, it finds the shortest path between two entities, therefore, will not include I and M in the path; Second, even if the longer path is considered, the depth aggregator should reach the same result as the shorted one since the source and destination are the same."
        },
        {
            "heading": "5 Conclusion",
            "text": "In this work, we introduce DepWiGNN, a novel graph-based method that facilitates depth-wise propagation in graphs, enabling effective capture of long dependencies while mitigating the challenges of over-smoothing and excessive layer stacking. Our approach incorporates a node memory scheme leveraging the TPR mechanism, enabling simple arithmetic operations for memory updating and retrieval, instead of relying on additional neural layers. Experiments on two recently released textual multi-hop spatial reasoning datasets demonstrate the superiority of DepWiGNN in collecting long dependencies over the other three typical GNNs and its immunity to the over-smoothing problem.\nLimitation\nUnlike the classical GNNs, which use onedimensional embedding as the node memory, our method applies a two-dimensional matrix-shaped\nnode memory. This poses a direct increase in memory requirement. The system has to assign extra space to store a matrix with shape Rdh\u00d7dh for each node in the graph, which makes the method less scalable. However, it is a worthwhile trade-off because the two-dimensional node memory can potentially store dh\u22121 number of spatial fillers bound with node embeddings while keeping the size fixed, and it does not suffer from information overloading as faced by the one-dimension memory since the spatial fillers can be straightforwardly retrieved from the memory.\nAnother issue is the time complexity of finding the shortest path between every pair of nodes. For all experiments in this paper, since the edges do not have weights, the complexity is O((n+e)*n), where n and e are the numbers of nodes and edges, respectively. However, things will get worse if the edges in the graph are weighted. We believe this is a potential future research direction for the improvement of the algorithm."
        },
        {
            "heading": "A Preliminary of Tensor Product Representation",
            "text": "Consider the implicit relation hidden in the sentence \"X is over K.\" Our method decomposes this sentence into two groups of role r \u2208 R and filler f \u2208 F symbol components, i.e., R = {X,K} and F = {above, below}. The role and filler symbols are projected into role VR and filler VF vector space, respectively. The TPR of these symbol structures is defined as the tensor T within the vector space VR \u2297 VF , where \u2297 denotes the tensor product (equivalent to outer product when role and filler are one-dimension vectors). In this example, we can drive the equation of writing our example to T:\nT = fabove \u2297 rX + fbelow \u2297 rK = fabover T X + fbelowr T K\n(16)\nThe tensor product \u2297 acts as a binding operator to bind the corresponding role and filler.\nThe tensor inner product serves as the unbinding operator in TPR, which is employed to reconstruct the previously reserved fillers from T. Given the\nunbinding vector uK , we can retrieve the stored filler fbelow from the T. In the most ideal cases, if the role vectors are orthogonal to each other, uK equals rK . When T \u2208 R2, it can be expressed as the matrix multiplication.:\nT \u00b7 uK = (faboverTX + fbelowrTK) \u00b7 rK = \u03b1fbelow \u221d fbelow\n(17)"
        },
        {
            "heading": "B Hyperparameter Settings",
            "text": "Detailed hyperparameter settings for each experiment are provided in Table 5. Epoch numbers for DepWiNet with the three classical GNNs are grouped (in order) for simplicity."
        }
    ],
    "title": "DepWiGNN: A Depth-wise Graph Neural Network for Multi-hop Spatial Reasoning in Text",
    "year": 2023
}