{
    "abstractText": "Explaining black-box model behavior with natural language has achieved impressive results in various NLP tasks. Recent research has explored the utilization of subsequences from the input text as a rationale, providing users with evidence to support the model decision. Although existing frameworks excel in generating high-quality rationales while achieving high task performance, they neglect to account for the unreliable link between the generated rationale and model decision. In simpler terms, a model may make correct decisions while attributing wrong rationales, or make poor decisions while attributing correct rationales. To mitigate this issue, we propose a unified twostage framework known as Self-Attribution and Decision-Making (SADM). Through extensive experiments on five reasoning datasets from the ERASER benchmark, we demonstrate that our framework not only establishes a more reliable link between the generated rationale and model decision but also achieves competitive results in task performance and the quality of rationale. Furthermore, we explore the potential of our framework in semi-supervised scenarios.",
    "authors": [
        {
            "affiliations": [],
            "name": "Yanrui Du"
        },
        {
            "affiliations": [],
            "name": "Sendong Zhao"
        },
        {
            "affiliations": [],
            "name": "Haochun Wang"
        },
        {
            "affiliations": [],
            "name": "Yuhan Chen"
        },
        {
            "affiliations": [],
            "name": "Rui Bai"
        },
        {
            "affiliations": [],
            "name": "Zewen Qiang"
        },
        {
            "affiliations": [],
            "name": "Muzhen Cai"
        },
        {
            "affiliations": [],
            "name": "Bing Qin"
        }
    ],
    "id": "SP:16f6dfef8f54aba212e76384885103574fea4e76",
    "references": [
        {
            "authors": [
                "Gino Brunner",
                "Yang Liu",
                "Damian Pascual",
                "Oliver Richter",
                "Massimiliano Ciaramita",
                "Roger Wattenhofer."
            ],
            "title": "On identifiability in transformers",
            "venue": "arXiv preprint arXiv:1908.04211.",
            "year": 2019
        },
        {
            "authors": [
                "Nadia Burkart",
                "Marco F Huber."
            ],
            "title": "A survey on the explainability of supervised machine learning",
            "venue": "Journal of Artificial Intelligence Research, 70:245\u2013 317.",
            "year": 2021
        },
        {
            "authors": [
                "Yuhan Chen",
                "Nuwa Xi",
                "Yanrui Du",
                "Haochun Wang",
                "Chen Jianyu",
                "Sendong Zhao",
                "Bing Qin."
            ],
            "title": "From artificially real to real: Leveraging pseudo data from large language models for low-resource molecule discovery",
            "venue": "arXiv preprint",
            "year": 2023
        },
        {
            "authors": [
                "Christopher Clark",
                "Kenton Lee",
                "Ming-Wei Chang",
                "Tom Kwiatkowski",
                "Michael Collins",
                "Kristina Toutanova."
            ],
            "title": "Boolq: Exploring the surprising difficulty of natural yes/no questions",
            "venue": "arXiv preprint arXiv:1905.10044.",
            "year": 2019
        },
        {
            "authors": [
                "Jay DeYoung",
                "Sarthak Jain",
                "Nazneen Fatema Rajani",
                "Eric Lehman",
                "Caiming Xiong",
                "Richard Socher",
                "Byron C Wallace."
            ],
            "title": "Eraser: A benchmark to evaluate rationalized nlp models",
            "venue": "arXiv preprint arXiv:1911.03429.",
            "year": 2019
        },
        {
            "authors": [
                "Yanrui Du",
                "Jing Yan",
                "Yan Chen",
                "Jing Liu",
                "Sendong Zhao",
                "Hua Wu",
                "Haifeng Wang",
                "Bing Qin."
            ],
            "title": "Less learn shortcut: Analyzing and mitigating learning of spurious feature-label correlation",
            "venue": "arXiv preprint arXiv:2205.12593.",
            "year": 2022
        },
        {
            "authors": [
                "Yanrui Du",
                "Sendong Zhao",
                "Yuhan Chen",
                "Rai Bai",
                "Jing Liu",
                "Hua Wu",
                "Haifeng Wang",
                "Bing Qin."
            ],
            "title": "The calla dataset: Probing llms\u2019 interactive knowledge acquisition from chinese medical literature",
            "venue": "arXiv preprint arXiv:2309.04198.",
            "year": 2023
        },
        {
            "authors": [
                "Yanrui Du",
                "Sendong Zhao",
                "Yuhan Chen",
                "Rai Bai",
                "Jing Liu",
                "Hua Wu",
                "Haifeng Wang",
                "Bing Qin."
            ],
            "title": "Gls-csc: A simple but effective strategy to mitigate chinese stm models\u2019 over-reliance on superficial clue",
            "venue": "arXiv preprint arXiv:2309.04162.",
            "year": 2023
        },
        {
            "authors": [
                "Jonathan St BT Evans."
            ],
            "title": "Heuristic and analytic processes in reasoning",
            "venue": "British Journal of Psychology, 75(4):451\u2013468.",
            "year": 1984
        },
        {
            "authors": [
                "Shi Feng",
                "Eric Wallace",
                "Alvin Grissom II",
                "Mohit Iyyer",
                "Pedro Rodriguez",
                "Jordan Boyd-Graber."
            ],
            "title": "Pathologies of neural models make interpretations difficult",
            "venue": "arXiv preprint arXiv:1804.07781.",
            "year": 2018
        },
        {
            "authors": [
                "Robert Geirhos",
                "J\u00f6rn-Henrik Jacobsen",
                "Claudio Michaelis",
                "Richard Zemel",
                "Wieland Brendel",
                "Matthias Bethge",
                "Felix A Wichmann."
            ],
            "title": "Shortcut learning in deep neural networks",
            "venue": "Nature Machine Intelligence, 2(11):665\u2013673.",
            "year": 2020
        },
        {
            "authors": [
                "Asish Ghoshal",
                "Srinivasan Iyer",
                "Bhargavi Paranjape",
                "Kushal Lakhotia",
                "Scott Wen-tau Yih",
                "Yashar Mehdad."
            ],
            "title": "Quaser: Question answering with scalable extractive rationalization",
            "venue": "Proceedings of the 45th International ACM SIGIR Conference on",
            "year": 2022
        },
        {
            "authors": [
                "Aya Abdelsalam Ismail",
                "Hector Corrada Bravo",
                "Soheil Feizi."
            ],
            "title": "Improving deep learning interpretability by saliency guided training",
            "venue": "Advances in Neural Information Processing Systems, 34:26726\u2013 26739.",
            "year": 2021
        },
        {
            "authors": [
                "Gautier Izacard",
                "Edouard Grave."
            ],
            "title": "Leveraging passage retrieval with generative models for open domain question answering",
            "venue": "arXiv preprint arXiv:2007.01282.",
            "year": 2020
        },
        {
            "authors": [
                "Sarthak Jain",
                "Byron C Wallace."
            ],
            "title": "Attention is not explanation",
            "venue": "arXiv preprint arXiv:1902.10186.",
            "year": 2019
        },
        {
            "authors": [
                "Daniel Khashabi",
                "Snigdha Chaturvedi",
                "Michael Roth",
                "Shyam Upadhyay",
                "Dan Roth."
            ],
            "title": "Looking beyond the surface: A challenge set for reading comprehension over multiple sentences",
            "venue": "Proceedings of the 2018 Conference of the North American Chap-",
            "year": 2018
        },
        {
            "authors": [
                "Yuxuan Lai",
                "Chen Zhang",
                "Yansong Feng",
                "Quzhe Huang",
                "Dongyan Zhao"
            ],
            "title": "Why machine reading comprehension models learn shortcuts? arXiv preprint arXiv:2106.01024",
            "year": 2021
        },
        {
            "authors": [
                "Kushal Lakhotia",
                "Bhargavi Paranjape",
                "Asish Ghoshal",
                "Wen-tau Yih",
                "Yashar Mehdad",
                "Srinivasan Iyer."
            ],
            "title": "Fid-ex: Improving sequence-to-sequence models for extractive rationale generation",
            "venue": "arXiv preprint arXiv:2012.15482.",
            "year": 2020
        },
        {
            "authors": [
                "Eric Lehman",
                "Jay DeYoung",
                "Regina Barzilay",
                "Byron C Wallace."
            ],
            "title": "Inferring which medical treatments work from reports of clinical trials",
            "venue": "arXiv preprint arXiv:1904.01606.",
            "year": 2019
        },
        {
            "authors": [
                "Tao Lei",
                "Regina Barzilay",
                "Tommi Jaakkola."
            ],
            "title": "Rationalizing neural predictions",
            "venue": "arXiv preprint arXiv:1606.04155.",
            "year": 2016
        },
        {
            "authors": [
                "Mike Lewis",
                "Yinhan Liu",
                "Naman Goyal",
                "Marjan Ghazvininejad",
                "Abdelrahman Mohamed",
                "Omer Levy",
                "Ves Stoyanov",
                "Luke Zettlemoyer"
            ],
            "title": "Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension",
            "year": 2019
        },
        {
            "authors": [
                "Pantelis Linardatos",
                "Vasilis Papastefanopoulos",
                "Sotiris Kotsiantis."
            ],
            "title": "Explainable ai: A review of machine learning interpretability methods",
            "venue": "Entropy, 23(1):18.",
            "year": 2020
        },
        {
            "authors": [
                "Yanchen Liu",
                "Jing Yan",
                "Yan Chen",
                "Jing Liu",
                "Hua Wu."
            ],
            "title": "Smoa: Sparse mixture of adapters to mitigate multiple dataset biases",
            "venue": "arXiv preprint arXiv:2302.14413.",
            "year": 2023
        },
        {
            "authors": [
                "Tyler McDonnell",
                "M\u00fccahid Kutlu",
                "Tamer Elsayed",
                "Matthew Lease."
            ],
            "title": "The many benefits of annotator rationales for relevance judgments",
            "venue": "IJCAI, pages 4909\u20134913.",
            "year": 2017
        },
        {
            "authors": [
                "Tyler McDonnell",
                "Matthew Lease",
                "Mucahid Kutlu",
                "Tamer Elsayed."
            ],
            "title": "Why is that relevant? collecting annotator rationales for relevance judgments",
            "venue": "Proceedings of the AAAI Conference on Human Computation and Crowdsourcing, volume 4, pages",
            "year": 2016
        },
        {
            "authors": [
                "Sharan Narang",
                "Colin Raffel",
                "Katherine Lee",
                "Adam Roberts",
                "Noah Fiedel",
                "Karishma Malkan."
            ],
            "title": "Wt5?! training text-to-text models to explain their predictions",
            "venue": "arXiv preprint arXiv:2004.14546.",
            "year": 2020
        },
        {
            "authors": [
                "Bhargavi Paranjape",
                "Mandar Joshi",
                "John Thickstun",
                "Hannaneh Hajishirzi",
                "Luke Zettlemoyer."
            ],
            "title": "An information bottleneck approach for controlling conciseness in rationale extraction",
            "venue": "arXiv preprint arXiv:2005.00652.",
            "year": 2020
        },
        {
            "authors": [
                "John W Payne",
                "James R Bettman",
                "Eric J Johnson."
            ],
            "title": "Adaptive strategy selection in decision making",
            "venue": "Journal of experimental psychology: Learning, Memory, and Cognition, 14(3):534.",
            "year": 1988
        },
        {
            "authors": [
                "Danish Pruthi",
                "Mansi Gupta",
                "Bhuwan Dhingra",
                "Graham Neubig",
                "Zachary C Lipton."
            ],
            "title": "Learning to deceive with attention-based explanations",
            "venue": "arXiv preprint arXiv:1909.07913.",
            "year": 2019
        },
        {
            "authors": [
                "Marco Tulio Ribeiro",
                "Sameer Singh",
                "Carlos Guestrin."
            ],
            "title": " why should i trust you?\" explaining the predictions of any classifier",
            "venue": "Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining, pages 1135\u2013",
            "year": 2016
        },
        {
            "authors": [
                "Sofia Serrano",
                "Noah A Smith"
            ],
            "title": "Is attention interpretable? arXiv preprint arXiv:1906.03731",
            "year": 2019
        },
        {
            "authors": [
                "Hua Shen",
                "Tongshuang Wu",
                "Wenbo Guo",
                "TingHao\u2019Kenneth\u2019 Huang"
            ],
            "title": "Are shortest rationales the best explanations for human understanding? arXiv preprint arXiv:2203.08788",
            "year": 2022
        },
        {
            "authors": [
                "Daniel Smilkov",
                "Nikhil Thorat",
                "Been Kim",
                "Fernanda Vi\u00e9gas",
                "Martin Wattenberg."
            ],
            "title": "Smoothgrad: removing noise by adding noise",
            "venue": "arXiv preprint arXiv:1706.03825.",
            "year": 2017
        },
        {
            "authors": [
                "Mukund Sundararajan",
                "Ankur Taly",
                "Qiqi Yan."
            ],
            "title": "Axiomatic attribution for deep networks",
            "venue": "International conference on machine learning, pages 3319\u2013 3328. PMLR.",
            "year": 2017
        },
        {
            "authors": [
                "Derek Tam",
                "Anisha Mascarenhas",
                "Shiyue Zhang",
                "Sarah Kwan",
                "Mohit Bansal",
                "Colin Raffel."
            ],
            "title": "Evaluating the factual consistency of large language models through summarization",
            "venue": "arXiv preprint arXiv:2211.08412.",
            "year": 2022
        },
        {
            "authors": [
                "Ian Tenney",
                "Dipanjan Das",
                "Ellie Pavlick."
            ],
            "title": "Bert rediscovers the classical nlp pipeline",
            "venue": "arXiv preprint arXiv:1905.05950.",
            "year": 2019
        },
        {
            "authors": [
                "James Thorne",
                "Andreas Vlachos",
                "Christos Christodoulopoulos",
                "Arpit Mittal."
            ],
            "title": "Fever: a large-scale dataset for fact extraction and verification",
            "venue": "arXiv preprint arXiv:1803.05355.",
            "year": 2018
        },
        {
            "authors": [
                "Hugo Touvron",
                "Thibaut Lavril",
                "Gautier Izacard",
                "Xavier Martinet",
                "Marie-Anne Lachaux",
                "Timoth\u00e9e Lacroix",
                "Baptiste Rozi\u00e8re",
                "Naman Goyal",
                "Eric Hambro",
                "Faisal Azhar"
            ],
            "title": "Llama: Open and efficient foundation language models",
            "year": 2023
        },
        {
            "authors": [
                "Haochun Wang",
                "Sendong Zhao",
                "Zewen Qiang",
                "Zijian Li",
                "Nuwa Xi",
                "Yanrui Du",
                "MuZhen Cai",
                "Haoqiang Guo",
                "Yuhan Chen",
                "Haoming Xu"
            ],
            "title": "Knowledgetuning large language models with structured medical knowledge bases for reliable response generation",
            "year": 2023
        },
        {
            "authors": [
                "Tianlu Wang",
                "Diyi Yang",
                "Xuezhi Wang."
            ],
            "title": "Identifying and mitigating spurious correlations for improving robustness in nlp models",
            "venue": "arXiv preprint arXiv:2110.07736.",
            "year": 2021
        },
        {
            "authors": [
                "Sarah Wiegreffe",
                "Ana Marasovi\u0107",
                "Noah A Smith."
            ],
            "title": "Measuring association between labels and freetext rationales",
            "venue": "arXiv preprint arXiv:2010.12762.",
            "year": 2020
        },
        {
            "authors": [
                "Omar Zaidan",
                "Jason Eisner."
            ],
            "title": "Modeling annotators: A generative approach to learning from annotator rationales",
            "venue": "Proceedings of the 2008 conference on Empirical methods in natural language processing, pages 31\u201340.",
            "year": 2008
        },
        {
            "authors": [
                "Yilun Zhou",
                "Serena Booth",
                "Marco Tulio Ribeiro",
                "Julie Shah"
            ],
            "title": "Do feature attribution methods correctly attribute features",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Large-scale pre-trained models (Lewis et al., 2019; Touvron et al., 2023) have achieved state-of-theart results on various tasks (Wang et al., 2023; Du et al., 2023a; Chen et al., 2023), but their decisionmaking process is opaque. Recent work (Geirhos et al., 2020; Lai et al., 2021; Wang et al., 2021; Du et al., 2022; Liu et al., 2023; Du et al., 2023b) have revealed that models often rely on superficial clues for predictions, which can make their decisions unconvincing. Therefore, it is valuable to motivate models to provide trustworthy rationales to back up their decisions, which facilitates their implementation in real-world applications.\n\u2217 Corresponding author\nRecent studies (Ismail et al., 2021; Shen et al., 2022) have concentrated on the ERASER (DeYoung et al., 2019) benchmark, which encourages models to obtain the extracted subsequences from the input text as a rationale to support their decision. The WT5 (Narang et al., 2020) framework and its variant FID-Ex (Lakhotia et al., 2020) have shown superiority on this benchmark, which generates the rationale and classification decision in a parallel way. However, such parallel frameworks raise a serious problem: is the link between the rationale and classification decision generated by models reliable? In the upper example of Fig. 1, we observe that the model attributes the correct rationale, that\n\u201cThe Nice Guys is a 2016 American neo-noir action comedy film directed by Shane Black\u201d, but still mistakenly supports the claim that \u201cThe Nice Guys was directed by Stephen Spielberg\u201d. In the lower example of Fig. 1, it is evident that the model attributes the rationale that \u201cThe Others (Los Otros) is a 2001 Spanish-American supernatural gothic horror film with elements of psychological horror\u201d, which is entirely unrelated to the claim. However, despite the wrong rationale, the model still correctly supports the claim that \u201cThe Others (2001 film) won awards\u201d. These instances highlight a significant challenge in developing a model with explanations in natural language form, that the generated rationale does not genuinely support and convincingly justify the model decision.\nTo mitigate the above issue, we introduce a unified two-stage framework called Self-Attribution and Decision-Making (SADM). Our SADM framework adopts distinct architectures for training and inference processes. For the training process, we train the model by jointly optimizing both the selfattribution and decision-making objectives. For the inference process, we adopt a two-stage format, which is inspired by the two-stage inference theory of human (Evans, 1984). The model is first prompted to extract the rationale from the given input (known as the self-attribution stage), and then, the model is prompted to utilize the extracted rationale to make informed decisions (known as the decision-making stage). Moreover, our SADM framework incorporates the Fusion-InDecoder (FID) (Izacard and Grave, 2020) architecture to address the challenges posed by lengthy texts, and the Sentence Mark (SM) (Lakhotia et al., 2020) strategy to mitigate the issue of random and irrelevant rationale generation during the selfattribution stage. To further enhance the model\u2019s comprehension at the decision-making stage, we also introduce a Reasoning Augment Learning (RAL) strategy.\nIn our experiments, we introduce the RSQ metric to quantitatively assess the reliable link between generated rationales and model decisions. Experimental results consistently show significant improvements in the RSQ metric for our SADM framework. For task performance and the quality of rationale, our SADM framework also outperforms strong baselines overall. Moreover, we conduct ablation experiments to analyze the contribution of each component within our framework."
        },
        {
            "heading": "2 Background",
            "text": ""
        },
        {
            "heading": "2.1 Task Form",
            "text": "We work with supervised data containing quadruples (q, p, r, y). Here, q represents a question or claim, p represents a passage that can answer or judge q, r corresponds to the rationale, typically a subsequence extracted from the passage p, and y represents the classification target. Our objective is to train a model f where the input is denoted as x = (q, p). The desired outcome from the model is twofold: a classification result and a rationale to explain its decision-making behavior, that is, (r, y) = f(q, p)."
        },
        {
            "heading": "2.2 Related Work",
            "text": "Rationale. The rationale is defined as the condensed and logically coherent subsequences from the input text, yet still adequate for the model to make correct decisions (Lei et al., 2016; Linardatos et al., 2020; Burkart and Huber, 2021). Previous works (McDonnell et al., 2016, 2017; Arous et al., 2021) have shown that rationales bring many benefits, including more reliable judgments, greater transparency for evaluating human raters, and added value from the rationales themselves.\nMethods. Existing methods are mainly divided into two categories: pipeline-based frameworks and parallel frameworks.\nFor pipeline-based frameworks, one way is a post-hoc explanation. After models make decisions, humans attempt to analyze why models give a specific decision. Common methods include attention mechanism (Tenney et al., 2019) (assign soft weights to tokens by self-attention matrix), LIME (Ribeiro et al., 2016) (approximate model behavior locally by repeatedly perturbing inputs), and gradient (Sundararajan et al., 2017; Smilkov et al., 2017) (gradient of each token vector to represent their attribution), etc. However, recent work (Feng et al., 2018; Serrano and Smith, 2019; Jain and Wallace, 2019; Pruthi et al., 2019; Brunner et al., 2019; Zhou et al., 2022) have pointed out that the above methods often exhibit counterintuitive behaviors and lack credibility. The other way is a pre-hoc explanation. Models are encouraged to first generate the rationale and then make decisions based on it. The BERT2BERT framework utilizes two independent models to extract rationales and make decisions, with the reparameterization method to jointly optimize two models during training. Based on the\nBERT2BERT framework, the IB framework proposes an information bottleneck method instead of the reparameterization method to jointly optimize the models. Moreover, the QUASER framework incorporates a sentence selector and a multi-task training objective to improve the model\u2019s task performance, which aims at the sequence-to-sequence (seq2seq) models.\nFor parallel frameworks, WT5 and FID-Ex are representative, where the latter is a variant of the former. For the WT5 framework, task-specific phrases such as \u201cexplain fact verification task\u201d are prepended to the input text. The model then generates a classification decision, followed by the extracted rationale. Notably, the position of the classification decision and extracted rationale can be interchanged, referred to as WT5-INVERSE (WT5INV). Empirical evidence suggests that WT5 generally outperforms WT5-INV in terms of performance. Furthermore, FID-Ex retains the same mode as WT5 but introduces the fusion-in-decoder architecture to address input length limitations. Additionally, it employs a sentence mark strategy to prevent the generation of random rationales.\nFurthermore, prior research (Wiegreffe et al., 2020) has measured the association between freetext rationales and model decisions. Their findings indicate that the parallel frameworks offer substantial advantages over the pipeline framework. Differ-\nently, our study concentrates on extracted rationale scenarios. In comparison to parallel frameworks, our proposed pipeline SADM framework demonstrates more pronounced advantages."
        },
        {
            "heading": "3 SADM",
            "text": "In this section, we first introduce our overall SADM framework (Sec. 3.1), and then we describe how our SADM framework works in detail from two aspects: Training Process (Sec. 3.2) and Inference Process (Sec. 3.3)."
        },
        {
            "heading": "3.1 Overall Framework",
            "text": "The theory of human inference (Evans, 1984) suggests that there are two processes involved in human inference: the heuristic process and the analytic process. During the heuristic process, individuals gather task-relevant information, while the analytic process involves manipulating and processing the gathered information to make judgments. Drawing inspiration from this cognitive thinking, we propose our SADM framework, as depicted in Fig. 2. Firstly, we employ the Trationale template to prompt the model to generate a task-related rationale, the process called self-attribution. Subsequently, we employ the other Tanswer template to prompt the model to make a decision based on the generated rationale, the process called decisionmaking. As for the choice of prompt templates,\nwe utilize natural language oriented toward human understanding. Consider the FEVER dataset as an example. For the Trationale template, we design it as follows: \u201cExtract the rationale from the passage to assess the claim\u201d. Similarly, the Tanswer template is formulated as: \u201cRefer to the following information to judge the claim\u201d. Notably, we use discrete prompt templates, which do not introduce additional parameters or training costs.\nMoreover, similar to FID-Ex and QUASER frameworks, we implement the FID architecture based on the seq2seq models to effectively handle lengthy text inputs. The FID architecture can be described as follows: Firstly, the lengthy passage p is divided into multiple segments denoted as {seg1, seg2, ..., segn}. Next, q is combined with each segment and encoded separately in the encoder module to generate multiple vector representations {e1, e2, ..., en}. Finally, all the vector representations are concatenated as e1 \u2295 e2...\u2295 en and forwarded to the decoder module for further processing."
        },
        {
            "heading": "3.2 Training Process",
            "text": "For the training process, we introduce our training objective, the Sentence Mark (SM) strategy, and the Reasoning Augment Learning (RAL) strategy.\nTraining Objective. Our SADM framework is designed to achieve two training objectives: \u2022 Objective Orationale: Training the model to gen-\nerate a task-related rationale at the prompt of Trationale template. \u2022 Objective Odecision: Training the model to make a decision based on the generated rationale at the prompt of Tanswer template. As shown in Fig. 3, for the objective Orationale, we provide the model with Trationale, q, and p as input, while using the human-annotated rationale r as the learning objective. For objec-\ntive Odecision, we provide Tanswer, q, and humanannotated rationale r as input, with the golden target y as the learning objective. We adopt a joint training strategy that involves adding the losses Lrationale and Ldecision of two objectives. Moreover, to calculate the losses Lrationale and Ldecision, we employ the teacher-forcing strategy. Given an input sequence x1, ..., xt and a target sequence y1, ..., yu, we maximize the probability p(yi|x1, ..., xt, y1, ..., yi\u22122, yi\u22121) for each yi in the target sequence y1, ..., yu to obtain the loss.\nSentence Mark (SM). Recent research (Tam et al., 2022) has highlighted the creative nature of generative models. However, in our specific task, there is a concern that the generative model may produce random and irrelevant natural language as a rationale. To mitigate this issue, we adopt the sentence mark strategy (Lakhotia et al., 2020), which involves adding an index number before each sentence in the passage p. For instance, a passage consists of n sentences (s1, ..., sn). After adding the index, the passage takes the form of (S1 : s1, ..., SN : sn), where uppercase characters are the sentence indexes. If applying the SM strategy, when optimizing the objective Orationale during the training process, we need to take the sentence indexes of human-annotated rationales as the learning objective instead of the rationale in natural language form.\nReasoning Augment Learning (RAL). Cognitive science (Payne et al., 1988) shows that humans have the ability to make reasonable decisions regardless of whether they possess fine-grained information (such as human-annotated rationale) or coarse-grained information (such as the whole passage). Therefore, we imitate human beings, intending to equip the model with the ability to perceive information at different levels of granularity, thereby enhancing its reasoning ability. To accomplish this, we leverage the wealth of information available in supervised data. As shown in Fig. 3, for the objective Odecision, we add two new formats of training samples. We respectively provide the model with Tanswer, q, and p as input, as well as Tanswer, q, r, and p as input. The golden target y is regarded as the learning objective and the calculated loss is added to Ldecision."
        },
        {
            "heading": "3.3 Inference Process",
            "text": "The inference process initiates an auto-regressive process. During the inference process, we em-\nploy a two-stage process as shown in Fig. 2: selfattribution and decision-making.\nSelf-attribution. When the model is prompted with the Trationale template, it generates the rationale based on the claim q and passage p. If with the SM strategy, the model first generates sentence indexes, which are then used to locate the corresponding rationale from the passage p. And if without the SM strategy, the model directly generates the rationale in natural language form.\nDecision-making. When the model is prompted with the Tanswer template, it makes a decision based on the generated rationale. Moreover, we provide the model with the option to make a decision by considering a combination of the generated rationale and passage."
        },
        {
            "heading": "4 Evaluation Metrics",
            "text": "Consistent with prior work (Paranjape et al., 2020; Ghoshal et al., 2022), we assess task performance using accuracy and evaluate the quality of generated rationales using the Intersection-Over-Union F1 score (IOU F1) and Token F1 (TF1). We also follow the prior research (Wiegreffe et al., 2020) to take Rationale Accuracy (R-Acc) to evaluate the quality of generated rationales. Additionally, we introduce a novel metric, the Reasoning Success Quotient (RSQ), to gauge the extent of the reliable link between the generated rationale and model decision.\nIOU F1 and TF1 metrics. The IOU F1 and TF1 metrics are used to evaluate the quality of rationale at the sentence level and the token level respectively. Whether at the sentence level or the token level, the precision and recall rates are calculated for each test sample. Based on them, the F1 value can be calculated. To obtain the IOU F1 or TF1 metrics, the corresponding F1 values of all test samples are averaged. The detailed calculation process is described in App. A.\nR-Acc metric. We first train a model f with questions and annotated rationales and then evaluate the performance (accuracy) of model f on generated rationales to reflect the quality of the generated rationales.\nRSQ metric. We propose the RSQ metric to measure the reliable link between the generated rationale and model decision. Specifically, we categorize the test samples into four classes:\n\u2022 rcdc: Samples where both the generated rationale and model decision are correct. \u2022 rwdw: Samples where both the generated rationale and model decision are wrong. \u2022 rcdw: Samples where the generated rationale is correct, but the model decision is wrong. \u2022 rwdc: Samples where the generated rationale is wrong, but the model decision is correct. The RSQ metric is calculated as follows.\nRSQ = Num(rcdc + rwdw)\nNum(rcdc + rwdw + rcdw + rwdc) (1)\nwhere Num represents the number of samples. As for how to assess whether the generated rationale or model decision is correct, for the model decision, we determine whether the predicted target aligns with the golden target. For the generated rationale, we assess its correctness using the recall (described in detail in App. A). If the recall rate exceeds a certain threshold, mainly set at 0.5 in our work, we consider the generated rationale to be correct.\nBased on the RSQ metric, we also propose RSQW and RSQ-C metrics to guide a more detailed analysis. The RSQ-W measures the proportion of wrong decisions made by the model when the model attributes the correct rationales. The RSQ-W metric is as follows:\nRSQ\u2212W = Num(rcdw) Num(rcdc + rcdw)\n(2)\nThe RSQ-C measures the proportion of correct decisions made by the model when the model attributes the wrong rationales. The RSQ-C metric is as follows:\nRSQ\u2212C = Num(rwdc) Num(rwdw + rwdc)\n(3)"
        },
        {
            "heading": "5 Experiment",
            "text": ""
        },
        {
            "heading": "5.1 Datasets",
            "text": "The statistics of datasets used in our experiments are shown in Tab. 2. The FEVER (Thorne et al., 2018) dataset aims to judge whether the given passage supports or refutes the claim. The MultiRC (Khashabi et al., 2018) dataset aims to assign True or False to the question concatenating with the answer choice based on the given passage. The BoolQ (Clark et al., 2019) dataset aims to answer the question with True or False labels based on the given passage. The Evidence Inference (Evi\nInf) (Lehman et al., 2019) dataset concatenates the (intervention, outcome, comparator) triplet into the question, and aims to judge whether the intervention significantly increases, decreases, or has no effect on the outcome based on the given passage. The Movie Reviews (Mov Rev) (Zaidan and Eisner, 2008) dataset aims to analyze the sentiment of the given passage with positive or negative labels, where the question is uniformly set to \u201cWhat is the sentiment of this review?\u201d. Overall, all five datasets belong to the reasoning tasks. Moreover, the ERASER benchmark provides the annotated rationale at the phrase level for the Mov Rev dataset, and the sentence level for the others. Following the prior work, we convert the phrase level to the sentence level annotations."
        },
        {
            "heading": "5.2 Training Details",
            "text": "Consistent with previous work (Paranjape et al., 2020; Ghoshal et al., 2022), we select T5-base as our main model and use the integrated interface T5ForConditionalGeneration1 from huggingface to load the model. We run all experiments on a single NVIDIA 80g-a100 GPU machine. We set the learning rate to 1e-4, the batch size to 16, and the total\n1https://huggingface.co/docs/transformers/.\ntraining steps to 15,000 steps, where we evaluate the IOU F1 metric on the validation set every 500 steps to choose the best checkpoint. For datasets with lengthy input, we apply the FID architecture. For the BoolQ, Mov Rev, and Evi Inf datasets, we use a maximum of 512 subword tokens per segment input, where we use 10 segments for the BoolQ and Evi Inf datasets and 6 for the Mov Rev dataset. For the FEVER and MultiRC datasets, we only use one segment and set the maximum input subword lengths to 512 and 1024 respectively."
        },
        {
            "heading": "5.3 Baselines",
            "text": "In our study, we compare our SADM framework against several baselines, including BERT2BERT, IB, WT5, WT5-INV, and FID-Ex frameworks. The experimental results for BERT2BERT and IB are reported from the original work, where BERT2BERT is applied in full-supervised scenarios and IB is applied in semi-supervised scenarios. For WT5, WT5-INV, and FID-Ex frameworks, we re-implemented them based on the details provided in the original work. However, it is important to note that there is limited availability of open-source code for these baselines, which presents a challenge in aligning the TF1 metric. To ensure fairness, we do not report the TF1 metric mentioned in the prior work. On the other hand, for task performance and IOU F1 metrics, we successfully aligned them."
        },
        {
            "heading": "5.4 Experiment Results",
            "text": "Full-supervised scenario. We select the WT5, WT5-INV,FID-Ex and BERT2BERT frameworks as baselines. Since the WT5-INV framework can not obtain a stable performance on the Mov Rev dataset, we do not report its results. As shown in Tab. 1, experimental results demonstrate the promising potential of the SADM framework. For both task performance (Perf.) and the quality of rationale (IOU F1, TF1, and R-Acc), our framework demonstrates varying degrees of improvement across five datasets. Notably, our framework has exhibited more significant improvements in the RSQ metric, which indicates a more reliable link between the generated rationale and model decision. Specifically, we observe 1.3 points improvement on the FEVER dataset, 3.1 points improvement on the MultiRC dataset, 1.2 points improvement on the BoolQ dataset, 1.9 points improvement on the Evi Inf dataset, and 21.7 points improvement on the Mov Rev dataset. Furthermore, experimental results show that the FID architecture only pro-\nvides improvements for the Evi Inf dataset, whereas it does not exhibit substantial gains for the BoolQ and Evi Inf datasets. We attribute this observation to the fact that, in the Evi Inf dataset, rationale tends to appear in the middle or toward the end of the passage. Hence, addressing the limitation of input length with the FID becomes imperative.\nSemi-supervised scenario. Considering the expensive cost of rationale annotation, semisupervised scenarios are more likely to be applied in the real world. We select the IB framework, a variant of the BERT2BERT framework, and the FID-Ex framework, which demonstrates good performance in the full-supervised scenario, as baselines. Following previous settings, we utilize only 25% of the training data with annotated rationales. As shown in Tab. 3, on the Mov Rev dataset, our SADM framework achieves lower performance than the FID-Ex framework in task performance and the quality of rationale but still outperforms in RSQ metric. On the other four datasets, we observe an average improvement of 3.7 points in task performance, 1.3 points in IOU F1, 0.9 points in TF1, and 4.2 points in the RSQ metric. Over-\nall, our framework demonstrates more significant advantages in the semi-supervised scenario."
        },
        {
            "heading": "6 Analysis",
            "text": "In our analysis, we conduct ablation experiments (in Sec. 6.1) to evaluate the effectiveness of each strategy in our SADM framework. We also consider different choices of threshold in the RSQ metric (in Sec. 6.2) to provide robust results. Furthermore, we provide quantitative analysis to evaluate the lack of a reliable link between the rationale and model decision in the competitive FID-Ex framework, which will be shown in detail in App. B."
        },
        {
            "heading": "6.1 Ablation Study",
            "text": "We evaluate the performance of SADM without the SM strategy and SADM without the RAL strategy. As shown in Tab. 4, experimental results show that the performance of the SADM framework decreases to a certain extent when either the SM strategy or RAL strategy is removed, which indicates that both the SM strategy and RAL strategy play a positive role. Notably, we specifically verify the effect of the RAL strategy on model reasoning ability. We propose the Rationale-Centric Precision (RCP) metric, which focuses on the pro-\nportion of correct decisions that can be made when the model is provided with the annotated rationale at the decision-making stage. Our experimental results show that when the RAL strategy is removed, the RCP metric decreases by an average of 1.3 points across the five datasets. Such a phenomenon underscores the critical significance of the RAL strategy in enhancing the model\u2019s reasoning ability at the decision-making stage.\nAdditionally, as introduced in Sec. 3.1, the passage is an optional input at the decision-making stage. For our SADM framework, the model can make a decision based on a combination of generated rationale and passage. As shown in Tab. 4, experimental results show that considering both generated rationale and passage at the decision-making stage can improve task performance, but slightly damage the reliable link between the generated rationale and model decision. Such a phenomenon is reasonable. When the model is provided with more contextual information, it will make more correct decisions, but at the same time, the information it focuses on will be more scattered."
        },
        {
            "heading": "6.2 Choice of Threshold",
            "text": "To ensure a standardized evaluation of the generated rationale, we have selected a threshold of 0.5 for the recall rate in the RSQ metric. However, we are concerned that the choice of the threshold will\nbring bias in our experimental results. Therefore, we have also conducted experiments with alternative threshold values of 0.6, 0.7, 0.8, 0.9, and 1.0, which allows us to present a robust evaluation. As shown in Tab. 5, compared to strong baselines, results consistently showcase the significant advantage of our SADM framework across the entire range of threshold values in the RSQ metric."
        },
        {
            "heading": "7 Conclusion",
            "text": "Our proposed SADM framework establishes a more reliable link between the generated rationale and model decision while improving task performance and rationale quality. Furthermore, we observe significant advantages in semi-supervised scenarios. In the future, we will explore how to optimize our framework to attain greater performance improvements."
        },
        {
            "heading": "8 Limitation",
            "text": "Despite the numerous advantages exhibited by our SADM framework, we believe that it still has the following limitations: \u2022 Our SADM framework achieves good perfor-\nmance in full-supervised and semi-supervised scenarios, but in the future, we will put more effort into thinking about how to apply our SADM framework in the unsupervised scenario. \u2022 In our work, we only design natural language oriented toward human understanding as prompt templates. Is this necessarily the best for the model? We will further explore their influence."
        },
        {
            "heading": "9 Ethics Statement",
            "text": "We conduct all experiments on publicly available datasets with authorization from the respective maintainers. All data we use do not involve personal privacy, ethical issues, or sensitive topics."
        },
        {
            "heading": "Acknowledgements",
            "text": "This work was supported by the National Key R&D Program of China [2021ZD0113302]; the National Natural Science Foundation of China [62206079]; and the Heilongjiang Provincial Natural Science Foundation of China [YQ2022F006]."
        },
        {
            "heading": "B Quantitative Analysis",
            "text": "Our competitive baseline FID-Ex framework involves generating the classification decision followed by rationale in a parallel way. To quantitatively assess the reliable link between the generated rationale and model decision, we conduct experiments illustrated in Fig. 5. We apply a masking\ntechnique to the model decision and then initialize the model with the prompt \"Answer :< pad > Explanation :\" to encourage the generation of rationale. We believe that if there is a reliable link between the generated rationale and the model decision, the IOU F1 and TF1 metrics will change significantly after the masking of the model decision. However, interestingly, as presented in Tab. 6, despite masking the model decision, we observe minimal changes in both the IOU F1 and TF1 metrics. This somewhat suggests that, for the FID-Ex framework, the rationale may be generated independently, with no reliable link to the model decision."
        }
    ],
    "title": "Make Your Decision Convincing! A Unified Two-Stage Framework: Self-Attribution and Decision-Making",
    "year": 2023
}