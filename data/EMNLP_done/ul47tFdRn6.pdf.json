{
    "abstractText": "Recent advances in image and video creation, especially AI-based image synthesis, have led to the production of numerous visual scenes that exhibit a high level of abstractness and diversity. Consequently, Visual Storytelling (VST), a task that involves generating meaningful and coherent narratives from a collection of images, has become even more challenging and is increasingly desired beyond real-world imagery. While existing VST techniques, which typically use autoregressive decoders, have made significant progress, they suffer from low inference speed and are not well-suited for synthetic scenes. To this end, we propose a novel diffusion-based system DIFFUVST, which models the generation of a series of visual descriptions as a single conditional denoising process. The stochastic and non-autoregressive nature of DIFFUVST at inference time allows it to generate highly diverse narratives more efficiently. In addition, DIFFUVST features a unique design with bi-directional text history guidance and multimodal adapter modules, which effectively improve inter-sentence coherence and imageto-text fidelity. Extensive experiments on the story generation task covering four fictional visual-story datasets demonstrate the superiority of DIFFUVST over traditional autoregressive models in terms of both text quality and inference speed.",
    "authors": [
        {
            "affiliations": [],
            "name": "Shengguang Wu"
        },
        {
            "affiliations": [],
            "name": "Mei Yuan"
        },
        {
            "affiliations": [],
            "name": "Qi Su"
        }
    ],
    "id": "SP:55d677892e52f32d57e1f3840dd487b456086bc3",
    "references": [
        {
            "authors": [
                "Jacob Austin",
                "Daniel D. Johnson",
                "Jonathan Ho",
                "Daniel Tarlow",
                "Rianne van den Berg"
            ],
            "title": "Structured denoising diffusion models in discrete state-spaces",
            "year": 2021
        },
        {
            "authors": [
                "Hong Chen",
                "Yifei Huang",
                "Hiroya Takamura",
                "Hideki Nakayama"
            ],
            "title": "Commonsense knowledge aware concept selection for diverse and informative visual storytelling",
            "year": 2021
        },
        {
            "authors": [
                "Ting Chen",
                "Ruixiang Zhang",
                "Geoffrey Hinton"
            ],
            "title": "Analog bits: Generating discrete data using diffusion models with self-conditioning",
            "year": 2022
        },
        {
            "authors": [
                "Prafulla Dhariwal",
                "Alexander Nichol."
            ],
            "title": "Diffusion models beat gans on image synthesis",
            "venue": "Advances in Neural Information Processing Systems, 34:8780\u2013 8794.",
            "year": 2021
        },
        {
            "authors": [
                "Alexey Dosovitskiy",
                "Lucas Beyer",
                "Alexander Kolesnikov",
                "Dirk Weissenborn",
                "Xiaohua Zhai",
                "Thomas Unterthiner",
                "Mostafa Dehghani",
                "Matthias Minderer",
                "Georg Heigold",
                "Sylvain Gelly"
            ],
            "title": "An image is worth 16x16 words: Transformers",
            "year": 2020
        },
        {
            "authors": [
                "Shansan Gong",
                "Mukai Li",
                "Jiangtao Feng",
                "Zhiyong Wu",
                "Lingpeng Kong"
            ],
            "title": "Diffuseq: Sequence to sequence text generation with diffusion models",
            "year": 2022
        },
        {
            "authors": [
                "Zhengfu He",
                "Tianxiang Sun",
                "Kuanning Wang",
                "Xuanjing Huang",
                "Xipeng Qiu."
            ],
            "title": "Diffusionbert: Improving generative masked language models with diffusion models",
            "venue": "arXiv preprint arXiv:2211.15029.",
            "year": 2022
        },
        {
            "authors": [
                "Jonathan Ho",
                "Ajay Jain",
                "Pieter Abbeel"
            ],
            "title": "Denoising diffusion probabilistic models",
            "year": 2020
        },
        {
            "authors": [
                "Jonathan Ho",
                "Tim Salimans"
            ],
            "title": "Classifier-free diffusion guidance",
            "year": 2022
        },
        {
            "authors": [
                "Emiel Hoogeboom",
                "Didrik Nielsen",
                "Priyank Jaini",
                "Patrick Forr\u00e9",
                "Max Welling"
            ],
            "title": "Argmax flows and multinomial diffusion: Learning categorical distributions",
            "year": 2021
        },
        {
            "authors": [
                "Neil Houlsby",
                "Andrei Giurgiu",
                "Stanislaw Jastrzebski",
                "Bruna Morrone",
                "Quentin de Laroussilhe",
                "Andrea Gesmundo",
                "Mona Attariyan",
                "Sylvain Gelly"
            ],
            "title": "Parameter-efficient transfer learning for nlp",
            "year": 2019
        },
        {
            "authors": [
                "Chao-Chun Hsu",
                "Zi-Yuan Chen",
                "Chi-Yang Hsu",
                "ChihChia Li",
                "Tzu-Yuan Lin",
                "Ting-Hao \u2019Kenneth\u2019 Huang",
                "Lun-Wei Ku"
            ],
            "title": "Knowledge-enriched visual storytelling",
            "year": 2020
        },
        {
            "authors": [
                "Ting-Yao Hsu",
                "Chieh-Yang Huang",
                "Yen-Chia Hsu",
                "Ting-Hao \u2019Kenneth\u2019 Huang"
            ],
            "title": "Visual story postediting",
            "year": 2019
        },
        {
            "authors": [
                "Junjie Hu",
                "Yu Cheng",
                "Zhe Gan",
                "Jingjing Liu",
                "Jianfeng Gao",
                "Graham Neubig"
            ],
            "title": "What makes a good story? designing composite rewards for visual storytelling",
            "year": 2019
        },
        {
            "authors": [
                "Qiuyuan Huang",
                "Zhe Gan",
                "Asli Celikyilmaz",
                "Dapeng Wu",
                "Jianfeng Wang",
                "Xiaodong He."
            ],
            "title": "Hierarchically structured reinforcement learning for topically coherent visual story generation",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2019
        },
        {
            "authors": [
                "Yunjae Jung",
                "Dahun Kim",
                "Sanghyun Woo",
                "Kyungsu Kim",
                "Sungjin Kim",
                "In So Kweon."
            ],
            "title": "Hideand-tell: Learning to bridge photo streams for visual storytelling",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 11213\u2013",
            "year": 2020
        },
        {
            "authors": [
                "Taehyeong Kim",
                "Min-Oh Heo",
                "Seonil Son",
                "KyoungWha Park",
                "Byoung-Tak Zhang."
            ],
            "title": "GLAC net: Glocal attention cascading networks for multi-image cued story generation",
            "venue": "CoRR, abs/1805.10973.",
            "year": 2018
        },
        {
            "authors": [
                "Jie Lei",
                "Linjie Li",
                "Luowei Zhou",
                "Zhe Gan",
                "Tamara L. Berg",
                "Mohit Bansal",
                "Jingjing Liu"
            ],
            "title": "Less is more: Clipbert for video-and-language learning via sparse sampling",
            "year": 2021
        },
        {
            "authors": [
                "Junnan Li",
                "Dongxu Li",
                "Caiming Xiong",
                "Steven Hoi"
            ],
            "title": "2022a. Blip: Bootstrapping language-image pretraining for unified vision-language understanding and generation",
            "year": 2022
        },
        {
            "authors": [
                "Xiang Lisa Li",
                "John Thickstun",
                "Ishaan Gulrajani",
                "Percy Liang",
                "Tatsunori B. Hashimoto"
            ],
            "title": "2022b. Diffusion-lm improves controllable text generation",
            "year": 2022
        },
        {
            "authors": [
                "Yitong Li",
                "Zhe Gan",
                "Yelong Shen",
                "Jingjing Liu",
                "Yu Cheng",
                "Yuexin Wu",
                "Lawrence Carin",
                "David Carlson",
                "Jianfeng Gao"
            ],
            "title": "Storygan: A sequential conditional gan for story visualization",
            "year": 2018
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter"
            ],
            "title": "Decoupled weight decay regularization",
            "year": 2017
        },
        {
            "authors": [
                "Huaishao Luo",
                "Lei Ji",
                "Botian Shi",
                "Haoyang Huang",
                "Nan Duan",
                "Tianrui Li",
                "Jason Li",
                "Taroon Bharti",
                "Ming Zhou"
            ],
            "title": "Univl: A unified video and language pre-training model for multimodal understanding and generation",
            "year": 2020
        },
        {
            "authors": [
                "Adyasha Maharana",
                "Mohit Bansal."
            ],
            "title": "Integrating visuospatial, linguistic, and commonsense structure into story visualization",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing.",
            "year": 2021
        },
        {
            "authors": [
                "Adyasha Maharana",
                "Darryl Hannan",
                "Mohit Bansal"
            ],
            "title": "Storydall-e: Adapting pretrained text-to-image transformers for story continuation",
            "year": 2022
        },
        {
            "authors": [
                "Zainy M Malakan",
                "Ghulam Mubashar Hassan",
                "Ajmal Mian."
            ],
            "title": "Vision transformer based model for describing a set of images as a story",
            "venue": "Australasian Joint Conference on Artificial Intelligence, pages 15\u201328. Springer.",
            "year": 2022
        },
        {
            "authors": [
                "Alexander Quinn Nichol",
                "Prafulla Dhariwal."
            ],
            "title": "Improved denoising diffusion probabilistic models",
            "venue": "Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 8162\u20138171.",
            "year": 2021
        },
        {
            "authors": [
                "Alexander Quinn Nichol",
                "Prafulla Dhariwal."
            ],
            "title": "Improved denoising diffusion probabilistic models",
            "venue": "International Conference on Machine Learning, pages 8162\u20138171. PMLR.",
            "year": 2021
        },
        {
            "authors": [
                "Xichen Pan",
                "Pengda Qin",
                "Yuhong Li",
                "Hui Xue",
                "Wenhu Chen"
            ],
            "title": "Synthesizing coherent story with auto-regressive latent diffusion models",
            "year": 2022
        },
        {
            "authors": [
                "Alec Radford",
                "Jeffrey Wu",
                "Rewon Child",
                "David Luan",
                "Dario Amodei",
                "Ilya Sutskever"
            ],
            "title": "Language models are unsupervised multitask learners",
            "venue": "OpenAI blog,",
            "year": 2019
        },
        {
            "authors": [
                "Hareesh Ravi",
                "Kushal Kafle",
                "Scott Cohen",
                "Jonathan Brandt",
                "Mubbasir Kapadia."
            ],
            "title": "Aesop: Abstract encoding of stories, objects, and pictures",
            "venue": "2021 IEEE/CVF International Conference on Computer Vision (ICCV), pages 2032\u20132043.",
            "year": 2021
        },
        {
            "authors": [
                "Robin Rombach",
                "Andreas Blattmann",
                "Dominik Lorenz",
                "Patrick Esser",
                "Bj\u00f6rn Ommer"
            ],
            "title": "Highresolution image synthesis with latent diffusion models",
            "year": 2021
        },
        {
            "authors": [
                "Olaf Ronneberger",
                "Philipp Fischer",
                "Thomas Brox."
            ],
            "title": "U-net: Convolutional networks for biomedical image segmentation",
            "venue": "Medical Image Computing and Computer-Assisted Intervention\u2013MICCAI 2015: 18th International Conference, Munich, Germany,",
            "year": 2015
        },
        {
            "authors": [
                "Victor Sanh",
                "Lysandre Debut",
                "Julien Chaumond",
                "Thomas Wolf"
            ],
            "title": "Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter",
            "year": 2019
        },
        {
            "authors": [
                "Yang Song",
                "Conor Durkan",
                "Iain Murray",
                "Stefano Ermon"
            ],
            "title": "Maximum likelihood training of scorebased diffusion models",
            "year": 2021
        },
        {
            "authors": [
                "Yang Song",
                "Stefano Ermon"
            ],
            "title": "Generative modeling by estimating gradients of the data distribution",
            "year": 2019
        },
        {
            "authors": [
                "Yang Song",
                "Stefano Ermon"
            ],
            "title": "Improved techniques for training score-based generative models",
            "year": 2020
        },
        {
            "authors": [
                "Yang Song",
                "Jascha Sohl-Dickstein",
                "Diederik P. Kingma",
                "Abhishek Kumar",
                "Stefano Ermon",
                "Ben Poole"
            ],
            "title": "Score-based generative modeling through stochastic differential equations",
            "year": 2020
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin."
            ],
            "title": "Attention is all you need",
            "venue": "Advances in neural information processing systems, 30.",
            "year": 2017
        },
        {
            "authors": [
                "Oriol Vinyals",
                "Alexander Toshev",
                "Samy Bengio",
                "Dumitru Erhan"
            ],
            "title": "Show and tell: A neural image caption generator",
            "year": 2014
        },
        {
            "authors": [
                "Peng Wang",
                "An Yang",
                "Rui Men",
                "Junyang Lin",
                "Shuai Bai",
                "Zhikang Li",
                "Jianxin Ma",
                "Chang Zhou",
                "Jingren Zhou",
                "Hongxia Yang"
            ],
            "title": "Ofa: Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework",
            "year": 2022
        },
        {
            "authors": [
                "Xin Wang",
                "Wenhu Chen",
                "Yuan-Fang Wang",
                "William Yang Wang."
            ],
            "title": "No metrics are perfect: Adversarial reward learning for visual storytelling",
            "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1:",
            "year": 2018
        },
        {
            "authors": [
                "Pengcheng Yang",
                "Fuli Luo",
                "Peng Chen",
                "Lei Li",
                "Zhiyi Yin",
                "Xiaodong He",
                "Xu Sun."
            ],
            "title": "Knowledgeable storyteller: A commonsense-driven generative model for visual storytelling",
            "venue": "Proceedings of the Twenty-Eighth International Joint Conference on",
            "year": 2019
        },
        {
            "authors": [
                "Lili Yao",
                "Nanyun Peng",
                "Ralph Weischedel",
                "Kevin Knight",
                "Dongyan Zhao",
                "Rui Yan"
            ],
            "title": "Planand-write: Towards better automatic storytelling",
            "year": 2018
        },
        {
            "authors": [
                "Youngjae Yu",
                "Jiwan Chung",
                "Heeseung Yun",
                "Jongseok Kim",
                "Gunhee Kim."
            ],
            "title": "Transitional adaptation of pretrained models for visual storytelling",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12658\u201312668.",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Visual Storytelling (VST) is the challenging task of generating a series of meaningful and coherent sentences to narrate a set of images. Compared to image and video captioning (Vinyals et al., 2014; Luo et al., 2020; Wang et al., 2022; Lei et al., 2021), VST extends the requirements of image-to-text generation beyond plain descriptions of images in isolation. A VST system is expected to accurately portray the visual content of each image while also\n\u2217Corresponding author.\ncapturing the semantic links across the given sequence of images as a whole.\nSo far, prior works on visual storytelling (Wang et al., 2018; Kim et al., 2018; Malakan et al., 2022) typically utilize an autoregressive design that involves an extra sequential model built on top of basic image captioners to capture the relationship among images. More elaborate frameworks with multi-stage generation pipelines have also been proposed that guide storytelling via e.g. storylineplanning (Yao et al., 2018), external knowledge engagement (Yang et al., 2019; Hsu et al., 2019, 2020), and concept selection (Chen et al., 2021). Effective as these existing methods are for describing real-world photo streams (e.g. the upper row in Figure 1), albeit with their increasingly complex structural designs, their ability to generate stories from fictional scenes (e.g. the lower row in Figure 1) remains unverified. In addition, because these models are typically trained to infer in an autoregressive way, they produce captions token by token while conditioning on only previous (unidirectional) history, which prohibits these architectures from refining prefix of sentences based on later generated tokens. As a result, existing autoregressive models are restricted to generating less\ninformative narratives at a low inference speed.\nTo tackle the above issues of VST \u2013 especially given the increasing demand for narrating fictional visual scenes \u2013 we propose a novel visual storytelling framework DIFFUVST (Diffusion for Visual StoryTelling) based on diffusion models utilizing continuous latent embeddings, which have demonstrated promising capabilities for image synthesis (Rombach et al., 2021; Pan et al., 2022) and language modelling (Hoogeboom et al., 2021; Austin et al., 2021; Li et al., 2022b). Our model generates a set of visual descriptions simultaneously in a non-autoregressive manner by jointly denoising multiple random vectors into meaningful word embeddings conditioned on the image features of all panels (i.e., all image-text pairs in a visual-story sequence). Specifically, DIFFUVST leverages a transformer encoder to learn to restore the word embeddings of the ground-truth story texts from sequences of Gaussian vectors. To enhance the visual-truthfulness of the generated texts, DIFFUVST proposes multimodal feature extractor and adapter modules with pretrained backbone that provide in-domain story-image features as condition and story-text features as classifier-free guidance (Ho and Salimans, 2022). Since these features represent all panels across the entire picture stream, they serve as the \u201cglobal history\u201d of all the preceding and following frames and texts, which connects the captioning of the current image to other context panels more closely, thus improving the coherence of the whole visual narrative.\nWe validate the effectiveness of DIFFUVST by performing the visual story generation task on four visual-story datasets with non-real-world imagery: AESOP (Ravi et al., 2021), FlintstonesSV (Maharana and Bansal, 2021), PororoSV (Li et al., 2018), and DiDeMoSV (Maharana et al., 2022) with the latter three converted from Story Visualization (SV) to fictional VST datasets. Quantitative results show that DIFFUVST outperforms strong autoregressive baselines across all four datasets. In addition, DIFFUVST manages to reduce the inference time by a large factor thanks to its non-autoregressive nature.\nOur contributions are summarized as follows:\n(1) We model the visual narration of a set of images as one single denoising process and propose DIFFUVST, a diffusion-based method for visual storytelling. To our best knowledge, this work is the first to leverage diffusion models and adopt a non-autoregressive approach in visual storytelling.\n(2) We propose global-history guidance with adapted multimodal features in DIFFUVST that enhance the coherence and visual-fidelity of the generated stories.\n(3) We demonstrate the effectiveness and practical value of our system in face of the surging demand for narrating synthetic scenes by conducting extensive experiments on four fictional visualstory datasets. DIFFUVST outperforms strong autoregressive models in terms of performance while achieving significantly faster inference speeds."
        },
        {
            "heading": "2 Related work",
            "text": ""
        },
        {
            "heading": "2.1 Visual storytelling",
            "text": "Visual storytelling (VST) aims to produce a set of expressive and coherent sentences to depict an image stream. Existing work in this area can be broadly divided into two groups of approaches: end-to-end frameworks and multi-stage systems. In the end-to-end pipeline, models are developed to autoregressively generate multi-sentence stories given the image stream in a unified structure (Wang et al., 2018; Kim et al., 2018). Meanwhile, multistage approaches that introduce more planning or external knowledge have also shown impressive performance (Yao et al., 2018; Hsu et al., 2020; Chen et al., 2021). Further, some other works are devoted to adopting more elaborate learning paradigms to improve the informativeness and controllability of story generation (Yang et al., 2019; Hu et al., 2019; Jung et al., 2020)."
        },
        {
            "heading": "2.2 Diffusion models",
            "text": "Diffusion models are a family of probabilistic generative models that first progressively destruct data by injecting noise, and then learn to restore the original features through incremental denoising. Current studies are mostly based on three formulations: Denoising Diffusion Probabilistic Models (DDPMs) (Ho et al., 2020; Nichol and Dhariwal, 2021a), Score-based Generative Models (SGMs) (Song and Ermon, 2019, 2020), and Stochastic Differential Equations (Score SDEs) (Song et al., 2021, 2020), which have shown outstanding performance in image synthesis. Recently, diffusion models have also been employed to address text generation tasks. Either a discrete diffusion method for text data (Austin et al., 2021; He et al., 2022; Chen et al., 2022) is designed, or the word embedding space is leveraged for continuous diffusion (Li et al., 2022b; Gong et al., 2022)."
        },
        {
            "heading": "3 Background",
            "text": ""
        },
        {
            "heading": "3.1 Diffusion in continuous domain",
            "text": "A diffusion model involves a forward process and a reverse process. The forward process constitutes a Markov chain of latent variables x1, ..., xT by incrementally adding noise to sample data:\nq(xt|xt\u22121) = N(xt; \u221a 1\u2212 \u03b2txt\u22121, \u03b2tI) (1)\nwhere \u03b2t \u2208 (0, 1) denotes the scaling of variance of time step t, and xT tends to approximate a standard Gaussian Noise N(xt; 0, I) if the time step t = T is large enough. In the reverse process, a model parameterised by p\u03b8 (typically a U-Net (Ronneberger et al., 2015) or a Transformer (Vaswani et al., 2017)) is trained to gradually reconstruct the previous state xt\u22121 from the noised samples at time step t, where \u00b5\u03b8(xt, t) is model\u2019s prediction on the mean of xt\u22121 conditioned on xt:\np\u03b8(xt\u22121|xt) = N(xt\u22121;\u00b5\u03b8(xt, t), \u2211 \u03b8 (xt, t)) (2) The original training objective is to minimize the negative log-likelihood when generating x0 from the Gaussian noise xT produced by (1):\nEq[\u2212 log(p\u03b8(x0))] = Eq[\u2212 log( \u222b p\u03b8(x0:T )d(x1:T ))] (3)\nFurthermore, a simpler loss function can be obtained following DDPM (Ho et al., 2020):\nL = T\u2211 t=1 Eq(xt|x0)\u2225\u00b5\u03b8(xt, t)\u2212 \u00b5\u0302(xt, x0)\u2225 2 (4)\nwhere \u00b5\u0302(xt, x0) represents the mean of posterior q(xt\u22121|xt, x0)."
        },
        {
            "heading": "3.2 Classifier-free guidance",
            "text": "Classifier-free guidance (Ho and Salimans, 2022) serves to trade off mode coverage and sample fidelity in training conditional diffusion models. As an alternative method to classifier guidance (Dhariwal and Nichol, 2021) which incorporates gradients of image classifiers into the score estimate of a diffusion model, the classifier-free guidance mechanism jointly trains a conditional and an unconditional diffusion model, and combines the resulting conditional and unconditional score estimates.\nSpecifically, an unconditional model p\u03b8(z) parameterized through a score estimator \u03f5\u03b8(z\u03bb) is\ntrained together with the conditional diffusion model p\u03b8(z|c) parameterized through \u03f5\u03b8(z\u03bb, c). The conditioning signal in the unconditional model is discarded by randomly setting the class identifier c to a null token \u2205 with some probability puncond which is a major hyperparameter of this guidance mechanism. The sampling could then be formulated using the linear combination of the conditional and unconditional score estimates, where w controls the strength or weight of the guidance:\n\u03f5\u0303\u03b8(z\u03bb, c) = (1 + w)\u03f5\u03b8(z\u03bb, c)\u2212 w\u03f5\u03b8(z\u03bb) (5)"
        },
        {
            "heading": "4 DIFFUVST",
            "text": "Figure 2 presents the overall architecture and training flow of our model DIFFUVST, which consists of two pretrained multimodal encoders (one for image and one for text) with their separate adapter layers, a feature-fusion module and a transformerencoder as the denoising learner."
        },
        {
            "heading": "4.1 Multimodal encoding with adapters",
            "text": "To obtain the cross-modal features of each imagetext panel, we employ a multimodal backbone (in this work: BLIP (Li et al., 2022a)), which serves to provide a solid initial representation of the image stream and its paired text sequences. However, since BLIP is pretrained on large-scale internet data which contains primarily real-world photos, the extracted image- and text- features only represent the most general visual and textual knowledge. Hence, we further propose an adapter module for transferring the BLIP-encoded multimodal features to the specific domain of fictional scenes and texts. Motivated by the adapter design in parameter-efficient transfer learning (Houlsby et al., 2019), we implement the feature-adapters as additional linear layers following the respective last layer of the visual and textual encoders. This mechanism allows the simultaneous exploitation of both the information stored in the pretrained BLIP checkpoint and the freshly learned knowledge from our training data, which leads to the final feature vectors (Fv and Ft) that are well adapted for fictional image-text domains."
        },
        {
            "heading": "4.2 Feature-fusion for global history condition and guidance",
            "text": "In line with the formulation in Diffusion-LM (Li et al., 2022b), our model DIFFUVST performs visual storytelling as a conditional denoising task in continuous domain. To condition this denoising process (i.e., the generation of story texts) on the\ncorresponding visual scenes, we utilize the adapted visual features of all images [ F 1v , F 2 v , ..., F N v ] in each story sample as the condition signal, where N is the number of images in a single story sample. For simplicity, we further denote the features of all image panels [ F 1v , F 2 v , ..., F N v ] and text panels[\nF 1t , F 2 t , ..., F N t ] in a visual story sample as F \u2217v\nand F \u2217t . Specifically, we concatenate them with the corrupted input embeddings xT to get [xT ;F \u2217v ]. Since these feature segments contain the visual information of all panels in the storyline, they provide global visual history across the entire image stream and thus serve as a more comprehensive condition for coherent storytelling.\nFurthermore, we adapt the classifier-free guidance mechanism (Section 3.2) to include a certain degree of ground-truth textual information during training. We do this by fusing the textual features of each panel of the ground-truth story into the above mixed vectors as [xT ;F \u2217v ;F \u2217 t ], which serves as the final input to our denoising model. However, we mask out the textual feature segment F \u2217t with a probability of punguide (i.e., such textual guidance is provided with a chance of 1\u2212 punguide). In\naccordance with puncond in the original classifierfree guidance proposal (Ho and Salimans, 2022), punguide regulates the percentage of training without extra textual condition that will be unavailable during inference.\nFormally, based on the classifier-free guidance mechanism in (5), our DIFFUVST model samples with the feature guidance of global textual history, as expressed in:\n\u03f5\u0303\u03b8([xT ;F \u2217 v ] ;F \u2217 t )\n= (1 + w)\u03f5\u03b8([xT ;F \u2217 v ] ;F \u2217 t )\u2212 w\u03f5\u03b8([xT ;F \u2217v ])\n(6)\nwhere w controls the guidance strength of the global textual history. We further discuss the effects of the guidance strength w and unguided probability punguide in detail in Section 5.4.\nJust like the global visual features, these additional signals from the target texts also represent the global history of the whole set of panels in a story sample \u2013 but from a textual angle. Hence, such guidance contributes to the bidirectional attention for text generation and improves the overall fluency and coherence of the generated story."
        },
        {
            "heading": "4.3 Transformer-encoder for denoising",
            "text": "Our model adopts a BERT-like transformerencoder architecture to perform the reconstruction of the ground-truth word embeddings x0 (of all the consecutive texts) from a random noise xT . This noisy input is enriched with the global visual- and textual-history features ([xT ;F \u2217v ;F \u2217 t ]) as described in the above fusion module (Section 4.2). We add an additional segment embedding layer to distinguish the adapted BLIP-features from the corrupted sentence embedding xT .\nOur model is trained to directly predict the ground-truth word embeddings x0 from any diffusion timestep t sampled from a large T in the same spirit with Improved DDPM (Nichol and Dhariwal, 2021b) and Diffusion-LM (Li et al., 2022b). Instead of using a large T , we sample a subset of size T \u2032 from the total T timesteps to serve as the actual set of diffusion time steps: S = s0, ..., sT \u2032 |st < st\u22121 \u2208 (0, T ]. This greatly accelerates the reverse process, which then requires significantly fewer denoising steps, as evidenced in Section 5.3. We also add a x1 restoring loss term \u2225\u00b5\u03b8(x1, 1)\u2212 \u00b5\u0302(x1, x0)\u2225 as an absolute error loss (L1-loss) to the L1-version of the basic continuous diffusion loss L as defined in (4) in order to regulate the performance of model on restoring x0 from a slightly noised x1 and to improve the model\u2019s stability of prediction. This gives L\u2032 as the natural combination of the x1 restoring loss and the L1-version of L. Furthermore, we add a rounding term LR parameterized by Eq[\u2212 log p(W |x\u0302)] = Eq[\u2212 log \u220fl i=1 p(Wi|x\u0302i)] for rounding the predicted x0 embeddings back to discrete tokens, where W is the ground truth sentence and l the generated sequence length. x\u0302 represents the predicted word embeddings.\nOur final loss function is the sum of the restoring x0-embedding loss L\u2032 and the rounding loss LR, with a rounding term coefficient \u03bb controlling the relative importance of L\u2032 and LR:\nLfinal = L \u2032 + \u03bbLR\n= T \u2032\u2211 t\u2208S Eq(xt|x0)[\u2225\u00b5\u03b8(xt, t)\u2212 \u00b5\u0302(xt, x0)\u2225\n+ \u2225\u00b5\u03b8(x1, 1)\u2212 \u00b5\u0302(x1, x0)\u2225 \u2212 \u03bb log p\u03b8(w|x\u0302)]\n(7)\nIn this work, we set the rounding term coefficient (\u03bb) as a dynamic \u03bb strategy where \u03bb is updated after every gradient descent step to ensure that the\nrelative weight of L\u2032 and LR is equal (i.e., L \u2032\nLR = 1).\nThis encourages both loss items in (7) to decrease at a relatively same rate.\nThe predicted embedding x0 is then projected through a language model head to produce all panels of text sequences in parallel that finally form a narrative together."
        },
        {
            "heading": "5 Experiments",
            "text": ""
        },
        {
            "heading": "5.1 Experimental setup",
            "text": "Datasets: We use four visual-story datasets featuring synthetic style and content as our testbed: AESOP (Ravi et al., 2021), PororoSV (Li et al., 2018), FlintstonesSV (Maharana and Bansal, 2021) and DiDeMoSV (Maharana et al., 2022). It is worth noting that while AESOP was originally designed as an image-to-text dataset, the other three datasets were initially created for story visualization (SV) tasks. However, in our work, we employ them in a reverse manner as visual storytelling datasets due to their fictional art style, which aligns well with the objective of narrating non-real-world imagery. For more detailed statistics regarding these datasets, please refer to Appendix A.1. Evaluation metrics: We evaluate the quality of generated stories using quantitative NLG metrics: BLEU, ROUGE-L, METEOR and CIDEr. We also compare the inference speed of different methods with the average generation time of each test sample using the same hardware and environment. Baselines: Considering the accessibility and reproducibility of existing methods on our four fictional visual storytelling datasets, we compare the performance of our diffusion-based model to three widely recognized open-source autoregressive baselines: GLAC-Net (Kim et al., 2018), TAPM (Yu et al., 2021), BLIP-EncDec (Li et al., 2022a), which are retrained on our four fictional visual storytelling datasets respectively:\n\u2022 GLAC-Net (Kim et al., 2018) employs a LSTM-encoder-decoder structure with a twolevel attention (\u201cglobal-local\u201d attention) and an extra sequential mechanism to cascade the hidden states of the previous sentence to the next sentence serially.\n\u2022 TAPM (Yu et al., 2021) uses a pretrained visual encoder and language generator aligned with Adaptation Loss and finetuned with Sequential Coherence Loss. For our task, we utilize a pretrained ViT (Dosovitskiy et al.,\n2020) as the visual encoder and GPT2 (Radford et al., 2019) for the language generator, ensuring a fair comparison, as our DIFFUVST also employs a ViT encoder (BLIP-ImageModel) for image features (Section 4.1).\n\u2022 BLIP-EncDec (Li et al., 2022a) leverages the pretrained BLIP captioning model (Li et al., 2022a), fine-tuned on the image-text pairs from our datasets. During inference, we incorporate generated captions from previous images in the textual history of the story sequence, introducing a sequential dependency for captioning the current image. Given that BLIP-EncDec builds upon a well-established captioning model, it serves as a strong baseline, with the mentioned modification specifically for visual storytelling.\nWe follow the hyperparameter settings specified in each of the baselines and utilize their official codes for training on our four fictional datasets. DIFFUVST implementation details: The multimodal image- and text- feature extractors of DIFFUVST are realized as branches of the pretrained\nBLIP (Li et al., 2022a) and we keep their parameters frozen. Our denoising transformer is initialized with the pretrained DISTILBERT-BASEUNCASED (Sanh et al., 2019). To keep the groundtruth story-text embeddings (x0) stable for training the denoising model, we keep the pretrained word embedding layers and language model head frozen throughout the training process. More detailed hyperparameters for of DIFFUVST\u2019s training and inference can be found in Appendix A.2."
        },
        {
            "heading": "5.2 Text generation quality and efficiency",
            "text": "The best metric performances of DIFFUVST in comparison to the baselines are reported in Table 1. With regards to the key guidance-related hyperparameters w and punguide, the best results are achieved with w = 0.3/0.7/1.0/0.3 and punguide = 0.5/0.7/0.7/0.5 for FlintstonesSV/PororoSV/DiDoMoSV/AESOP datasets respectively. As evident from Table 1, DIFFUVST consistently outperforms these baselines across nearly all metrics and datasets. While there are marginal exceptions, such as the slightly lower B@4 score on the PororoSV and ASEOP datasets and the CIDEr score on DiDeMoSV compared to\nTAPM, these do not diminish DIFFUVST\u2019s overall superior performance. Notably, given TAPM (Yu et al., 2021) has outperformed most state-of-the-art visual storytellers in their results on VIST, including AREL (Wang et al., 2018), INet (Jung et al., 2020), HSRL (Huang et al., 2019), etc. (as indicated in the TAPM paper), we have good reason to believe that our model can also surpass these models, further demonstrating the effectiveness of DIFFUVST. Moreover, the guided versions of DIFFUVST consistently outperform the fullyunguided (guidance strength w = 0) counterparts, which proves our adaptation of classifier-free guidance (Ho and Salimans, 2022) for textual history guidance as highly effective.\nIn Table 1, we also compared the efficiency of story generation by measuring the average inference time for a single sample on a NVIDIATesla-V100-32GB GPU. Since visual storytelling contains multiple image-text panels, our diffusionbased system that produces all tokens of all text panels simultaneously is able to generate story at a much faster (about 10 times or more) rate than the autoregressive baselines. This acceleration greatly enhances the efficiency of visual storytelling.\nIn short, DIFFUVST not only achieves top-tier performance (especially with global history guidance), but also excels in efficiency, making it an overall superior choice for visual storytelling."
        },
        {
            "heading": "5.3 Inference speed and denoising steps:",
            "text": "The inference time of DIFFUVST is, however, positively correlated with the number of denoising steps, the increment of which is likely to result in a more refined story output. Figure 4 illustrates the change of evaluation results of DIFFUVST with respect to the number of denoising steps. Apparently, our model does not require many steps to reach a stable performance and surpass the autoregressive (AR)-baseline (GLACNet as a demonstrative example), which is a benefit of our model formulation that directly predicts x0 rather than intermediate steps xT\u22121 from the random noise xT . This property further speeds up the story generation, since our model can already produce decent results in only a few denoising steps.\nAlso noteworthy from Figure 4 is that more denoising steps could result in worse performance. Although a larger number of denoising steps typically enhances generation quality, as is the usual case in image synthesis, we believe an excessive\namount of denoising iterations may lead to less satisfactory text generation, especially from the perspective of NLG metrics such as CIDEr. We attribute this phenomenon to the over-filtering of noise. Our denoising transformer is designed to predict ground-truth text embeddings x0 from any noisy inputs xt. Thus, even a single denoising step can effectively filter out a substantial amount of noise. When this filtering process is applied for too many iterations than necessary, however, it continuously tries to reach a \"less noisy\" text representation and may inadvertently remove valuable linguistic details and result in more uniform and less varied text. And metrics like CIDEr are known to lay greater weight on less common n-grams, which explains why the degrading of performance is the most noticeable on the CIDEr metric."
        },
        {
            "heading": "5.4 Textual feature guidance",
            "text": "We have also experimentally verified the effectiveness of our adaptation of classifier-free guidance (Ho and Salimans, 2022) to including textual\nfeatures as additional cues for training our DIFFUVST model, but with specific probabilities and strengths. The key hyperparameters here are the unguided probability punguide and the guidance strength w. The former regulates the probability of training the model solely on visual representations, without any textual cues (i.e., not guided by global textual history). The latter controls the weighting of the guided model, as shown in (6).\nIn Table 2 and Figure 3, we take DiDeMoSV as an example dataset and show the varying test performances of our DIFFUVST model when trained with different configurations of guidance strength w \u2208 {0.0, 0.3, 0.5, 0.7, 1.0} and unguided probability punguide \u2208 {0.3, 0.5, 0.7}. If w = 0.0, the model is essentially trained without textual feature guidance at all, which corresponds to the unguided version of DIFFUVST in Table 1.\nAs Table 2 and Figure 3 show, DIFFUVST achieves the best BLEU@4 and CIDEr results on DiDeMoSV with a strong guidance weight w = 1.0 and low guidance rate (i.e., high unguided probability punguide = 0.7). Depending on punguide, the increase of textual guidance strength during training does not necessarily lead to a better model performance, whereas a higher unguided probability consistently produces superior results - regardless of the strength w. With a larger punguide, the model is trained to concentrate more on the visual features to learn to tell a story instead of relying too much on the ground-truth textual cues, which will be unavailable at inference time. However, if DIFFUVST receives no textual guidance at all (w = 0.0), it may not satisfactorily produce a fluent narration, because the model has to ignore the global textual context altogether, which is also unwanted for storytelling. This is especially demonstrated by the BLEU@4 metric where a fully-unguided model obtains the lowest score.\nBased on these observations, we conclude that a relatively small portion of ground-truth textual guidance is ideal for training our DIFFUVST model. Too much textual signals would not fully develop the model\u2019s cross-modal ability to caption images into texts and too little conditioning supervision from the story context tends to harm the overall text generation quality. Interestingly, our conclusion also resonates with the findings reported by Ho and Salimans (2022), which emphasize that the sample quality would benefit the most from a small amount of unconditional classifier-free guidance."
        },
        {
            "heading": "5.5 Multimodal feature adapter",
            "text": "In this subsection, we discuss the effects of the adapter modules that transfer the image-text features extracted from the pretrained encoders. The motivations for including the adapter modules are two-fold: Firstly, the adapters serve to transfer the pretrained image-text features to the domain of fictional scenes; Secondly, since these multimodal features are utilized to provide guidance for the optimization of the denoising language model, we need the additional trainable parameters introduced in the adapter modules to better align the encoded features with the capabilities of the denoising LM, thereby enhancing the overall framework integrity of DIFFUVST.\nTo empirically validate the significance of the adapter modules within our DIFFUVST system, we present here the ablation studies where we compare the performance of our model with and without the adapter layers. The results on four datasets are shown in Table 3. We keep all other hyperparameter settings, including \u03bb, guidance strength, and guidance probability identical. The \"Adapter(+)\" versions are thus identical to \u201cDIFFUVST guided\u201d reported in Table 1.\nAs vividly demonstrated in Table 3, the presence of adapter modules significantly impacts model performance across all evaluation metrics and datasets, where DIFFUVST built without adapters consis-\ntently underperforms the fully-equipped version. Therefore, these adapter modules, designed to transfer encoded features to the current domain while also enabling more learnable parameters for aligned feature representation, are indeed a crucial component of our DIFFUVST framework."
        },
        {
            "heading": "6 Conclusion",
            "text": "In this paper, we tackle the challenges posed by Visual Storytelling, and present a diffusion-based solution, DIFFUVST that stands out as a novel effort in introducing a non-autoregressive approach to narrating fictional image sequences. Underpinned by its key components, including multimodal encoders with adapter layers, a feature-fusion module and a transformer-based diffusion language model, DIFFUVST models the narration as a single denoising process conditioned on and guided by bidirectional global image- and text- history representations. This leads to generated stories that are not just coherent but also faithfully represent the visual content. Extensive experiments attest the outstanding performance of our non-autoregressive DIFFUVST system both in terms of text generation\nquality and inference speed compared to the traditional autoregressive visual storytelling models. Furthermore, we present comprehensive ablation analyses of our method, which demonstrate that our elaborate designs, based on diffusion-related mechanisms, hold great potential for producing expressive and informative descriptions of visual scenes parallelly with a high inference speed.\nLimitations\nOur diffusion-based system DIFFUVST inherits a main limitation that resides in the need to carefully search for an optimal hyperparameter setting among various choices and to explain the reason behind its validity. The diffusion process contains numerous key variables that potentially exerts substantial influence on the final model performance - for instance, the strengths and probability of classifierfree guidance as discussed in Section 5.4. Although we are able to deduce a reasonable setup of such parameters via extensive experimental runs, the exact mechanism of how these diffusion-based parameter choices affect our storytelling model remains to be explored."
        },
        {
            "heading": "A Appendix A. Further Report on Experimental Details",
            "text": "A.1 Dataset Statistics As stated in section 5.1, we use four visual-story datasets of synthetic style and content for our experiments: AESOP (Ravi et al., 2021), PororoSV (Li et al., 2018), FlintstonesSV (Maharana and Bansal, 2021) and DiDeMoSV (Maharana et al., 2022). Here are the specific statistics of these datasets: AESOP contains visual stories made of 3 image-text panels, with the visual parts generated by clipart objects. It contains 6,024/991 samples in train/val sets (we use the validation set for testing on AESOP as well). In PororoSV and FlintstonesSV, which contain 10,191/2,334/2,208 and 20,132/2,071/2,309 samples in train/val/test sets respectively, each story sample is a 5-frame image sequence paired\nwith 5 consecutive texts. DiDeMoSV uses a 3- frame image-text pair sequence as an individual story with a total number of 11,550/2,707/3,378 samples in train/val/test sets.\nA.2 diffuvst implementation parameters For DIFFUVST\u2019s training and inference, we use the following setup: We set the maximum text sequence length to 32 for AESOP/PororoSV/FlintstonesSV (i.e., a maximum story length of 32 \u2217 N for N images) and 16 for DiDeMoSV which features shorter captions. During training, we randomly sample 30 noised xT from 1000 diffusion timesteps to form the noisy input embeddings to DIFFUVST. For the strength of textual feature guidance w and the unguided probability punguide, we have explored multiple configurations and leave a detailed discussion of their effects in Section 5.4. During inference, we use random vectors as noisy input xT , and replace all guidance text features with zeros and mask them out. For testing, we denoise a total of 30 steps to iteratively refine the predicted word embeddings and report the best metric performances. We also polish the generated text with unique-consecutive method, where the consecutively repeated words are reduced to only one. We train the whole denoising system with the loss function defined in (7) and the rounding term coefficient (\u03bb) is set as a dynamic \u03bb strategy as stated in Section 4.3. For each dataset, we train DIFFUVST for 30 epochs using AdamW optimizer (Loshchilov and Hutter, 2017) with the initial learning rate of 1e\u2212 4 which is then updated by a Cosine Annealing schedule and a weight decay rate of 1e\u2212 2."
        }
    ],
    "title": "DIFFUVST: Narrating Fictional Scenes with Global-History-Guided Denoising Models",
    "year": 2023
}