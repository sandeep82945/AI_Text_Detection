{
    "abstractText": "Commonsense Knowledge Graphs (CSKGs) are crucial for commonsense reasoning, yet constructing them through human annotations can be costly. As a result, various automatic methods have been proposed to construct CSKG with larger semantic coverage. However, these unsupervised approaches introduce spurious noise that can lower the quality of the resulting CSKG, which cannot be tackled easily by existing denoising algorithms due to the unique characteristics of nodes and structures in CSKGs. To address this issue, we propose GOLD (Global and Local-aware Denoising), a denoising framework for CSKGs that incorporates entity semantic information, global rules, and local structural information from the CSKG. Experiment results demonstrate that GOLD outperforms all baseline methods in noise detection tasks on synthetic noisy CSKG benchmarks. Furthermore, we show that denoising a real-world CSKG is effective and even benefits the downstream zero-shot commonsense question-answering task. Our code and data are publicly available at https: //github.com/HKUST-KnowComp/GOLD.",
    "authors": [
        {
            "affiliations": [],
            "name": "Zheye Deng"
        },
        {
            "affiliations": [],
            "name": "Weiqi Wang"
        },
        {
            "affiliations": [],
            "name": "Zhaowei Wang"
        },
        {
            "affiliations": [],
            "name": "Xin Liu"
        },
        {
            "affiliations": [],
            "name": "Yangqiu Song"
        },
        {
            "affiliations": [],
            "name": "IsA UsedFor"
        },
        {
            "affiliations": [],
            "name": "Rene Magritte"
        },
        {
            "affiliations": [],
            "name": "IsA CapableOf"
        }
    ],
    "id": "SP:f8e55a24dcef13edf5f1a66ad3437911e2c7a735",
    "references": [
        {
            "authors": [
                "Yangqiu Song"
            ],
            "title": "Complex query answering on",
            "year": 2023
        },
        {
            "authors": [
                "Choi."
            ],
            "title": "Abductive commonsense reasoning",
            "venue": "In",
            "year": 2020
        },
        {
            "authors": [
                "Yonatan Bisk",
                "Rowan Zellers",
                "Ronan Le Bras",
                "Jianfeng Gao",
                "Yejin Choi."
            ],
            "title": "PIQA: reasoning about physical commonsense in natural language",
            "venue": "The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Ap-",
            "year": 2020
        },
        {
            "authors": [
                "Antoine Bordes",
                "Nicolas Usunier",
                "Alberto Garc\u00edaDur\u00e1n",
                "Jason Weston",
                "Oksana Yakhnenko."
            ],
            "title": "Translating embeddings for modeling multirelational data",
            "venue": "Advances in Neural Information Processing Systems 26: 27th Annual Conference on",
            "year": 2013
        },
        {
            "authors": [
                "Antoine Bosselut",
                "Hannah Rashkin",
                "Maarten Sap",
                "Chaitanya Malaviya",
                "Asli Celikyilmaz",
                "Yejin Choi."
            ],
            "title": "COMET: commonsense transformers for automatic knowledge graph construction",
            "venue": "Proceedings of the 57th Conference of the Association for Compu-",
            "year": 2019
        },
        {
            "authors": [
                "Chunkit Chan",
                "Jiayang Cheng",
                "Weiqi Wang",
                "Yuxin Jiang",
                "Tianqing Fang",
                "Xin Liu",
                "Yangqiu Song."
            ],
            "title": "Chatgpt evaluation on sentence level relations: A focus on temporal, causal, and discourse relations",
            "venue": "CoRR, abs/2304.14827.",
            "year": 2023
        },
        {
            "authors": [
                "Jiangjie Chen",
                "Wei Shi",
                "Ziquan Fu",
                "Sijie Cheng",
                "Lei Li",
                "Yanghua Xiao"
            ],
            "title": "Say what you mean! large language models speak too positively about negative commonsense knowledge",
            "year": 2023
        },
        {
            "authors": [
                "Kewei Cheng",
                "Nesreen K. Ahmed",
                "Yizhou Sun."
            ],
            "title": "Neural compositional rule learning for knowledge graph reasoning",
            "venue": "CoRR, abs/2303.03581.",
            "year": 2023
        },
        {
            "authors": [
                "Kewei Cheng",
                "Jiahao Liu",
                "Wei Wang",
                "Yizhou Sun."
            ],
            "title": "Rlogic: Recursive logical rule learning from knowledge graphs",
            "venue": "KDD \u201922: The 28th ACM SIGKDD Conference on Knowledge Discovery and",
            "year": 2022
        },
        {
            "authors": [
                "Yangqiu Song",
                "Bin He"
            ],
            "title": "2021b. DISCOS: bridg",
            "year": 2021
        },
        {
            "authors": [
                "Fabian M. Suchanek"
            ],
            "title": "Fast rule mining in on",
            "year": 2015
        },
        {
            "authors": [
                "Li Guo"
            ],
            "title": "Jointly embedding knowledge graphs",
            "year": 2016
        },
        {
            "authors": [
                "Li Guo"
            ],
            "title": "Knowledge graph embedding with",
            "year": 2018
        },
        {
            "authors": [
                "Xu Han",
                "Shulin Cao",
                "Xin Lv",
                "Yankai Lin",
                "Zhiyuan Liu",
                "Maosong Sun",
                "Juanzi Li."
            ],
            "title": "Openke: An open toolkit for knowledge embedding",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, EMNLP 2018: Sys-",
            "year": 2018
        },
        {
            "authors": [
                "Mutian He",
                "Tianqing Fang",
                "Weiqi Wang",
                "Yangqiu Song."
            ],
            "title": "Acquiring and modelling abstract commonsense knowledge via conceptualization",
            "venue": "CoRR, abs/2206.01532.",
            "year": 2022
        },
        {
            "authors": [
                "Pengcheng He",
                "Jianfeng Gao",
                "Weizhu Chen."
            ],
            "title": "DeBERTav3: Improving deBERTa using ELECTRAstyle pre-training with gradient-disentangled embedding sharing",
            "venue": "The Eleventh International Conference on Learning Representations.",
            "year": 2023
        },
        {
            "authors": [
                "Shengbin Jia",
                "Yang Xiang",
                "Xiaojun Chen",
                "Kun Wang",
                "Shijia E."
            ],
            "title": "Triple trustworthiness measurement for knowledge graph",
            "venue": "The World Wide Web Conference, WWW 2019, San Francisco, CA, USA, May 13-17, 2019, pages 2865\u20132871. ACM.",
            "year": 2019
        },
        {
            "authors": [
                "Yu Jin Kim",
                "Beong-woo Kwak",
                "Youngwook Kim",
                "Reinald Kim Amplayo",
                "Seung-won Hwang",
                "Jinyoung Yeo."
            ],
            "title": "Modularized transfer learning with multiple knowledge graphs for zero-shot commonsense reasoning",
            "venue": "Proceedings of the 2022 Con-",
            "year": 2022
        },
        {
            "authors": [
                "Diederik P. Kingma",
                "Jimmy Ba."
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings.",
            "year": 2015
        },
        {
            "authors": [
                "Jonathan Lajus",
                "Luis Gal\u00e1rraga",
                "Fabian M. Suchanek."
            ],
            "title": "Fast and exact rule mining with AMIE 3",
            "venue": "The Semantic Web - 17th International Conference, ESWC 2020, Heraklion, Crete, Greece, May 31-June 4, 2020, Proceedings, volume 12123",
            "year": 2020
        },
        {
            "authors": [
                "Xiang Li",
                "Aynaz Taheri",
                "Lifu Tu",
                "Kevin Gimpel."
            ],
            "title": "Commonsense knowledge base completion",
            "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12, 2016, Berlin, Germany, Volume 1: Long",
            "year": 2016
        },
        {
            "authors": [
                "Yankai Lin",
                "Zhiyuan Liu",
                "Huan-Bo Luan",
                "Maosong Sun",
                "Siwei Rao",
                "Song Liu."
            ],
            "title": "Modeling relation paths for representation learning of knowledge bases",
            "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2015
        },
        {
            "authors": [
                "H. Liu",
                "P. Singh."
            ],
            "title": "Conceptnet \u2014 a practical commonsense reasoning tool-kit",
            "venue": "BT Technology Journal, 22(4):211\u2013226.",
            "year": 2004
        },
        {
            "authors": [
                "Rui Liu",
                "Zheng Lin",
                "Yutong Tan",
                "Weiping Wang."
            ],
            "title": "Enhancing zero-shot and few-shot stance detection with commonsense knowledge graph",
            "venue": "Findings of the Association for Computational Linguistics: ACL/IJCNLP 2021, Online Event, August",
            "year": 2021
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Roberta: A robustly optimized BERT pretraining approach",
            "venue": "CoRR, abs/1907.11692.",
            "year": 2019
        },
        {
            "authors": [
                "Kaixin Ma",
                "Filip Ilievski",
                "Jonathan Francis",
                "Yonatan Bisk",
                "Eric Nyberg",
                "Alessandro Oltramari."
            ],
            "title": "Knowledge-driven data construction for zero-shot evaluation in commonsense question answering",
            "venue": "Thirty-Fifth AAAI Conference on Artificial Intelli-",
            "year": 2021
        },
        {
            "authors": [
                "Chaitanya Malaviya",
                "Chandra Bhagavatula",
                "Antoine Bosselut",
                "Yejin Choi."
            ],
            "title": "Commonsense knowledge base completion with structural and semantic context",
            "venue": "The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The",
            "year": 2020
        },
        {
            "authors": [
                "Elan Markowitz",
                "Keshav Balasubramanian",
                "Mehrnoosh Mirtaheri",
                "Murali Annavaram",
                "Aram Galstyan",
                "Greg Ver Steeg."
            ],
            "title": "Statik: Structure and text for inductive knowledge graph completion",
            "venue": "Findings of the Association for Computational Linguistics:",
            "year": 2022
        },
        {
            "authors": [
                "Nasrin Mostafazadeh",
                "Aditya Kalyanpur",
                "Lori Moon",
                "David W. Buchanan",
                "Lauren Berkowitz",
                "Or Biran",
                "Jennifer Chu-Carroll."
            ],
            "title": "GLUCOSE: generalized and contextualized story explanations",
            "venue": "Proceedings of the 2020 Conference on Empirical",
            "year": 2020
        },
        {
            "authors": [
                "Stephen H. Muggleton",
                "Luc De Raedt."
            ],
            "title": "Inductive logic programming: Theory and methods",
            "venue": "J. Log. Program., 19/20:629\u2013679.",
            "year": 1994
        },
        {
            "authors": [
                "Jianmo Ni",
                "Gustavo Hern\u00e1ndez \u00c1brego",
                "Noah Constant",
                "Ji Ma",
                "Keith B. Hall",
                "Daniel Cer",
                "Yinfei Yang."
            ],
            "title": "Sentence-t5: Scalable sentence encoders from pre-trained text-to-text models",
            "venue": "Findings of the Association for Computational Linguistics: ACL 2022,",
            "year": 2022
        },
        {
            "authors": [
                "OpenAI."
            ],
            "title": "Chatgpt: Optimizing language models for dialogue",
            "venue": "OpenAI.",
            "year": 2022
        },
        {
            "authors": [
                "der",
                "Paul F. Christiano",
                "Jan Leike",
                "Ryan Lowe"
            ],
            "title": "Training language models to follow instructions with human feedback",
            "venue": "NeurIPS",
            "year": 2022
        },
        {
            "authors": [
                "Chengwei Qin",
                "Aston Zhang",
                "Zhuosheng Zhang",
                "Jiaao Chen",
                "Michihiro Yasunaga",
                "Diyi Yang"
            ],
            "title": "Is chatgpt a general-purpose natural language processing task solver? CoRR, abs/2302.06476",
            "year": 2023
        },
        {
            "authors": [
                "Meng Qu",
                "Junkun Chen",
                "Louis-Pascal A.C. Xhonneux",
                "Yoshua Bengio",
                "Jian Tang."
            ],
            "title": "Rnnlogic: Learning logic rules for reasoning on knowledge graphs",
            "venue": "9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria,",
            "year": 2021
        },
        {
            "authors": [
                "Julien Romero",
                "Simon Razniewski"
            ],
            "title": "Mapping and cleaning open commonsense knowledge bases with generative translation",
            "year": 2023
        },
        {
            "authors": [
                "Ali Sadeghian",
                "Mohammadreza Armandpour",
                "Patrick Ding",
                "Daisy Zhe Wang."
            ],
            "title": "DRUM: end-toend differentiable rule mining on knowledge graphs",
            "venue": "Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information",
            "year": 2019
        },
        {
            "authors": [
                "Tara Safavi",
                "Jing Zhu",
                "Danai Koutra."
            ],
            "title": "Negater: Unsupervised discovery of negatives in commonsense knowledge bases",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event /",
            "year": 2021
        },
        {
            "authors": [
                "Keisuke Sakaguchi",
                "Ronan Le Bras",
                "Chandra Bhagavatula",
                "Yejin Choi."
            ],
            "title": "Winogrande: an adversarial winograd schema challenge at scale",
            "venue": "Commun. ACM, 64(9):99\u2013106.",
            "year": 2021
        },
        {
            "authors": [
                "Maarten Sap",
                "Ronan Le Bras",
                "Emily Allaway",
                "Chandra Bhagavatula",
                "Nicholas Lourie",
                "Hannah Rashkin",
                "Brendan Roof",
                "Noah A. Smith",
                "Yejin Choi."
            ],
            "title": "ATOMIC: an atlas of machine commonsense for if-then reasoning",
            "venue": "The Thirty-Third AAAI Con-",
            "year": 2019
        },
        {
            "authors": [
                "Maarten Sap",
                "Hannah Rashkin",
                "Derek Chen",
                "Ronan Le Bras",
                "Yejin Choi."
            ],
            "title": "Social iqa: Commonsense reasoning about social interactions",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th Inter-",
            "year": 2019
        },
        {
            "authors": [
                "Mike Schuster",
                "Kuldip K. Paliwal."
            ],
            "title": "Bidirectional recurrent neural networks",
            "venue": "IEEE Trans. Signal Process., 45(11):2673\u20132681.",
            "year": 1997
        },
        {
            "authors": [
                "Jianhao Shen",
                "Chenguang Wang",
                "Linyuan Gong",
                "Dawn Song."
            ],
            "title": "Joint language semantic and structure embedding for knowledge graph completion",
            "venue": "Proceedings of the 29th International Conference on Computational Linguistics, COLING 2022,",
            "year": 2022
        },
        {
            "authors": [
                "Robyn Speer",
                "Joshua Chin",
                "Catherine Havasi"
            ],
            "title": "Conceptnet 5.5: An open multilingual graph of general knowledge",
            "venue": "In Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence, February",
            "year": 2017
        },
        {
            "authors": [
                "Ying Su",
                "Zihao Wang",
                "Tianqing Fang",
                "Hongming Zhang",
                "Yangqiu Song",
                "Tong Zhang."
            ],
            "title": "MICO: A multi-alternative contrastive learning framework for commonsense knowledge representation",
            "venue": "Findings of the Association for Computational Linguistics:",
            "year": 2022
        },
        {
            "authors": [
                "Zhiqing Sun",
                "Zhi-Hong Deng",
                "Jian-Yun Nie",
                "Jian Tang."
            ],
            "title": "Rotate: Knowledge graph embedding by relational rotation in complex space",
            "venue": "7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "Alon Talmor",
                "Jonathan Herzig",
                "Nicholas Lourie",
                "Jonathan Berant."
            ],
            "title": "Commonsenseqa: A question answering challenge targeting commonsense knowledge",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
            "year": 2019
        },
        {
            "authors": [
                "Niket Tandon",
                "Gerard de Melo",
                "Gerhard Weikum."
            ],
            "title": "Acquiring comparative commonsense knowledge from the web",
            "venue": "Proceedings of the TwentyEighth AAAI Conference on Artificial Intelligence, July 27 -31, 2014, Qu\u00e9bec City, Qu\u00e9bec, Canada,",
            "year": 2014
        },
        {
            "authors": [
                "Niket Tandon",
                "Gerard de Melo",
                "Gerhard Weikum"
            ],
            "title": "Webchild 2.0 : Fine-grained commonsense knowledge distillation",
            "venue": "In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics,",
            "year": 2017
        },
        {
            "authors": [
                "Th\u00e9o Trouillon",
                "Johannes Welbl",
                "Sebastian Riedel",
                "\u00c9ric Gaussier",
                "Guillaume Bouchard."
            ],
            "title": "Complex embeddings for simple link prediction",
            "venue": "Proceedings of the 33nd International Conference on Machine Learning, ICML 2016, New York City, NY,",
            "year": 2016
        },
        {
            "authors": [
                "Petar Velickovic",
                "Guillem Cucurull",
                "Arantxa Casanova",
                "Adriana Romero",
                "Pietro Li\u00f2",
                "Yoshua Bengio."
            ],
            "title": "Graph attention networks",
            "venue": "6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018,",
            "year": 2018
        },
        {
            "authors": [
                "Bin Wang",
                "Guangtao Wang",
                "Jing Huang",
                "Jiaxuan You",
                "Jure Leskovec",
                "C.-C. Jay Kuo."
            ],
            "title": "Inductive learning on commonsense knowledge graph completion",
            "venue": "International Joint Conference on Neural Networks, IJCNN 2021, Shenzhen, China, July 18-22,",
            "year": 2021
        },
        {
            "authors": [
                "Bo Wang",
                "Tao Shen",
                "Guodong Long",
                "Tianyi Zhou",
                "Ying Wang",
                "Yi Chang."
            ],
            "title": "Structure-augmented text representation learning for efficient knowledge graph completion",
            "venue": "WWW \u201921: The Web Conference 2021, Virtual Event / Ljubljana, Slovenia, April",
            "year": 2021
        },
        {
            "authors": [
                "Weiqi Wang",
                "Tianqing Fang",
                "Wenxuan Ding",
                "Baixuan Xu",
                "Xin Liu",
                "Yangqiu Song",
                "Antoine Bosselut."
            ],
            "title": "CAR: conceptualization-augmented reasoner for zero-shot commonsense question answering",
            "venue": "CoRR, abs/2305.14869.",
            "year": 2023
        },
        {
            "authors": [
                "Weiqi Wang",
                "Tianqing Fang",
                "Baixuan Xu",
                "Chun Yi Louis Bo",
                "Yangqiu Song",
                "Lei Chen."
            ],
            "title": "CAT: A contextualized conceptualization and instantiation framework for commonsense reasoning",
            "venue": "Proceedings of the 61st Annual Meeting of the As-",
            "year": 2023
        },
        {
            "authors": [
                "Zhaowei Wang",
                "Quyet V. Do",
                "Hongming Zhang",
                "Jiayao Zhang",
                "Weiqi Wang",
                "Tianqing Fang",
                "Yangqiu Song",
                "Ginny Y. Wong",
                "Simon See"
            ],
            "title": "COLA: contextualized commonsense causal reasoning",
            "year": 2023
        },
        {
            "authors": [
                "Zhigang Wang",
                "Juan-Zi Li."
            ],
            "title": "Text-enhanced representation learning for knowledge graph",
            "venue": "Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence, IJCAI 2016, New York, NY, USA, 9-15 July 2016, pages 1293\u20131299.",
            "year": 2016
        },
        {
            "authors": [
                "Jason Wei",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Maarten Bosma",
                "Brian Ichter",
                "Fei Xia",
                "Ed H. Chi",
                "Quoc V. Le",
                "Denny Zhou."
            ],
            "title": "Chain-of-thought prompting elicits reasoning in large language models",
            "venue": "NeurIPS.",
            "year": 2022
        },
        {
            "authors": [
                "Peter West",
                "Chandra Bhagavatula",
                "Jack Hessel",
                "Jena D. Hwang",
                "Liwei Jiang",
                "Ronan Le Bras",
                "Ximing Lu",
                "Sean Welleck",
                "Yejin Choi."
            ],
            "title": "Symbolic knowledge distillation: from general language models to commonsense models",
            "venue": "Proceedings of the",
            "year": 2022
        },
        {
            "authors": [
                "Scao",
                "Sylvain Gugger",
                "Mariama Drame",
                "Quentin Lhoest",
                "Alexander M. Rush."
            ],
            "title": "Transformers: State-of-the-art natural language processing",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demon-",
            "year": 2020
        },
        {
            "authors": [
                "Ruobing Xie",
                "Zhiyuan Liu",
                "Fen Lin",
                "Leyu Lin."
            ],
            "title": "Does william shakespeare REALLY write hamlet? knowledge representation learning with confidence",
            "venue": "Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th in-",
            "year": 2018
        },
        {
            "authors": [
                "Bishan Yang",
                "Wen-tau Yih",
                "Xiaodong He",
                "Jianfeng Gao",
                "Li Deng."
            ],
            "title": "Embedding entities and relations for learning and inference in knowledge bases",
            "venue": "3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9,",
            "year": 2015
        },
        {
            "authors": [
                "Fan Yang",
                "Zhilin Yang",
                "William W. Cohen."
            ],
            "title": "Differentiable learning of logical rules for knowledge base reasoning",
            "venue": "Advances in Neural Information",
            "year": 2017
        },
        {
            "authors": [
                "Liang Yao",
                "Chengsheng Mao",
                "Yuan Luo."
            ],
            "title": "KG-BERT: BERT for knowledge graph completion",
            "venue": "CoRR, abs/1909.03193.",
            "year": 2019
        },
        {
            "authors": [
                "Michihiro Yasunaga",
                "Antoine Bosselut",
                "Hongyu Ren",
                "Xikun Zhang",
                "Christopher D. Manning",
                "Percy Liang",
                "Jure Leskovec."
            ],
            "title": "Deep bidirectional language-knowledge graph pretraining",
            "venue": "NeurIPS.",
            "year": 2022
        },
        {
            "authors": [
                "Changlong Yu",
                "Weiqi Wang",
                "Xin Liu",
                "Jiaxin Bai",
                "Yangqiu Song",
                "Zheng Li",
                "Yifan Gao",
                "Tianyu Cao",
                "Bing Yin."
            ],
            "title": "Folkscope: Intention knowledge graph construction for discovering e-commerce commonsense",
            "venue": "CoRR, abs/2211.08316.",
            "year": 2022
        },
        {
            "authors": [
                "Hengtong Zhang",
                "Tianhang Zheng",
                "Jing Gao",
                "Chenglin Miao",
                "Lu Su",
                "Yaliang Li",
                "Kui Ren."
            ],
            "title": "Data poisoning attack against knowledge graph embedding",
            "venue": "Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJ-",
            "year": 2019
        },
        {
            "authors": [
                "Hongming Zhang",
                "Xin Liu",
                "Haojie Pan",
                "Haowen Ke",
                "Jiefu Ou",
                "Tianqing Fang",
                "Yangqiu Song."
            ],
            "title": "ASER: towards large-scale commonsense knowledge acquisition via higher-order selectional preference over eventualities",
            "venue": "Artificial Intelligence,",
            "year": 2022
        },
        {
            "authors": [
                "Houyu Zhang",
                "Zhenghao Liu",
                "Chenyan Xiong",
                "Zhiyuan Liu."
            ],
            "title": "Grounded conversation generation as guided traverses in commonsense knowledge graphs",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,",
            "year": 2020
        },
        {
            "authors": [
                "Qinggang Zhang",
                "Junnan Dong",
                "Keyu Duan",
                "Xiao Huang",
                "Yezi Liu",
                "Linchuan Xu."
            ],
            "title": "Contrastive knowledge graph error detection",
            "venue": "Proceedings of the 31st ACM International Conference on Information & Knowledge Management, Atlanta, GA,",
            "year": 2022
        },
        {
            "authors": [
                "Wen Zhang",
                "Bibek Paudel",
                "Liang Wang",
                "Jiaoyan Chen",
                "Hai Zhu",
                "Wei Zhang",
                "Abraham Bernstein",
                "Huajun Chen."
            ],
            "title": "Iteratively learning embeddings and rules for knowledge graph reasoning",
            "venue": "The World Wide Web Conference, WWW 2019, San Fran-",
            "year": 2019
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "The emergence of Commonsense Knowledge Graphs (CSKGs) has significantly impacted the field of commonsense reasoning (Liu et al., 2021; Zhang et al., 2020) as CSKGs provide commonsense knowledge that is often not explicitly stated in the text and difficult for machines to capture systematically (Davis and Marcus, 2015). While existing methods bank on expensive and timeconsuming crowdsourcing to collect commonsense knowledge (Sap et al., 2019a; Mostafazadeh et al., 2020), it remains infeasible to obtain CSKGs that are large enough to cover numerous entities and situations in the world (He et al., 2022; Tandon et al., 2014). To overcome this limitation, various automatic CSKG construction methods have been\npaint house brush\nIsA\nUsedFor\nRene Magritte\nIsA\nCapableOf\npaint door\nUsedFor\nbrush your teeth\npainter CapableOf\npaint\nHas Prer\nequ isite\nrun out of paint\nNotDesires\nCapableOf\n\u274c\nFigure 1: A subgraph of CSKG with two instances of noise. One noise is (paint, UsedFor, brush your teeth), as there should be no relation between these two nodes. Another noise is (painter, IsA, paint house) because the correct relation should be CapableOf.\nproposed to acquire commonsense knowledge at scale (Bosselut et al., 2019), including prompting Large Language Model (LLM) (West et al., 2022; Yu et al., 2022), rule mining from massive corpora (Tandon et al., 2017; Zhang et al., 2022a), and knowledge graph population (Fang et al., 2021a,b, 2023). Although those methods are effective, they still suffer from noises introduced by construction bias and the lack of human supervision. Therefore, how to identify noise in large-scale CSKG accurately and efficiently becomes a crucial research question.\nTo tackle this issue, noise detection algorithms have been proposed for conventional entitybased KGs by primarily adopting two approaches: learning-based and rule-based. Learning-based methods like TransE (Bordes et al., 2013) learn representations of entities and relations that adhere to specific relation compositions like translation assumption or relational rotation. To enhance their performance, researchers also incorporate local information around the head and tail entities, such as different paths from head to tail (Lin et al., 2015; Xie et al., 2018; Jia et al., 2019) and neighboring triples (Zhang et al., 2022b). These methods aim to improve their ability to capture the complex relationships between entities in KGs. However,\nthey are not easily adaptable to the unique characteristics of CSKGs. In CSKGs, nodes are noncanonicalized, free-form text, meaning nodes with different descriptions may have related semantics. As illustrated in Figure 1, \u201cpaint door\u201d and \u201cpaint house\u201d are two distinct nodes but imply related semantics (Speer et al., 2017). Additionally, when detecting noise (paint, UsedFor, brush your teeth), \u201cbrush your teeth\u201d is an isolated node that cannot be distinguished based on any structural information. Only through the power of a language model can it be learned that \u201cpaint\u201d and \u201cbrush your teeth\u201d are uncorrelated, thus detecting such noise. The aforementioned methods overlook this semantic information and cannot generalize to semantically similar events with diverse structural information.\nOn the other hand, rule-based methods utilize logical rules in KGs for noise detection. For instance, as shown in Figure 1, the correct relation between \u201cpainter\u201d and \u201cpaint house\u201d should be CapableOf. This can be easily detected through the learned logical rule: CapableOf(x, y) \u2190 CapableOf(x, z)\u2227HasPrerequisite(y, z). Belth et al. (2020) similarly propose an approach based on information theory that extracts sub-graph patterns to identify the noise. However, the sparsity of edges in CSKGs (Malaviya et al., 2020) posits a serious challenge to learning structural information well, as the number of learnable rules decreases significantly. This requires a generalizable rulelearning ability at the noise detector side to expand the rule bank accordingly, which is currently lacking. Therefore, applying noise detection models for KGs directly to CSKGs can result in incomplete learning of both semantic and structural information in the CSKGs.\nIn order to detect noises in CSKGs effectively, it is important to consider both the semantic information and the global and local structural information jointly. However, these factors have not been given enough importance in existing denoising approaches. To address this gap, we propose GOLD (Global and Local-aware Denoising), a CSKG noise detector that uses a PLM-based triple encoder and two noise detectors that take into account both global and local structures (Section 4). Specifically, the triple encoder extracts the semantic information contained in the free-text formatted nodes in CSKGs. To identify correct patterns, the global detector uses high-frequency patterns extracted through rule mining, which intrinsically uses a\nrule encoder to generalize the learned rules and guide noise detection. The local detector, inspired by Zhang et al. (2022b), adopts a graph neural network to efficiently measure the similarity of aggregated semantic information of neighboring triples of the head and tail nodes to help detect noise. Extensive experiments on two manually synthesized noisy-CSKG benchmarks demonstrate the efficacy and state-of-the-art performance of GOLD. Further experiments and analyses with ATOMIC10X (West et al., 2022), a large-scale CSKG distilled from GPT3, demonstrates its proficiency in identifying noise within real-world CSKGs, while also yielding advantages in the downstream zero-shot commonsense question-answering task.\nIn summary, in this paper, we make the following contributions:\n\u2022 We introduce a new task: CSKG denoising, which can be applied to various CSKG construction and LLM distillation works.\n\u2022 We propose a novel framework GOLD, which outperforms all existing methods (Section 6.1) and LLMs (Section 6.3).\n\u2022 We show that GOLD successfully detects noises in real-world CSKGs (Section 6.5) and such denoising extrinsically benefits downstream zero-shot commonsense questionanswering task (Section 6.4)."
        },
        {
            "heading": "2 Related Work",
            "text": ""
        },
        {
            "heading": "2.1 Knowledge Graph Noise Detection",
            "text": "Many existing knowledge graph noise detection approaches utilize some local information while simultaneously training embeddings to satisfy the relational assumption. Path information is the most commonly used type of local information, as the reachable path from the head entity to the tail entity has been proven crucial for noise detection in knowledge graphs (Lin et al., 2015; Xie et al., 2018; Jia et al., 2019). Zhang et al. (2022b) show that contrastive learning between the information of neighboring triples of the head and tail entities is more effective because of the triple-level contrasting instead of entity or graph-level, leading to superior performance compared to all path-based methods. Clustering methods (Ge et al., 2020) are also used to partition noise from triples, and an active learning-based classification model is proposed to detect and repair dirty data. While these methods consider local information, our work also accounts for semantic information and the global\ninformation of the knowledge graph to guide noise detection, better mitigating the impact of noise on local information. Regarding direct noise detection in CSKGs, Romero and Razniewski (2023) study the problem of mapping the open KB into the structured schema of an existing one, while our methods only use the CSKG to be denoised itself, not relying on any other CSKG."
        },
        {
            "heading": "2.2 Knowledge Graph Rule Mining",
            "text": "Another related line of work is knowledge graph rule mining, which is essential to our method. This task has received great attention in the knowledge graph completion. The first category of methods is Inductive Logical Programming (ILP) (Muggleton and Raedt, 1994), which uses inductive and logical reasoning to learn rules. On the other hand, AMIE (Gal\u00e1rraga et al., 2013) proposes a method of association rule mining, which explores frequently occurring patterns in the knowledge graph to extract rules and counts the number of instances supporting the discovered rules and their confidence scores. AMIE+ (Gal\u00e1rraga et al., 2015) and AMIE 3 (Lajus et al., 2020) further improve upon this method by introducing several pruning optimizations, allowing them to scale well to large knowledge graphs. SWARM (Barati et al., 2017) also introduces a statistical method for rule mining in large-scale knowledge graphs that focuses on both instance-level and schema-level patterns. However, it requires type information of entities, which is not available in the CSKG and, therefore, cannot be applied to CSKG. Recently, with the success of deep learning, the idea of ILP has been neuralized, resulting in a series of neuralsymbolic methods. Neural LP (Yang et al., 2017) and DRUM (Sadeghian et al., 2019) both propose end-to-end differentiable models for learning firstorder logical rules for knowledge graph reasoning. Despite the great success achieved by the combination of Recurrent Neural Network (RNN) (Schuster and Paliwal, 1997) with rule mining (Qu et al., 2021; Cheng et al., 2022, 2023), neuralized methods are intuitively hard to interpret due to the confidence scores output by neural networks. Furthermore, jointly learning rules and embedding has been proven to be effective (Guo et al., 2016), and iteratively learning between them can also promote the effectiveness of both (Guo et al., 2018; Zhang et al., 2019b). For noise detection in knowledge graphs, Belth et al. (2020) learn higher-order pat-\nterns based on subgraphs to help refine knowledge graphs, but it requires type information of node and hence cannot be applied to the CSKG."
        },
        {
            "heading": "2.3 Knowledge Graph Completion with Pretrained Language Models",
            "text": "Aside from specifically designed noise-detection methods, the line of works targetting KG completion can also be transferred to tackle noisedetection tasks. Previous research has shown that PLMs can achieve outstanding performance on KG completion tasks for both conventional KGs (Wang and Li, 2016; An et al., 2018; Yao et al., 2019; Wang et al., 2021b; Markowitz et al., 2022; Shen et al., 2022) and CSKGs (Su et al., 2022; Yasunaga et al., 2022) due to their ability to capture linguistic patterns and semantic information. However, two limitations still exist. First, performing edge classification using a PLM requires optimizing a large number of parameters on textual data that has been transformed from edges in CSKGs. Such fine-tuning is not only computationally expensive but also incapable of learning structural features in graphs, which are essential for accurately identifying and classifying edges. Second, recent studies (Safavi et al., 2021; Chen et al., 2023) have shown that language models, regardless of their scale, struggle to acquire implicit negative knowledge through costly language modeling. This makes them potentially vulnerable to noise detection tasks, as these noises typically belong to negative knowledge. Therefore, more sophisticated manipulations of the semantic information extracted by PLMs are needed to leverage them for noise detection tasks efficiently."
        },
        {
            "heading": "3 Problem Definition",
            "text": "Noises in CSKG Commonsense knowledge represents not only basic facts in traditional knowledge graphs but also the understanding possessed by most people (Liu and Singh, 2004), we evaluate whether a triple is a noise from two perspectives:\n\u2022 Truthfulness: It should be consistent with objective facts. For example, (London, IsA, city in France) is not true because London is not in France but in England. \u2022 Reasonability: It should align with logical reasoning and be consistent with cultural norms. For example, (read newspaper, MotivatedByGoal, want to eat vegetables) is not logically reasonable. The two nodes are\nnot directly related, and there is no clear relationship between them. Another example is that (hippo, AtLocation, in kitchen) violates our understanding and experience of reality because hippos are large mammals that are highly unlikely and unrealistic to be found in a kitchen.\nIf a triple fails to satisfy any of the aspects mentioned above, we define it as noise.\nCSKG Denoising A CSKG can be represented as G = (V,R, E), where V is a set of nodes,R is a set of relations, and E \u2286 V \u00d7 R \u00d7 V is a set of triples or edges. Given a triple (h, r, t) \u2208 E in a CSKG, we concatenate the language descriptions of h, r, and t and determine whether this description conforms to commonsense. We note that each triple violates commonsense to a different degree, and we define noise detection as a ranking problem to standardize the evaluation process better. Thus, we model noise detection as a ranking process where a scoring function f : E \u2192 R indicates the likelihood of the triple being noisy."
        },
        {
            "heading": "4 The GOLD Method",
            "text": "Our proposed method GOLD comprises four components: triple encoder, global noise detector, local noise detector, and comprehensive evaluation scorer. An overview is presented in Figure 2. First, we leverage a PLM to encode the natural language descriptions of nodes and relations in CSKGs to obtain their sentence embeddings, thus further encoding the triples. When detecting noise, we evaluate the likelihood of a triple being noise from both a global and local perspective. From the global perspective, we aim to identify high-frequency patterns in the knowledge graph, as a small amount of noise is less likely to affect correct high-frequency patterns (Belth et al., 2020). To accomplish this, we employ rule mining to extract high-quality rules from the knowledge graph. From the local perspective, we adopt graph networks to aggregate the neighboring triple information around both the head and tail nodes of a given edge, allowing us to estimate if there is any correlation. Finally, based on these two aspects of detection, we obtain a comprehensive score indicating the noise level."
        },
        {
            "heading": "4.1 Triple Encoder",
            "text": "As we mentioned earlier, the nodes in CSKG are linguistic descriptions that are not restricted to any\nspecific canonicalized form. If their semantic information is ignored, it will inevitably affect the accuracy of noise detection. Therefore, the Triple Encoder (TE) employs a PLM to encode the semantics of each node and relation. For instance, considering an example of triple (h, r, t), their embeddings are defined as:\nsh = LM(h), sr = LM(r), st = LM(t), (1)\nwhere LM is a frozen PLM that maps the input text to an embedding. To strike a balance between capturing the relationship between h, r, and t and maintaining model efficiency, we opt an efficient RNN as our encoding method for the CSKG triples:\neh, er, et = RNN(sh, sr, st). (2)\nThen, we simply concatenate them together to get the representation of the triple (h, r, t):\nTE(h, r, t) = [eh\u2225er\u2225et] . (3)"
        },
        {
            "heading": "4.2 Global Rule Mining",
            "text": "To detect noisy triples, scoring (h, r, t) only from a local perspective, such as modeling the neighbors of h and t, or analyzing the path from h to t may not be sufficient to eliminate the interference of noisy triples, as it is difficult to determine what is noise from local structures alone. In commonsense knowledge graphs, the noise ratio should not be excessively high. So, learning high-frequency patterns from a global perspective is likely to cover correct triples. In turn, patterns can guide us in identifying the noise data when detecting violations.\nTo incorporate the global information of the entire CSKG when determining the probability of a triple being noise, we use the method of rule mining to first extract high-frequency, high-confidence, and interpretable rules from the CSKG. Taking into account both the interpretability and efficiency of the model, we employ AMIE 3 (Lajus et al., 2020), a rule mining method based on the frequency of each pattern, to generate logical rules automatically with the following format:\nrh(x, y)\u2190 rb1(x, z1) \u2227 \u00b7 \u00b7 \u00b7 \u2227 rbk (zk\u22121, y), (4)\nwhere rh(x, y) is rule head and rb1(x, z1) \u2227 \u00b7 \u00b7 \u00b7 \u2227 rbk(zk\u22121, y) is rule body, x, y, z1, . . ., zk\u22121 are nodes, rh, rb1 . . ., rbk are relations. As depicted in\nEquation (4), the rule body consists of k triples:\nt1 = (x, b1, z1), t2 = (z1, b2, z2), \u00b7 \u00b7 \u00b7 , tk = (zk\u22121, bk, y). (5) To address the issue of poor generalization of mined rules due to sparsity in edges in CSKGs, we consider a rule body rb as a sequence and employ an RNN as the neuralized Rule Encoder (RE) to generalize the rules:\nRE(rb) = RNN (TE(t1),TE(t2), \u00b7 \u00b7 \u00b7 ,TE(tk)) . (6)\nSpecifically, for each relation as the rule head, we retain the top krules rules with the highest confidence score given by AMIE 3 for training the rule encoder. In cases where there is no corresponding instance for a rule body, we fill all triples in the rule body with (x, h, y) to align the energy scores of the other triples. And we believe that a well-generalized rule encoder can learn a representation that can explicitly infer the rule head rh, i.e., (x, h, y). Hence, we align the dimensions of the outputs from TE and RE and define the energy function as follows:\nEglobal(h, r, t) = \u2211\n(rb,rh)\n\u2225RE(rb)\u2212 TE(rh)\u22252. (7)"
        },
        {
            "heading": "4.3 Local Neigboring Triple Learning",
            "text": "Structural information plays a significant role in enhancing performance for KG noise detection tasks. Most methods require that the relationship between two nodes should be equivalent to a translation between their embeddings (Xie et al., 2018; Zhang et al., 2022b). We relax this restriction and aim to determine some level of contextual correlation between two related nodes. As for the specific relation, our global rule mining component will learn its corresponding representation. To capture the contextual semantic information of the triples around nodes, we adopt Graph Attention Network (GAT) (Velickovic et al., 2018) to aggregate the information of the neighboring triples.\nWe use a transformation matrix W \u2208 RF\u00d7d to map the i-th triple (hi, ri, ti) to the embedding\nvi = W [ehi ||eri ||eti ] (8)\nwhere F is the dimension of the latent space and d is the embedding dimension of the triple, and perform the self-attention function a : RF \u00d7RF \u2192 R on the triples to get wij = a (vi,vj), which indicates the context of the j-th triple to the i-th triple. To compute the attention of the neighboring\ntriples on the head and tail nodes, respectively, we define the neighboring triples of the node e asNe = {(h\u0303, r\u0303, t\u0303)|h\u0303 = e\u2228 t\u0303 = e}, and then use the softmax function to normalize the coefficients:\n\u03b1ij(h) = softmaxj(h)(wij(h))\n= exp(wij(h))\u2211\nk(h)\u2208Nhi exp(wik(h))\n,\n\u03b2ij(t) = softmaxj(t)(wij(t))\n= exp(wij(t))\u2211\nk(t)\u2208Nti exp(wik(t))\n,\n(9)\nwhere \u03b1ij(h) represents the attention of the j (h)th triple on node hi, while \u03b2ij(t) represents the attention of the j(t)-th triple on node ti. It is worth noting that the j(h)-th triple is required to meet the condition of being a neighbor of node hi, and similarly, the j(t)-th triple must also be a neighbor of node ti.\nWe use the normalized attention coefficients to calculate a linear combination of the corresponding embeddings, which then serves as the final output:\npi = \u03c3 (\u2211 j(h)\u2208Nhi \u03b1ij(h)vj(h) ) ,\nqi = \u03c3 (\u2211 j(t)\u2208Nti \u03b2ij(t)vj(t) ) ,\n(10)\nwhere pi is obtained from the perspective of the neighbors of node hi, qi is obtained from the perspective of the neighbors of node ti, and \u03c3 represents a nonlinearity.\nWe simply employ the Euclidean distance between them to measure the correlation between hi and ti and obtain the energy function of triple (hi, ri, ti) under local perception as follows:\nElocal(hi, ri, ti) = \u2225pi \u2212 qi\u22252. (11)"
        },
        {
            "heading": "4.4 Jointly Learning and Optimization",
            "text": "The overall energy function of each triple (h, r, t) is obtained by combining the global and local energy functions. We have:\nE(h, r, t) = Eglobal(h, r, t) + \u03bbElocal(h, r, t), (12)\nwhere \u03bb is a hyperparameter. We use negative sampling to minimize the margin-based ranking loss\nL = \u2211 i+\u2208E \u2211 i\u2212\u2208E\ni+\nmax ( 0, \u03b3 + E(i+)\u2212 E(i\u2212) ) , (13)\nwhere i+ represents a positive triple (h, r, t), and i\u2212 represents a negative triple. We follow the setting of DistMult (Yang et al., 2015): a set of neg-\native examples Ei+ is constructed based on i+ by replacing either h or t with a random node e\u0303 \u2208 V: Ei+ = {(e\u0303, r, t)|e\u0303 \u2208 V} \u222a {(h, r, e\u0303)|e\u0303 \u2208 V} \u2212 E . (14)"
        },
        {
            "heading": "5 Experimental Setup",
            "text": ""
        },
        {
            "heading": "5.1 Datasets",
            "text": "To evaluate the detection capability of denoising models, we follow the method introduced by Xie et al. (2018) to construct benchmark datasets for evaluation, which involves generating noise with manually defined sampling rules and injecting it back into the original CSKG. We select ConceptNet (Speer et al., 2017) and ATOMIC (Sap et al., 2019a) as two source CSKGs due to their manageable scale and diverse coverage of edge semantics, including various entities, events, and commonsense relations. Since these manually curated CSKGs do not contain noise naturally, we synthesize noise for each CSKG separately using meticulously designed rules, as done by Jia et al. (2019), that incorporate modifications on existing edges and random negative sampling. This approach, as demonstrated by Jia et al. (2019), ensures that the resulting noises not only maintain being highly informative, thus more challenging for the model to detect, but also stimulate several types of noise that may appear in real-world CSKGs. More details for noise synthesis are provided in Appendix A.1."
        },
        {
            "heading": "5.2 Evaluation Metrics",
            "text": "We use two common metrics to evaluate the performance of all methods.\nRecall@k. Given that there are k noisy triples in the dataset, we sort all triples by their score in descending order, where a higher score indicates a higher probability of being a noisy triple. We then select the top k triples and calculate the recall rate:\nRecall@k = | Noisy Triples in the top-k list |\nk . (15)\nAUC. Area Under the ROC Curve (AUC) measures the probability that a model will assign a higher score to a randomly chosen noisy triple than a randomly chosen positive triple. A higher AUC score indicates a better performance."
        },
        {
            "heading": "5.3 Competing Methods",
            "text": "We compare our model with state-of-the-art models, which can be mainly divided into three categories: (i) structure embedding-based methods that are unaware of noise, including TransE (Bordes\net al., 2013), DistMult (Yang et al., 2015), ComplEx (Trouillon et al., 2016), and RotateE (Sun et al., 2019); (ii) embedding-based methods that are aware of noise, including CKRL (Xie et al., 2018) and CAGED (Zhang et al., 2022b); (iii) language model-based methods that encode both semantic and structural embeddings and are unaware of noise, including KG-BERT (Yao et al., 2019) and LASS (Shen et al., 2022). KGist (Belth et al., 2020) as a rule-based method requires node type information, which is unavailable in the CSKG, making it infeasible to use as a baseline. More detailed descriptions are in Appendix A.2."
        },
        {
            "heading": "5.4 Implementation Details",
            "text": "We leverage three families of PLMs from the Huggingface Library (Wolf et al., 2020) to build our GOLD framework, including RoBERTa (Liu et al., 2019), DeBERTa-v3 (He et al., 2023), and Sentence-T5 (Ni et al., 2022). Detailed variants of these PLMs are included in Table 1. We train GOLD with an Adam (Kingma and Ba, 2015) optimizer, with the learning rate set to 1e-3. The default number of training epochs is 10, with a margin \u03b3 of 5 and a rule length set to 3. Additionally, we conduct a grid search for \u03bb, ranging from 0 to 1, to find the best hyperparameter for krules from 0 to 500. Further information regarding the implementation is discussed in Appendix A.3."
        },
        {
            "heading": "6 Experiments and Analyses",
            "text": ""
        },
        {
            "heading": "6.1 Main Results",
            "text": "The performance of all models on the six datasets in the noise detection task is shown in Table 1. In general, GOLD can detect noise in CSKG more accurately, outperforming all baseline methods by a large margin. Unlike baseline models based on language models, whose performance significantly increases with the size of the language model, our GOLD method consistently surpasses the baseline across different language model backbones with small performance variation. Specifically, when using the RoBERTa family of language models, our GOLD method achieves an average accuracy improvement of 8.64% and 8.50% compared to LASS methods on the ConceptNet and ATOMIC dataset series, respectively. Among the language models we use, the Sentence-T5-xxl model exhibits the best overall performance, with the highest accuracy improvement over 10.14% and 9.17% on the ConceptNet and ATOMIC dataset series, respectively, compared to the baseline. Additionally, the AUC score also improves by 1.02% and 0.62%."
        },
        {
            "heading": "6.2 Ablation Study",
            "text": "In this section, we conduct an ablation study on the ConceptNet-N10 dataset to evaluate the contribution of each component in our proposed model.\nThe results of this study are presented in Table 2. Overall, we observe that removing any of the components results in varying degrees of performance degradation, emphasizing the essentiality of each component in our GOLD model.\nInfluence of Language Model We remove the PLM from the triple encoder and use random embeddings to encode the information of nodes and relations, obtaining the embeddings sh, sr, st in Equation (1). This results in a 5.7% decrease in the model\u2019s accuracy and a 1.3% decrease in AUC, indicating that the PLM indeed contributes to understanding the semantic information of nodes. It is worth noting that even after removing the language model, the accuracy and AUC still outperform all competing methods.\nInfluence of Global Rule Mining We remove the global rule encoder, which results in a 3.8% decrease in accuracy and a 1.0% decrease in AUC, implying the important role of the rule encoder in guiding noise detection. Furthermore, as we train the rule encoder using the top krules rules with the highest confidence score for each relation from the rules mined by AMIE 3, we test the impact of different values of krules on the accuracy using three datasets from the ConceptNet series. We vary krules among {100, 200, 300, 400, 500}. The results are shown in Figure 3. We observe that when the noise level is relatively low, i.e., in the N5 dataset, krules = 200 achieves the best performance, and adding more rules degrades the model\u2019s performance. However, increasing the number of rules improves the model\u2019s performance to some extent when the noise level is high, such as in the N10 and N20 datasets. We analyze that this is because as the noise level increases, learning local information becomes more prone to being misled. Hence, more rules are needed to provide global guidance.\nInfluence of Local Neighbor Learning Moreover, we remove the local neighbor information learning component, resulting in a significant decrease of 30.1% in accuracy and 5.7% in AUC, demonstrating the crucial role of neighboring triple information in noise detection. More comprehensive ablation studies are in Appendix C."
        },
        {
            "heading": "6.3 Comparison with ChatGPT",
            "text": "Recent breakthroughs in Large Language Models (LLMs), such as GPT-3.5 (Brown et al., 2020; Ouyang et al., 2022) and ChatGPT (OpenAI, 2022), have demonstrated remarkable performance across a diverse range of NLP tasks (Chan et al., 2023; Qin et al., 2023). In light of this, we benchmark these LLMs on our defined noise detection task to establish another competitive baseline for comparison. To accomplish this, we randomly select 1,000 triples from our poisoned ConceptNet-N10 CSKG and ask the LLMs to rank them by iteratively comparing two triples and merge-sorting them (more detailed information in Appendix B). This evaluation setting ensures that the LLMs follow an objective that is mostly identical to GOLD. The results, as shown in Table 3, indicate that both LLMs perform significantly poorly on our task, leaving a substantial gap compared to GOLD. One possible explanation is that these LLMs operate in a zero-shot setting and lack prior knowledge of noisy knowledge contained in CSKGs. This highlights the significance of GOLD, which exhibits a keen sensitivity to noise in CSKGs through fine-tuning."
        },
        {
            "heading": "6.4 Downstream Benefits of Denoising CSKG",
            "text": "We finally validate the effectiveness of our proposed noise detection framework by investigating whether eliminating noise from ATOMIC10X would yield extrinsic benefits for downstream tasks, specifically, zero-shot commonsense QuestionAnswering (QA) (Ma et al., 2021). This task involves performing QA on commonsense benchmarks, such as Abductive NLI (aNLI; Bhagavatula et al., 2020), CommonsenseQA (CSQA; Talmor et al., 2019), PhysicalIQA (PIQA; Bisk et al., 2020), SocialIQA (SIQA; Sap et al., 2019b), and WinoGrande (WG; Sakaguchi et al., 2021), without accessing their respective training data. Ma et al. (2021) proposed a technique that fine-tunes a PLM on synthetic QA pairs constructed from CSKGs, which has been further improved by Kim et al. (2022) using modularized transfer learning and Wang et al. (2023a) with conceptualizations (Wang et al., 2023b). Specifically, the head node and relation of an edge are transformed into a question using natural language templates, and the tail node serves as the ground-truth answer. Distractors are tails of other edges sampled from the same CSKG whose head node does not share common keywords with the question. A PLM is then fine-tuned on such synthetic QA entries using marginal ranking loss to serve as a general QA model. To this extent, we keep the QA synthesis protocol and model training process fixed and ablatively study the role of leveraging different CSKGs, in our case, raw ATOMIC10X and noise-cleaned ATOMIC10X. We use accuracy as the evaluation metric and trained three QA models separately on (1) the original ATOMIC10X, (2) ATOMIC10X denoised with LASS, and (3) ATOMIC10X denoised with GOLD, where the former two served as the baselines. The results are reported in Table 4. We observe that cleaning ATOMIC10X with GOLD outperforms both baselines on average, indicating that denoising CSKG is potentially useful for automatically generated CSKGs and that GOLD is superior to other noise detection frameworks on real-world CSKGs."
        },
        {
            "heading": "6.5 Case Study",
            "text": "We present specific case studies on the mined logical rules and detected noises in the real large-scale CSKG in Appendix D. Those cases directly show the effectiveness of our proposed method."
        },
        {
            "heading": "7 Conclusions",
            "text": "In this paper, we propose GOLD, a noise detection framework leveraging the power of language models, global rules, and local structural information. This method is motivated by the fact that nodes in CSKGs are in free-text format, and correct patterns are unlikely to be drowned out by noise. Experimental results indicate that our method achieves state-of-the-art performances in CSKG noise detection tasks. This method shows promising directions for automatically obtaining a large-scale CSKG with minimal noise, as well as effectively representing knowledge for downstream tasks.\nLimitations\nIn our experiments, we follow the approach of previous noise detection literature (Xie et al., 2018; Jia et al., 2019) and inject synthesized noise back into the original CSKGs. Although this noise injection technique has been deemed reliable in previous works, further investigation is necessary to verify its rigor in the field of commonsense reasoning. This is because such noise can typically be classified as negative commonsense knowledge, which, as suggested by Chen et al. (2023), should be verified by whether it can be grounded as negative knowledge. Alternatively, we could inject noise from the perspective of graph attacks (Zhang et al., 2019a) to increase the difficulty of noise detection and improve the model\u2019s robustness.\nEthics Statement\nThis paper introduces GOLD, a novel denoising framework for CSKG noise detection that is both global and local-aware. The experiments presented in this paper utilize open-source datasets, including ConceptNet, ATOMIC, ATOMIC10X, and five commonsense question-answering benchmarks. The crowdsourced datasets, such as ConceptNet, ATOMIC, and the five commonsense questionanswering benchmarks, have been manually cu-\nrated and further processed to ensure that they are anonymized and desensitized. The experiments align with their intended usage, which is for research purposes. Additionally, while ATOMIC10X is generated using language models, its prompt engineering ensures that no harmful content is generated, which has been verified by manual inspection (West et al., 2022). Therefore, to the best of the authors\u2019 knowledge, we believe that GOLD introduces no additional risk."
        },
        {
            "heading": "Acknowledgements",
            "text": "The authors would like to thank the anonymous reviewers for their valuable feedback. The authors of this paper were supported by the NSFC Fund (U20B2053) from the NSFC of China, the RIF (R6020-19 and R6021-20), and the GRF (16211520 and 16205322) from RGC of Hong Kong. We also thank the UGC Research Matching Grants (RMGS20EG01-D, RMGS20CR11, RMGS20CR12, RMGS20EG19, RMGS20EG21, RMGS23CR05, RMGS23EG08)."
        },
        {
            "heading": "A Experimental Setup Details",
            "text": "A.1 Datasets\nConceptNet ConceptNet, or CN-100K, was first proposed by (Li et al., 2016). It contains Open Mind Common Sense (OMCS) in the ConceptNet 5 dataset. CN-82K dataset (Wang et al., 2021a) is a uniformly sampled version of the CN-100K dataset.\nATOMIC ATOMIC contains over 300K everyday commonsense knowledge nodes, organized as ifthen relations. It proposes nine types of if-then relations to distinguish various aspects of events, such as causality, intents, and mental states. Malaviya et al. constructed a dataset from ATOMIC for the task of CSKG completion.\nIn our experiments, we follow Wang et al. (2021a) to use CN-82K and ATOMIC. Unlike CSKG completion settings, we merge the train, valid, and test split to get training and testing sets because noise detection is a ranking task requiring training and testing on the entire knowledge graph. To introduce noisy triples, we follow Xie et al. (2018) and Jia et al. (2019) to add noisy triples to these two datasets separately manually. Specifically, the noise we generate is divided into four parts, with a probability of 1/4 for randomly generating a new triple (h\u0302, r\u0302, t\u0302) where h\u0302, t\u0302 \u2208 V, r\u0302 \u2208 R, and probabilities of 1/4 each for modifying the head node, relation, or tail node of an existing triple. When modifying an existing triple, we randomly sample a ground truth triple (h, r, t) \u2208 E from the CSKG and then replace one of its components with a randomly chosen node h\u0302, t\u0302 \u2208 V , or relation r\u0302 \u2208 R, to create a new triple (h\u0302, r, t), (h, r\u0302, t) or (h, r, t\u0302). The process of generating noisy triples requires ensuring that they do not exist in the original CSKG. Taking (hotel room, UsedFor, temporary residence) from ConceptNet and (John works long hours, xIntent, to make more money) from ATOMIC as examples, Table 6 presents several examples of noise generated by replacing the head node, relation, and tail nodes, as well as examples of newly generated triples. It can be observed that these noises are still informative and theoretically challenging to detect, aligning with our previous definition of noises in CSKG in Section 3. Hence, we believe that the noise generated through the above method is effective for model training. The\nstatistical information for the datasets is presented in Table 5.\nA.2 Competing Methods We compare GOLD with three categories of algorithms, beginning with four structure embeddingbased methods that are unaware of noise. Here, h, r, t represent the embeddings of the head entity, relation, and tail entity, respectively.\n\u2022 TransE (Bordes et al., 2013) The score function is \u2225h+ r \u2212 t\u2225, where h, r, t \u2208 Rd.\n\u2022 DistMult (Yang et al., 2015) The score function is \u27e8r,h, t\u27e9, where \u27e8\u00b7\u27e9 denotes the generalized dot product, and h, r, t \u2208 Rd.\n\u2022 ComplEx (Trouillon et al., 2016) The score function is R (\u27e8r,h, t\u0304\u27e9), where h, r, t \u2208 Cd.\n\u2022 RotatE (Sun et al., 2019) The score function is \u2225h\u25e6r\u2212t\u22252, where \u25e6 denotes the Hadamard product, and h, r, t \u2208 Cd.\nNext, we consider two embedding-based methods that capture noise using local information:\n\u2022 CKRL (Xie et al., 2018) They introduce the triple confidence and path confidence to conventional translation-based methods for knowledge representation learning.\n\u2022 CAGED (Zhang et al., 2022b) They propose a contrastive learning framework to capture noise by aggregating triple information around the head and tail entities while also learning the traditional translation embedding.\nWe also evaluate our methods against fine-tuned language models:\n\u2022 KG-BERT (Yao et al., 2019) They first propose concatenating the triples into textual descriptions and transforming the representation learning into a triplet classification problem. We evaluate the performance of noise detection by using scores designed for classification.\n\u2022 LASS (Shen et al., 2022) They propose a joint language semantic and structure embedding for knowledge graph completion. We also use the scores designed for triplet classification to evaluate the performance. Experimental results from their paper demonstrate that their\nmodel outperforms other PLM-based methods in triplet classification tasks. Hence, we select it as our baseline. In particular, their model is tested on four backbones, namely BERT-base, BERT-large, RoBERTa-base, and RoBERTalarge. We also conduct experiments on these four backbones.\nA.3 Implemention Details For the embedding-based baseline models, we use the implementation from OpenKE(Han et al., 2018). For the rest, we use the released code corresponding to each paper to perform experiments. In order to align the performance of different models, we set the dimension of all embeddings apart from language models to 100, the number of negative samples to 1, and the batch size to 256. Our model also follows these settings. For the remaining hyperparameters of baseline models, we follow the settings proposed in the original paper and perform a grid search when modifications are necessary."
        },
        {
            "heading": "B Details of the Zero-shot Noise Detection",
            "text": "ChatGPT cannot directly sort a large number of triples, so we implement a merge sort in Algorithm 1 to sort the triples in descending order of their noise level. When comparing the order of two triples, we draw inspirations from Wang et al.\n(2023c) and call the ASKCHATGPT function to employ ChatGPT to choose which triple is more likely to be noisy from two triples. Inspired by chain-ofthought (CoT) prompting (Wei et al., 2022), we guide ChatGPT in the prompt to first provide the specific reasoning process and then compel it to provide the answer. The prompt used for comparing which of the two triples is more likely to be noise is listed in Table 7. We use OpenAI\u2019s API1 to prompt ChatGPT and retrieve its response.\nPrompt"
        },
        {
            "heading": "C Full Results of Ablation Study",
            "text": "In this section, we provide a comprehensive supplementary ablation study. The results of all exper-\n1https://chat.openai.com/\nAlgorithm 1 Merge Sort guided by ChatGPT Input: A triple list L Output: A tiple list sorted from high to low according to the noise level Function: MERGESORT(L) 1: h\u2190 |L|/2 2: Lleft \u2190 MERGESORT(L[1, 2, \u00b7 \u00b7 \u00b7 , h]) 3: Lright \u2190 MERGESORT(L[h+ 1, \u00b7 \u00b7 \u00b7 |L|]) 4: i\u2190 1 5: j \u2190 1 6: for k \u2190 1 to |L| do 7: if i > h then 8: L[k]\u2190 Lright[j] 9: j \u2190 j + 1 10: else if j > h then 11: L[k]\u2190 Lleft[i] 12: i\u2190 i+ 1 13: else if ASKCHATGPT(Li, Lj) = Li then 14: L[k]\u2190 Lleft[i] 15: i\u2190 i+ 1 16: else 17: L[k]\u2190 Lright[j] 18: j \u2190 j + 1 19: end if 20: end for 21: return L\niments conducted on the six datasets are listed in Table 8.\nInfluence of Language Model By removing the PLM from the triple encoder, we observe an average decrease of 6.1% in accuracy on the ConceptNet series datasets and an average decrease of 9.7% on the ATOMIC series datasets. This indicates that PLM has a greater impact on the accuracy of the ATMOIC datasets, as the average number of words per node in ATOMIC is much higher than that in\nConceptNet. Therefore, PLM plays a more crucial role in capturing semantic information.\nInfluence of Global Rule Mining After eliminating the global rule encoder, the accuracy of the ConceptNet series and ATOMIC series datasets decreases by 3.9% and 1.6%, respectively. Our analysis suggests that the lower number of relations in the ATOMIC datasets, only 9 compared to 34 in the ConceptNet datasets, results in a significantly lower number of learnable rules compared to the ConceptNet. As a result, the global rule encoder provides limited assistance in the ATOMIC datasets, and its contribution is not as significant as in the ConceptNet datasets.\nInfluence of Local Neighbor Learning The local neighbor learning component exhibits the highest contribution across all datasets, as evidenced by the average accuracy drops of 33.1% and 21.0% in accuracy, as well as 6.7% and 4.2% in AUC after its removal on ConceptNet series and ATOMIC series datasets, respectively. We believe that the reason why this component has a smaller impact on the ATOMIC datasets is still due to the limited number of relations, leading to a less diverse set of information learned from the neighboring triple information.\nInfluence of Translation Assumption We attempt to investigate whether the model would benefit from the incorporation of a translation assumption, such as the h+ r \u2248 t relation in TransE (Bor-\ndes et al., 2013), where h, r, t represents the embedding of the head entity, relation, and tail entity respectively. Inspired by this, we also integrate an energy function based on the translation assumption into our approach. We design the energy function for the translation part as follows:\nEtranslation(h, r, t) = \u2225eh + er \u2212 et\u22252. (16)\nBy adding Equation (16) to Equation (12), we obtain a new overall energy function as follows:\nE(h, r, t) = Eglobal(h, r, t)\n+ \u03bbElocal(h, r, t) + \u03bb(t)Etranslation(h, r, t),\n(17)\nwhere \u03bb and \u03bb(t) are both hyperparameters. We perform a grid search for them between 0.001 to 1 and report the best results in Table 8. The experimental results indicate that the energy function based on the translation assumption in the form of Equation (16) cannot provide significant assistance to our model. The overall impact on precision is negative, with an average decrease of 0.4%. This suggests that our GOLD method does not need to rely on such translation assumption constraints when performing noise detection task. It can implicitly learn the relationship between nodes using the energy functions of the global and local parts."
        },
        {
            "heading": "D Case Studies",
            "text": "Mined Logical Rules We list the most frequent rules mined from the ConceptNet-N10 dataset using AMIE 3 and present them in Table 9. We can\nobserve that these rules are highly interpretable and not affected by mixed-in noise. Therefore, they can be treated as ground truth to validate the entire knowledge graph (Bai et al., 2023).\nDetected Noise We conduct our proposed GOLD method on the ATOMIC10X dataset and examine the triples with noise levels in the top 1%. We list ten specific examples that violate reasonability (see Section 3) in Table 10. The results show that our method can effectively extract noise triples from a large-scale CSKG."
        }
    ],
    "title": "GOLD: A Global and Local-aware Denoising Framework for Commonsense Knowledge Graph Noise Detection",
    "year": 2023
}