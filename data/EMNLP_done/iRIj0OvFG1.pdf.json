{
    "abstractText": "We present a novel approach to multilingual audio-visual speech recognition tasks by introducing a single model on a multilingual dataset. Motivated by a human cognitive system where humans can intuitively distinguish different languages without any conscious effort or guidance, we propose a model that can capture which language is given as an input speech by distinguishing the inherent similarities and differences between languages. To do so, we design a prompt fine-tuning technique into the largely pre-trained audio-visual representation model so that the network can recognize the language class as well as the speech with the corresponding language. Our work contributes to developing robust and efficient multilingual audio-visual speech recognition systems, reducing the need for language-specific models.",
    "authors": [
        {
            "affiliations": [],
            "name": "Joanna Hong"
        },
        {
            "affiliations": [],
            "name": "Se Jin Park"
        },
        {
            "affiliations": [],
            "name": "Yong Man Ro"
        }
    ],
    "id": "SP:794e0d640d77c4ee87caf29a684a13785d1cfff2",
    "references": [
        {
            "authors": [
                "Triantafyllos Afouras",
                "Joon Son Chung",
                "Andrew Senior",
                "Oriol Vinyals",
                "Andrew Zisserman."
            ],
            "title": "Deep audio-visual speech recognition",
            "venue": "IEEE transactions on pattern analysis and machine intelligence.",
            "year": 2018
        },
        {
            "authors": [
                "Triantafyllos Afouras",
                "Joon Son Chung",
                "Andrew Zisserman."
            ],
            "title": "Lrs3-ted: a large-scale dataset for visual speech recognition",
            "venue": "arXiv preprint arXiv:1809.00496.",
            "year": 2018
        },
        {
            "authors": [
                "Mohamed Anwar",
                "Bowen Shi",
                "Vedanuj Goswami",
                "WeiNing Hsu",
                "Juan Pino",
                "Changhan Wang."
            ],
            "title": "Muavic: A multilingual audio-visual corpus for robust speech recognition and robust speech-to-text translation",
            "venue": "arXiv preprint arXiv:2303.00628.",
            "year": 2023
        },
        {
            "authors": [
                "Alexei Baevski",
                "Yuhao Zhou",
                "Abdelrahman Mohamed",
                "Michael Auli"
            ],
            "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Xize Cheng",
                "Linjun Li",
                "Tao Jin",
                "Rongjie Huang",
                "Wang Lin",
                "Zehan Wang",
                "Huangdai Liu",
                "Ye Wang",
                "Aoxiong Yin",
                "Zhou Zhao"
            ],
            "title": "Mixspeech: Crossmodality self-learning with audio-visual stream mixup for visual speech translation and recognition",
            "year": 2023
        },
        {
            "authors": [
                "Joon Son Chung",
                "Arsha Nagrani",
                "Andrew Zisserman."
            ],
            "title": "Voxceleb2: Deep speaker recognition",
            "venue": "arXiv preprint arXiv:1806.05622.",
            "year": 2018
        },
        {
            "authors": [
                "Alex Graves",
                "Santiago Fern\u00e1ndez",
                "Faustino Gomez",
                "J\u00fcrgen Schmidhuber."
            ],
            "title": "Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks",
            "venue": "Proceedings of the 23rd international conference on Machine learning,",
            "year": 2006
        },
        {
            "authors": [
                "Joanna Hong",
                "Minsu Kim",
                "Jeongsoo Choi",
                "Yong Man Ro."
            ],
            "title": "Watch or listen: Robust audio-visual speech recognition with visual corruption modeling and reliability scoring",
            "venue": "arXiv preprint arXiv:2303.08536.",
            "year": 2023
        },
        {
            "authors": [
                "Minsu Kim",
                "Jeong Hun Yeo",
                "Yong Man Ro."
            ],
            "title": "Distinguishing homophenes using multi-head visualaudio memory for lip reading",
            "venue": "Proceedings of the 36th AAAI Conference on Artificial Intelligence, Vancouver, BC, Canada, volume 22.",
            "year": 2022
        },
        {
            "authors": [
                "Suyoun Kim",
                "Takaaki Hori",
                "Shinji Watanabe."
            ],
            "title": "Joint ctc-attention based end-to-end speech recognition using multi-task learning",
            "venue": "2017 IEEE international conference on acoustics, speech and signal processing (ICASSP), pages 4835\u20134839. IEEE.",
            "year": 2017
        },
        {
            "authors": [
                "Hao Li",
                "Jinguo Zhu",
                "Xiaohu Jiang",
                "Xizhou Zhu",
                "Hongsheng Li",
                "Chun Yuan",
                "Xiaohua Wang",
                "Yu Qiao",
                "Xiaogang Wang",
                "Wenhai Wang"
            ],
            "title": "Uniperceiver v2: A generalist model for large-scale vision and vision-language",
            "year": 2022
        },
        {
            "authors": [
                "Pingchuan Ma",
                "Stavros Petridis",
                "Maja Pantic."
            ],
            "title": "End-to-end audio-visual speech recognition with conformers",
            "venue": "ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 7613\u20137617. IEEE.",
            "year": 2021
        },
        {
            "authors": [
                "Youssef Mroueh",
                "Etienne Marcheret",
                "Vaibhava Goel."
            ],
            "title": "Deep multimodal learning for audio-visual speech recognition",
            "venue": "2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 2130\u20132134. IEEE.",
            "year": 2015
        },
        {
            "authors": [
                "Bowen Shi",
                "Wei-Ning Hsu",
                "Kushal Lakhotia",
                "Abdelrahman Mohamed."
            ],
            "title": "Learning audio-visual speech representation by masked multimodal cluster prediction",
            "venue": "arXiv preprint arXiv:2201.02184.",
            "year": 2022
        },
        {
            "authors": [
                "Andreas Zinonos",
                "Alexandros Haliassos",
                "Pingchuan Ma",
                "Stavros Petridis",
                "Maja Pantic."
            ],
            "title": "Learning cross-lingual visual speech representations",
            "venue": "arXiv preprint arXiv:2303.09455.",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "With the great advancements of deep learning, automatic audio-visual speech recognition (AVSR) technology has achieved remarkable progress (Mroueh et al., 2015; Afouras et al., 2018a; Baevski et al., 2020; Kim et al., 2022; Ma et al., 2021; Hong et al., 2023). It utilizes multimodal inputs, including both audio and visual cues, providing several advantages to the deep learning-based speech recognition branch. One of the benefits is that it can accurately recognize speech in noisy environments, such as crowded restaurants or corrupted video conferencing situations. This capability is critical for advancing the field of automatic speech recognition technology in the future.\nNevertheless, outstanding performances in audio-visual speech recognition have been mostly shown in monolingual datasets, particularly in English. Few recent studies have started focusing on multilingual speech recognition tasks, but they are still in their infancy. One work (Zinonos et al.,\n\u2020Corresponding Author.\n2023) has presented cross-lingual visual speech representation learning and shows multilingual models with more data outperform monolingual ones. The other works, MuAViC (Anwar et al., 2023) and MixSpeech (Cheng et al., 2023), have newly introduced a multilingual audio-visual corpus for speech recognition and speech-to-text translation task.\nWhile the recent multilingual speech recognition studies have shown remarkable advances, they have only focused on pre-training the multilingual speech recognition model or audio-visual speech representation model, followed by fine-tuning with the specific language (Zinonos et al., 2023). This is due to the imbalance of dataset language distribution and each language\u2019s distinctive characteristic. However, producing a language-specific speech recognition model can be time-consuming and inefficient. Most importantly, it does not correspond to a real-life situation, where humans intuitively recognize the language when others are speaking.\nInspired by the human understanding perspective, in this paper, we design a single model multilingual audio-visual speech recognition framework that the model can not only determine which language is taken into the input speech but also recognize the speech correctly. To do so, we newly introduce an audio-visual guiding linguistic representation extractor. With the largely trained audio-visual speech representation model (Shi et al., 2022), we fine-tune the model by utilizing prompts so that the model can extract comprehensive linguistic information from the audio and video inputs. We set only a small amount of downstream task-specific parameters as the learnable parameters for extracting linguistic representation into the input space so that comprehensive linguistic information can be produced. Furthermore, we consider the imbalanced distribution issue that the multilingual datasets contain, with some languages having significantly fewer samples than others. Inspired by (Li et al., 2022), we suggest a weighted objective\nfunction in order to balance each language distribution. By training our model on a diverse set of languages, we aim to capture their inherent similarities and differences, allowing our model to recognize and transcribe speech in multiple languages with greater accuracy and efficiency. We validate the effectiveness of our proposed model using MuAViC (Anwar et al., 2023), a multilingual audio-visual corpus for robust speech recognition and robust speech-to-text translation providing 1200 hours of audio-visual speech in 9 languages. Therefore, our work contributes to developing efficient and robust multilingual audio-visual speech recognition systems. We also believe that our proposed approach has several potential benefits: reducing the need for language-specific models, improving the performance of speech recognition in low-resource languages, and enabling more effective multilingual communication."
        },
        {
            "heading": "2 Methodology",
            "text": "Given the input multilingual video sequence, xv = {x1, . . . , xL} \u2208 RL\u00d7H\u00d7W\u00d7C where L, H , W , and C are the frame length, height, width, and channel sizes, respectively, and the paired input audio sequence, xa \u2208 RS , where S represents the length of audio, we design a model that properly recognizes the given input video and audio with the correct language. We aim to utilize both visual and audio information so that the proposed architecture can successfully recognize not only the language but also the content of the input video. To this end, we initially propose a language prompt adopted from the largely pre-trained audio-visual speech representation model (Shi et al., 2022), which we call it linguistic representation extractor. Further, we design a multilingual transformer decoder given the inputs of language class, linguistic representation, and the combined features from the audio and\nvisual transformer encoders. We will explain the detailed aforementioned techniques in the following subsections."
        },
        {
            "heading": "2.1 Linguistic Representation Extractor",
            "text": "The first thing when recognizing one\u2019s speech in a multinational society is to distinguish the language the speaker is presenting. When identifying the language of the speech, it is important to verify the speaker\u2019s accent, pronunciation, and representative vocabulary from the speaker\u2019s facial movements and audio information. Inspired by the intuitive human understanding procedure, we design a linguistic representation extractor from the largely pretrained audio-visual speech representation model. We add trainable continuous embeddings, so-called prompts, to the original sequence of input features in order to fine-tune the pre-trained model relevant to recognizing multilingual input signals."
        },
        {
            "heading": "2.1.1 Prompt fine-tuning for Linguistic Representation Extractor",
            "text": "For the input multilingual video sequence xv and audio sequence xa, audio features fa \u2208 RT\u00d7D and visual features fv \u2208 RT\u00d7D, are extracted from through Audio Front and Visual Front, respectively.\nfv = Fv(xv) and fa = Fa(xa). (1)\nThe audio features fa and the visual features fv are concatenated followed by layer normalization and linear projection, producing the audio-visual features fav \u2208 RT\u00d7D. Then, we apply Audio-Visual prompts into every layer of the Audio-Visual Transformer Encoder \u03a8, along with the audio-visual features fav. For each audio-visual prompt, we assign language-representative prompts, P \u2208 Rn\u00d7d, which are trained to extract linguistic-relative features from the pre-trained audio-visual representa-\ntion model. Here, n is the number of prompts.\n(_, fav,i) = \u03a8i(Pav,i\u22121, fav,i\u22121) (2)\n(P\u0302av,L, fav,L) = \u03a8L(Pav,L\u22121, fL\u22121), (3)\nfor the i-th layer \u03a8i, where i = 1, 2, . . . , L. We now call eav = (P\u0302av,L, fav,L) \u2208 R(n+T )\u00d7d, audiovisual prompt embedding feature."
        },
        {
            "heading": "2.1.2 Multilingual Classifier for Linguistic Class Prompt",
            "text": "After extracting the audio-visual prompt embedding feature eav, we firstly train the prompt tuning module by updating the gradient of the learnable parameters of the prompts with the classification loss. As indicated in Figure 1, we propose a multilingual classifier in order to distinguish the input language class. The multilingual classifier C consists of four blocks of 1D convolution layer, batch normalization, and Relu activation, followed by two linear layers with Relu activation.\nlogitpred = C(eav). (4)\nThe output language class is logitpred \u2208 Rm, where m represents the number of languages for training. Then, the logit is updated through crossentropy objective function:\nLclass = CE(logitpred, logitgt). (5)\nTherefore, by updating the correct language label, the model can be provided with guidance when recognizing the multilingual speech correctly in the backbone network that will be expressed in further sections."
        },
        {
            "heading": "2.2 Objective Functions",
            "text": "The proposed multilingual AVSR framework is trained in an end-to-end manner. For the objective function, we utilize joint CTC/attention (Kim et al., 2017). CTC (Graves et al., 2006) loss is defined as:\npc(y|x) \u2248 \u03a0Tt=1p(yt|x), (6)\nwith an independent assumption of each output, and attention-based loss is defined as:\npa(y|x) = \u03a0Nj=1p(yj |y<j , x). (7)\nHere, the current prediction is determined by previous predictions and inputs, thus including the learning of the internal language model, where\nN represents the total length of ground-truth text. Then, the total objective can be written as follows,\nLctc = log pa(y|x), (8) Latt = log pc(y|x), (9)\nLtotal = \u03b1 \u00b7 Lctc + (1\u2212 \u03b1) \u00b7 Latt + \u03b2 \u00b7 Lclass, (10)\nwhere \u03b1 and \u03b2 are weight parameters for balancing three loss terms."
        },
        {
            "heading": "2.2.1 Objective Functions for Balancing the Language Distribution",
            "text": "An objective function weight is designed to balance the distribution of language data, due to the issue of language imbalance in the multilingual dataset. This weight \u03b3 is calculated as the inverse root of the data distribution ratio r for each language in each mini-batch:\n\u03b3 = 1\u221a r . (11)\nTherefore, the updated total objective function can be re-written as follows,\nLtotal = \u03b3 \u00b7 (\u03b1 \u00b7 Lctc + (1\u2212 \u03b1) \u00b7 Latt + \u03b2 \u00b7 Lclass). (12)\nThe rationale behind this design comes from the observation that the multilingual dataset often exhibits an uneven distribution of samples across different languages. Thus, when updating the loss during training, it becomes crucial to employ a balancing loss function with a smaller weight for languages that contain a larger number of samples and a larger weight for languages that have fewer samples.\nBy incorporating this weight into the objective function, the model is encouraged to assign greater importance to underrepresented languages during the learning process. This approach aims to mitigate the adverse effects of language imbalance and prevent the model from being biased toward dominant languages. Thus, the model becomes more capable of effectively recognizing and transcribing speech in languages with limited available data."
        },
        {
            "heading": "3 Experimental Setup",
            "text": ""
        },
        {
            "heading": "3.1 Datasets",
            "text": "The MuAViC dataset (Anwar et al., 2023) is a multilingual audio-visual corpus consisting of roughly 1,200 hours of transcribed data spanning 9 languages: English, Arabic, German, Greek, Spanish, French, Italian, Portuguese and Russian. It is collected from TED and TEDx talk recordings, where\nnative or non-native speakers (only one speaker most of the time) deliver public speech on stage and cameras capture stage scenes switching among different viewpoints."
        },
        {
            "heading": "3.2 Implementation Details",
            "text": "We use largely pre-trained visual frontend, audio frontend, and audio-visual transformer encoder (Shi et al., 2022), trained on LRS3-TED (Afouras et al., 2018b) and VoxCeleb2 English (Chung et al., 2018). We fine-tune the encoding models guided by the prompts and the multilingual classifier in an end-to-end manner, such that the prompts learn the meaningful content, i.e., the language class of the input speech and how the speech is delivered, for making the correct prediction in the multilingual scheme. We set \u03b1 = 0.1 and \u03b2 = 10.0. For training and testing, we follow the same noise injection protocol as (Anwar et al., 2023)."
        },
        {
            "heading": "4 Experimental Result",
            "text": "In the experimental result in Table 1, we report the performances of our proposed model with attention loss only and both attention and CTC loss along with the previous multilingual model (Anwar et al., 2023) performance. The monolingual model refers to the model that is separately trained on each language and the multilingual refers to the model that is jointly trained on all the 8 non-English languages. The previous model did not include En in their multilingual model, so it remains blank. Note that we test the provided trained model for reporting previous work performance.\nClean Environment. We evaluate audio-visual speech recognition in a clean environment. As shown in the first section of Table 1, our proposed model outperforms the previous model (Anwar et al., 2023) in several languages, where the greatest improvement (7.91 WER reduction, 14% rel-\native) has been made in one of the low-resourced language, German (DE). Such results demonstrate that the proposed method effectively enables multilingual audio-visual speech recognition in a unified framework, improving the performance in lowresource languages.\nNoisy Environment. In the second section of Table 1, we evaluate the proposed method in a noisy setup. The proposed model achieves average WER of 47.18, excluding English while the previous multilingual model achieves 59.75 WER, which is a 4.31% relative improvement. The performance gap between the previous monolingual model (Anwar et al., 2023) is even smaller with an average of 2.4% relative WER. We contribute such improvement to the language-representative prompt which allows the model to embed language-specific features from the audio-visual input, simulating the monolingual framework."
        },
        {
            "heading": "5 Conclusion",
            "text": "We introduce the multilingual audio-visual speech recognition model by training a single model on a diverse set of languages. The proposed model finetunes a largely trained audio-visual representation model with prompts to provide meaningful language information. It has presented a promising starting point for future research endeavors.\nAcknowledgement This work was partially supported by the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIT) (No.NRF-2022R1A2C2005529), and Institute of Information & communications Technology Planning & Evaluation (IITP) grant funded by the Korea government(MSIT) (No.2022-0-00124, Development of Artificial Intelligence Technology for Self-Improving Competency-Aware Learning Capabilities)."
        }
    ],
    "title": "Intuitive Multilingual Audio-Visual Speech Recognition with a Single-Trained Model",
    "year": 2023
}