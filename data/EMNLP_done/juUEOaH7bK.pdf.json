{
    "abstractText": "A cost-effective alternative to manual data labeling is weak supervision (WS), where data samples are automatically annotated using a predefined set of labeling functions (LFs), rulebased mechanisms that generate artificial labels for the associated classes. In this work, we investigate noise reduction techniques for WS based on the principle of k-fold crossvalidation. We introduce a new algorithm ULF for Unsupervised Labeling Function correction, which denoises WS data by leveraging models trained on all but some LFs to identify and correct biases specific to the held-out LFs. Specifically, ULF refines the allocation of LFs to classes by re-estimating this assignment on highly reliable cross-validated samples. Evaluation on multiple datasets confirms ULF\u2019s effectiveness in enhancing WS learning without the need for manual labeling.1",
    "authors": [
        {
            "affiliations": [],
            "name": "Anastasiia Sedova"
        },
        {
            "affiliations": [],
            "name": "Benjamin Roth"
        }
    ],
    "id": "SP:65ba8cac4523cd13bcbcb2eb67f378e4ef853fee",
    "references": [
        {
            "authors": [
                "T\u00falio C. Alberto",
                "Johannes V. Lochter",
                "Tiago A. Almeida."
            ],
            "title": "Tubespam: Comment spam filtering on youtube",
            "venue": "Proceedings of the 2015 IEEE 14th International Conference on Machine Learning and Applications (ICMLA).",
            "year": 2015
        },
        {
            "authors": [
                "Tiago A. Almeida",
                "Jos\u00e9 Mar\u00eda G. Hidalgo",
                "Akebo Yamakami."
            ],
            "title": "Contributions to the study of sms spam filtering: New collection and results",
            "venue": "Proceedings of the 11th ACM Symposium on Document Engineering, DocEng \u201911.",
            "year": 2011
        },
        {
            "authors": [
                "Abhijeet Awasthi",
                "Sabyasachi Ghosh",
                "Rasna Goyal",
                "Sunita Sarawagi."
            ],
            "title": "Learning from rules generalizing labeled exemplars",
            "venue": "Proceedings of the 8th International Conference on Learning Representations, ICLR 2020.",
            "year": 2020
        },
        {
            "authors": [
                "Benedikt Boecking",
                "Willie Neiswanger",
                "Eric P. Xing",
                "Artur Dubrawski."
            ],
            "title": "Interactive weak supervision: Learning useful heuristics for data labeling",
            "venue": "Proceedings of the 9th International Conference on Learning Representations, ICLR 2021.",
            "year": 2021
        },
        {
            "authors": [
                "Salva R\u00fchling Cachay",
                "Benedikt Boecking",
                "Artur Dubrawski."
            ],
            "title": "End-to-end weak supervision",
            "venue": "Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021.",
            "year": 2021
        },
        {
            "authors": [
                "Oishik Chatterjee",
                "Ganesh Ramakrishnan",
                "Sunita Sarawagi."
            ],
            "title": "Robust data programming with precision-guided labeling functions",
            "venue": "Proceedings of the Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innova-",
            "year": 2020
        },
        {
            "authors": [
                "Siqi Chen",
                "Jun Xiao",
                "Long Chen."
            ],
            "title": "Video scene graph generation from single-frame weak supervision",
            "venue": "The Eleventh International Conference on Learning Representations, ICLR 2023.",
            "year": 2023
        },
        {
            "authors": [
                "David P.A. Corney",
                "Dyaa Albakour",
                "Miguel MartinezAlvarez",
                "Samir Moussa"
            ],
            "title": "What do a million news articles look like",
            "venue": "In Proceedings of the First International Workshop on Recent Trends in News Information Retrieval",
            "year": 2016
        },
        {
            "authors": [
                "Surabhi Datta",
                "Kirk Roberts."
            ],
            "title": "Weakly supervised spatial relation extraction from radiology reports",
            "venue": "JAMIA Open, 6(2). Ooad027.",
            "year": 2023
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association",
            "year": 2019
        },
        {
            "authors": [
                "Jason A. Fries",
                "Ethan Steinberg",
                "Saelig Khattar",
                "Scott L. Fleming",
                "Jose Posada",
                "Alison Callahan",
                "Nigam H. Shah."
            ],
            "title": "Ontology-driven weak supervision for clinical entity classification in electronic health records",
            "venue": "Nature Communications, 12(1).",
            "year": 2021
        },
        {
            "authors": [
                "Daniel Fu",
                "Mayee Chen",
                "Frederic Sala",
                "Sarah Hooper",
                "Kayvon Fatahalian",
                "Christopher Re."
            ],
            "title": "Fast and three-rious: Speeding up weak supervision with triplet methods",
            "venue": "Proceedings of the 37th International Conference on Machine Learning, volume",
            "year": 2020
        },
        {
            "authors": [
                "Michael A. Hedderich",
                "David Ifeoluwa Adelani",
                "Dawei Zhu",
                "Jesujoba O. Alabi",
                "Udia Markus",
                "Dietrich Klakow."
            ],
            "title": "Transfer learning and distant supervision for multilingual transformer models: A study on african languages",
            "venue": "Proceedings of the 2020 Con-",
            "year": 2020
        },
        {
            "authors": [
                "Michael A. Hedderich",
                "Lukas Lange",
                "Dietrich Klakow."
            ],
            "title": "ANEA: distant supervision for low-resource named entity recognition",
            "venue": "CoRR, abs/2102.13129.",
            "year": 2021
        },
        {
            "authors": [
                "Giannis Karamanolakis",
                "Subhabrata Mukherjee",
                "Guoqing Zheng",
                "Ahmed Hassan Awadallah."
            ],
            "title": "Self-training with weak supervision",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics:",
            "year": 2021
        },
        {
            "authors": [
                "Zornitsa Kozareva",
                "Ellen Riloff",
                "Eduard Hovy."
            ],
            "title": "Semantic class learning from the web with hyponym pattern linkage graphs",
            "venue": "Proceedings of ACL-08: HLT. Association for Computational Linguistics.",
            "year": 2008
        },
        {
            "authors": [
                "Xin Li",
                "Dan Roth."
            ],
            "title": "Learning question classifiers",
            "venue": "Proceedings of the 19th International Conference on Computational Linguistics, COLING 2002.",
            "year": 2002
        },
        {
            "authors": [
                "Yankai Lin",
                "Shiqi Shen",
                "Zhiyuan Liu",
                "Huanbo Luan",
                "Maosong Sun."
            ],
            "title": "Neural relation extraction with selective attention over instances",
            "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers).",
            "year": 2016
        },
        {
            "authors": [
                "Alessio Mazzetto",
                "Dylan Sam",
                "Andrew Park",
                "Eli Upfal",
                "Stephen Bach."
            ],
            "title": "Semi-supervised aggregation of dependent weak supervision sources with performance guarantees",
            "venue": "Proceedings of the 24th International Conference on Artificial Intelligence",
            "year": 2021
        },
        {
            "authors": [
                "R\u00e9"
            ],
            "title": "Training complex models with multi-task",
            "year": 2019
        },
        {
            "authors": [
                "Anastasiia Sedova",
                "Lena Zellinger",
                "Benjamin Roth"
            ],
            "title": "Learning for NLP",
            "year": 2021
        },
        {
            "authors": [
                "Sophia Y. Wang",
                "Justin Huang",
                "Hannah Hwang",
                "Wendeng Hu",
                "Shiqi Tao",
                "Tina HernandezBoussard"
            ],
            "title": "Leveraging weak supervision to perform named entity recognition in electronic health records progress notes to identify the ophthalmology",
            "year": 2022
        },
        {
            "authors": [
                "Yisen Wang",
                "Xingjun Ma",
                "Zaiyi Chen",
                "Yuan Luo",
                "Jinfeng Yi",
                "James Bailey."
            ],
            "title": "Symmetric cross entropy for robust learning with noisy labels",
            "venue": "Proceedings of the 2019 IEEE/CVF International Conference on Computer Vision, ICCV.",
            "year": 2019
        },
        {
            "authors": [
                "Zihan Wang",
                "Jingbo Shang",
                "Liyuan Liu",
                "Lihao Lu",
                "Jiacheng Liu",
                "Jiawei Han."
            ],
            "title": "Crossweigh: Training named entity tagger from imperfect annotations",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing",
            "year": 2019
        },
        {
            "authors": [
                "Tzu-Tsung Wong",
                "Po-Yang Yeh."
            ],
            "title": "Reliable accuracy estimates from k -fold cross validation",
            "venue": "IEEE Transactions on Knowledge and Data Engineering, PP:1\u20131.",
            "year": 2019
        },
        {
            "authors": [
                "Yue Yu",
                "Simiao Zuo",
                "Haoming Jiang",
                "Wendi Ren",
                "Tuo Zhao",
                "Chao Zhang."
            ],
            "title": "Fine-tuning pretrained language model with weak supervision: A contrastive-regularized self-training approach",
            "venue": "Proceedings of the 2021 Conference of the North",
            "year": 2021
        },
        {
            "authors": [
                "Jun Yue",
                "Leyuan Fang",
                "Pedram Ghamisi",
                "Weiying Xie",
                "Jun Li",
                "Jocelyn Chanussot",
                "Antonio Plaza."
            ],
            "title": "Optical remote sensing image understanding with weak supervision: Concepts, methods, and perspectives",
            "venue": "IEEE Geoscience and Remote Sensing Maga-",
            "year": 2022
        },
        {
            "authors": [
                "Ziqian Zeng",
                "Weimin Ni",
                "Tianqing Fang",
                "Xiang Li",
                "Xinran Zhao",
                "Yangqiu Song."
            ],
            "title": "Weakly supervised text classification using supervision signals from a language model",
            "venue": "Findings of the Association for Computational Linguistics: NAACL 2022.",
            "year": 2022
        },
        {
            "authors": [
                "Jieyu Zhang",
                "Linxin Song",
                "Alexander Ratner."
            ],
            "title": "Leveraging instance features for label aggregation in programmatic weak supervision",
            "venue": "In Proceedings of the 26th International Conference on Artificial Intelligence and Statistics (AISTATS) 2023.",
            "year": 2023
        },
        {
            "authors": [
                "Jieyu Zhang",
                "Yue Yu",
                "Yinghao Li",
                "Yujing Wang",
                "Yaming Yang",
                "Mao Yang",
                "Alexander Ratner."
            ],
            "title": "WRENCH: A comprehensive benchmark for weak supervision",
            "venue": "Thirty-fifth Conference on Neural Information Processing Systems Datasets and Bench-",
            "year": 2021
        },
        {
            "authors": [
                "Zhang"
            ],
            "title": "Results of the feature-based ULF compared towards the feature-based baselines. All results are averaged over 10 trials and reported with the standard error of the mean. The results marked with ",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "A large part of today\u2019s machine learning success rests upon large amounts of annotated training data. However, collecting manual annotation (even in a reduced amount, e.g., for fine-tuning large pretrained models (Devlin et al., 2019), active learning (Sun and Grishman, 2012), or semi-supervised learning (Kozareva et al., 2008)) is tedious and expensive. An alternative approach is weak supervision (WS), where data is labeled in an automated process using one or multiple WS sources such as keywords (Hedderich et al., 2021), knowledge bases (Lin et al., 2016), and heuristics (Varma and R\u00e9, 2018), which are encoded as labeling functions (LFs, Ratner et al. 2020). LFs are applied to unlabeled datasets to obtain weak training labels, which are cheap, but often conflicting and error-prone, requiring improvement (see Table 1).\nWe focus on enhancing the quality of weak labels using k-fold cross-validation. Intuitively, by\n1We make our code available within the Knodle framework (Sedova et al., 2021): https://github.com/knodle.\nleaving out a portion of the data during training, the model avoids overfitting to errors specific to that part. Hence, a mismatch between predictions of a model trained on a large portion of the dataset and the labels of the held-out portion can indicate potential noise specific to the held-out portion. Previous cross-validation-based denoising approaches (Northcutt et al., 2021; Wang et al., 2019b) split the data samples into folds randomly; a direct application of these methods to WS data ignores valuable knowledge stemming from the WS process. In our work, we leverage this knowledge by splitting the data based on matching LFs in the samples. The intuition is the following: a mismatch between predictions of a model trained on a large portion of the LFs and labels generated by held-out LFs can indicate noise specific to the held-out LFs. By performing cross-validation for each LF, noise associated with all LFs can be identified.\nIn contrast to correcting labels as in previous work for supervised settings, we utilize the crossvalidation principle in a WS setting to adjust and improve the LF-to-class assignment. In some cases, an LF correctly captures some samples but mislabels others. For example, one of the LFs used to annotate the YouTube dataset (Alberto et al., 2015) is the keyword my, which is effective in identifying spam messages such as subscribe to my chan-\nnel or check my channel out (see Table 1). However, considering this LF as solely indicative of the spam class would be unwarranted, as numerous non-spam messages also contain the word my (e.g., Sample 3). The weak labeling of such samples can results in a tie (i.e., one vote for SPAM and one vote for HAM), potentially leading to incorrect label assignments through (random) tie-breaking.\nTo address such cases, we introduce a new method ULF: Unsupervised Labeling Function correction (Figure 1), which comprises both erroneous labels detection and correction by leveraging weakly supervised knowledge. ULF re-estimates the joint distribution between LFs and class labels during cross-validation based on highly confident class predictions and their co-occurrence with matching LFs. Importantly, this reestimation is performed out-of-sample, meaning it is guided by the data itself without involving additional manual supervision. Instead of a hard assignment of naive WS (i.e., an LF either corresponds to the class or not), ULF performs a fine-adjusted one, which helps to correct the label mistakes. For ex-\nample, such assignment reduces the association of the LF \"my\" with the SPAM class, resulting in dominant HAM probability in Samples 2 and 3 in Figure 1. Moreover, ULF successfully labels samples with no LFs matched, as, e.g., Sample 4, unlike other methods that filter them out (Ratner et al., 2020). We conduct extensive experiments using feature-based and pre-trained models to demonstrate the effectiveness of our method. To the best of our knowledge, we are the first to adapt crossvalidation denoising methods to WS problems and refine the LFs-to-class allocation in the WS setting."
        },
        {
            "heading": "2 Related Work",
            "text": "Weak supervision (WS) has been widely applied to different tasks across various domains, such as text classification (Zeng et al., 2022), relation extraction (Datta and Roberts, 2023), named entity recognition (Wang et al., 2022), video analysis (Chen et al., 2023), medical domain (Fries et al., 2021), image classification (Yue et al., 2022). Weak annotations are easy to obtain but prone to errors. Approaches to improving noisy data include building a specific model architecture (Karamanolakis et al., 2021), using additional expert annotations (Mazzetto et al., 2021), identifying and removing or downweighting harmful samples (Northcutt et al., 2021; Sedova et al., 2023), or learning from manual user guidance (Boecking et al., 2021; Chatterjee et al., 2020). ULF is compatible with any classifier and do not require any manual supervision; instead of removing the samples, ULF corrects the labels, utilizing as much WS data as possible. K-fold cross-validation, a reliable method for assessing trained model quality (Wong and Yeh, 2019), is also often used to detect errors in manual annotations (Northcutt et al., 2021; Wang et al., 2019b,a; Teljstedt et al., 2015), but has not been applied to a WS setting. We propose WS extensions to some of these methods in Appendix B and use cross-validation in ULF."
        },
        {
            "heading": "3 ULF: Unsupervised Labeling Function Correction",
            "text": "In this section, we present the key elements of ULF. More details can be found in Appendix A, the pseudocode is provided in Algorithm 1.\nGiven a dataset X = {x1, x2, ..., xN} to be used for K-class classifier training. In WS setting, we do not have any gold training labels, but only a set of LFs L = {l1, l2, ..., lL}. An LF lj matches a sample xi if some condition formulated in lj\nholds for xi. Following Sedova et al. (2021), we store this information in a binary matrix ZN\u00d7L, where Zijf = 1 means that LF lj matches sample xi. A set of LFs matched in sample xi is denoted by Lxi , where Lxi \u2282 L and |Lxi | \u2208 [0, |L|]. The LFs to class correspondence is stored in a binary matrix TL\u00d7K , where Tij = 1 means the LF li corresponds to class j (i.e., li assigns the samples to the class j)2. The weak training labels Y\u0303 = {y\u03031, y\u03032, ..., y\u0303n}, y\u0303j \u2208 K are obtained by multiplying Z and T , apply majority vote, and break the ties randomly. The main goal of the ULF algorithm is to refine the T matrix. The graphical explanation is provided in Figure 2.\nFirst, class probabilities are predicted for each sample using k-fold cross-validation on the training set X and weak labels Y\u0303 . We propose different ways of splitting the data into k folds f1, ..., fk, with the most reliable method being splitting by signatures (refer to Appendix A for details and other possible splitting methods). The samples\u2019 signatures, i.e., the sets of LFs matched in each sample, are collected, split into k folds, and used to create data folds: Xtraini = {xj |Lxj /\u2208 fi}, Xouti = {xj |Lxj \u2208 fi} (1). Next, k models are separately trained on each of k\u22121 folds and applied\n2The initial manual class assignment is typically such that each LF corresponds to one class, covering a prototypical case. However, assigning an LF to multiple classes is theoretically possible and compatible with ULF.\nto the held-out folds, resulting in out-of-sample predicted probabilities PN\u00d7K . The out-of-sample label y\u0302i for each sample i is determined by selecting the class with the highest probability, provided that this highest probability exceeds the class average threshold tj :\ntj :=\n\u2211 xi\u2208Xy\u0303=j p(y\u0303 = j;xi, \u03b8)\n|Xy\u0303=j | (2)\nThat is, a sample xi is confidently assigned to class j if the out-of-sample probability of it belonging to class j is higher than the average out-of-sample probability of all samples initially assigned to this class. If no probability exceeds the class thresholds (e.g., all probabilities are equally small), the sample is disregarded as unreliable for further calculations.\nThese assignments are used to build an LFs-toclasses confidence matrix CL\u00d7K , which estimates the joint distribution between matched LFs and predicted labels. For each LF li and each class kj , the confidence matrix CL\u00d7K is populated by counting the number of samples that have LF li matched and confidently assigned to class kj :\nCli,y\u0302j = |{xi \u2208 X : y\u0302i = y\u0303j , li \u2208 Lxi}|. (3)\nAlgorithm 1: ULF: Unsupervised Labeling Function Correction for Weak Supervision Input: unsupervised training data X ,\nsamples to LFs matrix ZN\u00d7L, LFs to classes matrix TL\u00d7K , CV model g (\u00b7; \u03b80), end model h (\u00b7; \u03b8\u20320)\n1 Calculate noisy labels Y\u0303 \u2190 ZT 2 for iter = 1, 2, ..., I do 3 Split the data into k folds f1, ..., fk 4 for fi, i \u2208 [1, 2, ..., k] do 5 Build Xtraini , Xouti sets (Eq. 1) 6 Y\u0303traini = {y\u0303 \u2208 Y\u0303 : \u2200x \u2208 Xtraini} 7 \u03b8i = train(Xtraini , Y\u0303traini) 8 Calculate p(y\u0303 = j;xi, \u03b8i) for \u2200xi \u2208 Xouti , 1 \u2264 j \u2264 K 9 Calculate labels y\u0302i w.r.t. the thresholds\ntj (Eq. 2) 10 Calculate LFs-to-class confidence matrix Cl,y\u0302 (Eq. 3) 11 Estimate Q\u0302l,y\u0302 joint matrix (Eq. 4) 12 Recalculate T \u2217 matrix (Eq. 5) 13 Calculate improved labels Y\u0303 \u2190 ZT \u2217 14 \u03b8\u2032 = train(X , Y\u0303 ) Output :Trained \u03b8\u2032\nSubsequently, CL\u00d7K is calibrated and normalized to Q\u0302L\u00d7K in order to align with the data proportions of the Z matrix:\nQ\u0302li,y\u0302j = ( Cli,y\u0302j \u00b7 L\u2211 m=1 Zlm,y\u0302j ) / ( L\u2211 m=1 Clm,y\u0302j ) ,\n(4)\nwhere \u2211\ni\u2208L, j\u2208K Cli,y\u0302j = n, K\u2211 j=1 Zlm,y\u0302j = K\u2211 j=1 Q\u0302lm,y\u0302j .\nThis calibration ensures that Q\u0302L\u00d7K sums up to the total number of training samples, and the sum of counts for each LF is the same as in the original Z matrix; thus, Q\u0302L\u00d7K can be utilized as a crossvalidated re-estimation of T . Finally, a refined T \u2217 matrix is calculated as follows:\nT \u2217 = p \u2217 Q\u0302+ (1\u2212 p) \u2217 T. (5)\nHere, the hyperparameter p, p \u2208 [0, 1], determines the extent to which information from the original T matrix should be retained. The resulting T \u2217 matrix is utilized to generate improved labels for additional ULF iterations or training the final classifier.\nUnlabeled samples. ULF also takes advantage of unlabeled samples that do not have any LFs matched. A portion of these samples, determined by the hyperparameter \u03bb, is randomly labeled and included in cross-validation training, with reestimation in subsequent iterations. To leverage all unlabeled samples in a fine-tuning-based setting, we also include the optional Cosine self-training step (Yu et al., 2021), which can be executed during cross-validation and/or final classifier training."
        },
        {
            "heading": "4 Experiments",
            "text": "Datasets and baselines. We evaluate ULF on four WS English datasets: (1) YouTube Spam Classification (Alberto et al., 2015); (2) Spouse Relation Classification (Corney et al., 2016); (3) Question Classification from TREC-6 (Li and Roth, 2002); (4) SMS Spam Classification (Almeida et al., 2011), and two topic classification WS African datasets: (5) Yor\u00f9b\u00e1 and (6) Hausa (Hedderich et al., 2020). For all datasets, we utilize the LFs provided by dataset authors.\nWe compare our results towards the (1) Gold baseline (the only classifier which exploits gold labels) and the most popular and recent WS baselines: (2) Majority Vote, (3) MeTaL (Ratner et al., 2019), (4) Snorkel-DP (Ratner et al., 2020), (5) Flying Squid (Fu et al., 2020), (6) WeaSEL (Cachay et al., 2021), and (7) FABLE (Zhang et al., 2023). More details are provided in Appendices C and D.\nResults. We run experiments using RoBERTa following Zhang et al. (2021) for English datasets and mulitlingual BERT following Devlin et al. (2019) for others (more implementation details are provided in Appendix F). Table 2 presents the results of the best combination of cross-validation and final models; each of them can be either simple\nfine-tuning or followed by Cosine training step (Yu et al., 2021). On average, ULF outperforms the baselines and achieves better results on four out of six datasets. The weakest performance was observed on the Yor\u00f9b\u00e1 dataset, which is explained by the extremely high number of labeling functions (19897) and the smallest training dataset size (1340 samples) when compared to the other datasets.\nResults of other combinations are provided in Table 3. Two out of the four combinations achieve average scores of 67.6 and 67.1, demonstrating a better performance compared to the baselines. Although the Cosine contrastive self-training considerably improves the results, the ULF high performance does not rely solely dependent on it. This is evident in the fact that the most effective configuration for all other datasets except TREC incorporates the use of Cosine in only one of the two model training steps. Moreover, FT_FT setting, which does not involve Cosine at all, also demonstrates compatible results across all datasets.\nCase Study. We provide a YouTube dataset case study. Figure 3 shows the initial and adjusted T matrices after two ULF iterations in FT_FT setting. Some LFs underwent minimal adjustments (such as keyword_subscribe and regex_check_out, which clearly corresponded to one class), while contentious LFs (like short_comment, i.e., short comments are non-spam) were significantly adjusted. The adjustment was slightly improved after the second iteration of ULF; however, a single iteration was already sufficient for most of the settings, as demonstrated by our experiments (see Appendix F). Table 4 shows mislabeled samples and their corrected labels after ULF application. In (1), the original equal voting was changed to 1.58 for HAM and 2.42 for SPAM after T matrix correction, explicitly determining the label as SPAM. Similarly, labels assigned by a clear majority vote, such as (2), were also corrected. Next, there are sam-\nples where improved and gold labels do not match (i.e., where ULF, strictly speaking, failed). However, these samples are quite controversial: e.g., (3) might be a spam message if the link was different (our model does not check the link\u2019s content), while (4) can be interpreted differently and perceived as a spam comment. Finally, (5), not covered by any LFs and initially randomly assigned to the HAM class, has been corrected to the SPAM class."
        },
        {
            "heading": "5 Conclusion & Future Work",
            "text": "In our work, we focused on denoising WS data by leveraging information from LFs. Our approach assumes that the noise specific to some LFs can be detected by training a model that does not use those LFs signals and then comparing its predictions to the labels generated by the held-out LFs. This idea is used in our method ULF, which improves the weak labels based on the data itself, without leveraging external knowledge. Extensive experiments validate the effectiveness of our approach and support our initial hypothesis of the significant role of LFs in denoising WS data. In future work, we plan to try ULF for other tasks, such as sequence tagging and image classification, and perform more experiments on weakly supervised datasets with different peculiarities and in different languages."
        },
        {
            "heading": "Limitations",
            "text": "In our work, we did not focus on the task of creating labeling functions. Rather, our primary objective is to improve the model performance with a fixed set of already provided labeling functions, and to enable better generalization to new data.\nAll the datasets and their corresponding labeling functions used in our experiments are weakly supervised datasets that have been extensively utilized in previous research. The provided labeling functions for these datasets, as well as other wellknown weakly supervised datasets, are considered reliable. ULF does not require the majority of LFs to have high precision; however, if we consider a significantly different setting where the majority of labeling functions are highly unreliable (e.g., generated by a noisy automatic process), cross-validation as done in ULF may not be as effective as in a more standard WS setting.\nIn our experiments, we restricted ourselves to NLP datasets and tasks, as creating labeling functions for weak supervision is particularly intuitive for language-related tasks. We leave the exploration of other data modalities for future research."
        },
        {
            "heading": "Ethics Statement",
            "text": "While our method can lead to better and more helpful predictions by the models trained on the noisy data we cannot guarantee that these predictions are perfect and can be trusted as the sole basis for decision-making, especially in life-critical applications (e.g. healthcare). Machine learning systems can pick up and perpetuate biases in the data, and if our algorithms are used for real-world applications, the underlying data and the predictions of the resulting models should be critically analyzed with respect to such biases. We build our work on previously published datasets and did not hire annotators."
        },
        {
            "heading": "Acknowledgement",
            "text": "We thank the anonymous reviewers for their constructive feedback. This research has been funded by the Vienna Science and Technology Fund (WWTF)[10.47379/VRG19008] \u201cKnowledge-infused Deep Learning for Natural Language Processing\u201d."
        },
        {
            "heading": "A Details on ULF method",
            "text": "In this section, we give a more formal description of the ULF algorithm as well as discuss its details."
        },
        {
            "heading": "A.1 A detailed explanation of Z, T, and Y matrices",
            "text": "Figure 2 represents the weak annotation encoded with Z and T matrices. The matrix ZN\u00d7L represents the information regarding the matches of labeling functions (LFs) in samples. For instance, in the case of keyword-based LFs, an LF matches a sample if this keyword is present in this sample (this sample is then assigned to the class associated with this LF). This matrix is binary: if an LF lj matches a sample xi, Zij = 0.\nThe matrix TL\u00d7K signifies the correspondence between LFs and classes. Original T matrix is binary: each element Tkl represents whether an LF lk corresponds to class l. Tkl = 1 indicates that an LF lk corresponds to class l and assigns this class to all samples where it matches. For example, the keyword subscribe corresponds to the class SPAM; any sample containing the word subscribe will receive one vote for the SPAM class.\nBy multiplying the matrices Z and T and applying majority vote, we obtain weak labels Y\u0303 . The T \u2217 matrix denotes the improved version of the T matrix achieved through ULF. Note that the T \u2217 matrix is not binary anymore: instead of hard assignments, it contains soft ones. The improved, clean labels can be obtained by multiplying the improved matrix T \u2217 with the original matrix Z."
        },
        {
            "heading": "A.2 Data Splitting into Folds for Cross-Validation",
            "text": "First, the training data is split into k folds for crossvalidation training. We analyzed three possible ways of splitting:\n\u2022 randomly (ULFrndm): the samples are assigned to folds the same way as it would be done in standard k-fold cross-validation irrespective of LFs matching;\n\u2022 by LF (ULFlfs): the LFs are randomly split into k folds {f1, ..., fk} and each fold fi is iteratively taken as held-out LFs, while others become training LFs. All samples where training LFs match become training set Xtraini ; the rest build a hold-out set and are used for re-estimation Xouti :\nXtraini = {xj |Lxj \u2229 fi = \u2205} Xouti = X \\Xtraini (7)\n(7)\n\u2022 by signatures (ULFsgn): for each training sample xi, we define its signature Lxi as the set of matching LFs. For instance, if LFs lf1, lf4, and lf7 match in a sample xi, its signature is represented as {1, 4, 7}. Next, the signatures are split into k folds, each of which becomes in turn a test fold, while others compose training folds. All samples with signatures present in the training folds are considered as training samples Xtraini , while the remaining samples form the hold-out set Xouti .\nXtraini = {xj |Lxj /\u2208 fi} Xouti = {xj |Lxj \u2208 fi}\n(8)\nAfter the data is split into folds, k models are separately trained on Xtraini , i \u2208 [1, k] and applied on the held-out folds Xouti to obtain the out-ofsample predicted probabilities PN\u00d7K ."
        },
        {
            "heading": "A.3 Out-of-sample Labels Calculation",
            "text": "The predicted probabilities PN\u00d7K are used to calculate the out-of-sample labels y\u0302.\nThe class with the highest probability is selected:\ny\u0302i = argmax 1\u2264j\u2264K p(y\u0303 = j;xi, \u03b8), (9)\nif and only if the probability p(y\u0303 = j;xi, \u03b8) exceeds the class j average threshold tj :\ntj := \u2211\nxi\u2208Xy\u0303=j\np(y\u0303 = j;xi, \u03b8)/|Xy\u0303=j | (10)\nIn other words, the class average threshold tj is calculated by summing up the probabilities of the j class for samples initially assigned to that class and then dividing it by the number of those samples. A sample xi is considered as belonging to the class j if and only if this sample confidently belongs to the corresponding class. If no probability exceeds the class thresholds for a sample (e.g., it belongs to all classes with equally small probabilities), it is disregarded in further calculations as unreliable."
        },
        {
            "heading": "A.4 LFs-to-Class Estimation Matrix",
            "text": "To refine the LFs to class allocation, ULF reestimates the joint distribution between matched LFs and predicted labels. For each LF li and each class kj , the confidently assigned to the class kj samples with the LF li matched are calculated; the counts are saved as a LFs-confident matrix CL\u00d7K :\nNext, the confident matrix is calibrated and normalized to Q\u0302L\u00d7K to correspond to the data proportion in the Z matrix:\nQ\u0302li,y\u0302j = ( Cli,y\u0302j \u00b7 L\u2211 m=1 Zlm,y\u0302j ) / ( L\u2211 m=1 Clm,y\u0302j ) ,\n(11)\nwhere \u2211\ni\u2208L, j\u2208K Cli,y\u0302j = n, K\u2211 j=1 Zlm,y\u0302j = K\u2211 j=1 Q\u0302lm,y\u0302j .\nAlgorithm 2: Train (X , Y\u0303 ) in featurebased ULF\n1 \u03b8 = AdamW(\u03b8, X , Y\u0303 ) Output :Trained \u03b8\nAlgorithm 3: Train (X , Y\u0303 ) in ULF with pretrained language model fine-tuning (optionally: with additional Cosine selftraining step)\n1 Xlab = {x \u2208 X : |Lxi | > 0} 2 Y\u0303lab = {y\u0303 \u2208 Y\u0303 : \u2200x \u2208 Xlab}\n# 1. fine-tune \u03b8 with Xlab and Y\u0303lab 3 \u03b8 = AdamW(\u03b8, Xlab, Y\u0303lab)\n# 2. (optional) contrastive self-training of \u03b8 with X\n4 Calculate pseudo labels ypsd 5 for step = 1, 2, ..., num_steps do 6 Select confident samples 7 Calculate classification loss Lc(\u03b8, ypsd),\ncontrastive regularizer R1(\u03b8, ypsd), confidence regularizer R1(\u03b8)\n(see Yu et al. (2021) for exact formulas and explanation)\n9 L(\u03b8, ypsd) = Lc + \u03bbR1 +R1 10 \u03b8 = AdamW(\u03b8, X)\nOutput :Trained \u03b8"
        },
        {
            "heading": "A.5 T Matrix Update",
            "text": "The joint matrix Q\u0302 is used for improving the LFs-toclass matrix T that contains the initial LFs-to-class allocations. T and Q\u0302 are summed with multiplying coefficients p and 1\u2212p, where p \u2208 [0, 1]. The value of p balances the initial manual label assignment with the unsupervised re-estimation and determines how much information from the estimated assignment matrix Q\u0302 should be preserved in the refined matrix T \u2217:\nT \u2217 = p \u2217 Q\u0302+ (1\u2212 p) \u2217 T (12)\nWith the multiplication of Z and the newly recalculated T \u2217 matrices, an updated set of labels Y \u2217 is calculated. It can either be used for rerunning the denoising process or training the end classifier."
        },
        {
            "heading": "B Weakly Supervised Extension of Denoising Models",
            "text": "To validate our hypothesis regarding the importance of considering labeling functions in noise detection in WS data, we adopt two cross-validationbased methods originally designed for denoising manually annotated data: CrossWeigh (Wang et al., 2019b) and Cleanlab (Northcutt et al., 2021). We adapt these methods for the weakly supervised setting and introduce our extensions: WSCrossWeigh and WSCleanlab. A key difference between the original methods and our extensions is the approach used to split the data into folds for cross-validation training. Instead of random splitting, we split the data based on the labeling functions that match in the samples.\nIn this section, we outline the original methods and introduce our WS extensions. Additionally, we conducted experiments, which are detailed in Appendix E."
        },
        {
            "heading": "B.1 Weakly Supervised CrossWeigh",
            "text": "The original CrossWeigh framework (CW, Wang et al. 2019b) was proposed for tracing inconsistent labels in the crowdsourced annotations for the named entity recognition task. After randomly splitting the data into k folds and building k training and hold-out sets, CrossWeigh additionally filters the training samples that include the entities matched in hold-out folds samples. The intuition behind this approach is that if an entity is constantly mislabeled, the model would be misguided, but the model trained without it would eliminate this confusion. We consider this approach quite promising for detecting unreliable LFs in weakly supervised data similarly. If a potentially erroneous LF systematically annotates the samples wrongly, reliable model-trained data without it will not make this mistake in its prediction, and, thus, the error will be traced and reduced. Our new Weakly Supervised CrossWeigh method (WSCW) allows splitting the data not entirely randomly but considering the LFs so that all LFs matched in a test fold are eliminated from the training folds. More formally, firstly, we randomly split labeling functions L into k folds {f1, ..., fk}. Then, we iteratively take LFs from\neach fold fi as test LFs and the others as training LFs. So, all samples where no LFs from hold-out fold match become training samples, while the rest are used for testing.\nXouti = X \\Xtraini After that we train the k separate models on Xtraini and evaluate them on Xouti . In the same way, as in the original CrossWeigh algorithm, the labels predicted by the trained model for the samples in the hold-out set y\u0302 are compared to the initial noisy labels y. All samples Xj where y\u0302j \u0338= yj are claimed to be potentially mislabeled; their influence is reduced in further training. The whole procedure of error detection is performed t times with different partitions to refine the results. The sample weights wxN are then calculated as wxj = \u03f5\ncj , where cj is the number of times a sample xj was classified as mislabeled, 0 \u2264 cj \u2264 t, and \u03f5 is a weight reducing coefficient."
        },
        {
            "heading": "B.2 Weakly Supervised Cleanlab",
            "text": "The second method we introduce is Weakly Supervised Cleanlab (WSCL) - an adaptation of Cleanlab framework (Northcutt et al., 2021) for weak supervision. In the same way as in WSCW, not data samples, but the labeling functions L are split into k folds {f1, ..., fk} and used for building the Xtraini and Xouti sets, 1 < i < k, for training k models. In contrast to WSCW, for each sample, xi the label is not directly predicted on the Xouti , but the probability vector of class distribution p\u0302(y = j;xi, \u03b8), j \u2208 K is considered. The exact labels y\u0302 are calculated later on with respect to the class expected self-confidence value tj (see Northcutt et al. 2021):\ntj :=\n\u2211 Xj p\u0302(y = j;xi, \u03b8)\n|Xj | , (13)\nwhere Xj = {xi \u2208 Xy=j} , 1 < j < c A sample xi is considered to confidently belong to class j \u2208 K if the probability of class j is greater than expected self-confidence for this class tj or the maximal one in the case of several classes is probable:\ny\u0302i = argmax j\u2208[K]:\np\u0302(y=j;xi,\u03b8)\u2265tj\np\u0302(y = j;xi, \u03b8) (14)\nThe samples with no probability that would exceed the thresholds have no decisive label and do not participate in further denoising.\nAfter that, a class-to-class confident joint matrix Cy,y\u0302 is calculated, where:\nCy,y\u0302[j][k] = |{xi \u2208 X|yi = j, y\u0302i = j}| Notably, Cy,y\u0302 contains only the information about correspondence between noisy and out-ofsample predicted labels (the same way as in Northcutt et al. (2021)). So, it gives the idea about the number of samples with presumably erroneous noisy labels y but does not give us any insights about the erroneous labeling functions that assigned this noisy label to this sample (in contrast to the ULF approach we present in Section 3).\nThe confident matrix Cy,y\u0302 is then calibrated and normalized in order to obtain an estimate matrix of the joint distribution between noisy and out-ofsample predicted labels Q\u0302y,y\u0302, which determines the number of samples to be pruned. We perform the pruning by noise rate following the Cleanlab default setting: n \u00b7 Qyi,y\u0302j , i \u0338= j samples with max(p\u0302(y = j) \u2212 p\u0302(y = i)) are eliminated in further training."
        },
        {
            "heading": "C Datasets",
            "text": "In this section, we give a brief overview of the dataset and the examples of labeling function we used in our experiments. The dataset statistics is provided in Table 5.\nYouTube (Alberto et al., 2015) A spam detection dataset was collected from the YouTube video comments. The samples that are not relevant to the video (e.g. advertisement of user\u2019s channel or ask for subscription) are classified as SPAM, while others belong to the HAM class. We use the same labeling functions as in Ratner et al. (2020); they were created using keywords, regular expressions, and heuristics. For example, a labeling function KEYWORD_MY corresponds to class SPAM, meaning that if a sample contains the word \"my\" it will be assigned to the SPAM class. Among other labeling functions are KEYWORD_SUBSCRIBE, KEYWORD_PLEASE, KEYWORD_SONG (keyword-based), SHORT_COMMENT (if a comment is short, it is probably not spam; thus, samples less than 5 words long would be classed as HAM).\nSpouse (Corney et al., 2016) A relation extraction dataset based on the Signal Media One-Million\nNews Articles Dataset, which main task is to define whether there is a spouse relation in a sample. We use the Snorkel annotation (Snorkel); the labeling functions were created based on keywords (e.g., husband), spouse relationships extracted from DBPedia (Lehmann et al., 2014), and language patterns (e.g., check whether the people mentioned in a sample have the same last name).\nTREC (Li and Roth, 2002) A question classification dataset that maps each data sample to one of 6 classes. The labeling functions were generated based on keywords (Awasthi et al., 2020), e.g. which, what, located, situated keywords relate a sample to the class LOCATION.\nSMS (Almeida et al., 2011) A spam detection dataset comprised of text messages. The annotation proposed by Awasthi et al. (2020) includes keyword-based and regular expression-based labeling functions. For example, a regex-based labeling function: ( |^)(won|won)[^\\w]* ([^\\s]+ )*\n(claim,|claim)[^\\w]*( |$)\ncorresponds to class SPAM (e.g., as in a sample 449050000301 You have won a ??2,000 price! To claim, call 09050000301.).\nYor\u00f9b\u00e1 and Hausa (Hedderich et al., 2020) Topic classification datasets of the second (Hausa) and the third (Yor\u00f9b\u00e1) most spoken languages in Africa comprised news headlines. The weak keyword rules are provided by the authors."
        },
        {
            "heading": "D Baselines",
            "text": "We compare our method ULF towards the most recent weakly-supervised baselines. Note that ULF does not use manually annotated data and cannot be directly compared to models that do (Karamanolakis et al., 2021; Awasthi et al., 2020).\nGold A classifier is trained using the gold, manual labels. It is the only model which is trained with manual labels in our experiments.\nMajority Vote A classifier is trained using weak labels obtained by applying labeling functions to the samples, selecting the class with majority voting, breaking the ties randomly.\nMeTaL (Ratner et al., 2019) A classifier is trained with labels which are obtained by combining signals from multiple labeling functions and training a hierarchical multi-task network.\nSnorkel-DP (Ratner et al., 2020) A classifier is trained using generative and discriminative Snorkel steps.\nFlyingSquid (Fu et al., 2020) A classifier is trained using noisy labels that are rectified exploiting an Ising model by a triplet formulation.\nWeaSEL (Cachay et al., 2021) A classifier is trained using a probabilistic encoder and a downstream model combined with a specifically defined noise-aware loss function. As WeaSEL is an endto-end system, we were unable to include the Cosine step to it; that is the reason why (potential) WeaSEL+Cosine baseline is absent in Table 2.\nFABLE (Zhang et al., 2023) A classifier is trained using noisy labels that are inferred leveraging the instance features and the mixture coefficients of the EBCC model.\nIn all baseline runs, we adhered to the hyperparameters and their associated search spaces as suggested by the methods\u2019 authors. However, for some of the baselines, we have to change the setting proposed in the original papers in order to provide a fair comparison. For instance, in FABLE, we fine-tuned the hyper-parameters of the final classifier training, similar to how we did it for other baselines, even though the authors did not conduct such fine-tuning in their experiments and did not assume the presence of a gold validation set. It is also important to note that due to energy considerations and resource constraints, we retrieved the best hyper-parameter values by random search, rather than grid search. Consequently, for some of the results, direct comparison is not possible."
        },
        {
            "heading": "E Feature-based Experiments",
            "text": "In Section 4, we presented the results of our finetuning-based ULF implementation. In addition to it, we also provide a feature-based ULF implementation that does not rely on pre-trained language models but can be run with an arbitrary model for the feature-based prediction.\nWe run the feature-based ULF experiments for four datasets: YouTube, Spouse, TREC, and SMS. The datasets selection was motivated by previous work that includes feature-based methods (Zhang et al., 2021, 2023) and run experiments with these datasets. This choice allows for a direct comparison of our results with those studies. (For the finetuning-based experiments, which typically yield better results, we expanded our experimental setup and included the low-resource language datasets Yor\u00f9b\u00e1 and Hausa, see Section 4.)\nWe compare the feature-based ULF approach to the same weakly supervised baselines used for comparing the fine-tuning-based ULF approach. Those are: Majority Vote, MeTaL (Ratner et al., 2019), Snorkel-DP (Ratner et al., 2020), FlyingSquid (Fu et al., 2020), FABLE (Zhang et al., 2023), and WeaSEL baseline (Cachay et al., 2021), which also uses logistic regression in the setting presented in the original paper. Additionally, we include two methods for learning with noisy labels we already discussed in Appendix B - CrossWeigh (Wang et al., 2019b) and Cleanlab (Northcutt et al., 2021), also feature-based in the original setting - together with our weakly supervised extensions WSCrossWeigh and WSCleanlab. The feature-based ULF method was run in all possible data splitting settings: randomly (ULFrndm), by LFs (ULFlfs), and by signatures (ULFsgn), see Appendix A for more\ndetails. The feature-based ULF and the corresponding baselines are realized in our experiments with a logistic regression model; the training data are encoded with TF-IDF vectors. The results of featurebased experiments are provided in Table 6.\nResults. Our weakly supervised extensions to CrossWeigh and Cleanlab methods consistently outperform the base versions and most of other baselines, supporting our hypothesis of LFs\u2019 importance in applying cross-validation techniques to weakly supervised settings. Feature-based ULF also shows the best result overall on most datasets and even outperforms the model trained on YouTube data with manual annotations.\nWe also use feature-based ULF to compare the data splitting strategies. ULFrndm incorporates a standard cross-validation with random data splitting, disregarding any signal originating from the weak annotation. This approach can be viewed for estimating the ULF performance alone, independent of any weak signals. The lower performance performance of ULFlfs compared to other configurations may be due to multiple LF matches in many data samples, leading to multiple overlaps during cross-validation training (i.e., the samples were reestimated multiple times). In ULFsign, on the contrary, each data sample is considered only once in each denoising round resulting in a better performance. However, even in the worst-performing settings (ULFlfs and ULFrndm) our feature-based ULF outperforms the majority of the baselines.\nThe signature-based splitting, which demon-\nstrated the best performance for feature-based ULF, was chosen for the fine-tuning-based ULF experiments (see Table 2).\nF Implementation Details\nAll our experiments used the validation set for hyper-parameter tuning, early stopping, and model selection. The gradient-based optimization was performed with AdamW Optimizer and linear learning rate scheduler. All results are reproducible with the seed value 1111.\nULF-specific parameter search space was defined heuristically. All parameter search spaces are provided in Table 7. The number of iterations I was also estimated based on the validation set: initially, it is set to I = 20, but if training labels do not change after three iterations, the algorithm stops, and the last saved model is used for final testing. The actual number of iterations, alongside other hyper-parameter values, can be found in Tables 8 and 9. These tables show that a single iteration yields the optimal result in most scenarios, with two iterations being the second most commonly selected option.\nIn fine-tuning-based ULF, in addition to other hyper-parameters, we include the label prediction parameter: if it equals \"soft\", the probabilistic labels are used for training; otherwise (\"hard\") a label is the one-hot encoding of the most probable class (the ties are broken randomly).\nIn order to reduce computational load, we performed the random parameter search instead of the\ngrid search in all our experiments. Specifically, we tried 10 random parameter combinations from search space and selected the one which performed the best on the validation set. For feature-based ULF and corresponding baselines, the model with the retrieved best hyper-parameter values was run ten times with different initializations; the average values with the standard error of the mean are reported. For fine-tuning-based ULF and corresponding baselines, the model with the retrieved best hyper-parameter values was run once on the test set; this value is reported. The retrieved hyperparameters are provided in Table 8 for the best feature-based ULF setting, ULFsng, and in Table 9 for all fine-tuning and cosine combinations of the fine-tuning-based ULF.\nBoth feature-based and fine-tuning-based ULF were implemented with Python and PyTorch (Paszke et al., 2019) in the setting of the weak supervision framework Knodle (Sedova et al., 2021). By providing access to all WS components Knodle allowed us to implement and benchmark all algorithms described above. The pre-trained language models were downloaded from HuggingFace (https://huggingface.co/models). We followed the Wrench (Zhang et al., 2021) encoding method and used their implementation for most of the baselines (apart from Cleanlab and CrossWeigh which are not included in the Wrench framework).\nFeature-based ULF experiments were performed on a machine with a CPU frequency of 2.2GHz with 40 cores. Fine-tuning-based ULF experiments were run on a single Tesla V100 GPU on Nvidia DGX-1. The full setup took 20 hours on average for each dataset for feature-based settings and 96 hours for fine-tuning-based settings."
        }
    ],
    "title": "ULF: Unsupervised Labeling Function Correction using Cross-Validation for Weak Supervision",
    "year": 2023
}