{
    "abstractText": "We apply a continuous relaxation of L0 regularization (Louizos et al., 2018), which induces sparsity, to study the inductive biases of LSTMs. In particular, we are interested in the patterns of formal languages which are readily learned and expressed by LSTMs. Across a wide range of tests we find sparse LSTMs prefer subregular languages over regular languages and the strength of this preference increases as we increase the pressure for sparsity. Furthermore LSTMs which are trained on subregular languages have fewer non-zero parameters, and LSTMs trained on human-attested subregular patterns have fewer non-zero parameters than those trained on unattested patterns. We conjecture that this subregular bias in LSTMs is related to the cognitive bias for subregular language observed in human phonology, in that both are downstream of a bias for simplicity in a suitable computational description language.",
    "authors": [
        {
            "affiliations": [],
            "name": "Charles Torres"
        },
        {
            "affiliations": [],
            "name": "Richard Futrell"
        }
    ],
    "id": "SP:f2150e81b6ee54814842f367e32c788414fa9f82",
    "references": [
        {
            "authors": [
                "Enes Avcu",
                "Arild Hestvik."
            ],
            "title": "Unlearnable phonotactics",
            "venue": "Glossa: a journal of general linguistics, 5(1).",
            "year": 2020
        },
        {
            "authors": [
                "Enes Avcu",
                "Chihiro Shibata",
                "Jeffrey Heinz."
            ],
            "title": "Subregular complexity and deep learning",
            "venue": "CLASP Papers in Computational Linguistics, page 20.",
            "year": 2017
        },
        {
            "authors": [
                "Gilmer",
                "George E. Dahl",
                "Ashish Vaswani",
                "Kelsey R. Allen",
                "Charles Nash",
                "Victoria Langston",
                "Chris Dyer",
                "Nicolas Heess",
                "Daan Wierstra",
                "Pushmeet Kohli",
                "Matthew M. Botvinick",
                "Oriol Vinyals",
                "Yujia Li",
                "Razvan Pascanu"
            ],
            "title": "Relational inductive biases",
            "year": 2018
        },
        {
            "authors": [
                "Benjamin L Edelman",
                "Surbhi Goel",
                "Sham Kakade",
                "Cyril Zhang."
            ],
            "title": "Inductive biases and variable creation in self-attention mechanisms",
            "venue": "Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning",
            "year": 2022
        },
        {
            "authors": [
                "Jonathan Frankle",
                "Michael Carbin."
            ],
            "title": "The lottery ticket hypothesis: Finding sparse, trainable neural networks",
            "venue": "International Conference on Learning Representations.",
            "year": 2019
        },
        {
            "authors": [
                "Felix A Gers",
                "E Schmidhuber."
            ],
            "title": "LSTM recurrent networks learn simple context-free and contextsensitive languages",
            "venue": "IEEE Transactions on Neural Networks, 12(6):1333\u20131340.",
            "year": 2001
        },
        {
            "authors": [
                "Thomas Graf."
            ],
            "title": "Subregular linguistics: Bridging theoretical linguistics and formal grammar",
            "venue": "Theoretical Linguistics, 48:145\u2013184.",
            "year": 2022
        },
        {
            "authors": [
                "Peter Gr\u00fcnwald."
            ],
            "title": "Model selection based on minimum description length",
            "venue": "Journal of Mathematical Psychology, 44(1):133\u2013152.",
            "year": 2000
        },
        {
            "authors": [
                "Michael Hahn."
            ],
            "title": "Theoretical limitations of selfattention in neural sequence models",
            "venue": "Transactions of the Association for Computational Linguistics, 8:156\u2013 171.",
            "year": 2020
        },
        {
            "authors": [
                "Jeffrey Heinz."
            ],
            "title": "The computational nature of phonological generalizations",
            "venue": "Phonological typology, phonetics and phonology, pages 126\u2013195.",
            "year": 2018
        },
        {
            "authors": [
                "Jeffrey Heinz",
                "William Idsardi."
            ],
            "title": "What complexity differences reveal about domains in language",
            "venue": "Topics in Cognitive Science, 5(1):111\u2013131.",
            "year": 2013
        },
        {
            "authors": [
                "Jeffrey Heinz",
                "Chetan Rawal",
                "Herbert G. Tanner."
            ],
            "title": "Tier-based strictly local constraints for phonology",
            "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 58\u201364, Portland,",
            "year": 2011
        },
        {
            "authors": [
                "John Hewitt",
                "Michael Hahn",
                "Surya Ganguli",
                "Percy Liang",
                "Christopher D. Manning."
            ],
            "title": "RNNs can generate bounded hierarchical languages with optimal memory",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language",
            "year": 2020
        },
        {
            "authors": [
                "Sepp Hochreiter",
                "J\u00fcrgen Schmidhuber."
            ],
            "title": "Long short-term memory",
            "venue": "Neural Computation, 9(8):1735\u2013 1780.",
            "year": 1997
        },
        {
            "authors": [
                "Ronald M. Kaplan",
                "Martin Kay."
            ],
            "title": "Regular models of phonological rule systems",
            "venue": "Computational Linguistics, 20(3):331\u2013378.",
            "year": 1994
        },
        {
            "authors": [
                "Regine Lai."
            ],
            "title": "Learnable vs",
            "venue": "Unlearnable Harmony Patterns. Linguistic Inquiry, 46(3):425\u2013451.",
            "year": 2015
        },
        {
            "authors": [
                "Christos Louizos",
                "Max Welling",
                "Diederik P Kingma"
            ],
            "title": "Learning sparse neural networks through L0 regularization",
            "year": 2018
        },
        {
            "authors": [
                "R. Thomas McCoy",
                "Robert Frank",
                "Tal Linzen."
            ],
            "title": "Does syntax need to grow on trees? Sources of hierarchical inductive bias in sequence-to-sequence networks",
            "venue": "Transactions of the Association for Computational Linguistics, 8:125\u2013140.",
            "year": 2020
        },
        {
            "authors": [
                "Kevin McMullin",
                "Gunnar \u00d3lafur Hansson."
            ],
            "title": "Inductive learning of locality relations in segmental phonology",
            "venue": "Laboratory Phonology, 10(1).",
            "year": 2019
        },
        {
            "authors": [
                "Martin A Nowak",
                "Natalia L Komarova",
                "Partha Niyogi."
            ],
            "title": "Computational and evolutionary aspects of language",
            "venue": "Nature, 417(6889):611\u2013617.",
            "year": 2002
        },
        {
            "authors": [
                "Bo Peng",
                "Eric Alcaide",
                "Quentin Anthony",
                "Alon Albalak",
                "Samuel Arcadinho",
                "Huanqi Cao",
                "Xin Cheng",
                "Michael Chung",
                "Matteo Grella",
                "Kranthi Kiran GV"
            ],
            "title": "RWKV: Reinventing RNNs for the transformer era",
            "venue": "Computing Research Repository,",
            "year": 2023
        },
        {
            "authors": [
                "Brandon Prickett."
            ],
            "title": "Modelling a subregular bias in phonological learning with recurrent neural networks",
            "venue": "Journal of Language Modelling, 9.",
            "year": 2021
        },
        {
            "authors": [
                "Shauli Ravfogel",
                "Yoav Goldberg",
                "Tal Linzen."
            ],
            "title": "Studying the inductive biases of RNNs with synthetic variations of natural languages",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics:",
            "year": 2019
        },
        {
            "authors": [
                "Jon Rawski",
                "Aniello De Santo",
                "Jeffrey Heinz."
            ],
            "title": "Reconciling minimum description length with grammar-independent complexity measures",
            "venue": "MIT Workshop on Simplicity in Grammar Learning.",
            "year": 2017
        },
        {
            "authors": [
                "James Rogers",
                "Geoffrey K Pullum."
            ],
            "title": "Aural pattern recognition experiments and the subregular hierarchy",
            "venue": "Journal of Logic, Language and Information, 20:329\u2013342.",
            "year": 2011
        },
        {
            "authors": [
                "Mirac Suzgun",
                "Yonatan Belinkov",
                "Stuart Shieber",
                "Sebastian Gehrmann."
            ],
            "title": "LSTM networks can perform dynamic counting",
            "venue": "pages 44\u201354.",
            "year": 2019
        },
        {
            "authors": [
                "Gail Weiss",
                "Yoav Goldberg",
                "Eran Yahav."
            ],
            "title": "On the practical computational power of finite precision RNNs for language recognition",
            "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),",
            "year": 2018
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Despite the wide application of neural networks for sequence modeling, their inductive biases remain incompletely understood: for example, whether RNNs have an inductive bias favoring the kind of hierarchical structures used in sentence formation in natural language (Weiss et al., 2018; Ravfogel et al., 2019; Hewitt et al., 2020; McCoy et al., 2020; Edelman et al., 2022). It is important to study the inductive biases of neural networks not only from a scientific perspective, but also a practical one: if neural networks can be developed that have inductive biases toward the same kinds of formal language structures that characterize human language, then these neural networks should be able to learn human language more easily.\nMany patterns in natural language are characterized by a particular subset of the regular languages, called subregular languages. These languages have been hypothesized to form a bound on learnable human phonological patterns (Heinz, 2018),\nas well as patterns within tree structures characterizing syntax (Graf, 2022). The particular subregular languages theorized to be human learnable are strictly piecewise (SP), strictly local (SL), and multiple tier-based strictly local (MTSL). Artificial language learning tasks have shown these languages to be easier to acquire for humans (Lai, 2015; Avcu and Hestvik, 2020; McMullin and Hansson, 2019).\nHere we take up the question of inductive biases for subregular languages in LSTMs (Hochreiter and Schmidhuber, 1997), using a relaxation of L0 regularization to study networks that are constrained to be simple in the sense of network sparsity. We train these networks on several sets of toy languages which contrast subregular languages against regular languages in a controlled manner. This technique not only allows us to study the inductive biases of sparse networks, but also to address an outstanding question in the literature on subregular languages: whether there exists a computational description language under which subregular languages are simpler than other regular languages (Heinz and Idsardi, 2013).\nTo preview our results, we find that simple LSTMs favor subregular languages: such languages can be represented by LSTMs with fewer parameters, and sparse LSTMs favor subregular languages in generalization.\nThe remainder of the paper is structured as follows: Section 2 gives relevant background on subregular languages, Section 3 gives our general methods for training sparse neural networks under L0 regularization, Section 4 presents our experiments on matched pairs of regular and subregular languages, and Section 5 concludes."
        },
        {
            "heading": "2 Background",
            "text": "It is known that phonological constraints should be regular given the most common formalisms in linguistics (Kaplan and Kay, 1994). However, the regular language class is too permissive: many pat-\nterns not attested in natural language are members of it. The subregular hypothesis is an attempt to improve this bound, lowering it to certain subclasses of the regular languages (Heinz, 2018). There are two versions of the hypothesis\u2014the weak version and the strong version\u2014hypothesizing different subregular languages as the boundary for human learnable patterns. The strong subregular hypothesis asserts that phonological constraints are within the strictly local (SL) and strictly piecewise (SP) subclasses, whereas the weak hypothesis raises this bound to the tier-based strictly local (TSL) languages (the union of multiple constraints being multiple tier-based strictly local or MTSL). These languages are not the only subregular languages, but they are the most important subregular languages for the hypothesis.\nWhat the SL, SP, and TSL languages all share in common is that they are defined by prohibiting features in the strings of that language. The SL and SP languages (corresponding to the strong subregular hypothesis) are defined by sets of prohibited substrings and subsequences respectively. These prohibitions are meant to capture phonological constraints associated with local restrictions (SL) or harmony-like patterns (SP). The TSL languages are slightly more complex than the other two, forbidding substrings after characters have been removed from a string (the retained characters are defined by a second set, usually labeled T for tier). To demonstrate these prohibitions in action examples of these languages are given in Table 1, however curious readers can look to Heinz (2018), Rogers and Pullum (2011), and Heinz et al. (2011) for a rigorous treatment of these languages as well as definitions for other subregular languages which are not part of the subregular hypothesis.\nThe recent interest in subregular languages and their ability to capture phonological patterns has\nstimulated some work on the learnability of subregular patterns by RNNs. Results in this area have been mixed. LSTMs and RNNs both show difficulty when trained to predict membership in subregular languages (Avcu et al., 2017). Nevertheless, other work has still shown that the learning of subregular patterns is easier for sequence to sequence RNNs than the learning of other patterns, echoing human-like preferences (Prickett, 2021). Work in this area is still limited.\nWhat can explain these preferences in LSTMs or humans? Previous work has attempted to tie a preference for simplicity to biases in natural language, for example by using the minimum description length (MDL) principle, a formulation of Occam\u2019s razor favoring simple hypotheses that account for given data, where a hypothesis is simple if it can be described using a small number of bits in some description language (Gr\u00fcnwald, 2000). Can the subregular preference be similarly explained? Heinz and Idsardi (2013) levelled a challenge against accounts of human language learning based on this method in explaining this preference: It is easy to find examples where a subregular and regular language both have the same description length in some computational formalism (for example a finite-state automaton, FSA). While they left open the possibility that a computational formalism may be found where description length can distinguish these examples, they expressed doubt that such a thing could be found. A similar challenge was raised in Rawski et al. (2017) where MDL accounts were challenged to explain learning biases.\nIs there a computational representation that, when given access to data consistent with a subregular and regular language, would learn only the subregular pattern by using an MDL method? We attempt to address these challenges. We will show that when controlling for description length (num-\nber of nonzero parameters) in our case studies, neural networks prefer subregular patterns over regular ones on the basis of simplicity, where FSAs do not. That this is true in LSTMs leaves open the possibility that MDL accounts are capable of explaining this bias in other systems, as in human language learning."
        },
        {
            "heading": "3 General Methods",
            "text": "A crucial part of our method requires finding optimal tradeoffs between accuracy and network complexity. We operationalize the complexity of a neural network as its number of nonzero parameters, reflecting the size of the subnetwork required to achieve a given accuracy. As such, we would like to use the L0 norm, which counts the number of nonzero entries in the parameter vector \u03b8:\n\u2225\u03b8\u22250 = |\u03b8|\u2211 j=1 1\u03b8j \u0338=0. (1)\nIdeally, the L0 norm would form a part of our objective function like so:\nR(\u03b8) = 1\nN N\u2211 i=1 L(p\u03b8(xi), yi) + \u03bb\u2225\u03b8\u22250, (2)\nwhere L(p\u03b8(xi), yi) is the log likelihood of the i\u2019th label yi given input xi in the training data under the model with parameters \u03b8. However, the L0 norm is not differentiable, and cannot serve as part of our objective function during training.\nWe get around this limitation using a relaxation of L0 regularization developed by Louizos et al. (2018). For the j\u2019th parameter in our model \u03b8j , we assign a masking parameter \u03c0j > 0. This parameterizes a distribution with cumulative distribution function Q(\u00b7|\u03c0j), from which we can sample a mask value zj \u2208 [0, 1], which critically can take the values of true 0 or 1. The vector of values z generated in this way is then used as a mask to filter the parameters \u03b8, generating a vector of masked parameters \u03b8\u0303 where \u03b8\u0303j = zj\u03b8j .\nThis parameterization lends us a new objective function where the labels yi are predicted from the inputs xi using the masked parameters \u03b8\u0303, and the model is penalized for having too few masked\nparameters:\nR(\u03b8, \u03c0) = E z\u223cQ(\u00b7|\u03c0)\n[ 1\nN N\u2211 i=1 L(p\u03b8\u0303(xi), yi)\n] + (3)\n\u03bb |\u03b8|\u2211 j=1 Q(zj > 0|\u03c0j),\nwhere the expectation is over mask vectors z drawn conditional on the vector of masking parameters \u03c0. The regularization term now reflects the expected number of parameters whose value is not masked.\nWe use Equation 3 in all our experiments below, with different loss functions L as required. When L is a form of cross-entropy this term represents the cost of encoding the data using the distribution specified by the neural network. Under these conditions, the objective becomes a form of minimum description length as a two part code: with the regularization term encoding the distribution, and L encoding the data (Gr\u00fcnwald, 2000). Though our approach focuses on the MDL interpretation, this formulation also has a Bayesian interpretation, optimizing the negative log likelihood with the regularization term as a prior and the loss term as a posterior as we show in Appendix A."
        },
        {
            "heading": "4 Experiments",
            "text": ""
        },
        {
            "heading": "4.1 Subregular Battery",
            "text": "The first experiment measures the complexity of LSTMs that learn subregular and regular languages. We call this the subregular battery: an experiment\nrun on three sets of three formal languages, where the description length of the generating FSAs are controlled within each set. Each set of languages contains one strictly local and one strictly piecewise (which are both subregular) and one regular language. One set of languages was used in Heinz and Idsardi (2013)\u2019s argument that minimum description length fails to separate subregular languages from other regular ones; we offer two additional sets. The languages are described in Table 2."
        },
        {
            "heading": "4.1.1 Methods",
            "text": "The method can be split into 3 steps: data generation, network training, and evaluation.\nData generation For each language, we sample 2000 words from a probabilistic finite state automaton (pFSA) designed to generate words from that language following a geometric distribution on word length. The geometric distribution on word length is important for two reasons: (1) it implies a constant stopping probability in all states, meaning that the distribution over string lengths does not depend on the structure of the strings in a language, and (2) it is memoryless, meaning that the network need not keep track of the total characters observed so far to attain high accuracy. Maintaining a geometric distribution on string lengths is the simplest method for attaining the same distribution over string lengths across languages while sampling from pFSAs.\nThis data is then split into training, development, and testing sets at an 80/10/10 ratio. Sets are constructed such that no duplicate words existed between sets. This was done by grouping words by form, and then randomly assigning them to the training, testing, or development sets before reshuffling them within the group.\nNetwork training Each LSTM has the same architecture: a 3-dimensional embedding layer, a 5 unit LSTM layer, and a 2-layer perceptron decoder with a hidden width of 5 units. We train LSTMs for up to 400 epochs on the training set. For the subregular battery, we use cross entropy loss with the aforementioned L0 regularization technique as our objective function. After each epoch, we evaluate development set performance (including the approximate L0 penalty), and if improvement on it has not increased for 50 epochs, training is halted.1 Otherwise we use the best performing network on 1This high patience is necessary due to the stochastic nature of the L0 regularization technique.\nthe development set over all 400 epochs. We repeat this procedure five times for various regularization penalties to get data on performance at different levels of network complexity.\nEvaluation Once data is collected, we evaluate the trained networks in terms of the trade-off of network complexity\u2014measured as the number of nonzero parameters\u2014and error, defined as the average per-character KL divergence of the LSTMs from the target distribution (the pFSAs from which the languages were sampled). For a target language with distribution QG and a model P\u03b8, this divergence is\nD[QG\u2225P\u03b8] = H[QG, P\u03b8]\u2212H[QG]. (4)\nThus, using the KL divergence here instead of cross entropy allows us to control for the entropy H[QG] of the target language G. We measure an approximate per-character KL divergence using the N sample strings in the evaluation set for language G:\nD\u0302[QG\u2225P\u03b8] = 1\nN N\u2211 i=1 1 Ti Ti\u2211 t=1 ln QG(x t i | x<ti ) P\u03b8(x t i | x <t i ) ,\n(5) where Ti is the length of the i\u2019th sample string in the evaluation set, xti is the t\u2019th character of the i\u2019th sample string, and x<ti is the prefix of characters up to index t in the n\u2019th sample string.\nWe use area under the curve (AUC) as a metric to evaluate complexity. This is defined as the area under the convex hull of (complexity, KL approximation) points. A small area under the curve indicates that it possible to achieve low error with a simple network; a large area under the curve means that error is high even with a more complex network. We determine if there is a significant difference between languages by performing a permutation test. This is done by randomly sampling points from the union of two languages and comparing the AUC statistic in the permuted samples."
        },
        {
            "heading": "4.1.2 Results",
            "text": "Figure 2 shows the trade-off of number of parameters and KL divergence for the three sets of languages. The areas under the curve are shaded, and the points which closely mimic the target distribution are shown in color. For a given level of complexity, the areas under the curve for the regular languages exceed both the piecewise testable and strictly local languages. The specific AUC numbers are shown in Table 3. In every case the\ndifferences between both subregular languages and regular languages are statistically significant and in the expected direction as determined by a permutation test.\nWe augment this test with an additional posthoc analysis of the most accurately performing networks (which we define as having a KL divergence of less than .01 bits) to determine if the size of the minimal LSTMs are also significantly different. This analysis is also done with a permutation test. These networks (Table 4) attain a lower complexity score for the subregular languages, a significant difference in all cases except *ac."
        },
        {
            "heading": "4.1.3 Discusssion",
            "text": "This experiment\u2019s results demonstrate that, at least in the languages chosen, strictly piecewise and strictly local languages better favor a tradeoff be-\ntween simplicity and complexity as demonstrated in our AUC analysis. We also see that they can be generated by functions implemented with simpler LSTMs in all but one case. In our post-hoc analysis we find that all but one subregular language is significantly simpler to accurately mimic in comparison with the regular language. These results also demonstrate that checking for the presence of two substrings is more complicated than one, and checking for a longer substring is more complicated than a shorter one. But if compared within a similar strictly regular pattern (here represented by controlling for generating FSA size) the preference is clearly for subregular languages."
        },
        {
            "heading": "4.2 Standard Harmony and First-Last Harmony",
            "text": "In the second test we investigate whether a preference exists between two subregular languages. One language is MTSL, a class of subregular languages believed to be learnable in the subregular hypothesis. The other is a locally testable (LT) language, which is believed to not be human learnable. This test is inspired by previous experimental works (Lai, 2015; Avcu and Hestvik, 2020) which were investigations of standard vowel harmony (attested in human languages) and first-last harmony (not attested). These two languages are not controlled for FSA size, but are languages that serve as important evidence for the subregular hypothesis.\nThe difference between standard vowel harmony (SH) and first-last (FL) harmony languages is shown in Table 5. In an SH language we require languages to begin and end with an a or b, and we forbid both a and b from occurring in the same string. In an FL harmony pattern, we require a word to begin and end with a or b, and to begin and end with the same character, but allow any charac-\nters in-between. These examples are a simplistic representation of patterns that are attested in human language (standard harmony) versus not attested (first-last harmony).\nIn the experiment below, we apply a similar test and evaluation method as in the subregular battery to demonstrate that the functions which generate SH patterns (which are MTSL) are simpler than functions which generate FL patterns (which are LT)."
        },
        {
            "heading": "4.2.1 Methods",
            "text": "We use LSTMs with with the same architecture as in section 4.1. The only difference is that the objective for these networks is to predict the correct continuation set: the set of possible characters that can grammatically follow after a prefix, an objective introduced in Gers and Schmidhuber (2001) and used elsewhere (Suzgun et al., 2019). The network is trained to predict, for each possible character continuing a prefix, whether that character is licit or not\u2014that is, whether that character is a member of the continuation set C. Given a continuation set C and an LSTM equipped with a decoder that reads in a prefix x of characters and produces a vector of real values \u2208 R|C|, the loss for the network is\nLcs(p\u03b8(x), C) = 1 |C| \u2211 i\u2208|C| \u2212Ci log(p\u03b8(x)i)\u2212 (6)\n(1\u2212 Ci) log(1\u2212 p\u03b8(x)i)\nThis breaks with the prior work using the continuation set objective, as we use the binary crossentropy loss instead of mean squared error.\nUsing this loss function is necessary to control for differences in entropy between the SH and FL languages when conducting comparisons. Above, we control for differences in entropy by using an approximate KL divergence as an accuracy metric and we control for length-related effects on LSTM\ncomplexity by making the lengths of strings geometrically distributed. This is not possible when comparing the SH and FL languages, because they cannot be generated by pFSAs in a way that yields a geometric distribution on string length.\nData generation To accommodate this new task, we sampled words uniformly given length. For SH data, words were generated such that they began and ended with the same character (either b or c) with a geometrically sampled number of characters in-between (p = .05) consistent with the SH pattern (see table 5). For FL languages the same procedure was followed except that the center characters were consistent with the FL pattern.\nNetwork training Training was performed similarly to the subregular battery. This was for a maximum of 200 epochs, potentially terminating earlier if development set performance did not improve for 50 epochs.\nEvaluation At the end of training, (complexity, continuation-set loss) pairs were calculated from the best network\u2019s performance on the heldout test set. As in Section 4.1 we use AUC as the performance metric, the only difference being the points are now (complexity, continuation-set loss) instead of (complexity, KL approximation)."
        },
        {
            "heading": "4.2.2 Results",
            "text": "Our results are shown in Figure 3. Again, we see a similar pattern as in the subregular battery, but this time showing the human-learnable subregular language is preferred over the unlearnable one. Standard harmony performance deterioriates at the 40 parameter mark, whereas first-last harmony deteriorates at the 75 parameter mark. The areas approximated by the frontier points (shaded) remain significantly different under a permutation test, with p = .009."
        },
        {
            "heading": "4.2.3 Discussion",
            "text": "These results continue the pattern seen in the subregular battery, but demonstrate not just a subregular preference, but a human-learnable subregular preference. While prior studies have shown a preference and higher performance for SH over FL with sequence to sequence networks, this experiment demonstrates that beyond a preference, these functions are more easily represented in simpler LSTMs."
        },
        {
            "heading": "4.3 Generalization",
            "text": "Above, we found that subregular languages, and particularly human-learnable ones, were represented by simpler LSTMs. The question remains: does this simplicity constraint affect generalization?\nHere, we test whether a pressure for sparsity in LSTMs yields a bias towards learning a subregular language when the network is trained on data that is consistent with both a subregular and regular language, an approach suggested by Rawski et al. (2017). More precisely, the test is as follows. Take two formal languages Gsub and Greg, one subregular and one regular. Now, using their intersection Gsub\u2229Greg as training data, what does the induced grammar correspond to, Gsub or Greg?"
        },
        {
            "heading": "4.3.1 Methods",
            "text": "As our pair of langauges, we use languages from the subregular battery in Section 4.1. We take as Gsub the language \u2217ac and as Greg the regular language which forbids b after an odd number of as. This language is the test as described in Rawski et al. (2017).\nData generation For each network, we generate data from Gsub \u2229Greg, dividing these into a training set and a development set with approximately 1600 words in the training set and 200 words in\nthe testing set. We generate two additional testing datasets for Gsub and Greg individually, checking the sets such that no words in the testing set appeared in the training or development data of 200 words each. This matches the size of the dataset to that of Section 4.1.\nNetwork training With the data generated, our LSTMs are trained according to the per character cross entropy loss, regularized as in Equation 3, with a procedure identical to that used in Section 2.\nEvaluation After training, performance on Gsub and Greg is evaluated using the model which performed best on the development set over the course of training. As in the subregular analysis, we calculate the approximate KL divergence by subtracting the true per-character entropy as determined by the generating pFSA from the cross-entropy as in Equation 5.\nOur analysis, unlike the previous ones, is qualitative. We will investigate the subregular bias, operationalized as the difference:\nPrefGsub := D\u0302[QGreg\u2225P\u03b8]\u2212 D\u0302[QGsub\u2225P\u03b8], (7)\nwhere the approximate divergences D\u0302 are calculated over the evaluation sets for the two languages Gsub and Greg, as in Eq. 5. The value PrefGsub is positive when the model\u2019s distribution over strings more closely resembles the one defined by the pFSA for language Gsub."
        },
        {
            "heading": "4.3.2 Results",
            "text": "The results of our generalization experiment show a clear preference for the subregular language regardless of complexity (Figure 4). However, unlike prior results, we also see a clear increase in preferences as network complexity is constrained. This difference is especially pronounced in the highlighted region. In the lowest complexity range, there is a collapse in performance where the network can do nothing but predict a uniform distribution on all characters at all times.\nExamining the KL divergences for the individual languages, as in Figure 5, we find that this result is driven by high accuracy in modeling the subregular language Gsub using a small number of parameters. Accuracy in modeling the regular language Greg is mostly poor, while accuracy modeling subregular language Gsub is highest when the expected number of parameters is between 25 and 75."
        },
        {
            "heading": "4.3.3 Discussion",
            "text": "While our prior results in the complexity evaluations have shown that subregular languages are generated with simpler networks, these results demonstrate a more clear inductive bias. LSTMs with constrained complexity generalize better to the subregular language than to the regular language. This indicates that, not only is the subregular function simpler, but that simpler networks prefer subregular languages.\nBut we also see that this inductive bias exists even in low regularization regimes. This result is consistent with previous findings, and may be related to other phenomena like previously observed locality biases (Ravfogel et al., 2019) which are a basic constraint in all RNN functioning (Battaglia et al., 2018)."
        },
        {
            "heading": "5 Conclusion",
            "text": "In our complexity measurement experiments we observe that subregular languages are represented by consistently simpler networks than similar regular languages, and that human-learnable subregular patterns are represented by simpler networks than unlearnable ones. This result is surprising, given that the use of a description length criterion to distinguish these languages had been deemed unlikely (Heinz and Idsardi, 2013).\nFurthermore, we found that in the generalization experiment, data consistent with a subregular and regular language leads to a preferential generalization towards learning the subregular distribution, and that this preference increases as the network becomes simpler. This supports the idea that a change in computational representation systems may favor subregular over regular languages, with the caveat that comparing subregular languages with radically different complexities in FSA representation may\nmean that this generalization pattern does not hold. Why do LSTMs favor subregular languages? We argue that our results may explain this preference via the lottery ticket hypothesis (Frankle and Carbin, 2019). This hypothesis states that neural networks learn by tuning \u201cwinning tickets\u201d\u2014which are subnetworks that perform well on the task at initialization\u2014which explains why larger neural networks perform well (they \u201cbuy more tickets\u201d). The existence of smaller networks (as we define them) for subregular languages means that any neural network will contain more tickets for subregular languages at initialization, and thus have an inductive bias toward such languages. If this line of reasoning is true, L0 regularization does not introduce anything qualitatively new in LSTM training, but shifts the probability mass to favor tickets with smaller subnetworks.\nPerhaps more controversially, we also believe these results may be of interest to human cognition. While the human brain is not an LSTM, our results indicate that a subregular bias can be downstream of a bias for simplicity within an appropriate computational formalism, showing that rich (and often puzzling) constraints can be downstream from simple rules. A bias like this can be construed as a universal grammar\u2014a bound on learnable formal languages (Nowak et al., 2002).\nMore needs to be done, of course. This work is far from a proof of the simplicity of subregular languages in LSTMs. Likewise, more subregular languages ought to be investigated. We may find, for example, that the true inductive bias for LSTMs is not exactly subregular, or that only certain aspects of subregularity are simple to implement. But we find the current results exciting and intriguing, for its relationship to network complexity and function implementation, its potential to explain LSTM inductive biases, and the demonstration of subregular biases resulting from computational complexity constraints."
        },
        {
            "heading": "6 Limitations",
            "text": "The largest limitation of this work is that it is experimental, and not a proof. We try to show several demonstrations of the simplicity of subregularity, and believe further work should be done, but are aware of the limitations of this kind of work in addressing matters of mathematical curiosity.\nWe also understand that, because this work addresses these issues in an experimental way, there\nis no certainty that our object of study (subregular languages and their patterns) are what LSTMs are truly biased towards.\nOur work uses LSTMs, rather than the Transformer architectures which underlie recently influential large language models (Brown et al., 2020). Although LSTMs are not currently the most popular architecture, recurrent models are recently seeing a revival (Peng et al., 2023) due to their inherent advantages such as being able to model indefinitely long sequences. In contrast to LSTMs, Transformers are known to be highly constrained in their strict formal expressivity (Hahn, 2020); however, their formal inductive biases in practice are not yet thoroughly explored.\nEthical considerations\nWe foresee no ethical issues arising from this work."
        },
        {
            "heading": "Acknowledgements",
            "text": "This work was supported by NSF grant #1947307 to R.F. We thank Connor Mayer, Jon Rawski, and Aniello de Santo for helpful discussion."
        },
        {
            "heading": "A L0 regularization as maximum a posteriori (MAP) estimation",
            "text": "We show that L0 regularization is equivalent to MAP estimation with an exponential prior on the\nnumber of non-zero parameters in the model. Consider the standard MAP estimation problem:\n\u03b8\u0302MAP = argmax \u03b8\nf(x|\u03b8)q(\u03b8), (8)\nwhere f is a distribution conditional on parameters \u03b8 and q is a prior distribution on \u03b8. Now consider the expected value of non-zero parameters \u03b8\u2032. In Equation 3, \u03b8\u2032 is the regularization term, as shown below:\n|\u03b8|\u2211 j=1 Q(zj > 0 | \u03c0j) (9)\n= |\u03b8|\u2211 j=1 \u222b q(zj | \u03c0j) \u00b7 1 (zj > 0) dzj (10)\n= |\u03b8|\u2211 j=1 \u222b q(z | \u03c0) \u00b7 1 (zj > 0) dz (11)\n= \u222b q(z | \u03c0) |\u03b8|\u2211 j=1 1 (zj > 0) dz (12)\n= E z\u223cQ(\u00b7|\u03c0)  |\u03b8|\u2211 j=1 1 (zj > 0)  (13) = \u03b8\u2032, (14)\nwith probability density function q(\u00b7 | \u03c0) corresponding to cumulative distribution function Q(\u00b7 | \u03c0), and indicator function 1(\u00b7). If we consider an exponential prior on \u03b8\u2032 we would have:\nq(\u03b8\u2032) = \u03bbe\u2212\u03bb\u03b8 \u2032 . (15)\nThen by Eq. 15 and the monotonicity of the log function, the MAP estimation problem is equivalent to finding\n\u03b8\u0302MAP = argmin \u03b8\n\u2212 log f(x|\u03b8)+\u03bb\u03b8\u2032\u2212log \u03bb, (16)\nwhich is equal to our objective in Eq. 3 up to an additive constant \u2212 log \u03bb that does not depend on the parameters to be optimized."
        }
    ],
    "title": "Simpler neural networks prefer subregular languages",
    "year": 2023
}