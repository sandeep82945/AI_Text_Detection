{
    "abstractText": "Korean morphological variations present unique opportunities and challenges in natural language processing (NLP), necessitating an advanced understanding of morpheme-based sentence construction. The complexity of morphological variations allows for diverse sentence forms based on the syntactic-semantic integration of functional morphemes (i.e., affixes) to lexical morphemes (i.e., roots). With this in mind, we propose a method CHEF, replicating the morphological transformations inherent in sentences based on lexical and functional morpheme combinations through generative data augmentation. CHEF operates using a morpheme blender and a label discriminator, thereby enhancing the diversity of Korean sentence forms by capturing the properties of agglutination while maintaining label consistency. We conduct experiments on Korean multiple classification datasets, improving model performance in fulland few-shot settings. Our proposed method boosts performance beyond the preceding data augmentation methods without incurring external data usage. We demonstrate that our approach achieves comparable results yielded by augmentation techniques that use large language models (LLMs).",
    "authors": [
        {
            "affiliations": [],
            "name": "Jaehyung Seo"
        },
        {
            "affiliations": [],
            "name": "Hyeonseok Moon"
        },
        {
            "affiliations": [],
            "name": "Jaewook Lee"
        },
        {
            "affiliations": [],
            "name": "Sugyeong Eo"
        },
        {
            "affiliations": [],
            "name": "Chanjun Park"
        },
        {
            "affiliations": [],
            "name": "Heuiseok Lim"
        }
    ],
    "id": "SP:f0ab171321112e39f6d5d45f0442906b998b3847",
    "references": [
        {
            "authors": [
                "Daniel M Bikel."
            ],
            "title": "Automatic wordnet mapping using word sense disambiguation",
            "venue": "2000 Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora, pages 142\u2013147.",
            "year": 2000
        },
        {
            "authors": [
                "Samuel Bowman",
                "Gabor Angeli",
                "Christopher Potts",
                "Christopher D Manning."
            ],
            "title": "A large annotated corpus for learning natural language inference",
            "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 632\u2013",
            "year": 2015
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners. Advances in neural information processing",
            "year": 2020
        },
        {
            "authors": [
                "Xiang Dai",
                "Heike Adel."
            ],
            "title": "An analysis of simple data augmentation for named entity recognition",
            "venue": "Proceedings of the 28th International Conference on Computational Linguistics, pages 3861\u2013 3867, Barcelona, Spain (Online). International Com-",
            "year": 2020
        },
        {
            "authors": [
                "Guillaume Daval-Frerot",
                "Yannick Weis."
            ],
            "title": "WMD at SemEval-2020 tasks 7 and 11: Assessing humor and propaganda using unsupervised data augmentation",
            "venue": "Proceedings of the Fourteenth Workshop on Semantic Evaluation, pages 1865\u2013",
            "year": 2020
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "arXiv preprint arXiv:1810.04805.",
            "year": 2018
        },
        {
            "authors": [
                "Jingfei Du",
                "Edouard Grave",
                "Beliz Gunel",
                "Vishrav Chaudhary",
                "Onur Celebi",
                "Michael Auli",
                "Veselin Stoyanov",
                "Alexis Conneau."
            ],
            "title": "Self-training improves pre-training for natural language understanding",
            "venue": "Proceedings of the 2021 Conference of the North",
            "year": 2021
        },
        {
            "authors": [
                "Alexander Fabbri",
                "Simeng Han",
                "Haoyuan Li",
                "Haoran Li",
                "Marjan Ghazvininejad",
                "Shafiq Joty",
                "Dragomir Radev",
                "Yashar Mehdad."
            ],
            "title": "Improving zero and fewshot abstractive summarization with intermediate fine-tuning and data augmentation",
            "venue": "Proceedings of",
            "year": 2021
        },
        {
            "authors": [
                "Marzieh Fadaee",
                "Arianna Bisazza",
                "Christof Monz."
            ],
            "title": "Data augmentation for low-resource neural machine translation",
            "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 567\u2013573.",
            "year": 2017
        },
        {
            "authors": [
                "Angela Fan",
                "Shruti Bhosale",
                "Holger Schwenk",
                "Zhiyi Ma",
                "Ahmed El-Kishky",
                "Siddharth Goyal",
                "Mandeep Baines",
                "Onur Celebi",
                "Guillaume Wenzek",
                "Vishrav Chaudhary"
            ],
            "title": "Beyond english-centric multilingual machine translation",
            "venue": "The Journal of Machine",
            "year": 2021
        },
        {
            "authors": [
                "Hongyu Guo."
            ],
            "title": "Nonlinear mixup: Out-of-manifold data augmentation for text classification",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, pages 4044\u20134051.",
            "year": 2020
        },
        {
            "authors": [
                "Jiyeon Ham",
                "Yo Joong Choe",
                "Kyubyong Park",
                "Ilji Choi",
                "Hyungjoon Soh."
            ],
            "title": "Kornli and korsts: New benchmark datasets for korean natural language understanding",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 422\u2013",
            "year": 2020
        },
        {
            "authors": [
                "Nitish Joshi",
                "He He."
            ],
            "title": "An investigation of the (in) effectiveness of counterfactually augmented data",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3668\u20133681.",
            "year": 2022
        },
        {
            "authors": [
                "Akbar Karimi",
                "Leonardo Rossi",
                "Andrea Prati."
            ],
            "title": "Aeda: An easier data augmentation technique for text classification",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2021, pages 2748\u20132754.",
            "year": 2021
        },
        {
            "authors": [
                "Myung Hee Kim",
                "Nathalie Colineau."
            ],
            "title": "An enhanced mapping scheme of the universal part-ofspeech for korean",
            "venue": "Proceedings of the 12th Language Resources and Evaluation Conference, pages 3826\u20133833.",
            "year": 2020
        },
        {
            "authors": [
                "Nayeon Kim",
                "Jun-Hyung Park",
                "Joon-Young Choi",
                "Eojin Jeon",
                "Youjin Kang",
                "SangKeun Lee."
            ],
            "title": "Break it down into bts: Basic, tiniest subword units for korean",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2022
        },
        {
            "authors": [
                "Sosuke Kobayashi."
            ],
            "title": "Contextual augmentation: Data augmentation by words with paradigmatic relations",
            "venue": "Proceedings of NAACL-HLT, pages 452\u2013 457.",
            "year": 2018
        },
        {
            "authors": [
                "Fanshuang Kong",
                "Richong Zhang",
                "Xiaohui Guo",
                "Samuel Mensah",
                "Yongyi Mao."
            ],
            "title": "Dropmix: A textual data augmentation combining dropout with mixup",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 890\u2013",
            "year": 2022
        },
        {
            "authors": [
                "Varun Kumar",
                "Ashutosh Choudhary",
                "Eunah Cho."
            ],
            "title": "Data augmentation using pre-trained transformer models",
            "venue": "Proceedings of the 2nd Workshop on Life-long Learning for Spoken Language Systems, pages 18\u201326.",
            "year": 2020
        },
        {
            "authors": [
                "Young-Jun Lee",
                "Chae-Gyun Lim",
                "Ho-Jin Choi."
            ],
            "title": "Korean-specific emotion annotation procedure using n-gram-based distant supervision and koreanspecific-feature-based distant supervision",
            "venue": "Proceedings of the Twelfth Language Resources and",
            "year": 2020
        },
        {
            "authors": [
                "Mike Lewis",
                "Yinhan Liu",
                "Naman Goyal",
                "Marjan Ghazvininejad",
                "Abdelrahman Mohamed",
                "Omer Levy",
                "Veselin Stoyanov",
                "Luke Zettlemoyer"
            ],
            "title": "BART: Denoising sequence-to-sequence pre-training for natural language",
            "year": 2020
        },
        {
            "authors": [
                "Bill Yuchen Lin",
                "Wangchunshu Zhou",
                "Ming Shen",
                "Pei Zhou",
                "Chandra Bhagavatula",
                "Yejin Choi",
                "Xiang Ren."
            ],
            "title": "Commongen: A constrained text generation challenge for generative commonsense reasoning",
            "venue": "Findings of the Association for Computa-",
            "year": 2020
        },
        {
            "authors": [
                "Qi Liu",
                "Matt Kusner",
                "Phil Blunsom."
            ],
            "title": "Counterfactual data augmentation for neural machine translation",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech-",
            "year": 2021
        },
        {
            "authors": [
                "Yinhan Liu",
                "Jiatao Gu",
                "Naman Goyal",
                "Xian Li",
                "Sergey Edunov",
                "Marjan Ghazvininejad",
                "Mike Lewis",
                "Luke Zettlemoyer."
            ],
            "title": "Multilingual denoising pretraining for neural machine translation",
            "venue": "Transactions of the Association for Computational Linguis-",
            "year": 2020
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Roberta: A robustly optimized bert pretraining approach",
            "venue": "arXiv preprint arXiv:1907.11692.",
            "year": 2019
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter."
            ],
            "title": "Decoupled weight decay regularization",
            "venue": "International Conference on Learning Representations.",
            "year": 2019
        },
        {
            "authors": [
                "David Lowell",
                "Brian Howard",
                "Zachary C. Lipton",
                "Byron Wallace."
            ],
            "title": "Unsupervised data augmentation with naive augmentation and without unlabeled data",
            "venue": "Proceedings of the 2021 Conference on",
            "year": 2021
        },
        {
            "authors": [
                "Andrew Matteson",
                "Chanhee Lee",
                "Youngbum Kim",
                "Heuiseok Lim."
            ],
            "title": "Rich character-level information for Korean morphological analysis and part-ofspeech tagging",
            "venue": "Proceedings of the 27th International Conference on Computational Linguistics,",
            "year": 2018
        },
        {
            "authors": [
                "Dheeraj Mekala",
                "Tu Vu",
                "Timo Schick",
                "Jingbo Shang."
            ],
            "title": "Leveraging qa datasets to improve generative data augmentation",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 9737\u20139750.",
            "year": 2022
        },
        {
            "authors": [
                "Jiao Ou",
                "Jinchao Zhang",
                "Yang Feng",
                "Jie Zhou."
            ],
            "title": "Counterfactual data augmentation via perspective transition for open-domain dialogues",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 1635\u20131648,",
            "year": 2022
        },
        {
            "authors": [
                "Long Ouyang",
                "Jeffrey Wu",
                "Xu Jiang",
                "Diogo Almeida",
                "Carroll Wainwright",
                "Pamela Mishkin",
                "Chong Zhang",
                "Sandhini Agarwal",
                "Katarina Slama",
                "Alex Ray"
            ],
            "title": "Training language models to follow instructions with human",
            "year": 2022
        },
        {
            "authors": [
                "Chanjun Park",
                "Jaehyung Seo",
                "Seolhwa Lee",
                "Chanhee Lee",
                "Hyeonseok Moon",
                "Sugyeong Eo",
                "Heuiseok Lim."
            ],
            "title": "BTS: Back TranScription for speech-totext post-processor using text-to-speech-to-text",
            "venue": "Proceedings of the 8th Workshop on Asian Transla-",
            "year": 2021
        },
        {
            "authors": [
                "Sungjoon Park",
                "Jeongmin Byun",
                "Sion Baek",
                "Yongseok Cho",
                "Alice Oh."
            ],
            "title": "Subword-level word vector representations for korean",
            "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages",
            "year": 2018
        },
        {
            "authors": [
                "Sungjoon Park",
                "Jihyung Moon",
                "Sungdong Kim",
                "Won Ik Cho",
                "Jiyoon Han",
                "Jangwon Park",
                "Chisung Song",
                "Junseong Kim",
                "Yongsook Song",
                "Taehwan Oh"
            ],
            "title": "2021b. Klue: Korean language understanding evaluation",
            "venue": "arXiv preprint arXiv:2105.09680",
            "year": 2021
        },
        {
            "authors": [
                "Timo Schick",
                "Hinrich Sch\u00fctze."
            ],
            "title": "Generating datasets with pretrained language models",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6943\u2013 6951.",
            "year": 2021
        },
        {
            "authors": [
                "Rico Sennrich",
                "Barry Haddow",
                "Alexandra Birch."
            ],
            "title": "Improving neural machine translation models with monolingual data",
            "venue": "Proceedings of the 54th",
            "year": 2016
        },
        {
            "authors": [
                "Jaehyung Seo",
                "Seounghoon Lee",
                "Chanjun Park",
                "Yoonna Jang",
                "Hyeonseok Moon",
                "Sugyeong Eo",
                "Seonmin Koo",
                "Heui-Seok Lim."
            ],
            "title": "A dog is passing over the jet? a text-generation dataset for korean commonsense reasoning and evaluation",
            "venue": "Findings of the",
            "year": 2022
        },
        {
            "authors": [
                "Hyun-Je Song",
                "Seong-Bae Park."
            ],
            "title": "Korean morphological analysis with tied sequence-to-sequence multi-task model",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference",
            "year": 2019
        },
        {
            "authors": [
                "Jae Jung Song."
            ],
            "title": "The Korean language: Structure, use and context",
            "venue": "Routledge.",
            "year": 2006
        },
        {
            "authors": [
                "Lichao Sun",
                "Congying Xia",
                "Wenpeng Yin",
                "Tingting Liang",
                "S Yu Philip",
                "Lifang He."
            ],
            "title": "Mixuptransformer: Dynamic data augmentation for nlp tasks",
            "venue": "Proceedings of the 28th International Conference on Computational Linguistics, pages 3436\u2013",
            "year": 2020
        },
        {
            "authors": [
                "Jason Wei",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Maarten Bosma",
                "Fei Xia",
                "Ed H Chi",
                "Quoc V Le",
                "Denny Zhou"
            ],
            "title": "Chain-of-thought prompting elicits reasoning in large language models",
            "venue": "In Advances in Neural Information Processing Systems",
            "year": 2022
        },
        {
            "authors": [
                "Jason Wei",
                "Kai Zou."
            ],
            "title": "Eda: Easy data augmentation techniques for boosting performance on text classification tasks",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference",
            "year": 2019
        },
        {
            "authors": [
                "Jiaxin Wen",
                "Yeshuang Zhu",
                "Jinchao Zhang",
                "Jie Zhou",
                "Minlie Huang."
            ],
            "title": "AutoCAD: Automatically generate counterfactuals for mitigating shortcut learning",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2022, pages 2302\u20132317,",
            "year": 2022
        },
        {
            "authors": [
                "Kang Min Yoo",
                "Dongju Park",
                "Jaewook Kang",
                "Sang-Woo Lee",
                "Woomyoung Park."
            ],
            "title": "GPT3Mix: Leveraging large-scale language models for text augmentation",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2021, pages 2225\u20132239,",
            "year": 2021
        },
        {
            "authors": [
                "Soyoung Yoon",
                "Gyuwan Kim",
                "Kyumin Park."
            ],
            "title": "Ssmix: Saliency-based span mixup for text classification",
            "venue": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 3225\u2013 3234.",
            "year": 2021
        },
        {
            "authors": [
                "Hongyi Zhang",
                "Moustapha Cisse",
                "Yann N Dauphin",
                "David Lopez-Paz."
            ],
            "title": "mixup: Beyond empirical risk minimization",
            "venue": "International Conference on Learning Representations.",
            "year": 2018
        },
        {
            "authors": [
                "Xiang Zhang",
                "Junbo Zhao",
                "Yann LeCun."
            ],
            "title": "Character-level convolutional networks for text classification",
            "venue": "Advances in Neural Information Processing Systems, volume 28. Curran Associates, Inc.",
            "year": 2015
        },
        {
            "authors": [
                "Jing Zhou",
                "Yanan Zheng",
                "Jie Tang",
                "Li Jian",
                "Zhilin Yang."
            ],
            "title": "Flipda: Effective and robust data augmentation for few-shot learning",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "As an agglutinative language, Korean encompasses a rich array of functional morphemes (Song, 2006; Park et al., 2018). Deep learning-based research in Korean NLP aims to enhance linguistic construction efficiency through morpheme segmentation. Previous studies have explored morphological analysis to improve model performance (Song and Park, 2019; Lee et al., 2020; Kim and Colineau, 2020; Kim et al., 2022). The Korean language presents distinct challenges and opportunities with respect to transformations contingent upon the combinatory patterns between lexical and functional morphemes (Matteson et al., 2018; Seo et al., 2022).\n\u2020 Corresponding author\nConsequently, the incorporation of morphological alterations following combinatory rules enables a significant diversity of linguistic forms to be generated from given lexical morphemes.\nIn this sense, Korean datasets are constrained in capturing the intricate rules governing the interactions between lexical and functional morphemes. As a result, the representations derived from these datasets only contain a small fraction of the potential linguistic forms. For instance, Figure 1 demonstrates multiple approaches to represent the concept of \u2018eat\u2019 by employing different ending forms. Additionally, previous data augmentation approaches are rarely designed to generate synthetic data that considers the morphological characteristic of the Korean language, resulting in limited effectiveness in data augmentation. To address these limitations,\nit is crucial to develop data augmentation methods that explicitly consider morphological diversity. By incorporating such properties into the augmentation process, we can contribute to the robustness of models and facilitate performance enhancements.\nIn this paper, we propose CHEF, a data augmentation method designed to construct new synthetic data in alignment with the combinations of lexical and functional morphemes. CHEF is composed of a morpheme blender1 and label discriminator. For the initial phase, we prepare Korean lexical morphemes from the training dataset as ingredients. The morpheme blender employs the lexical morphemes and generates the new synthetic sentence by leveraging the knowledge of derivational and inflectional rules in conjunction with functional morphemes obtained from a pre-trained generative model. Morpheme blender can generate diverse sentence expressions by incorporating lexical and functional morphemes, yet it does not guarantee the conservation of label information. To adjust the unintended blending, the label discriminator controls the synthetic data generation process of the morpheme blender. Its primary objective is to prevent substantial shifts in meaning and ensure alignment with the original labels. The teacher forcing of the morpheme blender incorporates global information about synthetic data completed by combinations of lexical and functional morphemes. This is achieved through contrastive learning with the label discriminator. Integrating these two modules, CHEF ensures that synthetic data maintains label consistency while blending new morpheme combinations.\nAs illustrated in Figure 1, CHEF augmentation enriches text data by attaching various functional morphemes to a given set of lexical morphemes from the original sentence while preserving the label information. The main contributions of our method are summarized as follows:\n\u2022 Despite difficulties enhancing performance in full-shot settings through data augmentation, CHEF demonstrates effectiveness and robustness through in-depth analysis.\n\u2022 We observe that employing contrastive learning between a label discriminator and a morpheme blender is a suitable data augmentation approach for maintaining label consistency.\n1Morpheme blender takes morphemes and generates a sentence that contains given morphemes. In this sense, we denote it as a blender.\n\u2022 CHEF unlocks the morphological diversity in the training data without using any additional external data and approximates or even outperforms the LLM-based data augmentations.\n\u2022 CHEF exhibits effectiveness even with small amounts of data augmentation."
        },
        {
            "heading": "2 Related Work",
            "text": "Data augmentation is widely recognized in deep learning research as a valuable approach to addressing the scarcity of annotated data. It aids in ensuring that the distribution of the training data maintains robustness even in unseen tests. Over time, several methods have been developed to augment textual content, expanding the diversity and quantity of available training data.\nZhang et al. (2015) proposed a method of word substitution using a synonym thesaurus based on rules, which is followed by augmentation methods utilizing a thesaurus in the studies of (Dai and Adel, 2020; Daval-Frerot and Weis, 2020). Wei and Zou (2019) suggested an easy data augmentation (EDA) method using word-level replacement, random insertion, random deletion, and random swap. Karimi et al. (2021) introduced an easier data augmentation (AEDA) technique, improving text classification performance by randomly inserting six pre-defined punctuation marks. Another strategy involves Mixup-based data augmentation (Zhang et al., 2018), combining two sample data to create new training data. Mixup augmentation was initially applied to image data and extended to text-based deep neural networks (Guo, 2020). Subsequently, it has also been applied to textual data augmentation using Transformer-based methods, serving as a technique to mitigate model overfitting and enhance generalization capabilities (Sun et al., 2020; Yoon et al., 2021; Kong et al., 2022).\nSennrich et al. (2016) proposed a back translation method for augmenting data by performing a round-trip translation on data written in the original language, using a neural machine translation system trained on human-annotated data. Data augmentation based on back translation has evolved as one of the crucial techniques since it allows for rewriting the entire sentence, rather than just wordlevel alterations (Fabbri et al., 2021; Lowell et al., 2021; Park et al., 2021a).\nRecently, research on data augmentation using pre-trained language models has been actively pursued (Kumar et al., 2020; Du et al., 2021; Schick\nand Sch\u00fctze, 2021; Zhou et al., 2022; Mekala et al., 2022). Among them, the data augmentation technique through counterfactual is particularly noteworthy, demonstrating improvements to the stateof-the-art performance levels across various benchmark datasets (Liu et al., 2021; Joshi and He, 2022; Ou et al., 2022; Wen et al., 2022). With the advent of LLMs, considerable progress has been made in overcoming augmentation constraints through generative AI (Yoo et al., 2021). Thus, we proceed assuming that it is feasible to secure textual content with LLMs. Nevertheless, it is still necessary to expend financial and temporal resources to identify the optimal direction for augmentation. As a result, we focus on determining which augmentation method could benefit model learning from Korean data. Given the diverse derivational and inflectional forms inherent in the agglutinative Korean language, we propose CHEF - an augmentation method that considers these linguistic properties."
        },
        {
            "heading": "3 CHEF",
            "text": "Taking inspiration from CommonGen (Lin et al., 2020) and Korean CommonGen (Seo et al., 2022), we focus on the data-to-text generation capabilities of sequence-to-sequence models. CommonGen and Korean CommonGen necessitate generative commonsense reasoning based on given concept sets, where given concept sets are used to determine the feasibility of constructing plausible sentences. Both conditional generation tasks are to make generative language models learn a function f : C \u2192 T that maps a set of input concept\nset C = {c1, ..., cn} to make a target sentence T based on the relation within C. We perceive a parallel between this process and combining ingredients to create a complete dish. In particular, Seo et al. (2022) have shown that combining lexical morphemes yields optimal parsing for generating Korean sentences. Consequently, we concentrate on harnessing the sentence augmentation effect of sequence-to-sequence models by assembling ingredients of morpheme sets."
        },
        {
            "heading": "3.1 Preliminary",
            "text": "The primary objective of CHEF is to augment dataset D = {(inpi, li)}Ni=1, where inpi = (hi, pi) denotes textual input and li is its corresponding label. Considering multiple sentence settings such as NLI and STS, we regard hi as a hypothesis and define auxiliary input pi as a premise. If the input comprises a single sentence (e.g., YNAT), we regard hi as the same text as inpi, and pi is conceived as an empty string.\nThe structure of CHEF is illustrated in Figure 2. It consists of two components: the label discriminator module (MD) that helps maintain label consistency and the morpheme blender (MG) that augments data by referring each morpheme set. In this framework, the label discriminator module is presented as an encoder architecture, such as BERT (Devlin et al., 2018), and the morpheme blender module possesses an encoder-decoder architecture, such as BART (Lewis et al., 2020)2.\n2We adopt Korean pre-trained language models: KoBART (https://github.com/SKT-AI/KoBART) and KLUE-BERT (https://github.com/KLUE-benchmark/KLUE)\nBoth modules are trained to generate synthetic data according to the task and label to be augmented. In the following sections, we describe each module in detail."
        },
        {
            "heading": "3.2 Label Discriminator",
            "text": "Label discriminator module (MD) helps retain the intended label consistency during the data augmentation process. MD is designed to take inpi and offer probabilities to be classified into each label. MD encode inp into the vector space by applying a linear pooling layer followed by the softmax, to the [BOS] position of the last hidden state. We denote the processed output as MD(inpi) \u2208 R1\u00d7nc , where nc indicates the number of classes for the task dealt with D. MD is supervised to maximize the probability of each inpi to be classified into li.\nConcisely, MD is a task module trained with D that provides the label expectation probability obtained by the full contextualized sentence inpi, and is utilized as a label instructor of the morpheme blender, described in the latter sections."
        },
        {
            "heading": "3.3 Morpheme Blender",
            "text": "Ingredients Preparation The main objective of the morpheme blender (MG) is to synthesize the sentence by given morpheme ingredients and its targeting label. For training MG, we extract lexical morpheme ingredientsmi = {mi1, \u00b7 \u00b7 \u00b7 ,mini} from the hi, where ni denotes the number of lexical morphemes in hi. In preparing mi, we use a Korean morphological analyzer, mecab-ko3, to make the lexical morpheme set for hi. We leverage that the diversity of Korean sentences that can be combined for the same lexical morpheme ingredients is potentially high. Therefore, we construct the morpheme set based on substantial lexical morphemes, allowing the morphemes within the model-aware recipe to be combined with various endings, functional morphemes, and particles.\nWe also define a label mapping function \u03c8 : l \u2192 v that maps label li into the label word vi that has a form of natural language. For instance, in the case of NLI, we adopt \"contradiction\" for the label \"-1\", \"neutral\" for \"0\", and \"entailment\" for \"1\".\nIn gathering these, we establish the blender that takes morpheme ingredients mi (optionally with pi) and label word vi as inputs, and returns newly synthesized sentence h\u2032i. Detailed training procedures are described in the following section.\n3https://github.com/hephaex/mecab-ko\nTeacher Forcing For attaining ingredient synthesizing capacity, MG is trained to generate hi by referring to the concatenated sequence of mi, pi, and vi. The encoder framework of MG, denoted as EncG, receives a concatenated sequence and generates a contextualized representation by capturing the bidirectional interactions of the words in the input. Subsequently, MG produces sentences by utilizing the contextual representation acquired by the encoder. We can define the loss function Lce for training sequence-to-sequence generation of MG as in equation (1). For clarity in expression, we denote [\u00b7 \u00b7 \u00b7 ] as a sequentialized concatenation of all the elements in it, and define the concatenated input seqi = [mi, pi, vi].\nLce = \u2212 1\n|D| N\u2211 i=1 \u2211 j logPMG(h i j | hi<j ; seqi) (1)\nThrough the sequence-to-sequence training, MG can generate a whole sentence that covers a given morpheme set with weakly reflecting the label information granted by vi in seqi.\nLabel Consistency Supervision Even if label information is reflected in the generation process by feeding it as an input, we find that generation outputs from MG still suffer from label inconsistency. To alleviate this, we exploit the auxiliary training objective utilizing pre-trained discriminator MG to consider label consistency.\nWe argue that by aligning the encoded outputs of MD and EncG, we can make EncG better embed contextual relations with the corresponding morpheme sets and task-specific labels and promote label maintenance. This can be distilled to the training objective that maximizes the similarity simi defined as the following equations:\nriG = SoftMax(EncG(seq i) \u00b7WG) (2)\nsimi(inp) = riG \u00b7MD(inp))\n\u2225riG\u2225\u2225MD(inp))\u2225 (3)\nNote that through the pre-training of MD, the encoded output of MD represents the label information of the whole sentence hi that MG should generate. The auxiliary training objective aims to make the label representation encoded through the set of ingredients seqi, be aligned with the label representation yielded by hi.\nFor the direct supervision, we define contrastive sample set Ci \u2282 D for each (inpi, li) \u2208 D, and\ncontrastive label set li = L \\{li} for each label li, where L denotes the set of all possible labels considered in D. In comprising Ci, we randomly extract a single sample from D for each label in li. Then the loss function Lcont for learning label consistency is defined as equation (4)\nLicont = \u2212 log exp(simi(inpi)/\u03c4)\u2211\n(inpj ,lj)\u2208Ci exp(sim i(inpj)/\u03c4)\n(4)\nWG denotes the linear pooling layer that maps the encoded representation on the [BOS] position into the label classification probability. The temperature parameter \u03c4 controls the sharpness of the softmax distribution with larger values leading to a smoother distribution and smaller values leading to a more peaked distribution.\nIn summing these, MG is trained with Ltotal defined as the following equation:\nLcont = \u2212 1\n|D| N\u2211 i=1 Licont (5)\nLtotal = (1\u2212 \u03bb)Lce + \u03bbLcont (6)\nIn equation (5) and (6), \u03bb is the balance parameter between the two losses."
        },
        {
            "heading": "3.4 Augmentation Pipeline",
            "text": "In utilizing MG, we generate a single data for each (inpi, li) in D. The pipeline is as follows:\n1. Extract morpheme set mi of hi in inpi\n2. Generate hiaug = MG([m i, pi, vi]), where vi\nis the label word of li.\n3. hiaug is regarded as an augmented data for D, which label is li.\nConsidering that excessive augmentation may lead to error accumulation and the following label confusion, we apply CHEF to the small fraction of D in implementing full-shot learning."
        },
        {
            "heading": "4 Experimental Settings",
            "text": "We introduce experimental settings used for the experiments. More details are in Appendix A"
        },
        {
            "heading": "4.1 Datasets",
            "text": "We adopt Korean multiple classification benchmark datasets. Each dataset is used for training and evaluation according to the proposed task format.\nKLUE-NLI The KLUE-NLI dataset (Park et al., 2021b) has explicitly been curated for the natural language inference (NLI) task (Bowman et al., 2015). The training, validation, and test data comprises 24,998, 3,000, and 3,000 sentence pairs. In this task, models are required to process pairs of sentences, referred to as the premise and hypothesis, and deduce the underlying relationship, which could be entailment, contradiction, or neutral.\nKorNLI The KorNLI dataset (Ham et al., 2020) is also designed for Korean natural language inference. It is generated by translating the English Standard NLI (SNLI) and Multi-Genre NLI (MNLI) datasets, as well as the Cross-lingual NLI (XNLI) dataset into Korean. The training data of the KorNLI dataset consists of 942,854 sentence pairs, machine-translated from the SNLI and MNLI datasets, while the evaluation data comprise 7,500 translated sentence pairs from the XNLI dataset.\nKLUE-STS The KLUE-STS dataset (Park et al., 2021b) has been meticulously assembled for the semantic textual similarity (STS) task, encompassing 11,668 sentence pairs for training, 519 for validation, and 1,037 for testing. In this task, models assess pairs of sentences and determine their degree of semantic similarity.\nKLUE-YNAT KLUE-YNAT dataset (Park et al., 2021b) has been designed for the topic classification task. The dataset includes training, validation, and test data composed of 45,678, 9,107, and 9,107 samples. In this task, models are tasked with processing sentences and assigning them to predefined news categories based on the underlying topic.\nNSMC The NSMC dataset4 has been constructed for the sentiment analysis task in Korean. It is derived from movie reviews and their respective ratings from the NAVER platform. The training data of the NSMC dataset comprises 150,000 reviews, and the test set consists of 50,000 reviews. In this task, models are required to process individual sentences and determine their underlying sentiment, which can be positive or negative."
        },
        {
            "heading": "4.2 Models",
            "text": "The morpheme blender employs an encoderdecoder structure and uses KoBART (Lewis et al., 2020), a pre-trained generative language model for\n4https://github.com/e9t/nsmc\nKorean, as its backbone. We choose KoBART because it exhibits acceptable Korean text generation abilities even in its small model parameters (124M) (Seo et al., 2022). KoBART takes an input sequence consisting of a lexical morpheme set, a label word, and an optional premise to generate synthetic data. The label discriminator features an encoder architecture and utilizes KLUEBERT-base (Devlin et al., 2018), a pre-trained language model for Korean, as its backbone. KLUEBERT-base conveys the original sentence\u2019s label to KoBART through a contrastive loss. We opt for the KLUE-BERT-base as it is the most compact model among the evaluated alternatives, mitigating the possible distillation effects that could emerge during the discrimination process due to model sizes (Park et al., 2021b). To validate the efficacy of CHEF, we select the following Korean language understanding models: BERTKBase (KLUE-BERTbase), RoBERTaKBase (KLUE-RoBERTa-base), and RoBERTaKLarge (KLUE-RoBERTa-large)."
        },
        {
            "heading": "4.3 Compared Methods",
            "text": "The experiments are conducted based on full- and few-shot learning. We conduct comparative ex-\nperiments using BackTranslation (BT) (Sennrich et al., 2016), Korean-EDA (KoEDA)5(Wei and Zou, 2019), AEDA (Karimi et al., 2021), and Mixup (Zhang et al., 2018; Sun et al., 2020). We also employ GPT-3.5 (Ouyang et al., 2022; Brown et al., 2020) and GPT-4 (OpenAI, 2023) as backbone augmentation models. Our LLM-based approaches included semantic-based paraphrasing (PARA) (Fadaee et al., 2017; Kobayashi, 2018) and counterfactual augmentation (CA) (Liu et al., 2021; Ou et al., 2022)."
        },
        {
            "heading": "5 Experimental Results",
            "text": "In this section, we evaluate a set of multiple classification datasets with our proposed method."
        },
        {
            "heading": "5.1 Full-shot Learning",
            "text": "As shown in Table 1, we compare the effectiveness and extent of performance improvement of CHEF against other data augmentation methods in the full-shot settings. We proportionally augmented the data with respect to the size of the training dataset, ensuring that an amount corresponding to 1% of the training data is added to each label.\n5https://github.com/toriving/KoEDA\nAugmentation methods employing LLMs exhibit the generation of sentences with high diversity and superior qualitative properties. However, they occasionally demonstrate decreases in performance of up to 2%. These declines can be attributed to the significant alteration of the overall data distribution, which makes it challenging to fully account for the underlying labeling scheme intrinsic to the task. In the case of counterfactual augmentation, the instability in label transformations is amplified, even when provided with chain-of-thought prompts (Wei et al., 2022). Original sentences and labels undergo counterfactual changes without any review for conformance, resulting in increased variability that hinders the effectiveness of the augmentation process.\nBT cannot guarantee the preservation of the same labels, resulting in performance decrease cases. AEDA does not show the same level of improvements for all tasks as reported in (Karimi et al., 2021) when applied to pre-trained Korean language models. Furthermore, Mixup exhibits notable performance improvements in KLUE-NLI; however, these enhancements do not consistently ensure improved results across other tasks. KoEDA utilizes a morphological analyzer to segment the data and leverages a thesaurus for Korean. This approach shows modest performance improvements, validating the effectiveness of morpheme-based data augmentation in line with the characteristics of the Korean language. However, its performance enhancements are limited due to challenges in ensuring label consistency and the absence of contextual understanding. We observe that CHEF exhibits the most significant performance improvements. By leveraging a comprehensive understanding of derivation and inflectional properties in conjunction with functional morphemes, CHEF effectively enhances models across various scenarios. As a result, CHEF overcomes these challenges and demon-\nstrates the efficacy of data augmentation for fullshot learning."
        },
        {
            "heading": "5.2 Changes in Augmentation Size",
            "text": "Figure 3 presents the performance changes according to each dataset\u2019s augmentation ratio of CHEF. As the amount of synthetic data increases, the probability of the involvement of data that renders negative noise accumulations. In the full-shot setting, simply expanding the number of synthetic data samples does not guarantee a performance improvement. However, CHEF maintains the effectiveness of data augmentation and shows superior enhancements compared to the baseline in most cases where data augmentation is limited to within 10% (More details in Appendix 7)."
        },
        {
            "heading": "5.3 Few-shot and Synthetic-only",
            "text": "We further probe the efficacy of CHEF in the NLI task, which is relatively capricious data augmentation effects in Section \u00a75.2. We conduct comparative experiments in a few-shot setting with 32 samples and add 32 augmented sentences for each label. Table 2 shows the results averaged and maximum value over three different seeds. The data augmentation methods using the LLMs are more effective in low-resource regimes than in full-shot settings. CHEF significantly boosts the model\u2019s performance, even in situations with limited data. Furthermore, we evaluate the effectiveness of solely using the synthetic data generated by CHEF (i.e., CHEFSynOnly) to train the models. The outcomes reveal a spectrum of enhancements in performance and demonstrate the quality of the CHEF augmentation."
        },
        {
            "heading": "5.4 Ablation Study",
            "text": "To precisely evaluate the importance of labeling consistency, we conduct ablation studies by systematically altering the label discriminator com-\nponent within the CHEF. The variations in performance resulting from the exclusion or absence of pre-training in the discriminator are presented in Table 3. Upon removal of the label discriminator, we observe a decline in the augmentation effectiveness in the full-shot setting. Notably, even without the pre-training phase of the discriminator, CHEF maintains its capacity for enhancing performance. By integrating the morpheme blender and the pretrained label discriminator using contrastive loss, CHEF achieves the highest level of performance. These empirical findings provide compelling evidence for the effectiveness of incorporating the discriminator to ensure label consistency."
        },
        {
            "heading": "5.5 Cross-domain Analysis",
            "text": "We conduct cross-domain experiments in the NLI task. As described in \u00a74.1, KLUE-NLI and KorMNLI/SNLI are datasets derived from different sources. To assess performance, we employ cross-\nevaluation of the model trained on data augmented using CHEF. Table 4 presents the observed performance improvements in three out of four cases, compared to the baselines without augmentation. The cross-domain effectiveness of CHEF is more pronounced in the few-shot setting. Furthermore, few-shot CHEF outperforms models trained and evaluated solely on a single domain. These results alleviate concerns regarding the degradation of the model\u2019s generalization ability due to the augmentation effects, reaffirming its robust performance across different domains."
        },
        {
            "heading": "5.6 Larger and Multilingual Blender",
            "text": "To comprehend the implications of employing a larger generative model for CHEF, we evaluate the performance leveraging the mBART-large (Liu et al., 2020). To avoid unintended knowledge distillation effects resulting from using larger discriminator modules, we maintain the same discriminator. As presented in Table 5, the results present that using a larger generative model as the blender does not necessarily ensure higher performance, but comparable effects can be achieved as well. We further find that employing a multilingual model as the blender yields similar augmentation effects."
        },
        {
            "heading": "5.7 Morpheme Ingredients Variants",
            "text": "Table 6 shows the CHEF\u2019s performance employing different morpheme ingredients. We modify\nthe lexical morphemes by substituting them with synonyms or altering them to antonyms considering labels. Replacing lexical morphemes based on antonyms or synonyms is not guaranteed to preserve the compositionality across other lexical morphemes. The involvement of non-contextualized morphemes leads to diminished generative capabilities and the generation of low-quality synthetic data, resulting in a decline in performance."
        },
        {
            "heading": "6 Conclusion",
            "text": "We introduce CHEF, a novel data augmentation method designed for the Korean language, which is inherently agglutinative and rich in morphological variations. CHEF leverages the combinatory properties of lexical and functional morphemes in Korean to construct linguistically diverse and labelconsistent synthetic data. By incorporating a morpheme blender and a label discriminator module, CHEF ensures that the generated synthetic data preserves the label information of the original dataset while introducing new linguistic forms through morphological alterations. Our experiments demonstrate the effectiveness and robustness of CHEF across various scenarios. In future work, we plan to explore the adaptability of the CHEF architecture to other morphologically rich languages and further optimize the interaction between the morpheme blender and label discriminator.\nLimitations\nThis study proposes an effective augmentation method suitable for Korean datasets, considering the unique linguistic characteristics of the Korean\nlanguage. However, the method of this study primarily focuses on Korean and does not sufficiently consider other languages. This area can be further explored and improved in future research. Also, due to the performance limitations of the off-theshelf pre-trained generative model, unnecessary word duplication occurs in sentences augmented using CHEF. This issue can be addressed by introducing a more optimized decoding strategy or employing a more advanced generative model, expected to produce higher-quality results. Moreover, The mecab-ko analyzer, which we used for constructing the set of morphemes, can have an error rate depending on the domain and data it is applied to. The proportion of morpheme combination rules in CHEF may differ based on which morpheme analyzer is used to construct the morpheme set. This presents a potential risk: if a less proficient morpheme analyzer is used, it may fail to recover the original morphemes accurately and be susceptible to errors when processing data from untrained domains. Therefore, leveraging a more advanced morpheme analyzer could enable data augmentation that more accurately reflects the linguistic characteristics of the Korean language.\nEthics Statement\nWe employed Korean multiple classification benchmark datasets in our experiments. Data augmentation was conducted by altering the morphological composition of sentences present in the training datasets. Excluding the NSMC dataset, each benchmark dataset has been officially released and has undergone validation to ensure ethical considerations using human annotators. The NSMC dataset may contain some unethical expressions among negative reviews. Furthermore, we acknowledge that the pre-trained language models (PLMs), used as the backbone for the morpheme blender, could have been exposed to toxic data during the pretraining process, thereby possessing the potential to generate inappropriate synthetic data."
        },
        {
            "heading": "Acknowledgments",
            "text": "This research was supported by the MSIT (Ministry of Science and ICT), Korea, under the ITRC (Information Technology Research Center) support program (IITP-2023-2018-0-01405) supervised by the IITP (Institute for Information & Communications Technology Planning & Evaluation). This work was supported by Institute of Information &\ncommunications Technology Planning & Evaluation (IITP) grant funded by the Korea government (MSIT) (No. 2020-0-00368, A Neural-Symbolic Model for Knowledge Acquisition and Inference Techniques). This research was supported by the MSIT (Ministry of Science and ICT), Korea, under the ICT Creative Consilience program (IITP-20232020-0-01819) supervised by the IITP(Institute for Information & communications Technology Planning & Evaluation)."
        },
        {
            "heading": "A Experimental Details",
            "text": "We trained our models on a single NVIDIA A6000 GPU (48GB) and AMD EPYC 7513 32-Core Processor CPUs.\nCHEF Modules As a discriminator, CHEF uses KLUE-BERT-base6(Devlin et al., 2018; Park et al., 2021a), which has 768 embedding sizes, 768 hidden sizes, 12 layers, and 12 attention heads. As a morpheme blender, CHEF employs KoBARTbase7(Lewis et al., 2020), where each encoder and decoder have 768 hidden sizes, 6 layers, and 16 attention heads, respectively.\nKorean Language Understanding Models We used three pre-trained language models for Korean. KLUE-BERT-base (Devlin et al., 2018; Park et al., 2021a) has 768 embedding sizes, 768 hidden sizes, 12 layers, and 12 attention heads. KLUERoBERTa-base (Liu et al., 2019; Park et al., 2021a) also has 768 embedding sizes, 768 hidden sizes, 12 layers, and 12 attention heads. KLUE-RoBERTalarge (Liu et al., 2019; Park et al., 2021a) has 1024 embedding sizes, 1024 hidden sizes, 24 layers, and 16 attention heads.\nEDA & BackTranslation We applied BackTranslation (BT) (Sennrich et al., 2016) and easy data augmentation (EDA) (Wei and Zou, 2019) to the data to generate synthetic data. For BT, we used M2M100 (Fan et al., 2021) to translate Korean text into Japanese and back into Korean. The model we used for BT is facebook-M2M100 (418M), which has a 1024 embedding size, 12 layers, and 16 attention heads. We used KoEDA 8, a library that utilizes the Korean WordNet (Bikel, 2000) for EDA.\n6https://github.com/KLUE-benchmark/KLUE 7https://github.com/SKT-AI/KoBART 8https://github.com/toriving/KoEDA\nAEDA & Mixup We used data augmentation with the default settings presented in the AEDA paper (Karimi et al., 2021)9. Mixup (Zhang et al., 2018; Sun et al., 2020), which is suitable for Transformer-based models, was implemented by randomly shuffling sample indices within a batch. Subsequently, the hidden state of each sample and the label scalar were mixed at a ratio of 0.2, following the default \u03bb value, to create synthetic data.\nLarge Language Models We included GPT-3.5 (gpt-3.5-turbo-0301) (Brown et al., 2020; Ouyang et al., 2022) and GPT-4 (gpt-4-0314) (OpenAI, 2023) as the large language models. We applied augmentation to the given training data using the OpenAI API10 and LangChain11. As depicted in Figure 4 and 5, the prompt consists of an example in a one-shot template for the given task and instructions for the augmentation method. The cost incurred due to the OpenAI API calls amounted to $245.17, and the data augmentation approach utilizing LLMs did not show a significant improvement in model performance relative to the cost incurred.\nHyperparameters Hyperparameters to train the CHEF are batch size 8, learning rate 1\u00d7 10\u22125, AdamW optimizer (Loshchilov and Hutter, 2019) (\u03b21 = 0.9, \u03b22 = 0.999, \u03f5 = 1e \u2212 8), lambda 0.2, max source length 200, max target length 168, and 5 epochs. To train Korean language understanding models is batch size 32, learning rate 2\u00d7 10\u22125, AdamW optimizer (\u03b21 = 0.9, \u03b22 = 0.999, \u03f5 = 1e\u22128), and 20 epochs. In the case of few-shot learning, all other hyperparameter settings remained unchanged except for the epoch adjusted to 30, considering the model\u2019s overfitting point.\nDecoding Strategy Within the framework of CHEF augmentation, we imposed specific constraints on the decoding strategy. We established the following settings: a beam size of 10, a maximum sequence length of 168, a minimum sequence length of 5, a repetition penalty of 2, and a norepeat n-gram size of 3 to penalize the generation of duplicate tokens."
        },
        {
            "heading": "B Qualitative Analysis",
            "text": "We applied CHEF to several benchmark datasets and conducted qualitative analyses of the aug-\n9https://github.com/akkarimi/aeda_nlp 10https://openai.com/ 11https://python.langchain.com/en/latest/\nmented output. CHEF augmentations typically take the following form of three types of variations.\nAs described in Figure 6, we observe that the outputs generated by CHEF primarily involve modifications of particles and determiners that are closely associated with nouns. However, no significant changes are observed in the verbs of the sentences. The combination of lexical morphemes introduces slight variations in the meaning of the generated sentences but does not lead to changes in the labels assigned to them.\nAs shown in Figure 7, the augmented output primarily consisted of conjugation variations, leading to a diverse range of sentences with different ending conjunction rules. However, there is minimal impact on the overall meaning of the sentences themselves. Notably, the augmented hypotheses are labeled as having significantly lower similarity to the corresponding premises regarding semantic similarity evaluation.\nAs illustrated in Figure 8, the augmentation process involves single sentences resembling news headlines fitting into the \"Life & Culture\" section. Unlike the other benchmark datasets, KLUEYNAT is characterized by the fact that the original sentences contain relatively fewer functional morphemes. Based on this characteristic of the dataset, CHEF was applied to augment the data by changing the order or role of the lexical morphemes in the sentence."
        }
    ],
    "title": "CHEF in the Language Kitchen: A Generative Data Augmentation Leveraging Korean Morpheme Ingredients",
    "year": 2023
}