{
    "abstractText": "The pruning objective has recently extended beyond accuracy and sparsity to robustness in language models. Despite this, existing methods struggle to enhance robustness against adversarial attacks when continually increasing model sparsity and require a retraining process. As humans step into the era of large language models, these issues become increasingly prominent. This paper proposes that the robustness of language models is proportional to the extent of pre-trained knowledge they encompass. Accordingly, we introduce a post-training pruning strategy designed to faithfully replicate the embedding space and feature space of dense language models, aiming to conserve more pretrained knowledge during the pruning process. In this setup, each layer\u2019s reconstruction error not only originates from itself but also includes cumulative error from preceding layers, followed by an adaptive rectification. Compared to other state-of-art baselines, our approach demonstrates a superior balance between accuracy, sparsity, robustness, and pruning cost with BERT on datasets SST2, IMDB, and AGNews, marking a significant stride towards robust pruning in language models.",
    "authors": [
        {
            "affiliations": [],
            "name": "Jianwei Li"
        },
        {
            "affiliations": [],
            "name": "Qi Lei"
        },
        {
            "affiliations": [],
            "name": "Wei Cheng"
        },
        {
            "affiliations": [],
            "name": "Dongkuan Xu"
        }
    ],
    "id": "SP:8133b9b94146b53328289ddf9e2324c792d16646",
    "references": [
        {
            "authors": [
                "Galen Andrew",
                "Jianfeng Gao."
            ],
            "title": "Scalable training of L1-regularized log-linear models",
            "venue": "Proceedings of the 24th International Conference on Machine Learning, pages 33\u201340.",
            "year": 2007
        },
        {
            "authors": [
                "Tianlong Chen",
                "Jonathan Frankle",
                "Shiyu Chang",
                "Sijia Liu",
                "Yang Zhang",
                "Zhangyang Wang",
                "Michael Carbin."
            ],
            "title": "The lottery ticket hypothesis for pretrained bert networks",
            "venue": "Advances in neural information processing systems, 33:15834\u201315846.",
            "year": 2020
        },
        {
            "authors": [
                "Meier-Hellstern",
                "Douglas Eck",
                "Jeff Dean",
                "Slav Petrov",
                "Noah Fiedel"
            ],
            "title": "2022. Palm: Scaling language modeling with pathways",
            "year": 2022
        },
        {
            "authors": [
                "Francesco Croce",
                "Sylvestre-Alvise Rebuffi",
                "Evan Shelhamer",
                "Sven Gowal."
            ],
            "title": "Seasoning model soups for robustness to adversarial and natural distribution shifts",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recog-",
            "year": 2023
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "arXiv preprint arXiv:1810.04805.",
            "year": 2018
        },
        {
            "authors": [
                "Mengnan Du",
                "Varun Manjunatha",
                "Rajiv Jain",
                "Ruchi Deshpande",
                "Franck Dernoncourt",
                "Jiuxiang Gu",
                "Tong Sun",
                "Xia Hu."
            ],
            "title": "Towards interpreting and mitigating shortcut learning behavior of nlu models",
            "venue": "Proceedings of the 2021 Conference of the North",
            "year": 2021
        },
        {
            "authors": [
                "Mengnan Du",
                "Subhabrata Mukherjee",
                "Yu Cheng",
                "Milad Shokouhi",
                "Xia Hu",
                "Ahmed Hassan Awadallah."
            ],
            "title": "Robustness challenges in model distillation and pruning for natural language understanding",
            "venue": "Proceedings of the 17th Conference of the European",
            "year": 2023
        },
        {
            "authors": [
                "Jonathan Frankle",
                "Michael Carbin."
            ],
            "title": "The lottery ticket hypothesis: Finding sparse, trainable neural networks",
            "venue": "arXiv preprint arXiv:1803.03635.",
            "year": 2018
        },
        {
            "authors": [
                "Jonathan Frankle",
                "Gintare Karolina Dziugaite",
                "Daniel Roy",
                "Michael Carbin."
            ],
            "title": "Linear mode connectivity and the lottery ticket hypothesis",
            "venue": "International Conference on Machine Learning, pages 3259\u20133269. PMLR.",
            "year": 2020
        },
        {
            "authors": [
                "Elias Frantar",
                "Dan Alistarh."
            ],
            "title": "Optimal brain compression: A framework for accurate posttraining quantization and pruning",
            "venue": "arXiv preprint arXiv:2208.11580.",
            "year": 2022
        },
        {
            "authors": [
                "Elias Frantar",
                "Dan Alistarh"
            ],
            "title": "Sparsegpt: Massive language models can be accurately pruned in one-shot",
            "year": 2023
        },
        {
            "authors": [
                "Siddhant Garg",
                "Goutham Ramakrishnan."
            ],
            "title": "BAE: BERT-based adversarial examples for text classification",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6174\u20136181, Online. Association for",
            "year": 2020
        },
        {
            "authors": [
                "Mitchell Gordon",
                "Kevin Duh",
                "Nicholas Andrews."
            ],
            "title": "Compressing BERT: Studying the effects of weight pruning on transfer learning",
            "venue": "Proceedings of the 5th Workshop on Representation Learning for NLP, pages 143\u2013155, Online. Association for Com-",
            "year": 2020
        },
        {
            "authors": [
                "Shupeng Gui",
                "Haotao Wang",
                "Haichuan Yang",
                "Chen Yu",
                "Zhangyang Wang",
                "Ji Liu."
            ],
            "title": "Model compression with adversarial robustness: A unified optimization framework",
            "venue": "Advances in Neural Information Processing Systems, 32.",
            "year": 2019
        },
        {
            "authors": [
                "B. Hassibi",
                "D.G. Stork",
                "G.J. Wolff."
            ],
            "title": "Optimal brain surgeon and general network pruning",
            "venue": "IEEE International Conference on Neural Networks, pages 293\u2013299 vol.1.",
            "year": 1993
        },
        {
            "authors": [
                "Sara Hooker",
                "Nyalleng Moorosi",
                "Gregory Clark",
                "Samy Bengio",
                "Emily Denton."
            ],
            "title": "Characterising bias in compressed models",
            "venue": "arXiv preprint arXiv:2010.03058.",
            "year": 2020
        },
        {
            "authors": [
                "Di Jin",
                "Zhijing Jin",
                "Joey Tianyi Zhou",
                "Peter Szolovits."
            ],
            "title": "Is bert really robust? a strong baseline for natural language attack on text classification and entailment",
            "venue": "Proceedings of the AAAI conference on artificial intelligence, volume 34, pages",
            "year": 2020
        },
        {
            "authors": [
                "Yann LeCun",
                "John Denker",
                "Sara Solla."
            ],
            "title": "Optimal brain damage",
            "venue": "Advances in neural information processing systems, 2.",
            "year": 1989
        },
        {
            "authors": [
                "Jinfeng Li",
                "Shouling Ji",
                "Tianyu Du",
                "Bo Li",
                "Ting Wang."
            ],
            "title": "Textbugger: Generating adversarial text against real-world applications",
            "venue": "arXiv preprint arXiv:1812.05271.",
            "year": 2018
        },
        {
            "authors": [
                "Linyang Li",
                "Ruotian Ma",
                "Qipeng Guo",
                "Xiangyang Xue",
                "Xipeng Qiu."
            ],
            "title": "BERT-ATTACK: Adversarial attack against BERT using BERT",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages",
            "year": 2020
        },
        {
            "authors": [
                "Linyang Li",
                "Ruotian Ma",
                "Qipeng Guo",
                "Xiangyang Xue",
                "Xipeng Qiu."
            ],
            "title": "Bert-attack: Adversarial attack against bert using bert",
            "venue": "arXiv preprint arXiv:2004.09984.",
            "year": 2020
        },
        {
            "authors": [
                "Linyang Li",
                "Xipeng Qiu."
            ],
            "title": "Token-aware virtual adversarial training in natural language understanding",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 8410\u20138418.",
            "year": 2021
        },
        {
            "authors": [
                "Chen Liang",
                "Simiao Zuo",
                "Minshuo Chen",
                "Haoming Jiang",
                "Xiaodong Liu",
                "Pengcheng He",
                "Tuo Zhao",
                "Weizhu Chen."
            ],
            "title": "Super tickets in pre-trained language models: From model compression to improving generalization",
            "venue": "Proceedings of the 59th",
            "year": 2021
        },
        {
            "authors": [
                "Andrew L. Maas",
                "Raymond E. Daly",
                "Peter T. Pham",
                "Dan Huang",
                "Andrew Y. Ng",
                "Christopher Potts."
            ],
            "title": "Learning word vectors for sentiment analysis",
            "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human",
            "year": 2011
        },
        {
            "authors": [
                "Aleksander Madry",
                "Aleksandar Makelov",
                "Ludwig Schmidt",
                "Dimitris Tsipras",
                "Adrian Vladu."
            ],
            "title": "Towards deep learning models resistant to adversarial attacks",
            "venue": "arXiv preprint arXiv:1706.06083.",
            "year": 2017
        },
        {
            "authors": [
                "R Thomas McCoy",
                "Junghyun Min",
                "Tal Linzen."
            ],
            "title": "Berts of a feather do not generalize together: Large variability in generalization across models with similar test set performance",
            "venue": "Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpret-",
            "year": 2020
        },
        {
            "authors": [
                "John Morris",
                "Eli Lifland",
                "Jin Yong Yoo",
                "Jake Grigsby",
                "Di Jin",
                "Yanjun Qi."
            ],
            "title": "Textattack: A framework for adversarial attacks, data augmentation, and adversarial training in nlp",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natu-",
            "year": 2020
        },
        {
            "authors": [
                "Timothy Niven",
                "Hung-Yu Kao."
            ],
            "title": "Probing neural network comprehension of natural language arguments",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4658\u20134664, Florence, Italy. Association for Compu-",
            "year": 2019
        },
        {
            "authors": [
                "Sai Prasanna",
                "Anna Rogers",
                "Anna Rumshisky."
            ],
            "title": "When BERT Plays the Lottery, All Tickets Are Winning",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 3208\u20133229, Online. Association for",
            "year": 2020
        },
        {
            "authors": [
                "Shuhuai Ren",
                "Yihe Deng",
                "Kun He",
                "Wanxiang Che."
            ],
            "title": "Generating natural language adversarial examples through probability weighted word saliency",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1085\u2013",
            "year": 2019
        },
        {
            "authors": [
                "Victor Sanh",
                "Thomas Wolf",
                "Alexander Rush."
            ],
            "title": "Movement pruning: Adaptive sparsity by fine-tuning",
            "venue": "Advances in Neural Information Processing Systems, 33:20378\u201320389.",
            "year": 2020
        },
        {
            "authors": [
                "Vikash Sehwag",
                "Shiqi Wang",
                "Prateek Mittal",
                "Suman Jana."
            ],
            "title": "Hydra: Pruning adversarially robust neural networks",
            "venue": "Advances in Neural Information Processing Systems, 33:19655\u201319666.",
            "year": 2020
        },
        {
            "authors": [
                "Song",
                "Mohammad Shoeybi",
                "Yuxiong He",
                "Michael Houston",
                "Saurabh Tiwary",
                "Bryan Catanzaro"
            ],
            "title": "Using deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model",
            "year": 2022
        },
        {
            "authors": [
                "Richard Socher",
                "Alex Perelygin",
                "Jean Wu",
                "Jason Chuang",
                "Christopher D. Manning",
                "Andrew Ng",
                "Christopher Potts."
            ],
            "title": "Recursive deep models for semantic compositionality over a sentiment treebank",
            "venue": "Proceedings of the 2013 Conference on Empiri-",
            "year": 2013
        },
        {
            "authors": [
                "Joe Stacey",
                "Pasquale Minervini",
                "Haim Dubossarsky",
                "Sebastian Riedel",
                "Tim Rockt\u00e4schel."
            ],
            "title": "Avoiding the Hypothesis-Only Bias in Natural Language Inference via Ensemble Adversarial Training",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods",
            "year": 2020
        },
        {
            "authors": [
                "E. Kelly Buchanan",
                "Kevin Patrick Murphy",
                "D. Sculley",
                "Yarin Gal",
                "Zoubin Ghahramani",
                "Jasper Snoek",
                "Balaji Lakshminarayanan."
            ],
            "title": "Plex: Towards reliability using pretrained large model extensions",
            "venue": "First Workshop on Pre-training: Perspectives, Pit-",
            "year": 2022
        },
        {
            "authors": [
                "Manoj-Rohit Vemparala",
                "Nael Fasfous",
                "Alexander Frickenstein",
                "Sreetama Sarkar",
                "Qi Zhao",
                "Sabine Kuhn",
                "Lukas Frickenstein",
                "Anmol Singh",
                "Christian Unger",
                "Naveen-Shankar Nagaraja"
            ],
            "title": "Adversarial robust model compression using in-train pruning",
            "year": 2021
        },
        {
            "authors": [
                "Jindong Wang",
                "Xixu Hu",
                "Wenxin Hou",
                "Hao Chen",
                "Runkai Zheng",
                "Yidong Wang",
                "Linyi Yang",
                "Haojun Huang",
                "Wei Ye",
                "Xiubo Geng"
            ],
            "title": "On the robustness of chatgpt: An adversarial and out-of-distribution perspective",
            "year": 2023
        },
        {
            "authors": [
                "Yuan Hu",
                "Qiyuan Bian",
                "Zhihua Liu",
                "Shan Qin",
                "Bolin Zhu",
                "Xiaoyu Xing",
                "Jinlan Fu",
                "Yue Zhang",
                "Minlong Peng",
                "Xiaoqing Zheng",
                "Yaqian Zhou",
                "Zhongyu Wei",
                "Xipeng Qiu",
                "Xuanjing Huang"
            ],
            "title": "TextFlint: Unified multilingual robustness evaluation toolkit",
            "year": 2021
        },
        {
            "authors": [
                "Mitchell Wortsman",
                "Gabriel Ilharco",
                "Samir Yitzhak Gadre",
                "Rebecca Roelofs",
                "Raphael Gontijo-Lopes",
                "Ari S. Morcos",
                "Hongseok Namkoong",
                "Ali Farhadi",
                "Yair Carmon",
                "Simon Kornblith",
                "Ludwig Schmidt"
            ],
            "title": "Model soups: averaging weights of multiple",
            "year": 2022
        },
        {
            "authors": [
                "Zhiheng Xi",
                "Rui Zheng",
                "Tao Gui",
                "Qi Zhang",
                "Xuanjing Huang."
            ],
            "title": "Efficient adversarial training with robust early-bird tickets",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 8318\u20138331, Abu",
            "year": 2022
        },
        {
            "authors": [
                "Canwen Xu",
                "Wangchunshu Zhou",
                "Tao Ge",
                "Ke Xu",
                "Julian McAuley",
                "Furu Wei."
            ],
            "title": "Beyond preserved accuracy: Evaluating loyalty and robustness of BERT compression",
            "venue": "Proceedings of the 2021 Conference",
            "year": 2021
        },
        {
            "authors": [
                "Shaokai Ye",
                "Kaidi Xu",
                "Sijia Liu",
                "Hao Cheng",
                "Jan-Henrik Lambrechts",
                "Huan Zhang",
                "Aojun Zhou",
                "Kaisheng Ma",
                "Yanzhi Wang",
                "Xue Lin."
            ],
            "title": "Adversarial robustness vs",
            "venue": "model compression, or both? In Proceedings of the IEEE/CVF International Conference",
            "year": 2019
        },
        {
            "authors": [
                "Xiang Zhang",
                "Junbo Zhao",
                "Yann LeCun"
            ],
            "title": "Character-level convolutional networks for text classification",
            "year": 2016
        },
        {
            "authors": [
                "Rui Zheng",
                "Bao Rong",
                "Yuhao Zhou",
                "Di Liang",
                "Sirui Wang",
                "Wei Wu",
                "Tao Gui",
                "Qi Zhang",
                "Xuanjing Huang."
            ],
            "title": "Robust lottery tickets for pre-trained language models",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Lin-",
            "year": 2022
        },
        {
            "authors": [
                "Chen Zhu",
                "Yu Cheng",
                "Zhe Gan",
                "Siqi Sun",
                "Tom Goldstein",
                "Jingjing Liu."
            ],
            "title": "Freelb: Enhanced adversarial training for natural language understanding",
            "venue": "arXiv preprint arXiv:1909.11764.",
            "year": 2019
        },
        {
            "authors": [
                "Gordon"
            ],
            "title": "2021), due to its efficiency and effective knowledge transfer to downstream tasks. Sanh et al. (2020) adds penalty terms to the loss function to eliminate redundant weights",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Pruning is a widely recognized compression method employed to decrease the model size and accelerate model inference (Frankle and Carbin, 2018; Chen et al., 2020; Prasanna et al., 2020; Chen et al., 2021). In the age of large language models (Andrew and Gao, 2007; Brown et al., 2020; Chowdhery et al., 2022; OpenAI, 2023; Touvron et al., 2023; Ouyang et al., 2022; Smith et al., 2022), the necessity of pruning has increased because it greatly reduces deployment costs (Frantar and Alistarh, 2023). In addition to the significant computation cost, the robustness of language models has emerged as a crucial factor that demands attention. This is primarily because models need to\nremain resilient against adversarial attacks, even in challenging real-world circumstances (Tran et al., 2022; Wang et al., 2023). Therefore, exploring robust pruning strategies against adversarial attacks in language models could potentially yield a substantial impact (Xu et al., 2021; Du et al., 2023).\nRecent research has extended the pruning of language models beyond accuracy and sparsity, with an emphasis on the trade-off between accuracy, sparsity, robustness and cost (Du et al., 2023; Xu et al., 2021; Liang et al., 2021; Xi et al., 2022). Zheng et al. (2022) propose a joint optimization objective to guide the pruning and adversarial training simultaneously. Their approach views the identified subnetworks as robust tickets, which can be trained as normal and offer enhanced robustness. Despite achieving state-of-the-art results on target datasets, these methods still display vulnerabilities, as evidenced by a significant gap between metrics of clean accuracy 1 and accuracy under attack. Moreover, the performance also rapidly declines when sparsity exceeds a moderate level. Expanding on their work, Xi et al. (2022) propose using robust early-bird tickets to reduce the computational cost from adversarial training. However, they face similar challenges regarding the trade-off between robustness and sparsity. In summary, existing robust pruning works often demonstrate limited sparsity, insufficient robustness, and expensive cost, indicating the ongoing challenge of the balance between accuracy and the other three aspects.\nTo address this challenge, this paper investigates why language models are susceptible to adversarial attacks. (Wang et al., 2021; Garg and Ramakrishnan, 2020; Jin et al., 2020). Previous studies have indicated that language models frequently capitalize on biases and artifacts inherent in datasets as predictive shortcuts, which impedes reasoning ability and skills to develop advanced semantic comprehension. (Du et al., 2021; Niven and Kao, 2019;\n1accuracy without adversarial attacks\nMcCoy et al., 2020; Du et al., 2023). This reliance leads to a more severe loss of pre-trained knowledge during the pruning process. Furthermore, the adversarial samples in Natural Language Processing (NLP) are crafted by replacing components of sentences with semantically similar counterparts, thereby retaining high semantic similarity in the entire sentence (Li et al., 2020a; Ren et al., 2019; Jin et al., 2020). In this way, language models that depend on spurious features from particular words can not defend against adversarial attacks constructed by replacing those words with semantically similar alternatives. To put it more plainly, this primarily stems from the fact that, without pre-trained knowledge, the sparse language model treats the substitute word simply as an integer identifier. Based on the above observation, we explore the following questions in this paper:\nQuestion 1. What is the core to defend against adversarial attacks for sparse language models?\nThis paper proposes that the robustness of sparse language models is directly proportional to the amount of pre-trained knowledge retained after pruning. Intuitively, the robustness of a sparse language model is fundamentally tied to its capability to distill advanced semantic features from input sentences. This capability is largely established during the pre-training phase of dense language models, emphasizing the pivotal role of acquired semantic knowledge. The extensive experiments well support our statement.\nQuestion 2. How can we efficiently prevent the loss of pre-trained knowledge in pruning to preserve or even enhance robustness?\nPrevious research has demonstrated that pruning exacerbates the model\u2019s dependency on spurious features (Xu et al., 2021; Du et al., 2023). We further confirm that traditional pruning methods lead to a considerable loss of pre-trained knowledge and poor robustness. To prevent the above things, we propose a pruning approach that minimizes damage to the embedding space and feature space of dense language models, striving to replicate the features in each layer completely. Specifically, for each layer, we iteratively eliminate a single weight at a time and counterbalance the loss by updating the remaining weights based on the Hessian Matrix. In this setup, the reconstruction error at each layer arises not only from its own layer but also incorporates the accumulated error from preceding layers. This is achieved by adaptively updating\nthe pruning-dependent information in accordance with the sparse output generated by previous layers. Concurrently, there\u2019s an ongoing effort to correct these errors collectively. Moreover, our method, being a post-training approach, is cost-effective for current language models, as it circumvents rigorous retraining processes. Extensive experiments show that our approach achieves a better trade-off between accuracy, sparsity, robustness, and pruning cost in SST2, AGNews, and IMDB compared with other state-of-art methods."
        },
        {
            "heading": "2 Related Work",
            "text": "Textual Adversarial Attacks and Defense. Textual adversarial attacks pose a significant challenge to the robustness of language models. These attacks, formulated by carefully altering certain segments of sentences with semantically similar counterparts, aim to fool language models (Jin et al., 2020; Li et al., 2020a). To enhance the robustness of language models and defend against adversarial attacks, a range of potent defensive strategies, such as adversarial training, has been proposed. (Madry et al., 2017; Zhu et al., 2019; Li and Qiu, 2021). Different from their research, which focuses on dense models, we explore the robustness in the context of language model pruning.\nRobust Model Pruning. Prior studies indicate that sparse models tend to underperform in Compression Identified Examples (CIE), suggesting that the pruning process exacerbates the inherent algorithmic biases hidden within the datasets (Hooker et al., 2020). In Computer Vision (CV), simultaneous optimization of model pruning and adversarial training has been advocated as an effective solution to this issue (Gui et al., 2019; Ye et al., 2019; Sehwag et al., 2020; Vemparala et al., 2021). In NLP, Du et al. (2023) propose to prevent model overfitting on easy samples by leveraging sample difficulty in the context of pruning. Concurrently, Xu et al. (2021) suggest the generation of robust subnetworks through Knowledge Distillation and Posttraining Quantization. Taking a different approach, Liang et al. (2021) strive to enhance model generalizability by extracting the super tickets, while Zheng et al. (2022) and Xi et al. (2022) seek to identify robust tickets. Despite recent advancements, achieving enhanced robustness alongside increased sparsity remains a challenge. This paper significantly promotes a better trade-off among accuracy, robustness, sparsity, and pruning cost."
        },
        {
            "heading": "3 Preliminary",
            "text": ""
        },
        {
            "heading": "3.1 Shortcut Learning and Mitigation",
            "text": "Recent studies provide evidence that language models are inclined to capitalize on inherent biases and spurious features present in datasets, using these as convenient predictive shortcuts (Niven and Kao, 2019; Du et al., 2021; McCoy et al., 2020). This tendency impedes the development of more advanced semantic understanding and reasoning capacity necessary for NLU tasks. Various preliminary studies have begun to address this bias issue, such as adversarial training and posterior regularization (Stacey et al., 2020; Chen et al., 2021). From a unique perspective, we let language models against adversarial attacks by mitigating this shortcut issue through weight averaging. This method will be elaborated further in Section 4.2."
        },
        {
            "heading": "3.2 Pruning with Hessian Matrix",
            "text": "Drawing inspiration from (LeCun et al., 1989; Hassibi et al., 1993), previous study has provided mathematical formulations for effectively eliminating a single weight from a layer and updating the remaining weights to correct the resulting error according to the information from Hessian Matrix (Frantar and Alistarh, 2022). The equations are presented below:\nwp = argmin wp w2p [H\u22121]pp wr\u2212 = wp\n[H\u22121]pp \u00b7 H\u22121:,p\n(1)\nwhere H is the Hessian Matrix, wp represents the single weight that will be pruned, while wr denotes the remaining weights that will be updated. The notation [H\u22121]pp refers to the pth diagonal entry of the inverse Hessian Matrix, and H\u22121:,p represents its pth column. However, the inversion of the Hessian Matrix requires updates at each weight removal, which is exceedingly costly. Frantar and Alistarh (2022) observes that Hessian values across different weight matrix rows are independent, as a single weight removal only impacts its respective row output. Accordingly, they simplify the calculation of Hessian Matrix H and leverage the Gaussian elimination technique to accelerate the update of H\u22121, as described mathematically below:\nH = XXT\nH\u22121\u2212p = (H \u22121 \u2212 1 [H\u22121]pp H\u22121:,p H \u22121 p,: )\u2212p\n(2)\nHere, \u2212p denotes the removal action of a single weight at index p. A more detailed explanation can\nbe found in the Appendix."
        },
        {
            "heading": "4 Methodology",
            "text": "This section proposes a pruning method for language models that can better balance accuracy, sparsity, robustness, and pruning cost. Figure 1 depicts the architecture of this method."
        },
        {
            "heading": "4.1 Rethink Robust Model Pruning",
            "text": "Given that the predominant challenge in robust pruning primarily centers on robustness and pruning cost, we mainly focus on these two aspects in this paper. To enhance the robustness, we explore the root cause of the poor performance of sparse language models under adversarial attacks. We note that adversarial samples are often crafted by replacing certain words in the sentence with semantically similar substitutes. Thus it is essential to ensure that the representation of the original words and their substitutes remain similar in the embedding space and feature space even after pruning. Based on the above observation, we propose to maintain a highly close alignment between the sparse and dense language models. In other words, robust pruning is supposed to seek sparse parameters W\u0302l that minimize the discrepancy between the outputs of dense and sparse layers. The problem can be formally expressed as follows:\nargminW\u0302 l EXl L(fl(Xl,Wl), fl(Xl, W\u0302l)) s.t. \u2225W\u0302l\u22250 \u2264 k (3)\nHere, each layer of language models is represented by a mathematical function fl(Wl, Xl), and Xl denotes inputs, k designates the total number of weights that remain non-zero after the pruning process. Predominantly, the Mean Squared Error (MSE) is usually employed to measure the pruning error of each layer. Therefore, the preceding problem can be further reformulated using the MSE, as expressed in the subsequent equation:\nargminW\u0302l ||WlXl \u2212 W\u0302lXl|| 2 s.t. \u2225W\u0302l\u22250 \u2264 k (4)\nTo reduce the pruning cost, we adopt a posttraining setting in our strategy. Specifically, we only utilize a small subset of data to calibrate the weights and generate sparse substitutes to replace them. In summary, our pruning method does not need a rigorous retraining process."
        },
        {
            "heading": "4.2 Weight Averaging for Robust Dense Model",
            "text": "We also realize that language models may rely on surface-level or spurious features in the data\ndifferent knowledge; 2 we then employ a greedy algorithm to only average the weights of models that contribute to the final performance. B: Second, 3 we apply our adaptive pruning method to generate robust and sparse language models in a layer-wise setting. Specifically, we optimize the 1 original independent pruning process of each layer to 2 an adaptive way. This requires subsequent layers to update the Hessian Matrix and the optimal dense weight according to the sparse outputs of preceding layers, thereby inheriting and correcting the accumulated error together.\nrather than capturing sophisticated semantic features. Thus, when sparse language models fail to defend against adversarial attacks, it becomes challenging to determine whether the failure stems from the pruning methods or inherent issues within the dense model. We circumvents this risk by constructing a robust and dense model before pruning.\nInspired by Croce et al. (2023) and Wortsman et al. (2022), we generate a robust language model via weight averaging. The key idea is to train multiple models with different hyperparameters and settings, allowing each model to capture distinct nuances of the data and generalize in diverse ways. By averaging their weights, we can create a robust model that benefits from collective knowledge. Specifically, we order these models in descending order based on the accuracy under attack. Then, we selectively average the weights that contribute to the final robustness. Finally, we obtain a robust and dense model as the foundation of subsequent operations. This approach ensures that any detected vulnerabilities in sparse language models result from the pruning process, eliminating the possibility of them arising from spurious features. More\ndetails can be found in Algorithm 3."
        },
        {
            "heading": "4.3 Ada-Pruning for Robust Sparse Model",
            "text": ""
        },
        {
            "heading": "4.3.1 Notation",
            "text": "To accurately replicate the dense model\u2019s behavior regarding embedding space and feature space of each layer, we use the method described in Section 3.2 as the backbone. However, its layer-wise setting, which treats each layer as an independent pruning problem, introduces limitations in realizing a globally optimal solution. To elaborate, let\u2019s consider a single layer as an example in the following sections. We\u2019ll use Xl, Wl, and Yl to represent the input, weight, and output of the layer, respectively, with the subscript l indicating lth layer. The use of a hat, as seen in X\u0302l, W\u0302l, or Y\u0302l, represents the input, weight, or output within a sparse context."
        },
        {
            "heading": "4.3.2 Adaptive Hessian Matrix",
            "text": "After completing the pruning of the lth layer, a certain amount of error stemming from the sparse matrix operation inevitably arises. No matter how minor this error might be, it\u2019s important to realize that the output of this layer, denoted as Y\u0302l, influ-\nAlgorithm 1 Prune linear layers {l1..ln} of BERT with target sparsity s and calibration data X Require: Collect original X,W, Y for l 1: procedure LAYERWISE PRUNING({l1..ln}) 2: for i\u2190 1 to n do 3: Wi, Xi, Yi\u2190 li 4: - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 5: # Adaptive update 6: H\u22121i \u2190 (XiX T i ) \u22121\n7: if i \u0338= 0 then 8: Wi \u2190 H\u22121i X T i Yi\n9: end if 10: - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 11: # Pruning with Hessian Matrix 12: din \u2190 input dimension 13: k \u2190 int (din \u00b7 s) 14: for j \u2190 1 to k do \u25b7 parallel in rows 15: p\u2190 argminp\u2208din 1[H\u22121i ]pp\n\u00b7 [Wi]2p 16: Wi \u2190Wi \u2212 [Hi]\u22121:,p 1[H\u22121i ]pp \u00b7 [Wi]p 17: tmp\u2190 [Hi]\u22121:,p [Hi]\u22121p,: 18: H\u22121i \u2190 H\n\u22121 i \u2212 1[H\u22121i ]pp tmp\n19: Wi \u2190Wi remove [Wi]p 20: end for 21: - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 22: # Adaptive update 23: Yi \u2190WiXi 24: Xi+1 \u2190 post-process(Yi) 25: end for 26: return {Wi..Wn} 27: end procedure\nences the input of the subsequent layer, denoted as X\u0302l+1. As a result, the initial Hessian Matrix for the (l + 1)th layer, defined as Hl+1 = Xl+1XTl+1, becomes outdated. Thus it\u2019s crucial to recalculate the Hessian Matrix to obtain more precise pruningdependent information. We suggest adaptively updating the Hessian Matrix for the subsequent layer after pruning the preceding layers."
        },
        {
            "heading": "4.3.3 Adaptive Dense Weight",
            "text": "We also note that the loss generated by removing a single weight depends on the current weight Wl from corresponding layer, as derived from Equation 1. However, an inevitable fact is that the original dense weight Wl is not optimal for the expected dense output Yl after pruning the preceding layers (0\u0302th . . . \u02c6(l \u2212 1)th). Given that the input Xl has been altered to X\u0302l due to the accumulated error, it would be suboptimal to continue using the original weight Wl to calculate the pruning loss for the current layer. To be more clear, the result of X\u0302lWl could substantially deviate from the original output Yl. This is incompatible with our goal of producing an output Y\u0302l identical to the original Yl in the pruning process. Thus, it\u2019s essential to update the dense weight so that X\u0302lW\u0304l can approximates the original\noutput Yl more closely. Here, W\u0304l denotes the updated dense weight, and we design the following equations to derive W\u0304l:\nW\u0304l = (X\u0302 T l X\u0302l) \u22121X\u0302Tl Yl (5)\nwhere T represents the transpose operation, and \u22121 denotes the inverse operation. To ensure that X\u0302Tl X\u0302l is invertible, we also introduce a regularization term, such as 1e \u2212 4, to the diagonal entries of the matrix. Finally, we can compute the pruning loss more accurately with the updated weight W\u0304l.\nWe also calibrate the optimal weights for nonpruned layers (such as the pooler layer and classification layer in BERT) with Equation 5, aligning the dense layers\u2019 output with the altered input. Algorithm 1 provides detailed steps for the code implementation, offering a comprehensive overview of our methodology. We also provide a comprehensive analysis of the computational complexity of our method in the Appendix."
        },
        {
            "heading": "5 Experiments",
            "text": "We first compare our method against several baseline methods, assessing accuracy, robustness, sparsity, and cost. Then, an ablation study is performed to elucidate the contributions of each part in our method. Finally, we augment our core findings with additional experiments and analyses to further illuminate our method."
        },
        {
            "heading": "5.1 Baselines and Datasets",
            "text": "Consistent with the previous works (Devlin et al., 2018; Du et al., 2023; Xu et al., 2021; Zheng et al., 2022; Xi et al., 2022), BERTbase serves as the foundational model for all our experiments. We compare our approach with various baselines including:RobustT (Zheng et al., 2022), which optimizes the pruning mask and input perturbation simultaneously for robust tickets; Bag-of-Ticks (Xu et al., 2021), which improves sparse model robustness via Knowledge Distillation and Post-Training Quantization; RMC (Du et al., 2023), a technique preventing sparse language models from overfitting on easy samples using sample difficulty; SuperTicket (Liang et al., 2021), which identifies a super mask during pruning to reduce variance while preserving bias. Our evaluation primarily involves three text classification datasets: Internet Movie Database (IMDB, Maas et al. 2011), AG News Corpus (AGNEWS, Zhang et al. 2016), and Stanford Sentiment Treebank for binary classification (SST-2, Socher et al. 2013)."
        },
        {
            "heading": "5.2 Robustness Evaluation",
            "text": "We assess our model\u2019s effectiveness against adversarial attacks using the TextFooler, which substitutes crucial words in sentences with semantically similar synonyms (Jin et al., 2020). Following previous works (Zheng et al., 2022; Xi et al., 2022), our evaluations utilize key metrics like Clean Accuracy Acc% (accuracy on clean test data), Accuracy Under Attack Aua% (accuracy when subjected to adversarial attacks), and Attack Success Rate Asr% (ratio of successful text perturbations to total attempts). A robust method is expected to show higher clean accuracy and accuracy under attack coupled with a lower attack success rate. We also evaluate more attack methods in the Appendix."
        },
        {
            "heading": "5.3 Implementation Details",
            "text": "To begin with, we employ the technique mentioned in Section 4.2 to generate a robust language model for each dataset. Subsequently, we use our method to prune these robust language models with a small calibration dataset. All experimental results are the average of five trials, each initiated with different\nseeds. Furthermore, we assess the performance under three different levels of sparsity: 30%, 50%, and 87.5%. Additional implementation details can be found in Appendix."
        },
        {
            "heading": "5.4 Main Result on Robustness Evaluation",
            "text": "Table 1 provides a comprehensive comparison of various robust pruning methods, evaluated across three distinct datasets: SST2, AGNEWS, and IMDB, and under varying degrees of model sparsity. Key observations can be made as follows: 1) Our strategy even enhances the robustness of language models after pruning. We believe this enhancement stems from the regularization effect of sparse architecture. 2) Our strategy distinguishes itself by consistently surpassing other methods in the Aua% and Asr%s, regardless of the dataset or the level of sparsity. These results imply that our strategy effectively maintains robustness during the pruning of language models. 3) Impressively, our method achieves higher robustness even with fewer parameters compared to several other approaches, which further underscores the effectiveness of our robust pruning method. 4) Although the Acc% of\nour method is generally lower than other baselines at lower sparsity levels, the improvement of robustness (reflected in Aua% and Asr%) far outweighs the degree of accuracy degradation. 5) At higher levels of sparsity, our method outperforms other baselines across all metrics. 6) Our method does not require model retraining, confirming that our approach offers a better trade-off between accuracy, robustness, sparsity, and pruning cost.\nBeyond Bertbase, our methodology was also extended to Bertlarge, a model encompassing 330M parameters. The resulting performance, as presented in Table 3, reaffirms the superiority of our\nmethod when compared to the baselines. Moreover, we explore the effectiveness of our methods within a structured pruning context, and once again, our approach outperforms the state-of-the-art method: EarlyRobust (Xi et al., 2022). More details can be found in Appendix."
        },
        {
            "heading": "5.5 Ablation Study",
            "text": "To elucidate the contributions of each part of our approach, we conduct an ablation study with the following settings:We replace our pruning technique with methods known as LTH and IMP (Frankle et al., 2020; Frankle and Carbin, 2018), and supple-\nment them with the additional adversarial training method FreeLB (Zhu et al., 2019). The results are presented in Table 2. From the results, we can make the following key observations: 1) Sparse language models generated by traditional pruning methods performs even worse than the vanilla finetuned dense model. This highlights the challenges associated with robust pruning. 2) Our approach consistently generates more robust sparse language models than conventional pruning methods, even supplemented with adversarial training methods. 3) We conjecture that the limited effect of adversarial training here stems from the discrete nature of word tokens and the substantial loss of pre-trained knowledge during pruning."
        },
        {
            "heading": "5.6 Discussion",
            "text": "In this section, we design additional experiments to illustrate our robust pruning method further."
        },
        {
            "heading": "5.6.1 Pretrained Knowledge Detection",
            "text": "To demonstrate the effectiveness of our robust pruning mechanism in preserving pre-trained knowledge, we\u2019ve chosen adversarial samples that are effectively defended by our method but not by others. We then visualize the attention scores of them in Figure 2. Our method demonstrates superior performance, as evidenced by more reasonable attention scores that align more closely with those from the robust and dense model. In addition, we visualize the distance of sentence representation from sparse language models and their dense counterparts in the feature space. As depicted in Table 4 and Figure 5, our method results in smaller distances between the dense and sparse representations. These findings indicate the superior ability of our robust pruning method to preserve semantic knowledge and maintain cognizance. In other words, our method outperforms others in maintaining robustness during pruning."
        },
        {
            "heading": "5.6.2 Impact of Calibration Data",
            "text": "The calibration data is crucial for our methodology because it directly affects the computation of the Hessian Matrix. As outlined in Algorithm 1, the Hessian Matrix can be derived from H = XTX . To further explore the impact of the number of data points, we designed experiments that gradually increased the number of data points used in our strategy. The results of these experiments are detailed in Figure 3. Our observations indicate that as the number of used data points increases, the robustness and accuracy of the sparse language modes increase, but only up to a certain threshold. We hypothesize that the model can initially retain\nmore general knowledge as data points increase. However, once a threshold is crossed where the new data cannot provide additional information for general features, adding more data points from a similar distribution no longer contributes to model robustness and accuracy."
        },
        {
            "heading": "5.6.3 Impact of Sparsity",
            "text": "As illustrated in Figure 4, we explore the robustness and accuracy of our sparse language models across a range of sparsity levels. In a departure from previous studies Zheng et al. (2022), our observations indicate that as sparsity increases, robustness decreases with a similar pace like accuracy. This trend suggests that the impact of increasing sparsity on model robustness might be less severe than previously assumed. This disparate pattern may stem from the post-training nature of our method. Furthermore, our observations regarding the trend in robustness align with the findings of previous studies by Zheng et al. (2022) and Liang et al. (2021). We note that the robustness of our sparse language models initially improves as sparsity escalates up to a certain threshold. After crossing this threshold, the robustness begins to decline. However, it sustains a level of robustness that is higher than the peak value observed in other models and does not collapse even with 10x compression. This finding further highlights the outstanding performance of our method in robust pruning."
        },
        {
            "heading": "6 Conclusion",
            "text": "In this paper, we investigate the application of robust pruning methods for language models. We propose an adaptive pruning method and place a special emphasis on replicating the embedding and feature space of dense models to preserve as much pre-trained knowledge as possible. The effectiveness of this approach is confirmed through a series\nof experiments conducted across various tasks."
        },
        {
            "heading": "Limitations",
            "text": "This work introduces a post-training method that can robustly prune the language models without model retraining. Despite bypassing the rigorous retraining process, the computational cost of our method remains significant due to the calculation of the Hessian Matrix and its inverse. Consequently, this approach may not be feasible for language models comprised of billions of parameters. As a next step, we aim to refine our technique to devise a more efficient strategy to replicate the feature space and embedding space of language models"
        },
        {
            "heading": "Acknowledgements",
            "text": "The authors wish to thank the anonymous reviewers for their helpful comments."
        },
        {
            "heading": "Ethics Statement",
            "text": "This work complies with the ACL Ethics Policy and we have carried out our research following the highest ethical standards. In our work on developing a new pruning strategy to enhance robustness in language models, we carefully considered the broader implications and ethical dimensions of this innovation.\nWhile our research primarily concerns the improvement of model accuracy, sparsity, and robustness, we acknowledge that the use of these enhanced models can potentially be dual-use, which means they can be applied in both beneficial and harmful ways. An improved model can contribute positively by enhancing various NLP applications such as text summarization, machine translation, and sentiment analysis, potentially increasing efficiency and the overall quality of output. Fur-\nthermore, these advancements could contribute to reducing the computational resources required for training and using large language models, which aligns with efforts to reduce the environmental impact of machine learning.\nHowever, the increased robustness of models against adversarial attacks could also be used maliciously if the technology falls into the wrong hands. Bad actors could potentially exploit robust models for the generation of disinformation or manipulation of public sentiment, for instance. Furthermore, although our technique aims to faithfully replicate the feature space of dense models, bias present in the original training data could be preserved in the pruned models. Consequently, decisions made based on the output of these models could perpetuate these biases.\nWe encourage the use of our findings and methods for applications that promote the public good and contribute to human welfare. Further, we recommend that researchers and practitioners using this technique take into account potential biases in their training data and consider strategies for minimizing their impact. In the future, we hope to conduct more research on mitigating bias and other ethical issues associated with our pruning strategy. It is our belief that technology should be developed and used in a way that is transparent, fair, and beneficial to all."
        },
        {
            "heading": "A Appendix-A",
            "text": ""
        },
        {
            "heading": "A.1 Pruning with Hessian Matrix",
            "text": "As described in Section 3.2, we prune each layer of language models in a layer-wise setting. It involves an iterative step that removes a single weight for each step and updates the remaining weights until the desired sparsity level is attained. While this approach yields a locally optimal solution, it involves a computationally expensive step: calculating the Hessian matrix at each iteration. It is important to note that storing the information for a Hessian Matrix, denoted as H , requires d\u00d7d memory, and updating it has a computational complexity of O(d4), where d = drow \u00b7 dcol."
        },
        {
            "heading": "A.2 Accelerated Pruning with Hessian Matrix",
            "text": "Previous research highlights that the Hessian values across different rows of the weight matrix are independent. This is because the removal of a single weight in each row of the matrix only affects its corresponding row value. Consequently, we can simplify the objective function with\u2211drow\ni=1\u2225Wi,:X \u2212 W\u0302i,:X\u222522, and a separate Hessian Matrix of appropriate size (dcol \u00d7 dcol) for each row is sufficient to locate the optimal weight for removal. Additionally, since the output Y = WX of the dense layer remains fixed, and the objective function for each row takes the standard form of least squares, its Hessian Matrix can be calculated by H = 2XXT (Frantar and Alistarh, 2022).\nAs the Hessian Matrix H is no longer dependent on the weight, we only need to compute H once. After each pruning step, the Hessian Matrix HM (M means the operation of removing or masking one single weight) can be obtained by masking the value at the corresponding location. However, when it comes to H\u22121, the aforementioned trick cannot be applied as (H\u22121)M \u0338= (HM )\u22121, making the computation still expensive. Frantar and Alistarh (2022) uses the Gaussian elimination technique for a more efficient update of H\u22121. A mathematical exposition of this technique is provided below:\nH\u22121\u2212p = (H \u22121 \u2212 1 [H\u22121]pp H\u22121:,p H \u22121 p,: )\u2212p (6)\nwhere \u2212p meas remove single weight at index p. For more comprehensive details, please refer to the work of Frantar and Alistarh (2022)."
        },
        {
            "heading": "B Appendix-B",
            "text": ""
        },
        {
            "heading": "B.1 Efficiency Analysis of Hessian Matrix",
            "text": "We recognize the importance of addressing the efficiency concern related to Hessian Matrix calculation. However, grasping the intricate balance between computational complexities and their broader implications is crucial. To provide clarity, we offer an in-depth analysis of computational complexities from both micro and macro viewpoints, contrasting it with approaches that necessitate model retraining."
        },
        {
            "heading": "B.2 Micro Perspective",
            "text": "When considering models like Bertbase and Bertlarge, the computational requirements for the Hessian Matrix of one layer do not exceed that of model retraining in most cases. To clarify it, we analyze the complexity of our method step by step based on the Algorithm 2.\nAlgorithm 2 Prune a linear layer l of BERT with target sparsity s and calibration data X\n1: Input: Collect original X , W , Y for l. 2: procedure PRUNING(l) 3: Set W , X , Y \u2190 l\nAdaptive Update 1: 4: Calculate H\u22121\u2190 (XXT )\u22121 5: Set W \u2190 H\u22121XTY\nPruning with Hessian Matrix: 6: Set din\u2190 input dimension. 7: Set k\u2190 int(din \u00b7 s). 8: for j = 1 to k (parallel in rows) do 9: Set p\u2190 argminp\u2208din 1[H\u22121]pp \u00b7 [W ] 2 p. 10: Set W \u2190W \u2212 [H]\u22121:,p 1[H\u22121]pp \u00b7 [W ]p. 11: Set A\u2190 [H]\u22121:,p 12: Set B \u2190 [H]\u22121p,: 13: Set H\u22121\u2190 H\u22121 \u2212 1\n[H\u22121]ppAB\n14: Remove [W ]p from W 15: end for\nAdaptive Update 2: 16: Set Y \u2190WX . 17: Update X of next layer with post-\nprocess(Y ) 18: end procedure\nNotations: To facilitate the understanding, we first introduce the notations essential for the complexity analysis. The sparsity ratio, a value lying between 0 and 1, is denoted by s. The input dimension of the linear layer is represented by din, and the output\ndimension, aligning with the weight matrix\u2019s other dimension, is symbolized by dout. We use d = din \u00d7 dout to illustrate the comprehensive size of the weight matrix. The batch size and the sequence length are, respectively, given by n and seq.\nAdaptive Update (1): In this phase, the matrix multiplication XXT plays a pivotal role. Given the dimensions of X as n \u00d7 seq, din and that of XT as din, seq\u00d7 n, the resulting matrix has a shape of din \u00d7 din. This multiplication alone possesses a complexity of O(n\u00d7 seq\u00d7 d2in). Additionally, matrix inversion is another vital step with a complexity of O(d3in). The computation of H \u22121 i X T i Yi further contributes to the complexity, having a magnitude of O(n\u00d7 seq\u00d7 din \u00d7 dout).\nPruning with the Hessian Matrix: In this context, the outer loop spans dout iterations. Within each row of W , an inner loop determined by k = int(din \u00d7 s) is executed. This loop comprises various operations with O(d2in). Summing up, the inner loop complexity is O(k\u00d7d2in). Consequently, the combined complexity for the pruning phase is O(din \u00d7 s \u00d7 d2in \u00d7 dout), simplifying to O(d3in \u00d7 s\u00d7 dout).\nAdaptive Update (2): The matrix multiplication Y = WX dominates with a complexity of O(n\u00d7 seq \u00d7 din \u00d7 dout). Summing complexities for a single layer yields O(2n\u00d7 seq\u00d7 din\u00d7 dout +n\u00d7 seq\u00d7d2in+2d3in+d3in\u00d7s\u00d7dout), with the dominant terms being O(d3in \u00d7 dout). Thus, pruning a layer has a complexity of O(d3in \u00d7 dout), which is also proved by Frantar and Alistarh (2022).\nKey observations: A pivotal observation is that this complexity remains uninfluenced by the batch size n because calibration data keeps n restricted to a constant fall in [128, 1024]. The cubic relationship with din is the primary driver behind the complexity, and for larger din, this can escalate\nsubstantially."
        },
        {
            "heading": "B.3 Comparison with Re-Training Method",
            "text": "In contrast, when training a single layer using SGD, the complexity is approximately O(n\u00d7seq\u00d7din\u00d7 dout). This complexity scales linearly with the batch size n, which can increase markedly with large datasets and the number of training epochs. Although the complexity of the pruning operation remains consistent regardless of n, the training complexity escalates, posing computational challenges for extensive datasets, prolonged sequences, and increased training epochs. We also dive deeper into the comparative insights.\nBatch Size: Our pruning method capitalizes on calibration data, thus constricting n to moderate values, notably between 128 to 1024. This sharply diverges from the conventional training paradigm where n can inflate significantly due to extensive datasets and number of training epochs, thereby magnifying its computational requisites.\nDimensionality Dependency: The intrinsic complexity of our pruning algorithm reveals a cubic dependency on din. This can render it computationally onerous, especially for layers endowed with an extensive din. Conversely, traditional training exhibits a linear correlation with both din and dout.\nIn summary, the computational demands of our pruning method, particularly for layers with a large din, are unquestionably stringent. However, it\u2019s important to recognize the significant computational burden introduced by traditional training, mainly because of its responsiveness to large dataset sizes. Understanding this balance and trade-off is crucial when comparing the effectiveness and suitability of our pruning approach to traditional retraining."
        },
        {
            "heading": "B.4 Macro Perspective",
            "text": "Predicable Processing Time and Promised Output: Notably, from a broader view, while our approach introduces a dependency for each layer and potentially increases processing times, the number of layers in common language models is limited. This suggests that we can accurately predict the time needed to complete the pruning process, and expect positive results in return.\nLayer-by-Layer Computation for Resource Efficiency: While the sum of Hessian Matrix computations of the entire language model is time-intensive, our approach uniquely addresses this by employing a layer-by-layer resolution strategy. This methodology means there\u2019s no necessity to simultaneously load the entire model into the memory of computational resources. Consequently, from a memory allocation standpoint, our pruning with the Hessian Matrix can be viewed as a resource-saving measure.\nEfficient Post-training Pruning: A post-training pruning strategy is at the heart of our methodology. Unlike many other approaches that might require recurrent training sessions or exhaustive reiterations, ours stands out in its ability to save significant resources by strategically avoiding these\nprocesses.\nComputational Commitment: While it\u2019s acknowledged that pruning with the Hessian Matrix does possess computational time costs, it\u2019s paramount to understand our larger vision. The ultimate objective isn\u2019t merely to save time but to sculpt a model characterized by three pillars: sparsity, robustness, and high performance. Such a model offers considerable advantages in real-world scenarios. Thus, the computational expenses encountered in the training phase can be viewed less as costs and more as strategic investments."
        },
        {
            "heading": "C Appendix-C",
            "text": ""
        },
        {
            "heading": "C.1 More Adversarial Attacks",
            "text": "To demonstrate the superiority of our method, we have incorporated further experiments targeting two more recognized adversarial attacks: BERTAttack and TextBugger (Li et al., 2020b, 2018). BERT-Attack, powered by BERT, guarantees fluency and retains semantics in its adversarial outputs. Conversely, TextBugger integrates both character and word-level perturbations to yield adversarial instances, thereby introducing a new set of challenges for our defense mechanism. We use stateof-the-art methods (RobustT and EarlyRobust) as baselines and describe the results in Table 6 (Zheng\net al., 2022; Xi et al., 2022). Our approach consistently demonstrated superiority in the robustness of sparse language models across various sparsity levels and datasets."
        },
        {
            "heading": "C.2 More Pruning Baseline",
            "text": "As recommended by the reviewer, we have included Movement Pruning (Sanh et al., 2020) as an additional baseline in our experiments. Our original selection of baselines was grounded on their capacity to simultaneously address accuracy, sparsity, robustness, and pruning cost. It should be noted that Movement Pruning predominantly emphasizes accuracy and sparsity.\nNevertheless, to offer a complete perspective, we have included Movement Pruning in our experimental evaluation. The comparative results are presented in Table 7. It is evident that, while our method may trail slightly in terms of clean accuracy, it significantly outperforms Movement Pruning under adversarial conditions, highlighting the robustness of our approach."
        },
        {
            "heading": "D Appendix-D",
            "text": ""
        },
        {
            "heading": "D.1 More Implementation Details",
            "text": "We utilize various hyperparameters and settings to fine-tune multiple downstream models for each dataset. The hyperparameters and settings employed are presented in Table 8. Subsequently, we apply the technique of weight average in a greedy manner to derive robust and dense models. The detailed procedure is outlined step-by-step in Algorithm 3.\nAlgorithm 3 Greedy Weight Averaging 1: procedure GREEDYWA({h1, . . . , hk}) 2: {\u03b81, . . . , \u03b8k} \u2190 {h1, . . . , hk} 3: {m1, . . . ,mk} \u2190 {\u03b81, . . . , \u03b8k} 4: Sort({\u03b81, . . . , \u03b8k}) with {m1, . . . ,mk} \u2193 5: ingredients\u2190 \u2205 6: for i = 1 to k do 7: if Eval(average(ingredients \u222a {\u03b8i})) \u2265 8: Eval(average(ingredients)) then 9: ingredients\u2190 ingredients \u222a {\u03b8i} 10: end if 11: end for 12: return average(ingredients) 13: end procedure\nWe adopt Textattack (Morris et al., 2020) to implement the method of adversarial attacks. Moreover, Aua% and Suc% are evaluated on all 872 test examples for SST-2, 500 randomly selected test samples for IMDB and AG NEWS.\nThe number of calibration data in our main experiments ranges from 256 to 1024 sentences. During pruning, we conduct our experiments on a server with a single NVIDIA 3090 GPU. Due to the layer-wise setting, we do not need to occupy substantial GPU memory, and our adaptive rule enables us to obtain an end-to-end rectification effect similar to SGD optimization.\nD.2 Impact of Structured Pruning\nDrawing inspiration from the work by Xi et al. (2022), we also investigate the impact of structured pruning in our strategy. In particular, we evaluate our method\u2019s performance under N:M structured patterns and summarize the results in Table 4. We made several key observations from these experiments: 1) our method consistently produces better robust pruning results than other robust pruning methods in the context of structured pruning. 2) As proven by Xi et al. (2022), structured pruning enhances the robustness of subnetworks in comparison to unstructured pruning. Our experiments confirm the positive impact of structured patterns in pruning, solidifying the effectiveness of our robust pruning method."
        },
        {
            "heading": "E Appendix-E",
            "text": ""
        },
        {
            "heading": "E.1 Model Pruning",
            "text": "Pruning aims to eliminate redundant elements in neural networks, traditionally targeting elements of the smallest magnitude, which includes weights, output sensitivity, gradients, and Hessian matrices of training loss, among others. Pruning pre-trained language models like BERT has been an active field of research. Prasanna et al. (2020) demonstrated that unstructured pruning yields more sparse and accurate models. Pruning at the pre-training stage has been favored by researchers like Gordon et al. (2020) and Chen et al. (2021), due to its efficiency and effective knowledge transfer to downstream tasks. Sanh et al. (2020) adds penalty terms to the loss function to eliminate redundant weights. Frantar and Alistarh (2022) introduce an effective post-training pruning method, which is the first approach that prunes a language model in a one-shot manner without significant degradation in accuracy. However, these studies neglect robustness, focusing mainly on the accuracy-sparsity trade-off. Recent work has begun to note the issue of robustness for sparse language models, but the challenge of enhancing robustness with increased sparsity per-\nTable 7: Comparison between our method and Movement Pruning under various attacks and sparsity levels.\nMethod Dataset Attack Sparsity Accuracy Accuracy under attack Ours SST2 TextFooler 2x 88.31% 43.12% Movement Pruning SST2 TextFooler 2x 90.6% 14.85% Ours SST2 TextFooler 4x 86.93% 40.15% Movement Pruning SST2 TextFooler 4x 90.5% 8.27% Ours SST2 TextFooler 8x 85.6% 37.63% Movement Pruning SST2 TextFooler 8x 90.0% 9.14% Ours SST2 TextBugger 2x 88.31% 50.34% Movement Pruning SST2 TextBugger 2x 90.6% 24.85% Ours SST2 TextBugger 4x 86.93% 49.08% Movement Pruning SST2 TextBugger 4x 90.5% 21.35% Ours SST2 TextBugger 8x 85.6% 48.85% Movement Pruning SST2 TextBugger 8x 90.0% 15.13%\nsists (Zheng et al., 2022; Du et al., 2023; Xu et al., 2021; Liang et al., 2021; Xi et al., 2022), and the underlying causes of low robustness in language models remain elusive."
        },
        {
            "heading": "E.2 Post-Training Pruning",
            "text": "Pruning methods can be categorized into PostTraining Pruning and In-Training Pruning according to if the pruning methods need extra model retraining. In the former, we are given a trained but uncompressed model, together with a small amount of calibration data. we must produce an accurate compressed model in one shot, i.e., a single compression step, without retraining and with limited computational costs. This is motivated by practical scenarios such as the large language models, which are hard to train or even finetune because of the complicated training process. In this paper, our method is a Post-Training pruning method."
        },
        {
            "heading": "E.3 Layer-wise Pruning",
            "text": "Layerwise Pruning is an important approach to optimizing language models, offering a distinct methodology compared to end-to-end pruning. Unlike end-to-end pruning, which simultaneously evaluates and prunes the entire model as a whole, layerwise pruning tackles each layer of the neural network individually. This means pruning decisions are based on a layer-specific analysis, often using a metric like the magnitude of the weights to determine which parameters within that layer are least significant and can be removed without substantially impacting the layer\u2019s output. By selectively reducing the number of parameters in each layer, layerwise pruning can effectively decrease the computational requirements and memory footprint of language models while maintaining their accuracy. The layerwise approach offers an advantage in that it provides a more granular level of control over the pruning process, which can be beneficial in preserving model performance while achieving efficiency gains."
        }
    ],
    "title": "Towards Robust Pruning: An Adaptive Knowledge-Retention Pruning Strategy for Language Models",
    "year": 2023
}