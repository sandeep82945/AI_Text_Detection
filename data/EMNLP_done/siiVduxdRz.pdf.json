{
    "abstractText": "Incorporating language-specific (LS) modules or Mixture-of-Experts (MoE) are proven methods to boost performance in multilingual model performance, but the scalability of these approaches to hundreds of languages or experts tends to be hard to manage. We present Language-specific Matrix Synthesis (LMS), a novel method that addresses the issue. LMS utilizes parameter-efficient and lightweight modules, reducing the number of parameters while outperforming existing methods, e.g., +1.73 BLEU over Switch Transformer on OPUS-100 multilingual translation. Additionally, we introduce Fuse Distillation (FD) to condense multilingual knowledge from multiple LS modules into a single shared module, improving model inference and storage efficiency. Our approach demonstrates superior scalability and performance compared to state-of-the-art methods.1",
    "authors": [
        {
            "affiliations": [],
            "name": "Haoran Xu"
        },
        {
            "affiliations": [],
            "name": "Weiting Tan"
        },
        {
            "affiliations": [],
            "name": "Shuyue Stella Li"
        },
        {
            "affiliations": [],
            "name": "Yunmo Chen"
        },
        {
            "affiliations": [],
            "name": "Benjamin Van Durme"
        },
        {
            "affiliations": [],
            "name": "Philipp Koehn"
        },
        {
            "affiliations": [],
            "name": "Kenton Murray"
        }
    ],
    "id": "SP:fe9fbf76680992e022a01a1a7f7f2a275a3def99",
    "references": [
        {
            "authors": [
                "Armen Aghajanyan",
                "Sonal Gupta",
                "Luke Zettlemoyer."
            ],
            "title": "Intrinsic dimensionality explains the effectiveness of language model finetuning",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and",
            "year": 2021
        },
        {
            "authors": [
                "Roee Aharoni",
                "Melvin Johnson",
                "Orhan Firat."
            ],
            "title": "Massively multilingual neural machine translation",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language",
            "year": 2019
        },
        {
            "authors": [
                "Mikel Artetxe",
                "Sebastian Ruder",
                "Dani Yogatama."
            ],
            "title": "On the Cross-lingual Transferability of Monolingual Representations",
            "venue": "Proceedings of ACL 2020.",
            "year": 2020
        },
        {
            "authors": [
                "Ankur Bapna",
                "Orhan Firat."
            ],
            "title": "Simple, scalable adaptation for neural machine translation",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th",
            "year": 2019
        },
        {
            "authors": [
                "Christos Baziotis",
                "Mikel Artetxe",
                "James Cross",
                "Shruti Bhosale."
            ],
            "title": "Multilingual machine translation with hyper-adapters",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 1170\u20131185,",
            "year": 2022
        },
        {
            "authors": [
                "Alexandra Chronopoulou",
                "Dario Stojanovski",
                "Alexander Fraser."
            ],
            "title": "Language-family adapters for low-resource multilingual neural machine translation",
            "venue": "Proceedings of the The Sixth Workshop on Technologies for Machine Translation",
            "year": 2023
        },
        {
            "authors": [
                "Alexis Conneau",
                "Kartikay Khandelwal",
                "Naman Goyal",
                "Vishrav Chaudhary",
                "Guillaume Wenzek",
                "Francisco Guzm\u00e1n",
                "Edouard Grave",
                "Myle Ott",
                "Luke Zettlemoyer",
                "Veselin Stoyanov"
            ],
            "title": "Unsupervised cross-lingual representation learning",
            "year": 2020
        },
        {
            "authors": [
                "Carlos Escolano",
                "Marta R. Costa-juss\u00e0",
                "Jos\u00e9 A.R. Fonollosa",
                "Mikel Artetxe."
            ],
            "title": "Multilingual machine translation: Closing the gap between shared and language-specific encoder-decoders",
            "venue": "Proceedings of the 16th Conference of the European",
            "year": 2021
        },
        {
            "authors": [
                "Angela Fan",
                "Shruti Bhosale",
                "Holger Schwenk",
                "Zhiyi Ma",
                "Ahmed El-Kishky",
                "Siddharth Goyal",
                "Mandeep Baines",
                "Onur Celebi",
                "Guillaume Wenzek",
                "Vishrav Chaudhary"
            ],
            "title": "Beyond english-centric multilingual machine translation",
            "year": 2020
        },
        {
            "authors": [
                "Angela Fan",
                "Shruti Bhosale",
                "Holger Schwenk",
                "Zhiyi Ma",
                "Ahmed El-Kishky",
                "Siddharth Goyal",
                "Mandeep Baines",
                "Onur Celebi",
                "Guillaume Wenzek",
                "Vishrav Chaudhary"
            ],
            "title": "Beyond english-centric multilingual machine translation",
            "year": 2021
        },
        {
            "authors": [
                "William Fedus",
                "Barret Zoph",
                "Noam Shazeer"
            ],
            "title": "Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity",
            "year": 2021
        },
        {
            "authors": [
                "Ze-Feng Gao",
                "Peiyu Liu",
                "Wayne Xin Zhao",
                "Zhong-Yi Lu",
                "Ji-Rong Wen."
            ],
            "title": "Parameter-efficient mixture-of-experts architecture for pre-trained language models",
            "venue": "arXiv preprint arXiv:2203.01104.",
            "year": 2022
        },
        {
            "authors": [
                "Jordan Hoffmann",
                "Sebastian Borgeaud",
                "Arthur Mensch",
                "Elena Buchatskaya",
                "Trevor Cai",
                "Eliza Rutherford",
                "Diego de Las Casas",
                "Lisa Anne Hendricks",
                "Johannes Welbl",
                "Aidan Clark"
            ],
            "title": "An empirical analysis of compute-optimal large language model",
            "year": 2022
        },
        {
            "authors": [
                "Edward J Hu",
                "Phillip Wallis",
                "Zeyuan Allen-Zhu",
                "Yuanzhi Li",
                "Shean Wang",
                "Lu Wang",
                "Weizhu Chen"
            ],
            "title": "Lora: Low-rank adaptation of large language models",
            "venue": "In International Conference on Learning Representations",
            "year": 2021
        },
        {
            "authors": [
                "Haoming Jiang",
                "Pengcheng He",
                "Weizhu Chen",
                "Xiaodong Liu",
                "Jianfeng Gao",
                "Tuo Zhao."
            ],
            "title": "SMART: Robust and efficient fine-tuning for pretrained natural language models through principled regularized optimization",
            "venue": "Proceedings of the 58th",
            "year": 2020
        },
        {
            "authors": [
                "Diederik P Kingma",
                "Jimmy Ba."
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "arXiv preprint arXiv:1412.6980.",
            "year": 2014
        },
        {
            "authors": [
                "Taku Kudo",
                "John Richardson."
            ],
            "title": "SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System",
            "year": 2018
        },
        {
            "authors": [
                "Sneha Kudugunta",
                "Yanping Huang",
                "Ankur Bapna",
                "Maxim Krikun",
                "Dmitry Lepikhin",
                "Minh-Thang Luong",
                "Orhan Firat."
            ],
            "title": "Beyond distillation: Task-level mixture-of-experts for efficient inference",
            "venue": "Findings of the Association for Computational",
            "year": 2021
        },
        {
            "authors": [
                "Yoohwan Kwon",
                "Soo-Whan Chung."
            ],
            "title": "Mole: Mixture of language experts for multi-lingual automatic speech recognition",
            "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 1\u20135. IEEE.",
            "year": 2023
        },
        {
            "authors": [
                "Dmitry Lepikhin",
                "HyoukJoong Lee",
                "Yuanzhong Xu",
                "Dehao Chen",
                "Orhan Firat",
                "Yanping Huang",
                "Maxim Krikun",
                "Noam Shazeer",
                "Zhifeng Chen"
            ],
            "title": "GS}hard: Scaling giant models with conditional computation and automatic",
            "year": 2021
        },
        {
            "authors": [
                "Brian Lester",
                "Rami Al-Rfou",
                "Noah Constant."
            ],
            "title": "The power of scale for parameter-efficient prompt tuning",
            "venue": "arXiv preprint arXiv:2104.08691.",
            "year": 2021
        },
        {
            "authors": [
                "Xiang Lisa Li",
                "Percy Liang."
            ],
            "title": "Prefix-tuning: Optimizing continuous prompts for generation",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on",
            "year": 2021
        },
        {
            "authors": [
                "Zehui Lin",
                "Liwei Wu",
                "Mingxuan Wang",
                "Lei Li."
            ],
            "title": "Learning language specific sub-network for multilingual machine translation",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International",
            "year": 2021
        },
        {
            "authors": [
                "Marta R NLLB Team",
                "Costa-juss\u00e0",
                "James Cross",
                "Onur \u00c7elebi",
                "Maha Elbayad",
                "Kenneth Heafield",
                "Kevin Heffernan",
                "Elahe Kalbassi",
                "Janice Lam",
                "Daniel Licht",
                "Jean Maillard"
            ],
            "title": "No language left behind: Scaling human-centered machine translation",
            "year": 2022
        },
        {
            "authors": [
                "Xiaoman Pan",
                "Boliang Zhang",
                "Jonathan May",
                "Joel Nothman",
                "Kevin Knight",
                "Heng Ji."
            ],
            "title": "Crosslingual name tagging and linking for 282 languages",
            "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1:",
            "year": 2017
        },
        {
            "authors": [
                "Kishore Papineni",
                "Salim Roukos",
                "Todd Ward",
                "WeiJing Zhu."
            ],
            "title": "Bleu: a method for automatic evaluation of machine translation",
            "venue": "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311\u2013318,",
            "year": 2002
        },
        {
            "authors": [
                "Jerin Philip",
                "Alexandre Berard",
                "Matthias Gall\u00e9",
                "Laurent Besacier."
            ],
            "title": "Monolingual adapters for zero-shot neural machine translation",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 4465\u2013",
            "year": 2020
        },
        {
            "authors": [
                "Telmo Pessoa Pires",
                "Robin M Schmidt",
                "Yi-Hsiu Liao",
                "Stephan Peitz."
            ],
            "title": "Learning language-specific layers for multilingual machine translation",
            "venue": "arXiv preprint arXiv:2305.02665.",
            "year": 2023
        },
        {
            "authors": [
                "Matt Post."
            ],
            "title": "A call for clarity in reporting bleu scores",
            "venue": "WMT 2018, page 186.",
            "year": 2018
        },
        {
            "authors": [
                "Uri Shaham",
                "Maha Elbayad",
                "Vedanuj Goswami",
                "Omer Levy",
                "Shruti Bhosale."
            ],
            "title": "Causes and cures for interference in multilingual translation",
            "venue": "arXiv preprint arXiv:2212.07530.",
            "year": 2022
        },
        {
            "authors": [
                "Noam Shazeer",
                "*Azalia Mirhoseini",
                "*Krzysztof Maziarz",
                "Andy Davis",
                "Quoc Le",
                "Geoffrey Hinton",
                "Jeff Dean"
            ],
            "title": "Outrageously large neural networks: The sparsely-gated mixture-of-experts layer",
            "venue": "In International Conference on Learning",
            "year": 2017
        },
        {
            "authors": [
                "Qian Wang",
                "Jiajun Zhang."
            ],
            "title": "Parameter differentiation based multilingual neural machine translation",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, 10, pages 11440\u201311448.",
            "year": 2022
        },
        {
            "authors": [
                "Zhen Wang",
                "Rameswar Panda",
                "Leonid Karlinsky",
                "Rogerio Feris",
                "Huan Sun",
                "Yoon Kim."
            ],
            "title": "Multitask prompt tuning enables parameter-efficient transfer learning",
            "venue": "The Eleventh International Conference on Learning Representations.",
            "year": 2023
        },
        {
            "authors": [
                "Zirui Wang",
                "Zachary C. Lipton",
                "Yulia Tsvetkov."
            ],
            "title": "On negative interference in multilingual models: Findings and a meta-learning treatment",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing",
            "year": 2020
        },
        {
            "authors": [
                "Zirui Wang",
                "Zachary C Lipton",
                "Yulia Tsvetkov."
            ],
            "title": "On negative interference in multilingual models: Findings and a meta-learning treatment",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
            "year": 2020
        },
        {
            "authors": [
                "Haoran Xu",
                "Maha Elbayad",
                "Kenton Murray",
                "Jean Maillard",
                "Vedanuj Goswami."
            ],
            "title": "Towards being parameter-efficient: A stratified sparsely activated transformer with dynamic capacity",
            "venue": "arXiv preprint arXiv:2305.02176.",
            "year": 2023
        },
        {
            "authors": [
                "Haoran Xu",
                "Philipp Koehn",
                "Kenton Murray."
            ],
            "title": "The importance of being parameters: An intradistillation method for serious gains",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 170\u2013183,",
            "year": 2022
        },
        {
            "authors": [
                "Haoran Xu",
                "Kenton Murray."
            ],
            "title": "Por qu\u00e9 n\u00e3o utiliser alla spr\u00e5k? mixed training with gradient optimization in few-shot cross-lingual transfer",
            "venue": "Findings of the Association for Computational Linguistics: NAACL 2022, pages 2043\u20132059,",
            "year": 2022
        },
        {
            "authors": [
                "Biao Zhang",
                "Ankur Bapna",
                "Rico Sennrich",
                "Orhan Firat."
            ],
            "title": "Share or not? learning to schedule language-specific capacity for multilingual translation",
            "venue": "International Conference on Learning Representations.",
            "year": 2021
        },
        {
            "authors": [
                "Biao Zhang",
                "Philip Williams",
                "Ivan Titov",
                "Rico Sennrich."
            ],
            "title": "Improving massively multilingual neural machine translation and zero-shot translation",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Multilingual models confer the benefit of facilitating cross-lingual learning; however, they also grapple with the issue of language interference (Conneau et al., 2020; Wang et al., 2020a; Shaham et al., 2022). Recent studies aim to alleviate negative language interference through the introduction of language-specific (LS) modules (Zhang et al., 2020; Fan et al., 2020; Zhang et al., 2021; Fan et al., 2021; Pires et al., 2023). In this setup, each language batch is processed through its designated module rather than a shared module. Although this approach is promising and barely inflates the number of FLOPs like Mixture-ofExperts (MoE) (Shazeer et al., 2017; Lepikhin et al., 2021),2 the number of parameters becomes\n* Equal contribution 1We release our code at: https://github.com/fe1ixxu/ LMS_FD. 2Each pass through the model utilizes only the corresponding language-specific component. The additional\ndifficult to manage and sometimes impractical when working with a large variety of languages. This is because the fundamental element forming LS or MoE modules is typically the full-rank weight matrix derived from a densely connected layer, which causes a rapid increase in the number of parameters with a large number of languages or experts.3\nIn this paper, we first scrutinize the parameter efficiency of language-specific modules from the perspective of using fewer parameters. Consequently, a necessary question arises (RQ1): can we approximate the original dense weight matrix using substantially fewer parameters? To answer this question, we propose novel and parameter-efficient method, Language-Specific\ncomputational cost may only come from communication among devices (such as ALLToALL) or gate routing.\n3Although MoE employs a routing mechanism to keep the number of experts smaller than the number of languages, the parameter cost remains substantial.\nMatrix Synthesis (LMS), which can achieve similar performance to switch transformer even with three to four times smaller LS parameters (as shown in Figure 1).\nThen, we further investigate parameter efficiency from the perspective of knowledge density in each LS module. Given recent discoveries that the performance improvement of sparsely activated models diminishes with an increase in the number of experts (Hoffmann et al., 2022; Gao et al., 2022; Xu et al., 2023), we hypothesize that knowledge in these experts (or LS modules) is over-estimated. Hence, we propose another question (RQ2): Could a single shared module encapsulate the same level of knowledge as language-specific modules? In addressing this question, we introduce the Fuse Distillation (FD) method to examine the feasibility of condensing the multilingual knowledge into a single module.\nOur main contributions are summarized as follows:\n\u2022 We propose the parameter-efficient and lightweight LMS method, which substantially outperforms previous LS methods or MoE with fewer than or the same number of parameters, e.g., +1.73 BLEU over Switch Transformer on OPUS-100 multilingual translation.\n\u2022 We introduce FD to condense multilingual knowledge from LS modules into a shared module. FD is able to use only 2M more parameters (1% increase) to achieve the 65% of performance gains from Switch Transformer which use 760M more parameters (314% increase) during inference.\n\u2022 LMS and FD show strong generalization performance among multiple tasks, including multilingual machine translation (MMT) (Zhang et al., 2020), multilingual named-entity recognition (MNER) (Pan et al., 2017), and multilingual question answering (MQA) (Artetxe et al., 2020)."
        },
        {
            "heading": "2 Lightweight LS Modules",
            "text": "In this section, we address RQ1 by constructing LS modules with significantly fewer parameters."
        },
        {
            "heading": "2.1 Language-Specific Matrix Synthesis",
            "text": "Language-specific modules are typically composed of linear projections, whose weights are fullrank matrices in previous studies. We propose\nthe Language-specific Matrix Synthesis (LMS) method to form low-rank matrices to approximate the full-rank ones. This is inspired by the concept of \u201cintrinsic dimension\u201d in pre-trained language models (Aghajanyan et al., 2021; Hu et al., 2021) and \u201cintrinsic rank\u201d in trainable matrices, leading to the idea that features are learned in a subspace. Specifically, as shown in Figure 2, our LS matrix is derived from the multiplication of an LS \u2018vertical\u2019 matrix with an LS \u2018flat\u2019 matrix. Formally speaking, let W \u2208 Rr\u00d7c be a weight matrix in the model and we want to build parallel LS matrices which have the same size. Hence, for each language li, i \u2208 {1, 2, \u00b7 \u00b7 \u00b7 , L} with L being the number of languages, there exists an LS vertical matrix W liv \u2208 Rr\u00d7d and an LS flat matrix W li f \u2208 R d\u00d7c (d \u226a min(r, c)) that we use to approximate the full-rank matrix. Here, we propose two synthesis methods: language-wise and pair-wise synthesis.\nLanguage-Wise Synthesis Most multilingual tasks, such as conventional multilingual questionanswering, are characterized by a languagemonolithic nature: a single example only pertains to a single language, and examples from different\nlanguages build the multilingual data. Under such circumstances, a naive way to assemble a language-specific matrix for a given language, li, is straightforwardly using its corresponding vertical and flat matrices, such that W li = W liv W li f .\nPair-Wise Synthesis Cross-lingual tasks like MMT can also be accomplished using languagewise synthesis, wherein the encoder uses the source language matrix and the decoder uses the target language matrix. However, we posit that this is not the optimal strategy for MMT tasks due to the lack of learning bilingual information. Motivated by this, we introduce a pair-wise synthesis method to accommodate the bilingual context in each example in MMT. In this strategy, the language-specific matrix is a composition of the vertical matrix from the source language li and the flat matrix from the target language lj : W li\u2192lj = W liv W lj f . The difference between the language-wise and pairwise synthesis approaches is depicted in Figure 2. In Section 5, we will demonstrate that the pair-wise synthesis approach is more effective.\nAfter deriving a language-specific matrix, we incorporate it into the original full-rank matrix, as opposed to performing an isolated forward pass of the model like MoE and conventional LS methods. This approach stems from our hypothesis that the employment of low-rank matrices alone may not sufficiently facilitate the learning of features. Therefore, given an input xi associated with a source language li and a target language lj (li and lj are the same for language-monolithic tasks), our modified forward pass yields the output xo:\nxo = (W +W li\u2192lj )xi = (W +W li v W lj f )xi. (1)"
        },
        {
            "heading": "2.2 Where to Implement?",
            "text": "We primarily focus on incorporating languagespecific matrices generated using the LMS method into the linear projection of each feedforward network (FFN) layer in every transformer layer. Recall from earlier that r and c are the number of rows and columns in the matrix, and L is the number of languages. Thus, the total number of language-specific parameters added is given by 2L \u00b7 N \u00b7 d \u00b7 (c + r), where N represents the number of layers. We also conduct an ablation study to examine the performance when implementing LMS in attention layers in Section 6. For initialization, we employ a random Gaussian distribution for vertical matrices and zeros for flat matrices suggested by Hu et al. (2021).\n3 Can We Fuse Multilingual Knowledge in A Single Module?\nIn this section, we introduce Fuse Distillation (FD) and use a preliminary experiment to answer RQ2: whether we can condense the multilingual knowledge from language-specific modules into a single module."
        },
        {
            "heading": "3.1 Fuse Distillation",
            "text": "Let us first consider a language- (or task-) level MoE (Kudugunta et al., 2021), where we replace a single FFN layer with L FFN modules. L is the number of languages, as defined previously. The slight difference from the original design is we discard the routing gate and make each expert language-specific, i.e., an expert only serves batches in its corresponding language. Given recent findings that model improvements diminish with an increasing number of experts (Hoffmann et al., 2022; Gao et al., 2022; Xu et al., 2023), we hypothesize that information contained in experts is sparse and can be condensed into a shared module. To fuse knowledge from L FFN layers to the shared one, we propose the following training scheme and name this method Fuse Distillation:\nWe first add an additional shared FFN parallel to an existing model with L FFN layers as shown in Figure 3. During training, each batch undergoes two forward passes and one backward pass. In the first forward pass, the batch is processed through its language-specific FFN module; in the second pass, the batch is routed through the shared FFN. To fuse the language-specific knowledge contained within the L FFN modules into the shared FFN module, a distillation loss between the outputs from the two forward passes is also incorporated:\nLfd = KL(g(pl) \u2225 ps). (2)\nwhere pl denotes the probability output for the LS pass, and ps represents the shared pass output. The function g(\u00b7) signifies that gradients will not be traced back, so only the shared module learns from LS modules but LS ones do not learn from this loss. The backward pass also involves optimizing the model by minimizing the Cross-Entropy loss (CE) between the target and predicted values (the regular training loss). Thus, the total loss is:\nL = 1 2 (CE(y \u2225 pl) +CE(y \u2225 ps)) + Lfd, (3)\nwhere y denotes gold labels.\nThen, during the inference stage, we discard the LS modules. The model only forward passes the shared FFN for inference. To evaluate whether the shared FFN has effectively learned all LS information, we conduct a comparison between its results and those obtained via the routing through LS modules instead."
        },
        {
            "heading": "3.2 Preliminary Experiments",
            "text": "Our preliminary experiments are conducted under three settings: (1) Naive MMT: A basic multilingual translation model is trained without any modifications. (2) FD: This setting utilizes our proposed fuse distillation method. (3) FD-LS: We train the model with the FD method, but during the inference stage, the input is processed through its language-specific FFN module instead of the shared module as the original language-level MoE did.\nWe carry out our experiments using the IWSLT benchmarks, focusing on the many-to-many translation model paradigm. Following Lin et al. (2021); Xu et al. (2022), we collect 8 Englishcentric language pairs from the IWSLT\u201914 dataset, with sizes ranging from 89K to 169K sentences. We train all methods with the same number of steps and leave detailed training settings in Appendix A. We report sacreBLEU scores (Papineni et al., 2002; Post, 2018) with the FLORES-200 tokenizer (NLLB Team et al., 2022)."
        },
        {
            "heading": "3.3 Results and Analysis",
            "text": "Overview results of these 4 settings are shown in Table 1. The reported scores are the average of both xx\u2192en and en\u2192xx directions. As anticipated, after applying language-specific modules for each FFN layer, FD-LS has considerable enhancements over the naive MMT (+1.50 BLEU gains). Importantly, after discarding LS modules, FD only performs slightly worse than FD-LS (+1.17 vs. +1.50) with much fewer parameters for inference (48M vs. 149M). This observation underscores the feasibility of condensing multilingual knowledge into a single FFN module, thereby reducing the need of a large number of LS parameters for inference."
        },
        {
            "heading": "4 Combining LMS and FD",
            "text": "We have shown the success of multilingual information condensation by fuse distillation. We are interested in further reducing the parameters\nneeded by utilizing the language-specific matrix synthesis method during inference, so we then attempt to incorporate the FD method within LMS. Similar to Section 3.1, apart from the LS vertical and flat matrices, we introduce shared vertical and flat matrices, denoted as W sharedv and W sharedf , respectively. To employ the fuse distillation method, each batch is required to undergo two forward passes. The initial pass navigates through the LS matrix W + W liv W lj f , while the subsequent pass traverses the shared matrix W + W sharedv W shared f . These two passes generate two respective outputs, pl and ps. Given the common parameter W shared across both paths, we utilize symmetric KL divergence (Jiang et al., 2020) for distillation, as opposed to the traditional KL divergence:\nL\u2032fd = 1\n2 (KL(pl \u2225 ps) +KL(ps \u2225 pl)). (4)\nThus, the backward pass optimizes both the standard prediction loss and the fuse distillation\nloss. In Figure 4, we provide a comprehensive comparison of space complexity for generating extra LS (or expert) modules, among conventional LS modules, Mixture-of-Experts, and our proposed methods. Notably, our methods demonstrate substantial reductions in parameter usage during both training and inference."
        },
        {
            "heading": "5 Experiments",
            "text": "We evaluate our LMS and LMS+FD methods using three tasks: MMT, MNER, and MQA. Similar to Section 3.2, we have two routing options for the LMS+FD method during inference time: 1) evaluating the model by passing the shared route (denoted as LMS+FD-Share, the default setting), or 2) passing the language-specific module (denoted as LMS+FD-LS). We present results for both routes to show the performance difference between using the condensed module and the original LS modules. Considering the computational cost for MMT, we run all methods once with the same random seed. For the other two tasks, we run experiments with 3 different random seeds and report the average scores. For ease of implementation, we build homogeneous\nbatches (i.e., a batch only containing sentences in one language or one language direction) and only activate the corresponding LS module.4"
        },
        {
            "heading": "5.1 Baselines",
            "text": "We compare our approaches against two strong baselines that incorporate additional parameters to mitigate language interference.\nCLSR: The first baseline is Conditional Language-Specific Routing (CLSR) (Zhang et al., 2021), which employs LS linear projections following FFN or attention layer. Following their best settings, we set the budget p = 0.3 for LS routing. The original setting used shared LS projections across all encoder or decoder sublayers. We also consider a non-shared version, where each sublayer has its own LS projection, and denote it as CLSR*.\nSwitch Transformer: We also consider Switch Transformer (Fedus et al., 2021) as the second strong baseline, which uses similar FLOPs as our methods.5 We use 16 experts for every two layers\n4This does not apply to Switch Transformer. 5The design of the Switch Transformer, which employs top-1 routing, bears similarity to our model in that it processes\nwith a gate balance loss with a weight of 0.01."
        },
        {
            "heading": "5.2 Multilingual Machine Translation",
            "text": "Data and Training settings We concentrate on the many-to-many translation setting, with results reported from two benchmarks. The first is the English-centric IWSLT\u201914 dataset, as aforementioned in Section 3.2. Additionally, we examine the OPUS-100 dataset (Zhang et al., 2020), which encompasses 100 languages in total, including 94 development/test language pairs. We preprocess the data by sentencepiece (Kudo and Richardson, 2018), establishing a vocabulary size of 32K for the IWSLT\u201914 dataset and 64K for the OPUS-100 dataset. We utilize transformersmall and transformerbig for IWSLT\u201914 and OPUS-100, respectively. We fix the training steps for all methods for a fair comparison. For IWSLT\u201914, we use d = 32 as the rank for low-rank matrices. For OPUS-100, we consider three settings: (i) d = 64 to match the parameter size of the Switch Transformer, (ii) d = 16 to match the parameter size of CLSR, and (iii) d = 4 for very\nthrough a single module in each expert layer.\nlightweight LS model construction. The default LMS setting for MMT tasks is pair-wise unless otherwise specified. We discuss more training details in Appendix A.\nEvaluation We report results in terms of sacreBLEU (Post, 2018), tokenized by FLORES200 tokenizer (NLLB Team et al., 2022), and win ratio (WR) (Zhang et al., 2020) which is the proportion of language pairs on which our method beats the baseline. For IWSLT\u201914, we report the scores averaged by xx\u2192en and en\u2192xx directions. For OPUS-100, we split the 94 test language pairs into three groups based on their training data size suggested by Zhang et al. (2020): high-resource (> 0.9M, 45 languages), low-resource (< 0.1M, 21 languages) and medium-resource (others, 28 languages), and report the averaged scores in each category. We use beam search with a width of 5 and use a length penalty of 1.\nLMS performance: Light and Effective LS Module The primary results for IWSLT\u201914 and OPUS-100 are presented in Table 2 and Table 3, respectively. In the IWSLT\u201914 dataset, LMS\nsignificantly surpasses both the Switch Transformer and CLSR, despite having considerably fewer parameters. For OPUS-100, our methods and the baselines are evaluated with approximately equal extra parameters (e.g., 1002M in the Switch Transformer and 989M in LMS with d = 64). Compared with the gains from Switch transformer (+2.66 for en\u2192xx and +0.84 for xx\u2192en), our pairwise LMS method achieves substantially higher gains (+3.60 and +3.35). Similarly, our LMS method also outperforms CLSR (+0.02 and +1.83) with a comparable number of extra parameters. These results show the strong parameter efficiency of LMS for the MMT tasks. With merely 47M parameters (d = 4), our LMS method matches the Switch Transformer\u2019s performance for en\u2192xx and the CLSR\u2019s performance for xx\u2192en.\nLanguage-Wise or Pair-Wise? We compare language- and pair-wise synthesis in both IWSLT\u201914 and OPUS-100 (d = 64) datasets. On average, pair-wise synthesis outperforms languagewise synthesis by 0.27 BLEU points on IWSLT\u201914 (+1.05 vs. +0.78). Moreover, the pair-wise method (+3.60 and +3.35) also shows superior performance on the OPUS-100 dataset compared with the language-wise one (+2.09 and + 2.09). Notably, pair-wise synthesis with d = 16 surpassed the performance of language-wise synthesis with d = 64, even though the latter has 4 times more extra parameters. Hence, this discovery strongly advocates for the use of pair-wise synthesis over the language-wise approach."
        },
        {
            "heading": "FD performance: Can FD Fuse 95 Languages?",
            "text": "On the IWSLT\u201914 8-language MMT dataset, we observe negligible differences between LMS and LMS+FD (+1.05 vs. +0.88), suggesting successful condensation of information from various language-specific modules into the shared module. In the 95-language (94 languages plus English) scenario of OPUS-100, FD with a dimensionality of 16 utilizes only an additional 2M parameters (less than 1% increase compared to the 242M naive model) to attain 65% of the performance improvements from Switch Transformer (+1.13 vs. +1.75 on average), which requires 760M additional parameters (a 314% increase). While FD may not condense all multilingual information due to restricted parameter capacity, its parameter efficiency is commendable."
        },
        {
            "heading": "5.3 Multilingual Named-Entity Recognition",
            "text": "Data and Settings We evaluate our methods on Wikiann Named-Entity Recognition (Pan et al., 2017) dataset. We randomly select 24 languages to conduct experiments. The model architecture is based on pre-trained XLM-Rbase, attached with a feed-forward token-level classifier. We set the dropout rate as 0.1 and run 20 epochs for all methods. We set d = 32 for low-rank matrices and report F1 scores.\nResults The overall results are shown in Table 4. When applying LMS to each FFN layer for 24 languages, the model size increases by only 70M, while yielding a 0.55 F1 improvement. After implementing LMS+FD, the performance improves by 0.67 with the LS route and achieves a 0.33 gain with the shared route, which requires only an additional 3M parameters. Full results are shown in Appendix B."
        },
        {
            "heading": "5.4 Multilingual Question Answering",
            "text": "Data and Settings We pick 6 languages from TyDiQA (Typologically Diverse Question Answering)-Gold Passage to conduct the MQA experiments (Artetxe et al., 2020). Following Xu and Murray (2022), the representations of subwords in XLM-Rbase are input to a span classification head; a linear layer computing the answer\u2019s start and end. We set d = 32 for low-rank matrices, dropout rate = 0.1, and run 20 epochs.\nResults The overall results are shown in Table 5. Upon the application of LMS and LMS+FD, all methods exhibit improved performance with a slight increase in parameters. Notably, LMS+FDShare outperforms LMS+FD-LS. This suggests that FD may be more effective in fusing knowledge when the number of languages is relatively small. Full results are shown in Appendix C."
        },
        {
            "heading": "6 Ablation Study",
            "text": ""
        },
        {
            "heading": "6.1 Is LMS Parameter-Efficient?",
            "text": "Here, we examine the parameter efficiency of the LMS method, i.e., whether an increase in extra parameters yields a proportional enhancement in model performance. We conduct experiments with d ranging from 4 to 60 in increments of 8 to observe the resulting performance variations. For comparison, we examine the Switch Transformer with 4, 8, 12, 16 experts to assess its parameter efficiency. We focus on the MMT task using the OPUS-100 dataset. Due to computational demands, we limit experiments to randomly selected 15 languages from OPUS-100, designated as OPUS15. We leave training details in Appendix D.\nWe report the average BLEU gains over all translation directions in Figure 1. The plot reveals that the LMS curve is steeper compared to that of the Switch Transformer, indicating a higher parameter efficiency for our method, i.e., it achieves greater model performance with fewer additional parameters. Compared with a 16-expert Switch Transformer, LMS with d = 52 yields similar performance by using 3.7 times smaller parameters (51M vs. 189M). Numeric results are in Appendix E."
        },
        {
            "heading": "6.2 Applying LMS to The Attention Layer",
            "text": "In our default design, the LMS is solely applied to FFN layers. We are interested in assessing the potential benefits of extending LMS to the attention layer (in each K, Q, V, output projection). We consider three model variants: (1) LMS applied only to FFN layers (default design), (2) LMS applied only to the attention layers, and (3) LMS applied to both FFN and attention layers. We conduct experiments on OPUS-15, with a fixed rank value of d = 20.\nWe show the averaged BLEU of all translation directions of the three designs in Table 6. LMS applied only to attention layers yields inferior performance compared to LMS applied only to FFN layers with a similar number of extra\nparameters. Moreover, applying LMS to both FFN and attention layers results in a marginal improvement over its application solely to FFN layers. This outcome suggests that LS information is primarily situated in FFN layers, aligning with the previous findings of Wang et al. (2020b)."
        },
        {
            "heading": "7 Related Work",
            "text": "Language-Specific Modules To mitigate language interference, previous studies incorporate language-specific modules into models, such as additional language-aware linear projections (Zhang et al., 2020; Fan et al., 2020; Zhang et al., 2021; Fan et al., 2021), LS layer normalization (Zhang et al., 2020). Feed-Forward Networks (Kwon and Chung, 2023), or even entire languagedependent transformer layers (Escolano et al., 2021; Wang and Zhang, 2022; Pires et al., 2023). Similar to LS modules, Mixture-of-Experts (MoE) are also able to reduce language interference (Shazeer et al., 2017; Lepikhin et al., 2021; Fedus et al., 2021; Xu et al., 2023). However, the parameter count of LS (or expert) drastically increases when scaling to numerous languages. Zhang et al. (2021) address this issue by sharing all LS modules across all encoder or decoder layers. However, this does not fundamentally resolve the problem, given that the complexity of constructing LS modules remains unaltered and that different layers may need to learn varying types of LS information.\nLightweight Modules Our proposed techniques draw inspiration from another research line, lightweight fine-tuning, wherein the model undergoes fine-tuning on a parameter subset significantly smaller than that of the original model, such as prefix tuning (Li and Liang, 2021), prompt tuning (Lester et al., 2021), multitask prompt tuning (Wang et al., 2023), LoRA (Hu et al., 2021). In the multilingual machine translation setting, previous studies use language-pair adapters (Bapna and Firat, 2019) to fine-tune a specific\ndirection. This approach also extends to languagewise adapters (Philip et al., 2020), languagefamily adapters (Chronopoulou et al., 2023), hyperadapters (Baziotis et al., 2022) to facilitate the cross-lingual learning. In light of the efficient lightweight modules, we propose LMS to help LS modules scale to hundreds of languages."
        },
        {
            "heading": "8 Conclusion",
            "text": "The construction of language-specific modules (or experts) using full-rank matrices tends to be parameter-intensive and inefficient, especially as the number of languages (or experts) increases. To address this, we have introduced the Language-Specific Matrix Synthesis (LMS) method that approximates the original full-rank matrix. Notably, pair-wise synthesis, a variant of the LMS methods, exhibits commendable performance in MMT tasks. Further, we have proposed the Fuse Distillation (FD) approach to condense multilingual information into a shared module, thereby further diminishing parameter requirements during inference. Our methods outperform CLSR and Switch Transformer in MMT tasks and also demonstrate their effectiveness in MNER and MQA tasks.\nLimitations\nOne limitation of our LMS method is that it necessitates the construction of homogeneous batches, i.e., batches containing sentences exclusively in one language or language direction. However, this limitation could potentially be addressed by implementing ALLToALL communications amongst devices, a strategy that is already widely employed in Mixture of Experts (MoE) models (Lepikhin et al., 2021), which is a topic we intend to explore in future research. In each forward pass of an FFN layer, we need an additional step to multiply two small matrices, creating the low-rank large matrix. The additional cost of this operation is negligible, as the computational complexity of the FLOPs/tok for a Feedforward linear projection, given an input dimension c and output dimension r, is O(r \u00b7 c), while the complexity for constructing the low-rank matrix with rank d is O(d \u00b7 (r + c)). For example, in our ablation study, when r = 2048, c = 512, and d = 20, the difference in computational load can be 2048\u00d751220\u00d7(512+2048) \u2248 20 times less. In terms of actual training time, no significant\ndifferences were observed; the discrepancy was less than 1 second per 100 updates. Additionally, a potentially effective strategy to enhance multilingual information encapsulation in FD could involve using a larger shared module relative to other lightweight LS modules. This could be an intriguing avenue for future research."
        },
        {
            "heading": "Acknowledgements",
            "text": "We thank anonymous reviewers for their insightful feedback. We also extend our gratitude to Lingfeng Shen, Hieu Hoang, Young Jin Kim, Hany Hassan Awadalla, Stephen Rawls, and Amr Sharaf for their valuable suggestions. This work was supported in part by IARPA BETTER (#2019-19051600005). The views and conclusions contained in this work are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, or endorsements of ODNI, IARPA, or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copyright annotation therein. This work is also supported in part by an Amazon Initiative for Artificial Intelligence (AI2AI) Faculty Research Award."
        },
        {
            "heading": "A Training Details for IWSLT\u201914 and",
            "text": "OPUS-100\nTo balance the training data, we also over-sample low-resource languages with a temperature of T = 5 (Aharoni et al., 2019) for the OPUS-100 data and T = 2 for the IWSLT\u201914 data. We preprocess the data by sentencepiece (Kudo and Richardson, 2018), establishing a vocabulary size of 32K for the IWSLT\u201914 dataset and 64K for the OPUS-100 dataset. We pre-pend a special language id symbol at the beginning of the source sentence to indicate the target language. We build homogeneous batches (i.e., a batch only containing sentences in one language direction) and only activate the corresponding language-specific matrix. We set the dropout rate as 0.1 for both datasets. For the IWSLT\u201914 dataset, we fix the training steps at 150K with 8K warm-up steps for all methods, with a batch size of 4096 tokens. For OPUS, we fix the training steps at 100K with 8K warm-up steps for all methods, with a batch size of 4096 tokens but accumulating gradients 4 times. We train all models on 4 RTX 6000 GPUs. For the IWSLT\u201914 dataset, we employ the transformersmall model (with an FFN dimension of 1024 and an embedding dimension of 512), while the transformerbig model (with an FFN dimension of 4096 and an embedding dimension of 1024) is utilized for training the OPUS-100 dataset. The maximum learning rate is 0.0005. The optimizer is Adam (Kingma and Ba, 2014) with inverse_sqrt learning rate scheduler and weight decay of 0. We use beam search with a width of 5 and use a length penalty of 1."
        },
        {
            "heading": "B Full Results for MNER",
            "text": "We show the full results of MNER in Table 7."
        },
        {
            "heading": "C Full Results for MQA",
            "text": "We show the full results of MQA in Table 8."
        },
        {
            "heading": "D Training Details for The Ablation Study",
            "text": "We randomly pick 15 languages from the OPUS100 data to build a smaller 15-language data (OPUS-15) for the ablation study: eu, pt, bg, sk, zh, sl, de, hr, nb, ga, rw, as, fy, mr, se. We conduct the ablation study under the many-to-many translation settings. To balance the training data, we sample the data with a temperature of T = 5.\nWe preprocess the data by sentencepiece (Kudo and Richardson, 2018), establishing a vocabulary size of 32K vocabulary. we fix the training steps at 50K with 8K warm-up steps for all methods, with a batch size of 4096 tokens. We employ the transformerbase model (with an FFN dimension of 2048 and an embedding dimension of 512) for training the OPUS-15 dataset. The other settings are the same as Appendix A."
        },
        {
            "heading": "E Numeric Results for The Ablation Study",
            "text": "Figure 1 shows the averaged BLEU over all directions. Here, We show the detailed numeric results in Figure 9."
        }
    ],
    "title": "Condensing Multilingual Knowledge with Lightweight Language-Specific Modules",
    "year": 2023
}