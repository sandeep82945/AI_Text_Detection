{
    "abstractText": "End-to-end spoken language understanding (SLU) remains elusive even with current large pretrained language models on text and speech, especially in multilingual cases. Machine translation has been established as a powerful pretraining objective on text as it enables the model to capture high-level semantics of the input utterance and associations between different languages, which is desired for speech models that work on lower-level acoustic frames. Motivated particularly by the task of crosslingual SLU, we demonstrate that the task of speech translation (ST) is a good means of pretraining speech models for end-to-end SLU on both intraand cross-lingual scenarios. By introducing ST, our models reach higher performance over baselines on monolingual and multilingual intent classification as well as spoken question answering using SLURP, MINDS14, and NMSQA benchmarks. To verify the effectiveness of our methods, we also create new benchmark datasets from both synthetic and real sources, for speech summarization and low-resource/zero-shot transfer from English to French or Spanish. We further show the value of preserving knowledge for the ST pretraining task for better downstream performance, possibly using Bayesian transfer regularizers.",
    "authors": [
        {
            "affiliations": [],
            "name": "Mutian He"
        },
        {
            "affiliations": [],
            "name": "Philip N. Garner"
        }
    ],
    "id": "SP:e3a4d0c46e8c8a35f76c3a2c341b1eb65a9fb97f",
    "references": [
        {
            "authors": [
                "Rosana Ardila",
                "Megan Branson",
                "Kelly Davis",
                "Michael Kohler",
                "Josh Meyer",
                "Michael Henretty",
                "Reuben Morais",
                "Lindsay Saunders",
                "Francis M. Tyers",
                "Gregor Weber."
            ],
            "title": "Common Voice: A massivelymultilingual speech corpus",
            "venue": "LREC 2020, pages",
            "year": 2020
        },
        {
            "authors": [
                "Naveen Arivazhagan",
                "Ankur Bapna",
                "Orhan Firat",
                "Roee Aharoni",
                "Melvin Johnson",
                "Wolfgang Macherey."
            ],
            "title": "The missing ingredient in zero-shot neural machine translation",
            "venue": "CoRR, abs/1903.07091.",
            "year": 2019
        },
        {
            "authors": [
                "Siddhant Arora",
                "Siddharth Dalmia",
                "Pavel Denisov",
                "Xuankai Chang",
                "Yushi Ueda",
                "Yifan Peng",
                "Yuekai Zhang",
                "Sujay Kumar",
                "Karthik Ganesan",
                "Brian Yan",
                "Ngoc Thang Vu",
                "Alan W. Black",
                "Shinji Watanabe"
            ],
            "title": "ESPnet-SLU: Advancing spoken language",
            "year": 2022
        },
        {
            "authors": [
                "Mikel Artetxe",
                "Holger Schwenk."
            ],
            "title": "Massively multilingual sentence embeddings for zeroshot cross-lingual transfer and beyond",
            "venue": "Trans. Assoc. Comput. Linguistics, 7:597\u2013610.",
            "year": 2019
        },
        {
            "authors": [
                "Alexei Baevski",
                "Yuhao Zhou",
                "Abdelrahman Mohamed",
                "Michael Auli"
            ],
            "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
            "venue": "NeurIPS",
            "year": 2020
        },
        {
            "authors": [
                "Antonio Valerio Miceli Barone",
                "Barry Haddow",
                "Ulrich Germann",
                "Rico Sennrich."
            ],
            "title": "Regularization techniques for fine-tuning in neural machine translation",
            "venue": "EMNLP 2017, pages 1489\u20131494. ACL.",
            "year": 2017
        },
        {
            "authors": [
                "Emanuele Bastianelli",
                "Andrea Vanzo",
                "Pawel Swietojanski",
                "Verena Rieser."
            ],
            "title": "SLURP: A spoken language understanding resource package",
            "venue": "EMNLP 2020, pages 7252\u20137262. ACL.",
            "year": 2020
        },
        {
            "authors": [
                "Yonatan Belinkov",
                "Nadir Durrani",
                "Fahim Dalvi",
                "Hassan Sajjad",
                "James R. Glass."
            ],
            "title": "What do neural machine translation models learn about morphology? In ACL 2017, pages 861\u2013872",
            "venue": "ACL.",
            "year": 2017
        },
        {
            "authors": [
                "Yonatan Belinkov",
                "Nadir Durrani",
                "Fahim Dalvi",
                "Hassan Sajjad",
                "James R. Glass."
            ],
            "title": "On the linguistic representational power of neural machine translation models",
            "venue": "Comput. Linguistics, 46(1):1\u201352.",
            "year": 2020
        },
        {
            "authors": [
                "Iz Beltagy",
                "Matthew E. Peters",
                "Arman Cohan."
            ],
            "title": "Longformer: The long-document transformer",
            "venue": "CoRR, abs/2004.05150.",
            "year": 2020
        },
        {
            "authors": [
                "Yue Cao",
                "Xiaojun Wan",
                "Jin-ge Yao",
                "Dian Yu."
            ],
            "title": "MultiSumm: Towards a unified model for multilingual abstractive summarization",
            "venue": "AAAI 2020, pages 11\u201318. AAAI Press.",
            "year": 2020
        },
        {
            "authors": [
                "Kai-Wei Chang",
                "Wei-Cheng Tseng",
                "Shang-Wen Li",
                "Hung-yi Lee."
            ],
            "title": "An exploration of prompt tuning on generative spoken language model for speech processing tasks",
            "venue": "Interspeech 2022, pages 5005\u20135009. ISCA.",
            "year": 2022
        },
        {
            "authors": [
                "Ya-Hsin Chang",
                "Yun-Nung Chen."
            ],
            "title": "Contrastive learning for improving ASR robustness in spoken language understanding",
            "venue": "Interspeech 2022, pages 3458\u20133462. ISCA.",
            "year": 2022
        },
        {
            "authors": [
                "Xuxin Cheng",
                "Bowen Cao",
                "Qichen Ye",
                "Zhihong Zhu",
                "Hongxiang Li",
                "Yuexian Zou."
            ],
            "title": "ML-LMCL: Mutual learning and large-margin contrastive learning for improving ASR robustness in spoken language understanding",
            "venue": "Findings of ACL 2023,",
            "year": 2023
        },
        {
            "authors": [
                "Xuxin Cheng",
                "Zhihong Zhu",
                "Ziyu Yao",
                "Hongxiang Li",
                "Yaowei Li",
                "Yuexian Zou."
            ],
            "title": "GhostT5: Generate More Features with Cheap Operations to Improve Textless Spoken Question Answering",
            "venue": "Interspeech 2023, pages 1134\u20131138. ISCA.",
            "year": 2023
        },
        {
            "authors": [
                "Zewen Chi",
                "Li Dong",
                "Shuming Ma",
                "Shaohan Huang",
                "Saksham Singhal",
                "Xian-Ling Mao",
                "Heyan Huang",
                "Xia Song",
                "Furu Wei."
            ],
            "title": "mT6: Multilingual pretrained text-to-text transformer with translation pairs",
            "venue": "EMNLP 2021, pages 1671\u20131683. ACL.",
            "year": 2021
        },
        {
            "authors": [
                "Zewen Chi",
                "Li Dong",
                "Furu Wei",
                "Wenhui Wang",
                "XianLing Mao",
                "Heyan Huang."
            ],
            "title": "Cross-lingual natural language generation via pre-training",
            "venue": "AAAI 2020, pages 7570\u20137577. AAAI Press.",
            "year": 2020
        },
        {
            "authors": [
                "Yung-Sung Chuang",
                "Chi-Liang Liu",
                "Hung-yi Lee",
                "Lin-Shan Lee."
            ],
            "title": "SpeechBERT: An audio-andtext jointly learned language model for end-to-end spoken question answering",
            "venue": "Interspeech 2020, pages 4168\u20134172. ISCA.",
            "year": 2020
        },
        {
            "authors": [
                "Yu-An Chung",
                "Chenguang Zhu",
                "Michael Zeng."
            ],
            "title": "SPLAT: Speech-language joint pre-training for spoken language understanding",
            "venue": "NAACL-HLT 2021, pages 1897\u20131907. ACL.",
            "year": 2021
        },
        {
            "authors": [
                "Alexis Conneau",
                "Alexei Baevski",
                "Ronan Collobert",
                "Abdelrahman Mohamed",
                "Michael Auli."
            ],
            "title": "Unsupervised cross-lingual representation learning for speech recognition",
            "venue": "Interspeech 2021, pages 2426\u2013 2430. ISCA.",
            "year": 2021
        },
        {
            "authors": [
                "Melvin Johnson"
            ],
            "title": "XTREME-S: Evaluating cross-lingual speech representations",
            "venue": "In Interspeech",
            "year": 2022
        },
        {
            "authors": [
                "Alexis Conneau",
                "Kartikay Khandelwal",
                "Naman Goyal",
                "Vishrav Chaudhary",
                "Guillaume Wenzek",
                "Francisco Guzm\u00e1n",
                "Edouard Grave",
                "Myle Ott",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Unsupervised cross-lingual representation learning at scale",
            "venue": "ACL",
            "year": 2020
        },
        {
            "authors": [
                "Alexis Conneau",
                "Guillaume Lample."
            ],
            "title": "Crosslingual language model pretraining",
            "venue": "NeurIPS 2019, pages 7057\u20137067.",
            "year": 2019
        },
        {
            "authors": [
                "Yiming Cui",
                "Wanxiang Che",
                "Ting Liu",
                "Bing Qin",
                "Ziqing Yang."
            ],
            "title": "Pre-training with whole word masking for Chinese BERT",
            "venue": "IEEE ACM Trans. Audio Speech Lang. Process., 29:3504\u20133514.",
            "year": 2021
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "NAACL-HLT 2019, pages 4171\u20134186. ACL.",
            "year": 2019
        },
        {
            "authors": [
                "Linhao Dong",
                "Zhecheng An",
                "Peihao Wu",
                "Jun Zhang",
                "Lu Lu",
                "Zejun Ma."
            ],
            "title": "CIF-PT: bridging speech and text representations for spoken language understanding via continuous integrate-and-fire pretraining",
            "venue": "Findings of ACL 2023, pages 8894\u20138907.",
            "year": 2023
        },
        {
            "authors": [
                "Akiko Eriguchi",
                "Melvin Johnson",
                "Orhan Firat",
                "Hideto Kazawa",
                "Wolfgang Macherey."
            ],
            "title": "Zero-shot cross-lingual classification using multilingual neural machine translation",
            "venue": "CoRR, abs/1809.04686.",
            "year": 2018
        },
        {
            "authors": [
                "Mattia Antonino Di Gangi",
                "Roldano Cattoni",
                "Luisa Bentivogli",
                "Matteo Negri",
                "Marco Turchi."
            ],
            "title": "MuST-C: A multilingual speech translation corpus",
            "venue": "NAACL-HLT 2019, pages 2012\u20132017. ACL.",
            "year": 2019
        },
        {
            "authors": [
                "Heting Gao",
                "Junrui Ni",
                "Kaizhi Qian",
                "Yang Zhang",
                "Shiyu Chang",
                "Mark Hasegawa-Johnson."
            ],
            "title": "WavPrompt: Towards few-shot spoken language understanding with frozen language models",
            "venue": "Interspeech 2022, pages 2738\u20132742. ISCA.",
            "year": 2022
        },
        {
            "authors": [
                "Parisa Haghani",
                "Arun Narayanan",
                "Michiel Bacchiani",
                "Galen Chuang",
                "Neeraj Gaur",
                "Pedro J. Moreno",
                "Rohit Prabhavalkar",
                "Zhongdi Qu",
                "Austin Waters."
            ],
            "title": "From audio to semantics: Approaches to end-to-end spoken language understanding",
            "venue": "IEEE Spoken",
            "year": 2018
        },
        {
            "authors": [
                "Chan-Jan Hsu",
                "Ho-Lam Chung",
                "Hung-yi Lee",
                "Yu Tsao."
            ],
            "title": "T5lephone: Bridging speech and text self-supervised models for spoken language understanding via phoneme level T5",
            "venue": "ICASSP 2023, volume abs/2211.00586.",
            "year": 2023
        },
        {
            "authors": [
                "Wei-Ning Hsu",
                "Benjamin Bolte",
                "Yao-Hung Hubert Tsai",
                "Kushal Lakhotia",
                "Ruslan Salakhutdinov",
                "Abdelrahman Mohamed."
            ],
            "title": "HuBERT: Self-supervised speech representation learning by masked prediction of hidden units",
            "venue": "IEEE ACM Trans. Audio Speech",
            "year": 2021
        },
        {
            "authors": [
                "Zhiqi Huang",
                "Milind Rao",
                "Anirudh Raju",
                "Zhe Zhang",
                "Bach Bui",
                "Chul Lee."
            ],
            "title": "MTL-SLT: Multi-task learning for spoken language tasks",
            "venue": "4th Workshop on NLP for Conversational AI, ConvAI@ACL 2022, pages 120\u2013130. ACL.",
            "year": 2022
        },
        {
            "authors": [
                "Mandar Joshi",
                "Danqi Chen",
                "Yinhan Liu",
                "Daniel S. Weld",
                "Luke Zettlemoyer",
                "Omer Levy."
            ],
            "title": "SpanBERT: Improving pre-training by representing and predicting spans",
            "venue": "Trans. Assoc. Comput. Linguistics, 8:64\u201377.",
            "year": 2020
        },
        {
            "authors": [
                "Mihir Kale",
                "Scott Roy."
            ],
            "title": "Machine translation pre-training for data-to-text generation - A case study in Czech",
            "venue": "the 13th International Conference on Natural Language Generation, INLG 2020, pages 91\u201396. ACL.",
            "year": 2020
        },
        {
            "authors": [
                "Mihir Kale",
                "Aditya Siddhant",
                "Rami Al-Rfou",
                "Linting Xue",
                "Noah Constant",
                "Melvin Johnson."
            ],
            "title": "nmT5 - Is parallel data still relevant for pre-training massively multilingual language models? In ACL/IJCNLP 2021, pages 683\u2013691",
            "venue": "ACL.",
            "year": 2021
        },
        {
            "authors": [
                "Seongbin Kim",
                "Gyuwan Kim",
                "Seongjin Shin",
                "Sangmin Lee."
            ],
            "title": "Two-stage textual knowledge distillation for end-to-end spoken language understanding",
            "venue": "ICASSP 2021, pages 7463\u20137467. IEEE.",
            "year": 2021
        },
        {
            "authors": [
                "Diederik P. Kingma",
                "Jimmy Ba."
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "ICLR 2015.",
            "year": 2015
        },
        {
            "authors": [
                "James Kirkpatrick",
                "Razvan Pascanu",
                "Neil Rabinowitz",
                "Joel Veness",
                "Guillaume Desjardins",
                "Andrei A Rusu",
                "Kieran Milan",
                "John Quan",
                "Tiago Ramalho",
                "Agnieszka Grabska-Barwinska"
            ],
            "title": "Overcoming catastrophic forgetting",
            "year": 2017
        },
        {
            "authors": [
                "Zhenzhong Lan",
                "Mingda Chen",
                "Sebastian Goodman",
                "Kevin Gimpel",
                "Piyush Sharma",
                "Radu Soricut."
            ],
            "title": "ALBERT: A lite BERT for self-supervised learning of language representations",
            "venue": "ICLR 2020. OpenReview.net.",
            "year": 2020
        },
        {
            "authors": [
                "Mike Lewis",
                "Yinhan Liu",
                "Naman Goyal",
                "Marjan Ghazvininejad",
                "Abdelrahman Mohamed",
                "Omer Levy",
                "Veselin Stoyanov",
                "Luke Zettlemoyer"
            ],
            "title": "BART: Denoising sequence-to-sequence pre-training for natural language",
            "year": 2020
        },
        {
            "authors": [
                "Chia-Hsuan Li",
                "Szu-Lin Wu",
                "Chi-Liang Liu",
                "Hungyi Lee."
            ],
            "title": "Spoken SQuAD: A study of mitigating the impact of speech recognition errors on listening comprehension",
            "venue": "Interspeech 2018, pages 3459\u20133463. ISCA.",
            "year": 2018
        },
        {
            "authors": [
                "Xian Li",
                "Changhan Wang",
                "Yun Tang",
                "Chau Tran",
                "Yuqing Tang",
                "Juan Miguel Pino",
                "Alexei Baevski",
                "Alexis Conneau",
                "Michael Auli."
            ],
            "title": "Multilingual speech translation from efficient finetuning of pretrained models",
            "venue": "ACL/IJCNLP 2021, pages 827\u2013838. ACL.",
            "year": 2021
        },
        {
            "authors": [
                "Xuhong Li",
                "Yves Grandvalet",
                "Franck Davoine."
            ],
            "title": "Explicit inductive bias for transfer learning with convolutional networks",
            "venue": "ICML 2018, volume 80 of Machine Learning Research, pages 2830\u20132839. PMLR.",
            "year": 2018
        },
        {
            "authors": [
                "Hank Liao."
            ],
            "title": "Speaker adaptation of context dependent deep neural networks",
            "venue": "ICASSP 2013, pages 7947\u20137951. IEEE.",
            "year": 2013
        },
        {
            "authors": [
                "Chin-Yew Lin."
            ],
            "title": "ROUGE: A package for automatic evaluation of summaries",
            "venue": "Text Summarization Branches Out, pages 74\u201381, Barcelona, Spain. ACL.",
            "year": 2004
        },
        {
            "authors": [
                "Guan-Ting Lin",
                "Yung-Sung Chuang",
                "Ho-Lam Chung",
                "Shu-Wen Yang",
                "Hsuan-Jui Chen",
                "Shuyan Annie Dong",
                "Shang-Wen Li",
                "Abdelrahman Mohamed",
                "Hung-yi Lee",
                "Lin-Shan Lee"
            ],
            "title": "DUAL: Discrete spoken unit adaptive learning for textless",
            "year": 2022
        },
        {
            "authors": [
                "Yinhan Liu",
                "Jiatao Gu",
                "Naman Goyal",
                "Xian Li",
                "Sergey Edunov",
                "Marjan Ghazvininejad",
                "Mike Lewis",
                "Luke Zettlemoyer."
            ],
            "title": "Multilingual denoising pretraining for neural machine translation",
            "venue": "Trans. Assoc. Comput. Linguistics, 8:726\u2013742.",
            "year": 2020
        },
        {
            "authors": [
                "Yichao Lu",
                "Phillip Keung",
                "Faisal Ladhak",
                "Vikas Bhardwaj",
                "Shaonan Zhang",
                "Jason Sun."
            ],
            "title": "A neural interlingua for multilingual machine translation",
            "venue": "WMT 2018, pages 84\u201392. ACL.",
            "year": 2018
        },
        {
            "authors": [
                "David J.C. MacKay."
            ],
            "title": "A practical bayesian framework for backpropagation networks",
            "venue": "Neural Comput., 4(3):448\u2013472.",
            "year": 1992
        },
        {
            "authors": [
                "Wesley J. Maddox",
                "Pavel Izmailov",
                "Timur Garipov",
                "Dmitry P. Vetrov",
                "Andrew Gordon Wilson."
            ],
            "title": "A simple baseline for Bayesian uncertainty in deep learning",
            "venue": "NeurIPS 2019, pages 13132\u201313143.",
            "year": 2019
        },
        {
            "authors": [
                "Jonathan Mallinson",
                "Rico Sennrich",
                "Mirella Lapata."
            ],
            "title": "Zero-shot crosslingual sentence simplification",
            "venue": "EMNLP 2020, pages 5109\u20135126. ACL.",
            "year": 2020
        },
        {
            "authors": [
                "Bryan McCann",
                "James Bradbury",
                "Caiming Xiong",
                "Richard Socher."
            ],
            "title": "Learned in translation: Contextualized word vectors",
            "venue": "NIPS 2017, pages 6294\u2013 6305.",
            "year": 2017
        },
        {
            "authors": [
                "Razvan Pascanu",
                "Yoshua Bengio."
            ],
            "title": "Revisiting natural gradient for deep networks",
            "venue": "ICLR 2014.",
            "year": 2014
        },
        {
            "authors": [
                "Adam Poliak",
                "Yonatan Belinkov",
                "James R. Glass",
                "Benjamin Van Durme."
            ],
            "title": "On the evaluation of semantic phenomena in neural machine translation using natural language inference",
            "venue": "NAACL-HLT 2018, pages 513\u2013523. ACL.",
            "year": 2018
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Tao Xu",
                "Greg Brockman",
                "Christine McLeavey",
                "Ilya Sutskever."
            ],
            "title": "Robust speech recognition via large-scale weak supervision",
            "venue": "ICML 2023, volume 202 of Machine Learning Research, pages 28492\u201328518. PMLR.",
            "year": 2023
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J. Liu."
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "J. Mach. Learn. Res., 21:140:1\u2013140:67.",
            "year": 2020
        },
        {
            "authors": [
                "Alessandro Raganato",
                "Ra\u00fal V\u00e1zquez",
                "Mathias Creutz",
                "J\u00f6rg Tiedemann."
            ],
            "title": "An evaluation of language-agnostic inner-attention-based representations in machine translation",
            "venue": "the 4th Workshop on Representation Learning for NLP, RepL4NLP@ACL",
            "year": 2019
        },
        {
            "authors": [
                "Pranav Rajpurkar",
                "Jian Zhang",
                "Konstantin Lopyrev",
                "Percy Liang."
            ],
            "title": "Squad: 100, 000+ questions for machine comprehension of text",
            "venue": "EMNLP 2016, pages 2383\u20132392. ACL.",
            "year": 2016
        },
        {
            "authors": [
                "Anirudh Raju",
                "Milind Rao",
                "Gautam Tiwari",
                "Pranav Dheram",
                "Bryan Anderson",
                "Zhe Zhang",
                "Chul Lee",
                "Bach Bui",
                "Ariya Rastrow."
            ],
            "title": "On joint training with interfaces for spoken language understanding",
            "venue": "Interspeech 2022, pages 1253\u20131257. ISCA.",
            "year": 2022
        },
        {
            "authors": [
                "Machel Reid",
                "Mikel Artetxe."
            ],
            "title": "PARADISE: Exploiting parallel data for multilingual sequence-tosequence pretraining",
            "venue": "NAACL 2022, pages 800\u2013 810. ACL.",
            "year": 2022
        },
        {
            "authors": [
                "Alexander M. Rush",
                "Sumit Chopra",
                "Jason Weston."
            ],
            "title": "A neural attention model for abstractive sentence summarization",
            "venue": "EMNLP 2015, pages 379\u2013 389. ACL.",
            "year": 2015
        },
        {
            "authors": [
                "Elizabeth Salesky",
                "Matthew Wiesner",
                "Jacob Bremerman",
                "Roldano Cattoni",
                "Matteo Negri",
                "Marco Turchi",
                "Douglas W. Oard",
                "Matt Post."
            ],
            "title": "The multilingual tedx corpus for speech recognition and translation",
            "venue": "Interspeech 2021, pages 3655\u20133659. ISCA.",
            "year": 2021
        },
        {
            "authors": [
                "Michael Saxon",
                "Samridhi Choudhary",
                "Joseph P. McKenna",
                "Athanasios Mouchtaris."
            ],
            "title": "End-toend spoken language understanding for generalized voice assistants",
            "venue": "Interspeech 2021, pages 4738\u2013 4742. ISCA.",
            "year": 2021
        },
        {
            "authors": [
                "Sebastian Schuster",
                "Sonal Gupta",
                "Rushin Shah",
                "Mike Lewis."
            ],
            "title": "Cross-lingual transfer learning for multilingual task oriented dialog",
            "venue": "NAACL-HLT 2019, pages 3795\u20133805. ACL.",
            "year": 2019
        },
        {
            "authors": [
                "Holger Schwenk",
                "Matthijs Douze."
            ],
            "title": "Learning joint multilingual sentence representations with neural machine translation",
            "venue": "2nd Workshop on Representation Learning for NLP, Rep4NLP@ACL 2017, pages 157\u2013167. ACL.",
            "year": 2017
        },
        {
            "authors": [
                "Seunghyun Seo",
                "Donghyun Kwak",
                "Bowon Lee."
            ],
            "title": "Integration of pre-trained networks with continuous token interface for end-to-end spoken language understanding",
            "venue": "ICASSP 2022, pages 7152\u2013 7156. IEEE.",
            "year": 2022
        },
        {
            "authors": [
                "Dmitriy Serdyuk",
                "Yongqiang Wang",
                "Christian Fuegen",
                "Anuj Kumar",
                "Baiyang Liu",
                "Yoshua Bengio."
            ],
            "title": "Towards end-to-end spoken language understanding",
            "venue": "ICASSP 2018, pages 5754\u20135758. IEEE.",
            "year": 2018
        },
        {
            "authors": [
                "Xing Shi",
                "Inkit Padhi",
                "Kevin Knight."
            ],
            "title": "Does string-based neural MT learn source syntax? In EMNLP 2016, pages 1526\u20131534",
            "venue": "ACL.",
            "year": 2016
        },
        {
            "authors": [
                "Ravid Shwartz-Ziv",
                "Micah Goldblum",
                "Hossein Souri",
                "Sanyam Kapoor",
                "Chen Zhu",
                "Yann LeCun",
                "Andrew Gordon Wilson."
            ],
            "title": "Pre-train your loss: Easy Bayesian transfer learning with informative priors",
            "venue": "NeurIPS 2022.",
            "year": 2022
        },
        {
            "authors": [
                "Aditya Siddhant",
                "Melvin Johnson",
                "Henry Tsai",
                "Naveen Ari",
                "Jason Riesa",
                "Ankur Bapna",
                "Orhan Firat",
                "Karthik Raman."
            ],
            "title": "Evaluating the cross-lingual effectiveness of massively multilingual neural machine translation",
            "venue": "AAAI 2020, pages 8854\u20138861.",
            "year": 2020
        },
        {
            "authors": [
                "Sho Takase",
                "Naoaki Okazaki."
            ],
            "title": "Multi-task learning for cross-lingual abstractive summarization",
            "venue": "LREC 2022, pages 3008\u20133016. European Language Resources Association.",
            "year": 2022
        },
        {
            "authors": [
                "Rob van der Goot",
                "Ibrahim Sharaf",
                "Aizhan Imankulova",
                "Ahmet \u00dcst\u00fcn",
                "Marija Stepanovic",
                "Alan Ramponi",
                "Siti Oryza Khairunnisa",
                "Mamoru Komachi",
                "Barbara Plank"
            ],
            "title": "From masked language modeling to translation: Non-english auxiliary tasks",
            "year": 2021
        },
        {
            "authors": [
                "Ra\u00fal V\u00e1zquez",
                "Alessandro Raganato",
                "J\u00f6rg Tiedemann",
                "Mathias Creutz."
            ],
            "title": "Multilingual NMT with a language-independent attention bridge",
            "venue": "the 4th Workshop on Representation Learning for NLP, RepL4NLP@ACL 2019, pages 33\u201339. ACL.",
            "year": 2019
        },
        {
            "authors": [
                "Changhan Wang",
                "Anne Wu",
                "Jiatao Gu",
                "Juan Pino."
            ],
            "title": "CoVoST 2 and massively multilingual speech translation",
            "venue": "Interspeech 2021, pages 2247\u20132251. ISCA.",
            "year": 2021
        },
        {
            "authors": [
                "Yingzhi Wang",
                "Abdelmoumene Boumadane",
                "Abdelwahab Heba"
            ],
            "title": "2021b. A fine-tuned wav2vec 2.0/HuBERT benchmark for speech emotion recognition, speaker verification and spoken language understanding",
            "year": 2021
        },
        {
            "authors": [
                "Ruochen Xu",
                "Chenguang Zhu",
                "Yu Shi",
                "Michael Zeng",
                "Xuedong Huang."
            ],
            "title": "Mixed-lingual pre-training for cross-lingual summarization",
            "venue": "AACL/IJCNLP 2020, pages 536\u2013541. ACL.",
            "year": 2020
        },
        {
            "authors": [
                "Han Zhu",
                "Li Wang",
                "Gaofeng Cheng",
                "Jindong Wang",
                "Pengyuan Zhang",
                "Yonghong Yan."
            ],
            "title": "Wav2vec-S: Semi-supervised pre-training for lowresource ASR",
            "venue": "Interspeech 2022, pages 4870\u2013 4874. ISCA.",
            "year": 2022
        },
        {
            "authors": [
                "Junnan Zhu",
                "Qian Wang",
                "Yining Wang",
                "Yu Zhou",
                "Jiajun Zhang",
                "Shaonan Wang",
                "Chengqing Zong."
            ],
            "title": "NCLS: Neural cross-lingual summarization",
            "venue": "EMNLP-IJCNLP 2019, pages 3052\u20133062. ACL.",
            "year": 2019
        },
        {
            "authors": [
                "SLT (Huang"
            ],
            "title": "2022) to build the synthetic spoken version of Gigaword (Rush et al., 2015), using 9 speakers from Google TTS, with total 131.5H audio. We follow the data split in the original Gigaword dataset with a small and noisy test split",
            "year": 2015
        }
    ],
    "sections": [
        {
            "text": "By introducing ST, our models reach higher performance over baselines on monolingual and multilingual intent classification as well as spoken question answering using SLURP, MINDS14, and NMSQA benchmarks. To verify the effectiveness of our methods, we also create new benchmark datasets from both synthetic and real sources, for speech summarization and low-resource/zero-shot transfer from English to French or Spanish. We further show the value of preserving knowledge for the ST pretraining task for better downstream performance, possibly using Bayesian transfer regularizers."
        },
        {
            "heading": "1 Introduction",
            "text": "Modern artificial intelligence is characterized by large pretrained language models (PTLMs) with strong language capabilities to be adapted to various downstream tasks. The success of PTLMs rests on carefully-designed pretraining tasks to bestow the capability we expect on the model. Current PTLMs are mostly trained on self-supervised tasks, which started from masked language modelling (MLM) and next sentence prediction (NSP) in BERT (Devlin et al., 2019), but recently evolved into more difficult ones such as whole word (Cui et al., 2021) or span masking (Joshi et al., 2020),\ntext infilling, and token deletion (Lewis et al., 2020). While the rather simple NSP has been replaced by sentence permutation, document rotation (Lewis et al., 2020), and sentence order prediction (Lan et al., 2020). All those efforts introduced more challenges in the pretraining phase to mine stronger semantic supervision signals out of unlabelled data.\nSuch semantic-rich supervision is particularly relevant for pretrained spoken language models like wav2vec2 (Baevski et al., 2020) and HuBERT (Hsu et al., 2021) based on MLM on (sub-)phonetic units from lower-level audio signals, which are less informative and require models to carry out additional labor on acoustics. Therefore, their high-level capacities are more restricted. This may explain why automatic speech recognition (ASR) models finetuned upon them with paired data still have a role in fully end-to-end (E2E) SLU, often as a pretrained feature extractor (Seo et al., 2022; Arora et al., 2022). Unlike the cascaded SLU in which ASR produces transcripts for text processing, in such E2E systems ASR as an auxiliary or additional pretraining task provides strong supervision to explicitly link audio to representations that correspond to the denser and semantic-richer textual space, which is valuable for downstream understanding tasks.\nOn texts, self-supervised objectives are rather effective thanks to enormous text data with high information density, but supervised tasks are still used in many cases, machine translation (MT) being an often-seen one. A pioneer of the current PTLM paradigm, CoVe (McCann et al., 2017), is a seq2seq model pretrained on MT that achieved the then state-of-the-art on various downstream tasks. Belinkov et al. (2020) further validate language capabilities of MT on morphological, syntactic, and semantic levels, and T5 (Raffel et al., 2020) uses an ensemble of supervised tasks including MT. Furthermore, when trained with inputs of multiple languages, the model encoder may align and push representations for inputs in different lan-\nguages with similar meaning together to have the same output in the target language, thanks to the guidance from paired data (Johnson et al., 2017; Schwenk and Douze, 2017). With this semanticcentric language agnosticity, such an encoder can achieve few/zero-shot transfer to another language in downstream tasks (Eriguchi et al., 2018).\nInspired by those works, we hypothesize that the counterpart of multilingual MT on speech, i.e., E2E multilingual speech translation (ST) that directly maps speech of various languages to texts in other languages, will also be effective as a pretraining task on E2E SLU, for three critical advantages:\n1. It requires high-level understanding as an interpreter must \u201cunderstand\u201d the utterance before interpreting it into a different language, unlike ASR that transcribes speech verbatim and MLM on phonetic units that needs less semantic understanding.\n2. It captures long-term dependency and a global view of the full input, in contrast to ASR and MLM which can often be resolved with local context.\n3. It enables better cross-lingual transfer in comparison with multilingual ASR models and selfsupervised PTLMs without the supervision that promotes language agnosticity.\nAdmittedly, ST data is only available in a limited number of language pairs, but for each covered language, there are infinite number of diverse downstream SLU tasks with only rich data in English. It is a practical need to enroll various such languages to an English-only model trained on each specific SLU task. Therefore, as shown in Figure 1, we may pretrain the model on speech translation between English and the target language French in both directions (i.e. En\u2194Fr), and then fine-tune on downstream tasks with an additional classifier, reusing the encoder. We show the benefit of our method on a variety of tasks for semantic understanding of speech, including mono- & multilingual intent classification (IC), spoken question answering (SQA), as well as speech summarization, for which we create a synthetic dataset following Huang et al. (2022). Then we show the strong advantage on cross-lingual transfer to French. All the experiments are focused on comparing ST with other tasks like ASR as the pretraining or auxiliary task to verify our core hypothesis above. In addition, to show that our method applies to other languages as well, we also conducted experiments using Spanish as the target language. This is evaluated by creating the French and Spanish version of the English IC\nbenchmark SLURP (Bastianelli et al., 2020), using both real and synthetic sources. On all the tasks, our approach outperforms previous baselines and ASR pretraining, often by a large margin.\nFurthermore, unlike knowledge for selfsupervised objectives loosely connected to target SLU tasks, knowledge to handle tasks with closer link to semantics such as ST will be more valuable, following our core hypothesis. Hence it should be helpful to preserve such knowledge instead of direct fine-tuning with the risk of catastrophic forgetting. Therefore, we introduce multi-task learning as well as Bayesian regularizers for knowledge preservation, namely L2-SP (Li et al., 2018b) and EWC (Kirkpatrick et al., 2017), which show benefits especially in low-resource cases.\nTo summarize, our contributions are three-fold: 1. We demonstrate the effectiveness of speech translation pretraining on multiple SLU tasks, especially in cross-lingual transfer cases.\n2. We confirm the value of preserving ST pretraining knowledge for downstream tasks and the capability of Bayesian regularizers to achieve that.\n3. We build several new datasets for speech summarization and cross-lingual SLU.\nOur code, models, and datasets will be released at https://github.com/idiap/ translation-aided-slu."
        },
        {
            "heading": "2 Model Pretraining",
            "text": "As in Figure 1, we first build a speech translator using an architecture established by Li et al. (2021), that connects pretrained models on speech and text with a CNN adaptor: Audio signals are fed into the lower half of the multilingual wav2vec2, XLSR-53 (Conneau et al., 2021), to extract the (sub-)phonetic representations into a 320x-downsampled sequence. The upper half (12 layers) of XLSR is discarded for computational efficiency, as those parameters are found focused on MLM pretraining and less useful for downstream tasks (Zhu et al., 2022). Given the phonetic level embeddings produced by the half XLSR, the task is similar to machine translation to map them to the output text, for which we leverage the MT model based on mBART (Liu et al., 2020). While the length of this sequence is still much longer than the corresponding text. To better align it with typical textual embeddings as in mBART inputs, a 3-layer 8x-downsampling CNN adaptor is inserted. A target embedding is then prepended to specify the target language or task, similar to the target token used in mBART. To promote language agnosticity, we do not indicate the source language. Furthermore, it has been found that explicitly promoting language agnosticity may help zero-shot transfer (Arivazhagan et al., 2019), hence we\u2019ve also attempted to add language adversarial training on the encoder outputs during pretraining and fine-tuning, using a language classifier of a 2-layer MLP to predict the language of the input speech, with gradient reversal layer to explicitly align the representations between different languages.\nBased on the architecture, we fine-tune the model using a combination of the En\u2192Fr portion of MuST-C (Gangi et al., 2019), and the Fr\u2192En portion of TEDx (Salesky et al., 2021), both derived from TED talks, plus the Fr\u2192En portion of CoVoST2 (Wang et al., 2021a) based on general sentences in Common Voice (Ardila et al., 2020), with texts further cleaned and sentences that are too long or contain foreign characters removed. Unlike Li et al. (2021), the whole model is fine-tuned for best pretraining results. To compare, we also experiment with pretraining on the task of ASR instead. As the data are paired with both translations and\ntranscripts, we use the same ST dataset for ASR training to build a multilingual (En+Fr) ASR model. We\u2019ve tried to jointly train on ASR+ST in a multitask manner as well. With a total of >700 hours paired speech data, we achieve satisfactory results on the pretraining tasks as indicated in Table 1, and ASR+ST training shows better performance compared to the single-task ones. Starting from the ASR and ST models, we further add Spanish portion from the same set of ST datasets. As a result, we obtain an ST model supporting both En\u2194Fr and En\u2194Es (Spanish), and a tri-lingual En+Fr+Es ASR model, both with similar satisfactory results, details available in Appendix E."
        },
        {
            "heading": "3 Downstream Adaptation",
            "text": ""
        },
        {
            "heading": "3.1 Tasks",
            "text": "We then fine-tune the whole model on a variety of direct downstream tasks as follows.\nSLURP is a large and challenging English SLU dataset recently proposed, with 72.2k real speech recordings and 69.3k synthetic audio for a broad range of speech commands given to voice assistants. We use its IC labels to classify the input into 18 scenarios and 46 actions.\nMINDS-14 is a multilingual IC dataset for banking scenarios with 14 types of intents in 14 languages with around 600 utterances per language, and we use four subsets (en-AU, en-GB, en-US, and fr-FR) under a 3:2:5 train-dev-test split in XTREME-S (Conneau et al., 2022). The rather scarce training data demand data-efficient multilingual modelling.\nNMSQA or Natural Multi-Speaker Question Answering is a spoken QA dataset consisting of audio for the questions and segmented context articles from SQuAD (Rajpurkar et al., 2016), with 97.6k\nquestion-answer pairs given in >300 hours of synthetic audio from 12 speakers produced by Amazon TTS, coupled with a 60-speaker real test set of 2.7 hours of recordings. In this task, the goal is similar to textual QA to predict the correct span in the spoken context audio that answers the question, and the performance is measured by Audio Overlapping Score (AOS) (Li et al., 2018a), defined as AOS = X \u2229 Y /X \u222a Y , in which X is the predicted audio span and Y the ground truth.\nSpoken Gigaword is the synthetic spoken version of the summarization or headline generation task on Gigaword (Rush et al., 2015), proposed by Huang et al. (2022), aimed at generating a brief headline from a short piece of English spoken news. As it was not released, we follow their method to filter the data and create a synthetic dataset of 131.5 hours of audio using Google TTS from 9 neural voices in en-US, with 50k training samples, 1k validation samples, and 385 test samples, as a result of filtering out the frequent noise in the test set.\nSynthetic data are used in those established datasets for training and evaluation. Despite being possibly different from real data, it is observed that they are often reliable to reflect model performances and well correlated with real cases."
        },
        {
            "heading": "3.2 Methods",
            "text": "For these downstream tasks, we reuse the encoder further pretrained on ST/ASR (with French, unless otherwise stated). It should be noted that the 12- layer mBART encoder we use is slightly smaller than half XLSR. So when connected, the total encoder size and the computational cost to fine-tune it is comparable with fine-tuning the whole original XLSR. Upon the encoder we stack a 3-layer transformer, which is also transferred from a PTLM. As for IC, we use layer 2-4 from pretrained XLM-R (Conneau et al., 2020) for possibly better understanding capabilities, stacked with linear classifier heads over mean-pooled outputs. Particularly, for SLURP in which the intent consists of a scenario and an action, two heads are used. As for SQA, we use layer 2-4 of pretrained Longformer (Beltagy et al., 2020), a PTLM dedicated for long utterances due to the length of each segment in the data, as in Lin et al. (2022). Two linear classifiers are then applied to each frame to predict the start and end of the span, along with an answer existence classifier over mean-pooled outputs to predict if the answer exists in the provided seg-\nment. We then concatenate the question audio with each segment in the spoken article as model inputs, and pick the predicted answer span from the segment with the highest answer existence likelihood. For these two tasks, the pretrained decoder is simply discarded. While speech summarization is more similar to ASR and distinct from other downstream tasks that the model first needs to capture general meaning of the speech as encoded representations, and then generate a textual summary by the decoder, which demands a seq2seq architecture identical to the ST/ASR pretraining task. Hence we reuse the whole encoder-decoder model and formulate the task as generation in an extra \u201ctarget language\u201d. With the needs to both understand the general meaning and generate in the same language, we hypothesize that combining ASR and ST will lead to the best results.\nFurthermore, as mentioned above, direct model fine-tuning may lead to catastrophic forgetting of the knowledge on ASR or ST and harm semantic understanding capabilities. Hence we also tried a multi-task joint training approach on both the pretraining and target task. Results are compared between the model pretrained with ST, ASR, or both, or one directly derived from self-supervised pretraining without further supervision (None), plus other baselines. More, the recent Whisper (Radford et al., 2023) is trained on multiple speech tasks including ASR and ST, which matches our idea despite not aiming at SLU. Hence we also try to fine-tune the Whisper encoder, using the medium version with size similar to our encoder."
        },
        {
            "heading": "3.3 Results",
            "text": "English IC Following the previous works, we report the test accuracy on SLURP as in Table 2. It can be observed that the models with ST pretraining outperform those trained on ASR only, while adding ASR to ST pretraining makes limited improvements, though it gives better WER and BLEU during pretraining; it is the same case for the model with the extra Spanish ST task introduced in pretraining. However, ASR does help considering the None model directly fine-tuned from selfsupervised PTLMs without any additional pretraining. By joint training with both the pretraining and downstream task, results are consistently improved. However, despite being a strong ASR+ST model, Whisper is found not suitable for fine-tuning on SLURP in this way as shown by the low accuracy.\nThis might be explained by the fact that Whisper is trained on En ASR and X\u2192En ST but not for En\u2192X ST. Our hypothesis is that the ST pretraining on a specific language would enhance the semantic understanding capabilities of the model in that language, which may not help Whisper much on the English SLURP benchmark. Also, Whisper is trained on 30-second chunks, while SLURP contains more shorter utterances.\nHuBERT, used in multiple baselines in Table 2, has been found stronger on various downstream tasks compared to wav2vec2. Owing to the lack of a multilingual HuBERT (large) model, we rely on the multilingual wav2vec2 as our acoustic encoder. However, we reach much better results compared to many notable baselines, including the approach of jointly generating the intents and slots (Wang et al., 2021b), with 87.13% accuracy, the highest among wav2vec2-based baselines. We also reach slightly higher accuracy than its HuBERT version, which was the previous state-of-the-art. The very recent CIF-PT (Dong et al., 2023), concurrent with ours also injects more semantic signal, but by learning frame-to-token alignments on the encoder and then distilling from PTLMs, significantly pushing the state-of-the-art on this monolingual benchmark. Nevertheless the method is distinct from ours, raising the possibility of applying both methods or-\nthogonally for further improvement, and we maintain advantages on cross-lingual transfer and possibly also generative tasks by reusing a pretrained seq2seq decoder, as elaborated below.\nMultilingual IC We then report the accuracy on MINDS-14 as in Table 3 on four languages plus the average accuracy across languages, compared to a baseline directly fine-tuned from XLSR. The results are consistent with the monolingual case that ST pretraining can significantly improve the performance on SLU tasks, that joint training is beneficial, and that adding ASR gives limited gains.\nSpoken QA We compare our methods with results reported by Lin et al. (2022), including the results from a cascaded pipeline that fine-tunes Longformer upon transcripts from wav2vec2-based ASR, and the DUAL approach that fine-tunes Longformer upon units pre-extracted by a frozen HuBERT, hence not fully end-to-end. For fair comparison, we fine-tune the classifier built by layers 2\u20134 of Longformer and the top 5 layers of the mBART encoder, while the rest of the model is frozen and used as a feature extractor, so that they have a comparable number of trainable parameters with the baselines. Therefore we do not conduct experiments on joint training in this task as most shared parameters are frozen. The results reported in the more recent T5lephone (Hsu et al., 2023) including the E2E and cascaded approach are also mentioned, though they are almost twice as large as other models. All the baselines enjoy a view of the whole article, while in our experiments we use a model that works on a shorter context window with the question and each segment in the article individually, in order to have an end-to-end architecture given our computational resources that is consistent with other experiments. Therefore, the baselines possess a strong advantage over ours. However, as shown in Table 4, the additional pretraining stage leads to better results compared to all the E2E baselines, which further demonstrates the advantage of our approach. Particularly, ST considerably improves the performance and could successfully beat the cascaded system reported by Lin et al. (2022) in the more challenging test portion.\nSpeech summarization We report the results compared between different auxiliary tasks as in Table 5 using the ROUGE-1/2/L metrics (Lin, 2004). In the experiments, we observed that simply finetuning the model rapidly leads to overfitting, hence\nwe perform joint-training only, and use a special target embedding to indicate the summarization task. ASR is helpful on the summarization task as the ST+ASR model consistently outperforms the ST one, while the ST one is still better than the ASR-only model, signifying the importance of the semantic understanding capability brought by ST pretraining. In addition, we compare with a cascaded baseline that first transcribes the inputs with our ASR model, which introduces WER of 9.1% and 8.9% on dev and test respectively. Then we leverage a BART-based model finetuned on the full textual Gigaword with ROUGE1/2/L=37.28/18.58/34.53 to produce the summaries. When applied to the relatively simple utterances in Spoken Gigaword, it reaches a higher performance on dev, which suggests the challenges for E2E systems in our benchmark, though the gap is narrow compared to our E2E approach with ST+ASR pretraining, and on the noisier test set our E2E models consistently get much better results."
        },
        {
            "heading": "4 Cross-lingual Transfer",
            "text": "For cross-lingual transfer, IC models trained on SLURP are then applied on/fine-tuned to French/Spanish data below:\nDatasets A French version of SLURP, SLURPFr, is created to evaluate the cross-lingual transfer capabilities of the model, which is based on MASSIVE (FitzGerald et al., 2023), a translation of SLURP texts into multiple languages. With the same input domain and output categories, zero-shot transfer becomes possible. We first produce the audio for the 16.5k French samples in MASSIVE with a 7:2:1 train-dev-test split using Google TTS from four different WaveNet-based speakers. Then we invite two native French speakers to read out a total of 477 randomly-selected category-balanced held-out utterances, forming the real test set. To mimic SLURP, we record the audio indoors with two microphones under both near-field and far-field conditions. We also define a 100-shot per category subset with 4.5k samples in total to simulate a condition with even lower resource. SLURP-Es is created in a way similar to SLURP-Fr with 16.5k Spanish samples in MASSIVE, though we are unable to create a real set.\nExperiments The advantage of our method on cross-lingual transfer is evaluated under the fulldata, 100-shot, and zero-shot cases, using different pretraining strategies compared to the None model trained on SLURP without further supervision but directly upon the multilingual self-supervised pretrained models. Hence it is noteworthy that all the compared models have been pretrained in a multilingual way. As given in Table 6 on French, extra multilingual ST/ASR supervision consistently leads to better results on different data amounts. ST pretraining outperforms ASR, similar to previ-\nous experiments, while ST+ASR joint pretraining brings some improvements in the zero-shot case. Notably, the gap between ASR and ST models becomes larger with fewer data, especially with zero shot, which implies the importance of ST on cross-lingual transfer. Accuracy on the real near-field speech is reported, which correlates well with those on synthetic ones, indicating that performance on synthetic speech is reliable for evaluation. The ST+Adv. model incorporates language adversarial training during pretraining and fine-tuning to further promote language agnosticity as mentioned above, which outperforms other models in most cases, particularly with zero shot, implying the usefulness of language adversarial training and the importance of language agnosticity of input\nfeatures to the classifier. In addition, we build a cascaded system which first translates the speech into English text using our ST model, with BLEU scores of 26.08/26.34/21.56 on dev/test/real. Then a BART-based textual SLURP model with 85.7% test accuracy is used. This essentially zero-shot system gives a competitive 62.2% real accuracy, which poses challenges to the future development of E2E cross-lingual models. The model on Spanish, based on En+Fr+Es ST/ASR pretraining along with training on SLURP in an identical protocol, shows similar results as in Table 7, indicating the applicability of our approach to other languages."
        },
        {
            "heading": "5 Pretraining Knowledge Preservation",
            "text": "Methods As mentioned above, the knowledge to perform ASR/ST that connects speech and semantic-rich texts could be valuable for downstream tasks, which motivates us to use joint training above that maintains the performance on the pretraining tasks. This is verified by the considerable performance improvement or gap between the joint and single models. However, it is computationally intensive and requires access to the pretraining data. To alleviate the gap without joint training, we intend to explore Bayesian transfer/continual learning regularizers that limit the parameter shift by applying a prior on the parameters,\nbased on the Laplacian approximation of posterior parameter distributions in pretraining (MacKay, 1992). Particularly, the L2-SP method formulate the prior as an isotropic Gaussian distribution with the pretrained parameters \u03b80 as the mean and identical variance for all parameters, which leads to an L2 regularization term with weight \u03b1 centered at \u03b80 in the loss for the maximum likelihood estimation of the parameters (Li et al., 2018b). While elastic weight consolidation (EWC) (Kirkpatrick et al., 2017) considers the variance of each parameter \u03b8i decided by the Fisher diagonal Fi, which could be further estimated using squared gradients by averaging over the stochastic gradient descent (SGD) trajectory. However, for optimization we use the Adam algorithm\n\u03b8t \u2190 \u03b8t\u22121 \u2212 \u03b1 \u00b7 m\u0302t/( \u221a v\u0302t + \u03f5), (1)\nthat already computes v\u0302t, an exponential moving average of squared gradients (Kingma and Ba, 2015), close to linear averaging with a smoothing parameter \u03b22 = 0.999. Hence we reuse them to set the per-parameter weight \u03b1Fi for regularization. For both methods, the hyperparameter \u03b1 is used to control the strength of the knowledge preservation or the restraint to the parameter update. See Appendix A for more theoretical explanations.\nExperiments We experiment with these regularizers, targeted on ST-pretraining on SLURP and MINDS-14, plus the ST+ASR pretraining on MINDS-14 which has a considerable 1.32% accuracy gap. We try to use various weights \u03b1 for L2-SP regularization ranging from 1e-5 to 1e-2. Then we inspect the distribution of the approximated Fi, which ranges from 1e-20 to 1e-5 as in Appendix F. For optimization stability we clamp\nthe weight \u03b1Fi above 1e-2, and use EWC weights of 2e2, 2e4, 2e6, and 2e7 to roughly match the magnitude of those for the L2-SP regularizer.\nResults are shown in Figure 2, and for MINDS14 the average accuracies are reported. In the case of SLURP, it is possible that the amount of data is already sufficient that the preservation of the pretraining knowledge could be helpful only if it is carried out in a fully adaptive way, namely joint training. Therefore, the regularizers lead to limited help or even harm to the accuracy when the weight is large. However, under the low-resource condition in MINDS-14, both regularizers are effective. As in Li et al. (2018b), although being more flexible and adaptive, EWC doesn\u2019t necessarily lead to better transfer learning. This is consistent with our observations: Both regularizers can successfully overcome the accuracy gap or even go beyond the joint training model under an appropriate weight, while the best regularizer varies in different cases, though the more adaptive EWC has a chance to reach better results as in the MINDS-14 ST+ASR case. In this way, we demonstrate the effectiveness of Bayesian parameter-preserving regularizers for transfer learning on such large pretrained models."
        },
        {
            "heading": "6 Related Work",
            "text": "Translation as an auxiliary task It has been found that representations from MT models capture various aspects of the input utterance such as syntax (Shi et al., 2016), morphology (Belinkov et al., 2017), and also semantic inferences (Poliak et al., 2018; Belinkov et al., 2020). Hence MT has been established as a pretraining task as in CoVe (McCann et al., 2017) for various downstream tasks. But unlike this paper, recent works\non the direction has been focused on multilingual and cross-lingual cases, starting from attempts to reuse MT representations as sentence embeddings for text classification (Shi et al., 2016; Lu et al., 2018), and, particularly often, for semantic similarity and bi-text mining (Schwenk and Douze, 2017; V\u00e1zquez et al., 2019; Raganato et al., 2019; Artetxe and Schwenk, 2019). As for pretraining PTLMs to be fine-tuned, MT proves effective for downstream cross-lingual tasks on few-shot and zero-shot transfer (Eriguchi et al., 2018), while often accompanied with similar tasks like translation language modelling (Conneau and Lample, 2019; Kale et al., 2021), cross-lingual MLM (Chi et al., 2021), and dictionary denoising (Reid and Artetxe, 2022). Particularly, MT has been used as an auxiliary task for cross-lingual intent classification on texts (Schuster et al., 2019; Siddhant et al., 2020; van der Goot et al., 2021), and is widely used on cross-lingual generation, including summarization (Zhu et al., 2019; Cao et al., 2020; Xu et al., 2020; Takase and Okazaki, 2022), simplification (Mallinson et al., 2020), question generation (Chi et al., 2020), and data-to-text generation (Kale and Roy, 2020).\nEnd-to-end SLU Cascaded SLU methods work on ASR transcripts, for which error propagation is a major challenge (Chang and Chen, 2022; Cheng et al., 2023a). Hence recently end-to-end methods have gained popularity (Serdyuk et al., 2018; Haghani et al., 2018), especially with the performance gap compared with cascaded systems mitigated in many cases thanks to the PTLM paradigm. Besides directly fine-tuning existing PTLMs on speech (Wang et al., 2021b; Arora et al., 2022), there are also explorations for end-to-end interface to connect pretrained models on speech and text (Saxon et al., 2021; Seo et al., 2022; Raju et al., 2022), as well as joint speech-text modelling, pretraining, or distillation (Chuang et al., 2020; Chung et al., 2021; Kim et al., 2021; Villatoro-Tello et al., 2023; Dong et al., 2023), prompt tuning for PTLMs (Gao et al., 2022; Chang et al., 2022), combining PTLM features (Cheng et al., 2023b), and multitask learning with ASR (Huang et al., 2022).\nBayesian transfer learning Viewing the pretrained model not as a point estimation but a distribution is critical for continual learning as in EWC (Kirkpatrick et al., 2017), and the idea has been also applied to transfer learning to regularize fine-\ntuning as in L2-SP for image classification (Li et al., 2018b), though similar regularizers have been used on MT (Barone et al., 2017) and ASR (Liao, 2013). More recently, Shwartz-Ziv et al. (2022) propose to approximate the prior using SGD trajectory as in SWAG (Maddox et al., 2019) for transfer learning."
        },
        {
            "heading": "7 Conclusion",
            "text": "We confirm our hypothesis that speech translation can be a powerful pretraining and joint-training means for various end-to-end models on tasks involving semantic understanding of speech. Particularly, it benefits multilingual scenarios and crosslingual transfer, including the zero-shot case. We also create two new datasets for the above tasks. Furthermore, we demonstrate the effectiveness of Bayesian regularizers to preserve the knowledge from pretraining for downstream tasks.\nLimitations\nSome of the limitations of our paper are: 1. The best results are mostly achieved with multi-task learning, which adaptively preserves the knowledge from the pretraining task, but much slower, computationally intensive, and energy consuming. Therefore we explore the regularizers from continual learning for knowledge preservation, while there are some other continual learning approaches (e.g. Learning without Forgetting, Gradient Episodic Memory) that might be helpful. Also, we haven\u2019t explored alternative regularization approaches and light-weight tuning.\n2. On the monolingual case (i.e. on SLURP), despite getting much better result under fair comparison with the alternative training methods and other baselines based on wav2vec2, our result is only slightly better than the HuBERT-based generative approach (Wang et al., 2021b), which is the state-of-the-art before us. Very recently, CIF-PT (Dong et al., 2023), parallel with our work in time, reaches 1.9% higher than both types of models, marking a new state-of-the-arts. This approach appears to be orthogonal to ours and the two methods might be jointly applied to the SLU model to reach even better results, but this is left for future work.\n3. The dataset we built is relatively small and with limited number of real samples.\nEthics Statement\nWe honor the ACL Code of Ethics. Particularly, as our work involves data collection, we go through a\nformal process at the institution for collecting audio data, strictly follow the general and local rules for data protection, and receive full consent of participants to process and release the data. Since cross-lingual transfer is highlighted in our work, the work could have positive societal impacts for the application of speech and language technology in the non-English population. We believe that there is little chance for the method to be misused, except in cases of misusing SLU, such as mass surveillance. We also emphasize the reproducibility, and will release relevant code and models."
        },
        {
            "heading": "Acknowledgements",
            "text": "This work received funding under project SteADI, Swiss National Science Foundation grant 197479."
        },
        {
            "heading": "A Bayesian Transfer Learning",
            "text": "As in the standard machine learning configuration, we determine the parameters by optimizing loss with an L2 regularizer, i.e. minimizing L(D; \u03b8) + \u03b1\u2225\u03b8\u222522 for the parameters \u03b8 \u2208 RN given data D = {(x, y)} and the hyperparameter \u03b1, in which the cross-entropy loss L corresponds to the negative log-likelihood \u2212 log p(y|\u03b8) of the label upon model outputs. This can be formulated as maximum likelihood estimation (MLE) of \u03b8 by maximizing log p(\u03b8|D), which is equal to log p(D|\u03b8) + log p(\u03b8)\u2212 log p(D) by Bayes\u2019 theorem. With constant p(D) and a zero-mean isotropic Gaussian prior N (0, \u03c32I) on \u03b8 with scalar \u03c3, the optimization objective corresponds to\nlog p(\u03b8|D) \u221d log p(D|\u03b8) + log p(\u03b8) = log p(D|\u03b8) + log(N (\u03b8; 0, \u03c32I))\n\u221d \u2212L(D; \u03b8)\u2212 1 2\u03c32 N\u2211 i=1 \u03b82i\n(2) Hence L2 regularization can be viewed as giving an isotropic zero-mean Gaussian prior to the model parameters that assigns higher probability to closeto-zero parameters, with a larger \u03b1 indicating a smaller scalar \u03c32. While instead of zero, L2-SP (Li et al., 2018b) proposes to limit the parameter shift from the pretrained ones during fine-tuning by assigning a Gaussian prior N (\u03b80, \u03c32) centered at pretrained parameters \u03b80, which has been found to lead to better downstream performance.\nNevertheless, it is an over-simplification of the prior as different parameters are unequal and some parameters are more critical for the performance on the pretraining task than others. The importance of a parameter can be represented by posterior distribution p(\u03b8|Dp) near \u03b80 on the pretraining data Dp that corresponds to the pretraining loss L(Dp; \u03b8) \u221d p(Dp|\u03b8). In this way, elastic weight consolidation (EWC) (Kirkpatrick et al., 2017) assigns a Gaussian prior N (\u03b80, \u03c32I) with diagonal covariance \u03c3i according to the estimated posterior distribution (i.e. loss landscape) of \u03b8i on the pretraining task. A parameter \u03b8i with larger impact to the L(Dp; \u03b8) will have sharper p(\u03b8i|Dp) and smaller \u03c32i = 1/(\u03b1Fi), thus less flexibility in finetuning, lower variance in the fine-tuning prior, and higher weight for its L2 regularizer under the goal of preserving knowledge for the pretraining task.\nTo estimate this posterior distribution or loss landscape on the pretraining data Dp, we can\nperform Taylor expansion for the log likelihood log f(\u03b8) = log p(\u03b8|Dp) near the parameters after pretraining, namely \u03b80, which is assumed to be near to the optimum, making\u2207 log f(\u03b80) \u2248 0. Hence,\nlog f(\u03b8) = log f(\u03b80) +\u2207 log f(\u03b80)(\u03b8 \u2212 \u03b80)\n+ 1\n2 (\u03b8 \u2212 \u03b80)THlog f (\u03b80)(\u03b8 \u2212 \u03b80) + \u00b7 \u00b7 \u00b7\n\u2248 log f(\u03b80) + 1\n2 (\u03b8 \u2212 \u03b80)THlog f (\u03b80)(\u03b8 \u2212 \u03b80)\n(3)\nTherefore, through a second-order expansion, p(\u03b8|Dp) is approximated by a Gaussian distribution corresponding to the negation of the quadratic term above, with \u03b80 being the mean and the Hessian matrix corresponding to the inverse covariance. To estimate the Hessian matrix, we use Bayes\u2019 theorem and take a flat prior on Dp, forming\nHlog f (\u03b8) = \u22022 log p(\u03b8|Dp)\n\u2202\u03b82\n= \u22022 log p(Dp|\u03b8)\n\u2202\u03b82\n= Ex\u223cp(x|\u03b8)[ \u22022 log p(x|\u03b8)\n\u2202\u03b82 ]\n(4)\nWhile the Fisher information matrix can be written as\nF = \u2212Ex\u223cp(x|\u03b8)[ \u22022 log p(x|\u03b8)\n\u2202\u03b82 ], (5)\nTherefore, the posterior distribution of the parameter \u03b8 on the pretraining task is approximated by a Gaussian distribution with the mean \u00b5 = \u03b80 and the inverse covariance \u03a3\u22121 = F . The Fisher matrix can then be estimated by squared gradients as in Pascanu and Bengio (2014), and EWC further simplifies it by only considering diagonal terms.\nB Implementation Details\nWe pretrain the model following the common settings in the field on single 24GB V100 GPUs using the Adam optimizer with a learning rate schedule of 20k linear warmup steps from 0 to 1e-4, followed by an inverse-sqrt decay to 3e-5. Models are selected and early stopping is performed according to the WER or BLEU on the dev set. 5-beam search is used during evaluation. The PTLMs we use are the 24-layer \u201clarge\u201d versions provided by Hugging Face. A dynamic batching strategy is adopted to accommodate input utterance with different lengths. Accompanied with gradient accumulation, an average batch size of \u223c25 with \u223c500 target tokens\nper step is used. The wav2vec2 part is frozen for the first 10k steps, and utterances shorter than 0.1s or longer than 10s are not used during the first 20k steps. The L2 regularization with \u03b1=5e-3 is applied to the weights, except the Bayesian transfer learning experiments. The setting is similar for the fine-tuning cases except that the encoder is frozen during the initial steps, and for joint-training models a 1:3 ratio between data for the pretraining and target task is used. While for smaller datasets including MINDS-14, SLURP-Fr, and Spoken Gigawords, the data ratio, dropout rate, and learning rate schedule are further tuned to avoid overfitting. We also build and compare with several cascaded pipelines based on our ST model, for which we directly use the model outputs with beam search without external LM as the model already leverages a strong language model. More details could be found from the source code."
        },
        {
            "heading": "C Examples",
            "text": "Several examples in the SLURP IC benchmark and the predictions from different models are provided here in Table 8 for a more direct demonstration for the understanding capability of the models."
        },
        {
            "heading": "D Dataset Details",
            "text": "Three new datasets are introduced in this work. Among them, SLURP-Fr is our main dataset for experiments on cross-lingual transfer, while we additionally carry out a series of experiments on Spanish (Es) to show that our methods work on more than one language. For the synthetic portion of SLURP-Fr/Es, we built the dataset based on MASSIVE, textual translation of SLURP, each using 4 speakers from Google TTS, with total 11.3H and 13.9H audio respectively. Therefore the contents are identical to MASSIVE. While for the real portion of SLURP-Fr, we leverage two native French\nspeakers to read the held-out samples from MASSIVE. The dataset size and mean length (in seconds) of the utterances are given in Table 9.\nAs for speech summarization, we follow MTLSLT (Huang et al., 2022) to build the synthetic spoken version of Gigaword (Rush et al., 2015), using 9 speakers from Google TTS, with total 131.5H audio. We follow the data split in the original Gigaword dataset with a small and noisy test split (which we further filtered) and randomly sample from the train and dev split. The resultant size and mean length of the utterances are given in Table 10."
        },
        {
            "heading": "E Spanish Experiments",
            "text": "Similar to the default type of models using only data with English and French, we first introduce the Spanish data to the En+Fr ASR model or the En\u2194Fr ST model to pretrain a En+Fr+ES ASR model as well as a En\u2194Fr+En\u2194Es ST model, with satisfactory results as in Table 11. Both ASR and ST models are then fine-tuned with joint training on SLURP, reaching 87.63% and 89.59% accuracy respectively. Then they are used for cross-lingual\ntransfer to SLURP-Es."
        },
        {
            "heading": "F EWC Weight Distribution",
            "text": "The distributions of the log estimated Fisher diagonals for each weight matrix or bias vector are illustrated in Figure 3. It can be observed that most weights are concentrated around 1e-5 to 1e-10, and they are close to each other as the standard deviations are at the similar magnitude. Hence with \u03b1=1e-7, most weights will reach the 1e-2 clamping threshold. The exceptions are the biases for key projection in attention modules, which correspond to the lower-left cluster and have much smaller weights."
        }
    ],
    "title": "The Interpreter Understands Your Meaning: End-to-end Spoken Language Understanding Aided by Speech Translation",
    "year": 2023
}