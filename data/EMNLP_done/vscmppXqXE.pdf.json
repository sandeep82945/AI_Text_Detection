{
    "abstractText": "Human experts write summaries using different techniques, including extracting a sentence from the document and rewriting it, or fusing various information from the document to abstract it. These techniques are flexible and thus difficult to be imitated by any single method. To address this issue, we propose an adaptive model, GEMINI, that integrates a rewriter and a generator to mimic the sentence rewriting and abstracting techniques, respectively. GEMINI adaptively chooses to rewrite a specific document sentence or generate a summary sentence from scratch. Experiments demonstrate that our adaptive approach outperforms the pure abstractive and rewriting baselines on three benchmark datasets, achieving the best results on WikiHow. Interestingly, empirical results show that the human summary styles of summary sentences are consistently predictable given their context. We release our code and model at https: //github.com/baoguangsheng/gemini.",
    "authors": [
        {
            "affiliations": [],
            "name": "Guangsheng Bao"
        },
        {
            "affiliations": [],
            "name": "Zebin Ou"
        },
        {
            "affiliations": [],
            "name": "Yue Zhang"
        }
    ],
    "id": "SP:6f9866006de671ac190e3411a8df99d46d3a844a",
    "references": [
        {
            "authors": [
                "Mehdi Allahyari",
                "Seyedamin Pouriyeh",
                "Mehdi Assefi",
                "Saeid Safaei",
                "Elizabeth D Trippe",
                "Juan B Gutierrez",
                "Krys Kochut."
            ],
            "title": "Text summarization techniques: A brief survey",
            "venue": "International Journal of Advanced Computer Science and Applications (ijacsa),",
            "year": 2017
        },
        {
            "authors": [
                "Jimmy Lei Ba",
                "Jamie Ryan Kiros",
                "Geoffrey E Hinton."
            ],
            "title": "Layer normalization",
            "venue": "arXiv preprint arXiv:1607.06450.",
            "year": 2016
        },
        {
            "authors": [
                "Sanghwan Bae",
                "Taeuk Kim",
                "Jihoon Kim",
                "Sanggoo Lee."
            ],
            "title": "Summary level training of sentence rewriting for abstractive summarization",
            "venue": "Proceedings of the 2nd Workshop on New Frontiers in Summarization, pages 10\u201320, Hong Kong, China. Asso-",
            "year": 2019
        },
        {
            "authors": [
                "Guangsheng Bao",
                "Yue Zhang."
            ],
            "title": "Contextualized rewriting for text summarization",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 12544\u201312553.",
            "year": 2021
        },
        {
            "authors": [
                "Guangsheng Bao",
                "Yue Zhang."
            ],
            "title": "A general contextualized rewriting framework for text summarization",
            "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing.",
            "year": 2023
        },
        {
            "authors": [
                "Regina Barzilay",
                "Kathleen R McKeown."
            ],
            "title": "Sentence fusion for multidocument news summarization",
            "venue": "Computational Linguistics, 31(3):297\u2013328.",
            "year": 2005
        },
        {
            "authors": [
                "Yen-Chun Chen",
                "Mohit Bansal."
            ],
            "title": "Fast abstractive summarization with reinforce-selected sentence rewriting",
            "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 675\u2013686, Melbourne,",
            "year": 2018
        },
        {
            "authors": [
                "Jianpeng Cheng",
                "Maria Lapata."
            ],
            "title": "Neural summarization by extracting sentences and words",
            "venue": "54th Annual Meeting of the Association for Computational Linguistics, pages 484\u2013494. Association for Computational Linguistics.",
            "year": 2016
        },
        {
            "authors": [
                "Zi-Yi Dou",
                "Pengfei Liu",
                "Hiroaki Hayashi",
                "Zhengbao Jiang",
                "Graham Neubig."
            ],
            "title": "Gsum: A general framework for guided neural abstractive summarization",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association",
            "year": 2021
        },
        {
            "authors": [
                "Sebastian Gehrmann",
                "Yuntian Deng",
                "Alexander M Rush."
            ],
            "title": "Bottom-up abstractive summarization",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4098\u20134109.",
            "year": 2018
        },
        {
            "authors": [
                "Tanya Goyal",
                "Nazneen Rajani",
                "Wenhao Liu",
                "Wojciech Kry\u015bci\u0144ski."
            ],
            "title": "Hydrasum: Disentangling style features in text summarization with multidecoder models",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language",
            "year": 2022
        },
        {
            "authors": [
                "Max Grusky",
                "Mor Naaman",
                "Yoav Artzi"
            ],
            "title": "Newsroom: A dataset of 1.3 million summaries with diverse extractive strategies",
            "venue": "In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics:",
            "year": 2018
        },
        {
            "authors": [
                "Karl Moritz Hermann",
                "Tom\u00e1s Kocisk\u00fd",
                "Edward Grefenstette",
                "Lasse Espeholt",
                "Will Kay",
                "Mustafa Suleyman",
                "Phil Blunsom."
            ],
            "title": "Teaching machines to read and comprehend",
            "venue": "Advances in Neural Information Processing Systems 28: Annual Conference on Neu-",
            "year": 2015
        },
        {
            "authors": [
                "Dandan Huang",
                "Leyang Cui",
                "Sen Yang",
                "Guangsheng Bao",
                "Kun Wang",
                "Jun Xie",
                "Yue Zhang"
            ],
            "title": "What have we achieved on text summarization",
            "venue": "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
            "year": 2020
        },
        {
            "authors": [
                "Robert A Jacobs",
                "Michael I Jordan",
                "Steven J Nowlan",
                "Geoffrey E Hinton."
            ],
            "title": "Adaptive mixtures of local experts",
            "venue": "Neural computation, 3(1):79\u201387.",
            "year": 1991
        },
        {
            "authors": [
                "Hongyan Jing",
                "Kathleen R McKeown."
            ],
            "title": "The decomposition of human-written summary sentences",
            "venue": "Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval, pages 129\u2013136.",
            "year": 1999
        },
        {
            "authors": [
                "Mahnaz Koupaee",
                "William Yang Wang."
            ],
            "title": "Wikihow: A large scale text summarization dataset",
            "venue": "arXiv preprint arXiv:1810.09305.",
            "year": 2018
        },
        {
            "authors": [
                "Mike Lewis",
                "Yinhan Liu",
                "Naman Goyal",
                "Marjan Ghazvininejad",
                "Abdelrahman Mohamed",
                "Omer Levy",
                "Veselin Stoyanov",
                "Luke Zettlemoyer"
            ],
            "title": "BART: Denoising sequence-to-sequence pre-training for natural language",
            "year": 2020
        },
        {
            "authors": [
                "Chin-Yew Lin."
            ],
            "title": "Rouge: A package for automatic evaluation of summaries",
            "venue": "Text summarization branches out, pages 74\u201381.",
            "year": 2004
        },
        {
            "authors": [
                "Yang Liu",
                "Mirella Lapata."
            ],
            "title": "Text summarization with pretrained encoders",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing",
            "year": 2019
        },
        {
            "authors": [
                "Yixin Liu",
                "Zi-Yi Dou",
                "Pengfei Liu."
            ],
            "title": "Refsum: Refactoring neural summarization",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1437\u20131448.",
            "year": 2021
        },
        {
            "authors": [
                "Yixin Liu",
                "Pengfei Liu."
            ],
            "title": "Simcls: A simple framework for contrastive learning of abstractive summarization",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference",
            "year": 2021
        },
        {
            "authors": [
                "Yixin Liu",
                "Pengfei Liu",
                "Dragomir Radev",
                "Graham Neubig."
            ],
            "title": "Brio: Bringing order to abstractive summarization",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2890\u20132903.",
            "year": 2022
        },
        {
            "authors": [
                "Mani Maybury."
            ],
            "title": "Advances in automatic text summarization",
            "venue": "MIT press.",
            "year": 1999
        },
        {
            "authors": [
                "Joshua Maynez",
                "Shashi Narayan",
                "Bernd Bohnet",
                "Ryan McDonald."
            ],
            "title": "On faithfulness and factuality in abstractive summarization",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1906\u20131919.",
            "year": 2020
        },
        {
            "authors": [
                "Ramesh Nallapati",
                "Feifei Zhai",
                "Bowen Zhou."
            ],
            "title": "Summarunner: A recurrent neural network based sequence model for extractive summarization of documents",
            "venue": "Thirty-First AAAI Conference on Artificial Intelligence.",
            "year": 2017
        },
        {
            "authors": [
                "Ramesh Nallapati",
                "Bowen Zhou",
                "Cicero dos Santos",
                "\u00c7a\u011flar Gul\u00e7ehre",
                "Bing Xiang."
            ],
            "title": "Abstractive text summarization using sequence-to-sequence rnns and beyond",
            "venue": "Proceedings of The 20th SIGNLL Conference on Computational Natural Language",
            "year": 2016
        },
        {
            "authors": [
                "Shashi Narayan",
                "Shay B Cohen",
                "Mirella Lapata."
            ],
            "title": "Don\u2019t give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Lan-",
            "year": 2018
        },
        {
            "authors": [
                "Ani Nenkova",
                "Kathleen McKeown."
            ],
            "title": "A survey of text summarization techniques",
            "venue": "Mining text data, pages 43\u201376. Springer.",
            "year": 2012
        },
        {
            "authors": [
                "Richard Yuanzhe Pang",
                "He He."
            ],
            "title": "Text generation by learning from demonstrations",
            "venue": "International Conference on Learning Representations.",
            "year": 2020
        },
        {
            "authors": [
                "Mathieu Ravaut",
                "Shafiq Joty",
                "Nancy Chen."
            ],
            "title": "Summareranker: A multi-task mixture-of-experts reranking framework for abstractive summarization",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume",
            "year": 2022
        },
        {
            "authors": [
                "Alexander M Rush",
                "Sumit Chopra",
                "Jason Weston."
            ],
            "title": "A neural attention model for abstractive sentence summarization",
            "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 379\u2013389.",
            "year": 2015
        },
        {
            "authors": [
                "Abigail See",
                "Peter J. Liu",
                "Christopher D. Manning."
            ],
            "title": "Get to the point: Summarization with pointergenerator networks",
            "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1073\u2013",
            "year": 2017
        },
        {
            "authors": [
                "Oriol Vinyals",
                "Meire Fortunato",
                "Navdeep Jaitly."
            ],
            "title": "Pointer networks",
            "venue": "Advances in Neural Information Processing Systems, 28:2692\u20132700.",
            "year": 2015
        },
        {
            "authors": [
                "Liqiang Xiao",
                "Lu Wang",
                "Hao He",
                "Yaohui Jin."
            ],
            "title": "Copy or Rewrite: Hybrid Summarization with Hierarchical Reinforcement Learning",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence (AAAI).",
            "year": 2020
        },
        {
            "authors": [
                "Jingqing Zhang",
                "Yao Zhao",
                "Mohammad Saleh",
                "Peter Liu."
            ],
            "title": "Pegasus: Pre-training with extracted gap-sentences for abstractive summarization",
            "venue": "International Conference on Machine Learning, pages 11328\u201311339. PMLR.",
            "year": 2020
        },
        {
            "authors": [
                "Ming Zhong",
                "Pengfei Liu",
                "Yiran Chen",
                "Danqing Wang",
                "Xipeng Qiu",
                "Xuan-Jing Huang."
            ],
            "title": "Extractive summarization as text matching",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6197\u20136208.",
            "year": 2020
        },
        {
            "authors": [
                "Qingyu Zhou",
                "Nan Yang",
                "Furu Wei",
                "Shaohan Huang",
                "Ming Zhou",
                "Tiejun Zhao."
            ],
            "title": "A joint sentence scoring and selection framework for neural extractive document summarization",
            "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing, 28:671\u2013",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Text summarization aims to automatically generate a fluent and succinct summary for a given text document (Maybury, 1999; Nenkova and McKeown, 2012; Allahyari et al., 2017). Two dominant methods have been used, namely extractive and abstractive summarization techniques. Extractive methods (Nallapati et al., 2017; Narayan et al., 2018; Liu and Lapata, 2019; Zhou et al., 2020; Zhong et al., 2020) identify salient text pieces from the input and assemble them into an output summary, leading to faithful but possibly redundant and incoherent summaries (Chen and Bansal, 2018; Gehrmann et al., 2018; Cheng and Lapata, 2016), where rewriting techniques could be used to further reduce redundancy and increase coherence (Bae et al., 2019; Bao and Zhang, 2021). In contrast, abstractive methods (Rush et al., 2015; Nallapati et al., 2016;\n\u2217* Corresponding author.\nSee et al., 2017; Lewis et al., 2020) apply natural language generation (NLG) technologies in synthesizing the output, which obtains more concise and coherent summaries, but at the cost of degraded faithfulness (Huang et al., 2020; Maynez et al., 2020).\nThe effectiveness of extractive and abstractive methods relies on the summary style of summaries. Study shows that human take different styles in writing each summary sentence (Jing and McKeown, 1999), which we categorize broadly into extractive and abstractive styles. Extractive style mainly conveys ideas directly from article sentences, while abstractive style conveys new ideas that are entailed from various article sentences. One example is shown in Figure 1, where the summary consists of two sentences. The first sentence is generated by rewriting sentence <S2> from the document. In contrast, the second summary sentence is generated by abstracting various sentences. These styles are flexible and thus difficult to be imitated by a single method.\nIn this paper, we aim to mimic human summary styles, which we believe can increase our ability to control the style and deepen our understanding of how human summaries are produced. To better adapt to summary styles in human summaries, we propose an adaptive model, GEMINI, which\ncontains a rewriter and a generator to imitate the extractive style and the abstractive style, respectively. For the rewriter, we adopt contextualized rewriting (Bao and Zhang, 2021) integrated with an inner sentence extractor. For the generator, we use standard seq2seq summarizer (Lewis et al., 2020).\nThe rewriter and the generator are integrated into a single decoder, using a style controller to switch the style of a generation. The style controller assigns different group tags so that relevant sentences in the input can be used to guide the decoder. In the abstractive style, the decoder does not focus on a specific sentence, while in the extractive style, the decoder is guided by attending more to a particular rewritten sentence. In order to train such an adaptive summarizer, we generate oracle extractive/abstractive styles using automatic detection of sentence-level summary styles.\nWe evaluate our model on three representative benchmark datasets. Results show that GEMINI allows the model to better fit training data thanks to differentiating summary styles. Our adaptive rewriter-generator network outperforms the strong abstractive baseline and recent rewriter models with a significant margin on the benchmarks. Interestingly, experiments also show that the summary style of a sentence can be consistently predicted during test time, which shows that there is underline consistency in the human choice of summary style of a summary sentence in context. To our knowledge, we are the first to explicitly control the style per summary sentence, and at the same time, achieve improved ROUGE scores. Our automatic style detection metric allows further quantitative analysis of summary styles in the future."
        },
        {
            "heading": "2 Related Work",
            "text": "Abstractive summarizers achieve competitive results by fine-tuning on pre-trained seq2seq model (Lewis et al., 2020; Zhang et al., 2020). Our work is in line, with the contribution of novel style control over the summary sentence, so that a model can better fit human summaries in the \u201csentence rewriting\u201d and \u201clong-range abstractive\u201d styles. Our generator is a standard seq2seq model with the same architecture as BART (Lewis et al., 2020), and our rewriter is related to previous single sentence rewriting (Chen and Bansal, 2018; Bae et al., 2019; Xiao et al., 2020) and contextualized rewriting (Bao and Zhang, 2021).\nOur rewriter uses the contextualized rewriting\nmechanism, which considers the document context of each rewritten sentence, so that important information from the context can be recalled and crosssentential coherence can be maintained. However, different from Bao and Zhang (2021), which relies on an external extractor to select sentences, we integrate an internal extractor using the pointer mechanism (Vinyals et al., 2015), analogous to NeuSum (Zhou et al., 2020) to select sentences autoregressively. To our knowledge, we are the first to integrate a rewriter and a generator into a standalone abstractive summarizer.\nOur model GEMINI can also be seen as a mixture of experts (Jacobs et al., 1991), which dynamically switches between a rewriter and a generator. A related work is the pointer-generator network (See et al., 2017) for seq2seq modeling, which can also viewed as a mixture-of-expert model. Another related work is the HYDRASUM, which has two experts for decoding. These models can be viewed as a soft mixture of experts, which learns latent experts and makes decisions by integrating their outputs. In contrast, our model can be viewed as a hard mixture of experts, which consults either the rewriter or the generator in making a decision. A salient difference between the two models is that our GEMINI makes decisions for each sentence, while previous work makes decisions at the token level. The goals of the models are very different."
        },
        {
            "heading": "3 Summary Style at Sentence Level",
            "text": ""
        },
        {
            "heading": "3.1 Human Evaluation of Summary Style",
            "text": "We conduct a human evaluation of the summary style of each summary sentence, annotating as extractive style if the summary sentence can be implied by one of the article sentences, and abstractive style if the summary sentence requires multiple article sentences to imply. We sample 100 summary sentences for each dataset and ask three annotators to annotate them, where we take the styles receiving at least two votes as the final labels.\nThe averaged distributions of styles are shown in Table 1. CNN/DM is mostly extractive style, which has 88.6% summary sentences written in the extractive style. In contrast, XSum is mostly\nabstractive style, which has 88.2% summary sentences written in the abstractive style. WikiHow has a more balanced distribution of summary styles, with about 60% summary sentences in the extractive style. The results suggest that real summaries are a mixture of styles, even for the well-known extractive dataset CNN/DM and abstractive dataset XSum."
        },
        {
            "heading": "3.2 Automatic Detection of Summary Style",
            "text": "Because of the high cost of human annotation, we turn to automatic approaches to detect the summary styles at the sentence level. Previous studies mainly measure the summary style at the summary level. For example, Grusky et al. (2018) propose the coverage and density of extractive fragments to measure the extractiveness. See et al. (2017) propose the proportion of novel n-grams, which appear in the summary but not in the input document, as indicators of abstractiveness. We adopt these metrics to sentences and take them as baselines of our approach.\nWe propose fusion index to measure the degree of fusing context information (Barzilay and McKeown, 2005), considering two factors: 1) How much information from the summary sentence can be recalled from one document sentence? If the recall is high, the sentence is more likely to be extractive style because it can be produced by simply rewriting the document sentence. 2) How many document sentences does the summary sentence relate to? A larger number of such sentences indicates a higher fusion degree, where the summary sentence is more likely to be abstractive style. Our fusion index is calculated on these two factors.\nRecall. We measure the percentage of recallable information by matching the summary sentence back to document sentences. Given a summary sentence S and a source document D = {S1, S2, ..., S|D|}, we find the best-match\nRC(S | D) = max 1\u2264i\u2264|D| R(S | Si), (1)\nwhere R(S|Si) is an average of ROUGE-1/2/L recalls of S given Si, representing the percentage of information of sentence S covered by sentence Si.\nScatter. We measure the scattering of the content of a summary sentence by matching the sentence to all document sentences. If the matching scores are equally distributed on all document sentences, the scatter is high; If the matching scores are all zero except for one document sentence, the scatter is low. We calculate the scatter using the distribution entropy derived from the matching scores.\nSC(S | D) = \u2212 K\u2211\nj=1\npj log pj/ logK, (2)\nwhere pj is the estimated probability whether S is generated from the corresponding document sentence Si, calculated using the top-K best-matches {rj}|Kj=1 that\npj = rj/ K\u2211 j=1 rj ,\n{rj}|Kj=1 = Top({R(S|Si) | 1 \u2264 i \u2264 |D|},K).\n(3)\nThe hyper-parameter K is determined using an empirical search on the human evaluation set.\nFusion index. We calculate fusion index (FI) from Recall (RC) and Scatter (SC)\nFI(S | D) = (1\u2212RC(S | D)) \u2217 SC(S | D), (4)\nwhich represents the degree of fusion, where 0 means no fusion, 1 means extreme fusion, and others between.\nWe evaluate our metrics together with the candidates from previous studies, reporting the Pearson correlation with human-annotated summary styles as shown in Table 2. Our proposed fusion index gives the best correlation with summary styles. In contrast, among previous metrics, only the novel 1-gram has the closest correlation but is still lower than the fusion index by about 0.14 on average. The results suggest that the fusion index is a more suitable extractive-abstractive measure at the sentence level."
        },
        {
            "heading": "3.3 Oracle Label for Summary Style",
            "text": "We produce oracle extractive/abstractive labels using automatic fusion index so that we can train a\nsummarizer with explicit style control at the sentence level. If the fusion index is higher than a threshold, we treat the sentence as extractive style. If the fusion index of the summary sentence is lower than the threshold, we treat the sentence as abstractive style. We search for the best threshold for each dataset using development experiments."
        },
        {
            "heading": "4 GEMINI: Rewriter-Generator Network",
            "text": "As Figure 2 shows, GEMINI adopts pre-trained BART (Lewis et al., 2020), using a style controller to decide the summary style. According to the style, either the rewriter or the generator is activated to generate each sentence.\nIntuitively, GEMINI performs the best when it works on a dataset with a balanced style, which enables the rewriter and generator to complement each other, and when the oracle styles are of high accuracy so that the training supervision can be of high quality."
        },
        {
            "heading": "4.1 Input and Output",
            "text": "We introduce special identifier tokens, including \u201c<S>\u201d - the start of a document, \u201c<Sk>\u201d - the start of a sentence k, and \u201c</S>\u201d - the end of a sentence.\nWe express the input document as \u201c<S> <S1> sentence one . </S> <S2> sentence two . </S> <S3> sentence three . </S> ...\u201d, where the sequence starts with the identifier token \u201c<S>\u201d and enclose each sentence with \u201c<Sk>\u201d and \u201c</S>\u201d. We express the output summary as \u201c<S2> sentence one . </S> <S> sentence two . </S> ...\u201d, where each sentence starts with \u201c<Sk>\u201d or \u201c<S>\u201d and ends with \u201c</S>\u201d. If a summary sentence starts with \u201c<Sk>\u201d, the decoder will generate the sentence according to the k-th document sentence. If a summary sentence starts with \u201c<S>\u201d, the decoder will generate the sentence according to the whole document."
        },
        {
            "heading": "4.2 Document Encoder",
            "text": "We extend the embedding table to include the identifier tokens so that the style controller can match their embeddings to decide the summary style. We follow contextualized rewriting (Bao and Zhang, 2021) to allocate a group tag to each input sentence so that the rewriter can rely on these group tags to locate the rewritten sentence using multi-head attention, as the group tags 1\u20dd 2\u20dd 3\u20dd in Figure 2 illustrate.\nSpecifically, the first summary sentence and the second document sentence have the same group\ntag of 2\u20dd, which are converted to group-tag embeddings and added to the token embeddings in the sentence. Using a shared group-tag embedding table between the encoder and decoder, the decoder can be trained to concentrate on the second document sentence when generating the first summary sentence.\nFormally, we express the group-tag embedding table as EMBtag and generate a group-tag sequence GX uniquely from X according to the index of each sentence as\nGX = {k if wi \u2208 Sk else 0}| |X| i=1, (5)\nwhere for a token wi in the k-th sentence Sk we assign a group tag of number k. We convert GX into embeddings and inject it in the BART encoder.\nWe take the standard Transformer encoding layers, which contain a self-attention module and a feed-forward module:\nx(l) = LN(x(l\u22121) + SELFATTN(x(l\u22121))), x(l) = LN(x(l) + FEEDFORWARD(x(l))), (6)\nwhere LN denotes layer normalization (Ba et al., 2016). The last layer L outputs the final encoder output xout = x(L). The vectors xemb and xout are passed to the decoder for prediction."
        },
        {
            "heading": "4.3 Summary Decoder",
            "text": "We extend the BART decoder with a style controller and a rewriter, while for the generator, we use the default decoder.\nStyle Controller. We use attention pointer (Vinyals et al., 2015) to predict the styles, where we weigh each input sentence (attention on \u201c<Sk>\u201d) or the whole document (attention on \u201c<S>\u201d). If \u201c<S>\u201d receives the largest attention score, we choose the abs style, and if <Sk> receives the most attention score, we choose the ext style.\nAt the beginning of the summary or at the end of a summary sentence, we predict the style of the next summary sentence. We match the token output to the encoder outputs to decide the selection.\nymatch = yout \u00d7 (xout \u2217 \u03b1+ xemb \u2217 (1\u2212 \u03b1))T , (7)\nwhere \u03b1 is a trainable scalar to mix the encoder outputs xout and token embeddings xemb. We match these mix embeddings to the decoder outputs yout, obtaining the logits ymatch of the pointer distribution. We only keep the logits for sentence identifiers, including \u201c<S>\u201d and \u201c<Sk>\u201d, and predict the distribution using a softmax function.\nRewriter and Generator. We use the standard decoder as the backbone for both the rewriter and the generator. For the rewriter, we follow Bao and Zhang (2021) to apply group-tag embeddings to the input of the decoder. For the generator, we do not apply group-tag embeddings, so that it does not correspond to any document sentence.\nFormally, given summary Y = {wj}||Y |j=1, we generate GY uniquely from Y according to the identifier token \u201c<Sk>\u201d and \u201c<S>\u201d that for each token wi in the sentence started with \u201c<Sk>\u201d the group-tag is k and for that with \u201c<S>\u201d the grouptag is 0. For instance, if Y is the sequence \u201c<S2> w1 w2 </S> <S> w3 w4 </S> <S7> ...\u201d, GY will be {2, 2, 2, 2, 0, 0, 0, 0, 7, ...}.\nWe do not change the decoding layers, which contain a self-attention module, a cross-attention module, and a feed-forward module for each:\ny(l) = LN(y(l\u22121) + SELFATTN(y(l\u22121))),\ny(l) = LN(y(l) + CROSSATTN(y(l), xout)),\ny(l) = LN(y(l) + FEEDFORWARD(y(l))),\n(8)\nwhere LN denotes layer normalization (Ba et al., 2016). The last layer L outputs the final decoder output yout. The decoder output yout is then matched with token embeddings for predicting the next tokens."
        },
        {
            "heading": "4.4 Training and Inference",
            "text": "We use MLE loss for both token prediction and style prediction. We calculate the overall loss as\nL = Ltoken + \u03ba \u2217 Lstyle, (9)\nwhere Lstyle is the MLE loss of the style prediction, and Ltoken is the MLE loss of the token prediction. Because of the different nature between style and\ntoken predictions, we use a hyper-parameter \u03ba to coordinate their convergence speed of them. In practice, we choose \u03ba according to the best performance on the development set.\nDuring inference, the style controller is activated first to decide on an ext/abs style. If it chooses the ext style, the matched sentence identifier \u201c<Sk>\u201d will be used to generate group tags for the following tokens. If it chooses the abs style, we make the tokens with a special group tag of 0."
        },
        {
            "heading": "5 Experimental Settings",
            "text": "We use three English benchmark datasets representing different styles and domains as shown in Table 3.\nCNN/DailyMail (Hermann et al., 2015) is the most popular single-document summarization dataset, comprising online news articles and human written highlights.\nXSum (Narayan et al., 2018) is an abstractive style summarization dataset built upon new articles with a one-sentence summary written by professional authors.\nWikiHow (Koupaee and Wang, 2018) is a diverse summarization dataset extracted from the online knowledge base WikiHow, which is written by human authors.\nTo generate oracle styles, we use fusion index thresholds of \u03b3 = 0.7, \u03b3 = 0.7, and \u03b3 = 0.3 for CNN/DM, XSum, and WikiHow, respectively.\nWe train GEMINI using a two-stage strategy: pre-finetuning the new parameters and then joint fine-tuning all the parameters. Since we introduce additional structure and new parameters to the pre-trained BART, direct joint fine-tuning of the two types of parameters can cause a downgrade of the pre-trained parameters. We introduce prefinetuning to prepare the random-initialized parameters by freezing the pre-trained parameters and fine-tuning for 8 epochs before the joint fine-tuning. We use the same MLE loss for the two stages, using a convergence coordination parameter of \u03ba = 1.1."
        },
        {
            "heading": "6 Results",
            "text": ""
        },
        {
            "heading": "6.1 Automatic Evaluation",
            "text": "As Table 4 shows, we evaluate our model on three benchmark datasets, in comparison with other BART-based models in fair settings. We report automatic metric ROUGE-1/2/L (Lin, 2004).\nCompared to the abstractive BART baseline, GEMINI improves the ROUGE scores by an average of 1.01, 0.48, and 1.25 on CNN/DM, XSum, and WikiHow, respectively. The improvements on\nROUGE-L of CNN/DM and ROUGE-2 of WikiHow are especially significant, reaching 1.44 and 1.56, respectively. The results suggest that the adaptive approach has an obvious advantage over the pure abstractive model.\nCompared with the rewriter baseline BARTRewriter, which relies on an external extractor to provide rewriting sentences, GEMINI improves the ROUGE-1/L scores by about 1.0 on CNN/DM, demonstrating the effectiveness of the adaptive approach compared with the pure rewriter. Compared with the HYDRASUM, which expresses the summary styles implicitly using a mixture of experts but obtains lower ROUGE scores than BART baseline, GEMINI achieves improved ROUGE scores using an explicit style control. The results demonstrate the advantage of a model with adaptive styles over pure rewriter and implicit style control.\nTable 4 lists additional recent work using larger pre-training, assembling, reranking, and reinforcement learning techniques. However, these models are not directly comparable with our model. For example, PEGASUS (large) has 568M parameters, outnumbering BART (large) 400M by 42%. BRIO takes 20 hours per epoch and a total of 15 epochs to train on CNN/DM using 4 NVIDIA RTX 3090 GPUs, while our model only takes 2 hours per epoch and a total of 11 epochs to train on the same device, using only 7% computing resources of BRIO. We do not take these models as our baselines because they are in different lines of work using techniques perpendicular to this study. More importantly, we focus on the sentence-level summary style and its control instead of only ROUGE improvement."
        },
        {
            "heading": "6.2 Human Evaluation",
            "text": "We conduct a human evaluation to quantitatively measure the quality of generated summaries. We compare GEMINI with BART and BART-Rewriter baselines, studying four qualities, including informativeness, conciseness, readability, and faithfulness. We follow recent HYDRASUM (Goyal et al., 2022) and SummaReranker (Ravaut et al.,\n2022) to sample 50 documents from the test set of CNN/DM for the evaluation. We hire three graduate students with professional English proficiency (TOEFL scores above 100 out of 120) to annotate each candidate summary from 1 (worst) to 5 (best), and report the average scores across the three annotators.\nAs shown in Table 5, GEMINI achieves the best score overall, especially on conciseness and readability. Compared to BART-Rewriter, GEMINI gives close scores on informativeness and faithfulness, but higher scores on conciseness and readability. Compared to BART, GEMINI obtains better scores on the four metrics.\nThe explicit style control of GEMINI plays the role of a rough planner, which restricts the generation of content. Empirically, it generates summaries with a smaller number of sentences (3.3 sents/summary) than BART (3.9 sents/summary) and BART-Rewriter (3.7 sents/summary) on CNN/DM. As a result, GEMINI produces more concise summaries with lengths 20% shorter than BART and 10% shorter than BART-Rewriter but still obtains higher n-gram recalls as the higher ROUGE-1/2 in Table 4 shows, suggesting that the summaries generated by GEMINI tend to have denser information. We speculate the high readability of GEMINI is a result of its auto-regressive modeling of style prediction and sentence generation, through which the style transitions are optimized. We list two cases in Appendix A to illustrate the advantage of GEMIN on conciseness and readability."
        },
        {
            "heading": "6.3 Ablation Study",
            "text": "Rewriter vs. Generator. We further study our adaptive model by observing the contribution of the rewriter and the generator individually. We categorize the test samples in CNN/DM into 10 buckets according to the average fusion index, obtaining an averaged ROUGE-1/2/L score per bucket. We con-\ntrast the ROUGE scores of the rewriter (rwt) and the generator (gen) as Figure 3. We can see that the rewriter dominates the performance on the region of low fusion index, while the generator rules the region of high fusion index. The distribution of ROUGE scores for the rewriter and the generator illustrates the specialization of the two styles.\nPre-finetuning. Pre-finetuning changes the distribution of randomly-initialized parameters. Take GEMINI on CNN/DM as an example. The initial average norms of embeddings for sentence identifiers and group tags are both 0.06, which are set purposely to a small value to reduce the negative impact on the pre-trained network. If we fine-tune the model directly, the trained model has average norms of 0.44 and 0.17 for sentence identifiers and group tags, respectively. However, if we prefinetune the model for 8 epochs, the average norms climb to 0.92 and 0.63, respectively. After a followup fine-tuning, the average norms converge to 0.66 and 0.50, respectively, which are much higher than the model fine-tuned directly.\nIf we remove pre-finetuning, the performance of GEMINI on CNN/DM decreases from 45.27, 21.77, and 42.34 to 44.76, 21.60, and 41.71, respectively, on ROUGE-1/2/L, suggesting the necessity of the pre-finetuning stage."
        },
        {
            "heading": "7 Analysis",
            "text": "We try to answer two key questions related to sentence-level summary style control as follows."
        },
        {
            "heading": "7.1 Are sentence-level summary styles predictable?",
            "text": "We use oracle styles to train our model, which allows the model to be more easily trained. However, during testing, the summary style of each sentence\ncan be arbitrarily chosen. It remains an interesting research question whether the style in the test set is predictable to some extent.\nWe try to answer this question using a quantitative analysis of style distribution on CNN/DM. We obtain an F1 of 0.78 for the style prediction, where more detailed distributions are shown as the upper sub-figures in Figure 4. The distribution of the predicted styles matches that of the oracle styles, except on a narrow range among sentence positions above 15, where the predicted abs style has less possibility than the oracle abs style. For the distribution of style transitions, as the under sub-figures show, the transitions ext-to-abs and abs-to-ext show some difference between the prediction and the oracle, where the predicted style transitions are overall less frequent than the oracle style transitions but with the same trends. The figures demonstrate the consistency in the distribution of predicted and oracle styles, suggesting that the styles are predictable to a certain degree.\nWe further evaluate the contribution of predicted styles by comparing the performance with the GEMINI decoding using randomly selected styles and oracle styles. As shown in Table 6, when we replace the predicted styles with random styles, the performance decreases by an average of 2.58 ROUGE points on CNN/DM and by an average of 1.26 on WikiHow. The performance drop indicates that the model\u2019s predictions of styles provide useful information to generate high-quality summaries. The prediction ability comes from the understanding of the input document. This also suggests that there is a consistent tendency in choosing the summary style of a summary sentence given its existing context."
        },
        {
            "heading": "7.2 When will the adaptive model perform the best?",
            "text": "Intuitively, GEMINI works best on a dataset with a balanced style so that the rewriter and generator complement each other. More importantly, the oracle styles need to be accurate to obtain highquality supervision signals.\nFirst, the distribution of the dataset matters. Among the three datasets, WikiHow has the most balanced style which has 61.1% of the summary sentences preferring a rewriter. As a result, GEMINI obtains the most performance improvement on WikiHow by an average of 1.25 ROUGE points compared to the pure abstractive baseline, confirming our intuition about the relation between dataset distribution and model performance.\nSecond, complementary rewriter and generator are the essential prerequisites. As Figure 3 shows, the rewriter and generator on CNN/DM have relatively balanced capabilities, when the fusion index is below 0.55 the rewriter is preferred and above 0.55 the generator is preferred. In comparison, as shown in Figure 5, the rewriter and generator on WikiHow have skewed capabilities, where the rewriter is weak and only preferred when the fusion index is below 0.15. Consequently, GEMINI only generates ext-style for 19.3% of the summary sentences in contrast to the human assessment of 61.1%. The analysis suggests that GEMINI on WikiHow could be further enhanced by using a better rewriter, which could potentially be realized by using an improved sentence extractor.\nLast, the quality of oracle styles determines the specialization of the two generators. As shown in Table 2, the Pearson correlation of the fusion index on WikiHow is only 0.56, which is much less than 0.76 on CNN/DM. It suggests a further improvement space on oracle styles by developing better automatic metrics."
        },
        {
            "heading": "8 Discussion",
            "text": "In this paper, we simulate human summary styles at the sentence level and provide a better understanding of how human summaries are produced. Based on this understanding, researchers could use GEMINI for different purposes. First, the automatic metric Fusion Index could be used to analyze the style distribution when developing a new dataset. Second, GEMINI could be used to control summary style at the sentence level. Since the ext-style summary sentences are naturally less likely to hallucinate than abs-style summary sentences, we could control the faithful risks by determining the proportion of abs-style summary sentences. Last, GEMINI produces an explicit style for each summary sentence, so we could even warn the faithful risks by marking those abs-style summary sentences in critical applications.\nLimitations. GEMINI can fit the summarization style of a specific dataset so that the rewriter and generator specialize in different situations, which may enhance the quality of the rewriter and generator. However, we do not know if such an adaptive method actually improves the \u201cabstraction\u201d ability of a summary generation model. We do not have a reliable measure for the abstraction ability to evaluate the trained generator for now. We will consider it in our future research."
        },
        {
            "heading": "9 Conclusion",
            "text": "We investigated the sentence-level summary styles of human-written summaries, and evaluated the style distributions of the three benchmark datasets. We propose a fusion-index measure to detect sentence-level summary style automatically, using which we generate oracle styles for training our adaptive model. Experiments show that our GEMINI significantly outperforms pure abstractive and rewriter baselines on the benchmarks, demonstrating the effectiveness of fine-grained control in summary styles."
        },
        {
            "heading": "Acknowledgments",
            "text": "We appreciate the constructive comments from the anonymous reviewers. We gratefully acknowledge funding from the Pioneer and \"Leading Goose\" R&D Program of Zhejiang (No.2022SDXHDX0003) and the National Natural Science Foundation of China (NSFC No.62161160339)."
        },
        {
            "heading": "A Case Study",
            "text": "As Table 7 shows, the summaries generated by GEMINI are generally more readable and concise than the summaries generated by the BART and BART-Rewriter."
        }
    ],
    "title": "GEMINI: Controlling The Sentence-Level Summary Style in Abstractive Text Summarization",
    "year": 2023
}