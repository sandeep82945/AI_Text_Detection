{
    "abstractText": "This paper introduces the Chinese Essay Discourse Coherence Corpus (CEDCC), a multi-task dataset for assessing discourse coherence. Existing research tends to focus on isolated dimensions of discourse coherence, a gap which the CEDCC addresses by integrating coherence grading, topical continuity, and discourse relations. This approach, alongside detailed annotations, captures the subtleties of real-world texts and stimulates progress in Chinese discourse coherence analysis. Our contributions include the development of the CEDCC, the establishment of baselines for further research, and the demonstration of the impact of coherence on discourse relation recognition and automated essay scoring. The dataset and related codes is available at https: //github.com/cubenlp/CEDCC_corpus.",
    "authors": [
        {
            "affiliations": [],
            "name": "Hongyi Wu"
        },
        {
            "affiliations": [],
            "name": "Xinshu Shen"
        },
        {
            "affiliations": [],
            "name": "Man Lan"
        },
        {
            "affiliations": [],
            "name": "Yuanbin Wu"
        },
        {
            "affiliations": [],
            "name": "Xiaopeng Bai"
        },
        {
            "affiliations": [],
            "name": "Shaoguang Mao"
        }
    ],
    "id": "SP:e8dedef17b9ff962db1551adfb9e7adc9710248d",
    "references": [
        {
            "authors": [
                "Hesam Amoualian",
                "Wei Lu",
                "Eric Gaussier",
                "Georgios Balikas",
                "Massih R. Amini",
                "Marianne Clausel."
            ],
            "title": "Topical coherence in LDA-based models through induced segmentation",
            "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational",
            "year": 2017
        },
        {
            "authors": [
                "Regina Barzilay",
                "Mirella Lapata."
            ],
            "title": "Modeling local coherence: An entity-based approach",
            "venue": "Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL\u201905), pages 141\u2013148, Ann Arbor, Michigan. Association",
            "year": 2005
        },
        {
            "authors": [
                "Federico Bianchi",
                "Silvia Terragni",
                "Dirk Hovy."
            ],
            "title": "Pre-training is a hot topic: Contextualized document embeddings improve topic coherence",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International",
            "year": 2021
        },
        {
            "authors": [
                "David M. Blei",
                "Andrew Y. Ng",
                "Michael I. Jordan."
            ],
            "title": "Latent dirichlet allocation",
            "venue": "J. Mach. Learn. Res., 3:993\u20131022.",
            "year": 2003
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
            "year": 2019
        },
        {
            "authors": [
                "Youmna Farag",
                "Helen Yannakoudakis",
                "Ted Briscoe."
            ],
            "title": "Neural automated essay scoring and coherence modeling for adversarially crafted input",
            "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computa-",
            "year": 2018
        },
        {
            "authors": [
                "Vanessa Wei Feng",
                "Ziheng Lin",
                "Graeme Hirst."
            ],
            "title": "The impact of deep hierarchical discourse structures in the evaluation of text coherence",
            "venue": "COLING 2014, 25th International Conference on Computational Linguistics, Proceedings of the Conference: Technical",
            "year": 2014
        },
        {
            "authors": [
                "Linea Flansmose Mikkelsen",
                "Oliver Kinch",
                "Anders Jess Pedersen",
                "Oph\u00e9lie Lacroix."
            ],
            "title": "DDisCo: A discourse coherence dataset for Danish",
            "venue": "Proceedings of the Thirteenth Language Resources and",
            "year": 2022
        },
        {
            "authors": [
                "Jian Guan",
                "Xiaoxi Mao",
                "Changjie Fan",
                "Zitao Liu",
                "Wenbiao Ding",
                "Minlie Huang."
            ],
            "title": "Long text generation by modeling sentence-level and discourse-level coherence",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguis-",
            "year": 2021
        },
        {
            "authors": [
                "Camille Guinaudeau",
                "Michael Strube."
            ],
            "title": "Graphbased local coherence modeling",
            "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 93\u2013103, Sofia, Bulgaria. Association for Com-",
            "year": 2013
        },
        {
            "authors": [
                "Andrew T. Jebb",
                "Vincent Ng",
                "Louis Tay."
            ],
            "title": "A review of key likert scale development advances: 1995\u20132019",
            "venue": "Frontiers in Psychology, 12.",
            "year": 2021
        },
        {
            "authors": [
                "Gerhard Kremer",
                "Katrin Erk",
                "Sebastian Pad\u00f3",
                "Stefan Thater."
            ],
            "title": "What substitutes tell us - analysis of an \u201call-words\u201d lexical substitution corpus",
            "venue": "Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics,",
            "year": 2014
        },
        {
            "authors": [
                "Alice Lai",
                "Joel Tetreault."
            ],
            "title": "Discourse coherence in the wild: A dataset, evaluation and methods",
            "venue": "Proceedings of the 19th Annual SIGdial Meeting on Discourse and Dialogue, pages 214\u2013223, Melbourne, Australia. Association for Computational Linguistics.",
            "year": 2018
        },
        {
            "authors": [
                "Mike Lewis",
                "Yinhan Liu",
                "Naman Goyal",
                "Marjan Ghazvininejad",
                "Abdelrahman Mohamed",
                "Omer Levy",
                "Veselin Stoyanov",
                "Luke Zettlemoyer"
            ],
            "title": "BART: Denoising sequence-to-sequence pre-training for natural language",
            "year": 2020
        },
        {
            "authors": [
                "Jiwei Li",
                "Dan Jurafsky."
            ],
            "title": "Neural net models of open-domain discourse coherence",
            "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 198\u2013209, Copenhagen, Denmark. Association for Computa-",
            "year": 2017
        },
        {
            "authors": [
                "Ziheng Lin",
                "Hwee Tou Ng",
                "Min-Yen Kan."
            ],
            "title": "Automatically evaluating text coherence using discourse relations",
            "venue": "The 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, Proceedings of the Conference,",
            "year": 2011
        },
        {
            "authors": [
                "Yang Liu",
                "Mirella Lapata."
            ],
            "title": "Text summarization with pretrained encoders",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing,",
            "year": 2019
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Roberta: A robustly optimized BERT pretraining approach",
            "venue": "CoRR, abs/1907.11692.",
            "year": 2019
        },
        {
            "authors": [
                "William C Mann",
                "Sandra A Thompson."
            ],
            "title": "Rhetorical structure theory: Toward a functional theory of text organization",
            "venue": "Text-interdisciplinary Journal for the Study of Discourse, 8(3):243\u2013281.",
            "year": 1988
        },
        {
            "authors": [
                "WILLIAM C. MANN",
                "SANDRA A. THOMPSON."
            ],
            "title": "Rhetorical structure theory: Toward a functional theory of text organization",
            "venue": "Text - Interdisciplinary Journal for the Study of Discourse, 8(3):243\u2013 281.",
            "year": 1988
        },
        {
            "authors": [
                "Rada Mihalcea",
                "Paul Tarau."
            ],
            "title": "Textrank: Bringing order into text",
            "venue": "Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing , EMNLP 2004, A meeting of SIGDAT, a Special Interest Group of the ACL, held in con-",
            "year": 2004
        },
        {
            "authors": [
                "Elham Mohammadi",
                "Timothe Beiko",
                "Leila Kosseim."
            ],
            "title": "On the creation of a corpus for coherence evaluation of discursive units",
            "venue": "Proceedings of the Twelfth Language Resources and Evaluation Conference, pages 1067\u20131072, Marseille, France. European",
            "year": 2020
        },
        {
            "authors": [
                "Rashmi Prasad",
                "Nikhil Dinesh",
                "Alan Lee",
                "Eleni Miltsakaki",
                "Livio Robaldo",
                "Aravind Joshi",
                "Bonnie Webber"
            ],
            "title": "The Penn Discourse TreeBank 2.0",
            "venue": "In Proceedings of the Sixth International Conference on Language Resources and Evaluation (LREC\u201908)",
            "year": 2008
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J. Liu."
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "J. Mach. Learn. Res., 21:140:1\u2013140:67.",
            "year": 2020
        },
        {
            "authors": [
                "Hannah Rohde",
                "Alexander Johnson",
                "Nathan Schneider",
                "Bonnie L. Webber."
            ],
            "title": "Discourse coherence: Concurrent explicit and implicit relations",
            "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018, Mel-",
            "year": 2018
        },
        {
            "authors": [
                "Aili Shen",
                "Meladel Mistica",
                "Bahar Salehi",
                "Hang Li",
                "Timothy Baldwin",
                "Jianzhong Qi."
            ],
            "title": "Evaluating document coherence modeling",
            "venue": "Transactions of the",
            "year": 2021
        },
        {
            "authors": [
                "Disha Shrivastava",
                "Abhijit Mishra",
                "Karthik Sankaranarayanan."
            ],
            "title": "Modeling topical coherence in discourse without supervision",
            "venue": "CoRR, abs/1809.00410.",
            "year": 2018
        },
        {
            "authors": [
                "Hongyi Wu",
                "Hao Zhou",
                "Man Lan",
                "Yuanbin Wu",
                "Yadong Zhang."
            ],
            "title": "Connective prediction for implicit discourse relation recognition via knowledge distillation",
            "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics",
            "year": 2023
        },
        {
            "authors": [
                "Hao Xiong",
                "Zhongjun He",
                "Hua Wu",
                "Haifeng Wang."
            ],
            "title": "Modeling coherence for discourse neural machine translation",
            "venue": "The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial In-",
            "year": 2019
        },
        {
            "authors": [
                "Zhilin Yang",
                "Zihang Dai",
                "Yiming Yang",
                "Jaime G. Carbonell",
                "Ruslan Salakhutdinov",
                "Quoc V. Le."
            ],
            "title": "Xlnet: Generalized autoregressive pretraining for language understanding",
            "venue": "Advances in Neural Information Processing Systems 32: Annual Confer-",
            "year": 2019
        },
        {
            "authors": [
                "Tianyi Zhang",
                "Varsha Kishore",
                "Felix Wu",
                "Kilian Q. Weinberger",
                "Yoav Artzi."
            ],
            "title": "Bertscore: Evaluating text generation with BERT",
            "venue": "8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenRe-",
            "year": 2020
        },
        {
            "authors": [
                "Hao Zhou",
                "Man Lan",
                "Yuanbin Wu",
                "Yuefeng Chen",
                "Meirong Ma."
            ],
            "title": "Prompt-based connective prediction method for fine-grained implicit discourse relation recognition",
            "venue": "arXiv preprint arXiv:2210.07032.",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Discourse coherence, a fundamental aspect of both language comprehension and generation, involves understanding how words, phrases, sentences, and paragraphs within a text are interconnected to create a cohesive and coherent message. Due to its significance, it has been widely incorporated into various natural language processing (NLP) tasks, such as automated essay scoring (Farag et al., 2018), machine translation (Xiong et al., 2019) and text generation (Guan et al., 2021).\nExisting discourse coherence analyses present various frameworks for understanding coherence realization, most of which can be grouped into three categories: (a) Overall Coherence Grading, which assigns a coherence score to a given text, based on its overall logical flow and consistency; (b) Topical Continuity Modelling, which focuses on analysing the distribution and relationship of topics within discourses; and (c) Discourse Relationship Recognition, which involves detecting semantic relations between two segments of a text.\nEach of these elements highlights a distinct aspect of discourse coherence. Specifically, for coherence grading, Lai and Tetreault (2018) assert that inherent coherence is a fundamental feature of human-written texts, crucial for the global comprehension of an essay and its quality assessment. In terms of topical continuity, Amoualian et al. (2017) argue that a coherent text should maintain a logical progression of ideas, eschewing abrupt topic transitions. They primarily utilize topic models to extract and designate topics within a text, thereby quantifying topical coherence. Furthermore, Feng et al. (2014) suggest that text coherence is closely tied to its discourse structure and relations.\nDespite the multifaceted nature of discourse coherence, current research often examines isolated dimensions, overlooking their interplay. For example, most coherence grading studies (Shen et al., 2021; Flansmose Mikkelsen et al., 2022) often perceive coherence as an automatic scoring task, neglecting aspects like topical continuity and discourse relations, which impairs the interpretability of results. Although some studies aim to integrate these dimensions, progress has been hindered by the lack of a comprehensive dataset. For instance, methods proposed by Shrivastava et al. (2018) and Lin et al. (2011) operate in unsupervised or artificially structured contexts, but these fail to capture the complexity of real-world texts. Furthermore, the scarcity and limited diversity of Chinese discourse coherence datasets have somewhat constrained advancements in this field.\nTo address the shortcomings of existing research, we introduce the Chinese Essay Discourse Coherence Corpus (CEDCC), a multi-task dataset designed to assess essay discourse coherence. The CEDCC, sourced from various middle schools, encompasses a diverse range of topics, genres, and regions. Each essay has been meticulously annotated by linguistic experts, as depicted in Figure 1. The CEDCC addresses key limitations in prior\nwork: firstly, it breaks from the isolationist approach of previous studies. With the integration of coherence grading, topical continuity, and discourse relations, it provides a more comprehensive understanding of discourse coherence. Secondly, the detailed annotations, particularly evident in Figure 1(c), capture the nuances of real-world texts. Factors such as off-topic sections, misused connectives, inappropriate clauses, and incoherent logic that influence coherence grade are highlighted. Finally, by offering a diverse dataset for Chinese discourse coherence analysis, CEDCC stimulates progress in this area.\nOur contributions are summarised as follows:\n\u2022 We develop the CEDCC, a comprehensive multi-task dataset for discourse coherence assessment, enhancing understanding of Chinese middle school student essays.\n\u2022 We establish baselines for the CEDCC, setting a reference point for future discourse coherence research.\n\u2022 Through insightful experiments, we illustrate the impact of coherence on discourse relation recognition and the value of fine-grained an-\nnotations for automated essay scoring, encouraging multi-dimensional discourse analysis."
        },
        {
            "heading": "2 Related Work",
            "text": "In this section, we delve into three core aspects of discourse coherence. For each, we discuss its objectives, relevant datasets, and their limitations, establishing the backdrop for our proposed dataset."
        },
        {
            "heading": "2.1 Discourse Coherence Grading",
            "text": "Discourse Coherence Grading (DCG) in NLP measures text coherence through assessing semantic, structural, and logical aspects. Several studies, such as those using the Grammarly Corpus of Discourse Coherence (GCDC) by Lai and Tetreault (2018), the INSteD dataset for pre-trained language models by Shen et al. (2021), and DDisCo dataset comprising Danish texts by Flansmose Mikkelsen et al. (2022), have contributed significantly to this field by developing specific datasets.\nHowever, these studies often focus on grading overall coherence without pinpointing the specific factors affecting it. Consequently, the derived scores may lack interpretability and offer an incomplete view of discourse coherence. Moreover, most existing resources primarily target English or\nother European languages, leaving a noticeable gap in Chinese discourse coherence analysis.\nOur CEDCC dataset aims to address these limitations. It predefines common factors impacting discourse coherence, such as off-topic sections, misused connectives, inappropriate clauses, and incoherent logic. Besides providing overall coherence scores, our annotators meticulously annotate these factors, thereby improving the interpretability of the coherence grading. Finally, by focusing on Chinese middle school student essays, the CEDCC fills the gap in Chinese discourse coherence evaluation."
        },
        {
            "heading": "2.2 Topical Continuity Modelling",
            "text": "Topical continuity modelling aims to quantify the consistency of topics within a document to ensure a cohesive narrative. Several techniques have been proposed for this task. For instance, Amoualian et al. (2017) introduced an LDA-based model to create topic-coherent segments within documents, while Shrivastava et al. (2018) proposed an unsupervised metric to evaluate topic coherence by analyzing latent topic structures. More recently, pretrained contextualized document embeddings have been used to enhance topic coherence in neural topic models (Bianchi et al., 2021). However, these methods often struggle to capture fine-grained topical variations and require substantial computational resources for processing large volumes of text.\nIn contrast, our CEDCC dataset addresses this issue by providing granular, sentence-level annotations for both paragraph and overall topics. These detailed annotations capture the nuanced topical variations within documents, adding depth to the task of topical continuity modelling. Furthermore, the focus on these finer details reduces the reliance on intensive pre-processing and segmentation techniques, which often limit the effectiveness of current models. Hence, the CEDCC dataset can serve as a valuable tool for advancing research in topical continuity modelling, especially for tasks that require an understanding of finer topical variations within a document."
        },
        {
            "heading": "2.3 Discourse Relation Recognition",
            "text": "Discourse Relation Recognition (DRR) is a significant area in NLP that seeks to identify and classify the relationships between text segments. Numerous efforts, such as the works by Zhou et al. (2022) and Wu et al. (2023), have greatly advanced the DRR field. Given the intimate connection between discourse relations and coherence, researchers have\nproposed using DRR for coherence assessment, as exemplified by the works of Lin et al. (2011) and Feng et al. (2014), and the investigation into concurrent explicit and implicit relations by Rohde et al. (2018).\nHowever, due to the absence of datasets featuring both discourse coherence grading and relations annotation, prior methods often resort to sentence ordering tasks for coherence assessment (Mohammadi et al., 2020). These tasks involve juxtaposing well-structured text with randomly arranged sentences, which, despite their usefulness, may not fully encapsulate the intricacies of real-world text coherence. To rectify this limitation, our CEDCC dataset includes both discourse relations annotations and coherence scores. This integrated approach offers a more nuanced understanding of how discourse relations contribute to overall text coherence, addressing the deficiencies of previous methods."
        },
        {
            "heading": "3 Corpus Construction",
            "text": "This section delineates the process of collection and annotation for the Chinese Essay Discourse Coherence Corpus (CEDCC), designed for extensive discourse coherence analysis."
        },
        {
            "heading": "3.1 Data Collection",
            "text": "For the construction of the CEDCC, we collected 501 essays from secondary school students\u2019 exam compositions and daily practice. These essays, ranging from 603 to 1,600 tokens with an average of approximately 713.18 tokens, were meticulously selected based on criteria such as genre and teacherassigned scores. As depicted in Figure 2(a), our dataset spans eight distinct genres, and the distribution of teacher-provided scores is illustrated in Figure 2(b).\nWe specifically chose secondary school compositions for their significance in discourse coherence research. These essays offer genuine instances of both coherent and incoherent discourse combined with formal language usage. The teacher-assigned scores further enable a correlation between discourse coherence ratings and the overall essay evaluation, addressing a primary challenge in Chinese automated essay scoring which often struggles to integrate discourse coherence due to the absence of appropriate datasets like CEDCC."
        },
        {
            "heading": "3.2 Fine-grained Annotation Format",
            "text": "For each essay in our corpus, our annotation contains three components, i.e., discourse coherence grade, topic sentences and discourse relations.\nDiscourse Coherence Grade In our dataset, each essay\u2014comprising its title and main text\u2014undergoes a coherence grading process. Adhering to conventions from prior studies (Lai and Tetreault, 2018; Flansmose Mikkelsen et al., 2022), annotators use a three-tier coherence grading system: excellent (scored 2), average (scored 1), or poor (scored 0). They also identify and annotate specific issues affecting coherence, such as offtopic sections, misused connectives, and instances of illogical flow. Given the multifaceted nature of discourse coherence, we are contemplating adopting a broader grading scale, possibly a Likert scale (Jebb et al., 2021), in future iterations.\nTopic Sentences Essays in our dataset are annotated with topic sentences that capture the main themes of their paragraphs, with a primary topic sentence signifying the essay\u2019s overarching theme. These can be located anywhere within a paragraph or inferred if explicitly absent.\nIn contrast to traditional topic continuity models, which emphasize topic words, we prioritize topic sentences for their richer thematic context. This approach suits the varied structures and styles of secondary school essays and has proven effective, as detailed in 4.3.2.\nDiscourse Relations Informed by resources such as the Rhetorical Structure Theory (Mann and Thompson, 1988) and the Penn Discourse TreeBank (Prasad et al., 2008), our annotation scheme\nclassifies discourse relations into explicit and implicit relations among paragraph and sentence pairs, based on the presence or absence of connectives.\nAnnotators identified explicit connectives, pinpointed their arguments, and labeled the discourse relation for explicit relations. For relations without explicit connectives, they inferred implicit relations. Our annotation scheme is tailored to accommodate the unique characteristics of Chinese discourse, such as complex sentence structures and common sentence grouping practices. For a detailed overview of our discourse relation annotation scheme, please refer to Appendix A."
        },
        {
            "heading": "3.3 Annotation Process",
            "text": "Our annotation team, consisting of language students and expert reviewers, underwent a training session before starting the annotation process. The dataset was divided into five groups for efficient and consistent annotation. The whole process, involving grading discourse coherence, identifying topic sentences, and defining discourse relations, took three months, resulting in a total of 501 annotated essays. For a detailed overview of our annotation process, please refer to Appendix B."
        },
        {
            "heading": "3.4 Data Statistics",
            "text": "We present an overview of our dataset\u2019s main characteristics, spanning coherence grades, topic sentences, and discourse relations. The distribution of coherence grades, including Excellent, Average, and Poor, across each genre, is depicted in Table 1. We also detail the frequency and distribution of specific issues impacting coherence in Table 2.\nEach essay averages 6 to 8 primary topic sentences. Considering essays typically have 7-8 paragraphs with 5-8 sentences each, we identified a\nmain topic sentence for every paragraph. While most of these topic sentences start their respective paragraphs, the overall distribution is more diverse, as detailed in Table 3.\nFinally, we examine discourse relations in our corpus, considering both those between adjacent paragraphs and within sentences. The distribution of these explicit and implicit relations is presented in Table 4. These relations are further classified into thirteen fine-grained relations, which correspond to four coarse-grained categories, as detailed in Figure 4."
        },
        {
            "heading": "3.5 Inner Annotator Agreements",
            "text": "To ensure the annotation quality, Inter-Annotator Agreement (IAA) (Kremer et al., 2014) was measured across various tasks associated with identifying off-topic information, misuse of connectives, inappropriate clause structures, illogical flow in text, sentence connectives and relations, as well as paragraph connectives and relations. Table 5 shows the IAA scores for these tasks across different annotation batches. Further details on IAA calculation can be found in Appendix C."
        },
        {
            "heading": "3.6 Ethical Issues",
            "text": "All data annotators and expert reviewers were compensated for their contributions. Furthermore, we have acquired explicit permission from both the authors of the essays and their guardians to use the essays for annotation and publication purposes. To safeguard the privacy of the students, all essays in the dataset have been anonymized, ensuring no personal identifiers are present. We deeply appreciate the trust and support shown by all involved parties."
        },
        {
            "heading": "4 Experiments",
            "text": ""
        },
        {
            "heading": "4.1 Tasks",
            "text": "Our annotated dataset serves as the foundation for three core tasks, each delving into distinct facets of discourse coherence:\n\u2022 Discourse Coherence Grading: This task assesses the overall coherence of essays, providing a holistic view of the textual flow and structure.\n\u2022 Topic Sentence Extraction: This task homes in on the crux of each paragraph, pinpointing the primary sentence that best embodies the central theme. It operates at a microlevel, spotlighting the nuances that contribute to the broader coherence.\n\u2022 Discourse Relation Recognition: This task is dedicated to unveiling the semantic and logical relations both within sentences and among them. It\u2019s instrumental in understanding the intricate interconnections that underpin the textual coherence.\nEach task, while distinct, contributes to our understanding of discourse coherence. The global\ncoherence of an essay, for example, is influenced by its topic transitions and interwoven discourse relations. Topic sentence extraction, in turn, sheds light on both macro-level coherence and the underlying discourse relations. Together, these tasks position our dataset as a robust tool for probing discourse coherence, paving the path for advancements in NLP and pedagogical research."
        },
        {
            "heading": "4.2 Baselines and Evaluation Metrics",
            "text": "We test several existing models on each of our discourse assessment tasks. These models comprise traditional NLP models, pre-trained transformer models such as BERT(Devlin et al., 2019) and RoBERTa(Liu et al., 2019), as well as large-scale language models like ChatGPT, specifically the gpt-3.5-turbo version1. We employ both zero-shot and few-shot learning for all tasks. The prompt and details for utilizing ChatGPT can be found in Appendix D.\nDiscourse Coherence Grading: In line with the work of (Lai and Tetreault, 2018), we test a variety of models: Entity-based Models (EGRID (Barzilay and Lapata, 2005) and EGRAPH (Guinaudeau and Strube, 2013)) and neural network models (CLIQUE (Li and Jurafsky, 2017), SENTAVG and PARSEQ (Lai and Tetreault, 2018) ) have shown promise in previous research for this task, and we further include the aforementioned pre-trained and large-scale language models.\nTopic Sentence Extraction: Inspired by methods from the extractive summarization field, we test models of different types: LEAD-3 and OR-\n1https://openai.com/blog/chatgpt\nACLE are popular summarization baselines; TextRank(Mihalcea and Tarau, 2004) and LDA(Blei et al., 2003) as unsupervised methods; transformerbased models like BERT-ext and BERT-abs, which apply the extractive and abstractive summarization techniques, respectively, based on the code2 from BERTSum(Liu and Lapata, 2019). Additionally, we include BART(Lewis et al., 2020), T5(Raffel et al., 2020), and ChatGPT, which employ end-toend generation methods.\nDiscourse Relation Recognition: For this task, we focus on a variety of pre-trained models including BERT, RoBERTa, XLNet(Yang et al., 2019), and ChatGPT, which have shown success in natural language understanding in prior work.\nFor the evaluation, we employ metrics tailored to each task. For both the Discourse Coherence Grading and Discourse Relation Recognition tasks, the models\u2019 performance is assessed using Precision (P ), Recall (R), F1-score (F1), and Accuracy (Acc). For the Topic Sentence Extraction task, we use ROUGE-1(R1), ROUGE-2(R2) and ROUGEL (RL,n-grams overlap measures), BLEU (another n-grams overlap measure considering up to 4-grams), and BERTScore(Zhang et al., 2020) (a word overlap measure based on contextual BERT embeddings). This approach allows for a taskfocused examination of baselines and metrics, providing clear distinctions between models and the rationale behind their selection and evaluation."
        },
        {
            "heading": "4.3 Main Results and Analysis",
            "text": "In this section, we present and analyse the results of the benchmark model for each subtask in turn. We use the annotations from the first 401 compositions in the dataset as the training set, and the annotations from the last 100 compositions as the test set.\n2https://github.com/nlpyang/PreSumm"
        },
        {
            "heading": "4.3.1 Discourse Coherence Grading",
            "text": "Table 6 showcases the performance of diverse models for the Discourse Coherence Grading task. The Transformer-based models BERT and RoBERTa, especially the latter, displayed superior performance in distinguishing discourse coherence, reflecting their effective utilization of contextual information in the text.\nEntity-based models, namely EGRID and EGRAPH, scored lower than Transformer-based models but still managed to compete with Neural Network models such as SENTAVG, CLIQUE, and PARSEQ. This indicates that even simpler approaches like entity transition patterns hold significant importance in determining discourse coherence. The Large-scale Language Models, represented by ChatGPT and ChatGPT3\u2212shot, showed commendable performance, with ChatGPT3\u2212shot notably outperforming its zero-shot counterpart."
        },
        {
            "heading": "4.3.2 Topic Sentence Extraction",
            "text": "As Table 7 illustrates, the ORACLE model, due to its utilization of ground truth labels, unsurprisingly achieves the highest scores across all metrics. Among Transformer-based models, BART and T5 outperform others, demonstrating their effectiveness in the topic sentence extraction task. Interestingly, the less complex models, LEAD-3 and LDA, also yield competitive performance. This finding suggests the significant roles of initial sentences and topic coherency for this particular task.\nIn contrast, when considering pre-trained language models, it is found that ChatGPT and its few-shot variant perform less optimally compared to other models. Despite the remarkable language understanding capabilities of ChatGPT, the model appears to lack a full comprehension of our topic sentence extraction task, despite a detailed intro-\nduction given in the prompt. However, its performance notably improves when provided with several examples, indicating the potential benefits of example-based fine-tuning in enhancing its task comprehension."
        },
        {
            "heading": "4.3.3 Discourse Relation Recognition",
            "text": "Tables 8 and 9 delineate the varied model performance in discourse relation recognition. At the paragraph level, BERTlarge generally outperforms, highlighting its aptitude in complex contexts, yet certain categories are better addressed by RoBERTalarge and XLNetmid, underscoring the potential for specialized optimizations. On the sentence-level, BERT models take the lead, but RoBERTalarge exhibits prowess in Implicit Elaboration and Causal relations, emphasizing its nuanced relation handling.\nAn intriguing observation during inter-paragraph experiments was the underperformance of explicit relations compared to implicit ones, despite the presence of cues like so. Delving into this, our examination (Figure 3) revealed that the cue therefore, typically signaling causality in inter-sentence\ncontexts, often prefaced a summary or generalized statement at the paragraph level, thus indicating generalization more than causation. This reflects the nuanced semantic intricacies between paragraphs and the associated modeling challenges, suggesting a refinement in future recognition work to accommodate these variances.\nChatGPT variants show moderate results, suggesting enhancement opportunities. Conclusively, no single model excels consistently, pointing towards the merit of hybrid or specialized strategies. Further granular recognition details are available in Appendix E."
        },
        {
            "heading": "5 Discussion",
            "text": ""
        },
        {
            "heading": "5.1 The Impact of Coherence on Discourse Relation Recognition",
            "text": "According to Table 10, while the performances vary across different coherence levels, there are some noticeable patterns. For instance, XLNet consistently achieves its best overall Acc and F1 scores\nfor essays with the highest coherence level. This suggests that for certain models, essays with better coherence might facilitate improved discourse relation recognition. However, for models like BERT and RoBERTa, the relationship between coherence and performance is less straightforward, indicating further nuances in how text coherence might influence discourse recognition across models.\nThis outcome underscores the importance of\nconsidering text coherence in the application of discourse relation recognition models, hinting at possible enhancements from employing coherence grades, provided they are obtainable. Additionally, the need for more robust models capable of dealing with text of varying coherence levels is evident, a crucial requirement considering the wide-ranging coherence grades present in real-world data.\nInterestingly, the recognition of implicit relations seems to be less sensitive to the coherence grade, highlighting the inherent difficulty of inferring these relations. This underscores the need for more advanced techniques capable of identifying implicit discourse relations, which could significantly enhance the overall performance of discourse relation recognition models."
        },
        {
            "heading": "5.2 The Impact of Fine-grained Annotations on Discourse Coherence Grading",
            "text": "As illustrated in Table 11, the inclusion of fine-grained annotations bolsters the ChatGPT model\u2019s performance in discourse coherence grading. Specifically, this method improves recall substantially (53.4%), indicating better detection of various error categories in essays.\nFurthermore, this approach yields the highest accuracy (60%) among the compared models, validating the value of fine-grained annotations. These findings reveal that even with infrequent error categories, weakly supervised methods can utilize finegrained annotations effectively for enhanced performance, underlining their utility in such contextdependent tasks."
        },
        {
            "heading": "6 Conclusion",
            "text": "In this paper, we introduced the Chinese Essay Discourse Coherence Corpus (CEDCC), a richly annotated, diverse dataset aiming to address limitations in current discourse coherence research. Our dataset integrates aspects of coherence grading, topical continuity, and discourse relations, thereby breaking away from the isolated approach of previous studies. We further demonstrated the importance of fine-grained annotations and the role of\ntext coherence in discourse relation recognition. Our findings set a benchmark for future discourse analysis research in Chinese, potentially sparking further advancements in the field.\nLimitations\nThe limitations of our corpus include:\n\u2022 Limited by data scale: While our dataset is among the largest in this field, its size is still constrained. The diversity and complexity of discourse coherence phenomena imply that the larger the dataset, the more comprehensive its coverage of these phenomena. Consequently, the current size of our dataset might limit the performance and generalizability of models trained on it.\n\u2022 Constraints of manual annotation: Our dataset relies significantly on manual annotations by linguistic experts. Nonetheless, due to the labor-intensive and time-consuming nature of this process, there are inevitable limitations on the volume of annotated data. Further, the inherent subjectivity of manual annotation might lead to potential inconsistencies and bias in the annotated labels.\n\u2022 Dependent on essay quality: Our dataset consists of essays authored by middle school students, which might contain language inaccuracies or logical inconsistencies absent in more formal, polished texts. These could affect the learning and generalization capabilities of the models, particularly when applied to other types of text such as academic or formal writing."
        },
        {
            "heading": "Acknowledgement",
            "text": "We appreciate the support from National Natural Science Foundation of China with the Main Research Project on Machine Behavior and HumanMachine Collaborated Decision Making Methodology (72192820 & 72192824), Pudong New Area Science & Technology Development Fund (PKX2021-R05), Science and Technology Commission of Shanghai Municipality (22DZ2229004), Shanghai Trusted Industry Internet Software Collaborative Innovation Center and East China Normal University International Conference Grant Programme."
        },
        {
            "heading": "A Annotation Scheme of Relations",
            "text": "Our annotation scheme, inspired by Rhetorical Structure Theory (RST), Penn Discourse Treebank (PDTB), Chinese compound sentence theory, and Chinese sentence group theory (MANN and THOMPSON, 1988; Prasad et al., 2008), is intricately tailored to the nuances of Chinese discourse. It encompasses four tiers and thirteen labels, capturing a broad spectrum of logical semantic types in Chinese discourse. This versatility allows for a unified analysis across diverse discourse units.\nWhile our scheme might not explicitly differentiate arguments in the manner of the PDTB, it imbues directionality in discourse relations. Unlike the traditional undirected relations, our model delineates both parallel and subordinate relationships among discourse units:\n\u2022 Parallel Relations: These involve discourse units of equal standing, without any hierarchical distinction. Examples include relations like Co-occurrence and Elaboration. In cases such as coherence or contrast, the order of discourse units can often be changed without affecting the overall meaning.\n\u2022 Subordinate Relations: These denote a hierarchy, where one discourse unit takes precedence over the other. Relations like Reversal and Causal are illustrative of this category, where the primary discourse unit can be seen as the main clause, and the subsequent as the subordinate clause.\nThis distinction ensures our scheme neither oversimplifies nor overcomplicates the importance of each discourse relation. It strikes an optimal balance: intricate enough to convey the depth of Chinese discourse yet straightforward enough to minimize potential annotation challenges and subjectivity.\nFigure 4 visually illustrates our schema, emphasizing the interplay between coarse-grained and fine-grained discourse relation labels. Detailed definitions of these fine-grained labels follow:\n\u2022 Coherence (Co-occurrence): This label describes aspects of the same event, related events, or contrasting situations that coexist, co-occur, or oppose in meaning. These aspects can be reordered without altering the overall sentence meaning.\nCo-occurrence \u5171\u73b0\u5173\u7cfb\nCoherence \u5e76\u5217\u5173\u7cfb Sequence \u987a\u627f\u5173\u7cfb Progression \u9012\u8fdb\u5173\u7cfb\nContrast \u5bf9\u6bd4\u5173\u7cfb\nElaboration \u89e3\u8bf4\u5173\u7cfb\nRefinement \u7ec6\u5316\u5173\u7cfb\nGeneralization \u6cdb\u5316\u5173\u7cfb\nCausal \u56e0\u679c\u5173\u7cfb\nBackground \u80cc\u666f\u5173\u7cfb\nObjective Causal \u5ba2\u89c2\u56e0\u679c\u5173\u7cfb\nSubjective Inference \u4e3b\u89c2\u63a8\u8bba\u5173\u7cfb Specific Conditional \u7279\u5b9a\u6761\u4ef6\u5173\u7cfb\nHypothetical Conditional \u5047\u8bbe\u6761\u4ef6\u5173\u7cfb\nReversal \u53cd\u8f6c\u5173\u7cfb\nConcession \u8f6c\u6298\u5173\u7cfb Turnabout \u8ba9\u6b65\u5173\u7cfb\n\u2022 Background (Causal): This label describes the introduction of events, places, histories, etc., which often occurs in chapters and forms a contextual relationship with the main body of the chapter.\n\u2022 Objective Causal (Causal): This label is used when one discourse unit states a cause, and another unit states the objective result that ensues from the cause.\n\u2022 Subjective Inference (Causal): This label applies when one discourse unit states the factual basis, and another unit states the subjective conclusion inferred from it.\n\u2022 Specific Conditional (Causal): This label is used when one discourse unit presents a specific condition, and another unit states the result inferred from that condition.\n\u2022 Hypothetical Conditional (Causal): This label is used when one discourse unit presents a hypothetical condition, and another unit describes the outcome if the condition were met or the measures needed to meet it."
        },
        {
            "heading": "B Detailed Annotation Process",
            "text": "Our annotation process was carried out by a team composed of four undergraduates, four postgraduates from language-related fields, and four expert reviewers with experience in Chinese teaching. The principle of minimal changes was followed in the process to retain the original language used by the secondary school students.\nBefore the actual annotation process, the team underwent a training session to familiarize themselves with the tasks. The tasks included grading the discourse coherence, identifying the topic sentences, and defining discourse relations.\nTo ensure efficiency and consistency, the data was divided into five groups for annotation. The initial annotation was done by the undergraduate and postgraduate students, while the expert reviewers validated and corrected their work. This process was aimed at maintaining the quality and consistency of the annotations.\nFurthermore, we organized weekly online discussions to address any common issues that arose during the annotation process. The discussion also served as a platform to make necessary adjustments in the annotation process.\nThe entire process spanned three months, during which a total of 501 essays were annotated. This structured approach ensured a streamlined annotation process, resulting in a richly annotated corpus that can facilitate subsequent language model training and research.\nC Inter-Annotator Agreement (IAA) Calculation\nIn this study, we adopted an Inter-Annotator Agreement (IAA) measure, computed as follows:\nPA = 1|P | \u2211 (i,j)\u2208P PAij\nPAij = \u2211N\nk=1 1 N |sik\u2229sjk| |sik\u222asjk|\n(1)\nIn the above equations, PA denotes the overall Pairwise Agreement (PA) among the annotators, with |P | representing the total number of annotator pairs. PAij represents the agreement for each pair of annotators i and j, summed across all N text fragments. sik and s j k denote the annotations made by annotator i and j on text fragment k, respectively.\nTable 5 in the main text presents the IAA scores across different tasks performed during various data batch submissions. Each row corresponds to an individual data batch, with each entry indicating the respective IAA score for the corresponding task. The rigorous calculation of IAA allows us to evaluate the quality and reliability of our annotation process, revealing areas of high agreement and highlighting areas for potential improvement."
        },
        {
            "heading": "D ChatGPT Prompt Configuration",
            "text": "For all tasks, including Discourse Coherence Grading, Topic Sentence Extraction, and Discourse Relation Recognition, we employ both zero-shot and few-shot learning strategies. Additionally, we discuss how the incorporation of finegrained annotations enhances the performance of the ChatGPT model in discourse coherence grading task. Please note that the original prompts were written in Chinese. We provide here their English translated versions.\nD.1 Discourse Coherence Grading The prompts we use for this task are as follows:\nIn the zero-shot setting, we ask the model:\nGiven an essay, you are tasked with assessing its overall coherence and providing a grade. The grade should be 0 for\nincoherent, 1 for average, and 2 for excellent coherence. The provided essay will be in the format \u2018essay_id \\t essay_title \\t essay_content\u2019, where \u2018\\t\u2019 represents a tab space, and different paragraphs in the essay content are separated by \u2018\\n\u2019. Please return the result in the format \u2018\"id\":[essay_id],\"LogicGrade\":[grade]\u2019.[T]\nIn the 3-shot setting, the prompt is modified as follows:\nGiven an essay, you are tasked with assessing its overall coherence and providing a grade. The grade should be 0 for incoherent, 1 for average, and 2 for excellent coherence. The provided essay will be in the format \u2018essay_id \\t essay_title \\t essay_content\u2019, where \u2018\\t\u2019 represents a tab space, and different paragraphs in the essay content are separated by \u2018\\n\u2019. Please return the result in the format \u2018\"id\":[essay_id],\"LogicGrade\":[grade]\u2019. Here are three sample essays with their coherence grades for reference. The format of these samples is \u2018essay_id \\t essay_title \\t essay_content \\t coherence_grade\u2019, where \u2018\\t\u2019 represents a tab space.[S][T]\nHere, [S] represents the three provided samples, and [T] is the associated information for the essay that needs to be assessed.\nD.2 Topic Sentence Extraction For the task of topic sentence extraction, we use the following prompts for both zero-shot and 3-shot learning settings:\nIn the zero-shot setting, we present the model with this prompt:\nYour task is to extract the topic sentence from the following paragraph. The topic sentence should be a complete sentence that summarizes, narrates, and explains the theme of the paragraph. You should select as complete a sentence as possible from the original text. If the paragraph does not have a clear topic sentence, please answer \u2018There is no topic sentence.\u2019 The current paragraph is: [P]. Please provide the topic sentence.\nIn the 3-shot setting, the model is prompted with:\nYour task is to extract the topic sentence from the following paragraph. The topic sentence should be a complete sentence that summarizes, narrates, and explains the theme of the paragraph. You should select as complete a sentence as possible from the original text. If the paragraph does not have a clear topic sentence, please answer \u2018There is no topic sentence.\u2019 Note that only a few paragraphs lack a clear topic sentence. Here are three examples to help you understand the task: [S] Now, the paragraph is: [P]. Please provide the topic sentence.\nHere, [S] represents the three provided samples, and [P] represents the content of the paragraph that needs to be processed.\nD.3 Discourse Relation Recognition\nFor the task of discourse relation recognition, we present the model with different prompts in the zero-shot and 3-shot settings:\nIn the zero-shot setting, the model receives the following prompt:\nYour task is to recognize the discourse relation between two texts. You can find possible discourse relations and their definitions in the following list: [D]. The input format is: {\"Text1\": \"First text\", \"Text2\": \"Second text\"}. For example, if the input is: {\"Text1\": \"I saw N\u00fcwa first killed a giant turtle, propping up the sky with its legs.\", \"Text2\": \"Then, she killed a black dragon.\"}, then you should output: \u2018Sequential relationship\u2019. [T]\nIn the 3-shot setting, we present the model with the following prompt:\nYour task is to recognize the discourse relation between two texts. You can find possible discourse relations and their definitions in the following list: [D]. Now, let\u2019s look at some examples: [S]. For example, if the input is: {\"Text1\": \"I saw N\u00fcwa first killed a giant turtle, propping up the sky with its legs.\", \"Text2\": \"Then, she killed a black dragon.\"}, then you should output: \u2018Sequential relationship\u2019. [T]\nHere, [D] provides the detailed definitions of discourse relations (see Appendix A for more details), [S] represents the given three examples, and [T] represents the text formatted according to the norms.\nD.4 Fine-Grained Annotation in Discourse Coherence Grading\nThe prompts we use for this task are as follows:\nYour task is to read and understand a middle school essay, and then identify and count the fine-grained factors that impact the coherence of the essay. This includes the number of times the topic sentence deviates, the number of times connectives are improperly used, the number of times sentences are illogically broken, and the number of times the logical relation between contexts does not flow smoothly. The essay is provided in the format \u2018Article ID\\tArticle Title\\tText Content\u2019, where \u2018\\t\u2019 denotes a tab space and \u2018\\n\u2019 indicates a paragraph break. You should return your result in the format \u2018\"id\":[Article ID],\"OffTopic\":[Number of Times Topic Sentence Deviates],\"MisusedConnectives\":[Number of Times Connectives Improperly Used],\"InappropriateClauses\":[Number of Times Sentences Illogically Broken],\"IncoherentLogic\":[Number of Times Logical Relation Between Contexts Doesn\u2019t Flow Smoothly]\u2019. It\u2019s important that you analyze every article on a word-for-word basis and mark all possible errors.\nHere is an example of how an essay analysis and returned error count would look like to help you understand the task:[E][T]\nHere, [E] represents the given example and [T] represents a well-formatted example of essay analysis and returned error count."
        },
        {
            "heading": "E Fine-grained Discourse Relation Recognition",
            "text": "As shown in Table 12, for explicit (inter-paragraph) discourse relations, RoBERTa performed the best with an overall F1 score of 16.02%, followed by\nXLNet at 15.80%, and BERT at 14.20%. In contrast, ChatGPT and ChatGPT3\u2212shot showed relatively poorer performance, scoring 9.48% and 7.00%, respectively. In terms of overall accuracy (Acc), XLNet led with a score of 42.17%, closely followed by RoBERTa at 40.96%. However, even under the best circumstances, there remains substantial room for improvement in these models.\nIn implicit discourse relations, XLNet again performed the best in terms of the overall F1 score, with 19.97%, followed by RoBERTa and BERT with 16.13% and 16.14%, respectively. However, the performance of ChatGPT and ChatGPT3\u2212shot lagged again in this task, scoring 5.66% and 6.57% respectively. In terms of overall accuracy (Acc), XLNet led with a score of 55.38%, while the performances of other models were not much different, and ChatGPT and ChatGPT3\u2212shot were still underperforming.\nIn Table 13, for discourse relations between sentences, the performance of the models is similar to the situation with inter-paragraph relations. XLNet, RoBERTa, and BERT performed well in most cases, while ChatGPT and ChatGPT3\u2212shot performed poorly in most tasks.\nIn conclusion, although large pre-trained language models have achieved significant results in many NLP tasks, there is still room for improvement in their performance on fine-grained discourse relation recognition tasks, especially in the recognition of implicit discourse relations. Notably, ChatGPT and ChatGPT3\u2212shot generally performed poorly in this task, possibly because their pre-training process lacked training data specifically targeting discourse relations. Therefore, in future work, we may need to pay more attention to specific training techniques for this task to improve model performance."
        }
    ],
    "title": "A Multi-Task Corpus for Assessing Discourse Coherence in Chinese Essays: Structure, Theme, and Logic Analysis",
    "year": 2023
}