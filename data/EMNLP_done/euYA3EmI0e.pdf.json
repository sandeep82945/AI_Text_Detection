{
    "abstractText": "Fine-grained entity typing (FET) is an essential task in natural language processing that aims to assign semantic types to entities in text. However, FET poses a major challenge known as the noise labeling problem, whereby current methods rely on estimating noise distribution to identify noisy labels but are confused by diverse noise distribution deviation. To address this limitation, we introduce Co-Prediction Prompt Tuning for noise correction in FET, which leverages multiple prediction results to identify and correct noisy labels. Specifically, we integrate prediction results to recall labeled labels and utilize a differentiated margin to identify inaccurate labels. Moreover, we design an optimization objective concerning divergent copredictions during fine-tuning, ensuring that the model captures sufficient information and maintains robustness in noise identification. Experimental results on three widely-used FET datasets demonstrate that our noise correction approach significantly enhances the quality of various types of training samples, including those annotated using distant supervision, ChatGPT, and crowdsourcing.",
    "authors": [
        {
            "affiliations": [],
            "name": "Minghao Tang"
        },
        {
            "affiliations": [],
            "name": "Yongquan He"
        },
        {
            "affiliations": [],
            "name": "Yongxiu Xu"
        },
        {
            "affiliations": [],
            "name": "Hongbo Xu"
        },
        {
            "affiliations": [],
            "name": "Wenyuan Zhang"
        },
        {
            "affiliations": [],
            "name": "Yang Lin"
        }
    ],
    "id": "SP:3f4613c21cbc5ff657a8a4600f3ec86473fb245a",
    "references": [
        {
            "authors": [
                "Devansh Arpit",
                "Stanis\u0142aw Jastrz\u0119bski",
                "Nicolas Ballas",
                "David Krueger",
                "Emmanuel Bengio",
                "Maxinder S Kanwal",
                "Tegan Maharaj",
                "Asja Fischer",
                "Aaron Courville",
                "Yoshua Bengio"
            ],
            "title": "A closer look at memorization in deep networks",
            "year": 2017
        },
        {
            "authors": [
                "Kurt Bollacker",
                "Colin Evans",
                "Praveen Paritosh",
                "Tim Sturge",
                "Jamie Taylor."
            ],
            "title": "Freebase: a collaboratively created graph database for structuring human knowledge",
            "venue": "Proceedings of the 2008 ACM SIGMOD international conference on Management of",
            "year": 2008
        },
        {
            "authors": [
                "Shuang Chen",
                "Jinpeng Wang",
                "Feng Jiang",
                "Chin-Yew Lin."
            ],
            "title": "Improving entity linking by modeling latent entity type information",
            "venue": "Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 7529\u20137537.",
            "year": 2020
        },
        {
            "authors": [
                "Tongfei Chen",
                "Yunmo Chen",
                "Benjamin Van Durme."
            ],
            "title": "Hierarchical entity typing via multi-level learning to rank",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, pages",
            "year": 2020
        },
        {
            "authors": [
                "Eunsol Choi",
                "Omer Levy",
                "Yejin Choi",
                "Luke Zettlemoyer."
            ],
            "title": "Ultra-fine entity typing",
            "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 87\u201396, Melbourne, Australia. Association for",
            "year": 2018
        },
        {
            "authors": [
                "Hongliang Dai",
                "Yangqiu Song",
                "Haixun Wang."
            ],
            "title": "Ultra-fine entity typing with weak supervision from a masked language model",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Confer-",
            "year": 2021
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
            "year": 2019
        },
        {
            "authors": [
                "Ning Ding",
                "Yulin Chen",
                "Xu Han",
                "Guangwei Xu",
                "Pengjun Xie",
                "Hai-Tao Zheng",
                "Zhiyuan Liu",
                "Juanzi Li",
                "Hong-Gee Kim."
            ],
            "title": "Prompt-learning for fine-grained entity typing",
            "venue": "arXiv preprint arXiv:2108.10604.",
            "year": 2021
        },
        {
            "authors": [
                "Fabrizio Gilardi",
                "Meysam Alizadeh",
                "Ma\u00ebl Kubli."
            ],
            "title": "Chatgpt outperforms crowd-workers for textannotation tasks",
            "venue": "arXiv preprint arXiv:2303.15056.",
            "year": 2023
        },
        {
            "authors": [
                "Dan Gillick",
                "Nevena Lazic",
                "Kuzman Ganchev",
                "Jesse Kirchner",
                "David Huynh."
            ],
            "title": "Contextdependent fine-grained entity type tagging",
            "venue": "CoRR, abs/1412.1820.",
            "year": 2014
        },
        {
            "authors": [
                "Bo Han",
                "Quanming Yao",
                "Xingrui Yu",
                "Gang Niu",
                "Miao Xu",
                "Weihua Hu",
                "Ivor Tsang",
                "Masashi Sugiyama"
            ],
            "title": "Co-teaching: Robust training of deep neural",
            "year": 2018
        },
        {
            "authors": [
                "James Y Huang",
                "Bangzheng Li",
                "Jiashu Xu",
                "Muhao Chen."
            ],
            "title": "Unified semantic typing with meaningful label inference",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human",
            "year": 2022
        },
        {
            "authors": [
                "Haiyun Jiang",
                "Yanghua Xiao",
                "Wei Wang."
            ],
            "title": "Explaining a bag of words with hierarchical conceptual labels",
            "venue": "World Wide Web, 23(3):1693\u20131713.",
            "year": 2020
        },
        {
            "authors": [
                "Diederik P. Kingma",
                "Jimmy Ba."
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings.",
            "year": 2015
        },
        {
            "authors": [
                "Na Li",
                "Zied Bouraoui",
                "Steven Schockaert."
            ],
            "title": "Ultra-fine entity typing with prior knowledge about labels: A simple clustering based strategy",
            "venue": "arXiv preprint arXiv:2305.12802.",
            "year": 2023
        },
        {
            "authors": [
                "Xiao Ling",
                "Daniel S. Weld."
            ],
            "title": "Fine-grained entity recognition",
            "venue": "Proceedings of the TwentySixth AAAI Conference on Artificial Intelligence, July 22-26, 2012, Toronto, Ontario, Canada. AAAI Press.",
            "year": 2012
        },
        {
            "authors": [
                "Lemao Liu",
                "Haisong Zhang",
                "Haiyun Jiang",
                "Yangming Li",
                "Enbo Zhao",
                "Kun Xu",
                "Linfeng Song",
                "Suncong Zheng",
                "Botong Zhou",
                "Dick Zhu"
            ],
            "title": "2021a. Texsmart: A system for enhanced natural language understanding",
            "venue": "In Proceedings of the 59th Annual Meet-",
            "year": 2021
        },
        {
            "authors": [
                "Qing Liu",
                "Hongyu Lin",
                "Xinyan Xiao",
                "Xianpei Han",
                "Le Sun",
                "Hua Wu."
            ],
            "title": "Fine-grained entity typing via label reasoning",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event",
            "year": 2021
        },
        {
            "authors": [
                "Duc Tam Nguyen",
                "Chaithanya Kumar Mummadi",
                "ThiPhuong-Nhung Ngo",
                "Thi Hoai Phuong Nguyen",
                "Laura Beggel",
                "Thomas Brox."
            ],
            "title": "SELF: learning to filter noisy labels with self-ensembling",
            "venue": "8th International Conference on Learning Represen-",
            "year": 2020
        },
        {
            "authors": [
                "Yasumasa Onoe",
                "Greg Durrett."
            ],
            "title": "Learning to denoise distantly-labeled data for entity typing",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
            "year": 2019
        },
        {
            "authors": [
                "Weiran Pan",
                "Wei Wei",
                "Feida Zhu."
            ],
            "title": "Automatic noisy label correction for fine-grained entity typing",
            "venue": "Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI 2022, Vienna, Austria, 23-29 July 2022, pages 4317\u20134323.",
            "year": 2022
        },
        {
            "authors": [
                "Kunyuan Pang",
                "Haoyu Zhang",
                "Jie Zhou",
                "Ting Wang."
            ],
            "title": "Divide and denoise: Learning from noisy labels in fine-grained entity typing with cluster-wise loss correction",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Lin-",
            "year": 2022
        },
        {
            "authors": [
                "Jonathan Raiman",
                "Olivier Raiman."
            ],
            "title": "Deeptype: multilingual entity linking by neural type system evolution",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 32.",
            "year": 2018
        },
        {
            "authors": [
                "Xiang Ren",
                "Wenqi He",
                "Meng Qu",
                "Lifu Huang",
                "Heng Ji",
                "Jiawei Han."
            ],
            "title": "Afet: Automatic fine-grained entity typing by hierarchical partial-label embedding",
            "venue": "Proceedings of the 2016 conference on empirical methods in natural language processing, pages 1369\u2013",
            "year": 2016
        },
        {
            "authors": [
                "Yu-Ming Shang",
                "Heyan Huang",
                "Xin Sun",
                "Wei Wei",
                "Xian-Ling Mao."
            ],
            "title": "Learning relation ties with a force-directed graph in distant supervised relation extraction",
            "venue": "ACM Transactions on Information Systems (TOIS).",
            "year": 2022
        },
        {
            "authors": [
                "Yuming Shang",
                "He-Yan Huang",
                "Xian-Ling Mao",
                "Xin Sun",
                "Wei Wei"
            ],
            "title": "Are noisy sentences useless for distant supervised relation extraction",
            "venue": "In Proceedings of the AAAI conference on artificial intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "Petter T\u00f6rnberg."
            ],
            "title": "Chatgpt-4 outperforms experts and crowd workers in annotating political twitter messages with zero-shot learning",
            "venue": "arXiv preprint arXiv:2304.06588.",
            "year": 2023
        },
        {
            "authors": [
                "Wei Wei",
                "ZhaoYan Ming",
                "Liqiang Nie",
                "Guohui Li",
                "Jianjun Li",
                "Feida Zhu",
                "Tianfeng Shang",
                "Changyin Luo."
            ],
            "title": "Exploring heterogeneous features for query-focused summarization of categorized community answers",
            "venue": "Information Sciences, 330:403\u2013423.",
            "year": 2016
        },
        {
            "authors": [
                "Ralph Weischedel",
                "Martha Palmer",
                "Mitchell Marcus",
                "Eduard Hovy",
                "Sameer Pradhan",
                "Lance Ramshaw",
                "Nianwen Xue",
                "Ann Taylor",
                "Jeff Kaufman",
                "Michelle Franchini"
            ],
            "title": "Ontonotes release 5.0 ldc2013t19",
            "venue": "Linguistic Data",
            "year": 2013
        },
        {
            "authors": [
                "Junshuang Wu",
                "Richong Zhang",
                "Yongyi Mao",
                "Hongyu Guo",
                "Jinpeng Huai."
            ],
            "title": "Modeling noisy hierarchical types in fine-grained entity typing: A contentbased weighting approach",
            "venue": "IJCAI, pages 5264\u2013 5270.",
            "year": 2019
        },
        {
            "authors": [
                "Junshuang Wu",
                "Richong Zhang",
                "Yongyi Mao",
                "Masoumeh Soflaei Shahrbabak",
                "Jinpeng Huai."
            ],
            "title": "Hierarchical modeling of label dependency and label noise in fine-grained entity typing",
            "venue": "Proceedings of the Thirtieth International Joint Conference on",
            "year": 2021
        },
        {
            "authors": [
                "Peng Xu",
                "Denilson Barbosa."
            ],
            "title": "Neural finegrained entity type classification with hierarchyaware loss",
            "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Lan-",
            "year": 2018
        },
        {
            "authors": [
                "Haoyu Zhang",
                "Dingkun Long",
                "Guangwei Xu",
                "Muhua Zhu",
                "Pengjun Xie",
                "Fei Huang",
                "Ji Wang."
            ],
            "title": "Learning with noise: improving distantly-supervised fine-grained entity typing via automatic relabeling",
            "venue": "Proceedings of the Twenty-Ninth International",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Fine-grained entity typing (FET) is a fundamental task in the field of natural language processing (NLP). It involves assigning specific types to entity mentions based on the contextual information surrounding them. The results of FET can be leveraged to enhance various downstream NLP tasks, including entity linking (Raiman and Raiman, 2018; Chen et al., 2020a), relation extraction (Shang et al., 2020, 2022), question answering (Wei et al., 2016) and other tasks (Jiang et al., 2020; Liu et al., 2021a).\nFET poses a formidable challenge that entails not only the classification of entities across diverse domains but also the distinction of entities that\n\u2217Yongxiu Xu is the corresponding author\nmay exhibit superficial resemblances but possess distinct underlying meanings. For example, the term \"apple\" could pertain to a technology corporation or a fruit. Achieving high performance in FET typically requires a considerable amount of annotated data, a resource that is both costly and time-consuming to acquire. To tackle this issue, researchers have explored various approaches for automatically labeling training samples. One particularly popular method is distant supervision (Ling and Weld, 2012; Gillick et al., 2014), which assumes that any text mentions an entity in existing knowledge bases (e.g., Freebase (Bollacker et al., 2008)) is related to the corresponding entity types. However, this technique can be susceptible to noise and ambiguity (refer to Figure 1a), since it fails to consider the contextual information surrounding the entities.\nMeanwhile, the large language models (LLMs), such as ChatGPT1, has drawn significant interest among researchers in exploring their applications in text annotation tasks. Recent studies (Gilardi et al., 2023; T\u00f6rnberg, 2023) indicate that zeroshot ChatGPT classifications outperform crowdworkers. However, despite their remarkable suc-\n1https://openai.com/blog/chatgpt\ncess in other domains, the weakly labels ChatGPT generated for FET still pose challenges. As shown in Figure 1b, it can generate accurate weakly labels, but its coverage of fine-grained types may be limited. This difficulty may arise from the challenge of designing a prompt that can accurately handle fine-grained entity types across multiple domains simultaneously. However, this incomplete recognition also results in noisy labels, as it mislabels the correct type as a negative sample.\nTraining deep models on data with noisy labels can result in the learning of incorrect patterns and subsequent incorrect predictions. As a result, several approaches have been proposed to tackle the issue of noisy labels in FET, including designing robust loss functions (Ren et al., 2016; Xu and Barbosa, 2018), estimating noise transition matrices (Wu et al., 2021; Pang et al., 2022), and correcting noisy labels (Onoe and Durrett, 2019; Zhang et al., 2021; Pan et al., 2022). Among these approaches, correcting noisy labels is often considered the most desirable, as it leads to more accurate and informative labels. However, the methods for noise correction often rely on estimating the distribution of noise, which can be challenging due to the diverse deviations arising from different labeling methods. For instance, distant supervision tends to generate inaccurate labels, while labels generated by ChatGPT are accurate but incomplete.\nIn this paper, we propose a novel noise correction method for FET using the co-prediction prompt-tuning technique, which leverages multiple prediction results to identify and correct noisy labels. Specifically, we first fine-tunes a pre-trained masked language model (PLM) on data with noisy labels using a co-prediction prompt. This prompt contains two mask tokens, which focus on different predictive capabilities and produce multiple predictions for each entity mention. Note that the presence of noisy labels may make it difficult for the two masks to arrive at a consensus on the outputs. In this scenario, one mask may start fitting the noise before another, leading to divergent copredictions. Hence, we design an optimization objective concerning divergent co-predictions during fine-tuning to maintain the fitting difference of masks on the noise labels and robustness to noise identification. Afterward, we integrate the multiple prediction results to recall unlabeled labels and use a differentiated margin to identify inaccurate labels.\nWe conduct experiments on three public FET datasets: OntoNotes and WiKi have distantly annotated training data; Ultra-Fine has crowdsourced training data. The experimental results show that the performance of a baseline model is significantly improves after training on the corrected data, indicating the effectiveness of our noise correction method. Additionally, we utilized ChatGPT to relabel a subset of training data from OntoNotes and Wiki and then optimized this data using our method. The experimental results demonstrate that our method can further improve the quality of the data annotated by ChatGPT."
        },
        {
            "heading": "2 Related Work",
            "text": ""
        },
        {
            "heading": "2.1 Automatic Labeling Techniques for FET",
            "text": "Due to the scarcity of manually labeled data, automatic labeling techniques have gained significant attention in FET. Some studies (Ling and Weld, 2012; Gillick et al., 2014) utilize distantly supervised learning to generate labeled training samples by mapping entity mentions to entities from public knowledge bases and selecting reliable mapping types as labels. However, the labels obtained through this method are independent of sentence semantics. On the other hand, some studies (Choi et al., 2018; Dai et al., 2021) use head supervision to label training samples, where head words are used to label entities. However, both distant supervision and head supervision have limitations in producing accurate labels.\nRecently, large language models such as ChatGPT have shown promising results in weakly supervised learning, even surpassing crowdsourced annotators in some domains (Gilardi et al., 2023; T\u00f6rnberg, 2023). However, due to the large number of entity types, designing effective prompts for ChatGPT to generate comprehensive label sets can be challenging in FET. In this paper, our goal is to develop a noise correction method that enhances the quality of weakly labeled data, instead of designing perfect prompts for ChatGPT or rules for distantly supervised learning."
        },
        {
            "heading": "2.2 Noise Learning Method for FET",
            "text": "The problem of noisy labeling presents a significant challenge in FET research. Several research studies have explored label dependencies. Ren et al. (2016) proposes a hierarchical partial-label loss to handle noisy labels. Wu et al. (2019) leverages a random walk process on hierarchical labels to weight out\nnoisy labels. Wu et al. (2021) learns the noise confusion matrix by modeling the hierarchical label structure. However, these approaches require a predefined hierarchical label structure, which can be difficult to establish in practice.\nSome studies assume that a subset of clean labels is available. Onoe and Durrett (2019) trains noise filter and relabel modules on the clean data. Pang et al. (2022) treats the clean labels as anchors to estimate noise transition matrices. However, the efficacy of these methods is heavily reliant on the size of the clean labels, which may not always be guaranteed. Hence, some approaches try to estimate noise distribution to produce pseudo-truth labels (Zhang et al., 2021) or filter out noisy labels (Pan et al., 2022). Nonetheless, unifying the noise distribution from different automatic labeling methods can be challenging. Differ from them, our approach employs co-prediction prompt tuning for noise correction, thus avoiding the pitfalls of estimating noise distribution for various types of weakly labels."
        },
        {
            "heading": "3 Methodology",
            "text": "In this section, we introduce the problem formulation and describe our noise correction method. As shown in Figure 2, our method mainly consists of two main steps: 1) Applying a co-prediction prompt to fine-tune PLMs with weakly labeled corpora; 2) Correcting noisy labels by analyzing the co-prediction results of this model."
        },
        {
            "heading": "3.1 Problem Formulation",
            "text": "The FET task aims to assign appropriate semantic types to an entity mention m based on contextual information provided by a sentence x.The semantic types are a subset of a pre-defined set of finegrained entity types Y = {y1, y2, . . . , yt}. If the training corpora is labeled by distant or weak supervision, two types of noisy labels may arise. The first type is inaccurate labeling, which occurs when an annotator assigns an incorrect semantic type to an entity mention. The second type is unlabeled labeling, which happens when the annotator fails to identify an appropriate semantic type.\nIn our approach, the goal is to improve the quality of the training corpus by correcting noise labels, specifically by removing inaccurate labels and recalling unlabeled labels."
        },
        {
            "heading": "3.2 Co-Prediction Prompt Tuning",
            "text": "The accurate identification of inaccurate or unlabeled labels is a crucial challenge in noise correction, given that manual identification of such labels is time-consuming and expensive. Recent studies (Han et al., 2018; Nguyen et al., 2020) have highlighted the \"memory effect\" (Arpit et al., 2017), which suggests that deep models tend to memorize clean labels before fitting to noisy labels when trained on data with noise. Building on this insight, we propose leveraging a co-prediction prompt-tuning technique for noisy label detection that explicitly captures the fitting difference."
        },
        {
            "heading": "3.2.1 Model Structure",
            "text": "There are two primary components in the promptbased model: prompt and verbalizer. The prompt provides task-related information to guide the pretrained language model generating relevant and accurate output. The verbalizer converts the output of the model into natural language.\nCo-Prediction Prompt Differ from traditional prompts, we adopt a co-prediction prompt for FET that contains two different mask tokens: [PMASK] and [NMASK]:\nT(c,m) = {c} [P] {m} belongs to [PMASK] rather than [NMASK], (1)\nwhere [P] represents a set of soft words that are randomly initialized trainable special tokens, and [PMASK] and [NMASK] are initialized by the embedding of original mask token [MASK]. Clearly, artificially words (e.g., \"belongs to\", \"rather than\") are used to guide the two mask tokens focus on different predictive abilities. The objective is to extract diverse knowledge from PLMs and create varying difficulty in fine-tuning the representations of the two mask tokens.\nVerbalizer Selection Following the previous prompt-based model (Ding et al., 2021), we utilize the soft verbalizer that stores a set of mappings from a soft word v \u2286 Vy to a label y \u2286 Y . As finegrained entity types typically have hierarchical type structure, such as \"/organization/company/news\", we use the average embedding of type tokens to initialize soft words.\nTo determine the probability distribution of entity mentions m with respect to the label set Y , the co-prediction model generates two distinct scores for each entity type y \u2208 Y by using [PMASK] and\n[NMASK]. The prediction score for each label y can be calculated as follows:\npp,y|(c,m) = p([PMASK] = v|T(c,m)), (2)\npn,y|(c,m) = p([NMASK] = v|T(c,m)), (3)\nwhere pp,y and pn,y are the prediction scores of [PMASK] and [NMASK] for label y."
        },
        {
            "heading": "3.2.2 Training with Noise",
            "text": "Drawing on insights from \"memory effect\" (Arpit et al., 2017), it is possible for one mask to fit noisy labels more quickly than another due to differences in their fine-tuning speeds. This fitting discrepancy can result in divergent predictions on noisy labels. Figure 2a provides an example in which the predictions generated by [PMASK] and [NMASK] for entity types \"Gov\" and \"Org\" are inconsistent. In accordance with the semantics of the co-prediction prompt, we define divergent co-predictions that satisfy one of the following criteria:{\npp,y \u2265 0.5 and pn,y \u2265 0.5 pp,y < 0.5 and pn,y < 0.5\n(4)\nOptimization Loss Function As the fine-tuning process progresses, both [PMASK] and [NMASK] will eventually fit all noisy labels, leading to a decrease of divergent co-predictions. To maintain the ability of co-prediction model to generate divergent results on noisy labels, we propose constraining the\nmodel\u2019s learning from the labels that exhibit divergent co-predictions. Hence, we adjust the training loss function as follows:\nL(m, c) = \u03b3 \u2211 Yk Ly,k + \u2211 Yt\u2212k Ly,t\u2212k, (5)\nwhere Yk denotes the labels with divergent copredictions, k is the number of these labels; Yt\u2212k denotes the labels with consistent co-prediction, t is the total number of labels; Ly is the co-prediction loss for each label y \u2208 Y ; \u03b3 is a hyper-parameter that represents the loss weight.\nAs [PMASK] and [NMASK] focus on opposite learning abilities, the co-prediction loss Ly is calculated as follows:\nLy = BCE(pp,y|(m, c), p\u0302y)+ BCE(pn,y|(m, c), 1\u2212 p\u0302y),\n(6)\nwhere p\u0302y \u2208 {0, 1} denotes the ground-truth for whether the entity mention m should be classified as label y given the context c, and BCE(\u00b7) denotes the binary cross-entropy loss function.\nDuring the initial training stage, it is crucial to recognize that labels with divergent co-predictions may include a large number of clean labels. Therefore, we start the model training with \u03b3 = 1. As the fine-tuning process progresses, we gradually decrease \u03b3 until it reaches the marginal value. Once the co-prediction model achieves the peak generalization performance, we terminate the model training process. We evaluate the effectiveness of this\ntraining strategy through empirical experiments in Section \u00a7 5.3."
        },
        {
            "heading": "3.3 Noisy Label Correction",
            "text": "In this section, we will detail how to correct noisy labels in the training data by recalling unlabeled labels and eliminating inaccurate labels.\nRecalling Unlabeled Labels The co-prediction model utilizes two mask strategies, [PMASK] and [NMASK], to extract knowledge from PLMs. By combining the predictions generated by both strategies, we can produce a more comprehensive set of predicted labels, some of which might be unlabeled in the training data. As shown in Figure 2b, the predicted label \"Pro\" is not included in the original golden labels, which suggests that it may be a potentially unlabeled label.\nEliminating Inaccurate Labels After obtaining a more comprehensive label set by recalling unlabeled labels, the next step is to optimize this set by removing inaccurate labels. As the co-prediction model retains the ability to generate divergent predictions on noisy labels, we can identify inaccurate labels by measuring the absolute difference in the co-prediction scores. Specifically, we can calculate the divergence score \u03b4y for each label y as follows:\n\u03b4y = |pp,y \u2212 (1\u2212 pn,y)| (7)\nWe can then classify each label y as either a clean or inaccurate label by comparing its divergence score \u03b4y to a margin threshold \u03f5. If \u03b4y > \u03f5, we classify the label y as inaccurate and remove it from the label set.\nDifferent weak supervision methods may result in different levels of label noise, so it is important to choose an appropriate margin threshold \u03f5 for each method. For example, distant supervision often results in inaccurate labels, so it may be more appropriate to use a small \u03f5 to remove such noise. In contrast, ChatGPT may produce accurate but incomplete labels, in which case a relatively larger \u03f5 could be adopted to ensure that more potentially relevant labels are retained."
        },
        {
            "heading": "4 Experimental Settings",
            "text": ""
        },
        {
            "heading": "4.1 Datasets",
            "text": "We evaluate our noise correction method on three publicly FET datasets. OntoNotes (Weischedel et al., 2013) used sentences from OntoNotes corpus, which was distantly annotated using DBpedia\nSpotlight (Gillick et al., 2014), with 253K training samples and 89 entity types. WiKi (Ling and Weld, 2012) used sentences from Wikipedia articles and news reports, and was distantly annotated using Freebase (Bollacker et al., 2008), with 2M training samples and 112 entity types. Ultra-Fine Choi et al. (2018) collected 6K samples through crowdsourcing, which contains 2,519 entity types. Statistics of datasets are in Table 1.\nMoreover, we employ ChatGPT to relabel 3k and 6k training samples from OntoNotes and WiKi, respectively. For Ultra-Fine, it is challenging to design a comprehensive prompt for data relabeling with a huge number of entity types. More details are in Appendix A."
        },
        {
            "heading": "4.2 Baseline Methods",
            "text": "We first consider previous noise relabeling or correction methods for FET, including LDET (Onoe and Durrett, 2019), NFETC-AR (Zhang et al., 2021), and ANLC (Pan et al., 2022). Meanwhile, we also compare our method with some noise learning methods for FET, including AFET (Ren et al., 2016), NFETC (Xu and Barbosa, 2018), and FCLC (Pang et al., 2022). Finally, we compare our method with other competitive FET systems, such as UFET (Choi et al., 2018), ML-L2R (Chen et al., 2020b), MLMET (Dai et al., 2021), LRN (Liu et al., 2021b), UNIST (Huang et al., 2022), and PKL (Li et al., 2023)."
        },
        {
            "heading": "4.3 Experimental Details",
            "text": "In our co-prediction model, we choose the BERTbase (Devlin et al., 2019) model as backbone. Then, the parameters of BERT are optimized by Adam optimizer (Kingma and Ba, 2015) with the learning rate of 2e-6, 2e-6 and 2e-5 on WiKi and OntoNotes, and Ultra-Fine, respectively. Other hyperparameters are in Appendix B.\nAfter obtaining the corrected training sets of each dataset, we train a basic prompt-based model using supervised learning to evaluate the effective-"
        },
        {
            "heading": "Model Acc Macro F1 Micro F1",
            "text": ""
        },
        {
            "heading": "With Distantly Annotated Training Samples",
            "text": ""
        },
        {
            "heading": "With ChatGPT Annotated Training Samples",
            "text": ""
        },
        {
            "heading": "Model Acc Macro F1 Micro F1",
            "text": ""
        },
        {
            "heading": "With Distantly Annotated Training Samples",
            "text": ""
        },
        {
            "heading": "With ChatGPT Annotated Training Samples",
            "text": "ness of our approach2. For this baseline, we finetune the BERT-base model with a standard prompt (without \"rather than [NMASK]\")."
        },
        {
            "heading": "4.4 Evaluation",
            "text": "Following prior works, we use the strict accuracy (Acc), Macro F1, and Micro F1 scores for OntoNotes and WiKi. As for Ultra-Fine, we use the Macro precision (P), recall (R), and F1 scores. Moreover, we conduct our experiment five times and report the mean and standard deviation values.\n2Our code link: https://github.com/mhtang1995/CPPT"
        },
        {
            "heading": "With Crowdsourced Annotated Training Samples",
            "text": ""
        },
        {
            "heading": "5 Results and Discussion",
            "text": ""
        },
        {
            "heading": "5.1 Main Results",
            "text": "Results on Distantly Annotated Data Table 2 and 3 present the results on OntoNotes and WiKi with distantly annotated training set. Compared with previous SOTA methods for FET, the simple baseline (BERT-base + Prompt Tuning) achieves on-par or inferior performance when training on the original training set. Afterward, this baseline model achieves the best Macro F1 scores over all datasets when training on the corrected training set, outperforming previous SOTA methods by a magnitude from 1.1 to 2.4 percentages.\nResults on ChatGPT Annotated Data Tables 2 and 3 also present the results on the OntoNotes and Wiki using a ChatGPT annotated training set. It is noteworthy that the baseline model achieves comparable or superior performance with a limited number of ChatGPT annotated training samples, as compared to a significantly larger number of distantly annotated training samples (3k vs. 253k in OntoNotes, and 9k vs. 2M in Wiki). Furthermore, we apply our noise correction approach on these training samples, resulting in significant performance improvements of the baseline model across all metrics. These results provide further evidence of the effectiveness of our approach in accurately identifying and correcting noisy labels.\nResults on Crowdsourced Annotated Data Table 4 presents the results on Ultra-Fine. The performance of the baseline model is significantly improved after training on the corrected training set, achieving comparable results with the previous best method. This finding indicates that our\nnoise correction method can enhance the quality of human-annotated training data, even in a challenging dataset such as Ultra-Fine.\nFurthermore, compared with previous best noise correction method (DenoiseFET), our approach has the advantage of being straightforward to implement, which does not require counting the noise distribution of each type and training additional models for noise correction."
        },
        {
            "heading": "5.2 Ablation Study",
            "text": "To evaluate the contribution of each component in our noise correction method, we conduct ablation studies on the three datasets.\nAs the co-prediction prompt plays a crucial role for noise correction, we first assessed its impact on the model\u2019s performance. To be specific, the case \"w/ co-predict\" denotes a baseline model (BERT-base + Co-Prediction Prompt Tuning) are trained on the corrected training set. Performance of [PMASK] are reported in Table 2, 3 and 4. However, the results show a slight decline in performance when the co-prediction prompt are used, which suggests that it may not enhance the peak generalization performance of the model.\nNext, we investigated the impact of each step in the noise correction process \u00a7 3.3. As shown in Table 5, the case \"w/o unl.\" indicates that the noisy labels are corrected without recalling unlabeled labels. As we can see, the baseline model achieves higher precision but lower recall scores over the all datasets. These results demonstrate the effectiveness of this step in recalling unlabeled labels and can be applied to various common types of weak labels. The case \"w/o ina.\" indicates that the noisy labels are corrected without eliminating inaccurate labels. Similarly, as shown in Tables 5, the baseline model achieves relatively lower precision but higher recall scores over all datasets.\nOverall, these ablation studies demonstrate the comprehensive effectiveness of our method in correcting noisy labels by combining the two essential steps: recalling unlabeled labels and eliminating inaccurate labeled labels. By utilizing the knowledge from PLMs, our method can effectively address the issue of insufficient coverage of fine-grained entity types in the training samples weakly annotated by the large language model ChatGPT."
        },
        {
            "heading": "5.3 Detailed Analysis",
            "text": "Effect of Loss Adjustment As previously discussed, we have developed a strategy to adjust"
        },
        {
            "heading": "With Distantly Annotated Training Samples",
            "text": ""
        },
        {
            "heading": "With ChatGPT Annotated Training Samples",
            "text": ""
        },
        {
            "heading": "With Crowdsourced Annotated Training Samples",
            "text": "losses for labels that demonstrate divergent copredictions. This approach aims to help the model maintain its ability to predict divergent results on noisy labels. To assess the efficacy of this strategy, we present the evolution trend of divergent co-predictions during the training process.\nFigure 3 displays the evolution curve of divergent co-predictions generated on the WiKi training set (weakly labeled by ChatGPT). As the model training progresses, the number of divergent copredictions decreases gradually. According to the memory effect theory, the reason may be that both [PMASK] and [NMASK] are gradually fitting more noisy labels. However, our loss adjustment strategy allows the model to generate more divergent co-predictions on the training data.\nTraining deep models on noisy labels can negatively impact their performance on unseen data. Therefore, we report the model\u2019s performance on WiKi test set. As seen, our strategy leads to a better peak generalization performance. This finding suggests that the labels with divergent co-predictions may indeed contain potentially noisy labels, and our strategy can mitigates their negative impact by restricting the model learning them.\nSensitivity of Hyperparameters We explore the performance changes with respect to different \u03b3 (Eq.5) and \u03f5 (Eq.7).\nSelecting an appropriate \u03b3 is crucial for the coprediction model to effectively identify noisy labels. Since it is difficult to directly evaluate the noise detection ability, selecting an appropriate \u03b3 relies on the model\u2019s generalization performance. Figure 4(a, b) present the results on Ultra-Fine and OntoNotes. To achieve optimal performance, hyper-parameter tuning for \u03b3 is necessary when training the co-prediction model in real-world scenarios.\nThe performance analysis of corrected training sets generated with different threshold \u03f5 is presented in Figure 4(c, d). The results clearly demonstrate the impact of varying \u03f5 values on the corrected training sets. A small \u03f5 can improve the precision score significantly, but it may also adversely affect the recall score. Therefore, choosing a reasonable \u03f5 value requires striking a balance between precision and recall scores to ensure optimal model performance.\nVisualization To demonstrate the effectiveness of our noise correction method, we visualize [PMASK] representations optimized separately by the original and corrected training sets. We choose to display the fine-grained type since some types have only a few test samples. Upon comparing Figure 5(a) and (b), we observe more prominent margins and clearer decision boundaries between different types after noise correction. However, some clusters still contain confusing samples. We attribute this to the fact that many samples have multiple fine-grained entity types (such as \"/organization/government\" and \"/location/country\"), but only one can be reported. This also highlights the complexity of noise correction in FET, as it requires determining the correctness of each type in the label set, rather than simply selecting one correct label."
        },
        {
            "heading": "6 Conclusion",
            "text": "This paper presents a novel noise correction method for FET through the utilization of co-prediction prompt tuning. Our approach is not only straightforward but also highly effective, leveraging a pretrained language model that has been fine-tuned with a co-prediction prompt. This fine-tuned model is capable of identifying and rectifying noisy labels. To evaluate the performance of our method, we con-\nducted experiments on three publicly available FET datasets, each containing different types of training samples, including those annotated through distant supervision, ChatGPT, and crowdsourcing. The results of our experiments demonstrate the versatility of our noise correction method, as it significantly enhances the quality of the training data in all three scenarios. Consequently, it leads to a substantial improvement in the overall performance of the FET model."
        },
        {
            "heading": "Limitations",
            "text": "We have developed a co-prediction prompt specifically for fine-tuning pre-trained masked language models. However, it is important to note that this is just one example of co-prediction prompt tuning, and there exist numerous possibilities for designing more engaging prompts. For instance, one can consider increasing the number of [MASK] tokens or designing prompts with more instructive words. These alternative approaches can be complemented by a novel process that utilizes the co-prediction results to further enhance the accuracy of noise correction. In addition, our method does not rely on prior knowledge or information about entity types, such as predefined hierarchical structures or detailed type introductions. While this information is often not readily available, it can be useful for identifying noisy labels."
        },
        {
            "heading": "Ethics Statement",
            "text": "To address ethical concerns, we provide the two detailed description: 1) All experiments were conducted on existing datasets derived from public scientific papers or technologies. 2) Our work does not contain any personally identifiable information and does not harm anyone."
        },
        {
            "heading": "Acknowledgements",
            "text": "This work was supported by Strategic Priority Research Program of Chinese Academy of Sciences (N0. XDC02040400)."
        },
        {
            "heading": "A Automatic Labeling with ChatGPT",
            "text": "In order to reduce both the cost and time required for labeling, we choose to relabel only a subset of high-quality training samples in OntoNotes and Wiki datasets.\nFirstly, we apply a frequency-based filtering approach to remove entity mentions from the training samples, specifically discarding those with a frequency lower than 10 in OntoNotes and 20 in Wiki. Additionally, nonsensical non-entity mentions such as \"yes\" and \"please\" are also removed. Finally, we randomly extract 3,000 and 9,000 training samples for OntoNotes and Wiki datasets, respectively.\nThese samples are merged with an artificially designed prompt and then fed into GPT-3.5 textdavinci-003 model via its official API3 for relabeling. For parameters, we use top-p 1, with temperature 0.7. An example of the labeling process is shown in Figure 6."
        },
        {
            "heading": "B Hyperparameters",
            "text": "We use a grid search to find the best hyperparameters depending on development set performances. The hyperparameters we used to fine-tuning BERT and correct noisy labels in three datasets are listed in Table 6.\n3https://openai.com/api/"
        }
    ],
    "title": "Learning to Correct Noisy Labels for Fine-Grained Entity Typing via Co-Prediction Prompt Tuning",
    "year": 2023
}