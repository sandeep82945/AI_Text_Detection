{
    "abstractText": "Dialogue segmentation is a crucial task for dialogue systems allowing a better understanding of conversational texts. Despite recent progress in unsupervised dialogue segmentation methods, their performances are limited by the lack of explicit supervised signals for training. Furthermore, the precise definition of segmentation points in conversations still remains as a challenging problem, increasing the difficulty of collecting manual annotations. In this paper, we provide a feasible definition of dialogue segmentation points with the help of document-grounded dialogues and release a large-scale supervised dataset called SuperDialseg, containing 9,478 dialogues based on two prevalent document-grounded dialogue corpora, and also inherit their useful dialoguerelated annotations. Moreover, we provide a benchmark including 18 models across five categories for the dialogue segmentation task with several proper evaluation metrics. Empirical studies show that supervised learning is extremely effective in in-domain datasets and models trained on SuperDialseg can achieve good generalization ability on out-of-domain data. Additionally, we also conducted human verification on the test set and the Kappa score confirmed the quality of our automatically constructed dataset. We believe our work is an important step forward in the field of dialogue segmentation. Our codes and data can be found from: https://github.com/ Coldog2333/SuperDialseg.",
    "authors": [
        {
            "affiliations": [],
            "name": "Junfeng Jiang"
        },
        {
            "affiliations": [],
            "name": "Chengzhang Dong"
        },
        {
            "affiliations": [],
            "name": "Sadao Kurohashi"
        },
        {
            "affiliations": [],
            "name": "Akiko Aizawa"
        }
    ],
    "id": "SP:fd681bcf6a0380e116d3946fd2d1f2a060678812",
    "references": [
        {
            "authors": [
                "Sebastian Arnold",
                "Rudolf Schneider",
                "Philippe Cudr\u00e9Mauroux",
                "Felix A. Gers",
                "Alexander L\u00f6ser."
            ],
            "title": "SECTOR: A neural model for coherent topic segmentation and classification",
            "venue": "Transactions of the Association for Computational Linguistics, 7:169\u2013184.",
            "year": 2019
        },
        {
            "authors": [
                "Joe Barrow",
                "Rajiv Jain",
                "Vlad Morariu",
                "Varun Manjunatha",
                "Douglas Oard",
                "Philip Resnik"
            ],
            "title": "A joint model for document segmentation and segment 7http://www.apache.org/licenses/LICENSE-2.0",
            "year": 2020
        },
        {
            "authors": [
                "Doug Beeferman",
                "Adam Berger",
                "John Lafferty."
            ],
            "title": "Statistical models for text segmentation",
            "venue": "Machine learning, 34(1):177\u2013210.",
            "year": 1999
        },
        {
            "authors": [
                "Narj\u00e8s Boufaden",
                "Guy Lapalme",
                "Yoshua Bengio."
            ],
            "title": "Topic segmentation : A first stage to dialogbased information extraction",
            "venue": "NLPRS.",
            "year": 2001
        },
        {
            "authors": [
                "Gillian Brown",
                "George Yule."
            ],
            "title": "Discourse analysis",
            "venue": "Cambridge university press.",
            "year": 1983
        },
        {
            "authors": [
                "Pawe\u0142 Budzianowski",
                "Tsung-Hsien Wen",
                "Bo-Hsiang Tseng",
                "I\u00f1igo Casanueva",
                "Stefan Ultes",
                "Osman Ramadan",
                "Milica Ga\u0161i\u0107."
            ],
            "title": "MultiWOZ - a large-scale multi-domain Wizard-of-Oz dataset for task-oriented dialogue modelling",
            "venue": "Proceedings of",
            "year": 2018
        },
        {
            "authors": [
                "Jiaao Chen",
                "Diyi Yang."
            ],
            "title": "Multi-view sequenceto-sequence models with conversational structure for abstractive dialogue summarization",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages",
            "year": 2020
        },
        {
            "authors": [
                "Freddy Y.Y. Choi."
            ],
            "title": "Advances in domain independent linear text segmentation",
            "venue": "1st Meeting of the North American Chapter of the Association for Computational Linguistics.",
            "year": 2000
        },
        {
            "authors": [
                "Arman Cohan",
                "Iz Beltagy",
                "Daniel King",
                "Bhavana Dalvi",
                "Dan Weld."
            ],
            "title": "Pretrained language models for sequential sentence classification",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th Inter-",
            "year": 2019
        },
        {
            "authors": [
                "Abhishek Das",
                "Satwik Kottur",
                "Khushi Gupta",
                "Avi Singh",
                "Deshraj Yadav",
                "Jos\u00e9 MF Moura",
                "Devi Parikh",
                "Dhruv Batra."
            ],
            "title": "Visual dialog",
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition, pages 326\u2013335.",
            "year": 2017
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "arXiv preprint arXiv:1810.04805.",
            "year": 2018
        },
        {
            "authors": [
                "Emily Dinan",
                "Stephen Roller",
                "Kurt Shuster",
                "Angela Fan",
                "Michael Auli",
                "Jason Weston."
            ],
            "title": "Wizard of wikipedia: Knowledge-powered conversational agents",
            "venue": "International Conference on Learning Representations.",
            "year": 2019
        },
        {
            "authors": [
                "Jacob Eisenstein",
                "Regina Barzilay."
            ],
            "title": "Bayesian unsupervised topic segmentation",
            "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP \u201908, page 334\u2013343, USA. Association for Computational Linguistics.",
            "year": 2008
        },
        {
            "authors": [
                "Mihail Eric",
                "Lakshmi Krishnan",
                "Francois Charette",
                "Christopher D. Manning."
            ],
            "title": "Key-value retrieval networks for task-oriented dialogue",
            "venue": "Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue, pages 37\u201349, Saarbr\u00fccken, Germany.",
            "year": 2017
        },
        {
            "authors": [
                "Song Feng",
                "Siva Sankalp Patel",
                "Hui Wan",
                "Sachindra Joshi."
            ],
            "title": "MultiDoc2Dial: Modeling dialogues grounded in multiple documents",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6162\u20136176,",
            "year": 2021
        },
        {
            "authors": [
                "Song Feng",
                "Hui Wan",
                "Chulaka Gunasekara",
                "Siva Patel",
                "Sachindra Joshi",
                "Luis Lastras."
            ],
            "title": "doc2dial: A goal-oriented document-grounded dialogue dataset",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language",
            "year": 2020
        },
        {
            "authors": [
                "Xiachong Feng",
                "Xiaocheng Feng",
                "Libo Qin",
                "Bing Qin",
                "Ting Liu."
            ],
            "title": "Language model as an annotator: Exploring DialoGPT for dialogue summarization",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics",
            "year": 2021
        },
        {
            "authors": [
                "Chris Fournier."
            ],
            "title": "Evaluating text segmentation using boundary edit distance",
            "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1702\u20131712, Sofia, Bulgaria. Association for Compu-",
            "year": 2013
        },
        {
            "authors": [
                "Haomin Fu",
                "Yeqin Zhang",
                "Haiyang Yu",
                "Jian Sun",
                "Fei Huang",
                "Luo Si",
                "Yongbin Li",
                "Cam Tu Nguyen."
            ],
            "title": "Doc2Bot: Accessing heterogeneous documents via conversational bots",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP",
            "year": 2022
        },
        {
            "authors": [
                "Haoyu Gao",
                "Rui Wang",
                "Ting-En Lin",
                "Yuchuan Wu",
                "Min Yang",
                "Fei Huang",
                "Yongbin Li."
            ],
            "title": "Unsupervised dialogue topic segmentation with topicaware contrastive learning",
            "venue": "Proceedings of the 46th International ACM SIGIR Conference on Re-",
            "year": 2023
        },
        {
            "authors": [
                "Jeroen Geertzen",
                "Harry Bunt."
            ],
            "title": "Measuring annotator agreement in a complex hierarchical dialogue act annotation scheme",
            "venue": "Proceedings of the",
            "year": 2006
        },
        {
            "authors": [
                "Robert Geirhos",
                "J\u00f6rn-Henrik Jacobsen",
                "Claudio Michaelis",
                "Richard Zemel",
                "Wieland Brendel",
                "Matthias Bethge",
                "Felix A Wichmann."
            ],
            "title": "Shortcut learning in deep neural networks",
            "venue": "Nature Machine Intelligence, 2(11):665\u2013673.",
            "year": 2020
        },
        {
            "authors": [
                "Goran Glava\u0161",
                "Federico Nanni",
                "Simone Paolo Ponzetto."
            ],
            "title": "Unsupervised text segmentation using semantic relatedness graphs",
            "venue": "Proceedings of the Fifth Joint Conference on Lexical and Computational Semantics, pages 125\u2013130, Berlin, Germany.",
            "year": 2016
        },
        {
            "authors": [
                "Goran Glava\u0161",
                "Swapna Somasundaran."
            ],
            "title": "Twolevel transformer and auxiliary coherence modeling for improved text segmentation",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, 34(05):7797\u20137804.",
            "year": 2020
        },
        {
            "authors": [
                "Zheng Gong",
                "Shiwei Tong",
                "Han Wu",
                "Qi Liu",
                "Hanqing Tao",
                "Wei Huang",
                "Runlong Yu."
            ],
            "title": "Tipster: A topic-guided language model for topic-aware text segmentation",
            "venue": "Database Systems for Advanced Applications: 27th International Conference, DAS-",
            "year": 2022
        },
        {
            "authors": [
                "Barbara J. Grosz",
                "Candace L. Sidner."
            ],
            "title": "Attention, intentions, and the structure of discourse",
            "venue": "Computational Linguistics, 12(3):175\u2013204.",
            "year": 1986
        },
        {
            "authors": [
                "Marti A. Hearst."
            ],
            "title": "Text tiling: Segmenting text into multi-paragraph subtopic passages",
            "venue": "Computational Linguistics, 23(1):33\u201364.",
            "year": 1997
        },
        {
            "authors": [
                "Julia Hirschberg",
                "Diane Litman."
            ],
            "title": "Empirical studies on the disambiguation of cue phrases",
            "venue": "Computational Linguistics, 19(3):501\u2013530.",
            "year": 1993
        },
        {
            "authors": [
                "Adam Janin",
                "Don Baron",
                "Jane Edwards",
                "Dan Ellis",
                "David Gelbart",
                "Nelson Morgan",
                "Barbara Peskin",
                "Thilo Pfau",
                "Elizabeth Shriberg",
                "Andreas Stolcke"
            ],
            "title": "The icsi meeting corpus",
            "year": 2003
        },
        {
            "authors": [
                "Jaehun Jung",
                "Bokyung Son",
                "Sungwon Lyu."
            ],
            "title": "AttnIO: Knowledge Graph Exploration with In-andOut Attention Flow for Knowledge-Grounded Dialogue",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Process-",
            "year": 2020
        },
        {
            "authors": [
                "Omri Koshorek",
                "Adir Cohen",
                "Noam Mor",
                "Michael Rotman",
                "Jonathan Berant."
            ],
            "title": "Text segmentation as a supervised learning task",
            "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Hu-",
            "year": 2018
        },
        {
            "authors": [
                "Alex Krizhevsky",
                "Ilya Sutskever",
                "Geoffrey E Hinton."
            ],
            "title": "Imagenet classification with deep convolutional neural networks",
            "venue": "Communications of the ACM, 60(6):84\u201390.",
            "year": 2017
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Roberta: A robustly optimized bert pretraining approach",
            "venue": "arXiv preprint arXiv:1907.11692.",
            "year": 2019
        },
        {
            "authors": [
                "Zhengyuan Liu",
                "Angela Ng",
                "Sheldon Lee",
                "Ai Ti Aw",
                "Nancy F Chen."
            ],
            "title": "Topic-aware pointergenerator networks for summarizing spoken conversations",
            "venue": "2019 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), pages",
            "year": 2019
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter."
            ],
            "title": "Decoupled weight decay regularization",
            "venue": "International Conference on Learning Representations.",
            "year": 2018
        },
        {
            "authors": [
                "Longxuan Ma",
                "Wei-Nan Zhang",
                "Mingda Li",
                "Ting Liu."
            ],
            "title": "A survey of document grounded dialogue systems (dgds)",
            "venue": "arXiv preprint arXiv:2004.13818.",
            "year": 2020
        },
        {
            "authors": [
                "I. McCowan",
                "J. Carletta",
                "W. Kraaij",
                "S. Ashby",
                "S. Bourban",
                "M. Flynn",
                "M. Guillemot",
                "T. Hain",
                "J. Kadlec",
                "V. Karaiskos",
                "M. Kronenthal",
                "G. Lathoud",
                "M. Lincoln",
                "A. Lisowska",
                "W. Post",
                "Dennis Reidsma",
                "P. Wellner."
            ],
            "title": "The ami meeting corpus",
            "venue": "Pro-",
            "year": 2005
        },
        {
            "authors": [
                "Long Ouyang",
                "Jeff Wu",
                "Xu Jiang",
                "Diogo Almeida",
                "Carroll L Wainwright",
                "Pamela Mishkin",
                "Chong Zhang",
                "Sandhini Agarwal",
                "Katarina Slama",
                "Alex Ray"
            ],
            "title": "Training language models to follow instructions with human feedback",
            "year": 2022
        },
        {
            "authors": [
                "Rebecca J. Passonneau",
                "Diane J. Litman."
            ],
            "title": "Discourse segmentation by human and automated means",
            "venue": "Computational Linguistics, 23(1):103\u2013139.",
            "year": 1997
        },
        {
            "authors": [
                "Adam Paszke",
                "Sam Gross",
                "Francisco Massa",
                "Adam Lerer",
                "James Bradbury",
                "Gregory Chanan",
                "Trevor Killeen",
                "Zeming Lin",
                "Natalia Gimelshein",
                "Luca Antiga"
            ],
            "title": "Pytorch: An imperative style, high-performance deep learning",
            "year": 2019
        },
        {
            "authors": [
                "Jeffrey Pennington",
                "Richard Socher",
                "Christopher Manning."
            ],
            "title": "GloVe: Global vectors for word representation",
            "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1532\u20131543, Doha,",
            "year": 2014
        },
        {
            "authors": [
                "Lev Pevzner",
                "Marti A. Hearst."
            ],
            "title": "A Critique and Improvement of an Evaluation Metric for Text Segmentation",
            "venue": "Computational Linguistics, 28(1):19\u2013",
            "year": 2002
        },
        {
            "authors": [
                "Stephen Roller",
                "Emily Dinan",
                "Naman Goyal",
                "Da Ju",
                "Mary Williamson",
                "Yinhan Liu",
                "Jing Xu",
                "Myle Ott",
                "Eric Michael Smith",
                "Y-Lan Boureau",
                "Jason Weston"
            ],
            "title": "Recipes for building an open-domain chatbot",
            "year": 2021
        },
        {
            "authors": [
                "Lifeng Shang",
                "Zhengdong Lu",
                "Hang Li."
            ],
            "title": "Neural responding machine for short-text conversation",
            "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Lan-",
            "year": 2015
        },
        {
            "authors": [
                "Yiping Song",
                "Lili Mou",
                "Rui Yan",
                "Li Yi",
                "Zinan Zhu",
                "Xiaohua Hu",
                "Ming Zhang."
            ],
            "title": "Dialogue session segmentation by embedding-enhanced texttiling",
            "venue": "Interspeech 2016, pages 2706\u20132710.",
            "year": 2016
        },
        {
            "authors": [
                "Ryuichi Takanobu",
                "Minlie Huang",
                "Zhongzhou Zhao",
                "Fenglin Li",
                "Haiqing Chen",
                "Liqiang Nie",
                "Xiaoyan Zhu."
            ],
            "title": "A weakly supervised method for topic segmentation and labeling in goal-oriented dialogues via reinforcement learning",
            "venue": "IJCAI-ECAI,",
            "year": 2018
        },
        {
            "authors": [
                "Teven Le Scao",
                "Sylvain Gugger",
                "Mariama Drame",
                "Quentin Lhoest",
                "Alexander Rush."
            ],
            "title": "Transformers: State-of-the-art natural language processing",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing:",
            "year": 2020
        },
        {
            "authors": [
                "Chien-Sheng Wu",
                "Steven C.H. Hoi",
                "Richard Socher",
                "Caiming Xiong."
            ],
            "title": "TOD-BERT: Pre-trained natural language understanding for task-oriented dialogue",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing",
            "year": 2020
        },
        {
            "authors": [
                "Jinxiong Xia",
                "Cao Liu",
                "Jiansong Chen",
                "Yuchen Li",
                "Fan Yang",
                "Xunliang Cai",
                "Guanglu Wan",
                "Houfeng Wang."
            ],
            "title": "Dialogue topic segmentation via parallel extraction network with neighbor smoothing",
            "venue": "Proceedings of the 45th International ACM SIGIR",
            "year": 2022
        },
        {
            "authors": [
                "Huiyuan Xie",
                "Zhenghao Liu",
                "Chenyan Xiong",
                "Zhiyuan Liu",
                "Ann Copestake."
            ],
            "title": "TIAGE: A benchmark for topic-shift aware dialog modeling",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2021, pages 1684\u20131690, Punta",
            "year": 2021
        },
        {
            "authors": [
                "Linzi Xing",
                "Giuseppe Carenini."
            ],
            "title": "Improving unsupervised dialogue topic segmentation with utterance-pair coherence scoring",
            "venue": "Proceedings of the 22nd Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 167\u2013",
            "year": 2021
        },
        {
            "authors": [
                "Yi Xu",
                "Hai Zhao",
                "Zhuosheng Zhang."
            ],
            "title": "Topicaware multi-turn dialogue modeling",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, 35(16):14176\u201314184.",
            "year": 2021
        },
        {
            "authors": [
                "Chenxu Yang",
                "Zheng Lin",
                "Jiangnan Li",
                "Fandong Meng",
                "Weiping Wang",
                "Lanrui Wang",
                "Jie Zhou."
            ],
            "title": "TAKE: Topic-shift aware knowledge sElection for dialogue generation",
            "venue": "Proceedings of the 29th International Conference on Computational Linguis-",
            "year": 2022
        },
        {
            "authors": [
                "Leilan Zhang",
                "Qiang Zhou."
            ],
            "title": "Topic segmentation for dialogue stream",
            "venue": "2019 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC), pages 1036\u2013 1043. IEEE.",
            "year": 2019
        },
        {
            "authors": [
                "Saizheng Zhang",
                "Emily Dinan",
                "Jack Urbanek",
                "Arthur Szlam",
                "Douwe Kiela",
                "Jason Weston"
            ],
            "title": "Personalizing dialogue agents: I have a dog, do you have pets too",
            "venue": "In Proceedings of the 56th Annual Meeting of the Association",
            "year": 2018
        },
        {
            "authors": [
                "Ming Zhong",
                "Yang Liu",
                "Yichong Xu",
                "Chenguang Zhu",
                "Michael Zeng."
            ],
            "title": "Dialoglm: Pre-trained model for long dialogue understanding and summarization",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, 36(10):11765\u201311773.",
            "year": 2022
        },
        {
            "authors": [
                "Kangyan Zhou",
                "Shrimai Prabhumoye",
                "Alan W Black."
            ],
            "title": "A dataset for document grounded conversations",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 708\u2013713, Brussels, Belgium. Association",
            "year": 2018
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Recently, the importance of dialogue segmentation for dialogue systems has been highlighted in academia (Song et al., 2016; Xing and Carenini, 2021; Gao et al., 2023) and industry (Xia et al., 2022). Dialogue segmentation aims to divide a dialogue into several segments according to the\n*Corresponding Author\ndiscussed topics, and therefore benefits the dialogue understanding for various dialogue-related tasks, including dialogue summarization (Liu et al., 2019b; Chen and Yang, 2020; Feng et al., 2021b; Zhong et al., 2022), response generation (Xie et al., 2021), response selection (Xu et al., 2021), knowledge selection (Yang et al., 2022), and dialogue information extraction (Boufaden et al., 2001). When using Large Language Models (LLMs) as dialogue systems (e.g., ChatGPT1), dialogue segmentation methods can also help us to save tokens during long-term conversations while minimizing information loss. Figure 1 shows an example of dialogue segmentation.\nOwing to the scarcity of supervised dialogue segmentation datasets, previous studies developed unsupervised methods (Song et al., 2016; Xu et al., 2021; Xing and Carenini, 2021) but easily reached the ceiling. Noting that models trained with largescale supervised text segmentation datasets are effective for segmenting documents (Koshorek et al., 2018; Arnold et al., 2019; Barrow et al., 2020), however, Xie et al. (2021) experimented that applying them directly on dialogues were inadequate because dialogue has its own characteristics.\nPrevious studies showed that deep learning (DL) models usually perform better with supervised training signals and converge faster (Krizhevsky et al., 2017; Koshorek et al., 2018). Therefore, despite the expensive cost of manual annotation, previous works still attempted to collect some annotated datasets. However, these datasets are either small, containing only 300 \u223c 600 samples for training (Xie et al., 2021; Xia et al., 2022), or not opensourced, due to the privacy concerns (Takanobu et al., 2018) and other issues (e.g., copyright issues from the industry (Xia et al., 2022)). Therefore, a large and publicly available supervised dataset is urgently needed. In addition, some studies on spoken dialogue systems provided manual annota-\n1https://openai.com/blog/chatgpt\ntions on speech data (Janin et al., 2003; McCowan et al., 2005), but because of the noisy transcripts of spoken conversations, these corpora are also not suitable for training dialogue segmentation models.\nApart from the cost, another challenge for constructing a high-quality annotated dataset is the vague definition of segmentation points in dialogues. In contrast to documents, which have clear structures such as sections and paragraphs, segmenting dialogues is particularly confusing and difficult. Existing annotated datasets generally suffer from disagreement among annotators (Xie et al., 2021), resulting in relatively low Kappa scores2 or boundary ambiguity (Xia et al., 2022). Inspired by Xing and Carenini (2021), we conjecture that the document-grounded dialogue system (DGDS) (Ma et al., 2020) can provide a criterion for defining the boundary of dialogue segments. Specifically, in a document-grounded dialogue, each utterance is grounded on a piece of relevant knowledge within a document, illustrating what speakers concentrate on or think about when talking. This information can reflect the discussing topic for each utterance. Based on this characteristic, we construct a supervised dialogue segmentation dataset, SuperDialseg, based on two DGDS datasets, containing 9,478 dialogues. Moreover, we inherit their rich dialoguerelated annotations for broader applications.\nUsually, dialogue segmentation is considered as a subtask that aids other downstream tasks, and thus, it lacks a standard and comprehensive benchmark for comparison. Therefore, in addition to proposing the supervised dataset, we conducted an extensive comparison including 18 models across five categories using appropriate evaluation metrics.\n2TIAGE\u2019s Kappa: 0.479 (0.41-0.6: moderate agreement)\nIn summary, our contribution is threefold.\n\u2022 We constructed a large-scale supervised dialogue segmentation dataset, SuperDialseg, from two DGDS corpora, inheriting useful dialogue-related annotations.\n\u2022 We conducted empirical studies to show the good generalization ability of SuperDialseg and emphasized the importance of considering dialogue-specific characteristics for dialogue segmentation.\n\u2022 We provide an extensive benchmark including 18 models across five categories. We believe it is essential and will be valuable for the dialogue research community."
        },
        {
            "heading": "2 Related Work",
            "text": ""
        },
        {
            "heading": "2.1 Text Segmentation",
            "text": "Before dialogue segmentation, text segmentation was already a promising task for analyzing document structures and semantics by segmenting a long document into several pieces or sections. To the best of our knowledge, cue phrases were used for text segmentation as early as 1983 (Brown and Yule., 1983; Grosz and Sidner, 1986; Hirschberg and Litman, 1993; Passonneau and Litman, 1997). Subsequently, Hearst (1997) proposed TextTiling to distinguish topic shifts based on the lexical cooccurrence. Choi (2000) and Glava\u0161 et al. (2016) also proposed clustering-based and graph-based methods for text segmentation. Before large-scale datasets existed, these unsupervised methods performed relatively well but easily reached the ceiling. Facing this challenge, Koshorek et al. (2018)\ncollected 727k documents from Wikipedia and segmented them based on the natural document structures. With the Wiki727K dataset, they formulated text segmentation as a supervised learning problem and consequently led to many outstanding algorithms (Arnold et al., 2019; Barrow et al., 2020; Glava\u0161 and Somasundaran, 2020; Gong et al., 2022). However, previous work (Xie et al., 2021) experimented that it is inadvisable to directly apply these studies to dialogue systems because dialogue possesses unique characteristics."
        },
        {
            "heading": "2.2 Document-grounded Dialogue System",
            "text": "Recently, limitations such as lacking common sense (Shang et al., 2015) and knowledge hallucination (Roller et al., 2021) have been identified in dialogue systems. To solve these issues, previous studies focused on extracting useful information from knowledge graphs (Jung et al., 2020), images (Das et al., 2017), and documents (Zhou et al., 2018; Dinan et al., 2019), yielding informative responses. Feng et al. (2020, 2021a) proposed two DGDS corpora with rich annotations, including role information, dialogue act, and grounding reference of each utterance, which enable knowledge identification learning and reveal how interlocutors think and seek information during the conversation."
        },
        {
            "heading": "2.3 Dialogue Segmentation",
            "text": "Unlike documents that have natural structures (e.g., titles, sections, and paragraphs), dialogue segmentation involves several challenges, including the vague definition of segmentation points, which increases the difficulty of data collection even for humans. Recently, Xia et al. (2022) manually collected 562 annotated samples to construct the CSTS dataset, and Xie et al. (2021) constructed the TIAGE dataset containing 300 samples for training. Not only was the size of annotated datasets too small for supervised learning, they also observed a similar issue: such datasets suffered from boundary ambiguity and noise frequently, even when the segmentation points were determined by human annotators. Zhang and Zhou (2019) claimed that they collected 4k manually labeled topic segments and automatically augmented them to be approximately 40k dialogues in total; however, until now, these annotated data are still not publicly available.\nFor evaluation, Xu et al. (2021) constructed the Dialseg711 dataset by randomly combining dialogues in different domains. Automatically generating large-scale datasets in this way seems plausi-\nble, but the dialogue flow around the segmentation point is not natural, resulting in semantic fractures. Though the dataset can still serve as an evaluation dataset, this characteristic can be interpreted as shortcuts (Geirhos et al., 2020), leading to poor generalization in real-world scenarios. Based on a prevalent DGDS dataset, doc2dial (Feng et al., 2020), Xing and Carenini (2021) used the training and validation sets to construct an evaluation dataset for dialogue segmentation but missed some crucial details. For example, as shown in Figure 1, the user begins the conversation with a general query referring to the document title but the following utterance from the agent refers to the first section. In this case, Xing and Carenini (2021) mistakenly assigned them to different segments, even though they are still discussing the same topic. Furthermore, most existing datasets neglected useful dialogue-related annotations like role information and dialogue act, while we inherit these annotations from the existing DGDS corpora to support future research. We will describe our dataset construction approach in Section 3.2. Table 1 summarizes the datasets mentioned above."
        },
        {
            "heading": "3 The SuperDialseg Dataset",
            "text": "Documents have natural structures, whereas natural structures for dialogues are hard to define. In this paper, we utilize the well-defined document structure to define the dialogue structure."
        },
        {
            "heading": "3.1 Data Source",
            "text": "Doc2dial (Feng et al., 2020) and MultiDoc2Dial (Feng et al., 2021a) are two popular and highquality document-grounded dialogue corpora. Compared to other DGDS datasets (Zhou et al., 2018; Dinan et al., 2019), they contain fine-grained\ngrounding references for each utterance, as well as additional dialogue-related annotations like dialogue acts and role information. Based on these annotations, we can construct a supervised dialogue segmentation dataset without human effort. Detailed information about doc2dial and MultiDoc2Dial can be found in Appendix B."
        },
        {
            "heading": "3.2 Data Construction Approach",
            "text": "Since doc2dial and MultiDoc2Dial provide the grounding texts for utterances, such that we can understand what the utterances are about according to the grounding references in documents. Take Figure 1 as an example. The user query \u2018Will my child receive the full payment up front or monthly payments\u2019 refers to the information from the section \u2018Benefits For Your Family\u2019. Then, the agent reads the document, seeking information from the same section, and provides a proper response. In this case, we know they are discussing a same topic because the subject of these utterances locates in the same section of the document. During this interaction in the DGDS scenario, we find that it provides an alignment between documents and dialogues. In this manner, we can define the segmentation points of dialogue based on the inherent structure of documents considering the grounding reference of each utterance. Furthermore, the grounding reference can reflect what interlocutors are thinking about when talking, which precisely matches the definition discussed in Section 1. Thus, this validates the rationality of our data collection method. In this paper, we focus on segmenting this type of dialogue, which is also dominant in recent ChatGPT-like dialogue systems (e.g., ChatPDF3). Surprisingly, our empirical studies indicate that models trained on our datasets can also perform well in other types of dialogue (i.e. non-document-grounded dialogue). We will discuss it later in Section 5.4.\nBased on this idea, we determine that one utterance starts a new topic when its grounding reference originates from a different section in the grounded document compared to its previous utterance. Besides, there may be multiple grounding references for one utterance. In such a case, if all references of the current utterance are not located in the same section of any reference of the previous utterance, we regard it as the beginning of a new topic. Otherwise, these two utterances refer to the same topic. Note that MultiDoc2Dial has already\n3https://www.chatpdf.com/\nhad document-level segmentation points, but our definition is more fine-grained and can describe the fine-grained topic shifts in conversations.\nFurthermore, we carefully consider some details in the DGDS datasets. In the doc2dial and MultiDoc2Dial datasets, users sometimes ask questions that require agents\u2019 further confirmation, like \u2018Hello. I\u2019d like to learn about your retirement program\u2019 in Figure 1. However, in this case, though these queries and their following utterances are discussing the same topic, their grounding spans may not locate in the same section, leading to mistakes. Therefore, we corrected it as a nonsegmentation point if the dialogue act of utterances from users is \u2018query condition\u2019 (the user needs further confirmation). Besides, we also removed the dialogues with meaningless utterances like \u2018null\u2019/\u2018Null\u2019 to further improve the quality of our supervised dataset.\nMoreover, the dev/test sets of the DGDS datasets contain some unseen documents with respect to the training set. Consequently, some dialogues in the dev/test sets discuss unseen topics with respect to the training set. Therefore, following the official splitting can serve as an excellent testbed to develop dialogue segmentation models with high generalization ability. Algorithm 1 shows the procedure of determining the segmentation points for a document-grounded dialogue. Note that the gi is a set containing single/multiple grounding reference(s).\nAlgorithm 1 Segment a dialogue\nRequire: U = {ui}Ni=0: utterances Require: G = {gi}Ni=0: grounding spans Require: Sec(g): return the section IDs Require: R = {ri}Ni=0: roles Require: DA = {dai}Ni=0: dialogue acts\n1: for i = 1, \u00b7 \u00b7 \u00b7 , N do 2: if Sec(gi) \u2229 Sec(gi \u2212 1) = \u2205 then 3: L(ui\u22121) = 1 4: end if 5: end for 6: for i = 0, \u00b7 \u00b7 \u00b7 , N do 7: if dai == query condition and ri == user then 8: L(ui) = 0 9: end if\n10: end for 11: L(uN ) = 1 12: return L: List of segmentation points.\nBesides, doc2dial and MultiDoc2Dial provide useful dialogue-related annotations about the role (user or agent) and dialogue act of each utterance (e.g., respond solution). We inherit them to enrich our supervised dialogue segmentation corpus. Intuitively, dialogue segments are characterized by specific patterns of roles and dialogue acts. For example, when users ask questions, the agents will provide answers or ask another question to confirm some details. The discussed topics are generally maintained until the problem is solved. Therefore, we hypothesize that introducing dialogue act information is helpful for dialogue segmentation. We conducted a pilot experiment on the relationship between the segmentation points and the dialogue acts, proving our hypothesis. Figure 2 illustrates the distribution of dialogue acts for all utterances and those serve as the segmentation points in the test set of SuperDialseg. We observe that most dialogue acts are highly related to the segmentation points. Specifically, utterances with dialogue acts DA-1, DA-5, DA-6, and DA-7 probably act as segmentation points, which indicates that a topic usually ends when solutions are provided, or the previous query has no solution. Therefore, these extra dialogue-related annotations are significant for the task of dialogue segmentation. We also conducted an analysis in Section 5.4.4 to further confirm our hypothesis.\nTable 2 summarizes the statistics of the SuperDialseg dataset. Detailed distribution of the number\nof segment is shown in Appendix C."
        },
        {
            "heading": "3.3 Human Verification",
            "text": "To verify the quality of our proposed dataset, we conducted human verification to confirm the rationality of our data collection method. We randomly selected 100 dialogues from the test set of SuperDialseg and each dialogue was annotated by two human annotators. We collected the annotations from the crowd-sourcing platform, Amazon Mechanical Turk 4, and 20 valid workers participated in our annotation task. Each assignment contains 10 dialogues to be annotated.\nSince it is a verification process, it should be conducted more carefully than collecting data for training. Therefore, we only allowed workers whose HIT approval rate is greater than 95% to access and also rejected some invalid results, for example, including too many consecutive segmentation points. Detailed rejection rules and the user interface of our verification task can be found in Appendix D. For each dialogue, we asked two unique workers to do the annotation and calculated the Cohen\u2019s Kappa scores between their annotations and the labels constructed by our data collection method. Finally, the Kappa score is 0.536, which is higher than TIAGE\u2019s Kappa score. Therefore, we can conclude that SuperDialseg has similar or even higher quality than the manually collected corpus (i.e., TIAGE) so that it can serve as a good training resource and testbed for the task of dialogue segmentation. Note that our automatic data collection method does not require human effort and can be easily scaled up as long as extra annotated document-grounded dialogue corpora are available."
        },
        {
            "heading": "4 Task definition",
            "text": "Dialogue Segmentation aims to segment a dialogue D = {U1, U2, ..., Un} into several pieces according to their discussed topics, where Ui contains not only the utterance ui, but also some properties including the role ri and the dialogue act dai.\n4https://www.mturk.com/\nWith the SuperDialseg dataset, we define it as a supervised learning task by labeling each utterance with y = 1 if it is the end of a segment, otherwise, y = 0."
        },
        {
            "heading": "5 Experiments",
            "text": ""
        },
        {
            "heading": "5.1 Datasets",
            "text": "SuperDialseg is our proposed dataset, containing 9,478 conversations with rich annotations. Details are provided in Section 3.\nTIAGE (Xie et al., 2021) is a manually annotated dataset extended from PersonaChat (Zhang et al., 2018) comprising 500 samples, where 300, 100, and 100 samples are for the training, testing, and validation sets, respectively.\nDialSeg711 (Xu et al., 2021) is an evaluation dataset containing 711 multi-turn conversations, which are synthesized by combining singletopic dialogues sampled from the MultiWOZ (Budzianowski et al., 2018) and the Stanford Dialogue Dataset (Eric et al., 2017)."
        },
        {
            "heading": "5.2 Comparison Methods",
            "text": "We construct an extensive benchmark involving 18 models across five categories, including random methods, unsupervised traditional methods, unsupervised DL-based methods, LLM-based methods, and supervised DL-based methods.\nRandom Methods. We design two random methods to show the bottom bound of the dialogue segmentation task. In an n-turn dialogue, we randomly select the number of segmentation points c from {0, \u00b7 \u00b7 \u00b7 , n \u2212 1}. Then, Random places a segmentation point with the probability cn , whereas Even distributes the segmentation boundaries evenly based on the interval bnc c.\nUnsupervised Traditional Methods. We select three representative algorithms in this category. BayesSeg (Eisenstein and Barzilay, 2008) is a Bayesian lexical cohesion segmenter. GraphSeg (Glava\u0161 et al., 2016) segments by solving maximum clique problem of a semantic association graph with utterance nodes. TextTiling (Hearst, 1997) determines the segmentation point when the calculated depth score of a pair of adjacent utterances is beyond a given threshold.\nUnsupervised DL-based Methods. Song et al. (2016) enhanced TextTiling by word embeddings. We use Glove embedding (Pennington et al., 2014) here and denote it as TextTiling+Glove.\nTextTiling+[CLS] (Xu et al., 2021) further enhanced TextTiling with [CLS] representation from pre-trained BERT (Devlin et al., 2018). Xu et al. (2021) improved the TextTiling algorithm to be the GreedySeg algorithm to provide more accurate dialogue segments for response selection. Xing and Carenini (2021) designed TextTiling+NSP, which uses the next sentence prediction (NSP) score of BERT as the similarity between two adjacent utterances to enhance TextTiling, and also pre-trained a coherence scoring model (CSM) on large-scale dialogue corpora with 553k samples to compute the coherence score of two adjacent utterances instead. Koshorek et al. (2018) proposed an LSTM-based supervised text segmentation model called TextSeg. Since it is trained on the Wiki727K dataset without using annotated dialogue segmentation datasets, we regard it as an unsupervised method for dialogue segmentation, denoted as TextSegtext.\nLLM-based Methods. Recently, LLMs achieve remarkable success in various fields. We investigate how well they can segment dialogues by applying the InstructGPT (Ouyang et al., 2022) and ChatGPT based on simple prompt templates.\nSupervised DL-based Methods. We design some strong baselines based on pre-trained language models (PLMs), including BERT, RoBERTa (Liu et al., 2019a), and TOD-BERT (Wu et al., 2020). Besides, we train TextSeg but using SuperDialseg or TIAGE dataset, denoted as TextSegdial. Xie et al. (2021) proposed RetroTS-T5, which is a generative model for dialogue segmentation.\nWe try our best to reproduce the reported performances in published papers with official opensourced resources and our implementation. Most of the results are similar or even superior to those reported. Further details about all the methods and the implementation are provided in Appendix A for reproducing the results in our benchmark."
        },
        {
            "heading": "5.3 Evaluation Metrics",
            "text": "To evaluate the model performances, we use the following metrics: (1) Pk-error (Beeferman et al., 1999) checks whether the existences of segmentation points within a sliding window in predictions and references are coincident; (2) WindowDiff (WD) (Pevzner and Hearst, 2002) is similar to Pk, but it checks whether the numbers of segmentation points within a sliding window in predictions and references are the same; Pk and WD compute soft errors of segmentation points within sliding\nwindows, and lower scores indicate better performances. (3) F1 score; (4) mean absolute error (MAE) calculates the absolute difference between the numbers of segmentation points in predictions and references; (5) By considering the soft errors and the F1 score simultaneously, we use the following score for convenient comparison:\nScore = 2 \u2217 F1 + (1\u2212 Pk) + (1\u2212WD)\n4 , (1)\nsuggested by the ICASSP2023 General Meeting Understanding and Generation Challenge (MUG).5\nFor the Pk and WD, previous works adopted different lengths of sliding windows (Xu et al. (2021) used four, but Xing and Carenini (2021) used different settings), leading to an unfair comparison. Following Fournier (2013), we set the length of the sliding window as half of the average length of segments in a dialogue, which is more reasonable because each dialogue has a different average length of segments. In contrast to Xing and\n5https://2023.ieeeicassp.org/ signal-processing-grand-challenges/\nCarenini (2021), we use F1 (binary) instead of F1 (macro) in the evaluation, because we only care about the segmentation points."
        },
        {
            "heading": "5.4 Experimental Results and Analysis",
            "text": "We conduct extensive experiments to demonstrate the effectiveness of our proposed dataset and reveal valuable insights of dialogue segmentation. Table 3 shows the results of our benchmark."
        },
        {
            "heading": "5.4.1 Supervised vs. Unsupervised",
            "text": "When trained on SuperDialseg, supervised methods outperform all unsupervised methods on SuperDialseg by large margins, and they surpass most of the unsupervised methods on TIAGE when trained on TIAGE, which highlights the crucial role of supervised signals in dialogue segmentation. Despite the domain gap, supervised methods exhibit strong generalization ability on out-of-domain data. For example, when trained on SuperDialseg, RoBERTa surpasses all unsupervised methods on Dialseg711 and all except CSM on TIAGE."
        },
        {
            "heading": "5.4.2 SuperDialseg vs. TIAGE",
            "text": "Although TIAGE is the first open-sourced supervised dataset for dialogue segmentation, it only has 300 training samples, which may not be sufficient for deep learning models. Our experiments show that trained on TIAGE, BERT and TOD-BERT perform poorly on Dialseg711 with low F1 scores, while they even perform worse than random methods on SuperDialseg. Additionally, we observe that when trained on TIAGE, RetroTS-T5 usually generates strange tokens such as \u2018<extra_id_0> I\u2019 and \u2018Doar pentru\u2019, rather than \u2018negative\u2019/\u2018positive\u2019. These observations reflect that small training sets generally result in unstable training processes.\nBesides, we try to conduct a comparison between SuperDialseg and TIAGE on the same scale. We randomly selected 300/100 training/validation samples from SuperDialseg, denoted as SuperDialseg300. Table 4 shows that though the performances are worse than those trained with the full training set, training with the subset of SuperDialseg is still more stable and performs relatively well on all datasets. We believe it is because SuperDialseg has more diverse patterns for dialogue segmentation. Furthermore, except for RoBERTa, other models trained on SuperDialseg outperform those trained on TIAGE when evaluating on Dialseg711, which proves the superiority of our proposed dataset."
        },
        {
            "heading": "5.4.3 DialogueSeg vs. TextSeg",
            "text": "It may seem plausible that models trained on large amounts of encyclopedic documents can be used to segment dialogues if we treat them as general texts. However, our results show that even though TextSegtext is trained with more than 100 times (w.r.t. SuperDialseg) and even 2,400 times (w.r.t. TIAGE) more data than TextSegdial, it performs significantly worse than TextSegdial on all evaluation datasets. We attribute this to the fact that TextSegtext neglects the special characteristics of dialogues, and thus, is unsuitable for dia-\nlogue segmentation. For example, documents generally discuss a topic with many long sentences, whereas topics in conversations generally involve fewer relatively shorter utterances. Therefore, even though we have large-scale text segmentation corpora, learning only on documents is not sufficient for segmenting dialogues, which convinces the necessity of constructing large-scale supervised segmentation datasets specially for dialogues."
        },
        {
            "heading": "5.4.4 Effect of Dialogue-related Annotations",
            "text": "SuperDialseg provides additional dialogue-related annotations such as role information and dialogue acts. We conduct an analysis to show their effect on dialogue segmentation. We introduce role information and dialogue act by adding trainable embeddings to the inputs, denoted as the Multi-view RoBERTa (MVRoBERTa). Table 5 shows that MVRoBERTa outperforms the vanilla RoBERTa, which confirms that considering dialogue-related annotations is important for dialogue segmentation."
        },
        {
            "heading": "5.4.5 Other Valuable Findings",
            "text": "CSM is a strong unsupervised method, as it achieves the best performance on TIAGE and Dialseg711 compared with other unsupervised methods, LLM-based methods, and supervised methods, demonstrating the potential of unsupervised approaches in this task. However, it does not perform consistently well on SuperDialseg. As previously discussed in Section 5.1, utterances in Dialseg711 are incoherent when topics shift, and dialogues in TIAGE \u2018rush into changing topics\u2019 as stated in Xie et al. (2021). Therefore, CSM, which mainly learns sentence coherence features and coarse topic\ninformation, fails to segment real-world conversations with fine-grained topics. This observation strengthens the necessity of our SuperDialseg dataset for evaluating dialogue segmentation models properly. Surprisingly, ChatGPT, despite using a simple prompt, achieves the best performance on SuperDialseg and Dialseg711 compared with other methods without accessing annotated corpora. Moreover, LLM-based methods perform competitively on TIAGE. We believe with better prompt engineering, they can serve as stronger baselines. Additionally, BayesSeg performs relatively well compared to some DL-based methods, showing the importance of topic modeling in this task. We highlight this as a future direction in this area."
        },
        {
            "heading": "6 Conclusions",
            "text": "In this paper, we addressed the necessities and challenges of building a large-scale supervised dataset for dialogue segmentation. Based on the characteristics of DGDS, we constructed a large-scale supervised dataset called SuperDialseg, also inheriting rich annotations such as role information and dialogue act. Moreover, we confirmed the quality of our automatically constructed dataset through human verification. We then provided an extensive benchmark including 18 models across five categories on three dialogue segmentation datasets, including SuperDialseg, TIAGE, and Dialseg711.\nTo evidence the significance and superiority of our proposed supervised dataset, we conducted extensive analysis in various aspects. Empirical studies showed that supervised signals are essential for dialogue segmentation and models trained on SuperDialseg can also be generalized to out-ofdomain datasets. Based on the comparison between the existing dataset (i.e. TIAGE) and SuperDialseg on the same scale, we observed that models trained on SuperDialseg had a steady training process and performed well on all datasets, probably due to its diversity in dialogue segmentation patterns, therefore, it is suitable for this task. Furthermore, the comparison between RoBERTa and MVRoBERTa highlighted the importance of dialogue-related annotations previously overlooked in this task.\nAdditionally, we also provided some insights for dialogue segmentation, helping the future development in this field. Moreover, our dataset and codes are publicly available to support future research.6\n6https://github.com/Coldog2333/SuperDialseg\nLimitations\nIn this study, we notice that our proposed method of introducing dialogue-related information including role information and dialogue act (i.e., MVRoBERTa) performs well but the way of exploiting these annotations is simple. We believe there is much room for improvement. Therefore, we need to find a more proper and effective way to make full use of the dialogue-related information, expanding the potential applications of supervised dialogue segmentation models.\nBesides, our dataset is only available for training and evaluating English dialogue segmentation models. Recently, Fu et al. (2022) proposed a Chinese DGDS dataset with fine-grained grounding reference annotations, which can also be used to construct another supervised dataset for Chinese dialogue segmentation. We leave it as a future work.\nEthics Statement\nIn this paper, we ensured that we adhered to ethical standards by thoroughly reviewing the copyright of two involved DGDS datasets, the doc2dial and the MultiDoc2Dial. We constructed the SuperDialseg dataset in compliance with their licenses (Apache License 2.0).7 We took care of the rights of the creators and publishers of these datasets and added citations properly.\nAs for the process of human verification, we collected the data from Amazon Mechanical Turk. Each assignment cost the workers 20 minutes on average. Each worker was well paid with $2.5 after completing one assignment, namely, the hourly wage is $7.5, which is higher than the federal minimum hourly wage in the U.S. ($7.25)."
        },
        {
            "heading": "Acknowledgements",
            "text": "This work was supported by JST SPRING, Grant Number JPMJSP2108."
        },
        {
            "heading": "A Details for Reproduction",
            "text": "A.1 Comparison Methods\nTo guarantee a fair comparison in our benchmark, we use the open-sourced codes and pre-trained models of the comparison methods as much as possible to evaluate their performances. However, some additional details are required to reproduce their reported performances. Therefore, we conduct a similar level of hyperparameter tuning for each model to obtain similar or better results than those reported in previously published papers.\nFor the GraphSeg, we set the value of the relatedness threshold as 0.5 and the minimum segmentation sizes as 3 to obtain a better performance than that reported in Xing and Carenini (2021). For TextTiling+Glove, we used the version that was pre-trained with 42 billion tokens of web data from Common Crawl8. For GreedySeg and CSM, we corrected some inconsistencies in their opensourced codes with respect to their original published papers and obtained better performances than their reported results.\nFor the result of LLM-based methods, we used the OpenAI API9 with the best model text-davinci-003 for InstructGPT and the recent model gpt-3.5-turbo-0613 for ChatGPT in June 2023. The maximum number of completion tokens was 512, and the temperature was set to 0. We used the definition from our paper as described in Section 4 as the task instruction, together with the dialogue input, and an output example specifying the output format to form the input prompt. Table 6 shows our constructed prompt templates for the dialogue segmentation task.\nFor the PLM-based supervised methods, we experimented with the hierarchical architectures but did not achieve satisfactory results. Therefore, we followed the framework of Cohan et al. (2019) to concatenate all the utterances as a long sequence as input and use the [SEP]/</s> tokens at the end of each utterance except for TOD-BERT to perform the classification. Note that there are two separate tokens </s> in the original implementation of RoBERTa, we use the first </s> token to do classification. For TOD-BERT, each utterance has a special token representing its role, so we use the special role tokens [sys] and [usr] to perform classification. The maximum number of tokens in an utterance is 25. The size of the utterance-level sliding window during training and inference is |T | = 20, and the stride is 1.\nOther implementations that are not specified follow the official codes and their papers with default hyperparameters.\nA.2 Implementation Details\nWe implemented all the models using PyTorch (Paszke et al., 2019) and downloaded model weights of PLMs from huggingface Transformers (Wolf et al., 2020). We also utilized Pytorch Light-\n8https://nlp.stanford.edu/projects/glove/ 9https://openai.com/api/\nning 10 to build the deep learning pipeline in our experiments. The AdamW (Loshchilov and Hutter, 2018) optimizer was used to optimize models\u2019 parameters with a learning rate of 1e-5 and a batch size of 8. L2-regularization with weight decay of 1e-3 was also applied. We trained our supervised models with 20 epochs for SuperDialseg and 40 epochs for TIAGE and SuperDialseg-300, respectively. We employed early stopping when the calculated score (metric (5) in Section 5.3) did not improve for half of the total number of epochs. For RetroTS-T5, we trained the generative T5 for 3 epochs on SuperDialseg and 40 epochs on TIAGE.\nAll experiments were conducted on a single A100 80GB GPU. Except for RetroTS-T5, all experiments took no longer than 1 hour to finish, whereas training RetroTS-T5 on SuperDialseg required approximately six hours. To reduce experimental error, we did experiments in several times and reported the average performances in this paper. T-test was also conducted to confirm the significance of the comparison."
        },
        {
            "heading": "B Detailed Information for Data Source",
            "text": "In real-world applications, the demand for guiding end users to seek relevant information and complete tasks is rapidly increasing. Feng et al. (2020) proposed a goal-oriented document-grounded dialogue dataset, called doc2dial, to help this line of research. To avoid additional noise from the post-hoc human annotations of dialogue data (Geertzen and Bunt, 2006), they designed the dialogue flows in advance. A dialogue flow is a series of interactions between agents and users, where each turn contains a dialogue scene that consists of dialogue acts, role information, and grounding contents from a given document. Specifically, by varying these three factors that are further constrained by the semantic graph extracted from documents and dialogue history, they dynamically built diverse dialogue flows turn-by-turn. Then, they asked crowdsourcing annotators to create the utterances for each dialogue scene within a dialogue flow. It should be noted that although the dialogue flows were generated automatically, the crowdsourcing annotators would reject some unnatural dialogue flows if they found any dialogue turn that was not feasible to write an utterance. Moreover, each dialogue was created based on a unique dialogue flow.\nUsually, goal-oriented information-seeking con10https://pytorch-lightning.readthedocs.io/\nversations involved multiple documents. Based on the doc2dial dataset, Feng et al. (2021a) randomly combined sub-dialogues from doc2dial which ground on different documents to construct the MultiDoc2Dial dataset. Therefore, each utterance only grounds on a single document. To maintain the natural dialogue flow after the combination, they only split a sub-dialogue after an agent turn with dialogue act as \u2018responding with an answer\u2019 while the next turn is not \u2018asking a follow-up question\u2019. Besides, they also recruited crowdsourcing annotators to rewrite some unnatural utterances if the original utterances from doc2dial are inappropriate after combination.\nFurther details about the data collection methods for doc2dial and MultiDoc2Dial can be found in the original papers (Feng et al., 2020, 2021a)."
        },
        {
            "heading": "C Detailed Information of SuperDialseg",
            "text": "Figure 3 shows the detailed distribution of the number of segments in the test set of SuperDialseg."
        },
        {
            "heading": "D Detailed Information about Human Verification",
            "text": "Figure 4 shows the user interface of our human verification task. We randomly sampled 100 samples from the test set of SuperDialseg for human verification. For the sake of convenience in designing the user interface, we only selected those dialogues with less than 15 utterances.\nWe set up a series of constraints to ensure the quality of collected human verification data. First of all, we set up a constraint that only those workers whose HIT approval rate for all requesters\u2019 HITs\nis greater than 95% can access our task, which ensures the quality of our recruited annotators. However, we still observed some bad annotations. Therefore, we rejected those assignments if they satisfied any of the following conditions:\n\u2022 The last utterance was not annotated as a segmentation point, indicating that the annotator did not read our given instructions carefully or did not fully understand our task.\n\u2022 The \u2018N/A\u2019 utterance was annotated as a segmentation point, indicating that the annotator did not read our given instructions carefully.\n\u2022 The example was modified, indicating that the annotator did not read our given instructions carefully.\n\u2022 An annotation had more than three consecutive segmentation points, which is impossible in our cases.\n\u2022 All the annotations remained in default.\nBesides, we rejected those workers and would not approve any annotations from them ever if they satisfied any of the following conditions, because they were probably spam users:\n\u2022 One submitted an assignment that had more than 10 consecutive segmentation points.\n\u2022 One submitted an assignment whose all the annotations remained in default.\nWe hope that our practical experience in collecting verification data can also be helpful for future researchers who want to collect annotated dialogue segmentation data by crowdsourcing."
        },
        {
            "heading": "U1: [Utterance 1]",
            "text": ""
        },
        {
            "heading": "U1: [Utterance 1]",
            "text": ""
        }
    ],
    "title": "SuperDialseg: A Large-scale Dataset for Supervised Dialogue Segmentation",
    "year": 2023
}