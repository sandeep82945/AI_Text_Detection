{
    "abstractText": "In many Natural Language Processing applications, neural networks have been found to fail to generalize on out-of-distribution examples. In particular, several recent semantic parsing datasets have put forward important limitations of neural networks in cases where compositional generalization is required. In this work, we extend a neural graph-based semantic parsing framework in several ways to alleviate this issue. Notably, we propose: (1) the introduction of a supertagging step with valency constraints, expressed as an integer linear program; (2) a reduction of the graph prediction problem to the maximum matching problem; (3) the design of an incremental early-stopping training strategy to prevent overfitting. Experimentally, our approach significantly improves results on examples that require structural generalization in the COGS dataset, a known challenging benchmark for compositional generalization. Overall, our results confirm that structural constraints are important for generalization in semantic parsing.",
    "authors": [
        {
            "affiliations": [],
            "name": "Alban Petit"
        },
        {
            "affiliations": [],
            "name": "Caio Corro"
        },
        {
            "affiliations": [],
            "name": "Fran\u00e7ois Yvon"
        }
    ],
    "id": "SP:c2155cf58844f8d6e01b33d00d56757df9cf211e",
    "references": [
        {
            "authors": [
                "Ekin Aky\u00fcrek",
                "Afra Feyza Aky\u00fcrek",
                "Jacob Andreas."
            ],
            "title": "Learning to recombine and resample data for compositional generalization",
            "venue": "International Conference on Learning Representations.",
            "year": 2021
        },
        {
            "authors": [
                "Jacob Andreas."
            ],
            "title": "Good-enough compositional data augmentation",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7556\u20137566, Online. Association for Computational Linguistics.",
            "year": 2020
        },
        {
            "authors": [
                "Srinivas Bangalore",
                "Aravind K. Joshi."
            ],
            "title": "Supertagging: An approach to almost parsing",
            "venue": "Computational Linguistics, 25(2):237\u2013265.",
            "year": 1999
        },
        {
            "authors": [
                "Guillaume Bonfante",
                "Bruno Guillaume",
                "Mathieu Morey."
            ],
            "title": "Dependency constraints for lexical disambiguation",
            "venue": "Proceedings of the 11th International Conference on Parsing Technologies (IWPT\u201909), pages 242\u2013253, Paris, France. Associa-",
            "year": 2009
        },
        {
            "authors": [
                "Marie Candito."
            ],
            "title": "Auxiliary tasks to boost biaffine semantic dependency parsing",
            "venue": "Findings of the Association for Computational Linguistics: ACL 2022, pages 2422\u20132429, Dublin, Ireland. Association for Computational Linguistics.",
            "year": 2022
        },
        {
            "authors": [
                "Henry Conklin",
                "Bailin Wang",
                "Kenny Smith",
                "Ivan Titov."
            ],
            "title": "Meta-learning to compositionally generalize",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natu-",
            "year": 2021
        },
        {
            "authors": [
                "Caio Corro."
            ],
            "title": "On the inconsistency of separable losses for structured prediction",
            "venue": "Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume. Association for Computational Linguistics.",
            "year": 2023
        },
        {
            "authors": [
                "David F Crouse."
            ],
            "title": "On implementing 2d rectangular assignment algorithms",
            "venue": "IEEE Transactions on Aerospace and Electronic Systems, 52(4):1679\u20131696.",
            "year": 2016
        },
        {
            "authors": [
                "R\u00f3bert Csord\u00e1s",
                "Kazuki Irie",
                "Juergen Schmidhuber."
            ],
            "title": "The devil is in the detail: Simple tricks improve systematic generalization of transformers",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 619\u2013",
            "year": 2021
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
            "year": 2019
        },
        {
            "authors": [
                "Timothy Dozat",
                "Christopher D. Manning."
            ],
            "title": "Deep biaffine attention for neural dependency parsing",
            "venue": "International Conference on Learning Representations.",
            "year": 2017
        },
        {
            "authors": [
                "Timothy Dozat",
                "Christopher D. Manning."
            ],
            "title": "Simpler but more accurate semantic dependency parsing",
            "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 484\u2013490, Melbourne,",
            "year": 2018
        },
        {
            "authors": [
                "Catherine Finegan-Dollak",
                "Jonathan K. Kummerfeld",
                "Li Zhang",
                "Karthik Ramanathan",
                "Sesh Sadasivam",
                "Rui Zhang",
                "Dragomir Radev."
            ],
            "title": "Improving textto-SQL evaluation methodology",
            "venue": "Proceedings of the 56th Annual Meeting of the Association for",
            "year": 2018
        },
        {
            "authors": [
                "Jeffrey Flanigan",
                "Sam Thomson",
                "Jaime Carbonell",
                "Chris Dyer",
                "Noah A. Smith."
            ],
            "title": "A discriminative graph-based parser for the Abstract Meaning Representation",
            "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics",
            "year": 2014
        },
        {
            "authors": [
                "Daniel Furrer",
                "Marc van Zee",
                "Nathan Scales",
                "Nathanael Sch\u00e4rli."
            ],
            "title": "Compositional generalization in semantic parsing: Pre-training vs",
            "venue": "specialized architectures.",
            "year": 2021
        },
        {
            "authors": [
                "Jonathan Gordon",
                "David Lopez-Paz",
                "Marco Baroni",
                "Diane Bouchacourt."
            ],
            "title": "Permutation equivariant models for compositional generalization in language",
            "venue": "International Conference on Learning Representations.",
            "year": 2020
        },
        {
            "authors": [
                "Jonas Groschwitz",
                "Matthias Lindemann",
                "Meaghan Fowlie",
                "Mark Johnson",
                "Alexander Koller."
            ],
            "title": "AMR dependency parsing with a typed semantic algebra",
            "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Vol-",
            "year": 2018
        },
        {
            "authors": [
                "Jonathan Herzig",
                "Jonathan Berant."
            ],
            "title": "Spanbased semantic parsing for compositional generalization",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natu-",
            "year": 2021
        },
        {
            "authors": [
                "Jonathan Herzig",
                "Peter Shaw",
                "Ming-Wei Chang",
                "Kelvin Guu",
                "Panupong Pasupat",
                "Yuan Zhang."
            ],
            "title": "Unlocking compositional generalization in pre-trained models using intermediate representations",
            "venue": "arXiv preprint arXiv:2104.07478.",
            "year": 2021
        },
        {
            "authors": [
                "Sepp Hochreiter",
                "J\u00fcrgen Schmidhuber."
            ],
            "title": "Long short-term memory",
            "venue": "Neural Comput., 9(8):1735\u20131780.",
            "year": 1997
        },
        {
            "authors": [
                "Dora Jambor",
                "Dzmitry Bahdanau."
            ],
            "title": "LAGr: Label aligned graphs for better systematic generalization in semantic parsing",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages",
            "year": 2022
        },
        {
            "authors": [
                "Robin Jia",
                "Percy Liang."
            ],
            "title": "Data recombination for neural semantic parsing",
            "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 12\u201322, Berlin, Germany. Association for Computa-",
            "year": 2016
        },
        {
            "authors": [
                "Roy Jonker",
                "Ton Volgenant."
            ],
            "title": "A shortest augmenting path algorithm for dense and sparse linear assignment problems",
            "venue": "DGOR/NSOR: Papers of the 16th Annual Meeting of DGOR in Cooperation with NSOR/Vortr\u00e4ge der 16. Jahrestagung der DGOR",
            "year": 1988
        },
        {
            "authors": [
                "Aravind K Joshi",
                "Leon S Levy",
                "Masako Takahashi."
            ],
            "title": "Tree adjunct grammars",
            "venue": "Journal of computer and system sciences, 10(1):136\u2013163.",
            "year": 1975
        },
        {
            "authors": [
                "Laura Kallmeyer."
            ],
            "title": "Parsing Beyond Context-Free Grammars",
            "venue": "Springer Science & Business Media.",
            "year": 2010
        },
        {
            "authors": [
                "Richard M. Karp."
            ],
            "title": "Reducibility among Combinatorial Problems, pages 85\u2013103",
            "venue": "Springer US, Boston, MA.",
            "year": 1972
        },
        {
            "authors": [
                "Najoung Kim",
                "Tal Linzen."
            ],
            "title": "COGS: A compositional generalization challenge based on semantic interpretation",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 9087\u20139105, Online. As-",
            "year": 2020
        },
        {
            "authors": [
                "Kris Korrel",
                "Dieuwke Hupkes",
                "Verna Dankers",
                "Elia Bruni."
            ],
            "title": "Transcoding compositionally: Using attention to find more generalizable solutions",
            "venue": "Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP,",
            "year": 2019
        },
        {
            "authors": [
                "Brenden Lake",
                "Marco Baroni."
            ],
            "title": "Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks",
            "venue": "Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Ma-",
            "year": 2018
        },
        {
            "authors": [
                "Matthias Lindemann",
                "Alexander Koller",
                "Ivan Titov"
            ],
            "title": "Compositional generalisation with structured reordering and fertility",
            "year": 2023
        },
        {
            "authors": [
                "Chenyao Liu",
                "Shengnan An",
                "Zeqi Lin",
                "Qian Liu",
                "Bei Chen",
                "Jian-Guang Lou",
                "Lijie Wen",
                "Nanning Zheng",
                "Dongmei Zhang."
            ],
            "title": "Learning algebraic recombination for compositional generalization",
            "venue": "Findings of the Association for Computational Lin-",
            "year": 2021
        },
        {
            "authors": [
                "Chunchuan Lyu",
                "Ivan Titov."
            ],
            "title": "AMR parsing as graph prediction with latent alignment",
            "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 397\u2013407, Melbourne, Australia. Association",
            "year": 2018
        },
        {
            "authors": [
                "Andr\u00e9 F.T. Martins",
                "M\u00e1rio A.T. Figueiredo",
                "Pedro M.Q. Aguiar",
                "Noah A. Smith",
                "Eric P. Xing."
            ],
            "title": "An augmented lagrangian approach to constrained map inference",
            "venue": "International Conference on Machine Learning.",
            "year": 2011
        },
        {
            "authors": [
                "Ryan McDonald",
                "Fernando Pereira",
                "Kiril Ribarov",
                "Jan Haji\u010d."
            ],
            "title": "Non-projective dependency parsing using spanning tree algorithms",
            "venue": "Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural",
            "year": 2005
        },
        {
            "authors": [
                "Stefan M\u00fcller."
            ],
            "title": "Grammatical theory: From transformational grammar to constraint-based approaches",
            "venue": "Language Science Press.",
            "year": 2016
        },
        {
            "authors": [
                "Radford M. Neal",
                "Geoffrey E. Hinton."
            ],
            "title": "A view of the EM algorithm that justifies incremental, sparse, and other variants",
            "venue": "Learning in graphical models, pages 355\u2013368. Springer.",
            "year": 1998
        },
        {
            "authors": [
                "Barbara Partee."
            ],
            "title": "Compositionality",
            "venue": "Varieties of formal semantics.",
            "year": 1984
        },
        {
            "authors": [
                "Panupong Pasupat",
                "Sonal Gupta",
                "Karishma Mandyam",
                "Rushin Shah",
                "Mike Lewis",
                "Luke Zettlemoyer."
            ],
            "title": "Span-based hierarchical semantic parsing for task-oriented dialog",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language",
            "year": 2019
        },
        {
            "authors": [
                "Alban Petit",
                "Caio Corro"
            ],
            "title": "On graph-based reentrancy-free semantic parsing",
            "year": 2023
        },
        {
            "authors": [
                "Linlu Qiu",
                "Peter Shaw",
                "Panupong Pasupat",
                "Pawel Nowak",
                "Tal Linzen",
                "Fei Sha",
                "Kristina Toutanova."
            ],
            "title": "Improving compositional generalization with latent structure and data augmentation",
            "venue": "Proceedings of the 2022 Conference of the North Ameri-",
            "year": 2022
        },
        {
            "authors": [
                "Jacob Russin",
                "Jason Jo",
                "Randall O\u2019Reilly",
                "Yoshua Bengio"
            ],
            "title": "Compositional generalization by factorizing alignment and translation",
            "venue": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop,",
            "year": 2020
        },
        {
            "authors": [
                "Kai Sheng Tai",
                "Richard Socher",
                "Christopher D. Manning."
            ],
            "title": "Improved semantic representations from tree-structured long short-term memory networks",
            "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th",
            "year": 2015
        },
        {
            "authors": [
                "Pia Wei\u00dfenhorn",
                "Lucia Donatelli",
                "Alexander Koller."
            ],
            "title": "Compositional generalization with a broadcoverage semantic parser",
            "venue": "Proceedings of the 11th Joint Conference on Lexical and Computational Semantics, pages 44\u201354, Seattle, Washington. Associa-",
            "year": 2022
        },
        {
            "authors": [
                "Jingfeng Yang",
                "Le Zhang",
                "Diyi Yang."
            ],
            "title": "SUBS: Subtree substitution for compositional semantic parsing",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technolo-",
            "year": 2022
        },
        {
            "authors": [
                "Yongjing Yin",
                "Jiali Zeng",
                "Yafu Li",
                "Fandong Meng",
                "Jie Zhou",
                "Yue Zhang."
            ],
            "title": "Consistency regularization training for compositional generalization",
            "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1:",
            "year": 2023
        },
        {
            "authors": [
                "Xingxing Zhang",
                "Jianpeng Cheng",
                "Mirella Lapata."
            ],
            "title": "Dependency parsing as head selection",
            "venue": "Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 665\u2013676,",
            "year": 2017
        },
        {
            "authors": [
                "Hao Zheng",
                "Mirella Lapata."
            ],
            "title": "Compositional generalization via semantic tagging",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2021, pages 1022\u20131032, Punta Cana, Dominican Republic. Association for Computational",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Semantic parsing aims to transform a natural language utterance into a structured representation. However, models based on neural networks have been shown to struggle on out-of-distribution utterances where compositional generalization is required, i.e., on sentences with novel combinations of elements observed separately during training (Lake and Baroni, 2018; Finegan-Dollak et al., 2018; Keysers et al., 2020). Jambor and Bahdanau (2022) showed that neural graph-based semantic parsers are more robust to compositional generalization than sequence-to-sequence (seq2seq) models. Moreover, Herzig and Berant (2021), Wei\u00dfenhorn et al. (2022) and Petit and Corro (2023) have shown that introducing valency and type constraints in a structured decoder improves compositional generalization capabilities.\nIn this work, we explore a different method for compositional generalization, based on supertagging. We demonstrate that local predictions (with global consistency constraints) are sufficient for compositional generalization. Contrary to Herzig and Berant (2021) and Petit and Corro (2023), our approach can predict any semantic graph (including ones with reentrancies), and contrary to Wei\u00dfenhorn et al. (2022) it does not require any intermediate representation of the semantic structure.\nMoreover, our experiments highlight two fundamental features that are important to tackle compositional generalization in this setting. First, as is well known in the syntactic parsing literature, introducing a supertagging step in a parser may lead to infeasible solutions. We therefore propose an integer linear programming formulation of supertagging that ensures the existence of at least one feasible parse in the search space, via the so-called companionship principle (Bonfante et al., 2009, 2014). Second, as the development dataset used to control training is in-distribution (i.e., it does not test for compositional generalization), there is a strong risk of overfitting. To this end, we propose an incremental early-stopping strategy that freezes part of the neural network during training.\nOur contributions can be summarized as follows:\n\u2022 we propose to introduce a supertagging step in a graph-based semantic parser;\n\u2022 we show that, in this setting, argument identification can be reduced to a matching problem;\n\u2022 we propose a novel approach based on inference in a factor graph to compute the weaklysupervised loss (i.e., without gold alignment);\n\u2022 we propose an incremental early-stopping strategy to prevent overfitting;\n\u2022 we evaluate our approach on COGS and observe that it outperforms comparable baselines on compositional generalization tasks.\nNotations. A set is written as {\u00b7} and a multiset as J\u00b7K. We denote by [n] the set of integers {1, ..., n}. We denote the sum of entries of the Hadamard product as \u27e8\u00b7, \u00b7\u27e9 (i.e., the standard scalar product if arguments are vectors). We assume the input sentence contains n words. We use the term \u201cconcept\u201d to refer to both predicates and entities."
        },
        {
            "heading": "2 Semantic parsing",
            "text": ""
        },
        {
            "heading": "2.1 COGS",
            "text": "The principle of compositionality states that\n\u201cThe meaning of an expression is a function of the meanings of its parts and of the way they are syntactically combined.\u201d (Partee, 1984)\nLinguistic competence requires compositional generalization, that is the ability to understand new utterances made of known parts, e.g., understanding the meaning of \u201cMarie sees Pierre\u201d should entail the understanding of \u201cPierre sees Marie\u201d.\nThe Compositional Generalization Challenge based on Semantic Interpretation (COGS, Kim and Linzen, 2020) dataset is designed to evaluate two types of compositional generalizations. First, lexical generalization tests a model on known grammatical structures where words are used in unseen roles. For example, during training the word hedgehog is only used as a subject; the model needs to generalize to cases where it appears as an object. Second, structural generalization tests a model on syntactic structures that were not observed during training. Illustrations are in Table 1.\nThe error analysis presented by Wei\u00dfenhorn et al. (2022) emphasizes that neural semantic parsers\nachieve good accuracy for lexical generalization but fail for structural generalization.\nSemantic graph construction. A semantic structure in the COGS dataset is represented as a logical form. We transform this representation into a graph as follows:\n1. For each concept instance, we add a labeled vertex.\n2. For each argument p of a concept instance p\u2032, we create a labeled arc from the vertex representing p\u2032 to the vertex representing p.\n3. COGS explicitly identifies definiteness of nouns. Therefore, for each definite noun that triggers a concept, we create a vertex p with label definite and we create an arc with label det from p to the vertex representing the noun\u2019s concept.1 Indefiniteness is marked by the absence of such structure.\nThis transformation is illustrated in Figure 1."
        },
        {
            "heading": "2.2 Graph-based decoding",
            "text": "The standard approach (Flanigan et al., 2014; Dozat and Manning, 2018; Jambor and Bahdanau, 2022) to graph-based semantic parsing is a two-step pipeline:\n1. concept tagging;\n2. argument identification.\n1Using the determiner as the head of a relation may be surprising for readers familiar with syntactic dependency parsing datasets, but there is no consensus among linguists about the appropriate dependency direction, see e.g., M\u00fcller (2016, Section 1.5) for a discussion.\nThe second step is a sub-graph prediction problem. Concept tagging. We assume that each word can trigger at most one concept. Let T be the set of concepts, including a special tag \u2205 \u2208 T that will be used to identify semantically empty words (i.e., words that do not trigger any concept). Let \u03bb \u2208 Rn\u00d7T be tag weights computed by the neural network. Without loss of generality, we assume that \u03bbi,\u2205 = 0, \u2200i \u2208 [n]. We denote a sequence of tags as a boolean vector x \u2208 {0, 1}n\u00d7T where xi,t = 1, t \u0338= \u2205, indicates that word i triggers concept t. This means that \u2200i \u2208 [n], \u2211 t\u2208T xi,t = 1. Given weights \u03bb, computing the sequence of tags of maximum linear weight is a simple problem.\nArgument identification. We denote L the set of argument labels, e.g., agent \u2208 L. The second step assigns arguments to concepts instances. For this, we create a labeled graph G = (V,A) where:\n\u2022 V = {i \u2208 [n]|xi,\u2205 \u0338= 0} is the set of vertices representing concept instances;\n\u2022 A \u2286 V \u00d7 V \u00d7 L is the set of labeled arcs, where (i, j, l) \u2208 A denotes an arc from vertex i to vertex j labeled l.\nIn practice, we construct a complete graph, including parallel arcs with different labels, but excluding self-loops. Given arc weights \u00b5 \u2208 RA, argument identification reduces to the selection of the subset of arcs of maximum weight such at most one arc with a given direction between any two vertices is selected. Again, this problem is simple. We denote a set of arcs as a boolean vector z \u2208 {0, 1}A where zi,j,l = 1 indicates that there is an arc from vertex i to vertex j labeled l in the prediction.\nMultiple concepts per words. More realistic semantic parsing scenarios require to trigger more than one concept per word. This only impacts the concept tagging step: one must change the model to allow the prediction of several concepts per word,\nfor example via multi-label prediction or using several tagging layers (Jambor and Bahdanau, 2022). The argument identification step is left unchanged, i.e., we create one vertex per predicted concept in the graph."
        },
        {
            "heading": "3 Supertagging for graph-based semantic parsing",
            "text": "In the syntactic parsing literature, supertagging refers to assigning complex descriptions of the syntactic structure directly at the lexical level (Bangalore and Joshi, 1999). For example, while an occurrence of the verb \u2018to walk\u2019 can be described in a coarse manner via its part-of-speech tag, a supertag additionally indicates that this verb appears in a clause with a subject on the left and a verbal phrase on the right, the latter also potentially requiring an object on its right, see Figure 2 for an illustration in the formalism of lexicalized TreeAdjoining Grammars (LTAGs, Joshi et al., 1975).\nWe propose to introduce an intermediary semantic supertagging step in a graph-based semantic parser. The pipeline of Section 2.2 becomes:\n1. concept tagging;\n2. semantic supertagging;\n3. argument identification.\nThis new pipeline is illustrated on Figure 3. Note that the introduction of the novel step does not impact the concept tagging step. As such, our approach is also applicable to datasets that would require multiple concepts prediction per word (see Section 2.2)."
        },
        {
            "heading": "3.1 Semantic supertagging",
            "text": "In our setting, a supertag indicates the expected arguments of a concept instance (potentially none for an entity) and also how the concept is used. Contrary to syntactic grammars, our supertags do not impose a direction. In the following, we refer to an expected argument as a substitution site and to an expected usage as a root.\nFormally, we define a (semantic) supertag as a multiset of tuples (l, d) \u2208 L \u00d7 {\u2212,+} where l is a label and d indicates either substitution site or root, e.g., (agent,\u2212) is a substitution site and (agent,+) is a root. For example, in Figure 1, the supertag associated with \u2019like\u2019 is J(agent,\u2212), (ccomp,\u2212)K\nand the one associated with \u2019prefer\u2019 is J(agent,\u2212), (xcomp,\u2212), (ccomp,+)K. The set of all supertags is denoted S.\nThe Companionship principle (CP).2 Let us first consider the following simple example: assuming we would like to parse the sentence \u201cMarie ate\u201d, yet associate the transitive supertag J(agent,\u2212), (theme,\u2212)K to the verb. In this case, the argument identification step will fail: the verb has no object in this sentence. The CP states that each substitution site must have a potential root in the supertag sequence. That is, in the supertagging step, we must make sure that the number of substitution sites with a given label exactly matches the number of roots with the same label, to ensure that there will exist at least one feasible solution for the next step of the pipeline. As such, supertagging here is assigning tags in context.\nTheorem 1. Given a set of supertags, a sequence of concept instances and associated supertag weights, the following problem is NP-complete: is there\n2We borrow the name from (Bonfante et al., 2009, 2014), although our usage is slightly different.\na sequence of supertag assignments with linear weight \u2265 m that satisfies the CP?\nProof. First, note that given a sequence of supertags, it is trivial to check in linear time that its linear weight is \u2265 m and that it satisfies the CP, therefore the problem is in NP. We now prove NPcompleteness by reducing 3-dimensional matching to supertagging with the CP.\n3-dim. matching is defined as follows: Let A = {a(i)}ni=1, B = {b(i)}ni=1 and C = {c(i)}ni=1 be 3 sets of n elements and D \u2286 A\u00d7B\u00d7C. A subset D\u2032 \u2286 D is a 3-dim. matching if and only if, for any two distinct triples (a, b, c) \u2208 D\u2032 and (a\u2032, b\u2032, c\u2032) \u2208 D\u2032, the following three conditions hold: a \u0338= a\u2032, b \u0338= b\u2032 and c \u0338= c\u2032.\nThe following decision problem is known to be NP-complete (Karp, 1972): given A, B, C and D, is there a 3-dim. matching D\u2032 \u2286 D with |D\u2032| \u2265 n?\nWe reduce this problem to supertagging with the CP as follows. We construct an instance of the problem with 3n concept instances a(1), ..., a(n), b(1), ..., b(n), c(1), ..., c(n). The supertag set S is defined as follows, where their associated weight is 0 except if stated otherwise:\n\u2022 For each triple (a, b, c) \u2208 D, we add a supertag J(b,\u2212), (c,\u2212)K with weight 1 if and only if it is predicted for concept a;\n\u2022 For each b \u2208 B, we add a supertag J(b,+)K with weight 1 if and only if it is predicted for concept b;\n\u2022 For each c \u2208 C, we add a supertag J(c,+)K with weight 1 if and only if it is predicted for concept c.\nIf there exists a sequence of supertag assignment satisfying the CP that has a weight \u2265 m = 3n, then there exists a solution for the 3-dim. matching problem, given by the supertags associated with concept instances a(1), ..., a(n).\nNote that an algorithm for the supertagging decision problem could rely on the maximisation variant as a subroutine. This result motivates the use of a heuristic algorithm. We rely on the continuous relaxation of an integer linear program that we embed in a branch-and-bound procedure.We first explain how we construct the set of supertags as it impacts the whole program.\nSupertag extraction. To improve generalization capabilities, we define the set of supertags as\ncontaining (1) the set of all observed supertags in the training set, augmented with (2) the crossproduct of all root combinations and substitution site combinations. For example, if the training data contains supertags J(ccomp,+), (agent,\u2212)K and J(agent,\u2212), (theme,\u2212)K, we also include J(ccomp,+), (agent,\u2212), (theme,\u2212)K and J(agent,\u2212)K in the set of supertags.\nFormally, let S+ (resp. S\u2212) be the set of root combinations (resp. substitution site combinations) observed in the data. The set of supertags is:\nS = { s+ \u222a s\u2212 \u2223\u2223\u2223\u2223 s+ \u2208 S+ \u2227 s\u2212 \u2208 S\u2212\u2227 s+ \u222a s\u2212 \u0338= JK } .\nNote that the empty multiset can not be a supertag. Supertag prediction. Let y\u2212 \u2208 {0, 1}n\u00d7S\u2212 and y+ \u2208 {0, 1}n\u00d7S+ be indicator variables of the substitution sites and roots, respectively, associated with each word, e.g. y\u2212i,s = 1 indicates that concept instance at position i \u2208 [n] has substitution sites s \u2208 S\u2212. We now describe the constraints that y\u2212 and y+ must satisfy. First, each position in the sentence should have exactly one set of substitution sites and one set of roots if and only if they have an associated concept:\u2211\ns\u2208S\u2212 y\u2212i,s = 1\u2212 xi,\u2205 \u2200i \u2208 [n] (1)\u2211 s\u2208S+ y+i,s = 1\u2212 xi,\u2205 \u2200i \u2208 [n] (2)\nNext, we forbid the empty supertag:\ny\u2212i,JK + y + i,JK \u2264 1 \u2200i \u2208 [n] (3)\nFinally, we need to enforce the companionship principle. We count in v\u2212s,l the number of substitution sites with label l \u2208 L in s \u2208 S\u2212, and similarly in v+s,l for roots. We can then enforce the number of roots with a given label to be equal to the number of substitution sites with the same label as follows:\u2211\ni\u2208[n], s\u2208S\u2212\ny\u2212i,sv \u2212 s,l = \u2211 i\u2208[n], s\u2208S+ y+i,jv + s,l \u2200l \u2208 L . (4)\nAll in all, supertagging with the companionship principle reduces to the following integer linear program:\nmax y\u2212,y+\n\u27e8y\u2212,\u03d5\u2212\u27e9+ \u27e8y+,\u03d5+\u27e9,\ns.t. (1\u20134),\ny\u2212 \u2208 {0, 1}n\u00d7S\u2212 ,y+ \u2208 {0, 1}n\u00d7S+ .\nIn practice, we use the CPLEX solver.3\nTiming. We initially implemented this ILP using the CPLEX Python API. The resulting implementation could predict supertags for only \u2248 10 sentences per second. We reimplemented the ILP using the CPLEX C++ API (via Cython) with a few extra optimizations, leading to an implementation that could solve \u2248 1000 instances per second."
        },
        {
            "heading": "3.2 Argument identification",
            "text": "The last step of the pipeline is argument identification. Note that in many cases, there is no ambiguity, see the example in Figure 3: as there is at most one root and substitution site per label, we can infer that the theme of concept instance eat is cake, etc. However, in the general case, there may be several roots and substitution sites with the same label. In the example of Figure 1, we would have 3 agent roots after the supertagging step.\nFor ambiguous labels after the supertagging step, we can rely on a bipartite matching (or assignment) algorithm. Let l \u2208 L be an ambiguous label. We construct a bipartite undirected graph as follows:\n\u2022 The first node set C contains one node per substitution site with label l;\n\u2022 The second node set C \u2032 contains one node per root with label l;\n\u2022 we add an edge for each pair (c, c\u2032) \u2208 C \u00d7C \u2032 with weight \u00b5i,j,l, where i \u2208 [n] and j \u2208 [n] are sentence positions of the substitution site represented by c and the root represented by c\u2032, respectively.\nWe then use the Jonker-Volgenant algorithm (Jonker and Volgenant, 1988; Crouse, 2016) to compute the matching of maximum linear weight with complexity cubic w.r.t. the number of nodes. Note that thanks to the companionship principle, there is always at least one feasible solution to this problem, i.e., our approach will never lead to a \u201cdead-end\u201d and will always predict a (potentially wrong) semantic parse for any given input."
        },
        {
            "heading": "4 Training objective",
            "text": "Supervised loss. Let (x\u0302, y\u0302\u2212, y\u0302+, z\u0302) be a gold annotation from the training dataset. We use separable negative log-likelihood losses (NLL) for each step as they are fast to compute and work well in\n3https://www.ibm.com/products/ ilog-cplex-optimization-studio\npractice (Zhang et al., 2017; Corro, 2023). The concept loss is a sum of one NLL loss per word:\n\u2113concept(\u03bb; x\u0302) =\u2212 \u27e8\u03bb, x\u0302\u27e9+ \u2211 i\u2208[n] log \u2211 t\u2208T exp\u03bbi,t .\nFor supertagging, we use the following losses:\n\u2113sub.(\u03d5 \u2212; y\u0302\u2212) =\u2212 \u27e8\u03d5\u2212, y\u0302\u2212\u27e9 + \u2211 i\u2208[n] log \u2211 s\u2208S\u2212 exp\u03d5\u2212i,s ,\n\u2113root(\u03d5 +; y\u0302+) =\u2212 \u27e8\u03d5+, y\u0302+\u27e9 + \u2211 i\u2208[n] log \u2211 s\u2208S+ exp\u03d5+i,s .\nFinally, for argument identification we have one loss per couple of positions in the sentence:\n\u2113arg.(\u00b5; z) =\u2212 \u27e8\u00b5, z\u27e9 + \u2211\n(i,j)\u2208[n]\u00d7[n] log \u2211 l\u2208L exp\u00b5i,j,l .\nNote that for the concept loss, we have a special empty tag with null score for the case where there is no concept associated with a word in the gold output (and similarly for argument identification).\nWeakly-supervised loss. In practice, it is often the case that we do not observe the alignment between concept instances and words in the training dataset, which must therefore be learned jointly with the parameters. To this end, we follow an \u201chard\u201d EM-like procedure (Neal and Hinton, 1998):\n\u2022 E step: compute the best possible alignment between concept instances and words;\n\u2022 M step: apply one gradient descent step using the \u201cgold\u201d tuple (x\u0302, y\u0302\u2212, y\u0302+, z\u0302) induced by the alignment from the E step.\nNote that the alignment procedure in the E step is NP-hard (Petit and Corro, 2023, Theorem 2), as the scoring function is not linear. For example, assume two concept instances p and p\u2032 such that p\u2032 is an argument of p. If p and p\u2032 are aligned with i and j, respectively, the alignment score includes the token tagging weights induced by this alignment plus the weight of the labeled dependency from i to j.\nWe propose to reduce the E step to maximum a posteriori (MAP) inference in a factor graph, see Figure 4. We define one random variable (RV) taking values in [n] per concept instance. The assignment of these RVs indicate the alignment between\nconcept instances and words. Unary factors correspond to tagging weights, e.g. aligning a concept t \u2208 T with word i \u2208 [n] induces weight \u03bbi,t. Binary factors correspond to argument identification: for each arc the semantic graph, we add a binary factor between the two concept instances RVs that will induce the dependency weight given the RVs assignment. Finally, there is a global factor acting as an indicator function, that forbids RVs assignments where different concept instances are aligned with the same word. We use AD3 (Martins et al., 2011) for MAP inference in this factor graph."
        },
        {
            "heading": "5 Related work",
            "text": "Compositional generalization. Compositional generalization has been a recent topic of interest in semantic parsing. This is because failure to generalize is an important source of error, especially in seq2seq models (Lake and Baroni, 2018; Finegan-Dollak et al., 2018; Herzig and Berant, 2021; Keysers et al., 2020). Several directions have been explored in response. Zheng and Lapata (2021) rely on latent concept tagging in the encoder of a seq2seq model, while Lindemann et al. (2023) introduce latent fertility and re-ordering layers in their model. Another research direction uses data augmentation methods to improve generalization (Jia and Liang, 2016; Andreas, 2020; Aky\u00fcrek et al., 2021; Qiu et al., 2022; Yang et al., 2022).\nSpan-based methods have also been shown to improve compositional generalization (Pasupat et al., 2019; Herzig and Berant, 2021; Liu et al., 2021). Particularly, Liu et al. (2021) explicitly represent input sentences as trees and use a Tree-LSTM (Tai et al., 2015) in their encoder. While this\nparser exhibits strong performance, this approach requires work from domain experts to define the set of operations needed to construct trees for each dataset. Other line of work that seek to tackle compositional generalization issues include using pretrained models (Herzig et al., 2021; Furrer et al., 2021), specialized architectures (Korrel et al., 2019; Russin et al., 2020; Gordon et al., 2020; Csord\u00e1s et al., 2021) and regularization (Yin et al., 2023).\nGraph-based semantic parsing. Graph-based methods have been popularized by syntactic dependency parsing (McDonald et al., 2005). To reduce computational complexity, Dozat and Manning (2018) proposed a neural graph-based parser that handles each dependency as an independent classification problem. Similar approaches were applied in semantic parsing, first for AMR parsing (Lyu and Titov, 2018; Groschwitz et al., 2018). Graph-based approaches have only recently been evaluated for compositional generalization. The approach proposed by Petit and Corro (2023) showed significant improvements compared to existing work on compositional splits of the GeoQuery dataset. However, their parser can only generate trees. Wei\u00dfenhorn et al. (2022) and Jambor and Bahdanau (2022) introduced approaches that can handle arbitrary graphs, a requirement to successfully parse COGS."
        },
        {
            "heading": "6 Experiments",
            "text": "We use a neural network based on a BiLSTM (Hochreiter and Schmidhuber, 1997) and a biaffine layer for arc weights (Dozat and Manning, 2017). More detail are given in Appendix A. As usual in the compositional generalization literature, we\nevaluate our approach in a fully supervised setting, i.e., we do not use a pre-trained neural network like BERT (Devlin et al., 2019). Code to reproduce the experiments is available online.4"
        },
        {
            "heading": "6.1 Early stopping",
            "text": "COGS only possesses an in-distribution development set and the accuracy of most parsers on this set usually reaches 100%. Previous work by Conklin et al. (2021) emphasized that the lack of a development set representative of the generalization set makes model selection difficult and hard to reproduce. They proposed to sample a small subset of the generalization set that is used for development. Both their work and LaGR (Jambor and Bahdanau, 2022) use this approach and sample a subset of 1000 sentences from the generalization set to use as their development set. However, we argue that this development set leaks compositional generalization information during training.\nWe propose a variant of early stopping to prevent overfitting on the in-distribution data without requiring a compositional generalization development set. We incrementally freeze layers in the neural network as follows: each subtask (predic-\n4https://github.com/alban-petit/ semantic-supertag-parser\ntion of tags, supertags, dependencies) is monitored independently on the in-distribution development set. As soon as one of these tasks achieves 100% accuracy, we freeze the shared part of the neural architecture (word embeddings and the BiLSTM). We also freeze the layers that produce the scores of the perfectly predicted task. For each subsequent task that achieves perfect accuracy, the corresponding layers are also frozen. This early stopping approach prevents overfitting.\nWe also experimented using the hinge loss instead of the NLL loss as it shares similar properties to our early stopping strategy: once a prediction is correct (including a margin between the gold output and other outputs), the gradient of the loss becomes null. We however found that this loss yields very low experimental results (null exact match score on the test set)."
        },
        {
            "heading": "6.2 Results",
            "text": "All results are exact match accuracy, i.e., the ratio of semantic structures that are correctly predicted. We report the overall accuracy,5 the accuracy over all lexical generalization cases as well as the individual accuracy for each structural generalization\n5As COGS contains 1,000 sentences for each generalization, case, this number mostly reflects the accuracy for lexical generalization, which account for 85.7% of the test set.\ncase. We report mean accuracy over 3 runs. External baselines. We compare our method to several baselines: (1) the seq2seq models of Kim and Linzen (2020), Aky\u00fcrek et al. (2021) and Zheng and Lapata (2021); (2) two graph-based models, LAGR (Jambor and Bahdanau, 2022) and the AM parser of Wei\u00dfenhorn et al. (2022); (3) LeAR (Liu et al., 2021), a semantic parser that relies on a more complex Tree-LSTM encoder (Tai et al., 2015). We also report the performance of LeAR when a BiLSTM is used in the encoder instead of the Tree-LSTM.\nOur baselines. We also report results for our model using the standard graph-based semantic parsing pipeline (Section 2.2), that is without the intermediary supertagging step. Note that, in this case, the supertagging loss becomes an auxiliary loss, as proposed by Candito (2022).\nResult comparison. We observe that our approach outperforms every baseline except LEAR. Importantly, our method achieves high exact match accuracy on the structural generalization examples, although the Obj to subj PP generalization remains difficult (our approach only reaches an accuracy of 75.0% for this case).\nWe now consider the effect of our novel inference procedure compared to our standard graphbased pipeline. It predicts PP recursion and CP recursion generalizations perfectly, where the baseline accuracy for these cases is 0. For Obj to subj PP generalization, our best configuration reaches an accuracy of 75.0%, 5 times more than the baselines. All in all, the proposed inference strategy improves results in the three structural generalizations subsets, and brings lexical generalization cases closer to 100% accuracy.\nImpact of training procedure. The early stopping approach introduced above has a clear impact\nfor Obj to subj PP, resulting in a 23.9 points increase (from 51.1 to 75.0). Such improvements are not observed for the baselines. From this, we conclude that our neural architecture tends to overfit the COGS training set and that some measures must be taken to mitigate this behaviour.\nSuppertagging accuracy. We report in Table 3 the supertagging accuracy with and without enforcing the companionship principle. We observe a sharp drop in accuracy for the Obj to Subj PP generalization when the companionship principle is not enforced. This highlights the importance of structural constraints to improve compositional generalization. We observe that the many error are due to the presence of the prepositional phrase just after the subject: this configuration causes the supertagger to wrongly assign a theme root to the subject, instead of agent. When the companionship principle is enforced, this mistake is corrected. An illustration is in Figure 5."
        },
        {
            "heading": "7 Conclusion",
            "text": "We proposed to introduce a supertagging step in a graph-based semantic parser. We analysed complexities and proposed algorithms for each step of our novel pipeline. Experimentally, our method significantly improves results for cases where compositional generalization is needed.\nLimitations\nOne limitation of our method is that we cannot predict supertags unseen during training (e.g., combinaison of roots unseen at training time). Note however that this problem is well-known in the syntactic parsing literature, and meta-grammars could be used to overcome this limitation. Another downside of our parser is the use of an ILP solver. Although it is fast when using the COGS dataset, this may be an issue in a more realistic setting. Finally, note that our method uses a pipeline, local predictions in the first steps cannot benefit from argument identification scores to fix potential errors."
        },
        {
            "heading": "Acknowledgments",
            "text": "We thank the anonymous reviewers and metareviewer for their comments and suggestions. This work was funded by the UDOPIA doctoral program in Artifial Intelligence from Universit\u00e9 Paris-Saclay (ANR-20-THIA-0013) and benefited from computations done on the Saclay-IA platform."
        },
        {
            "heading": "A Neural architecture",
            "text": "The neural architecture used in our experiments to produce the weights \u03bb, \u03d5+, \u03d5\u2212 and \u00b5 is composed of:\n\u2022 An embedding layer of dimension 200 followed by a bi-LSTM (Hochreiter and Schmidhuber, 1997) with a hidden size of 400.\n\u2022 A linear projection of dimension 300 followed by a RELU activation and another linear projection of dimension |T | to produce \u03bb.\n\u2022 A linear projection of dimension 200 followed by a RELU activation and another linear projection of dimension |S+| to produce \u03d5+.\n\u2022 A linear projection of dimension 200 followed by a RELU activation and another linear projection of dimension |S\u2212| to produce \u03d5\u2212.\n\u2022 A linear projection of dimension 200 followed by a RELU activation and a bi-affine layer to produce \u00b5.\nWe apply dropout with a probability of 0.3 over the outputs of each layer except the final layer for each weight matrix. The learning rate is 5\u00d7 10\u22124 and there are 30 sentences per mini-batch."
        }
    ],
    "title": "Structural generalization in COGS: Supertagging is (almost) all you need",
    "year": 2023
}