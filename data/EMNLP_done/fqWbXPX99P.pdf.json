{
    "abstractText": "Neural network models are vulnerable to adversarial examples, and adversarial transferability further increases the risk of adversarial attacks. Current methods based on transferability often rely on substitute models, which can be impractical and costly in real-world scenarios due to the unavailability of training data and the victim model\u2019s structural details. In this paper, we propose a novel approach that directly constructs adversarial examples by extracting transferable features across various tasks. Our key insight is that adversarial transferability can extend across different tasks. Specifically, we train a sequence-to-sequence generative model named CT-GAT (Cross-Task Generative Adversarial ATtack) using adversarial sample data collected from multiple tasks to acquire universal adversarial features and generate adversarial examples for different tasks. We conduct experiments on ten distinct datasets, and the results demonstrate that our method achieves superior attack performance with small cost. You can get our code and data at: https: //github.com/xiaoxuanNLP/CT-GAT",
    "authors": [
        {
            "affiliations": [],
            "name": "Minxuan Lv"
        },
        {
            "affiliations": [],
            "name": "Chengwei Dai"
        },
        {
            "affiliations": [],
            "name": "Kun Li"
        },
        {
            "affiliations": [],
            "name": "Wei Zhou"
        },
        {
            "affiliations": [],
            "name": "Songlin Hu"
        }
    ],
    "id": "SP:840cb763990501860700f5b1a4acb46f3c6f1244",
    "references": [
        {
            "authors": [
                "Moustafa Alzantot",
                "Yash Sharma",
                "Ahmed Elgohary",
                "Bo-Jhang Ho",
                "Mani Srivastava",
                "Kai-Wei Chang."
            ],
            "title": "Generating natural language adversarial examples",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2018
        },
        {
            "authors": [
                "Kalyani Asthana",
                "Zhouhang Xie",
                "Wencong You",
                "Adam Noack",
                "Jonathan Brophy",
                "Sameer Singh",
                "Daniel Lowd"
            ],
            "title": "Tcab: A large-scale text classification attack benchmark",
            "year": 2022
        },
        {
            "authors": [
                "Yangyi Chen",
                "Hongcheng Gao",
                "Ganqu Cui",
                "Fanchao Qi",
                "Longtao Huang",
                "Zhiyuan Liu",
                "Maosong Sun"
            ],
            "title": "Why should adversarial perturbations be imperceptible? rethink the research paradigm in adversarial nlp",
            "year": 2022
        },
        {
            "authors": [
                "Siddhartha Datta"
            ],
            "title": "Learn2weight: Parameter adaptation against similar-domain adversarial attacks",
            "year": 2022
        },
        {
            "authors": [
                "Thomas Davidson",
                "Dana Warmsley",
                "Michael W. Macy",
                "Ingmar Weber."
            ],
            "title": "Automated hate speech detection and the problem of offensive language",
            "venue": "National Conference on Artificial Intelligence.",
            "year": 2017
        },
        {
            "authors": [
                "Lucas Dixon",
                "John Li",
                "Jeffrey Sorensen",
                "Nithum Thain",
                "Lucy Vasserman."
            ],
            "title": "Measuring and mitigating unintended bias in text classification",
            "venue": "Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society, AIES \u201918, page 67\u201373, New York, NY,",
            "year": 2018
        },
        {
            "authors": [
                "Javid Ebrahimi",
                "Anyi Rao",
                "Daniel Lowd",
                "Dejing Dou"
            ],
            "title": "Hotflip: White-box adversarial examples for text classification",
            "year": 2018
        },
        {
            "authors": [
                "Steffen Eger",
                "G\u00f6zde G\u00fcl Sahin",
                "Andreas R\u00fcckl\u00e9",
                "Ji-Ung Lee",
                "Claudia Schulz",
                "Mohsen Mesgar",
                "Krishnkant Swarnkar",
                "Edwin Simpson",
                "Iryna Gurevych."
            ],
            "title": "Text processing like humans do: Visually attacking and shielding NLP systems",
            "venue": "CoRR,",
            "year": 2019
        },
        {
            "authors": [
                "Antigoni-Maria Founta",
                "Constantinos Djouvas",
                "Despoina Chatzakou",
                "Ilias Leontiadis",
                "Jeremy Blackburn",
                "Gianluca Stringhini",
                "Athena Vakali",
                "Michael Sirivianos",
                "Nicolas Kourtellis"
            ],
            "title": "Large scale crowdsourcing and characterization of twitter abusive",
            "year": 2018
        },
        {
            "authors": [
                "Wee Chung Gan",
                "Hwee Tou Ng."
            ],
            "title": "Improving the robustness of question answering systems to question paraphrasing",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6065\u20136075, Florence, Italy. Asso-",
            "year": 2019
        },
        {
            "authors": [
                "Ji Gao",
                "Jack Lanchantin",
                "Mary Lou Soffa",
                "Yanjun Qi."
            ],
            "title": "Black-box generation of adversarial text sequences to evade deep learning classifiers",
            "venue": "CoRR, abs/1801.04354.",
            "year": 2018
        },
        {
            "authors": [
                "Chuan Guo",
                "Alexandre Sablayrolles",
                "Herv\u00e9 J\u00e9gou",
                "Douwe Kiela"
            ],
            "title": "Gradient-based adversarial attacks against text transformers",
            "year": 2021
        },
        {
            "authors": [
                "Di Jin",
                "Zhijing Jin",
                "Joey Tianyi Zhou",
                "Peter Szolovits."
            ],
            "title": "Is BERT really robust? natural language attack on text classification and entailment",
            "venue": "CoRR, abs/1907.11932.",
            "year": 2019
        },
        {
            "authors": [
                "Mike Lewis",
                "Yinhan Liu",
                "Naman Goyal",
                "Marjan Ghazvininejad",
                "Abdelrahman Mohamed",
                "Omer Levy",
                "Ves Stoyanov",
                "Luke Zettlemoyer"
            ],
            "title": "Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension",
            "year": 2019
        },
        {
            "authors": [
                "Jinfeng Li",
                "Shouling Ji",
                "Tianyu Du",
                "Bo Li",
                "Ting Wang."
            ],
            "title": "Textbugger: Generating adversarial text against real-world applications",
            "venue": "CoRR, abs/1812.05271.",
            "year": 2018
        },
        {
            "authors": [
                "Linyang Li",
                "Ruotian Ma",
                "Qipeng Guo",
                "Xiangyang Xue",
                "Xipeng Qiu."
            ],
            "title": "BERT-ATTACK: Adversarial attack against BERT using BERT",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages",
            "year": 2020
        },
        {
            "authors": [
                "Aiwei Liu",
                "Honghai Yu",
                "Xuming Hu",
                "Shu\u2019ang Li",
                "Li Lin",
                "Fukun Ma",
                "Yawen Yang",
                "Lijie Wen"
            ],
            "title": "Character-level white-box adversarial attacks against transformers via attachable subwords substitution",
            "year": 2022
        },
        {
            "authors": [
                "Andrew L. Maas",
                "Raymond E. Daly",
                "Peter T. Pham",
                "Dan Huang",
                "Andrew Y. Ng",
                "Christopher Potts."
            ],
            "title": "Learning word vectors for sentiment analysis",
            "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human",
            "year": 2011
        },
        {
            "authors": [
                "Ray Oshikawa",
                "Jing Qian",
                "William Yang Wang."
            ],
            "title": "A survey on natural language processing for fake news detection",
            "venue": "International Conference on Language Resources and Evaluation.",
            "year": 2018
        },
        {
            "authors": [
                "Nicolas Papernot",
                "Patrick D. McDaniel",
                "Ian J. Goodfellow."
            ],
            "title": "Transferability in machine learning: from phenomena to black-box attacks using adversarial samples",
            "venue": "CoRR, abs/1605.07277.",
            "year": 2016
        },
        {
            "authors": [
                "Shuhuai Ren",
                "Yihe Deng",
                "Kun He",
                "Wanxiang Che."
            ],
            "title": "Generating natural language adversarial examples through probability weighted word saliency",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1085\u2013",
            "year": 2019
        },
        {
            "authors": [
                "Suranjana Samanta",
                "Sameep Mehta."
            ],
            "title": "Towards crafting text adversarial samples",
            "venue": "CoRR, abs/1707.02812.",
            "year": 2017
        },
        {
            "authors": [
                "Richard Socher",
                "Alex Perelygin",
                "Jean Wu",
                "Jason Chuang",
                "Christopher D. Manning",
                "Andrew Ng",
                "Christopher Potts."
            ],
            "title": "Recursive deep models for semantic compositionality over a sentiment treebank",
            "venue": "Proceedings of the 2013 Conference on Empiri-",
            "year": 2013
        },
        {
            "authors": [
                "Christian Szegedy",
                "Wojciech Zaremba",
                "Ilya Sutskever",
                "Joan Bruna",
                "Dumitru Erhan",
                "Ian J. Goodfellow",
                "Rob Fergus."
            ],
            "title": "Intriguing properties of neural networks",
            "venue": "2nd International Conference on Learning Representations, ICLR 2014, Banff, AB, Canada,",
            "year": 2014
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N. Gomez",
                "Lukasz Kaiser",
                "Illia Polosukhin."
            ],
            "title": "Attention is all you need",
            "venue": "CoRR, abs/1706.03762.",
            "year": 2017
        },
        {
            "authors": [
                "Zhibo Wang",
                "Hongshan Yang",
                "Yunhe Feng",
                "Peng Sun",
                "Hengchang Guo",
                "Zhifei Zhang",
                "Kui Ren."
            ],
            "title": "Towards transferable targeted adversarial examples",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages",
            "year": 2023
        },
        {
            "authors": [
                "Ellery Wulczyn",
                "Nithum Thain",
                "Lucas Dixon."
            ],
            "title": "Ex machina: Personal attacks seen at scale",
            "venue": "CoRR, abs/1610.08914.",
            "year": 2016
        },
        {
            "authors": [
                "Yong Xie",
                "Dakuo Wang",
                "Pin-Yu Chen",
                "Jinjun Xiong",
                "Sijia Liu",
                "Oluwasanmi Koyejo."
            ],
            "title": "A word is worth a thousand dollars: Adversarial attack on tweets fools stock prediction",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of",
            "year": 2022
        },
        {
            "authors": [
                "Liping Yuan",
                "Xiaoqing Zheng",
                "Yi Zhou",
                "Cho-Jui Hsieh",
                "Kai-Wei Chang",
                "Xuanjing Huang."
            ],
            "title": "Generating universal language adversarial examples by understanding and enhancing the transferability across neural models",
            "venue": "CoRR, abs/2011.08558.",
            "year": 2020
        },
        {
            "authors": [
                "Liping Yuan",
                "Xiaoqing Zheng",
                "Yi Zhou",
                "Cho-Jui Hsieh",
                "Kai wei Chang"
            ],
            "title": "On the transferability of adversarial attacksagainst neural text classifier",
            "year": 2021
        },
        {
            "authors": [
                "Yuan Zang",
                "Fanchao Qi",
                "Chenghao Yang",
                "Zhiyuan Liu",
                "Meng Zhang",
                "Qun Liu",
                "Maosong Sun."
            ],
            "title": "Word-level textual adversarial attacking as combinatorial optimization",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational",
            "year": 2020
        },
        {
            "authors": [
                "Guoyang Zeng",
                "Fanchao Qi",
                "Qianrui Zhou",
                "Tingji Zhang",
                "Bairu Hou",
                "Yuan Zang",
                "Zhiyuan Liu",
                "Maosong Sun."
            ],
            "title": "Openattack: An opensource textual adversarial attack toolkit",
            "venue": "CoRR, abs/2009.09191.",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Neural network-based natural language processing (NLP) is increasingly being applied in real-world tasks(Oshikawa et al., 2018; Xie et al., 2022; OpenAI, 2023). However, neural network models are vulnerable to adversarial examples(Papernot et al., 2016; Samanta and Mehta, 2017; Liu et al., 2022). Attackers can bypass model-based system monitoring by intentionally constructing adversarial samples to fulfill their malicious objectives, such as propagating rumors or hate speech. Even worse, researchers have discovered the phenomenon of adversarial transferability, where adversarial examples can propagate across models trained on the same or similar tasks(Papernot et al., 2016; Jin et al., 2019; Yuan et al., 2020; Datta, 2022; Yuan\n\u2217Kun Li is the corresponding author.\net al., 2021). This transferability enables attackers to target victim models using adversarial examples crafted by substitute models.\nThe majority of existing research on adversarial transferability involves training one or more substitute models that perform identical or similar tasks to the victim model (Datta, 2022; Yuan et al., 2021). However, due to the constraints of the black box scenario, the attacker lacks access to the training data and structural details of the victim model, making it exceedingly challenging to train comparable substitute models. Consequently, it becomes arduous to achieve a satisfactory success rate in adversarial attacks solely through transferability between models based on the same task.\nTo mitigate the above challenges, we introduce\na method that directly constructs adversarial examples by extracting transferable features across various tasks, without the need for constructing task-specific substitute models. Our key insight is that adversarial transferability is not limited to models trained on the same or similar tasks, but rather extends to models across different tasks. There are some observations that support our idea. For instance, we discovered that adversarial word substitution rules offer a variety of highly transferable candidate replacement words. As illustrated in Figure 1, a larger pool of candidate words can be utilized to generate a diverse set of highly transferable adversarial samples, thereby compensating for the shortcomings of previous methods that relied on greedy search. In particular, we train a sequence-tosequence generative model CT-GAT (Cross-Task Gerative Adversarial ATtack) using adversarial sample data obtained from multiple tasks. Remarkably, we find that the generated adversarial sample can effectively attack test tasks, even in the absence of specific victim model information.\nWe conduct attack experiments on five securityrelated NLP tasks across ten datasets, adhering to the Advbench(Chen et al., 2022) paradigm of Security-oriented Natural Language Processing (SoadNLP), a framework that mirrors real-world application scenarios more closely. And the experiments demonstrate that our method achieves the best attack performance while using a relatively small number of queries.\nTo summarize our contributions to this paper are as follows:\n\u2022 We introduce an approach that leverages texts to learn an effective, transferable adversarial text generator.\n\u2022 Our method can combine adversarial features from different tasks to enhance the adversarial effect.\n\u2022 We test the effectiveness of our method in the decision-based black-box scenario. The experiments demonstrate that our method achieves the best attack performance while using a relatively small number of queries."
        },
        {
            "heading": "2 Related Work",
            "text": "In this section, we review the recent works on adversarial attacks, with a focus on transferability-based approaches.\nWhite-box attacks are prominent methods in attack methods, which can obtain high attack success rate and low distortion adversarial data with few access times. Typical white-box attack methods included CWBA(Liu et al., 2022), GBDA(Guo et al., 2021). The effectiveness of obtaining adversarial samples efficiently in these scenarios stemmed from the fact that attackers have comprehensive access to information, including the model\u2019s structure, parameters, gradients, and training data.\nBlack-box attacks assume that the attacker only has knowledge of the confidence or decision of the target\u2019s output, including query-based attacks and transfer-based attacks. Query-based attack methods include PWWS(Ren et al., 2019), TextBugger(Li et al., 2018), Hotflip(Ebrahimi et al., 2018), etc. These methods often required hundreds of queries to successfully obtain low-distortion adversarial samples, and the attack effect may have been worse in the decision-based scenario.\nAdversarial transferability has been observed across different models(Szegedy et al., 2014). This poses a threat to the security of decision-based black-box models. Attackers employ white-box attacks to obtain adversarial samples on substitute models. This method is prone to overfitting, resulting in limited transferability. To address this issue, some methods proposed using mid-layer features instead of the entire model to obtain more generalized adversarial samples(Wang et al., 2023). However, this approach has not been widely applied in the text adversarial domain. Model aggregation methods were primarily used in the text domain, although the overall research in this area was not extensive. Additionally, there were approaches that extracted adversarial perturbation rules as a basis for adversarial features(Yuan et al., 2021). This rule-based approach provided insights into leveraging the distinctive characteristics of different adversarial samples."
        },
        {
            "heading": "3 Methodology",
            "text": ""
        },
        {
            "heading": "3.1 Textual Adversarial Attack",
            "text": "For a natural language classifier f , it can accurately classify the original input text into the label ytrue based on the maximum a priori probability:\nargmax yi\u2208Y\nf(yi|x) = ytrue (1)\nWhere ytrue represents the ground truth label of input x. To fool the classifier, the attacker in-\ntroduces a perturbation \u03b4 to the original input x, thereby creating an adversarial sample xadv.\nargmax yi\u2208Y\nf(yi|xadv) \u0338= ytrue, xadv = x+ \u03b4 (2)\nTypically, the perturbation \u03b4 is explicitly constrained, such as by setting a limit on the number of modifications(Li et al., 2018) or by imposing semantic constraints(Gan and Ng, 2019). In other words, the adversarial sample xadv is intended to remain similar to the original sample x. In our approach, this constraint is not explicitly imposed but is implicitly reflected in the training data. The adversarial examples in training data are obtained under explicit constraint conditions."
        },
        {
            "heading": "3.2 Transferability-Based Generator for Perturbations",
            "text": "To effectively learn the distributional characteristics from original sentences to adversarial sentences and generate diverse adversarial samples, we choose to utilize the encoder-decoder architecture of Transformer(Vaswani et al., 2017). The encoder uses self-attention mechanism, which can comprehensively extract the features and context information of the original sentence. The decoder can handle variable-length problems, enabling the learning of complex attacks such as character-level attacks and phrase-level attacks. The use of the encoder-decoder architecture allows us to combine the strengths of both components. The learning objective of this task can be formalized as maximizing\nthe following likelihood: L(U, U\u0302) = \u2211 i logP (ui|u<i, X,\u0398) (3)\nWe first train victim models on multiple datasets across different tasks, each using a distinct model structure. Successful adversarial samples are obtained from various attack methods on each victim model, and these samples are then mixed to serve as training data for the generator. We aim to learn features with high transferability across tasks and models through this data, and to integrate various attack methods to generate a more diverse set of adversarial samples. Due to the efficiency issues with traditional attack methods, where generating a large number of adversarial samples is time-consuming, we directly utilize the TCAB(Asthana et al., 2022) dataset as training data.\nThe TCAB dataset consists of two tasks with a total of six datasets: hate speech detection (Hatebase(Davidson et al., 2017), Civil Comments1, and Wikipedia(Wulczyn et al., 2016; Dixon et al., 2018)) and sentiment analysis (SST2(Socher et al., 2013), Climate Change2, and IMDB(Maas et al., 2011)).\nThe TCAB dataset consists of adversarial samples at different granularities, including word-level adversarial samples (Genetic(Alzantot et al., 2018), TextFooler etc.), character-level adversarial samples (DeepWordBug(Gao et al., 2018), HotFlip\n1https://www.kaggle.com/c/ jigsaw-unintended-bias-in-toxicity-classification\n2https://www.kaggle.com/edqian/ twitter-climate-change-sentiment-dataset\netc.), and mixed-granularity adversarial samples (TextBugger, Viper(Eger et al., 2019), etc.). These methods include both white-box attacks and blackbox attacks. The adversarial samples of each task are obtained by attacking BERT, RoBERTa, and XLNet respectively. Our generation network is trained across diverse datasets, models, and attack methods to learn universal adversarial features.\nWe don\u2019t strictly define the bounds of perturbations as previous methods did because it is implicitly reflected in the training data. In the adversarial sample generation stage, we can control the perturbation degree of the adversarial samples by adjusting the temperature parameter in the model logits."
        },
        {
            "heading": "4 Experiments and Results",
            "text": "In this chapter, we evaluate the performance of CT-GAT in the decision-based black-box scenario, using the security-oriented NLP benchmark Advbench. To demonstrate the high interpretability of CT-GAT in practical settings, we also conducted a manual evaluation."
        },
        {
            "heading": "4.1 Implementation",
            "text": "Victim Model BERT is a widely recognized pretrained transformer that is highly representative in natural language understanding (NLU) tasks. We chose to fine-tune the BERT model as the victim model for all tasks.\nGeneration Model We choose BART (Lewis et al., 2019) as our generation model. BART learns to generate text that is semantically similar to the original data by trying to recover it from its perturbed version. This enables BART to fully learn the similarities and differences between different word meanings and to have the ability to preserve semantic invariance. Moreover, BART is trained on a large amount of text and possesses the ability to analyze parts of speech, which allows it to further learn complex patterns of replacement during fine-tuning."
        },
        {
            "heading": "4.2 Baseline",
            "text": "Our baseline method follows the experimental setup of Advbench. We use the NLP attack package OpenAttack 3 (Zeng et al., 2020) to implement some common attacks. We implement the rocket attack using the source code provided by\n3https://github.com/thunlp/OpenAttack\nAdvbench. Specifically, the attack methods we employ include (1)TextFooler, (2)PWWS, (3)BERTAttack(Li et al., 2020), (4)SememePSO(Zang et al., 2020), (5)DeepWordBug(Gao et al., 2018), (6)ROCKET(Chen et al., 2022). Furthermore, we train CT-GATword using word-level adversarial samples from the TCAB to examine if it learns imperceptible perturbation features, as characterlevel attacks often introduce grammar errors and increase perplexity."
        },
        {
            "heading": "4.3 Datasets",
            "text": "We evaluate CT-GAT on nine datasets from Advbench, which include tasks such as misinformation, disinformation, toxic content, spam, and sensitive information detection (excluding the HSOL dataset(Davidson et al., 2017)). Since the HSOL dataset and the dataset used to train our adversarial sample generator are the same, we substitute it with the Founta dataset (Founta et al., 2018) in this study."
        },
        {
            "heading": "4.4 Evaluation metrics",
            "text": "To evaluate the effectiveness of CT-GAT, we need to measure the attack success rate, query count, perturbation degree, and text quality. (1) ASR is defined as the percentage of successful adversarial samples. (2)The query count is defined as the average number of queries required to make adversarial samples. (3)The perturbation degree is measured by the Levenshtein distance. (4)Text quality is measured by the relative increase in perplexity (% PPL) and the absolute increase in the number of grammer errors (\u2206I). (5)Semantic similarity is an evaluative metric quantifying the extent to which the meaning of the original sentence is preserved postperturbation. We represent this using the cosine similarity of Universal Sentence Encoder (USE) vectors."
        },
        {
            "heading": "4.5 Experimental Results",
            "text": "ASR and Query Count The ASR and query count results are shown in Table 1. Our method CTGAT achieves excellent results on all tasks and datasets. CT-GAT achieves the highest ASR and the least number of queries on majority of datasets. On a few datasets, even though it do not achieve the highest ASR, it achieved the second highest ASR. With only 2.3%-5.6% lower than the highest ASR method, the number of queries was reduced by 31.9%-91.2%. The ASR of CT-GAT did not achieve optimal results on the EDENCE, FAS, and\nEnron datasets, which is related to the degree of perturbation. This can be seen more clearly in Table 3, where our method has half the perturbation rate of the method with the highest ASR on these three datasets, which indicates that the imperceptibility learned from the data is constraining our perturbation method. Although we can increase the ASR by increasing the sampling temperature, this may greatly affect the readability and change the original sentence meaning.\nTo further evaluate the effectiveness of CT-GAT, we conduct additional experiments on two datasets, Jigsaw20184 and EDENCE, which have similar average query counts to the baseline. We conduct attacks on the model under strict constraints on the number of access queries, as shown in Figure 3. The differences in our replication of the ROCKET method are large, therefore they are not included in the graph for comparison. We observe that CT-GAT consistently outperforms other methods and achieves higher attack success rates even at extremely low query counts.\nPerturbation Degree, Text Quality and Consistency From Table 3, we can observe the level of perturbation and text quality of CT-GAT, and\n4https://www.kaggle.com/c/ jigsaw-toxic-comment-classification-challenge\nthe baseline. Considering the previous attack results, the CT-GATword method demonstrates good performance in terms of perturbation rate and the number of introduced grammar errors while maintaining satisfactory attack effectiveness. This may be attributed to the model\u2019s ability to discern parts of speech and assess the importance of substitute words. To some extent, this demonstrates the capability of CT-GAT to learn the distribution of adversarial examples and the implicit constraint of imperceptibility.\nHowever, perplexity does not necessarily reflect the real reading experience. For example, the ROCKET method uses a large number of repeated words or letters inserted into the original sentence, which makes the model easy to predict the words and reduces the perplexity. CT-GAT, due to the introduction of character-level attacks, does not seem outstanding in terms of grammatical errors and fluency. But in fact, we may not encounter too many obstacles when reading in practice. We follow the description of the Advbench, which emphasizes that the purpose of the attack is to propagate harmful information. We design human evaluations to measure the effectiveness of the method.\nIn terms of consistency, the cosine similarity of our method is at a relatively high level, indicating\nthat the sentences before and after the perturbation by our method maintain good semantic consistency. To further analyze semantic consistency, we adopt a human evaluation approach.\nHuman Evalution Automatic metrics provide insights into the quality of the text. To verify whether adversarial examples are truly effective in real-world applications, we adopt a manual evaluation approach.\nWe choose two tasks, hate speech detection and sensitive information detection, because these tasks have clear definitions and are easy to evaluate manually. We select the Founta dataset and the FAS\ndataset, each consisting of 40 pairs of original texts and adversarial texts. For each pair of samples, we ask 5 evaluators to assess the following aspects sequentially: (1) whether the adversarial sample conveys malicious intent (or contains sensitive information), and (2) the degree to which the intended meaning of the original sample and the adversarial sample aligns, measured on a scale of 0-2. The label of 0 indicates no relevance at all, 1 indicates partial conveyance of the original sentence\u2019s meaning, and 2 indicates substantial conveyance of the original sentence\u2019s meaning.\nThe results of the human evaluation are presented in Table 2. Labels of 1 or 2 are considered meaningful. It can be observed that CT-GAT can largely preserve the original meaning and effectively convey malicious intent."
        },
        {
            "heading": "4.6 Case Study",
            "text": "We select three successful adversarial examples from EDENCE and Amazon-LB. It can be observed from Table 4. The perturbation patterns of these adversarial samples originate from previous attack methodologies, effectively learning and integrating prior adversarial features at both the character and word levels. This includes tactics such as word segmentation, similar character substitution, and synonym replacement."
        },
        {
            "heading": "5 Further Analysis",
            "text": "In this chapter, our objective is to thoughtfully examine the areas where prior research could possibly be further enriched, particularly in terms of transferability. Simultaneously, we will elucidate the factors that enable our approach to effectively harness certain transferable attributes."
        },
        {
            "heading": "5.1 Cross-task Adversarial Features",
            "text": "In this section, we aim to validate our hypothesis that leveraging adversarial features from different tasks can enhance the attack success rate for the current task. Based on previous research(Yuan et al., 2021), we can formulate the extraction of adversarial features as a perturbation substitution rule. We propose Cross-task Adversarial Word Replacement rules (CAWR) to expand the rules in order to extract adversarial features across different tasks. The specific algorithm details can be found in Appendix A.\nWe partition the TCAB(Asthana et al., 2022) dataset into three subsets for our study: the Hate-\nbase dataset, the hate speech detection datasets (comprising Hatebase, Civil Comments), and a mixed dataset that combines sentiment detection with the previous abuse detection datasets and additional datasets (SST-2, Climate Change, and IMDB). We extract rules from these subsets and attack models on Jigsaw2018, Founta(Founta et al., 2018), and tweet5. Table 5 shows the results. It can be inferred that, overall, regardless of whether the adversarial sample construction method is simple and rule-based or based on model generation, the amalgamation of different task methods tends to result in a higher rate of successful attacks, or in other terms, enhanced transferability."
        },
        {
            "heading": "5.2 Selection of Words",
            "text": "In this subsection, we discuss some evidence that CT-GAT can learn transferable adversarial features\n5https://github.com/sharmaroshan/ Twitter-Sentiment-Analysis\ndirectly from the text. The process of generating adversarial samples can be considered a search problem(Zang et al., 2020), where we aim to find the vulnerable words and the optimal substitute words.\nVulnerable Words The selection of vulnerable words is the process of choosing words to be replaced. We aim to investigate whether different models exhibit similar patterns of vulnerable (or important) word distributions, which could be one of the manifestations of transferability across different models for the same task. We measure the vulnerability of words by sequentially masking them in a sentence and comparing the difference in confidence scores of the correct class before and after masking the input to the model.\nWe rank the vulnerability levels of the same sentence across different models and compare the Jaccard similarity of the selected top words to assess the consistency among different models. As shown in Table 6, We find some consistency both between\ndifferent model architectures and among training data from similar domains.\nWe extract the substitution rules from CTGATword for generating adversarial samples and find that the average perturbation rate is 16.6%. The proportion of the words replaced by CTGATword that belong to the top 20% vulnerable word set is 36%. These results indicate that it is possible to learn the vulnerable words of a sentence directly from the adversarial text.\nSubstitution preference Figure 1 presents the heatmap of transferability metrics for replacement rules, as derived through the HAWR algorithm by Yuan et al. (2021). Upon intuitive observation, it is apparent that certain replaced words possess multiple highly transferable replacements. Simultaneously, we compute the Gini inequality coefficient by tallying the number of word replacement rules in successful adversarial samples. Through statistical analysis of the replacement preferences\nfor each substituted word, we derived a Gini imbalance coefficient of 0.161. This suggests that the process of word replacement adheres to a discernible pattern, rather than manifesting as chaotic, and further demonstrates a certain level of diversity, which aligns with our observations from the heatmap. The prior application of greedy methods to select the most transferable replacement rules exhibits certain constraints, whereas the technique of employing a network for searching can effectively discern diverse patterns.\nThe previously mentioned transferable elements can all be represented in the statistical rules of the text. This offers the potential for employing neural network methodologies to learn transferable characteristics from adversarial samples."
        },
        {
            "heading": "5.3 Defense",
            "text": "Learning to map from adversarial samples back to the original sample distribution is relatively easier compared to the inverse process. To further validate the ability to learn adversarial sample distributions from text, we conduct defensive experiments using generative models. We use adversarial samples as input and the original samples as predictions. Generative models can serve as a data cleansing method by cleaning the input text before feeding it into the victim model. Appendix B presents the defensive experiments conducted on two victim models trained using CGFake and Hatebase datasets.\nOverall, using this method can effectively reduce the success rate of attackers and increase the cost of queries. CT-GAT demonstrates excellent defense against character-level attack methods like DeepWordBug, indicating the presence of strong patterns in character-level attacks. However, the performance against word-level attacks is not particularly outstanding. This mainly occurs because the attack method gradually intensifies its attack magnitude after a failed attack. This surpasses the defense model\u2019s capability, which is trained on imperceptible samples."
        },
        {
            "heading": "6 Conclusion",
            "text": "In this article, we propose a cross-task generative adversarial attack (CT-GAT), which is based on our insight that adversarial transferability exists across different tasks. Our method solves the problem of needing to train a substitute model for the victim model. Experiment demonstrates that our method\nnot only significantly increases the attack success rate, but also drastically reduces the number of required queries, thus underscoring its practical utility in real-world applications.\nLimitations\nOur method has the following limitations: (1) CTGAT was trained on a limited dataset. While it has demonstrated effectiveness within the scope of our experiments, the ability to generalize to broader, real-world scenarios may be constrained due to the restricted data diversity. To enhance the applicability of our method in a wider range of contexts, it would be necessary to incorporate more diverse datasets. This implies that for future enhancements of this work, gathering and integrating data from various sources or environments should be considered to improve the model\u2019s broad applicability. (2) Our method does not interact with the victim model. If we could adjust the generation strategy based on query results, it might enhance the success rate of attacks.\nEthics Statement\nThe datasets we use in our paper, Advbench(Chen et al., 2022), TCAB(Asthana et al., 2022) and Founta dataset(Founta et al., 2018), are both opensource. We do not disclose any non-open-source data, and we ensure that our actions comply with ethical standards. However, it is worth noting that our method is highly efficient in generating adversarial examples and does not require access to the victim model. This could potentially be misused by users for malicious activities such as attacking communities, spreading rumors, or disseminating spam emails. Therefore, it is important for the research community to pay more attention to securityrelated studies."
        },
        {
            "heading": "A Algorithm of CAWR",
            "text": "We define the rules for word substitutions as (w \u2192 w\u0302), where w is a word in the original sentence x, and w\u0302 represents the corresponding replacement word in the adversarial sentence x\u0302. We propose an algorithm to discover Cross-task Adversarial Word Replacement rules. We employ h(w \u2192 w\u0302) to denote the significance statistics of the substitution rule, which is measured by the change in model confidence. We employ a large-scale statistical approach to identify highly transferable rules (see Algorithmic 1). After obtaining the rules, we\ncan achieve text perturbation by sequentially replacing the most significant words until reaching the perturbation threshold.\nAlgorithm 1 Adversarial rules Extraction Require:\nD: A set of test datasets Di on different tasks i M: A set of modelsMi on different tasks i\nEnsure: a set of word replacement rules as well as their salience.\n1: for each datasets Di in D do 2: for each instance (x, y) in Di do 3: x\u0302\u2190 attack(x,Mi) 4: z \u2190 argmaxMi(x\u0302) 5: if z \u0338= y then 6: for each w\u0302 that replaces w do 7: if (synonym(w\u0302)\u222aw\u0302)\u2229synonym(w)\u0338= \u2205 then 8: c(wi \u2192 w\u0302i) = c(wi \u2192 w\u0302i) + 1 9: \u2206 = (Mi(x)[y]\u2212Mi(x\u0302)[y]) 10: h(wi \u2192 w\u0302i) = h(wi \u2192 w\u0302i) + \u2206 11: for each word replacement rule do 12: h(w \u2192 w\u0302) = h(w \u2192 w\u0302)/c(w \u2192 w\u0302)"
        },
        {
            "heading": "B Defense",
            "text": "The following are the defense effects on the CGFake and Hatebase datasets. Here, Query refers to the average number of attack queries for successful attack samples, not the average of the total number of queries."
        }
    ],
    "title": "CT-GAT: Cross-Task Generative Adversarial Attack based on Transferability",
    "year": 2023
}