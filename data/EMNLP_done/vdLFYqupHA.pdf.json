{
    "abstractText": "Large Language Models (LLMs) have gained significant popularity for their impressive performance across diverse fields. However, LLMs are prone to hallucinate untruthful or nonsensical outputs that fail to meet user expectations in many real-world applications. Existing works for detecting hallucinations in LLMs either rely on external knowledge for reference retrieval or require sampling multiple responses from the LLM for consistency verification, making these methods costly and inefficient. In this paper, we propose a novel referencefree, uncertainty-based method for detecting hallucinations in LLMs. Our approach imitates human focus in factuality checking from three aspects: 1) focus on the most informative and important keywords in the given text; 2) focus on the unreliable tokens in historical context which may lead to a cascade of hallucinations; and 3) focus on the token properties such as token type and token frequency. Experimental results on relevant datasets demonstrate the effectiveness of our proposed method, which achieves state-of-the-art performance across all the evaluation metrics and eliminates the need for additional information.1",
    "authors": [
        {
            "affiliations": [],
            "name": "Tianhang Zhang"
        },
        {
            "affiliations": [],
            "name": "Lin Qiu"
        },
        {
            "affiliations": [],
            "name": "Qipeng Guo"
        },
        {
            "affiliations": [],
            "name": "Cheng Deng"
        },
        {
            "affiliations": [],
            "name": "Yue Zhang"
        },
        {
            "affiliations": [],
            "name": "Zheng Zhang"
        },
        {
            "affiliations": [],
            "name": "Chenghu Zhou"
        },
        {
            "affiliations": [],
            "name": "Xinbing Wang"
        },
        {
            "affiliations": [],
            "name": "Luoyi Fu"
        }
    ],
    "id": "SP:0fb30f9e3cd8dccdd5c3a34f9536397c8f71356b",
    "references": [
        {
            "authors": [
                "Hussam Alkaissi",
                "Samy I McFarlane."
            ],
            "title": "Artificial hallucinations in chatgpt: implications in scientific writing",
            "venue": "Cureus, 15(2).",
            "year": 2023
        },
        {
            "authors": [
                "Noune",
                "Baptiste Pannier",
                "Guilherme Penedo"
            ],
            "title": "Falcon-40B: an open large language model with state-of-the-art performance",
            "year": 2023
        },
        {
            "authors": [
                "David Baidoo-Anu",
                "Leticia Owusu Ansah."
            ],
            "title": "Education in the era of generative artificial intelligence (ai): Understanding the potential benefits of chatgpt in promoting teaching and learning",
            "venue": "Available at SSRN 4337484.",
            "year": 2023
        },
        {
            "authors": [
                "Yejin Bang",
                "Samuel Cahyawijaya",
                "Nayeon Lee",
                "Wenliang Dai",
                "Dan Su",
                "Bryan Wilie",
                "Holy Lovenia",
                "Ziwei Ji",
                "Tiezheng Yu",
                "Willy Chung"
            ],
            "title": "A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity",
            "year": 2023
        },
        {
            "authors": [
                "Samy Bengio",
                "Oriol Vinyals",
                "Navdeep Jaitly",
                "Noam Shazeer."
            ],
            "title": "Scheduled sampling for sequence prediction with recurrent neural networks",
            "venue": "Advances in neural information processing systems, 28.",
            "year": 2015
        },
        {
            "authors": [
                "Sidney Black",
                "Stella Biderman",
                "Eric Hallahan",
                "Quentin Anthony",
                "Leo Gao",
                "Laurence Golding",
                "Horace He",
                "Connor Leahy",
                "Kyle McDonell",
                "Jason Phang"
            ],
            "title": "Gpt-neox-20b: An open-source autoregressive language model",
            "venue": "BigScience",
            "year": 2022
        },
        {
            "authors": [
                "Meng Cao",
                "Yue Dong",
                "Jackie Chi Kit Cheung."
            ],
            "title": "Hallucinated but factual! inspecting the factuality of hallucinations in abstractive summarization",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1:",
            "year": 2022
        },
        {
            "authors": [
                "Wei-Lin Chiang",
                "Zhuohan Li",
                "Zi Lin",
                "Ying Sheng",
                "Zhanghao Wu",
                "Hao Zhang",
                "Lianmin Zheng",
                "Siyuan Zhuang",
                "Yonghao Zhuang",
                "Joseph E. Gonzalez",
                "Ion Stoica",
                "Eric P. Xing"
            ],
            "title": "Vicuna: An opensource chatbot impressing gpt-4 with 90%* chatgpt",
            "year": 2023
        },
        {
            "authors": [
                "David Dale",
                "Elena Voita",
                "Lo\u00efc Barrault",
                "Marta R Costa-juss\u00e0."
            ],
            "title": "Detecting and mitigating hallucinations in machine translation: Model internal workings alone do well, sentence similarity even better",
            "venue": "arXiv preprint arXiv:2212.08597.",
            "year": 2022
        },
        {
            "authors": [
                "David Demeter",
                "Gregory Kimmel",
                "Doug Downey."
            ],
            "title": "Stolen probability: A structural weakness of neural language models",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2191\u20132197.",
            "year": 2020
        },
        {
            "authors": [
                "Yue Dong",
                "John Wieting",
                "Pat Verga."
            ],
            "title": "Faithful to the document or to the world? mitigating hallucinations via entity-linked knowledge in abstractive summarization",
            "venue": "arXiv preprint arXiv:2204.13761.",
            "year": 2022
        },
        {
            "authors": [
                "Nouha Dziri",
                "Sivan Milton",
                "Mo Yu",
                "Osmar R Zaiane",
                "Siva Reddy"
            ],
            "title": "On the origin of hallucinations in conversational models: Is it the datasets or the models",
            "venue": "In Proceedings of the 2022 Conference of the North American Chapter of the Association",
            "year": 2022
        },
        {
            "authors": [
                "Alexander R Fabbri",
                "Wojciech Kry\u015bci\u0144ski",
                "Bryan McCann",
                "Caiming Xiong",
                "Richard Socher",
                "Dragomir Radev."
            ],
            "title": "Summeval: Re-evaluating summarization evaluation",
            "venue": "Transactions of the Association for Computational Linguistics, 9:391\u2013409.",
            "year": 2021
        },
        {
            "authors": [
                "Tobias Falke",
                "Leonardo FR Ribeiro",
                "Prasetya Ajie Utama",
                "Ido Dagan",
                "Iryna Gurevych."
            ],
            "title": "Ranking generated summaries by correctness: An interesting but challenging application for natural language inference",
            "venue": "Proceedings of the 57th Annual Meet-",
            "year": 2019
        },
        {
            "authors": [
                "Zorik Gekhman",
                "Jonathan Herzig",
                "Roee Aharoni",
                "Chen Elkind",
                "Idan Szpektor."
            ],
            "title": "Trueteacher: Learning factual consistency evaluation with large language models",
            "venue": "arXiv preprint arXiv:2305.11171.",
            "year": 2023
        },
        {
            "authors": [
                "Nuno M Guerreiro",
                "Duarte Alves",
                "Jonas Waldendorf",
                "Barry Haddow",
                "Alexandra Birch",
                "Pierre Colombo",
                "Andr\u00e9 FT Martins."
            ],
            "title": "Hallucinations in large multilingual translation models",
            "venue": "arXiv preprint arXiv:2303.16104.",
            "year": 2023
        },
        {
            "authors": [
                "Nuno M Guerreiro",
                "Elena Voita",
                "Andr\u00e9 FT Martins."
            ],
            "title": "Looking for a needle in a haystack: A comprehensive study of hallucinations in neural machine translation",
            "venue": "arXiv preprint arXiv:2208.05309.",
            "year": 2022
        },
        {
            "authors": [
                "Matthew Honnibal",
                "Ines Montani."
            ],
            "title": "spacy 2: Natural language understanding with bloom embeddings, convolutional neural networks and incremental parsing",
            "venue": "To appear, 7(1):411\u2013420.",
            "year": 2017
        },
        {
            "authors": [
                "Dandan Huang",
                "Leyang Cui",
                "Sen Yang",
                "Guangsheng Bao",
                "Kun Wang",
                "Jun Xie",
                "Yue Zhang"
            ],
            "title": "What have we achieved on text summarization",
            "venue": "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
            "year": 2020
        },
        {
            "authors": [
                "Yichong Huang",
                "Xiachong Feng",
                "Xiaocheng Feng",
                "Bing Qin."
            ],
            "title": "The factual inconsistency problem in abstractive text summarization: A survey",
            "venue": "arXiv preprint arXiv:2104.14839.",
            "year": 2021
        },
        {
            "authors": [
                "Touseef Iqbal",
                "Shaima Qureshi."
            ],
            "title": "The survey: Text generation models in deep learning",
            "venue": "Journal of King Saud University-Computer and Information Sciences, 34(6):2515\u20132528.",
            "year": 2022
        },
        {
            "authors": [
                "Mohd Javaid",
                "Abid Haleem",
                "Ravi Pratap Singh."
            ],
            "title": "Chatgpt for healthcare services: An emerging stage for an innovative perspective",
            "venue": "BenchCouncil Transactions on Benchmarks, Standards and Evaluations, page 100105.",
            "year": 2023
        },
        {
            "authors": [
                "Ziwei Ji",
                "Nayeon Lee",
                "Rita Frieske",
                "Tiezheng Yu",
                "Dan Su",
                "Yan Xu",
                "Etsuko Ishii",
                "Ye Jin Bang",
                "Andrea Madotto",
                "Pascale Fung."
            ],
            "title": "Survey of hallucination in natural language generation",
            "venue": "ACM Computing Surveys, 55(12):1\u201338.",
            "year": 2023
        },
        {
            "authors": [
                "Zden\u011bk Kasner",
                "Simon Mille",
                "Ond\u0159ej Du\u0161ek."
            ],
            "title": "Text-in-context: Token-level error detection for tableto-text generation",
            "venue": "Proceedings of the 14th International Conference on Natural Language Generation, pages 259\u2013265.",
            "year": 2021
        },
        {
            "authors": [
                "Wojciech Kry\u015bci\u0144ski",
                "Bryan McCann",
                "Caiming Xiong",
                "Richard Socher."
            ],
            "title": "Evaluating the factual consistency of abstractive text summarization",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
            "year": 2020
        },
        {
            "authors": [
                "Philippe Laban",
                "Tobias Schnabel",
                "Paul N Bennett",
                "Marti A Hearst."
            ],
            "title": "Summac: Re-visiting nlibased models for inconsistency detection in summarization",
            "venue": "Transactions of the Association for Computational Linguistics, 10:163\u2013177.",
            "year": 2022
        },
        {
            "authors": [
                "Peter Lee",
                "Sebastien Bubeck",
                "Joseph Petro."
            ],
            "title": "Benefits, limits, and risks of gpt-4 as an ai chatbot for medicine",
            "venue": "New England Journal of Medicine, 388(13):1233\u20131239.",
            "year": 2023
        },
        {
            "authors": [
                "Mike Lewis",
                "Yinhan Liu",
                "Naman Goyal",
                "Marjan Ghazvininejad",
                "Abdelrahman Mohamed",
                "Omer Levy",
                "Veselin Stoyanov",
                "Luke Zettlemoyer"
            ],
            "title": "Bart: Denoising sequence-to-sequence pre-training for natural language",
            "year": 2020
        },
        {
            "authors": [
                "Jiongnan Liu",
                "Jiajie Jin",
                "Zihan Wang",
                "Jiehan Cheng",
                "Zhicheng Dou",
                "Ji-Rong Wen."
            ],
            "title": "Reta-llm: A retrieval-augmented large language model toolkit",
            "venue": "arXiv preprint arXiv:2306.05212.",
            "year": 2023
        },
        {
            "authors": [
                "Tianyu Liu",
                "Yizhe Zhang",
                "Chris Brockett",
                "Yi Mao",
                "Zhifang Sui",
                "Weizhu Chen",
                "William B Dolan."
            ],
            "title": "A token-level reference-free hallucination detection benchmark for free-form text generation",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for",
            "year": 2022
        },
        {
            "authors": [
                "Shayne Longpre",
                "Kartik Perisetla",
                "Anthony Chen",
                "Nikhil Ramesh",
                "Chris DuBois",
                "Sameer Singh."
            ],
            "title": "Entity-based knowledge conflicts in question answering",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Process-",
            "year": 2021
        },
        {
            "authors": [
                "Alejandro Lopez-Lira",
                "Yuehua Tang."
            ],
            "title": "Can chatgpt forecast stock price movements? return predictability and large language models",
            "venue": "Return Predictability and Large Language Models (April 6, 2023).",
            "year": 2023
        },
        {
            "authors": [
                "Potsawee Manakul",
                "Adian Liusie",
                "Mark JF Gales."
            ],
            "title": "Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models",
            "venue": "arXiv preprint arXiv:2303.08896.",
            "year": 2023
        },
        {
            "authors": [
                "Joshua Maynez",
                "Shashi Narayan",
                "Bernd Bohnet",
                "Ryan McDonald."
            ],
            "title": "On faithfulness and factuality in abstractive summarization",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1906\u20131919.",
            "year": 2020
        },
        {
            "authors": [
                "Sewon Min",
                "Kalpesh Krishna",
                "Xinxi Lyu",
                "Mike Lewis",
                "Wen-tau Yih",
                "Pang Wei Koh",
                "Mohit Iyyer",
                "Luke Zettlemoyer",
                "Hannaneh Hajishirzi."
            ],
            "title": "Factscore: Fine-grained atomic evaluation of factual precision in long form text generation",
            "venue": "arXiv preprint",
            "year": 2023
        },
        {
            "authors": [
                "Niels M\u00fcndler",
                "Jingxuan He",
                "Slobodan Jenko",
                "Martin Vechev."
            ],
            "title": "Self-contradictory hallucinations of large language models: Evaluation, detection and mitigation",
            "venue": "arXiv preprint arXiv:2305.15852.",
            "year": 2023
        },
        {
            "authors": [
                "Feng Nan",
                "Ramesh Nallapati",
                "Zhiguo Wang",
                "Cicero dos Santos",
                "Henghui Zhu",
                "Dejiao Zhang",
                "Kathleen Mckeown",
                "Bing Xiang."
            ],
            "title": "Entity-level factual consistency of abstractive text summarization",
            "venue": "Proceedings of the 16th Conference of the Euro-",
            "year": 2021
        },
        {
            "authors": [
                "Shashi Narayan",
                "Shay B Cohen",
                "Mirella Lapata."
            ],
            "title": "Don\u2019t give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Lan-",
            "year": 2018
        },
        {
            "authors": [
                "Long Ouyang",
                "Jeffrey Wu",
                "Xu Jiang",
                "Diogo Almeida",
                "Carroll Wainwright",
                "Pamela Mishkin",
                "Chong Zhang",
                "Sandhini Agarwal",
                "Katarina Slama",
                "Alex Ray"
            ],
            "title": "Training language models to follow instructions with human",
            "year": 2022
        },
        {
            "authors": [
                "Artidoro Pagnoni",
                "Vidhisha Balachandran",
                "Yulia Tsvetkov."
            ],
            "title": "Understanding factuality in abstractive summarization with frank: A benchmark for factuality metrics",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Asso-",
            "year": 2021
        },
        {
            "authors": [
                "Hannah Rashkin",
                "David Reitter",
                "Gaurav Singh Tomar",
                "Dipanjan Das."
            ],
            "title": "Increasing faithfulness in knowledge-grounded dialogue with controllable features",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics",
            "year": 2021
        },
        {
            "authors": [
                "Vikas Raunak",
                "Siddharth Dalmia",
                "Vivek Gupta",
                "Florian Metze."
            ],
            "title": "On long-tailed phenomena in neural machine translation",
            "venue": "Findings of the",
            "year": 2020
        },
        {
            "authors": [
                "Cl\u00e9ment Rebuffel",
                "Marco Roberti",
                "Laure Soulier",
                "Geoffrey Scoutheeten",
                "Rossella Cancelliere",
                "Patrick Gallinari."
            ],
            "title": "Controlling hallucinations at word level in data-to-text generation",
            "venue": "Data Mining and Knowledge Discovery, pages 1\u201337.",
            "year": 2022
        },
        {
            "authors": [
                "Malik Sallam."
            ],
            "title": "The utility of chatgpt as an example of large language models in healthcare education, research and practice: Systematic review on the future perspectives and potential limitations",
            "venue": "medRxiv, pages 2023\u201302.",
            "year": 2023
        },
        {
            "authors": [
                "Xinyue Shen",
                "Zeyuan Chen",
                "Michael Backes",
                "Yang Zhang."
            ],
            "title": "In chatgpt we trust? measuring and characterizing the reliability of chatgpt",
            "venue": "arXiv preprint arXiv:2304.08979.",
            "year": 2023
        },
        {
            "authors": [
                "Yiqiu Shen",
                "Laura Heacock",
                "Jonathan Elias",
                "Keith D Hentel",
                "Beatriu Reig",
                "George Shih",
                "Linda Moy"
            ],
            "title": "Chatgpt and other large language models are double-edged swords",
            "year": 2023
        },
        {
            "authors": [
                "Dan Su",
                "Xiaoguang Li",
                "Jindi Zhang",
                "Lifeng Shang",
                "Xin Jiang",
                "Qun Liu",
                "Pascale Fung."
            ],
            "title": "Read before generate! faithful long form question answering with machine reading",
            "venue": "Findings of the Association for Computational Linguistics: ACL 2022, pages 744\u2013",
            "year": 2022
        },
        {
            "authors": [
                "Rohan Taori",
                "Ishaan Gulrajani",
                "Tianyi Zhang",
                "Yann Dubois",
                "Xuechen Li",
                "Carlos Guestrin",
                "Percy Liang",
                "Tatsunori B. Hashimoto."
            ],
            "title": "Stanford alpaca: An instruction-following llama model",
            "venue": "https:// github.com/tatsu-lab/stanford_alpaca.",
            "year": 2023
        },
        {
            "authors": [
                "Ahmed Tlili",
                "Boulus Shehata",
                "Michael Agyemang Adarkwah",
                "Aras Bozkurt",
                "Daniel T Hickey",
                "Ronghuai Huang",
                "Brighter Agyemang."
            ],
            "title": "What if the devil is my guardian angel: Chatgpt as a case study of using chatbots in education",
            "venue": "Smart Learning Envi-",
            "year": 2023
        },
        {
            "authors": [
                "Hugo Touvron",
                "Thibaut Lavril",
                "Gautier Izacard",
                "Xavier Martinet",
                "Marie-Anne Lachaux",
                "Timoth\u00e9e Lacroix",
                "Baptiste Rozi\u00e8re",
                "Naman Goyal",
                "Eric Hambro",
                "Faisal Azhar"
            ],
            "title": "2023a. Llama: Open and efficient foundation language models",
            "year": 2023
        },
        {
            "authors": [
                "Hugo Touvron",
                "Louis Martin",
                "Kevin Stone",
                "Peter Albert",
                "Amjad Almahairi",
                "Yasmine Babaei",
                "Nikolay Bashlykov",
                "Soumya Batra",
                "Prajjwal Bhargava",
                "Shruti Bhosale"
            ],
            "title": "2023b. Llama 2: Open foundation and fine-tuned chat models",
            "year": 2023
        },
        {
            "authors": [
                "Liam van der Poel",
                "Ryan Cotterell",
                "Clara Meister."
            ],
            "title": "Mutual information alleviates hallucinations in abstractive summarization",
            "venue": "EMNLP 2022. arXiv.",
            "year": 2022
        },
        {
            "authors": [
                "Ben Wang",
                "Aran Komatsuzaki."
            ],
            "title": "GPT-J6B: A 6 Billion Parameter Autoregressive Language Model",
            "venue": "https://github.com/kingoflolz/ mesh-transformer-jax.",
            "year": 2021
        },
        {
            "authors": [
                "Shijie Wu",
                "Ozan Irsoy",
                "Steven Lu",
                "Vadim Dabravolski",
                "Mark Dredze",
                "Sebastian Gehrmann",
                "Prabhanjan Kambadur",
                "David Rosenberg",
                "Gideon Mann."
            ],
            "title": "Bloomberggpt: A large language model for finance",
            "venue": "arXiv preprint arXiv:2303.17564.",
            "year": 2023
        },
        {
            "authors": [
                "Yijun Xiao",
                "William Yang Wang."
            ],
            "title": "On hallucination and predictive uncertainty in conditional language generation",
            "venue": "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages",
            "year": 2021
        },
        {
            "authors": [
                "Weijia Xu",
                "Sweta Agrawal",
                "Eleftheria Briakou",
                "Marianna J Martindale",
                "Marine Carpuat."
            ],
            "title": "Understanding and detecting hallucinations in neural machine translation via model introspection",
            "venue": "arXiv preprint arXiv:2301.07779.",
            "year": 2023
        },
        {
            "authors": [
                "Susan Zhang",
                "Stephen Roller",
                "Naman Goyal",
                "Mikel Artetxe",
                "Moya Chen",
                "Shuohui Chen",
                "Christopher Dewan",
                "Mona Diab",
                "Xian Li",
                "Xi Victoria Lin"
            ],
            "title": "Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068",
            "year": 2022
        },
        {
            "authors": [],
            "title": "1942), better known by his professional name <PERSON> Michael Savage, is an <NORP> American radio host, author, activist, nutritionist, and conservative political commentator. He is the host of <ORG> The Savage Nation, a nationally syndicated talk show that aired on <ORG> Talk Radio Network across <GPE> the United States until <DATE> 2012, and in <DATE> 2009 was the <ORDINAL> second most listened-to radio talk",
            "year": 2009
        },
        {
            "authors": [
                "PERSON> Tommy Nutter"
            ],
            "title": "1943\u20131992) was a <NORP> British tailor who was a major figure in the fashion world of <DATE> the late 1960s and <DATE> early 1970s. He was known for his flamboyant style and his work with <ORG> the Rolling Stones, <PERSON> Elton John, and other celebrities. He was born in <GPE> London and began his career as an apprentice tailor at <DATE> the age",
            "venue": "He opened his own shop, Nutters of Savile Row,",
            "year": 1969
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Large Language Models (LLMs) have garnered substantial attention for their remarkable performance across various domains, such as finance (Wu et al., 2023; Lopez-Lira and Tang, 2023), medicine (Javaid et al., 2023; Lee et al., 2023), and education (Tlili et al., 2023; BaidooAnu and Owusu Ansah, 2023). These models exhibit an extraordinary capacity to generate natural language texts with high levels of coherence, fluency, and informativeness. Nevertheless, a significant obstacle confronting LLMs is the risk of producing hallucinations (Shen et al., 2023b; Sallam,\n1Code can be found at https://github.com/zthang/ focus.\n2023), which refers to the generated text that is untruthful or nonsensical (Ji et al., 2023; Bang et al., 2023). Hallucinations are a common occurrence in almost all applications (Xu et al., 2023), thus undermining the reliability and trustworthiness of LLMs, especially in scenarios where accuracy and veracity are essential.\nExisting studies on hallucination detection for LLMs can be broadly divided into two categories: (i) retrieval-based methods (Min et al., 2023; Liu et al., 2023), which evaluate the veracity of the given text against knowledge bases, and (ii) sampling-based methods (M\u00fcndler et al., 2023; Manakul et al., 2023), which assess information consistency between the evaluated text and additional sampled responses from the same LLM. However, retrieval-based methods depend heavily on the external knowledge that may not always be accessible. And sampling-based methods require multiple responses from the LLM for the information consistency verification or model training, making these methods costly and inefficient.\nTo address the above issues, we propose a novel reference-free, uncertainty-based method to detect hallucinations in LLMs that are factually incorrect according to the world knowledge. The proposed method relies exclusively on the LLM output text, thereby eliminating the need for additional resources like sampled responses from LLM or external knowledge, as well as further training based on such data. Our basic idea is to use a proxy language model for calculating the probability of each token in a given text. Based on the calculated probability, we can compute the hallucination score at both token and sentence level using uncertainty-based metrics (Guerreiro et al., 2022; Xiao and Wang, 2021), where tokens and sentences with high hallucination scores are identified as candidate hallucinated contents. Our assumption is that a powerful enough LLM should assign a low probability to tokens that make up hallucinated information, since\nthey deviate from the world knowledge the model has learned during its training stage.\nThe above method serves as a base framework, which can be limited by the inherent characteristics of the prediction probability from a naive proxy model. Such a model functions as a general probability estimator, its predictions reflect syntactic, semantic and other sources of information, which can hinder the focus on hallucination itself as illustrated in Figure 1a.\nFirstly, the proxy model ignores varying degrees of informativeness, which may introduce noise. Secondly, the probabilities assigned by LMs are general and can deviate from factuality confidence in different contexts. For instance, the proxy model can be overconfident if the historical context contains surface tokens that are correlated with a hallucinated token, or the historical context features exposure bias (Bengio et al., 2015; Iqbal and Qureshi, 2022) due to the auto-regressive nature of generative process. One example is shown in Figure 1a, where hallucinated tokens \u201c2012 Summer Olympics\u201d are assigned high probabilities. In addition, a proxy model can be underconfident if there are many plausible choices of topic directions to continue a context, despite that the hallucination involves different tokens within only one topic direction. One example is shown in Figure 1a, where the factual token \u201c1992\u201d received a low probability due to the competitors like \u201cWest\u201d and \u201cCoral\u201d.\nTo strengthen the focus on hallucination, we take inspiration from human factuality checking, which can include at least three specific considerations as depicted in Figure 1b:\n\u2022 Focus on the informative keywords: the keywords that express salient information will be extracted for the calculation of hallucination scores\nat both sentence-level and passage-level.\n\u2022 Focus on the preceding words: we propagate the uncertainties of previous tokens to the subsequent ones according to their attention weights to alleviate the overconfidence problem. This approach is based on the hypothesis that words that are strongly connected to unreliable tokens may also be influenced by these inaccuracies, which can trigger a chain reaction of hallucinations.\n\u2022 Focus on the token properties: the predicted token probability is conditioned on its entity type (if any) and adjusted by its inverse document frequency (IDF). This results in a probability distribution that aligns more closely with human evaluation in a posterior manner, thus mitigating the underconfidence problem.\nIn summary, our primary contribution is that we introduce a novel reference-free, uncertainty-based approach for detecting hallucinations in LLMs. Our approach does not require additional sampled responses or external knowledge bases, making it simple and cost-effective. Experimental results demonstrate that our proposed method achieves state-of-the-art performance on the WikiBio GPT-3 dataset across various models with different scales, and shows effectiveness in detecting hallucinations within summaries generated by small models."
        },
        {
            "heading": "2 Related Work",
            "text": ""
        },
        {
            "heading": "2.1 Hallucinations in Text Generation",
            "text": "Hallucinations are prevalent phenomenon in deep learning-based models employed for various text generation tasks (Xu et al., 2023), such as abstractive summarization (Huang et al., 2021; Nan et al., 2021), dialogue generation (Dziri et al.,\n2022; Rashkin et al., 2021) and question answering (Longpre et al., 2021; Su et al., 2022). Hallucinations present significant challenges in text generation tasks, as they can lead to inaccurate or misleading results, which is unacceptable in most user-oriented applications (Liu et al., 2022; Xu et al., 2023; Rebuffel et al., 2022)."
        },
        {
            "heading": "2.2 Hallucination Detection",
            "text": "Previous studies on hallucination detection have primarily concentrated on identifying hallucinations produced by small models (fewer than 1b parameters) that are tailored for specific tasks. For instance, Kasner et al. (2021) combined a rulebased system and a pretrained language model to identify hallucinations in table-to-text generation. Guerreiro et al. (2022) adopted the average log-probability across all the tokens in the output sequence as the model uncertainty metric for detecting hallucinations in machine translation. Dale et al. (2022) attempted to detect hallucinations by evaluating the percentage of the source contribution to the generated text. However, the hallucination patterns exhibited by LLMs tend to be divergent from those in small models (Guerreiro et al., 2023), posing challenges for the generalization of these methods on detecting hallucinations in LLMs. Accordingly, hallucination detection in small models is not within the primary scope of this paper.\nThe widespread incorporation of LLMs across a diverse range of applications has drawn substantial attention from researchers towards the issue of hallucinations within LLMs (Bang et al., 2023; Shen et al., 2023a; Alkaissi and McFarlane, 2023). For instance, Min et al. (2023) introduced FACTSCORE to evaluate the correctness of each atomic fact in the generated text by referencing a knowledge source. M\u00fcndler et al. (2023) aimed to detect hallucinations by examining whether two sampled sentences generated at the same position within a text contradict each other. A recent work by Manakul et al. (2023) proposed SelfCheckGPT, a black-box approach for detecting hallucinations in LLM-generated responses. The primary premise of SelfCheckGPT is that when the LLM is uncertain about a given concept, the sampled responses may contain inconsistent facts. Nonetheless, these methods either rely on external knowledge bases or multiple responses sampled from LLM, which are resource-intensive and inefficient."
        },
        {
            "heading": "3 Methods",
            "text": "A proxy model is utilized in our method for uncertainty assessment in cases where token-level probabilities are inaccessible, such as GPT-3 (Ouyang et al., 2022). Although previous work by Manakul et al. (2023) has demonstrated the ineffective performance of using a proxy model, we attribute it to the uncertainty metrics employed. These metrics, such as the average entropy and average loss for all tokens in the sentence, are insufficiently aligned with human evaluation. We believe this issue stems from the inherent disparities in how models and humans perceive and assess information, thus limiting the capability of the uncertainty-based approach for hallucination detection.\nTo mitigate this problem, we imitate how humans perform factuality checking from three aspects, which will be discussed in following sections."
        },
        {
            "heading": "3.1 Keywords selection",
            "text": "Prior works (Pagnoni et al., 2021; Krys\u0301cin\u0301ski et al., 2020) suggest that entities are the most frequently hallucinated words in text generation. This aligns with the intuition that, when evaluating the veracity of generated results, our primary focus lies on keywords that convey the most crucial information. In this regard, we only focus on keywords identified by Spacy (Honnibal and Montani, 2017) when calculating the hallucination score at both sentencelevel and passage-level. The keywords identified by Spacy can be classified into two groups. The first group comprises 18 distinct types of named entities, including person, location, date, event, organization, and others. The second group encompasses nouns that do not belong to the first group.\nSpecifically, for a given text r, we will compute a hallucination score hi for the i-th token ti in r. To fully utilize both local and global uncertainty information, hi is the sum of the negative log probability and entropy when generating ti:\nhi = \u2212log(pi(ti)) +Hi (1)\nHi = 2\u2212 \u2211 v\u2208V pi(v)\u2217log2(pi(v)) (2)\nwhere pi(v) denotes the probability of generating the token v over all tokens in the vocabulary V at position i. The hallucination score hs for the sentence s is calculated by a weighted sum, where the\nweight is determined by whether ti is a keyword:\nhs = 1\u2211|s|\u22121\ni=0 I(ti \u2208 K) |s|\u22121\u2211 i=0 I(ti \u2208 K) \u2217 hi (3)\nwhere |s| is the number of tokens in s, K denotes the set of keywords, I(\u00b7) is an indicator function. Moreover, this formulation can be extended to compute the passage-level hallucination score by averaging hallucination scores of keywords in the given passage."
        },
        {
            "heading": "3.2 Hallucination propagation",
            "text": "Several studies (Guerreiro et al., 2022; Xiao and Wang, 2021) have utilized token probability as a measure for hallucination detection. However, probabilities derived from a language model may not accurately reflect the factuality confidence in the generated content. Some hallucinated tokens can be assigned high probabilities when the history context contains hallucinated information, which we term as the overconfidence issue. This issue is exacerbated by the self-attention mechanism that is commonly used in transformer-based LLMs, since it introduces exposure bias (Bengio et al., 2015; Iqbal and Qureshi, 2022), which refers to the discrepancy between training and inference caused by the use of teacher forcing during the training stage. Consequently, the generated text is accepted as factual claims, even though it may be non-factual.\nFigure 2 provides an example that illustrates the overconfidence issue. Considering the following text: \u201cMackenzie Caquatto is an American former artistic gymnast, who competed at the 2012 Summer Olympics in London. Caquatto was born in 1992, and began gymnastics at the age of three. She competed on the uneven bars and balance beam at the 2012 Summer Olympics.\u201d Notably, the term\n\u201c2012\u201d makes two appearances, with the probability of its initial occurrence being significantly lower than the probability of its subsequent appearance. The visualized self-attention matrix reveals that considerable attention is given to the same phrase in the first sentence (circled with a blue box) when generating \u201c2012 Summer Olympics\u201d in the last sentence. However, the claim \u201cMackenzie Caquatto competed at the 2012 Summer Olympics in London\u201d is untruthful.\nThis observation inspired us to introduce a \u201cpenalty\u201d for tokens generated with attentions paid to unreliable tokens. In other words, we consider the hallucination scores of preceding tokens and apply them as a penalty to the current token based on their respective attention weights. Here, we only consider propagation between keywords. Specifically, we first check if the current token is a keyword as described in Section 3.1. If not, the penalty is set to zero. If it is a keyword, we normalize the attention weights between the current token and all previous keywords to obtain a penalty weight. The penalty for the current token is computed as a weighted sum of the hallucination scores associated with the previous tokens. Since the penalty can be transmitted to all the subsequent tokens via multi-hop, a coefficient \u03b3 \u2208 [0, 1] is introduced to ensure that the penalty diminishes geometrically with the increase in the number of hops.\nLet h\u0302i represent the hallucination score of the i-th token ti with an accumulated penalty, the calculation of h\u0302i can be expressed as follows:\nh\u0302i = hi + I(ti \u2208 K) \u2217 \u03b3 \u2217 pi (4)\npi = i\u22121\u2211 j=0 wi,j \u2217 h\u0302j (5)\nwi,j = I(ti \u2208 K) \u2217 atti,j\u2211i\u22121 k=0 I(ti \u2208 K) \u2217 atti,k\n(6)\nwhere pi represents the penalty of the i-th token, atti,j denotes the attention weight between ti and tj after max-pooling for all the layers and attention heads."
        },
        {
            "heading": "3.3 Probability correction",
            "text": "Apart from the overconfidence issue, there are also instances where the model exhibits underconfidence, which can also lead to deviations in token probability from factuality confidence. We believe such underconfidence is related to the token properties, including the entity type and token frequency. As shown in Figure 1a, when generating the subsequent words following \u201cCaquatto was born in\u201d. The model may have multiple possible choices of topic directions such as \u201cWest chester\u201d, \u201cCoral Springs\u201d, \u201c1992\u201d et al, despite that the hallucination involves different tokens within a specific topic direction. Consequently, the probability of generating the date \u201c1992\u201d would be relatively low, given the existence of several other viable options.\nThis highlights the stark disparities in how models and humans assess information: when evaluating the plausibility of \u201c1992\u201d, the model focuses meticulously on all the feasible choices with different entity types. In contrast, humans tend to intuitively include it within a tailored set of candidate words that predominantly consists of terms related to dates. Suppose there are n tokens t0:n\u22121 = t0, t1, ..., tn\u22121 in a model response r. Let c(t0:i) denote the set of ideal candidate words for ti given the first i + 1 tokens. According to the Bayes rule, the probability of generating ti given t0:i\u22121 and the candidate set can be expressed as:\np(ti|t0:i\u22121, c(t0:i))\n= p(c(t0:i)|t0:i\u22121, ti) \u2217 p(ti|t0:i\u22121)\np(c(t0:i)|t0:i\u22121)\n= p(ti|t0:i\u22121)\np(c(t0:i)|t0:i\u22121)\n= p(ti|t0:i\u22121)\u2211\nv\u2208c(t0:i) p(v|t0:i\u22121)\n(7)\nIt suggests that when assessing the rationality of a given word, the focus should be directed towards similar words rather than encompassing all possible choices. However, constructing such a candidate set poses a challenge during the model generation\nstage, given all words are tokenized into sentence pieces. To tackle this problem, we leverage the incontext learning capability of the proxy model by inserting the entity type2 preceding every named entity identified by Spacy as shown in Figure 3. The entity type serves as a constraint in generation, thereby enabling us to approximate the ideal candidate set c(t0:i) in Equation 7 using tokens with a generation probability greater than a threshold \u03c1. Accordingly, the token probability distribution is corrected to assign higher probability to tokens adhering to the given entity type.\nAdditionally, as outlined in previous studies (Raunak et al., 2020; van der Poel et al., 2022; Demeter et al., 2020), tokens with low frequency are likely to receive lower prediction probabilities, potentially leading to the underconfidence in the model. To mitigate this issue, the probability of token t is further corrected by its token IDF:\np\u0302(t) = p\u0303(t) \u2217 idf(t)\u2211\nv\u2208V p\u0303(v) \u2217 idf(v) (8)\nwhere p\u0303(t) denotes the probability of token t across all tokens in the vocabulary V with entity type provided. The token IDF is calculated based on 1M documents sampled from RedPajama dataset3."
        },
        {
            "heading": "3.4 Putting things together",
            "text": "To combine all the methods proposed above, we replace the token probability in Equation 1 and Equation 2 with p\u0302(t). Subsequently, we apply hallucination propagation to obtain the token-level hallucination score with penalty accumulated. The sentence-\n2The entity type inserted in the text do not participate in hallucination propagation or hallucination score calculation.\n3https://huggingface.co/datasets/togethercomputer/ RedPajama-Data-1T-Sample\nlevel and passage-level hallucination scores are calculated based on Equation 3."
        },
        {
            "heading": "4 Experiments and Results",
            "text": ""
        },
        {
            "heading": "4.1 Experiment setting",
            "text": "Dataset. We evaluated our proposed method on WikiBio GPT-3 dataset (Manakul et al., 2023), which, to the best of our knowledge, is the only publicly accessible dataset for LLM hallucination detection at present. Additionally, to assess the extent to which our proposed method can be applied for detecting hallucinations produced by different models, and in particular small models, we conducted supplementary experiments on the XSumFaith (Maynez et al., 2020) and FRANK (Pagnoni et al., 2021) datasets. Given the primary focus of this paper is hallucination detection in LLM as discussed in Section 2.2, the details and results of the two datasets are provided in Appendix A.\nThe WikiBio GPT-3 dataset comprises 1908 annotated sentences from 238 Wikipedia passages generated by text-davinci-003. Each sentence is assigned one of the following labels: 1) major inaccurate, if the sentence is irrelevant to the given topic; 2) minor inaccurate, if the sentence includes non-factual information verifiable via web search; 3) accurate, if the sentence does not contain any hallucination. We provided some examples from the dataset in Appendix D. The dataset also included 20 stochastically-sampled responses from text-davinci-003 for each passage, but these were not utilized in our experiments as our method does not necessitate additional sampled responses.\nIn accordance with the setting in Manakul et al. (2023), sentences labeled as major inaccurate and minor inaccurate are grouped into the non-factual class, while remaining sentences are grouped into the factual class. For the non-factual* class, we first remove passages where all sentences are labeled as major inaccurate. Then, we classify remaining major inaccurate sentences as non-factual*. Baselines. (i) GPT-3 (Ouyang et al., 2022) Uncertainties: GPT-3 (text-davinci-003) API returns top5 probabilities for each generated token, which can be used to quantify its uncertainty using negative log probability and entropy. (ii) SelfCheckGPT: SelfCheckGPT (Manakul et al., 2023) is a blackbox method for detecting hallucinations in LLMs, which demands additional responses sampled from the same LLM for the consistency verification. Metrics. To ensure a fair comparison, we adopt\nsame metrics employed by SelfCheckGPT. The Area Under the Precision-Recall Curve (AUC-PR) is used to measure the performance of sentencelevel hallucination detection, while the Pearson and Spearman\u2019s correlation coefficient are applied to evaluate the agreement between the passagelevel hallucination score and human judgement. For space saving, AUC-PR of non-factual class is abbreviated as NonFact or NoFac in the following sections, and similarly for the other classes. Proxy model. To demonstrate the generalizability of our proposed method across different scales of LLMs, we conduct experiments on 22 diverse proxy models. The specific details of these proxy models can be found in Appendix E. Prompts. In experiments where entity types are not provided, we use the prompt \u201cThis is a passage from Wikipedia about {concept}:\u201d. Conversely, when entity types are inserted before named entities, the prompt is \u201cPlease complete the passage below using appropriate words that follow to the given type with < > wrapped. This is a passage from Wikipedia about {concept}:\u201d."
        },
        {
            "heading": "4.2 Main results",
            "text": "The performance comparison between our proposed method and the baseline approaches is presented in Table 1. Due to space limitations, we only display the results of LLaMA family. Comprehensive comparison results for all proxy models can be found in the Appendix H. The hyperparameters \u03b3 and \u03c1 are set to 0.9 and 0.01, respectively. The baseline results are referenced from Manakul et al. (2023). Other implementation details can be found in Appendix B. Our key findings are as follows: Proxy model surpasses all the baselines. Leveraging three proposed focus mechanisms, LLaMA30b consistently outperforms SelfCheckGPTCombination4 and other baselines across all five metrics. Significantly, this is achieved without resorting to sampled responses or further training, exhibiting superior efficiency compared to SelfCheckGPT. As presented in Table 1, the performance of LLaMA family improves as the model size increases. However, this improvement is not\n4We noticed that on June 11th, the authors of SelfCheckGPT updated their results on GitHub (but not for their arXiv paper). The new approach entails a large number of ChatGPT queries for text inconsistency assessment. We do not include the results in this paper since they are contemporaneous with our work, as well as the comparison is not fair.\nlinearly correlated to the model size as shown in Figure 9 of Appendix F. LLaMA-65b even exhibits slightly inferior performance compared to LLaMA30b in four of the five metrics.\nMoreover, the comprehensive results across 22 proxy models as demonstrated in Table 8 affirm that within the same model family, models with more parameters tend to perform better. This can be attributed to their broader and more accurate understanding of world knowledge. In addition, when comparing different model families, models that exhibit superior performance on general NLP tasks often perform well on the WikiBio GPT-3 dataset. These observations provide valuable insights for future exploration and enhancement of our hallucination detection method. Focus allows small-scale models to achieve comparable performance to GPT-3. As shown in Table 1, LLaMA-7b achieves comparable or even superior performance when compared with GPT-3 uncertainties. This observation suggests that despite being a powerful LLM with 175b parameters, GPT-3 may be similarly plagued by issues of overconfidence and underconfidence. However, neither the attention weights nor the full probability distribution of GPT-3 are accessible, otherwise, the incorporation of focus would enable uncertainties of GPT-3 to yield considerably enhanced results."
        },
        {
            "heading": "4.3 Analysis",
            "text": "Table 2 presents the results of our ablation study conducted on LLaMA-30b. The average hallucination score in Equation 1 without any proposed\ntricks serves as the baseline in the first row, with each trick incorporated incrementally in the succeeding rows. The ablation studies on the remaining 21 proxy models are detailed in Appendix H. Focus on the informative keywords. By focusing on the keywords, improvements are observed across nearly all metrics. Notably, the Pearson and Spearman correlation coefficients are improved by 5.04% and 7.48%, respectively. These results suggest a stronger correlation between the keyword uncertainties and passage-level human judgments. Focus on the preceding words. When hallucination propagation is incorporated on the basis of keyword selection, remarkably, substantial improvements can be observed across all metrics. Particularly, the AUC-PR of the non-factual class exhibited a significant increase of 3.67% on LLaMA30b. This enhancement can be attributed to the successful remediation of the overconfidence issue as discussed in Section 3.2.\nThe overall performance of LLaMA-30b with \u03b3 ranging from 0 to 1 (no hallucination propagation when \u03b3 is set to zero) is illustrated in Figure 10 of\nAppendix G. It is evident that most metrics improve as \u03b3 increases. However, a performance decline is noticeable when \u03b3 exceeds 0.8, indicating that an excessive focus on the preceding words could also lead to a deterioration in performance. Focus on the token properties. Further enhancements in model performance can be achieved by incorporating entity type information and token IDF, leading to drastic improvements as evidenced in the last two rows. Specifically, the AUC-PR of the factual class increases by 10.76%, and both correlation coefficients improve by approximately 18%. This demonstrates the effectiveness of probability correction in mitigating the underconfidence problem as discussed in Section 3.3. Nevertheless, we observe little improvement for the non-factual* class when considering only the entity type property on multiple proxy models. The reason behind this observation will be explained in Section 4.4.2.\nThe performance impact of varying \u03c1 values is depicted in Figure 11 of Appendix G. Generally, \u03c1 = 0.01 delivers optimal results. A large \u03c1 could lead to the omission of crucial information due to a restricted candidate set, while a small \u03c1 might introduce noise by including irrelevant tokens."
        },
        {
            "heading": "4.4 Case study",
            "text": ""
        },
        {
            "heading": "4.4.1 Non-factual cases detected by hallucination propagation",
            "text": "Table 3 showcases two examples of hallucinated content accurately identified by hallucination propagation. In the first case, the pink sentence erroneously assigns the role of singer and songwriter to Paul Taylor, who was actually a keyboardist/guitarist of the band Winger. This error originates from the model\u2019s preceding hallucination (purple text)\n\u201cPaul Taylor is an American singer-songwriter\u201d. In the second case, the pink sentence duplicates existing text, consequently producing a significantly low value of h owing to the overconfidence problem. With the introduction of the penalty, the hallucination score increases by approximately fifty-fold, demonstrating the effectiveness of focusing on the hallucination scores of the preceding words.\nThe attention heat maps corresponding to the two cases can be found in Appendix C."
        },
        {
            "heading": "4.4.2 Failure cases after entity type provision",
            "text": "To explain the decrease in AUC-PR of the nonfactual* class when entity types are specified for each named entity, we computed the sentence-level average hallucination score h for each category in Table 4.\nWe notice that the average hallucination score h for all classes decreases when entity type information is provided, since the probability is corrected to be more confident for the keywords. However, this decrease is especially noticeable in the major inaccurate category due to the fact that sentences labeled as major inaccurate contain more hallucinated keywords. As a result, distinguishing between major inaccurate and minor inaccurate becomes more challenging. Given that the nonfactual* class only includes sentences classified as major inaccurate, this increased difficulty in differentiation contributes to the observed decrease in AUC-PR for the non-factual* class."
        },
        {
            "heading": "5 Conclusion",
            "text": "In this paper, we propose a reference-free, uncertainty-based method for detecting hallucinations in LLMs. The proposed method aims to imitate human factuality checking by considering three aspects: focus on informative keywords, focus on preceding words and focus on token properties. Our experimental results empirically demonstrate the effectiveness of the proposed method for hallucination detection at both sentence and passage level, without requiring any external knowledge or training data. We have also analyzed how each of\nthe three focus mechanisms impacts the overall performance when using different proxy models as the backbone. The results on XSumFaith and FRANK datasets further showcase the potential capability of the proposed method for detecting hallucinations produced by small models. We hope our work can contribute to the field of LLM research and help improve the reliability and factuality of LLMs.\nLimitations\nThe keyword identification and named entity recognition in our approach is based on Spacy, which may introduce some errors as observed in our practice. For instance, the television drama \u201cThe Great Ambition\u201d could erroneously be classified as an organization. Such failures can result in the calculated probability becoming unreliable, leading to a decrease in performance. Additionally, the categories of named entities in real-world scenarios are considerably more diverse than those identifiable by Spacy, such as food, vehicles, and other specialized domains.\nA further limitation arises from our assumption that LLM proxies are consistently current with factual knowledge. However, LLMs are not continuously updated post-training, hence they may lack recently emerged factual information. This could influence the assigned probabilities and in turn affect our hallucination detection\u2019s effectiveness.\nEthics Statement\nWe maintained privacy in our approach, as our method does not require user data and we conducted experiments on publicly available datasets, upholding privacy and anonymity standards. Despite the intention of this work is to improve LLMs\u2019 reliability, potential misuse such as using it to enhance AI-generated misinformation or deepfake content, is condemned. We are dedicated to ethical AI research, addressing potential concerns, and maintaining a balance between technological progress and ethical responsibility."
        },
        {
            "heading": "A Results for detecting hallucinations generated by small models",
            "text": "To assess the effectiveness of our proposed method in detecting hallucinations produced by small models, we conducted experiments using two hallucination detection datasets extracted from the test split of the SummaC benchmark (Laban et al., 2022): XSumFaith and FRANK. These datasets consist of summaries generated by small models such as TransS2S (Vaswani et al., 2017), TCONVS2S (Narayan et al., 2018), and BART (Lewis et al., 2020).\nAlthough the benchmark includes a total of six datasets, it should be noted that some of them contain summarizations not produced by generative models, such as Polytope (Huang et al., 2020) and SummEval (Fabbri et al., 2021). Additionally, certain datasets (Krys\u0301cin\u0301ski et al., 2020; Falke et al., 2019) label any content not present in the input as extrinsic hallucination5. However, such extrinsic hallucination might actually be factual (Dong et al., 2022; Cao et al., 2022).\n5Only about 5% of such cases in XSumFaith as reported in the original paper, hence, we have included this dataset in our analysis.\nSpecifically, for the FRANK dataset, which provides the error type of each sample, we removed the instances that were labeled as OutE (statement contains information not present in the source article) for the reason discussed above. For XSumFaith, we excluded the human-written summaries since they may differ in style from model-generated summaries (Gekhman et al., 2023). The statistics of the two datasets are shown in Table 5.\nWe report the AUC-PR for the non-factual class and factual class and balanced-accuracy in Table 6 and Table 7. Our method performs well across all three metrics when applied to the XSumFaith dataset. However, we observed that, for the FRANK dataset, using only the negative log probability yields better results compared to using the sum of negative log probability and entropy. Furthermore, focus on the keywords proves less effective than considering all tokens in the passage. We attribute this discrepancy to the unique characteristics of the FRANK dataset, which contains hallucinations such as predicate errors, pronoun errors, and preposition errors. Therefore, we only use the negative log probability of token t as its hallucination score and disregard keyword selection for FRANK dataset. These results highlight the effectiveness of focusing on token property information, but little enhancement is observed when solely relying on hallucination propagation. Further investigation is left for the future work.\nB Implementation Details\nOur experiments were conducted on an AWS p3dn.24xlarge instance, each of which is equipped with 8 NVIDIA V100 32GiB GPUs, 96 CPU cores, and 768 GiB RAM. In order to prevent the influence of type tags when calculating the token probability, we set the probability of token \u201c<\" to zero. When using the SFT version of LLaMA, the prompt as described in Section 4.1 is formatted to follow the Alpaca (Taori et al., 2023) pattern: \u201c### Instruction: {instruction} ### Response: {response}\u201d.\nFor the experiments on the two summarization datasets, we excluded instances where the token count exceeded LLaMA\u2019s maximum context length of 2048, resulting in the elimination of 16 cases from the XSumFaith dataset. The prompts employed are \u201c{document} TL;DR\u201d and \u201cSummarize the following text using appropriate words that follow to the given type: {document} TL;DR\u201d, without and with the provision of entity types, respectively.\nEntity types are also provided in the prompts as few-shot examples. For instance, the prompt for the concept \u201cmichael savage\u201d is \u201cThis a passage from <ORG> Wikipedia about <PERSON> michael savage:\u201d."
        },
        {
            "heading": "C More attention heat map cases",
            "text": "Figure 4 and Figure 5 provide visualizations of the attention heat maps of the two cases mentioned in Section 4.4.1. The attentions that are erroneously directed towards preceding unreliable tokens are marked within a blue box."
        },
        {
            "heading": "D Examples of passages with entity types provided",
            "text": "Figure 6 to Figure 8 illustrate three examples of Wikipedia passages generated by text-davinci-003, along with their corresponding prompts. Before\ninputting each passage into the proxy model for hallucination detection, the entity types are provided before each named entity recognized by Spacy."
        },
        {
            "heading": "E Details of the proxy models",
            "text": "The 22 proxy models used in our experiments include LLaMA-{7b, 13b, 30b, 65b} (Touvron et al., 2023a), LLaMA-2{7b, 13b, 70b} (Touvron et al., 2023b), OPT-{125m, 1.3b, 13b, 30b} (Zhang et al., 2022), GPT-J-6b (Wang and Komatsuzaki, 2021) GPT-NeoX-20b (Black et al., 2022), Falcon{7b, 40b} (Almazrouei et al., 2023), Vicuna-{7b, 13b, 33b} (Chiang et al., 2023), RedPajama-{3b, 7b} (Computer, 2023) and instruction tuning versions of LLaMA-{13b, 30b}-SFT6."
        },
        {
            "heading": "F Performance comparison of LLaMA family",
            "text": "Figure 9 presents the performance comparison among the LLaMA family. Models with a larger parameter size generally demonstrate superior performance on the WikiBio GPT-3 dataset. However, despite being twice the size of LLaMA-30b, LLaMA-65b underperforms across four out of the five evaluated metrics compared to LLaMA-30b."
        },
        {
            "heading": "G Hyper parameters analysis",
            "text": "Figure 10 shows the performance of LLaMA-30b with \u03b3 ranging from 0 to 1. When \u03b3 is set to zero, no penalty is accumulated to the token hallucination score. Figure 11 depicts the performance impact of varying \u03c1. Setting \u03c1 either too large or too small leads to a decrease in performance."
        },
        {
            "heading": "H Additional Results",
            "text": "The main results including all the 22 proxy models are shown in Table 8. As observed in Table 9 to Table 29, our method consistently outperforms the performance achieved by solely relying on the uncertainty metric. The optimal setting may vary across models, we attribute this to the different generation patterns exhibited by each model.\n6https://huggingface.co/ausboss/llama-30b-supercot"
        }
    ],
    "title": "Enhancing Uncertainty-Based Hallucination Detection with Stronger Focus",
    "year": 2023
}