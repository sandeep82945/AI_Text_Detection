{
    "abstractText": "Maintaining a consistent persona is a key quality for any open domain dialogue system. Current state-of-the-art systems do this by training agents with supervised learning or online reinforcement learning (RL). However, systems trained with supervised learning often lack consistency as they are never punished for uttering contradictions. Additional training with RL can alleviate some of these issues, however the training process is expensive. Instead, we propose an offline RL framework to improve the persona consistency of dialogue systems. Our framework allows us to combine the advantages of previous methods as we can inexpensively train our model on existing data as in supervised learning, while punishing and rewarding specific utterances as in RL. We also introduce a simple importance sampling method to reduce the variance of importance weights in offline RL training which we call Variance-Reducing MLE-Initialized (VaRMI) importance sampling. Our automatic and human evaluations show that our framework improves both the persona consistency and dialogue quality of a state-of-the-art social chatbot.",
    "authors": [
        {
            "affiliations": [],
            "name": "Ryan Shea"
        },
        {
            "affiliations": [],
            "name": "Zhou Yu"
        }
    ],
    "id": "SP:f58503a1a6826e33bbc5f5078df40560e05363ab",
    "references": [
        {
            "authors": [
                "Pawe\u0142 Budzianowski",
                "Tsung-Hsien Wen",
                "Bo-Hsiang Tseng",
                "I\u00f1igo Casanueva",
                "Stefan Ultes",
                "Osman Ramadan",
                "Milica Ga\u0161i\u0107."
            ],
            "title": "MultiWOZ - a largescale multi-domain Wizard-of-Oz dataset for taskoriented dialogue modelling",
            "venue": "Proceedings of the",
            "year": 2018
        },
        {
            "authors": [
                "Michael C. Frank",
                "Noah D. Goodman."
            ],
            "title": "Predicting pragmatic reasoning in language games",
            "venue": "Science, 336(6084):998\u2013998.",
            "year": 2012
        },
        {
            "authors": [
                "Hado van Hasselt",
                "Yotam Doron",
                "Florian Strub",
                "Matteo Hessel",
                "Nicolas Sonnerat",
                "Joseph Modayil"
            ],
            "title": "Deep reinforcement learning and the deadly triad",
            "year": 2018
        },
        {
            "authors": [
                "Youngsoo Jang",
                "Jongmin Lee",
                "Kee-Eung Kim."
            ],
            "title": "Gpt-critic: Offline reinforcement learning for end-toend task-oriented dialogue systems",
            "venue": "International Conference on Learning Representations (ICLR).",
            "year": 2022
        },
        {
            "authors": [
                "Natasha Jaques",
                "Judy Hanwen Shen",
                "Asma Ghandeharioun",
                "Craig Ferguson",
                "Agata Lapedriza",
                "Noah Jones",
                "Shixiang Gu",
                "Rosalind Picard."
            ],
            "title": "Humancentric dialog training via offline reinforcement learning",
            "venue": "Proceedings of the 2020 Conference on Em-",
            "year": 2020
        },
        {
            "authors": [
                "Hyunwoo Kim",
                "Byeongchang Kim",
                "Gunhee Kim."
            ],
            "title": "Will I sound like me? improving persona consistency in dialogues through pragmatic selfconsciousness",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language",
            "year": 2020
        },
        {
            "authors": [
                "Ilia Kulikov",
                "Alexander Miller",
                "Kyunghyun Cho",
                "Jason Weston."
            ],
            "title": "Importance of search and evaluation strategies in neural dialogue modeling",
            "venue": "Proceedings of the 12th International Conference on Natural Language Generation, pages 76\u201387, Tokyo,",
            "year": 2019
        },
        {
            "authors": [
                "Sergey Levine",
                "Aviral Kumar",
                "G. Tucker",
                "Justin Fu."
            ],
            "title": "Offline reinforcement learning: Tutorial, review, and perspectives on open problems",
            "venue": "ArXiv, abs/2005.01643.",
            "year": 2020
        },
        {
            "authors": [
                "Margaret Li",
                "Stephen Roller",
                "Ilia Kulikov",
                "Sean Welleck",
                "Y-Lan Boureau",
                "Kyunghyun Cho",
                "Jason Weston."
            ],
            "title": "Don\u2019t say that! making inconsistent dialogue unlikely with unlikelihood training",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for",
            "year": 2020
        },
        {
            "authors": [
                "Sen Huang",
                "Johannes Welbl",
                "Sven Gowal",
                "Alexey Cherepanov",
                "James Molloy",
                "Daniel J. Mankowitz",
                "Esme Sutherland Robson",
                "Pushmeet Kohli",
                "Nando de Freitas",
                "Koray Kavukcuoglu",
                "Oriol Vinyals"
            ],
            "title": "Competition-level code generation with alpha",
            "year": 2022
        },
        {
            "authors": [
                "Qian Liu",
                "Yihong Chen",
                "Bei Chen",
                "Jian-Guang Lou",
                "Zixuan Chen",
                "Bin Zhou",
                "Dongmei Zhang."
            ],
            "title": "You impress me: Dialogue generation via mutual persona perception",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Lin-",
            "year": 2020
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Roberta: A robustly optimized bert pretraining approach",
            "venue": "arXiv preprint arXiv:1907.11692.",
            "year": 2019
        },
        {
            "authors": [
                "Alexander Miller",
                "Will Feng",
                "Dhruv Batra",
                "Antoine Bordes",
                "Adam Fisch",
                "Jiasen Lu",
                "Devi Parikh",
                "Jason Weston."
            ],
            "title": "ParlAI: A dialog research software platform",
            "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing:",
            "year": 2017
        },
        {
            "authors": [
                "R\u00e9mi Munos",
                "Tom Stepleton",
                "Anna Harutyunyan",
                "Marc G. Bellemare"
            ],
            "title": "Safe and efficient offpolicy reinforcement learning",
            "year": 2016
        },
        {
            "authors": [
                "Richard Yuanzhe Pang",
                "He He."
            ],
            "title": "Text generation by learning from demonstrations",
            "venue": "International Conference on Learning Representations.",
            "year": 2021
        },
        {
            "authors": [
                "Doina Precup",
                "Richard S. Sutton",
                "Satinder Singh."
            ],
            "title": "Eligibility traces for off-policy policy evaluation",
            "venue": "International Conference on Machine Learning.",
            "year": 2000
        },
        {
            "authors": [
                "Stephen Roller",
                "Emily Dinan",
                "Naman Goyal",
                "Da Ju",
                "Mary Williamson",
                "Yinhan Liu",
                "Jing Xu",
                "Myle Ott",
                "Kurt Shuster",
                "Eric M. Smith",
                "Y-Lan Boureau",
                "Jason Weston"
            ],
            "title": "Recipes for building an opendomain chatbot",
            "year": 2020
        },
        {
            "authors": [
                "Vincent Michalski",
                "Alexandre Nguyen",
                "Joelle Pineau",
                "Yoshua Bengio"
            ],
            "title": "A deep reinforcement learning chatbot",
            "year": 2017
        },
        {
            "authors": [
                "Weiyan Shi",
                "Yu Li",
                "Saurav Sahay",
                "Zhou Yu."
            ],
            "title": "Refine and imitate: Reducing repetition and inconsistency in persuasion dialogues via reinforcement learning and human demonstration",
            "venue": "Findings of the",
            "year": 2021
        },
        {
            "authors": [
                "Jason Weston"
            ],
            "title": "Blenderbot 3: a deployed conversational agent that continually learns to responsibly engage",
            "year": 2022
        },
        {
            "authors": [
                "Charlie Snell",
                "Sherry Yang",
                "Justin Fu",
                "Yi Su",
                "Sergey Levine."
            ],
            "title": "Context-aware language modeling for goal-oriented dialogue systems",
            "venue": "Findings of the Association for Computational Linguistics: NAACL 2022, pages 2351\u20132366, Seattle, United States. Asso-",
            "year": 2022
        },
        {
            "authors": [
                "Charlie Victor Snell",
                "Ilya Kostrikov",
                "Yi Su",
                "Sherry Yang",
                "Sergey Levine."
            ],
            "title": "Offline RL for natural language generation with implicit language q learning",
            "venue": "The Eleventh International Conference on Learning Representations.",
            "year": 2023
        },
        {
            "authors": [
                "Haoyu Song",
                "Yan Wang",
                "Wei-Nan Zhang",
                "Xiaojiang Liu",
                "Ting Liu."
            ],
            "title": "Generate, delete and rewrite: A three-stage framework for improving persona consistency of dialogue generation",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Compu-",
            "year": 2020
        },
        {
            "authors": [
                "Haoyu Song",
                "Wei-Nan Zhang",
                "Yiming Cui",
                "Dong Wang",
                "Ting Liu."
            ],
            "title": "Exploiting persona information for diverse generation of conversational responses",
            "venue": "Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI-19,",
            "year": 2019
        },
        {
            "authors": [
                "Haoyu Song",
                "Weinan Zhang",
                "Jingwen Hu",
                "Ting Liu."
            ],
            "title": "Generating persona consistent dialogues by exploiting natural language inference",
            "venue": "AAAI Conference on Artificial Intelligence.",
            "year": 2019
        },
        {
            "authors": [
                "Siddharth Verma",
                "Justin Fu",
                "Sherry Yang",
                "Sergey Levine."
            ],
            "title": "CHAI: A CHatbot AI for task-oriented dialogue with offline reinforcement learning",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational",
            "year": 2022
        },
        {
            "authors": [
                "Sean Welleck",
                "Ilia Kulikov",
                "Stephen Roller",
                "Emily Dinan",
                "Kyunghyun Cho",
                "Jason Weston"
            ],
            "title": "Neural text generation with unlikelihood training",
            "year": 2019
        },
        {
            "authors": [
                "Sean Welleck",
                "Jason Weston",
                "Arthur Szlam",
                "Kyunghyun Cho."
            ],
            "title": "Dialogue natural language inference",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3731\u20133741, Florence, Italy. Association for",
            "year": 2019
        },
        {
            "authors": [
                "Adina Williams",
                "Nikita Nangia",
                "Samuel Bowman."
            ],
            "title": "A broad-coverage challenge corpus for sentence understanding through inference",
            "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Lin-",
            "year": 2018
        },
        {
            "authors": [
                "Ronald J. Williams."
            ],
            "title": "Simple statistical gradientfollowing algorithms for connectionist reinforcement learning",
            "venue": "Mach. Learn., 8(3\u20134):229\u2013256.",
            "year": 1992
        },
        {
            "authors": [
                "Yifan Wu",
                "George Tucker",
                "Ofir Nachum"
            ],
            "title": "Behavior regularized offline reinforcement learning",
            "year": 2019
        },
        {
            "authors": [
                "Semih Yavuz",
                "Abhinav Rastogi",
                "Guan-Lin Chao",
                "Dilek Hakkani-Tur."
            ],
            "title": "DeepCopy: Grounded response generation with hierarchical pointer networks",
            "venue": "Proceedings of the 20th Annual SIGdial Meeting on Discourse and Dialogue, pages 122\u2013132, Stock-",
            "year": 2019
        },
        {
            "authors": [
                "Saizheng Zhang",
                "Emily Dinan",
                "Jack Urbanek",
                "Arthur Szlam",
                "Douwe Kiela",
                "Jason Weston"
            ],
            "title": "Personalizing dialogue agents: I have a dog, do you have pets too",
            "venue": "In Proceedings of the 56th Annual Meeting of the Association for Computational Lin-",
            "year": 2018
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "The rapid advancements of large language models in recent years has enabled the development of dialogue agents that can generate remarkably fluent and natural responses (Shuster et al., 2022; Thoppilan et al., 2022). These dialogue systems are typically trained on large amounts of unlabeled text data with some additional fine-tuning on dialogue tasks. While this does allow models to effectively learn many of the patterns and syntax of natural language, dialogue agents still suffer from many problems including a lack of consistency (Li et al., 2020; Kim et al., 2020; Song et al., 2019a).\nTo resolve consistency issues in the context of social dialogue, prior work has proposed conditioning dialogue generation on a persona describing\nthe agent (Zhang et al., 2018). This persona consists of descriptions such as \u201cI enjoy skiing\u201d or \u201cI have blonde hair\u201d (Figure 2). Given the advantages of persona grounded dialogue, previous research has been focused making dialogue agents more persona consistent (Liu et al., 2020; Song et al., 2020). Existing methods to improve persona consistency are typically centered around the use of supervised learning or online RL (Song et al., 2019b; Zhang et al., 2018). These methods have been somewhat successful, but still face many problems. Supervised learning methods only focus on encouraging persona entailing examples without properly punishing contradictions. This results in dialogue systems that are insensitive to contradictory utterances, leading to inconsistency (Kim et al., 2020).\nSome work has attempted resolve the problems with supervised learning through the use of online RL (Song et al., 2019b; Liu et al., 2020). However, the training process for RL is quite expensive since the dialogue model must continuously generate new training samples. Furthermore, online RL methods require the use of accurate critics to evaluate the generated bot utterances. These critics must incentivize persona consistency while also enforcing strong constraints on dialogue fluency, as without them the model will degenerate (Verma et al., 2022; Song et al., 2019b). This requires training multiple, separate critic models or using human critics during training which is also expensive.\nGiven these challenges, we propose an offline RL framework to improve the persona consistency of open domain dialogue systems (Figure 1). Offline RL has several advantages over existing training methods. Unlike supervised learning, offline RL explicitly punishes contradictory utterances during training. This further improves persona consistency by making the bot more sensitive to contradictions. Unlike online RL, offline RL does not require our dialogue model to generate new samples during training. Instead, we can inexpensively train our\nmodel using large existing datasets that have been collected/synthesized for supervised learning. We exploit this pre-existing data to train our model on human annotated reward labels instead of classifier based rewards which are common in online RL. Training on human-annotated rewards also reduces the chance of training failures due to policy divergence. This can arise in settings where value function approximation is needed to determine Q-values and may require the use of behavior regularization (van Hasselt et al., 2018; Wu et al., 2019).\nDespite the advantages of offline RL, offline RL training can suffer from high variance due to the need for importance sampling. To alleviate this, we introduce an importance sampling method called VaRMI to reduce the variance of importance weights. This method can be applied beyond our task to other settings where policy-gradient offline RL training is used.\nPrior work has explored the application of offline RL on task-oriented dialogue (Verma et al., 2022; Snell et al., 2022; Jang et al., 2022). Task oriented dialogue is a natural extension of offline RL as crafting a reward function is straightforward. Applying offline RL to social dialogue is less clear as there is no obvious reward to use for our policy. We exploit the fact that persona consistency is a key component of open domain dialogue. Intuitively, this makes sense as humans naturally speak with a persona during a conversation. Prior studies have shown that improving persona consistency also improves the quality of social dialogue (Zhang et al.,\n2018; Roller et al., 2020). Our contributions can be summarized as follows:\n\u2022 We propose an offline RL framework to build persona consistent dialogue agents. This includes a persona consistency critic that uses ground truth, human annotated rewards instead of noisy, classifier-based rewards.\n\u2022 We introduce VaRMI, a simple importance sampling method to reduce the variance of importance weights in policy gradient offline RL training.\n\u2022 Our approach improves the persona consistency of BlenderBot3 (BB3) according to both automatic and human evaluations. Along with improving persona consistency, human evaluations also show that our approach improves the dialogue quality of the model."
        },
        {
            "heading": "2 Related Work",
            "text": "Persona Consistent Dialogue In recent years, persona-based dialogue generation has typically been centered around the PersonaChat dataset (Zhang et al., 2018). One easy method to achieve persona consistent dialogue is to simply fine-tune a model on this dataset using supervised learning (Roller et al., 2020; Shuster et al., 2022; Yavuz et al., 2019). However agents trained in this manner still suffer from consistency issues for reasons discussed previously.\nGiven this, prior work has been centered around improving persona consistency by more explicitly\nencouraging entailing utterances and discouraging contradictory ones. Many of these methods involve the use of online RL such as Song et al., 2019b which uses a natural language inference (NLI) classifier and naturalness module as a critic or Liu et al., 2020 which uses mutual persona perception. Some other approaches attempt to improve consistency without any additional training of the dialogue policy. These methods use frameworks such as multistage re-writing (Song et al., 2020) or Bayesian rational speech acts (Kim et al., 2020; Frank and Goodman, 2012). Multistage re-writing is limited by its inability to handle multi-turn persona consistency. On the other hand the Bayesian RSA has an increased computational cost during inference time due to the modified decoding scheme. This results longer response times from the bot as well as the need for greedy decoding, which reduces response diversity and dialogue quality.\nSome methods also propose the use of unlikelihood training as a method to improve persona consistency (Li et al., 2020; Welleck et al., 2019a). However, unlikelihood training suffers from the fact that it does not explicitly reward entailing utterances and instead treats entailing and neural utterances as equally good (Li et al., 2020). Furthermore, unlikelihood training punishes contradictory utterances at a token level which can lead to incoherent responses and uninterpretable behaviors (Shi et al., 2021). Our offline RL method can instead distill utterance level information about contradictions and entailment to improve training and maintain coherence.\nOffline RL Offline RL applications to dialogue tasks are somewhat limited, with the ones that have been proposed focused on task oriented dialogue. This includes tasks such as price negotiation (Verma et al., 2022) or task oriented dialogue benchmarks such as MultiWOZ (Jang et al., 2022;\nBudzianowski et al., 2018). Furthermore many previous studies choose to use offline RL methods centered around Q-learning (Jaques et al., 2020; Snell et al., 2023). While these methods can be effective for dialogue tasks, they require training additional models to steer the dialogue policy towards optimal actions. This adds to both the complexity and resources needed to train and deploy dialogue models for real world applications. We introduce a policy-gradient based offline RL framework with fixed rewards which eliminates the need to use any additional models during training or deployment. Instead our training method can be set up as a supervised learning task with modified loss, which gives it the advantage of being much more simple and efficient to train and deploy.\nDespite these advantages, policy-gradient offline RL has seen limited use in practice due to the high variance that arises as a result of importance sampling. Variance reduction for importance weights emerged in off-policy learning (Munos et al., 2016) and has been widely studied in the context of offline RL (Pang and He, 2021; Levine et al., 2020). Given this, we introduce VaRMI to reduce importance weight variance and improve offline RL training."
        },
        {
            "heading": "3 Method",
            "text": "In this section, we discuss our offline RL framework to improve persona consistency as well as our novel method of importance sampling. Section 3.1 gives an overview of how offline RL training is performed. Section 3.2 details our VaRMI importance sampling method. Lastly, section 3.3 outlines our framework and section 3.4 discusses how we implement our framework on a dialogue model."
        },
        {
            "heading": "3.1 Offline RL",
            "text": "Our offline RL training approach uses a policygradient method to optimize the RL objective. Which is defined as:\nJ(\u03b8) = E\u03c4\u223cp(\u03c0\u03b8(\u03c4)) [ T\u2211 t=0 \u03b3tr(st, at) ]\nwhere \u03c4 denotes a trajectory of states, st, and actions, at, and \u03b3 denotes the discount factor. The policy gradient is obtained by directly computing the gradient of the RL objective with respect to our\npolicy (Williams, 1992) which gives:\n\u2207\u03b8J(\u03b8) =\nE\u03c4\u223cp(\u03c0\u03b8(\u03c4)) [ T\u2211 t=0 \u2207\u03b8 log \u03c0\u03b8(at|st)Q\u0302(st, at) ]\nwhere Q\u0302(st, at) is the estimated return from the current state. In our case this is an utterance-level reward, taking a value of -1 or 1 given by our critic, which reflects whether or not the utterance adheres to the persona it has been given. Our reward function does not consider response fluency as our training is conducted offline (see Section 3.3 for reward function details). Our training samples only include fluent responses originating from the\nPersonaChat dataset. Therefore our model will not encounter issues where it utters incoherent, nonsensical responses which is a common problem when performing training online.\nWhen using policy-gradient methods for online RL we collect samples from our policy directly to compute the gradient with respect to our policy. However in offline RL our samples come from some behavioural policy \u03c0b that is different from the policy we want to optimize.\nIn order to estimate expectations under our policy \u03c0\u03b8 given samples from \u03c0b we can use importance sampling to obtain an unbiased estimator of our policy gradient (Precup et al., 2000):\n\u2207\u03b8J(\u03b8) =\nE\u03c4\u223cp(\u03c0b(\u03c4)) [ T\u2211 t=0 wt\u2207\u03b8 log \u03c0\u03b8(at|st)Q\u0302(st, at) ]\nwhere wt = \u220ft\nt\u2032=0 \u03c0\u03b8(at\u2032 |st\u2032 ) \u03c0b(at\u2032 |st\u2032 )\nare importance weights. In practice we use a per-action approximation of our importance weights wt \u2248 \u03c0\u03b8(at|st)\u03c0b(at|st) to reduce the variance of our gradient at the cost of adding bias to the estimator. Empirical work has shown that this approach can work well when our two policies are sufficiently similar (Levine et al., 2020; Pang and He, 2021; Serban et al., 2017), which we argue is the case here since we initialize \u03c0\u03b8 to the MLE solution of our task before training.\nGiven that we do not know the \u03c0b that produced our samples, we need to make some assumptions about our behavioral policy in order to derive our importance weights during training. We test two different assumptions to derive these importance weights which are described next.\nFor our first importance sampling method, we assume that all of our training samples have the same likelihood under \u03c0b which allows us to ignore it during optimization. This gives us wt = \u03c0\u03b8(at|st) for the importance weights. This method of importance sampling is what is used in the GOLD algorithm (Pang and He, 2021) and has been shown to work well in various scenarios where \u03c0b is unknown (Li et al., 2022). We refer to this importance sampling method as the GOLD method. Our second method of importance sampling is discussed in detail in the next section."
        },
        {
            "heading": "3.2 VaRMI Importance Sampling",
            "text": "The biggest issue that faces policy-gradient based offline RL methods is the fact that the gradient\nestimator can have high variance (Levine et al., 2020). This comes from the fact that importance sampling is needed to correct for the distributional shift between \u03c0\u03b8 and \u03c0b. We introduce VaRMI to alleviate this issue and improve training for policygradient offline RL.\nFor our VaRMI importance sampling method, we reduce the variance of our importance weights by taking advantage of the fact that we initialize \u03c0\u03b8 to the MLE solution of our task before beginning offline RL training. This means that \u03c0\u03b8 has already been trained on a large amount of positive reward examples and we can assume a minimal amount of distributional shift during offline RL. In other words, we are assuming that \u03c0\u03b8 has learned the \u03c0b that generates \u201dgood\u201c examples to an arbitrary degree. Therefore we set wt =\n\u03c0\u03b8(at|st) \u03c0b(at|st) \u2248 1 for\nour positive reward candidates and wt = \u03c0\u03b8(at|st) for our negative reward candidates. This simple method effectively eliminates a large portion of our importance weights to reduce variance at the cost of adding bias to our estimator. In our setting, this means that we set the importance weights of persona entailing utterances (\u201dgood\u201c examples) to one and set the weight of contradictory utterances (\u201dbad\u201c examples) to their likelihood under our policy.\nOur use of VaRMI is limited to persona consistency, but can be applied to other tasks as long as the following conditions hold.\n1. There is some notion of absolute positive and negative rewards for the task. This is in contrast to relative positive and negative rewards that come from subtracting reward values by a baseline.\n2. The acting policy has been initialized to the MLE solution for the task.\nThese conditions are easily satisfied for a wide variety of tasks within dialogue and beyond. While this is promising, more work needs to done to determine how well this method generalizes to tasks with more complex rewards, longer time steps, and other tasks unrelated to persona consistency. We leave this analysis for future work."
        },
        {
            "heading": "3.3 Framework",
            "text": "In this section we go over the details of our framework. This includes how we construct our critic to use human annotated rewards for persona consistency and how we generate our offline dataset.\nOur critic is constructed by performing a mapping between the dialogue natural language inference (DNLI) (Welleck et al., 2019b) and PersonaChat (Zhang et al., 2018) datasets (Figure 2). The PersonaChat dataset is a crowd sourced dialogue dataset where two workers are given a persona and asked to chat while adopting said persona. The dataset consists of 10,907 dialogues in total with 1,000 set aside for validation and 968 set aside for testing. The DNLI dataset contains 310,110 sentence pairs from the PersonaChat dataset along with human annotated triples for each sentence. Each sentence pair comes with a label for entailment, neutrality, or contradiction which is based on the overlap between the triples.\nSince the sentences in DNLI come directly from PersonaChat, we can easily perform a mapping between the two datasets to obtain a set of dialogue samples and corresponding entailment labels. When performing our mapping, we only consider pairs in the DNLI dataset that have one sentence map to a dialogue utterance in the PersonaChat training set and have the other sentence map to a persona. We then add the DNLI persona to the existing persona set and use the matching sentence as the next-utterance candidate.\nSince we are inserting new personas into the PersonaChat dataset during the mapping process, we need to ensure that our data does not include persona sets where two personas contradict each other. To do this, we filter out any personas in our dataset that contradict the one we have inserted. We achieve this by using the human annotated triples corresponding to each persona. We take a conservative approach and remove any personas whose triples have any entity overlap.\nEach persona in the PersonaChat training set is present in the DNLI dataset, therefore we can use this method for all of the personas. We do some additional filtering with a NLI classifier (Liu et al., 2019) as there are situations for some longer personas where the triple does not capture all relevant information for deriving entailment. We also filter out all sentences that are labeled as neutral with respect to the inserted persona, as we consider these utterances to have a reward of zero. After performing our mapping and filtering, we are left with around 42K utterance candidates that can be used for training with offline RL.\nAn item in our dataset consists of a persona, dialogue context, utterance candidate, and entailment\nlabel. The persona and dialogue context are concatenated together to form our state, the utterance candidate is used as \u03c4 , and our estimated return is given from the entailment label. Example dialogues from our mapped dataset can be seen in Table 1."
        },
        {
            "heading": "3.4 Implementation",
            "text": "We implement our method on BlenderBot3 (BB3), an open-source, state-of-the-art dialogue system developed by Meta for open-domain dialogue (Shuster et al., 2022). BB3 has already been fine-tuned on the several datasets, including PersonaChat, in an attempt to \u201cblend\u201d several conversational skills. BB3 achieves a perplexity of \u2248 5.8 on the PersonaChat dataset and the authors note that performing additional fine-tuning results in overfitting (Shuster et al., 2022). While BB3 has been shown to perform well across a variety of conversational domains, it has been known to suffer from consistency issues, with human evaluations showing that it is actually less consistent than the first iteration of BlenderBot (Roller et al., 2020; Shuster et al., 2022).\nWe train the three billion (3B) parameter version\nof BB3 for four epochs with both of our importance sampling methods. We use a learning rate of 5e\u22127 for GOLD and 1e\u22126 for VaRMI. We implement our method within the ParlAI framework (Miller et al., 2017).\nWe find that BB3\u2019s modules for dynamic memory, internet search, and memory decision tend to be error prone and degrade dialogue performance. Therefore we choose to disable these modules during deployment. Disabling them also helps us better isolate the effects of persona consistency as the model\u2019s responses are now only conditioned on its persona."
        },
        {
            "heading": "4 Experiments",
            "text": "We test the effectiveness of our offline RL framework for persona consistency using both automatic and human evaluations. Our results show that both importance sampling methods are able to improve the persona consistency of BB3. Human evaluations also show that our VaRMI importance sampling method improves the overall dialogue quality of the model."
        },
        {
            "heading": "4.1 Evaluation Datasets",
            "text": "DNLI Evaluation Set Along with the with the base DNLI dataset, Welleck et al., 2019b also release a separate evaluation set to test the persona consistency of dialogue models. Whereas the the base DNLI dataset contains sentence pairs along with entailment labels. The DNLI evaluation dataset consists of personas and dialogue histories from the PersonaChat evaluation set along with 31 utterance candidates. Ten of these candidates are contradictory, ten are entailing, ten are neutral, and one is the actual next utterance. The model then ranks these candidates with the goal of ranking gold and entailing utterances highest. The evaluation set contains a total of 542 dialogues for testing.\nMapped DNLI-PersonaChat Dataset We also perform evaluation on 5k dialogues from our mapped dataset. We hold out these dialogues from our training and split them into positive and negative utterance candidates based on their entailment. The goal of our offline RL framework is to encourage entailing candidates and discourage contradictions. By tracking model performance on these two sets we can evaluate the success of our training methods."
        },
        {
            "heading": "4.2 Automatic Evaluation",
            "text": ""
        },
        {
            "heading": "Results on Mapped DNLI-PersonaChat Dataset",
            "text": "Figure 3 shows the resulting loss trajectories on our positive and negative utterance sets over the course of training. Epoch 0 shows the loss on both sets before any offline RL training is performed. We can see that the gap in loss between both sets is relatively small at this point, which indicates that our baseline model is less sensitive to contradictory utterances.\nWhen performing training with GOLD the loss for both sets increases over the course of training. However, we do note that the loss for the negative candidates increases more than for the positive candidates. This suggests that our model is becoming more sensitive to contradictions although it may also be being disincentivized to picking entailing utterances, albeit to a lesser degree.\nThe results with VaRMI training are more aligned with what we expect. After training for four epochs, the loss on the positive candidates has decreased below what its value was prior to training with offline RL while the loss on the negative candidates has nearly doubled. This suggests that this method is successfully incentivizing our model to choose entailing utterances and avoid contradictory ones. We also see that the loss on the contradictory utterances changes much more than the loss for entailing utterances. This is likely due to the fact that our model has already been trained on many persona entailing examples during imitation learning\nand therefore there is less room for improvement on these examples.\nResults on DNLI Evaluation Dataset Table 2 shows the results of our training methods on the DNLI evaluation dataset. We compare these results against BB3 trained using imitation learning only as well as a baseline trained with online RL using the framework defined in Song et al., 2019b. Full details of how we implement this baseline can be found in Appendix A.\nHits@1 indicates the percentage of top-1 candidates returned by the model that match the gold next utterance value. Entail@1 indicates the percent of top candidates returned by the model that have the same underlying triple as the gold next utterance value, this can be viewed as a more lenient version of Hits@1. Contradict@1 indicates the percent of top candidates returned by the model that have a triple that contradicts the triple for the gold next utterance value. Lastly, Rand@1 indicates the percent of top candidates returned by the model that have a triple that neither contradicts nor entails the triple for the gold next utterance value.\nBoth methods of offline training outperform the baselines on this task with the GOLD method performing the best a reducing contradictions while the VaRMI method does the best in all the other categories. This includes ranking both gold and entailing utterances highly and reducing the number of neutral candidates. While neutral utterances\ncan sometimes be the best option under some conversational circumstances, the gold utterances in this evaluation set are all entailing. Therefore gold or entailing utterances should always be ranked highest.\nAll of the improvements of the offline training methods over the BB3 and BB3+RL baselines and are statistically significant based on two-sample z-tests. However, none of the differences between GOLD and VaRMI are significant. We also note that online-RL training results in no significant differences compared to BB3 trained with supervised learning only."
        },
        {
            "heading": "4.3 Human Evaluation",
            "text": "Setup For our human evaluation we gathered 90 individuals via email, social media, and in-person recruiting to test our models. Each person was randomly assigned to test one of our three systems, giving us 30 responses per model. Each user was instructed to chat with our bot for at least seven turns and then answer a post-chat survey where they rated the quality of the conversation on a scale from 1-5 as well as how well the bot represented its persona during the conversation on a scale from\n1-5. They could also optionally provide any complaints/suggestions they had about the bot in a text box. More details on our human evaluation can be seen in Appendix C.\nThe bot\u2019s persona was randomly selected from a set of 967 personas from the PersonaChat dataset. Users were only shown the bot\u2019s persona after they completed the chat and could reference both the bot\u2019s persona and their chat history during the postchat survey.\nResults The results of our human evaluation are shown in Table 3. Given that we have a different user for each conversation, and to remain consistent with prior work in the area (Kim et al., 2020; Welleck et al., 2019b), we apply Bayesian calibration (Kulikov et al., 2019) to our results to correct for annotator bias and inter-annotator variability.\nOur results show that both of our offline RL methods improve the bot\u2019s consistency with respect to its persona, with GOLD doing the best in this regard. While our VaRMI importance sampling method also improves the dialogue quality over the BB3 baseline, the GOLD importance sampling method performs worse in terms of quality compared to both other methods."
        },
        {
            "heading": "4.4 User Comments and Error Analysis",
            "text": "We received several complaints and suggestions about overall bot quality from users. Many of these comments were shared between all bots. The two biggest comments we received were that the bot\u2019s language was awkward and that it had a tendency to switch topics abruptly.\nWe also received some complaints about our bots over representing their persona during the course of the chat. These complaints were particularly bad for the GOLD method. Several users reported that the bot would ignore what they had just said and instead just talk about its persona. Some also reported that the conversation felt almost scripted, again due to the fact that the bot was overly fixated on its persona. These comments, while not universal, validate the results we see from our human evaluations. While the GOLD bot does do a good job of representing its persona, it may have traded better persona consistency for some level of dialogue quality.\nThis also raises a question about how well a chatbot should be representing its persona over the course of a chat. In some settings it may be very unnatural to fully represent one\u2019s persona over the\ncourse of a conversation. This is especially true for our scenario where the chat was often only seven turns. Therefore the optimal consistency score for our bots may vary depending on the type of conversation being had. The optimal overall score for consistency may be closer to what VaRMI obtained due the the fact that it was able to improve both consistency and quality over our baseline.\nThe BB3 baseline model was the only model where we received several complaints about the bot not adequately representing its persona. Table 4 shows a snippet of a conversation from our human evaluation where the bot exhibited persona inconsistency. Table 5 shows a conversation with our VaRMI trained bot with a similar persona. This bot was able to correct the contradictions and improve consistency. Full conversations from our human evaluation can be found in Appendix C."
        },
        {
            "heading": "5 Conclusion and Future Work",
            "text": "In this paper, we demonstrated that offline RL can be effectively used to improve the quality and utility of open-domain dialogue systems. To do this, we applied offline RL to a persona consistency task and demonstrated its ability to improve persona consistency and dialogue quality over a system trained with only imitation learning. We developed a persona consistency critic that uses human anno-\ntated labels for persona consistency as well as a novel importance sampling method called VaRMI. Our automatic and human evaluations show that our framework is able to successfully improve the persona consistency of BB3 as well as the overall dialogue quality of the system.\nA promising direction of future work is to extend our framework to improve other aspects of open domain dialogue such as reducing hallucinations and offensive language. Given the ability of LLMs to generate quality synthetic data, this can be done more easily without having to collect human conversations. It is also worth exploring how well VaRMI can generalize to other tasks. Offline policy gradient methods have seen somewhat limited use due to their high variance so it is worth testing if VaRMI can reduce these issues more broadly."
        },
        {
            "heading": "6 Limitations",
            "text": "The biggest limitation with our framework is the fact that our number of training samples is always fixed. In order to train on more samples we need to either collect more data from human users or synthesize data from a LLM. Both of which are more expensive than online RL methods which can generate an arbitrary number of samples with no increase in cost over time.\nOur human experiments were also limited to some degree by the size of our language model. Due to resource constraints, we were only able to use the 3B parameter version of BB3 which is much smaller than many existing state-of-the-art language models. Unfortunately, the next largest version of BB3 is 30B parameters which is a much larger model than what our current resources allow us to train. For future bots we may want to focus more effort on making the language model bigger, that alone should reduce some of the quality complaints we received."
        },
        {
            "heading": "7 Ethical Concerns",
            "text": "By giving a language model a persona we are also encouraging it to pretend to be human. Even when users ask the bot if it is a bot it will be incentivized to say no, as admitting it is a bot will almost certainly contradict its persona. Therefore it is important to be up front with users that they are speaking with a chatbot before they begin conversing. In all of our experiments we made it clear that they were speaking with a bot and instructed them not to give any personally identifiable information to the bot."
        },
        {
            "heading": "8 Acknowledgements",
            "text": "We would like to thank Tencent for supporting this work through a research gift."
        },
        {
            "heading": "A Additional Implementation Details",
            "text": ""
        },
        {
            "heading": "A.1 Offline RL",
            "text": "All training was conducted on two NVIDIA RTX A6000 GPUs. One training epoch takes about 8 hours and 30 minutes and evaluating the model on our test dataset takes about 30 minutes. We evaluate our model on the test data after each training epoch, therefore the total training time takes about 36 hours per model. We did not use any validation data during the training process.\nDuring offline RL training we follow Pang and He, 2021 and lower bound our importance weights by a small, adjustable value \u03b1. We do this to increase the speed of training, as without this lowerbound our importance weights can become vanishingly small and slow training progress. Additional details about our data and hyper-parameters can be found in our source code1."
        },
        {
            "heading": "A.2 Online RL Baseline",
            "text": "We implement our online RL baseline based on the framework defined in Song et al., 2019b. This framework consists of a persona-consistency module and a naturalness module, both of which are used to generate rewards for RL training. For the persona consistency module we used a large RoBERTa model (Liu et al., 2019) fine-tuned on the Multi-Genre Natural Language Inference dataset (Williams et al., 2018). Song et al., 2019b notes that omitting the naturalness module results in superior persona consistency at the expense of dialogue quality. Given that our automatic evaluations do not consider dialogue quality, we chose to omit the naturalness module in order to achieve the best results on consistency.\nB VaRMI Variance Reduction\nTo show that VaRMI can reduce the variance of our importance weights in practice, we estimated the variation of a subset of our weights with bootstrapping. Using dialogues from our mapped dataset, we found that the importance sampling weights using the GOLD method had an average coefficient of variation of 3.81 while the average for the weights using VaRMI is 1.91, which is a reduction of about half. This makes intuitive sense as about half of the dialogues in our sample were persona entailing examples. That means that when we\n1https://github.com/ryanshea10/ personachat_offline_rl\ndo importance sampling with VaRMI, half of our importance weights are always one, giving them a variance of zero. This is in contrast to GOLD where the importance weights are always equivalent to the estimated likelihood of the utterance under our model."
        },
        {
            "heading": "C Human Evaluation",
            "text": ""
        },
        {
            "heading": "C.1 Recruiting",
            "text": "We chose not to use Amazon Mechanical Turk (AMT) to recruit for our human evaluation. From previous studies, we found that many of of the users recruited on AMT were often disengaged with the task and sometimes did not even write in fluent English. These problems persisted even when requiring participants to have completed 400 HITs with a 95% completion rate and be located in the United States.\nGiven these challenges, we chose to recruit users using a combination of email, social media, and in-person recruiting. We found that the quality of these conversations were much better than what we received on AMT, however the number of responses we were able to collect was comparatively lower. The response rate for users recruited through email and social media were particularly poor, which may be due to the large amount recruiting and advertising that occurs on these platforms. Recruiting in-person yielded a much higher response rate however we were able to reach less people as we had limited recruiters. For future studies we may want to prioritize recruiting in-person to maximize the number of respondents.\nUltimately we were able to collect 90 quality conversations to use for our evaluation. We did not record any demographic information from our respondents in order to reduce the time needed to perform evaluation and improve response rate. The respondents we contacted for evaluation were mostly college students, therefore the results of our evaluation are representative of this group."
        },
        {
            "heading": "C.2 Survey",
            "text": "Figure 4 shows a screenshot of our post-chat survey. Users could view their chat history by clicking the \u201cView Chat\u201d drop down button. We presented users with four different questions during our human evaluation. However, during our analysis we found that the results for questions 3 and 4 were either redundant with the results for questions 1 and 2 or yielded no statistically significant results\n(before and after performing Bayesian calibration). Therefore the results presented in Section 4.3 only reflect the responses for questions 1 and 2."
        },
        {
            "heading": "C.3 Full Conversations",
            "text": "Tables 6-11 show examples of full conversations from our human evaluation. Two conversations are presented for each of our three bots."
        },
        {
            "heading": "Chatbot Persona (BB3+VaRMI)",
            "text": ""
        },
        {
            "heading": "Chatbot Persona (BB3)",
            "text": ""
        },
        {
            "heading": "Chatbot Persona (BB3+VaRMI)",
            "text": ""
        },
        {
            "heading": "Chatbot Persona (BB3+GOLD)",
            "text": ""
        },
        {
            "heading": "Chatbot Persona (BB3)",
            "text": ""
        },
        {
            "heading": "Chatbot Persona (BB3+GOLD)",
            "text": ""
        }
    ],
    "title": "Building Persona Consistent Dialogue Agents with Offline Reinforcement Learning",
    "year": 2023
}