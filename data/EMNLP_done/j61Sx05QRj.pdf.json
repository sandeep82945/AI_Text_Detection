{
    "abstractText": "Neuro-symbolic (NS) models for knowledge graph completion (KGC) combine the benefits of symbolic models (interpretable inference) with those of distributed representations (parameter sharing, high accuracy). While several NS models exist for KGs with static facts, there is limited work on temporal KGC (TKGC) for KGs where a fact is associated with a time interval. In response, we propose a novel NS model for TKGC called NeuSTIP, which performs link prediction and time interval prediction in a TKG. NeuSTIP learns temporal rules with Allen predicates, which ensure temporal consistency between neighboring predicates in the rule body. We further design a unique scoring function that evaluates the confidence of the candidate answers while performing link and time interval predictions by utilizing the learned rules. Our empirical evaluation on two time interval based TKGC datasets shows that our model shows competitive performance on link prediction and establishes a new state of the art on time prediction.",
    "authors": [
        {
            "affiliations": [],
            "name": "Ishaan Singh"
        },
        {
            "affiliations": [],
            "name": "Navdeep Kaur"
        },
        {
            "affiliations": [],
            "name": "Garima Gaur Mausam"
        }
    ],
    "id": "SP:252eb370b5fc2999f490d2dcb73b3da1bc5cfec5",
    "references": [
        {
            "authors": [
                "Ralph Abboud",
                "\u0130smail \u0130lkan Ceylan",
                "Thomas Lukasiewicz",
                "Tommaso Salvatori."
            ],
            "title": "BoxE: A Box Embedding Model for Knowledge Base Completion",
            "venue": "NeuRIPS.",
            "year": 2020
        },
        {
            "authors": [
                "James F. Allen."
            ],
            "title": "Maintaining Knowledge about Temporal Intervals",
            "venue": "Commun. ACM, 26(11):832\u2013843.",
            "year": 1983
        },
        {
            "authors": [
                "Antoine Bordes",
                "Nicolas Usunier",
                "Alberto GarciaDuran",
                "Jason Weston",
                "Oksana Yakhnenko."
            ],
            "title": "Translating Embeddings for Modeling Multirelational Data",
            "venue": "NeurIPS, pages 2787\u20132795.",
            "year": 2013
        },
        {
            "authors": [
                "Ling Cai",
                "Krzysztof Janowicz",
                "Bo Yan",
                "Rui Zhu",
                "Gengchen Mai."
            ],
            "title": "Time in a Box: Advancing Knowledge Graph Completion with Temporal Scopes",
            "venue": "Proceedings of the 11th on Knowledge Capture Conference, K-CAP \u201921, page 121\u2013128,",
            "year": 2021
        },
        {
            "authors": [
                "Kyunghyun Cho",
                "Bart van Merri\u00ebnboer",
                "Caglar Gulcehre",
                "Dzmitry Bahdanau",
                "Fethi Bougares",
                "Holger Schwenk",
                "Yoshua Bengio."
            ],
            "title": "Learning Phrase Representations using RNN Encoder\u2013 Decoder for Statistical Machine Translation",
            "venue": "In",
            "year": 2014
        },
        {
            "authors": [
                "Shib Sankar Dasgupta",
                "Swayambhu Nath Ray",
                "Partha Talukdar."
            ],
            "title": "HyTE: Hyperplane-based Temporally aware Knowledge Graph Embedding",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages",
            "year": 2018
        },
        {
            "authors": [
                "Luis Antonio Gal\u00e1rraga",
                "Christina Teflioudi",
                "Katja Hose",
                "Fabian Suchanek."
            ],
            "title": "AMIE: Association Rule Mining under Incomplete Evidence in Ontological Knowledge Bases",
            "venue": "Proceedings of the 22nd International Conference on World Wide Web, WWW",
            "year": 2013
        },
        {
            "authors": [
                "Alberto Garc\u00eda-Dur\u00e1n",
                "Sebastijan Dumancic",
                "Mathias Niepert."
            ],
            "title": "Learning Sequence Encoders for Temporal Knowledge Graph Completion",
            "venue": "Conference on Empirical Methods in Natural Language Processing.",
            "year": 2018
        },
        {
            "authors": [
                "Rishab Goel",
                "Seyed Mehran Kazemi",
                "Marcus Brubaker",
                "Pascal Poupart."
            ],
            "title": "Diachronic Embedding for Temporal Knowledge Graph Completion",
            "venue": "AAAI.",
            "year": 2020
        },
        {
            "authors": [
                "Zhen Han",
                "Peng Chen",
                "Yunpu Ma",
                "Volker Tresp."
            ],
            "title": "Explainable Subgraph Reasoning for Forecasting on Temporal Knowledge Graphs",
            "venue": "ICLR.",
            "year": 2021
        },
        {
            "authors": [
                "Zhen Han",
                "Zifeng Ding",
                "Yunpu Ma",
                "Yujia Gu",
                "Volker Tresp."
            ],
            "title": "Learning Neural Ordinary Equations for Forecasting Future Links on Temporal Knowledge Graphs",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Lan-",
            "year": 2021
        },
        {
            "authors": [
                "Zhen Han",
                "Yunpu Ma",
                "Yuyi Wang",
                "Stephan G\u00fcnnemann",
                "Volker Tresp."
            ],
            "title": "Graph Hawkes Neural Network for Forecasting on Temporal Knowledge Graphs",
            "venue": "AKBC.",
            "year": 2020
        },
        {
            "authors": [
                "Prachi Jain",
                "Sushant Rathi",
                "Mausam",
                "Soumen Chakrabarti."
            ],
            "title": "Temporal Knowledge Base Completion: New Algorithms and Evaluation Protocols",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
            "year": 2020
        },
        {
            "authors": [
                "Tingsong Jiang",
                "Tianyu Liu",
                "Tao Ge",
                "Lei Sha",
                "Baobao Chang",
                "Sujian Li",
                "Zhifang Sui."
            ],
            "title": "Towards Time-Aware Knowledge Graph Completion",
            "venue": "Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical",
            "year": 2016
        },
        {
            "authors": [
                "Woojeong Jin",
                "Meng Qu",
                "Xisen Jin",
                "Xiang Ren."
            ],
            "title": "Recurrent Event Network: Autoregressive Structure Inference over Temporal Knowledge Graphs",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing",
            "year": 2020
        },
        {
            "authors": [
                "Diederik P. Kingma",
                "Jimmy Ba."
            ],
            "title": "Adam: A Method for Stochastic Optimization",
            "venue": "3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings.",
            "year": 2015
        },
        {
            "authors": [
                "Thomas N. Kipf",
                "Max Welling."
            ],
            "title": "SemiSupervised Classification with Graph Convolutional Networks",
            "venue": "Proceedings of the 5th International Conference on Learning Representations, ICLR \u201917.",
            "year": 2017
        },
        {
            "authors": [
                "Timoth\u00e9e Lacroix",
                "Guillaume Obozinski",
                "Nicolas Usunier."
            ],
            "title": "Tensor Decompositions for Temporal Knowledge Base Completion",
            "venue": "International Conference on Learning Representations.",
            "year": 2020
        },
        {
            "authors": [
                "Yushan Liu",
                "Yunpu Ma",
                "Marcel Hildebrandt",
                "Mitchell Joblin",
                "Volker Tresp."
            ],
            "title": "TLogic: Temporal Logical Rules for Explainable Link Forecasting on Temporal Knowledge Graphs",
            "venue": "AAAI.",
            "year": 2021
        },
        {
            "authors": [
                "Jin Luo",
                "Hong Shen",
                "YanFeng Hu",
                "Chen Peng."
            ],
            "title": "TaCE: Time-aware Convolutional Embedding Learning for Temporal Knowledge Graph Completion",
            "venue": "under review.",
            "year": 2022
        },
        {
            "authors": [
                "Xin Mei",
                "Libin Yang",
                "Xiaoyan Cai",
                "Zuowei Jiang."
            ],
            "title": "An Adaptive Logical Rule Embedding Model for Inductive Reasoning over Temporal Knowledge Graphs",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2022
        },
        {
            "authors": [
                "Christian Meilicke",
                "Melisachew Wudage Chekol",
                "Daniel Ruffinelli",
                "Heiner Stuckenschmidt."
            ],
            "title": "Anytime Bottom-up Rule Learning for Knowledge Graph Completion",
            "venue": "Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intel-",
            "year": 2019
        },
        {
            "authors": [
                "Johannes Messner",
                "Ralph Abboud",
                "\u0130smail \u0130lkan Ceylan."
            ],
            "title": "Temporal Knowledge Graph Completion using Box Embeddings",
            "venue": "AAAI, volume abs/2109.08970.",
            "year": 2022
        },
        {
            "authors": [
                "Ananjan Nandi",
                "Navdeep Kaur",
                "Parag Singla",
                "Mausam ."
            ],
            "title": "Simple Augmentations of Logical Rules for Neuro-Symbolic Knowledge Graph Completion",
            "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics",
            "year": 2023
        },
        {
            "authors": [
                "Meng Qu",
                "Junkun Chen",
                "Louis-Pascal A.C. Xhonneux",
                "Yoshua Bengio",
                "Jian Tang."
            ],
            "title": "RNNLogic: Learning Logic Rules for Reasoning on Knowledge Graphs",
            "venue": "ICLR, pages 1\u201321.",
            "year": 2021
        },
        {
            "authors": [
                "Ali Sadeghian",
                "Mohammadreza Armandpour",
                "Anthony Colas",
                "Daisy Zhe Wang."
            ],
            "title": "ChronoR: Rotation Based Temporal Knowledge Graph Embedding",
            "venue": "AAAI.",
            "year": 2021
        },
        {
            "authors": [
                "Ali Sadeghian",
                "Mohammadreza Armandpour",
                "Patrick Ding",
                "Daisy Zhe Wang."
            ],
            "title": "DRUM: EndTo-End Differentiable Rule Mining On Knowledge Graphs",
            "venue": "NeuRIPS, volume 32. Curran Associates, Inc.",
            "year": 2019
        },
        {
            "authors": [
                "Zhiqing Sun",
                "Zhi-Hong Deng",
                "Jian-Yun Nie",
                "Jian Tang."
            ],
            "title": "RotatE: Knowledge Graph Embedding by Relational Rotation in Complex Space",
            "venue": "ICLR.",
            "year": 2019
        },
        {
            "authors": [
                "Rakshit Trivedi",
                "Hanjun Dai",
                "Yichen Wang",
                "Le Song."
            ],
            "title": "Know-Evolve: Deep Temporal Reasoning for Dynamic Knowledge Graphs",
            "venue": "Proceedings of the 34th International Conference on Machine Learning - Volume 70, ICML\u201917, page 3462\u20133471.",
            "year": 2017
        },
        {
            "authors": [
                "Th\u00e9o Trouillon",
                "Johannes Welbl",
                "Sebastian Riedel",
                "Eric Gaussier",
                "Guillaume Bouchard."
            ],
            "title": "Complex Embeddings for Simple Link Prediction",
            "venue": "ICML, pages 2071\u20132080.",
            "year": 2016
        },
        {
            "authors": [
                "Shikhar Vashishth",
                "Soumya Sanyal",
                "Vikram Nitin",
                "Partha Talukdar."
            ],
            "title": "Composition-based Multirelational Graph Convolutional Networks",
            "venue": "International Conference on Learning Representations.",
            "year": 2020
        },
        {
            "authors": [
                "Liang Wang",
                "Wei Zhao",
                "Zhuoyu Wei",
                "Jingming Liu."
            ],
            "title": "SimKGC: Simple Contrastive Knowledge Graph Completion with Pre-trained Language Models",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume",
            "year": 2022
        },
        {
            "authors": [
                "Jiapeng Wu",
                "Meng Cao",
                "Jackie Chi Kit Cheung",
                "William L. Hamilton."
            ],
            "title": "TeMP: Temporal Message Passing for Temporal Knowledge Graph Completion",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing",
            "year": 2020
        },
        {
            "authors": [
                "Siheng Xiong",
                "Yuan Yang",
                "Faramarz Fekri",
                "James Clayton Kerce."
            ],
            "title": "TILP: Differentiable Learning of Temporal Logical Rules on Knowledge Graphs",
            "venue": "ICLR.",
            "year": 2023
        },
        {
            "authors": [
                "Chengjin Xu",
                "M. Nayyeri",
                "Fouad Alkhoury",
                "Hamed Shariat Yazdi",
                "Jens Lehmann."
            ],
            "title": "TeRo: A Time-aware Knowledge Graph Embedding via Temporal Rotation",
            "venue": "International Conference on Computational Linguistics.",
            "year": 2020
        },
        {
            "authors": [
                "Fan Yang",
                "Zhilin Yang",
                "William W Cohen."
            ],
            "title": "Differentiable Learning of Logical Rules for Knowledge Base Reasoning",
            "venue": "NeuRIPS, volume 30. Curran Associates, Inc.",
            "year": 2017
        },
        {
            "authors": [
                "Liang Yao",
                "Chengsheng Mao",
                "Yuan Luo."
            ],
            "title": "KGBERT: BERT for Knowledge Graph Completion",
            "venue": "AAAI.",
            "year": 2019
        },
        {
            "authors": [
                "Yuyu Zhang",
                "Xinshi Chen",
                "Yuan Yang",
                "Arun Ramamurthy",
                "Bo Li",
                "Yuan Qi",
                "Le Song."
            ],
            "title": "Efficient Probabilistic Logic Reasoning with Graph Neural Networks",
            "venue": "International Conference on Learning Representations.",
            "year": 2020
        },
        {
            "authors": [
                "Cunchao Zhu",
                "Muhao Chen",
                "Changjun Fan",
                "Guangquan Cheng",
                "Yan Zhan."
            ],
            "title": "Learning from History: Modeling Temporal Knowledge Graphs with Sequential Copy-Generation Networks",
            "venue": "AAAI Conference on Artificial Intelligence.",
            "year": 2020
        },
        {
            "authors": [
                "Zhaocheng Zhu",
                "Zuobai Zhang",
                "Louis-Pascal Xhonneux",
                "Jian Tang."
            ],
            "title": "Neural Bellman-Ford Networks: A General Graph Neural Network Framework for Link Prediction",
            "venue": "Advances in Neural Information Processing Systems, 34.",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Knowledge Graphs (KGs) are factual information repositories, where each fact is encoded as r(s, o), where s and o are the real-world entities and r is the relationship between them. For instance, the fact presidentOf(Joe Biden, USA) represents the fact that Joe Biden is the president of USA. Temporal KGs extend these to entity-entity relations that have a temporal facet, for e.g., workedAt(Einstein, ETH_Zurich, [1912,1914]). In this work, we study Temporal KGs that maintain temporal facts, r(s, o, T ), annotating each fact r(s, o) with the time period T for which it holds.\nWhile being a popular source of structured information, KGs are often incomplete. To this end, the problem of enriching KGs by inferring missing information is formulated as a KG completion (KGC) task. In the context of static (non-temporal)\nKGs, a key KGC task is link prediction, viz, given a query r(s, ?), predict o for which the fact r(s, o) holds in the real-world. This problem is fairly well-studied and has been tackled in many different ways. Existing works can be broadly categorized into GNN-based solutions (Zhu et al., 2021; Vashishth et al., 2020), LM-based approaches (Yao et al., 2019; Wang et al., 2022), KG embeddingbased models (Sun et al., 2019; Trouillon et al., 2016), and neuro-symbolic (NS) solutions (Yang et al., 2017; Qu et al., 2021). Of special interest to us are NS approaches, which bring together humaninterpretable deduction capabilities of symbolic models with parameter sharing and other benefits of distributed representations in neural models.\nTemporal KG completion (TKGC) extends KGC task to TKGs. In addition to link prediction, it involves an additional task of time-interval prediction \u2013 given a query r(s, o, ?), infer the time interval when the fact holds true. The majority of existing solutions are KG-embedding based (Dasgupta et al., 2018; Jain et al., 2020; Xu et al., 2020), and do not incorporate symbolic rules. NS approaches have received very limited attention for TKGC. TLogic and ALRE-IR (Liu et al., 2021; Mei et al., 2022) study NS-TKGC, but for time-instant KGs (i.e., KGs where a fact associated with an instant, not an interval). TILP (Xiong et al., 2023) is a very recent model for time-interval TKGC, but it can only do link prediction. To the best of our knowledge, no NS-TKGC approach exists that performs both link and time-interval prediction.\nA key challenge in building an NS-TKGC model is to design a unified rule language, which is intuitive and interpretable to humans, and also effective on both prediction tasks. Secondly, ideally, the confidence of a rule should be computed based on both the statistical properties of the rule groundings and also the similarity scores of latent representations (which is not the case in existing models).\nContributions: We propose NeuSTIP (Neuro\nSymbolic Link and Time Interval Prediction), the first comprehensive NS framework for TKGC, which is effective on both link prediction and time interval prediction tasks. It uses an intuitive rule language that integrates the complete set of Allen algebra relations and KG relations, with the goal of enforcing temporal consistency between neighboring predicates in the rule body. It also uses a novel way to compute confidence of temporal rules that combines both symbolic and embedding information. Moreover, to the best of our knowledge, it is the first NS-TKGC model to perform timeinterval prediction.We evaluate the performance of NeuSTIP using two benchmark time interval KGC datasets, WIKIDATA12k and YAGO11k (Dasgupta et al., 2018). We find that a hybrid of NeuSTIP with a KG-embedding model TimePlex (Jain et al., 2020) obtains best results on both prediction tasks in both datasets. We release the NeuSTIP1 codebase for further research."
        },
        {
            "heading": "2 Related Work",
            "text": "There are three main types of TKGC models in literature: purely embedding-based, multi-hop reasoning based, and rule-based. Embedding-based Models: All these models learn embeddings of entities and relations and define an associated scoring function to assess the validity of a temporal fact. Earlier methods incorporated time within entity embeddings (Dasgupta et al., 2018; Garc\u00eda-Dur\u00e1n et al., 2018), and used TransE-based scoring (Bordes et al., 2013). Recent methods (Messner et al., 2022; Sadeghian et al., 2021) adapt static KGE models such as RotatE (Sun et al., 2019), BoxE (Abboud et al., 2020) for TKGC task. Other approaches such as TransETAE (Jiang et al., 2016), Timeplex (Jain et al., 2020) explicitly learn time embeddings, and additionally model temporal constraints between pairs of tuples in the TKG. These models are generally less interpretable, due to their complete reliance on latent embeddings. Multi-hop Reasoning Models: Such models exploit neighborhood information of an entity by employing distinct graph neural network architectures (Kipf and Welling, 2017). Models such as TeMP (Wu et al., 2020), RE-NET (Jin et al., 2020), xERTE (Han et al., 2021a), CyGNet (Zhu et al., 2020) exploit self-attention/GRU, RNN, timeaware attention mechanism, and Copy-Generation\n1https://github.com/dair-iitd/NeuSTIP.git\nmodel respectively to integrate time information in a GNN. GNN-based models are generally computationally expensive (Luo et al., 2022), and do not scale well to large datasets.\nNeuro-Symbolic (Rule-based) Models: These models combine neural embeddings with explicit rule learning and inference, getting benefits of both interpretable inference of symbolic models and parameter sharing of neural models. TLogic (Liu et al., 2021) and ALRE-IR (Mei et al., 2022) are NS link-prediction models designed for time-instant TKGs. Probably the closest to our work is a very recent model TILP (Xiong et al., 2023) that performs all possible constrained walks on time interval datasets to learn temporal logic rules and adopts attention mechanism to score each rule. There are two key differences between TILP and NeuSTIP. Firstly, TILP\u2019s rules require expressing temporal relations between all pairs of intervals mentioned in the rule body. This blows up the number of rules, forcing TILP to use only three Allen relations. In contrast, NeuSTIP encodes temporal relations only between pairs of neighboring intervals in a rule path, and uses all 13 Allen relations. Our experiments show that NeuSTIP performs better inferences compared to TILP. Secondly, and more importantly, TILP is developed only for link prediction, whereas NeuSTIP introduces specific score and loss functions for time-interval prediction."
        },
        {
            "heading": "2.1 Time-Interval Prediction",
            "text": "Time prediction in TKG is relatively underexplored. Existing research includes Know-Evolve (Trivedi et al., 2017) and GHNN (Han et al., 2020), which perform time instant prediction by modeling a given TKG fact as a point process. Time-interval prediction is studied in embedding-based models such as Time2Box (Cai et al., 2021) and Timeplex (Jain et al., 2020) models \u2013 they develop novel TKGE-based scoring functions for the task. To the best of our understanding, no NS TKGC method exists for time-interval prediction, a gap that NeuSTIP aims to fill."
        },
        {
            "heading": "3 Preliminaries",
            "text": "A knowledge graph maintains a set of entities E , and relations R, and each fact r(s, o) is denoted by a directed edge from subject entity s to object entity o, with the label r. A temporal KG (TKG) K additionally maintains the time domain, which is suitably discretized into a set T of discrete time\ninstants, with two special elements tmin and tmax denoting the minimum and maximum time instants attainable in the TKG. A temporal fact is r(s, o, T ), where T = [tb, te] denotes the time interval during which the relation was true (tb, te \u2208 T ). It is represented in the TKG as an edge s\n(r, T )\u2212\u2212\u2212\u2192 o between entities s and o (see Figure 1 (left)). For ease of model design, a common pre-processing step increments the TKG by adding inverse relations (and therefore inverse edges) to the graph (Jain et al., 2020). Any TKG Completion model is evaluated using (1) link prediction queries: tail prediction, r(s, ?, T ) and head prediction, r\u22121(o, ?, T ), and (2) time interval prediction queries: r(s, o, ?)."
        },
        {
            "heading": "3.1 Background on Allen Algebra",
            "text": "Our model is based on Allen\u2019s interval calculus (Allen, 1983), which is a formal system to represent relations between time intervals. It defines 13 exhaustive and pairwise-disjoint relation types. E.g., given two time intervals T1 = [t1b, t1e] and T2 = [t2b, t2e], the Allen relation overlaps(T1, T2) holds if t1b < t2b < t1e < t2e (See Appendix A for more details). To avoid ambiguity, we refer to relations between time intervals as Allen relations and relations r \u2208 R between entities as KG relations. Our model uses all 13 Allen relations in its rule language."
        },
        {
            "heading": "4 The Proposed NeuSTIP Model",
            "text": "We first describe NeuSTIP\u2019s rule language and its algorithm for mining rules. We then discuss its model for scoring a candidate answer, for both link and time interval prediction queries (See Figure 2). Further, we describe the loss function and inference procedures."
        },
        {
            "heading": "4.1 Mining of Temporal Rules",
            "text": "NeuSTIP learns first-order logic rules of the form:\nrh(e1, em+1, T1) \u2190\u2212 \u2227mi=1 ( ai(Ti, Ti+1)\n\u2227ri(ei, ei+1, Ti+1) )\n(1)\nwhere ai and ri denote the Allen relations and KG relations respectively, and ei and Ti are variables that will ground in the entity set E and time interval set T \u00d7 T ; m is the rule length. The rule body can be seen as a path from e1 to em+1, where each KG relation ri shares the object entity with the subject entity of relation ri+1, 1 \u2264 i < m. Similarly, Allen relations ai specify relations between two consecutive time intervals, with the first Allen predicate a1(T1, T2) of the rule body being a relation between the time interval in the rule head and the first time interval mentioned in the body. Refer to Appendix B for examples of temporal rules.\nNeuSTIP mines temporal rules by, for each known fact rh(s, o, T ) \u2208 K, finding ground paths in TKG from s to o of length up to a max limit. For instance, as shown in the center of Figure 1, for the fact wBi(David, London, T6), all paths from David to London are found. Each ground path gets converted to exactly one first-order rule (by replacing entities and time intervals with variables), based on the specific KG relations in the path, and the specific Allen relations between each pair of time intervals (as shown in Figure 1). We highlight that our rule extraction process does not involve sampling walks, rather we use all walks \u2013 a departure from existing non-temporal rule mining approaches (Qu et al., 2021). We choose this because our preliminary analysis suggested that TKGs are sparser than non-temporal KGs, hence, sampling or random walks may miss important rules."
        },
        {
            "heading": "4.2 Scoring Candidate Answers",
            "text": "NeuSTIP first finds candidate answers c and then scores each answer. For a link prediction query\nrh(s, ?, T ), finding candidates is straightforward: find all relevant rules, identify each rule\u2019s groundings, and mark groundings of variable em+1 as candidate answer entities. However, for interval prediction rh(s, o, ?), it grounds all the relevant rules, by ignoring the first Allen relation a1(T1, T2). This is because it cannot be a priori ground, since T1 is unknown (the intent is to predict it). It then identifies all time intervals T1, which are consistent with the predicate a1(T1, T2), and outputs those as candidate answers. For example, if a1 = before and T2 = [1990, 2000], then it will output intervals [tb, te] such that tb < te < 1990.\nFor interval prediction queries, NeuSTIP scores start and end time instants separately. Its scoring function is motivated by RNNLogic (Qu et al., 2021), and extended to temporal setting: the score of a candidate c \u2208 {o, tb, te} is computed as\nscore(c) = \u2211 Lj\u2208L\nscore(c, Lj) =\u2211 Lj\u2208L \u2211 path\u2208P \u03c8(Lj)[c] \u00b7 \u03d5(path)[c] (2)\nHere, L is the set of all first-order rules Lj that generate answer c, and P is the set of ground paths of\nLj consistent with c. Note that a candidate answer c can be arrived at by firing multiple rules in the rule set L. Further, a given rule Lj \u2208 L can be grounded to different paths which when followed in TKG arrive at candidate c. Our scoring function (in Equation 2) takes into account the significance of all such rules and the corresponding paths that lead to the generation of candidate c by aggregating rule score \u03c8(Lj)[\u00b7] and path score \u03d5(path)[\u00b7]. Since c is of three types, the score of each rule Lj has three components \u03c8(Lj)[o], \u03c8(Lj)[tb] and \u03c8(Lj)[te] used to score object entities, start time instants and end time instants, respectively. Similarly, the path scores are also specified using three components. We now explain NeuSTIP model that computes the rule score \u03c8(Lj)[\u00b7] and path score \u03d5(path)[\u00b7]."
        },
        {
            "heading": "4.2.1 Rule Score",
            "text": "NeuSTIP uses a novel neuro-symbolic rule scoring function (Equation 4) \u2013 it combines a rule embedding based score \u03c8emb and a statistical measurebased score \u03c8stat to compute \u03c8.\nEmbedding based rule score: NeuSTIP learns three embeddings for a KG rule head relation r:\nrO, rB , and rE , used for scoring for object entity, start time instants, and end time instant candidates, respectively. It also computes an embedding Lj for each rule Lj (motivated by Mei et al. (2022)) by passing the sequence of Allen and KG relations in Lj\u2019s rule body through a Gated Recurrent Unit (GRU) (Cho et al., 2014). For a candidate answer c, depending upon its type (denoted as object: O, start time: B, end time: E), the rule score of Lj is computed as the cosine similarity between the rule embedding Lj and the relevant rule head embedding r\u2022h (\u2208 {rOh , rBh , rEh }),\n\u03c8emb(Lj)[c] = sim(Lj, r \u2022 h) (3)\nFurther details on GRU model are in Appendix C.\nStatistical measure based score: NeuSTIP leverages PCA score for each rule as an additional measure of its accuracy. This symbolic rule confidence metric acts as a prior of the rules and helps in informed initialization of the rule score. PCA score estimates the fraction of entities predicted by the rule that are known to be true at training time (Gal\u00e1rraga et al., 2013). See Appendix D for more information. We denote it by \u03c8stat(Lj)[c], based on which type of entity being predicted by the rule.\nNeuSTIP computes the aggregate rule score as:\n\u03c8(Lj)[c] = \u03c8emb(Lj)[c] \u2217 \u03c8stat(Lj)[c] (4)"
        },
        {
            "heading": "4.2.2 Path Score",
            "text": "For link prediction, the model exploits a relatively simple approach and sets \u03d5(path)[o] = 1 for each path that reaches the target answer o. This implies that the score of candidate entities is dependent on the total number of groundings and the quality of the corresponding rules. A low-quality rule with a less number of groundings cannot generate a highscoring candidate entity.\nFor time prediction, a rule can only be partially ground. Notice the first Allen relation a1 specifies the Allen relation between the time interval T1 of rh and T2 of r1 (See Equation 1). Since T1 is what we intend to predict, thus condition a1(T1, T2) cannot be ground. However, Allen relation A1 and time interval T2 guide NeuSTIP (discussed in Section 4.2) to find its candidate start time t1b and end time t1e of time interval T1. For scoring the candidates, NeuSTIP computes two Gaussian distributions, N (\u00b5Brr\u2032 , \u03c3Brr\u2032) and N (\u00b5Err\u2032 , \u03c3Err\u2032) \u2013 they represent the start/end time difference of pairs of KG relations (r, r\u2032) with the same subject entity.\nNotice from Equation 1, relations rh and r1 will capture two facts about the same subject e1. Therefore, given the fact r1(e1, e2, [t2b, t2e]), NeuSTIP calculates path score for start time candidate t1b in query rh(e1, em+1, ?) as,\n\u03d5(path)[t1b] = N (t1b \u2212 t2b|\u00b5Brhr1 , \u03c3 B rhr1 ) (5)\nThe same approach is used to estimate \u03d5(path)[t1e]."
        },
        {
            "heading": "4.3 Loss Function",
            "text": "NeuSTIP is trained by minimizing two loss functions LLP and LTP for link prediction and interval prediction, respectively. The score of each candidate object entity score(o) is normalized to P (o) by computing softmax over the entity set E . Similarly, the scores of start time and end time candidates are normalized over the time instant set T . For a given query, we denote the set of known correct answers as D and the set of other answers as N. We further use set S to denote the subset of N with a higher score than that of the gold entity. For link prediction, the loss LLP is computed as\u2211\nn\u2208N P (n) + \u2211 o\u2208D (\u2211 e\u2208S(P (e)\u2212 P (o)) |S| ) (6)\nSimilarly, for time interval prediction let D denote the set of known true intervals. We construct two sets Db and De with the start time instants and end time instants of intervals in D. Further, Nb = T \\Db and Ne =T \\De. Because time instants tb and te are numerical, NeuSTIP uses a loss function that captures differences between true and other times via LTP as follows:\u2211\ntb\u2208Db \u2211 nb\u2208Nb ( P (nb)\u2212 P (tb) ) \u2217 d ( nb, tb ) +\n\u2211 te\u2208De \u2211 ne\u2208Ne ( P (ne)\u2212 P (te) ) \u2217 d ( ne, te ) (7)\nwhere d(\u00b7, \u00b7) is the time instant distance function. We define the distance function in Appendix E."
        },
        {
            "heading": "4.4 Inference",
            "text": "At the test time, we perform link prediction by ranking all objects o based on score(o). For timeinterval prediction for query r(s, o, ?), NeuSTIP constructs a T \u00d7 T matrix, whose each entry is P (tb)\u2217P (te)\u2217N (te\u2212tb|\u00b5intvr , \u03c3intvr ) where P (tb) and P (te) are the probabilities defined above, and\nN (\u00b7 |\u00b5intvr , \u03c3intvr ) is the Gaussian distribution over the duration of time-intervals of target relation r in the query. After populating the matrix with data, we consider its upper-triangular matrix T I in order to ensure te \u2265 tb. We return the predicted time interval as [tb, te] = argmaxtb,te(T I)."
        },
        {
            "heading": "4.5 Extended Model with Timeplex",
            "text": "Inspired by RNNLogic, we further ensemble our model with an embedding-based model, TimePlex (Jain et al., 2020) to integrate the complementary features of NeuSTIP, our rule-based model and an embedding-based model as follows:\nens_score(c) = score(c)+ \u03b7 \u2217Timplex(c) (8)\nwhere c \u2208 {o, tb, te} and \u03b7 is a learnable parameter. score(c) is NeuSTIP score (Equation 2)."
        },
        {
            "heading": "5 Experiments",
            "text": ""
        },
        {
            "heading": "5.1 Experimental Setup",
            "text": "Datasets and Metrics: We evaluate the proposed model on two standard time-interval TKGC datasets: WIKIDATA12k and YAGO11k (Dasgupta et al., 2018). For both datasets, we consider the temporal granularity to be 1 year. The dataset statistics are reported in Appendix F. For link prediction, we report the standard metrics of Mean Reciprocal Rank (MRR), Hits@1, and Hits@10 and use time-aware filtered measures in each model (Jain et al., 2020). For time interval prediction, we employ the aeIOU metric (Jain et al., 2020) as:\naeIOU(T ev, T pr) = max{1, vol(T ev \u2229 T pr)}\nvol(T ev \u22d3 T pr)\nsuch that T ev and T pr are gold and predicted time intervals, vol() refers to the size of the time interval (which would be in terms of number of years for these datasets), T ev \u2229 T pr refers to overlap in timeinterval, T ev\u22d3T pr is the smallest single contiguous interval containing T ev and T pr.\nBaselines: Link prediction in TKGC is relatively well-studied. We compare against 10 embedding and rule-based/neuro-symbolic solutions. Specifically, we choose a total of 4 rule based models \u2013 Neural-LP (Yang et al., 2017) and AnyBurl (Meilicke et al., 2019) handling static (non-temporal) KG, and TLogic (Liu et al., 2021) (time-instants) and TILP (Xiong et al., 2023) (time-intervals) working with temporal KGs. In embedding based approaches, we consider ComplEx (Trouillon et al.,\n2016), TA-ComplEx (Garc\u00eda-Dur\u00e1n et al., 2018), HyTE (Dasgupta et al., 2018), DE-SimplE (Goel et al., 2020), TNT-Complex (Lacroix et al., 2020), and TimePlex (Jain et al., 2020). Apart from ComplEx (Trouillon et al., 2016), all the other embedding-based solutions are proposed especially for temporal KGs. The original TimePlex model itself has two versions \u2013 a base model and the full model. The full version adds two temporal consistency gadgets, modeling relation recurrence, and typical duration distributions between two relations. In the same vein, TILP also has two variants, and its full model additionally introduces temporal features such as recurrence, and duration distribution into the model. We compare against both versions of the TimePlex and TILP models.\nFor interval prediction, NeuSTIP is the first neuro-symbolic model \u2013 so there is no existing NS model to directly compare against. We resort to using only embedding-based models for comparisons: HyTE, TNT-Complex, and TimePlex.\nWe report results from two variants of our proposed model, NeuSTIP (base) and NeuSTIP with KGE. The base model is trained exclusively with the proposed temporal rules and corresponding candidate score, as in Equation 2. In NeuSTIP with KGE variant, we integrate the state-of-the-art KG embedding model, Timeplex (full) using an ensemble (see Section 4.5). The specifics of the training procedure of NeuSTIP and hyperparameter settings are reported in Appendix G, H and I. For all comparisons, where possible we report published results since our datasets are standard and exact splits have been used as is in earlier works."
        },
        {
            "heading": "5.2 Results and Observations",
            "text": "Link Prediction: We report the performance of NeuSTIP and other baseline methods in Table 1. We observe that the performance of our NeuSTIP (base) is comparable with the base model of TimePlex for WIKIDATA12k, while the gap is more pronounced on YAGO11k as even our base model outperforms TimePlex (base) on all the three metrics with a gain of over 4 MRR pts. Similarly, our base model is comparable to the base model of the best temporal rule-based model (TILP-base) for WIKIDATA12k whereas our base model outperforms it on all three metrics on the YAGO11k dataset. We also observe that our \u2018NeuSTIP w/ KGE\u2019 model, outperforms the state-of-the-art models on 2 out of 3 metrics on YAGO11k, and on all metrics on WIKIDATA12k.\nTime Interval Prediction Table 2 reports the results of time interval prediction. We observe that the performance of our base model (NeuSTIP (Base)) is better than that of all models on both datasets. Our \u2018NeuSTIP w/ KGE\u2019 model yields further improvements, outperforming the state-ofthe-art Timeplex model on YAGO11k by a strong margin of more than 7 aeIOU pts. Overall, our work establishes a new state of the art for timeinterval prediction in TKGC."
        },
        {
            "heading": "6 Analysis",
            "text": "We investigate several research questions to gain further insights into our model: Q1. What is the effect of each component of NeuSTIP on link and time-interval prediction? Q2. How does our model perform in the inductive learning setting where the train and the test entities are disjoint? Q3. How does NeuSTIP perform when there is limited training data available? Q4. Are the rules generated by our model\nhuman-interpretable? Q5. How important is it to consider all the 13 Allen predicates in our proposed model? Q6. What is the effect of rule length on model performance?\nFor these analysis questions, unless otherwise stated, we use NeuSTIP (base) model \u2013 this allows us to directly assess the impact of design choices made in our model.\nAblation study: In order to understand the incremental contribution of each component (Q1), we conduct an ablation study by removing three components from NeuSTIP: Allen predicates (TR), PCA score (PCA) (Equation 4) and duration distribution (INTV) (Section 4.4). Please note that duration distribution is employed only during time interval prediction, so it will not change the results of link prediction. Likewise, Allen predicates (TR) are critical in predicting potential time intervals in our algorithm (Section 4.2.2) and cannot be absent from the model while doing time prediction.\nTable 3 compares the link prediction performance of NeuSTIP model ablations. We observe that the presence of Allen relation constraints is critical for our model \u2013 it aids performance substantially in both datasets. We hypothesize that without them, low precision rules get too many groundings, which confuse the model. Next, the absence of PCA score (NeuSTIP - PCA) also hurts performance, as PCA acts as informed prior to the rule score.\nTable 4 shows the effect of removing the duration distribution (INTV) and PCA score (PCA) on the time interval prediction of the model. We observe that both components help the model, although the contributions are not huge. See Appendix K for more details on ablation study. Inductive Setting: It is generally believed that NS models perform well in inductive settings. To verify this for NeuSTIP (Q2), we conduct an experiment where the training and testing entities are\ndisjoint. In order to generate the data for this experiment, we randomly select a subset of the temporal facts from the test data. Then, we remove any such temporal facts from the train data, which share a common entity with these test temporal facts. (Appendix L). We perform link and time interval prediction on the newly generated data in Table 5 and 6. We utilize TLogic and Timeplex as baselines for link and time interval prediction respectively. It can be clearly seen that our model outperforms Timeplex by a significant margin on aeIOU, because embedding-based models generally struggle in the inductive setting. Further, NeuSTIP outperforms TLogic on all metrics for link prediction supporting the hypothesis that NeuSTIP can generalize well to new data.\nLimited training data: NS models have another advantage that they can learn in the presence of less amount of training data (Q3). We test this in Figure 3 on WIKIDATA12k dataset where we compare NeuSTIP (base) against Timeplex (base) on test data while training the models at varied training data sizes. We observe that when limited data is available (e.g. 10% data) NeuSTIP\u2019s rules are still able to capture the patterns in the data, while Timeplex struggles to perform well (Appendix M).\nHuman interpretability of logical rules: One advantage of our temporal rule-based model is that the predictions are in a human-interpretable form (Q4), whereas the predictions of embedding-based\nmodels are opaque in nature. Here, we illustrate one real example from the YAGO11k dataset \u2013 it shows the reasoning behind predicting the correct entity/time interval by the rules of NeuSTIP model: Query: (Franz Dahlem, isAffiliatedTo, ?, [1920, 1946]) Correct Answer: Communist Party of Germany The rule that grounds the gold object: isAffiliatedTo(E1, E2, T1) \u2190 During(T1, T2)\u2227 isMarriedTo(E1, E3, T2)\u2227 Contains(T2, T3) \u2227 isAffiliatedTo(E3, E2, T3) The groundings: E1: Franz Dahlem, E2: Communist Party of Germany, E3: Kathe Dahlem, T1: [1920,1946], T2: [1899, 1974], T3: [1920,1946]\nThe above rule provides an explanation of why Franz Dahlem was affiliated to Communist Party of Germany at a given time interval by reasoning that his wife Kathe Dahlem was also affiliated to the party during their marriage. (See Appendix N).\nImportance of All Allen predicates: Recall that TILP is a recent model, which only uses three Allen relations in temporal rules: \u2018before\u2019, \u2018after\u2019 and a new aggregate relation \u2018touching\u2019, which combines all other Allen relation into one. In Q5, we wish to probe the value of considering all 13 Allen relations in NeuSTIP\u2019s rules, and compare it to a NeuSTIP version which uses the exact three relations used in TILP. We call this model NeuSTIP (ITG). The comparisons for link and time prediction of NeuSTIP and NeuSTIP (ITG) are provided in Table 7.\nWe observe that integrating many Allen relations into one has a profound impact on H@1 for both datasets, as the rules lose their preciseness. They become more generic and cause other entities to get many groundings, confusing the model. We further notice that integrating Allen predicates has farreaching consequences on time prediction because Allen predicates play a crucial role in deciding and scoring the start and end time of time intervals in NeuSTIP. On further looking at data, we notice that Allen predicates that yield deterministic start/end points, such as equals, finishes, are quite important for good performance in time interval prediction. They should not be aggregated together, as that makes the scores assigned to start/end instances get distributed over a larger range, which affects the model performance.\nRule Length: We perform experiments to study the effect of rule length on the performance of NeuSTIP (Q6). We define a rule of rule length i as one whose rule body consists of i KG relations and i Allen relations. A rule set of maximum (max.) rule length i consists of all rules that have length from 1 to i in it. For efficiency purposes, we restrict the maximum rule length to 3. Our results for link and time interval prediction are presented in Table 8.\nFrom the table, we conclude that the rules of length 3 perform significantly better than the rules of length 2 because length 2 rules fail to capture the entity chains (A\u2192 D\u2192 C\u2192 B) in cyclic rules such as rh(A, B, T1) \u2190 A1(T1, T2), r(A, D, T2), A2(T2, T3), r\u22121(D, C, T3), A3(T3, T4), r(C, B, T4) in its body which are important while finding the path from the head to the tail of a temporal fact.\nTo analyse the model further, we performed additional experiments on the effect of NeuSTIP performance on different types of temporal relations \u2013 instant relations (duration = 0), short relations (du-\nration \u2264 5), and long relations (duration > 5). We find NeuSTIP outperforms baselines in all but one case (instant relations) reiterating the importance of finding groundings (Appendix P) in a rule-based model. Further details are in Appendix O."
        },
        {
            "heading": "7 Conclusions",
            "text": "In this paper, we develop a novel neuro-symbolic TKGC method that represents the temporal information of TKGs in a temporal rule language it defines, and uses rule groundings to ascribe a confidence score to a candidate answer, which can be an entity or a time interval. For that, it defines a novel scoring function. And, it also uses novel loss formulation for training. The key novelty of the proposed formulation is that it can perform both link prediction and time interval predictions in the neurosymbolic setting. Compared to previous methods, our model has made substantial improvements in both link prediction and time interval prediction performance over two benchmark datasets. Furthermore, we show that our model is generalizable to new entities, is human-interpretable, and works well with limited data while answering a given link prediction and time prediction query. We release our code 2 and datasets for further research.\nIn the future, we will explore combinations of NeuSTIP with other orthogonal paradigms such as models that learn box embeddings instead of vector embeddings (Cai et al., 2021), and models based on graph neural network (Zhu et al., 2021). Exploring different methods of ensembling our rule-based and embedding-based models (apart from linear ensembling) can be another future work direction."
        },
        {
            "heading": "Acknowledgements",
            "text": "This work is supported by IBM AI Horizons Network, grants from Google, Verisk, and Huawei, and the Jai Gupta chair fellowship by IIT Delhi. We also acknowledge travel support from the Professional Development Allowance (PDA) fund of IIT Delhi. We thank the IIT Delhi HPC facility for its computational resources. We sincerely thank Ananjan Nandi for useful discussions during the course of the research.\nLimitations\nOne limitation of our model, like rule-based models in general, is that they cannot directly capture the\n2https://github.com/dair-iitd/NeuSTIP.git\nnumeric features present in the data. Although we exploited the gadgets subsumed in Timeplex (Jain et al., 2020) to capture numeric features present in TKGs, capturing these features directly inside our model is an interesting future direction. Further, our proposed model currently deals with closed path rules only and could benefit more if open paths can be exploited for time prediction.\nEthics Statement\nWe anticipate no substantial ethical issues arising due to our work on link prediction and time interval prediction for Neuro-Symbolic TKGC."
        },
        {
            "heading": "A Allen\u2019s Interval Calculus",
            "text": "The temporal facts considered in our setting encode time intervals in the last argument, requiring the use of Allen relations as a formal technique that captures the temporal relations between the time intervals present in the data. As discussed in Section 3.1, we utilize all the 13 Allen relations possible between any two time intervals in our temporal rules. In this section, we describe all 13 Allen relations in Figure 4 in detail. Each of the relations in Allen algebra calculus can be written as set of rules. For example if we have one time interval X = [Xstart, Xend] and another time interval Y = [Ystart, Yend] then the Allen relation before exists between them, i.e. before(X, Y) iff Xstart < Xend < Ystart < Yend. Similarly, the constraints for each of the 13 Allen relations is defined in the last column named \u2018Chronological Sequence\u2019 of Figure 4."
        },
        {
            "heading": "B The Details and an Example of Temporal Logic Rule Extraction",
            "text": "We elaborate on temporal logic rule extraction defined in Section 4.1 along with an example. In order to carry out all-walks, it expresses TKG as graph GAW wherein each quadruple is expressed as s\n(r, T)\u2212\u2212\u2212\u2192 o denoting an edge (r, T) between entities s and o (see Figure 1 (left)). NeuSTIP mines all walks over GAW in three steps: (a) First, beginning at s, it performs time-agnostic walks of length m on GAW such that the final node of the walk is o (Figure 1 (middle)). It then expresses these walks in logical form. At this stage, these all walks exclusively consist of KG relations. (b) Next, it introduces Allen relations into the all-walks captured thus far, in order to bind the time intervals existing between neighboring KG relations in the walk. Further, a special Allen relation is introduced to bind the time interval of target temporal fact and the first KG relation in the walk. (c) At the final step, it substitutes the constants with variables to generalize grounded rule into a final rule (Fig. 1(right)).\nOur example is based upon a fragment of the YAGO11k dataset which is shown in Fig-\nure 1. In this said example, pF, pF\u22121, wBi, iMT denote abbreviations for the relations playsFor, playFor\u22121, wasBornIn, isMarriedTo respectively. The relations A1 to A5 represent Allen relations. In order to learn a rule that is based upon target temporal fact wBi(David, London, T6) the model would first obtain a walk David (iMt,T4)\u2212\u2212\u2212\u2212\u2212\u2192 Victoria (wBi,T5)\u2212\u2212\u2212\u2212\u2212\u2212\u2192 London on GAW . This walk would further be expressed in the logical form as iMt(David, Victoria, T4)\u2227 wBi(Victoria, London, T5).\nIn the next step (b), when Allen relations are introduced into the walk, the corresponding example would be expressed as A4(T6, T4) \u2227 iMt(David, Victoria, T4) \u2227 A5(T4 , T5) \u2227 wBi(Victoria, London, T5). Please note that these Allen relations denote one of the 13 relations in Allen Algebra calculus. The final rule after introducing variables is wBi(A, B, C)\u2190 A4(C, F) \u2227 iMt(A, D, F) \u2227 A5(F, G) \u2227 wBi(D, B, G). This rule is expressed without the entity and time interval variables in Figure 1(right) as wBi\u2190 A4 \u2227 iMt \u2227 A5 \u2227 wBi."
        },
        {
            "heading": "C GRU for Embedding-based Rule Score",
            "text": "In this section, we provide the details of GRU utilized in Section 4.2.1. NeuSTIP learns a unique embedding Lj representation for the body \u2227mt=1At\u2227rt of a given rule Lj . Motivated by ARLE-IR model (Mei et al., 2022), we employ Gated Recurrent Unit (GRU) model (Cho et al., 2014) to learn Lj embedding. At time t, the input of the form xt = [At; rt] is fed to GRU where At and rt are the embedding vectors of t-th Allen predicate At and KG relation rt in a rule Lj\u2019s body. GRU unit utilizes the following functions in order to generate the hidden-layer embedding ht at time t:\nrt = \u03c3 ( Wr \u00b7 xt + Ur \u00b7 ht\u22121 + br ) (9)\nzt = \u03c3 ( Wz \u00b7 xt + Uz \u00b7 ht\u22121 + bz ) (10)\nnt = tanh ( Wn \u00b7 xt + r\u2299 ht\u22121 + bn ) (11) ht = (1\u2212 zt)\u2299 nt + zt \u2299 ht\u22121 (12)\nwhere rt is the reset gate that allows the hidden state to discard information that is insignificant in the future and zt is the update gate that controls how much information from ht\u22121 is carried over to ht. The final hidden state embedding hm after m sequential steps of GRU represents the path embedding Lj of a given rule Lj ."
        },
        {
            "heading": "D PCA Score Metric for Temporal Data",
            "text": "Here we explain PCA score utilized in Section 4.2.1. PCA metric (Gal\u00e1rraga et al., 2013) is based on the Partial Closed World assumption according to which if we know one object o for a given s and T in a temporal fact (s, rh, o, T) then we know all the o\u2032 for that s and T. If we consider temporal rule Lj to be B \u21d2 rh(s, o, T), the PCA score of this rule for link prediction, \u03c8stat(Lj)[o], is:\n#(s, o, T) : |N(s, B, o, T)| > 0 \u2227 rh(s, o, T) \u2208 P #(s, o, T) : |N(s, B, o, T)| > 0 \u2227 \u2203o\u2032 : rh(s, o\u2032, T) \u2208 P\nHere, N(s, B, o, T) denotes the path in the body B of the rule Lj . This implies that we divide the number of positive examples P satisfied by the rule by the total number of (s, o, T) satisfied by the rule such that rh(s, o\u2032, T) is a positive example for some o\u2032. Similarly, we define the PCA score for start time instance tb, \u03c8stat(Lj)[tb], as\n#(s, o, tb) : |Ntb| > 0 \u2227 \u2203te \u2208 T, rh(s, o, [tb, te]) \u2208 P #(s, o, tb) : |Ntb| > 0 \u2227 \u2203T\u2032 : rh(s, o, T\u2032) \u2208 P\n|Ntb| is a notation for |N(s, B, o, tb)|. This implies that we divide the number of positive examples P satisfied by the rule by the total number of (s, o, T) satisfied by the rule such that rh(s, o, T\u2032) is a positive example for some T\u2032."
        },
        {
            "heading": "E Distance Computation between Time Instances",
            "text": "In order to find the distance d between two time instances ta and tb i.e. d ( ta \u2212 tb ) , in Section\n4.3 the model sorts the years in T in increasing order and assign a unique id to each of them. The difference d is then taken between those ids. The difference is then divided by the maximum difference between any two ids, in order to follow the constraint that 0 \u2264 d (.) \u2264 1."
        },
        {
            "heading": "F Data Statistics",
            "text": "The details of the datasets used for experimentation in Section 5 are provided in Table 9. We utilize two standard TKG datasets - YAGO11k and WIKIDATA12k for our experimentation. Both these datasets are time interval-based datasets. We utilize the standard train, valid and test splits for these datasets in our experiments."
        },
        {
            "heading": "G Experimental Details for NeuSTIP",
            "text": "For all the results reported for the proposed model in Table 1 and 2, we optimize parameters of the loss function defined in Section 4.3 with an Adam optimizer (Kingma and Ba, 2015) while decreasing\nthe learning rate by Cosine Annealing ensuring that the minimum learning rate at any time during the training does not fall below the one-fifth of its initial value. To get the best results for link prediction, we train our model for 5000 epochs. Likewise, for time interval prediction, we train the model for 2000 epochs. We set a dimensionality for all the Allen relation embeddings, KG relation embeddings, and the rule head embeddings (which have the same dimension as the hidden dimension of the GRU) to be 32. Besides, we set the maximum rule length as 3 for both datasets.\nFurther, \u03b7 in Equation 8 is a learnable parameter that is trained along with the rule score \u03c8(Lj)[c] in Equation 4. The initial value of \u03b7 is chosen as a hyperparameter and is tuned over the dev set selecting the best value of MRR for link prediction and aeIOU for time prediction.\nDuring the training of the model, we select the best validation model for link prediction based on the MRR metric and the best validation model for time interval prediction based on the aeIOU metric. Further details of the hyperparameters adopted for all the experiments are provided in Appendix H and more details about restrictions on the ruleset are explained in Appendix I."
        },
        {
            "heading": "H Hyper-Parameter Settings for NeuSTIP",
            "text": "For both link and time interval prediction in Table 1 and 2, we set our learning rate to be 1e\u22123 for both datasets. Table 10 lists the values of the coefficient \u03b7 (in Equation 8) which we multiply to the overall KGE score while ensembling with our rule-based model for the two datasets. For link prediction, we consider \u03b7 from the set {0, 1e-3, 1e-2, 1}, and select the best value of \u03b7 based on MRR on the dev set. For time interval prediction, we consider \u03b7 from the set {0, 1e-3, 1e-2, 1e-1, 1} and determine the best value of \u03b7 based on aeIOU on the validation set."
        },
        {
            "heading": "I More Implementation Details",
            "text": "During the process of rule extraction in Section (4.1), given the temporal fact rh(s, o, T) in the\nrule head, we disallow this temporal fact to reoccur in the body of the rule to avoid mining cyclic rules. Further, there are some cases in time prediction where we have 0 groundings for all the instances with respect to both start and end scores. In such cases, for a given relation r, we predict t_start as mean_start[r], and t_end as mean_start[r] + mean_offset[r]. Here, mean_start[r] and mean_offset[r] are the average start and the average offset of intervals for the relation r, computed in terms of the assigned ids (as explained in Appendix E).\nWhile computing \u03d5(path) (Eqn. 5) for the start and end time instances in time interval prediction, there are also cases when rh = r1 (see Eqn 1). For such cases, the Gaussian distributions are computed for the difference between successive occurrences of start/end time for a fixed (s, r). In such cases, given the fact r1(e1, e2, [t2b, t2e]), NeuSTIP calculates path score for start time candidate t1b in query rh(e1, em+1, ?) as,\n\u03d5(path)[t1b] = N (abs(t1b \u2212 t2b)|\u00b5Brhr1 , \u03c3 B rhr1\n) (13)\nHere, abs represents absolute value. The same approach is used to estimate \u03d5(path)[t1e].\nAdditionally, the loss function for Time Interval prediction (see Section 4.3) is normalized for each positive start instance by dividing it by\u2211\nnb\u2208Nb d(nb, tb) for a given tb, and similarly for each positive end instance."
        },
        {
            "heading": "J Statistical Richness of the Rules",
            "text": "We compute metrics such as the number of rules, and the average number of groundings per rule as a measure for computing the statistical richness of the rules obtained in our model. The proposed methodology of performing all walks in NeuSTIP generated 8186 rules for YAGO11k and 31,807 rules for WIKIDATA12k dataset. The reason for obtaining a moderate number of rules while we consider all walks on TKGC is the sparsity of temporal KGCs as already discussed in Section 4.1. Further, the average number of groundings per rule considering the train set is 17.87 for YAGO11k, and 885.21 for WIKIDATA12k, which is sufficiently high."
        },
        {
            "heading": "K Link Prediction for Ablation Study",
            "text": "This section provides the details of the experimental setup for the ablation study on link prediction\nperformed in Table 3 in Section 6 (Q1). In order to perform link prediction for the models: NeuSTIP, NeuSTIP- TR and NeuSTIP-PCA, we utilize the same initial value of hyperparameters as reported in Table 10 in Appendix H. To train the model with rules that are without the Allen relations (NeuSTIPTR), we essentially treat the absence of Allen relations as a special \u2018NOREL\u2019 constraint, and we feed the embedding of this constraint as input xt (Appendix C) to the GRU at every step instead of giving the Allen relation embeddings as input."
        },
        {
            "heading": "L Experimental Setup for Inductive Learning Study",
            "text": "This section explains the experimental setup for the inductive learning study (Q2) performed in Section 6. Our experimental setup is motivated by a similar study performed in DRUM model (Sadeghian et al., 2019).\nData generation: In order to generate our inductive test data, we randomly select a subset of the temporal facts from the standard test data. In our experiments, we chose 50% of the standard test data. We train our model on one set of inductive training data that helps the model in learning rules along with their confidence. It is to be noticed that if we utilize the inductive training data as background knowledge at the test time, then because of the disjoint set of entities at the train and the test time, we would not be able to obtain any groundings of the rules. To overcome this, we employ the standard train data and validation data as the background knowledge at the test time in order to ground the body of the rules. The key point here is that the rules along with their confidences that are learned on one set of entities can be applied to a different set of entities as well since the rules are generalizable and are not bound to fixed entities.\nWe compared NeuSTIP with TLOGIC (Liu et al., 2021) which addresses the problem of link forecasting on temporal knowledge graphs. Unlike our solution, TLOGIC works with the time-instant dataset. Therefore, for a comparable setup, we used our datasets only while treating the start time instant of the interval as the timestamp of the respective fact. We used the publicly available 3 TLOGIC implementation and ran it using the same training data and background knowledge as that for NeuSTIP. The ablation study of TLOGIC sug-\n3https://github.com/liu-yushan/TLogic gested that the higher value of the hyperparameters,\nlike the number of random walks and time window size, leads to better model performance. Therefore, for both datasets, we increased the number of random walks to 20K from its default value of 200 and set the time window size to 1000. Similar to NeuSTIP, we generated rules of lengths 1,2, and 3 using the exponential transition distribution of TLOGIC that performs better than the uniform distribution."
        },
        {
            "heading": "M Limited Data Study",
            "text": "This section is complementary to the limited data study conducted in Section 6 (Q3) as we perform a limited data study for the YAGO11k dataset here. We consider different percentages of the original data (10% to 100%) and train NeuSTIP (base) on this data and test the model using the standard test data. We compare our model against the Timeplex (base) model in Figure 5. As can be seen, in limited training data areas (10% of total training data), the gap between the time-prediction performance of our model and Timeplex becomes quite significant. While embedding-based models are expected to not perform well in the data-scarce scenario, the strategy of NeuSTIP to handle the cases where no relevant rule is grounded helps in improving the model performance significantly. Such a scenario is often encountered in the limited data setting. To this end, when no rule is grounded for query r(s, o, ?), NeuSTIP uses a priori knowledge of the mean of start time instants tavgb and the average duration (intv) of relation r and to predict the time interval as [tavgb , t avg b + intv]."
        },
        {
            "heading": "N Human interpretability of Rules",
            "text": "This section provides more examples of the interpretability feature (Q4) of NeuSTIP put forward in\nSection 6. Next, we provide another example of an interpretable rule from the YAGO11k dataset for time prediction as below.\nQuery: (Donna Hanover, isMarriedTo, Rudy Giuliani, ?) Correct Answer: [1984, 2002] Rule grounding gold start and gold end: isMarriedTo(E1, E2, T1) \u2190 Equals(T1, T2) \u2227 isMarriedTo\u22121(E1, E2, T2) The Grounding: E1: Donna Hanover, E2: Rudy Giuliani, T2: [1984,2002]\nThe above rule provides the temporal information that since Rudy was married to Donna for a given time interval, Donna was also married to Rudy for the exact same time interval due to the symmetric nature of the isMarriedTo relation.\nNext, we present two more examples from WIKIDATA12k dataset on how the rules of NeuSTIP provide human interpretable predictions. We begin by considering an example of link prediction in WIKIDATA12k dataset.\nQuery: (Ammerschwihr, liate, ?, [1920, present]) Correct Answer: Haut-Rhin The rule that grounds the gold object: liate(E1, E2, T1) \u2190 MetBy(T1, T2) \u2227 liate (E1, E3, T2) \u2227 Equals(T2, T3) \u2227 liate\u22121(E3, E4 , T3) \u2227 Meets(T3, T4) \u2227 liate(E4, E2, T4) Groundings: E1: Ammerschwihr, E2: HautRhin, E3: Upper Alsace, E4: Soultzmatt, T1 : [1920, present], T2: [1871, 1920], T3: [1871, 1920], T4: [1920, present]\nHere, the abbreviation liate in WIKIDATA12k stands for located in the administrative territorial entity. The above rule provides an explanation of why Ammerschwihr is located in the administrative entity of Haut-Rhin since 1920, by reasoning that Ammerschwihr and Soultzmatt were both a part of the entity Upper Alsace for the same time interval just before 1920, and Soultzmatt became a part of Haut-Rhin in year 1920. The next example explains time prediction in WIKIDATA12k by NeuSTIP ruleset.\nQuery: (Turku, country, Russian Empire, ?) Correct Answer: [1809,1917] Rule that grounds the gold start and gold end: country(E1, E2, T1)\u2190 Meets(T1, T2)\u2227 countr y(E1, E3, T2) \u2227 Equals(T2, T3) \u2227 country\u22121( E3, E4, T3) \u2227 MetBy(T3, T4) \u2227 country(E4, E2 , T4) Rule Grounding: E1: Turku, E2: Russian\nEmpire, E3: Finland, E4: Mikkeli Province, T2: [1917,present], T3: [1917,present], T4: [0,1917]\nThe above rule explains that Turku is a part of the country Finland since 1917, and another place Mikkeli Province is also a part of Finland since 1917. Just before this, Mikkeli Province was a part of Russian Empire, so the rule provides us the temporal information that Turku was also a part of Russian Empire just before being a part of Finland."
        },
        {
            "heading": "O Time Interval Prediction for Distinct Relation Classes",
            "text": "The goal of this experiment is to observe the performance of the proposed model for different classes of relations present in the TKG. Motivated by Timeplex model (Jain et al., 2020), we categorize the relations into three classes: Instant, Short and Long. Instant relations are those whose start time and the end time coincide in a given time interval. Each relation in the Short category has an average time duration of less than five years. Each relation in the Long category has an average time duration greater than five years. A categorywise detail of relations in YAGO11k and WIKIDATA12k datasets is shown in Table 11.\nThe detailed results of NeuSTIP (base), NeuSTIP w/ KGE, and Timeplex across different relation classes are presented in Table 12. As we observe NeuSTIP (base) considerably outperforms Timeplex in five out of six cases. This supports the hypothesis that our proposed model captured the patterns present in different relation classes by its learned rules. Further, NeuSTIP w/ KGE consistently outperforms the Timeplex model. Please note that the reason for the performance degradation of NeuSTIP (base) for Instant category in YAGO11K is that there are no groundings available\nfor many temporal facts lying in this category. Accordingly, the explanation for the high performance in Long category of YAGO11k is that the model discovers perfect rules for a considerable amount of temporal facts which results in a performance boost. For instance, for the relation isMarriedTo that belongs to the Long category, the model discovers the inverse of isMarriedTo in the rule set and yields better performance by exploiting the symmetric nature of this relation. The other Long relations where we perform better than TimePlex are hasWonPrize and isAffiliatedTo. In these relations, there are a substantial number of temporal facts for which closed walk groundings exist, and hence the target start time and end time are often grounded in these cases, leading to better performance."
        },
        {
            "heading": "P Effect of Groundings on the Model Performance",
            "text": "This empirical study is motivated by the question: how critical are the rule groundings in order to achieve superior performance in the proposed model? In order to conduct this study, we train the NeuSTIP (base) in a typical setting. However, at the test time, we categorize our test temporal facts into two categories: GR and NGR. For a link prediction query (s,r,?,T), GR encodes those temporal facts for which the true tail answer \u2018o\u2019 is reached by following the grounded path of at least one rule\u2019s body while beginning the path at the temporal fact head \u2018s\u2019, otherwise the temporal fact is assigned to NGR category. Likewise, for a time prediction query r(s,o,?), GR encodes those temporal facts for which the true start time \u2018tb\u2019 and the true end time \u2018te\u2019 are grounded by some rule. Specifically, if we\nobtain a positive score in Equation 5 for the true start time and true end time, then we will assign the temporal fact to GR, otherwise we assign it to NGR. Here, of course, the path also has to be from the head \u2018s\u2019 to the tail \u2018o\u2019 in the rule body. The effect of rule groundings on link prediction and time interval prediction are presented in Table 13.\nAs can be observed from the table, the performance of the model gets disappointingly low when the model can not discover the groundings of a target tail in a given rule set (NGR class). Based on this study, we conclude that the performance of the model critically depends upon whether a path exists from the head to tail in a given TKG in order to generate non-zero scores for a temporal fact. This is typical behavior of NS-KGC models and similar behavior has been observed in the past for static NSKGC models such as RNNLogic (Qu et al., 2021), ExpressGNN (Zhang et al., 2020) models. However, this effect can be alleviated to some extent by rule augmentation techniques recently proposed in Nandi et al. (2023). Contrastingly, the TKGE models would always generate a non-zero score for a given temporal fact because they employ a scoring function that composes the embeddings to generate the resulting score without fail."
        },
        {
            "heading": "Q Effect of the first Allen Relation on Model Performance",
            "text": "It is very difficult to study the impact of Allen relations present in a given rule in our current rule language in general because multiple Allen relations are present in one rule. Further, Allen relations are interleaved between the KG relations to bind the time of two neighboring KG relations in the rule body. However, the first Allen relation a1 present in rule body (Equation 1) has its unique contribution to the score computation, especially in the case of time interval prediction as discussed in the paper. Hence, for our study, we divide Allen predicates into two categories: DET refers to the Allen predicates that fix either the start or the end time of the time interval for e.g.: finishedby, meets, metby, starts, startedby, equals and NONDET refers to the Allen predicates where both ends are free, for example: before, after, overlaps, during, contains, overlappedby.\nWe, then, categorize the rules into two categories based upon whether the first Allen relation of the rule falls into DET or NONDET category while ignoring the rules in the other category and predicting scores in Equation 2 based on rules in a given category. We also consider a third category - OVERALL - that constitutes the results in Table 1 and 2 of the paper that are obtained with all rules considered. The results for the link prediction and time interval prediction in three categories are presented in Table 14.\nResults in DET category are better than NONDET category for link prediction for YAGO11k dataset and the results of NONDET category are better than DET category for link prediction for WIKIDATA12k dataset. For time interval prediction, DET alone can help the model in achieving performance similar to OVERALL model performance for time prediction. Further, for link prediction, using only one of them reduces the per-\nformance as compared to the OVERALL mode. This is because the head Allen relation does not as directly influence the entity reached by the rule during link prediction as compared to the time prediction. The impact on link prediction seems to be governed by the total number of rules, as more rules help in grounding the gold entity because more paths are available to ground them. Because of this, when the rules are divided into DET and NONDET categories, fewer rules are available for link prediction, resulting in a drop in performance. In summary, using both sets of relations finds competitive performance in all settings.\nR Influence of Ensembling on the Proposed Model\nOur goal here is to study the question: when we ensemble NeuSTIP with Timeplex in Table 1, how do they influence each other? In order to answer this, we considered the ranks of NeuSTIP (base), Timeplex and an ensemble (NeuSTIP w/ KGE) individually for each test quadruple obtained during link prediction and performed a comparative study to understand how the KGE models contribute to the performance of NeuSTIP.\nOur major conclusion after analysis is that NeuSTIP mainly fails for the cases when no rule gets fired for a given quadruple for the gold entity resulting in a zero score in Equation 2 in paper. For instance, we found 796 such quadruples (out of a total 4102 test instances) in YAGO11k and 609 quadruples (out of 8124 quadruples in test data) in WIKIDATA12k, where our model generated zero score. For such instances, the average rank of our model was 5327 for YAGO11k, and 6307 for WIKIDATA12k while the average rank of Timeplex for such cases was 1654.51 for YAGO11k and 1420.07 for WIKIDATA12k. Therefore, our model relies on ensembling for the cases when it obtains no groundings for any rules learnt by NeuSTIP for a given quadruple. However, for the cases when the model obtains the groundings, it performs better than the corresponding Timeplex and Ensemble (NeuSTIP w/ KGE) model. For instance, when NeuSTIP obtained its groundings, its average rank is 51.1, while the average rank of Timeplex for such cases is 357.54, and is 169.70 for ensemble for YAGO11k datasets. Similarly, the average rank of NeuSTIP for cases when groundings were found is 38.09 while the average rank of Timeplex for such cases is 103.27, and ensemble for such cases\nis 88.65 for WIKIDATA12k. Hence, we conclude that NeuSTIP relies on ensembling when it finds no grounding but outperforms the ensembling and the Timeplex model when it obtains the groundings for the candidate entity in link prediction for the rules learnt by NeuSTIP.\nS Influence of Ensembling on Contemperaneous Models\nThe significant performance gain of NeuSTIP after ensembling it with Timeplex (a pure neural approach) directed us to study the impact of the ensembling approach on the other baseline models. To that end, we experimented (results in Table 15 below) with the state-of-the-art baseline model TILP (Xiong et al., 2023) ensembled with the Timeplex for different values of eta hyperparameter that influence the weightage of both approaches (see Equation 8). In our preliminary attempt at ensembling, we observed a different trend compared to what we noted in NeuSTIP, i.e., the ensembling did not improve the performance of TILP. This counter-intuitive observation points towards the sophisticated process of ensembling. An in-depth analysis of ensembling methods for leveraging the goodness of symbolic and neural approaches can be an interesting future research direction."
        },
        {
            "heading": "T Future Link Forecasting using NeuSTIP Model",
            "text": "Our goal here is to perform future link forecasting on NeuSTIP (base) such that time instances in train and test sets are chronologically ordered and are disjoint. We now explain the experimental setup followed by the results.\nExperimental Setup of NeuSTIP (base): We first sorted YAGO11k/WIKIDATA12k datasets according to the start time of time intervals, to make them suitable for forecasting ensuring the time in train and test sets are disjoint. Specifically, we list the possible values of the start time for train, validation, and test phases in YAGO11k/WIKIDATA12k datasets in the Table 16. The size of YAGO11k dataset (number of quadruples) after this split (without adding inverses) is: 16,806 (train), 1823 (valid), 1880 (test). Likewise, the size of WIKIDATA12k dataset after this split is : 33,368 (train), 3649 (valid), 3604 (test) without adding inverses. We then conducted our experiment with the following constraints: (a) during the rule generation phase, we ensure that the start time of any interval occurring in the body of the rule is strictly less than that of the quadruple used in the head. Consequently, our model finds fewer rules. (b) Similarly during grounding the rules, while performing parameter learning, we maintain the above condition. (c) While running inference on the test set, this condition is automatically ensured due to the way in which the dataset is split. It is important to note that due to fewer rules and lesser groundings found in this split, our model\u2019s performance also reduces. Results of NeuSTIP (base) for Link Forecasting: we obtained the final results of link forecasting by employing time-aware filtering (Jain et al., 2020) that we have employed in all the experiments in our paper. The results are given in Table 17.\nLink Forecasting for TANGO model: We now explain link forecasting in TANGO model (Han et al., 2021b) which is a past model in literature from which our link forecasting experiment has been motivated. Please note that the results obtained below are not directly comparable to the results of link forecasting in NeuSTIP (base) provided above because NeuSTIP is a time-interval dataset and TANGO is a time-instance dataset, their time-aware filtering techniques are different from each other. We are providing these results only for completeness.\nExperimental Setup of TANGO model: We obtained results of link forecasting for the TANGO model on the YAGO11k/WIKIDATA12k datasets. We further (a) consider the start time of all the quadruples, since the TANGO model is inherently designed for time instance datasets and cannot directly work with time-interval datasets as our model. (b) We utilized the publicly available code4, and ran it on the default set of hyperparameters after pre-processing the datasets in exactly the same way as the model does for ICEWS05-15 dataset.\nBelow, in Table 18, we provide the results of applying the TANGO model for link forecasting\non YAGO11k/WIKIDATA12k datasets for different filtering settings. For the definition of the raw, timeaware, and time-unaware filtering please refer to TANGO model (Han et al., 2021b). NeuSTIP vs TANGO comparison: Lastly, we obtained the results of NeuSTIP (Base) for link forecasting for raw (without filtering) settings so that the results of TANGO and NeuSTIP can be compared directly for raw settings in Table 19.\nAs can be observed, our model outperforms TANGO for link forecasting on both YAGO11k and WIKIDATA12k on MRR and H@10.\n4https://github.com/TemporalKGTeam/TANGO"
        }
    ],
    "title": "NeuSTIP: A Neuro-Symbolic Model for Link and Time Prediction in Temporal Knowledge Graphs",
    "year": 2023
}