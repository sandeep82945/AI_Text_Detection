{
    "abstractText": "Synaesthesia refers to the description of perceptions in one sensory modality through concepts from other modalities. It involves not only a linguistic phenomenon, but also a cognitive phenomenon structuring human thought and action, which makes understanding it challenging. As a means of cognition, synaesthesia is rendered by more than sensory modalities, cue and stimulus can also play an important role in expressing and understanding it. In addition, understanding synaesthesia involves many cognitive efforts, such as identifying the semantic relationship between sensory words and modalities. Therefore, we propose a unified framework focusing on annotating all kinds of synaesthetic elements and fully exploring the relationship among them. In particular, we introduce a new annotation scheme, including sensory modalities as well as their cues and stimuli, which facilitate understanding synaesthetic information collectively. We further design a structure generation model to capture the relations among synaesthetic elements and generate them jointly. Through extensive experiments, the importance of the proposed dataset can be verified by the statistics and progressive performances. In addition, our proposed model yields state-of-theart results, demonstrating its effectiveness.",
    "authors": [
        {
            "affiliations": [],
            "name": "Kun Sheng"
        },
        {
            "affiliations": [],
            "name": "Zhongqing Wang"
        },
        {
            "affiliations": [],
            "name": "Qingqing Zhao"
        },
        {
            "affiliations": [],
            "name": "Xiaotong Jiang"
        },
        {
            "affiliations": [],
            "name": "Guodong Zhou"
        }
    ],
    "id": "SP:091b3c8b658daa28e931b96d27c097fe1de9ff09",
    "references": [
        {
            "authors": [
                "Hongjie Cai",
                "Rui Xia",
                "Jianfei Yu."
            ],
            "title": "Aspect-category-opinion-sentiment quadruple extraction with implicit aspects and opinions",
            "venue": "ACL/IJCNLP 2021, pages 340\u2013350.",
            "year": 2021
        },
        {
            "authors": [
                "Cesare Campagnano",
                "Simone Conia",
                "Roberto Navigli."
            ],
            "title": "SRL4E - semantic role labeling for emotions: A unified evaluation framework",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Pa-",
            "year": 2022
        },
        {
            "authors": [
                "Richard E Cytowic."
            ],
            "title": "The man who tasted shapes: A bizarre medical mystery offers revolutionary insights into emotions",
            "venue": "Reasoning, and Consciousness.",
            "year": 1993
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference",
            "year": 2019
        },
        {
            "authors": [
                "Joseph L. Fleiss."
            ],
            "title": "Measuring nominal scale agreement among many raters",
            "venue": "Psychological Bulletin, 76:378\u2013382.",
            "year": 1971
        },
        {
            "authors": [
                "Daniel Gildea",
                "Daniel Jurafsky."
            ],
            "title": "Automatic labeling of semantic roles",
            "venue": "Comput. Linguistics, 28(3):245\u2013288.",
            "year": 2002
        },
        {
            "authors": [
                "Xiaotong Jiang",
                "Qingqing Zhao",
                "Yunfei Long",
                "Zhongqing Wang."
            ],
            "title": "Chinese synesthesia detection: New dataset and models",
            "venue": "Findings of the Association for Computational Linguistics: ACL 2022, Dublin, Ireland, May 22-27, 2022, pages 3877\u2013",
            "year": 2022
        },
        {
            "authors": [
                "Diederik P. Kingma",
                "Jimmy Ba."
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings.",
            "year": 2015
        },
        {
            "authors": [
                "Mike Lewis",
                "Yinhan Liu",
                "Naman Goyal",
                "Marjan Ghazvininejad",
                "Abdelrahman Mohamed",
                "Omer Levy",
                "Veselin Stoyanov",
                "Luke Zettlemoyer"
            ],
            "title": "BART: denoising sequence-to-sequence pretraining for natural language",
            "year": 2020
        },
        {
            "authors": [
                "Yaojie Lu",
                "Hongyu Lin",
                "Jin Xu",
                "Xianpei Han",
                "Jialong Tang",
                "Annan Li",
                "Le Sun",
                "Meng Liao",
                "Shaoyi Chen."
            ],
            "title": "Text2event: Controllable sequence-tostructure generation for end-to-end event extraction",
            "venue": "ACL/IJCNLP 2021, pages 2795\u20132806.",
            "year": 2021
        },
        {
            "authors": [
                "Christopher D. Manning",
                "Hinrich Sch\u00fctze."
            ],
            "title": "Foundations of Statistical Natural Language Processing",
            "venue": "MIT Press, Cambridge, MA, USA.",
            "year": 1999
        },
        {
            "authors": [
                "Llu\u00eds M\u00e0rquez",
                "Xavier Carreras",
                "Kenneth C. Litkowski",
                "Suzanne Stevenson."
            ],
            "title": "Semantic role labeling: An introduction to the special issue",
            "venue": "Comput. Linguistics, 34(2):145\u2013159.",
            "year": 2008
        },
        {
            "authors": [
                "Paul F. Christiano",
                "Jan Leike",
                "Ryan Lowe."
            ],
            "title": "Training language models to follow instructions with human feedback",
            "venue": "CoRR, abs/2203.02155.",
            "year": 2022
        },
        {
            "authors": [
                "Yanna Popova."
            ],
            "title": "Image schemas and verbal synaesthesia",
            "venue": "From Perception to Meaning, pages 395\u2013420. De Gruyter Mouton.",
            "year": 2008
        },
        {
            "authors": [
                "Alec Radford",
                "Karthik Narasimhan"
            ],
            "title": "Improving language understanding by generative pretraining",
            "year": 2018
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J. Liu."
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "Journal of Machine Learning Research,",
            "year": 2020
        },
        {
            "authors": [
                "Yeshayahu Shen."
            ],
            "title": "Cognitive constraints on poetic figures",
            "venue": "Cognitive Linguistics, 8:33\u201371.",
            "year": 1997
        },
        {
            "authors": [
                "Yeshayahu Shen",
                "Ravid Aisenman."
            ],
            "title": "Heard melodies are sweet, but those unheard are sweeter\u2019: Synaesthetic metaphors and cognition",
            "venue": "Language and Literature, 17(2):107\u2013121.",
            "year": 2008
        },
        {
            "authors": [
                "Francesca Strik Lievers."
            ],
            "title": "Synaesthesia: A corpus-based study of cross-modal directionality",
            "venue": "Functions of Language, 22.",
            "year": 2015
        },
        {
            "authors": [
                "Francesca Strik Lievers",
                "Chu-Ren Huang."
            ],
            "title": "A lexicon of perception for the identification of synaesthetic metaphors in corpora",
            "venue": "Proceedings of the Tenth International Conference on Language Resources and Evaluation LREC 2016, Portoro\u017e,",
            "year": 2016
        },
        {
            "authors": [
                "Francesca Strik Lievers",
                "Ge Xu",
                "Hongzhi Xu."
            ],
            "title": "a methodology for the extraction of lexicalized synaesthesia from corpora",
            "venue": "The 19th international congress of linguists.",
            "year": 2013
        },
        {
            "authors": [
                "Stephen Ullmann."
            ],
            "title": "The principles of semantics",
            "venue": "Oxford: Basil Blackwell.",
            "year": 1957
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N. Gomez",
                "Lukasz Kaiser",
                "Illia Polosukhin."
            ],
            "title": "Attention is all you need",
            "venue": "NIPS 2017, pages 5998\u20136008.",
            "year": 2017
        },
        {
            "authors": [
                "Oriol Vinyals",
                "Lukasz Kaiser",
                "Terry Koo",
                "Slav Petrov",
                "Ilya Sutskever",
                "Geoffrey E. Hinton."
            ],
            "title": "Grammar as a foreign language",
            "venue": "NIPS 2015, pages 2773\u20132781.",
            "year": 2015
        },
        {
            "authors": [
                "Joseph Williams."
            ],
            "title": "Synaesthetic adjectives: A possible law of sematic change",
            "venue": "Language, 52(2):461\u2013478.",
            "year": 1976
        },
        {
            "authors": [
                "Bodo Winter."
            ],
            "title": "Sensory Linguistics: Language, perception and metaphor",
            "venue": "Converging Evidence in Language and Communication Research. John Benjamins.",
            "year": 2019
        },
        {
            "authors": [
                "Qingqing Zhao."
            ],
            "title": "Embodied Conceptualization or Neural Realization: A Corpus-Driven Study of Mandarin Synaesthetic Adjectives",
            "venue": "Frontiers in Chinese Linguistics. Springer Singapore.",
            "year": 2020
        },
        {
            "authors": [
                "Qingqing Zhao",
                "Kathleen Ahrens",
                "Chu-Ren Huang."
            ],
            "title": "Linguistic synesthesia is metaphorical: a lexical-conceptual account",
            "venue": "Cognitive Linguistics, 33(3):553\u2013583.",
            "year": 2022
        },
        {
            "authors": [
                "Qingqing Zhao",
                "Chu-Ren Huang",
                "Kathleen Ahrens."
            ],
            "title": "Directionality of linguistic synesthesia in mandarin: A corpus-based study",
            "venue": "Lingua, 232:102744.",
            "year": 2019
        },
        {
            "authors": [
                "Qingqing Zhao",
                "Chu-Ren Huang",
                "Yunfei Long."
            ],
            "title": "Synaesthesia in Chinese: A corpus-based study on gustatory adjectives in mandarin",
            "venue": "Linguistics, 56(5):1167\u20131194.",
            "year": 2018
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Synaesthesia, based on the Greek roots \u2018syn\u2019 (together) and \u2018aisthesia\u2019 (perception), describes a situation in which perceptions in different sensory modalities are associated in both perceptual experiences and verbal expressions (Cytowic, 1993; Popova, 2008; Shen and Aisenman, 2008). Synaesthesia in verbal expressions is a language usage whereby lexical items in one sensory modality are employed to describe perceptions in another sensory modality (Zhao et al., 2018; Zhao,\n\u2217* Corresponding author\nShe said in a sweet tone, \u201cThat\u2019s what I taught him!\u201d\nCue\nStimulus\nSensory word (Taste Hearing)\nROOT\nOriginal Synaesthetic\nWord Sensory Modality Cue Stimulus Sensory Modality\nstructure generation model\n2020). For instance, as shown in Figure 1, the gustatory adjective \u201csweet\u201d can be used to describe an auditory perception, as in the phrase \u201csaid in a sweet tone\u201d.\nThere are extensive studies on linguistic synaesthesia from various perspectives (Shen, 1997; Winter, 2019; Zhao et al., 2022). Nevertheless, synaesthesia has received little attention in natural language processing. As one of the initial study, Jiang et al. (2022) constructed a humanannotated Chinese synaesthesia dataset, where sensory words, original and synaesthetic sensory modalities are annotated. They further designed a pipeline system to identify the sensory word, and to detect the original and synaesthetic sensory modalities for the sensory word.\nAs a means of cognition and communication, synaesthesia is rendered by more than sensory modalities, cue and stimulus can also contribute to synaesthetic conceptualization. Cue and stimulus are the trigger words or expressions that cause the synaesthetic usages. As shown in Figure 1, since the cue \u201ctone\u201d and the stimulus \u201csaid\u201d are both auditory words, it is easy to find that \u201csweet\u201d is transferred from taste to hearing. In addition, understanding synaesthesia involves many cognitive efforts, such as identifying the semantic re-\nlationship between sensory words and modalities, as well as interpreting sensory modalities with the cue and the stimulus, which might be difficult for computers to deal with.\nIn this study, we propose a unified framework to address the above challenges by annotating all kinds of synaesthetic elements and fully exploring the relationship among them. In particular, we firstly extend the previous dataset by collecting more samples from social media. We then propose a new annotation framework, including sensory modalities as well as their cues and stimuli, which facilitate understanding synaesthesia information and help to improve the performance of automatic synaesthesia comprehension systems. Furthermore, we propose a structure generation model to capture the relations among synaesthetic elements, and generate all elements jointly. Afterwards, we employ a structure acquisition and composition based framework to improve the generation model by capturing the interdependency among synaesthetic elements.\nThe statistics show the potential and usefulness of the new annotation scheme. In addition, the experimental results demonstrate the effectiveness of the proposed model.We release our dataset named CSyna at https://github.com/dinoksaur/csyna."
        },
        {
            "heading": "2 Related Works",
            "text": "Studies on linguistic synaesthesia from a linguistic perspective focus on the directionality patterns and underlying mechanisms for synaesthetic transfers between different modalities. Note that \u201csynaesthesia\u201d and \u201csynesthesia\u201d are used interchangeably in the literature. For consistency, we use \u201csynaesthesia\u201d in this paper. For instance, previous studies (Ullmann, 1957; Williams, 1976; Strik Lievers, 2015; Zhao et al., 2019) found that the transfers of linguistic synaesthesia conformed to certain patterns, rather than mapping randomly. In terms of the mechanisms underlying synaesthetic transfers, Zhao et al. (2018) and Winter (2019) have suggested that linguistic synaesthesia is grounded in multiple mechanisms. In addition, Strik Lievers et al. (2013) and Strik Lievers and Huang (2016) focus on identifying linguistic synaesthetic expressions in natural languages. However, their studies are conducted by semiautomatic methods with lots of manual strategies. There are no comprehensive computational models with automatic synaesthesia detection em-\nployed in previous methods. Recently, Jiang et al. (2022) constructed a human-annotated Chinese synaesthesia dataset. They further proposed a pipeline system to identify sensory words, and to detect the original and synaesthetic sensory modalities. However, the annotated samples in their dataset is limited, and cue and stimulus are missing in their annotation scheme. Therefore, we propose a unified framework focusing on annotating all kinds of synaesthetic elements and fully exploring the relationship among them. In particular, we propose a new annotation scheme, including sensory modalities as well as their cues and stimuli, which are helpful for understanding synaesthesia information. We further employ a structure generation model to fully explore the relations between synaesthetic elements and to generate them jointly."
        },
        {
            "heading": "3 Data Annotation and Analysis",
            "text": "In this section, we first give a new scheme of synaesthesia. We then show the collection and annotation process of the dataset. After that, we give some fundamental statistics and analyses."
        },
        {
            "heading": "3.1 Scheme of Annotation",
            "text": "In this study, we propose a unified annotation scheme, which aims to investigate the capabilities of automatic systems not only to distinguish between sensory modalities, but also to capture their semantic constituents. Following the task of semantic role labeling (Gildea and Jurafsky, 2002), we aims to answer the question as \u201cWho express What sensory modalities, towards Whom and Why?\u201d (M\u00e0rquez et al., 2008; Campagnano et al., 2022). We thus take a subset of semantic roles, namely, cue and stimulus, to identify who or what the participants are in an action or event denoted by a sensory modality. The following is the description of the annotation scheme:\n\u2022 Sensory Word is an adjective that expresses a sensory modality in a sentence.\n\u2022 Original Sensory Modality is the original domain of a sensory word, which is always stationary, and not influenced by the context of the sensory word. The five \u2018Aristotelian\u2019 senses (Strik Lievers, 2015; Winter, 2019; Zhao, 2020), including Touch, Taste, Smell, Vision, and Hearing, are used in this study.\n\u2022 Synaesthetic Sensory Modality is the actual sensory modality of a sensory word, which is influenced by the sensory word and its context.\n\u2022 Cue is a trigger word or an expression modified by a sensory word.\n\u2022 Stimulus is an entity, action or event that causes the synaesthetic sensory modality identified by the cue.\nAs shown in Figure 1, the sensory word \u201csweet\u201d expresses the synaesthetic sensory modality hearing, which is different from its original sensory modality taste. In addition, the synaesthetic sensory modality is caused by its cue \u201ctone\u201d and stimulus \u201csaid\u201d. Therefore, the new scheme gives a comprehensive study of synaesthesia, which can be used to explore what and why synaesthesia would happen in a sentence. Additionally, it facilitates understanding synaesthesia information and helps to improve the performance of automatic synaesthesia comprehension systems."
        },
        {
            "heading": "3.2 Data Collection and Annotation",
            "text": "With the goal of creating a large-scale synaesthesia dataset to support research on understanding synaesthesia, we extend Jiang et al. (2022)\u2019s dataset by collecting more samples from social media. Each sentence is ensured to contain at least one sensory adjective as the candidate sensory sentence. The detail statistic of the dataset can be found in Section 3.3.\nFollowing the new scheme, the annotation of synaesthesia includes the original sensory modality, the synaesthetic sensory modality, the sensory word, the cue and the stimulus. In particular, we invited expert annotators to complete this challenging annotation task, which required a relatively deep understanding of synaesthetic units. The annotator team comprised five annotators who are postgraduate student researchers majoring in computational linguistics with synaesthesia or metaphor study backgrounds. The annotators formed groups of two, plus one extra person. We used cross-validation in annotation process: the two-member groups annotated, and the fifth person intervened if they disagreed.\nAnnotations of synaesthesia with multiple information rely on annotators\u2019 introspection, which might be subjective. Therefore, we added a guideline course, detailed instructions, and many sam-\nples. We also held regular meetings to discuss annotation problems and matters that needed attention. The guidelines changed three times when new problems emerged or good improvement methods were found. The kappa score was used to measure inter-annotator agreements (Fleiss, 1971). The agreement on identification of sensory modality was k = 0.78; detection of cue and stimulus was k = 0.71, which means they are substantially reliable."
        },
        {
            "heading": "3.3 Statistic and Analysis",
            "text": "We firstly give the summary statistics of the dataset in Table 1. There are 24,000 sentences with 236 sensory words in the proposed new dataset, which is much larger than the previous dataset (Jiang et al., 2022). In addition, we also annotate 2,397 cues and 706 stimuli in the dataset. On average, each sentence includes 0.994 cue and 0.304 stimulus. It shows that most of sentences contain at least one cue or stimulus.\nWe then analyze the distribution of original and synaesthetic sensory modalities in Table 2. Among the five sensory modalities, vision and touch are the most frequent original sensory modalities. In addition, although synaesthesia rarely occurs in the auditory modality, the visual modality tends to transfer to the auditory modality. Thus, synaesthetic sentences with the auditory modality have the largest number (The examples can be found in Figure 2).\nFurthermore, we give the transfer probability between the original and synaesthetic modalities,\nalong with examples in Figure 2. Since there are too many combinations, we just show the most frequent original and synaesthetic pairs in the figure. Based on the synaesthesia transfer probability, we find that: tactile adjectives are the most likely to be used for vision, with the transfer probability of 61.5%. The association between vision and hearing is similar to that between touch and vision. Specifically speaking, visual adjectives are the most likely to be associated with hearing. Meanwhile, we also find that the synaesthetic sensory modalities are deeply influenced by the cue and stimulus. For example, although \u201charsh\u201d is a tactile adjective, \u201cvoice\u201d and \u201chear\u201d trigger it to express hearing. In addition, \u201cappearance\u201d makes the sensory modality of \u201cfresh\u201d transferring from taste to vision."
        },
        {
            "heading": "4 Synaesthetic Structure Generation",
            "text": "Given a sentence, we employ a structure generation model to generate all the synaesthetic ele-\nments from a sentence. As shown in Figure 3, we firstly convert all the synaesthetic elements into a tree structure. We then employ a tree generation model to explore the relations among synaesthetic elements and generate them jointly. Afterwards, we utilize a structure acquisition and composition based framework to improve the structure generation model by capturing the interdependency among synaesthetic elements. In this section, we give the description of synaesthetic tree construction and the structure tree generation model, and then discuss the structure acquisition and composition framework in the next section."
        },
        {
            "heading": "4.1 Synaesthetic Tree Construction",
            "text": "As shown in the right side of Figure 3, the synaesthetic tree models a sentence using a rooted directed acyclic graph, highlighting its main elements (e.g. sensory word, cue, stimulus) and relations. Given a sentence, we convert synaesthetic elements into the synaesthetic tree as below:\n\u2022 We create the original and synaesthetic node to represent the original and synaesthetic sensory information respectively, and connect them with a virtual root node;\n\u2022 The sensory word and the original sensory modality are linked to the original node as leaves.\n\u2022 The synaesthetic sensory modality, the cue and the stimulus are linked to the synaesthetic node as leaves.\nTherefore, the tree structure is very important for understanding synaesthesia information and learning the correlations among synaesthetic elements. For instance, the connections between the sensory word and the original sensory modality can be used to identify the sensory word. In addition, the sub-tree of the synaesthetic sensory modality is helpful for detecting the synaesthetic sensory modality based on the cue and the stimulus.\nSince it is much easier to generate a sequence than a tree (Vinyals et al., 2015; Lu et al., 2021), we linearize the synaesthetic tree to the target sequence. As shown in the bottom of Figure 3, we linearize a tree into a token sequence via depthfirst traversal, where \u201c(\u201d and \u201c)\u201d are structure indicators used to represent the structure of linear expressions."
        },
        {
            "heading": "4.2 Synaesthetic Tree Generation",
            "text": "We then employ a text generation model to generate the linearized synaesthetic tree from the given sentence.\nIn this study, we employ a sequence-tosequence model to generate the synaesthetic tree via a transformer-based encoder-decoder architecture (Vaswani et al., 2017). Given the token sequence x = x1, ..., x|x| as input, the sequence-tosequence model outputs the linearized representation y = y1, ..., y|y|. To this end, the sequence-tosequence model first computes the hidden vector representation H = h1, ..., h|x| of the input sentence via a multi-layer transformer encoder:\nH = Encoder(x1, ..., x|x|) (1)\nwhere each layer of Encoder is a transformer block with the multi-head attention mechanism.\nAfter the input token sequence is encoded, the decoder predicts the output tree structure tokenby-token with the sequential input tokens\u2019 hidden\nvectors. At the i-th step of generation, the selfattention decoder predicts the i-th token yi in the linearized form, and decoder state hdi as:\nyi, h d i = Decoder([H;h d 1, ..., h d i\u22121], yi\u22121) (2)\nwhere each layer of Decoder is a transformer block that contains self-attention with decoder state hdi and cross-attention with encoder state H .\nThe generated output structured sequence starts from the start token \u201c\u27e8bos\u27e9\u201d and ends with the end token \u201c\u27e8eos\u27e9\u201d. The conditional probability of the whole output sequence p(y|x) is progressively combined by the probability of each step p(yi|y<i, x):\np(y|x) = |y|\u220f i=1 p(yi|y<i, x) (3)\nwhere y<i = y1...yi\u22121, and p(yi|y<i, x) are the probabilities over target vocabulary V normalized by softmax.\nSince all tokens in linearized representations are also natural language words, we adopt the pretrained language model T5 (Raffel et al., 2020) as our transformer-based encoder-decoder architecture. In this way, the general text generation knowledge can be directly reused."
        },
        {
            "heading": "4.3 Objective Functions and Training",
            "text": "In this subsection, we show the objective functions and the training process of the proposed model.\nThe goal is to maximize the output linearized tree XT probability given the sentence X . Therefore, we optimize the negative log-likelihood loss function:\nL = \u2212 1 |\u03c4 | \u2211 (X,XT )\u2208\u03c4 log p(XT |X; \u03b8) (4)\nwhere \u03b8 is the model parameters, and (X,XT ) is a (sentence, tree) pair in training set \u03c4 , then\nlog p(XT |X; \u03b8) =\n= n\u2211\ni=1\nlog p(xiT |x1T , x2T , ...xi\u22121T , X; \u03b8) (5)\nwhere p(xiT |x1T , x2T , ...x i\u22121 T , X; \u03b8) is calculated by the decoder."
        },
        {
            "heading": "5 Structure Acquisition and Composition",
            "text": "Since the alignment between sentences and synaesthetic elements, and the interdependency among synaesthetic elements are very important for synaesthesia analysis, we then propose a novel strategy to improve the structure generation model by addressing the above issues. As shown in Figure 4, we firstly decompose the original task into several subtasks. Each subtask corresponds to mapping the natural language sentence to one or more synaesthetic elements. Afterwards, we feature a prompt-based learning strategy to separately acquire the structural knowledge of subtasks and employ the learned knowledge to tackle the main task, i.e., generating all the synaesthetic elements. In the structure acquisition stage, we train the model with all the subtasks in a multitask learning manner; in the structure composition stage, we fine-tune the model with the main task to combine the acquired structural knowledge of subtasks and learn the dependency between them."
        },
        {
            "heading": "5.1 Structure Acquisition",
            "text": "As shown in Figure 4, we decompose the synaesthetic structure generation task into four subtasks. Basically, a subtask aims to translate the sentence to one or more synaesthetic elements. For example, the SENSORY _WORD subtask aims to generate the sensory word given the sentence.\nWe then train the sequence-to-sequence model with all subtasks using multi-task learning. We assign each subtask a special token, which is also used to denote its corresponding synaesthetic el-\nements. Then we construct a task prompt for each subtask based on the elements it contains. For example, The special token corresponding to SENSORY _WORD is \u201c[ SENSORY_WORD ]\u201d. The input for each subtask simply adds a task prompt to the input for the original task."
        },
        {
            "heading": "5.2 Structure Composition",
            "text": "Training the model with multiple subtasks cannot capture the interdependency between them. In this stage, we fine-tune the model with the main task, i.e., generating all the synaesthetic elements to capture such information. As shown in Figure 4, we combine the prompts of subtasks to construct the prompt of the main task to guide the model to composite the knowledge of subtasks."
        },
        {
            "heading": "6 Experiments",
            "text": "In this section, we conduct various experiments to investigate the effectiveness of the proposed method on the synaesthetic elements generation task. In addition, we give several analyses and discussions to show the importance of the proposed new dataset."
        },
        {
            "heading": "6.1 Setting",
            "text": "We evaluate our proposed structure generation model on the new Chinese synaesthesia dataset. There are totally 24,000 sentences in the proposed new dataset. We split the dataset into the training set (80%), the validation set (10%) and the test set (10%). It should be noted that these sets are separated by sensory words, which means that the\nsensory words among different sets are totally different.\nWe employ T51 and fine-tune its parameters for generation models. We tune the parameters of our models by grid searching on the validation dataset. We select the best models by early stopping using the Accuracy results on the validation dataset. The model parameters are optimized by Adam (Kingma and Ba, 2015) with a learning rate of 1e-4. The batch size is 16. Our experiments are carried out with an Nvidia RTX 3090 GPU. The experimental results are obtained by averaging ten runs with the random initialization.\nWe use F1-score as the evaluation metric for the sensory word, cue, and stimulus extraction, and weighted F1-score (Manning and Sch\u00fctze, 1999) as the evaluation metric for the sensory modality detection. In addition, a synaesthetic quintet (sensory word, original sensory modality, synaesthetic sensory modality, cue, stimulus) is viewed as correct if and only if the five elements, as well as their combinations, are exactly the same as those in the gold quintet. On this basis, we calculate F1score (Cai et al., 2021) as the overall evaluation metric for the synaesthesia analysis."
        },
        {
            "heading": "6.2 Main Results",
            "text": "We firstly compare the proposed structure generation model with various strong baselines on Table 3, where,\n\u2022 BERT+CRF adopts BERT (Devlin et al., 2019) as the textual encoder, followed by a CRF output layer (Radford and Narasimhan, 2018).\n\u2022 Radical (Jiang et al., 2022) is a pipeline system, which employs a radical-based neural network model to identify the sensory word\u2019s\n1https://huggingface.co/t5-base\nboundary and to jointly classify the original and synaesthetic sensory modalities.\n\u2022 BART employs the pre-trained language model BART (Lewis et al., 2020) to generate the synaesthetic elements individually2.\n\u2022 T5 utilizes T5 (Raffel et al., 2020) to generate the synaesthetic elements individually, which can be considered as the baseline method of the proposed model.\n\u2022 Human engaged two volunteers to identify and label the synaesthetic elements, and the results are considered as representative of human performance.\n\u2022 ChatGPT is a sibling model to InstructGPT (Ouyang et al., 2022), which is trained to follow an instruction in a prompt and provides a detailed response. We ask it to generate the synaesthetic elements from the input sentences3.\nFrom the results, we find that the basic generation model (i.e., BART, T5) cannot give an acceptable result, the performance of which is even lower than BERT+CRF. It may be due to that the basic generation model generates each element one by one, ignoring the correlations among them. In addition, the performance of ChatGPT is much lower than other models. One of the possible reasons is that we do not fine-tune it on the training data, and we only give some prompts to let it understand the synaesthesia analysis task. Based on analyzing its outputs, we find that it is hard for ChatGPT to predict the original and synaesthetic sensory modalities, and it also cannot capture the relations among all these synaesthetic elements.\n2https://huggingface.co/facebook/ bart-base\n3https://openai.com/chatgpt\nIn contrast, our proposed model outperforms all the previous studies significantly (p < 0.05) in all settings. Compared to native speakers, our proposed model surpasses human performance, making it easier to generate all five synaesthetic elements correctly. This indicates that the tree structure is much more helpful for capturing the correlations among synaesthetic elements. Furthermore, the results also indicate the effectiveness of the structure acquisition and composition strategy, which is used to learn alignment between sentences and synaesthetic elements, and the interdependency among synaesthetic elements. The details of the case study are shown in Appendix A."
        },
        {
            "heading": "6.3 Impact of Different Factors",
            "text": "As shown in Table 4, we employ ablation experiments to analyze the impact of different factors in the proposed structure generation model.\nIf we remove the structure acquisition and composition strategy (-SAC), the performance drops to 60.7%. It indicates that this strategy is very important for generating a valid tree and capturing the interdependency among synaesthetic elements. If we remove the tree structure, and only generate the flat sequence, the performance drops to 60.5%. It shows that the tree structure is beneficial to capture the correlations between synaesthetic elements. Furthermore, we also find that the proposed model is much more effective than the simple text generation model. If we remove both the structure acquisition and composition strategy and the tree structure, the proposed model degrades to a T5-based sequence-to-sequence model, and the performance drops to 59.8%."
        },
        {
            "heading": "6.4 Effects of Synaesthetic Elements",
            "text": "We then analyze the effect of synaesthetic elements in Table 5, where w denotes the sensory word, o and s are the original and synaesthetic sensory modalities respectively, and c and t are\nthe cue and the stimulus respectively. In addition, the rows in the table mean different combinations of synaesthetic elements. For example, (w, s) means that we used the proposed structure generation model to generate the sensory word and the synaesthetic sensory modality jointly.\nFrom the table, we can find that: 1) The sensory modalities are deeply correlated with the sensory words. If we jointly generate the sensory word with original (w, o) or synaesthetic sensory modalities (w, s), the performance of the sensory word extraction is improved more than 5%. 2) The original and synaesthetic sensory modalities are correlated, and the joint prediction (w, o, s) is better than predicting them individually. 3) Cue c and stimulus t are both helpful for extracting the sensory word and predicting the synaesthetic sensory modality. If we integrate them into the generation model, the performance of the sensory word extraction and the synaesthetic sensory modality prediction are higher than before. 4) When we generating all the elements jointly (ours), the performance of the proposed model can achieve the best performance.\nIn summary, the above observations show that all the synaesthetic elements are correlated, and the structure generation model is helpful for generating them jointly. Furthermore, the results among subtasks (i.e., (w, o), (w, s, c)) and the main task (ours) also indicate the importance of the structure acquisition and composition strategy, which is used to learn from all the subtasks to the main task."
        },
        {
            "heading": "7 Conclusion",
            "text": "Synaesthesia refers to the description of perceptions in one sensory modality through concepts from other modalities. It involves not only a linguistic phenomenon, but also a cognitive phenomenon structuring human thought and action, which makes understanding it challenging and re-\nwarding. In this study, we give a comprehensive study on the synaesthesia analysis from a computational perspective. In particular, we firstly introduce a new annotation framework, including annotating the sensory modality as well as their cues and stimuli, which facilitates understanding the synaesthesia information. We further design a structure generation model to fully explore the relations among synaesthetic elements and to generate them jointly. The statistics show the potential and usefulness of the proposed new dataset. In addition, the experimental results demonstrate the effectiveness of the proposed model.\nLimitation\nOur work focuses on employing a unified framework for the synaesthesia analysis. However, some more general topics should be addressed as future work, such as the metaphor and sensory modality analysis. Another limitation is the computational complexity of the proposed model, whose training time is slower than the classification-based models."
        },
        {
            "heading": "Acknowledgements",
            "text": "We would like to thank the anonymous reviewers for their insightful and valuable comments. Also, we gratefully acknowledge funding from the National Natural Science Foundation of China (No. 62076175, No.61976146), and Jiangsu Innovation Doctor Plan."
        },
        {
            "heading": "A Case Study",
            "text": "We provide case studies in Figure 5, wherein we select three examples to demonstrate the impact of the proposed model compared with baselines.\nThe first example and the second example are both about the effect of capturing the interdependency between sensory words and cues. With the help of tree structure and structure acquisition and composition, the proposed model can generate accurate sensory words and cues. Sensory word modifies cue, and the results of baseline models do not adhere to this rule.\nThe third example pertains to the connection between stimulus and synaesthetic sensory modality. Although the other three synaesthetic ele-\nments are accurate, baseline models fail to comprehend that the stimulus \u201csaid\u201d does not refer to the synaesthetic sensory modality indicated by the cue \u201cface\u201d."
        }
    ],
    "title": "A Unified Framework for Synaesthesia Analysis",
    "year": 2023
}