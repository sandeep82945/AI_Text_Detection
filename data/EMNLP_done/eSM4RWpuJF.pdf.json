{
    "abstractText": "Pre-trained language models (PLMs) have achieved outstanding achievements in abstractive single-document summarization (SDS). However, such benefits may not fully extend to multi-document summarization (MDS), where the handling of cross-document information is more complex. Previous works either design new MDS architectures or apply PLMs bluntly with concatenated source documents as a reformulated SDS task. While the former does not utilize previous pre-training efforts and may not generalize well across different domains, the latter may not sufficiently attend to the intricate cross-document relationships unique to MDS tasks. Instead, we enforce hierarchy on both the encoder and decoder to better utilize a PLM to facilitate multi-document interactions for the MDS task. Across 10 MDS benchmarks from various domains, our method outperforms or is competitive with the previous best models, including those with additional MDS pre-training or with more parameters. It outperforms its corresponding PLM backbone by up to 3 ROUGEL and is favored by humans.1",
    "authors": [
        {
            "affiliations": [],
            "name": "Chenhui Shen"
        },
        {
            "affiliations": [],
            "name": "Liying Cheng"
        },
        {
            "affiliations": [],
            "name": "Xuan-Phi Nguyen"
        },
        {
            "affiliations": [],
            "name": "Yang You"
        },
        {
            "affiliations": [],
            "name": "Lidong Bing"
        }
    ],
    "id": "SP:c7515ff94e0476290069947de5cdda07b70501f3",
    "references": [
        {
            "authors": [
                "Reinald Kim Amplayo",
                "Stefanos Angelidis",
                "Mirella Lapata."
            ],
            "title": "Unsupervised opinion summarization with content planning",
            "venue": "Proceedings of AAAI.",
            "year": 2020
        },
        {
            "authors": [
                "Reinald Kim Amplayo",
                "Mirella Lapata."
            ],
            "title": "Informative and controllable opinion summarization",
            "venue": "Proceedings of EACL.",
            "year": 2019
        },
        {
            "authors": [
                "Reinald Kim Amplayo",
                "Mirella Lapata."
            ],
            "title": "Unsupervised opinion summarization with noising and denoising",
            "venue": "Proceedings of ACL.",
            "year": 2020
        },
        {
            "authors": [
                "Regina Barzilay",
                "Kathleen McKeown",
                "Michael Elhadad."
            ],
            "title": "Information fusion in the context of multi-document summarization",
            "venue": "Proceedings of ACL.",
            "year": 1999
        },
        {
            "authors": [
                "Tal Baumel",
                "Matan Eyal",
                "Michael Elhadad."
            ],
            "title": "Query focused abstractive summarization: Incorporating query relevance, multi-document coverage, and summary length constraints into seq2seq models",
            "venue": "arXiv preprint arXiv:1801.07704.",
            "year": 2018
        },
        {
            "authors": [
                "Iz Beltagy",
                "Kyle Lo",
                "Arman Cohan."
            ],
            "title": "Scibert: A pretrained language model for scientific text",
            "venue": "Proceedings of EMNLP-IJCNLP.",
            "year": 2019
        },
        {
            "authors": [
                "Iz Beltagy",
                "Matthew E Peters",
                "Arman Cohan."
            ],
            "title": "Longformer: The long-document transformer",
            "venue": "arXiv preprint arXiv:2004.05150.",
            "year": 2020
        },
        {
            "authors": [
                "Lidong Bing",
                "Piji Li",
                "Yi Liao",
                "Wai Lam",
                "Weiwei Guo",
                "Rebecca J Passonneau."
            ],
            "title": "Abstractive multidocument summarization via phrase selection and merging",
            "venue": "Proceedings of ACL-IJCNLP.",
            "year": 2015
        },
        {
            "authors": [
                "Moye Chen",
                "Wei Li",
                "Jiachen Liu",
                "Xinyan Xiao",
                "Hua Wu",
                "Haifeng Wang."
            ],
            "title": "Sgsum: Transforming multi-document summarization into sub-graph selection",
            "venue": "Proceedings of EMNLP.",
            "year": 2021
        },
        {
            "authors": [
                "Janara Christensen",
                "Stephen Soderland",
                "Oren Etzioni"
            ],
            "title": "Towards coherent multi-document summarization",
            "venue": "In Proceedings of the NAACL-HLT",
            "year": 2013
        },
        {
            "authors": [
                "Eric Chu",
                "Peter J. Liu."
            ],
            "title": "Meansum: A neural model for unsupervised multi-document abstractive summarization",
            "venue": "International Conference on Machine Learning.",
            "year": 2018
        },
        {
            "authors": [
                "Arman Cohan",
                "Franck Dernoncourt",
                "Doo Soon Kim",
                "Trung Bui",
                "Seokhwan Kim",
                "Walter Chang",
                "Nazli Goharian."
            ],
            "title": "A discourse-aware attention model for abstractive summarization of long documents",
            "venue": "Proceedings of NAACL-HLT (Short Papers).",
            "year": 2018
        },
        {
            "authors": [
                "Peng Cui",
                "Le Hu."
            ],
            "title": "Topic-guided abstractive multi-document summarization",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2021, pages 1463\u20131472, Punta Cana, Dominican Republic. Association for Computational Linguistics.",
            "year": 2021
        },
        {
            "authors": [
                "Ori Ernst",
                "Avi Caciularu",
                "Ori Shapira",
                "Ramakanth Pasunuru",
                "Mohit Bansal",
                "Jacob Goldberger",
                "Ido Dagan."
            ],
            "title": "Proposition-level clustering for multidocument summarization",
            "venue": "Proceedings of NAACLHLT.",
            "year": 2022
        },
        {
            "authors": [
                "Alexander Richard Fabbri",
                "Irene Li",
                "Tianwei She",
                "Suyi Li",
                "Dragomir Radev."
            ],
            "title": "Multi-news: A largescale multi-document summarization dataset and abstractive hierarchical model",
            "venue": "ACL.",
            "year": 2019
        },
        {
            "authors": [
                "Robert M French."
            ],
            "title": "Catastrophic forgetting in connectionist networks",
            "venue": "Trends in cognitive sciences.",
            "year": 1999
        },
        {
            "authors": [
                "Kavita A. Ganesan",
                "ChengXiang Zhai",
                "Jiawei Han."
            ],
            "title": "Opinosis: A graph based approach to abstractive summarization of highly redundant opinions",
            "venue": "Proceedings of Coling.",
            "year": 2010
        },
        {
            "authors": [
                "Sebastian Gehrmann",
                "Yuntian Deng",
                "Alexander M Rush."
            ],
            "title": "Bottom-up abstractive summarization",
            "venue": "Proceedings of EMNLP.",
            "year": 2018
        },
        {
            "authors": [
                "Demian Gholipour Ghalandari",
                "Chris Hokamp",
                "Nghia The Pham",
                "John Glover",
                "Georgiana Ifrim."
            ],
            "title": "A large-scale multi-document summarization dataset from the wikipedia current events portal",
            "venue": "Proceedings of ACL.",
            "year": 2020
        },
        {
            "authors": [
                "Xiaotao Gu",
                "Yuning Mao",
                "Jiawei Han",
                "Jialu Liu",
                "You Wu",
                "Cong Yu",
                "Daniel Finnie",
                "Hongkun Yu",
                "Jiaqi Zhai",
                "Nicholas Zukoski."
            ],
            "title": "Generating representative headlines for news stories",
            "venue": "Proceedings of The Web Conference.",
            "year": 2020
        },
        {
            "authors": [
                "Mandy Guo",
                "Joshua Ainslie",
                "David Uthus",
                "Santiago Ontanon",
                "Jianmo Ni",
                "Yun-Hsuan Sung",
                "Yinfei Yang."
            ],
            "title": "LongT5: Efficient text-to-text transformer for long sequences",
            "venue": "Findings of NAACL.",
            "year": 2022
        },
        {
            "authors": [
                "Hiroaki Hayashi",
                "Prashant Budania",
                "Peng Wang",
                "Chris Ackerson",
                "Raj Neervannan",
                "Graham Neubig."
            ],
            "title": "Wikiasp: A dataset for multi-domain aspectbased summarization",
            "venue": "TACL.",
            "year": 2021
        },
        {
            "authors": [
                "Chris Hokamp",
                "Demian Gholipour Ghalandari",
                "Nghia The Pham",
                "John Glover."
            ],
            "title": "Dyne: Dynamic ensemble decoding for multi-document summarization",
            "venue": "arXiv preprint arXiv:2006.08748.",
            "year": 2020
        },
        {
            "authors": [
                "Chris Hokamp",
                "Demian Gholipour Ghalandari",
                "Nghia The Pham",
                "John Glover."
            ],
            "title": "Dyne: Dynamic ensemble decoding for multi-document summarization",
            "venue": "ArXiv, abs/2006.08748.",
            "year": 2020
        },
        {
            "authors": [
                "Luyang Huang",
                "Shuyang Cao",
                "Nikolaus Parulian",
                "Heng Ji",
                "Lu Wang."
            ],
            "title": "Efficient attentions for long document summarization",
            "venue": "Proceedings of NAACLHLT.",
            "year": 2021
        },
        {
            "authors": [
                "Philippe Laban",
                "Tobias Schnabel",
                "Paul N. Bennett",
                "Marti A. Hearst."
            ],
            "title": "SummaC: Re-visiting NLIbased models for inconsistency detection in summarization",
            "venue": "TACL.",
            "year": 2022
        },
        {
            "authors": [
                "Logan Lebanoff",
                "Kaiqiang Song",
                "Fei Liu."
            ],
            "title": "Adapting the neural encoder-decoder framework from single to multi-document summarization",
            "venue": "Proceedings of EMNLP.",
            "year": 2018
        },
        {
            "authors": [
                "Mike Lewis",
                "Yinhan Liu",
                "Naman Goyal",
                "Marjan Ghazvininejad",
                "Abdelrahman Mohamed",
                "Omer Levy",
                "Veselin Stoyanov",
                "Luke Zettlemoyer"
            ],
            "title": "Bart: Denoising sequence-to-sequence pre-training for natural language",
            "year": 2020
        },
        {
            "authors": [
                "Piji Li",
                "Lidong Bing",
                "Wai Lam."
            ],
            "title": "Readeraware multi-document summarization: An enhanced model and the first dataset",
            "venue": "Proceedings of the Workshop on New Frontiers in Summarization.",
            "year": 2017
        },
        {
            "authors": [
                "Piji Li",
                "Lidong Bing",
                "Wai Lam."
            ],
            "title": "Actor-critic based training framework for abstractive summarization",
            "venue": "arXiv preprint arXiv:1803.11070.",
            "year": 2018
        },
        {
            "authors": [
                "Piji Li",
                "Wai Lam",
                "Lidong Bing",
                "Weiwei Guo",
                "Hang Li."
            ],
            "title": "Cascaded attention based unsupervised information distillation for compressive summarization",
            "venue": "Proceedings of EMNLP.",
            "year": 2017
        },
        {
            "authors": [
                "Piji Li",
                "Zihao Wang",
                "Wai Lam",
                "Zhaochun Ren",
                "Lidong Bing."
            ],
            "title": "Salience estimation via variational auto-encoders for multi-document summarization",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence.",
            "year": 2017
        },
        {
            "authors": [
                "Wei Li",
                "Xinyan Xiao",
                "Jiachen Liu",
                "Hua Wu",
                "Haifeng Wang",
                "Junping Du."
            ],
            "title": "Leveraging graph to improve abstractive multi-document summarization",
            "venue": "Proceedings of ACL.",
            "year": 2020
        },
        {
            "authors": [
                "Chin-Yew Lin."
            ],
            "title": "Rouge: A package for automatic evaluation of summaries",
            "venue": "Text summarization branches out.",
            "year": 2004
        },
        {
            "authors": [
                "Peter J Liu",
                "Mohammad Saleh",
                "Etienne Pot",
                "Ben Goodrich",
                "Ryan Sepassi",
                "Lukasz Kaiser",
                "Noam Shazeer."
            ],
            "title": "Generating wikipedia by summarizing long sequences",
            "venue": "ICLR.",
            "year": 2018
        },
        {
            "authors": [
                "Shuaiqi Liu",
                "Jiannong Cao",
                "Ruosong Yang",
                "Zhiyuan Wen."
            ],
            "title": "Highlight-transformer: Leveraging key phrase aware attention to improve abstractive multidocument summarization",
            "venue": "Findings of ACLIJCNLP.",
            "year": 2021
        },
        {
            "authors": [
                "Yang Liu",
                "Mirella Lapata."
            ],
            "title": "Hierarchical transformers for multi-document summarization",
            "venue": "Proceedings of ACL.",
            "year": 2019
        },
        {
            "authors": [
                "Yang Liu",
                "Mirella Lapata."
            ],
            "title": "Text summarization with pretrained encoders",
            "venue": "Proceedings of EMNLP-IJCNLP.",
            "year": 2019
        },
        {
            "authors": [
                "Yixin Liu",
                "Ansong Ni",
                "Linyong Nan",
                "Budhaditya Deb",
                "Chenguang Zhu",
                "Ahmed H Awadallah",
                "Dragomir Radev."
            ],
            "title": "Leveraging locality in abstractive text summarization",
            "venue": "Proceedings of EMNLP.",
            "year": 2022
        },
        {
            "authors": [
                "Yao Lu",
                "Yue Dong",
                "Laurent Charlin."
            ],
            "title": "Multixscience: A large-scale dataset for extreme multidocument summarization of scientific articles",
            "venue": "Proceedings of EMNLP.",
            "year": 2020
        },
        {
            "authors": [
                "Daraksha Parveen",
                "Michael Strube."
            ],
            "title": "Multidocument summarization using bipartite graphs",
            "venue": "Proceedings of TextGraphs-9.",
            "year": 2014
        },
        {
            "authors": [
                "Ramakanth Pasunuru",
                "Mengwen Liu",
                "Mohit Bansal",
                "Sujith Ravi",
                "Markus Dreyer."
            ],
            "title": "Efficiently summarizing text and graph encodings of multi-document clusters",
            "venue": "Proceedings of NAACL-HLT.",
            "year": 2021
        },
        {
            "authors": [
                "Laura Perez-Beltrachini",
                "Mirella Lapata."
            ],
            "title": "Multi-document summarization with determinantal point process attention",
            "venue": "Journal of Artificial Intelligence Research.",
            "year": 2021
        },
        {
            "authors": [
                "Ratish Puduppully",
                "Mark Steedman."
            ],
            "title": "Multidocument summarization with centroid-based pretraining",
            "venue": "arXiv preprint arXiv:2208.01006.",
            "year": 2022
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J Liu."
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "Journal of Machine Learning Research.",
            "year": 2020
        },
        {
            "authors": [
                "Eva Sharma",
                "Chen Li",
                "Lu Wang."
            ],
            "title": "BIGPATENT: A large-scale dataset for abstractive and coherent summarization",
            "venue": "Proceedings of ACL.",
            "year": 2019
        },
        {
            "authors": [
                "Chenhui Shen",
                "Liying Cheng",
                "Lidong Bing",
                "Yang You",
                "Luo Si."
            ],
            "title": "Sentbs: Sentence-level beam search for controllable summarization",
            "venue": "EMNLP.",
            "year": 2022
        },
        {
            "authors": [
                "Chenhui Shen",
                "Liying Cheng",
                "Ran Zhou",
                "Lidong Bing",
                "Yang You",
                "Luo Si."
            ],
            "title": "Mred: A meta-review dataset for structure-controllable text generation",
            "venue": "Findings of ACL.",
            "year": 2022
        },
        {
            "authors": [
                "Yun-Zhu Song",
                "Yi-Syuan Chen",
                "Hong-Han Shuai."
            ],
            "title": "Improving multi-document summarization through referenced flexible extraction with creditawareness",
            "venue": "proceedings of NAACL-HLT.",
            "year": 2022
        },
        {
            "authors": [
                "Shangqing Tu",
                "Jifan Yu",
                "Fangwei Zhu",
                "Juanzi Li",
                "Lei Hou",
                "Jian-Yun Nie."
            ],
            "title": "UPER: Boosting multi-document summarization with an unsupervised prompt-based extractor",
            "venue": "Proceedings of Coling.",
            "year": 2022
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin."
            ],
            "title": "Attention is all you need",
            "venue": "Proceddings of NeurIPS.",
            "year": 2017
        },
        {
            "authors": [
                "Lu Wang",
                "Wang Ling."
            ],
            "title": "Neural network-based abstract generation for opinions and arguments",
            "venue": "Proceedings of NAACL-HLT.",
            "year": 2016
        },
        {
            "authors": [
                "Ruben Wolhandler",
                "Arie Cattan",
                "Ori Ernst",
                "Ido Dagan"
            ],
            "title": "How \u201cmulti\u201d is multi-document summarization",
            "venue": "In Proceedings of EMNLP",
            "year": 2022
        },
        {
            "authors": [
                "Wen Xiao",
                "Iz Beltagy",
                "Giuseppe Carenini",
                "Arman Cohan."
            ],
            "title": "Primera: Pyramid-based masked sentence pre-training for multi-document summarization",
            "venue": "Proceedings of ACL.",
            "year": 2022
        },
        {
            "authors": [
                "Michihiro Yasunaga",
                "Rui Zhang",
                "Kshitijh Meelu",
                "Ayush Pareek",
                "Krishna Parasuram Srinivasan",
                "Dragomir R. Radev."
            ],
            "title": "Graph-based neural multidocument summarization",
            "venue": "Proceddings of CoNLL.",
            "year": 2017
        },
        {
            "authors": [
                "Manzil Zaheer",
                "Guru Guruganesh",
                "Kumar Avinava Dubey",
                "Joshua Ainslie",
                "Chris Alberti",
                "Santiago Ontanon",
                "Philip Pham",
                "Anirudh Ravula",
                "Qifan Wang",
                "Li Yang"
            ],
            "title": "Big bird: Transformers for longer sequences. NeurIPS",
            "year": 2020
        },
        {
            "authors": [
                "Jingqing Zhang",
                "Yao Zhao",
                "Mohammad Saleh",
                "Peter Liu."
            ],
            "title": "Pegasus: Pre-training with extracted gap-sentences for abstractive summarization",
            "venue": "Proceedings of ICML.",
            "year": 2020
        },
        {
            "authors": [
                "Hao Zhou",
                "Weidong Ren",
                "Gongshen Liu",
                "Bo Su",
                "Wei Lu."
            ],
            "title": "Entity-aware abstractive multidocument summarization",
            "venue": "Findings of ACLIJCNLP.",
            "year": 2021
        },
        {
            "authors": [
                "Shen"
            ],
            "title": "2022b) to truncate the end of the combined source",
            "venue": "D Additional Results",
            "year": 2022
        },
        {
            "authors": [
                "Xiao"
            ],
            "title": "ROUGE-2 results on 10 datasets including: Multinews, WCEP, Multi-XScience (M-XSc), Rotten Tomatoes (RT), MReD, MReD+, and 4 Wikipedia domains",
            "year": 2022
        },
        {
            "authors": [
                "Laban"
            ],
            "title": "2022) can output a score for the entailment of each sentence towards an input document from a range of -1 to 1, where -1 indicates total contradiction, and 1 indicates perfect entailment",
            "venue": "We use a threshold of 0.59,",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Multi-document summarization (MDS) was first proposed by Barzilay et al. (1999), framed as a task of producing a single summary using a set of related documents, and has been extensively studied (Xiao et al., 2022; Song et al., 2022; Tu et al., 2022; Liu et al., 2021; Li et al., 2020; Liu and Lapata, 2019a; Fabbri et al., 2019). MDS is inherently much more complex than single-document summarization (SDS). Specifically, unlike SDS which requires the extraction of crucial information in a single article, MDS requires handling not only the contradictions from multiple sources but also the\n\u2217 Chenhui is under the Joint PhD Program between Alibaba and National University of Singapore.\n\u2020 Corresponding author. 1Our code and data are fully released at https://github.\ncom/DAMO-NLP-SG/HierEncDec.\nrepetitions and larger amounts of trivial information across documents (Zhou et al., 2021; Cui and Hu, 2021; Liu and Lapata, 2019a; Lebanoff et al., 2018; Li et al., 2017a, 2018; Yasunaga et al., 2017; Ganesan et al., 2010).\nWhile MDS could be superficially converted to SDS by concatenating multiple documents into a pseudo-single document (Fabbri et al., 2019; Liu et al., 2018; Lebanoff et al., 2018), the irregular and complex multi-document information remains. As a result, it continues to pose challenges due to its large difference from the mostly coherent PLM pretraining data of continuous text segments (Baumel et al., 2018). On the other hand, many works design specialized MDS architectures (Xiao et al., 2022; Liu et al., 2021; Liu and Lapata, 2019a; Fabbri et al., 2019). However, they require large supervised MDS datasets on the scale of 100K training data (Liu et al., 2021; Liu and Lapata, 2019a; Liu et al., 2018; Fabbri et al., 2019) and may not generalize to unseen domains. Neither do they utilize the capabilities of large PLMs (Lewis et al., 2020; Raffel et al., 2020; Zhang et al., 2020) that were pre-trained on coherent data.\nGiven that MDS data are scarce and costly to obtain (Lebanoff et al., 2018), we propose a simple hierarchical encoding-decoding scheme to effectively fine-tune a PLM on smaller-scale MDS datasets ranging from about 2K to 45K training samples. We aim to better adapt the PLM\u2019s knowledge gained during pre-training (which is more useful to SDS due to a smaller data shift) for MDS finetuning in both the encoder and the decoder. In the encoder, we preserve the original attention mechanism within the same document while re-purposing \u201c<s>\u201d as a document-level representation for highlevel interactions. This approach ensures that intradocument tokens can be well-represented using the pre-training knowledge, while the document will still interact with one other on a higher level. In the cross-attention layers of the decoder, we impose\ndocument-level importance scaling on the attention weights, which allows each document to influence the output tokens differently but more appropriately. As a result, our method can still make use of the general language modeling capabilities of PLMs, while learning to handle the complex crossdocument information during fine-tuning.\nWe conduct experiments on various MDS datasets (Shen et al., 2022b; Lu et al., 2020; Ghalandari et al., 2020; Fabbri et al., 2019; Wang and Ling, 2016) across a vast range of domains, namely, news, scientific literature, movie critics, peer reviews, and specific Wikipedia topics. Our experimental results show that our hierarchical scheme outperforms strong state-of-the-art models (including those with additional MDS pre-training or larger model sizes). It also consistently improves the PLM backbone on all 10 datasets by up to 3 ROUGE-L and, on the manually inspected datasets, is preferred by humans. In addition, our detailed attention, content analyses, and ablation reveal that our method ensures more coherent encoder representations for intra-document tokens, as well as obtains the wider cross-attention coverage of the different documents during decoding. In this manner, our method sufficiently utilizes the pre-training knowledge for encoding, while encouraging the decoding process to consider all documents by more proper weights, rather than over-focusing a few documents and ignoring the rest."
        },
        {
            "heading": "2 Related Work",
            "text": "MDS Models Previous works have designed specific neural abstractive summarization architectures for the MDS task (Liu et al., 2021; Liu and Lapata, 2019a; Liu et al., 2018; Fabbri et al., 2019). However, it is hard to adapt the existing PLMs to these specific designs. Thus, these models have to be trained from scratch using relatively large MDS datasets on the scale of hundreds of thousands of examples. The same applies to graph-based networks (Chen et al., 2021; Li et al., 2020; Parveen and Strube, 2014; Christensen et al., 2013) that explicitly facilitate document interactions with the graphical designs. As a result, all these models may work well in the specific domain it was trained in, but may not generalize well to the smaller MDS datasets of other domains. Recent works experiment with additional MDS pre-training (Xiao et al., 2022; Puduppully and Steedman, 2022) with an existing PLM. Nevertheless, the MDS pre-training\ncorpus (e.g., NewSHead (Gu et al., 2020)) is much smaller in scale than the pre-training corpora, and re-training PLMs with much smaller-sized MDS corpus may lead to catastrophic forgetting (French, 1999) of the pre-trained knowledge.\nPLMs for MDS PLMs (Lewis et al., 2020; Raffel et al., 2020; Zhang et al., 2020) have achieved state-of-the-art performances for SDS. Some works have directly used these PLMs for MDS by concatenating multiple documents (Guo et al., 2022; Xiao et al., 2022; Shen et al., 2022a,b), including the works that focus on longer context windows (Zaheer et al., 2020; Beltagy et al., 2020). However, concatenating the content of multiple documents together prevents the PLMs from distinguishing or processing the contents differently. By essentially processing the MDS inputs as if they were a single document, the PLMs may not handle the cross-document information well. An alternative is to conduct summarization through multiple stages (Song et al., 2022; Tu et al., 2022; Ernst et al., 2022; Hokamp et al., 2020a; Li et al., 2020; Lebanoff et al., 2018; Li et al., 2017b,c; Bing et al., 2015), where salient contents are first extracted or summarized from multiple documents, then passed to a PLM for the final abstraction stage. However, the inputs to the PLM now consist of groups of incoherent sentences or phrases stitched together from multiple sources, presenting a significant distributional shift from the PLMs\u2019 original pre-training corpora.\nOur method is different from the previous notable works (Xiao et al., 2022; Beltagy et al., 2020) in both that we make novel use of the start-of-document tokens as a document-level representation, as well as further utilizing these tokens during our hierarchical decoding scheme to enable cross-document interactions."
        },
        {
            "heading": "3 Preliminaries and Notations",
            "text": "We focus on adapting encoder-decoder PLMs for the MDS task. The encoder-decoder Transformer (Vaswani et al., 2017) is a common underlying architecture for PLMs frequently used for summarization tasks (Beltagy et al., 2020; Lewis et al., 2020; Zhang et al., 2020). The encoder uses selfattention to encode the source tokens, whereas the decoder uses cross-attention with source tokens and self-attention with the generated tokens for decoding. Our method targets the attention involving the\nsource tokens, i.e., the self-attention in the encoder and the cross-attention in the decoder.\nFormally, the encoder updates the representation of each source token across all layers. The representation of the ith token in the jth layer, h(j)i , can be computed as a weighted sum of the hidden states of the context tokens in the previous layer:\nh (j) i = K\u2211 k=1 wkV h (j\u22121) k (1)\nwhere V is the value projection matrix, K is the total number of input tokens, and the weight wk is obtained by a softmax calculation of the token attention scores:\nw = (w0, ..., wK) = softmax(a0, ..., aK) (2)\nThe attention scores a = (a0, ..., aK) are computed with the query (Q) and key matrices (K):\na = Qh (j\u22121) i \u00b7 [Kh (j\u22121) 0 , ...,Kh (j\u22121) K ] (3)\nFor the decoder, cross-attention is calculated between the last generated token and the source tokens. The weights calculations are similar to Equation (2) and Equation (3) except that the Q matrix takes in the representation of the last generated token (after decoder self-attention), and the K matrix takes in the representations of the source tokens from the last encoder layer.\nLastly, for multi-document encoding, we follow the common approach (Xiao et al., 2022; Shen et al., 2022b; Fabbri et al., 2019) to feed multiple inputs into the model by concatenating one document after another. With N documents, the input sequence can be represented as X = {D0, D1, ..., DN}, where Dn = {xn,0, xn,1, ...} are the input tokens from the nth document."
        },
        {
            "heading": "4 Method",
            "text": "To better leverage the strong language modeling capabilities of PLMs, we propose an encodingdecoding scheme to better equip the PLMs for the MDS setting. Our design fulfills the following requirements: (1) It leverages the language modeling capability of PLMs gained during pre-training; and (2) It facilitates the model to better process crossdocument information during fine-tuning with the MDS datasets.\nFor the first requirement, we preserve the token interactions within the same document for both the\nencoder and decoder. For the second requirement, we use the PLM\u2019s \u201c<s>\u201d token for the hierarchical representation of the documents in the encoder, then make further use of these tokens for hierarchical attention scaling in the decoder."
        },
        {
            "heading": "4.1 Hierarchical Encoding Scheme",
            "text": ""
        },
        {
            "heading": "4.1.1 Leveraging PLM Knowledge",
            "text": "To better leverage the knowledge for source token representations gained during pre-training, we apply the following modifications to the encoder: restricted intra-document full attention, and position restart.\nRestricted Intra-Document Full Attention Since the PLM\u2019s self-attention is only applied to tokens from the same congruent source during pretraining (Lewis et al., 2020), it may not handle cross-document token representations well. Thus, to conform with the pre-training process, we restrict the self-attention for each token to only its sibling tokens within the same document (Figure 1 top left). In this way, the tokens are only allowed to see words within their own documents, which avoids leaking information or being influenced by off-context or contradictory information from other documents. Some works (Shen et al., 2022b; Fabbri et al., 2019) have directly applied full attention to tokens from all documents (Figure 2a) or use a local attention window (e.g., LED (Beltagy et al., 2020) and PRIMERA (Xiao et al., 2022)) (Figure 2b) where each token only attends to a small window of surrounding tokens. In both approaches, the token is allowed to be influenced by tokens from other documents, thus potentially causing improper representations.\nPosition Restart We restart the positional encoding for each document (Figure 1 bottom), in order to signal to our modified encoder that subsequent words are from the next document and are not continuous paragraphs of the previous document. As a result, the encoding process of each document will conform with the single-document encoding process that PLMs were pre-trained on."
        },
        {
            "heading": "4.1.2 Handling Cross-Document Information",
            "text": "Leveraging the PLM knowledge alone can only lead to the independent processing of each document. To equip the model with capabilities to better learn cross-document information during the fine-tuning stage, we make use of global tokens for document-level information encapsulations.\nblack lines to indicate document borders, blue cells for local attention, and orange cells for document-level attention. Best viewed in color.\nment borders in the MDS setting, blue cells for local attention, and orange cells for global attention.\nStart-of-Document (SOD) Representations Previous approaches have employed either a single \u201c<s>\u201d token for retaining global information of the full input sequence through self-attention (e.g., LED) or have introduced multiple special \u201c<doc-sep>\u201d tokens between documents for the same purpose (e.g., PRIMERA). In contrast, we adopt a different strategy by utilizing the \u201c<s>\u201d token to capture information from individual documents. Specifically, we position the \u201c<s>\u201d token at the start of each document, serving as the\nSOD token. However, instead of letting the SOD tokens attend to all tokens in the documents and act as global tokens, we restrict their attention to the corresponding document tokens that they represent. This is because the \u201c<s>\u201d commonly resides at the sequence\u2019s outset of the pre-training inputs of PLMs, and has also been conventionally employed to encapsulate a text sequence in Transformer architectures. Consequently, the PLMs have already acquired the capability to capture the document information with the \u201c<s>\u201d tokens. In this way, we leverage the \u201c<s>\u201d tokens as a high-level document representation to further facilitate cross-document interactions in both the encoder and decoder (Section 4.2).\nDocument-Level Attention Unlike LED or PRIMERA which enable full attention on global tokens (shown with orange blocks in Figure 2b), we only allow the SOD tokens to attend to samedocument tokens and SOD tokens from other documents, as shown with the orange blocks in the \u201chierarchical self-attention\u201d diagram in Figure 1. This allows each SOD token to encode the informa-\ntion in the same document, while only exchanging cross-document information with other SOD tokens."
        },
        {
            "heading": "4.2 Hierarchical Decoding Scheme",
            "text": "We further design a hierarchical decoder to leverage the document encapsulations in the SOD tokens, while leveraging the existing pre-training knowledge for decoding. As discussed in Section 3, the PLM\u2019s decoder first carries out self-attention with the previously generated tokens, followed by crossattention with the source tokens. We do not modify the self-attention mechanism which is independent of the source tokens. Instead, we take advantage of the document-level information in the SOD encoder output tokens, by scaling the cross-attention attention weights towards the source tokens from the respective documents accordingly.\nWe illustrate the cross-attention mechanism in Figure 1 (\u201chierarchical cross-attention\u201d diagram). Formally, given N documents, we denote the crossattention scores in the decoder toward each individual document as an = (an,0, ..., an,kn), where kn is the number of tokens in the nth document. Similar to Equation (2), we calculate the cross-attention weights for tokens within each document:\nwn = (wn,0, ..., wn,kn) = softmax(an) (4)\nNext, we rely on the SOD tokens to decide the relative importance of each document, then scale the cross-attention weights accordingly. To do so, we first obtain the normalized scaling factor for each document as:\nS0, ..., Sn = softmax(a0,0, ..., an,0) (5)\nwhere an,0 \u2208 an is the attention score for the SOD token in the nth document. We derive the normalized attention weights for tokens in each document:\nw\u0304n = (w\u0304n,0, ..., w\u0304n,k) = Sn \u00b7wn (6)\nIn this manner, our MDS decoder can better grasp the relative degree of importance of each document, while preserving relative cross-attention weights within the same document during decoding."
        },
        {
            "heading": "5 Experiments",
            "text": ""
        },
        {
            "heading": "5.1 Benchmark Datasets",
            "text": "We conduct experiments on a wide range of MDS datasets as follows: Multinews (Fabbri et al., 2019), WCEP (Ghalandari et al., 2020), Multi-Xscience\n(Lu et al., 2020), Rotten tomatoes (Wang and Ling, 2016), and the recently released MReD dataset (Shen et al., 2022b). These datasets are gathered from a wide range of domains, namely news, events, scientific literature, movie reviews, and peer reviews. Table 1 provides the statistics of each dataset (see download links in Appendix A).\nDue to the scarcity of MDS datasets, we further compile several datasets in different domains. In the MReD peer-review dataset (Shen et al., 2022b), multiple reviews of the same research paper are used as the inputs, whereas the meta-review is used as the summary. We extend this dataset by including the corresponding rebuttals and refer to it as \u201cMReD+\u201d. In addition, we compiled MDS datasets from four domains of Wikipedia articles2, namely, \u201cFilm\u201d, \u201cMeanOfTransportation\u201d, \u201cSoftware\u201d, and \u201cTown\u201d. These datasets use the Wikipedia lead section text as the summary and the individual sections as multi-document inputs.\n5.2 Evaluated Models3\nBART We fine-tune the the pre-trained \u201cbartlarge\u201d (Lewis et al., 2020), which uses full attention (Figure 2a) for all source tokens.\nLED LED (Beltagy et al., 2020) is a competitive baseline for long text summarization. It is directly initialized from \u201cbart-large\u201d, but uses globallocal attention to better handle long context inputs. Specifically, each local token has slidingwindow attention and also attends to the global tokens, whereas global tokens attend to all source tokens. This model contains slightly more parameters due to separate V , Q, and K (see Equation (1) and Equation (3)) for the global tokens and local tokens.\n2Original dataset from WikiAsp (Hayashi et al., 2021). 3The model checkpoints are detailed in Appendix B.\nLongT5 LongT5 (Guo et al., 2022) adopts summarization pre-training strategies (Zhang et al., 2020) with T5 (Raffel et al., 2020) for long context summarization. It uses transient global attention, which differs from LED in that the global tokens are obtained by summing the embeddings of tokens in k non-overlapping continuous blocks which divides the full input source. Due to resource constraints, we could only fine-tune the base model on our machines. Thus, we also provide our method results using the \u201cbart-base\u201d backbone of a smaller model size.\nPRIMERA PRIMERA (Xiao et al., 2022) uses additional MDS pre-training on the NewSHead dataset on top of the LED model. It introduces a special \u201c<doc-sep>\u201d token in between the documents for the same function as the global token in LED. It also uses a moving context window, which may still capture tokens across two consecutive documents (see Figure 2b).\nHED (ours) We apply our Hierarchical Encoding Decoding scheme (HED) on top of \u201cbart-large\u201d to obtain \u201cBART+HED\u201d. Since HED doesn\u2019t introduce new parameters, the \u201cBART+HED\u201d is strictly comparable with the BART baseline. Moreover, we apply HED with the \u201cbart-large-cnn\u201d checkpoint, resulting in the \u201cBART-cnn+HED\u201d model. The \u201cbart-large-cnn\u201d checkpoint involves additional pre-training with an SDS dataset in the news domain (i.e., CNN/DM) over the \u201cbart-large\u201d checkpoint. While we do not conduct additional pre-training ourselves, we can still compare \"BART-cnn+HED\" with \"BART+HED\" to assess the knowledge transferred during the pre-training stage using an SDS dataset to the MDS downstream tasks. Moreover, this can be juxtaposed with the comparison between PRIMERA and LED, which reveals the benefits derived from extra MDS pre-training on a news dataset (i.e. NewSHead) for the MDS task."
        },
        {
            "heading": "5.3 Experimental Setup",
            "text": "We fine-tune all evaluated models discussed above with cross-entropy loss on all datasets. Following PRIMERA (Xiao et al., 2022), we use source and target truncation of 4096 and 1024 respectively. Amongst all evaluated models, LongT5 and LED can readily accept a source input of 4096 tokens. BART can only accept a maximum of 1024 tokens, so we repeatedly copy BART\u2019s positional\nembeddings 4 times, similar to how LED is derived from BART by Beltagy et al. (2020). Following PRIMERA, for Multinews, WCEP, and MultiXscience, we truncate the end of each source document4. For the rest of the datasets, we follow Shen et al. (2022b) and truncate the end of the combined documents. See more explanations for our choices of truncation for different datasets in Appendix C.\nWe use Adam optimizer with a learning rate of 5e \u2212 5, and without any warm-up or weight decay. All models are trained on a single A10080G GPU, for the same number of training steps on the same dataset as shown in Table 1. The number of training steps for each dataset is determined based on the development set losses on the \u201cbartlarge\u201d baseline5. We evaluate all model outputs on the ROUGE (Lin, 2004) metric and provide the F1 values of ROUGE-1, ROUGE-2, and ROUGE-L. We also conduct human evaluations on selected benchmarks.\nAdditionally, we compile results directly reported by other papers (Guo et al., 2022; Song et al., 2022; Liu et al., 2022; Tu et al., 2022; Shen et al., 2022b; Pasunuru et al., 2021) in Appendix D\u2019s Table 6 to Table 10 to give a better overview of the current performances on different datasets. However, as the settings for reported models differ vastly, the results are not strictly comparable."
        },
        {
            "heading": "6 Results",
            "text": ""
        },
        {
            "heading": "6.1 Main Results",
            "text": "We show the ROUGE-1 and ROUGE-L results in Table 2 for all 10 datasets (see ROUGE-2 scores in Appendix Table 11). The upper section of Table 2 displays the performance of comparatively smaller models. Notably, our \u201cBART+HED\u201d method surpasses LongT5 across nearly all benchmarks, despite having approximately half the model size. Moving to the lower section of Table 2, our \u201cBART+HED\u201d model consistently exhibits improvement over the corresponding \u201cBART\u201d backbone, demonstrating the effectiveness of our proposed method.\nIn addition, \u201cBART-cnn+HED\u201d generally outperforms \u201cBART+HED\u201d. This shows that our proposed method can effectively facilitate knowledge\n4For instance, if there are 4 documents, each document is truncated to 4096 / 4 = 1024 tokens.\n5We have experimented on a few datasets and observed that the same number of training steps is selected by the \u201cbart-base\u201d model.\ntransfer, even for the knowledge gained with pretraining on an SDS news dataset. On the other hand, PRIMERA uses additional MDS pre-training with a news dataset on top of LED, but experiences some obvious performance drops, especially on the datasets of MReD, MRed+, and Software. As discussed previously, the MDS pre-training corpora characteristics deviate significantly from those used during the PLM\u2019s original pre-training. Thus, it is likely that additional MDS pre-training for PRIMERA on only one news dataset (i.e., NewShead) has led to catastrophic forgetting of the other domains. As a result, the model loses some knowledge gained during its previous pre-training stage, and performs especially badly on domains that deviate significantly from the news domain.\nNonetheless, our method does not guarantee improvements with additional SDS pre-training, as evidenced by the drop in ROUGE-L for the Rotten Tomatoes. This dataset is very small, comprising of around 2K training examples. It is also unique in the sense that each sample consists of an average of 100 documents (see Table 1 \u201c#Docs\u201d). In this case, additional SDS pre-training may negatively impact the model\u2019s ability to handle inputs with a large number of documents, especially with very limited MDS fine-tuning data.\nAnother interesting observation is that LED performs relatively strongly, particularly on the WCEP and Multinews datasets. According to Wolhandler et al. (2022), the gold summaries of WCEP and Multinews indicate a relatively limited degree of multi-text merging, suggesting that information\nfrom a single document suffices for summarization. Consequently, although LED lacks an inherent architecture for handling cross-document information, it can still perform well because of its optimized capability to process long-context inputs."
        },
        {
            "heading": "6.2 Human Evaluation",
            "text": "We conduct human evaluations on the Multinews and MReD datasets of vastly different domains. Specifically, we randomly sample from each dataset 50 test samples to be evaluated independently by 2 random evaluators among a pool of 5 volunteers in the research field of artificial intelligence. Each evaluator is given both outputs from \u201cBART\u201d and \u201cBART+HED\u201d in a double-blind manner, and asked to rate 1 point for the better summary and 0 for the worse, while giving both 0.5 for a tie.\nThe evaluation criteria are (1) Fluency - the overall flow and grammar of the summary; (2) Relevance - the selection of important content from input documents; (3) Abstractiveness - the extent to which the summary consists of rephrased contents and avoids copying extensively from the input; (4) Salience - the amount of salient information included in the summary; and (5) Coverage - the minimal number of input documents required to supply sufficient information for the summary. The latter new criteria are newly proposed by us to better assess the MDS task (see more details in Appendix E).\nThe results are reported in Table 3. For both evaluated benchmarks, \u201cBART+HED\u201d outperforms \u201cBART\u201d overall. In terms of writing\nMu ltin\news WC EP\nMu lti-X\nSci enc e MR eD\nWik i-To\nwn\nWik i-Fi\nlm\nWik i-So\nftw are\nWik i-M\nean OfT\nran s\n1\n2\n3 4 R el at iv e A tte nt io\nn BART BART+HED\n(a) Relative document self attention of \u201cBART+HED\u201d over \u201cBART\u201d in the encoder. For better visualization, we exclude the result for Rotten Tomatoes, which is 19.5.\nMu ltin\news WC EP\nMu lti-X\nSci enc e MR eD\nWik i-to\nwn\nWik i-fil\nm\nWik i-so\nftw are\nWik i-M\nean OfT\nran s\n0.8\n1\nR el\nat iv\ne C\nD S\nBART BART+HED\n(b) Relative cross-document standard deviation of \u201cBART+HED\u201d over \u201cBART\u201d in the decoder. We exclude the result for Rotten Tomatoes, which is statistically insignificant.\nFigure 3: Document-level attention analysis. Results are statistically significant with p \u2264 0.001.\nstyle, \u201cBART+HED\u201d has significantly better abstractiveness for both datasets. This suggests that \u201cBART+HED\u201d summarizes using its own words rather than copying long phrases and sentences verbatim from the source, thanks to the hierarchical structure that encourages explicit crossdocument information handling. While higher levels of abstraction may lead to fluency degradation, \u201cBART+HED\u201d still performs competitively with \u201cBART\u201d in fluency on the Multinews dataset, and even surpasses \u201cBART\u201d on the MReD dataset. This indicates that despite our modifications, \u201cBART+HED\u201d still retains sufficient language modeling capability from \u201cBART\u201d.\nIn terms of content, our method also achieves higher levels of relevance than \u201cBART\u201d for both datasets. This supports that our model is better at comparing input documents during decoding (also supported by Section 6.3 later) and extracting the most important information for the summary. Moreover, as MReD is from the peer-review domain, the summaries, which are meta-reviews, are highly condensed (Shen et al., 2022b) and pose a stronger challenge due to frequently conflicting input documents from disagreeing reviewers. In this dataset, \u201cBART+HED\u201d is superior in both salience and coverage with much higher margins, suggesting that our method is particularly effective for such complex MDS tasks."
        },
        {
            "heading": "6.3 Attention Analysis",
            "text": "To better understand the generation process, we conduct attention weights analysis on 200 test samples per dataset for both the encoder and decoder.\nEncoder Analysis Figure 3a presents the relative (normalized over the \u201cBART\u201d baseline) attention of each source token toward its belonged document (a.k.a. self document) of \u201cBART+HED\u201d over the\n\u201cBART\u201d baseline in the encoder. Unsurprisingly, the self document token attention for \u201cBART+HED\u201d is significantly higher than the baseline across all datasets. This observation affirms that each token representation is influenced more by the coherent context of its own document while ignoring the potentially misleading or irrelevant information from other documents.\nDecoder Analysis During decoding, we propose the Cross-Document Standard deviation (CDS) metric (see Appendix G) to measure the standard deviation of each predicted token\u2019s normalized cross-attention weights toward different documents. We plot the relative CDS of our \u201cBART+HED\u201d model over \u201cBART\u201d in Figure 3b. A higher CDS indicates that the current decoding step only pays concentrated attention to a few documents, whereas a smaller CDS indicates that the attention is more evenly spread out across different documents. In Figure 3b, it is evident that our model has significantly smaller CDS values across all datasets. This shows that during the decoding process, our model pays attention more evenly across documents rather than focusing on a specific document, which helps it produce more comprehensive summaries that consider more documents.6"
        },
        {
            "heading": "6.4 Content Analysis",
            "text": "To further validate if our method indeed produces more salient summaries, we conduct entailmentbased content analyses of the generated summaries across all datasets. Inspired by Laban et al. (2022), we treat each sentence in the summary as one information unit, and then calculate the average Number\n6Rotten Tomatoes has near-zero absolute CDS values, which is likely due to the much higher numbers of documents per sample. These results are not included in Figure 3b due to statistical insignificance.\nof Entailed source Documents (NED) per sentence (see Appendix H). The higher the NED, the more salient the summary potentially is, since it contains information that is entailed by more documents.\nAs shown in Table 4, on most datasets, \u201cBART+HED\u201d has statistically higher NED, suggesting that our method may generate more salient summaries. One special case is Multi-Xscience (M-XSc), which uses the related work section of a paper as the target summary, and the abstract of the cited papers as input documents. Upon inspection, we discover that the summaries generated by \u201cBART+HED\u201d are much more abstractive and succinct (consistent with the human evaluation results on other datasets in Section 6.2), resulting in low scores below the threshold value for the entailment model used; on the other hand, the generations of \u201cBART\u201d copies extensively and are thus easily classified as positive entailments. When a smaller threshold is used (Appendix H), our method still outperforms the \u201cBART\u201d backbone in general, and the NED difference on the Multi-Xscience dataset for both models reduces to an insignificant level (p > 0.05)."
        },
        {
            "heading": "6.5 Ablation Study",
            "text": "To investigate the effectiveness of each of our proposed components, we present the average performance gain from the BART baseline by using only a subset of our proposed components, namely the SOD \u201c<s>\u201d token, encoder hierarchical attention (HAE), decoder hierarchical attention (HAD), and position restart (PR). Note that HAE and HAD depend on \u201c<s>\u201d and HAD depends on HAE (Section 4), so they cannot be used along without their dependent components. We show the averaged results across 10 datasets in Table 5 (see full results in Appendix F).\nAs compared to the baseline BART (row 0), simply adding the additional \"<s>\" tokens (row 1) can result in large performance gains. Next, by adding HAE in row 2, we gain some small improvements. Rows 3 and 4 show the gains of adding either HAD or PR on top of row 2. Interestingly, adding HAD or PR separately has little impact, but combining them leads to a significant gain (row 5). This shows that position restart matters more for HAD than for HAE, because it helps HAD to distinguish the different documents, while our HAE encoder already restricts the attention of each source token to the same document and is thus less affected."
        },
        {
            "heading": "7 Conclusion",
            "text": "In this paper, we work on the abstractive multidocument summarization (MDS) task. We propose a hierarchical encoding-decoding scheme, which allows effective fine-tuning of the PLM on a specific MDS task dataset without any new parameters. Our proposed scheme makes novel use of global tokens for document-level interactions in both the encoder and decoder. It can leverage the generalizing capability of PLM across a wide variety of domains. Evaluation results on 10 MDS datasets show that our approach consistently outperforms the previous best models and our PLM backbone.\nLimitations\nTheoretically, our encoding-decoding scheme can reduce the space complexity of MDS summarization from O(( \u2211n=N n=1 nk)\n2) to O((Max(n0, n1, ..., nk)\n2). The former is the square of the total input length from all documents, whereas the latter is simply the square of the longest document. This is because a significant amount of self-attention is no longer calculated due to our restricted intra-document full attention mechanism. However, we have not implemented such optimization in our work as the actual realization is more complicated. As a result, our work also faces the common challenge of computational resources for long document summarization. We leave the investigations for better computational efficiency to future work.\nDue to the inefficient computation for long doc-\nument summarization, we focus on smaller-sized MDS datasets for PLM fine-tuning. We did not conduct experiments on the much larger datasets of WikiSum (Liu et al., 2018), Arxiv and PubMed (Cohan et al., 2018), and GovReport (Huang et al., 2021) as they are much larger in scale and require much more computational resources. Nevertheless, we believe that our encoding-decoding scheme has demonstrated consistent trends of improvement across a wide range of MDS domains.\nEthics Statement\nThis paper involves the preparation of several datasets. For MReD+, we have obtained approval from the ICLR committee to use data collected from the OpenReview7 portal. The authors of MReD (Shen et al., 2022b) already released the data for non-commercialized public usage.\nFor the Wikipedia domain datasets, we pair subportions of the articles organized by WikiSum (Liu et al., 2018) and WikiAsp (Hayashi et al., 2021), which are publicly available datasets."
        },
        {
            "heading": "Acknowledgements",
            "text": "Yang You is being sponsored by NUS startup grant (Presidential Young Professorship), Singapore MOE Tier-1 grant, ByteDance grant, ARCTIC grant, SMI grant and Alibaba grant."
        },
        {
            "heading": "A Datasets Downloads",
            "text": "We download Multinews from https: //github.com/Alex-Fabbri/Multi-News, WCEP from https://github.com/complementizer/ wcep-mds-dataset, Multi-Xsceince from https://github.com/yaolu/Multi-XScience, Rotten Tomatoes from https://web.eecs.umich. edu/~wangluxy/publications.html, MReD from https://github.com/Shen-Chenhui/MReD. For MReD, we use the uncontrolled version of MReD since controllable summarization is not our focus. We will release the processed data of MReD+ and the 4 Wikipedia domain datasets with our code."
        },
        {
            "heading": "B Model Checkpoints",
            "text": "Our \u201cbart-base\u201d checkpoints are downloaded from https://huggingface.co/facebook/bart-base, \u201cbart-large\u201d from https://huggingface.co/ facebook/bart-large, LED from https: //huggingface.co/allenai/led-large-16384, LongT5 from https://huggingface.co/google/ long-t5-tglobal-base, and PRIMERA from https://github.com/allenai/PRIMER.\nIn addition, depending on the model size as specified in Table 2, our \u201cBART\u201d was either initialized from the \u201cbart-base\u201d (139M) or \u201cbart-large\u201d (406M) checkpoints. our \u201cBARTcnn+HED\u201d was initialized from the \u201cbart-large-cnn\u201d checkpoint from https://huggingface.co/facebook/ bart-large-cnn."
        },
        {
            "heading": "C Truncation Settings",
            "text": "We follow Xiao et al. (2022) to use a per-document truncation for Multinews, WCEP, and MultiXscience. This setting is reasonable for Multinews and WCEP in the news domain, because the leading sentences of a news article may account for the overall event and, thus contain more salient information for summarization. Also, for MultiXscience, rather than delving into specific techniques and solutions, the beginning of the abstracts may give the overall description of the task and key ideas, and thus may provide a more suitable background for summarization into the related works sections.\nHowever, the above setting may not be equally reasonable for other domains. For the peer-review domain like MReD, the beginning sentences of reviews often give an abstract of the paper instead of discussing personal opinions that matter more\nto the meta-review. For Rotten Tomatoes which contains mostly single-sentence documents, perdocument truncation may lead to incomprehensible broken sentences. For the Wikipedia domains, the encyclopedia-based information presented may be of equal importance regardless of their passage position. Thus for all these datasets, we follow Shen et al. (2022b) to truncate the end of the combined source."
        },
        {
            "heading": "D Additional Results",
            "text": "We gather the available results reported by other papers and present them from Table 6 to Table 10. For newer datasets such as MReD (Shen et al., 2022b), there have not been results reported except for the original paper. Also, small datasets such as Rotten Tomatoes are also less studied and we could not find very recent results. Naturally, for our newly compiled datasets from four Wikipedia\ndomains, there are no previously reported results. Note that the additionally reported results are meant for a better understanding of the current best performances only, because many of the reported models are not directly comparable to our setting (or comparable with each other) due to different model sizes and experimental settings. For instance, LongT5-large (Guo et al., 2022) uses 780M parameters, and LongT5-xl (Guo et al., 2022) uses 3B parameters, which are not on the same scale as the BART models. Moreover, multi-stage summarization models use multiple PLMs. For instance, REFLECT (Song et al., 2022) uses 2 RoBERTabase and BART-large, whereas UPPER + LED (Tu et al., 2022) uses GPT and LED for the extraction and abstraction stages respectively. Not all the models use the same source truncation either. In one extreme case, PageSum (Liu et al., 2022) uses the full source input with multiple encoding stages. Lastly, certain models didn\u2019t use pre-training data and are trained directly on the MDS dataset, such as Hi-MAP (Fabbri et al., 2019) and HighlightTransformer (Liu et al., 2021).\nIn addition, we also experiment with BigBird (Zaheer et al., 2020) with the same experimental setup in Section 5 on 5 datasets. Unfortunately, we could only find BigBird\u2019s checkpoints for summarization trained on one of the following datasets: ArXiv (Cohan et al., 2018), PubMed (Cohan et al., 2018), or BigPatent (Sharma et al., 2019). We use the Huggingface checkpoint at https://huggingface. co/google/bigbird-pegasus-large-arxiv. As shown by Table 6, 7, 8, 9, and 10, BigBird can still per-\nform reasonably on larger datasets such as Multinews, Multi-Xscience, and WCEP, but it lags behind other more competitive models. However, for small datasets such as Rotten Tomatoes, there is serious performance degradation. Nevertheless, given that ArXiv\u2019s domain is quite close to MReD, BigBird can reach competitive performance on this dataset."
        },
        {
            "heading": "E Human Evaluation",
            "text": "We define 2 evaluation criteria specifically for the MDS setting: salience and coverage. MDS involves distilling important information from multiple input documents. Thus, we use salience and coverage to measure how the summary makes use of the input documents from 2 perspectives.\nFirst, we treat each sentence in the summary as an information unit and count the total number of input documents that support the information units for the whole summary. If the summary includes consensus or opinions commonly agreed on by most input documents, it should have a larger document count as compared to another summary that overly focuses on trivial details or contains extensive hallucinations. We normalize the total count by the total number of sentences in the summary and regard it as the salience of the summary.\nSecond, we measure the minimum number of input documents required to generate the corre-\nsponding summary. The summary should represent all input documents as much as possible, to provide holistic perspectives. We also normalize this count over the total number of sentences in the summary and regard it as the coverage of the summary.\nFor instance, if document #1 contains information A and B, document #2 contains information A and C, document #3 contains information C and D, and the summary contains information A, B, and C in 3 sentences, then the salience of the summary is 1.67, and the coverage is 0.67. For salience, because A is supported by 2 documents, B is supported by 2 documents, and C is supported by 1 document, it is a total of 5 documents divided by 3 summary sentences. For coverage, using documents #1 and #3 alone is sufficient for the summary, and hence it is 2 documents, divided by 3 summary sentences.\nFor each pair of summaries, we first convert the score from each annotator to a head-to-head score, before calculating the final average score between the 2 annotators."
        },
        {
            "heading": "F Ablation",
            "text": "In this section, we show the detailed ablation results for all 10 datasets in Table 12. It can be seen that consistent improvements can be achieved by adding the HierEnc. Next, implementing decoder attention scaling with position restart can make further use of the \u201c<s>\u201d tokens to achieve the best results."
        },
        {
            "heading": "G Cross-Document Standard Deviation",
            "text": "We provide details of the Cross-Document Standard Deviation (CDS) calculation below. First, during the model\u2019s inference stage, we can easily obtain the normalized cross-attention weights of each decoded token toward the source tokens in all attention heads and layers. We average these weights for all attention heads and layers for each individual token, then aggregate the weights for the tokens belonging to the same document. Thus, the cross-attention for the nth document can be calculated as:\nwDn = nK\u2211 k=1 wn,0, ..., wn,k, (7)\nwhere nK is the total number of tokens in the nth document. Next, we normalize the cross-attention for all documents as:\n{ \u02c6wD0 , ... \u02c6wDn} = softmax{wD0 , ..., wDn} (8)\nThen, we can obtain the CDS score for the ith generated token over the source documents by:\nCDSi = Std{ \u02c6wD0 , ... \u02c6wDn} (9)\nNote that for ease of denotation, we do not specify the i value in Equation (7) and Equation (8), but the set of weights {wn,0, ..., wn,k} are specific to the ith token. Finally, we can obtain a single CDS value for each test instance by averaging the CDS values for all generated tokens.\nWe can calculate for both the \u201cBART+HED\u201d and \u201cBART\u201d models over the same test input, then divide the CDS value of \u201cBART+HED\u201d by that of \u201cBART\u201d to obtain the relative CDS value. Our relative CDS value provided Figure 3b is the average of all 200 analyzed examples."
        },
        {
            "heading": "H Content Analysis",
            "text": "Inspired by Laban et al. (2022), we use a natural language entailment (NLI) model8 to evaluate which documents each generated sentence in the summary makes use of.\nSpecifically, the code of Laban et al. (2022) can output a score for the entailment of each sentence towards an input document from a range of -1 to 1, where -1 indicates total contradiction, and 1 indicates perfect entailment. We use a threshold of 0.59, and if one generated sentence has a higher entailment score than the threshold with one document, we consider this sentence relies on the information of that particular document.\nIn this way, for each sentence, we can count the total number of documents entailed. We average this number across all summaries generated by\n8Following Laban et al. (2022), we use the Huggingface checkpoint https://huggingface.co/tals/ albert-xlarge-vitaminc-mnlias our NLI model.\n9Although from our experience with the code, a score as low as 0.2 can correctly identify a correct entailment, we use a higher threshold just to be sure not to mistakenly include any wrong documents.\neach system on the same dataset, and name this metric \u201cNED\u201d. A higher NED number indicates that the summary has on average more entailed documents for each sentence. Thus, it is likely that the summary includes more crucial information that is agreed upon by more documents.\nIn addition, we present the NED using threshold = 0.2 in Table 13. The general trend is similar to using threshold = 0.5 (Table 4), that our method produces summaries with higher NED over most datasets. Moreover, with a lower NED requirement, we can see that the gap between \u201cBART\u201d and \u201cBART+HED\u201d on Multi-Xscience reduces to an insignificant level. This supports our hypothesis that \u201cBART+HED\u201d has lower NED on this particular dataset due to its much more abstractive generations. The NLI model may mistakenly classify the much more abstractive summaries as not entailed to any document, and may not indicate that the salience of \u201cBART+HED\u201d is worse than \u201cBART\u201d."
        }
    ],
    "title": "A Hierarchical Encoding-Decoding Scheme for Abstractive Multi-document Summarization",
    "year": 2023
}