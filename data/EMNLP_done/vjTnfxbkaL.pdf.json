{
    "abstractText": "Aspect-Based Argument Mining (ABAM) is a critical task in computational argumentation. Existing methods have primarily treated ABAM as a nested named entity recognition task, overlooking the need for tailored strategies to effectively address the specific challenges of ABAM tasks. To this end, we propose a layer-based Hierarchical Enhancement Framework (HEF) for ABAM, and introduce three novel components: the Semantic and Syntactic Fusion (SSF) component, the Batchlevel Heterogeneous Graph Attention Network (BHGAT) component, and the Span Mask Interactive Attention (SMIA) component. These components serve the purposes of optimizing underlying representations, detecting argument unit stances, and constraining aspect term recognition boundaries, respectively. By incorporating these components, our framework enables better handling of the challenges and improves the performance and accuracy in argument unit and aspect term recognition. Experiments on multiple datasets and various tasks verify the effectiveness of the proposed framework and components. 1",
    "authors": [
        {
            "affiliations": [],
            "name": "Yujie Fu"
        },
        {
            "affiliations": [],
            "name": "Yang Li"
        },
        {
            "affiliations": [],
            "name": "Suge Wang"
        },
        {
            "affiliations": [],
            "name": "Xiaoli Li"
        },
        {
            "affiliations": [],
            "name": "Deyu Li"
        },
        {
            "affiliations": [],
            "name": "Jian Liao"
        },
        {
            "affiliations": [],
            "name": "JianXing Zheng"
        }
    ],
    "id": "SP:b527ec099313f14895e5719ff4987d054ac51499",
    "references": [
        {
            "authors": [
                "Khalid Al-Khatib",
                "Yufang Hou",
                "Henning Wachsmuth",
                "Charles Jochim",
                "Francesca Bonin",
                "Benno Stein."
            ],
            "title": "End-to-end argumentation knowledge graph construction",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, pages 7367\u2013",
            "year": 2020
        },
        {
            "authors": [
                "Roy Bar-Haim",
                "Lilach Eden",
                "Roni Friedman",
                "Yoav Kantor",
                "Dan Lahav",
                "Noam Slonim."
            ],
            "title": "From arguments to key points: Towards automatic argument summarization",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for",
            "year": 2020
        },
        {
            "authors": [
                "Tuhin Chakrabarty",
                "Christopher Hidey",
                "Smaranda Muresan",
                "Kathy McKeown",
                "Alyssa Hwang."
            ],
            "title": "AMPERSAND: Argument mining for PERSuAsive oNline discussions",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural",
            "year": 2019
        },
        {
            "authors": [
                "Atsushi Fujii",
                "Tetsuya Ishikawa."
            ],
            "title": "A system for summarizing and visualizing arguments in subjective documents: Toward supporting decision making",
            "venue": "Proceedings of the Workshop on Sentiment and Subjectivity in Text, pages 15\u201322.",
            "year": 2006
        },
        {
            "authors": [
                "Debela Gemechu",
                "Chris Reed."
            ],
            "title": "Decompositional argument mining: A general purpose approach for argument graph construction",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages",
            "year": 2019
        },
        {
            "authors": [
                "Qipeng Guo",
                "Xipeng Qiu",
                "Pengfei Liu",
                "Yunfan Shao",
                "Xiangyang Xue",
                "Zheng Zhang."
            ],
            "title": "Startransformer",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language",
            "year": 2019
        },
        {
            "authors": [
                "William L. Hamilton",
                "Rex Ying",
                "Jure Leskovec."
            ],
            "title": "Representation learning on graphs: Methods and applications",
            "venue": "CoRR, abs/1709.05584.",
            "year": 2017
        },
        {
            "authors": [
                "Tatsuki Kuribayashi",
                "Hiroki Ouchi",
                "Naoya Inoue",
                "Paul Reisert",
                "Toshinori Miyoshi",
                "Jun Suzuki",
                "Kentaro Inui."
            ],
            "title": "An empirical study of span representations in argumentation structure parsing",
            "venue": "Proceedings of the 57th Annual Meeting of the",
            "year": 2019
        },
        {
            "authors": [
                "Fei Li",
                "Zheng Wang",
                "Siu Cheung Hui",
                "Lejian Liao",
                "Dandan Song",
                "Jing Xu",
                "Guoxiu He",
                "Meihuizi Jia."
            ],
            "title": "Modularized interaction network for named entity recognition",
            "venue": "Proceedings of the 59th Annual Meeting of the",
            "year": 2021
        },
        {
            "authors": [
                "Jingye Li",
                "Hao Fei",
                "Jiang Liu",
                "Shengqiong Wu",
                "Meishan Zhang",
                "Chong Teng",
                "Donghong Ji",
                "Fei Li."
            ],
            "title": "Unified named entity recognition as wordword relation classification",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, vol-",
            "year": 2022
        },
        {
            "authors": [
                "Hu Linmei",
                "Tianchi Yang",
                "Chuan Shi",
                "Houye Ji",
                "Xiaoli Li."
            ],
            "title": "Heterogeneous graph attention networks for semi-supervised short text classification",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing",
            "year": 2019
        },
        {
            "authors": [
                "Amita Misra",
                "Pranav Anand",
                "Jean E. Fox Tree",
                "Marilyn Walker."
            ],
            "title": "Using summarization to discover argument facets in online idealogical dialog",
            "venue": "Proceedings of the 2015 Conference of the North American Chapter of the Association",
            "year": 2015
        },
        {
            "authors": [
                "Amita Misra",
                "Brian Ecker",
                "Marilyn Walker."
            ],
            "title": "Measuring the similarity of sentential arguments in dialogue",
            "venue": "Proceedings of the 17th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 276\u2013287.",
            "year": 2016
        },
        {
            "authors": [
                "Nils Reimers",
                "Benjamin Schiller",
                "Tilman Beck",
                "Johannes Daxenberger",
                "Christian Stab",
                "Iryna Gurevych."
            ],
            "title": "Classification and clustering of arguments with contextualized word embeddings",
            "venue": "Proceedings of the 57th Annual Meeting of the",
            "year": 2019
        },
        {
            "authors": [
                "Christian Stab",
                "Tristan Miller",
                "Benjamin Schiller",
                "Pranav Rai",
                "Iryna Gurevych."
            ],
            "title": "Cross-topic argument mining from heterogeneous sources",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages",
            "year": 2018
        },
        {
            "authors": [
                "Dietrich Trautmann."
            ],
            "title": "Aspect-based argument mining",
            "venue": "Proceedings of the 7th Workshop on Argument Mining, pages 41\u201352.",
            "year": 2020
        },
        {
            "authors": [
                "Dietrich Trautmann",
                "Johannes Daxenberger",
                "Christian Stab",
                "Hinrich Sch\u00fctze",
                "Iryna Gurevych."
            ],
            "title": "Fine-grained argument unit recognition and classification",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, pages 9048\u20139056.",
            "year": 2020
        },
        {
            "authors": [
                "Petar Veli\u010dkovi\u0107",
                "Guillem Cucurull",
                "Arantxa Casanova",
                "Adriana Romero",
                "Pietro Li\u00f2",
                "Yoshua Bengio."
            ],
            "title": "Graph attention networks",
            "venue": "International Conference on Learning Representations.",
            "year": 2018
        },
        {
            "authors": [
                "Henning Wachsmuth",
                "Benno Stein",
                "Yamen Ajjour."
            ],
            "title": "PageRank\u201d for argument relevance",
            "venue": "Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 1117\u2013",
            "year": 2017
        },
        {
            "authors": [
                "Kai Wang",
                "Weizhou Shen",
                "Yunyi Yang",
                "Xiaojun Quan",
                "Rui Wang."
            ],
            "title": "Relational graph attention network for aspect-based sentiment analysis",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages",
            "year": 2020
        },
        {
            "authors": [
                "Xiao Wang",
                "Houye Ji",
                "Chuan Shi",
                "Bai Wang",
                "Yanfang Ye",
                "Peng Cui",
                "Philip S Yu."
            ],
            "title": "Heterogeneous graph attention network",
            "venue": "Proceedings of the World Wide Web Conference, page 2022\u20132032.",
            "year": 2019
        },
        {
            "authors": [
                "Lu Xu",
                "Zhanming Jie",
                "Wei Lu",
                "Lidong Bing."
            ],
            "title": "Better feature integration for named entity recognition",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language",
            "year": 2021
        },
        {
            "authors": [
                "Hang Yan",
                "Tao Gui",
                "Junqi Dai",
                "Qipeng Guo",
                "Zheng Zhang",
                "Xipeng Qiu."
            ],
            "title": "A unified generative framework for various NER subtasks",
            "venue": "Proceedings of the 59th Annual Meeting of the",
            "year": 2021
        },
        {
            "authors": [
                "Hang Yan",
                "Yu Sun",
                "Xiaonan Li",
                "Xipeng Qiu."
            ],
            "title": "An embarrassingly easy but strong baseline for nested named entity recognition",
            "venue": "arXiv preprint arXiv:2208.04534.",
            "year": 2022
        },
        {
            "authors": [
                "Liang Yao",
                "Chengsheng Mao",
                "Yuan Luo."
            ],
            "title": "Graph convolutional networks for text classification",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 7370\u20137377.",
            "year": 2019
        },
        {
            "authors": [
                "Chen Zhang",
                "Qiuchi Li",
                "Dawei Song."
            ],
            "title": "Aspect-based sentiment classification with aspect-specific graph convolutional networks",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and",
            "year": 2019
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Argument mining, a critical task within computational argumentation, has gained considerable attention, evident from available datasets (Stab et al., 2018; Trautmann et al., 2020), emerging tasks (Wachsmuth et al., 2017; Al-Khatib et al., 2020), and machine learning models associated to this domain (Kuribayashi et al., 2019; Chakrabarty et al., 2019). As aspect-level sentiment analysis tasks have flourished, Aspect-Based Argument Mining (ABAM) has recognized the need to decompose argument units into smaller attributes and define aspect terms as components with specific meanings in arguments (Trautmann, 2020).\n\u2217Corresponding author. Email: wsg@sxu.edu.cn. 1Codes available at https://github.com/sxu-fyj/ABAM.\nPrevious works have attempted to combine aspects and arguments, but they often lack a clear definition of the relevant task. For instance, Fujii and Ishikawa (2006) primarily focuses on summarizing viewpoints and defining auguring points. Similarly, Misra et al. (2015) further groups the arguments under discussion into aspects of the argument. Furthermore, Gemechu and Reed (2019) considers aspects in argument relations as part of four functional components. Only recently, a study by Trautmann (2020) specifically addresses aspect term extraction and emphasizes the definition, introducing the concept of Aspect-Based Argument Mining (ABAM). The main objective of ABAM is to identify the argument units that support corresponding stances under a controversial topic, along with the aspect terms mentioned within these argument units. In this context, an argument unit is typically defined as a short text or span, providing evidence or reasoning about the topic, supporting or opposing it (Stab et al., 2018). On the other hand, an aspect term is defined as the crucial facet/part the argument unit is trying to address, representing a core aspect of the argument (Trautmann, 2020).\nFigure 1 illustrates four examples within the topic of nuclear energy and school uniforms. For the first set of examples (example 1 and example 2), argument unit (yellow or blue) opinion are expressed around aspect terms (italics framed in red), such as cost, nuclear plant, and maintenance. Similarly, the second set of examples (example 3 and example 4) revolves around several aspect terms such as students, and bullying. This targeted approach enables a more precise analysis by zooming in on specific aspects that are essential to the argument unit. Moreover, these aspect terms enable the comparison of support or opposing opinions at the aspect level, thereby facilitating the acquisition of more nuanced and fine-grained conclusions.\nABAM, as defined by Trautmann (2020), is treated as a Nested Named Entity Recognition (NNER) task which presents three key challenges: 1) How to construct a robust underlying representation to effectively encode contextual information? In the realm of Natural Language Processing (NLP), the significance of a robust underlying representation serves as a cornerstone for achieving excellent model performance. 2) How to mine the correlation between opinion expressions corresponding to different stances under the same topic? Since different users may give different expressions of viewpoints on same stances within a given topic. As shown in Figure 1, authors express different opinions around similar aspect terms under the same topic. Exploring the relationship between these opinion expressions can greatly assist in determining the corresponding stances of different argument units accurately. 3) How to leverage task-specific features to improve the extraction of argument units and aspect terms? By investigating the unique task characteristics and data properties of the ABAM task, we aim to enhance the model\u2019s performance significantly.\nOverall, we propose a novel Hierarchical Enhancement Framework (HEF), consisting of four modules: basic module, argument unit enhancement module, aspect term enhancement module, and decision module. With regard to the three challenges above, the paper presents three key components accordingly. In the basic module, we propose the Semantic and Syntactic Fusion (SSF) component to fine-tune the representation of the pretrained language model. This fine-tuning helps us complete the initial recognition stage of argument units and aspect terms. Next, in the argument unit\nenhancement module, we leverage the argument unit boundary information provided by the basic module. By integrating a span-based method and utilizing the proposed Batch-Level Heterogeneous Graph Attention Network (BHGAT) component, we are able to judge the stance of the argument unit, thereby refining the categorization of the initially recognized argument units. Moving on to the aspect term enhancement module, we introduce the Span Mask Interactive Attention (SMIA) component. By incorporating span masks and interactive guidance through attention mechanisms, we can better capture and identify aspect terms within the specified boundaries. Finally, in the decision module, we combine the initial recognition results with the enhancement results to produce the final output. Our contribution can be summarized as follows:\n\u2022 We propose a general SSF component to enhancing underlying representations, which can simultaneously capture both semantic and syntactic information.\n\u2022 We propose a novel framework BHGAT, which can seamlessly integrates the strengths of two distinct types of Graph Neural Networks (GNNs) within a batch-level setting.\n\u2022 We propose a task-special SMIA component, which used to effectively constrain the recognition range of aspect terms.\n\u2022 Experiments on multiple datasets and various tasks verify the effectiveness of the proposed framework and components."
        },
        {
            "heading": "2 Related Work",
            "text": "The objective of Misra et al. (2015) is to identify specific arguments and counter-arguments in social media texts, categorize them into different aspects, and utilize this aspect information to generate argument summaries. Similarly, Misra et al. (2016) focus on inducing and identifying argument aspects across multiple conversations, ranking the extracted arguments based on their similarity, and generating corresponding summaries. However, these earlier works have been limited to a few specific topics. In recent research, the approach has been extended to cover a broader range of 28 topics, introducing a novel corpus for aspect-based argument clustering (Reimers et al., 2019). Furthermore, Gemechu and Reed (2019) decompose propositions into four functional components: aspects, target concepts, and opinions on aspects and target concepts. By leveraging the relationships\namong these components, they infer argument relations and gain a deeper understanding of the argument structure. In a different study, Bar-Haim et al. (2020) focus on summarizing the arguments, supporting each side of a debate, mapping them to a concise list of key points, which are similar to the aspect terms highlighted earlier. Lastly, Trautmann (2020) redefines the aspect-based argument mining task based on clause-level argument unit recognition and classification in heterogeneous document collections (Trautmann et al., 2020)."
        },
        {
            "heading": "3 Framework",
            "text": "This paper proposes a HEF for aspect-based argument mining task, which consists of four modules: basic module, argument unit enhancement module, aspect term enhancement module and decision module. The architecture of the HEF is visually depicted in figure 2."
        },
        {
            "heading": "3.1 Task Definition",
            "text": "Following Trautmann (2020), we formulate the ABAM task as a nested named entity recognition task with a two-level nested structure. Given an input sentence of n tokens W text = [wtext1 , w text 2 , ..., w text n ], the topic is W\ntopic = [wtopic1 , ..., w topic m ], the target argument unit (AU) label sequence is Y AU = [yAU1 , y AU 2 , ..., y AU n ] and the target aspect term (AT) label sequence is Y AT = [yAT1 , y AT 2 , ..., y AT n ], where y\nAU \u2208 {Bcon, Icon, Econ, Bpro, Ipro, Epro, O} and yAT \u2208 {Basp, Iasp, Easp, O}."
        },
        {
            "heading": "3.2 Basic Module",
            "text": "The sentence and topic are concatenated as the input to BERT: [CLS], wtext1 , ..., w text n , [SEP], wtopic1 , ..., w topic m , [SEP], where [CLS] and [SEP] are special tokens. The contextualized representations of each token X = [xw1 , x w 2 , ..., x w n ] can be given as: xwt = BERT(w text t ) (1)\nNote we also incorporate orthographic and morphological features of words by combining character representations (Xu et al., 2021). The characters representation with in wtexti as w char i . Then we use LSTM to learn the final hidden state xct as the character representation of wtexti :\nxct = LSTM(w char t ) (2)\nThe final token representation is obtained as follows:\nxt = [x w t ;x c t ;x p t ] (3)\nwhere [; ] denotes concatenation, and xpt is the partof-speech tagging of wtextt .\nEncoder. The LSTM is widely used for capturing sequential information in either the forward or backward direction. However, it faces challenges when dealing with excessively long sentences, as it may struggle to retain long-distance dependencies between words. To address this limitation and exploit syntactic information, we propose the Semantic and Syntactic Fusion (SSF) component by sentence-level GNNs, aiming to bridge the gap between distant words by effectively encoding both sequential semantic information and spatial syntactic information.\nThe input of SSF: previous cell state ct\u22121, previous hidden state ht\u22121, current cell input xt, and an additional graph-encoded representation gt, where c0 and h0 are initialized to zero vectors, g (l) t is a graph-encoded representation generated using Graph Attention Network (GAT), which are capable of bringing in structured information through graph structure (Hamilton et al., 2017).\nThe hidden state ht of SSF are computed as follows:\nft = \u03c3(W (f)xt + U (f)ht\u22121 + b (f)) (4)\nit = \u03c3(W (i)xt + U (i)ht\u22121 + b (i)) (5)\not = \u03c3(W (o)xt + U (o)ht\u22121 + b (o)) (6)\nc\u0303t = tanh(W (c)xt + U (c)ht\u22121 + b (c)) (7)\nct = ft \u2217 ct\u22121 + it \u2217 c\u0303t (8)\nmt = \u03c3(W (m)xt + U (m)ht\u22121 +Q (m)g (l) t + b (m)) (9)\nst = tanh(W (s)xt + U (s)ht\u22121 +Q (s)g (l) t + b (s)) (10)\nht = ot \u2217 tanh(ct) +mt \u2217 tanh(st) (11)\nwhere, ft, it, ot, ct are equivalent to the forget gate, input gate, output gate, and cell unit in the traditional LSTM respectively, mt and st are used to control the information flow of g(l)t . Finally ht is the output of SSF.\nStar-Transformer (Guo et al., 2019) can measure the position information more explicitly and is more sensitive to the order of the input sequence. Building upon this insight, we use the output of SSF component as the input of Star-Transformer to re-encode the context to complete the encoder part.\nhstart = Star-Transformer(ht) (12)\nDecoder. CRF has been widely used in NER task (Xu et al., 2021; Li et al., 2021). For an input sentence, the probability scores zAUt and z AT t for all tokens xi \u2208 X over the argument unit tags and aspect term tags are calculated by CRF decoder:\nzAUt = p(y AU t |hstart ) = CRF (WAUhstart + bAU ) (13)\nzATt = p(y AT t |hstart ) = CRF (WAThstart + bAT ) (14)"
        },
        {
            "heading": "3.3 Argument Unit Enhancement Module",
            "text": "Motivated by span-based methods, we utilize argument unit labels zAUt that predicted by the basic module to obtain boundary information of argument unit spans, and re-evaluating the stance of each span, thereby correcting the zAUt labels, which is argument unit enhancement module (AUE). We observe that different users often express similar opinions, when discussing similar aspect terms. Learning these similar opinion expressions can assist in distinguishing the corresponding stances of argument units. Furthermore, pre-trained language models are widely adopted due to their ability to generate robust contextual representations. However, different contexts can yield different representations for the same word. Exploring the correlation between different context representations of the same word can aid in optimizing the underlying representations. To this end, we propose the Batch-level Heterogeneous Graph Attention Network (BHGAT) component. BHGAT combines\nthe strengths of sentence-level GNNs (Zhang et al., 2019; Wang et al., 2020) and corpus-level GNNs (Wang et al., 2019; Yao et al., 2019; Linmei et al., 2019). While utilizing the deep contextual word representations generated by pre-trained language models, BHGAT facilitates communication of opinion expressions among different samples and establishes correlations between different representations of the same word. This enables the optimization of the representations of various heterogeneous nodes within the graph. Constructing a Graph Neural Network involves defining an initial representation for each node, an adjacency matrix, and a node update method.\nNode initialization. In our proposed BHGAT, we distinguish between two types of nodes: argument unit nodes haui and word nodes h star t . The specific operation is as follows:\nhaui = [h star starti ;h star endi ; aui(topic)] (15)\nwhere hstarstarti and h star endi are the starting and ending word representation of i-th argument unit aui, and aui(topic) is the topic of aui.\nHG(0) = [hau1 , ..., h au nau , h star 1 , ..., h star nw ] (16)\nwhere HG(0) is initial representation of nodes in BHGAT, nau is the number of argument units, and nw is the number of words.\nAdjacency matrix. The adjacency matrix A of BHGAT includes 4 parts: [au, au] part, [au, word]\npart, [word, au] part, [word, word] part.\nA =\n[ [au, au] [au,word]\n[word, au] [word,word]\n] (17)\nThe (aui, auj) captures the relationships between argument units within a batch, facilitating communication and understanding among argument units that share the same topic, and allowing us to learn similar opinion expressions:\n(aui, auj) =\n{ 1, if aui(topic) = auj(topic)\n0, otherwise (18)\nThe (aui, wordj) represents the association between argument units and words, using the attention mechanism between nodes to complete the update of argument unit nodes and word nodes, which is:\n(aui, wordj) = { 1, if wordj in aui 0, otherwise\n(19)\nSimilarly, the (wordi, auj) is denoted as:\n(wordi, auj) = { 1, if wordi in auj 0, otherwise\n(20)\nThe (wordi, wordj) focuses on node representations of the same word in different contexts. This part facilitates the information interaction of word nodes between different argument units, which is conducive to optimize the dynamic representations of the underlying words.\n(wordi, wordj) = { 1, if wordi = wordj in different AU 0, otherwise\n(21)\nFurthermore, the diagonal values of A are all ones.\nNode update. We adopt the method of information aggregation to complete node updates, similar to GAT (Velic\u030ckovic\u0301 et al., 2018). The specific operation is as follows:\n\u03b1ij = exp(LeakyReLU(aT [Whgi\u2016Whgj ]))\u2211\nk\u2208Ni exp(LeakyReLU(a T [Whgi\u2016Whgj ]))\n(22)\nhg (l+1) i = \u03c3(\n1\nK K\u2211 k=1 \u2211 j\u2208Ni \u03b1kijW khg (l) j ) (23)\nwhere hg(l)i is the representation of nodes for l-th layer.\nFinally, we perform stance classification on the representations of argument unit nodes. The probability paui of different stance belonging to aui is calculated as follows:\npaui = softmax(W hghg (l) i +b hg), i \u2208 {1, ..., nau} (24)\nwhere paui = [p con aui , p pro aui , p non aui ] is the stance probability distribution of the argument unit aui. Through BHGAT, we first obtain the stance classes of the corresponding argument units. Then, we map the obtained stance classes to the enhanced argument unit recognition space, resulting in the vector zAUEt . z AUE t can be divided into two parts: the boundary part and the stance part. In the mapping process, first, according to the boundary of the argument unit label zAUt , it is judged whether the boundary of zAUEt corresponding to the tokent is B-*, I-*, E-* or O. Then according to pconsi , p pro si , pnonsi , we determine the stance part in z AUE t (*-con, *-pro, O) as follows:\nzAUEt =  [pconaui , 0, 0, p pro aui , 0, 0, p non aui ], t = saui [0, pconaui , 0, 0, p pro aui , 0, p non aui ], saui < t < eaui [0, 0, pconaui , 0, 0, p pro aui , p non aui ], t = eaui\n[0, 0, 0, 0, 0, 0, 1], otherwise (25)\nwhere saui is the start position of argument unit aui and eaui is the end position."
        },
        {
            "heading": "3.4 Aspect Term Enhancement Module",
            "text": "To enhance the label sequence results zATt of initially identified aspect terms, we introduce the Aspect Term Enhancement (ATE) module. Since an aspect term is specific to a corresponding argument unit, it is essential to establish a component that constrains the recognition range of aspect terms within the text. Building upon this concept, we propose the Span Mask Interactive Attention (SMIA) component, which ensures that the attention mechanism focuses on the relevant argument unit spans while effectively disregarding irrelevant text. The overall process can be formulated as:\nHmask = Att(HstarWQ, HstarWK , HstarWV ) (26)\nAtt(Q,K, V ) = softmax(MASK + QKT\u221a dk )\u00b7V (27)\nmaskt =\n{ \u2212\u221e, yAUt = O\n0, yAUt 6= O (28)\nOnce we obtain the new context representation, we proceed to feed it into the decoder model, which\nis responsible for generating the aspect term enhanced label sequence zATEt .\nzATEt = p(y AT t |hmaskt ) = CRF (WATEhmaskt + bATE)\n(29)"
        },
        {
            "heading": "3.5 Decision Module",
            "text": "Through the AUE and ATE module, we can obtain the enhanced argument unit label probability zAUEt and the enhanced aspect term label probability zATEt . Finally, we fuse the probabilities in the two label spaces (initial, enhanced) as the final output.\nz\u0303ATt = z ATE t + z AT t (30)\nz\u0303AUt = z AUE t + z AU t (31)"
        },
        {
            "heading": "3.6 Objective Function",
            "text": "The first part aims to minimize two negative logprobability of the correct sequence of labels in basic module.\nLNER = \u2212 T\u2211\nt=1\nlog(p(yATt |zATt ))\u2212 T\u2211\nt=1\nlog(p(yAUt |zAUt ))\n(32)\nwhere zAUt and z AT t represent the predicted sequence, yAUt and y AT t represent the correct sequence. The second part loss is the cross-entropy loss for stance classification of argument unit span in AUE module, denoted as:\nLAUE = \u2212 n\u2211\ni=1 m\u2211 j=1 y stancej aui log(p stancej aui ) (33)\nwhere n is the number of argument units, and m is the number of stance classes.\nSimilar to the first part, the third part uses the negative log-likelihood loss in ATE module.\nLATE = \u2212 T\u2211 t=1 log(p(yATt |zATEt )) (34)\nFinally, the fourth part also aim to minimize negative log-likelihood for enhanced label probability distribution.\nLE = \u2212 T\u2211\nt=1\nlog(p(yATt |z\u0303ATt ))\u2212 T\u2211\nt=1\nlog(p(yAUt |z\u0303AUt ))\n(35) The final loss function is defined as follows:\nL = LNER + LAUE + LATE + LE (36)"
        },
        {
            "heading": "4 Experiments",
            "text": "To evaluate the effectiveness of HEF framework and the corresponding components, we conducted experiments on four datasets."
        },
        {
            "heading": "4.1 Datasets",
            "text": "ABAM2. We employ the latest ABAM dataset, which was released in 2020 and comprises 8 topics (Trautmann, 2020). The statistics are presented in the table 1. We followed the inner dataset split (2268 / 307 / 636 for train / dev / test) defined in the ABAM corpus (Trautmann, 2020).\nAURC-83. The argument unit recognition and classification (AURC) dataset published in 2020, consists of 8 topics (Trautmann et al., 2020). The statistics are shown in the table 2. We used the inner dataset split (4000 / 800 / 2000 for train / dev / test) given by Trautmann et al. (2020).\nSemEval-2016 Task 6A4. The dataset has been divided into training and test set for each of the five claims. Each sample can be classified three categories: against, none and favor.\nABAM argument segment5. The dataset is a collection of argument units in ABAM. Each argument unit can be classified into two categories: PRO and CON.\nTable 3 shows the distribution of SemEval-2016 Task 6A and ABAM argument segment.\n2https://github.com/trtm/ABAM 3https://github.com/trtm/AURC 4https://github.com/sxu-fyj/stance 5https://github.com/trtm/ABAM"
        },
        {
            "heading": "4.2 The experimental setup",
            "text": "Evaluation Metrics: For different tasks, we provide specific evaluation metrics to assess the performance. For aspect-based argument mining task, we conduct model evaluation using the ABAM dataset at the segment-level and token-level. In segment-level evaluation, we consider a prediction as correct only if the model correctly identifies the boundary and category of an entity. We use exact matching F16 to measure the accuracy. At the token-level, we proposed two evaluation methods: Token-Nested evaluation and Token-Flat evaluation. In the Token-Nested evaluation, we extend the stance labels by incorporating aspect information, resulting in six possible combinations: [NON, O], [NON, ASP], [PRO, O], [PRO, ASP], [CON, O], and [CON, ASP]. We report both Macro-F1 and Micro-F1 scores for this evaluation. In TokenFlat evaluation, we concatenate the sequence labels of aspect term recognition and argument unit recognition to generate a label sequence twice the sentence length, containing four label categories: [ASP, PRO, CON, O]. We report both Macro-F1 and Micro-F1 scores for this evaluation. For the argument unit recognition and classification task, we employ the segment-level evaluation metric on the AURC-8 dataset. The Macro-F1, Micro-F1 and separate F1 scores for each category are provided. Finally, for the stance detection task, we report the Macro-F1 score, Micro-F1 score, and F1 scores on the SemEval-2016 Task 6A and ABAM argument segment datasets.\nCompared Models: We compare the HEF framework with the following state-of-the-art methods:\n\u2022 CNN-NER (Yan et al., 2022) utilizes a convolutional neural network to capture the interactions among neighboring entity spans.\n\u2022 W2NER (Li et al., 2022) introduces a novel approach to tackle named NER by framing it as a word-word relation classification problem.\n6https://github.com/chakki-works/seqeval\n\u2022 Span, BPE, Word (Yan et al., 2021) present a new formulation of the NER task as an entityspan sequence generation problem."
        },
        {
            "heading": "4.3 Experimental results and analysis",
            "text": ""
        },
        {
            "heading": "4.3.1 Comparison results of different methods for ABAM",
            "text": "We show the performance comparison of different methods in table 4. All comparison methods use the code provided by their respective original papers, and undergo testing and evaluation on the ABAM dataset.\nBased on the results presented in Table 4, our proposed method demonstrates superior performance compared to all the existing methods, both in segment-level and token-level evaluation metrics. This improvement can be attributed to the inclusion of two key modules: Argument Unit Enhancement (AUE) and Aspect Term Enhancement (ATE). Specifically, our method shows substantial improvements in MicF1, MacF1, Token-Flat, and Token-Nested evaluation metrics, with gains of at least 0.0767, 0.1274, 0.0647, and 0.0745, respectively, compared to the other comparison methods. The ATE module effectively constrains the recognition range of aspect terms, leading to a significant improvement in the F1 score for aspect term recognition (ASP column)."
        },
        {
            "heading": "4.3.2 Ablation experiments for ABAM",
            "text": "To evaluate the individual impact of each functional module or component within the HEF framework on model performance, we conduct a series of ablation experiments. The results of these experiments are presented in Table 5. -w/o topic, -w/o SSF, -w/o Star-Transformer, -w/o AUE and -w/o ATE represents our model after removing the topic in BERT, SSF, Star-Transformer, AUE module, and ATE module, respectively.\nThe experimental results in the table above clearly demonstrate that the removal of different modules or components has a significant impact on the performance of the HEF framework. In particular, the absence of the AUE module has a substantial negative effect on overall performance. The utilization of BHGAT to re-judge the category of argument unit spans has proven to be an effective strategy for correcting samples in the basic module where the boundary is correctly identified but the category judgment is incorrect. Moreover, the inclusion of the topic information in BERT\ncontributes significantly to the framework\u2019s performance. The SSF and Star-Transformer components also play crucial roles in generating highquality underlying representations. The absence of these components has a detrimental impact on the model\u2019s performance. Lastly, due to aspect terms only exist within the argument unit spans. The addition of the ATE module improves the accuracy of aspect term range extraction."
        },
        {
            "heading": "4.3.3 Effectiveness of SSF component",
            "text": "To evaluate the effectiveness of SSF component, we conducted experiments on two sequence labeling datasets, AURC-8 and ABAM. For the sequence labeling task, we use LSTM and SSF as the encoder and CRF as the decoder. The experimental results are presented in Table 6.\nBased on the experimental results in Table 6, we observe the scores on the ABAM dataset are significantly higher compared to those in the AURC-8 dataset. This discrepancy can be attributed to the inherent dissimilarities between the two datasets. In the AURC-8 dataset, argument units may not exist in every sample, while the ABAM dataset ensures the presence of at least one argument unit in each sample. By integrating spatial syntactic information with the LSTM-encoded sequential semantic information, the SSF component demonstrates a clear performance advantage, leading to significant improvements on both datasets."
        },
        {
            "heading": "4.3.4 Effectiveness of BHGAT component",
            "text": "To comprehensively assess the superiority of this component, we conduct experiments on two datasets: SemEval 2016 Task 6A and ABAM ar-\ngument segment datasets. In our experiments, we incorporate the BHGAT component into the BERTbased framework and compare the experimental results, as shown in Table 7.\nTable 7 clearly demonstrate that the inclusion of the BHGAT component has resulted in significant performance improvements. This improvement can be attributed to several key factors. Firstly, the BHGAT component has the ability to capture and leverage information from multiple samples that share the same topic. By considering the expressions of different stances and the embedding representations of words in different contexts, the BHGAT component enhances the model\u2019s discriminative power and facilitates more accurate stance detection. Furthermore, the versatility of the BHGAT component is noteworthy. It can be seamlessly integrated into various frameworks, enabling performance enhancements across different models. This flexibility makes the BHGAT component highly adaptable, particularly in classification tasks that involve topic information, such as stance detection."
        },
        {
            "heading": "4.3.5 Effectiveness of SMIA component",
            "text": "The SMIA component introduced in the ATE module aims to restrict the range of aspect term recognition. To assess its effectiveness, we present the confusion matrix based on the Token-Nested evaluation index in figure 3.\nIn figure 3, the confusion matrix is presented with a dimension of 6x6. It is important to note that in real data, there is no combination of the aspect label ASP and the stance label NON, as aspect terms only exist within argument units. As a result, the\nfifth row of the confusion matrix is always zero. In the confusion matrix, we focus on the fifth column of each confusion matrix. The model identifies aspect terms and argument units separately, during the prediction process, if the prediction range of term is not constrained, it may generate a wrong match between the aspect term label ASP and the stance label NON. However, by observing the fifth column of each confusion matrix, we can observe a significant reduction in misjudgment after adding the span mask constraints imposed by the SMIA component. This outcome reinforces the effectiveness of the SMIA component in constraining the recognition of aspect terms."
        },
        {
            "heading": "5 Conclusion",
            "text": "This paper presents a novel layer-based approach for the aspect-based argument mining task, utiliz-\ning a hierarchical enhancement framework consisting of four modules: basic module, argument unit enhancement module, aspect term enhancement module, and decision module. The SSF component plays a crucial role in optimizing underlying representations, which can be utilized across various tasks. It enhances the framework\u2019s capability by incorporating syntactic information into the encoding process, improving performance on sequence labeling tasks. The BHGAT component, effective for classification tasks involving topic information, enhances the framework\u2019s generalization capabilities. The SMIA component is specifically designed for aspect-based argument mining tasks, aiming to constrain the recognition range of aspect terms. It effectively improves the accuracy of aspect term recognition and contributes to the overall performance of the framework.\nLimitations\nHowever, it should be noted that the proposed BHGAT is currently only suitable for classification tasks with topic information. Its generalization to more general tasks needs further investigation in our future work. In addition, our current framework has primarily focused on adopting a layerbased method for Nested Named Entity Recognition (NNER), without extensively exploring how to mine the correlation between argument units and aspect terms. In future work, it is essential to delve deeper into the correlation between these two entities and fully utilize the guiding information between them."
        },
        {
            "heading": "Acknowledgements",
            "text": "The authors would like to thank all anonymous reviewers for their valuable comments and suggestions which have significantly improved the quality and presentation of this paper. The works described in this paper are supported by the National Key Research and Development Program of China (2022QY0300-01), National Natural Science Foundation of China (62106130, 62076158, 62072294, 62272286), Natural Science Foundation of Shanxi Province, China (20210302124084), Scientific and Technological Innovation Programs of Higher Education Institutions in Shanxi, China (2021L284), and CCF-Zhipu AI Large Model Foundation of China (CCF-Zhipu202310)."
        }
    ],
    "title": "Hierarchical Enhancement Framework for Aspect-based Argument Mining",
    "year": 2023
}