{
    "abstractText": "When we transfer a pretrained language model to a new language, there are many axes of variation that change at once. To disentangle the impact of different factors like syntactic similarity and vocabulary similarity, we propose a set of controlled transfer studies: we systematically transform the language of the GLUE benchmark, altering one axis of crosslingual variation at a time, and then measure the resulting drops in a pretrained model\u2019s downstream performance. We find that models can largely recover from syntactic-style shifts, but cannot recover from vocabulary misalignment and embedding matrix re-initialization, even with continued pretraining on 15 million tokens. Moreover, good-quality tokenizers in the transfer language do not make vocabulary alignment easier. Our experiments provide insights into the factors of cross-lingual transfer that researchers should most focus on when designing language transfer scenarios.",
    "authors": [
        {
            "affiliations": [],
            "name": "Zhengxuan Wu"
        },
        {
            "affiliations": [],
            "name": "Alex Tamkin"
        },
        {
            "affiliations": [],
            "name": "Isabel Papadimitriou"
        }
    ],
    "id": "SP:9cb45542c04cbcbf20c3d4736dee9f727e81bff6",
    "references": [
        {
            "authors": [
                "Mostafa Abdou",
                "Vinit Ravishankar",
                "Artur Kulmizev",
                "Anders S\u00f8gaard."
            ],
            "title": "Word order does matter and shuffled language models know it",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),",
            "year": 2022
        },
        {
            "authors": [
                "Mikel Artetxe",
                "Gorka Labaka",
                "Eneko Agirre."
            ],
            "title": "Generalizing and improving bilingual word embedding mappings with a multi-step framework of linear transformations",
            "venue": "Thirty-second AAAI conference on artificial intelligence.",
            "year": 2018
        },
        {
            "authors": [
                "Mikel Artetxe",
                "Sebastian Ruder",
                "Dani Yogatama."
            ],
            "title": "On the cross-lingual transferability of monolingual representations",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4623\u20134637.",
            "year": 2020
        },
        {
            "authors": [
                "Alexandra Chronopoulou",
                "Dario Stojanovski",
                "Alexander Fraser."
            ],
            "title": "Reusing a pretrained language model on languages with limited corpora for unsupervised nmt",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language",
            "year": 2020
        },
        {
            "authors": [
                "Kevin Clark",
                "Minh-Thang Luong",
                "Quoc V Le",
                "Christopher D Manning."
            ],
            "title": "Electra: Pre-training text encoders as discriminators rather than generators",
            "venue": "arXiv preprint arXiv:2003.10555.",
            "year": 2020
        },
        {
            "authors": [
                "Ameet Deshpande",
                "Partha Talukdar",
                "Karthik Narasimhan."
            ],
            "title": "When is BERT multilingual? isolating crucial ingredients for cross-lingual transfer",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computa-",
            "year": 2022
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "arXiv preprint arXiv:1810.04805.",
            "year": 2018
        },
        {
            "authors": [
                "Philipp Dufter",
                "Hinrich Sch\u00fctze."
            ],
            "title": "Identifying necessary elements for bert\u2019s multilinguality",
            "venue": "arXiv preprint arXiv:2005.00396.",
            "year": 2020
        },
        {
            "authors": [
                "Merrill F Garrett."
            ],
            "title": "Syntactic processes in sentence production",
            "venue": "New approaches to language mechanisms, 30:231\u2013256.",
            "year": 1976
        },
        {
            "authors": [
                "Evangelia Gogoulou",
                "Ariel Ekgren",
                "Tim Isbister",
                "Magnus Sahlgren."
            ],
            "title": "Cross-lingual transfer of monolingual models",
            "venue": "arXiv preprint arXiv:2109.07348.",
            "year": 2021
        },
        {
            "authors": [
                "Karthikeyan K",
                "Zihan Wang",
                "Stephen Mayhew",
                "Dan Roth."
            ],
            "title": "Cross-lingual ability of multilingual bert: An empirical study",
            "venue": "International Conference on Learning Representations.",
            "year": 2020
        },
        {
            "authors": [
                "Taku Kudo",
                "John Richardson."
            ],
            "title": "Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing",
            "venue": "arXiv preprint arXiv:1808.06226.",
            "year": 2018
        },
        {
            "authors": [
                "Alexandre Lacoste",
                "Alexandra Luccioni",
                "Victor Schmidt",
                "Thomas Dandres."
            ],
            "title": "Quantifying the carbon emissions of machine learning",
            "venue": "arXiv preprint arXiv:1910.09700.",
            "year": 2019
        },
        {
            "authors": [
                "Zhenzhong Lan",
                "Mingda Chen",
                "Sebastian Goodman",
                "Kevin Gimpel",
                "Piyush Sharma",
                "Radu Soricut."
            ],
            "title": "Albert: A lite bert for self-supervised learning of language representations",
            "venue": "arXiv preprint arXiv:1909.11942.",
            "year": 2019
        },
        {
            "authors": [
                "Hang Le",
                "Lo\u00efc Vial",
                "Jibril Frej",
                "Vincent Segonne",
                "Maximin Coavoux",
                "Benjamin Lecouteux",
                "Alexandre Allauzen",
                "Benoit Crabb\u00e9",
                "Laurent Besacier",
                "Didier Schwab."
            ],
            "title": "Flaubert: Unsupervised language model pre-training for french",
            "venue": "Proceedings of the",
            "year": 2020
        },
        {
            "authors": [
                "Davis Liang",
                "Hila Gonen",
                "Yuning Mao",
                "Rui Hou",
                "Naman Goyal",
                "Marjan Ghazvininejad",
                "Luke Zettlemoyer",
                "Madian Khabsa."
            ],
            "title": "Xlm-v: Overcoming the vocabulary bottleneck in multilingual masked language models",
            "venue": "arXiv preprint arXiv:2301.10472.",
            "year": 2023
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "ROBERTa: A robustly optimized BERT pretraining approach",
            "venue": "arXiv preprint arXiv:1907.11692.",
            "year": 2019
        },
        {
            "authors": [
                "Stephen Merity",
                "Caiming Xiong",
                "James Bradbury",
                "Richard Socher."
            ],
            "title": "Pointer sentinel mixture models",
            "venue": "arXiv preprint arXiv:1609.07843.",
            "year": 2016
        },
        {
            "authors": [
                "Benjamin Muller",
                "Antonios Anastasopoulos",
                "Beno\u00eet Sagot",
                "Djam\u00e9 Seddah."
            ],
            "title": "When being unseen from mbert is just the beginning: Handling new languages with multilingual language models",
            "venue": "NAACL-HLT 2021-2021 Conference of the North",
            "year": 2021
        },
        {
            "authors": [
                "Kelechi Ogueji",
                "Yuxin Zhu",
                "Jimmy Lin."
            ],
            "title": "Small data? no problem! exploring the viability of pretrained multilingual language models for lowresourced languages",
            "venue": "Proceedings of the 1st Workshop on Multilingual Representation Learning, pages",
            "year": 2021
        },
        {
            "authors": [
                "Tolulope Ogunremi",
                "Dan Jurafsky",
                "Christopher Manning."
            ],
            "title": "Mini but mighty: Efficient multilingual pretraining with linguistically-informed data selection",
            "venue": "Findings of the Association for Computational Linguistics: EACL 2023, pages 1251\u20131266,",
            "year": 2023
        },
        {
            "authors": [
                "Akintunde Oladipo",
                "Odunayo Ogundepo",
                "Kelechi Ogueji",
                "Jimmy Lin."
            ],
            "title": "An exploration of vocabulary size and transfer effects in multilingual language models for african languages",
            "venue": "3rd Workshop on African Natural Language Processing.",
            "year": 2022
        },
        {
            "authors": [
                "Pedro Javier Ortiz Su\u00e1rez",
                "Beno\u00eet Sagot",
                "Laurent Romary."
            ],
            "title": "Asynchronous pipelines for processing huge corpora on medium to low resource infrastructures",
            "venue": "Proceedings of the Workshop on Challenges in the Management of Large Corpora (CMLC-",
            "year": 2019
        },
        {
            "authors": [
                "Isabel Papadimitriou",
                "Richard Futrell",
                "Kyle Mahowald"
            ],
            "title": "When classifying arguments, BERT doesn\u2019t care about word order...except when it matters",
            "venue": "In Proceedings of the Society for Computation",
            "year": 2022
        },
        {
            "authors": [
                "Isabel Papadimitriou",
                "Dan Jurafsky."
            ],
            "title": "Learning music helps you read: Using transfer to study linguistic structure in language models",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages",
            "year": 2020
        },
        {
            "authors": [
                "Vaidehi Patil",
                "Partha Talukdar",
                "Sunita Sarawagi."
            ],
            "title": "Overlap-based vocabulary generation improves cross-lingual transfer among related languages",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics",
            "year": 2022
        },
        {
            "authors": [
                "Jonas Pfeiffer",
                "Ivan Vuli\u0107",
                "Iryna Gurevych",
                "Sebastian Ruder."
            ],
            "title": "MAD-X: An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
            "year": 2020
        },
        {
            "authors": [
                "Jonas Pfeiffer",
                "Ivan Vuli\u0107",
                "Iryna Gurevych",
                "Sebastian Ruder."
            ],
            "title": "UNKs everywhere: Adapting multilingual language models to new scripts",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 10186\u201310203,",
            "year": 2021
        },
        {
            "authors": [
                "Thang Pham",
                "Trung Bui",
                "Long Mai",
                "Anh Nguyen"
            ],
            "title": "Out of order: How important is the sequential order of words in a sentence in natural language understanding tasks? In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021",
            "year": 2021
        },
        {
            "authors": [
                "Peng Qi",
                "Yuhao Zhang",
                "Yuhui Zhang",
                "Jason Bolton",
                "Christopher D. Manning."
            ],
            "title": "Stanza: A Python natural language processing toolkit for many human languages",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics:",
            "year": 2020
        },
        {
            "authors": [
                "Maithra Raghu",
                "Chiyuan Zhang",
                "Jon Kleinberg",
                "Samy Bengio."
            ],
            "title": "Transfusion: Understanding transfer learning for medical imaging",
            "venue": "Advances in neural information processing systems, 32.",
            "year": 2019
        },
        {
            "authors": [
                "Nils Reimers",
                "Iryna Gurevych."
            ],
            "title": "Making monolingual sentence embeddings multilingual using knowledge distillation",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 4512\u20134525.",
            "year": 2020
        },
        {
            "authors": [
                "Phillip Rust",
                "Jonas Pfeiffer",
                "Ivan Vuli\u0107",
                "Sebastian Ruder",
                "Iryna Gurevych."
            ],
            "title": "How good is your tokenizer? on the monolingual performance of multilingual language models",
            "venue": "arXiv preprint arXiv:2012.15613.",
            "year": 2020
        },
        {
            "authors": [
                "Rico Sennrich",
                "Barry Haddow",
                "Alexandra Birch."
            ],
            "title": "Neural machine translation of rare words with subword units",
            "venue": "arXiv preprint arXiv:1508.07909.",
            "year": 2015
        },
        {
            "authors": [
                "Koustuv Sinha",
                "Robin Jia",
                "Dieuwke Hupkes",
                "Joelle Pineau",
                "Adina Williams",
                "Douwe Kiela."
            ],
            "title": "Masked language modeling and the distributional hypothesis: Order word matters pre-training for little",
            "venue": "arXiv preprint arXiv:2104.06644.",
            "year": 2021
        },
        {
            "authors": [
                "Alex Tamkin",
                "Trisha Singh",
                "Davide Giovanardi",
                "Noah Goodman."
            ],
            "title": "Investigating transferability in pretrained language models",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020.",
            "year": 2020
        },
        {
            "authors": [
                "Ke Tran."
            ],
            "title": "From english to foreign languages: Transferring pre-trained language models",
            "venue": "arXiv preprint arXiv:2002.07306.",
            "year": 2020
        },
        {
            "authors": [
                "Giorgos Vernikos",
                "Andrei Popescu-Belis."
            ],
            "title": "Subword mapping and anchoring across languages",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2021, pages 2633\u20132647.",
            "year": 2021
        },
        {
            "authors": [
                "Alex Wang",
                "Amanpreet Singh",
                "Julian Michael",
                "Felix Hill",
                "Omer Levy",
                "Samuel Bowman."
            ],
            "title": "Glue: A multi-task benchmark and analysis platform for natural language understanding",
            "venue": "Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyz-",
            "year": 2018
        },
        {
            "authors": [
                "Dingquan Wang",
                "Jason Eisner."
            ],
            "title": "The galactic dependencies treebanks: Getting more data by synthesizing new languages",
            "venue": "Transactions of the Association for Computational Linguistics, 4:491\u2013505.",
            "year": 2016
        },
        {
            "authors": [
                "Yonghui Wu",
                "Mike Schuster",
                "Zhifeng Chen",
                "Quoc V Le",
                "Mohammad Norouzi",
                "Wolfgang Macherey",
                "Maxim Krikun",
                "Yuan Cao",
                "Qin Gao",
                "Klaus Macherey"
            ],
            "title": "Google\u2019s neural machine translation system: Bridging the gap between human and machine",
            "year": 2016
        },
        {
            "authors": [
                "Zhengxuan Wu",
                "Nelson F Liu",
                "Christopher Potts."
            ],
            "title": "Identifying the limits of cross-domain knowledge transfer for pretrained models",
            "venue": "arXiv preprint arXiv:2104.08410.",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "What makes it hard for neural networks to learn new languages? Large language models (LLMs) require vast datasets for pretraining, making it challenging to train LLMs from scratch for lowresource languages (Devlin et al., 2018; Liu et al., 2019; Lacoste et al., 2019; Clark et al., 2020). For such languages, an appealing approach is to transfer knowledge from an LLM trained for a highresource language, especially since pretrained models can transfer knowledge across even extreme shifts (Papadimitriou and Jurafsky, 2020; Tamkin et al., 2020). A range of methods have been explored to enable such crosslingual transfer of English LLMs, using techniques such as adaptive pretraining (Reimers and Gurevych, 2020), and embedding retraining (Artetxe et al., 2020; Tran, 2020). To better understand the factors affecting\nIn Chinese, \u201cOolong\u201d can refer to an unexpected change or development. \u2217Equal contribution. \u2020Corresponding author.\nsuccessful transfer, we present a set of controlled transfer studies to compare the effects of different aspects of a cross-lingual shift.\nOur controlled studies consist of transferring an English model to a language that is transformed from English on just one axis of variation. Realistic transfer scenarios involve languages that differ across multiple axes of variation at one time. Our experiments serve to disentangle these effects, and identify the issues that practitioners should most focus on when doing cross-lingual transfer learning. We examine three factors that are salient in a transfer learning context:\n\u2022 Word-order syntactic differences: Languages vary greatly in the ways that their syntax orders words. Syntactic topological similarities are generally considered an important factor when deciding transfer language pairs. We test the effects of different levels of wordorder perturbation in transfer learning.\n\u2022 Word identity alignments: Transferring to a new language requires learning the meaning, or word embeddings, of new words, and how\ntheir layer 0 embeddings correspond to the old language. We experiment with the effect of reinitializing or shuffling the rows of the layer 0 word embedding matrix before transfer.\n\u2022 Tokenizer quality We test the effect of bad tokenizer quality by reinitializing the word embedding matrix and transferring to English data tokenized with French and Dutch tokenizers that are suboptimal quality for English tokenization.\nWe test the effect of these factors on transfer learning both by 1) directly fine-tuning on t-English versions of the GLUE benchmark, as well as 2) continuing masked language model pre-training on 15 million tokens of t-English wikitext. In all cases, we find that word identity alignment provides the greatest stumbling block for transfer learning. Reinitializing or shuffling the rows of the embedding matrix has a very negative effect on downstream learning which we cannot reverse in the low-data regime that we are simulating. If the embedding matrix is reinitialized and a new tokenizer is used, the effect of reinitialization overshadows any effect that the quality of the new tokenizer might cause. In the case of syntactic word-order transformations, we find that even in the low-data transfer learning regime, the models we test can adapt to word order shifts as long as vocabulary information is kept.\nWe run experiments on RoBERTa, DeBERTa, and XLM-R in order to test transfer learning beyond the training set languages for both monolingual and multilingual models. Our method allows us to disentangle the effects of correlated factors by inspecting them one at a time.1"
        },
        {
            "heading": "2 Related Work",
            "text": "As self-supervised pretraining advances the state of NLP in high-resource languages, research into widening these successes beyond high-resource languages has become widespread and important. Methodologies for best transferring a monolingual or multilingual model to an unseen language are widely explored. Ogueji et al. (2021) and Ogunremi et al. (2023), showcase the positive effects of pretraining on closer and related languages to the target language, even if this is less data than larger pretrained models, in part because of the possibility of shared vocabulary (Oladipo et al., 2022). Our\n1Our code is available publicly at https://github.com/ frankaging/oolong-crosslingual.\nexperiments build off previous efforts that try to enable crosslingual transfer from pretrained monolingual LLMs to new languages (Artetxe et al., 2018, 2020; Tran, 2020; Reimers and Gurevych, 2020; Gogoulou et al., 2021).\nWith respect to vocabulary sharing and adaptation, Liang et al. (2023) show that training a multilingual model with a massive vocabulary that separates out languages outweighs the benefits of vocabulary sharing between language (Patil et al., 2022), while in the transfer regime Chronopoulou et al. (2020) showcase the importance of maintaining vocabulary overlap. Techniques mapping subword embeddings to their new synonyms, or keeping subwords in the same script across languages, prove effective for cross-lingual transfer (Vernikos and Popescu-Belis, 2021; Pfeiffer et al., 2021, 2020; Muller et al., 2021). The importance of embedding intialization statistics is discussed in (Raghu et al., 2019).\nResults on the importance of syntactic shifts remain broad, with work on multilingual training suggesting that syntactic shifts are significant compared to vocabulary effects (K et al., 2020), and that syntactic structure plays a role in developing parallel multilingual encodings (Dufter and Sch\u00fctze, 2020), while Deshpande et al. (2022) show intersecting effects of vocabulary and word order shifts.\nUnderstanding the direct relationship between the effect of syntactic shifts and the effect of vocabulary and tokenizer shifts remains an important problem in understanding transfer learning. Our work creates a framework for decomposing and disentangling the difficulties of transfer in controlled studies, giving researchers pointers for what aspects of language variation make transfer difficult."
        },
        {
            "heading": "3 Methods",
            "text": "Our methodology consists of taking a pretrained model, and transferring to a t-English: a systematically transformed version of English data that differs from English on one axis of variation. The different t-Englishes that we use are described and motivated below, and examples are in Table 1. We consider two low-data transfer environments: Direct Fine-tuning, where we transfer the English pretrained model directly to t-GLUE, transformed GLUE datasets (Wang et al., 2018), and Continued Pretraining, where we first do masked language modeling training on 15 million tokens of the WikiText-103M corpus (Merity et al., 2016)\nTransformation Type Sentence / Sequence\ntransformed to t-English. 2"
        },
        {
            "heading": "3.1 Transformed English (t-Englishes)",
            "text": "Syntactic Shifts While syntax is a crucial aspect of language (Garrett, 1976), how sensitive or invariant lanugage models are to syntactic information is a complex topic (Pham et al., 2021; Sinha et al., 2021; Papadimitriou et al., 2022; Abdou et al., 2022). In the domain of transfer learning, we investigate a set of syntactic transformations that isolate syntactic word-order shifts from the other factors that differ between languages. We bound our syntactic transformation experiments with a random shuffle control, where no word order information from the original language can be used to decode the new language. We also do the simple, but drastic baseline of reversing the order of all of the words in the input. In order to test the effect of more realistic syntactic changes, we transform the English data into t-Englishes that follow the word-order statistics of other language. Using the Galactic Dependencies package (Wang and Eisner, 2016) with Stanza (Qi et al., 2020) to transform our corpora to match the ordering of words in noun phrases and verb phrases of French ({Nfr,Vfr}) and Japanese ({Nja,Vja}) and also perform a mixed transformation with French noun order and Japanese verb order ({Nfr,Vja}).\nWord identity alignment Previous works have consistently found that good embeddings are crucial for enabling effective crosslingual transfer (Tran, 2020; Artetxe et al., 2020). However,\n2For comparison, the pretraining data for RoBERTa contains 3.3B tokens, so 15M tokens is about 0.45% of its pretraining data. This is comparable to the size of the OSCAR corpus for Yiddish (Ortiz Su\u00e1rez et al., 2019).\nthese gains may due to several factors, including better initialization statistics (Raghu et al., 2019), or to a learned alignment between the learned embeddings and the pretrained transformer layers (Wu et al., 2021). Here, we test the baseline effect of reinitializing the embedding layer while transferring to the same language that the model was pretrained. We compare this to a scenario where the rows of the embedding matrix are shuffled, meaning that vector statistics are broadly similar but each word has been swapped with another and the model needs to find the mapping during fine-tuning.\nTokenizer How much does tokenizer quality matter, if the price of a better tokenizer is having to reinitialize the whole word embedding matrix? Though quality tokenizers undoubtedly play an important role in multilingual NLP (Rust et al., 2020), we wish to compare the effect of tokenizer quality when the word identity alignment problem remains constant. While re-initializing the embedding matrix, we compare the effects of the original RoBERTa tokenizer, to two tokenizers that produce low-quality tokenizations for English text: the French FlauBERT (Le et al., 2020) and the Dutch DutchBERT (de Vries et al., 2019). The nonEnglish tokenizers used to tokenize English text simulate the effect of having a bad, non-languagespecific tokenizer in the low data regime (see Appendix B for statistics on how the different tokenizers work on English)."
        },
        {
            "heading": "4 Results",
            "text": "We present the main results of our transfer experiments. Our experimental details (e.g. hyperparameter choices) with a per-task breakdown of t-\nGLUE performance as well as additional results on DeBERTa and XLM-R are included in Appendix A."
        },
        {
            "heading": "4.1 Syntax matters, but training can mostly recover",
            "text": "Word order permutations have an effect on model performance, but the models that we test can recover relatively well from linguistic word order permutations when there are no vocabulary confounders. As shown in Figure 2, simply by fine-tuning on GLUE RoBERTa can recover from linguistic-style syntactic shifts relatively well, though this is significantly worse for random word order permutations that have no consistency or syntactic backing. These differences are all lessened\nwith continued pretraining on 15M tokens of the transformed t-English data. These results suggest that syntactic shifts have real but limited impact on crosslingual transfer when disentangled from vocabulary learning effects."
        },
        {
            "heading": "4.2 Good embeddings matter most, bad embeddings can ruin a good tokenizer",
            "text": "Looking at the isolated effect of vocabulary, we find that in the low-data transfer regime the model has a hard time reconstructing a reinitialized embedding matrix. As shown in Figure 3, reinitializing the embedding matrix causes huge failures for the direct fine-tune case, and the quality of the tokenizer (language-bespoke versus not) do not have an effect\nbeyond this. Our results suggest that tokenization may thus be a \u201clower-order bit\u201d for crosslingual transfer, which has little impact until good word embeddings are learned. In the direct fine-tuning case, shuffling the word embedding matrix is significantly better than reinitializing the embeddings, though this difference disappears with continued pretraining."
        },
        {
            "heading": "5 Conclusions",
            "text": "In this paper, we propose a paradigm to study crosslingual transfer through transformations which simulate and disentangle the linguistic changes across languages. Our results suggest that solving the embedding alignment problem is the \"high-order bit\" for crosslingual transfer: it has the largest impact on finetuning performance and is the least improved by continued pretraining. Thus, future progress on solving this problem in large-scale transformers may have outsized impact.\nLimitations\nOur paper is about multilinguality in NLP. However, using multiple natural languages would make it impossible to disentangle different factors. By using controlled variants of a single language, we can create a controllable environment to investigate and understand the factors that affect real cross-lingual transfer in a multilingual setting.\nDespite looking at general factors that differ between languages, and using empirical syntactic patterns from non-English languages, the fact remains that all of our experiments are centered on English and t-Englishes, and this may introduce Englishcentric biases.\nOur scope is mainly restricted to English LLMs (vs other languages), three components of crosslingual shifts (vs other potential factors), and GLUE tasks (vs other kinds of NLP tasks). While our experiments are not an exhaustive list of linguistic properties that affect cross-lingual transfer, we aim to focus on crucial factors that change between languages, grounded by the literature. Our paradigm is extensible to other model architectures while we focus on RoBERTa in this paper with additional results on DeBERTa and XLM-R included in Appendix A.\nEthics Statement\nOur experiments provide a controlled environment to test hypotheses about what influences crosslingual transfer. However, English-based experimentations affecting other languages should not be used to determine courses of action for lowresource NLP without supplementary in-language experiments."
        },
        {
            "heading": "Acknowledgements",
            "text": "We would like to thank Nelson Liu, Mirac Suzgun, and Tol\u00falo. p\u00e9. \u00d2g\u00fanr\u00e8.m\u00ed for useful discussions and comments on drafts. This research was funded in part by NSF award number IIS-2128145."
        },
        {
            "heading": "A Results on other models",
            "text": "We present the results in Figures 2 and 3 for two more models: DeBERTa and the cross-lingual model XLM-R:"
        },
        {
            "heading": "B Sequence Length Distribution",
            "text": "As described in Section 3.1, we try four different tokenizers to substitute for our RoBERTa (Liu et al., 2019) model that uses the Byte-Pair Encoding (BPE) (Sennrich et al., 2015) tokenizer. Specifically, we substitue with the WordPiece tokenizer (Wu et al., 2016) used by BERT (Devlin\net al., 2018) (i.e., BERT Tokenizer in Table 1) and the SentencePiece tokenizer (Kudo and Richardson, 2018) used by Albert (Lan et al., 2019) (i.e., Albert Tokenizer in Table 1). Additionally, we substitute with two new non-English tokenizers including the French FlauBERT (Le et al., 2020) (FlauBERT Tokenizer in Table 1) and the Dutch DutchBERT (de Vries et al., 2019) (DutchBERT Tokenizer in Table 1). As shown in Figure 7, we plot the distributions of sequence lengths as a measure of the heterogeneity introduced by new tokenizers to ensure variences across tokenized sequence lengths. Specifically, we see there are inferior tokenizers such as FlauBERT Tokenizer with a 22.15% increase in sequence length. Our results are consistent with previous findings (Rust et al., 2020) where sequence length distributions are closer."
        },
        {
            "heading": "C Training Set-up Details",
            "text": "Downstream Task. We use the GLUE benchmark (Wang et al., 2018) to evaluate model performance, which covers nine different NLP tasks. We report scores on the development sets for each task by fine-tuning our pre-trained or mid-tuned models. We fine-tune for 5 epochs for the smaller datasets (WNLI and MRPC) and 3 epochs for the others. For the performance metrics, we use Matthew\u2019s Correlation for CoLA, Pearson correlation for STSB, and accuracy for all the other datasets.\nHyperparameter and Infrastructure. For each of the mid-tuning and fine-tuning experiments, we collect averaged results from 3 runs with distinct random seeds. We tune our models with two learning rates {2e\u22125, 4e\u22125}, and report the best results from these two learning rates. Fine-tuning with 9 GLUE tasks takes about 8 hours on 4 NVIDIA\nTitan 12G GPUs. Mid-tuning with our subset of WikiText-103M corpus takes about 18 hours with the same infrastructure."
        },
        {
            "heading": "D Detailed GLUE Task Performance",
            "text": "Table 2 shows performance break-down for individual GLUE task under different transformations as described in Section 3.1. The individual t-GLUE and GLUE results are included in Table 2. We find a consistent picture across most of the tasks, with some interesting effects like CoLA (which is more syntax-sensitive) being impacted more by syntactic shifts."
        }
    ],
    "title": "Oolong: Investigating What Makes Transfer Learning Hard with Controlled Studies",
    "year": 2023
}