{
    "abstractText": "Non-compositional expressions, by virtue of their non-compositionality, are a classic \u2018pain in the neck\u2019 for NLP systems. Different from the general language modeling and generation tasks that are primarily compositional, generating non-compositional expressions is more challenging for current neural models, including large pre-trained language models. The main reasons are 1) their non-compositionality, and 2) the limited data resources. Therefore, to make the best use of available data for modeling non-compositionality, we propose a dynamic curriculum learning framework, which learns training examples from easy ones to harder ones thus optimizing the learning step by step, but suffers from the forgetting problem. To alleviate the forgetting problem brought by the arrangement of training examples, we also apply a continual learning method into our curriculum learning framework. Our proposed method combined curriculum and continual learning, to gradually improve the model\u2019s performance on the task of non-compositional expression generation. Experiments on idiomatic expression generation and metaphor generation affirm the effectiveness of our proposed curriculum learning framework and the application of continual learning. Our codes are available at https: //github.com/zhjjn/CL2Gen.git.",
    "authors": [
        {
            "affiliations": [],
            "name": "Jianing Zhou"
        },
        {
            "affiliations": [],
            "name": "Ziheng Zeng"
        },
        {
            "affiliations": [],
            "name": "Hongyu Gong"
        },
        {
            "affiliations": [],
            "name": "Suma Bhat"
        }
    ],
    "id": "SP:e70d524836ec77ea65a0f449d00841a4a4136fe9",
    "references": [
        {
            "authors": [
                "Ruchit Agrawal",
                "Vighnesh Chenthil Kumar",
                "Vigneshwaran Muralidharan",
                "Dipti Misra Sharma."
            ],
            "title": "No more beating about the bush: A step towards idiom handling for indian language nlp",
            "venue": "Proceedings of the Eleventh International",
            "year": 2018
        },
        {
            "authors": [
                "Rahaf Aljundi",
                "Eugene Belilovsky",
                "Tinne Tuytelaars",
                "Laurent Charlin",
                "Massimo Caccia",
                "Min Lin",
                "Lucas Page-Caccia."
            ],
            "title": "Online continual learning with maximal interfered retrieval",
            "venue": "Advances in Neural Information Processing Systems, 32:11849\u2013",
            "year": 2019
        },
        {
            "authors": [
                "Miriam Amin",
                "Peter Fankhauser",
                "Marc Kupietz",
                "Roman Schneider."
            ],
            "title": "Data-driven identification of idioms in song lyrics",
            "venue": "MWE 2021, page 13.",
            "year": 2021
        },
        {
            "authors": [
                "Timothy Baldwin",
                "Su Nam Kim."
            ],
            "title": "Multiword expressions",
            "venue": "Nitin Indurkhya and Fred J. Damerau, editors, Handbook of Natural Language Processing, Second Edition, pages 267\u2013292. Chapman and Hall/CRC.",
            "year": 2010
        },
        {
            "authors": [
                "Yoshua Bengio",
                "J\u00e9r\u00f4me Louradour",
                "Ronan Collobert",
                "Jason Weston."
            ],
            "title": "Curriculum learning",
            "venue": "Proceedings of the 26th annual international conference on machine learning, pages 41\u201348.",
            "year": 2009
        },
        {
            "authors": [
                "Rhys Biddle",
                "Aditya Joshi",
                "Shaowu Liu",
                "Cecile Paris",
                "Guandong Xu."
            ],
            "title": "Leveraging sentiment distributions to distinguish figurative from literal health reports on twitter",
            "venue": "Proceedings of The Web Conference 2020, pages 1217\u20131227.",
            "year": 2020
        },
        {
            "authors": [
                "Tuhin Chakrabarty",
                "Debanjan Ghosh",
                "Adam Poliak",
                "Smaranda Muresan."
            ],
            "title": "Figurative language in recognizing textual entailment",
            "venue": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 3354\u20133361.",
            "year": 2021
        },
        {
            "authors": [
                "Tuhin Chakrabarty",
                "Smaranda Muresan",
                "Nanyun Peng."
            ],
            "title": "Generating similes effortlessly like a pro: A style transfer approach for simile generation",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
            "year": 2020
        },
        {
            "authors": [
                "Tuhin Chakrabarty",
                "Xurui Zhang",
                "Smaranda Muresan",
                "Nanyun Peng."
            ],
            "title": "Mermaid: Metaphor generation with symbolism and discriminative decoding",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association",
            "year": 2021
        },
        {
            "authors": [
                "Arslan Chaudhry",
                "Marc\u2019Aurelio Ranzato",
                "Marcus Rohrbach",
                "Mohamed Elhoseiny"
            ],
            "title": "Efficient lifelong learning with a-gem",
            "venue": "In International Conference on Learning Representations",
            "year": 2018
        },
        {
            "authors": [
                "Zhiyuan Chen",
                "Nianzu Ma",
                "Bing Liu."
            ],
            "title": "Lifelong learning for sentiment classification",
            "venue": "In",
            "year": 2015
        },
        {
            "authors": [
                "Verna Dankers",
                "Christopher Lucas",
                "Ivan Titov."
            ],
            "title": "Can transformer be too compositional? analysing idiom processing in neural machine translation",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume",
            "year": 2022
        },
        {
            "authors": [
                "Michael Flor",
                "Beata Beigman Klebanov."
            ],
            "title": "Catching idiomatic expressions in efl essays",
            "venue": "Proceedings of the Workshop on Figurative Language Processing, pages 34\u201344.",
            "year": 2018
        },
        {
            "authors": [
                "Robert M French"
            ],
            "title": "Catastrophic interference in connectionist networks: Can it be predicted, can it be prevented",
            "venue": "In Proceedings of the 6th International Conference on Neural Information Processing Systems,",
            "year": 1993
        },
        {
            "authors": [
                "Ge Gao",
                "Eunsol Choi",
                "Yejin Choi",
                "Luke Zettlemoyer."
            ],
            "title": "Neural metaphor detection in context",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 607\u2013613.",
            "year": 2018
        },
        {
            "authors": [
                "Hongyu Gong",
                "Kshitij Gupta",
                "Akriti Jain",
                "Suma Bhat."
            ],
            "title": "Illinimet: Illinois system for metaphor detection with contextual and linguistic information",
            "venue": "Proceedings of the Second Workshop on Figurative Language Processing, pages 146\u2013153.",
            "year": 2020
        },
        {
            "authors": [
                "Hessel Haagsma",
                "Johan Bos",
                "Malvina Nissim."
            ],
            "title": "Magpie: A large corpus of potentially idiomatic expressions",
            "venue": "Proceedings of The 12th Language Resources and Evaluation Conference, pages 279\u2013 287.",
            "year": 2020
        },
        {
            "authors": [
                "James Kirkpatrick",
                "Razvan Pascanu",
                "Neil Rabinowitz",
                "Joel Veness",
                "Guillaume Desjardins",
                "Andrei A Rusu",
                "Kieran Milan",
                "John Quan",
                "Tiago Ramalho",
                "Agnieszka Grabska-Barwinska"
            ],
            "title": "Overcoming catastrophic forgetting in neural networks",
            "year": 2017
        },
        {
            "authors": [
                "Maximilian K\u00f6per",
                "Sabine Schulte im Walde."
            ],
            "title": "Distinguishing literal and non-literal usage of german particle verbs",
            "venue": "Proceedings of the 2016 conference of the north American chapter of the association for computational linguistics: Human",
            "year": 2016
        },
        {
            "authors": [
                "Qing Li",
                "Siyuan Huang",
                "Yining Hong",
                "Song-Chun Zhu."
            ],
            "title": "A competence-aware curriculum for visual concepts learning via question answering",
            "venue": "European Conference on Computer Vision, pages 141\u2013157. Springer.",
            "year": 2020
        },
        {
            "authors": [
                "Chin-Yew Lin."
            ],
            "title": "Rouge: A package for automatic evaluation of summaries",
            "venue": "Text summarization branches out, pages 74\u201381.",
            "year": 2004
        },
        {
            "authors": [
                "Changsheng Liu",
                "Rebecca Hwa."
            ],
            "title": "Phrasal substitution of idiomatic expressions",
            "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 363\u2013373.",
            "year": 2016
        },
        {
            "authors": [
                "Changsheng Liu",
                "Rebecca Hwa."
            ],
            "title": "Representations of context in recognizing the figurative and literal usages of idioms",
            "venue": "Thirty-First AAAI Conference on Artificial Intelligence.",
            "year": 2017
        },
        {
            "authors": [
                "Changsheng Liu",
                "Rebecca Hwa."
            ],
            "title": "Heuristically informed unsupervised idiom usage recognition",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1723\u20131731.",
            "year": 2018
        },
        {
            "authors": [
                "Tianlin Liu",
                "Lyle Ungar",
                "Jo\u00e3o Sedoc."
            ],
            "title": "Continual learning for sentence representations using conceptors",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language",
            "year": 2019
        },
        {
            "authors": [
                "Xuebo Liu",
                "Houtim Lai",
                "Derek F Wong",
                "Lidia S Chao."
            ],
            "title": "Norm-based curriculum learning for neural machine translation",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 427\u2013436.",
            "year": 2020
        },
        {
            "authors": [
                "David Lopez-Paz",
                "Marc\u2019Aurelio Ranzato"
            ],
            "title": "Gradient episodic memory for continual learning",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Rui Mao",
                "Chenghua Lin",
                "Frank Guerin."
            ],
            "title": "End-to-end sequential metaphor identification inspired by linguistic theories",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3888\u20133898.",
            "year": 2019
        },
        {
            "authors": [
                "Rosamund Moon"
            ],
            "title": "Fixed expressions and idioms in English: A corpus-based approach",
            "venue": "Oxford University Press.",
            "year": 1998
        },
        {
            "authors": [
                "Kishore Papineni",
                "Salim Roukos",
                "Todd Ward",
                "WeiJing Zhu."
            ],
            "title": "Bleu: a method for automatic evaluation of machine translation",
            "venue": "Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 311\u2013318.",
            "year": 2002
        },
        {
            "authors": [
                "Jing Peng",
                "Anna Feldman."
            ],
            "title": "Automatic idiom recognition with word embeddings",
            "venue": "Information Management and Big Data, pages 17\u201329. Springer.",
            "year": 2015
        },
        {
            "authors": [
                "Emmanouil Antonios Platanios",
                "Otilia Stretcu",
                "Graham Neubig",
                "Barnab\u00e1s P\u00f3czos",
                "Tom Mitchell."
            ],
            "title": "Competence-based curriculum learning for neural machine translation",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the",
            "year": 2019
        },
        {
            "authors": [
                "Alessandro Lenci"
            ],
            "title": "Lexical variability and com",
            "year": 2016
        },
        {
            "authors": [
                "Fan-Keng Sun",
                "Cheng-Hao Ho",
                "Hung-Yi Lee."
            ],
            "title": "Lamol: Language modeling for lifelong language learning",
            "venue": "International Conference on Learning Representations.",
            "year": 2019
        },
        {
            "authors": [
                "Brian Thompson",
                "Jeremy Gwinnup",
                "Huda Khayrallah",
                "Kevin Duh",
                "Philipp Koehn."
            ],
            "title": "Overcoming catastrophic forgetting during domain adaptation of neural machine translation",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter",
            "year": 2019
        },
        {
            "authors": [
                "Yiru Wang",
                "Weihao Gan",
                "Jie Yang",
                "Wei Wu",
                "Junjie Yan."
            ],
            "title": "Dynamic curriculum learning for imbalanced data classification",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 5017\u20135026.",
            "year": 2019
        },
        {
            "authors": [
                "Daphna Weinshall",
                "Gad Cohen",
                "Dan Amir."
            ],
            "title": "Curriculum learning by transfer learning: Theory and experiments with deep networks",
            "venue": "International Conference on Machine Learning, pages 5238\u20135246. PMLR.",
            "year": 2018
        },
        {
            "authors": [
                "Hu Xu",
                "Bing Liu",
                "Lei Shu",
                "S Yu Philip."
            ],
            "title": "Lifelong domain word embedding via meta-learning",
            "venue": "IJCAI.",
            "year": 2018
        },
        {
            "authors": [
                "Zhiwei Yu",
                "Xiaojun Wan."
            ],
            "title": "How to avoid sentences spelling boring? towards a neural approach to unsupervised metaphor generation",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association",
            "year": 2019
        },
        {
            "authors": [
                "Ziheng Zeng",
                "Suma Bhat."
            ],
            "title": "Idiomatic expression identification using semantic compatibility",
            "venue": "Transactions of the Association for Computational Linguistics, 9:1546\u20131562.",
            "year": 2021
        },
        {
            "authors": [
                "Ziheng Zeng",
                "Suma Bhat."
            ],
            "title": "Getting bart to ride the idiomatic train: Learning to represent idiomatic expressions",
            "venue": "arXiv preprint arXiv:2207.03679.",
            "year": 2022
        },
        {
            "authors": [
                "Friedemann Zenke",
                "Ben Poole",
                "Surya Ganguli."
            ],
            "title": "Continual learning through synaptic intelligence",
            "venue": "International Conference on Machine Learning, pages 3987\u20133995. PMLR.",
            "year": 2017
        },
        {
            "authors": [
                "Mingliang Zhang",
                "Fandong Meng",
                "Yunhai Tong",
                "Jie Zhou."
            ],
            "title": "Competence-based curriculum learning for multilingual machine translation",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2021, pages 2481\u20132493.",
            "year": 2021
        },
        {
            "authors": [
                "Jianing Zhou",
                "Hongyu Gong",
                "Suma Bhat."
            ],
            "title": "PIE: A parallel idiomatic expression corpus for idiomatic sentence generation and paraphrasing",
            "venue": "Proceedings of the 17th Workshop on Multiword",
            "year": 2021
        },
        {
            "authors": [
                "Jianing Zhou",
                "Hongyu Gong",
                "Srihari Nanniyur",
                "Suma Bhat."
            ],
            "title": "From solving a problem boldly to cutting the gordian knot: Idiomatic text generation",
            "venue": "arXiv preprint arXiv:2104.06541.",
            "year": 2021
        },
        {
            "authors": [
                "Jianing Zhou",
                "Ziheng Zeng",
                "Hongyu Gong",
                "Suma Bhat."
            ],
            "title": "Idiomatic expression paraphrasing without strong supervision",
            "venue": "arXiv preprint arXiv:2112.08592.",
            "year": 2021
        },
        {
            "authors": [
                "Lei Zhou",
                "Liang Ding",
                "Kevin Duh",
                "Shinji Watanabe",
                "Ryohei Sasano",
                "Koichi Takeda."
            ],
            "title": "Self-guided curriculum learning for neural machine translation",
            "venue": "Proceedings of the 18th International Conference on Spoken Language Translation (IWSLT 2021),",
            "year": 2021
        },
        {
            "authors": [
                "Competence-based CL: (Platanios"
            ],
            "title": "2019) proposed to select training examples based on a competence score. Training examples with a difficulty score lower than current competence score",
            "year": 2019
        }
    ],
    "sections": [
        {
            "text": "Non-compositional expressions, by virtue of their non-compositionality, are a classic \u2018pain in the neck\u2019 for NLP systems. Different from the general language modeling and generation tasks that are primarily compositional, generating non-compositional expressions is more challenging for current neural models, including large pre-trained language models. The main reasons are 1) their non-compositionality, and 2) the limited data resources. Therefore, to make the best use of available data for modeling non-compositionality, we propose a dynamic curriculum learning framework, which learns training examples from easy ones to harder ones thus optimizing the learning step by step, but suffers from the forgetting problem. To alleviate the forgetting problem brought by the arrangement of training examples, we also apply a continual learning method into our curriculum learning framework. Our proposed method combined curriculum and continual learning, to gradually improve the model\u2019s performance on the task of non-compositional expression generation. Experiments on idiomatic expression generation and metaphor generation affirm the effectiveness of our proposed curriculum learning framework and the application of continual learning. Our codes are available at https: //github.com/zhjjn/CL2Gen.git."
        },
        {
            "heading": "1 Introduction",
            "text": "Natural language has a common yet special class of constructions called non-compositional expressions that exhibit semantic non-compositionality, where the meaning of the expression cannot be inferred from that of its constituent words (e.g., metaphors and idioms) (Baldwin and Kim, 2010). They are commonly used for specific communicative intents (Moon et al., 1998; Baldwin and Kim, 2010) and are individually rare but collectively frequent, appearing frequently across genres (Moon et al., 1998; Haagsma et al., 2020). Most of the\nidioms indexed by the Oxford Dictionary have a frequency of less than 1 per million in the corpus of Contemporary American English (Rafatbakhsh and Ahmadi, 2019). They have been classically regarded as a \u201cpain in the neck\u201d to NLP systems (Sag et al., 2002) not only because of their non-compositionality, but also because of their contextual semantic ambiguity (used in noncompositional or compositional meaning depending on the context). Different NLP tasks related to non-compositional expressions have been studied, including sentiment analysis (Biddle et al., 2020), paraphrase generation (Zhou et al., 2021c), natural language inference (Chakrabarty et al., 2021a), metaphor detection (Su et al., 2020) and idiom usage recognition (Liu and Hwa, 2018). However, the generation of non-compositional expressions remains an important yet under-explored problem. Therefore, this paper focuses on the generation of non-compositional expressions.\nAs is shown in Table 1, non-compositional expression generation aims to generate the correct non-compositional expression given a sentence with original non-compositional expression masked. Its importance stems from that 1) noncompositional expressions are an important part of everyday human language use, and 2) their use imparts naturalness, fluency and stylistic enhancement. Therefore, the ability to generate non-compositional expressions renders machine generated language more natural and human-like whereas current SOTA pre-trained text generation models only pre-trained on normal compositional language tend to only generate compositional expressions (Zeng and Bhat, 2022). Besides, in our experiments, the simply fine-tuned model cannot correctly generate the idioms most of the time. Previously only a few of studies focus on metaphor generation (Yu and Wan, 2019; Chakrabarty et al., 2020; Stowe et al., 2021) whereas other types of non-compositional expressions remain under-\nexplored (e.g. idioms). The sparsity of literature and data resources presents challenges for the study of non-compositional expression generation.\nTo better utilize available data and alleviate the limitation on resources, curriculum learning (Bengio et al., 2009) aims to enable the models to begin training from easier examples proceeding to examples with an increasing level of difficulty. As such, curriculum learning consists of two core constituents: (1) Deciding the level of learning difficulty for each example, and (2) Scheduling the order of training examples based on that difficulty level. Curriculum learning has recently emerged as a promising direction for different fields including computer vision (Weinshall et al., 2018; Wang et al., 2019; Li et al., 2020) and natural language processing (Platanios et al., 2019; Liu et al., 2020; Zhou et al., 2021d; Zhang et al., 2021). However, despite the relative success on computer vision tasks, the application of curriculum learning for natural language processing is still limited to neural machine translation, which has rich data resources while other applications, such as noncompositional expression generation, with limited data remain under-explored.\nTo this end, we propose a novel curriculum learning framework for non-compositional expression generation to fill the research gap of generating non-compositional expressions including both metaphors and idioms. Our study is the first to focus on this task and utilizes curriculum learning to alleviate the problem caused by limited data resources. In our work, we use the representation distance and the perplexity score as the difficulty measurement and a dynamic scheduling method to order the examples. Specifically, we observe that curriculum learning orders the training examples according to difficulty level to create a gradual shift of distribution of domain difficulty, which will cause the well-known catastrophic forgetting prob-\nlem (French, 1993) ignored in previous curriculum learning works. Therefore, we propose RE-GEM, a continual learning algorithm to alleviate the forgetting of learned knowledge in the early stage.\nOverall, the main contributions are as follows:\n\u2022 We conduct a first study on non-compositional expression generation including both metaphors and idioms.\n\u2022 We propose a novel curriculum learning framework specifically designed for noncompositional expression generation that uses the distance between contextualized representations and word embeddings and perplexity score as a measure of difficulty level. It is dynamically updated with the training, based on which the training examples are scheduled.\n\u2022 We point out for the first time the forgetting problem caused by the curriculum learning and propose a scheme\u2014RE-GEM\u2014to alleviate this problem.\n\u2022 We evaluate our proposed framework on two tasks: idiomatic expression generation and metaphor generation. Experimental results on both tasks affirm the effectiveness of our framework. Detailed ablation studies and analysis are provided to support our claims."
        },
        {
            "heading": "2 Related Work",
            "text": "Non-compositional Expression. As an integral part of natural language, non-compositional expressions are classically regarded as a \u201cpain in the neck\u201d for NLP (Sag et al., 2002) due to their noncompositionality. Prior studies mainly focused on tasks related to non-compositional expressions, including identifying potentially idiomatic expressions (Salehi et al., 2014; Senaldi et al., 2016; Flor and Klebanov, 2018; Amin et al., 2021; Zeng and Bhat, 2021), disambiguating between their figurative/literal use (Peng and Feldman, 2015; K\u00f6per and im Walde, 2016; Liu and Hwa, 2017, 2018), detecting metaphors (Gao et al., 2018; Mao et al., 2019; Su et al., 2020; Gong et al., 2020), generating metaphors (Yu and Wan, 2019; Stowe et al., 2020; Chakrabarty et al., 2020; Stowe et al., 2021) and paraphrasing between non-compositional expressions and their literal counterparts (Liu and Hwa, 2016; Agrawal et al., 2018; Shirin and Raseek, 2018; Zhou et al., 2021a,b). However, owing to their non-compositionality and limitations on available and related data resources (Stowe et al., 2020,\n2021), the task of generating non-compositional expressions including both idioms and metaphors remains challenging and under-explored. Our study first focuses on alleviating the limitation on resource availability concerning the generation of both idioms and metaphors.\nCurriculum Learning. First proposed by (Bengio et al., 2009), curriculum learning enables machine learning model training to gradually proceed from easy examples to harder ones according to a measure of difficulty level for each example, thereby permitting a better utilization of available data resources. With growing research interests received, curriculum learning has been applied to different fields including computer vision (Weinshall et al., 2018; Wang et al., 2019; Li et al., 2020) and natural language processing. Despite its benefits observed in computer vision tasks, including image classification (Weinshall et al., 2018), human attribute analysis (Wang et al., 2019) and visual question answering (Li et al., 2020), it has seen limited applicability in NLP mainly to NMT (Platanios et al., 2019; Liu et al., 2020; Zhou et al., 2021d). As a result, curriculum learning methods, including difficulty measurement and scheduling strategies, are mainly designed for the NMT task, which is largely different from the task of processing noncompositionality (non-compositional expression generation). To this end, we propose our curriculum learning method specifically designed for noncompositional expression generation.\nContinual Learning Continual learning enables models to learn new knowledge and preserve knowledge acquired previously from a data stream with a continuously changing distribution. However, due to the well-known problem of catastrophic forgetting (French, 1993), continual learning is still challenging for current neural models. The same forgetting problem could also appear in curriculum learning because curriculum learning rearranges the examples according to their difficulty levels, which will naturally create a training data stream with continuously changing distribution on difficulty domain and thus cause the problem of forgetting. Although the catastrophic forgetting problem has been explored in both computer vision (Rebuffi et al., 2017; Kirkpatrick et al., 2017; Zenke et al., 2017; Aljundi et al., 2019) and natural language processing (Xu et al., 2018; Liu et al., 2019; Sun et al., 2019; Chen et al., 2015; Shu et al., 2016; Thompson et al., 2019), there are no avail-\nable studies on curriculum learning mentioning this forgetting problem. Our work is the first attempt to point out the issue of forgetting in curriculum learning and study mechanisms to alleviate it."
        },
        {
            "heading": "3 Framework",
            "text": "In this section, we briefly introduce our proposed curriculum learning method for non-compositional expression generation.\nCurriculum learning for efficiently leveraging available data resources consists of two main parts: a measure of difficulty of training instances, and an arrangement of the training examples using this measure. Accordingly, for non-compositional expression generation, we propose a data arrangement method for dynamically arranging the training examples according to a newly studied difficulty metric. In addition, due to the current large pre-trained language models\u2019 insufficiency in processing non-compositional expressions (Dankers et al., 2022), non-compositional expressions that are difficulty for LMs to understand would have a high perplexity score and the representations between non-compositional expressions and their constituent words would be large. Therefore, we use a combination of the representation distance and perplexity score as a measure of examples\u2019 difficulty.\nMoreover, in our experiments, we observe that following the curriculum learning principle of arranging the training examples based on their difficulty levels, the problem of forgetting arises due to the gradual shift of distribution in domain difficulty. Therefore, to alleviate this forgetting problem, we propose a simple yet effective continual learning method. Figure 1 demonstrates the workflow of our proposed curriculum learning framework and its details as follows."
        },
        {
            "heading": "3.1 Difficulty Metrics",
            "text": "In this section, we define the difficulty metric used by our framework. Previous works on curriculum learning mainly focus on compositional languages. Therefore, difficulty metrics, including sentence length, word rarity, embedding norm and etc, proposed in prior works cannot reflect the difficulty levels of non-compositional expressions in the sentences. As mentioned in Section 1, due to the non-compositionality, the meaning of the non-compositional expressions is different from the meaning of the constituent words. Therefore, the distance between the ideal representation of the\nDataset\nRandom Shuffle Batch Sampled Model\nVanilla\nBaselines\nnon-compositional expressions and the representations of the constituent words would be large. Utilizing this property, we first propose to use the distance between the contextualized representations of the non-compositional expressions and the original word embeddings of the constituent words to reflect the difficulty. A larger distance represents the model has already learned the representation of the expression instead of using the constituent words\u2019 embeddings, which means this non-compositional expression is easy for the model. On the contrary, a smaller distance means the target non-compositional expression is difficulty for the model. Therefore, the difficulty metric based on representation distance is calculated as follows:\ndr(Y ) = 1\nDIST(Y ; \u03b8) =\n1\n\u2225 l(Y )\u2212 Emb(Y ) \u2225\nwhere l(\u00b7) is the final layer of the model and Emb(\u00b7) is the embedding layer of the model.\nBesides, as mentioned in Section 1, due to the rareness of non-compositional expressions in large scale corpora relative to compositional expressions, large pre-trained models seldom see the use of non-compositional expressions during pre-training, which results in their inability to accurately capture the semantics of these expressions. Therefore, we assess the difficulty of training examples based on the models\u2019 familiarity with the non-compositional expressions. Toward this, we propose to utilize the perplexity score as a measure of the difficulty of each training example. This stems from the idea that in language modeling, perplexity is used as a quality measure for language models, which is indicative of its ability to predict the next word in a sequence of words. Perplexity score is built with ngrams that are extracted from text corpora: a lower perplexity score of an n-gram X , i.e.,\nPPL(X) = e\u2212 1 t \u2211t i logp\u03b8(xi|x<i),\nmeans the language model assigns a higher probability to generating X . Therefore, a lower perplexity on a non-compositional expression is indicative\nthat the language model is more familiar with this expression, i.e., this expression is easier for the language model and thus is more likely to generate it. The difficulty metric based on perplexity score is calculated as follows:\ndp(Y ) = PPL(Y ; \u03b8) = e\u2212 1 t \u2211t i logp\u03b8(yi|y<i)\nwhere Y is the target sentence in a training example and \u03b8 is the trainable parameters."
        },
        {
            "heading": "3.2 Scheduling Strategy",
            "text": "Having ascertained the difficulty level for each training example, the traditional curriculum learning scheme re-arranges the training examples using the difficulty level and fixes the order of the examples for the subsequent training process. However, after training the model on some training examples, it is expected that the perceived difficulty level of each training example will change. Therefore, it is unreasonable to use the same order of training examples for the entire training process. To address this issue, a competence score is proposed to dynamically reflect the model\u2019s ability. However, the competence score used by previous works, such as a number increased with time steps (Platanios et al., 2019), is actually not comparable to the difficulty score of training examples because the two measure unrelated aspects. To better reflect the dynamic difficulty levels, we propose a dynamic scheduling method that arranges the training examples.\nAfter each training epoch, the difficulty score d for each training example is updated using the model trained in the most recent epoch:\ndn(Y ) = d r(Y ) + dp(Y )\nwhere dn(Y ) is the difficulty score for training example (X;Y ) after the model has been fine-tuned for n epochs and \u03b8n refers to the trainable parameters of the model that has been fine-tuned for n epochs. X represents the input sentence with the target non-compositional expression masked in the training example and Y is the target sentence. After the difficulty scores for all the training examples\nAlgorithm 1: PPLCL Input: Dataset P, Model M and number of epochs N Output: Fine-tuned Model M\u2217\n1 D0 = D(P,M) ; 2 Sort P based on each difficulty level in D0, resulting\nin a re-arranged P0 ; 3 for n = 1;n \u2264 N do 4 M\u03b8n \u21d0 TRAIN(Pn\u22121); 5 Dn = \u2205,P\u2217n = \u2205 ; 6 for (X;Y ) \u2208 P do 7 dn(Y ) = d\nr(Y ) + dp(Y ) ; 8 if dn(Y ) \u0338= dn\u22121(Y ) then 9 Dn \u21d0 Dn \u22c3 {dn(Y )} ;\n10 P\u2217n \u21d0 P\u2217n \u22c3 (X;Y ) ; 11 else 12 continue ; 13 end 14 end 15 Sort P\u2217n based on Dn, resulting in Pn ; 16 end 17 return M\u2217 = M\u03b8n ;\nAlgorithm 2: TRAIN Input: Train Dataset P, Model M\u03b8 Output: Fine-tuned Model M\u2217\n1 M\u2190 {} ; 2 for t = 1; t \u2264 T do 3 for (X,Y) \u2208 Pt do 4 Mref = (Xref ,Yref ) \u223cM ; 5 g \u2190 \u2207\u03b8l(M\u03b8(X),Y) ; 6 gref \u2190 \u2207\u03b8l(M\u03b8(Xref ),Yref ) ; 7 if g\u22a4gref \u2265 0 then 8 \u03b8 \u2190 \u03b8 \u2212 \u03b1g ; 9 else 10 g\u0303 \u2190 gref \u2212 g\u22a4refg\ng\u22a4g g; \u03b8 \u2190 \u03b8\u2212\u03b1(g+ g\u0303);\n11 end 12 end 13 s\u2190 |M|\nT ;\n14 for i = 1; i \u2264 s do 15 (X,Y) \u223c Pt;M\u2190 (X,Y) 16 end 17 end 18 return M\u2217 = M\u03b8;\nhave been updated, the training examples will be re-arranged according to the new difficulty scores."
        },
        {
            "heading": "3.3 Continual Learning",
            "text": "Essentially, after the order of the training examples is re-arranged based on difficulty level, the curriculum learning scheme creates a gradual shift of the distribution in the domain difficulty, which will cause the problem of catastrophic forgetting, currently ignored by previous studies of curriculum learning (Bengio et al., 2009; Weinshall et al., 2018; Wang et al., 2019; Li et al., 2020; Platanios et al., 2019; Liu et al., 2020; Zhou et al., 2021d).\nDuring training, the model will first learn the examples with a lower difficulty level and then learn\nthose with a higher difficulty in each epoch. In this process, some knowledge about the examples with a lower difficulty level will be forgotten.\nTo alleviate the forgetting problem, we propose RE-GEM, a modified version of GEM (Lopez-Paz and Ranzato, 2017), which fits in our framework better compared with the traditional continual learning methods for the following reason.\nThe traditional continual learning methods like GEM aim to alleviate the forgetting problem and thus sacrifice the learning ability on new data and part of the overall performance. This is done with the use of an episodic memory, Mk, containing randomly sampled training examples from the data Pk for the time step k. When minimizing the loss on the current time step t, GEM treats the losses on Mk of step k < t as constraints by preventing their increase. An improved version of GEM (Chaudhry et al., 2018) was proposed to only treat the loss on a subset Mref of all the episodic memories for step k < t as a constraint instead of computing multiple losses. To guarantee the loss reduction on this episodic memory subset, Mref , their implementation first computes the loss gradient vector g on the current step and then computes the loss gradient vector gref on the subset Mref . Whenever the angle between g and gref is greater than 90\u00b0, the gradient g will be projected to g\u0302 as:\nminimizeg\u0302 1 2 \u2225 g \u2212 g\u0302 \u22252 s.t. g\u0302\u22a4gref \u2265 0\nand the parameters will be updated based on g\u0302, with the intent of avoiding the forgetting of previous data and learning from new data.\nInstead, our main focus is to learn from new data and then to alleviate the forgetting of previous data through the use of RE-GEM. Therefore, when the angle between g and gref is greater than 90\u00b0, we first project gref to g\u0303 as follows:\nminimizeg\u0303 1 2 \u2225 gref \u2212 g\u0303 \u22252 s.t. g\u0303\u22a4g \u2265 0 (1)\nThen the parameters will be updated based on both g and g\u0303 where g guarantees the successful learning of current new data and g\u0303 tries best to alleviate the forgetting of previous data. We leave exploring other forms of gradient updates to future work.\nThe constraint optimization problem in Eq.1 can be solved via the rule proposed in (Chaudhry et al., 2018) as follows:\ng\u0303 = gref \u2212 g\u22a4refg\ng\u22a4g g."
        },
        {
            "heading": "4 Experiments",
            "text": ""
        },
        {
            "heading": "4.1 Datasets",
            "text": "We use two datasets focusing on two kinds of non-compositional expressions\u2014MAGPIE (Haagsma et al., 2020) for idiom and MERMAID (Chakrabarty et al., 2021b) for metaphor. For the instances from MAGPIE, we mask the target idiom in each example. The position of the target idiom is provided in the dataset. The official training-development-testing splits are used. For MERMAID, the masked sentences have been provided and we use the available data splits."
        },
        {
            "heading": "4.2 Baselines",
            "text": "We tested six baseline models compare them with our proposed curriculum learning framework. The Vanilla model that does not use any CL methods, Competence-based CL (Platanios et al., 2019), Norm-based CL (Liu et al., 2020) and SGCL (Zhou et al., 2021d) are used as baselines. Due to space limitation, their description and experimental settings are provided in the Appendix."
        },
        {
            "heading": "4.3 Experimental Settings",
            "text": "For our framework, we utilize BART-base as our backbone model. For the task of idiomatic expression generation, the model is trained with batch size of 8 for 5 epochs. For metaphor generation, the model is trained with batch size of 16 for 10 epochs. Adam optimizer is used and the learning rate is set to 5 \u00d7 10\u22125. All the other parameters are set to their default. All of our experiments are performed 5 times and the mean of the results are reported. Beam search is used when decoding."
        },
        {
            "heading": "4.4 Evaluation Metrics",
            "text": "Automatic Evaluation Considering our focus non-compositional expression generation, we use the widely used text generation evaluation metrics\nROUGE (Lin, 2004) and BLEU (Papineni et al., 2002) for evaluation. To automatically evaluate how well the idiomatic expressions are generated, we extract the newly generated part in the output sentences and then compare it with the target noncompositional expressions in the references via phrase-level BLEU and ROUGE scores following (Zhou et al., 2021c). We also evaluate with a stricter metric of phrase-level Accuracy, in which the generated non-compositional expression is considered to be correct if and only if every word strictly matches the target expression. For metaphoric expression generation task, corpus-level BLEU score and ROUGE score are used for evaluation following (Chakrabarty et al., 2021b). Due to the fact that the target metaphoric expressions only contain one word and is measured by ROUGE-1 score, we did not use the phrase-level scores described above. Human Evaluation We used 100 instances from test sets for both tasks, and collected the outputs from the 3 best methods ranked by automatic evaluation. For each output sentence, two native English speakers, who were blind to the systems being compared, were asked to rate the output sentences. For idiom generation, we propose the following criteria: (1) Meaning (\"Are the output and the reference meaning the same thing?\") (2) Fitness (\"Does the generated idiom make sense in the context?\") (3) Fluency (\u201cHow fluent, grammatical, well formed and easy to understand are the generated utterances?\u201d) (4) Overall (\"What is the overall quality of the generated utterances?\"). For metaphor generation, criteria described in (Chakrabarty et al., 2021b) is used. More details are in the Appendix."
        },
        {
            "heading": "5 Results",
            "text": "As shown in Table 2, for the idiomatic expression generation task, our proposed framework achieves the best performance with respect to all the evalu-\nation metrics. Compared with the performance of the vanilla model, our framework outperforms it by 5 on accuracy, 3.78 on BLEU, 5.98 on phrase-level BLEU, 2.37 on Rouge and 5.08 on phrase-level ROUGE score after only 1 training epoch. After 5 training epochs, the improvements after convergence are 3 on accuracy, 2.49 on BLEU, 4.2 on phrase-level BLEU, 1.67 on ROUGE and 3.01 on phrase-level ROUGE.\nFor other baseline models, it is obvious that most of them are not competitive compared with the vanilla model. Some of the baseline methods even degrade the performance of the vanilla model.\nTable 3 presents the results on the task of metaphor generation task. As in the case of idiomatic expression generation, in this task our proposed framework achieves the best performance considering all the evaluation metrics. Compared with the performance of vanilla model, our framework outperforms it by 1.08 on BLEU, 0.92 on BLEU-2, 1.71 on BLEU-4, 0.45 on ROUGE-1, 0.91 on ROUGE-2 and 0.45 on ROUGE-L after only 1 training epoch. After 10 training epochs, the improvements increase to 3.98 on BLEU, 2.67 on BLEU-2, 1.97 on BLEU-4, 2.12 on ROUGE-1, 2.43 on ROUGE-2 and 2.13 on ROUGE-L. Table 5 presents the results of human evaluation. It is shown that our method still outperforms other baselines and vanilla model by large margin on both idiom and metaphor generation task.\nIt should be noted that for metaphor generation\ntask, all the baseline curriculum learning methods\u2019 influence on the performance is similar to that in the idiomatic expression generation task. That is, most of the baseline methods do are not competitive compared with the vanilla model, whereas our proposed curriculum learning framework shows an obvious improvement over the vanilla model.\nBased on the performance on both tasks, we see that the baseline curriculum learning methods cannot effectively improve the performance of the vanilla model (and may even negatively influence the performance). However, our proposed curriculum learning framework outperforms all the baseline models by reasonably large margins."
        },
        {
            "heading": "6 Analysis",
            "text": "Here we provide some ablation studies based on idiomatic expression generation to analyze the contribution of different modules used in our framework. Difficulty measurement. As shown in Table 8, using our difficulty metric can boost the performance of the vanilla model, which verifies the effectiveness of our proposed measurement of difficulty. Compared with the vanilla model, using our difficulty metric alone can improve the performance by 2 on accuracy, 0.71 on BLEU and 0.28 on ROUGE even without the scheduling method. In addition, compared with the other difficulty measurement methods used in previous studies (e.g. sentence length, word rarity and norm), ours shows a larger performance improvement (rows 3-6 in Table 8).\nUsing our difficulty metric can thus outperform the best among sentence length, word rarity and norm by 4 on accuracy, 4.21 on BLEU and 2.34 on ROUGE, demonstrating its superiority for noncompositional expression generation.\nDynamic scheduling strategy. The effectiveness of our dynamic scheduling strategy can be verified by comparing rows 2 and 7 in Table 8. We see that using our difficulty metric, the performance improves by 1.7 BLEU and 1.09 ROUGE points via dynamic scheduling compared with the performance when fixed scheduling is used, which confirms the effectiveness of our proposed dynamic scheduling method for curriculum learning.\nContinual learning scheme. As stated in Section 3.3, curriculum learning will cause the forgetting problem, which has been ignored by previous studies. As shown in Figure 2, compared with the training loss of the vanilla model, the training loss of the model using only curriculum learning without continual learning shows sudden peaks at the beginning of each epoch. This shows that the model tends to forget knowledge learned from earlier (easier) examples in each epoch after curriculum learning is applied. Additionally, when the model moves from easier examples to harder examples, the knowledge learned from easier examples is beneficial for learning harder ones resulting in a non-increase of the training loss when the model learns harder examples during each epoch.\nTo alleviate the forgetting problem, we proposed a continual learning algorithm called RE-GEM to help with the learning. As shown in rows 7 and\n8 in Table 8, the application of RE-GEM successfully improved the performance by 3 on accuracy, 1.37 on BLEU, 3.64 on phrase-level BLEU, 1 on ROUGE and 2.35 on phrase-level ROUGE scores when only perplexity score and dynamic scheduling are used. Besides, rows 8-11 show the advantage of our proposed RE-GEM over other continual learning methods, including ER (Robins, 1995), MIR (Aljundi et al., 2019) and AGEM (Chaudhry et al., 2018) when applied to curriculum learning. The performance of RE-GEM is better than the best among ER, MIR and AGEM by 3 on accuracy, 3.24 on phrase-level BLEU and 2.06 on phraselevel ROUGE, a trend persisting in Figure 2. When the training loss using curriculum learning with continual learning suddenly spikes, the peak of the one using RE-GEM is the lowest. This suggests our RE-GEM is more effective in alleviating the forgetting problem while maintaining the performance in curriculum learning, especially compared with the traditional continual learning methods."
        },
        {
            "heading": "7 Conclusion and Future Work",
            "text": "In this paper, we first utilize curriculum learning to better utilize available data for non-compositional expression generation. We propose a novel curriculum learning framework by utilizing representation distance and perplexity score as a measure of difficulty level and a dynamic scheduling method to better leverage the available training data. Furthermore, for the first time we study a continual learning algorithm to alleviate the forgetting problem resulting from curriculum learning. Experiments on\ntwo non-compositional expression generation tasks of idiomatic expression generation and metaphor generation show that the proposed curriculum learning framework can effectively boost the performance of non-compositional expression generation outperforming previously studied curriculum learning methods. Future works should explore other difficulty metrics, more effective scheduling methods and continual learning schemes to further alleviate the forgetting problem, and study them for other text generation problems."
        },
        {
            "heading": "8 Limitations",
            "text": "As stated previously, our proposed framework utilizes perplexity score as a measure of difficulty, which is based on that non-compositional expressions are low resource languages compared with compositional expressions. Therefore, current large pre-trained language models will assign low probabilities to non-compositional expressions because of their unfamiliarity to non-compositional expressions. However, when it comes to compositional expressions, perplexity score cannot be used for measuring difficulty level, which limits our framework to only non-compositional expression generation. Besides, our scheduling strategy only re-schedules training examples after each training epoch instead of each batch, which also limits the flexibility of scheduling training examples. Therefore, the order of training examples in each training epoch will still be fixed. More flexible and dynamic scheduling strategies should be explored.\nAnother limitation lies in the gradient update in our RE-GEM. For gradient computed based on current data and the gradient computed based on data in the memory, we use the same learning rate to update them. This could be improved by setting different learning rates for gradients computed based on different data."
        },
        {
            "heading": "Acknowledgements",
            "text": "We would like to acknowledge the assistance of Kellen Tan Cheng in performing the manual evaluation. This research was supported in part by the National Science Foundation under Grant No. IIS 2230817 and by a U.S. National Science Foundation and Institute of Education Sciences grant (2229612)."
        },
        {
            "heading": "A Baseline Models",
            "text": "Details about the baseline models:\n\u2022 Vanilla: The vanilla model directly use the pre-trained BART model for fine-tuning. For each training batch and epoch, random sampling is used to select training examples. No curriculum learning methods are applied.\n\u2022 Competence-based CL: (Platanios et al., 2019) proposed to select training examples based on a competence score. Training examples with a difficulty score lower than current competence score will be selected as candidates for training. In their study, they proposed two measures of difficulty score: sentence length and word rarity.\n\u2022 Norm-based CL: (Liu et al., 2020) proposed to use norm of word embeddings obtained from neural networks as a measure of both difficulty score and competence score. The selection of training examples is similar with the procedure described above in competencebased CL.\n\u2022 SGCL: (Zhou et al., 2021d) proposed to utilize sentence-level BLEU score as a measure of the learning difficulty level. For the arrangement of training examples, the original training set will be divided into several mutual exclusive subsets according to difficulty level. Then they proposed two ways of scheduling: fixed scheduling and dynamic scheduling. We use both scheduling methods as our baseline models. More details about this baseline are described in (Zhou et al., 2021d).\nB Implementation\nOur experiments and implementation are based on the Transformers library and PyTorch."
        },
        {
            "heading": "C Experimental Details",
            "text": "All our experiments are conducted with 2 NVIDIA V100 GPUs."
        },
        {
            "heading": "D Human Evaluation",
            "text": "Here we provide more details of the human evaluation.\nD.1 Idiom Generation Given reference sentences, annotators are expected to evaluate the quality of the generated sentences from three aspects:\n1. Meaning: Are the output and the reference meaning the same thing? If yes, the score should be 3. If they are similar but not exactly the same, the score should be 2. If they are not similar, the score should be 1.\n2. Fitness: Does the generated idiom make sense in the context? If the generated idiom always makes sense, the score should be 4. If the generated idiom makes sense under some circumstances, the score should be 3. If the generated idiom only makes sense under extreme circumstances, the score should be 2. If the generated idiom is invalid, the score should be 1.\n3. Fluency: check whether the transferred sentence is fluent and readable on a scale of 1 to 5, ranging from \u201chighly non-fluent\u201d to \u201cvery fluent\u201d. This should include the tense (present or past), number(singular or plural) and pronoun(himself, herself, someone, his, her etc.)\nD.2 Metaphor Generation Given input sentences and the reference sentences, annotators are expected to evaluate the quality of the generated sentences from four aspects. A scale of 1-5 where 1 denotes the worst and 5 be the best is used:\n1. Fluency: How fluent, grammatical, well formed and easy to understand are the generated utterances?\n2. Meaning: Are the input and the output referring or meaning the same thing?\n3. Creativity: How creative are the generated utterances?\n4. Metaphoricity: How metaphoric are the generated utterances\nD.3 Number of Parameters Considering that our proposed curriculum learning and continual learning do not introduce more parameters, the number of parameters is identical to the number of parameters in the underlying language model: 140M for BART(base).\nD.4 Average Runtime The whole training process for one epoch on two GPUs took approximately 40 minutes including 10 minutes for evaluating difficulties and 30 for fine-tuning."
        },
        {
            "heading": "E Case Study",
            "text": "In Table 9 and 10, we provide more generated examples from idiomatic expression generation and metaphor generation. Examples from different difficulty levels are selected for comparison. For both tasks, we could observe that most of the baseline models could correctly generate the target idiomatic expressions and metaphors when this example is regarded as easy for the model. However, when the example is selected from examples of medium difficulty levels, some baseline models start to generate wrong idiomatic expressions and metaphors. When it comes to the example from hard difficulty levels, all the baseline models cannot generate the correct idiomatic expressions and metaphors. Only our proposed method could still correctly generate the target idiomatic expressions and metaphors.\nTherefore, these generated examples confirm that different examples have different difficulty levels for the models, which justify the need for curriculum learning. Besides, these examples also demonstrate that our proposed method could effectively work by learning from an easy-to-hard order."
        }
    ],
    "title": "Non-compositional Expression Generation Based on Curriculum Learning and Continual Learning",
    "year": 2023
}