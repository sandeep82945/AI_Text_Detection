{
    "abstractText": "Deep architectures such as Transformers are sometimes criticized for having uninterpretable \u201cblack-box\u201d representations. We use causal intervention analysis to show that, in fact, some linguistic features are represented in a linear, interpretable format. Specifically, we show that BERT\u2019s ability to conjugate verbs relies on a linear encoding of subject number that can be manipulated with predictable effects on conjugation accuracy. This encoding is found in the subject position at the first layer and the verb position at the last layer, but is distributed across positions at middle layers, particularly when there are multiple cues to subject number.",
    "authors": [
        {
            "affiliations": [],
            "name": "Sophie Hao"
        },
        {
            "affiliations": [],
            "name": "Tal Linzen"
        }
    ],
    "id": "SP:01b8124390f55cbe8df864bb3f5cc6f54689e409",
    "references": [
        {
            "authors": [
                "Yossi Adi",
                "Einat Kermany",
                "Yonatan Belinkov",
                "Ofer Lavi",
                "Yoav Goldberg."
            ],
            "title": "Fine-grained Analysis of Sentence Embeddings Using Auxiliary Prediction Tasks",
            "venue": "ICLR 2017 Conference Track, Toulon, France. arXiv.",
            "year": 2017
        },
        {
            "authors": [
                "Yonatan Belinkov",
                "Nadir Durrani",
                "Fahim Dalvi",
                "Hassan Sajjad",
                "James Glass"
            ],
            "title": "What do Neural Machine Translation Models Learn about Morphology",
            "year": 2017
        },
        {
            "authors": [
                "Fahim Dalvi",
                "Nadir Durrani",
                "Hassan Sajjad",
                "Yonatan Belinkov",
                "Anthony Bau",
                "James Glass."
            ],
            "title": "What Is One Grain of Sand in the Desert? Analyzing Individual Neurons in Deep NLP Models",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2019
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
            "year": 2019
        },
        {
            "authors": [
                "Philipp Dufter",
                "Hinrich Sch\u00fctze."
            ],
            "title": "Analytical Methods for Interpretable Ultradense Word Embeddings",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natu-",
            "year": 2019
        },
        {
            "authors": [
                "Matthew Finlayson",
                "Aaron Mueller",
                "Sebastian Gehrmann",
                "Stuart Shieber",
                "Tal Linzen",
                "Yonatan Belinkov."
            ],
            "title": "Causal Analysis of Syntactic Agreement Mechanisms in Neural Language Models",
            "venue": "Proceedings of the 59th Annual Meeting of",
            "year": 2021
        },
        {
            "authors": [
                "Jon Gauthier",
                "Jennifer Hu",
                "Ethan Wilcox",
                "Peng Qian",
                "Roger Levy."
            ],
            "title": "SyntaxGym: An Online Platform for Targeted Evaluation of Language Models",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System",
            "year": 2020
        },
        {
            "authors": [
                "Mario Giulianelli",
                "Jack Harding",
                "Florian Mohnert",
                "Dieuwke Hupkes",
                "Willem Zuidema."
            ],
            "title": "Under the Hood: Using Diagnostic Classifiers to Investigate and Improve how Language Models Track Agreement Information",
            "venue": "Proceedings of the 2018",
            "year": 2018
        },
        {
            "authors": [
                "Yoav Goldberg."
            ],
            "title": "Assessing BERT\u2019s Syntactic Abilities",
            "venue": "Computing Research Repository, arXiv:1901.05287.",
            "year": 2019
        },
        {
            "authors": [
                "John Hewitt",
                "Christopher D. Manning."
            ],
            "title": "A Structural Probe for Finding Syntax in Word Representations",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech-",
            "year": 2019
        },
        {
            "authors": [
                "Dieuwke Hupkes",
                "Sara Veldhoen",
                "Willem Zuidema."
            ],
            "title": "Visualisation and \u2019Diagnostic Classifiers\u2019 Reveal How Recurrent and Recursive Neural Networks Process Hierarchical Structure",
            "venue": "Journal of Artificial Intelligence Research, 61:907\u2013926.",
            "year": 2018
        },
        {
            "authors": [
                "Dieuwke Hupkes",
                "Willem Zuidema."
            ],
            "title": "Diagnostic classification and symbolic guidance to understand and improve recurrent neural networks",
            "venue": "Interpreting, Explaining and Visualizing Deep Learning . . . now What? (NIPS 2017 Workshop), Long",
            "year": 2017
        },
        {
            "authors": [
                "Ganesh Jawahar",
                "Beno\u00eet Sagot",
                "Djam\u00e9 Seddah."
            ],
            "title": "What Does BERT Learn about the Structure of Language? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3651\u20133657, Florence, Italy",
            "venue": "Association for",
            "year": 2019
        },
        {
            "authors": [
                "Been Kim",
                "Martin Wattenberg",
                "Justin Gilmer",
                "Carrie Cai",
                "James Wexler",
                "Fernanda Viegas",
                "Rory Sayres."
            ],
            "title": "Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV)",
            "venue": "International Conference on",
            "year": 2018
        },
        {
            "authors": [
                "Olga Kovaleva",
                "Alexey Romanov",
                "Anna Rogers",
                "Anna Rumshisky."
            ],
            "title": "Revealing the Dark Secrets of BERT",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natu-",
            "year": 2019
        },
        {
            "authors": [
                "Yair Lakretz",
                "German Kruszewski",
                "Theo Desbordes",
                "Dieuwke Hupkes",
                "Stanislas Dehaene",
                "Marco Baroni."
            ],
            "title": "The emergence of number and syntax units in LSTM language models",
            "venue": "Proceedings of the 2019 Conference of the North American Chap-",
            "year": 2019
        },
        {
            "authors": [
                "Karim Lasri",
                "Tiago Pimentel",
                "Alessandro Lenci",
                "Thierry Poibeau",
                "Ryan Cotterell."
            ],
            "title": "Probing for the Usage of Grammatical Number",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),",
            "year": 2022
        },
        {
            "authors": [
                "Tal Linzen",
                "Emmanuel Dupoux",
                "Yoav Goldberg."
            ],
            "title": "Assessing the Ability of LSTMs to Learn Syntax-Sensitive Dependencies",
            "venue": "Transactions of the Association for Computational Linguistics, 4(0):521\u2013 535.",
            "year": 2016
        },
        {
            "authors": [
                "Nelson F. Liu",
                "Matt Gardner",
                "Yonatan Belinkov",
                "Matthew E. Peters",
                "Noah A. Smith."
            ],
            "title": "Linguistic Knowledge and Transferability of Contextual Representations",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the",
            "year": 2019
        },
        {
            "authors": [
                "Rebecca Marvin",
                "Tal Linzen."
            ],
            "title": "Targeted Syntactic Evaluation of Language Models",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1192\u20131202, Brussels, Belgium. Association for Computational",
            "year": 2018
        },
        {
            "authors": [
                "Shauli Ravfogel",
                "Yanai Elazar",
                "Hila Gonen",
                "Michael Twiton",
                "Yoav Goldberg."
            ],
            "title": "Null It Out: Guarding Protected Attributes by Iterative Nullspace Projection",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,",
            "year": 2020
        },
        {
            "authors": [
                "Shauli Ravfogel",
                "Grusha Prasad",
                "Tal Linzen",
                "Yoav Goldberg."
            ],
            "title": "Counterfactual Interventions Reveal the Causal Effect of Relative Clause Representations on Agreement Prediction",
            "venue": "Proceedings of the 25th Conference on Computational Natural Lan-",
            "year": 2021
        },
        {
            "authors": [
                "Anna Rogers",
                "Olga Kovaleva",
                "Anna Rumshisky."
            ],
            "title": "A Primer in BERTology: What We Know About How BERT Works",
            "venue": "Transactions of the Association for Computational Linguistics, 8:842\u2013866.",
            "year": 2020
        },
        {
            "authors": [
                "Thibault Sellam",
                "Steve Yadlowsky",
                "Ian Tenney",
                "Jason Wei",
                "Naomi Saphra",
                "Alexander D\u2019Amour",
                "Tal Linzen",
                "Jasmijn Bastings",
                "Iulia Raluca Turc",
                "Jacob Eisenstein",
                "Dipanjan Das",
                "Ellie Pavlick"
            ],
            "title": "The MultiBERTs: BERT Reproductions for Robustness Analysis",
            "year": 2022
        },
        {
            "authors": [
                "Ian Tenney",
                "Patrick Xia",
                "Berlin Chen",
                "Alex Wang",
                "Adam Poliak",
                "R. Thomas McCoy",
                "Najoung Kim",
                "Benjamin Van Durme",
                "Samuel R. Bowman",
                "Dipanjan Das",
                "Ellie Pavlick"
            ],
            "title": "What do you learn from context? Probing for sentence structure in contextualized word",
            "year": 2019
        },
        {
            "authors": [
                "Mycal Tucker",
                "Tiwalayo Eisape",
                "Peng Qian",
                "Roger Levy",
                "Julie Shah."
            ],
            "title": "When Does Syntax Mediate Neural Language Model Performance? Evidence from Dropout Probes",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the",
            "year": 2022
        },
        {
            "authors": [
                "Mycal Tucker",
                "Peng Qian",
                "Roger Levy."
            ],
            "title": "What if This Modified That? Syntactic Interventions with Counterfactual Embeddings",
            "venue": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 862\u2013875, Online. Association for Com-",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Although neural network language models (LMs) are sometimes viewed as uninterpretable \u201cblack boxes,\u201d substantial progress has been made towards understanding to which linguistic regularities LMs are sensitive and how they represent those regularities, in particular in the case of syntactic constraints such as subject\u2013verb agreement. This progress includes not only the discovery that LM predictions adhere to such constraints (e.g., Linzen et al., 2016), but also the development of tools that have revealed encodings of syntactic features in hidden representations (Adi et al., 2017; Giulianelli et al., 2018, among others).\nMost prior work on LMs\u2019 internal vector representations has demonstrated the existence of syntactic information in those vectors, but has not described how LMs use this information. This paper addresses the latter question using a causal intervention paradigm proposed by Ravfogel et al. (2021). We first hypothesize that at least one hidden layer of BERT (Devlin et al., 2019) encodes the grammatical number of third-person subjects and verbs in a low-dimensional number subspace of the hidden representation space, where singular number is linearly separable from plural number. We then\npredict that intervening on the hidden space by reflecting hidden vectors to the opposite side of the number subspace will cause BERT to generate plural conjugations for singular subjects, and vice versa. Our experiment confirms this prediction dramatically: BERT\u2019s verb conjugations are 91% correct before the intervention, and up to 85% incorrect after the intervention.\nIn addition to these findings, our experiment makes observations regarding the location of subject number encodings across token positions, and how it changes throughout BERT\u2019s forward computation. We find that subject number encodings originate in the position of the subject at the embedding layer, and move to the position of the inflected verb at the final layer. When the sentence contains additional cues to subject number beyond the subject itself, such as an embedded verb that agrees with the subject, subject number encodings propagate to other positions of the input at middle layers.\nUnlike our study, prior counterfactual intervention studies have not been able to consistently produce the expected changes in LM behavior. In Finlayson et al. (2021) and Ravfogel et al. (2021), for example, interventions only cause slight degradations in performance, leaving LM behavior mostly unchanged. These numerically weaker results show that LM behavior is influenced by linear feature encodings, but is ultimately driven by other representations, which may have a non-linear structure. In contrast, our results show that the linear encoding of subject number determines BERT\u2019s ability to conjugate verbs. The mechanism behind verb conjugation is therefore linear and interpretable, far from being a black box.1"
        },
        {
            "heading": "2 Background and Related Work",
            "text": "This study contributes to a rich literature on the representation of natural language syntax in LMs.\n1Code for our experiment is available at: https:// github.com/yidinghao/causal-conjugation\nWe briefly review this literature in this section; a more comprehensive overview is offered by Lasri et al. (2022).\nLMs and Syntax. A popular approach to the study of syntax in LMs is through the use of behavioral experiments. An influential example is Linzen et al. (2016), who evaluate English LSTM LMs on their ability to conjugate third-person present-tense verbs. Since verb conjugation depends on syntactic structure in theory, this study can be viewed as an indirect evaluation of the LM\u2019s knowledge of natural language syntax. Linzen et al.\u2019s methodology for evaluating verb conjugation is to compare probability scores assigned to different verb forms, testing whether an LM is more likely to generate correctly conjugated verbs than incorrectly conjugated verbs. Follow-up studies such as Marvin and Linzen (2018), Warstadt et al. (2019), and Gauthier et al. (2020) have refined the behavioral approach by designing challenge benchmarks with experimental controls on the structure of example texts, which allow for fine-grained evaluations of specific linguistic abilities.\nProbing and LM Representations. Another approach to syntax in LMs is the use of probing classifiers (Adi et al., 2017; Belinkov et al., 2017; Hupkes and Zuidema, 2017; Hupkes et al., 2018). By contrast with behavioral studies, probing studies analyze what information is encoded in LM representations. A typical analysis attempts to train the probing classifier to decode the value of a syntactic feature from hidden vectors generated by an LM. If this is successful, then the study concludes that the hidden space contains an encoding of the relevant information about the syntactic feature. When the probing classifier is linear, the study can additionally conclude that the encoding has a linear structure. An overview of probing results for BERT is provided by Rogers et al. (2020).\nCounterfactual Intervention. Counterfactual intervention enhances the results of a probing study by determining whether a feature encoding discovered by a linear probe is actually used by the LM, or whether the probe has detected a spurious pattern that does not impact model behavior. Early studies such as Giulianelli et al. (2018), Lakretz et al. (2019), Tucker et al. (2021), Tucker et al. (2022), and Ravfogel et al. (2021) provide evidence that manually manipulating representations of subject number can result in causal effects on LM verb\nconjugation and other linguistic abilities. The goal of this paper is to present an instance where linear encodings fully determine the verb conjugation behavior of an LM."
        },
        {
            "heading": "3 Methodology",
            "text": "Let h(l,i) \u2208 R768 be the hidden vector from layer l of BERTBASE for position i. Our hypothesis is that there is an orthonormal basis B = {b(1), b(2), . . . , b(768)} such that for some k \u226a 768, the first k basis vectors span a number subspace that linearly separates hidden vectors for singular-subject sentences from hidden vectors for plural-subject sentences. Our prediction is that the counterfactual intervention illustrated in Figure 1, where hidden vectors are reflected to the opposite side of the number subspace, will reverse the subject number encoded in the vectors when applied with sufficient intensity (as determined by the hyperparameter \u03b1), causing BERT to conjugate the main verb of a sentence as if its subject had the opposite number. This section describes (1) how our counterfactual intervention is defined, (2) how we find the basis vectors for the number subspace, and (3) how we measure the effect of this intervention on verb conjugation.\nCounterfactual Intervention. Suppose that the hidden vector h(l,i) is computed from an input consisting of a single sentence. The goal of our counterfactual intervention is to transform h(l,i) into a vector h\u0303 that BERT will interpret as a hidden vector representing the same sentence, but with the opposite subject number. To do so, we first assume that h(l,i) is written in terms of the basis B:\nh(l,i) = 768\u2211 j=1 \u03bbjb (j),\nwhere for each j, the coordinate \u03bbj is the scalar projection of h(l,i) onto the unit vector b(j):\n\u03bbj = ( h(l,i) )\u22a4 b(j).\nNext, we assume that the coordinates of h(l,i) along the number subspace, \u03bb1, \u03bb2, . . . , \u03bbk, collectively encode the input sentence\u2019s subject number, and that \u2212\u03bb1,\u2212\u03bb2, . . . ,\u2212\u03bbk encode the opposite subject number. We compute h\u0303 by simply moving these coordinates of h(l,i) towards the opposite subject number:\nh\u0303 = h(l,i) \u2212 \u03b1 k\u2211\nj=1\n\u03bbjb (j).\nThe variable \u03b1 is a hyperparameter that determines the intensity of the counterfactual intervention. When \u03b1 = 1, the coordinates along the number subspace are set to 0; h\u0303 is then interpreted as a vector that encodes no information about subject number. If our hypothesis about the number subspace is correct, then counterfactual intervention with \u03b1 \u2265 2 should result in a vector h\u0303 that encodes the opposite subject number.\nFinding the Number Subspace. We use the iterative nullspace projection (INLP, Ravfogel et al., 2020; Dufter and Sch\u00fctze, 2019) method to calculate the basis for the number subspace. We begin by using BERT to encode a collection of sentences and extracting the hidden vectors h(l,i) in the positions of main subjects. We then train a linear probe to detect whether these hidden vectors came from a singular subject or a plural subject, and take b(1) to be the probe\u2019s weight vector, normalized to unit length. To obtain b(j) for j > 1, we use the same procedure, but preprocess the data by applying counterfactual intervention with \u03b1 = 1 and k = j\u22121. This erases the subject number information captured by previously calculated basis vectors, ensuring that b(j) is orthogonal to b(1), b(2), . . . , b(j\u22121).\nMeasuring the Effect of Intervention. We evaluate BERT\u2019s verb conjugation abilities using a paradigm based on Goldberg (2019), where masked language modeling is performed on sentences with a third-person subject where the main verb, is or are, is masked out. We calculate conjugation accuracy by interpreting BERT\u2019s output as a binary classification, where the predicted label is \u201csingular\u201d if P[is] > P[are] and \u201cplural\u201d otherwise. To test our prediction about the causal effect of number encoding on verb conjugation, we measure conjugation accuracy before and after intervention with \u03b1 \u2265 2. If intervention causes conjugation accuracy to drop from \u2248100% to \u22480%, then we conclude that we have successfully encoded the opposite subject number into the hidden vectors. If conjugation accuracy drops to \u224850%, then number information has been erased, but we cannot conclude that the opposite subject number has been encoded."
        },
        {
            "heading": "4 Experiment",
            "text": "We test our prediction by performing an experiment using the bert-base-uncased instance of BERT. For each layer, we apply counterfactual intervention and measure its effect on conjugation accuracy. We perform two versions of our experiment: one where intervention is applied to all hidden vectors (\u201cglobal intervention\u201d), and one where intervention is only applied to hidden vectors in the subject position (\u201clocal intervention\u201d). We repeat our experiment five times, with each trial using linear probes trained on a freshly sampled, balanced dataset of 4,000 hidden vectors.\nData. We use data from Ravfogel et al. (2021), which consist of sentences with a relative clause intervening between the main subject and the main verb (e.g., The author that the teacher admires is happy). We sample the INLP training vectors from their training split, and we use their testing split to measure conjugation accuracy.\nHyperparameters. We tune the hyperparameters \u03b1 (intensity of intervention) and k (dimensionality of the number subspace) using a grid search over the range \u03b1 \u2208 {2, 3, 5} and k \u2208 {2, 4, 8}.\nMain Results. Figure 2 shows our results. The values of \u03b1 and k do not affect our results qualitatively, but they do exhibit a direct relationship with the magnitude of the effect of intervention on conjugation accuracy. We focus on the results for\n\u03b1 = 5 and k = 8, which exhibit the greatest impact of intervention on conjugation accuracy. The full hyperparameter tuning results can be found in Appendix A.\nOur prediction is confirmed when global intervention, where hidden vectors across all positions are modified, is applied to layer 8. Verb conjugations are 91.7% correct before intervention, but 84.6% incorrect after intervention. Local intervention on layer 8, where only the hidden vector in the subject position is modified, has a much weaker effect, only causing conjugation accuracy to drop to 57.5% (42.5% incorrect). These results show that BERT indeed uses a linear encoding of subject number to comply with subject\u2013verb agreement. The location of this linear encoding is not confined to the position of the subject, but is rather distributed across multiple positions.\nRedundant Cues to Number. Some sentences in our training and testing data contain an embedded verb that agrees with the main subject. For example, in the sentence The author that admires the teacher is happy, the singular verb admires agrees with the subject author. Since we can deduce the number of the subject from the number of this embedded verb, even in the absence of any direct access to a representation of the subject, in these sentences the embedded verb serves as a redundant cue to subject number.\nFigure 3 shows the effects of intervention broken down by the presence of cue redundancy. When there is no redundancy, near-zero conjugation accuracy is observed after both local and global intervention applied to layers 0\u20136. This shows that when the subject is the only word that conveys subject number, verb conjugation depends solely on the hidden vector in the subject position. By contrast, local intervention has no effect on conjugation accuracy in the presence of redundant cues, and neither does intervention in the positions of the subject and the embedded verb (the \u201csubj. + verb\u201d condition). This shows that the presence of a redundant cue to subject number causes BERT to distribute the encoding of subject number to multiple positions.\nUpper Layers. In layers 10\u201312, neither local nor global intervention has any effect on conjugation accuracy. We hypothesize that this is because, at these layers, the INLP linear probe cannot identify the number subspace using training vectors extracted from the subject position of sentences. To test this hypothesis, we extract INLP training vectors from the position of the main verb instead of the subject as before, and apply local interven-\ntion to the position of the masked-out main verb. Supporting our hypothesis, both local and global intervention result in near-zero conjugation accuracy (Figure 4, right), showing that at upper layers, only the position of the main verb is used by BERT for conjugation.\nRobustness. To verify that our results are robust to differences in model instance, we repeat our experiment using the MultiBERTs (Sellam et al., 2022), a collection of 25 BERTBASE models pretrained from different random initializations. As shown in the left side of Figure 5, we obtain similar results to Figure 2, indicating that our findings are not specific to bert-base-uncased.\nSide Effects. Does the number subspace encode information beyond number? To test this, we apply intervention to number-neutral words (i.e., all words other than nouns and verbs) along the number subspace. We find that this has no effect on masked language modeling perplexity for those words (Figure 5). In contrast, intervention on number-neutral words along a random 8- dimensional representation subspace increases perplexity by a factor of 52.8 on average. This shows that the number space selectively encodes number, such that manipulating hidden vectors along the number subspace does not affect predictions unrelated to number."
        },
        {
            "heading": "5 Discussion",
            "text": "In this section, we discuss our results in relation to our current knowledge about linear representations.\nBERT Layers. Probing studies have found that lower layers of BERT encode lexical features, while middle layers encode high-level syntactic features and upper layers encode task-specific features (Hewitt and Manning, 2019; Jawahar et al., 2019; Kovaleva et al., 2019; Liu et al., 2019; Tenney et al., 2019). Our results confirm this in the case of cue redundancy: at layer 8, the representation of subject number is not tied to any position; while at layer 12, it is tied to the [MASK] position, where it is most relevant for masked language modeling. When there is no cue redundancy, however, subject number is tied to the subject position until layer 9, suggesting that subject number is treated as a lexical feature of the subject rather than a sentence-level syntactic feature.\nEffect Size. Prior counterfactual intervention studies only report marginal changes in performance after intervention (e.g., Kim et al., 2018; Dalvi et al., 2019; Lakretz et al., 2019; Finlayson et al., 2021; Ravfogel et al., 2021). For example, the largest effect size reported by Ravfogel et al. (2021) is no more than 35 percentage points. These results suggest that the linear encoding is only a relatively small part of the model\u2019s representation of the feature. Our results improve upon prior work by identifying an aspect of LM behavior that is entirely driven by linear feature encodings."
        },
        {
            "heading": "6 Conclusion",
            "text": "Using a causal intervention analysis, this paper has revealed strong evidence that BERT hidden representations contain a linear encoding of main subject number that is used for verb conjugation during masked language modeling. This encoding originates from the word embeddings of the main subject and possible redundant cues, propagates to other positions at the middle layers, and migrates to the position of the masked-out verb at the upper layers. The structure of this encoding is interpretable, such that manipulating hidden vectors along this encoding results in predictable effects on conjugation accuracy.\nOur clean and interpretable results offer subject number as an example of a feature that a large language model might encode using a straightforwardly linear-structured representation scheme. For future work, we pose the question of what kinds of features may admit similarly strong results from a causal intervention study like this one.\nLimitations\nBelow we identify possible limitations of our approach.\nExperimental Control. By utilizing Ravfogel et al.\u2019s (2021) dataset, where sentences adhere to a uniform syntactic template, we have exerted tight experimental control over the structure of our test examples. This control has allowed us, for instance, to identify the qualitatively distinct results from Figure 3 between inputs with and without a redundant cue to subject number. In a more naturalistic setting, it is possible that verb conjugation may be conditioned by factors other than a linear encoding of subject number, such as semantic collocations or discourse context.\nAsymmetry of Findings. Although we have shown that BERT uses a linear encoding of subject number to conjugate verbs, we can never prove using our approach that BERT does not use a linear encoding of a feature to some end. In the instances where we are unable to encode the opposite subject number, we cannot rule out the possibility that BERT uses a linear encoding of subject number that cannot be detected using INLP.\nEthical Considerations\nWe do not foresee any ethical concerns arising from our work."
        },
        {
            "heading": "Acknowledgments",
            "text": "We thank the EMNLP 2023 reviewers and area chairs as well as members of the New York University (NYU) Computation and Psycholinguistics Lab for their feedback on this paper.\nThis work was supported in part through the NYU IT High Performance Computing resources, services, and staff expertise. This material is based upon work supported by the National Science Foundation (NSF) under Grant No. BCS-2114505."
        },
        {
            "heading": "A Hyperparameter Tuning Results",
            "text": "Our full hyperparameter tuning results are shown in Figure 6."
        }
    ],
    "title": "Verb Conjugation in Transformers Is Determined by Linear Encodings of Subject Number",
    "year": 2023
}