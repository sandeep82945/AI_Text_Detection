{
    "abstractText": "Fine-tuning pre-trained language models (LMs) has become the de facto standard in many NLP tasks. Nevertheless, fine-tuned LMs are still prone to robustness issues, such as adversarial robustness and model calibration. Several perspectives of robustness for LMs have been studied independently, but lacking a unified consideration in multiple perspectives. In this paper, we propose Robustifying LMs via Adversarial perturbation with Selective Training (ROAST), a simple yet effective fine-tuning technique to enhance the multi-perspective robustness of LMs in a unified way. ROAST effectively incorporates two important sources for the model robustness, robustness on the perturbed inputs and generalizable knowledge in pre-trained LMs. To be specific, ROAST introduces adversarial perturbation during finetuning while the model parameters are selectively updated upon their relative importance to minimize unnecessary deviation. Under a unified evaluation of fine-tuned LMs by incorporating four representative perspectives of model robustness, we demonstrate the effectiveness of ROAST compared to state-of-the-art finetuning methods on six different types of LMs, which indicates its usefulness in practice.",
    "authors": [
        {
            "affiliations": [],
            "name": "Jaehyung Kim"
        },
        {
            "affiliations": [],
            "name": "Yuning Mao"
        },
        {
            "affiliations": [],
            "name": "Rui Hou"
        },
        {
            "affiliations": [],
            "name": "Hanchao Yu"
        },
        {
            "affiliations": [],
            "name": "Davis Liang"
        },
        {
            "affiliations": [],
            "name": "Pascale Fung"
        },
        {
            "affiliations": [],
            "name": "Qifan Wang"
        },
        {
            "affiliations": [],
            "name": "Fuli Feng"
        },
        {
            "affiliations": [],
            "name": "Lifu Huang"
        },
        {
            "affiliations": [],
            "name": "Madian Khabsa"
        },
        {
            "affiliations": [],
            "name": "\u2020KAIST"
        },
        {
            "affiliations": [],
            "name": "\u2021Meta"
        }
    ],
    "id": "SP:17c973283f7417358f8fce3723a75454f58904a8",
    "references": [
        {
            "authors": [
                "Armen Aghajanyan",
                "Akshat Shrivastava",
                "Anchit Gupta",
                "Naman Goyal",
                "Luke Zettlemoyer",
                "Sonal Gupta."
            ],
            "title": "Better fine-tuning by reducing representational collapse",
            "venue": "International Conference on Learning Representations (ICLR).",
            "year": 2021
        },
        {
            "authors": [
                "Dara Bahri",
                "Hossein Mobahi",
                "Yi Tay."
            ],
            "title": "Sharpness-aware minimization improves language model generalization",
            "venue": "Annual Meeting of the Association for Computational Linguistics (ACL).",
            "year": 2022
        },
        {
            "authors": [
                "David Berthelot",
                "Nicholas Carlini",
                "Ian Goodfellow",
                "Nicolas Papernot",
                "Avital Oliver",
                "Colin A Raffel."
            ],
            "title": "Mixmatch: A holistic approach to semisupervised learning",
            "venue": "Advances in Neural Information Processing Systems (NeurIPS).",
            "year": 2019
        },
        {
            "authors": [
                "Shikha Bordia",
                "Samuel R Bowman."
            ],
            "title": "Identifying and reducing gender bias in word-level language models",
            "venue": "arXiv preprint arXiv:1904.03035.",
            "year": 2019
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners",
            "year": 2020
        },
        {
            "authors": [
                "Jiaao Chen",
                "Dinghan Shen",
                "Weizhu Chen",
                "Diyi Yang."
            ],
            "title": "Hiddencut: Simple data augmentation for natural language understanding with better generalizability",
            "venue": "Annual Meeting of the Association for Computational Linguistics (ACL).",
            "year": 2021
        },
        {
            "authors": [
                "Jifan Chen",
                "Eunsol Choi",
                "Greg Durrett"
            ],
            "title": "Can nli models verify qa systems",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP",
            "year": 2021
        },
        {
            "authors": [
                "Sanyuan Chen",
                "Yutai Hou",
                "Yiming Cui",
                "Wanxiang Che",
                "Ting Liu",
                "Xiangzhan Yu."
            ],
            "title": "Recall and learn: Fine-tuning deep pretrained language models with less forgetting",
            "venue": "Conference on Empirical Methods in Natural Language Processing (EMNLP).",
            "year": 2020
        },
        {
            "authors": [
                "Kevin Clark",
                "Minh-Thang Luong",
                "Quoc V Le",
                "Christopher D Manning."
            ],
            "title": "Electra: Pre-training text encoders as discriminators rather than generators",
            "venue": "International Conference on Learning Representations (ICLR).",
            "year": 2020
        },
        {
            "authors": [
                "Brian Davies."
            ],
            "title": "Integral transforms and their applications, volume 41",
            "venue": "Springer Science & Business Media.",
            "year": 2002
        },
        {
            "authors": [
                "Shrey Desai",
                "Greg Durrett."
            ],
            "title": "Calibration of pretrained transformers",
            "venue": "Conference on Empirical Methods in Natural Language Processing (EMNLP).",
            "year": 2020
        },
        {
            "authors": [
                "Xinshuai Dong",
                "Anh Tuan Luu",
                "Min Lin",
                "Shuicheng Yan",
                "Hanwang Zhang"
            ],
            "title": "How should pretrained language models be fine-tuned towards adversarial robustness? In Advances in Neural Information Processing Systems (NeurIPS)",
            "year": 2021
        },
        {
            "authors": [
                "Desmond Elliott",
                "Stella Frank",
                "Khalil Sima\u2019an",
                "Lucia Specia"
            ],
            "title": "Multi30k: Multilingual englishgerman image descriptions",
            "venue": "In Proceedings of the 5th Workshop on Vision and Language,",
            "year": 2016
        },
        {
            "authors": [
                "Pierre Foret",
                "Ariel Kleiner",
                "Hossein Mobahi",
                "Behnam Neyshabur."
            ],
            "title": "Sharpness-aware minimization for efficiently improving generalization",
            "venue": "International Conference on Learning Representations (ICLR).",
            "year": 2021
        },
        {
            "authors": [
                "Ian J Goodfellow",
                "Jonathon Shlens",
                "Christian Szegedy."
            ],
            "title": "Explaining and harnessing adversarial examples",
            "venue": "International Conference on Learning Representations (ICLR).",
            "year": 2015
        },
        {
            "authors": [
                "Chuan Guo",
                "Geoff Pleiss",
                "Yu Sun",
                "Kilian Q Weinberger."
            ],
            "title": "On calibration of modern neural networks",
            "venue": "International Conference on Machine Learning (ICML).",
            "year": 2017
        },
        {
            "authors": [
                "Demi Guo",
                "Yoon Kim",
                "Alexander M Rush."
            ],
            "title": "Sequence-level mixed sample data augmentation",
            "venue": "Conference on Empirical Methods in Natural Language Processing (EMNLP).",
            "year": 2020
        },
        {
            "authors": [
                "Abhishek Gupta",
                "Alagan Anpalagan",
                "Ling Guan",
                "Ahmed Shaharyar Khwaja."
            ],
            "title": "Deep learning for object detection and scene perception in self-driving cars: Survey, challenges, and open issues",
            "venue": "Array, 10:100057.",
            "year": 2021
        },
        {
            "authors": [
                "Suchin Gururangan",
                "Swabha Swayamdipta",
                "Omer Levy",
                "Roy Schwartz",
                "Samuel Bowman",
                "Noah A Smith."
            ],
            "title": "Annotation artifacts in natural language inference data",
            "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association",
            "year": 2018
        },
        {
            "authors": [
                "Pengcheng He",
                "Xiaodong Liu",
                "Jianfeng Gao",
                "Weizhu Chen."
            ],
            "title": "Deberta: Decoding-enhanced bert with disentangled attention",
            "venue": "International Conference on Learning Representations (ICLR).",
            "year": 2021
        },
        {
            "authors": [
                "Dan Hendrycks",
                "Xiaoyuan Liu",
                "Eric Wallace",
                "Adam Dziedzic",
                "Rishabh Krishnan",
                "Dawn Song."
            ],
            "title": "Pretrained transformers improve out-of-distribution robustness",
            "venue": "Annual Meeting of the Association for Computational Linguistics (ACL).",
            "year": 2020
        },
        {
            "authors": [
                "Dan Hendrycks",
                "Andy Zou",
                "Mantas Mazeika",
                "Leonard Tang",
                "Bo Li",
                "Dawn Song",
                "Jacob Steinhardt."
            ],
            "title": "Pixmix: Dreamlike pictures comprehensively improve safety measures",
            "venue": "Conference on Computer Vision and Pattern Recognition (CVPR).",
            "year": 2022
        },
        {
            "authors": [
                "Haoming Jiang",
                "Pengcheng He",
                "Weizhu Chen",
                "Xiaodong Liu",
                "Jianfeng Gao",
                "Tuo Zhao."
            ],
            "title": "Smart: Robust and efficient fine-tuning for pretrained natural language models through principled regularized optimization",
            "venue": "Annual Meeting of the",
            "year": 2020
        },
        {
            "authors": [
                "Di Jin",
                "Zhijing Jin",
                "Joey Tianyi Zhou",
                "Peter Szolovits."
            ],
            "title": "Is bert really robust? a strong baseline for natural language attack on text classification and entailment",
            "venue": "Proceedings of the AAAI conference on artificial intelligence.",
            "year": 2020
        },
        {
            "authors": [
                "Longlong Jing",
                "Yingli Tian."
            ],
            "title": "Self-supervised visual feature learning with deep neural networks: A survey",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence.",
            "year": 2020
        },
        {
            "authors": [
                "Divyansh Kaushik",
                "Eduard Hovy",
                "Zachary C Lipton."
            ],
            "title": "Learning the difference that makes a difference with counterfactually-augmented data",
            "venue": "International Conference on Learning Representations (ICLR).",
            "year": 2020
        },
        {
            "authors": [
                "Jacob Devlin Ming-Wei Chang Kenton",
                "Lee Kristina Toutanova."
            ],
            "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of NAACL-HLT.",
            "year": 2019
        },
        {
            "authors": [
                "Jaehyung Kim",
                "Dongyeop Kang",
                "Sungsoo Ahn",
                "Jinwoo Shin."
            ],
            "title": "What makes better augmentation strategies? augment difficult but not too different",
            "venue": "International Conference on Learning Representations (ICLR).",
            "year": 2022
        },
        {
            "authors": [
                "Klim Kireev",
                "Maksym Andriushchenko",
                "Nicolas Flammarion."
            ],
            "title": "On the effectiveness of adversarial training against common corruptions",
            "venue": "Uncertainty in Artificial Intelligence (UAI).",
            "year": 2022
        },
        {
            "authors": [
                "James Kirkpatrick",
                "Razvan Pascanu",
                "Neil Rabinowitz",
                "Joel Veness",
                "Guillaume Desjardins",
                "Andrei A Rusu",
                "Kieran Milan",
                "John Quan",
                "Tiago Ramalho",
                "Agnieszka Grabska-Barwinska"
            ],
            "title": "Overcoming catastrophic forgetting in neural networks",
            "year": 2017
        },
        {
            "authors": [
                "Simon Kornblith",
                "Mohammad Norouzi",
                "Honglak Lee",
                "Geoffrey Hinton."
            ],
            "title": "Similarity of neural network representations revisited",
            "venue": "International Conference on Machine Learning (ICML).",
            "year": 2019
        },
        {
            "authors": [
                "Ananya Kumar",
                "Aditi Raghunathan",
                "Robbie Jones",
                "Tengyu Ma",
                "Percy Liang."
            ],
            "title": "Fine-tuning can distort pretrained features and underperform out-ofdistribution",
            "venue": "International Conference on Learning Representations (ICLR).",
            "year": 2022
        },
        {
            "authors": [
                "Tom Kwiatkowski",
                "Jennimaria Palomaki",
                "Olivia Redfield",
                "Michael Collins",
                "Ankur Parikh",
                "Chris Alberti",
                "Danielle Epstein",
                "Illia Polosukhin",
                "Jacob Devlin",
                "Kenton Lee"
            ],
            "title": "Natural questions: a benchmark for question answering research",
            "year": 2019
        },
        {
            "authors": [
                "Zhenzhong Lan",
                "Mingda Chen",
                "Sebastian Goodman",
                "Kevin Gimpel",
                "Piyush Sharma",
                "Radu Soricut."
            ],
            "title": "Albert: A lite bert for self-supervised learning of language representations",
            "venue": "International Conference on Learning Representations (ICLR).",
            "year": 2020
        },
        {
            "authors": [
                "Ken Lang."
            ],
            "title": "Newsweeder: Learning to filter netnews",
            "venue": "International Conference on Machine Learning (ICML).",
            "year": 1995
        },
        {
            "authors": [
                "Hector Levesque",
                "Ernest Davis",
                "Leora Morgenstern."
            ],
            "title": "The winograd schema challenge",
            "venue": "Thirteenth international conference on the principles of knowledge representation and reasoning.",
            "year": 2012
        },
        {
            "authors": [
                "Linyang Li",
                "Ruotian Ma",
                "Qipeng Guo",
                "Xiangyang Xue",
                "Xipeng Qiu."
            ],
            "title": "Bert-attack: Adversarial attack against bert using bert",
            "venue": "Conference on Empirical Methods in Natural Language Processing (EMNLP).",
            "year": 2020
        },
        {
            "authors": [
                "Zongyi Li",
                "Jianhan Xu",
                "Jiehang Zeng",
                "Linyang Li",
                "Xiaoqing Zheng",
                "Qi Zhang",
                "Kai-Wei Chang",
                "Cho-Jui Hsieh."
            ],
            "title": "Searching for an effective defender: Benchmarking defense against adversarial word substitution",
            "venue": "Conference on Empirical Methods in",
            "year": 2021
        },
        {
            "authors": [
                "Alisa Liu",
                "Swabha Swayamdipta",
                "Noah A Smith",
                "Yejin Choi."
            ],
            "title": "Wanli: Worker and ai collaboration for natural language inference dataset creation",
            "venue": "arXiv preprint arXiv:2201.05955.",
            "year": 2022
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Roberta: A robustly optimized bert pretraining approach",
            "venue": "arXiv preprint arXiv:1907.11692.",
            "year": 2019
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter."
            ],
            "title": "Decoupled weight decay regularization",
            "venue": "International Conference on Learning Representations (ICLR).",
            "year": 2019
        },
        {
            "authors": [
                "Andrew Maas",
                "Raymond E Daly",
                "Peter T Pham",
                "Dan Huang",
                "Andrew Y Ng",
                "Christopher Potts."
            ],
            "title": "Learning word vectors for sentiment analysis",
            "venue": "Annual Meeting of the Association for Computational Linguistics (ACL).",
            "year": 2011
        },
        {
            "authors": [
                "Aleksander Madry",
                "Aleksandar Makelov",
                "Ludwig Schmidt",
                "Dimitris Tsipras",
                "Adrian Vladu."
            ],
            "title": "Towards deep learning models resistant to adversarial attacks",
            "venue": "International Conference on Learning Representations (ICLR).",
            "year": 2018
        },
        {
            "authors": [
                "Julian McAuley",
                "Jure Leskovec."
            ],
            "title": "Hidden factors and hidden topics: understanding rating dimensions with review text",
            "venue": "Proceedings of the 7th ACM conference on Recommender systems, pages 165\u2013172.",
            "year": 2013
        },
        {
            "authors": [
                "Tom McCoy",
                "Ellie Pavlick",
                "Tal Linzen."
            ],
            "title": "Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference",
            "venue": "Annual Meeting of the Association for Computational Linguistics (ACL).",
            "year": 2019
        },
        {
            "authors": [
                "Seung Jun Moon",
                "Sangwoo Mo",
                "Kimin Lee",
                "Jaeho Lee",
                "Jinwoo Shin."
            ],
            "title": "Masker: Masked keyword regularization for reliable text classification",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence.",
            "year": 2021
        },
        {
            "authors": [
                "Junhyun Nam",
                "Jaehyung Kim",
                "Jaeho Lee",
                "Jinwoo Shin."
            ],
            "title": "Spread spurious attribute: Improving worst-group accuracy with spurious attribute estimation",
            "venue": "International Conference on Learning Representations (ICLR).",
            "year": 2022
        },
        {
            "authors": [
                "Nathan Ng",
                "Kyunghyun Cho",
                "Marzyeh Ghassemi."
            ],
            "title": "Ssmba: Self-supervised manifold based data augmentation for improving out-of-domain robustness",
            "venue": "Conference on Empirical Methods in Natural Language Processing (EMNLP).",
            "year": 2020
        },
        {
            "authors": [
                "Yixin Nie",
                "Adina Williams",
                "Emily Dinan",
                "Mohit Bansal",
                "Jason Weston",
                "Douwe Kiela."
            ],
            "title": "Adversarial nli: A new benchmark for natural language understanding",
            "venue": "Annual Meeting of the Association for Computational Linguistics (ACL).",
            "year": 2020
        },
        {
            "authors": [
                "Timothy Niven",
                "Hung-Yu Kao."
            ],
            "title": "Probing neural network comprehension of natural language arguments",
            "venue": "Annual Meeting of the Association for Computational Linguistics (ACL).",
            "year": 2019
        },
        {
            "authors": [
                "Jungsoo Park",
                "Gyuwan Kim",
                "Jaewoo Kang."
            ],
            "title": "Consistency training with virtual adversarial discrete perturbation",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language",
            "year": 2022
        },
        {
            "authors": [
                "Christopher Potts",
                "Zhengxuan Wu",
                "Atticus Geiger",
                "Douwe Kiela."
            ],
            "title": "Dynasent: A dynamic benchmark for sentiment analysis",
            "venue": "Annual Meeting of the Association for Computational Linguistics (ACL).",
            "year": 2021
        },
        {
            "authors": [
                "Alec Radford",
                "Jeffrey Wu",
                "Rewon Child",
                "David Luan",
                "Dario Amodei",
                "Ilya Sutskever"
            ],
            "title": "Language models are unsupervised multitask learners",
            "venue": "OpenAI blog,",
            "year": 2019
        },
        {
            "authors": [
                "Shiori Sagawa",
                "Pang Wei Koh",
                "Tatsunori B Hashimoto",
                "Percy Liang."
            ],
            "title": "Distributionally robust neural networks for group shifts: On the importance of regularization for worst-case generalization",
            "venue": "International Conference on Learning Representations",
            "year": 2020
        },
        {
            "authors": [
                "Lakshay Sharma",
                "Laura Graesser",
                "Nikita Nangia",
                "Utku Evci."
            ],
            "title": "Natural language understanding with the quora question pairs dataset",
            "venue": "arXiv preprint arXiv:1907.01041.",
            "year": 2019
        },
        {
            "authors": [
                "Dinggang Shen",
                "Guorong Wu",
                "Heung-Il Suk."
            ],
            "title": "Deep learning in medical image analysis",
            "venue": "Annual review of biomedical engineering, 19:221.",
            "year": 2017
        },
        {
            "authors": [
                "Emily Sheng",
                "David C Uthus."
            ],
            "title": "Investigating societal biases in a poetry composition system",
            "venue": "Proceedings of the Second Workshop on Gender Bias in Natural Language Processing, pages 93\u2013106.",
            "year": 2020
        },
        {
            "authors": [
                "Richard Socher",
                "Alex Perelygin",
                "Jean Wu",
                "Jason Chuang",
                "Christopher D Manning",
                "Andrew Y Ng",
                "Christopher Potts."
            ],
            "title": "Recursive deep models for semantic compositionality over a sentiment treebank",
            "venue": "Conference on Empirical Methods in Natural Lan-",
            "year": 2013
        },
        {
            "authors": [
                "Jihoon Tack",
                "Sangwoo Mo",
                "Jongheon Jeong",
                "Jinwoo Shin."
            ],
            "title": "Csi: Novelty detection via contrastive learning on distributionally shifted instances",
            "venue": "Advances in Neural Information Processing Systems (NeurIPS).",
            "year": 2020
        },
        {
            "authors": [
                "Antti Tarvainen",
                "Harri Valpola."
            ],
            "title": "Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results",
            "venue": "Advances in Neural Information Processing Systems (NeurIPS).",
            "year": 2017
        },
        {
            "authors": [
                "James Thorne",
                "Andreas Vlachos",
                "Christos Christodoulopoulos",
                "Arpit Mittal."
            ],
            "title": "Fever: a large-scale dataset for fact extraction and verification",
            "venue": "NAACL-HLT.",
            "year": 2018
        },
        {
            "authors": [
                "Rheeya Uppaal",
                "Junjie Hu",
                "Yixuan Li."
            ],
            "title": "Is finetuning needed? pre-trained language models are near perfect for out-of-domain detection",
            "venue": "Annual Meeting of the Association for Computational Linguistics (ACL).",
            "year": 2023
        },
        {
            "authors": [
                "Alex Wang",
                "Amanpreet Singh",
                "Julian Michael",
                "Felix Hill",
                "Omer Levy",
                "Samuel R Bowman."
            ],
            "title": "Glue: A multi-task benchmark and analysis platform for natural language understanding",
            "venue": "International Conference on Learning Representations (ICLR).",
            "year": 2019
        },
        {
            "authors": [
                "Boxin Wang",
                "Shuohang Wang",
                "Yu Cheng",
                "Zhe Gan",
                "Ruoxi Jia",
                "Bo Li",
                "Jingjing Liu."
            ],
            "title": "Infobert: Improving robustness of language models from an information theoretic perspective",
            "venue": "International Conference on Learning Representations (ICLR).",
            "year": 2021
        },
        {
            "authors": [
                "Boxin Wang",
                "Chejian Xu",
                "Shuohang Wang",
                "Zhe Gan",
                "Yu Cheng",
                "Jianfeng Gao",
                "Ahmed Hassan Awadallah",
                "Bo Li."
            ],
            "title": "Adversarial glue: A multitask benchmark for robustness evaluation of language models",
            "venue": "Thirty-fifth Conference on Neural In-",
            "year": 2021
        },
        {
            "authors": [
                "Adina Williams",
                "Nikita Nangia",
                "Samuel Bowman."
            ],
            "title": "A broad-coverage challenge corpus for sentence understanding through inference",
            "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Lin-",
            "year": 2018
        },
        {
            "authors": [
                "Dongxian Wu",
                "Shu-Tao Xia",
                "Yisen Wang."
            ],
            "title": "Adversarial weight perturbation helps robust generalization",
            "venue": "Advances in Neural Information Processing Systems (NeurIPS).",
            "year": 2020
        },
        {
            "authors": [
                "Lijun Wu",
                "Juntao Li",
                "Yue Wang",
                "Qi Meng",
                "Tao Qin",
                "Wei Chen",
                "Min Zhang",
                "Tie-Yan Liu"
            ],
            "title": "Rdrop: Regularized dropout for neural networks. In Advances in Neural Information Processing Systems (NeurIPS)",
            "year": 2021
        },
        {
            "authors": [
                "Runxin Xu",
                "Fuli Luo",
                "Zhiyuan Zhang",
                "Chuanqi Tan",
                "Baobao Chang",
                "Songfang Huang",
                "Fei Huang."
            ],
            "title": "Raise a child in large language model: Towards effective and generalizable fine-tuning",
            "venue": "Conference on Empirical Methods in Natural Lan-",
            "year": 2021
        },
        {
            "authors": [
                "LI Xuhong",
                "Yves Grandvalet",
                "Franck Davoine."
            ],
            "title": "Explicit inductive bias for transfer learning with convolutional networks",
            "venue": "International Conference on Machine Learning (ICML).",
            "year": 2018
        },
        {
            "authors": [
                "Zhilin Yang",
                "Zihang Dai",
                "Yiming Yang",
                "Jaime Carbonell",
                "Russ R Salakhutdinov",
                "Quoc V Le."
            ],
            "title": "Xlnet: Generalized autoregressive pretraining for language understanding",
            "venue": "Advances in Neural Information Processing Systems (NeurIPS).",
            "year": 2019
        },
        {
            "authors": [
                "Dinghuai Zhang",
                "Kartik Ahuja",
                "Yilun Xu",
                "Yisen Wang",
                "Aaron Courville"
            ],
            "title": "Can subnetwork structure be the key to out-of-distribution generalization",
            "venue": "In International Conference on Machine Learning (ICML)",
            "year": 2021
        },
        {
            "authors": [
                "Hongyang Zhang",
                "Yaodong Yu",
                "Jiantao Jiao",
                "Eric Xing",
                "Laurent El Ghaoui",
                "Michael Jordan."
            ],
            "title": "Theoretically principled trade-off between robustness and accuracy",
            "venue": "International Conference on Machine Learning (ICML).",
            "year": 2019
        },
        {
            "authors": [
                "Xiang Zhang",
                "Junbo Zhao",
                "Yann LeCun."
            ],
            "title": "Character-level convolutional networks for text classification",
            "venue": "Advances in Neural Information Processing Systems (NeurIPS).",
            "year": 2015
        },
        {
            "authors": [
                "Wenxuan Zhou",
                "Fangyu Liu",
                "Muhao Chen."
            ],
            "title": "Contrastive out-of-distribution detection for pretrained transformers",
            "venue": "Conference on Empirical Methods in Natural Language Processing (EMNLP).",
            "year": 2021
        },
        {
            "authors": [
                "Chen Zhu",
                "Yu Cheng",
                "Zhe Gan",
                "Siqi Sun",
                "Tom Goldstein",
                "Jingjing Liu."
            ],
            "title": "Freelb: Enhanced adversarial training for natural language understanding",
            "venue": "International Conference on Learning Representations (ICLR).",
            "year": 2020
        },
        {
            "authors": [
                "datasets/yelp_review_full"
            ],
            "title": "Amazon (McAuley and Leskovec, 2013): The Amazon reviews dataset consists of reviews from amazon with a rating",
            "year": 2013
        },
        {
            "authors": [
                "huggingface.co/datasets/imdb"
            ],
            "title": "cIMDB (Kaushik et al., 2020): Given documents and their initial labels, (Kaushik et al., 2020) asked people to change each one so that it matched a counterfactual target",
            "year": 2020
        },
        {
            "authors": [
                "alisawuffles/wanli"
            ],
            "title": "NLI Diagnostics uses naturally-occurring sentences from several domains to evaluate the variety of linguistic phenomena",
            "venue": "Diag (Wang et al.,",
            "year": 2019
        },
        {
            "authors": [
                "\u2022 HANS (McCoy"
            ],
            "title": "2019): Based on the lexical overlap between the premise and the hypothesis, HANS seeks faulty syntactic heuristics",
            "year": 2019
        },
        {
            "authors": [
                "\u2022 WNLI (Levesque"
            ],
            "title": "2012): Winograd NIL (WNLI) is from the Winograd Schema Challenge",
            "venue": "(Levesque et al.,",
            "year": 2012
        },
        {
            "authors": [
                "\u25e6 TextFooler (Jin"
            ],
            "title": "2020): We denote the dataset with the adversarial texts from vanilla fine-tuned BERT as (1) TF-B. Similarly, the dataset with the adversarial text from vanilla fine-tuned RoBERTa",
            "year": 2020
        },
        {
            "authors": [
                "Wang"
            ],
            "title": "2021b) systematically apply 14 textual adversarial attack methods to GLUE tasks to construct AdvGLUE, which is further validated by humans for reliable annotations",
            "year": 2021
        },
        {
            "authors": [
                "\u25e6 AdvWeight (Bahri"
            ],
            "title": "2022) adds adversarial noise on the model parameters rather than input. It is noteworthy that this method is also called as Sharpness-Aware Minimization (SAM) (Foret et al., 2021)",
            "year": 2021
        },
        {
            "authors": [
                "\u25e6 AdvEmbed (Madry"
            ],
            "title": "2018) imposes adversarial perturbation on the word embeddings of input tokens. We set the same values between the magnitude of perturbation and step size for the gradient ascent step; a step size \u03b4",
            "year": 2018
        },
        {
            "authors": [
                "\u25e6 FreeLB (Zhu"
            ],
            "title": "2020) proposes an efficient way to construct multi-step adversarial perturbation via gradient accumulation",
            "year": 2020
        },
        {
            "authors": [
                "\u25e6 SMART (Jiang"
            ],
            "title": "2020) additionally incorporates the regularization based on Breg proximal point method. Specifically, they use EMA model (Tarvainen and Valpola, 2017) and consistency loss between fine-tuned",
            "year": 2017
        },
        {
            "authors": [
                "\u25e6 RIFT (Dong"
            ],
            "title": "2021) introduce regularization loss which is derived from information-theoretical perspective. RIFT encourages an objective model to retain the features learned from the pre-trained",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Fine-tuning pre-trained language models (Jing and Tian, 2020; Brown et al., 2020) has now become the de facto standard in many NLP tasks (Kenton and Toutanova, 2019; Liu et al., 2019; Wang et al., 2019). The typical practice for evaluating fine-tuned LMs is to measure the task performance (e.g., accuracy) on a fixed (validation) set of labeled samples. However, this evaluation approach may fall short in ensuring the reliability of fine-tuned LMs, as they are still prone to robustness issues in real-world usage. For example, their predictions are known to be still vulnerable to small word-level\n\u2217Work done during a Meta AI internship.\nperturbations (Jin et al., 2020; Li et al., 2020), and also can be biased by the superficial cues, e.g., keyword or negation (McCoy et al., 2019; Niven and Kao, 2019). As such, it is critical to additionally account for robustness during fine-tuning and evaluation of LMs.\nThe robustness of fine-tuned LMs has been investigated in various perspectives, such as adversarial robustness or distribution-shift generalization (Wang et al., 2021a; Zhou et al., 2021; Nam et al., 2022; Hendrycks et al., 2020). However, existing research exhibits certain limitations as these studies primarily focus on a single perspective of model robustness, rather than considering multiple perspectives simultaneously. Since real-world applications necessitate models to simultaneously exhibit robustness across multiple dimensions, a unified framework is crucial to build a reliable system. Moreover, as diverse and distinct methods are available for enhancing the robustness of each perspective, it is hard for practitioners to find an efficient approach for fine-tuning LMs to ensure such comprehensive reliability. Consequently, these limitations inspire us to explore a single effective way for fine-tuning LMs to improve its robustness in multiple perspectives.\nIn this paper, we propose a simple yet effective fine-tuning technique (Figure 1) that aims to improve the multi-perspective robustness of LMs, coined Robustifying LMs with Adversarial perturbation with Selective Training (ROAST). The high-level idea of ROAST is effectively incorporating two important sources for the model robustness during the fine-tuning: robustness on the perturbed input and generalizable knowledge learned during pre-training of LMs. While both factors have been separately demonstrated to enhance the various facets of robustness, the collaborative framework pursuing unified robustness has been less explored yet. Specifically, ROAST first generates adversarial perturbation and adds it to the training in-"
        },
        {
            "heading": "Language",
            "text": "put. Then, to prevent a large deviation from the pre-trained model while learning new task-relevant knowledge, ROAST updates model parameters selectively; after measuring their relative importance for solving a given task on the fly, only key responsible parameters are updated. We further justify this technique with a theoretical analysis, providing insight into the proposed selective training method.\nTo perform a unified evaluation of the multiperspective robustness of LMs, we construct a new evaluation benchmark using two popular NLP tasks, sentiment classification and entailment tasks; alongside the performance on the validation set, we integrate four distinct aspects of model robustness: distribution-shift generalization (Ng et al., 2020), adversarial robustness (Wang et al., 2021b), model calibration (Desai and Durrett, 2020), and anomaly detection (Tack et al., 2020). Under this robustness benchmark, we demonstrate the effectiveness of ROAST for enhancing the robustness of finetuned LMs. Notably, across the four robustness aspects along with a standard validation accuracy, ROAST yields 18.39% and 7.63% average relative improvement compared to the traditional finetuning methodology on sentiment classification and entailment tasks, respectively. Furthermore, we discover that ROAST significantly improves the robustness of six state-of-the-art LMs, which further indicates its practical usefulness as a simple yet effective solution for robustiying LMs."
        },
        {
            "heading": "2 Background",
            "text": "Multi-perspective of model robustness. For reliable deployment in real-world scenarios (Shen et al., 2017; Gupta et al., 2021), the models should be robust in various perspectives, not just be accurate on a given validation set, drawn from the trained distribution. To this end, various perspectives of model robustness have been investigated.\nDistribution-shift (i.e., out-of-domain) generalization evaluates how well the models can generalize to various forms of distribution shift from trained one at inference time (Ng et al., 2020; Liu et al., 2022). For example, a classifier trained on a sentiment classification dataset is expected to also perform well on another sentiment classification dataset, as both datasets are constructed to solve the same task. On the other hand, it is well known that the predictions of deep neural networks can be arbitrarily wrong, even with human-imperceptible adversarial perturbations (Goodfellow et al., 2015; Zhang et al., 2019; Wu et al., 2020). Since this problem is not exceptional for LMs with word- or sentence-level adversarial perturbation (Jin et al., 2020; Li et al., 2020), resiliency to the adversarial examples is also an important perspective of model robustness. In addition, model calibration considers the alignment between the model\u2019s predicted probability and the true correctness likelihood (Guo et al., 2017; Desai and Durrett, 2020), and hence it is essential for the reliability and interpretability of model prediction. Lastly, anomaly detection performance measures the model\u2019s capability to distinguish whether a given input is drawn out-ofdistribution (OOD) of the training set (Hendrycks et al., 2020; Zhou et al., 2021) or not. In the vision domain, Hendrycks et al. (2022) recently investigates the effectiveness of existing data augmentation methods to enhance robustness in multiple perspectives; however, such investigation has not yet been explored in NLP.\nEnhancing robustness of language models. To enhance the robustness of fine-tuned LMs upon (limited) training in the downstream tasks (Kim et al., 2022), various fine-tuning techniques have been explored. One line of work has explored perturbation-based regularization, which enhances model robustness by simulating the specific types\nof perturbation to the inputs during fine-tuning. For example, Wu et al. (2021); Chen et al. (2021a) utilize Dropout to impose a stochastic perturbation to consider a different view of input, and Ng et al. (2020) substitutes the words using pre-trained masked LMs (e.g., BERT (Kenton and Toutanova, 2019)). In addition, Zhu et al. (2020); Li et al. (2021) impose adversarial perturbation on word embeddings to generate a challenging view of inputs. More interestingly, Kireev et al. (2022) reveals that training with adversarial perturbation can be effective for improving model calibration. On the other hand, another line of work has focused on preserving the generalizable knowledge learned during pre-training of LMs; Aghajanyan et al. (2021) identifies a risk of losing the generalizable knowledge during fine-tuning and proposes a noise-based regularization to prevent it. Chen et al. (2020) directly minimizes the distance between the parameters of fine-tuned and pre-trained models as regularization, and Xu et al. (2021) proposes to only update a fixed subset of model parameters during the entire finetuning. A two-step strategy of linear probing and then full fine-tuning has recently been shown to be effective for distribution-shift generalization by reducing the deviation from the pre-trained model (Kumar et al., 2022). In addition, the recent result (Uppaal et al., 2023) indicates the importance of generalizable knowledge in pre-trained LMs for better anomaly detection. As both approaches enhance the different perspectives of model robustness, the effective framework for their collaborative utilization is expected to can serve as a unified way for robusifying LMs. However, such direction is under-explored from now on, and we try to fill this gap in this work."
        },
        {
            "heading": "3 Robustifying LMs via Adversarial Perturbation and Selective Training",
            "text": "Overview. In this section, we present our method, ROAST, that aims to enhance the multi-perspective robustness of LMs in a unified way. Our main idea is collaboratively incorporating two important sources of model robustness during fine-tuning; we improve the generalization of LMs on the perturbed input using adversarial perturbation and preserve the generalizable knowledge within pretrained LMs via selective training with gradient masking. Figure 1 shows an overview of ROAST. Next, we describe in detail our techniques to improve the robustness of fine-tuned LMs.\nAdversarial training. To improve the robustness of LM f\u0398, we first incorporate adversarial perturbation during fine-tuning. Specifically, at each training iteration, we construct an adversarial example x\u0303 for training example x. Instead of discrete tokenlevel adversarial perturbation (Jin et al., 2020), we consider embedding-level continuous perturbation (Zhu et al., 2020) which adds noise to the word embedding of each token. Specifically, to construct x\u0303, we use a single-step gradient ascent (Jiang et al., 2020) with a step size \u03b4 under \u2113\u221e norm: x\u0303 := x + \u03b4 \u00b7 (\u2202Ltask/\u2202x)/||\u2202Ltask/\u2202x||\u221e. We then train f\u0398 with a following training loss:\nLtrain = Ltask(x, y) + Ltask(x\u0303, y) + \u03bbLcons(x, x\u0303), (1)\nwhere Ltask is a task-specific loss (e.g., crossentropy) with given label y and Lcons(x, x\u0303) := DKL(f\u0398(x), f\u0398(x\u0303)) + DKL(f\u0398(x\u0303), f\u0398(x)) is bidirectional KL divergence (Wu et al., 2021). Selective model training via gradient masking. However, fine-tuning with adversarial perturbation could be suboptimal in some perspectives of model robustness, as it incurs a relatively high training loss compared to naive fine-tuning (Dong et al., 2021) and hence may lose useful pre-trained knowledge due to large updates. Hence, to reduce such a potential risk, we reduce an unnecessary deviation by explicitly constraining the update. Specifically, during each iteration of the model update, we selectively update the model parameters by masking out the gradient of less important ones to solve a given task. To measure the relative importance score s(\u03b8) of each model parameter \u03b8 \u2208 \u0398, we use the sum of the square of gradients1:\ns(\u03b8) = \u2211\n(x,y)\u2208D\n|g(\u03b8)|2, g(\u03b8) = \u2202Ltrain/\u2202\u03b8, (2)\nNote that s(\u03b8) is highly connected to Fisher\u2019s information matrix, which provides a point estimate of the task-relevant importance of the parameters (Kirkpatrick et al., 2017; Xuhong et al., 2018). However, the calculation of gradients for all samples in D significantly increases training cost. To alleviate this, we approximate them by using the gradients calculated during the backward pass of model training, i.e., which can be obtained for free. Although different values of \u03b8 are used to calculate the gradients at each training step, we empirically\n1For solving the given task with D and training loss Ltrain\nverify that such an approximation is effective and remark that similar approaches have been adopted in other domains (Berthelot et al., 2019).\nUsing the importance scores {s(\u03b8)|\u03b8 \u2208 \u0398}, ROAST decides whether to update each model parameter \u03b8 or not. Specifically, we first obtain a normalized score s\u0303(\u03b8) from the relative order between |s(\u03b8)| such that s\u0303(\u03b8min) = 0 \u2264 s\u0303(\u03b8) \u2264 1 = s\u0303(\u03b8max) where \u03b8min := argmin\u03b8\u2208\u0398 s(\u03b8) and \u03b8max := argmax\u03b8\u2208\u0398 s(\u03b8). Then, we set a sampling probability p(\u03b8) using a smooth approximation of the Heaviside step function with the logistic function (Davies, 2002):\np(\u03b8) = 1/ ( 1 + exp ( 2\u03b2(s\u0303(\u03b8)\u2212 \u03b1) )) , (3)\nwhere \u03b1 and \u03b2 are hyper-parameters that control the masking ratio and smoothness, respectively. Remarkably, p(\u03b8) becomes the hard thresholding that has been utilized in prior studies (Zhang et al., 2021; Xu et al., 2021) as \u03b2 \u2192 \u221e. Compared to this, our smooth approximation enables a more calibrated use of the importance score. Then, gradient mask m(\u03b8) is sampled from Bernoulli distribution with a probability p(\u03b8), and ROAST selectively updates model parameters by masking the gradient using m(\u03b8) with a scaling term 1/p(\u03b8):\n\u03b8 \u2190 \u03b8 \u2212 \u03b7 \u00b7 g\u0303(\u03b8), g\u0303(\u03b8) := ( m(\u03b8)/p(\u03b8) ) \u2299 g(\u03b8), (4)\nwhere \u03b7 denotes a learning rate. To demonstrate the soundness of proposed selective training with a masked gradient g\u0303(\u03b8), we derive a theoretical corollary based on previous result (Xu et al., 2021).\nCorollary. Under mild assumptions, the masked gradient g\u0303(\u03b8) becomes an unbiased estimator of the original gradient g(\u03b8), i.e., E[g\u0303(\u03b8)] = g(\u03b8), and the norm of covariance is upper bounded.\nWe present a formal derivation of Corollary in Appendix B. The corollary establishes that, under certain conditions, the masked gradient g\u0303(\u03b8) is an unbiased estimator of the original gradient g(\u03b8), which indicates that the masked gradient correctly identifies the true gradient on average; therefore, the variance introduced by masking doesn\u2019t introduce a systematic bias that could deviate the model from converging to its optimal parameters. In addition, the upper bound on the norm of covariance provides confidence in the stability of this estimator. In practical terms, it assures us that the masked gradient won\u2019t be too volatile or far off from the\ntrue gradient, thus safeguarding the convergence properties of the optimization process.\nIn practice, we accumulate the gradients during each training epoch to calculate the importance scores, and then generate the gradient masks for the next epoch. In addition, we empirically observe that ROAST can work without a scaling term and it sometimes outperforms the original. Hence, we consider whether to apply scaling as an additional hyper-parameter. The overall procedure of ROAST is described in Algorithm 1. More details are presented in Appendix A.3."
        },
        {
            "heading": "4 Experiments",
            "text": "In this section, we evaluate the effectiveness of ROAST to enhance the robustness of fine-tuned LMs in multi-perspectives. We first describe the experimental setups, including how the robustness evaluation benchmark is constructed, in Section 4.1. In Section 4.2, we present experimental results of ROAST and other baselines on the constructed benchmark. In Section 4.3, we provide more analysis of ROAST including (a) ablation study, (b) comparison with different methods sharing similar intuition, and (c) qualitative analysis."
        },
        {
            "heading": "4.1 Experimental Setup",
            "text": "Tasks and metrics. We select two popular NLP tasks, sentiment classification and entailment, to measure the robustness of fine-tuned LMs in a unified way. We take training sets of SST-2 (Socher et al., 2013) and MNLI (Williams et al., 2018) as training data for each task, respectively. \u2022 In-distribution performance (Accin): We first evaluate model performance with respect to training distribution: we measure the accuracy on the validation sets following the common practice. \u2022 Distribution-shift generalization (Accshift): To evaluate model capability on out-of-domain generalization, we measure the average accuracy on multiple distribution shifted datasets, i.e., different datasets with the same task. For entailment, we follow the setups in Liu et al. (2022) \u2013 7 different entailment datasets are used to evaluate the entailment classifier. For sentiment classification, we use 5 different datasets following Potts et al. (2021). \u2022 Adversarial robustness (Accadv): To measure the adversarial robustness of LMs, we first construct text-level adversarial examples using TextFooler (Jin et al., 2020) on vanilla fine-tuned BERT and RoBERTa models following Wang et al. (2021a).\nWe also consider the datasets constructed via human-in-the-loop dynamic adversarial data collection (Nie et al., 2020; Potts et al., 2021). In addition, we use the datasets from a recent benchmark for adversarial robustness, AdvGLUE (Wang et al., 2021b), which incorporate various types of adversarial noises. We report the average performance on these datasets. \u2022 Model calibration (ECE): To measure the model\u2019s calibration performance, we report the average Expected Calibration Error (Guo et al., 2017), calculated during the evaluations on different datasets including in-distribution, distributionshifted, and adversarial datasets. As a lower ECE indicates a better calibration unlike other considered robustness metrics, we denote it with (\u2193). \u2022 Anomaly detection (AUROC): To measure model performance on anomaly detection, we use multiple external datasets as anomaly samples following the setups in recent studies (Hendrycks et al., 2020; Zhou et al., 2021). We use the maximum softmax probability score (Hendrycks et al., 2020) and AUROC for the evaluation method and metric.\nTo evaluate multiple robustness aspects in a unified way, we first report average relative improvement \u2206avg compared to the vanilla algorithm across different evaluation metrics:\n\u2206avg := 1 |S| \u2211\ns\u2208S \u2206s, \u2206s =\ns\u2212 sbase smax \u2212 sbase , (5)\nS = {Accin,Accshift,Accadv,ECE,AUROC} and 0 \u2264 \u2206s \u2264 1. s and sbase are the performances of a given method and vanilla, respectively. smax denotes the best score of each metric: 100 for accuracy metrics, 0 for ECE, and 1 for AUROC. Here, we note that we present the AUROC values as 100 \u00d7 AUROC in our tables to maintain a consistent scale with other metrics. Also, we use a relative average instead of a direct average for measuring multi-perspective robustness, as it is more appropriate for the problem at hand; to evaluate LMs\u2019 multi-perspective robustness, it is crucial to equally incorporate multiple metrics from various perspectives. However, these metrics often have different scales, and the direct average is limited in preventing a single metric from dominating the aggregate results due to its numerical magnitude. In addition, we report the average rank, Rankavg, among multiple baseline algorithms averaged across five different measurements in S. More details about datasets are presented in Appendix A.1.\nBaselines. We compare ROAST with various fine-tuning algorithms; we first consider a na\u00efve fine-tuning method, denoted by Vanilla. We then consider a wide range of perturbation-based finetuning algorithms: (1a) WordDrop (Guo et al., 2020): dropping input words; (1b) HiddenCut (Chen et al., 2021a): dropping spanned hidden features; (1c) AdvWeight (Bahri et al., 2022): adding adversarial perturbation on the model parameters; (1d) AdvEmbed (Madry et al., 2018): adding adversarial perturbation on the word embeddings. (1e) FreeLB (Zhu et al., 2020): introducing efficient adversarial training with gradient accumulation. (1f ) SMART (Jiang et al., 2020): in addition to adversarial perturbation, introducing EMA-based Bregman proximal point regularization. (1g) RIFT (Dong et al., 2021): introducing adversarial finetuning method from an information-theoretical perspective. Furthermore, we consider recent state-ofthe-art algorithms that preserve the generalizable knowledge of pre-trained language models during fine-tuning: (2a) R3F (Aghajanyan et al., 2021): noise-based consistency regularization to prevent representation collapse; (2b) Weight Consolidation (WCons) (Chen et al., 2020): incorporation of \u21132 distance between trained and pre-trained models as a regularization; (2c) Child-tuning (C-Tune) (Xu et al., 2021): selective update of model parameters with a sampled child model; (2d) LP-FT (Kumar et al., 2022): two-step strategy of linear probing and then full fine-tuning. More details of baselines are presented in Appendix A.2.\nTraining details. All models are trained using AdamW (Loshchilov and Hutter, 2019) with its default parameters (\u03b21, \u03b22, \u03f5)=(0.9, 0.98, 1e-6) and a weight decay of 0.01. We use linear learning rate decay with warmup ratio 0.06 and learning rate \u03b7=1e-5 (Liu et al., 2019). The models are finetuned with batch size 16 for 10 epochs. All the experiments are conducted with RoBERTa-large (Liu et al., 2019) except for the experiments in Table 3. Both baselines and our method are optimized with their own hyper-parameters from a set of candidates (described in Appendix A.2) based on the validation set. For ROAST, we use \u03b4 = 0.1 with \u03bb \u2208 {0.01, 0.1, 0.5} for adversarial training. For the hyper-parameters of gradient masking, we use \u03b1 \u2208 [0.6, 0.95], \u03b2 \u2208 {1, 5, 10} along with a scaling term. More details are in Appendix A.3."
        },
        {
            "heading": "4.2 Main results",
            "text": "To evaluate the effectiveness of ROAST for improving the robustness of LMs, we compare it with various baselines by fine-tuning RoBERTa-large model (Liu et al., 2019). Tables 1 and 2 summarize the experimental results on sentiment classification and entailment task, respectively. First, it is worth noting that the common evaluation method using the accuracy on the validation set is not enough to capture the robustness of a given model. For example, all the baselines successfully improve the vanilla fine-tuning on the validation accuracy (Accin), but their robustness sometimes degrades severely as listed in Table 2. Such results support the necessity of considering multi-perspective model robustness in a unified manner, rather than naively focusing on validation accuracy. Also, we note that there is no single best method when considering multiple perspectives of model robustness; this result indicates the value of a unified measurement to facilitate robustness evaluation.\nIn this sense, one can observe that ROAST consistently outperforms the baseline fine-tuning methods. To be specific, across 4 different robustness metrics along with a validation accuracy, ROAST exhibits 18.39 % and 7.63% average relative improvement compared to vanilla fine-tuning on sentiment classification and entailment, respectively. As a result, ROAST achieves an average ranking of 2.7\nwhile the previous best method achieves 4.4. These results demonstrate that ROAST could serve as a simple yet strong method for robustifying LMs. Interestingly, advanced fine-tuning methods are more effective in the sentiment classification task than in the entailment task. One possible reason is the difference in the intrinsic difficulty of the task and training dataset size; while the size of the training data is much smaller for SST-2 (67k vs MNLI: 393k), the accuracy is much higher in the sentiment classification task. This indicates that LMs could be more vulnerable to overfitting, and hence regularization of training could be more effective.\nTo further demonstrate the effectiveness of ROAST, we verify its compatibility across different types of pre-trained LMs. Specifically, in addition to RoBERTa-large, we conduct additional experiments with five recent state-of-the-art LMs which have the similar number of model parameters: BERT-large (Kenton and Toutanova, 2019), ALBERT-xxlarge (Lan et al., 2020), XLNet-large (Yang et al., 2019), ELECTRA-large (Clark et al., 2020), and DeBERTa-large (He et al., 2021). We note that the best hyper-parameters found in Tables 1 and 2 are inherited without additional cost from a separate search. In Table 3, we present the experimental results by comparing the average relative improvement of ROAST compared to two representative baselines, AdvEmbed and WConsol,\nwhich show competitive performance in Tables 1 and 2. Here, to facilitate the comparison between different LMs, we calculate the average relative improvement using the vanilla fine-tuned BERT-large as the common baseline for sbase in Eq. 5. We observe that ROAST significantly improves the robustness of fine-tuned models regardless of the type of LMs. More interestingly, ROAST could be useful to reveal the true potential of LMs; DeBERTalarge becomes the most robust LM with ROAST while it was originally far from the best. Detailed results on each dataset and LM are presented in Appendix D and E, respectively."
        },
        {
            "heading": "4.3 More analysis with ROAST",
            "text": "Ablation study. To validate the proposed components of ROAST, we conduct an ablation study; specifically, we fine-tune RoBERTa-large on SST-2 by varying each component of ROAST: (a) adversarial perturbation (Advp) in Eq.1 and selective update of the model parameters with (b) thresholding (Thre) using relative importance s(\u03b8) in Eq.2, (c) scaling (Scal) with smooth approximation using p(\u03b8) in Eq.3, and (d) sampling (Samp) from Bernoulli distribution with m(\u03b8) in Eq.4. Then, we evaluate the robustness of each method using the constructed robustness benchmark as same as Table 1. We summarize the results in Table 4. As shown in Table 4, the incorporation of the proposed importance score via re-scaling, denoted by (d), performs slightly worse than naive adversarial training, denoted by (a). However, the incorporation of the importance score via a selective update with hard thresholding, denoted by (c), outperforms both.\nThis result indicates that the better robustness of s(\u03b8) in Table 4 is not solely due to incorporating the importance score into the model update, but rather to the careful design of using them via selective training with sparse gradients. Moreover, ROAST explicitly constrains the model update with m(\u03b8), significantly enhancing the robustness from its two distinct advantages for training the model. Firstly, it preserves the parameters by selectively training them using masked (i.e., sparse) gradients. Secondly, it reduces distortion from the original gradients by sampling the mask instead of deterministic selection. Here, the gain from the first advantage is not only obtainable from m(\u03b8). Hard thresholding via s(\u03b8) also shares the same advantages of selective training and it can be verified with the results in Table 4, i.e., (c) > (a), (b), (d). However, it is important to note that its effectiveness can be limited since thresholding can distort the gradients from the original direction through deterministic updates of important parameters. Therefore, the second advantage of m(\u03b8) is crucial, as it improves selective training by reducing the risk of distortion. By sampling the mask for updates from the distribution p(\u03b8), which prioritizes important parameters, m(\u03b8) continuously benefits from selective training while also covering overlooked parameters through\nstochastic sampling. This essential advantage of integrating m(\u03b8) is demonstrated by improvements over (c,d) in Table 4.\nComparison to methods with similar intuition. The key intuition of ROAST is effectively learning a given task under challenging perturbation while preserving the generalizable knowledge within pretrained LM. ROAST achieves this goal via selective model training, but one can consider different ways for the same purpose. For example, SMART (Jiang et al., 2020) and RIFT (Dong et al., 2021) introduce additional regularization to the preservation of pre-trained knowledge upon the adversarial training. To demonstrate the superiority of ROAST, we additionally consider the extra baselines with similar intuition, WCons\u2217 and LP-FT\u2217, by incorporating adversarial perturbation to the methods for the preservation. model robust under perturbations as well. As shown in Table 1 and 2, ROAST significantly outperforms SMART and RIFT; since both approaches additionally introduce the regularization loss, it can induce the learning difficulty to find Pareto optimal between learning and preservation. Also, in Table 5, one can further verify that ROAST significantly outperforms both WCons\u2217 and LP-FT\u2217. This result clearly demonstrates the effectiveness of the proposed selective\ntraining scheme compared to naive combination with existing works.\nAdditionally, we conduct experiments to verify the effect of different strategies of selective training. Instead of focusing on the relatively important parameters in ROAST (Eq.3), Min gives more weights on the unimportant ones by considering the reversed order to obtain the normalized score, i.e., s\u0303(\u03b8max) = 0 \u2264 s\u0303(\u03b8) \u2264 1 = s\u0303(\u03b8min). Rand randomly assigns s\u0303(\u03b8) regardless of the relative importance score s(\u03b8). From Table 5, we find that the effectiveness of Min and Rand is largely degraded compared to ROAST. Remarkably, Min shows worse results than Rand, which reveals the importance of focusing on the task-relevant, important model parameters for selective training. Qualitative analysis. We further present qualitative analysis on ROAST. To this end, we consider RoBERTa-large on SST-2 with two different strengths of adversarial perturbation to facilitate the analysis: Weak (\u03bb = 0.01) and Strong (\u03bb = 0.5). Then, we measure the similarity between the initial pre-trained model and fine-tuned one using centered kernel alignment (CKA) (Kornblith et al., 2019). Specifically, we measure the average CKA between each layer\u2019s hidden features across training epochs. In Figure 2(a), we observe that stronger adversarial training incurs larger deviation from the pre-trained model, and it could be effectively prevented by ROAST. This result indicates that ROAST helps fine-tuned model to preserve the generalizable knowledge of the pre-trained LM while learning a new task under adversarial perturbation, and hence can effectively improve the model robustness (Figure 2(b)). In addition, we investigate the dynamics of gradient masking by measuring\nthe intersection over union between (1) masks at the first epoch and other epochs (SST-2 (Init)), (2) masks at the last epoch and other epochs (SST-2 (Last)), and (3) masks from SST-2 and MNLI at each epoch (SST-2 (MNLI)). As the model is trained by focusing on the few task-relevant parameters under ROAST, the sampled masks become taskspecific and far from the initial one as training goes on (Figure 2(c)). The adaptability of ROAST for each task is further observed from a low similarity between the masks of different tasks."
        },
        {
            "heading": "5 Conclusion",
            "text": "In this paper, we propose to consider multiple aspects of model robustness for the reliable deployment of LMs in real-world settings. To improve the robustness of fine-tuned LMs, we present a new fine-tuning method (ROAST) by leveraging adversarial perturbation while preventing its potential risk with efficient selective training. Through extensive experiments under constructed benchmark, we demonstrate the effectiveness of ROAST and its generalizability to multiple state-of-the-art LMs. As investing multiple aspects of model robustness in a unified viewpoint is under-explored in the literature, we expect our work to contribute to this research direction to enable us to use the well-performing pre-trained language models with more reliability. Furthermore, since our proposed method of robust fine-tuning is task- and domainagnostic, we believe that ROAST can benefit other NLP tasks (e.g., question answering) and domains (e.g., vision and graph) as well, as interesting future work directions."
        },
        {
            "heading": "Limitations",
            "text": "Although we have conducted comprehensive experiments on two representative NLP tasks with a wide range of LMs and multiple representative robustness aspects, results and analysis on more datasets, tasks, and domains (e.g., computer vision) would likely draw a more decisive conclusion. In addition to empirical evidence, theoretical analysis of why gradient masking improves language model robustness in aspects like model calibration and anomaly detection is also worth further exploration."
        },
        {
            "heading": "Ethics Statement",
            "text": "The robustness issue of language models has been extensively investigated and a simple yet effective fine-tuning algorithm, ROAST, has been proposed to address this issue. Similar to other fine-tuning methods, the behavior of trained language models with the proposed method might highly rely on the training dataset. Therefore, they can suffer from the inherent problems of the given dataset, e.g., gender bias (Bordia and Bowman, 2019) or annotation artifacts (Gururangan et al., 2018). However, as the proposed idea of ROAST is general and can be easily extended to other training objectives, we believe that the concerns on such problems could be alleviated by incorporating recently proposed solutions for those problems (Sagawa et al., 2020; Moon et al., 2021); for example, one could add a new training objective in Eq.1 to address each problem while preserving the idea of gradient masking of the overall objective (Eq.2)."
        },
        {
            "heading": "A Details on Experimental Setups",
            "text": ""
        },
        {
            "heading": "A.1 Datasets for robustness benchmarks",
            "text": "As described in Section 4.1, we consider two popular NLP tasks, sentiment classification and entailment tasks, to verify the multiple aspects of model robustness with fine-tuned LMs. To fine-tune LMs for both tasks, we use SST-2 (Socher et al., 2013) and MNLI (Williams et al., 2018) datasets.\n\u25e6 SST-2: a binary single sentence classification task about movie reviews with human labels of their sentiment (positive or negative). It is composed of 67k training and 872 validation samples.\n\u2022 MNLI: a ternary entailment task which is composed of 393k training and 20k development samples. Given a pair of sentences (premise and hypothesis), the given task is to predict whether the hypothesis is an entailment, contradiction, or neutral with respect to the premise.\nBoth datasets are available at https: //gluebenchmark.com/tasks. After that we measure the following aspects using the described datasets.\n1. In-distribution accuracy (Accin): To measure the performance with respect to training distribution (i.e., in-distribution), we measure the accuracy on the validation sets provided from both datasets, following a usual practice (Wang et al., 2019).\n2. Distribution-shift generalization (Accshift): To evaluate the model\u2019s capability on distributionshift (i.e., out-of-domain) generalization, we measure the accuracy on multiple distribution shifted datasets. For sentiment classification, we use the following five different sentiment datasets based on the setups in (Potts et al., 2021).\n\u25e6 Yelp (Zhang et al., 2015): The Yelp reviews dataset consists of reviews from Yelp. It is extracted from the Yelp Dataset Challenge 2015 data. As Yelp has five star-rating categories, we bin these ratings by taking the lowest two ratings to be negative and the highest two ratings to be positive, following (Potts et al., 2021). This dataset is available at https://huggingface.co/ datasets/yelp_review_full.\n\u25e6 Amazon (McAuley and Leskovec, 2013): The Amazon reviews dataset consists of reviews from amazon with a rating from 1\nto 5. Hence, similar to Yelp, we take review score 1 and 2 as negative, and 4 and 5 as positive. This binarized dataset is available at https://huggingface.co/ datasets/amazon_polarity.\n\u25e6 IMDB (Maas et al., 2011): IMDB is a dataset for binary sentiment classification on movie reviews. It is composed of 25,000 labeled training samples and 25,000 test samples. We use IMDB dataset provided at https: //huggingface.co/datasets/imdb.\n\u25e6 cIMDB (Kaushik et al., 2020): Given documents and their initial labels, (Kaushik et al., 2020) asked people to change each one so that it matched a counterfactual target label, as long as they avoided making any unnecessary changes to facts that were semantically unrelated to the label\u2019s applicability and produced revisions that resulted in internally consistent documents. The constructed cIMDB dataset is publicly available at https://github.com/acmi-lab/ counterfactually-augmented-data.\n\u25e6 Poem (Sheng and Uthus, 2020): Poem is a binary single classification task about the sentiment of poem verses from Project Gutenberg. There are 892 training, 105 validation, 104 test samples, respectively. The dataset is available at https://huggingface. co/datasets/poem_sentiment\nFor entailment task, we follow the setups in the recent work (Liu et al., 2022); 7 different entailment datasets are used to evaluate the distributionshift generalization of entailment classifier; here, some of the datasets are binary classification rather than ternary (denoted by \u2217). Hence, following (Liu et al., 2022), the MNLI classifier is treated as binary classifier by merging the predicts as neutral or contradiction into not entailment. All the datasets are available at https://github.com/ alisawuffles/wanli.\n\u2022 Diag (Wang et al., 2019): NLI Diagnostics uses naturally-occurring sentences from several domains to evaluate the variety of linguistic phenomena.\n\u2022 HANS (McCoy et al., 2019): Based on the lexical overlap between the premise and the hypothesis, HANS seeks faulty syntactic heuristics.\n\u2022 QNLI (Wang et al., 2019): QNLI is a binary classification task that decides whether the given (question, sentence) pair contains the correct answer (entailment) or not.\n\u2022 WNLI (Levesque et al., 2012): Winograd NIL (WNLI) is from the Winograd Schema Challenge (Levesque et al., 2012), which checks the correct coreference through common sense. By replacing the proper referent, an entailed hypothesis is created, and by replacing the incorrect referent, a non-entailed hypothesis is created.\n\u2022 NQ-NLI (Chen et al., 2021b): Using Natural Questions QA dataset (Kwiatkowski et al., 2019), NQ-NLI creates a decontextualized sentence from the original context for the premise and a hypothesis from a question-andanswer candidate converted into a declarative form.\n\u2022 FEVER-NLI (Thorne et al., 2018): It is adapted from the FEVER dataset (Thorne et al., 2018). In each case, the hypothesis is a statement that is either supported (implied), refuted (contradicted), or neither (neutral), and the premise is a brief context from Wikipedia.\n\u2022 WANLI (Liu et al., 2022): Starting with an existing dataset such as MNLI, the new samples are automatically generated with GPT-3 focusing on the ambiguous samples. To further improve the quality of constructed dataset, automatic filtering is applied and then each sample is annotated by human labelers.\n3. Adversarial robustness (Accadv): To measure the adversarial robustness of model, we first construct the text-level adversarial examples using TextFooler (Jin et al., 2020) on vanilla fine-tuned BERT and RoBERT models, following (Wang et al., 2021a). We also consider the datasets constructed via dynamic adversarial data collection with human-in-the-loop (Nie et al., 2020; Potts et al., 2021). In addition, we use the datasets from a recent benchmark for adversarial robustness, AdvGLUE (Wang et al., 2021b), which incorporate the various types of adversarial noises. Overall, for the sentiment classification, the following five different adversarially constructed datasets are used to measure the adversarial robustness of fine-tuned LMs.\n\u25e6 TextFooler (Jin et al., 2020): We denote the dataset with the adversarial texts from vanilla fine-tuned BERT as (1) TF-B. Similarly, the dataset with the adversarial text from vanilla fine-tuned RoBERTa is denoted as (2) TF-R. The official code of TextFooler is available at https://github.com/jind11/ TextFooler.\n\u25e6 DynaSent (Potts et al., 2021): DynaSent is dynamically constructed through multiple iterations of training a classifier model and finding its adversarial samples by involving a human annotator in the loop. In our experiments, we use the dataset from the first round, (3) DynaSent-R1, and the dataset from the second round, (4) DynaSent-R2. As DynaSent is a ternary sentiment classification (Positive, Neutral, and Negative), we remove the samples with Neutral. Also, we use both validation and test sets for the evaluation. The datasets are publicly released at https://huggingface. co/datasets/dynabench/dynasent.\n\u25e6 AdvGLUE (Wang et al., 2021b): To construct principled and comprehensive benchmark for adversarial robustness in NLP tasks, (Wang et al., 2021b) systematically apply 14 textual adversarial attack methods to GLUE tasks to construct AdvGLUE, which is further validated by humans for reliable annotations. Hence, we measure the robustness of sentiment classifier using the dataset for SST-2 in AdvGLUE and denote it as (5) AdvGLUE (SST-2). AdvGLUE dataset is available at https://adversarialglue.github.io.\nSimilarly, for entailment task, we use the following nine different adversarially constructed datasets to evaluate the adversarial robustness of entailment classifier. Here, -m indicates that the dataset is constructed from MNLI\u2019s matched validation set and -mm indicates from mismatched validation set.\n\u2022 TextFooler (Wang et al., 2019): TF-B and TF-R are defined in the same way with the case of sentiment classification. Hence, we consider (1) TF-B-m, (2) TF-B-mm, (3) TF-R-m, and (4) TF-R-mm. We remark that the corresponding datasets constructed from other researchers are available at https://drive.google.com/file/d/ 1xWwABFkzJ6fEnR1f3xr-vkMesxdO7IZm/ view.\n\u2022 ANLI (Nie et al., 2020): Similar to the case of DynaSent, ANLI is dynamically constructed through multiple iterations of training a classifier model and finding its adversarial samples by involving a human annotator in the loop. As there are three rounds in overall, we utilize all of these datasets: (5) ANLIR1, (6) ANLI-R2, and (7) ANLI-R3. ANLI dataset is available at https://huggingface. co/datasets/anli.\n\u2022 AdvGLUE (Wang et al., 2021b): We use the datasets for MNLI in AdvGLUE and denote it as (8) AdvGLUE-m and (9) AdvGLUE-mm.\n4. Model calibration (ECE): To measure the model\u2019s calibration performance, we report the average Expected Calibration Error (Guo et al., 2017), denoted ECE, calculated during the all evaluations on different datasets including in-distribution, distribution-shifted, and adversarial datasets introduced in above. Namely, we report the average ECE across 11 datasets for sentiment classification and 18 datasets for entailment.\n5. Anomaly detection (AUROC): To measure the performance about the anomaly detection, we use the following four external datasets as anomaly samples based on the setups in the recent related works (Hendrycks et al., 2020; Zhou et al., 2021).\n\u25a1 WMT16 (Bojar et al., 2016): WMT is a translation dataset based on the data from statmt.org and versions exist for different years using a combination of data sources. We use the English source side of English source side of English-German WMT 16, following the previous works. WMT dataset could be downloaded from https://huggingface. co/datasets/wmt16.\n\u25a1 Multi30K (Elliott et al., 2016): Multi30K is a translation datasets which extends the Flickr30K dataset with German translations created by professional translators over a subset of the English descriptions, and independently crowd-sourced descriptions of the original English descriptions. The dataset is available at https://github.com/ multi30k/dataset.\n\u25a1 20 NG (Lang, 1995): 20 Newsgroup is a dataset for topic classification consists of 20 classes. 20 NG dataset is\npublicly available at http://qwone.com/ ~jason/20Newsgroups/.\n\u25a1 QQP (Sharma et al., 2019): QQP is a binary classification datasets for entailment task, where the goal is to determine if two questions in a given pair are semantically equivalent or not. As a part of GLUE benchmark, it is available at https://huggingface.co/ datasets/glue.\nIn addition, we consider that the one dataset becomes anomaly samples to the other, i.e., SST-2 become anomaly dataset with respect to MNLI. Hence, we use total six anomaly datasets in case of sentiment classification and five datasets in case of entailment, respectively."
        },
        {
            "heading": "A.2 Baselines",
            "text": "We consider various baseline fine-tuning algorithms in NLP tasks. Specifically, we first consider a wide range of perturbation-based fine-tuning algorithms and their training loss Ltrain can be described as follow:\nLtrain = Ltask ( f\u0398(x), y ) + Ltask ( f\u0303\u0398(x), y )\n+ \u03bbLcons(f\u0398(x), f\u0303\u0398(x)), (6)\nwhere f\u0303\u0398(x) indicates the perturbed prediction of model f\u0398 for input x. Also, Lcons is a bidirectional KL divergence introduced in Eq.1. Here, for better explanation, we slightly abuse the notations of inputs for Ltask and Lcons, compared to Eq.1. With Eq.6, the baselines in this categories only have a difference in how they impose the perturbation for the prediction:\n\u25e6 WordDrop (Guo et al., 2020) impose the perturbation by randomly dropping the input tokens with a probability pWd similar to Dropout. We select pWD \u2208 {0.05, 0.10, 0.15}.\n\u25e6 HiddenCut (Chen et al., 2021a) drops the contiguous spans within the hidden features of Transformer model during fine-tuning. As the attention-based strategy for sampling the spans shows the best results in (Chen et al., 2021a), we adopt it and tune the HiddenCut ratio pHC \u2208 {0.1, 0.2, 0.3} with the fixed selection ratio of 0.4. The official code is available at https://github.com/ SALT-NLP/HiddenCut.\n\u25e6 AdvWeight (Bahri et al., 2022) adds adversarial noise on the model parameters rather than input. It is noteworthy that this method is also called as Sharpness-Aware Minimization (SAM) (Foret et al., 2021). We tune the step size \u03c1 for gradient ascent step among {0.01, 0.03, 0.05}. We adopt the codes from https://github.com/davda54/sam.\n\u25e6 AdvEmbed (Madry et al., 2018) imposes adversarial perturbation on the word embeddings of input tokens. We set the same values between the magnitude of perturbation and step size for the gradient ascent step; a step size \u03b4 is tuned among {1e-5, 1e-3, 1e-1} under \u2113\u221e norm.\n\u25e6 FreeLB (Zhu et al., 2020) proposes an efficient way to construct multi-step adversarial perturbation via gradient accumulation. We follow the best hyper-parameters provided by the authors after careful tuning. The official code is available at https://github.com/zhuchen03/FreeLB.\n\u25e6 SMART (Jiang et al., 2020) additionally incorporates the regularization based on Breg proximal point method. Specifically, they use EMA model (Tarvainen and Valpola, 2017) and consistency loss between fine-tuned model. We set the same hyper-parameters in the paper, except the coefficient between each loss; for such coefficient, we tune among {0.01, 0.1, 1.0}. The official code is available at https://github.com/namisan/mt-dnn.\n\u25e6 RIFT (Dong et al., 2021) introduce regularization loss which is derived from information-theoretical perspective. RIFT encourages an objective model to retain the features learned from the pre-trained model throughout the entire fine-tuning process. We tune hyper-parameters \u03b1 among {0.1, 0.3, 0.7}, following the official code by authors: https://github.com/ dongxinshuai/RIFT-NeurIPS2021.\nAlso, we commonly tune the hyper-parameter \u03bb among {0.01, 0.1, 0.5} in addition to the specific hyper-parameter of each method.\nOn the other hand, we also consider the recent methods that prevents the model from deviating\ntoo much from the initial pre-trained model to preserve the generalizable knowledge of pre-trained language models during fine-tuning:\n\u2022 R3F (Aghajanyan et al., 2021) introduces a noise-based consistency regularization to prevent representation collapse. Hence, we use the same candidate for \u03bb \u2208 {0.01, 0.1, 0.5}. In addition, we consider the fixed variance of noise \u03c3=1e-5 with the two noise distributions as additional hyper-parameter [U ,N ] following the original paper (Aghajanyan et al., 2021). Also, the official code is available at https://github.com/pytorch/fairseq.\n\u2022 Weight Consolidation (Chen et al., 2020) incorporates \u21132 distance between trained and pre-trained models as a regularization during fine-tuning. To gradually control the strength of such regularization, the authors considers the sigmoid annealing function \u03bb(t) = 1/ ( 1 + exp(\u2212k \u00b7 (t \u2212 t0)) ) where t \u2208 [0, 1].\nHere, we tune the hyper-parameters k and t0 among {0.1, 0.5, 1.0} and {0.1, 0.3, 0.5}, respectively. We denote it as WConsol. We use the official code from https://github.com/ Sanyuan-Chen/RecAdam.\n\u2022 Child-tuning (Xu et al., 2021) selectively update the subset of model parameters (called child network) with a fixed child model. As the task-driven approach shows the better performance compared to task-free variant in (Xu et al., 2021), we adopt the task-driven one as baseline. We tune the child network\u2019s sparsity pD among {0.1, 0.2, 0.3} following the original paper (Xu et al., 2021). We denote this method as ChildTune in our paper. Official code by the authors is publicly released at https://github.com/ PKUnlp-icler/ChildTuning.\n\u2022 LP-FT (Kumar et al., 2022) uses a two-step strategy of linear probing and then full finetuning. For a linear probing, we train the linear classifier on the frozen backbone using Adam optimizer with a fixed learning rate \u03b7 = 1e-3 and 5 epochs. Then, we tune the learning rate \u03b7ft during the full fine-tuning among {1e-6, 3e-5, 1e-5}."
        },
        {
            "heading": "A.3 ROAST",
            "text": "As described in Section 4.1, we use a fixed step size \u03b4 = 0.1 for the gradient ascent step to construct\nAlgorithm 1 ROAST: Robustifying LMs via Adversarial Perturbation with Selective Training\nInput: Pre-trained LM f\u0398, training data D, learning rate \u03b7, update frequency T , masking ratio \u03b1, smoothness factor \u03b2, adversarial noise magnitude \u03b4, coefficient of regularization \u03bb /* Obtaining initial gradient */ G(\u0398)\u2190 InitGrad(f\u0398,D) for each iteration t do\nif t % T = 0 then /* Get relative importance */ {s(\u03b8)|\u03b8 \u2208 \u0398} \u2190 G(\u0398), G(\u0398)\u2190 \u2205 /* Sample gradient mask */ {m(\u03b8)|\u03b8 \u2208 \u0398} \u2190 Mask({s(\u03b8)}, \u03b1, \u03b2) end if /* Sampling training data */ (x, y) \u223c D /* Add adversarially perturbation */ x\u0303\u2190 x+ \u03b4 \u00b7 (\u2202Ltask/\u2202x)/||\u2202Ltask/\u2202x||\u221e /* Get gradient during backward */ g(\u03b8)\u2190 \u2202Ltrain/\u2202\u03b8, Ltrain = Ltask(x, y)+Ltask(x\u0303, y)+\u03bbLcons(x, x\u0303) /* Update with masked gradients */ \u03b8 \u2190 \u03b8 \u2212 \u03b7 \u00b7 ( (m(\u03b8)/p(\u03b8))\u2299 g(\u03b8) )\n/* Accumulate training gradients */ G(\u03b8)\u2190 G(\u03b8) \u222a g(\u03b8)\nend for\nthe adversarial perturbation. Also, similar to the case of perturbation-based regularization methods, we tune the coefficient of consistency regularization Lcons with \u03bb \u2208 {0.01, 0.1, 0.5} (Eq. 1). For the hyper-parameters of gradient masking, we use \u03b1 \u2208 [0.6, 0.95] and \u03b2 \u2208 {1, 5, 10} along with the application of a scaling term. We remark that a relatively higher masking ratio \u03b1 has been effective for sentiment classification, while the smaller \u03b1 has been effective for entailment during our experiments. Based on such observation, we tune \u03b1 among {0.95, 0.9, 0.8} for sentiment classification, {0.6, 0.65, 0.7} for entailment task along with \u03b2 \u2208 {1, 5, 10}. We accumulate the gradient information through each training epoch, then sample the gradient mask from them for the next epoch. In the case of InitGrad in Algorithm 1, similar to the setups in (Xu et al., 2021), we gather the sum of the square of gradients with respect to vanilla cross-entropy loss, since the classifier is not trained at that time. We use NVIDIA A100 GPUs in our experiments."
        },
        {
            "heading": "B Proof of Corollary",
            "text": "In this section, we present a formal proof of the Corollary. To this end, we first present the theoretical results by (Xu et al., 2021): Theorem. (Xu et al., 2021) Suppose L denotes the loss function on the parameter \u03b8, for multiple data instances in the training set x \u223c D, the gradients obey a Gaussian distribution N (\u2202L\u2202\u03b8 , \u03c32g1k). For a randomly sampled batch B \u223c S, when the learning algorithm is SGD with learning rate \u03b7, the probability of the gradient mask from Bernoulli distribution is p, then the mean and covariance of the update \u2206\u03b8 := \u2212\u03b7 ( \u2202L \u2202\u03b8 \u2299m(\u03b8) ) are\nE[\u2206\u03b8] = \u2212\u03b7\u2202L \u2202\u03b8 , \u03a3[\u2206\u03b8] = \u03b72\u03c32g1k\np|B| + (1\u2212 p)\u03b72diag{\u2202L\u2202\u03b8 }2 p ,\nwhere \u03a3 is the covariance matrix and diag(X) is the diagonal matrix of the vector X .\nHere, the key difference between the above problem setup (Xu et al., 2021) and our case is the probability for masking: (Xu et al., 2021) assumes the identical Bernoulli distribution, while we assume the element-wise Bernoulli distribution with the different probability for each model parameter. However, in below Corollay, we show that our problem can be also proved in the almost same way. Corollary. We consider the same assumption in Theorem by (Xu et al., 2021), expect the probability of the gradient mask follows different Bernoulli distribution for each parameter p(\u03b8) and m(\u03b8) \u223c Ber ( p(\u03b8) ) . Then, the mean of the update \u2206\u03b8 is,\nE[\u2206\u03b8] = \u2212\u03b7\u2202L \u2202\u03b8\nand its covariance is bounded as,\n|\u03a3[\u2206\u03b8]||F \u2264\nd \u2225\u2225\u2225\u2225\u2225 \u03b72\u03c32g1k p\u0302|B| + (1\u2212 p\u0302)\u03b72diag{\u2202L\u2202\u03b8 }2 p\u0302 \u2225\u2225\u2225\u2225\u2225 F ,\nwhere p\u0302 := min p(\u03b8). Proof. Let g(i) is the gradient of sample xi, 1 \u2264 i \u2264 |B|, then g(i) \u223c N (\u2202L\u2202\u03b8 , \u03c32g1k) by the assumption. Let g = \u2211|B| i=1 gi |B| , then we have\n\u2206\u03b8 = \u2212\u03b7 ( |B|\u2211\ni=1\ng(i) |B| ) \u2299m(\u03b8) = \u2212\u03b7g \u2299m(\u03b8)\nWhen we consider g, the followings are obtained:\nE[g] = \u2202L \u2202\u03b8 ,\u03a3[g] = \u03c32g1k |B|\nSuppose g\u0303 := ( m(\u03b8)/p(\u03b8) ) \u2299 g, then we have:\nE[g\u0303] = p(\u03b8) p(\u03b8) \u00d7 \u2202L \u2202\u03b8 = \u2202L \u2202\u03b8 = E[g]\nLet g\u0303i, gi, pi are the i-th dimension of g\u0303, g, p. Then,\nV ar[g\u0303i] = E[g\u03032i ]\u2212 (E[g\u0303i])2\n= piE[( gi pi )2]\u2212 (E[g\u0303i])2 = E[g2i ] pi \u2212 (E[g\u0303i])2 = (E[gi])2 + V ar[gi]\npi \u2212 (E[g\u0303i])2\n= V ar[gi]\npi + (1\u2212 pi)(E[g\u0303i])2 pi\nSince 1pi is a decreasing function regarding pi, one can derive following bound with p\u0302 := mini pi,\n||\u03a3[g\u0303]||F \u2264 \u2225\u2225\u2225\u2225\u2225 \u03c32g1k p\u0302|B| + (1\u2212 p\u0302)diag{\u2202L\u2202\u03b8 }2 p\u0302 \u2225\u2225\u2225\u2225\u2225 F ,"
        },
        {
            "heading": "C More Quantitative Results with ROAST",
            "text": "C.1 Generalization beyond embedding-level perturbation\nFor ROAST, we chose embedding-level perturbation (Zhu et al., 2020; Jiang et al., 2020) over tokenlevel perturbation (Jin et al., 2020; Li et al., 2020), as it is more computationally efficient and better suited for enhancing multi-perspective robustness. Specifically, the construction of token-level adversarial perturbations requires more computation to solve the discrete optimization problem, and additional regularization is often introduced to prevent degenerate cases such as significant changes in semantic or lexical violation (Jin et al., 2020; Li et al., 2020; Park et al., 2022). For example, a relatively simple construction of token-level perturbation with Park et al. (2022) requires 15% more times per iteration, compared to embedding-level perturbation. In addition, while the token-level adversarial perturbation is effective for adversarial robustness, it often comes at the cost of a decrease\nin clean accuracy (Dong et al., 2021) due to the relatively large perturbation on a discrete space. In contrast, the embedding-level perturbation can be constructed by adding small noise to continuous space, making it more feasible to train the model without the loss of accuracy (Zhu et al., 2020; Jiang et al., 2020)\nTo further validate the generalization of ROAST beyond the embedding-level perturbation, we conduct additional experiments by adapting RoAST with discrete token-level adversarial perturbations by fine-tuning RoBERTa-large using VAT-D (Park et al., 2022). We used the same hyper-parameters for discrete token-level perturbation as in Park et al. (2022), and for RoAST, we used the same values previously found with embedding-level perturbation. The results are shown in the table below.\nHere, we observe that the discrete token-level adversarial perturbation can improve the multiperspective robustness, especially for adversarial robustness (Accadv). However, it comes at the cost of degradation in clean accuracy (Accin). With RoAST, such degradation can be mitigated and the overall robustness of the model could be improved.\nC.2 Absolute average improvement of ROAST\nDuring the experiments, we used the average of relative improvement, instead of absolute improvement, as it is more appropriate to measure the multiperspective robustness than the average of absolute improvement, not to scale up the values. However, we also recognize its weak points, such as the risk of amplification, which is the reason why we additionally report Rankavg, which does not have similar issues. We emphasize that RoAST exhibits the lowest rank among the state-of-the-art fine-tuning methods on both sentiment classification and entailment tasks. Nevertheless, to address the concerns about this, we additionally calculate the absolute average improvement (Absavg) of {Accin, Accshift, Accadv, 100 - ECE, 100 * AUROC}, on the sentiment classification task. The results are presented in Table 7. Here, one can observe that our method still outperforms the state-of-the-art finetuning method with a large gap; RoAST exhibits 46.72% relative improvement on Absavg, compared to SMART (1.16% vs 0.79%). This result further demonstrates the effectiveness of our method, and we do believe that our empirical results clearly show the merit of the proposed framework.\nC.3 Additional comparison with FreeLB++\nHere, we present additional experimental results with FreeLB++ (Li et al., 2021) on the sentiment classification task, which runs adversarial perturbation for 10 steps without constraints of normbounded projection. We also tuned the adversarial step size appropriately as the number of steps varies. The results are summarized in Table 8. Here, one can observe that FreeLB++ outperforms FreeLB, especially in ACCadv (70.07\u2192 72.45). Consequently, FreeLB++ is better than FreeLB for multi-perspective robustness as well (\u2206avg : 9.21\u2192 11.02). However, RoAST still outperforms FreeLB++ with a large gap, which further demonstrates the effectiveness of our method. On the other hand, these results indicate a potential room for further improvement in our method at the additional cost of the increased number of steps, since RoAST also uses a single step for constructing adversarial perturbation."
        },
        {
            "heading": "C.4 Relationship between overfitting and robustness",
            "text": "To investigate the relationship between overfitting and robustness of LMs, we performed additional experiments by training RoBERTa-large on SST-2, using the Vanilla method with a constant learning rate for 100 epochs. As shown in Table 9, the model\u2019s robustness significantly decreases as the training progresses. This outcome confirms that preserving the parameters is critical for maintaining model robustness, which is a fundamental principle of selective training with m(\u03b8) in ROAST."
        },
        {
            "heading": "D Additional Results with Various LMs",
            "text": "First, we present the detailed results on the individual robustness metrics like Tables 1 and 2. In Table\n10 and 11, the results on sentiment classification and entailment are presented, respectively.\nNext, we provide the additional results with different vanilla algorithm to calculate \u2206avg; In Table 3, we use BERT-large as a universal vanilla algorithm to calculate relative improvement across different LMs to facilitate comparison in terms of multi-perspective robustness. This choice was made to provide additional insight into the question of \u201cwhich LM is the most robust\u201d. While answering this question is important, we also acknowledge that using the corresponding LM\u2019s score as the baseline could provide additional insights into how RoAST performs with each specific LM. Hence, we recalculate Table 3 with the corresponding LM\u2019s score and present it in Table 12. One can observe that ROAST mostly outperforms the baselines except in only 1 case, and achieves large improvements compared to baselines in both tasks. This result demonstrates the robustness and effectiveness of RoAST with respect to different LMs.\nLastly, we conducted additional experiments on the sentiment classification task with GPT2-large (Radford et al., 2019), a popular decoder-only LM, to validate the applicability of our approach. Following Radford et al. (2019), we added a linear classifier head on the last token\u2019s embedding output for fine-tuning. The results are presented in Table 13. Here, we first observe that the improvements with baseline methods are largely limited. We speculate that this ineffectiveness may occur due to the different nature of decoder-only LMs compared to encoder-only ones, such as BERT, as it could result in different effectiveness of the baseline algorithms and tuned hyper-parameters, which were originally developed and demonstrated only using encoderonly models; for instance, most of the baselines (Jiang et al., 2020; Zhu et al., 2020; Chen et al.,\n2020; Xu et al., 2021) have been demonstrated under encoder-only models and not shown the results of decoder-only ones. Nevertheless, our RoAST approach continues to enhance the multi-perspective robustness of GPT2-large, with an average relative improvement of 5.07% compared to the Vanilla method. These results indicate that the effectiveness of our approach is not limited to BERT-based LMs with Transformer-encoder architecture.\nE Individual Experimental Results\nNext, we present the results on each dataset for each robustness metric. First, we present the results from sentiment classification. Specifically, we report the accuracy and ECE on distribution shifted datasets in Table 14 and 15, respectively. Also, we report the accuracy and ECE on adversarially constructed datasets in Table 16 and 17, respectively. The results of anomaly detection are shown in Table 18. Next, we present the results from entailment task; we report the accuracy and ECE on distribution shifted datasets in Table 19 and 20, respectively. Then, we report the accuracy and ECE on adversarially constructed datasets in Table 21 and 22, respectively. Finally, we report the results of anomaly detection in Table 23."
        }
    ],
    "title": "ROAST: Robustifying Language Models via Adversarial Perturbation with Selective Training",
    "year": 2023
}