{
    "abstractText": "Fine-tuning on instruction data has been widely validated as an effective practice for implementing chat language models like ChatGPT. Scaling the diversity and quality of such data, although straightforward, stands a great chance of leading to improved performance. This paper aims to push the upper bound of opensource models further. We first provide a systematically designed, diverse, informative, large-scale dataset of instructional conversations, UltraChat, which does not involve human queries. Our objective is to capture the breadth of interactions between a human user and an AI assistant and employs a comprehensive framework to generate multi-turn conversation iteratively. UltraChat contains 1.5 million high-quality multi-turn dialogues and covers a wide range of topics and instructions. Our statistical analysis of UltraChat reveals its superiority in various key metrics, including scale, average length, diversity, coherence, etc., solidifying its position as a leading opensource dataset. Building upon UltraChat, we fine-tune a LLaMA model to create a powerful conversational model, UltraLM. Our evaluations indicate that UltraLM consistently outperforms other open-source models, including WizardLM and Vicuna, the previously recognized state-of-the-art open-source models.",
    "authors": [
        {
            "affiliations": [],
            "name": "Ning Ding"
        },
        {
            "affiliations": [],
            "name": "Yulin Chen"
        },
        {
            "affiliations": [],
            "name": "Bokai Xu"
        },
        {
            "affiliations": [],
            "name": "Yujia Qin"
        },
        {
            "affiliations": [],
            "name": "Shengding Hu"
        },
        {
            "affiliations": [],
            "name": "Zhiyuan Liu"
        },
        {
            "affiliations": [],
            "name": "Maosong Sun"
        },
        {
            "affiliations": [],
            "name": "Bowen Zhou"
        }
    ],
    "id": "SP:d78ead900c48f18da795aeff536ac3172fb26e56",
    "references": [
        {
            "authors": [
                "Yuvanesh Anand",
                "Zach Nussbaum",
                "Brandon Duderstadt",
                "Benjamin Schmidt",
                "Andriy Mulyar"
            ],
            "title": "Gpt4all: Training an assistant-style chatbot with large scale data distillation from gpt-3.5-turbo",
            "year": 2023
        },
        {
            "authors": [
                "Dario Amodei",
                "Tom Brown",
                "Jack Clark",
                "Sam McCandlish",
                "Chris Olah",
                "Jared Kaplan"
            ],
            "title": "A general language assistant as a laboratory for alignment",
            "year": 2021
        },
        {
            "authors": [
                "Yuntao Bai",
                "Andy Jones",
                "Kamal Ndousse",
                "Amanda Askell",
                "Anna Chen",
                "Nova DasSarma",
                "Dawn Drain",
                "Stanislav Fort",
                "Deep Ganguli",
                "Tom Henighan"
            ],
            "title": "Training a helpful and harmless assistant",
            "year": 2022
        },
        {
            "authors": [
                "Stella Biderman",
                "Hailey Schoelkopf",
                "Quentin Anthony",
                "Herbie Bradley",
                "Kyle O\u2019Brien",
                "Eric Hallahan",
                "Mohammad Aflah Khan",
                "Shivanshu Purohit",
                "USVSN Sai Prashanth",
                "Edward Raff"
            ],
            "title": "Pythia: A suite for analyzing large language models across training",
            "year": 2023
        },
        {
            "authors": [
                "Rishi Bommasani",
                "Drew A Hudson",
                "Ehsan Adeli",
                "Russ Altman",
                "Simran Arora",
                "Sydney von Arx",
                "Michael S Bernstein",
                "Jeannette Bohg",
                "Antoine Bosselut",
                "Emma Brunskill"
            ],
            "title": "On the opportunities and risks of foundation models",
            "year": 2021
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners",
            "venue": "Proceedings of NeurIPS",
            "year": 2020
        },
        {
            "authors": [
                "Sahil Chaudhary"
            ],
            "title": "Code alpaca: An instructionfollowing llama model for code generation",
            "year": 2023
        },
        {
            "authors": [
                "Wei-Lin Chiang",
                "Zhuohan Li",
                "Zi Lin",
                "Ying Sheng",
                "Zhanghao Wu",
                "Hao Zhang",
                "Lianmin Zheng",
                "Siyuan Zhuang",
                "Yonghao Zhuang",
                "Joseph E. Gonzalez",
                "Ion Stoica",
                "Eric P. Xing"
            ],
            "title": "Vicuna: An opensource chatbot impressing gpt-4 with 90%* chatgpt",
            "year": 2023
        },
        {
            "authors": [
                "Aakanksha Chowdhery",
                "Sharan Narang",
                "Jacob Devlin",
                "Maarten Bosma",
                "Gaurav Mishra",
                "Adam Roberts",
                "Paul Barham",
                "Hyung Won Chung",
                "Charles Sutton",
                "Sebastian Gehrmann"
            ],
            "title": "Palm: Scaling language modeling with pathways",
            "year": 2022
        },
        {
            "authors": [
                "Peter Clark",
                "Isaac Cowhey",
                "Oren Etzioni",
                "Tushar Khot",
                "Ashish Sabharwal",
                "Carissa Schoenick",
                "Oyvind Tafjord."
            ],
            "title": "Think you have solved question answering? try arc, the ai2 reasoning challenge",
            "venue": "ArXiv preprint, abs/1803.05457.",
            "year": 2018
        },
        {
            "authors": [
                "Mike Conover",
                "Matt Hayes",
                "Matt Mathur",
                "Xiangrui Meng",
                "Jianwei Xie",
                "Jun Wan",
                "Ali Ghodsi",
                "Patrick Wendell",
                "Patrick Zaharia"
            ],
            "title": "Hello dolly: Democratizing the magic of chatgpt with open models",
            "year": 2023
        },
        {
            "authors": [
                "Ning Ding",
                "Yujia Qin",
                "Guang Yang",
                "Fuchao Wei",
                "Zonghan Yang",
                "Yusheng Su",
                "Shengding Hu",
                "Yulin Chen",
                "Chi-Min Chan",
                "Weize Chen"
            ],
            "title": "Parameterefficient fine-tuning of large-scale pre-trained language models",
            "venue": "Nature Machine Intelligence,",
            "year": 2023
        },
        {
            "authors": [
                "Domeccleston."
            ],
            "title": "Sharegpt \u2013 share your wildest chatgpt conversations with one click",
            "venue": "Retrieved 23 May 2023.",
            "year": 2023
        },
        {
            "authors": [
                "Ben Wang",
                "Kevin Wang",
                "Andy Zou"
            ],
            "title": "A framework for few-shot language model evaluation",
            "year": 2021
        },
        {
            "authors": [
                "Xinyang Geng",
                "Arnav Gudibande",
                "Hao Liu",
                "Eric Wallace",
                "Pieter Abbeel",
                "Sergey Levine",
                "Dawn Song."
            ],
            "title": "Koala: A dialogue model for academic research",
            "venue": "Blog post.",
            "year": 2023
        },
        {
            "authors": [
                "Fabrizio Gilardi",
                "Meysam Alizadeh",
                "Ma\u00ebl Kubli."
            ],
            "title": "Chatgpt outperforms crowd-workers for textannotation tasks",
            "venue": "arXiv preprint arXiv:2303.15056.",
            "year": 2023
        },
        {
            "authors": [
                "Zhao",
                "Jun Zhu."
            ],
            "title": "Pre-trained models: Past, present and future",
            "venue": "AI Open.",
            "year": 2021
        },
        {
            "authors": [
                "Dan Hendrycks",
                "Collin Burns",
                "Steven Basart",
                "Andy Zou",
                "Mantas Mazeika",
                "Dawn Song",
                "Jacob Steinhardt."
            ],
            "title": "Measuring massive multitask language understanding",
            "venue": "Proceedings of ICLR.",
            "year": 2021
        },
        {
            "authors": [
                "Edward J Hu",
                "Yelong Shen",
                "Phillip Wallis",
                "Zeyuan Allen-Zhu",
                "Yuanzhi Li",
                "Shean Wang",
                "Lu Wang",
                "Weizhu Chen."
            ],
            "title": "Lora: Low-rank adaptation of large language models",
            "venue": "Proceedings of ICLR.",
            "year": 2021
        },
        {
            "authors": [
                "Yunjie Ji",
                "Yong Deng",
                "Yan Gong",
                "Yiping Peng",
                "Qiang Niu",
                "Lei Zhang",
                "Baochang Ma",
                "Xiangang Li."
            ],
            "title": "Exploring the impact of instruction data scaling on large language models: An empirical study on real-world use cases",
            "venue": "arXiv preprint",
            "year": 2023
        },
        {
            "authors": [
                "Hyunwoo Kim",
                "Jack Hessel",
                "Liwei Jiang",
                "Peter West",
                "Ximing Lu",
                "Youngjae Yu",
                "Pei Zhou",
                "Ronan Le Bras",
                "Malihe Alikhani",
                "Gunhee Kim",
                "Maarten Sap",
                "Yejin Choi"
            ],
            "title": "Soda: Million-scale dialogue distillation with social commonsense contextualization",
            "year": 2023
        },
        {
            "authors": [
                "Andreas K\u00f6pf",
                "Yannic Kilcher",
                "Dimitri von R\u00fctte",
                "Sotiris Anagnostidis",
                "Zhi-Rui Tam",
                "Keith Stevens",
                "Abdullah Barhoum",
                "Nguyen Minh Duc",
                "Oliver Stanley",
                "Rich\u00e1rd Nagyfi"
            ],
            "title": "Openassistant conversations\u2013democratizing large language model",
            "year": 2023
        },
        {
            "authors": [
                "Guohao Li",
                "Hasan Abed Al Kader Hammoud",
                "Hani Itani",
                "Dmitrii Khizbullin",
                "Bernard Ghanem"
            ],
            "title": "Camel: Communicative agents for \"mind\" exploration of large scale language model society",
            "year": 2023
        },
        {
            "authors": [
                "Xuechen Li",
                "Tianyi Zhang",
                "Yann Dubois",
                "Rohan Taori",
                "Ishaan Gulrajani",
                "Carlos Guestrin",
                "Percy Liang",
                "Tatsunori B. Hashimoto"
            ],
            "title": "2023b. Alpacaeval: An automatic evaluator of instruction-following models",
            "year": 2023
        },
        {
            "authors": [
                "Stephanie Lin",
                "Jacob Hilton",
                "Owain Evans."
            ],
            "title": "Truthfulqa: Measuring how models mimic human falsehoods",
            "venue": "arXiv preprint arXiv:2109.07958.",
            "year": 2021
        },
        {
            "authors": [
                "Shayne Longpre",
                "Le Hou",
                "Tu Vu",
                "Albert Webson",
                "Hyung Won Chung",
                "Yi Tay",
                "Denny Zhou",
                "Quoc V Le",
                "Barret Zoph",
                "Jason Wei"
            ],
            "title": "The flan collection: Designing data and methods for effective instruction tuning",
            "venue": "arXiv preprint arXiv:2301.13688",
            "year": 2023
        },
        {
            "authors": [
                "Philip M. McCarthy",
                "Scott Jarvis."
            ],
            "title": "Mtld, vocdd, and hd-d: A validation study of sophisticated approaches to lexical diversity assessment",
            "venue": "Behavior Research Methods, 42:381\u2013392.",
            "year": 2010
        },
        {
            "authors": [
                "OpenAI."
            ],
            "title": "Gpt-4 technical report",
            "venue": "arXiv.",
            "year": 2023
        },
        {
            "authors": [
                "TB OpenAI."
            ],
            "title": "Chatgpt: Optimizing language models for dialogue",
            "venue": "OpenAI.",
            "year": 2022
        },
        {
            "authors": [
                "Paul Christiano",
                "Jan Leike",
                "Ryan Lowe"
            ],
            "title": "Training language models to follow instructions with human feedback",
            "year": 2022
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J. Liu."
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "Journal of Machine Learning Research,",
            "year": 2020
        },
        {
            "authors": [
                "Victor Sanh",
                "Albert Webson",
                "Colin Raffel",
                "Stephen H Bach",
                "Lintang Sutawika",
                "Zaid Alyafeai",
                "Antoine Chaffin",
                "Arnaud Stiegler",
                "Teven Le Scao",
                "Arun Raja"
            ],
            "title": "Multitask prompted training enables zeroshot task generalization",
            "venue": "Proceedings of ICLR",
            "year": 2021
        },
        {
            "authors": [
                "John Schulman",
                "Filip Wolski",
                "Prafulla Dhariwal",
                "Alec Radford",
                "Oleg Klimov."
            ],
            "title": "Proximal policy optimization algorithms",
            "venue": "arXiv preprint arXiv:1707.06347.",
            "year": 2017
        },
        {
            "authors": [
                "Qingyi Si",
                "Tong Wang",
                "Naibin Gu",
                "Rui Liu",
                "Zheng Lin"
            ],
            "title": "Alpaca-cot: An instruction fine-tuning platform with instruction data collection and unified large lnguage models interface",
            "year": 2023
        },
        {
            "authors": [
                "Rohan Taori",
                "Ishaan Gulrajani",
                "Tianyi Zhang",
                "Yann Dubois",
                "Xuechen Li",
                "Carlos Guestrin",
                "Percy Liang",
                "Tatsunori B Hashimoto."
            ],
            "title": "Alpaca: A strong, replicable instruction-following model",
            "venue": "Stanford Center for Research on Foundation Models.",
            "year": 2023
        },
        {
            "authors": [
                "Yizhong Wang",
                "Yeganeh Kordi",
                "Swaroop Mishra",
                "Alisa Liu",
                "Noah A. Smith",
                "Daniel Khashabi",
                "Hannaneh Hajishirzi"
            ],
            "title": "Self-instruct: Aligning language model with self generated instructions",
            "year": 2022
        },
        {
            "authors": [
                "Jason Wei",
                "Maarten Bosma",
                "Vincent Y Zhao",
                "Kelvin Guu",
                "Adams Wei Yu",
                "Brian Lester",
                "Nan Du",
                "Andrew M Dai",
                "Quoc V Le."
            ],
            "title": "Finetuned language models are zero-shot learners",
            "venue": "arXiv preprint arXiv:2109.01652.",
            "year": 2021
        },
        {
            "authors": [
                "Can Xu",
                "Qingfeng Sun",
                "Kai Zheng",
                "Xiubo Geng",
                "Pu Zhao",
                "Jiazhan Feng",
                "Chongyang Tao",
                "Daxin Jiang"
            ],
            "title": "2023a. Wizardlm: Empowering large language models to follow complex instructions",
            "year": 2023
        },
        {
            "authors": [
                "Canwen Xu",
                "Daya Guo",
                "Nan Duan",
                "Julian McAuley."
            ],
            "title": "Baize: An open-source chat model with parameter-efficient tuning on self-chat data",
            "venue": "arXiv preprint arXiv:2304.01196.",
            "year": 2023
        },
        {
            "authors": [
                "Rowan Zellers",
                "Ari Holtzman",
                "Yonatan Bisk",
                "Ali Farhadi",
                "Yejin Choi"
            ],
            "title": "HellaSwag: Can a machine really finish your sentence",
            "venue": "In Proceedings of ACL,",
            "year": 2019
        },
        {
            "authors": [
                "Gao"
            ],
            "title": "AlpacaEval (Li et al., 2023b) is a hybrid evaluation dataset with altogether 805 instructions, which combines instructions from various existing evaluation sets, including self-instruct",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Large language models (Bommasani et al., 2021; Han et al., 2021; Chowdhery et al., 2022) (LLMs) have demonstrated exceptional generalization capability on a variety of language-related tasks. Notably, ChatGPT (OpenAI, 2022), an optimized version of GPT-3 (Brown et al., 2020) for conversation, along with GPT-4 (OpenAI, 2023), takes the user experience to another level via excelling in comprehending and generating responses in a natural and interactive manner. The introduction of\n\u2217 equal contributions \u2020 Corresponding authors\nChatGPT has spurred a surge in the adoption and implementation of general chat language models.\nIn addition to competing models developed by large corporations such as Bard1 and Claude2, the open-source community is actively engaged in training similar models, aiming to democratize access to AI technology. Notable examples in this regard include Alpaca (Taori et al., 2023), Vicuna (Chiang et al., 2023), Koala (Geng et al., 2023), Baize (Xu et al., 2023b), and Belle (Ji et al., 2023), etc., demonstrating promising performance. Experimental evidence strongly suggests that chat language models can be effectively trained through instruction fine-tuning (Wei et al., 2021; Sanh et al., 2021), and they also indicate that many data-efficient (Zhou et al., 2023) or computingefficient (Hu et al., 2021; Ding et al., 2023) methods can be applied. This paper, in another way, focuses more on the \"final one mile\" of chat language models, as evidence shows that the journey from 0 to 60 is easy, whereas progressing from 60 to 100 becomes exceedingly challenging. For instance, researchers have shown that by utilizing a small,\n1https://bard.google.com/ 2https://www.anthropic.com/index/\nintroducing-claude\nthoughtfully curated set of instructions, it is possible to train a model with satisfactory instructionfollowing capabilities. However, these approaches have yet to produce models that surpass the performance of Vicuna, the current leading open-source model, let alone outperform ChatGPT and GPT-4.\nThis paper believes that the most straightforward way, that is, the quality and diversity of training data, play a vital role in further improving the performance of chat language models. In other words, leveraging higher quality and more diverse data can yield better outcomes. To this end, we present UltraChat, a million-scale multi-turn instructional conversation data, to facilitate the construction of more powerful chat language models. UltraChat is carefully designed to capture the breadth of interactions that a human might have with an AI assistant. Specifically, we do not use specific tasks like question-answering or summarization to construct the data, but curate three sectors: Questions about the World, Creation and Writing, and Assistance on Existing Materials. Then we employ metainformation, in-context expansion, and iterative prompting to scale up the number of instructions. To construct informative and realistic multi-turn conversations, two separate ChatGPT Turbo APIs are adopted in the conversation generation, where one plays the role of the user to generate queries, and the other generates the response. We instruct the user model with carefully designed prompts to mimic human user behavior and call the two APIs iteratively.\nWe fine-tune a LLaMA-13B model on UltraChat to produce UltraLM and compare the model to a wide range of baselines, especially the opensource ones. The evaluation shows that our model could consistently outperform other models. As reported in Table 1, UltraLM achieves the highest performance scores that are independently assessed by GPT-4. Further evaluation results on challenging benchmarks and preference study with GPT-4 on various evaluation sets also show that UltraLM could surpass all other open-source models."
        },
        {
            "heading": "2 Related Work",
            "text": "Instruction Tuning. Recent works demonstrate LLMs\u2019 powerful capabilities in following human instructions. Wei et al. (2021) pioneered to finetune T5 (Raffel et al., 2020) on 60 NLP datasets verbalized with natural language instruction templates, i.e., instruction tuning. The fine-tuned model ex-\nhibits a strong ability in instruction understanding and generalizes well to unseen instructions (Sanh et al., 2021; Ouyang et al., 2022). Later, Longpre et al. (2023) show the benefits of scaling the number of tasks in out-of-distribution generalization. Wei et al. (2021) also conclude that the success of instruction tuning depends on the quality of the dataset and the design of prompts. To further regulate the tuned model\u2019s behavior, Ouyang et al. (2022); Schulman et al. (2017) propose to employ reinforcement learning to align model behaviors with human preferences. This technique combined with instruction tuning can further boost the model performance and has been successfully applied to LLMs such as ChatGPT.\nData Augmentation with LLMs. Collecting large-scale human-annotated instructions and their responses is time-consuming and labor-intensive. Alternatively, a more cost-effective and feasible approach to gathering top-notch data involves sampling from LLMs that have been well-tuned, e.g., ChatGPT and GPT-3.5. Recently, there is a surge of interest in distilling these powerful LLMs for data augmentation. For instance, using the technique of Self-Instruct (Wang et al., 2022), Alpaca (Taori et al., 2023) generate 52k high-quality instruction-response pairs based on seed tasks by \u201cdistilling\u201d Text-Davinci-003. The trained model performs almost on par with TextDavinci-003. The success of Alpaca boosts numerous later efforts on data augmentation with LLMs, such as code-alpaca (Chaudhary, 2023), alpacacot (Si et al., 2023), GPT4ALL (Anand et al., 2023), ShareGPT (Domeccleston, 2023), Dollyv2 (Conover et al., 2023), BELLE (Ji et al., 2023), Vicuna (Chiang et al., 2023), Koala (Geng et al., 2023), Baize (Xu et al., 2023b), etc. It is shown that increasing the scale of data could constantly improve the model performance. Besides, prompt engineering also affects data quality. CAMEL (Li et al., 2023a) design a multi-agent role-play environment for LLMs to simulate real human conversations."
        },
        {
            "heading": "3 Data Construction",
            "text": "LLMs are believed to be better annotators than human-being in many scenarios (Gilardi et al., 2023). However, employing LLMs such as ChatGPT directly for generating multi-turn conversations may yield satisfactory but less informative results, as it cannot enjoy the benefit of reinforce-\nMeta Topics SubTopics Questions about Various Conceptions\nRepresentative Entities\nMeta Questions\nDetailed Questions\nAssociated Questions\nMaterial Types\nMaterial Generation Instructions\nDetailed Instructions\nMaterials Questions or Instructions\nSector I: Questions about the World Human\nModel\nWikidata\nSearch Engine\nHuman\nModel\nSector II: Creation and Writing\nSector III: Assistance on Materials C4\nSector II\nUser Model\nAI Model\n3~7 rounds of generation\nMeta Topics SubTopics Questions about Various Conceptions\nRepresentative Entities\nMeta Questions\nDetailed Questions\nAssociated Questions\nMaterial Types\nMaterial Generation Instructions\nDetailed Instructions\nMaterials Questions or Instructions\nSector I: Questions about the World Human\nModel\nWikidata\nSearch Engine\nHuman\nModel\nSector II: Creation and Writing\nSector III: Assistance on Materials C4\nSector II\nUser Model\nAI Model\n3~7 rounds of generation\nPost-processing\nQuery/Instruct\nResponse\nExntend Generate\nGenerate Associate\nDetail\nGenerate Detail\nGenerate\nment learning with human feedback (RLHF) in the alignment process. Table 12 in Appendix A shows a comparison of directly generated multi-turn dialogue and a case in UltraChat with the same opening line. Two key points can be derived to ensure the quality of the data: (1) An opening line determines the topic of the dialogue. Opening lines should be highly diverse and encompass any task that a human user may request a chat model to perform. (2) A user determines the plot of the dialogue, and the output should be tailored to the current topic with diverse language styles and requests.\nTherefore, unlike traditional task-specific datasets, to construct a comprehensive opendomain instructional chat dataset, the design of data collection schema is crucial to capturing the breadth of interactions and ensuring data quality. UltraChat aims to cover a tremendous range of conversation data with a carefully designed tripartite schema: Questions about the World, Creation and Writing, and Assistance on Existing Materials. While the core of ensuring data diversity mainly depends on opening line diversity, we will first introduce the idea behind the sector design and then focus on specific measures to obtain a diverse set of opening lines and how to prompt the user properly."
        },
        {
            "heading": "3.1 Questions about the World",
            "text": "The first sector focuses on querying existing information in the world, including concepts, objects, and entities that exist in the real world. This is at\nthe core of human-AI interaction, as users often rely on AI assistants to provide quick and accurate answers to their questions.\nOur approach to gathering data for this sector involves two perspectives: one centered around topics and concepts, and the other around real-world entities. Initially, we request ChatGPT to generate 30 comprehensive topics that encompass various aspects of our daily lives, as shown in Table 2. Subsequently, we delve deeper into each topic by generating 30 to 50 subtopics or related concepts. Finally, we generate 10 different questions for each subtopic or concept and additionally request ChatGPT to generate 10 more questions based on each original question. The other source of data comes from real-world objects, which are derived from Wikidata3 entities. These entities are further refined by considering their frequencies in Wikipedia4 articles, specifically focusing on the 10,000 most frequently occurring entities. For each entity, we create 5 meta-questions, followed by 10 more specific questions and 20 extended questions. The extended questions aim to maintain some similarity to the original question while exploring distinct objects or topics. To create a dialogue, we filter and sample approximately 500,000 questions as opening lines. During the construction of each dialogue, we provide the user model with carefully crafted prompts that explicitly ask the model to respond\n3https://www.wikidata.org/ 4https://www.wikipedia.org/\nconcisely and meaningfully, taking into account the context of the ongoing dialogue history."
        },
        {
            "heading": "3.2 Creation and Writing",
            "text": "The second part is concerned with the creation of new information with human-input conditions, ranging from writing emails to crafting stories and plays. This process reflects the AI\u2019s capacity to engage in original content generation alongside users and demonstrates the role of AI assistants as collaborative partners in a creative environment.\nWe first project all creations as text materials, and further categorize them into 20 different types as in Table 3. Then a ChatGPT model is employed to produce a diverse range of instructions for each type of writing, approximately 80% of which are further refined by ChatGPT model to generate more detailed instructions. These instructions serve as opening lines for dialogue generation. Throughout the generation process, the user prompt constantly reinforces the primary objective of the conversation, which is to generate and refine a piece of writing. This serves to ensure that the behavior of the user model remains focused and aligned with the intended purpose."
        },
        {
            "heading": "3.3 Assistance on Existing Materials",
            "text": "The third sector mainly addresses the modification of existing information, encompassing various tasks including rewriting, translation, summa-\nrization, and question-answering, etc. Modifying existing materials is a crucial aspect of human-AI interaction, as it allows the AI assistant to actively engage with the user\u2019s input, transforming it in various ways as instructed by the user.\nWe begin by gathering text pieces from the C4 corpus5. Each piece within the C4 corpus is associated with a source URL. To ensure a diverse range of text content and styles, we adopt the 20 material types outlined in the previous section and manually curate keywords for each type. Additionally, we classify the text in the corpus by matching the keywords in the corresponding URL. In total, we collect 100,000 text pieces from the C4 corpus, and for each piece, we prompt ChatGPT to generate five distinct instructions. We use a manually designed template to combine text and instructions, as depicted in Figure 4 in the appendix. Ultimately, the concatenated set of 500,000 pieces serves as the opening lines for the generated dialogues."
        },
        {
            "heading": "3.4 User Simulation and Refinement",
            "text": "Maintaining the desired behavior of the user model is crucial for achieving successful automatic dialogue generation. It has been observed that when the user model is solely provided with the current dialogue history, it tends to assume the role of an AI assistant. This \"role confounding\" situation\n5https://commoncrawl.org/\ncan significantly deteriorate the coherence of the multi-turn conversation. To address this, in addition to presenting the dialogue history, we include prompts explicitly instructing the model to adopt various user personalities. In Sector 2, a prompt is employed to remind the model of the primary purpose of the dialogue, thereby promoting a more natural conversation flow. Once the data generation process is complete, a further filtration step is performed to ensure overall data quality. We also exclude excessively polite statements to enhance the realism of user responses."
        },
        {
            "heading": "4 Data Analysis",
            "text": ""
        },
        {
            "heading": "4.1 Statistical Analysis",
            "text": "We conduct a statistical analysis of UltraChat and several other instruction datasets, as shown in Table 4. UltraChat stands out in terms of its scale, being one of the largest publicly available datasets. Moreover, it exhibits the highest average number of turns and the longest average length per instance of data. While SODA (Kim et al., 2023) also has many rounds, it is primarily composed of conceptual banter rather than instructional content. Additionally, the average number of tokens per dialogue in SODA is 231.8, whereas UltraChat boasts a remarkable 1467.4 tokens. To evaluate diversity, we measure both lexical diversity and topic diversity. UltraChat outperforms previous datasets in terms of lexical diversity. However, in terms of topic diversity, UltraChat falls slightly short compared to GPT4ALL (Anand et al., 2023) but still surpasses other datasets significantly. This may be attributed to the regularized embeddings resulting from a large number of tokens in each dialogue. We also conduct coherence evaluation with ChatGPT for multi-turn datasets. Notably, UltraChat and Baize data rank the highest in terms of coherence."
        },
        {
            "heading": "4.2 Human Assessment",
            "text": "Setup. To better evaluate the constructed data quality, we also conduct human assessment for UltraChat. Due to the difficulty of evaluation of multi-turn dialogue and the resulting formidable cost, we sample 500 representative dialogues for human evaluation, among which 300 are from UltraChat sector 1, 100 from sector 2 and sector 3 respectively. For each round of conversation, we ask the annotators to score the assistant\u2019s response on Helpfulness, Honesty, and Harmlessness (3H) principles (Askell et al., 2021). We also devise Coherence and Consistency criteria for the overall multi-turn dialogue quality evaluation. Coherence evaluates whether the dialogue flows logically and coherently, for which the annotators evaluate both the user\u2019s response and the assistant\u2019s response. Consistency means the assistant\u2019s responses do not contradict each other within the same dialogue. For example, it is inconsistent if the assistant asserts one specific event occurred in 1911 in the first round of conversation but mentions it as a 1901 event in the next round. Each metric is scored with 0, 0.5 or 1, where higher score means better quality. Therefore, for a K-round dialogue, we have 3K+2 metric annotations.\nAnnotation. Each dialogue is annotated independently by two well-trained annotators, and the score is averaged across two annotators. Meanwhile, due to the difficulty in identifying the hallucination problem, we allow the annotators to skip the dialogues that require expert knowledge or whose validity is hard to check. Altogether, we collect 14560 valid annotations in terms of metrics for both single-round and multi-round, and the Cohen\u2019s kappa coefficient is 0.358. The average time to annotate one dialogue is 10 minutes.\nResults. As shown in Table 5, the dataset scores high on all metrics, showing the effectiveness of the construction process. It is worth noting that the dataset is almost free from harmful content like hate speech and discrimination. Furthermore, we observe two trade-offs between data quality. The first is between helpfulness and honesty. While the honesty score is pretty high, helpfulness is compromised. It is because some user queries are out of the LLM ability scope (e.g., ask the LLM to compose a song). Under such circumstances, the LLM refuses to answer the question with an explanation of incapability, which is reasonable and expected but less helpful. The phenomenon is particularly prominent in part 3, as the simulated user often asks for information not existent in the text material. The second trade-off is between control and dialogue naturalness. While sector 2 and sector 3 data are constructed with more guidance and control when prompting for user simulation, they have clearly lower quality in coherence and consistency than sector 1."
        },
        {
            "heading": "5 Experiments",
            "text": "We developed UltraLM, an enhanced variant of the LLaMA-13B (Touvron et al., 2023) model, by training it on the UltraChat dataset. To improve the model\u2019s comprehension of dialogue context, we break down each dialogue into smaller sequences, limiting them to a maximum length of 2048 tokens. During the training process, we only calculate the loss for the model\u2019s responses. This approach ensured that the model had access to the relevant information from earlier parts of the conversation, enabling a more comprehensive understanding of the ongoing dialogue. By incorporating the preceding context, UltraLM was equipped to generate more contextually appropriate and coherent responses. We use standard cross-entropy loss to finetune the model. The model is trained with 128 A100 GPUs and the total batch size is 512.\nThe evaluation of the trained model is conducted in two folds. We first evaluate UltraLM on traditional benchmark datasets to delineate the\nknowledge scope and the multiple abilities of the language model. To better demonstrate the chat ability of language models, an automatic response quality evaluation is performed to showcase the model\u2019s proficiency in delivering accurate and informative content during chat interactions. Note that UltraLM is solely trained on UltraChat dataset without further finetuning on task specific datasets."
        },
        {
            "heading": "5.1 Experimental Setup",
            "text": "Baselines.7 We mainly compare with other opensource instruction-tuned language models based on LLaMA (Touvron et al., 2023) and Pythia (Biderman et al., 2023) backbone model. The main baseline models include Alpaca (Taori et al., 2023), Vicuna (Chiang et al., 2023), Koala (Geng et al., 2023), Dolly (Conover et al., 2023), OpenAssistant (K\u00f6pf et al., 2023), and WizardLM (Xu et al., 2023a). The parameter size of the baselines ranges from 7B to 13B, which is comparable to UltraLM. Our evaluation also includes other chat language models like ChatGPT (OpenAI, 2022), MPT (Mosaic, 2023), and Baize (Xu et al., 2023b). A detailed description of the main baselines can be found in Appendix A.1.\nDatasets. For benchmark evaluation, we choose four datasets: ARC-Challenge (Clark et al., 2018), HellaSwag (Zellers et al., 2019), MMLU (Hendrycks et al., 2021), and TruthfulQA (Lin et al., 2021), evaluating commonsense knowledge, professional knowledge and complex reasoning and understanding abilities. Each benchmark is constructed as multiple choice questions and therefore metrics are readily computable. The four datasets prove to be challenging even for the best-performing language models like ChatGPT.\nFor response quality evaluation, we use 3 datasets. We first create an evaluation set by ourselves. The curated set encompasses the Vicuna benchmark as well as an additional 300 questions and instructions generated by GPT-4. The questions/instructions covered a wide range of topics, including commonsense, world knowledge, professional knowledge (specifically physics and biology), mathematics, response generation, and writing tasks on different levels of difficulty. Apart from the curated set, we also adopt AlpacaEval (Li et al., 2023b), a widely acknowledged\n7Some baselines used in our experiments are continuously updated. All the results in this section are from the latest versions of these baselines before June 23, 2023\nopen-source evaluation set and leaderboard specifically designed for evaluating LLMs. The leaderboard is created based on the win-rate against Text-Davinci-003 automatically evaluated by GPT4. To further compare with the state-of-the-art model WizardLM (Xu et al., 2023a), comparison result obtained with GPT-4 on the released EvolInstruct (Xu et al., 2023a) test set is also reported. Further benchmark dataset and implementation details can be found in Appendix A.2 and A.3."
        },
        {
            "heading": "5.2 Benchmark Evaluation",
            "text": "As shown in Table 6, with pure instruction-tuning on the UltraChat dataset, UltraLM significantly improves over LLaMA-13B and achieves the best overall performance across four benchmarks. It is worth noting that UltraLM overtakes the current state-of-the-art model by nearly 2% on ARCChallenge and TruthfulQA. It shows that UltraLM is equipped with both broad and profound comprehension of the world and commonsense knowledge. The improvement could be attributed to the systematic and comprehensive data construction process of UltraChat sector 1, which effectively extends and deepens the discussion about world knowledge in automatic conversation generation. Meanwhile, the comparative inferiority in MMLU hints on the lack of professional knowledge in specific fields. It suggests the need for more advanced data generation techniques to build a specialized expert language model. As for HellaSwag, we notice that all models have only marginal improvement compared to LLaMA-13B. It is probably because HellaSwag is formatted as an in-text completion task instead\nof a straightforward text completion, and therefore benefits little from instruction tuning."
        },
        {
            "heading": "5.3 Response Quality Evaluation",
            "text": "Response Comparison. For our curated evaluation set, we conduct pairwise evaluation between UltraLM and each baseline model with GPT-4. Our evaluation prompt is designed to prioritize correctness over other factors such as informativeness. To mitigate the influence of presentation order of responses, we randomly determine the order of the responses for each question. Finally, we count the number of Win/Tie/Lose times against each baseline model, and the result is presented in Figure 2. UltraLM demonstrates superior performance compared to every open-source model, exhibiting an impressive winning rate of up to 98%. It is worth noting that UltraLM also outperforms Vicuna with 9% higher winning rate.\nIndependent Scoring. Given the instability of pairwise comparison, we also conduct independent quality scoring with GPT-4, as presented in Table 7. Notably, our model demonstrates superior performance compared to all the open-source counterparts by a significant margin in terms of overall scores. This breakdown also provides insights into the performance of each model on specific types of questions and instructions. Generally, all models perform better on simpler questions pertaining to commonsense knowledge and general world understanding. However, more complex tasks that involve reasoning and creative writing proves to be challenging for most models. Interestingly,\nAlpaca, despite having only 7 billion parameters, performs comparatively well with larger models on questions related to commonsense and world knowledge. Meanwhile, Dolly and OpenAssistant, which are based on Pythia (Biderman et al., 2023), display inferior performance compared to models based on LLaMA of similar or even smaller sizes. This observation highlights the significance of the underlying backbone language model.\nAlpacaEval. As shown in Table 8, UltraLM outperforms existing models in terms of win rate on the current AlpacaEval leaderboard and ranks 4th just below ChatGPT. This observation further testifies the response quality of UltraLM and is in line with results on our curated dataset. Furthermore, a significant gap is evident between UltraLM and ChatGPT performance, highlighting the substantial effort needed for open-source models to match the capabilities of ChatGPT.\nEvol-Instruct Evaluation. Figure 3 shows the automatic comparison results against WizardLM13B on Evol-Instruct test set. UltraLM overtakes WizardLM on most types of questions, with up to 29% increase in scores. It is important to acknowledge that WizardLM-13B is trained on the EvolInstruct training set, making UltraLM\u2019s success on the test set a noteworthy achievement. Moreover, the questions observed with the largest improvement are mainly complex problems that often require synthesized abilities. It demonstrates the success of UltraChat design schema, which can comprehensively boost the versatile capabilities of language models. The inferiority in math prob-\nlems is not surprising though, as no mathematical problems are intentionally generated in UltraChat.\nImpact of System Prompts. Using system prompts to adjust the role and response style of LLMs is a common practice. Although system prompts are not embedded in UltraLM\u2019s training data like others do (Chiang et al., 2023), they still appear to have a substantial influence on the response style of the generated output. Specifically, when the model is prompted to provide a \"helpful and detailed\" response, it tends to generate more pertinent details While such prompts may not improve the accuracy of an answer, they do raise overall quality with more informative response. To illustrate this effect, we conduct an ablation response comparison on UltraLM. Table 9 reveals significant improvements in response quality brought by system prompts across all tasks. We also inspect the detailed evaluation for deterministic questions (commonsense and world knowledge) and find that UltraLM without system prompt only incorrectly answers one question. Thus, the main benefit of the system prompt lies in enhanced informativeness rather than higher correctness."
        },
        {
            "heading": "6 Conclusion",
            "text": "In drawing to a close, our work introduces UltraChat, a structured design of multi-turn instructional conversation data primed to foster the growth of\ngeneral chat models. UltraChat encapsulates a broad range of human-AI interactions, further developing a series of dialogues across various topics and instructions. Statistically, UltraChat shows an impressive presence in critical metrics such as scale, average length, diversity, and consistency, further establishing itself as a leading open-source dataset. We leverage UltraChat to fine-tune the LLaMA model, leading to the development of the robust conversational model, UltraLM. Evaluation across multiple benchmarks reveals that UltraLM surpasses previous open-source models like WizardLM, Vicuna, Alpaca, and Koala in performance. We eagerly await the innovative research and development that will be catalyzed by our contributions in the field of AI conversational models.\nLimitations\nEvaluating the response quality of large language models is an extremely challenging task, and any assessments may have biases. For a comprehensive evaluation, we compared UltraLM\u2019s performance with other baselines across various benchmarks, utilizing GPT-4 to assess the response quality. Nevertheless, the need for additional, diverse evaluations remains to facilitate a more thorough understanding of our model\u2019s behavior and performance. Despite demonstrating promising results in experimental settings, UltraLM is not immune to the common pitfalls of large language models, including hallucination issues and potential ethical concerns associated with misuse. Additionally, the energy-intensive nature of UltraLM\u2019s training process represents a limitation, particularly when compared to models employing more efficient techniques such as parameter-efficient fine-tuning. In terms of UltraChat, it currently only contains English and there are no explicit methodologies incorporated for generating data to enhance the model\u2019s reasoning capabilities, representing another area for potential improvement.\nEthics Statement\nWhile the advancements of the UltraChat dataset and UltraLM are commendable, they still face ethical challenges that exist in the area of LLMs. For the sake of privacy protection, we do not use any online or even human queries/instructions to construct UltraChat but develop a framework to build scalable, diverse instructional data. Although extensive filtering operations are conducted, biased statements may still exist within the dataset.\nUltraLM, as the paper described, will be one of the most potent open-source chat language models. With great power comes increased responsibility and potential for misuse. There exists a substantial risk for the technology to be weaponized for spreading misinformation, propaganda, or even creating \u201cdeepfake\u201d text that could mislead or manipulate public discourse. This necessitates the establishment of robust policies and comprehensive research to prevent misuse and deter malicious applications of the technology. In this regard, the model\u2019s potential applications should be carefully evaluated for any possible negative consequences before deployment."
        },
        {
            "heading": "Acknowledgements",
            "text": "This work is supported by the National Key R&D Program of China (No. 2022ZD0119101), National Natural Science Foundation of China (No. 62236004), the Young Elite Scientists Sponsorship Program by CAST, and Institute Guo Qiang at Tsinghua University."
        },
        {
            "heading": "A Experimental Details",
            "text": "A.1 Baselines\nWe introduce the main open-source baseline models below. Alpaca (Taori et al., 2023) is an instructionfollowing language model derived from the LLaMA (Touvron et al., 2023) model that has been effectively optimized on 52,000 demonstrations of instruction data. The data is generated by SelfInstruct approach with Text-Davinci-003. Vicuna-13B (Chiang et al., 2023) is an opensourced chat model created by fine-tuning LLaMA on user-shared conversations collected from ShareGPT8. An automatic evaluation by GPT-4 demonstrates that Vicuna can yield over 90% response quality of ChatGPT. In following practices, Vicuna is widely acknowledged as the state-of-theart open-source chat model. This is evident in the Chat Arena9, where a total of 13,000 anonymous votes reveal that the quality score of vicuna-13B surpasses that of other open-source models. Koala-13B (Geng et al., 2023) is another LLaMAbased model fine-tuned on selected public dialogues. In existing open evaluations, Koala\u2019s performance will be slightly worse than vicuna, but it still remains a strong baseline. Dolly-V2 (Conover et al., 2023) is based on the Pythia (Biderman et al., 2023) model, which utilizes 15k human-generated instruction-following data. The data is organized by following InstructGPT (Ouyang et al., 2022), including brainstorming, classification, closed QA, generation, information extraction, open QA, and summarization. OpenAssistant-12B (K\u00f6pf et al., 2023) is also a Pythia-based model that attempts to democratize the alignment process of LLMs. The project collects a conversation corpus consisting of 161,443 messages distributed across 66,497 conversation trees and trains a model on these manually annotated data. WizardLM-13B (Xu et al., 2023a) is a LLaMAbased model finetuned on Evol-Instruct dataset, which contains 250k instructions. The data are generated with evolutionary strategy, where the initial instructions are rewritten by ChatGPT for several epochs to increase complexity progressively.\n8https://sharegpt.com/ 9https://lmsys.org/blog/\n2023-05-03-arena/\nA.2 Evaluation Dataset\nTable 11 presents the basic information of the evaluation datasets we use. Below we give a detailed description of each dataset. The AI2 Reasoning Challenge (ARC) (Clark et al., 2018) is comprised of advanced science questions and structured as multiple-choice questions. Each question is accompanied by 4 available choices and only one answer is correct. We use the challenge partition here for evaluation, which contains 1172 test examples. HellaSwag (Zellers et al., 2019) tests commonsense inference ability by evaluating how well the language model can predict the remaining part of a sentence. Each sample has 4 different text pieces as the candidate remaining part of a given sentence and only one of them is plausible. The task is shown to be easy for humans but challenging for language models. We use the validation split as in Gao et al. (2021). MMLU (Hendrycks et al., 2021) is a comprehensive dataset that consists of 57 different tasks covering assessments of multiple types of academic knowledge and problem-solving ability of language models, ranging over expert fields like mathematics, history, law, etc. TruthfulQA (Lin et al., 2021) assesses how well a model can identify true statements related to the real world. Its purpose is to determine the risks of producing false claims or spreading misinformation. The benchmark consists of questions written in various styles, covering 38 different categories, and is designed to be challenging. It includes two evaluation tasks: the multiple-choice task and the generation task. We use the multiple-choice task in the validation split as in Gao et al. (2021). AlpacaEval (Li et al., 2023b) is a hybrid evaluation dataset with altogether 805 instructions, which combines instructions from various existing evaluation sets, including self-instruct (Wang et al., 2022), Open Assistant (K\u00f6pf et al., 2023), helpful evaluation released by Anthropic (Bai et al., 2022), Vicuna (Chiang et al., 2023), and Koala (Geng et al., 2023). Evol-Instruct is a dataset released in Xu et al. (2023a). It is constructed with an evolutionary strategy by rewriting the instructions through multiple rounds to obtain instructions at different complexity levels. WizardLM (Xu et al., 2023a) is trained on the training set. We evaluate our model on the test set.\nOur evaluation set is composed of Vicuna (Chiang et al., 2023) test set and other instructions generated by GPT-4. The generated instructions are further split into different categories and difficulty levels to comprehensively evaluate the model\u2019s ability. Table 10 shows example data for each type of question.\nA.3 Implementation Details\nIn benchmark evaluation, the multiple choice questions are further transformed into log-likelihood calculation over each answer candidate to determine the final prediction. Specifically, each available answer is appended to the question and fed into the language model. The model calculates logits for the answer part and derives the likelihood of the answer. The most likely answer is taken as the final prediction. Moreover, we use likelihood normalized by answer length to mitigate the bias caused by length variation. For the first 3 benchmarks, accuracy is reported for each test sample directly. For Truthful QA, since there are multiple correct answers, apart from standard accuracy\n(mc1), we also report the ratio of likelihood sum of correct answers over all candidate answers (the mc2 metric). In Table 6, Acc. norm. means the accuracy is computed after normalizing the likelihood with response length.\nIn response quality evaluation, all evaluations are done by GPT-4. Figure 5 and Figure 6 present prompts used for pairwise scoring and independent scoring on our evaluation data. As for AlpacaEval and Evol-Instruct, we follow the official prompt implementation (Li et al., 2023b; Xu et al., 2023a) to ensure a fair comparison.\nAcross all experiments, UltraLM is run with temperature=0.5, top_p=0.95, max new token=2000, and early stopping."
        },
        {
            "heading": "B UltraChat Details",
            "text": "We present a comparison between multi-turn dialogues directly generated by ChatGPT the one in UltraChat in Table 12. Both have the same opening line. Figure 4 is the template used to concatenate text pieces from the C4 corpus and instructions generated by ChatGPT for data in UltraChat sector 3."
        },
        {
            "heading": "C Prompts",
            "text": "In this section, we provide system prompt used for response generation by UltraLM (Figure 7) and prompts for automatic evaluation by GPT-4 (Figure 5 and 6)."
        },
        {
            "heading": "D UltraLM Generation Examples",
            "text": "In this section, we list response examples of UltraLM-13B on different instructions from our\nevaluation set.\nSystem Prompt You are now a helpful assistant, your answer should focus on the question, give reasonable, selfconsistent, high-quality solutions to make users satisfied. Please try to make the answer more informative, detailed, and polite.\nExample C: Question of Common Knowledge\nUser: Which animal is known as man\u2019s best friend? UltraLM: There are many animals that humans enjoy as companion animals, but one of the absolute standout answers to the question \"which animal is known as a man\u2019s best friend?\" is the domestically-bred dog. Domesticated dogs have been by humans\u2019 sides for tens of thousands of years, providing faithful companionship and help; guarding people and property; and lending emotional support in everything from long, lonely days to military deployments. Dogs come in an almost mind-boggling variety of breeds of various sizes, with different energy levels, personalities, and temperaments, meaning there\u2019s almost certainly a pup out there who will be a perfect fit for whoever needs it most.\nExample D: Commonsense Reasoning\nUser: Which weighs more: a pound of feathers or a pound of bricks? UltraLM: A pound of feathers and a pound of bricks both weigh exactly one pound. This is because one pound is a unit of weight, just as a foot is a unit of length or a second is a unit of time. We use the pound (or other unit of weight) to describe the weight of different objects, regardless of what that object is made of. So the answer to your question is that a pound of feathers and a pound of bricks both weigh the same amount.\nExample E: Reasoning\nUser: \"If a farmer has 17 sheep and all but 9 die, how many are left? UltraLM: The total number of sheep that are left is 9.\nExample F: Reasoning\nUser: If a red house is made from red bricks, and a blue house is made from blue bricks, what is a greenhouse made from? UltraLM: A greenhouse is not made from green bricks, but rather from materials such as glass or plastic that allow light to pass through and create a controlled environment for plants to grow. Greenhouses can come in a variety of colors, including green, but the color of the building itself does not affect the plants growing inside.\nExample G: Creative Generation\nUser: Write a short dialogue between two planets. UltraLM: Mars: \"Hey there Neptune, how\u2019s it going?\" Neptune: \"Hi there Mars, I\u2019ve been better. It\u2019s getting a bit windy around here.\" Mars: \"Sorry to hear that. Have you tried using energy other than wind? Maybe solar power?\" Neptune: \"Yeah, but it\u2019s getting difficult to generate solar power in all of this mist and clouds. And we definitely can\u2019t use tides or waves here, they\u2019re too unpredictable.\" Mars: \"Oh, I see. Well, I hope you find a solution soon. How are your oceans holding up?\" Neptune: \"They\u2019ve been a bit choppy lately with these intense storms we\u2019ve been experiencing. But I guess it comes with living in a planet like this.\" Mars: \" Definitely not designed for the faint-hearted! Any other issues you\u2019re facing?\" Neptune: \"Not really. I think we have a stable temperature and atmosphere, and the wildlife here hasn\u2019t shown much complaint.\" Mars: \"That\u2019s lucky. I envy your tranquil oceans, we\u2019ve had way too much volcanic activity lately causing all sorts of disturbance in our lava streams.\" Neptune: \"That sounds rough, better invest in those climate-controlling nanites to stabilize things maybe?\" Mars \"Ha-ha, very funny. Just like we also could consider your frozen polar regions as an opportunity to experiment with our frost nanobots on.\" Neptune: \"A fun concept! But let\u2019s stick to the task at hand, if you need any technical advice, come right ahead, I don\u2019t want your planet getting any more disturbed than it already is.\nExample H: Creative Generation\nUser: Pretend to be a character in a post-apocalyptic world. Describe how you survive and the allies you encounter. UltraLM: As a survivor in a post-apocalyptic world, I have learned to be resourceful, alert, and adaptable. I have had to rely on my wits and my instincts to survive, and I have been fortunate enough to encounter many allies who have helped me along the way.One of my greatest survival tools has been my ability to scavenge for resources. I have learned to identify the best places to find food, water, and shelter, and I have become skilled at navigating the rough terrain that makes up my new world. When I first emerged from my bunker after the apocalypse hit, I was alone and unsure of what to do. But as I ventured out into the wasteland, I realized that there were others like me who were struggling to survive.I quickly learned to Trust my instincts and to be constantly alert for potential dangers. I have encountered wild animals, rogue survivors, and even mutated creatures over the years, and I have learned to defend myself against them. I still remember the first time I met a group of fellow survivors, and how grateful I was to have them by my side. We quickly formed a cohesive unit, and together we were able to fend off danger and defend our territory.As time passed, I encountered more and more allies who were willing to lend a hand. I found a group of farmers who taught me how to grow my own food, and a group of scientists who were working to find a cure for the lethal virus that had decimated the population. I even found a band of traveling performers who helped to lift our spirits and remind us that there was still joy to be found in this desolate world. Despite the challenges we faced, I am grateful to have these amazing people in my life. They have taught me that even in the darkest of times, there is always hope. We may not have a cure for the virus yet, but we are a determined lot, and we are all in this together. As long as we have each other, we will survive.\nExample I: Creative Generation\nUser: Write a poem about electromagnetism. UltraLM: Electromagnetism is a force we can\u2019t ignore, It\u2019s the reason for many things we adore. In a magnetic field, a wire will glow, And the electrons will align, both old and new. Electricity and magnetism are hand in hand, One creates a field, the other has a brand. The magnetic field can push and pull, On a wire with current, it will react in full. The attraction and repulsion it brings, Are the reason for many things. From motors to generators, and much more in between, Electromagnetism is at the root of what it means. It\u2019s a force that\u2019s all around us, From the compass to the light switch. With its power, it can do much, From a simple needle to a rocket\u2019s thrust. It\u2019s a force we can\u2019t deny, For it\u2019s a part of our lives. Electromagnetism is something to admire, For its strength and its might, it has no equal, it\u2019s sure.\nExample J: Literature Creation\nUser: Write a short story about the first second of the universe. UltraLM: In the first second of the universe, darkness reigned supreme. There was no light, no sound, no shape, no substance. Nothing yet was, but there was an intense power, a raw energy waiting to be unleashed. Within infinity, the first second held the potential for creation in its grasp. And then, with a mighty bang, a bright point of light emerged into existence. In it, the most fundamental particles were born, their forms yet unknown as they frenetically collided in a chaotic dance of creation. Within this primordial light, every force of nature and every component of life was stirred from pure energy to physical reality. The first atoms quickly assembled, and from these, stars and galaxies began to form. Their fusion creating a hot environment that pushed back the very void, shaping the cosmos through expanding and cooling waves. In this cosmic first, no structure of experience, not even time or space, could establish its rule. For an instant, the entirety of the universe existed in an embryonic nexus, a cosmic singularity from which all future existence originated. It was a microcosm forging the macrocosm of which it ultimately becomes a part, its moment of birth a metaphor for the cycles of creation that permeate the multiverse(s).From that single point of light, an infinite universe unfolded. For centuries, eons ahead, and forevermore, it remained the most foundational blink of its unceasing journey into the depths of the Great Beyond. The first second is the beginning of everything.\nExample K: Professional Knowledge\nUser: How does superconductivity work and what are its practical applications? UltraLM: Superconductivity is a phenomenon that allows materials to conduct electricity without resistance at extremely low temperatures. At low temperatures, the interaction between the particles in a superconducting material can produce a state called the \"Bose-Einstein condensate,\" which leads to the loss of electrical resistance. In materials, superconductivity comes when a magnetic force interacts with an Electronic current. This interplay in turn provides energy savings and speeds up current. Practical applications mainly include MRI machines, high-speed trains, magnetic levitation, supercharging power grids that can transfer energy more efficiently.One of the main applications of Superconductivity is being used in the Magnetic Resonance Imaging(MRI) machines used in hospitals around the world, to non-invasively scan patients for health diagnosis without exposing them to high levels of radiation. Another important practical application of superconductivity is in power grid operations where large currents can be transmitted to long distances by highly superconducting coil systems that minimize grid energy losses, reducing environmental pollution also by highly efficient electric motors that harness large quantities of electricity efficiently.Aside from these, future projects include superconducting propulsion on planes, trains becoming lighter if not magnetic and able to move on rails with a significant increase in speed. Furthermore, the prospects of supercapacitors replacing electric vehicle batteries being cost and energy efficient are currently being researched. Overall, superconductivity presents various exciting opportunities in multiple industries from high-precision engineering to reducing CO2 emissions and supporting a more environmentally friendly energy model where both cost efficiency and practicality exist hand in hand.\nExample L: Mathematics\nUser: \"What is the definition of a limit in calculus? UltraLM: In calculus, a limit is a value that a function approaches as its input approaches a certain value. The limit is a measure of how close the function is to that value. In other words, a limit is the value that a function \u201capproaches\" as its input gets \u201ccloser and closer\" to a certain value. For example, consider the function f(x) = 1x . As x approaches 0, the function \u201capproaches\" +\u221e, and the limit of the function as x approaches 0 is +\u221e. Formally, we can define the limit of a function f(x) as x approaches a certain value c as limx\u2192c f(x) = L, where L is a real number. If the limit exists, we say that the function has a limit at c and the value of the limit is L. If the limit does not exist, we say that the function has a removable discontinuity at c.\nExample M: Professional Knowledge\nUser: What are the main interpretations of quantum mechanics and how do they differ? UltraLM: Quantum mechanics is a fundamental theory in physics that describes the behavior of matter and energy at the smallest scales of space and time. There are several interpretations of quantum mechanics, each with its own set of assumptions and predictions about how the world works. Here are some of the main interpretations of quantum mechanics and how they differ:\n1. Copenhagen Interpretation: This is the most widely accepted interpretation of quantum mechanics. It states that until a measurement is made, a system exists in a superposition of states, representing all possible outcomes with equal probability. When a measurement is made, the system collapses into a single, definite state. This interpretation assumes that the act of measurement itself causes the system to collapse into a definite state, and that there is no objective reality until a measurement is made. 2. Many-Worlds Interpretation: This interpretation proposes that every possible outcome of a superposition of states actually occurs in a multiverse of parallel universes. Thus, when a measurement is made, the system splits into different universes, each corresponding to one of the possible outcomes. This interpretation assumes that all possibilities are realized, but only in different universes, and that the act of measurement does not cause the collapse of the wave function. 3. Bohmian Mechanics: This interpretation proposes that particles have a definite position in space, even when in a superposition of states. It suggests that the wave function guides the motion of particles, and that the act of measurement does not cause a collapse of the wave function, but merely provides information about the pre-existing reality. 4. de Broglie-Bohm Theory: This interpretation is very similar to Bohmian Mechanics, but it also assumes that all particles in the universe are connected by a hidden-variable, and that the motion of one particle determines the motion of all other particles. 5. Pilot-Wave Theory: This interpretation proposes that particles have a wave-like property that guides their motion through space. It assumes that the wave function does not collapse, but rather that the wave guides the motion of particles.\nIn conclusion, there are several interpretations of quantum mechanics, each with its own set of assumptions and predictions about how the world works. While they all attempt to describe the behavior of matter and energy at the smallest scales of space and time, they differ in their assumptions about the role of the observer, the nature of reality, and the collapse of the wave function.\nExample N: Social Knowledge\nUser: How can you determine if a person is genuinely interested in a conversation or simply being polite? UltraLM: As an AI language model, I don\u2019t have personal experience, but there are a few signs you can look out for to determine if a person is genuinely interested in a conversation or simply being polite. 1. Eye contact: Genuine interest in a conversation often shows in the form of consistently maintaining eye contact. If a person is not making eye contact or is looking away frequently, they may not be genuinely interested in the conversation. 2. Active listening: A person who is genuinely interested in a conversation will show it through active listening. They will be attentive, ask questions, and respond to what you say. 3. Expressions: If a person is interested in the conversation, they will show it through their expressions. They will have a relaxed facial expression, a smile, and positive body language. 4. Follow-up questions: If a person is genuinely interested in a conversation, they will follow up with questions related to what you said. This will show that they are interested in hearing more about what you have to say. 5. Tone of voice: A person who is genuinely interested in the conversation will have a friendly and enthusiastic tone of voice. They will show interest and engage in the conversation. Remember, people have different communication styles, and it\u2019s possible to be polite without showing much interest in the conversation. It\u2019s important to observe different cues and not jump to conclusions too quickly."
        }
    ],
    "title": "Enhancing Chat Language Models by Scaling High-quality Instructional Conversations",
    "year": 2023
}