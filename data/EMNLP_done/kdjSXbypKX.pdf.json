{
    "abstractText": "Efficiently retrieving FAQ questions that match users\u2019 intent is essential for online customer service. Existing methods aim to fully utilize the dynamic conversation context to enhance the semantic association between the user query and FAQ questions. However, the conversation context contains noise, e.g., users may click questions they don\u2019t like, leading to inaccurate semantics modeling. To tackle this, we introduce tags of FAQ questions, which can help us eliminate irrelevant information. We later integrate them into a reinforcement learning framework and minimize the negative impact of irrelevant information in the dynamic conversation context. We experimentally demonstrate our efficiency and effectiveness on conversational FAQ retrieval compared to other baselines.",
    "authors": [
        {
            "affiliations": [],
            "name": "Yue Chen"
        },
        {
            "affiliations": [],
            "name": "Dingnan Jin"
        },
        {
            "affiliations": [],
            "name": "Chen Huang"
        },
        {
            "affiliations": [],
            "name": "Jia Liu"
        },
        {
            "affiliations": [],
            "name": "Wenqiang Lei"
        }
    ],
    "id": "SP:fd91e9a6d2127c93df5586c26c513c273f618371",
    "references": [
        {
            "authors": [
                "Richard Bellman",
                "Robert Kalaba."
            ],
            "title": "On the role of dynamic programming in statistical communication theory",
            "venue": "IRE Transactions on Information Theory, 3(3):197\u2013203.",
            "year": 1957
        },
        {
            "authors": [
                "Antoine Bordes",
                "Nicolas Usunier",
                "Alberto GarciaDuran",
                "Jason Weston",
                "Oksana Yakhnenko."
            ],
            "title": "Translating embeddings for modeling multirelational data",
            "venue": "Advances in neural information processing systems, 26.",
            "year": 2013
        },
        {
            "authors": [
                "Jon Ander Campos",
                "Arantxa Otegi",
                "Aitor Soroa",
                "Jan Deriu",
                "Mark Cieliebak",
                "Eneko Agirre."
            ],
            "title": "Doqa\u2013 accessing domain-specific faqs via conversational qa",
            "venue": "arXiv preprint arXiv:2005.01328.",
            "year": 2020
        },
        {
            "authors": [
                "Yang Deng",
                "Yaliang Li",
                "Fei Sun",
                "Bolin Ding",
                "Wai Lam."
            ],
            "title": "Unified conversational recommendation policy learning via graph-based reinforcement learning",
            "venue": "Proceedings of the 44th International ACM SIGIR Conference on Research and Development in",
            "year": 2021
        },
        {
            "authors": [
                "Joseph L Fleiss",
                "Jacob Cohen."
            ],
            "title": "The equivalence of weighted kappa and the intraclass correlation coefficient as measures of reliability",
            "venue": "Educational and psychological measurement, 33(3):613\u2013619.",
            "year": 1973
        },
        {
            "authors": [
                "J. Gao",
                "C. Xiong",
                "P. Bennett",
                "N. Craswell"
            ],
            "title": "Neural approaches to conversational information retrieval",
            "year": 2022
        },
        {
            "authors": [
                "Sparsh Gupta",
                "Vitor R Carvalho."
            ],
            "title": "Faq retrieval using attentive matching",
            "venue": "Proceedings of the 42nd",
            "year": 2019
        },
        {
            "authors": [
                "Mladen Karan",
                "Jan \u0160najder."
            ],
            "title": "Faqir\u2013a frequently asked questions retrieval test collection",
            "venue": "Text, Speech, and Dialogue: 19th International Conference, TSD 2016, Brno, Czech Republic, September 12-16, 2016, Proceedings 19, pages 74\u201381. Springer.",
            "year": 2016
        },
        {
            "authors": [
                "Kimiya Keyvan",
                "Jimmy Xiangji Huang."
            ],
            "title": "How to approach ambiguous queries in conversational search: A survey of techniques, approaches, tools, and challenges",
            "venue": "ACM Computing Surveys, 55(6):1\u2013",
            "year": 2022
        },
        {
            "authors": [
                "Seokhwan Kim",
                "Mihail Eric",
                "Karthik Gopalakrishnan",
                "Behnam Hedayatnia",
                "Yang Liu",
                "Dilek HakkaniTur"
            ],
            "title": "Beyond domain apis: Task-oriented conversational modeling with unstructured knowledge",
            "year": 2020
        },
        {
            "authors": [
                "W. Lei",
                "G. Zhang",
                "X. He",
                "Y. Miao",
                "X. Wang",
                "L. Chen",
                "T.S. Chua."
            ],
            "title": "Interactive path reasoning on graph for conversational recommendation",
            "venue": "ACM.",
            "year": 2020
        },
        {
            "authors": [
                "Wenqiang Lei",
                "Xiangnan He",
                "Yisong Miao",
                "Qingyun Wu",
                "Richang Hong",
                "Min-Yen Kan",
                "Tat-Seng Chua."
            ],
            "title": "Estimation-action-reflection: Towards deep interaction between conversational and recommender systems",
            "venue": "Proceedings of the 13th Interna-",
            "year": 2020
        },
        {
            "authors": [
                "Wenqiang Lei",
                "Gangyi Zhang",
                "Xiangnan He",
                "Yisong Miao",
                "Xiang Wang",
                "Liang Chen",
                "Tat-Seng Chua."
            ],
            "title": "Interactive path reasoning on graph for conversational recommendation",
            "venue": "Proceedings of the 26th ACM SIGKDD international conference on",
            "year": 2020
        },
        {
            "authors": [
                "Ruirui Li",
                "Liangda Li",
                "Xian Wu",
                "Yunhong Zhou",
                "Wei Wang."
            ],
            "title": "Click feedback-aware query recommendation using adversarial examples",
            "venue": "The World Wide Web Conference, pages 2978\u20132984.",
            "year": 2019
        },
        {
            "authors": [
                "Yosi Mass",
                "Boaz Carmeli",
                "Haggai Roitman",
                "David Konopnicki."
            ],
            "title": "Unsupervised faq retrieval with question generation and bert",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 807\u2013812.",
            "year": 2020
        },
        {
            "authors": [
                "Leland McInnes",
                "John Healy",
                "Steve Astels."
            ],
            "title": "hdbscan: Hierarchical density based clustering",
            "venue": "J. Open Source Softw., 2(11):205.",
            "year": 2017
        },
        {
            "authors": [
                "Bhavika R Ranoliya",
                "Nidhi Raghuwanshi",
                "Sanjay Singh."
            ],
            "title": "Chatbot for university related faqs",
            "venue": "2017 International Conference on Advances in Computing, Communications and Informatics (ICACCI), pages 1525\u20131530. IEEE.",
            "year": 2017
        },
        {
            "authors": [
                "M Romero",
                "Alejandro Moreo",
                "Juan Luis Castro"
            ],
            "title": "A cloud of faq: A highly-precise faq retrieval system for the web 2.0",
            "venue": "Knowledge-Based Systems,",
            "year": 2013
        },
        {
            "authors": [
                "Corbin Rosset",
                "Chenyan Xiong",
                "Xia Song",
                "Daniel Campos",
                "Nick Craswell",
                "Saurabh Tiwary",
                "Paul Bennett."
            ],
            "title": "Leading conversational search by suggesting useful questions",
            "venue": "Proceedings of the web conference 2020, pages 1160\u20131170.",
            "year": 2020
        },
        {
            "authors": [
                "Wataru Sakata",
                "Tomohide Shibata",
                "Ribeka Tanaka",
                "Sadao Kurohashi."
            ],
            "title": "Faq retrieval using queryquestion similarity and bert-based query-answer relevance",
            "venue": "Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in",
            "year": 2019
        },
        {
            "authors": [
                "Asha Vishwanathan",
                "Rajeev Unnikrishnan Warrier",
                "Gautham Vadakkekara Suresh",
                "Chandra Shekhar Kandpal."
            ],
            "title": "Multi-tenant optimization for fewshot task-oriented faq retrieval",
            "venue": "arXiv preprint arXiv:2301.10517.",
            "year": 2023
        },
        {
            "authors": [
                "Lili Yu",
                "Howard Chen",
                "Sida Wang",
                "Tao Lei",
                "Yoav Artzi."
            ],
            "title": "Interactive classification by asking informative questions",
            "venue": "arXiv preprint arXiv:1911.03598.",
            "year": 2019
        },
        {
            "authors": [
                "Xinliang Frederick Zhang",
                "Heming Sun",
                "Xiang Yue",
                "Simon Lin",
                "Huan Sun."
            ],
            "title": "Cough: A challenge dataset and models for covid-19 faq retrieval",
            "venue": "arXiv preprint arXiv:2010.12800.",
            "year": 2020
        },
        {
            "authors": [
                "Yiming Zhang",
                "Lingfei Wu",
                "Qi Shen",
                "Yitong Pang",
                "Zhihua Wei",
                "Fangli Xu",
                "Bo Long",
                "Jian Pei."
            ],
            "title": "Multi-choice questions based multi-interest policy learning for conversational recommendation",
            "venue": "arXiv preprint arXiv:2112.11775.",
            "year": 2021
        },
        {
            "authors": [
                "Sijin Zhou",
                "Xinyi Dai",
                "Haokun Chen",
                "Weinan Zhang",
                "Kan Ren",
                "Ruiming Tang",
                "Xiuqiang He",
                "Yong Yu."
            ],
            "title": "Interactive recommender system via knowledge graph-enhanced reinforcement learning",
            "venue": "Proceedings of the 43rd international ACM SIGIR con-",
            "year": 2020
        },
        {
            "authors": [
                "Lin Zhu",
                "Xinnan Dai",
                "Qihao Huang",
                "Hai Xiang",
                "Jie Zheng."
            ],
            "title": "Topic judgment helps question similarity prediction in medical faq dialogue systems",
            "venue": "2019 International Conference on Data Mining Workshops (ICDMW), pages 966\u2013972. IEEE.",
            "year": 2019
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Retrieving FAQ questions that match users\u2019 intent during user-system conversations is critical for online customer service in large companies (e.g., Alibaba, Amazon). In this paper, we call this scenario Conversational FAQ retrieval. Normally, it employs an AI assistant to model user behaviors (e.g., queries and clicks) and iteratively retrieve FAQ questions until meeting users\u2019 intent (Ranoliya et al., 2017; Vishwanathan et al., 2023). To satisfy user experience (Gao et al., 2022), it is needed to design an efficient retrieval strategy to find FAQ questions that match user intent in minimal turns.\nCurrent methods focus on modeling the semantic information in the conversation context. They model the semantic similarity to perform FAQ retrieval by concatenating user queries and clicking questions (Rosset et al., 2020; Vishwanathan et al., 2023) or applying attention mechanisms (Li et al., 2019). Although these methods show promising results, they assume that users\u2019 behaviors strictly\n\u2020 Corresponding author.\nadhere to their intent. In fact, users may click questions containing information that is irrelevant to their intent due to domain unfamiliarity or misoperation (Keyvan and Huang, 2022). This information, which we call \"tags\"1 following (Yu et al., 2019; Romero et al., 2013), brings noise to the conversation context and disturbs the retrieval efficiency.\nTaking Figure.1 (a) as an example, the user\u2019s target question is \"How to cancel the automatic repayment of credit cards?\". Due to domain unfamiliarity, in Figure.1 (c), the user clicks the question \"How to cancel the automatic payment of bank cards\", because it contains the same tag (\"cancel\") with the user\u2019s intent. Unfortunately, this introduces irrelevant tags such as \"automatic payment\" and \"bank card\". As a result, the system retrieves irrelevant FAQ questions in subsequent turns (e.g., \"How to cancel the automatic payment of the credit cards?\" in the next turn). Therefore, such irrelevant information makes existing systems require more turns to hit the FAQ question that matches the user\u2019s intent.\nWe believe that the key to finding the right FAQ question in minimal turns is to reduce the impact of irrelevant tags in clicked questions. Thus, we need to estimate whether a tag is irrelevant to user intent. Accordingly, as shown in Figure.1 (b), we assume that a tag has a high probability of being irrelevant if the user seldom clicks FAQ questions containing this tag. Besides, as shown in Figure.1 (e), the probability of a tag being irrelevant can be gradually estimated along with the dynamics of the conversation. This motivates us to utilize reinforcement learning (RL) to model the dynamic changes of the tags\u2019 irrelevance estimation. By maximizing the cumulative reward based on the estimation, the RL model learns an optimized strategy to reduce the impact of irrelevant tags and obtain the user intent in minimal turns. More specifically, we convert the conversation context into a representation\n1Tags take forms of keywords or segments in questions.\nthat models the dynamic tags\u2019 irrelevance estimation. When taking the representation as the state, we punish the RL system when it retrieves FAQ questions containing irrelevant tags, and reward it when it retrieves users\u2019 desired FAQ questions in minimal turns. By doing so, the RL system can dynamically adjust its strategy to avoid retrieving questions that contain tags with high estimated irrelevance and achieve a successful retrieval as quickly as possible. In this way, we can effectively decrease the negative impact of irrelevant information in the conversation context, and retrieve users\u2019 desired FAQ questions in minimal turns. We call our method the Tag-aware conveRsational FAQ retrieVal via rEinforcement Learning (TRAVEL).\nTo sum up, we have the following contributions: (1) For the first time, we point out the significance of reducing the impact of irrelevant information introduced by noisy user behaviors in Conversational FAQ Retrieval. (2) We propose a tag-aware reinforcement learning strategy that models the dynamic changes of the tags\u2019 irrelevance to achieve successful FAQ retrieval in the minimal turn. (3) By developing new FAQ data, we test our method with intensive empirical studies. The results support the efficiency and effectiveness of our method."
        },
        {
            "heading": "2 Framework",
            "text": "Notation. We denote the collection of questionanswer pairs as FAQ = {(q1, a1), ..., (qn, an)},\nwhere qi is a FAQ question with ai as its answer. Each question qi is categorized into a set of tags Pqi = {pi1, pi2, ...pim}. For a user ui, there is a question qi that matches his/her intent, which needs to be retrieved. At the turn t of a conversation, the system performs retrieval based on the conversation context. The conversation context at the turn t consists of: Ht, which records the user\u2019s queries; Qtclick, which records the questions that user clicked; Qtrej , which records the questions that user ignore; The conversation ends if the system retrieves the question qi. Otherwise, it continues to retrieve until reaching the maximum turns T .\nFramework Overview. As depicted in Figure 2, We formulate TRAVEL as a multiturn tag-aware reinforcement learning framework, which aims to learn a promising policy \u03c0\u2217 = argmax\u03c0\u03f5\u03a0 E [\u2211T t=0 r(st,at) ] . Here, the action at indicates which FAQ question to retrieve from the candidates, and st capture the conversation context at the turn t. With the help of RL, TRAVEL can learn an efficient strategy to perform successful retrieval in minimal turns. Our TRAVEL contains two key components, i.e., Tag-Level State Representation and Conversational Retrieval Strategy Optimization. The former estimates irrelevant tags in the context and converts the conversation context into the state; The latter optimizes a retrieval strategy by RL given the state. Overall, we begin by elaborating on the RL environment setting of\nTRAVEL in section 2.1. Then, in section 2.2, we delve into the two key components of TRAVEL."
        },
        {
            "heading": "2.1 RL Environment Setting",
            "text": "We elaborate on how to formulate the conversational FAQ task into RL. It involves informing the system about the state and actions (i.e., questions to retrieve), transitioning between states, and providing feedback-based rewards.\nState. The state, formulated as st = {Ht, Qtclick, P tclick, Qtrej , P trej}, containing the conversation context up to the turn t. Here, Qtclick is a set of user-clicked questions, and P tclick is a set of tags associated with questions in Qtclick. Moreover, Qtrej and P t rej are the questions that the user did not click and their corresponding tags. Action. Given the state, the system takes an action at by finding out which FAQ question should be retrieved from the candidate set Qcand. In practice, we retrieve five questions at each turn.\nTransition. When the user clicks/ignores questions or launches a query at turn t, our conversation context changes. Namely, the state st is updated to a new state st+1, specifically, by adding the clicked qi and its tags to Qt+1click and P t+1 click, or appending them into the Qt+1rej and P t+1 rej if the question is not clicked. If the user launches a query at turn t, we add the new query into conversation history Ht+1.\nNoise-Aware Reward. To achieve successful retrieval in minimal turns, we reward the model when the retrieval succeeds and punish it when it fails or the turn number exceeds the maximum turns. It is also crucial to inform models when irrelevant tags are introduced by noisy user behavior, such as clicking questions with irrelevant tags. It enables models to adapt the strategy during conversations. In this paper, we propose five rewards: (1) rclick_suc,\na positive reward when the user clicks. However, if clicked questions contain irrelevant tags, the value of this reward is reduced. (2) rclick_fail, a negative reward when the user does not click any question. (3) rret_suc, a strong positive reward when the user successfully obtains his target question, (4) rextra_turn, a negative reward when the number of turns increases, (5) rquit, a strong negative reward when reaching the maximum turns.\n2.2 Components of TRAVEL\nTRAVEL consists of two components: Tag-Level State Representation and Conversational Retrieval Strategy Optimization. The Tag-Level State Representation component focuses on estimating irrelevant tags within the conversation context and transforming the context into the state representation. The Conversational Retrieval Strategy Optimization utilizes the state to determine a strategy for FAQ retrieval using Q-Learning, aiming to achieve accurate retrieval in minimal turns. It further enhances the RL process by pruning the action space, following previous work (Lei et al., 2020b)."
        },
        {
            "heading": "2.2.1 Tag-Level State Representation",
            "text": "This section explains the estimation of irrelevant tags in the context and the modeling of the conversation context at the tag level to obtain the state representation. The process involves two steps. First, graph representations of questions at the tag level are obtained. This eliminates irrelevant semantics within questions and captures correlations between tags and questions. Then, a tag-aware mechanism is proposed to model the conversation context. It estimates irrelevant tags in the context and obtains the state in a fine-grained way. This state records the dynamically estimated irrelevance of tags.\nTag-Level Question Representation. To eliminate irrelevant information and utilize correlations between questions and tags, a graph G is constructed with questions and tags as nodes. The node representations of a question qi and a tag pi are obtained using TransE (Bordes et al., 2013), denoted as eqi and epi .\nTag-Aware Mechanism. We employ the tagaware mechanism is employed to transform the conversation context into the state. This mechanism estimates whether a clicked question in the context contains irrelevant tags and calculates a weight to reflect this information. By doing so, the model becomes aware of the presence of irrelevant tags in the click question and implicitly eliminates them. The mechanism is defined as follows:\nvtn = Wn \u2217 gtn, (1)\ngtn = 1\u2223\u2223Qtrej\u2223\u2223 \u2211 n\u2208Qtrej eqn + 1\u2223\u2223P trej\u2223\u2223 \u2211 n\u2208P trej epn , (2)\nwhere Wn \u2208 Rd\u00d7d is a trainable parameters. The vtn is derived from unclicked questions and their tags that are ignored by users. This information captures tags information that contradicts users\u2019 intent, referred to as negative embedding. Furthermore, given the graph embedding of the clicked questions Qtclick = [eqi1 , eqi2 ...eqin ], we obtain the state representation st as follows:\nst = N\u2211 n=1 \u03b1neqn , (3)\n\u03b1n = exp\n( hT\u03c3 ( W ( vtn||eqn )))\u2211N n \u2032 =1 exp ( hT\u03c3 ( W ( vtn||eq\nn \u2032 ))) , (4) where hT and W are trainable metrics.\nThe st is calculated by combining the clicked question embeddings eqin using weights. Each weight \u03b1n is determined based on the score between vtn and clicked question embedding eqin . A higher weight indicates the presence of more irrelevant tags in the question within the context.\nIt\u2019s important to note that although this weight is applied to questions, it is implicitly mapped onto the corresponding tags of questions through the graph representation. Using the weight contained in st as a signal, our model can implicitly eliminate irrelevant information in the context."
        },
        {
            "heading": "2.2.2 Conversational Retrieval Strategy Optimization",
            "text": "Given the state st contains the information about the conversation context and irrelevant tags, we\nneed a strategy to retrieve the right question in minimal turns. Thus, we utilize the Dueling Q-network following previous work (Zhou et al., 2020). The Dueling Q-network is formulated as:\nQ(st,at) = f\u03b8V (st) + f\u03b8A (st,at) , (5)\nwhere f\u03b8V (.) and f\u03b8A (.) are two separate multilayer perceptions with parameters \u03b8V and \u03b8A. This equation takes the state st and an action (the graph representation of a FAQ question) as inputs and provides a score for that question. The score Q(st,at) represents the expected reward when taking the action at based on the state st. To maximize the cumulative expected reward, we follow the Bellman equation (Bellman and Kalaba, 1957):\nyt = Est+1 [ rt + \u03b3max\na\u2208A Q\u2217(st+1,at+1)|st,at\n] , (6)\nwhere yt denotes the Q\u2217(st,at). Since the reward is associated with retrieval accuracy, irrelevant tags information, and the number of turns as mentioned in section 2.1, maximizing the expected reward promotes the strategy to achieve accurate retrieval in minimal turns while avoiding retrieving other questions that contain irrelevant tags. Ultimately, for each action at, which corresponds to an FAQ question, the system selects the FAQ question with the highest Q-value to retrieve.\nTwo-Step Pruning Strategy. The performance of reinforcement learning is compromised by a large action space (Lei et al., 2020a). Thus, we propose two pruning strategies to shrink the action space. (1) We utilize semantic similarity pruning to shrink the action space following baselines (Vishwanathan et al., 2023). It models the semantics association between queries and FAQ questions. We select top-ks FAQ questions with the highest semantic scores, forming the candidate set Qtcand_sim. (2) Then, we use tag preference pruning to further reduce the action space. We choose top-kv questions with the highest score to form Qtcand_pre following the formula in Appendix A.5."
        },
        {
            "heading": "3 Experiments",
            "text": "This paper focuses on proposing a strategy to retrieve the appropriate FAQ question within a minimal number of turns. Consequently, our first research question investigates whether TRAVEL can outperform the baselines (including ChatGPT) given the limited turns. Subsequently, we explore\nwhether TRAVEL can maintain stable and superior performance in the face of different levels of noisy user behavior. Furthermore, we validate the efficiency of TRAVEL\u2019s individual components, ensuring their impact on performance. Finally, we verified if the tag-aware mechanism can actually estimate whether and how many irrelevant tags a clicked question contains in the context. The above research questions are as follows:\n\u2022 RQ1: Can TRAVEL outperform baselines in achieving more accurate retrieval given the limited number of turns?\n\u2022 RQ2: How does our method perform in the presence of different levels of noisy user behaviors compared to baselines?\n\u2022 RQ3: What is the impact of each component of TRAVEL on its performance?\n\u2022 RQ4: Can the tag-aware mechanism effectively estimate the presence and quantity of irrelevant tags the clicked questions contain?"
        },
        {
            "heading": "3.1 Dataset",
            "text": "We conduct experiments on our proposed data, since existing FAQ datasets do not contain the question tags and conversation history. Our dataset contains 72,013 conversation sessions. Each session is formulated as (ui, qi, Hi), where qi represented the user\u2019s intended question, Hi contains the conversation history between the user and system, and ui denoted the user along with their profile. There are 1449 FAQ questions and 1201 tags in the dataset. On average, each question contains 6 tags. See Appendix A.6 for more details of the dataset."
        },
        {
            "heading": "3.2 Experimental Settings",
            "text": ""
        },
        {
            "heading": "3.2.1 User Simulator",
            "text": "Due to the interactive nature, online experiments where the system interacts with real users and learns from their behaviors would be ideal. However, the trial-and-error strategy for training RL (Zhou et al., 2020) online would degrade the user\u2019s experience and the system\u2019s profit. Thus, following Lei et al. (2020c); Zhang et al. (2021), we simulate users\u2019 behaviors via a simulator.\nGiven a user ui and their target question qi, we simulate their click behaviors. Users\u2019 clicking behavior is influenced by two main factors: their intent (target) (Zhang et al., 2021) and their profile (Zhou et al., 2020). Specifically, users are inclined\nto click on items relevant to their intent or based on their interests (defined by their profiles). Therefore, the probability of user ui clicking a FAQ question qk is calculated as follows:\nr (ui, qk) = \u03b1 \u2217 rki + (1\u2212 \u03b1) \u2217 cki, (7)\ncki = f (qk, ei) , (8)\nwhere: (1) rki represents the probability of user ui clicking qk based on relevance to their target question qi. We define relevance as the number of overlapping tags between qk and the user\u2019s target qi. The click probabilities for different levels of relevance are derived from online statistics, as presented in Appendix A.4. (2) cki represents the probability of user clicking qk based on their interest profile ei. It is modeled as cki = f (qk, ei). We train the function f (.) using online data. Details of the hyperparameter \u03b1 are in Appendix A.1."
        },
        {
            "heading": "3.2.2 Baseline",
            "text": "We compare the TRAVEL with two classes of baselines methods (comparisons with ChatGPT are in Appendix A.2). The first is FAQ retrieval which represents the standard way to retrieve FAQ questions. The second is Question Suggestion which is used in web searches to predict the next question users may ask, which has a similarity to FAQ retrieval in the form of the task.\nFAQ retrieval: 1) BERT_TSUBAKI (Sakata et al., 2019)employs BERT to compute scores between queries and FAQ answers, and uses BM25 to compute scores between queries and FAQ questions; 2) SBERT_FAQ (Vishwanathan et al., 2023) is a fine-tuned BERT model optimized with triplet loss using FAQ questions; 3) DoQA (Campos et al., 2020) is a baseline for conversation-based question answering that utilizes only the first turn of the query; 4) CombSum (Mass et al., 2020) is a state-of-the-art FAQ retrieval method that calculates scores between the query and the question using both BM25 and BERT, and the score between the query and the answer using BERT; Question Suggestion: 5) CFAN (Li et al., 2019) is a multiturn question suggestion method that takes queries and clicked questions as input; 6) KnowledgeSelect (Kim et al., 2020) is a multi-turn retrieval method in conversational settings that utilizes BERT to model the multi-turn user queries; 7) DeepSuggest (Keyvan and Huang, 2022; Rosset et al., 2020) is a standard question suggestion method for conversations that incorporates multi-turn user queries and clicked questions using BERT."
        },
        {
            "heading": "3.2.3 Evaluation Metrics",
            "text": "Firstly, we evaluate the retrieval performance in the first turn using Recall@5. To assess the system\u2019s ability to successfully retrieve users\u2019 target questions within the limited k turns, we utilize the metric success rate (SR@k) (Lei et al., 2020c). To measure the average number of turns the model takes in conversations, we use AT(Average Turn) (Lei et al., 2020c), and to evaluate the retrieval ranking performance, we employ hNDCG@(T, K) (Deng et al., 2021). Furthermore, considering the importance of user experience, we limit the exposure of users to a large number of questions, which could be burdensome. Hence, we introduce the Average Shown (AS) to quantify the average number of FAQ questions seen by users."
        },
        {
            "heading": "3.2.4 Implementation Details",
            "text": "We split the dataset by 4:1:1 for training, validation, and testing and set the number of retrieved questions at each turn as 5. We set the maximum turn T as 10 during training. We set the ks of similaritybased as 100 and the kv of preference-based pruning as 10. We set the graph embedding size as 40. During the training procedure of DQN, the size of the experience buffer is 50000, and the sample size is 32. The learning rate is set to be 1e-4 with Adam optimizer. The discount factor \u03b3 is set to be 0.99. We adopt the reward settings to train the proposed method: rclick_suc = 0.03 \u2217m, rclick_fail = \u22120.1, rret_suc = 1, rextra_turn = \u22120.05, rquit = \u22120.3, where m denotes the number of tags that coincide between the clicked question and the user\u2019s target question. When the user clicks a question that introduces noise, the m is set to a minor value."
        },
        {
            "heading": "3.3 The overall comparison among different methods (RQ1)",
            "text": "This section presents the superior performance of TRAVEL compared to baselines in terms of achiev-\ning more accurate retrieval within limited turns. The results are presented in Table 1.2 The result indicates that TRAVEL outperforms all baselines.\nAnalyzing Table 1 reveals that while FAQ retrieval methods can deliver satisfactory results in the initial turn (Recall@5), their performance noticeably declines in subsequent retrieval turns (SR@k) compared to TRAVEL. TRAVEL, on the other hand, outperforms these methods by achieving a 13.71% higher SR@5 and a 25.28% lower AT compared to CombSum. This highlights the limitations of existing FAQ retrieval methods in adequately capturing and utilizing the multi-turn conversation context. These methods require numerous turns for successful retrieval.\nFurthermore, compared to existing question suggestion methods, TRAVEL also demonstrates superior performance. For instance, when compared to CFAN and DeepSuggest, which leverage multi-turn user queries and click questions, TRAVEL achieves 12.29%/5.01% higher SR@5 and 25.07%/7.05% lower AT , respectively. Additionally, TRAVEL outperforms these methods in terms of ranking effectiveness, with a 6.96%/2.21% higher hNDCG. These finds indicate that TRAVEL excels in ranking FAQ questions that align with the user\u2019s intent. The aforementioned experiments effectively demonstrate that TRAVEL outperforms state-ofthe-art question suggestion methods by effectively utilizing the conversation context for FAQ retrieval."
        },
        {
            "heading": "3.4 The Noise Robust Testing (RQ2)",
            "text": "In this section, we evaluate the stable performance of TRAVEL in comparison to baselines across varying levels of noisy user behaviors. The results indicate that TRAVEL consistently maintains stable\n2TRAVEL and DeepSuggest have consistent Recall@5. This is because there are no clicks in the first turn, so TRAVEL employs the same model as DeepSuggest. It can also use other models that have higher recall, which is not our focus.\nand superior performance. To investigate this, we vary the weight \u03b1 of the user simulator, setting it to 0.2, 0.5, 0.8, and 1. Appendix A.1 provides detailed experimental results that demonstrate how the user simulator deviates increasingly from real user behavior as the weights shift. This process can be considered as manually introducing noise. As illustrated in Figure 3, TRAVEL consistently achieves superior performance compared to the baselines, with an improvement of 4% on SR@5, even when different levels of noise are introduced. In contrast, the performance of the baselines shows considerable fluctuations as the noise values change. Notably, DeepSuggest experiences a decrease of 5% in SR@5 when the weight \u03b1 transitions from 0.8 to 0.5. In summary, this experiment provides compelling evidence of TRAVEL\u2019s capability to effectively handle conversation contexts in the presence of varying levels of user-introduced noise."
        },
        {
            "heading": "3.5 Ablation Study (RQ3)",
            "text": ""
        },
        {
            "heading": "3.5.1 Tag-Level State Representation",
            "text": "We assess the effectiveness of modeling the conversation context at the tag level, specifically through the tag-level question representation and tag-aware mechanism. The experiments show the important role of these two components in the model performance, especially for the tag-level question representation. As shown in Table 2 row (d), removing\nthe tag-level question representation leads to a significant decrease in performance across all metrics. For instance, the SR@5 drops by 21.52%. This illustrates the importance of modeling the question representation at the tag level. Additionally, removing the tag-aware mechanism (Table 2 row (c)) results in a 5.9% in SR@5. This highlights the benefit of incorporating the tag-aware mechanism. It is worth noting that the improvement brings by the tag-aware mechanism is smaller than that of the tag-level representation. This is because the tag-level representation has already captured the user\u2019s possible preferences for tags via the graph."
        },
        {
            "heading": "3.5.2 Noise-Aware Reward",
            "text": "Next, we evaluate the effectiveness of the noiseaware reward rclick_suc (section 3.1). The experiment demonstrates that the reward setting enhances the performance of TRAVEL Specifically, we perform the ablation study by setting all clicked behaviors to receive the same reward. Table 2 row (e) illustrates that replacing the noise-aware reward leads to a 6.8% decrease in SR@5. This demonstrates the significance of the noise-aware reward."
        },
        {
            "heading": "3.5.3 Two-Step Pruning",
            "text": "The top part in Table 2 (rows(a-b)) presents the results when the proposed pruning strategies are omitted. Notably, all metrics experience a noticeable decline when discarding them. Specifically, without semantic similarity pruning, the SR@5 decreases by 12.46%. Similarly, omitting the tag preference pruning leads to an 8.22% decrease in SR@5. These findings emphasize the critical role of utilizing semantic and tag information for effective pruning, which significantly enhances the performance of reinforcement learning (RL)."
        },
        {
            "heading": "3.6 Validation of the Tag-Aware Attention Mechanism (RQ4)",
            "text": "In this section, we aim to verify the effectiveness of the tag-aware mechanism in estimating the presence and quantity of irrelevant tags in clicked ques-\ntions. As explained in Section 2.2.1, the tag-aware attention mechanism is designed to assign a higher weight to inform the model when a clicked question contains many irrelevant tags.\nTo verify this, we compared the weight value of the Tag-Aware Mechanism in formula (3) with the weight value obtained from a fine-tuned sentence BERT. The sentence BERT takes the sentences of the clicked questions and the user\u2019s target question as input and outputs a score indicating the level of irrelevant information in the clicked questions. As depicted in Figure 4, the x-axis represents the degree of irrelevant tags in clicked questions of the context, where a larger value indicates fewer relevant tags and more irrelevant information. The comparison shows that tag-level modeling performs better in identifying the degree of irrelevant tags in the context. Specifically, as the degree increases from 0 to 10, the weight value obtained from the tag-aware mechanism exhibits a change of 24.12%. In contrast, the weight value obtained from the sentence BERT model only changes by 7.37%. This clearly demonstrates that our mechanism can better estimate the number of irrelevant tags in clicked questions compared to previous methods. These findings highlight the superiority of the tag-aware mechanism in accurately identifying irrelevant tags within the conversation context. This mechanism provides explanatory power for our approach and underscores its effectiveness."
        },
        {
            "heading": "4 Related Work",
            "text": "FAQ retrieval has broad applications in conversation such as conversational QA systems (Campos et al., 2020) and dialogue systems (Vishwanathan et al., 2023; Zhu et al., 2019). It is also widely used for online customer service in large companies such as Alibaba, Amazon, and Google. To satisfy\nuser experience in real-world applications, FAQ retrieval in conversation aims to perform successful retrieval in very few interactions. Technically, FAQ retrieval is achieved by modeling the semantic association between user queries and FAQ questions. Existing works use convolution neural networks (Karan and \u0160najder, 2016), long short-term memory (Gupta and Carvalho, 2019) or pre-trained language models to model the semantic similarity (Sakata et al., 2019; Zhang et al., 2020). To fully utilize the conversation context, some web question suggestion methods (Li et al., 2019) incorporate the user-clicked questions with queries as the input of the semantic model using concatenation or attention. However, existing methods mainly model the conversation context from the semantic level, instead of modeling user behavior through a policy network. Meanwhile, existing datasets (Karan and \u0160najder, 2016; Sakata et al., 2019; Zhang et al., 2020) do not contain tags or conversation history."
        },
        {
            "heading": "5 Conclusion",
            "text": "In this paper, we focus on proposing a strategy for retrieving FAQ questions that match user intent in minimal turns. To achieve this goal, we introduce a tag-aware conversational FAQ retrieval framework via reinforcement learning called TRAVEL. This framework is designed to eliminate the detrimental impact of irrelevant information in the conversation context. It contains two main components: TagLevel State Representation and Conversational Retrieval Strategy Optimization. TRAVEL mitigates the negative impact of irrelevant information in the context by estimating its degree of irrelevance and employs a reinforcement learning strategy for performing FAQ retrieval. To verify our ideas, we create a dataset and develop a user simulator. Through extensive experiments, we justify the effectiveness of the TRAVEL framework, offering valuable insights into achieving successful FAQ retrieval in the fewest possible turns."
        },
        {
            "heading": "6 Limitation",
            "text": "Due to the interactive nature of our framework, online experiments where the system interacts with real users and learns from their behaviors would be ideal. However, the trial-and-error strategy for training RL online would degrade the user\u2019s experience and the system\u2019s profit. Therefore, we propose a user simulator to conduct the experiments offline. Even if we have proven the authenticity of the user\nsimulator, this still leaves us with a gap from the real scenario."
        },
        {
            "heading": "7 Ethics Statement",
            "text": "This paper presents a conversation FAQ retrieval framework with a new dataset and user simulator. Although our datasets are collected from an e-commerce company, they are designed for normal users and have been widely used by the public for some time. We also have carefully checked our dataset to make sure they don\u2019t contain any personally identifiable information or sensitive personally identifiable information (the user interest profile in Section 3.2.1 has been successfully desensitized). Thus, we believe there are no privacy concerns."
        },
        {
            "heading": "Acknowledgement",
            "text": "This work was supported in part by the National Natural Science Foundation of China (No. 62272330); in part by the Fundamental Research Funds for the Central Universities (No. YJ202219)."
        },
        {
            "heading": "A Appendices",
            "text": "A.1 Evaluation of the User Simulator Through experiments, we validate the authenticity of the user simulator and determine the value of \u03b1 that best aligns with real user behaviors in formula (6). In this section, we assessed the fidelity of our simulator by comparing its behavior with actual user behaviors. We built a test set using online user click data, which included information on users and their clicked and unclicked FAQ questions. We then examined whether the simulator exhibited consistent click behavior with the users in this test set while varying the weight values (\u03b1) in the formula\n(6). As shown in Figure 5, we observed that the highest level of consistency between the simulator and the users in the test set was achieved when \u03b1 was set to 0.8. Moreover, across different values of \u03b1, the simulator consistently exhibited high levels of consistency (above 0.7). These experiments provide compelling evidence supporting the reliability of our simulator, indicating that it accurately emulates user behavior in the given context.\nA.2 The Comparison with ChatGPT\nWith ChatGPT\u2019s rapid development, we are curious about how well it would work in our scenarios. Due to the time-consuming inference of ChatGPT and the large length of the conversation, we did not test it on the complete testing set. We compare the performance of our method and ChatGPT on 1000 random samples from the complete testing set. It is worth noting that due to the limited input length of ChatGPT, we cannot feed all candidate questions as prompts to ChatGPT. Instead, only the 50 questions recalled by BM25 were used as input to ChatGPT. At the same time, we do an operation in favor of ChatGPT; that is, there must be the ground truth question in the recalled questions. This prevents ChatGPT from losing performance due to inaccurate BM25 recall. The experimental results are in Table 3. It shows that TRAVEL achieves better results (22.87% higher on Recall@5 and 12.83% higher on SR@5) compared to ChatGpt. This may be due to ChatGPT\u2019s lack of domain-specific a priori knowledge, which causes it to remain inadequate in domain-specific information retrieval.\nA.3 Parameter Sensitivity Analysis\nThe upper part of Table 4 summarizes the experimental results (SR) by varying the ks, which denotes the size of the semantic similarity pruning. When the ks is 50, the performance essentially de-\ncreases. This may be because the similarity model is limited by ambiguous queries, resulting in some ground-truth FAQ questions being over-filtered. When the ks is 150, the performance also decreases. This is because the pruning might be ineffective when the pruning size is too large. The middle part of Table 4 summarizes the results by varying the kv, which denotes the size of the preference-based pruning. The performance is better when we set the kv to a small value of 10. This result demonstrates the effectiveness of using tag preference pruning. The lower part shows the experimental results by varying the maximum turn T during training. Although the maximum turn T is set to 5 during testing, the performance is better when the T is set to 10 during training. This is probably because the DQN can learn a better strategy when trained in a long conversation session. The performance decreases when the T is set to a more significant value, such as 15. This may be because of the large gap in the conversation environment between training and testing when the length of the conversation session varies greatly.\nA.4 The Click Probabilities Table\nThis section shows the correlation between the probability of a user clicking and the tag overlap, which is calculated from online data mentioned in Appendix A.4. The results are shown in Table 5. It is important to note that this probability value only factors as part of the user simulator behavior.\nA.5 Tag Preference Pruning\nThe score of tag preference pruning is calculated by \u03c3 (si), where si is determined by:\n\u2211 qn\u2208Qtclick eTqieqn+ \u2211 pn\u2208P tclick eTqiepn\u2212 \u2211 qm\u2208Qtrej eTqieqm , (9)\nHere, eqn represents the graph embedding of the clicked question, epn represents the embedding of the corresponding tag of eqn , and eqm represents the embedding of the question the user didn\u2019t click.\nA.6 Data Collection\nConsidering that existing FAQ datasets lack conversation history and question tags, to verify our studies, we propose a new dataset with the support of a large Chinese financial enterprise. We built this dataset in three steps. Firstly, we generated the FAQ questions. Next, we assigned tags to each FAQ question. Finally, we assembled conversation sessions, including user profiles and conversation histories, to create the training and testing sets. This table shows the statistics of our dataset. Specifically, the dataset has 1449 questions and 1201 tags. On average, there are 6 tags per question. The dataset contains 65,100 users and their conversation histories. In total, we collected 72,013 conversation histories. The average length of each conversation is four turns.\nA.6.1 Questions Collection We collect massive user questions and cluster them using the HDBSCAN algorithm (McInnes et al., 2017). Each cluster was then reviewed by domain experts who selected 10-15 representative questions to form the set of FAQ questions. The final collection consists of 1449 user questions, covering the majority of user intentions in the given domain.\nA.6.2 Tags Labeling For tag labeling, we engaged three domain experts to pre-define the ontology of tags, which included business objects and user actions (e.g., business objects and user actions such as \"Product\" and \"Intent\" in Table 7). Using this ontology, we were able to assign multiple detailed tag values to each user question as shown in Table 7. To annotate the tag values, we initially assigned 20 crowd-workers to independently label ten randomly chosen questions. If the questions were simple or had few tags, another set of ten questions was provided. After the initial annotation, the workers resolved any disagreements and revised the annotation scheme. Subsequently, they collaboratively annotated an additional 30 questions, resulting in a high Fleiss kappa score of 0.691 (Fleiss and Cohen, 1973). Finally, these annotators proceeded to label the remaining questions, resulting in a high-quality corpus of 1449 questions with 1201 tag values.\nA.6.3 Conversation Collection Once the constructed FAQ questions were online, we collect 72013 conversation session data for training and testing. Each session is formulated as (ui, qi, Hi), where qi represented the user\u2019s intended question, Hi contains the conversation history between the user and system, and ui denoted the user along with their profile. Specifically, to determine the target question qi for each user, we gathered the FAQ questions they had clicked on from the online data, which we formulated as Qclick = {q1, q2, ...qn}. Among these clicked questions Qclick, we obtain qi based on the following rules: (a) The user terminated the conversation after clicking on qi. (b) The user refrained from conducting further searches for a certain period of time following the click. (c) The user did not seek assistance from human customer service after the search session. Once we obtained qi, we conducted a manual review to ensure its quality."
        }
    ],
    "title": "TRAVEL: Tag-Aware Conversational FAQ Retrieval via Reinforcement Learning",
    "year": 2023
}