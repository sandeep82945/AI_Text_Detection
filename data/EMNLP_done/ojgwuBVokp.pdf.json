{
    "abstractText": "Representation Learning on Knowledge Graphs (KGs) is essential for downstream tasks. The dominant approach, KG Embedding (KGE), represents entities with independent vectors and faces the scalability challenge. Recent studies propose an alternative way for parameter efficiency, which represents entities by composing entity-corresponding codewords matched from predefined small-scale codebooks. We refer to the process of obtaining corresponding codewords of each entity as entity quantization, for which previous works have designed complicated strategies. Surprisingly, this paper shows that simple random entity quantization can achieve similar results to current strategies. We analyze this phenomenon and reveal that entity codes, the quantization outcomes for expressing entities, have higher entropy at the code level and Jaccard distance at the codeword level under random entity quantization. Therefore, different entities become more easily distinguished, facilitating effective KG representation. The above results show that current quantization strategies are not critical for KG representation, and there is still room for improvement in entity distinguishability beyond current strategies. The code to reproduce our results is available here.",
    "authors": [
        {
            "affiliations": [],
            "name": "Jiaang Li"
        },
        {
            "affiliations": [],
            "name": "Quan Wang"
        },
        {
            "affiliations": [],
            "name": "Yi Liu"
        },
        {
            "affiliations": [],
            "name": "Licheng Zhang"
        },
        {
            "affiliations": [],
            "name": "Zhendong Mao"
        }
    ],
    "id": "SP:e33725e82c03e460276a7f4ffa047b07f2821cba",
    "references": [
        {
            "authors": [
                "Ralph Abboud",
                "\u0130smail \u0130lkan Ceylan",
                "Martin Grohe",
                "Thomas Lukasiewicz."
            ],
            "title": "The surprising power of graph neural networks with random node initialization",
            "venue": "Proceedings of the Thirtieth International Joint Conference on Artifical Intelligence (IJCAI).",
            "year": 2021
        },
        {
            "authors": [
                "Ivana Bala\u017eevi\u0107",
                "Carl Allen",
                "Timothy M Hospedales."
            ],
            "title": "Tucker: Tensor factorization for knowledge graph completion",
            "venue": "arXiv preprint arXiv:1901.09590.",
            "year": 2019
        },
        {
            "authors": [
                "Kurt Bollacker",
                "Colin Evans",
                "Praveen Paritosh",
                "Tim Sturge",
                "Jamie Taylor."
            ],
            "title": "Freebase: A collaboratively created graph database for structuring human knowledge",
            "venue": "Proceedings of the 2008 ACM SIGMOD International Conference on Management",
            "year": 2008
        },
        {
            "authors": [
                "Antoine Bordes",
                "Nicolas Usunier",
                "Alberto GarciaDuran",
                "Jason Weston",
                "Oksana Yakhnenko."
            ],
            "title": "Translating embeddings for modeling multirelational data",
            "venue": "Advances in Neural Information Processing Systems, volume 26. Curran Associates,",
            "year": 2013
        },
        {
            "authors": [
                "Mingyang Chen",
                "Wen Zhang",
                "Zhen Yao",
                "Yushan Zhu",
                "Yang Gao",
                "Jeff Z. Pan",
                "Huajun Chen."
            ],
            "title": "Entity-agnostic representation learning for parameterefficient knowledge graph embedding",
            "venue": "AAAI.",
            "year": 2023
        },
        {
            "authors": [
                "Vic Degraeve",
                "Gilles Vandewiele",
                "Femke Ongenae",
                "Sofie Van Hoecke."
            ],
            "title": "R-gcn: the r could stand for random",
            "venue": "arXiv preprint arXiv:2203.02424.",
            "year": 2022
        },
        {
            "authors": [
                "Tim Dettmers",
                "Pasquale Minervini",
                "Pontus Stenetorp",
                "Sebastian Riedel."
            ],
            "title": "Convolutional 2d knowledge graph embeddings",
            "venue": "Proceedings of the AAAI conference on artificial intelligence, volume 32.",
            "year": 2018
        },
        {
            "authors": [
                "Mikhail Galkin",
                "Etienne Denis",
                "Jiapeng Wu",
                "William L. Hamilton."
            ],
            "title": "Nodepiece: Compositional and parameter-efficient representations of large knowledge graphs",
            "venue": "International Conference on Learning Representations.",
            "year": 2022
        },
        {
            "authors": [
                "Ziniu Hu",
                "Yichong Xu",
                "Wenhao Yu",
                "Shuohang Wang",
                "Ziyi Yang",
                "Chenguang Zhu",
                "Kai-Wei Chang",
                "Yizhou Sun."
            ],
            "title": "Empowering language models with knowledge graph reasoning for open-domain question answering",
            "venue": "Proceedings of the 2022 Con-",
            "year": 2022
        },
        {
            "authors": [
                "Shaoxiong Ji",
                "Shirui Pan",
                "Erik Cambria",
                "Pekka Marttinen",
                "Philip S. Yu."
            ],
            "title": "A survey on knowledge graphs: Representation, acquisition, and applications",
            "venue": "IEEE Transactions on Neural Networks and Learning Systems, 33(2):494\u2013514.",
            "year": 2022
        },
        {
            "authors": [
                "Jiaang Li",
                "Quan Wang",
                "Zhendong Mao."
            ],
            "title": "Inductive relation prediction from relational paths and context with hierarchical transformers",
            "venue": "ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP).",
            "year": 2023
        },
        {
            "authors": [
                "Farzaneh Mahdisoltani",
                "Joanna Biega",
                "Fabian Suchanek."
            ],
            "title": "Yago3: A knowledge base from multilingual wikipedias",
            "venue": "7th biennial conference on innovative data systems research. CIDR Conference.",
            "year": 2014
        },
        {
            "authors": [
                "George A. Miller."
            ],
            "title": "Wordnet: A lexical database for english",
            "venue": "Commun. ACM, page 39\u201341.",
            "year": 1995
        },
        {
            "authors": [
                "Lawrence Page",
                "Sergey Brin",
                "Rajeev Motwani",
                "Terry Winograd."
            ],
            "title": "The pagerank citation ranking: Bringing order to the web",
            "venue": "Technical report, Stanford infolab.",
            "year": 1999
        },
        {
            "authors": [
                "Hao Peng",
                "Haoran Li",
                "Yangqiu Song",
                "Vincent Zheng",
                "Jianxin Li."
            ],
            "title": "Differentially private federated knowledge graphs embedding",
            "venue": "Proceedings of the 30th ACM International Conference on Information & Knowledge Management, pages 1416\u20131425.",
            "year": 2021
        },
        {
            "authors": [
                "Mrinmaya Sachan."
            ],
            "title": "Knowledge graph embedding compression",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics.",
            "year": 2020
        },
        {
            "authors": [
                "Tara Safavi",
                "Danai Koutra."
            ],
            "title": "CoDEx: A Comprehensive Knowledge Graph Completion Benchmark",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 8328\u20138350, Online. Association for",
            "year": 2020
        },
        {
            "authors": [
                "Ryoma Sato",
                "Makoto Yamada",
                "Hisashi Kashima."
            ],
            "title": "Random features strengthen graph neural networks",
            "venue": "Proceedings of the 2021 SIAM International Conference on Data Mining (SDM), pages 333\u2013341. SIAM.",
            "year": 2021
        },
        {
            "authors": [
                "Michael Schlichtkrull",
                "Thomas N Kipf",
                "Peter Bloem",
                "Rianne Van Den Berg",
                "Ivan Titov",
                "Max Welling."
            ],
            "title": "Modeling relational data with graph convolutional networks",
            "venue": "The Semantic Web: 15th International Conference, ESWC 2018, Heraklion, Crete,",
            "year": 2018
        },
        {
            "authors": [
                "Kai Sun",
                "Dian Yu",
                "Jianshu Chen",
                "Dong Yu",
                "Claire Cardie."
            ],
            "title": "Improving machine reading comprehension with contextualized commonsense knowledge",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Vol-",
            "year": 2022
        },
        {
            "authors": [
                "Zhiqing Sun",
                "Zhi-Hong Deng",
                "Jian-Yun Nie",
                "Jian Tang."
            ],
            "title": "Rotate: Knowledge graph embedding by relational rotation in complex space",
            "venue": "International Conference on Learning Representations.",
            "year": 2019
        },
        {
            "authors": [
                "Komal Teru",
                "Etienne Denis",
                "Will Hamilton."
            ],
            "title": "Inductive relation prediction by subgraph reasoning",
            "venue": "International Conference on Machine Learning, pages 9448\u20139457. PMLR.",
            "year": 2020
        },
        {
            "authors": [
                "Kristina Toutanova",
                "Danqi Chen",
                "Patrick Pantel",
                "Hoifung Poon",
                "Pallavi Choudhury",
                "Michael Gamon."
            ],
            "title": "Representing text for joint embedding of text and knowledge bases",
            "venue": "Proceedings of the 2015 conference on empirical methods in natural language",
            "year": 2015
        },
        {
            "authors": [
                "Th\u00e9o Trouillon",
                "Johannes Welbl",
                "Sebastian Riedel",
                "\u00c9ric Gaussier",
                "Guillaume Bouchard."
            ],
            "title": "Complex embeddings for simple link prediction",
            "venue": "International conference on machine learning, pages 2071\u2013 2080. PMLR.",
            "year": 2016
        },
        {
            "authors": [
                "Aaron van den Oord",
                "Oriol Vinyals",
                "Koray Kavukcuoglu."
            ],
            "title": "Neural discrete representation learning",
            "venue": "Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS\u201917, page 6309\u20136318.",
            "year": 2017
        },
        {
            "authors": [
                "Shikhar Vashishth",
                "Soumya Sanyal",
                "Vikram Nitin",
                "Partha Talukdar."
            ],
            "title": "Composition-based multirelational graph convolutional networks",
            "venue": "International Conference on Learning Representations.",
            "year": 2020
        },
        {
            "authors": [
                "Haoyu Wang",
                "Yaqing Wang",
                "Defu Lian",
                "Jing Gao."
            ],
            "title": "A lightweight knowledge graph embedding framework for efficient inference and storage",
            "venue": "Proceedings of the 30th ACM International Conference on Information & Knowledge Management, pages",
            "year": 2021
        },
        {
            "authors": [
                "Kai Wang",
                "Yu Liu",
                "Qian Ma",
                "Quan Z Sheng."
            ],
            "title": "Mulde: Multi-teacher knowledge distillation for lowdimensional knowledge graph embeddings",
            "venue": "Proceedings of the Web Conference 2021, pages 1716\u2013 1726.",
            "year": 2021
        },
        {
            "authors": [
                "Quan Wang",
                "Pingping Huang",
                "Haifeng Wang",
                "Songtai Dai",
                "Wenbin Jiang",
                "Jing Liu",
                "Yajuan Lyu",
                "Yong Zhu",
                "Hua Wu."
            ],
            "title": "Coke: Contextualized knowledge graph embedding",
            "venue": "arXiv preprint arXiv:1911.02168.",
            "year": 2019
        },
        {
            "authors": [
                "Quan Wang",
                "Zhendong Mao",
                "Bin Wang",
                "Li Guo."
            ],
            "title": "Knowledge graph embedding: A survey of approaches and applications",
            "venue": "IEEE Transactions on Knowledge and Data Engineering, 29(12):2724\u2013 2743.",
            "year": 2017
        },
        {
            "authors": [
                "Xiaozhi Wang",
                "Tianyu Gao",
                "Zhaocheng Zhu",
                "Zhengyan Zhang",
                "Zhiyuan Liu",
                "Juanzi Li",
                "Jian Tang."
            ],
            "title": "Kepler: A unified model for knowledge embedding and pre-trained language representation",
            "venue": "Transactions of the Association for Computational Linguis-",
            "year": 2021
        },
        {
            "authors": [
                "Liang Yao",
                "Chengsheng Mao",
                "Yuan Luo."
            ],
            "title": "Kgbert: Bert for knowledge graph completion",
            "venue": "arXiv preprint arXiv:1909.03193.",
            "year": 2019
        },
        {
            "authors": [
                "Zhanqiu Zhang",
                "Jianyu Cai",
                "Yongdong Zhang",
                "Jie Wang."
            ],
            "title": "Learning hierarchy-aware knowledge graph embeddings for link prediction",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, 34(03):3065\u20133072.",
            "year": 2020
        },
        {
            "authors": [
                "Zhanqiu Zhang",
                "Jie Wang",
                "Jieping Ye",
                "Feng Wu."
            ],
            "title": "Rethinking graph convolutional networks in knowledge graph completion",
            "venue": "The Web Conference 2022.",
            "year": 2022
        },
        {
            "authors": [
                "Yushan Zhu",
                "Wen Zhang",
                "Mingyang Chen",
                "Hui Chen",
                "Xu Cheng",
                "Wei Zhang",
                "Huajun Chen."
            ],
            "title": "Dualde: Dually distilling knowledge graph embedding for faster and cheaper reasoning",
            "venue": "Proceedings of the Fifteenth ACM International Conference on Web",
            "year": 2022
        },
        {
            "authors": [
                "Zhaocheng Zhu",
                "Zuobai Zhang",
                "Louis-Pascal Xhonneux",
                "Jian Tang."
            ],
            "title": "Neural bellman-ford networks: A general graph neural network framework for link prediction",
            "venue": "Advances in Neural Information Processing Systems, volume 34, pages",
            "year": 2021
        },
        {
            "authors": [
                "Teru"
            ],
            "title": "2020), including four versions of subsets generated from FB15k-237. We test the performance of NodePiece with the fully random entity quantization",
            "year": 2020
        },
        {
            "authors": [
                "Zhu et al",
                "2022 Galkin et al",
                "Li"
            ],
            "title": "The results are shown in Table 7. We can see that NodePiece\u2019s variants with random entity quantization perform as well as the original model in inductive link prediction",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Knowledge Graphs (KGs) comprise (head entity, relation, tail entity) triplets. They are crucial external knowledge sources for various natural language processing tasks (Hu et al., 2022; Sun et al., 2022). Learning representations on KGs is necessary for expressing complex semantics and supporting downstream tasks. The most dominant paradigm, KG Embedding (KGE), maps entities and relations to a vector space (Dettmers et al., 2018; Sun et al., 2019; Zhang et al., 2020). Despite the popularity, KGE models need to represent each\n\u2217Corresponding author: Quan Wang\nentity with an independent vector, which leads to a linear increase in the number of parameters with the number of entities. Consequently, scalability becomes a challenge for these models, posing difficulties in their implementation and deployment (Peng et al., 2021; Ji et al., 2022), especially for large-scale KGs (Safavi and Koutra, 2020; Mahdisoltani et al., 2014).\nA recently proposed parameter-efficient KG representation method uses compositional entity representations to reduce parameters (Galkin et al., 2022; Chen et al., 2023). Instead of learning separate representations like KGE, it represents entities by composing their matched codewords from predefined codebooks through an encoder, and requires fewer parameters since codewords are much fewer than entities. See Fig 1 for an illustration of this method. We refer to the process of obtaining corresponding codewords to each entity as entity quantization due to its similarity to vector quantization (van den Oord et al., 2017). Specifically, existing methods construct two codebooks, in which codewords are the entire relation set and a selec-\ntive subset of entities, i.e., anchors, respectively. From these two codebooks, each entity matches two groups of codewords: connected relations and anchors nearby (Galkin et al., 2022) or with similar adjacent relations (Chen et al., 2023). Chen et al. (2023) also regards matching degrees as codeword weights for more expressiveness. Matched results are denoted as entity codes, including matched codewords and optional weights. A subsequent encoder uses entity codes to compose corresponding codewords and generate entity representations. This approach performs closely to KGE with fewer parameters, making KG training and deployment more efficient.\nThe key to entity quantization lies in two steps: (1) codebook construction and (2) codeword matching. Previous studies have dedicated their efforts to designing quantization strategies, which include selecting proper KG elements to construct codebooks and measuring the connectivity between codewords and entities to match them. We conduct experiments to randomize these strategies from shallow to deep. Surprisingly, we find that random entity quantization approaches can achieve similar or even better results.\nWe design several model variants for experiments. First, to investigate the effectiveness of matching codewords with connectivity, we randomize the codeword matching step by randomly selecting codewords as matched results of entities. Moreover, we set codeword weights randomly or equally for (Chen et al., 2023) to verify whether designed weights from matching are critical. Finally, to explore whether mapping codewords to actual elements in the KG is critical, we randomly construct codebooks with codewords that have no actual meaning. We adopt random codeword matching to the randomly constructed codebook, to provide a fully random entity quantization. Counterintuitively, empirical results show that the above operations achieve similar results compared to complicated quantization strategies and may even improve the model performance.\nMoreover, we have verified that random entity quantization can better distinguish entities than current quantization strategies, which leads to more expressive KG representations (Zhang et al., 2022). Under the strategies designed by previous works, different entities could match the same codewords, making their code similar or identical. In contrast, random quantization leads to a lower possibility\nof matching same codewords and distributes entity codes more uniformly across a wide range. We prove this claim by analyzing the properties of entity codes. At the code level, we consider entity code as a whole and treat it as one sample of a random variable. The entropy of this variable can be derived from its distribution across all entity codes. We prove that random entity quantization has higher entropy and maximizes it with high probability, thus producing more diverse and unique entity codes. At the codeword level, each entity code indicates a set of matched codewords. We analyze the Jaccard distance between different sets and find that it is significantly increased by random entity quantization. As a result, different entities will have a more obvious dissimilarity when randomly quantized, making them easier to distinguish.\nIn summary, the contributions of our work are two-fold: (1) We demonstrate through comprehensive experiments that random entity quantization approaches perform similarly or even better than previously designed quantization strategies. (2) We analyze that this surprising performance is because random entity quantization has better entity distinguishability, by showing its produced entity codes have higher entropy and Jaccard distance. These results suggest that current complicated quantization strategies are not critical for the model performance, and there is potential for entity quantization approaches to increase entity distinguishability beyond current strategies."
        },
        {
            "heading": "2 Preliminaries",
            "text": ""
        },
        {
            "heading": "2.1 Knowledge Graph Representation",
            "text": "A knowledge graph G \u2286 E \u00d7R\u00d7 E is composed of entity-relation-entity triplets (h, r, t), where E is a set of entities, and R is a set of relations. Each triplet indicates a relation r \u2208 R between two entities h, t \u2208 E , where h is the head entity, and t is the tail entity. The goal of knowledge graph representation is to learn a vector representation ei for each entity ei \u2208 E , and rj for relation rj \u2208 R."
        },
        {
            "heading": "2.2 Compositional KG Representation",
            "text": "Compositional knowledge graph representation methods compose codewords from small-scale codebooks to represent entities. These methods obtain codewords for each entity by constructing codebooks and matching codewords. We refer to these two steps as entity quantization. The matched codewords are encoded to represent each entity.\nThis section presents two methods of this kind, i.e., NodePiece (Galkin et al., 2022) and EARL (Chen et al., 2023). We first introduce the definition of entity quantization and how these two methods represent entities with it. After that, we introduce how they are trained. Our subsequent experiments are based on these two methods."
        },
        {
            "heading": "2.2.1 Entity Quantization",
            "text": "We first formally define the entity quantization process. Existing entity quantization strategies construct a relation codebook Br = {r1, \u00b7 \u00b7 \u00b7 , rm} and an anchor codebook Ba = {a1, \u00b7 \u00b7 \u00b7 , an}. The codewords (r1, \u00b7 \u00b7 \u00b7 , rm) are all m relations in R and (a1, \u00b7 \u00b7 \u00b7 , an) are n anchors selected from all entities in E with certain strategies. After adding reverse edges to KG, each entity ei \u2208 E matches si = min(di, d)1 unique relations from all its di connected relations in Br, and employs anchor-matching strategies to match k anchors from Ba. Its matched codewords are denoted as a set Wi = {ri1, \u00b7 \u00b7 \u00b7 , risi , a i 1, \u00b7 \u00b7 \u00b7 , aik}. Each entity ei will get its entity code ci to represent Wi, which is a (m+n)-dimensional vector that is zero except for the (si + k) dimensions representing matched codewords. Values in these dimensions are set to 1 or optional codeword weights if provided.\nThen, we provide the detailed quantization process of both models.\nNodePiece NodePiece uses metrics such as Personalized PageRank Page et al. (1999) to pick some entities as codewords in Ba. Each ei \u2208 E matches nearest anchors from Ba as {ai1, \u00b7 \u00b7 \u00b7 , aik}.\nEARL EARL constructs Ba with 10% sampled entities. Each entity ei \u2208 E matches anchors which have the most similar connected relations. Matched codewords are assigned to designed weights. Weights of r \u2208 {ri1, \u00b7 \u00b7 \u00b7 , risi} are based on its connection count with ei, and weights of each a \u2208 {ai1, \u00b7 \u00b7 \u00b7 , aik} are based on the similarity between connected relation sets of ei and a.\nAfter quantization, codewords in Wi are composed by an encoder to output entity representation ei. The encoders of NodePiece and EARL are based on MLP and CompGCN (Vashishth et al., 2020), respectively."
        },
        {
            "heading": "2.2.2 Model Training",
            "text": "Here we introduce how to train both models. For each triplet (h, r, t), representations of h and t\n1d is a hyperparameter in Nodepiece, and is +\u221e in EARL.\nare obtained from above. Each relation rj \u2208 R is represented independently. Both models use RotatE (Sun et al., 2019) to score triplets with f(h, r, t) = \u2212||h \u25e6 r \u2212 t||, which maps entities and relations in complex space, i.e., h, r, t \u2208 Cd.\nNodePiece and EARL use different loss functions for different datasets, including binary crossentropy (BCE) loss and negative sampling selfadversarial loss (NSSAL). For a positive triplet (h, r, t), BCE loss can be written as:\nLBCE(h, r, t) = \u2212 log(\u03c3(f(h, r, t)))\n\u2212 n\u2211\ni=1\nlog(1\u2212 \u03c3(f(h\u2032i, r, t\u2032i))),\nwhere \u03c3 is the sigmoid function and (h\u2032i, r, t \u2032 i) is the i-th negative triplet for (h, r, t). NSSAL further considers that negative samples have varying difficulties:\nLNSSAL(h, r, t) = \u2212 log \u03c3(\u03b3 \u2212 f(h, r, t))\n\u2212 n\u2211\ni=1\np(h\u2032i, r, t \u2032 i) log \u03c3(f(h \u2032 i, r, t \u2032 i)\u2212 \u03b3),\nwhere \u03b3 is a fixed margin. p(h\u2032i, r, t \u2032 i) is the selfadversarial weight of (h\u2032i, r, t \u2032 i) and takes the following form:\np(h\u2032j , r, t \u2032 j) =\nexp\u03b1(f(h\u2032j , r, t \u2032 j))\u2211\ni exp\u03b1f(h \u2032 i, r, t \u2032 i) ,\nwhere \u03b1 is the temperature of sampling."
        },
        {
            "heading": "3 Experimental Setup",
            "text": "Though previous methods have designed complicated strategies for entity quantization, whether these strategies are critical for the model performance remains to be explored. Our experiments are to empirically test the effect of these quantization strategies. We therefore design a series of model variants using random entity quantization. We focus on showing the effectiveness of random entity quantization, rather than proposing new stateof-the-art models. Below we introduce the model variants we design, the datasets we test, and the training/evaluation protocols."
        },
        {
            "heading": "3.1 Model Variants",
            "text": "We design model variants based on existing models, NodePiece and EARL. We replace part of their designed quantization strategies with random approaches for each of these two models. The random\nmethods we use are generally divided into three types: (1) Randomly match entities to relations or anchors in codebooks. (2) Randomly or equally set codeword weights. (3) Randomly construct codebooks, where codewords do not refer to natural elements in the KG. We will discuss the details of these model variants in Section 4."
        },
        {
            "heading": "3.2 Datasets",
            "text": "We use three knowledge graph completion datasets in total. We employ FB15k-237 (Toutanova et al., 2015) and WN18RR (Dettmers et al., 2018) to demonstrate the effectiveness of random entity quantization and extend our conclusions to a larger scale KG, CoDEx-L(Safavi and Koutra, 2020). FB15k-237 is based on Freebase (Bollacker et al., 2008), a knowledge base containing vast factual information. WN18RR has derived from Wordnet (Miller, 1995), specifically designed to capture semantic relations between words. CoDEx-L is the largest version of recently proposed CoDEx datasets, which improve upon existing knowledge graph completion benchmarks in content scope and difficulty level. For consistency with compared methods, we exclude test triples that involve entities not present in the corresponding training set. Table 1 presents the statistics of these datasets. We also experiment with inductive relation prediction datasets(Teru et al., 2020) (details in Appendix C)."
        },
        {
            "heading": "3.3 Training and Evaluation Protocols",
            "text": "Training. Our experiments are based on the official implementation of NodePiece and EARL. We use the same loss functions and follow their hyperparameter setting for corresponding model variants. More details are provided in Appendix A.\nEvaluation. We generate candidate triplets by substituting either h or t with candidate entities for each triplet (h, r, t) in the test sets. The triples\nare then sorted in descending order based on their scores. We apply the filtered setting (Bordes et al., 2013) to exclude other valid candidate triplets from ranking. To assess the performance of the models, we report the mean reciprocal rank (MRR) and Hits@10. Higher MRR/H@10 indicates better performance. Additionally, we evaluate the efficiency of the models using Effi = MRR/#P , where #P represents the number of parameters. The results of NodePiece and EARL are from their original papers."
        },
        {
            "heading": "4 Random Entity Quantization",
            "text": "This section details the random variants we design and their experimental results. We design model variants by randomizing different steps of existing entity quantization strategies, including codeword matching and codebook construction. We find that these variants achieve similar performance to existing quantization strategies. These results suggest that the current entity quantization strategies are not critical for model performance."
        },
        {
            "heading": "4.1 Random Codeword Matching",
            "text": "We first randomize the codeword matching step, which includes strategies for (1) matching each entity to the corresponding codewords and (2) assigning weights to the matched codewords."
        },
        {
            "heading": "4.1.1 Matching Strategy",
            "text": "We randomize the codeword matching strategies to investigate whether current connectivitybased strategies are critical to the model performance. We design model variants by randomizing current methods\u2019 relation-matching or anchormatching strategies and keep other settings unchanged. Specifically, with the relation codebook Br and the anchor codebook Ba, we have the following model variants for both models.\n\u2022 +RSR: Each entity ei \u2208 E Randomly Selects si Relations (RSR) from Br and matches k anchors from Ba with the anchor-matching strategy designed by the current model, as matched codewords Wi.\n\u2022 +RSA: ei Randomly Selects k Anchors (RSA) from Ba, and matches si relations from Br with current relation-matching strategy, as Wi.\n\u2022 +RSR+RSA: ei randomly selects si relations from Br, and randomly selects k anchors from Ba, as Wi.\nFor all variants, we still assign codewords in Wi = {ri1, \u00b7 \u00b7 \u00b7 , risi , a i 1, \u00b7 \u00b7 \u00b7 , aik} with current connectivity-based weights, if required. Table 2 shows the performance of original models and their respective variants. Surprisingly, randomizing codeword-matching and relationmatching does not affect the overall performance of existing methods on both datasets, whether used together or separately. The results suggest that current complicated codeword matching strategies are not critical to the model performance.\nWe further study the impact of randomizing the codeword matching strategy with only one codebook. We remove Br or Ba respectively, and adopt different codeword matching approaches for the remaining codebook, forming following variants:\n\u2022 w/o anc: Remove the anchor codebook Ba. Match si relations from Br with the current relation-matching strategy as Wi = {ri1, \u00b7 \u00b7 \u00b7 , risi}.\n\u2022 w/o anc+RSR: Remove Ba. Randomly select si relations from Br as Wi = {ri1, \u00b7 \u00b7 \u00b7 , risi}.\n\u2022 w/o rel: Remove the relation codebook Br. Match k anchors from Ba with the current anchor-matching strategy as Wi = {ai1, \u00b7 \u00b7 \u00b7 , aik}.\n\u2022 w/o rel+RSA: Remove Br. Randomly select k anchors from Ba as Wi = {ai1, \u00b7 \u00b7 \u00b7 , aik}.\nTable 3 shows that when removing one codebook, random matching codewords from the remaining codebook performs better than using current designed matching strategies. It even performs similarly to the original methods in most\ncases. NodePiece performs poorly on WN18RR without Ba, because the number of relations in this dataset is small, and only using Br sharply decreases model parameters. The above results further validate the effectiveness of random codeword matching and demonstrate its robustness with only one codebook."
        },
        {
            "heading": "4.1.2 Codeword Weights from Matching",
            "text": "During the matching process, EARL will further assign weights to the matched codewords based on the connectivity between entities and codewords. We conduct experiments to explore whether such weights are critical. Specifically, we design following model variants for EARL using random or equal codeword weights, with the codebooks and codeword matching strategies unchanged:\n\u2022 +RW: Assign Random Weights (RW) to matched codewords.\n\u2022 +EW: Assign Equal Weights (EW) to matched codewords, i.e., set all codeword weights to 1.\nTable 4 shows that either using random weights or equal weights does not significantly impact the performance. Thus, we can conclude that the connectivity-based weights are not critical for model performance. For subsequent experiments, we use equal codeword weights for simplicity."
        },
        {
            "heading": "4.2 Random Codebook Construction",
            "text": "We construct codebooks randomly to investigate whether it is critical to construct codebooks with specific relations or entities as codewords, like current strategies. We design a model variant for each model, which uses a randomly constructed codebook B instead of Br and Ba. Codewords in B do not have any real meanings. Moreover, we adopt random codeword matching and equal codeword weights to such variants. We call this type of variant the fully random entity quantization (RQ) since it randomizes all parts of the entity quantization.\n\u2022 +RQ: Fully Random entity Quantization (RQ). Randomly construct a codebook whose size equals the sum of Br and Ba: B = {z1, . . . , zm+n}. Each entity ei \u2208 E randomly selects (s + k) codewords as its matched codewords Wi = {zi1, . . . , zis+k} with equal weights, where s = 1|E| \u2211|E| i=1 si\nand |E| is the number of entities.\nTable 5 shows variants of both models lead to similar or even better performance across all datasets with varying sizes. We will analyze this surprising observation in Section 5. Through #P (M) and Effi, we further show that random entity quantization requires equal or even fewer parameters with higher efficiency. It does not require composing each codebook\u2019s codewords separately, saving parameters for EARL. The above results show that constructing codebooks by KG elements is not critical to the model performance. From all\nvariants and results above, we can conclude that random entity quantization is as effective as current complicated quantization strategies."
        },
        {
            "heading": "5 Why Random Quantization Works",
            "text": "This section analyzes the reasons for the surprising performance of random entity quantization. The entity codes directly affect entity representation and model performance. We analyze the entity codes produced by different entity quantization approaches to compare their ability to distinguish different entities. We find random entity quantization obtains entity codes with greater entropy at the code level and Jaccard distance at the codeword level. Thus, it can better distinguish different entities and represent KGs effectively."
        },
        {
            "heading": "5.1 Code Level Distinguishability",
            "text": "We analyze the ability to distinguish entities at the code level of different entity quantization approaches. Specifically, we treat the entity code ci of each entity ei \u2208 E as a sampling of a random variable X . The entity codes of all entities represent |E| samplings of X . From these samplings, we get v different entity codes and their frequencies. We denote these codes as {x1. . . . , xv} and their frequencies as {f1, . . . , fv}. We denote l = m+n as the total number of codewords, where m and n are numbers of codewords in Br and Ba. The number of all possible entity codes is 2l. We estimate the probability distribution of X on all codes based on the relative frequency of different entity codes in the sampling results, and then derive its entropy:\nH(X) = \u2212 \u2211\ni=1,...,2l\nP (xi) \u00b7 log2 P (xi), (1)\nwhere P (xi) is the relative frequency of xi:\nP (xi) =\n{ fi |E| if i = 1, . . . , v,\n0 if i = v + 1, . . . , 2l.\nWe use the entropy H(X) in eq. 1 to measure the diversity of entity codes. A higher entropy means more diverse entity codes, thus indicating the better entity-distinguish ability of the quantization approach. In this sense, this entropy can reflect entity distinguishability at the code level.\nWe use equation 1 to derive the entity code entropy produced by random and previously designed quantization strategies. The results are listed in Table 6. The entropy of random quantization is averaged in 100 runs with different seeds, while the entropy of previous strategies is deterministic and does not change in different runs. Empirically, we demonstrate that random quantization achieves higher entropy, producing more diverse entity codes and enabling easier distinguishability between different entities.\nWe confirm that higher entity code entropy brings better model performance through extensive experiments. Specifically, after random entity quantization, we randomly select a subset of entity codes and set them to be identical, to obtain entity codes with different entropy values. Figure 2 shows the comparison experiments across these entity codes with EARL and NodePiece. As the entropy rises, the model performance gradually increases and eventually saturates. EARL performs better than NodePiece across varying entropy, as its GNN encoder involves the entity neighborhood and can better distinguish entities. From above, more diverse entity codes with higher entropy benefit the model performance, which aligns with our claims.\nIn addition, the entity code entropy is maximized when all entity codes are unique. The probability of random entity quantization to produce |E| unique entity codes is close to 1, as shown in Theorem 1. The detailed proof is in Appendix B. This theorem shows that random entity quantization expresses entities uniquely with high probability, thus distinguishing entities well. Theorem 1. The probability of random entity quantization to produce |E| unique entity codes is P = \u220f|E|\u22121 i=0 2l\u2212i 2l\n, which approaches 1 when 2l \u226b |E|.\nFrom above, we demonstrate random entity quantization produces diverse entity codes and clearly distinguishes entities at the code level."
        },
        {
            "heading": "5.2 Codeword Level Distinguishability",
            "text": "We further analyze the ability to distinguish entities at the codeword level of different entity quantization approaches. Specifically, for entities ei, ej \u2208 E with entity codes ci and ci, their corresponding sets of matched codewords are Wi = {ri1, \u00b7 \u00b7 \u00b7 , risi , a i 1, \u00b7 \u00b7 \u00b7 , aik} and Wj = {rj1, \u00b7 \u00b7 \u00b7 , r j sj , a j 1, \u00b7 \u00b7 \u00b7 , a j k}. The Jaccard distance between ci and ci is:\ndJ(ci, cj) = |Wi \u222aWj | \u2212 |Wi \u2229Wj |\n|Wi \u222aWj | ,\nwhere | \u00b7 | denotes the number of elements in a set. We use the Jaccard distance dJ(ci, cj) to measure the distinctiveness between entity codes ci and cj . A larger distance means their indicated codewords are more distinct and makes entities ei and ej more easily to be distinguished. In this sense, this distance can reflect the entity distinguishability at the codeword level.\nTo capture the overall distinguishability among all entity codes, we propose a k-nearest neighbor evaluation metric based on the Jaccard distance. This evaluation assesses the average distance between each entity code and its k nearest codes, denoted as Jk. A higher Jk means the entity codes are more distinct. We use different values of k to observe the distance among entity codes in different neighborhood ranges. The metric Jk is derived as:\nJk = 1 |E| \u00d7 k \u2211 ei\u2208E \u2211 ej\u2208kNN(ei) dJ(ci, cj),\nwhere |E| is the number of entities. kNN(ei) is a set of k entities whose codes are nearest to the\ncode of ei under Jaccard distance:\nkNN(ei) = argmin {el1 ,...,elk}\u2282E el1 ,...,elk \u0338=ei\n\u2211 j\u2208{l1,...,lk} dJ(ci, cj).\nFigure 3 shows the average Jaccard distance Jk between entity codes w.r.t. different numbers k of nearest codes. We can see that random entity quantization achieves higher Jk than current quantization strategies across the varying k. Thus, entity codes produced by random entity quantization are more distinct within different neighborhood ranges. Based on the above observation, random entity quantization makes different entities easier to distinguish at the codeword level."
        },
        {
            "heading": "5.3 Discussion",
            "text": "We can derive from the above that, entity distinguishability is the reason why current quantization strategies based on attribute similarity don\u2019t work better than the random approach. From Table 6 and Figure 3, it\u2019s proved that random entity quantization distinguishes entities better in both code and codeword levels. Furthermore, in Figure 2, we show that entity quantization strategies with higher entity distinguishability tend to perform better. Therefore, it\u2019s reasonable that random quantization works comparable to current strategies or even better."
        },
        {
            "heading": "6 Related Work",
            "text": "Knowledge Graph Embedding KG embedding aims at learning independent representations for\nentities and relations of a given KG, which benefits downstream tasks such as question answering (Hu et al., 2022), reading comprehension (Sun et al., 2022), and pre-trained language representation Wang et al., 2021c. Recent years have witnessed a growing number of KG embedding techniques being devised, including distance-based models (Bordes et al., 2013; Sun et al., 2019; Zhang et al., 2020), semantic matching models (Trouillon et al., 2016; Bala\u017eevic\u0301 et al., 2019), neural encoding models (Dettmers et al., 2018; Schlichtkrull et al., 2018; Wang et al., 2019), and text augmented models (Yao et al., 2019; Wang et al.). We refer readers to (Wang et al., 2017; Ji et al., 2022) for a comprehensive overview of the literature.\nParameter-Efficient KG Representation KG embedding methods face the scalability challenge. The number of parameters scales up linearly to the entity number. Several studies compress learned parameters from KG embedding models, trying to solve this issue. Incorporating knowledge distillation techniques, MulDE (Wang et al., 2021b) transfers knowledge from multiple teacher models to a student model. Expanding on this, DualDE (Zhu et al., 2022) considers the dual influence between the teacher and student and adapts the teacher to better align with the student, thus enhancing the performance of the distilled model. To directly compress existing models, Sachan (2020) discretizes the learned representation vectors for less parameter storage space, then maps the discrete vectors back to the continuous space for testing. LightKG (Wang et al., 2021a) introduces dynamic negative sampling and discretizes learned representations through vector quantization.\nHowever, the above methods firstly need to train KG embedding models with full parameter size. Recently proposed compositional parameterefficient KG representation models (Galkin et al., 2022; Chen et al., 2023), which are illustrated in this paper, enable a more efficient training process.\nRandom Features in Graph Representation In homogeneous graph learning, researchers prove that message-passing neural networks are more powerful when having randomly initialized node features (Sato et al., 2021; Abboud et al., 2021). In KGs, Zhang et al. (2022) finds that random perturbation of relations does not hurt the performance of graph convolutional networks (GCN) in KG completion. Degraeve et al. (2022) further implies\nthat an R-GCN with random parameters may be comparable to an original one. To the best of our knowledge, there is no existing study on representing entities with randomness in parameter-efficient compositional KG representation."
        },
        {
            "heading": "7 Conclusion",
            "text": "In conclusion, this paper demonstrates the effectiveness of random entity quantization in parameterefficient compositional knowledge graph representation. We explain this surprising result by illustrating that random quantization could distinguish different entities better than current entity quantization strategies. Thus, we suggest that existing complicated entity quantization strategies are not critical for model performance, and there is still room for entity quantization approaches to improve entity distinguishability beyond these strategies.\nLimitations\nThis paper only studies entity quantization with encoders proposed in early works, while designing more expressive encoders is also important and can improve the performance of parameter-efficient compositional knowledge graph representation. We leave this part as future work."
        },
        {
            "heading": "Acknowledgments",
            "text": "We would like to thank Ruichen Zheng for the discussion on measuring distinguishability, Zhengbo Wang, Haotian Zhang, Chengchao Xu for the discussion on entropy computation, and Jiahao Li for help to improve the technical writing of this paper.\nThis work is supported by the National Key Research and Development Program of China under Grant 2021YFF0901600, the National Science Fund for Excellent Young Scholars under Grant 62222212, and the National Natural Science Foundation of China under Grant 62376033."
        },
        {
            "heading": "A Training Details",
            "text": "We use the exact same hyperparameter settings as in the original papers of NodePiece and EARL. The results of model variants with random approaches are averaged in three runs with different seeds. We do not further tune hyperparameters for our proposed model variants. Thus, their performance may be underestimated. But so long as the model variants perform similarly to the original models, we can still make our conclusions. For NodePiece, we obtain matched anchors with its \u2019shortest-path\u2019 mode to be consistent with the original paper. For EARL, we use its released anchor set. We run our experiments with a single RTX 3090 GPU with 24GB RAM.\nWe use uniformly distributed random numbers in our model variants. It means that in model variants that randomly match entities to codewords, each codeword has an equal probability of being matched."
        },
        {
            "heading": "B Proof of Theorem 1",
            "text": "Proof. Since random entity quantization matches entities to codewords with equal probabilities, it produces all entity codes uniformly. Thus, the probability of random entity quantization to get |E| different entity codes is:\nProb = P (2l, |E|) (2l)|E| =\n2l! (2l\u2212|E|)!\n(2l)|E| = |E|\u22121\u220f i=0 2l \u2212 i 2l\nwhere P (n, r) is the number of permutations of selecting r elements from a set of n elements. l is the total number of codewords, and the total number of entity codes that can be produced is 2l \u226b |E|.\nC Inductive Relation Prediction Results\nBesides FB15K-237 and WN18RR datasets used in the main text, we further test the effectiveness of random entity quantization on inductive relation prediction, where NodePiece has shown superiority. This task requires learning from one KG, and generalizes to another KG with no shared entities for inference. We follow previous works (Zhu et al., 2021; Galkin et al., 2022; Li et al., 2023) and use the datasets proposed by Teru et al. (2020), including four versions of subsets generated from FB15k-237. We test the performance of NodePiece with the fully random entity quantization (RQ) as\ndescribed in Section 4.2, on these subsets. We set the learning rate to 1e-3 and train the variant with 300 epochs on v1/v4, and 120 epochs on v2/v3. The other hyperparameters remain the same as the original method. We use the exact same evaluation protocol as in previous works (Teru et al., 2020, Zhu et al., 2021; Galkin et al., 2022; Li et al., 2023). The results are shown in Table 7.\nWe can see that NodePiece\u2019s variants with random entity quantization perform as well as the original model in inductive link prediction. The results support and strengthen our claim that random entity quantization is effective, both in the transductive and inductive settings."
        }
    ],
    "title": "Random Entity Quantization for Parameter-Efficient Compositional Knowledge Graph Representation",
    "year": 2023
}