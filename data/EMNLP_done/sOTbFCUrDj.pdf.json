{
    "abstractText": "Math word problems (MWP) involving advanced operators such as linear equation solver cannot be easily tackled by earlier MWP methods, because the existing generation methods suffer from repeated sub-expression generation and deductive methods are restricted to dealing with binary operations. This paper propose a new multivariate directed acyclic graph (mDAG) as an alternative to the generation methods\u2019 binary expression tree or the deductive methods\u2019 binary directed acyclic graph. Then to produce the topological ordering of mDAG, we propose a generation-based deductive (GeDe) model, which equips a generation model with a re-encoder to keep the deductive property but avoid the expensive enumeration of the deductive methods. GeDe performs well on math problems with many operators on the widely used benchmarks as well as solving multivariate operators on our own CMWPA benchmark. Our code is available at https://github.com/hyx1999/GeDe",
    "authors": [
        {
            "affiliations": [],
            "name": "Yuxuan Hu"
        },
        {
            "affiliations": [],
            "name": "Jing Zhang"
        },
        {
            "affiliations": [],
            "name": "Haoyang Li"
        },
        {
            "affiliations": [],
            "name": "Cuiping Li"
        },
        {
            "affiliations": [],
            "name": "Hong Chen"
        }
    ],
    "id": "SP:dcccdd8bf0f319aeec84251518ea049914da1526",
    "references": [
        {
            "authors": [
                "Aida Amini",
                "Saadia Gabriel",
                "Shanchuan Lin",
                "Rik Koncel-Kedziorski",
                "Yejin Choi",
                "Hannaneh Hajishirzi."
            ],
            "title": "MathQA: Towards interpretable math word problem solving with operation-based formalisms",
            "venue": "Proceedings of the 2019 Conference",
            "year": 2019
        },
        {
            "authors": [
                "Daniel G. Bobrow."
            ],
            "title": "Natural language input for a computer problem solving system",
            "venue": "Technical report, USA.",
            "year": 1964
        },
        {
            "authors": [
                "Yixuan Cao",
                "Feng Hong",
                "Hongwei Li",
                "Ping Luo."
            ],
            "title": "A bottom-up dag structure extraction model for math word problems",
            "venue": "AAAI Conference on Artificial Intelligence.",
            "year": 2021
        },
        {
            "authors": [
                "Aakanksha Chowdhery",
                "Sharan Narang",
                "Jacob Devlin",
                "Maarten Bosma",
                "Gaurav Mishra",
                "Adam Roberts",
                "Paul Barham",
                "Hyung Won Chung",
                "Charles Sutton",
                "Sebastian Gehrmann"
            ],
            "title": "Palm: Scaling language modeling with pathways",
            "year": 2022
        },
        {
            "authors": [
                "Junyoung Chung",
                "Caglar Gulcehre",
                "KyungHyun Cho",
                "Yoshua Bengio."
            ],
            "title": "Empirical evaluation of gated recurrent neural networks on sequence modeling",
            "venue": "arXiv preprint arXiv:1412.3555.",
            "year": 2014
        },
        {
            "authors": [
                "Karl Cobbe",
                "Vineet Kosaraju",
                "Mohammad Bavarian",
                "Mark Chen",
                "Heewoo Jun",
                "Lukasz Kaiser",
                "Matthias Plappert",
                "Jerry Tworek",
                "Jacob Hilton",
                "Reiichiro Nakano",
                "Christopher Hesse",
                "John Schulman"
            ],
            "title": "Training verifiers to solve math word",
            "year": 2021
        },
        {
            "authors": [
                "Yiming Cui",
                "Wanxiang Che",
                "Ting Liu",
                "Bing Qin",
                "Shijin Wang",
                "Guoping Hu."
            ],
            "title": "Revisiting pre-trained models for Chinese natural language processing",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings,",
            "year": 2020
        },
        {
            "authors": [
                "Danqing Huang",
                "Jing Liu",
                "Chin-Yew Lin",
                "Jian Yin."
            ],
            "title": "Neural math word problem solver with reinforcement learning",
            "venue": "Proceedings of the 27th International Conference on Computational Linguistics, pages 213\u2013223, Santa Fe, New Mexico, USA.",
            "year": 2018
        },
        {
            "authors": [
                "Zhanming Jie",
                "Jierui Li",
                "Wei Lu."
            ],
            "title": "Learning to reason deductively: Math word problem solving as complex relation extraction",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages",
            "year": 2022
        },
        {
            "authors": [
                "Rik Koncel-Kedziorski",
                "Subhro Roy",
                "Aida Amini",
                "Nate Kushman",
                "Hannaneh Hajishirzi."
            ],
            "title": "MAWPS: A math word problem repository",
            "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Hu-",
            "year": 2016
        },
        {
            "authors": [
                "Nate Kushman",
                "Yoav Artzi",
                "Luke Zettlemoyer",
                "Regina Barzilay."
            ],
            "title": "Learning to automatically solve algebra word problems",
            "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages",
            "year": 2014
        },
        {
            "authors": [
                "Yihuai Lan",
                "Lei Wang",
                "Qiyuan Zhang",
                "Yunshi Lan",
                "Bing Tian Dai",
                "Yan Wang",
                "Dongxiang Zhang",
                "Ee-Peng Lim."
            ],
            "title": "Mwptoolkit: An open-source framework for deep learning-based math word problem solvers",
            "venue": "Proceedings of the AAAI Conference on",
            "year": 2022
        },
        {
            "authors": [
                "Jierui Li",
                "Lei Wang",
                "Jipeng Zhang",
                "Yan Wang",
                "Bing Tian Dai",
                "Dongxiang Zhang."
            ],
            "title": "Modeling intrarelation in math word problems with different functional multi-head attentions",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computa-",
            "year": 2019
        },
        {
            "authors": [
                "Shucheng Li",
                "Lingfei Wu",
                "Shiwei Feng",
                "Fangli Xu",
                "Fengyuan Xu",
                "Sheng Zhong."
            ],
            "title": "Graph-totree neural networks for learning structured inputoutput translation with applications to semantic parsing and math word problem",
            "venue": "Findings of the Asso-",
            "year": 2020
        },
        {
            "authors": [
                "Zhongli Li",
                "Wenxuan Zhang",
                "Chao Yan",
                "Qingyu Zhou",
                "Chao Li",
                "Hongzhi Liu",
                "Yunbo Cao."
            ],
            "title": "Seeking patterns, not just memorizing procedures: Contrastive learning for solving math word problems",
            "venue": "Findings of the Association for Computational",
            "year": 2022
        },
        {
            "authors": [
                "Paul Pu Liang",
                "Chiyu Wu",
                "Louis-Philippe Morency",
                "Ruslan Salakhutdinov."
            ],
            "title": "Towards understanding and mitigating social biases in language models",
            "venue": "Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021,",
            "year": 2021
        },
        {
            "authors": [
                "Zhenwen Liang",
                "Jipeng Zhang",
                "Lei Wang",
                "Wei Qin",
                "Yunshi Lan",
                "Jie Shao",
                "Xiangliang Zhang."
            ],
            "title": "MWP-BERT: Numeracy-augmented pre-training for math word problem solving",
            "venue": "Findings of the Association for Computational Linguistics: NAACL 2022,",
            "year": 2022
        },
        {
            "authors": [
                "Christian Liguda",
                "Thies Pfeiffer."
            ],
            "title": "Modeling math word problems with augmented semantic networks",
            "venue": "Natural Language Processing and Information Systems, pages 247\u2013252, Berlin, Heidelberg. Springer Berlin Heidelberg.",
            "year": 2012
        },
        {
            "authors": [
                "Xin Lin",
                "Zhenya Huang",
                "Hongke Zhao",
                "Enhong Chen",
                "Qi Liu",
                "Hao Wang",
                "Shijin Wang."
            ],
            "title": "Hms: A hierarchical solver with dependency-enhanced understanding for math word problem",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2021
        },
        {
            "authors": [
                "Yinhan Liu",
                "Jiatao Gu",
                "Naman Goyal",
                "Xian Li",
                "Sergey Edunov",
                "Marjan Ghazvininejad",
                "Mike Lewis",
                "Luke Zettlemoyer."
            ],
            "title": "Multilingual denoising pretraining for neural machine translation",
            "venue": "Trans. Assoc. Comput. Linguistics, 8:726\u2013742.",
            "year": 2020
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Roberta: A robustly optimized bert pretraining approach",
            "venue": "Cite arxiv:1907.11692.",
            "year": 2019
        },
        {
            "authors": [
                "Mengqi Miao",
                "Fandong Meng",
                "Yijin Liu",
                "Xiao-Hua Zhou",
                "Jie Zhou."
            ],
            "title": "Prevent the language model from being overconfident in neural machine translation",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics",
            "year": 2021
        },
        {
            "authors": [
                "Arkil Patel",
                "Satwik Bhattamishra",
                "Navin Goyal"
            ],
            "title": "Are nlp models really able to solve simple math word problems",
            "venue": "In Proceedings of the 2021 Conference of the North American Chapter of the Association",
            "year": 2021
        },
        {
            "authors": [
                "Subhro Roy",
                "Dan Roth."
            ],
            "title": "Mapping to declarative knowledge for word problem solving",
            "venue": "Transactions of the Association for Computational Linguistics, 6:159\u2013172.",
            "year": 2018
        },
        {
            "authors": [
                "Torsten Scholak",
                "Nathan Schucher",
                "Dzmitry Bahdanau."
            ],
            "title": "PICARD: Parsing incrementally for constrained auto-regressive decoding from language models",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2021
        },
        {
            "authors": [
                "Jianhao Shen",
                "Yichun Yin",
                "Lin Li",
                "Lifeng Shang",
                "Xin Jiang",
                "Ming Zhang",
                "Qun Liu."
            ],
            "title": "Generate & rank: A multi-task framework for math word problems",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2021, pages 2269\u20132279,",
            "year": 2021
        },
        {
            "authors": [
                "Yibin Shen",
                "Cheqing Jin."
            ],
            "title": "Solving math word problems with multi-encoders and multi-decoders",
            "venue": "Proceedings of the 28th International Conference on Computational Linguistics, pages 2924\u20132934, Barcelona, Spain (Online). International Committee",
            "year": 2020
        },
        {
            "authors": [
                "Ryan Steed",
                "Swetasudha Panda",
                "Ari Kobren",
                "Michael L. Wick."
            ],
            "title": "Upstream mitigation is not all you need: Testing the bias transfer hypothesis in pre-trained language models",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Compu-",
            "year": 2022
        },
        {
            "authors": [
                "Minghuan Tan",
                "Lei Wang",
                "Lingxiao Jiang",
                "Jing Jiang."
            ],
            "title": "Investigating math word problems using pretrained multilingual language models",
            "venue": "arXiv preprint arXiv:2105.08928.",
            "year": 2021
        },
        {
            "authors": [
                "Christoph Tillmann",
                "Hermann Ney."
            ],
            "title": "Word reordering and a dynamic programming beam search algorithm for statistical machine translation",
            "venue": "Computational Linguistics, 29(1):97\u2013133.",
            "year": 2003
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin."
            ],
            "title": "Attention is all you need",
            "venue": "Advances in neural information processing systems, 30.",
            "year": 2017
        },
        {
            "authors": [
                "Deng-Bao Wang",
                "Lei Feng",
                "Min-Ling Zhang."
            ],
            "title": "Rethinking calibration of deep neural networks: Do not be afraid of overconfidence",
            "venue": "Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems",
            "year": 2021
        },
        {
            "authors": [
                "Lei Wang",
                "Dongxiang Zhang",
                "Jipeng Zhang",
                "Xing Xu",
                "Lianli Gao",
                "Bing Tian Dai",
                "Heng Tao Shen."
            ],
            "title": "Template-based math word problem solvers with recursive neural networks",
            "venue": "Proceedings of the ThirtyThird AAAI Conference on Artificial Intelligence and",
            "year": 2019
        },
        {
            "authors": [
                "Xuezhi Wang",
                "Jason Wei",
                "Dale Schuurmans",
                "Quoc Le",
                "Ed Chi",
                "Denny Zhou."
            ],
            "title": "Self-consistency improves chain of thought reasoning in language models",
            "venue": "arXiv preprint arXiv:2203.11171.",
            "year": 2022
        },
        {
            "authors": [
                "Yan Wang",
                "Xiaojiang Liu",
                "Shuming Shi."
            ],
            "title": "Deep neural solver for math word problems",
            "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 845\u2013854, Copenhagen, Denmark. Association for Computa-",
            "year": 2017
        },
        {
            "authors": [
                "Jason Wei",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Maarten Bosma",
                "Brian Ichter",
                "Fei Xia",
                "Ed Chi",
                "Quoc Le",
                "Denny Zhou"
            ],
            "title": "2022. Chain of thought prompting elicits reasoning in large language models",
            "year": 2022
        },
        {
            "authors": [
                "Qinzhuo Wu",
                "Qi Zhang",
                "Jinlan Fu",
                "Xuanjing Huang."
            ],
            "title": "A knowledge-aware sequence-to-tree network for math word problem solving",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 7137\u2013",
            "year": 2020
        },
        {
            "authors": [
                "Zhipeng Xie",
                "Shichao Sun."
            ],
            "title": "A goal-driven tree-structured neural model for math word problems",
            "venue": "IJCAI, pages 5299\u20135305.",
            "year": 2019
        },
        {
            "authors": [
                "Jipeng Zhang",
                "Lei Wang",
                "Roy Ka-Wei Lee",
                "Yi Bin",
                "Yan Wang",
                "Jie Shao",
                "Ee-Peng Lim."
            ],
            "title": "Graph-totree learning for solving math word problems",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3928\u2013",
            "year": 2020
        },
        {
            "authors": [
                "Shiyue Zhang",
                "Benjamin Frey",
                "Mohit Bansal."
            ],
            "title": "Chrentranslate: Cherokee-english machine translation demo with quality estimation and corrective feedback",
            "venue": "Proceedings of the Joint Conference of the 59th Annual Meeting of the Association for Compu-",
            "year": 2021
        },
        {
            "authors": [
                "Susan Zhang",
                "Stephen Roller",
                "Naman Goyal",
                "Mikel Artetxe",
                "Moya Chen",
                "Shuohui Chen",
                "Christopher Dewan",
                "Mona Diab",
                "Xian Li",
                "Xi Victoria Lin"
            ],
            "title": "Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Solving Math Word Problems (MWPs) is the task of answering natural language problems that require mathematical reasoning ability (Bobrow, 1964). To achieve such a skill, researchers have proposed a variety of MWP solvers, each of which seeks to produce a specific logic form that can be used to calculate the answer to the problem.\nDeductive methods and generation-based methods are typically the two main approaches used to solve MWPs. Inspired by advances in machine translation, some generation-based methods directly adopt a sequence-to-sequence (seq2seq) model to generate the sequence of the math expression according to the problem (Wang et al., 2017). To further capture the structure of the math expression, some sequence-to-tree (seq2tree) methods (Xie and Sun, 2019) adopt a tree decoder to\n\u2217*Corresponding author.\ngenerate the binary expression tree, where each node denotes an operator or a quantity. These generation-based methods, however, suffer from a fatal flaw in that they require repeated generation of the same sub-expression (or sub-tree), which makes them inefficient. For example, in Figure 1 (a), the sub-expression (94\u2212 35\u00d7 2)\u00f7 (4\u2212 2) is generated four times. Humans, on the other hand, can represent repeated sub-expressions with an intermediate quantity that can be naturally reused in the following computation process.\nDeductive approaches (Cao et al., 2021; Jie et al., 2022) are suggested to address the aforementioned reuse issue. Specifically, deductive methods convert the math expression into a binary Directed Acyclic Graph (bDAG), where each node represents an operation that consists of a binary operator and two input quantities. The calculation result of an operation is represented by a new intermediate quantity. Then, these methods need to generate a topological ordering, i.e., an operation sequence, of the bDAG. By doing this, subsequent operations can easily reuse the previously generated intermediate quantities. As shown in Figure 1 (b), quantity q3 represents the sub-expression (94\u22122\u00d735)\u00f7(4\u22122), which is then reused by two subsequent operations denoted by quantity q4 and q8. When the operation sequence is inferred, these operations are computed consecutively to produce the final answer. Beyond the ability to reuse the intermediate quantity, deductive methods are more interpretable because the step-by-step generation of operations helps people understand how the reasoning works. To generate the operation at each reasoning step, existing deductive methods follow an \u201cenumerate-then-classify\u201d procedure. To be more precise, they create a collection of candidate operations by listing every possible combination of the quantities and operators, and then they use a classifier to choose the operation that has the highest probability, which can be viewed as a greedy search strategy.\nQuestion: There are several chickens and rabbits in a cage. Inside, we observe 94 feet and 35 heads. A chicken has 1 head and 2 feet. A rabbit has 1 head and 4 feet. The number of rabbits and chickens are denoted by x and y, respectively. Tell me the value of \ud835\udc99 \u00d7 \ud835\udc99 + \ud835\udc9a \u00d7\ud835\udc9a.\nOne obvious limitation of the aforementioned approaches is that they only take into account the basic binary operators such as +,\u2212,\u00d7,\u00f7. Although binary operators are the most fundamental in mathematics, there are some templated problems, such as solving linear equations, finding the extreme values of quadratic equations, and even integrating a function, that can be solved by existing advanced operators. Thus, we can abstract an advanced operator to tackle each templated problem. With these advanced operators, we can inject prior mathematical knowledge to reduce the difficulty of solving MWPs. However, problems requiring advanced operators are difficult to tackle using earlier MWP methods: generation-based methods inherently suffer from the reuse issue; deductive methods are limited by the assumption of binary operations.\nTo address this issue, we first define a multivariate Directed Acyclic Graph (mDAG) with each node involving a multivariate operation that consists of a basic or advanced operator and multiple input quantities. Compared to basic binary operators, advanced operators can receive multiple quantities and return multiple output quantities. For example, in Figure 1 (c), a linear equation solver requires 6 quantities (1, 1, 4, 2, 35, 94) and returns 2 intermediate quantities (q0, q1). Then, similar to the bDAG, we use the topological ordering of the mDAG to obtain a sequence of multivariate operations. To generate such a sequence, we propose GeDe, a Generation-based Deductive approach.\nCompared to generation-based techniques, GeDe has the deductive property that enables the reuse of intermediate quantities. Compared to deductive methods, GeDe employs a generation model to generate each multivariate operation, which avoids the need to enumerate a large number of possible multivariate operations.\nIn order to achieve this generative-based deductive capacity, we equip a generation model with a re-encoding strategy that jointly encodes the problem and intermediate quantities at each step of reasoning, yielding embeddings of the intermediate quantities that could be reused in the subsequent steps. In addition, we switch from the traditional greedy or beam search to a hierarchical beam search strategy, which is well suited to the equation generation requirement.\nContributions. (1) By extending bDAG to mDAG, we can directly address complex mathematical problems using pre-defined advanced operators. (2) We propose GeDe, a generation-based deductive model that keeps the deductive property while avoiding the high cost of enumeration. GeDe equips a generation model with the re-encoding and hierarchical beam search strategies to achieve the objective. (3) We automatically create a dataset named CMWPA for solving complicated MWPs that require both the basic binary operators and the advanced operators. It has been shown that GeDe not only effectively adapts advanced operators but also performs better on three existing MWP\ndatasets when more operations are involved."
        },
        {
            "heading": "2 Related Work",
            "text": ""
        },
        {
            "heading": "2.1 Math Word Problem",
            "text": "Early efforts to solve MWPs use rule-based approaches, which are only able to address a limited number of MWP scenarios (Kushman et al., 2014; Liguda and Pfeiffer, 2012; Roy and Roth, 2018). Deep learning models, on the other hand, are better capable of addressing a wider range of MWPs. The first seq2seq model for MWPs is proposed by Wang et al. (2017). This model employs RNN to encode the problem and produce mathematical expressions. To enhance the seq2seq model, additional techniques have been developed, including reinforcement learning (Huang et al., 2018), template-based methods (Wang et al., 2019), and group attention mechanisms (Li et al., 2019). Seq2tree, a tree structure decoder, is developed by Xie and Sun (2019). It replaces the original sequence decoder and greatly outperforms seq2seq models in terms of performance. KA-S2T (Wu et al., 2020) and MWPBERT (Liang et al., 2022) inject commonsense knowledge and quantities\u2019 properties to improve model performance. In order to encode the relationships between quantities in MWPs, Graph2tree (Li et al., 2020; Zhang et al., 2020) encodes the input problem using graph neural networks.\nIn addition to the generation models with seq2seq, seq2tree, or graph2tree structures, other efforts use deductive methods to solve MWPs step by step rather than directly generating the entire expression. Cao et al. (2021) represent the calculation process by bDAG and extract the bDAG structure by aggregating quantities and sub-expressions iteratively. Jie et al. (2022) view the task as a complex relation extraction problem and predict the relation of two quantities gradually. Compared with generation methods, deductive methods can easily employ the intermediate values to avoid repetitive generation. We expand the deductive methods to handle more complex advanced operators."
        },
        {
            "heading": "2.2 Large-scale Pre-trained Language Model",
            "text": "In-context few-shot learning or even zero-shot learning based on large-scale pre-trained language models, such as GPT-3 (Brown et al., 2020), PaLM (Chowdhery et al., 2022), and OPT (Zhang et al., 2022), has been thoroughly studied for multiple tasks, including math word problem solving (Cobbe et al., 2021; Wang et al., 2022; Wei\net al., 2022). This tuning-free methods have achieved promising performance, and their success mainly relies on the reasoning power of large-scale PLMs. However, the reasoning power is extremely expensive due to the large number of parameters, massive pre-training data, carefully designed pretraining objectives, and huge overhead of computational resources. In contrast, we investigate finetuning the small models."
        },
        {
            "heading": "3 Problem Definition",
            "text": "The goal of MWP is to generate a specific logic form that can be executed to answer the problem P = {p1, p2, .., pn} which consists of n tokens and m quantity tokens Q = {q1, q2, ..., qm}. Some commonsense constants, such as \u03c0 and e, may not explicitly appear in the problem; thus, we additionally add them to the quantity set Q.\nIn this paper, we define the multivariate1 directed acyclic graph (mDAG) as our target logic form, which describes the process of solving MWPs. The nodes of mDAG denote operations that consist of an operator and multiple quantities, and the edges represent the dependency between nodes. Our goal is to generate a operation sequence O = (o1, o2, ..., o|O|) which can be obtained from the topological ordering of mDAG. |O| is the number of operations. The t-th operation is a sequence of tokens ot = (at1, a t 2, ..., a t |ot|) with each token representing an operator or a quantity. Each operator is selected from the operator set V , which is predefined by the provided dataset. Each quantity is choosen from Q, which is initialized with the m quantity tokens in P and can gradually grow as the steps of reasoning progress. |ot| is the number of tokens of the t-th operation."
        },
        {
            "heading": "4 Approach",
            "text": ""
        },
        {
            "heading": "4.1 Overview",
            "text": "In general, the proposed GeDe method consists of two main components: the re-encoder and decoder. The former aims to jointly encode the problem and quantities, which can support the reuse of intermediate quantities. The latter is designed to generate an operation according to the output of the re-encoder. Since our target is an operation sequence, we need to perform multiple reasoning steps, with each step generating an operation. We illustrate the reasoning process in Figure 2. At each\n1The term \"multivariate\" means that the operator can receive multiple quantities and output multiple quantities.\nreasoning step, we update the input sequence by adding new intermediate quantities generated in the previous step. The updated input sequence is fed into the re-encoder and the decoder to generate an operation. The generation process is equipped with a hierarchical beam search strategy to enable both token-level beam search within an operation and operation-level beam search in the whole operation sequence."
        },
        {
            "heading": "4.2 Re-Encoder",
            "text": "This section delves into the re-encoder by explaining the input and the encoder respectively.\nSince we are only interested in the semantics of the quantities rather than their precise values, we first substitute each quantity in the original problem P with a general special token, [QTTi]. This leaves Pr devoid of any specific quantities. In order to obtain the encoder\u2019s input sequence, P tin, we concatenate Pr with all intermediate quantities, where each quantity signifies its corresponding operation.\nWe take the example in Figure 2 to explain the input. The given math problem contains six quantities, which are replaced by [QTT0] to [QTT5]. At reasoning step t, we have already generated the following operation:\n[LES] [ [QTT2] [QTT4] [QTT3] [QTT5] ] [ [QTT0] [QTT1] ] (1)\n= [LES][QTT2][QTT4][QTT3][QTT5][QTT0][QTT1]\nwhere LES stands for a multivariant operator of linear equation solver given the operands of a matrix made up of [QTT2], [QTT3], [QTT4], [QTT5] and a vector made up of [QTT0] and [QTT1]. In practice, the operation is represented by a sequence that expands the matrix and vector by row. Then we denote the outputs of this operation by two new\nquantities [QTT6] and [QTT7] and concatenate the sequence\n[QTT6][QTT7][=][LES][QTT2][QTT4] \u00b7 \u00b7 \u00b7 [QTT1] (2)\nwith the original input Pr to obtain P tin. We instantiate the re-encoder ME by a PLM (e.g., BERT or GPT) to represent the input sequence and obtain the reasoning state, i.e.,\nRt = ME(P tin), (3)\nwhere Rt \u2208 RN\u00d7H represents the reasoning state at step t. N denotes the length of the input sequence and H denotes the hidden size.\nFor the subsequent generation module, we extract the representation of each quantity from Rt according to their positions in P tin:\nQt = {Rt[i] | i \u2208 Iq}, (4)\nwhere Qt \u2208 RM\u00d7H , M denotes the number of quantities, Iq saves the indexes of all the quantities in P tin, and R\nt[i] denotes the i-th row of Rt. In summary, the original input is re-encoded with the previously generated intermediate quantities at each reasoning step to update the reasoning state and record all intermediate quantities, which may be reused in the subsequent generation process."
        },
        {
            "heading": "4.3 Decoder",
            "text": "We adopt a Gated Recurrent Unit (GRU) network (Chung et al., 2014) combined with the attention mechanism (Vaswani et al., 2017) as the decoder MD. Following the majority of the earlier works (Liang et al., 2022; Tan et al., 2021; Xie and Sun, 2019), we choose GRU instead of transformer for a fair comparison. Although some works choose pretrained transformer (Shen et al., 2021), their performance might not be improved due to the larger parameters but limited labeled data.\nOperation Generation. The decoder aims to provide an operation ot = (at1, a t 2, ..., a t |ot|) at each reasoning step t. To enable the auto-regressive generation, we insert a special beginning token ([BOS]) before the first token at1 and add a special ending token ([EOS] or [EOO]) after the last token at|ot| to re-create o t = (at0, a t 1, a t 2, ..., a t |ot|+1). While [EOS] only signifies the termination of the current operation, [EOO] stands for the final token of the complete operation sequence. The hidden state hti of each token a t i can be obtained by hti = GRU(h t i\u22121,a t i) where h t i\u22121 \u2208 R1\u00d7H represents the hidden state of the previous step, ht0 is initialized from the hidden state of the [CLS] token produced by the encoder, and ati \u2208 R1\u00d7H is the representation of the token ati. Next, using hti as the query to attend to current reasoning state Rt, we obtain the attention-enhanced state Ati = MHA(h t i,R\nt), where MHA denotes multihead attention (Vaswani et al., 2017). Finally, we determine the likelihood of the output token by determining how well Ati resembles the representation of quantities and operators, i.e.,\np(ati|o<t, at<i, P ) = softmax(Ati([V | Qt])T ), (5)\nwhere o<t represents o1, o2, ..., ot\u22121 before reasoning step t, at<i represents a t 0, a t 1, \u00b7 \u00b7 \u00b7 , ati\u22121 before the i-th token of step t, | is the matrix concatenation operator, V \u2208 R|V |\u00d7H and Qt \u2208 RM\u00d7H denote the representations of operators and t-th step\u2019s quantities respectively. When obtaining a new operation ot, we can determine the number of new quantities by the operator in ot and record these new intermediate quantities for the subsequent reasoning steps. When [EOS] has the highest probability, the decoding process of the current operation ends but a new operation generation starts instead. When [EOO] has the highest probability, the entire decoding process is complete.\nTraining Objective. Given a problem P and its ground truth operation sequence O, we maximize the probability of generating O by P , i.e.,\np(O|P ) = |O|\u220f t=1 |ot|+1\u220f i=1 p(ati|o<t, at<i, P ). (6)"
        },
        {
            "heading": "4.4 Hierarchical Beam Search",
            "text": "To enhance the generation quality during inference, beam search is used in many generation tasks as\na refined version of greedy search (Tillmann and Ney, 2003). However, using beam search in the deductive methods is difficult because the search space of the operation sequence is nested. In other words, we need to generate each operation based on tokens and generate the entire operation sequence based on operations. Therefore, previous deductive methods (Cao et al., 2021; Jie et al., 2022) only adopt the greedy search and leave the implementation of the beam search as further work. To address this challenge, we propose a hierarchical beam search strategy. Compared with the traditional beam search, the hierarchical beam search can control the generation process at two levels.\nSpecifically, the hierarchical beam search consists an inner beam search and an outer beam search. The former is a standard beam search which seeks a series of tokens to form a candidate operation. The latter is designed to search a complete operation sequence. The beam score of the inner beam search purely relies on the probabilities of tokens predicted by the decoder. Suppose the t-th step generates l tokens, the inner beam score ibst is calculated as:\nibst = log l\u220f i=1 p(ati) 1 l = 1 l l\u2211 i=1 log p(ati), (7)\nwhere p(ati) is computed by Eq (5). We use the inner beam scores of generated operations to approximate the distribution of operations to support the outer beam search. The probability of the t-th operation ot can be calculated as the softmax score of its inner beam score, i.e.,\np(ot) = softmax(exp(ibst)). (8)\nSuppose the entire operation sequence contains T operations, the outer beam score is computed as:\nobs = log( T\u220f t=1 p(ot)) 1 T = 1 T T\u2211 t=1 log p(ot). (9)\nAlgorithm 1 presents the hierarchical beam search algorithm. Each outer beam is denoted by the symbol beam, which keeps track of both the current operation sequence and the beam score. The empty operation sequence and score of zero are used to construct the initial outer beam initially (line 1). Then, we iteratively expand outer beams until they are all finished, i.e., all the outer beams are terminated with [EOO] (line 4-14). For\nAlgorithm 1 Hierarchical Beam Search Input: Math World Problem P , Beam size K Output: beams with Top-K operation sequences 1: beams\u2190 [InitialBeam]; 2: while not all beams are over do 3: beamsn \u2190 []; 4: for beam in beams do 5: if beam is over then 6: beamsn.append(beam); 7: else 8: ops\u2190 InnerBeamSearch(P, beam,K); 9: for op in ops do 10: beamnew \u2190 Extend(beam, op); 11: beamsn.append(beamnew); 12: end for 13: end if 14: end for 15: beams\u2190 GetTopK(beamsn,K); 16: end while\neach extensible outer beam, we search candidate operations ops using the inner beam search (line 8). The inner and the outer beam search share the same beam size K. Next, we extend outer beams with these candidate operations (line 9-12). At the end of each step, we only maintain the top-K outer beams according to their scores computed by Eq. (9) (line 15). Finally, beams save the top-K operation sequences. We discuss the complexity of GeDe in Appendix A.1"
        },
        {
            "heading": "4.5 Decoding Constraint",
            "text": "Logic forms need to obey clear grammatical rules. In order to guarantee the validity of the output, we provide two constraint strategies, one during and one after the decoding process. Inspired by PICARD (Scholak et al., 2021), an incremental grammar checker proposed for Text-to-SQL task, the constraint strategy during the decoding process is to filter out illegal beams at each decoding step in the inner beam search to prevent potential syntax errors in the generated operation. For example, when we detect that the current token generation step needs to generate an operator, we will reject all non-operators. Following (Jie et al., 2022), the after decoding constraint strategy eliminates candidate operations that are improbable to exist in realworld mathematical problems, such as \u201c[QTTi] \u2212 [QTTi]\u201d and \u201c[QTTi][QTTi]\u201d."
        },
        {
            "heading": "5 Experiments",
            "text": "In this section, we establish a dataset for multivariant advanced operators and show that the proposed GeDe is capable of doing these types of operations successfully. We also conduct experiments on four\nwidely-adopted MWP datasets to show the effectiveness of our model on binary operations."
        },
        {
            "heading": "5.1 Experimental Setup",
            "text": "Datasets. We consider four MWP datasets including our created CMWPA and three widelyused existing MWP datasets: MAWPS (KoncelKedziorski et al., 2016), Math23k (Wang et al., 2017), MathQA (Amini et al., 2019), and SVAMP (Patel et al., 2021). We use CMWPA to verify the validity of multivariate operations. Following (Tan et al., 2021), we perform pre-processing to filter out unsolvable problems. In all the datasets, we take into account the basic binary operators addition (+), subtraction (\u2212), multiplication (\u00d7), division (\u00f7), and exponentiation (\u02c6). For advanced operators used in the CMWPA dataset, we consider the linear equation solver, the quadratic function extremum solver, and the quadratic function integral solver. Appendix A.2 presents the statistics for each dataset.\nEvaluation Metric. Following previous work (Jie et al., 2022), we compare the predicted and the gold answer to calculate the accuracy as the evaluation metric. We parse out the operator and operands from the model predicted expression sequence and then use the corresponding operator executor to calculate the answers. We explain the details of the parsing and execution in Appendix A.3.\nImplementation Details. We adopt RoBERTabase2 (Liu et al., 2019) as our re-encoder for English datasets, and Chinese-RoBERTa-base3 (Cui et al., 2020) for Chinese datasets. The purpose of using the Roberta model is to make a more fair comparison with previous work. We can also use unidirectional attention models (e.g., GPT). We use AdmaW to optimize the loss function with a learning rate of 2e-5, a weight decay of 1e-2, and a batch size of 8. During inference, the beam size K is set to 4 by default. For CMWPA, Math23K, MathQA, and SVAMP we report accuracy on their test set. For MAWPS and Math23k, we follow previous works and also report 5-fold cross-validation performance. We conduct all experiments with a RTX 3090 (24G) GPU.\n2https://huggingface.co/roberta-base 3https://huggingface.co/hfl/chinese-roberta-wwm-ext"
        },
        {
            "heading": "5.2 Experiment on CMWPA",
            "text": "The existing MWP datasets only use basic binary operators as target logic form. Rewriting these logic forms to support advanced operators is expensive. Therefore, based on handcraft templates, we create a synthetic dataset named CMWPA (Complex Math Word Problems with Advanced operators).\nTo create the CMWPA dataset, we first define needed operators which include five binary operators (addition (+), subtraction (\u2212), multiplication (\u00d7), division (\u00f7), and exponentiation (\u02c6)), as well as three advanced operators, which can be used to solve linear equations (the [linear equation solver] operator), find the maximum value of quadratic functions (the [quadratic function extremum solver] operator), and find the definite integrals of quadratic functions (the [quadratic function integral solver] operator). For each operator, we write one or more templates to generate a text description and its operation. We only consider the quadratic function because the operations related to the quadratic function can be transformed to a series of binary operations for training the baseline model. The templates of CMWPA is described in Appendix A.4. In this dataset, for each problem, we provide two types of logic forms: multivariate operation sequence and binary operation sequence. An example is given in Appendix Table 5.\nWe conduct experiments on CMWPA to demonstrate that using advanced operators to solve complex MWPs is more effective than only using basic binary operators. Concretely, our proposed GeDe is applied to generate multivariate operation sequences. Then for fair comparison, we adopt GeDe to generate binary operation sequence.\nExperiment Results. Table 2 shows the accuracy and inference time on CMWPA, using mDAG as\nannotation, GeDe achieves 95.0% accuracy, which indicates that our proposed method can effectively support advanced operators to generate the multivariate operation sequence. However, when using the binary expression tree as the generation target, GeDe only achieves 32.0% accuracy. Because the average number of advanced operators in multivariate operation sequences is 2.98, which is significantly less than the average number of binary operators (i.e., 35.03) in binary expressions tree, using advanced operators to solve MWPs can essentially reduce the learning difficulty and lead to improved both the accuracy and efficiency."
        },
        {
            "heading": "5.3 Experiment on Existing MWP Datasets",
            "text": "Baselines. The baselines can be broadly categorized into four groups, sequence-to-sequence(S2S), sequence-to-tree(S2T), graph-to-tree(G2T), and deductive-reasoning(DR), where the first three of these are all generation-based methods but are instantiated with different encoders or decoders. We select baselines having reported the performances on at least one of the three datasets.\nExperiment Results. We start by running tests on MAWPS and Math23k. As shown in Table 1, our model achieves promising performance on both the datasets compared to previous state-of-the-art (SOTA) methods. Given that MAWPS only has an average of 1.41 binary operations, the proposed\nGeDe only slightly improves 0.3% accuracy on MAWPS compared to earlier baselines. This is not enough to demonstrate the benefits of the proposed model. On Math23k, GeDe performs equally well as the earlier SOTA method Generate&Rank. However, Generate&Rank fine-tunes a mBARTlarge (Liu et al., 2020) model with 610M parameters. In contrast, GeDe only involves 126M parameters and thus reveals a better parameter-efficiency. We further evaluate our method on MathQA, the most challenging MWP dataset with an average of 4.25 binary operations, and show results in Table 1. Our model greatly beats all baselines (+2.9%), which demonstrates the model\u2019s efficacy in handing complex MWPs. In summary, on three existing MWP datasets, the performances of GeDe are on par or better than those of the closest competitors. SVAMP is also a challenging dataset that is manually created to evaluate a model\u2019s robustness. On this dataset, GeDe achieves an accuracy of 45.7%, which can outperform the vast majority of baselines except the DR model.\nIn addition, we conduct experiments based on Roberta-large on the Math23k dataset. The model achieves an accuracy of 86.7% on the Math23K test set. Using Roberta-large improves the accuracy by 1.3% over using Roberta-base. This shows that using a larger PLM improves the performance of our method and outperforms the baseline Generate & Rank model on the Math23K test set.\nTo further highlight the advantages of the proposed GeDe, following (Jie et al., 2022), we provide a fine-grained analysis on MathQA based on various numbers of operations. To be more specific, we compare our model with the most powerful baseline RoBERTa-DR (Jie et al., 2022) and display the analysis results in Table 3. We observe that GeDe performs better on samples with 1, 3, and 4 operations, particularly on samples with at least 5 operations. This comparison indicates our model is more robust to problems requiring more reasoning steps, because the designed re-encoder can capture adequate interactions between the newly produced quantities and the original problem."
        },
        {
            "heading": "5.4 Ablation Study",
            "text": "In this section, we take a thorough ablation study on MathQA dataset to verify the effectiveness of the re-encode and the hierarchical beam search strategies in the proposed GeDe.\nEffect of Re-encoder. The proposed re-encoder\nin Section 4.2 can update both new quantities and reasoning state at each reasoning step. We investigate the two functions respectively.\nInstead of using dynamic quantity embeddings, we develop a variant model with static quantity embeddings. In other words, instead of having distinct embeddings updated based on various contexts in various math problems, [QTTi] in various math problems is assigned a unified embedding that is updated globally. Note we keep re-encoding the original problem with the newly produced quantities at each step t, but only the updated reasoning state Rt is leveraged. The comparison results in Table 4 show that without the dynamic quantity embeddings, the performance drops 1.2% on MathQA\u2019s test set. Since different MWPs\u2019 quantities reflect different semantics, it is preferable for them to be dynamically updated with their contexts.\nThen we completely remove the re-encoder and only allow the encoder to encode the original problem. Instead, we directly use the hidden state in the decoder\u2019s GRU network to represent the reasoning state. Table 4 shows that without the re-encoder, the performance drops 5.7%. In this variant model, although the quantities are dynamically updated according to various problems, the interactions between the quantities and the input problem are not fully exploited as the re-encoder does.\nEffect of Hierarchical Beam Search. Previous deductive methods (Cao et al., 2021; Jie et al., 2022) generate the operation sequence based on hierarchical greedy search, and regard the implementation of beam search as a future challenge. We imple-\nment hierarchical beam search in our GeDe to improve greedy search. We compare them, where the beam size is set to 1 to create a greedy search. As shown in Table 4, when the hierarchical beam search is disabled (beam size = 4) and replaced with the hierarchical greedy search (beam size = 1), the performance drops 0.5%. By observing the inner and outer beam scores in the generation process, for most of the samples, we find that the score of the first beam is significantly greater than that of the remaining beams, resulting in a relatively small gap between greedy and beam search. This problem, also referred to as neural networks\u2019 \u201cover-confidence\u201d, has been studied by some works (Miao et al., 2021; Wang et al., 2021). Such improvement is left in the further."
        },
        {
            "heading": "6 Conclusion and Future Work",
            "text": "This paper proposes a multivariant direct acyclic graph (mDAG) to describe a math expression in order to handle the advanced multivariant operators. Then to generate the topological ordering of mDAG, we propose a generation model equipped with a re-encode strategy to keep the deductive property but avoid the expensive enumeration of existing deductive methods. A hierarchical beam search algorithm is implemented to enable the inner token and outer operation searches. Extensive experiments on three standard datasets and one automatically created dataset demonstrate the proposed model\u2019s advantage in solving math problems with binary operators and advanced operators."
        },
        {
            "heading": "7 Limitations",
            "text": "From the time complexity analysis in Appendix A.1, we can see that our model will face the efficiency issue when it needs to generate a long operation sequence. At the same time, the reencode module needs to concatenate the problem description with generated operations, which may reach the input length limit of PLM. Therefore, our future work will study how to compress the input sequence during the generation process to address above issues."
        },
        {
            "heading": "8 Ethics Statement",
            "text": "For many years, public opinion has debated the pros and cons associated with artificial intelligence technology. One consensus is that advances in technology may be used in a variety of scenarios,\nleading to different influences. To provide an ethical analysis of this work and others on the same line, we will address three aspects: the possible positive or negative effects of our work, the impact of harmful information in datasets, and the equality and differences between different languages.\nFirst, the point of studying MWP is to explore the mathematical reasoning capabilities of artificial intelligence (Wei et al., 2022). However, the developed models may still be applied to harmful aspects, such as cheating in math exams.\nOn the other hand, the presence of harmful information in the training data may lead the model to learn some implicit biases (Liang et al., 2021; Steed et al., 2022). In our experiments, for the three existing datasets, we exactly follow the experimental setup of previous works to pre-process and remove the potential harmful information. For our manually created dataset CMWPA, our templates also do not contain any harmful information. However, in the inference phase, our model cannot reject answers when the user provides malicious input. Therefore, we need to employ extra efforts to avoid this issue when the model is deployed online.\nFinally, we use both English and Chinese datasets in our experiments to respect linguistic equality and better take into account language differences. The experimental results validate the robustness of our model across languages. Nevertheless, English and Chinese are the two most popular languages, and we should make greater efforts to concentrate on and preserve the development of minor languages in the field of natural language processing (Zhang et al., 2021)."
        },
        {
            "heading": "Acknowledgments",
            "text": "This work is supported by National Natural Science Foundation of China (62322214, 62072460, 62172424,62276270); Beijing Natural Science Foundation (4212022); the Public Computing Cloud at Renmin University of China."
        },
        {
            "heading": "A Appendix",
            "text": "A.1 Complexity Analysis\nConsider a math problem that has n words in the problem description and |O| operations in the solving process. A total of \u03ba words are needed to describe the |O| operations, i.e., \u03ba = \u2211|O| t=1 |ot|. GeDe needs to perform |O| operation re-encode steps and \u03ba token generation steps. For the \u03c4 -th re-encode step, its computational complexity is O((n + \u2211\u03c4\u22121 t=1 |ot|)2). For generating the tokens in o\u03c4 , its computational complexity is O(|o\u03c4 | \u2217 (n + \u2211\u03c4\u22121 t=1 |ot|)). Therefore, the overall time\ncomplexity is \u2211|O| \u03c4=1(n + \u2211\u03c4\u22121\nt=1 |ot|)2) + |o\u03c4 | \u2217 (n + \u2211\u03c4\u22121 t=1 |ot|) < O(|O| \u2217 n2 + \u03ba \u2217 (n + \u03ba)). If we use the unidirectional attention model as re-encoder, the complexity can be lowered to O(n2+\u03ba\u2217 (n+\u03ba)), which is the same as what the current seq2seq generation methods achieve. The additional time complexity is acceptable because |O| is typically not very large.\nA.2 Datasets Statistics\nThe statistics of datasets are presented in Table 6. CMWPA is a synthetic English dataset with 1000 training samples and 100 test and validation samples. MAWPS and MathQA are public English MWP datasets that contain 1.9K math problems and 20K math problems, respectively. Math23K is a public Chinese MWP dataset that contains 23K math problems. We use the average number of operations to assess the difficulty of a MWP dataset. As we can see, MAWPS is the simplest dataset because almost all problems require only one or two operations. MathQA is the most challenging dataset, requiring more operations and, hence, more steps in the reasoning process to obtain the answer. SVAMP is also a challenging dataset that is manually created to evaluate a model\u2019s robustness. They apply variations to the instances sampled from MAWPS. Such variations could include adding extra quantities, swapping the positions between noun phrases, etc.\nA.3 Parsing and Execution\nDue to the existence of higher-order operators, the way we calculate the answer is different from previous works. We implement the corresponding solving function using Python for each pre-defined operator, which is also included in our published code. During inference, for the generated operation sequence, we sequentially calculate the returned quantities for each operation. Naturally, the returned quantities of the last operation denote the answer to the problem. For a generated operation, we first parse out its operator and several operands. Then, we call the solving function corresponding to the operator to obtain the returned quantities.\nA.4 CMWPA Templates\nWe show the templates corresponding to the advanced operators as follows.\nTwo templates for the [linear equation solver] operator:\n\u2022 Text description: [q0] * [o0] + [q1] * [o1] = [q4]; [q2] * [o0] + [q3] * [o1] = [q5].\nOperation: [linear equation solver] [q0] [q1] [q2] [q3] [q4] [q5]\n\u2022 Text description: Determine [o0], [o1] as the result of inverse of matrix [ [ [q0] , [q1] ] , [ [q2] , [q3] ] ] times\nvector [ [q4] , [q5] ].\nOperation: [linear equation solver] [q0] [q1] [q2] [q3] [q4] [q5]\nOne template for the [quadratic function integral solver] operator:\n\u2022 Text description: Determine [o0] as the definite integral of quadratic function [q0] * x\u02c62 + [q1] * x + [q2]\nbetween the intervals [q3] and [q4].\nOperation: [quadratic function integral solver] [q3] [q4] [q0] [q1] [q2]\nOne template for the [quadratic function extremum solver] operator:\n\u2022 Text description: Determine [o0] as the the extremum value of quadratic function [q0] * x\u02c62 + [q1] * x + [q2].\nOperation: [quadratic function extremum solver] [q0] [q1] [q2]\nBased on the templates, we generate a sample as follows. First, we randomly initialize a candidate set of quantities. Then, we randomly select a template and fill in slots by randomly selecting quantities from the quantity candidate set. We reinput the returned quantities of the operation into the candidate set and repeat the above process several times. In this way, a problem description and its operation sequence are generated. We also convert the operation sequence into a pre-order binary expression as another type of annotation for training the seq2seq baseline.\nA.5 CMWPA Example We provide a sample of CMWPA in Table 5. This sample is initialized with 6 quantities and involves four types of operators: the subtraction operator, the [linear equation solver] operator, the [quadratic function integral solver] operator, and the [quadratic function extremum solver] operator. Two types of annotations are provided: the multivariant operation sequence and the pre-order binary expression (pre-order binary expression can be transformed into a binary operation sequence (bDAG) or a binary expression tree). For each operation in the multivariant operation sequence, we provide the operation, the input quantities, and the returned output quantities."
        }
    ],
    "title": "A Generation-based Deductive Method for Math Word Problems",
    "year": 2023
}