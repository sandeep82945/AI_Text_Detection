{
    "abstractText": "In recent years, few-shot relation classification has evoked many research interests. Yet a more challenging problem, i.e. none-of-the-above (NOTA), is under-explored. Existing works mainly regard NOTA as an extra class and treat it the same as known relations. However, such a solution ignores the overall instance distribution, where NOTA instances are actually outliers and distributed unnaturally compared with known ones. In this paper, we propose a density-aware prototypical network (D-Proto) to treat various instances distinctly. Specifically, we design unique training objectives to separate known instances and isolate NOTA instances, respectively. This produces an ideal instance distribution, where known instances are dense yet NOTAs have a small density. Moreover, we propose a NOTA detection module to further enlarge the density of known samples, and discriminate NOTA and known samples accurately. Experimental results demonstrate that the proposed method outperforms strong baselines with robustness towards various NOTA rates.1",
    "authors": [
        {
            "affiliations": [],
            "name": "Jianfeng Wu"
        },
        {
            "affiliations": [],
            "name": "Mengting Hu"
        },
        {
            "affiliations": [],
            "name": "Yike Wu"
        },
        {
            "affiliations": [],
            "name": "Bingzhe Wu"
        },
        {
            "affiliations": [],
            "name": "Yalan Xie"
        },
        {
            "affiliations": [],
            "name": "Mingming Liu"
        },
        {
            "affiliations": [],
            "name": "Renhong Cheng"
        }
    ],
    "id": "SP:178988bfa5384a867e472bcd39dcd5740a4c6682",
    "references": [
        {
            "authors": [
                "Livio Baldini Soares",
                "Nicholas FitzGerald",
                "Jeffrey Ling",
                "Tom Kwiatkowski."
            ],
            "title": "Matching the blanks: Distributional similarity for relation learning",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (ACL), pages",
            "year": 2019
        },
        {
            "authors": [
                "Sam Brody",
                "Sichao Wu",
                "Adrian Benton."
            ],
            "title": "Towards realistic few-shot relation extraction",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 5338\u2013 5345, Online and Punta Cana, Dominican Republic.",
            "year": 2021
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association",
            "year": 2019
        },
        {
            "authors": [
                "Bowen Dong",
                "Yuan Yao",
                "Ruobing Xie",
                "Tianyu Gao",
                "Xu Han",
                "Zhiyuan Liu",
                "Fen Lin",
                "Leyu Lin",
                "Maosong Sun."
            ],
            "title": "Meta-information guided metalearning for few-shot relation classification",
            "venue": "Proceedings of the 28th International Conference on",
            "year": 2020
        },
        {
            "authors": [
                "Tianyu Gao",
                "Xu Han",
                "Zhiyuan Liu",
                "Maosong Sun."
            ],
            "title": "Hybrid attention-based prototypical networks for noisy few-shot relation classification",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), volume 33, pages 6407\u20136414.",
            "year": 2019
        },
        {
            "authors": [
                "Tianyu Gao",
                "Xu Han",
                "Hao Zhu",
                "Zhiyuan Liu",
                "Peng Li",
                "Maosong Sun",
                "Jie Zhou"
            ],
            "title": "2019b. Fewrel 2.0: Towards more challenging few-shot relation classification",
            "venue": "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing",
            "year": 2019
        },
        {
            "authors": [
                "Matthew R. Gormley",
                "Mo Yu",
                "Mark Dredze."
            ],
            "title": "Improved relation extraction with feature-rich compositional embedding models",
            "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1774\u20131784.",
            "year": 2015
        },
        {
            "authors": [
                "Jiale Han",
                "Bo Cheng",
                "Wei Lu."
            ],
            "title": "Exploring task difficulty for few-shot relation extraction",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 2605\u20132616, Online and Punta Cana, Dominican Republic. Asso-",
            "year": 2021
        },
        {
            "authors": [
                "Xu Han",
                "Hao Zhu",
                "Pengfei Yu",
                "Ziyun Wang",
                "Yuan Yao",
                "Zhiyuan Liu",
                "Maosong Sun."
            ],
            "title": "Fewrel: A large-scale supervised few-shot relation classification dataset with state-of-the-art evaluation",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods",
            "year": 2018
        },
        {
            "authors": [
                "Iris Hendrickx",
                "Su Nam Kim",
                "Zornitsa Kozareva",
                "Preslav Nakov",
                "Diarmuid \u00d3 S\u00e9aghdha",
                "Sebastian Pad\u00f3",
                "Marco Pennacchiotti",
                "Lorenza Romano",
                "Stan Szpakowicz"
            ],
            "title": "Semeval-2010 task 8: Multiway classification of semantic relations between pairs",
            "year": 2010
        },
        {
            "authors": [
                "Mengting Hu",
                "Shiwan Zhao",
                "Honglei Guo",
                "Chao Xue",
                "Hang Gao",
                "Tiegang Gao",
                "Renhong Cheng",
                "Zhong Su."
            ],
            "title": "Multi-label few-shot learning for aspect category detection",
            "venue": "Proceedings of the 59th",
            "year": 2021
        },
        {
            "authors": [
                "Shantanu Kumar."
            ],
            "title": "A survey of deep learning methods for relation extraction",
            "venue": "arXiv preprint arXiv:1705.03645.",
            "year": 2017
        },
        {
            "authors": [
                "Steinar Laenen",
                "Luca Bertinetto."
            ],
            "title": "On episodes, prototypical networks, and few-shot learning",
            "venue": "Advances in Neural Information Processing Systems (NeurIPS), 34.",
            "year": 2021
        },
        {
            "authors": [
                "Fangchao Liu",
                "Hongyu Lin",
                "Xianpei Han",
                "Boxi Cao",
                "Le Sun."
            ],
            "title": "Pre-training to match for unified lowshot relation extraction",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 5785\u2013",
            "year": 2022
        },
        {
            "authors": [
                "Raymond Mooney",
                "Razvan Bunescu."
            ],
            "title": "Subsequence kernels for relation extraction",
            "venue": "Advances in neural information processing systems (NeurIPS),",
            "year": 2005
        },
        {
            "authors": [
                "Abiola Obamuyide",
                "Andreas Vlachos"
            ],
            "title": "Modelagnostic meta-learning for relation classification with limited supervision",
            "venue": "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (ACL),",
            "year": 2019
        },
        {
            "authors": [
                "Meng Qu",
                "Tianyu Gao",
                "Louis-Pascal Xhonneux",
                "Jian Tang."
            ],
            "title": "Few-shot relation extraction via bayesian meta-learning on relation graphs",
            "venue": "Proceedings of the 37th International Conference on Machine Learning (ICML), pages 7867\u20137876.",
            "year": 2020
        },
        {
            "authors": [
                "Ofer Sabo",
                "Yanai Elazar",
                "Yoav Goldberg",
                "Ido Dagan."
            ],
            "title": "Revisiting few-shot relation classification: Evaluation data and classification schemes",
            "venue": "Transactions of the Association for Computational Linguistics (TACL), 9:691\u2013706.",
            "year": 2021
        },
        {
            "authors": [
                "Jake Snell",
                "Kevin Swersky",
                "Richard Zemel."
            ],
            "title": "Prototypical networks for few-shot learning",
            "venue": "Advances in neural information processing systems (NeurIPS), 30.",
            "year": 2017
        },
        {
            "authors": [
                "Ming Tan",
                "Yang Yu",
                "Haoyu Wang",
                "Dakuo Wang",
                "Saloni Potdar",
                "Shiyu Chang",
                "Mo Yu."
            ],
            "title": "Out-ofdomain detection for low-resource text classification tasks",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing",
            "year": 2019
        },
        {
            "authors": [
                "Laurens Van der Maaten",
                "Geoffrey Hinton."
            ],
            "title": "Visualizing data using t-sne",
            "venue": "Journal of machine learning research (JMLR), 9(11).",
            "year": 2008
        },
        {
            "authors": [
                "Zhi-Xiu Ye",
                "Zhen-Hua Ling."
            ],
            "title": "Multi-level matching and aggregation network for few-shot relation classification",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (ACL), pages 2872\u20132881.",
            "year": 2019
        },
        {
            "authors": [
                "Dmitry Zelenko",
                "Chinatsu Aone",
                "Anthony Richardella."
            ],
            "title": "Kernel methods for relation extraction",
            "venue": "Journal of machine learning research (JMLR), 3(Feb):1083\u20131106.",
            "year": 2003
        },
        {
            "authors": [
                "Daojian Zeng",
                "Kang Liu",
                "Siwei Lai",
                "Guangyou Zhou",
                "Jun Zhao."
            ],
            "title": "Relation classification via convolutional deep neural network",
            "venue": "Proceedings of the 28th International Conference on Computational Linguistics (COLING), pages 2335\u20132344.",
            "year": 2014
        },
        {
            "authors": [
                "Ningyu Zhang",
                "Shumin Deng",
                "Zhanling Sun",
                "Xi Chen",
                "Wei Zhang",
                "Huajun Chen."
            ],
            "title": "Attention-based capsule networks with dynamic routing for relation extraction",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
            "year": 2018
        },
        {
            "authors": [
                "Yuhao Zhang",
                "Victor Zhong",
                "Danqi Chen",
                "Gabor Angeli",
                "Christopher D Manning."
            ],
            "title": "Position-aware attention and supervised data improve slot filling",
            "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
            "year": 2017
        },
        {
            "authors": [
                "Peng Zhou",
                "Wei Shi",
                "Jun Tian",
                "Zhenyu Qi",
                "Bingchen Li",
                "Hongwei Hao",
                "Bo Xu."
            ],
            "title": "Attention-based bidirectional long short-term memory networks for relation classification",
            "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational",
            "year": 2016
        },
        {
            "authors": [
                "Following Gao"
            ],
            "title": "2019b), we exploit the same hyper-parameters. For a fair comparison, all baselines and our work use the same encoder",
            "venue": "We choose BERT-base-uncased (Devlin et al.,",
            "year": 2019
        },
        {
            "authors": [
                "Liu"
            ],
            "title": "2022), while the sampling method in the testing is fixed sampling or random sampling",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Relation classification is a fundamental task in natural language processing field, aiming to recognize the relation between two entities in a given sentence. Recently, many works formulate this task in the few-shot learning scenario (Dong et al., 2020; Qu et al., 2020; Han et al., 2021; Brody et al., 2021). They focus on obtaining fast adaptation with a few samples by learning multiple episodes. Usually, each episode is composed of a support set and a query set, where the support set has N -way Kshot instances. As an example shown in Table 1, the support set provides two known relations, i.e. \u201ccontains\u201d and \u201cfounded by\u201d. The first query instance has \u201cfounded by\u201d relation, which can be\n\u2217Mengting Hu is the corresponding author. 1The code is released on GitHub at https://github.\ncom/Pisces-29/DProto.\nrecognized by reference to this support set. But for the second query instance, the relation between \u201cEinstein\u201d and \u201c1879\u201d is \u201cborn in\u201d. It cannot find a reference from the support set.\nGao et al. (2019b) define such relation as noneof-the-above (NOTA) and propose a new task setting. Its challenge relies on the accurate detection of both the known instances and NOTA ones. Previous works extend N -way to (N+1)-way, regarding NOTA relation as an extra class. Then the problem is further solved by classification (Gao et al., 2019b), or computing distance with pre-defined learnable vectors (Sabo et al., 2021), or multiple choice (Liu et al., 2022). Despite their popularity and superiority, they only focus on detecting NOTA effectively rather than refining the distribution of NOTA instances, leaving the overall distribution of all instances ignored. However, NOTA is not a simple (N + 1)-th class, which tends to be outliers for the known N relations. As an illustration, MNAV may lead some NOTA instances to cluster around a particular vector. However, the clustering of these instances from different categories together is unnatural. Therefore, motivated by this, we study from the view of instance distribution.\nIn this paper, our initial research question is what\ninstances distribution is ideal. For known samples, intra-class needs to be well clustered and interclass is clearly separated. And NOTA samples, without gathering, should be distributed away from all known instances. In light of these properties, we propose a density-aware prototypical network (D-Proto) that takes into account the density of different types of instances. Specifically, to guarantee the ideal instance distribution of known instances, we introduce neighborhood component analysis (NCA) (Laenen and Bertinetto, 2021), which involves all sample pairs out of the support and query sets. For NOTA instances, we propose a NOTA loss, which aims at pushing them away from all non-NOTA ones. By making NOTA instances isolated, both of them become more distinguishable in the embedding space.\nMoreover, the ideal distribution will lead to distinct densities for NOTA and known samples. That is to say, known instances are distributed densely since the intra-class is clustered together. NOTA\u2019s density is small as it is isolated from others. Therefore, we propose a NOTA detection module by fully enhancing the density properties. Concretely, to promote the density of known instances, we design intra-class prototype enhancement. By adding prototypes and degenerative prototypes, their densities could be further enlarged. Based on this, we propose a prototype enhanced local outlier factor (PLOF) to calculate the outlier extent of an instance through distances with its neighbors. Finally, we propose a base score that uses the extremal distance ratio between query samples and N prototypes. By fusing PLOF and base score, the final NOTA score is obtained to accurately discriminate between NOTA instances and known ones.\nIn summary, the main contributions of our work are as follows:\n\u2022 We propose D-Proto to address the more challenging few-shot relation classification task. To the best of our knowledge, this work is the first to model the overall instance distribution in an episode.\n\u2022 We introduce the NCA objective and propose a NOTA loss to achieve ideal instance distribution. Based on its ideality, we further propose a prototype enhanced local outlier factor (PLOF) and a base score to fully leverage the density properties.\n\u2022 Extensive experimental results demonstrated\nthe effectiveness and robustness of our approach on both fixed and random episode sampling with various NOTA rates."
        },
        {
            "heading": "2 Methodology",
            "text": ""
        },
        {
            "heading": "2.1 Formulation and Method Overview",
            "text": "Given the training set Dtrain, we sample multiple episodes, where each of them is composed of {S,Q,R}. The relation set R = {r1, r2, ..., rN} mentions N relations. The support set S follows N -way K-shot setting, which contains N relations, and each relation ri has K instances.\nS = {(sij , rij)}, 1 \u2264 i \u2264 N, 1 \u2264 j \u2264 K\nwhere sij denotes the j-th sample belonging to the relation ri. The query set Q is composed of two kinds of instances: known instances, which belongs to the relation set R of the support set; NOTA instances, without references from the support set.\nQ = {(q1, r1), ..., (qn, rn), (q1, r), ..., (qm, r)}\nwhere {(qj , rj)}nj=1 are the known instances and {(qj , r)}mj=1 are the NOTA instances. Here r denotes the NOTA relation. The query set Q is predicted with the help of support set S. Our goal is to recognize all samples in Q accurately.\nThe proposed approach is depicted in Figure 1. A sample is encoded with BERT E(\u00b7) (Devlin et al., 2019), yielding the vector representation of [CLS]. In the embedding space, we design two training objectives: 1) known sample separation Lnca aims to increase the separability of known samples by reducing intra-class distances and enlarging interclass distances; 2) NOTA sample isolation Lnota\npush NOTA samples to be distinct from known ones, making them more easily identifiable as outliers. Ultimately, our model is optimized by the combination of two training objectives.\nL = Lnca \u2212 \u03b1Lnota (1)\nwhere \u03b1 is a hyper-parameter to balance the contribution of NOTA loss.\nIn the testing phase, we propose NOTA detection module, fusing PLOF and base score into NOTA score. Each sample\u2019s NOTA score then reveals its outlier extent. NOTA samples will be easily discriminated from known samples. Next, we will introduce each module in detail."
        },
        {
            "heading": "2.2 Known Sample Separation",
            "text": "Although considering the overall instance distribution is intuitive, its ideal is not easy to attain in the few-shot scenario. Due to the limitation of samples, a sample and its neighbors are all important for quantifying density. Therefore, we introduce neighborhood component analysis (NCA) loss (Laenen and Bertinetto, 2021). It considers the distances between all sample pairs from both the support set and the query set. In other words, K samples of some relation are pulled together interactively and away from all other K \u00d7 (N \u2212 1) samples.\nAssume all instances in an episode are B = {(xi, ri)|(xi, ri) \u2208 {S \u222a Q}, 1\u2264i\u2264b}, then all the known instances are defined as Bk = {(xi, ri)|(xi, ri) \u2208 B, ri \u0338= r}, where xi is a instance, ri is its relation, r represents the NOTA relation. NCA loss is formulated as below.\nLnca = \u22121 |Bk| \u2211 i\u22081,...b ri \u0338=r log\n \u2211 j\u22081,..,b j \u0338=i ri=rj exp(\u2212d2(xi,xj))\n\u2211 k\u22081,...,b k \u0338=i rk \u0338=r exp(\u2212d2(xi,xk))\n (2)\nwhere xi = E(xi) is the vector representation for the instance xi. d(\u00b7, \u00b7) represents the Euclidean distance. |Bk| indicates the number of samples in Bk. With Lnca, the intra-class is clustered together and the inter-class is away from each other, making a good distribution for the density of known samples."
        },
        {
            "heading": "Adding prototypes",
            "text": ""
        },
        {
            "heading": "2.3 NOTA Sample Isolation",
            "text": "Since NOTA samples are outliers for N known relation clusters, they should be isolated from known ones. Meanwhile, if a query set has multiple NOTAs, they might belong to different relations like the second query instance of Table 1. Aggregating NOTA samples into the (N + 1)-th class may cause a negative impact on distinguishing relations. Therefore, we propose NOTA loss to push each NOTA instance away from all the known samples. The NOTA instances are denoted as Bnota = {(xi, ri)|(xi, ri) \u2208 B, ri = r}. Lnota is defined as below.\nLnota = \u22121 |Bnota| \u2211\ni\u22081,...,b ri=r\nlog\n \u2211 j\u22081,...,b rj \u0338=r exp(\u2212d2(xi,xj))  (3)\nIn this way, each NOTA sample is isolated from all others, which is beneficial for detecting known relations and NOTA via density."
        },
        {
            "heading": "2.4 NOTA Detection",
            "text": "To further quantify densities accurately for fewshot samples, we promote the testing phase with PLOF and base score. They are aggregated into the final NOTA score, which indicates an instance\u2019s outlier degree through its nearest instances and prototypes. Next, we will introduce them individually.\nPrototype Enhanced Local Outlier Factor (PLOF) Different from the naive prototypical network (Snell et al., 2017), we compute the sample pair distances as Eq. (2). However, during inference, we argue that the prototypes can be leveraged to enlarge the density. Therefore, as depicted in Figure 2, we first calculate the prototype for each relation as below and expand the relation\u2019s vector set with its prototype.\ncr i =\n1\nK K\u2211 j=1 E(sij) (4)\nTo further boost the density and improve the identification of NOTA samples, we propose a special type of prototype, referred to as a \"degenerative prototype\", which is computed with a degenerative set of instances.\ncr i m = 1 K \u2212 1 \u2211\nj=1,...,K j \u0338=m\nE(sij), 1 \u2264 m \u2264 K (5)\nIntuitively, each degenerative prototype cr i\nm is a vector obtained by averaging (K \u2212 1) support embeddings belonging to its relation ri after removing a support instance sim. As shown in Figure 2, the augmentation of prototypes and degenerative prototypes significantly increases the density of known instances and the outlier status of NOTAs, thus improving the detection.\nBefore calculating the PLOF of each instance, the original instance set B is expanded to Bplof = B \u222a P \u222a DP .\nP = {cri}, 1 \u2264 i \u2264 N\nDP = {crim}, 1 \u2264 i \u2264 N, 1 \u2264 m \u2264 K\nwhere P is the prototype set and DP is the degenerative prototype set.\nThen given a positive integer k (k < |Bplof |), the k-distance dk(q) of an vector q, is defined as that for an instance o \u2208 Bplof\\{q}, there are at least (k \u2212 1) instances v of which holds that d(q,v) < d(q,o), where v \u2208 Bplof\\{q,o}. Here the symbol \\ indicates except. After finding such instance o, dk(q) = d(q,o). The Euclidean distance between q and all the chosen instances are less than or equal to dk(q). As shown in Figure 3, dashed circles surround the chosen instances for each target q. These instances are called the k-neighborhood set of q, denoted as Nk(q) = {v \u2208 Bplof\\{q}|d(q,v) \u2264 dk(q)}. Based on k-neighborhood, the local reachability distance lrd is defined as follows:\nlrd(q) = |Nk(q)|\u2211\no\u2208Nk(q)max{dk(o), d(q,o)} (6)\nObviously, lrd(q) is the inverse of the average reachability distance based on the k-neighborhood of q. Based on it, we further define PLOF as the average of the ratio of the local reachability density of q and those of q\u2019s k-neighborhood.\nPLOF (q) =\n\u2211 v\u2208Nk(q) lrd(v) lrd(q)\n|Nk(q)| (7)\nBy considering the neighbors, PLOF better reflects the density. Meanwhile, the higher the PLOF value of q, the more probable that q is an outlier (e.g. a NOTA sample).\nBase Score Query instances are recognized with the help of support prototypes. We observe an interesting property on the distances between the query instances and support prototypes. As depicted in Figure 4, the non-NOTA query is close to its own prototype but far away from the other prototypes. Yet the phenomenon is different for the\nNOTA query, which is far away from all prototypes. This occurs because the proposed NOTA loss Lnota pushes NOTA instances away from those known instances. Based upon this distance property, we propose a base score, which calculates the ratio of extremal distance to prototypes.\nScorebase(q) =\nmin i=1,...,N\nd2(q, cr i )\nmax i=1,...,N\nd2(q, cr i )\n(8)\nThis definition could also help to distinguish non-NOTA and NOTA samples. The base score of a known query is smaller, while it is larger for a NOTA query.\nNOTA Score Finally, the NOTA score is the combination of PLOF and base score.\nScorenota(q) = PLOF (q) \u00b7 Scorebase(q) (9)\nDuring inference, a query instance can be detected by comparing its NOTA score with a threshold \u03c4 . Since the proposed NOTA score is welldesigned, using this score can more effectively recognize instances. In the real application, the threshold can be estimated with k-fold validation. In our experiments, we choose a fixed threshold for simplification."
        },
        {
            "heading": "3 Experiments",
            "text": ""
        },
        {
            "heading": "3.1 Datasets",
            "text": "FewRel 1.0 (Han et al., 2018) is a relation classification dataset containing 100 relations extracted from Wikipedia. Gao et al. (2019b) update it to FewRel 2.0 and propose the NOTA detection challenge. Our experiments follow the splits used in official FewRel benchmarks, which split the dataset into 64 classes for training, 16 for validation, and 20 for testing."
        },
        {
            "heading": "3.2 Episode Sampling",
            "text": "In \u00a72.1 we introduce the N -way K-shot setting. Each episode sampling support set follow the N - way K-shot setting. But there are two ways to sample a query set.\n\u2022 Fixed Sampling Gao et al. (2019b) propose fixing the NOTA rate of a query set to some specific value. For example, if a query set has 5 known queries and 5 NOTA ones, the NOTA rate of the query set would be calculated as 5/(5 + 5)\u00d7 100% = 50%. Then the total number of instances in that query set will be adjusted based on the NOTA rate.\n\u2022 Random Sampling Sabo et al. (2021) propose to fix the number of instances in the query set and assign a probability p to each instance for sampling. If p is less than a threshold \u03c4na, the instance is sampled as a known query. Otherwise, it is sampled as a NOTA. As such, the value of \u03c4na determines the NOTA rate. This sampling strategy results in a dynamic NOTA rate for each episode. For example, some episodes may have a NOTA rate of 100%, while others may not have any NOTA instances, resulting in a more realistic simulation of real-world scenarios."
        },
        {
            "heading": "3.3 Baseline Methods",
            "text": "The following baseline methods are chosen for comparison. O-Proto (Tan et al., 2019) aims to solve few-shot out-of-domain detection. It is based on the prototypical network, which detects NOTA by cosine similarity. BERT-Pair (Gao et al., 2019b) adopts BERT to match a query instance and a support instance, which then yields a score indicating whether they share the same relation. MNAV (Sabo et al., 2021) is also based on the prototypical network. NOTA class is represented by some learnable vectors. A query instance is evaluated by its similarity with the NOTA vectors in the embedding space. MCMN (Liu et al., 2022) uses a pre-training and fine-tuning paradigm that converts all candidate relation descriptions into multiplechoice prompts."
        },
        {
            "heading": "3.4 Experimental Results",
            "text": ""
        },
        {
            "heading": "3.4.1 Overall Results",
            "text": "The results of two sampling strategies are presented in Table 2. It can be observed that D-Proto outperforms strong baselines and achieves the best average results on both sampling strategies. Especially, D-Proto significantly outperforms MCMN by +2.75% on accuracy and +2.66% on F1 under random sampling. The results indicate the effectiveness of D-Proto, with great stability and robustness. As mentioned in \u00a73.2, random sampling better reflects practical applications. It can be seen that under fixed sampling, the proposed D-Proto achieves competitive results. And under random sampling, the improvements are more significant. This implies that D-Proto is more robust in realworld situations.\nMoreover, under random sampling, we see that the accuracy and F1 of MCMN decrease by 13.36% and 16.05%, respectively when the NOTA rate is\nset from 15% to 50%. On the contrary, D-Proto only sees decreases of 2.46% and 2.01%, respectively. A possible reason is that MCMN needs to meta-adapt with the support set. Yet a support set only has N -way K-shot samples, while NOTA exists in the query set. This tends to cause overfitting on the known instances but fails to detect NOTA. This further demonstrates the effectiveness and robustness of the proposed D-Proto model toward various NOTA rates.\nFurthermore, we evaluate the performance of the ChatGPT2 dialog system in the NOTA challenge. As demonstrated in Table 2, ChatGPT\u2019s performance on the NOTA task is far below other models. Therefore, we conclude that the model targeting the NOTA challenge in few-shot relation classification is still necessary. More details can be found in Appendix A.9."
        },
        {
            "heading": "3.4.2 NOTA Score Analysis",
            "text": "We further investigate the scores computed by DProto, which are depicted through box line diagrams in Figure 5. We first observe that the base scores alone are able to distinguish between known instances and NOTA ones, but the numerical values for the two classes are close. Specifically, the base scores of NOTA instances are generally greater than 0.4, while those of non-NOTA samples3 are\n2https://chat.openai.com 3In the context of the section, non-NOTA samples can be\nconsidered equivalent to known ones.\naround 0.1. This leads to difficulty in choosing a good threshold since the range of proper thresholds is small. Thus, base scores alone may not be sufficient to distinguish between the two classes accurately.\nSecondly, without adding prototypes and degenerative prototypes, we have the naive local outlier factor (LOF). It can be seen that both LOF and PLOF show numerical overlap for both non-NOTA and NOTA instances. However, in LOF, the scores for non-NOTA instances are centered around 0.9, while the scores for NOTA instances range from 1.0 to 1.2. The available range of threshold is limited. Contrarily, in PLOF, the scores for non-NOTA samples are in the range of 2 to 3, while the scores for NOTA instances are significantly larger than 4.\nFinally, combining PLOF and base NOTA score forms our final NOTA score. It is found that the NOTA score can make two types of instances have a clear difference. The gap between medians becomes more obvious, and the NOTA score for the NOTA instance becomes larger. These all contribute to the detection of NOTA instances, showing the effectiveness of our proposed NOTA score."
        },
        {
            "heading": "3.4.3 Ablation Study",
            "text": "To explore the impact of individual training objectives, we perform an ablation study. The results are reported in Table 3. It can be seen that removing Lnota causes significant decreases in all evaluation metrics. This verifies that NOTA loss effectively\nmakes NOTA isolated, helping to recognize relations with densities.\nThen removing the Base Score significantly impacts the model\u2019s performance. The Base Score is essential as it serves as the basis of PLOF. It can also be observed that the removal of either the prototype set P or the degenerative prototype set DP results in a significant decline in the evaluation metrics across different NOTA rates. Additionally, the removal of both P and DP results in an even more significant drop in the metrics. These findings suggest that both the prototype set and the degenerative prototype set are important components of the PLOF method and contribute significantly to its performance."
        },
        {
            "heading": "3.4.4 Degenerative Prototypes Analysis",
            "text": "We have a research question: does adding more degenerative prototypes also increase the perfor-\nmance of the model? To answer this question, we design the K \u2212 2 degenerative prototype set DPK\u22122. DPK\u22122 consists of all K \u2212 2 degenerative prototypes, where each one is computed by averaging the embeddings of the K \u2212 2 support instances belonging to its relation ri after excluding two support instances. It is worth noting that D-Proto only adds the degenerative prototypes DPK\u22121. Will including DPK\u22122 bring further gains?\nIn this experiment, we add various proportions of DPK\u22122 into the support set. The results are displayed in the second part of Table 3. Our expectation is that DPK\u22122 will increase the density of known samples and thus improve the performance. However, the results of this experiment show that while the performance of the D-Proto method does improve when DPK\u22122 is added under a NOTA rate of 50%, the performance decreases for NOTA rates of 15% and 30%. This suggests that while the use of DPK\u22122 can indeed improve the detection of NOTA instances at higher densities, it also leads to the false detection of known instances as NOTA, particularly at lower NOTA rates."
        },
        {
            "heading": "3.4.5 Visualization",
            "text": "To further evaluate the representations learned by different models, we visualize the embeddings using the t-SNE algorithm (Van der Maaten and Hinton, 2008) in Figure 6. Under both the 15% and 30% NOTA rates, compared with O-Proto and MNAV, our method can make each relation cluster more densely. For instance, in Figure 6 (a), the relation 1 is scattered around in O-Proto and MNAV. However, in our method, this class is well aggregated. This shows that our method can learn more separable representations for known instances.\nMoreover, in Figure 6 (a), we see that the NOTA instance is far away from known samples in our method. This observation is visible in the plot (b). This shows the validity of the proposed NOTA loss Lnota, making NOTA instances more isolated. In MNAV, the two NOTA instances are close to a known sample. One possible reason is that MNAV makes multiple NOTA instances approach learnable vectors. Different classes may aggregate with each other. This tends to cause negative effects on distinguishing samples, as embeddings are not well separated. In summary, our method learns the best representations. With the well-clustered known samples and the isolated NOTA samples, D-Proto can both recognize them through the density-aware NOTA detection module accurately."
        },
        {
            "heading": "4 Related Work",
            "text": "Previous works (Zelenko et al., 2003; Mooney and Bunescu, 2005; Zeng et al., 2014; Gormley et al., 2015; Zhou et al., 2016; Kumar, 2017; Zhang et al., 2018) for relation classification usually train models on a labeled dataset with a fixed number of classes, which cannot deal with unseen relations.\nThus few-shot scenario has drawn much attention recently. By training on multiple episodes, models can learn transferrable knowledge for unseen testing episodes (Hu et al., 2021). Although many efforts (Hendrickx et al., 2010; Zhang et al., 2017; Obamuyide and Vlachos, 2019; Gao et al., 2019a; Baldini Soares et al., 2019; Ye and Ling, 2019; Dong et al., 2020) have been devoted to the fewshot relation classification. However, a real-world issue that has been proposed is the NOTA challenge (Gao et al., 2019b). This issue breaks the assumption of the traditional N -way K-shot setting. Therefore, detecting NOTA instances during inference needs to be explored.\nPromising works have been proposed to detect NOTA instances in few-shot relation classification. One approach that has been widely adopted is the N + 1 classification. This approach has been proposed by several works. One work is to use a sentence-pair model for classification, as proposed by Gao et al. (2019b) in their work on few-shot learning. Another work, Sabo et al. (2021) in their work on revisiting NOTA challenge, is to compute the distance between learnable vectors. The third one, proposed by Liu et al. (2022), adopts the pretraining fine-tuning paradigm and selects the correct relation from a set of options that includes NOTA. The above three methods, by considering NOTA instances as an additional class, are able to learn the boundary between the known relations and the NOTA instances, allowing the model to learn a more comprehensive representation of the data."
        },
        {
            "heading": "5 Conclusion",
            "text": "In this paper, we propose D-Proto, a density-aware prototypical network for few-shot relation classification. We focus on the overall instance distribution and design special training objectives for distinct samples. For the known samples, intraclass becomes well-clustered and inter-class is wellseparated. The NOTA ones are isolated as outliers. We further propose a NOTA detection module, which considers the local density and distance property, to distinguish both types of instances. And we use prototypes and degenerative prototypes to enhance the density of known instances to better identify NOTA. Experiments on a popular dataset demonstrate that D-Proto outperforms strong baselines. We also provide score analysis and visualization to verify the effectiveness of our method."
        },
        {
            "heading": "Limitations",
            "text": "In this work, we study the few-shot relation classification task, which contains NOTA instances. The proposed D-Proto method achieves some gains on this problem. Yet our work still has the following two limitations.\nThe first limitation is that NOTA instances are detected with a threshold \u03c4 . This threshold relies on human experiences or k-fold validation. However, using a threshold in the binary way methods, including O-Proto and ours, is not a big problem. Our viewpoint is that N + 1 classification can efficiently accomplish the NOTA detection task by treating the NOTA as an additional class to classify with the known N classes. But the N + 1 classification method ignores the distribution of the data. Therefore, despite that we need a threshold, the advantages of our method are also obvious.\nThe second limitation is that our experiment is not absolutely fair. The reason is that BERT-Pair (Gao et al., 2019b) requires high GPU memory. In the fixed sampling method of the episode, we used two NVIDIA RTX A6000 with 48G memory each to train BERT-Pair. The other baselines are trained with an NVIDIA TESLA V100-32G. Except for this difference, all other settings are the same."
        },
        {
            "heading": "Acknowledgment",
            "text": "We sincerely thank all the anonymous reviewers for providing valuable feedback. This work is supported by the National Natural Science Foundation of China (Grant No. 62302245), the Ministry of Education of the People\u2019s Republic of China Humanities and Social Sciences Youth Foundation (Grant No. 23YJCZH240), and the youth program of National Science Fund of Tianjin, China (Grant No. 22JCQNJC01340)."
        },
        {
            "heading": "A Reproducibility",
            "text": "A.1 Implementation Details\nFollowing Gao et al. (2019b), we exploit the same hyper-parameters. For a fair comparison, all baselines and our work use the same encoder. We choose BERT-base-uncased (Devlin et al., 2019) as the encoder. It is initialized by the pre-trained parameters and optimized during training. For the FewRel dataset, we set the batch size to 2, which means we feed two episodes into the model per batch. Each model is trained for 30 epochs with 1000 batches per epoch and tested with 10000 batches in the testing. An early stopping strategy is adopted, indicating the model will stop training when the performance on the validation set does not improve for 6 epochs. The best model on the validation set is saved for evaluation.\nIn Eq. (1), we set the hyper-parameters \u03b1 to 1e-5 on the FewRel. For k of the local outlier factor in Eq. (7), we set it to be K. The reason is that in the N -way K-shot setting, a support instance has other (K \u2212 1) neighbors with the same relation. And each relation has at least one prototype. This is consistent with the definition of k-neighborhood. All the reported results using the FewRel dataset are the average of five runs with fixed seeds [5, 10, 15, 20, 25].\nAfter the hyperparameter search, the threshold \u03c4 is set to 0.9 in the experimental setting of FewRel.\nFor MCMN, the sampling method in the pre-\ntraining follows Liu et al. (2022), while the sampling method in the testing is fixed sampling or random sampling."
        },
        {
            "heading": "A.2 Computing Infrastructure",
            "text": "Since BERT-Pair (Gao et al., 2019b) requires high GPU memory in the fixed sampling of 5-way 5- shot setting, we use 2 NVIDIA A6000s with 48G memory each to train the model. The rest of the baselines and our model are trained on a NVIDIA TESLA V100 with 32G of GPU memory."
        },
        {
            "heading": "A.3 Number of Parameters per Model",
            "text": "Table 5 shows the number of hyperparameters for each model."
        },
        {
            "heading": "A.4 Evaluation Metrics",
            "text": "We use two primary evaluation metrics, accuracy and macro F1. The basic implementation of accu-\nracy is as follows:\nAccuracy = TP + TN\nTP + FP + TN + FN\nwhere TP are true positives, FP are false positives, TN are true negatives, and FN are false negatives. The definition of F1 is as follows:\nP = TP\nTP + FP\nR = TP\nTP + FN\nF1 = 2\u00d7 P \u00d7R P +R\nMacro F1 calculates F1 for each class and finds their unweighted mean."
        },
        {
            "heading": "A.5 Hyperparameters",
            "text": "Following Gao et al. (2019b), the hyperparameters not mentioned in Section A.1 are as follows:\n\u2022 Learning rate: 2e-5.\n\u2022 Weight decay: 0.01.\n\u2022 Warmup steps: 300.\n\u2022 Gradient accumulation: 1.\n\u2022 Max length: 128.\n\u2022 Hidden size: 768.\n\u2022 Test batch: 10000."
        },
        {
            "heading": "A.6 Links to Dataset",
            "text": "FewRel (Han et al., 2018): https://thunlp. github.io/2/fewrel2_nota.html.\nA.7 Links to Toolkit\n\u2022 BERT: https://huggingface.co/ transformers/v4.5.1/model_doc/bert. html#bertmodel\n\u2022 BERT Tokenizer: https://huggingface. co/transformers/v4.5.1/model_doc/ bert.html#berttokenizer\n\u2022 Local Outlier Factor(LOF): https: //scikit-learn.org/stable/modules/ generated/sklearn.neighbors. LocalOutlierFactor.html\n\u2022 Macro F1: https://scikit-learn.org/ stable/modules/generated/sklearn. metrics.f1_score.html"
        },
        {
            "heading": "A.8 Number of Instances per Method",
            "text": "Table 6 shows the number of instances included in an episode for each method in ablation studying. The episode sampling method is random sampling."
        },
        {
            "heading": "A.9 Prompt of ChatGPT",
            "text": "We demonstrate the prompt for using ChatGPT directly in the NOTA task, as follows:\nQuestion: The purpose of the relation classification task is to identify the relation between two entities (one is the head entity and the other is the tail entity) in a given sentence. For example, the following sentence \"California is a state in the United States\". Through the derivation of the whole sentence, the relation between the head entity \"California\" and the tail entity the \"United States\" is \"contains\". While the few-shot relation classification task consists of a support set and a query set. The support set generally contains N relations, and each relation contains K instances. This setup is called N -way K-shot. We need to predict the relation between two entities in each sentence given in the query set based on the support set. Now I need you to complete a few-shot relation classification task, which is set to 5-way 5-shot. The five categories in the support set are numbered from 0 to 4, respectively. Five sentences are given after\neach category. The head entity of each sentence is surrounded by two special characters [head] and [/head], and the tail entity is surrounded by two special characters [tail] and [/tail]. You need to predict the relationship between two entities for each sentence in the query set from the support set. That is, if the relation of a query instance is similar to that of all instances of category 0 in the support set, then the category predicted by the query instance is category 0. Notice! If the relation between the two entities of a sentence in the query set cannot be referenced in the support set, the relation between the two entities of the sentence is called NOTA (None-of-the-above). When the NOTA relation is encountered, the category number of the predicted result is 5. Warning! You just need to return the class number for each instance predicted in the query set. For example, the relation of the first instance in the query set is relation 0, and the second instance is relation 5 (NOTA). Then your reply is: [0, 5]. If you get it, then I\u2019ll give you support sets and query sets next. ChatGPT: Yes, I understand the task. Please provide me with the support set and query set. Question: The following is the support set (5-way 5-shot. Instances in each category are separated by a special character [sep].): ...... The following is the query set (The query set contains one instance.): ......"
        }
    ],
    "title": "Density-Aware Prototypical Network for Few-Shot Relation Classification",
    "year": 2023
}