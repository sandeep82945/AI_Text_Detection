{
    "abstractText": "Unlike recurrent models, conventional wisdom has it that Transformers cannot perfectly model regular languages. Inspired by the notion of working memory, we propose a new Transformer variant named RegularGPT. With its novel combination of WeightSharing, Adaptive-Depth, and Sliding-DilatedAttention, RegularGPT constructs working memory along the depth dimension, thereby enabling efficient and successful modeling of regular languages such as PARITY. We further test RegularGPT on the task of natural language length extrapolation and surprisingly find that it rediscovers the local windowed attention effect deemed necessary in prior work for length extrapolation.",
    "authors": [
        {
            "affiliations": [],
            "name": "Ta-Chung Chi"
        },
        {
            "affiliations": [],
            "name": "Ting-Han Fan"
        },
        {
            "affiliations": [],
            "name": "Alexander I. Rudnicky"
        },
        {
            "affiliations": [],
            "name": "Peter J. Ramadge"
        }
    ],
    "id": "SP:b7ed8dae359c5ab4e6269c965ae81e2f138ed799",
    "references": [
        {
            "authors": [
                "Eryn J Adams",
                "Anh T Nguyen",
                "Nelson Cowan."
            ],
            "title": "Theories of working memory: Differences in definition, degree of modularity, role of attention, and purpose",
            "venue": "Language, speech, and hearing services in schools, 49(3):340\u2013355.",
            "year": 2018
        },
        {
            "authors": [
                "Cem Anil",
                "Yuhuai Wu",
                "Anders Johan Andreassen",
                "Aitor Lewkowycz",
                "Vedant Misra",
                "Vinay Venkatesh Ramasesh",
                "Ambrose Slone",
                "Guy Gur-Ari",
                "Ethan Dyer",
                "Behnam Neyshabur."
            ],
            "title": "Exploring length generalization in large language models",
            "venue": "In",
            "year": 2022
        },
        {
            "authors": [
                "Kristijan Armeni",
                "Christopher Honey",
                "Tal Linzen."
            ],
            "title": "Characterizing verbatim short-term memory in neural language models",
            "venue": "Proceedings of the 26th Conference on Computational Natural Language Learning (CoNLL), pages 405\u2013424, Abu",
            "year": 2022
        },
        {
            "authors": [
                "Alan Baddeley."
            ],
            "title": "Working memory",
            "venue": "Science, 255(5044):556\u2013559.",
            "year": 1992
        },
        {
            "authors": [
                "Alan D Baddeley",
                "Graham Hitch."
            ],
            "title": "Working memory",
            "venue": "Psychology of learning and motivation, volume 8, pages 47\u201389. Elsevier.",
            "year": 1974
        },
        {
            "authors": [
                "Iz Beltagy",
                "Matthew E. Peters",
                "Arman Cohan."
            ],
            "title": "Longformer: The long-document transformer",
            "venue": "arXiv:2004.05150.",
            "year": 2020
        },
        {
            "authors": [
                "Satwik Bhattamishra",
                "Kabir Ahuja",
                "Navin Goyal."
            ],
            "title": "On the Ability and Limitations of Transformers to Recognize Formal Languages",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods",
            "year": 2020
        },
        {
            "authors": [
                "Guy E Blelloch."
            ],
            "title": "Prefix sums and their applications",
            "venue": "School of Computer Science, Carnegie Mellon University Pittsburgh, PA, USA.",
            "year": 1990
        },
        {
            "authors": [
                "Ta-Chung Chi",
                "Ting-Han Fan",
                "Peter Ramadge",
                "Alexander Rudnicky."
            ],
            "title": "KERPLE: Kernelized relative positional embedding for length extrapolation",
            "venue": "Advances in Neural Information Processing Systems.",
            "year": 2022
        },
        {
            "authors": [
                "Ta-Chung Chi",
                "Ting-Han Fan",
                "Alexander I. Rudnicky",
                "Peter J. Ramadge."
            ],
            "title": "Dissecting transformer length extrapolation via the lens of receptive field analysis",
            "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics",
            "year": 2023
        },
        {
            "authors": [
                "David Chiang",
                "Peter Cholak."
            ],
            "title": "Overcoming a theoretical limitation of self-attention",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 7654\u20137664, Dublin, Ireland. Associa-",
            "year": 2022
        },
        {
            "authors": [
                "N. Chomsky."
            ],
            "title": "Three models for the description of language",
            "venue": "IRE Transactions on Information Theory, 2(3):113\u2013124.",
            "year": 1956
        },
        {
            "authors": [
                "Noam Chomsky."
            ],
            "title": "Three models for the description of language",
            "venue": "IRE Transactions on information theory, 2(3):113\u2013124.",
            "year": 1956
        },
        {
            "authors": [
                "Kevin Clark",
                "Urvashi Khandelwal",
                "Omer Levy",
                "Christopher D. Manning."
            ],
            "title": "What does BERT look at? an analysis of BERT\u2019s attention",
            "venue": "Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for",
            "year": 2019
        },
        {
            "authors": [
                "Nelson Cowan."
            ],
            "title": "Attention and memory: An integrated framework",
            "venue": "Oxford University Press.",
            "year": 1998
        },
        {
            "authors": [
                "R\u00f3bert Csord\u00e1s",
                "Kazuki Irie",
                "J\u00fcrgen Schmidhuber."
            ],
            "title": "The neural data router: Adaptive control flow in transformers improves systematic generalization",
            "venue": "International Conference on Learning Representations.",
            "year": 2022
        },
        {
            "authors": [
                "Zihang Dai",
                "Zhilin Yang",
                "Yiming Yang",
                "Jaime Carbonell",
                "Quoc Le",
                "Ruslan Salakhutdinov."
            ],
            "title": "Transformer-XL: Attentive language models beyond a fixed-length context",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computa-",
            "year": 2019
        },
        {
            "authors": [
                "Mostafa Dehghani",
                "Stephan Gouws",
                "Oriol Vinyals",
                "Jakob Uszkoreit",
                "Lukasz Kaiser."
            ],
            "title": "Universal transformers",
            "venue": "International Conference on Learning Representations.",
            "year": 2019
        },
        {
            "authors": [
                "Gregoire Deletang",
                "Anian Ruoss",
                "Jordi Grau-Moya",
                "Tim Genewein",
                "Li Kevin Wenliang",
                "Elliot Catt",
                "Chris Cundy",
                "Marcus Hutter",
                "Shane Legg",
                "Joel Veness",
                "Pedro A Ortega."
            ],
            "title": "Neural networks and the chomsky hierarchy",
            "venue": "International Confer-",
            "year": 2023
        },
        {
            "authors": [
                "Adele Diamond."
            ],
            "title": "Executive functions",
            "venue": "Annual review of psychology, 64:135\u2013168.",
            "year": 2013
        },
        {
            "authors": [
                "Maha Elbayad",
                "Jiatao Gu",
                "Edouard Grave",
                "Michael Auli."
            ],
            "title": "Depth-adaptive transformer",
            "venue": "International Conference on Learning Representations.",
            "year": 2020
        },
        {
            "authors": [
                "Jeffrey L Elman."
            ],
            "title": "Finding structure in time",
            "venue": "Cognitive science, 14(2):179\u2013211.",
            "year": 1990
        },
        {
            "authors": [
                "K Anders Ericsson",
                "Walter Kintsch."
            ],
            "title": "Longterm working memory",
            "venue": "Psychological review, 102(2):211.",
            "year": 1995
        },
        {
            "authors": [
                "Alex Graves",
                "Greg Wayne",
                "Ivo Danihelka."
            ],
            "title": "Neural turing machines",
            "venue": "arXiv preprint arXiv:1410.5401.",
            "year": 2014
        },
        {
            "authors": [
                "Michael Hahn."
            ],
            "title": "Theoretical limitations of selfattention in neural sequence models",
            "venue": "Transactions of the Association for Computational Linguistics, 8:156\u2013171.",
            "year": 2020
        },
        {
            "authors": [
                "Sepp Hochreiter",
                "J\u00fcrgen Schmidhuber."
            ],
            "title": "Long short-term memory",
            "venue": "Neural computation, 9(8):1735\u20131780.",
            "year": 1997
        },
        {
            "authors": [
                "Michael I Jordan."
            ],
            "title": "Serial order: A parallel distributed processing approach",
            "venue": "Advances in psychology, volume 121, pages 471\u2013495. Elsevier.",
            "year": 1997
        },
        {
            "authors": [
                "Richard E Ladner",
                "Michael J Fischer."
            ],
            "title": "Parallel prefix computation",
            "venue": "Journal of the ACM (JACM), 27(4):831\u2013838.",
            "year": 1980
        },
        {
            "authors": [
                "Sivaramakrishnan Lakshmivarahan",
                "Sudarshan K Dhall."
            ],
            "title": "Parallel computing using the prefix problem",
            "venue": "Oxford University Press.",
            "year": 1994
        },
        {
            "authors": [
                "Zhenzhong Lan",
                "Mingda Chen",
                "Sebastian Goodman",
                "Kevin Gimpel",
                "Piyush Sharma",
                "Radu Soricut."
            ],
            "title": "Albert: A lite bert for self-supervised learning of language representations",
            "venue": "International Conference on Learning Representations.",
            "year": 2020
        },
        {
            "authors": [
                "Bingbin Liu",
                "Jordan T. Ash",
                "Surbhi Goel",
                "Akshay Krishnamurthy",
                "Cyril Zhang."
            ],
            "title": "Transformers learn shortcuts to automata",
            "venue": "International Conference on Learning Representations.",
            "year": 2023
        },
        {
            "authors": [
                "Eric Martin",
                "Chris Cundy."
            ],
            "title": "Parallelizing linear recurrent neural nets over sequence length",
            "venue": "International Conference on Learning Representations.",
            "year": 2018
        },
        {
            "authors": [
                "Akira Miyake",
                "Priti Shah"
            ],
            "title": "Models of working memory",
            "year": 1999
        },
        {
            "authors": [
                "Aida Nematzadeh",
                "Sebastian Ruder",
                "Dani Yogatama."
            ],
            "title": "On memory in human and artificial language processing systems",
            "venue": "Proceedings of ICLR Workshop on Bridging AI and Cognitive Science.",
            "year": 2020
        },
        {
            "authors": [
                "Klaus Oberauer."
            ],
            "title": "Access to information in working memory: exploring the focus of attention",
            "venue": "Journal of Experimental Psychology: Learning, Memory, and Cognition, 28(3):411.",
            "year": 2002
        },
        {
            "authors": [
                "Antonio Orvieto",
                "Samuel L Smith",
                "Albert Gu",
                "Anushan Fernando",
                "Caglar Gulcehre",
                "Razvan Pascanu",
                "Soham De."
            ],
            "title": "Resurrecting recurrent neural networks for long sequences",
            "venue": "arXiv preprint arXiv:2303.06349.",
            "year": 2023
        },
        {
            "authors": [
                "Ofir Press",
                "Noah Smith",
                "Mike Lewis."
            ],
            "title": "Train short, test long: Attention with linear biases enables input length extrapolation",
            "venue": "International Conference on Learning Representations.",
            "year": 2022
        },
        {
            "authors": [
                "Alec Radford",
                "Jeffrey Wu",
                "Rewon Child",
                "David Luan",
                "Dario Amodei",
                "Ilya Sutskever"
            ],
            "title": "Language models are unsupervised multitask learners",
            "venue": "OpenAI blog,",
            "year": 2019
        },
        {
            "authors": [
                "Jack Rae",
                "Ali Razavi"
            ],
            "title": "2020a. Do transformers need deep long-range memory",
            "venue": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,",
            "year": 2020
        },
        {
            "authors": [
                "Jack Rae",
                "Ali Razavi."
            ],
            "title": "Do transformers need deep long-range memory? In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7524\u20137529, Online",
            "venue": "Association for Computational Linguistics.",
            "year": 2020
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J Liu."
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "The Journal of Machine Learning Research,",
            "year": 2020
        },
        {
            "authors": [
                "Antoine Simoulin",
                "Benoit Crabb\u00e9."
            ],
            "title": "How many layers and why? An analysis of the model depth in transformers",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint",
            "year": 2021
        },
        {
            "authors": [
                "Jimmy T.H. Smith",
                "Andrew Warrington",
                "Scott Linderman."
            ],
            "title": "Simplified state space layers for sequence modeling",
            "venue": "The Eleventh International Conference on Learning Representations.",
            "year": 2023
        },
        {
            "authors": [
                "A\u00e4ron van den Oord",
                "Sander Dieleman",
                "Heiga Zen",
                "Karen Simonyan",
                "Oriol Vinyals",
                "Alexander Graves",
                "Nal Kalchbrenner",
                "Andrew Senior",
                "Koray Kavukcuoglu."
            ],
            "title": "Wavenet: A generative model for raw audio",
            "venue": "Arxiv.",
            "year": 2016
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141 ukasz Kaiser",
                "Illia Polosukhin."
            ],
            "title": "Attention is all you need",
            "venue": "Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc.",
            "year": 2017
        },
        {
            "authors": [
                "Jason Wei",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Maarten Bosma",
                "brian ichter",
                "Fei Xia",
                "Ed H. Chi",
                "Quoc V Le",
                "Denny Zhou"
            ],
            "title": "2022. Chain of thought prompting elicits reasoning in large language models",
            "venue": "In Advances in Neural Information Processing Systems",
            "year": 2022
        },
        {
            "authors": [
                "Dani Yogatama",
                "Cyprien de Masson d\u2019Autume",
                "Lingpeng Kong"
            ],
            "title": "Adaptive Semiparametric Language Models. Transactions of the Association for Computational Linguistics, 9:362\u2013373",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "It is long believed that Working Memory (WM), a term coined in 1960s to liken human minds to computers, plays an important role in humans\u2019 reasoning ability and the guidance of decision-making behavior (Baddeley and Hitch, 1974; Baddeley, 1992; Ericsson and Kintsch, 1995; Cowan, 1998; Miyake et al., 1999; Oberauer, 2002; Diamond, 2013; Adams et al., 2018). While no single definition encompasses all applications of WM (Adams et al., 2018), the following one should be shared by all of the theories of interest:\nWorking memory is a system of components that holds a limited amount of information temporarily in a heightened state of availability for use in ongoing processing. - Adams et al. (2018)\nWM is instantiated in the two major driving forces of sequence modeling: Recurrent neural networks\u2019(RNN) (Elman, 1990; Jordan, 1997; Hochreiter and Schmidhuber, 1997) short term memory modulated by their recurrent nature and gate design (Rae and Razavi, 2020a; Nematzadeh\net al., 2020; Armeni et al., 2022), and Transformers\u2019 (Vaswani et al., 2017) salient tokens heightened by self-attention.\nIn reality, self-attention often attends broadly (Clark et al., 2019), violating the limited amount of information notion of WM. Our hypothesis is that such violation is to blame for Transformers\u2019 failure on algorithmic reasoning of regular languages (Deletang et al., 2023; Liu et al., 2023) such as PARITY, a seemingly simple task that checks if the number of 1s in a bit string is even. Surprisingly, a Transformer can only count the number of 1s correctly when the sequence length is held fixed at training sequence length Ttr, and it fails miserably when the testing sequence length extrapolates to Tex > Ttr (Hahn, 2020; Bhattamishra et al., 2020; Chiang and Cholak, 2022; Deletang et al., 2023; Liu et al., 2023). In contrast, an RNN can extrapolate perfectly.\nThe goal of this work is therefore to improve Transformers\u2019 WM by limiting the amount of accessible information at a time. Existing attempts that use a combination of scratchpad and recency biases (Wei et al., 2022; Nye et al., 2022; Anil et al., 2022; Liu et al., 2023) is not optimal as it completely foregoes the parallelization property of a Transformer, making it as computationally inefficient as an RNN.\nThis begs the question: Does there exist a more efficient Transformer working memory design? The answer is affirmative thanks to the proposed RegularGPT, which boils down to the three design choices: Weight-Sharing, Adaptive-Depth, and Sliding-Dilated-Attention; Each of them has been proposed previously but it is the unique combination that sparks the successful and efficient learning of regular languages. We will further demonstrate its: 1) similar recursive parallel structure as linear RNN (Orvieto et al., 2023), resulting in log Ttr or log Tex layers, and 2) generalizability by showing strong performance on the task of Transformer\nnatural language length extrapolation (Press et al., 2022; Chi et al., 2022, 2023).\nIn this work, we use [N ] to denote the list of non-negative integers [0, . . . , N \u2212 1]. The Transformer model used in this work is always causal. It takes in an input sequence of T \u2264 Ttr units (can be tokens or bits) \u03c3i\u2208[T ], passes them through a fixed amount of L transformer layers, and finally computes the distribution over the vocabulary V via the prediction head Wo."
        },
        {
            "heading": "2 Background",
            "text": ""
        },
        {
            "heading": "2.1 Regular Language and Algorithmic Reasoning",
            "text": "The Chomsky hierarchy (Chomsky, 1956b) classifies formal languages into different hierarchies based on their increasing complexity. Each hierarchy represents a family of formal languages that can be solved by the corresponding automaton. At the lowest level resides the family of regular languages, which can be expressed using a finite state automaton (FSA), a computational model comprising a set of states and transitions connecting them.\nOur primary objective is to enhance the algorithmic reasoning of the Transformer model on regular languages by testing its language transduction capability under the extrapolation setting. Concretely, the model is trained only to predict desired outputs on a set of short length-T sequences with T \u2264 Ttr. Still, it must also predict the correct outputs for longer testing sequences of length Tex Ttr. It is worth noting that we evaluate our model via language transduction following recent work (Deletang et al., 2023; Liu et al., 2023), instead of the conventional language recognition protocol. Both settings are equally hard as they are underpinned by the same finite state semiautomaton. Interested readers may refer to Deletang et al. (2023) for further details regarding the two evaluation protocols. We also reveal the connection between RegularGPT and finite state semiautomaton later in \u00a77."
        },
        {
            "heading": "2.2 Failure Mode and An Inefficient Fix",
            "text": "The PARITY task involves a length T bit string \u03c31\u03c32 \u00b7 \u00b7 \u00b7\u03c3T where each bit \u03c3i is randomly sampled from a Bernoulli distribution with P(\u03c3i = 1) = 0.5. The goal is to determine whether the sequence contains an even or odd number of 1s.\nIt has been observed that a Transformer is incapable of performing length extrapolation on PARITY, but what could be its potential failure mode?\nPrevious work sheds light on this by showing that a Transformer might settle on the naive-summation approach (Anil et al., 2022; Deletang et al., 2023; Liu et al., 2023). Concretely, it sums up all the bits and outputs the summation modulo 2. This approach fails since unseen summations will be produced when the model takes sequences of length Tex > T as input or P(Si) deviates from 0.5.\nTo the best of our knowledge, the existing remedy (Liu et al., 2023; Anil et al., 2022) is to use scratchpad (Wei et al., 2022; Nye et al., 2022) along with recency biases (Press et al., 2022) to enforce the correct learning: They create a scratchpad that interleaves the sequence of input bits and intermediate answers (\u03c31, q1, \u03c32, q2, \u00b7 \u00b7 \u00b7 , \u03c3T , qT ), where qi = solve(\u03c31 \u00b7 \u00b7 \u00b7\u03c3i). The model is trained to predict all the \u03c3i\u2208[T ]. Recency biases play the role of limiting a Transformer\u2019s receptive field to only a few most recent \u03c3 and q at every timestep i. This is to prevent self-attention from ignoring q and giving the same naive-summation solution.\nScratchpad and recency biases jointly create the notion of WM along the temporal dimension similar to RNNs, thereby enabling successful extrapolation on regular languages. Nevertheless, we note that this fix is inefficient during inference since all the intermediate answers qi have to be generated sequentially before reaching the final answer qT . A desirable fix should only take in the input bits (\u03c31, \u03c32, \u00b7 \u00b7 \u00b7 , \u03c3n) and directly generate the final answer qT . In other words, our goal is to find an efficient WM design for a Transformer."
        },
        {
            "heading": "2.3 A Desirable Fix for PARITY (Figure 1)",
            "text": "An alternative solution to the PARITY problem is based on the spirit of divide-and-conquer, where we first divide the sequence into T/C chunks with each chunk of length C < T , and we compose the final answer by recursively merging the chunk outputs. This approach does not suffer from the unseen summation issue as the model was trained to handle a fixed amount of C bits at a time in its WM (chunk). It then recursively applies the already-seen results to compose the final solution when it encounters longer sequences during inference. More importantly, it is more efficient than the scratchpad and recency biases approach since it only requires logC T layers of parallel computations instead of 2T steps of sequential decoding."
        },
        {
            "heading": "3 Proposed Architecture of RegularGPT",
            "text": "We present our modifications to the vanilla Transformer below. Only the related operations will be expanded, and we follow all the other details of GPT2 (Radford et al., 2019)."
        },
        {
            "heading": "3.1 Sliding-Dilated-Attention",
            "text": "A Transformer layer at layer l consists of a selfattention operation denoted as SA(l) and feedforward network denoted as FFN(l). Originally, SA(l) computes the inter-token relationships across all T units. Instead, we set the chunk size to C and produce T/C non-overlapping chunks;1 Only the units within the same chunk inter-attend with each other. In practice, this can be achieved by an attention mask M (l) \u2208 RT\u00d7T at layer l. M (l) shares the same shape as the self-attention matrix (see Figure 1) and is defined as:\nM (l)mn =\n{ r(m\u2212n)/C` , if m\u2212n Cl \u2208 [C]\n\u2212 inf, otherwise (1)\nNote that M is a lower triangular matrix due to the causal nature of our model. ri\u2019s with i \u2208 [C] are learnable relative positional scalars. To be precise, each attention head has a different set of learnable biases ri\u2019s. Here, we drop the dependency on the head for notational simplicity.\nThe use of ri\u2019s is similar to the positional scalars of T5 (Rae and Razavi, 2020a) except that we do not use the log-binning strategy over m\u2212 n. It is to facilitate the extraction of global information instead of enforcing the windowed-attention effect (Raffel et al., 2020; Press et al., 2022; Chi\n1Whenever T is not divisible by C, we pad the input sequence such that its length is a multiple of C.\net al., 2022, 2023). M will then be added to the original self-attention matrix, creating the proposed Sliding-Dilated-Attention effect. The output of SA(l) will be transformed by the positionalindependent FFN(l) to produce o(l)i\u2208[T ].\nThe case of C = 2 is used as a possible construction of Theorem 1 in Liu et al. (2023). However, their focus is not on length extrapolation, hence lacking the below two proposed modifications."
        },
        {
            "heading": "3.2 Adaptive-Depth and Weight-Sharing",
            "text": "Since our Sliding-Dilated-Attention limits the number of accessible tokens at a time, we need an adaptive depth L\u0304 = logC T so that the final output can utilize every single piece of input information. However, when Tex > Ttr, the depth during inference will be higher than that during training. The simplest way to solve this challenge without further parameter updating is to perform Weight-Sharing across layers. To account for the possible performance loss due to Weight-Sharing, we first thicken the model by K times, resulting in a total number of K \u00b7 L\u0304 layers. Next, we share the weights across the K \u00b7 L\u0304 layers in the following way for k \u2208 [K]:\nSA(l\u00b7K+k) =SA(k) for l \u2208 [L\u0304] FFN(l\u00b7K+k) =FFN(k) for l \u2208 [L\u0304]\nIt can be equivalently interpreted as stacking more SA and FFN components within every Transformer layer, and the same thickened layer is reused L\u0304 times. This layer thickening design is only used in the natural language modeling experiments in \u00a76."
        },
        {
            "heading": "3.3 Where is the WM Notion?",
            "text": "Instead of instantiating WM along the temporal dimension as the combination of scratchpad and\nrecency biases, RegularGPT limits the amount of information along the depth dimension. As we have seen, the idea of breaking T units into several chunks limits the amount of accessible information at each layer, thereby enabling the WM notion. A similar argument was made by Yogatama et al. (2021) in a sense that they categorized Longformer (Beltagy et al., 2020), a transformer variant with local attention pattern, as a model of working memory. Finally, thanks to modern accelerators such as GPU, all chunks at a layer can be processed concurrently, and this further makes RegularGPT more favorable over the scratchpad and recency biases approach."
        },
        {
            "heading": "3.4 Complexity Analysis",
            "text": "The sparse attention pattern of RegularGPT suggests it might enjoy the same speedup provided by sparsified Transformers. The complexity of our model is O(TCK logC T ) where TC is the complexity of each self-attention module andK logC T is the total number of layers. On the other hand, the vanilla Transformer follows O(T 2L). To illustrate the possible speedup, if T = 512 and C = 128, then 512 \u00b7 128 \u00b7K log128 512 < 5122L whenK < 512L128 log128 512 \u2248 3.11L. Namely, as long asK < 3L, our model is likely to be more efficient than a vanilla Transformer."
        },
        {
            "heading": "4 Connection to Prior Work",
            "text": "Sliding-Dilated-Attention This special attention pattern dates back to pre-Transformer era such as Wavenet (van den Oord et al., 2016) with dilated convolution. It can also be viewed as a special form of Longformer attention pattern with systematic dilation (Beltagy et al., 2020).2 Limiting the\n2The original Longformer also adopts dilated attention on a few heads at higher layers but without the systematic pattern\nrange of attention in lower layers of a Transformer is also corroborated in Rae and Razavi (2020b), where they find such design does not deteriorate the performance.\nAdaptive-Depth and Weight-Sharing ALBERT (Lan et al., 2020) and Universal Transformer (Dehghani et al., 2019) share the parameters across layers. The weight sharing design makes them compatible with the idea of Adaptive Computation Time (Graves et al., 2014) and Dynamic Halting (Dehghani et al., 2019; Elbayad et al., 2020), which allocate different computational budget depending on the complexity of tasks (Simoulin and Crabb\u00e9, 2021; Csord\u00e1s et al., 2022). However, they lack the special Sliding-Dilated-Attention design that is necessary for ruling out naive solutions.\nLinear RNN Given x0 = 0 \u2208 RN and the input vectors u1 \u00b7 \u00b7 \u00b7uT , a linear RNN (Orvieto et al., 2023) for k \u2208 [T ] can be written as:\nxk = Axk\u22121 +Buk = k\u22121\u2211 j=0 AjBuk\u2212j\n= k\u22121\u2211 j=0 Ajvk\u2212j ,\nwhere we set vk\u2212j = Buk\u2212j . The operation can be accelerated by the parallel scan algorithm that permits efficient cumulative sum (Ladner and Fischer, 1980; Blelloch, 1990; Lakshmivarahan and Dhall, 1994; Martin and Cundy, 2018; Liu et al., 2023; Smith et al., 2023). As we can see in Figure 2, the routing path specified by the parallel scan algorithm is the same as our Sliding-Dilated-Attention illustrated in Figure 1."
        },
        {
            "heading": "5 Regular Language Experiments",
            "text": ""
        },
        {
            "heading": "5.1 Task Descriptions",
            "text": "We focus on the four tasks in section 1) (Deletang et al., 2023) of Table 1 as they will also be used in our analysis in \u00a77. For tasks in section 2), please refer to Bhattamishra et al. (2020) for details.\nEven Pairs A model needs to predict whether the total count of \"ab\" and \"ba\" pairs is even. In the example of \"aabba\", there is one \"ab\" and one \"ba\", resulting in a total count of 2, which is even. This task is equivalent to checking whether the first and last characters in a string are identical.\nused in this work.\nModular Arithmetic Given a sequence of numbers in {0, 1, 2, 3, 4} and operations in {+, -, \u00b7}, a model needs to compute the result modulo 5. For example, x = 1 + 2\u2212 4 evaluates to y = 4.\nParity Check A model needs to compute whether the number of bs in a given binary string is even. For example, the sequence x = aaabba contains 2 bs, which is even.\nCycle Navigation Given a sequence of movements on a cycle of length 5, a model needs to compute the end position. The possible movements are STAY, INCREASE, DECREASE encoded as {0, 1, 2}. The agent always starts at position 0. For example, 010211 means the agent stops at position 2 = 0 + 1 + 0\u2212 1 + 1 + 1."
        },
        {
            "heading": "5.2 Language Transduction and Extrapolation",
            "text": "First, we want to know if endowing a Transformer with the notion of WM really improves its length extrapolation capability on regular languages. We test RegularGPT and all the baselines on two sets of regular languages from prior work (Deletang et al., 2023; Bhattamishra et al., 2020).3 Prior work often\n3Our implementation is based on the codebase of Deletang et al. (2023) at: https://github.com/deepmind/ neural_networks_chomsky_hierarchy. We additionally implement the regular languages in the second section\nreports the maximum score across different hyperparameter settings and random seeds because their goal is to know if a model can extrapolate at all. We additionally report the average scores since we want to know if the model can consistently obtain good performance. The baseline models we compare against are an RNN and vanilla Transformer with Transformer-XL style relative positional embedding (Dai et al., 2019). Table 1 shows that RegularGPT with C = 2 acheives similar performance as an RNN and substantially outperforms a vanilla Transformer."
        },
        {
            "heading": "5.3 The Effect of Chunk Size C",
            "text": "We vary the chunk size C of RegularGPT to see its impact on the performance. The motivation for using a larger C is to reduce the number of layers (i.e., L\u0304 = logC T decreases in C) and increase the degree of parallelization. However, in Table 1, a larger C seems to pose a challenge to RegularGPT on the Modular Arithmetic task. Modular Arithmetic is a hard task with far more states and complicated state transitions. Increasing C is likely to increase the task difficulty by composing more state transitions at once. We will have an in-depth discussion of the theoretical reasons in \u00a77.\nof Table 1."
        },
        {
            "heading": "5.4 Robust to Probability Changes",
            "text": "Other than the length extrapolation experiment, we alter the probability of sampling 1s of PARITY, i.e., set P(\u03c3i) 6= 0.5. The results in Table 2 show that RegularGPT is robust to different sampling probabilities, indicating its successful modeling of the underlying regular language grammar. In contrast, a vanilla Transformer model struggles to achieve good performance even for the same length setting, again validating the fact that it only finds the naive-summation solution as discussed in \u00a72.2."
        },
        {
            "heading": "6 Natural Language Experiments",
            "text": "Given that RegularGPT has been battle-tested on the main experiment of regular languages, we now shift gear to benchmark its performance in the natural language scenario. Given a model trained on sequences of length Ttr, we test it on much longer sequences of length Tex Ttr during inference, and the goal is to observe similar perplexities. To optimize efficiency, we employ a random selection process to extract 1,000 chunks, each with Tex tokens from the testing set. Subsequently, we calculate the average perplexity of the last tokens within these chunks to ensure each of them has Tex \u2212 1 tokens as the context, thereby avoiding the issue of early token curse (Press et al., 2022; Chi et al., 2023). We compare our model against the existing methods that are known to demonstrate the ability of length extrapolation including T5 (Raffel et al., 2020), ALiBi (Press et al., 2022), and KERPLE (Chi et al., 2022).4 To counteract the loss of\n4We use the nanoGPT codebase: https: //github.com/karpathy/nanoGPT, and the OpenWebText2 dataset: https://huggingface.co/ datasets/the_pile_openwebtext2.\nexpressive power due to weight sharing, we thicken each layer of RegularGPT to K as detailed in \u00a73.\nIn Table 3, we first observe exploding perplexities for C = 32 after Tex \u2265 2048. RegularGPT might only learn to model dlog32 512e = 2 layers during training, hence it fails to recursively model more than 322 = 1024 tokens during inference. This is validated by C = 64 since this time it is able to extrapolate until 64dlog64 512e = 4096. While the above argument seems to suggest large C, setting C = 256 also deteriorates the performance. This might be due to the limited number of chunks (512/256 = 2) and ri\u2019s (in Eq. (1)) observed at the second layer, making the learning of ri\u2019s harder. Overall, C is a hyperparameter that needs to be carefully decided for RegularGPT on natural languages. We also observe that 128/12 performs better than 128/6, implying RegularGPT\u2019s performance could be improved by stacking more layers to counteract the performance loss due to Weight-Sharing.\nIt is worth noting that 128/12 performs relatively well and is close to previous methods designed specifically for the task of natural language extrapolation. We will analyze its inner workings in depth in Figure 4 and \u00a77, in which we find that RegularGPT learns the similar local receptive field as prior work, which is likely the key to its successful natural language extrapolation performance."
        },
        {
            "heading": "7 Discussion and Analysis",
            "text": ""
        },
        {
            "heading": "7.1 Regular Language and Finite State Semiautomaton",
            "text": "Regular language is the type of formal language recognized by an FSA (Chomsky, 1956a), which is a 5-tuple (Q,\u03a3, \u03b4, q0, F ), where Q is a finite non-empty set of states, \u03a3 is a finite non-empty set of symbols, q0 \u2208 Q is an initial state, \u03b4 : Q\u00d7 \u03a3 \u2192 Q is a transition function; F \u2286 Q is a set of final states. However, some of our tasks are better modeled by a finite-state transducer (FST) as discussed in \u00a72.1. To underpin both FSA and FST, we consider a semiautomation A = (Q,\u03a3, \u03b4) (i.e., an FSA without q0 and F ) and establish its connection to a Transformer model.\nLet \u03c3a:b be the sequence from position a (inclusive) to b (exclusive) out of a length T input sequence (i.e., 0 \u2264 a < b \u2264 T ). We define A(\u03c3a:b) : Q \u2192 Q as the (b \u2212 a)-step state transition relation after receiving \u03c3a:b.\nA(\u03c3a:b) = \u03b4(\u00b7|\u03c3b\u22121) \u25e6 \u00b7 \u00b7 \u00b7 \u25e6 \u03b4(\u00b7|\u03c3a),\nwhere f(\u00b7) \u25e6 g(\u00b7) = f(g(\u00b7)) denotes function composition. With abuse of notation, we define Aq(\u03c3a:b) \u2208 Q as the state after receiving \u03c3a:b if starting at q \u2208 Q\nAq(\u03c3a:b) = \u03b4(\u00b7|\u03c3b\u22121) \u25e6 \u00b7 \u00b7 \u00b7 \u25e6 \u03b4(\u00b7|\u03c3a), q0 = q."
        },
        {
            "heading": "7.2 Modeling Transition Composition",
            "text": "We want to show that the layers of RegularGPT with chunk size C = 2 can model the composition of two transition functions:\nA(\u03c3a:b) = A(\u03c3i:b)\u25e6A(\u03c3a:i) for i \u2208 [a+1, . . . , b).\nThis way, the regular language problem can be solved recursively using the construction outlined in \u00a73 and Figure 1. To formalize the statement, we first observe that A(\u03c3a:b), A(\u03c3a:i), and A(\u03c3i:b) can be represented in R|Q|2 :\nA(\u03c3a:b) =  OneHot|Q|(Aq0(\u03c3a:b)) OneHot|Q|(Aq1(\u03c3a:b))\n\u00b7 \u00b7 \u00b7 OneHot|Q|(Aq|Q|\u22121(\u03c3a:b))  \u2208 R|Q|2 , (2)\nwhere OneHot|Q|(i) is a one-hot vector of length |Q| with the i-th index being 1.\nThe next step is to mix A(\u03c3a:i) and A(\u03c3i:b) together and get A(\u03c3a:b). We show in Lemma 1 that a 2-layer ReLU network can learn (and so can a transformer layer) the composition. The proof of Lemma 1 is deferred to Appendix C.\nLemma 1 (Approximation for Binary Matrix Product). Let A,B \u2208 {0, 1}n\u00d7n be binary matrices of dimension n \u00d7 n. Then, there exists a two-layer ReLU network such that\nfmlp([Flat(A),Flat(B)]) = Flat(AB),\nwhere Flat(X)(i\u22121)n+j = Xi,j for i, j \u2208 [n] is the operation that flattens a matrix into a vector.\nNow, we can relate Lemma 1 to the FFN layers in RegularGPT. Following \u00a73, when chuck size C = 2 and thickness K = 1, the output vector o(l)i depends on input sequence \u03c3i\u22122l+1+1:i+1. Also, o (l) i is computed from o (l\u22121) i\u22122l and o (l\u22121) i , which depend on input sequences \u03c3i\u22122l+1+1:i\u22122l+1 and \u03c3i\u22122l+1:i+1, respectively. This observation implies that o(l)i likely models the transition function A(\u03c3i\u22122l+1+1:i+1), which we denote as o (l) i \u223c A(\u03c3i\u22122l+1+1:i+1). We will verify this assumption in \u00a77.3.\nIf o(l)i \u223c A(\u03c3i\u22122l+1+1:i+1) is true, Lemma 1 implies that RegularGPT\u2019s FFN models the transition function composition. This is immediate by setting o(l\u22121)\ni\u22122l \u223c Flat(A(\u03c3i\u22122l+1+1:i\u22122l+1)), o (l\u22121) i \u223c Flat(A(\u03c3i\u22122l+1:i+1)) and recognizing the fact that function composition is a matrix product under the representation of Eq. (2).\nThe next step is to explain the use of selfattention layers in RegularGPT. Although Lemma 1 has established a composition, it is unclear how the transitions are concatenated in the first place (i.e., [Flat(A),Flat(B)]). With a two-head selfattention and the learnable relative positional scalars, it is possible to adjust them so that the attention output contains the concatenated information [Flat(A),Flat(B)].\nRecall in Eq. (1), each head has a different set of scalars ri\u2019s. One concrete construction for concatenation is setting r0 = 0 and the remaining \u2212\u221e for the first head; r1 = 0 and the remaining \u2212\u221e for the second head. In other words, each head is only responsible for capturing one state transition. After the multi-head self-attention operation, we obtain the concatenation of two state transitions.\nFinally, when the prediction head reads out the answer, the operation is equivalent to a mapping from A(\u03c30:T ) \u2208 R|Q|\u00d7|Q| to Aq0(\u03c30:T ) = A(\u03c30:T ) \u25e6 q0 \u2208 R|Q|. Since we assume that o(l)T\u22121\nmodels A(\u03c30:T ), the transduction readout is performed by a linear map on o(l)T\u22121 as Woo (l) T\u22121. Our tree-structured construction also guarantees that the final answer could be derived using log2 T layers."
        },
        {
            "heading": "7.3 Verification of Transition Modeling",
            "text": "To verify whether our model learns the dynamics of a semiautomaton, we perform a clustering experiment to demystify the FFN output representations on the tasks of PARITY and Cycle Navigation. The two tasks are chosen as we can easily derive their state transition functions. For example, there are only two state transitions in PARITY:[\n1 0 0 1\n] or [ 0 1 1 0 ] and five state transitions in Cycle Navigation5:\nOneHot5((0 + k)mod 5) OneHot5((1 + k)mod 5) OneHot5((2 + k)mod 5) OneHot5((3 + k)mod 5) OneHot5((4 + k)mod 5)\n , for k \u2208 [0, ..., 4].\ne.g., k = 2 gives  0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 . Given a testing input sequence of length 500 that is much longer than the training length 40, we 5Cycle Navigation (\u00a75.1) has 3 one-step transitions (i.e., |A(\u03c3a:a+1)| = 3). Composing these transitions yields 5 different multi-step state transitions (i.e., |A(\u03c3a:b)| = 5 if b\u2212 a \u2265 5).\nextract the output o(l)i of all layers l, perform dimension reduction using PCA, and plot the dimensionreduced points on a 2D plane. Ideally, we want to see a limited number of clusters across all layers, indicating the model learns to capture the state transition function. As we can see in Figure 3, PARITY has 2 clusters and Cycle Navigation has 5 clusters. The clear clustering effect demonstrates RegularGPT\u2019s correct learning of state transition functions. This is in contrast to the naive-summation approach learned by a vanilla Transformer as shown in Figure B.4 of Deletang et al. (2023)."
        },
        {
            "heading": "7.4 Receptive Field Analysis",
            "text": "We resort to the gradient analysis tool (Chi et al., 2023) to inspect the receptive field of RegularGPT on regular and natural languages. It computes a cumulative sum of the gradient norms starting from the most recent token to the earliest one. A large magnitude of slope at a position means the most recent token has a high dependency on that position. Ideally, we would like to see the receptive field covering the whole input sequence for the case of regular languages because every single bit in the input sequence is important for the final results. This is equivalent to a slanted line going from the lower right to the upper left, which is validated in Figure 4a. As for natural language, we discover something interesting in Figure 4b in that RegularGPT settles on the local windowed-attention pattern as those enforced manually in prior work (Press et al., 2022; Chi et al., 2022, 2023). This suggests the task of natural language modeling mostly needs only local context to achieve good performance,\nwhich aligns with the common belief."
        },
        {
            "heading": "8 Conclusion",
            "text": "This paper introduces RegularGPT, a novel variant of the Transformer architecture inspired by the notion of working memory that can effectively model regular languages with high efficiency. Theoretical explanations and accompanying clustering visualizations are presented to illustrate how RegularGPT captures the essence of regular languages. Moreover, RegularGPT is evaluated on the task of natural language length extrapolation, revealing its intriguing rediscovery of the local windowed attention effect previously observed in related research. Notably, RegularGPT establishes profound connections with various existing architectures, thereby laying the groundwork for the development of future Transformer models that facilitate efficient algorithmic reasoning and length extrapolation.\nLimitations\nCurrently we set the chunk size C of RegularGPT to a constant. Can we make the chunk size more flexible? A flexible and data-driven C might further boost its performance on natural languages as they often demonstrate diverse patterns unlike regular languages underpinned by simple grammars. This might also improve the performance of RegularGPT when C 6= 128."
        },
        {
            "heading": "Acknowledgment",
            "text": "We thank the anonymous reviewers for their insightful feedback and suggestions. We thank Princeton Research Computing for the technical support on the Della and Adroit clusters. The first author\nacknowledges the support from the Boeing Company (2019-STU-PA-259). The fourth author acknowledges the support from NSF. (MRI Award: 1919452)."
        },
        {
            "heading": "A Hyperparameters for the Regular Language Experiments",
            "text": "We report the hyperpamaters used in the regular language experiments (Table 1) in Table 4."
        },
        {
            "heading": "B Hyperparameters for the Natural Language Experiments",
            "text": "We report the hyperpamaters used in the natural language experiments (Table 3) in Table 5."
        },
        {
            "heading": "C Proof of Lemma 1",
            "text": "Lemma 1 (Approximation for Binary Matrix Product). Let A,B \u2208 {0, 1}n\u00d7n be binary matrices of dimension n \u00d7 n. Then, there exists a two-layer ReLU network such that\nfmlp([Flat(A),Flat(B)]) = Flat(AB),\nwhere Flat(X)(i\u22121)n+j = Xi,j for i, j \u2208 [1, ..., n] is the operation that flattens a matrix into a vector.\nProof. Observe that a ReLU operation can perfectly approximate the multiplication of two binary scalars:\nReLU(a+ b\u2212 1) = a \u00b7 b, for a, b \u2208 {0, 1}.\nThe binary matrix product AB is composed of n3 binary scalar products of the form:\nAikBkj = x(i\u22121)n+kx(n+k\u22121)n+j\nfor i, j, k \u2208 [1, .., n],\nwhere x = [Flat(A),Flat(B)] is the concatenated flattened input. Our goal is to construct two neural network layers. The first layer computes all n3 binary scalar products. The second layer sums these products into the form of matrix product; i.e.,\u2211n\nk=1AikBkj . The first layer\u2019s binary weight matrix W (1) \u2208 {0, 1}2n2\u00d7n3 is constructed as:\nFor z \u2208 [1, ..., 2n2], i, j, k \u2208 [1, ..., n],\nW (1) z,(i\u22121)n2+(j\u22121)n+k ={ 1 if z = (i\u2212 1)n+ k or (n+ k \u2212 1)n+ j 0 otherwise.\n(3)\nThen, the first layer computes all n3 binary scalar products as follows:\nReLU ( [Flat(A),Flat(B)]W (1)\u22121>\nn3 ) (i\u22121)n2+(j\u22121)n+k\n= AikBkj for i, j, k \u2208 [1, ..., n].\nTo sum these n3 products into n2 results, the second layer\u2019s binary weight matrix W (2) \u2208 {0, 1}n3\u00d7n2 is constructed as:\nW (2) = In2 \u2297 1n =  1n 0n 0n . . . 0n 0n 1n 0n . . . 0n ...\n... 0n . . . 0n 1n  \u2208 {0, 1}n3\u00d7n2 ,\nwhere In2 is an n2 \u00d7 n2 identity matrix, \u2297 is the Kronecker product, 0n is an n-dimensional column vector of all zeros, and 1n is an n-dimensional column vector of all ones. We arrive at a twolayer ReLU network that perfectly approximates the multiplication of two binary matrices:\nfmlp([Flat(A),Flat(B)]) =ReLU ( [Flat(A),Flat(B)]W (1)\u22121>\nn3\n) W (2)\n= Flat(AB).\nD Illustration of Lemma 1\nD.1 Illustration of the Binary Weight Matrices We illustrate W (1) and W (2) of Lemma 1 as follows: import numpy as np\ndef get_W1 ( n ) : n2 = n*n W1 = np . z e r o s ( ( 2 * n*n , n * * 3 ) , d t y p e = i n t ) f o r i in range ( n ) :\nf o r j in range ( n ) : f o r k in range ( n ) :\nW1[ i *n+k , i *n2+ j *n+k ] = 1 W1[ n2+k*n+ j , i *n2+ j *n+k ] = 1\nre turn W1\ndef get_W2 ( n ) : eye = np . eye ( n*n , d t y p e = i n t ) ones = np . ones ( ( n , 1 ) , d t y p e = i n t ) W2 = np . kron ( eye , ones ) re turn W2\nget_W1(2) gives:\n[ [ 1 0 1 0 0 0 0 0] [0 1 0 1 0 0 0 0] [0 0 0 0 1 0 1 0] [0 0 0 0 0 1 0 1] [1 0 0 0 1 0 0 0] [0 0 1 0 0 0 1 0] [0 1 0 0 0 1 0 0] [0 0 0 1 0 0 0 1 ] ]\nget_W2(2) gives:\n[ [ 1 0 0 0] [1 0 0 0] [0 1 0 0] [0 1 0 0] [0 0 1 0] [0 0 1 0] [0 0 0 1] [0 0 0 1 ] ]\nD.2 An Illustrative Example for n = 2\nSuppose the input matrices are:\nA = [ 1 0 1 0 ] , B = [ 0 1 1 0 ] .\nThe concatenated flattened input becomes:\nx = [Flat(A),Flat(B)] = [1 0 1 0 0 1 1 0].\nThen, Lemma 1 is verified as follows: ReLU ( xW (1) \u2212 1>n3 ) W (2)\n=ReLU ([1 1 2 0 1 1 2 0]\u2212 1)W (2)\n=[0 0 1 0 0 0 1 0]W (2)\n=[0 1 0 1] =Flat ([\n0 1 0 1\n]) = Flat (AB) .\nHere is the Python code for the above example: A = np . a r r a y ( [ [ 1 , 0 ] , [ 1 , 0 ] ] ) . r e s h a p e ( \u22121) B = np . a r r a y ( [ [ 0 , 1 ] , [ 1 , 0 ] ] ) . r e s h a p e ( \u22121) x = np . c o n c a t e n a t e ( [ A, B ] ) . r e s h a p e (1 , \u22121 ) W1 = get_W1 ( 2 ) W2 = get_W2 ( 2 ) f l a t_AB = np . maximum ( x @ W1 \u22121 ,0) @ W2"
        }
    ],
    "title": "Transformer Working Memory Enables Regular Language Reasoning And Natural Language Length Extrapolation",
    "year": 2023
}