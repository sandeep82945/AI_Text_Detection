{
    "abstractText": "Abstracts derived from biomedical literature possess distinct domain-specific characteristics, including specialised writing styles and biomedical terminologies, which necessitate a deep understanding of the related literature. As a result, existing language models struggle to generate technical summaries that are on par with those produced by biomedical experts, given the absence of domain-specific background knowledge. This paper aims to enhance the performance of language models in biomedical abstractive summarisation by aggregating knowledge from external papers cited within the source article. We propose a novel attention-based citation aggregation model that integrates domain-specific knowledge from citation papers, allowing neural networks to generate summaries by leveraging both the paper content and relevant knowledge from citation papers. Furthermore, we construct and release a large-scale biomedical summarisation dataset that serves as a foundation for our research. Extensive experiments demonstrate that our model outperforms state-of-the-art approaches and achieves substantial improvements in abstractive biomedical text summarisation.s derived from biomedical literature possess distinct domain-specific characteristics, including specialised writing styles and biomedical terminologies, which necessitate a deep understanding of the related literature. As a result, existing language models struggle to generate technical summaries that are on par with those produced by biomedical experts, given the absence of domain-specific background knowledge. This paper aims to enhance the performance of language models in biomedical abstractive summarisation by aggregating knowledge from external papers cited within the source article. We propose a novel attention-based citation aggregation model that integrates domain-specific knowledge from citation papers, allowing neural networks to generate summaries by leveraging both the paper content and relevant knowledge from citation papers. Furthermore, we construct and release a large-scale biomedical summarisation dataset that serves as a foundation for our research. Extensive experiments demonstrate that our model outperforms state-of-the-art approaches and achieves substantial improvements in abstractive biomedical text summarisation.",
    "authors": [
        {
            "affiliations": [],
            "name": "Chen Tang"
        },
        {
            "affiliations": [],
            "name": "Shun Wang"
        },
        {
            "affiliations": [],
            "name": "Tomas Goldsack"
        },
        {
            "affiliations": [],
            "name": "Chenghua Lin"
        }
    ],
    "id": "SP:68ec8af14d36ef4f88e408d313b7e6c025ca58e8",
    "references": [
        {
            "authors": [
                "Chenxin An",
                "Ming Zhong",
                "Yiran Chen",
                "Danqing Wang",
                "Xipeng Qiu",
                "Xuanjing Huang."
            ],
            "title": "Enhancing scientific papers summarization with citation graph",
            "venue": "Proceedings of the AAAI conference on artificial intelligence, volume 35, pages 12498\u201312506.",
            "year": 2021
        },
        {
            "authors": [
                "Iz Beltagy",
                "Matthew E Peters",
                "Arman Cohan."
            ],
            "title": "Longformer: The long-document transformer",
            "venue": "arXiv preprint arXiv:2004.05150.",
            "year": 2020
        },
        {
            "authors": [
                "Khyathi Chandu",
                "Aakanksha Naik",
                "Aditya Chandrasekar",
                "Zi Yang",
                "Niloy Gupta",
                "Eric Nyberg."
            ],
            "title": "Tackling biomedical text summarization: OAQA at BioASQ 5B",
            "venue": "BioNLP 2017, pages 58\u2013 66, Vancouver, Canada,. Association for Computa-",
            "year": 2017
        },
        {
            "authors": [
                "Arman Cohan",
                "Franck Dernoncourt",
                "Doo Soon Kim",
                "Trung Bui",
                "Seokhwan Kim",
                "Walter Chang",
                "Nazli Goharian."
            ],
            "title": "A discourse-aware attention model for abstractive summarization of long documents",
            "venue": "Proceedings of the 2018 Conference of the North",
            "year": 2018
        },
        {
            "authors": [
                "Arman Cohan",
                "Luca Soldaini",
                "Nazli Goharian."
            ],
            "title": "Towards citation-based summarization of biomedical literature",
            "venue": "Proceedings of the Text Analysis Conference (TAC\u201914).",
            "year": 2014
        },
        {
            "authors": [
                "Jay DeYoung",
                "Iz Beltagy",
                "Madeleine van Zuylen",
                "Bailey Kuehl",
                "Lucy Lu Wang."
            ],
            "title": "MS\u02c62: Multidocument summarization of medical studies",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7494\u2013",
            "year": 2021
        },
        {
            "authors": [
                "Paul Gigioli",
                "Nikhita Sagar",
                "Anand Rao",
                "Joseph Voyles."
            ],
            "title": "Domain-aware abstractive text summarization for medical documents",
            "venue": "2018 IEEE International Conference on Bioinformatics and Biomedicine (BIBM), pages 2338\u20132343. IEEE.",
            "year": 2018
        },
        {
            "authors": [
                "Azadeh Givchi",
                "Reza Ramezani",
                "Ahmad BaraaniDastjerdi."
            ],
            "title": "Graph-based abstractive biomedical text summarization",
            "venue": "Journal of Biomedical Informatics, 132:104099.",
            "year": 2022
        },
        {
            "authors": [
                "Tomas Goldsack",
                "Zheheng Luo",
                "Qianqian Xie",
                "Carolina Scarton",
                "Matthew Shardlow",
                "Sophia Ananiadou",
                "Chenghua Lin."
            ],
            "title": "Overview of the biolaysumm 2023 shared task on lay summarization of biomedical research articles",
            "venue": "The 22nd Workshop",
            "year": 2023
        },
        {
            "authors": [
                "Tomas Goldsack",
                "Zhihao Zhang",
                "Chenghua Lin",
                "Carolina Scarton."
            ],
            "title": "Making science simple: Corpora for the lay summarisation of scientific literature",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages",
            "year": 2022
        },
        {
            "authors": [
                "Henglin Huang",
                "Chen Tang",
                "Tyler Loakman",
                "Frank Guerin",
                "Chenghua Lin."
            ],
            "title": "Improving Chinese story generation via awareness of syntactic dependencies and semantics",
            "venue": "Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the",
            "year": 2022
        },
        {
            "authors": [
                "Jinhyuk Lee",
                "Wonjin Yoon",
                "Sungdong Kim",
                "Donghyeon Kim",
                "Sunkyu Kim",
                "Chan Ho So",
                "Jaewoo Kang."
            ],
            "title": "Biobert: a pre-trained biomedical language representation model for biomedical text mining",
            "venue": "Bioinformatics, 36(4):1234\u20131240.",
            "year": 2020
        },
        {
            "authors": [
                "Ves Stoyanov",
                "Luke Zettlemoyer."
            ],
            "title": "Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension",
            "venue": "arXiv preprint arXiv:1910.13461.",
            "year": 2019
        },
        {
            "authors": [
                "Chin-Yew Lin."
            ],
            "title": "ROUGE: A package for automatic evaluation of summaries",
            "venue": "Text Summarization Branches Out, pages 74\u201381, Barcelona, Spain. Association for Computational Linguistics.",
            "year": 2004
        },
        {
            "authors": [
                "Tyler Loakman",
                "Chen Tang",
                "Chenghua Lin."
            ],
            "title": "TwistList: Resources and baselines for tongue twister generation",
            "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 579\u2013589, Toronto,",
            "year": 2023
        },
        {
            "authors": [
                "Keqin Peng",
                "Chuantao Yin",
                "Wenge Rong",
                "Chenghua Lin",
                "Deyu Zhou",
                "Zhang Xiong."
            ],
            "title": "Named entity aware transfer learning for biomedical factoid question answering",
            "venue": "IEEE/ACM Transactions on Computational Biology and Bioinformatics, 19(4):2365\u2013",
            "year": 2021
        },
        {
            "authors": [
                "Alec Radford",
                "Jeffrey Wu",
                "Rewon Child",
                "David Luan",
                "Dario Amodei",
                "Ilya Sutskever"
            ],
            "title": "Language models are unsupervised multitask learners",
            "venue": "OpenAI blog,",
            "year": 2019
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J Liu"
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "J. Mach. Learn. Res.,",
            "year": 2020
        },
        {
            "authors": [
                "Frederik Schulze",
                "Mariana Neves."
            ],
            "title": "Entitysupported summarization of biomedical abstracts",
            "venue": "Proceedings of the Fifth Workshop on Building and Evaluating Resources for Biomedical Text Mining (BioTxtM2016), pages 40\u201349, Osaka, Japan. The",
            "year": 2016
        },
        {
            "authors": [
                "Yue Shang",
                "Yanpeng Li",
                "Hongfei Lin",
                "Zhihao Yang."
            ],
            "title": "Enhancing biomedical text summarization using semantic relation extraction",
            "venue": "PLoS one, 6(8):e23862.",
            "year": 2011
        },
        {
            "authors": [
                "Sajad Sotudeh Gharebagh",
                "Nazli Goharian",
                "Ross Filice."
            ],
            "title": "Attend to medical ontologies: Content selection for clinical abstractive summarization",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1899\u2013",
            "year": 2020
        },
        {
            "authors": [
                "Edward Sun",
                "Yufang Hou",
                "Dakuo Wang",
                "Yunfeng Zhang",
                "Nancy X.R. Wang."
            ],
            "title": "D2S: Document-to-slide generation via query-based text summarization",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Associ-",
            "year": 2021
        },
        {
            "authors": [
                "Chen Tang",
                "Frank Guerin",
                "Yucheng Li",
                "Chenghua Lin."
            ],
            "title": "Recent advances in neural text generation: A task-agnostic survey",
            "venue": "arXiv preprint arXiv:2203.03047.",
            "year": 2022
        },
        {
            "authors": [
                "Chen Tang",
                "Chenghua Lin",
                "Henglin Huang",
                "Frank Guerin",
                "Zhihao Zhang."
            ],
            "title": "EtriCA: Eventtriggered context-aware story generation augmented by cross attention",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2022, pages",
            "year": 2022
        },
        {
            "authors": [
                "Chen Tang",
                "Hongbo Zhang",
                "Tyler Loakman",
                "Chenghua Lin",
                "Frank Guerin."
            ],
            "title": "Enhancing dialogue generation via dynamic graph knowledge aggregation",
            "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Vol-",
            "year": 2023
        },
        {
            "authors": [
                "Chen Tang",
                "Hongbo Zhang",
                "Tyler Loakman",
                "Chenghua Lin",
                "Frank Guerin."
            ],
            "title": "Terminology-aware medical dialogue generation",
            "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 1\u20135. IEEE.",
            "year": 2023
        },
        {
            "authors": [
                "Chen Tang",
                "Zhihao Zhang",
                "Tyler Loakman",
                "Chenghua Lin",
                "Frank Guerin."
            ],
            "title": "NGEP: A graph-based event planning framework for story generation",
            "venue": "Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Lin-",
            "year": 2022
        },
        {
            "authors": [
                "Rahul Tangsali",
                "Aditya Jagdish Vyawahare",
                "Aditya Vyankatesh Mandke",
                "Onkar Rupesh Litake",
                "Dipali Dattatray Kadam."
            ],
            "title": "Abstractive approaches to multidocument summarization of medical literature reviews",
            "venue": "Proceedings of the",
            "year": 2022
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin."
            ],
            "title": "Attention is all you need",
            "venue": "Advances in neural information processing systems, 30.",
            "year": 2017
        },
        {
            "authors": [
                "Qianqian Xie",
                "Jennifer Amy Bishop",
                "Prayag Tiwari",
                "Sophia Ananiadou."
            ],
            "title": "Pre-trained language models with domain knowledge for biomedical extractive summarization",
            "venue": "Knowledge-Based Systems, 252:109460.",
            "year": 2022
        },
        {
            "authors": [
                "Bohao Yang",
                "Chen Tang",
                "Chenghua Lin."
            ],
            "title": "Improving medical dialogue generation with abstract meaning representations",
            "venue": "arXiv preprint arXiv:2309.10608.",
            "year": 2023
        },
        {
            "authors": [
                "Bohao Yang",
                "Chen Tang",
                "Kun Zhao",
                "Chenghao Xiao",
                "Chenghua Lin"
            ],
            "title": "Effective distillation",
            "year": 2023
        },
        {
            "authors": [
                "Michihiro Yasunaga",
                "Jungo Kasai",
                "Rui Zhang",
                "Alexander R Fabbri",
                "Irene Li",
                "Dan Friedman",
                "Dragomir R Radev"
            ],
            "title": "Scisummnet: A large annotated corpus and content-impact models for scientific paper summarization with citation networks",
            "year": 2019
        },
        {
            "authors": [
                "Hongyi Yuan",
                "Zheng Yuan",
                "Ruyi Gan",
                "Jiaxing Zhang",
                "Yutao Xie",
                "Sheng Yu."
            ],
            "title": "Biobart: pretraining and evaluation of a biomedical generative language model",
            "venue": "arXiv preprint arXiv:2204.03905.",
            "year": 2022
        },
        {
            "authors": [
                "Weizhe Yuan",
                "Graham Neubig",
                "Pengfei Liu."
            ],
            "title": "Bartscore: Evaluating generated text as text generation",
            "venue": "Advances in Neural Information Processing Systems, 34:27263\u201327277.",
            "year": 2021
        },
        {
            "authors": [
                "Hongbo Zhang",
                "Chen Tang",
                "Tyler Loakmana",
                "Chenghua Lina",
                "Stefan Goetze."
            ],
            "title": "Cadge: Contextaware dialogue generation enhanced with graphstructured knowledge aggregation",
            "venue": "arXiv preprint arXiv:2305.06294.",
            "year": 2023
        },
        {
            "authors": [
                "Jingqing Zhang",
                "Yao Zhao",
                "Mohammad Saleh",
                "Peter Liu."
            ],
            "title": "Pegasus: Pre-training with extracted gap-sentences for abstractive summarization",
            "venue": "International Conference on Machine Learning, pages 11328\u201311339. PMLR.",
            "year": 2020
        },
        {
            "authors": [
                "Tianyi Zhang",
                "Varsha Kishore",
                "Felix Wu",
                "Kilian Q Weinberger",
                "Yoav Artzi."
            ],
            "title": "Bertscore: Evaluating text generation with bert",
            "venue": "arXiv preprint arXiv:1904.09675.",
            "year": 2019
        },
        {
            "authors": [
                "Kun Zhao",
                "Bohao Yang",
                "Chenghua Lin",
                "Wenge Rong",
                "Aline Villavicencio",
                "Xiaohui Cui."
            ],
            "title": "Evaluating open-domain dialogues in latent space with next sentence prediction and mutual information",
            "venue": "arXiv preprint arXiv:2305.16967.",
            "year": 2023
        },
        {
            "authors": [
                "Jie Zhou",
                "Ganqu Cui",
                "Shengding Hu",
                "Zhengyan Zhang",
                "Cheng Yang",
                "Zhiyuan Liu",
                "Lifeng Wang",
                "Changcheng Li",
                "Maosong Sun."
            ],
            "title": "Graph neural networks: A review of methods and applications",
            "venue": "AI open, 1:57\u201381.",
            "year": 2020
        }
    ],
    "sections": [
        {
            "text": "Improving Biomedical Abstractive Summarisation with Knowledge Aggregation from Citation Papers\nChen Tang1, Shun Wang2, Tomas Goldsack2 and Chenghua Lin2,3\u2217 1Department of Computer Science, The University of Surrey, UK\n2Department of Computer Science, The University of Sheffield, UK 3Department of Computer Science, The University of Manchester, UK chen.tang@surrey.ac.uk, chenghua.lin@manchester.ac.uk\n{swang209, tgoldsack1}@sheffield.ac.uk\nAbstract\nAbstracts derived from biomedical literature possess distinct domain-specific characteristics, including specialised writing styles and biomedical terminologies, which necessitate a deep understanding of the related literature. As a result, existing language models struggle to generate technical summaries that are on par with those produced by biomedical experts, given the absence of domain-specific background knowledge. This paper aims to enhance the performance of language models in biomedical abstractive summarisation by aggregating knowledge from external papers cited within the source article. We propose a novel attention-based citation aggregation model that integrates domain-specific knowledge from citation papers, allowing neural networks to generate summaries by leveraging both the paper content and relevant knowledge from citation papers. Furthermore, we construct and release a large-scale biomedical summarisation dataset that serves as a foundation for our research. Extensive experiments demonstrate that our model outperforms state-of-the-art approaches and achieves substantial improvements in abstractive biomedical text summarisation."
        },
        {
            "heading": "1 Introduction",
            "text": "Biomedical text summarisation plays a pivotal role in facilitating the comprehension of the vast and constantly expanding body of biomedical literature (Xie et al., 2022), which poses a significant challenge for clinicians and domain experts who strive to remain well-informed in their respective fields. To address this challenge, the generation of high-quality summaries from the extensive corpus of biomedical literature holds immense potential in supporting research and advancements within the biomedical domain (DeYoung et al., 2021).\nOne of the key challenges in biomedical natural language generation (NLG) lies in effectively\n*Corresponding author.\nhandling domain-specific terminologies that are prevalent in biomedical texts. Consequently, a plethora of research studies have been conducted with a primary focus on enhancing language quality by better integrating domain-specific knowledge in the biomedicine domain (Sotudeh Gharebagh et al., 2020; Tangsali et al., 2022; An et al., 2021; Tang et al., 2023b) However, most prior works have predominantly attempted to incorporate knowledge by leveraging additional annotations within the paper content. These annotations include frequent items (Givchi et al., 2022), named entities (Schulze and Neves, 2016; Peng et al., 2021), entity relations (Shang et al., 2011), as well as external knowledge systems such as biomedical ontologies (Chandu et al., 2017) and external terminology\nsearching tools (Gigioli et al., 2018). Surprisingly, the inclusion of external knowledge derived from citation papers has been rarely explored in previous biomedical studies. Existing corpora for biomedical text summarisation are typically constructed in a manner that models solely rely on the source article when generating a summary. However, as shown in Figure 1, there exists strong connections among papers in the citation network with shared research backgrounds, terminologies, and abstract styles, which will be a useful source of knowledge for improving biomedical abstractive summarisation but not captured in existing datasets.\nTo address this gap in the existing biomedical summarisation dataset, we construct a novel biomedical summarisation dataset utilising an opensource biomedical literature corpus provided by the Allen Institute1. During the dataset construction process, we applied rigorous filtering criteria to eliminate low-quality samples. Specifically, we discarded samples with an insufficient number of citations (less than three distinct citations), as well as unqualified papers whose unique identifiers (UIDs) or citation UIDs were inaccessible within the corpus. Additionally, we designed heuristic rules to select and transform the unstructured raw data corpus into a structured dataset in JsonL format. The final dataset comprises over 10,000 instances, with each instance having an average of 16 citations. To the best of our knowledge, this is the largest biomedical literature dataset2 specifically tailored for citation paper-enhanced biomedical text summarisation. Furthermore, we provide the corresponding methods for collecting the citation network, including cited papers and their associations.\nFacilitated by our biomedical summarisation dataset, we further propose a novel approach to biomedical document summarisation whereby we enhance neural models with external domainspecific knowledge in the form of the abstracts of cited papers. Accordingly, we introduce an attention-based network (Vaswani et al., 2017) that dynamically aggregates features extracted from the citation abstracts with the encoded content features of the main paper. This aggregation is achieved by applying attention mechanisms to the associated abstracts of all cited papers, which provides the subsequent summary decoding process with addi-\n1https://allenai.org/data/cord-19 2The sole viable dataset we have identified is The Text Analysis Conference (TAC) 2014 Biomedical Summarization track (Cohan et al., 2014) comprising mere 313 instances.\ntional features derived from abstracts of the citation papers. Within this framework, the base language model can effectively leverage both the features of the main paper and the additional domain-specific knowledge obtained from cited papers. Consequently, this integration leads to enhanced performance in text summarisation. Extensive experiments demonstrate that our model outperforms state-of-the-art baselines in abstractive biomedical text summarisation. We also conducted an in-depth quantitative analysis to verify the performance gain obtained by our attention-based citation knowledge enhancement framework3. Our contributions are summarised as follows:\n\u2022 We construct a large-scale biomedical literature dataset, which can be used for enhancing biomedical text summarisation with the extracted external knowledge from cited papers.\n\u2022 We propose a novel framework that can effectively leverage citation papers to enhance the performance of large-scale language models on abstractive summarisation of biomedical literature.\n\u2022 We conduct extensive experiments to evaluate the effectiveness of our proposed framework, including comparisons with SOTA models and an in-depth analysis of the performance gain achieved by aggregating different quantities of citations."
        },
        {
            "heading": "2 Related Work",
            "text": "In recent years, a variety of large-scale pre-trained models (PLMs), such as BART (Lewis et al., 2019); T5 (Raffel et al., 2020); GPT-2 (Radford et al., 2019), have demonstrated remarkable performance improvements across various tasks (Loakman et al., 2023; Zhang et al., 2023; Zhao et al., 2023; Tang et al., 2022b) in the Natural Language Generation (NLG) Domain. These PLMs have also been widely applied to biomedical text summarisation. These models, e.g. BioBERT (Lee et al., 2020) and BioBART (Yuan et al., 2022), have achieved remarkable performance by training on extensive biomedical literature corpora, such as Pubmed4 and MIMIC-III5. However, certain high-level knowledge, e.g., the understanding of medical terminologies, cannot be adequately captured solely\n3Our code and data resources is accessible at https:// github.com/tangg555/biomed-sum.\n4https://pubmed.ncbi.nlm.nih.gov/ 5https://physionet.org/content/mimiciii/1.4/\nthrough the implicit modeling of word probabilities. To address this limitation, the improvement of biomedical background knowledge understanding is able to necessitate the integration of additional knowledge systems, such as conceptual ontologies. These ontologies explicitly model representations of domain-specific knowledge learned by neural networks. Recent studies have proposed incorporating biomedical knowledge, including terminologies (Tang et al., 2023b)) and concepts (Chandu et al., 2017), to enhance the performance of these language models and bridge the gap between language understanding and specialized biomedical knowledge. Indeed, several notable works have focused on enhancing summarisation through citations in the open domain, such as An et al. (2021) and Yasunaga et al. (2019). However, it is important to highlight that the progress of language models in the biomedical domain has been hindered by the limited availability of datasets and resources. This scarcity has impeded the further advancement and improvement of pre-trained language models (PLMs) specifically tailored for biomedical applications. In this study, we require a dataset that contains retrievable citation papers, making traditional raw data corpora such as Pubmed and MIMIC-III inadequate. To date, the sole public dataset we could find is the Text Analysis Conference (TAC) 2014 Biomedical Summarization track (Cohan et al., 2014). However, this dataset is limited in size comprising merely 313 instances, and is somewhat outdated. Therefore, we construct a novel dataset for investigating biomedical citation-enhanced summarisation."
        },
        {
            "heading": "3 Dataset Construction",
            "text": ""
        },
        {
            "heading": "3.1 Construction Process",
            "text": "In order to create a dataset containing biomedical literature and its associated citations, we process a semi-structured raw corpus 6 released by Allen Institute. We refer to this dataset as BioCiteDB throughout the paper. The construction process of the dataset is outlined in algorithm 1, where C represents the raw corpus, and D represents the processed dataset. To ensure the quality and relevance of the data, the selected papers have to meet the following requirements: (1) The papers must include\n6We select the latest version of CORD-19 Historical Releases (2022-06-02 18.7 GB), which can be accessed at https: //ai2-semanticscholar-cord-19.s3-us-west-2. amazonaws.com/historical_releases.html.\nAlgorithm 1: Construction of BioCiteDB Input: Samples ci \u2208 C; Citation limit R Output: Json objects di \u2208 D\n1 Initialise D \u2190 \u2205 2 foreach ci in C do 3 Initialise object di with ci 4 retrieve files fj to a queue q 5 Initialise object pi 6 foreach fj in q do 7 if fj missing elements then 8 break 9 end\n10 extract distinct citations rn \u2208 fi 11 if |rn| >= R then 12 rn.uid\u2192 pi.citations 13 extend di with pi 14 break 15 end 16 end 17 D append di 18 end 19 foreach di in D do 20 foreach rn in di.citations do 21 if rn /\u2208 D then 22 exclude rn 23 end 24 end 25 end\nthe \"Introduction\" section, as it is considered the most crucial part for generating abstracts; (2) The papers must have at least three distinct citations to ensure the quality of curated data; (3) The essential elements of the papers, including UID (Pubmed id), Title, Abstract, Sections, and Citations, must be accessible within the raw corpus. As a result of this construction process, the dataset D comprises structured data in JsonL7 format, with each sample representing an individual paper."
        },
        {
            "heading": "3.2 Data Statistics",
            "text": "The statistical analysis of our processed dataset is presented in Table 1. Additionally, the distribution\n7https://manifold.net/doc/mfd9/jsonl.htm.\nof citations per paper is visualized in Figure 2 (a), (b), and (c), while the proportions of data size are depicted in Figure 2 (d) of the same figure. The results obtained from both the statistical analysis and visual representations in Table 1 and Figure 2 both validate the data quality of the constructed dataset, thus indicating the effectiveness of our data construction process and the consistency of the dataset splits. This validation supports the notion that training and inference tasks conducted on this dataset can be regarded as fair and reliable.\nAlgorithm 2: Extracting Citation Graph G. Input: di \u2208 D; hopmax; N Output: The set of related papers P\n1 Initialise current hop hopn = 0; a double-ended queue DQ\u2190 (di.uid, hopn); a queue recording visited nodes V Q; 2 while si in S do 3 pop uid and hopn from DQ 4 if hopn > hopmax then 5 return P 6 end 7 P \u2190 (uid, hopn) 8 if |P | > N then 9 return P\n10 end 11 get dj by uid 12 V Q\u2190 uid 13 foreach rn \u2208 dj .citations do 14 if rn.uid /\u2208 vq and rn \u2208 D then 15 P \u2190 rn.uid and V Q\u2190 rn.uid 16 end 17 end 18 end"
        },
        {
            "heading": "3.3 Extract Citation Graph",
            "text": "Scientific papers are intricately connected through citation relationships, forming a network of interconnected nodes. This citation graph provides valuable insights into the relatedness of papers. In order to retrieve relevant papers within this cita-\ntion graph, we propose an algorithm outlined in algorithm 2. hopmax defines the maximum number of hops between papers that the algorithm can traverse, and N specifies the maximum number of retrieved papers at each hop. As output, P represents papers as nodes, while citation relationships are represented as edges in the network. Due to the high computational cost of processing long documents for summarisation, we set hopmax to 1 and neighbormax to 12, taking into account the limitations of our available computing resources. However, it is worth noting that the attention-based citation aggregation module can be extended to incorporate Graph Attention Networks (Zhou et al., 2020), which have the capability to integrate multilayer citation graphs (Zhang et al., 2023)."
        },
        {
            "heading": "4 Methodology",
            "text": "As illustrated in Figure 3, our proposed framework is designed to enhance the performance of the base language model by leveraging the collective knowledge from a set of citation papers. For our experiments, we select BART (Lewis et al., 2019), a widely-used summarization model that has demonstrated promising results in the biomedical domain (Goldsack et al., 2022, 2023), as the base model. In this study, we adopt a strategy where we concatenate the abstracts of the citation papers with the input document to form the model\u2019s input. This approach is motivated by the goal of enabling the model to capture and emulate the writing style present in relevant papers. By incorporating this additional information, we aim to improve the model\u2019s ability to generate high-quality summaries that align with the conventions and patterns observed in the domain-specific literature."
        },
        {
            "heading": "4.1 Task Definition",
            "text": "The task is formulated as follows: Given a paper document di \u2208 D as the input, where D represents the paper corpus, and di denotes the i-th paper. In addition, the citations papers Dc = {dc1, dc2, ..., dck} are also provided as the input. The abstracts of dck \u2208 Dc are denoted absck. Either di or dcj consists of a sequence of words represented as X = {x1, x2, ..., xt} where xt denotes t-th word in X . The goal is to generate a summary Y = {y1, y2, ..., yt} by modeling the conditional probability distribution P (Y |X \u2208 di, X \u2208 Dc)."
        },
        {
            "heading": "4.2 Knowledge Aggregation from Citations",
            "text": "Input At the initial stage, both the input document di and its retrieved N citation abstracts absc are concatenated and encoded by language models. Byte-Pair Encoding (Radford et al., 2019) is implemented in the transformation from text into fixed word embeddings:\nEdoc = LMemb([TokCLS , xt \u2208 di]) (1) Eabscj = LMemb([Tok ABS , xt \u2208 abscj ]) (2)\nEQj = concat(Edoc, Eabscj ) (3)\nwhere LMemb represents the module responsible for tokenising and converting words into subword embeddings. TokCLS is a special token that signifies the global context tag in the input text. TokABS is a special token used to indicate the separation between the input document and the cited abstracts. EQj denotes the embeddings generated for the j-th (j \u2208 [1, N ]) document abstract pair.\nEncoding In order to capture the relevance of each cited abstract, we employ an attention mechanism to measure the importance of di with respect to abscj . The attention score is denoted as attn i j , and the process of aggregating knowledge is illustrated as follows:\nEQ = concat([EQ1 , ..., EQN ]) (4) Q = LMenc(EQ), Q \u2208 RN\u00d7L\u00d7M (5)\nwhere EQ denotes the matrix of embeddings for all composed Qj , and it is encoded by the language model encoder to generate the encoded features Q.\nQCLS = First_Pool(Q), QCLS \u2208 RN\u00d7M (6) Attn_logits = QCLSWQ, Attn \u2208 RN\u00d71 (7) Attn = softmax(Attn), Attn \u2208 RN\u00d71 (8) F = ATQ,F \u2208 RL\u00d7M (9)\nIn the above equations, First_Pool collects features that represent the global context of the input di and abscj pairs. As the hidden states of the neural encoder, Q incorporates features from both the documents and the abstracts. Therefore, the representations of the first position in Q (represented as QCLS) correspond to the global context token TokCLS . The attention logits matrix is obtained by applying a trainable parameter WQ \u2208 RM\u00d71 to the features of QCLS . After applying the softmax function for normalization, Attn represents the importance of the input features and is used to reweight the original encoded features Q, resulting in the final features F ."
        },
        {
            "heading": "4.3 Summary Generation",
            "text": "In line with other abstractive summarization systems, we employ an auto-regressive decoder to generate summary tokens yt in a sequential manner. The process is described as follows:\nHt = Decoder(y<t, F ) (10)\nP (yt|y<t, X) = softmax(HtWD) (11)\nyt sampling\u2190\u2212 P (yt|y<t, F ) (12)\nwhere t represents the current time step. X corresponds to the input, consisting of the words from di and absc1, ..., abs c j , provided to the neural model. Ht refers to the hidden state of the decoder module at time step t. This state is computed by the language models using the infused features F , which encapsulate the information from the input document and its cited abstracts, along with the previously predicted tokens y<t. WD denotes a trainable parameter, and P (yt|y<t, F ) represents the probability distribution over the vocabulary, which includes special tokens. Employing a sampling strategy, such as argmax, we obtain the predicted token yt."
        },
        {
            "heading": "4.4 Training and Inference",
            "text": "Finally, as shown in Figure 3, the neural model is trained to fit on the citation-enhanced training set by the following objective function:\nL = \u2212 1 N N\u2211 t=1 logP (yt|y<t, X) (13)\nwhere L is the cross-entropy loss employed to train the model in modeling the conditional probabilities over the token sequence P (yt|y<t, F ). By minimizing L, the language model learns to predict the referenced abstract corresponding to the input document."
        },
        {
            "heading": "5 Experiment",
            "text": ""
        },
        {
            "heading": "5.1 Experimental Setup",
            "text": "Baselines We include a range of competitive PLM models as our baselines. We also provide the results of two rule-based systems, namely LEAD-3 and ORACLE, which serve as benchmarks representing the upper and lower bounds of model performance. LEAD-3 extracts the first 3 sentences from the input as the summary, which can\nbe considered as the lower bound of the performance. ORACLE select sentences from the input document and compose a summary with the highest score8, which is the upper bound of extractive summarisation systems. The PLM models serve as baselines for abstractive biomedical summarisation. The Long-Document Transformer (LED) is a Transformer-based models which are able to process long sequences due to their selfattention operation (Beltagy et al., 2020). PEGASUS (Zhang et al., 2020) is pre-training large Transformer-based encoder-decoder models on massive text corpora with a new self-supervised objective, which is tailored for abstractive text summarisation. BART (Lewis et al., 2019) is a widely used PLM model based on a denoising autoencoder that has proved effective for long text generation tasks. Pubmed-X refers to several PLM-based baselines that have been pre-trained on a largescale biomedical literature corpus Pubmed (Cohan et al., 2018) (the size of the dataset is 215k), where X denotes the name of a PLM. Additionally, we include ChatGPT for comparison. However, as it is close-source and very expensive for training, we were unable to use ChatGPT as the base language model to fine-tune on our dataset. Instead, we compare the outputs of ChatGPT in a zero-shot setting.\nEvaluation Metrics In the domain of text summarisation (Sun et al., 2021; Tang et al., 2022a; Xie et al., 2022), ROUGE (Lin, 2004) is the most used metric for the evaluation of generated summaries. For evaluating the quality of the generated summaries, we implement the ROUGE metric with the python package of rouge_score. Specifically, we report the unigram and bigram overlaps (ROUGE-1 and ROUGE-2, respectively) between the generated summaries and the reference (golden) summaries. Additionally, we include the longest common subsequence (ROUGE-L) metric to evaluate the fluency of the generated summaries. For each ROUGE metric, we provide fine-grained measurements of precision, recall, and F-values, offering a comprehensive assessment of the summarisation performance.\nIn addition to the ROUGE metric, we conduct an extensive automatic evaluation utilising a broader range of evaluation metrics. Specifically, we em-\n8In this study, the score referenced by ORACLE is calculated as the mean value of the ROUGE-1, ROUGE-2, and ROUGE-L scores.\nploy BERTScore (Zhang et al., 2019) (BeS) and BartScore (Yuan et al., 2021) (BaS) to assess the quality of the generated outputs. We also introduce some readability metrics, e.g. Flesch-Kincaid (FLK) and Coleman-Liau Index (CLI), to evaluate the readability of the generated text. This comprehensive evaluation allows for a more robust assessment of the summarisation performance across multiple dimensions."
        },
        {
            "heading": "5.2 Implementation Details",
            "text": "All of the pre-trained models used are restored from the publicly available checkpoints on Hugging Face9. The checkpoints we selected include: LED10, PEGASUS11, BART12, Pubmed-LED13, Pubmed-PEGASUS14, and Pubmed-BART15.\nTo make the comparison fair, all input text is chunked according to the minimal input size limitation of selected language models. In our experiments, it is BART (1024 tokens). Models are trained for up to 10 epochs on a Tesla A40 machine, which has 40 GB GPU memory, and the best checkpoints are kept based on the perplexity of generated responses during validation for the\n9https://huggingface.co/models 10https://huggingface.co/allenai/\nled-base-16384 11https://huggingface.co/google/pegasus-x-base 12https://huggingface.co/facebook/bart-base 13https://huggingface.co/Blaise-g/led_pubmed_ sumpubmed_1 14https://huggingface.co/google/pegasus-pubmed 15https://huggingface.co/mse30/ bart-base-finetuned-pubmed\ngeneration on the testset. The batch size is set to 16, and the learning rate is 1e\u22124, with the Adam optimizer selected for training. For more details, please refer to the Appendix A.1."
        },
        {
            "heading": "5.3 Automatic Evaluation",
            "text": "The results of all experiments are presented in Table 2. It can be observed that our proposed framework (-w citation agg.) significantly outperforms all baseline models across all ROUGE scores (F1 scores), indicating substantial enhancements in the summarisation capability of biomedical papers. To be more specific, the incorporation of citation knowledge has contributed to a substantial improvement in recall, with ROUGE-1 exhibiting a 5.7% increase and ROUGE-2 demonstrating an 8.1% increase. This suggests that the integration of citation\nknowledge has facilitated the utilisation of more similar expressions extracted from the reference abstracts.\nIn addition, within our framework, the language model achieves substantially lower perplexity (PPL) and ROUGE-L scores, signifying an improvement in the language quality and reduced confusion during summary generation. We hypothesise that the decrease in PPL and ROUGE-L indicates that the language model has learned writing styles and relevant biomedical terminologies by referring to the abstracts of cited papers.\nRegarding the ablation study, -w one citation yields a slight improvement compared to the baseline model Pubmed-BART but exhibits a higher perplexity. This observation suggests that the direct inclusion of random citation content may introduce certain noise. In contrast, our attention-based mechanism enables the neural networks to dynamically select and aggregate important information from multiple citations, effectively addressing confusion issues associated with additional inputs.\nIn Table 3, we present the results of additional evaluation metrics. BertScore and BartScore, as machine learning-based metrics, measure the semantic similarity between the generated summaries and the reference abstracts. Flesch-Kincaid and Coleman-Liau metrics assess text readability on a vocabulary level. Across all these metrics, -w citation agg. outperforms all baseline models, showcasing the advantages of introducing citation knowledge with our framework. Further analysis within the \"Pubmed-BART\" model reveals that using a single citation results in a slight decrease in BERTScore and BartScore, along with a slightly higher Flesch-Kincaid score (14.78) and ColemanLiau Index score (13.51). However, employing citation aggregation leads to improvements across all metrics, with BERTScore (83.60), BartScore (- 3.35), Flesch-Kincaid score (13.82), and ColemanLiau Index score (13.20). This analysis confirms our initial hypothesis, that directly introducing a random citation may introduce noise that hampers model performance, while our aggregation model comprehensively considers all citation papers, effectively reducing the random noise introduced by a single citation."
        },
        {
            "heading": "5.4 Human Evaluation",
            "text": "In order to obtain a more comprehensive evaluation of the generated summaries, we also incorporate\nhuman evaluation. This evaluation focuses on four key aspects: fluency, readability, relevance, and informativeness. Fluency assessment aims to measure the overall quality of the language used in the summaries. Readability evaluation determines the extent to which the summaries are easily understandable by readers. Relevance assessment examines whether the content of the summaries is pertinent and aligned with the content of the input document. Informativeness measurement evaluates the extent to which the generated summaries provide sufficient and meaningful information derived from the given input. By incorporating human evaluation, we can assess subjective aspects of summary quality that automated metrics may not fully capture.\nConsidering the difficulty of evaluating generated summaries, which requires a thorough understanding of the content in both the source papers and the summaries, it is imperative that human evaluators possess a strong background in academic writing and biomedical knowledge. We invite 3 qualified evaluators by snowball sampling to rate 30 randomly sampled instances from the testset. In order to minimise biases and increase interannotator agreement, the evaluators were provided with the same annotation guide (see Appendix A.3). The results of the human evaluation are presented in Table 4. It can be observed that both the - w one citation and - w citation agg. models exhibit superior performance compared to other baseline models, thereby affirming the effectiveness of our proposed framework.\nTo delve further into the evaluation, the metrics of Relevance and Informativeness underscore the improved capability to extract relevant information from the input content and generate comprehensive abstracts. Additionally, the fluency and readability metrics assess the language quality, indicating that the language model generates abstracts that are more coherent and natural. However, it is important to note that the tested Pretrained Language Models (PLMs) exhibited a notable disparity in language quality when compared to the performance of ChatGPT. This discrepancy can be attributed to the substantial difference in model size, with ChatGPT having 130 billion parameters, whereas the tested PLMs have less than 5 billion parameters."
        },
        {
            "heading": "5.5 In-depth Analysis",
            "text": "To further investigate the impact of the citation knowledge aggregation module, we conduct an evaluation to assess the improvement in the generated abstracts. This evaluation involves comparing the performance of our proposed framework, denoted as -w citation agg., against the base model Pubmed-BART using ROUGE scores. The results, presented in Figure 4 as (a), (b), and (c), illustrate the increase in ROUGE scores (F value) for different numbers of citations. The inclusion of citations is shown to have a positive effect on the abstract generation process. The Gaussian kernel smoothed increasing curve, depicted in Figure 4 (d), indicates a clear trend: as more citation abstracts are introduced, the language model exhibits greater improvements. The results highlight the potential of leveraging citation information to enhance the quality of generated abstracts."
        },
        {
            "heading": "6 Conclusion",
            "text": "In conclusion, we proposed a novel attentionbased citation aggregation model that incorporates domain-specific knowledge from citation papers. By integrating this additional information, our model enables neural networks to generate summaries that benefit from both the paper content and the associated knowledge extracted from citation papers. Furthermore, we introduced a specialized biomedical summarisation dataset, which served as a valuable resource for evaluating and advancing our research. The effectiveness of our approach was demonstrated through extensive experiments, where our model consistently outperformed stateof-the-art methods in biomedical text summarisation. The results highlight the significant improvements achieved by leveraging knowledge from citation papers and the potential for our model to enhance the understanding of biomedical literature through natural language generation techniques."
        },
        {
            "heading": "Acknowledgements",
            "text": "Chen Tang is supported by the China Scholarship Council (CSC) for his doctoral study (File No.202006120039). We also gratefully acknowledge the anonymous reviewers for their insightful comments.\nLimitations\nIn the field of text summarisation, two main approaches are commonly employed: extractive summarisation and abstractive summarisation. While extractive summarisation composes summaries by directly selecting sentences from the input content, abstractive summarisation generates summaries\nthat are not bound to the input content, providing greater flexibility but posing challenges in management and control. In this work, due to resource and time constraints, we focused on implementing an abstractive summarisation model and did not further conduct experiments to develop an extractive summarisation counterpart using our proposed algorithm. However, it is worth noting that our proposed approach has shown promising results, emphasizing the importance of leveraging citation papers to enhance the performance of language models in generating high-quality biomedical summaries. Theoretically, the aggregation of knowledge from citation papers can also be beneficial for extractive summarization approaches.\nEthics Statement\nOur new dataset is derived from an existing publicly available corpus released by the Allen Institute, which is a comprehensive biomedical literature corpus licensed under the Apache License 2.0. We have diligently adhered to the terms and conditions of the license and followed all provided instructions. Furthermore, we have familiarized ourselves with and acknowledged the ACM Code of Ethics and Professional Conduct16. We approach our professional responsibilities with utmost seriousness, ensuring that our study upholds ethical principles in every aspect."
        },
        {
            "heading": "A Appendices",
            "text": "A.1 Implementation Details ChatGPT Prompts The performance of ChatGPT is highly reliable on the quality of input prompts. We manually design and test prompts of abstract summarisation, and select the best cases as the experimental results.\nOthers The Gaussian kernel smoothing used in Figure 4 is implemented with the gaussian_filter1d function from the python package of scipy.ndimage. The ROUGE score evaluation is implemented\nwith the python package rouge_score. The readability scores such as Flesch-Kincaid (FLK) and Coleman-Liau Index (CLI), are implemented with the python package py-readability-metrics. BertScore is bert_score, and BartScore is from the GitHub repository of https://github.com/ neulab/BARTScore.\nA.2 Automatic Evaluation\nTable 6 shows the full results of BertScore and BartScore.\nA.3 Human Evaluation\nIn addition to automatic evaluation metrics, we conducted a comprehensive human evaluation to assess the quality of biomedical summarization generated by the different models. The human evaluation aimed to capture important aspects of summarization, including fluency, readability, and relevance.\nFor the human evaluation, we recruited a group of expert annotators with a strong background in biomedical research. The annotators were provided with a set of summaries generated by each model and were asked to rate them on a Likert scale ranging from 1 to 5. The Likert scale allowed annotators to provide a subjective assessment of the summaries based on their expertise and judgment. The four aspects evaluated in the human evaluation were as follows: Fluency: Annotators assessed the language quality and coherence of the summaries. They considered factors such as grammar, sentence structure, and overall fluency of the generated text. Higher ratings on the Likert scale indicated better fluency. Readability: Evaluators focused on the readability and comprehensibility of the summaries. They assessed whether the generated summaries were clear, concise, and understandable to a non-expert audience. Higher ratings indicated better readability. Relevance: An important criterion was the relevance of the summaries to the original input documents. Annotators evaluated whether the summaries captured the main ideas, key findings, and important concepts present in the source documents. Higher ratings indicated greater relevance. Informativeness: Evaluate the extent to which the generated summaries provide sufficient and meaningful information derived from the given input. Assess the comprehensiveness and completeness\nof the summary. Consider the inclusion of important details and relevant facts.\nBy utilising a Likert scale with a range of 1 to 5, we were able to capture nuanced evaluations from the annotators. This human evaluation provided valuable insights into the overall performance of the models from the perspectives of fluency, readability, and relevance, allowing us to gain a deeper understanding of their summarization capabilities in the biomedical domain.\nA.4 Future Works\nIn this paper, we propose a novel framework designed to enhance the performance of biomedical text summarisation using pre-trained language models. Recent years have witnessed the emergence of increasingly potent open-source language models, exemplified by Llama 217 and Baichuan18. However, the practical implementation of our approach on these immensely large-scale models has been constrained by high computational demands. Consequently, we anticipate the need for more advanced GPU hardware or optimised models, such as distilled models (Yang et al., 2023b), to render the training of these models feasible. Presently, there are two primary ways for advancing our re-\n17https://ai.meta.com/llama/. 18https://github.com/baichuan-inc/Baichuan2\nsearch in citation network-enriched text summarisation:\nand we have to wait for more advanced GPU devices or more optimised models (e.g. distilled models) to make training those models to be practical. Currently, there are two main direction to further improve our citaion networks enhanced text summarisation: (1) The development of a more efficient neural network that can effectively incorporate the graph-based features derived from citations (Tang et al., 2023a; Yang et al., 2023a). (2) The identification and extraction of key information from both the input document and its associated citations to enhance language understanding (Huang et al., 2022; Tang et al., 2022c). We defer the exploration of these directions to future research endeavors."
        }
    ],
    "title": "Improving Biomedical Abstractive Summarisation with Knowledge Aggregation from Citation Papers",
    "year": 2023
}