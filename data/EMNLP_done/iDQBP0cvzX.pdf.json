{
    "abstractText": "Temporal question answering (QA) is a special category of complex question answering task that requires reasoning over facts asserting time intervals of events. Previous works have predominately relied on Knowledge Base Question Answering (KBQA) for temporal QA. One of the major challenges faced by these systems is their inability to retrieve all relevant facts due to factors such as incomplete KB and entity/relation linking errors (Patidar et al., 2022). A failure to fetch even a single fact will block KBQA from computing the answer. Such cases of KB incompleteness are even more profound in the temporal context. To address this issue, we explore an interesting direction where a targeted temporal fact extraction technique is used to assist KBQA whenever it fails to retrieve temporal facts from the KB. We model the extraction problem as an open-domain question answering task using offthe-shelf language models. This way, we target to extract from textual resources those facts that failed to get retrieved from the KB. Experimental results on two temporal QA benchmarks show promising ~30% & ~10% relative improvements in answer accuracies without any additional training cost.",
    "authors": [
        {
            "affiliations": [],
            "name": "Nithish Kannen"
        },
        {
            "affiliations": [],
            "name": "Udit Sharma"
        },
        {
            "affiliations": [],
            "name": "Sumit Neelam"
        },
        {
            "affiliations": [],
            "name": "Dinesh Khandelwal"
        },
        {
            "affiliations": [],
            "name": "Shajith Ikbal"
        },
        {
            "affiliations": [],
            "name": "Hima Karanam"
        },
        {
            "affiliations": [],
            "name": "L Venkata Subramaniam"
        }
    ],
    "id": "SP:8feea82311bc18463ab8fc9182dac89884ffcd96",
    "references": [
        {
            "authors": [
                "Abdalghani Abujabal",
                "Mohamed Yahya",
                "Mirek Riedewald",
                "Gerhard Weikum."
            ],
            "title": "Automated template generation for question answering over knowledge graphs",
            "venue": "Proceedings of the 26th International Conference on World Wide Web, WWW 2017, Perth,",
            "year": 2017
        },
        {
            "authors": [
                "Laura Banarescu",
                "Claire Bonial",
                "Shu Cai",
                "Madalina Georgescu",
                "Kira Griffitt",
                "Ulf Hermjakob",
                "Kevin Knight",
                "Philipp Koehn",
                "Martha Palmer",
                "Nathan Schneider"
            ],
            "title": "Abstract meaning representation",
            "year": 2013
        },
        {
            "authors": [
                "Nikita Bhutani",
                "Xinyi Zheng",
                "Kun Qian",
                "Yunyao Li",
                "H. Jagadish."
            ],
            "title": "Answering complex questions by combining information from curated and extracted knowledge bases",
            "venue": "Proceedings of the First Workshop on Natural Language Interfaces, pages 1\u201310,",
            "year": 2020
        },
        {
            "authors": [
                "Rishav Chakravarti",
                "Cezar Pendus",
                "Andrzej Sakrajda",
                "Anthony Ferritto",
                "Lin Pan",
                "Michael Glass",
                "Vittorio Castelli",
                "J William Murdock",
                "Radu Florian",
                "Salim Roukos",
                "Avi Sil."
            ],
            "title": "CFO: A framework for building production NLP systems",
            "venue": "Proceedings of",
            "year": 2019
        },
        {
            "authors": [
                "Tarc\u00edsio Souza Costa",
                "Simon Gottschalk",
                "Elena Demidova."
            ],
            "title": "Event-qa: A dataset for eventcentric question answering over knowledge graphs",
            "venue": "CoRR, abs/2004.11861.",
            "year": 2020
        },
        {
            "authors": [
                "Bin Fu",
                "Yunqi Qiu",
                "Chengguang Tang",
                "Yang Li",
                "Haiyang Yu",
                "Jian Sun."
            ],
            "title": "A survey on complex question answering over knowledge base: Recent advances and challenges",
            "venue": "CoRR, abs/2007.13069.",
            "year": 2020
        },
        {
            "authors": [
                "Zhen Jia",
                "Abdalghani Abujabal",
                "Rishiraj Saha Roy",
                "Jannik Str\u00f6tgen",
                "Gerhard Weikum."
            ],
            "title": "Tempquestions: A benchmark for temporal question answering",
            "venue": "Companion of the The Web Conference 2018 on The Web Conference 2018, WWW 2018,",
            "year": 2018
        },
        {
            "authors": [
                "Zhen Jia",
                "Abdalghani Abujabal",
                "Rishiraj Saha Roy",
                "Jannik Str\u00f6tgen",
                "Gerhard Weikum."
            ],
            "title": "TEQUILA: temporal question answering over knowledge bases",
            "venue": "Proceedings of the 27th ACM International Conference on Information and Knowledge",
            "year": 2018
        },
        {
            "authors": [
                "Zhen Jia",
                "Soumajit Pramanik",
                "Rishiraj Saha Roy",
                "Gerhard Weikum."
            ],
            "title": "Complex temporal question answering on knowledge graphs",
            "venue": "Proceedings of the 30th ACM international conference on information & knowledge management, pages 792\u2013802.",
            "year": 2021
        },
        {
            "authors": [
                "Ndivhuwo Makondo",
                "Nandana Mihindukulasooriya",
                "Tahira Naseem",
                "Sumit Neelam",
                "Lucian Popa",
                "Revanth Gangi Reddy",
                "Ryan Riegel",
                "Gaetano Rossiello",
                "Udit Sharma",
                "G.P. Shrivatsa Bhargav",
                "Mo Yu"
            ],
            "title": "Leveraging abstract meaning representation",
            "year": 2021
        },
        {
            "authors": [
                "Patrick S.H. Lewis",
                "Ethan Perez",
                "Aleksandra Piktus",
                "Fabio Petroni",
                "Vladimir Karpukhin",
                "Naman Goyal",
                "Heinrich K\u00fcttler",
                "Mike Lewis",
                "Wen-tau Yih",
                "Tim Rockt\u00e4schel",
                "Sebastian Riedel",
                "Douwe Kiela"
            ],
            "title": "Retrieval-augmented generation",
            "year": 2020
        },
        {
            "authors": [
                "Costas Mavromatis",
                "Prasanna Lakkur Subramanyam",
                "Vassilis N. Ioannidis",
                "Soji Adeshina",
                "Phillip R. Howard",
                "Tetiana Grinberg",
                "Nagib Hakim",
                "George Karypis"
            ],
            "title": "Tempoqr: Temporal question reasoning over knowledge graphs",
            "year": 2021
        },
        {
            "authors": [
                "nesh Khandelwal",
                "Srinivas Ravishankar",
                "Sairam Gurajada",
                "Maria Chang",
                "Rosario Uceda-Sosa",
                "Salim Roukos",
                "Alexander Gray",
                "Guilherme LimaRyan Riegel",
                "Francois Luus",
                "L Venkata Subramaniam"
            ],
            "title": "Sygma: System for generalizable modular",
            "year": 2021
        },
        {
            "authors": [
                "Khandelwal",
                "Srinivas Ravishankar",
                "Sairam Gurajada",
                "Maria Chang",
                "Rosario Uceda-Sosa",
                "Salim Roukos",
                "Alexander Gray",
                "Guilherme LimaRyan Riegel",
                "Francois Luus",
                "L Venkata Subramaniam"
            ],
            "title": "Sygma: System for generalizable modular question",
            "year": 2022
        },
        {
            "authors": [
                "Sumit Neelam",
                "Udit Sharma",
                "Hima Karanam",
                "Shajith Ikbal",
                "Pavan Kapanipathi",
                "Ibrahim Abdelaziz",
                "Nandana Mihindukulasooriya",
                "Young-Suk Lee",
                "Santosh Srivastava",
                "Cezar Pendus"
            ],
            "title": "2022b. A benchmark for generalizable and interpretable temporal ques",
            "year": 2022
        },
        {
            "authors": [
                "Pranav Rajpurkar",
                "Jian Zhang",
                "Konstantin Lopyrev",
                "Percy Liang."
            ],
            "title": "SQuAD: 100,000+ questions for machine comprehension of text",
            "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383\u20132392, Austin,",
            "year": 2016
        },
        {
            "authors": [
                "Nils Reimers",
                "Iryna Gurevych."
            ],
            "title": "Sentence-bert: Sentence embeddings using siamese bert-networks",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural",
            "year": 2019
        },
        {
            "authors": [
                "Apoorv Saxena",
                "Soumen Chakrabarti",
                "Partha Talukdar."
            ],
            "title": "Question answering over temporal knowledge graphs",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference",
            "year": 2021
        },
        {
            "authors": [
                "Apoorv Saxena",
                "Aditay Tripathi",
                "Partha Talukdar."
            ],
            "title": "Improving multi-hop question answering over knowledge graphs using knowledge base embeddings",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4498\u2013",
            "year": 2020
        },
        {
            "authors": [
                "Jiaxin Shi",
                "Shulin Cao",
                "Liangming Pan",
                "Yutong Xiang",
                "Lei Hou",
                "Juanzi Li",
                "Hanwang Zhang",
                "Bin He"
            ],
            "title": "Kqa pro: A large-scale dataset with interpretable programs and accurate sparqls for complex question answering over knowledge base",
            "year": 2020
        },
        {
            "authors": [
                "Haitian Sun",
                "Tania Bedrax-Weiss",
                "William W. Cohen."
            ],
            "title": "Pullnet: Open domain question answering with iterative retrieval on knowledge bases and text",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and",
            "year": 2019
        },
        {
            "authors": [
                "Haitian Sun",
                "Bhuwan Dhingra",
                "Manzil Zaheer",
                "Kathryn Mazaitis",
                "Ruslan Salakhutdinov",
                "William W. Cohen."
            ],
            "title": "Open domain question answering using early fusion of knowledge bases and text",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods",
            "year": 2018
        },
        {
            "authors": [
                "Priyansh Trivedi",
                "Gaurav Maheshwari",
                "Mohnish Dubey",
                "Jens Lehmann."
            ],
            "title": "Lc-quad: A corpus for complex question answering over knowledge graphs",
            "venue": "Proceedings of the 16th International Semantic Web Conference (ISWC), pages 210\u2013218. Springer.",
            "year": 2017
        },
        {
            "authors": [
                "Svitlana Vakulenko",
                "Javier David Fernandez Garcia",
                "Axel Polleres",
                "Maarten de Rijke",
                "Michael Cochez."
            ],
            "title": "Message passing for complex question answering over knowledge graphs",
            "venue": "Proceedings of the 28th ACM International Conference on Informa-",
            "year": 2019
        },
        {
            "authors": [
                "Jason Wei",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Maarten Bosma",
                "Brian Ichter",
                "Fei Xia",
                "Ed Chi",
                "Quoc Le",
                "Denny Zhou"
            ],
            "title": "Chain-of-thought prompting elicits reasoning in large language models",
            "year": 2023
        },
        {
            "authors": [
                "Tomer Wolfson",
                "Mor Geva",
                "Ankit Gupta",
                "Yoav Goldberg",
                "Matt Gardner",
                "Daniel Deutch",
                "Jonathan Berant."
            ],
            "title": "Break it down: A question understanding benchmark",
            "venue": "Trans. Assoc. Comput. Linguistics, 8:183\u2013198.",
            "year": 2020
        },
        {
            "authors": [
                "Dekun Wu",
                "Nana Nosirova",
                "Hui Jiang",
                "Mingbin Xu."
            ],
            "title": "A general fofe-net framework for simple and effective question answering over knowledge bases",
            "venue": "CoRR, abs/1903.12356.",
            "year": 2019
        },
        {
            "authors": [
                "Peiyun Wu",
                "Yunjie Wu",
                "Linjuan Wu",
                "Xiaowang Zhang",
                "Zhiyong Feng."
            ],
            "title": "Modeling global semantics for question answering over knowledge bases",
            "venue": "CoRR, abs/2101.01510.",
            "year": 2021
        },
        {
            "authors": [
                "Wenhan Xiong",
                "Mo Yu",
                "Shiyu Chang",
                "Xiaoxiao Guo",
                "William Yang Wang."
            ],
            "title": "Improving question answering over incomplete kbs with knowledgeaware reader",
            "venue": "Proceedings of the 57th Conference of the Association for Computational Linguis-",
            "year": 2019
        },
        {
            "authors": [
                "Wen-tau Yih",
                "Ming-Wei Chang",
                "Xiaodong He",
                "Jianfeng Gao."
            ],
            "title": "Semantic parsing via staged query graph generation: Question answering with knowledge base",
            "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics",
            "year": 2015
        },
        {
            "authors": [
                "Luke S. Zettlemoyer",
                "Michael Collins"
            ],
            "title": "Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars",
            "year": 2012
        },
        {
            "authors": [
                "Zhuosheng Zhang",
                "Yuwei Wu",
                "Junru Zhou",
                "Sufeng Duan",
                "Hai Zhao",
                "Rui Wang."
            ],
            "title": "Sg-net: Syntax-guided machine reading comprehension",
            "venue": "The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innova-",
            "year": 2020
        },
        {
            "authors": [
                "Zhuosheng Zhang",
                "Junjie Yang",
                "Hai Zhao."
            ],
            "title": "Retrospective reader for machine reading comprehension",
            "venue": "Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence,",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Complex Question Answering involves the integration of multiple facts identified and extracted from disjoint pieces of information (Vakulenko et al., 2019; Saxena et al., 2020; Fu et al., 2020; Neelam et al., 2022a). Two critical components in systems trying to achieve this are: 1) Knowledge Source - to retrieve relevant facts, and 2) Reasoning - to reason over relevant facts to derive the final answer. Both\n\u00a7This work was done when Nithish Kannen was at IBM Research, India as a Summer Intern. At that time he was also an (integrated bachelor\u2019s+master\u2019s) Electrical Engineering Student at IIT Kharagpur, India.\n\u2217Correspondence e-mails: nithishkannen@gmail.com, shajmoha@in.ibm.com, hkaranam@in.ibm.com\nNLP and Semantic Web communities have shown immense interest in this problem lately. Work in the NLP community has primarily focused on using textual data as a knowledge source, and typically uses deep-neural models to represent knowledge and for reasoning. While those approaches achieve impressive accuracies (Zhang et al., 2021, 2020), they are typically limited in their reasoning capabilities. Although reasoning is starting to receive attention in NLP community (Wei et al., 2023), reasoning over large amounts of unstructured knowledge in text is still a challenge. Work in the Semantic Web community has primarily focused on Knowledge Base Question Answering (KBQA), where a KB is used to retrieve facts. While betterequipped for complex reasoning (Bhutani et al., 2020; Wu et al., 2021), they typically suffer from incomplete knowledge (Patidar et al., 2022).\nIn this paper, we present a novel combination of successful elements from past approaches, i.e., KB-based and text-corpus based, for complex QA, in a way to overcome their individual limitations.\nOur combination strategy is to use a targeted extraction technique that would assist KBQA whenever it fails. KBQA failure is when it fails to retrieve relevant facts needed to answer the question from the KB because of reasons such as incomplete KB and inaccurate entity/relation linking. Identification of these specific points of KBQA failure is a critical step in our approach, which is enabled by decomposition of the question\u2019s logical representation obtained by semantic parsing. The targeted extraction technique compensates for those KB failures by extracting facts from textual resources in an open-domain QA fashion. Concretely, we make effective utilization of the KB (reliable but not exhaustive) and the textual resources (vast but noisy), essentially combining the best of both worlds.\nIn this work, we focus on temporal questions. They additionally involve reasoning over temporal facts, i.e., assertions on points and intervals in time of events1. For example, to answer Who was the President of the United States during World War 2?, we need to know the temporal facts of both World War 2 and the list of all US presidents.\nThe main contributions of our work are: 1) A novel combination of KBQA and textual-extraction for temporal question answering. 2) \u03bb-calculus based semantic representation to identify KBQA gaps. 3) An open-domain QA style approach for targeted extraction of temporal facts from textual resources using off-the-shelf models. We show that it is possible to achieve significant improvements on two temporal QA benchmarks even without any task-specific training and believe the promising initial results will foster research in this direction. Related Work: GRAFT-Net (Sun et al., 2018) and PullNet (Sun et al., 2019; Xiong et al., 2019) utilize both KB and textual resources but do not address temporal reasoning. Unlike approaches using end-to-end neural models, we adopt a modular approach as in (Neelam et al., 2022a) because of the flexibility it offers to combine textual extraction with the KBQA, with additional benefits such as interpretability and easy domain-adaptation. Another main difference in our textual extraction approach is the usage of LMs off-the-shelves, hence bypassing the need for domain-specific training and datasets that are hard to obtain. A detailed literature review is presented in A.6.\n1In this paper, we consider entities and facts (represented as triples in KBs) with associated time intervals as events. For example, {World War 2, (start time:1939, end time:1945)}."
        },
        {
            "heading": "2 Our Approach",
            "text": "Figure 2 shows a block diagram of our proposed approach, with two groups of modules: 1) Upper line of KBQA pipeline modules, and 2) Lower line of Extraction pipeline modules for targeted fact extraction. Our strategy is to bank on the KBQA pipeline, owing to its reliability, and trigger Extraction pipeline only to aid KBQA when it fails."
        },
        {
            "heading": "2.1 KBQA Pipeline",
            "text": "For KBQA pipeline, we adopt a modular design as in (Neelam et al., 2022a; Kapanipathi et al., 2021), with two high-level modules: (1) Question Understanding to derive logical semantic representation of the NL question and to further decompose it. (2) KB Linking and Answering to ground the Entity and Relation mentions in the logical representation onto the KB. Both these modules in our approach are similar to (Neelam et al., 2022a) except for the event specific decomposition described next.\nWe use \u03bb-expression constructed from Abstract Meaning Representation (AMR)(Banarescu et al., 2013) for logical semantic representation of the questions (Neelam et al., 2022a). Figure 1 gives an illustration of NL question, its AMR and \u03bbexpression. As shown, the \u03bb-expression compactly represents the mentions of events in the question as its sub-components, facts about those events needed from the knowledge source, and the reasoning steps needed to derive the final answer. We decompose \u03bb-expression into two components to help localize the points of KB failures: 1) Main-\u03bb: Part of the \u03bb-expression related to the unknown variable, i.e., main event being questioned. In Figure 1, a is the unknown variable, whose value if found is the answer. 2) Aux-\u03bb: part of the \u03bb-expression not related to the unknown variable, but related to the other events in the question as shown in Figure 1 . This part adds temporal constraints on the answer candidates. We use a simple rule-based approach to perform this decomposition, where unknown variable is used as anchor to segregate the respective components. We provide additional details in A.1."
        },
        {
            "heading": "2.2 Targeted Temporal Fact Extraction",
            "text": "We know a KBQA failure has occurred when the complete lambda expression fails to return an answer from the KB. The first critical step in our approach is to localize points of KBQA failure within the lambda expression. For this, we examine answers derived independently for the aux-\u03bb\nand main-\u03bb using the KBQA pipeline in order to classify the failure into one of the below cases2: \u2022 Aux Failure: Temporal fact corresponding to the\ntemporal constraint (aux-\u03bb) is missing in the KB. For example, in Figure 1, when time interval of World War 2 is missing in the KB. \u2022 Main Failure. Temporal fact(s) corresponding to the unknown variable is missing in the KB. For example, in Figure 1, when time interval of Franklin D. Roosevelt (the US president during WW2) as president in office is missing.\nThe goal now is to extract the identified missing facts from textual resources using the extraction pipeline (described next). In this way, we assist KBQA using targeted fact extraction, which has several advantages: 1) A focused extraction from text (guided by KB) makes textual fact extraction, which is usually noisy, reliable. 2) The complementing strengths of textual resources and KBs are leveraged to overcome their individual limitations. Aux Failure. Answer for aux-\u03bb is a time interval, i.e., composed of time intervals of all the events part of aux-\u03bb, that imposes temporal constraints on the answer candidates. We extract the KB missing fact through the extraction pipeline and construct a reformed \u03bb-expression, which is simply replacing the auxiliary part of the original \u03bb-expression with the extracted time interval. This will enable answer derivation without fetching facts related to aux-\u03bb from KB. We show an example of this in A.3.1. Main Failure: Here we attempt to extract missing facts corresponding to the main-\u03bb. However, we do not fully rely on textual resources for main-\u03bb, because it represents event with unknown variable.\n2Note that there could also be a combination of failures. We use the flow sequence in Algorithm 1 to handle all cases.\nWe take that part of the main \u03bb-expression that would fetch the answer candidates from the KB (leaving out the temporal fact specific components), and pass that onto the KBQA pipeline. For each answer candidate obtained, we extract the temporal information via the extraction pipeline as in A.3.2. Temporal Reasoning. Upon successful gathering of all the temporal facts (i.e., time intervals), either from the KB or from text, we use Temporal Reasoning module to select the final answer from among the answer candidates of the main-\u03bb. For example in Figure 1, \u03bb-expression component overlap(hi, ti) corresponds to the temporal reasoning step, which essentially represents the overlap between the time intervals ti (of Ww2) and hi (of answer candidates of the main-\u03bb). Candidates that comply with the reasoning condition are chosen as the final answer. Other temporal reasoning categories handled include before, after, now etc,. Algorithm 1 describes our overall flow sequence."
        },
        {
            "heading": "2.3 Extraction pipeline",
            "text": "There are 3 scenarios where we may look to extract facts from textual resources: 1) KBQA fails for aux-\u03bb because of Linking failure, 2) KBQA fails for aux-\u03bb because of missing temporal fact, and 3) KBQA fails for main-\u03bb because of missing temporal fact. We pose extraction as an open-domain QA problem where: 1) for each missing fact, the corresponding NL query is generated, 2) top-k passages relevant to the NL query are retrieved from textual resources, and 3) a Reading Comprehension QA (RCQA) style answer derivation is performed by treating NL query as the question and top-k retrieved passages as the context. We show that even a naive pipeline with pre-trained LMs as the back-\nbone demonstrates significant improvements and argue that an improved (domain-trained) pipeline will only further boost performance. \u03bb to NL. For example, for the aux-\u03bb in Figure 1, the equivalent NL query is When was WW2?. We use simple rules to convert \u03bb of the missing facts into its corresponding NL query. Since we deal with temporal facts, all the queries start with When, and we add was or did depending on whether the event being considered is entity-based or triple-based. Document retrieval. For each NL query, we perform document retrieval in 2 steps: first, get a list of relevant documents and then choose a top-k scored list of passages from them. (1) Entities of the question are extracted using BLINK (Wu et al., 2019) and Wikipedia pages of all the entities are collected allowing minimal lexical variations3. We also use NL text query generated from \u03bb-expression to search on MediaWiki API4. (2) We use Siamese-BERT networks (Reimers and Gurevych, 2019) to rank passages within the retrieved documents. The Bi-Encoder picks out top50 relevant passages based on passage-query similarity and the Cross-Encoder re-ranks them. We use public checkpoints5 trained on the MS-MARCO dataset (Bajaj et al., 2018). QA based fact extraction. We use top-3 ranked passages as context to derive answer for the NL query in Reading Comprehension QA (RCQA) style. We use BERT6 trained on the SQUAD (Rajpurkar et al., 2016)) dataset as the QA model. Note that RCQA style extraction gives one answer, which is sufficient for point-in-time extraction. For time intervals (start and end times) we further take the sentence identified by the RCQA model and use its AMR tree to obtain time interval by examining sibling nodes to that containing the RCQA answer."
        },
        {
            "heading": "3 Experimental Setup",
            "text": "We used Wikidata as KB and Wikipedia as textual resource, and evaluate on two aspects: 1) Temporal QA performance of the overall system and 2) Targeted temporal fact extraction performance. We experiment with 2 datasets: 1) TempQA-WD (Neelam et al., 2022b) with 839 questions and 2) TimeQuestions (Jia et al., 2021) (Train-9708, Test-3237). Baselines: 1) Only-KB - answers only from KBQA,\n3https://pypi.org/project/fuzzywuzzy/ 4https://www.mediawiki.org/wiki/Download 5https://www.sbert.net/ 6https://huggingface.co/\n2) KB+TemporalText - extracting temporal facts only from text and the remaining facts from KB, and 3) Open-Domain-QA - A state-of-the-art opendomain-QA system called RAG (Lewis et al., 2020) that answers purely from text. RAG consists of a retriever based on Dense Passage retrieval (DPR) and a generator based on BART jointly trained. We refer the readers to the original paper for more details on this baseline."
        },
        {
            "heading": "3.1 Implementation Details",
            "text": "Our system pipeline is implemented using Flow Compiler7 (Chakravarti et al., 2019) that stitches together the gRPC services of the individual modules. \u03bb-expressions are defined using ANTLR grammer. SPARQL queries are run on public Wikidata end point8. We reuse the KBQA pipeline implementation of (Neelam et al., 2022b)."
        },
        {
            "heading": "4 Results and Discussion",
            "text": "Table 1 shows performance comparison of our approach (KB+Text) against the baselines on TempQA-WD dataset. KB+Text achieves an improvement of 0.095 in F1 score (\u223c30% relative improvement) over Only-KB, demonstrating the effectiveness of our approach in making a targeted utilization of the textual resources to assist KBQA. Comparison to KB+TemporalText illustrates the reliability of facts obtained from KB, whenever available. Performance of Open-Domain QA is inferior to Only-KB, illustrating the superior reasoning capability of KBQA systems.\nSince extraction pipeline is a critical component in our system, we also evaluate its independent accuracy using a small set of 3709 NL queries (added in supplementary material) generated from our system for facts existing in KB. The extraction pipeline performs better than a state-of-the-art open domain QA system as shown in Table 3.\nDue to discrepancies discussed in A.4, we could not use our KBQA pipeline to evaluate on TimeQuestions dataset. Alternatively, we used the end-\n7https://github.com/IBM/flow-compiler 8https://query.wikidata.org/\nto-end trained EXAQT (Jia et al., 2021) system as the baseline KBQA system that was purposebuilt for the specific dataset. With this, we then built an equivalent of our proposed approach called EXAQT+Text, by running our Extraction pipeline wherever EXAQT fails to answer. The performance of EXAQT+Text is better for all threshold values with improvement reaching upto 10% as shown in Table 4 . This demonstrates that our approach of targeted extraction backing up KBQA is resilient to changes in underlying KBQA. In order to benefit future work, we conduct an error analysis by manually examining 50 randomly selected test samples from TempQA-WD that were incorrectly answered by out proposed system. We classify the error into following types: 1) Incorrect extraction: the text pipeline extracted incorrect facts, 2) KBQA error: KBQA produced erroneous answers after missing fact was correctly extracted from text, 3) Parsing error: incorrect parsing of the question, 4) Miscellaneous: consists of miscellaneous issues like lexical and date representation variations. Table 2 shows the results of the experiment and further highlights the scope of improvement in the extraction pipeline. Figure 3 is an illustration of how a KB Aux-Failure is handled in our pipeline. The\nmissing temporal fact \u20191924\u2019 is extracted from text and the \u03bb-expression is reformed.\nCode and experimental setup used for our experiments is at https://github.com/IBM/tempqawd/tree/main/targeted-extraction"
        },
        {
            "heading": "5 Conclusion",
            "text": "In this paper, we propose an approach to combine the knowledge resources of KB (structured) and text (unstructured) for temporal QA by using textual resources to aid wherever KBQA fails. In our approach, a semantic representation of the question as \u03bb-expression is used to represent the set of facts and reasoning required to answer a question. Those facts that failed to get fetched from KBQA are extracted from the textual resources using RCQA style fact extraction. This way, we perform targeted extraction of temporal facts to compensate for KBQA failures. The results of experimental evaluation on two temporal QA benchmark show the effectiveness of our approach even without any additional training cost. We additionally conduct error analysis to highlight the scope of improvement in order to guide future work."
        },
        {
            "heading": "6 Limitations",
            "text": "The proposed approach focuses only on temporal reasoning and work is required to generalize it across other reasoning types. In this work we addressed temporal QA as we identified missing KB facts as a major pain point in the temporal context (adding temporal facts to KBs has started gaining momentum only in recent times). The proposed approach will have to be modified to handle more complex questions that can feature multiple aux-\u03bb (temporal constraints). Our \u03bb decomposition algorithm that is hand-crafted works well for temporal questions, but may have to be tested for other reasoning tasks with appropriate modifications. It would be interesting to come up with learning-based methods for the same. The Extraction pipeline is triggered only when KBQA fails to return answer. However KBQA approaches can also produce wrong answers and we can potentially develop methods to predict such errors. To aid future research, we provide a detailed error analysis in Appendix highlighting areas that require improvement. Our Extraction pipeline has a large scope of improvement. The document retrieval approach is purely based on lexical overlap and considering query semantics would improve it. Our QA and ranker LMs can be finetuned for the domain in-focus. Alternatively, training endto-end neural extraction modules remains to be investigated. Performance metrics used to evaluate KBQA systems do not account for mismatches arising because of lexical variations or date representation variations. Using a better metric can provide a better picture of the overall performance."
        },
        {
            "heading": "A Appendix",
            "text": "A.1 Details of KBQA pipeline (Neelam et al., 2021)\nHere we describe the modules in the KBQA pipeline that is adopted from (Neelam et al., 2021)."
        },
        {
            "heading": "A.1.1 Question Understanding",
            "text": "The goal of Question Understanding is to 1) transform NL questions into corresponding \u03bbexpressions that logically represent the set of eventspecific facts needed from the KB and the reasoning needed to be performed to derive the answer and 2) further perform event-specific decomposition. We use method as in (Neelam et al., 2022a) to construct \u03bb-expressions of the questions from their AMR (Abstract Meaning Representation) (Banarescu et al., 2013). AMR encodes meaning of the sentence into a rooted directed acyclic graph where nodes and edges represent concepts and relations respectively. Such a representation is useful because event-specific decomposition of the question is represented to some extent as sub-paths and sub-graphs in the AMR graph. Figure 1 shows an illustration of AMR and \u03bb-expression for example question. This example illustrates how \u03bbexpression compactly represents, the mentions of events in the question (as its sub-components), facts about those events (that need to be fetched from the knowledge source), and the reasoning steps (that need to be performed to derive the final answer). \u03bb-expression constructed for the question is further processed to decompose into components:\n1. Main-\u03bb: Part of the \u03bb-expression related to the unknown variable, i.e., main event being questioned. For example in Figure 1, a is the unknown variable, whose value if found is answer to the question. 2. Aux-\u03bb: part of the \u03bb-expression not related to the unknown variable, but related to the rest of the events mentioned in the question. This part serves the purpose of adding temporal constraint to the candidate answer values for the unknown variable.\nWe use a rule based approach to perform decomposition, that simply uses unknown variable as anchor to segregate the respective components. Note that the decomposed \u03bb-expressions play a critical role in our approach to identify the points of failures of the KBQA pipeline and to further decide on the use of Extraction pipeline."
        },
        {
            "heading": "A.1.2 KB Linking and Answering",
            "text": "This is essentially a step to ground the Entity and Relation mentions of \u03bb-expression to the KB, i.e., map the elements of \u03bb-expression onto the corresponding KB elements. Relation mentions are the predicates (for example, have-org-role in Figure 1) and entity mentions are the arguments (for example, United States, president, and Ww2 in Figure 1). The goal of linking is to map, for example Ww2 to a node in KB corresponding to World War 2 (Wikidata id wd:Q362). After linking, we generate corresponding SPARQL queries that when executed on the KB endpoint would fetch the intended KB facts. Our approach to linking and SPARQL generation is similar to that of (Neelam et al., 2022a)."
        },
        {
            "heading": "A.2 Targeted Extraction Algorithm",
            "text": "Algorithm 1 Algorithm for the overall approach illustrated in Figure 2 and its flow sequence.\nlambda = GetLambda(question) ans_list = GetKBAnswer(lambda) if ans_list is empty then \u25b7 KBQA Failure\nmain, aux = Decompose_Lambda(lambda) ans_list = GetKBAnswer(aux) if ans_list is empty then \u25b7 Aux Failure\nfact = Extract_From_Text(aux) aux_fact = fact \u25b7 for later use reformed_lambda = ReformLambda(fact, lambda) ans_list = GetKBAnswer(reformed_lambda) if ans_list is not empty then\nreturn \u25b7 Ans Found end if\nend if ans_list = GetKBAnswer(main) candidate_facts = [] for candidate in ans_list do\nfact = Extract_From_Text(candidate) candidate_facts.append(fact, candidate)\nend for ans = TemporalReasoner(candidate_facts, auxfact)\nreturn \u25b7 Ans Found else\nreturn \u25b7 Ans Found, No missing fact in KB end if"
        },
        {
            "heading": "A.3 Details of Targeted Extraction from Text",
            "text": "Our goal is to use textual resources to assist KBQA failures, which can happen for two reasons: 1. Linking failure - when KB linking step fails to successfully map mentions in the \u03bb-expression to the corresponding KB entities and relations. For example, in Figure 1 when mention Ww2 fails to get linked to the World War 2 node in the KB.\n2. Missing facts - KBs are known to be incomplete, and hence may fail to fetch a specific fact, simply because it is not present in the KB. For example, if temporal information corresponding to World War 2 is not present in the KB, attempt to fetch time interval corresponding to \u03bb-expression part interval(ti, \u201cWw2\") would fail. \u03bb-expression specifies all facts that need to be fetched from the KB. A failure to fetch even a single fact would block KBQA from computing the final answer. To handle failures we need to know the specific facts that failed to get fetched from the KB, so that we can look for them in the textual resources. For this purpose, we categorize KBQA failure as below, based on the decomposed \u03bb-expressions where failure happens into Aux Failure and Main Failure. We present the flow sequence in Algorithm 1."
        },
        {
            "heading": "A.3.1 Aux Failure",
            "text": "This issue arises due to missing temporal fact corresponding to the aux-\u03bb in Upon successful extraction of time interval for aux-\u03bb, from textual resources, we construct a reformed \u03bb-expression from the original \u03bb-expression by simply replacing auxiliary part with the time interval of aux-\u03bb. For example, \u03bb-expression in Figure 1 will be reformed as: \u03bb a. have-org-role-91(h, a, \"United States\",\n\"president\") \u2227 interval(hi, h) \u2227 overlap(hi, (interval_start:1939-09-01, interval_end:194509-02)).\nNote that this reformed \u03bb-expression does not contain aux-\u03bb. Its NL equivalent is Who was the President of the United States during period from 1st September 1939 to 2nd September 1945? Thus if reformed \u03bb-expression is passed onto the KBQA pipeline (instead of original \u03bb-expression), it should result in the same answer, but without the need to fetch facts related to aux-\u03bb from the KB."
        },
        {
            "heading": "A.3.2 Main Failure",
            "text": "For example, in Figure 1 main-\u03bb corresponds to the list of all the US presidents and their time intervals in office as the president. We make an assumption that we can always get the list of all answer candidates from the KB itself, but may need to look into textual resources only for temporal fact about them. For example, in Figure 1 we take that part of the main \u03bb-expression that would fetch the answer candidates from the KB (leaving out the temporal fact specific components), i.e.,\n\u03bb a. have-org-role-91(h, a, \"United States\", \"president\")\nand pass that onto the KBQA pipeline. Then for each answer candidate obtained we try to extract time interval from the textual resource. For example, if Franklin D. Roosenvelt is one of the answer candidates, we try to extract the time interval of Franklin D. Roosenvelt being the US president from the textual resources.\nNote that we resort to extraction from textual resources only for those facts that failed to get fetched from the KB. We believe this approach of targeted extraction from the text is likely to be more accurate than unrestricted extraction, because we are looking to extract facts with a set of known variables and only one unknown variable."
        },
        {
            "heading": "A.4 Discrepancies in Evaluating on TimeQuestions",
            "text": "We could not directly evaluate our approach on TimeQuestions because a few discrepancies obseverd in that datasets such as, invalid answers (with change in time) and incorrect porting of the answers from old datasets without verifying validity on Wikidata. It is because TimeQuestions (Jia et al., 2021) is built from older datasets on different KBs, by mapping only the final answer entities onto the corresponding Wikidata entities and it seems while mapping the validity of the answer is not verified carefully. Hence, we resort to using EXAQT (Jia et al., 2021) built on TimeQuestions as the base KBQA pipeline and build an equivalent of our proposed approach called EXAQT+Text for comparison.\nA.5 Implementation Details\nOur system pipeline is implemented using Flow Compiler9 (Chakravarti et al., 2019) that stitches together the gRPC services of the individual modules. \u03bb-expressions are defined using ANTLR grammer. SPARQL queries are run on public Wikidata end point10. We reuse the KBQA pipeline implementation of (Neelam et al., 2022b)."
        },
        {
            "heading": "A.6 Related Work",
            "text": "Although Complex KBQA has been an active research topic (Vakulenko et al., 2019; Saxena et al., 2020; Wu et al., 2021; Shi et al., 2020), there has been very limited research focused on Temporal\n9https://github.com/IBM/flow-compiler 10https://query.wikidata.org/\nKBQA. Temporal Questions require identification of time intervals of events and temporal reasoning."
        },
        {
            "heading": "A.6.1 Temporal KBQA Datasets:",
            "text": "TempQuestions (Jia et al., 2018a) is one of the first publicly available temporal KBQA dataset consisting of 1271 questions. However, this dataset was annotated over FreeBase, which is no longer maintained and was officially discontinued in 2014. SYGMA (Neelam et al., 2022a) introduced a subset of TempQuestions that can be answered over wikidata called TempQA-WD. TimeQuestions(Jia et al., 2021) is another temporal QA dataset that is curated from existing QA datasets and mapping the answers to Wikidata. We use TempQAWD, and TimeQuestions data sets to evaluate our approach. CRONQUESTION (Saxena et al., 2021) is another temporal KBQA dataset that uses its own KB drawn from Wikidata. Event-QA dataset (Costa et al., 2020) is based on Event-KG, curated from DBpedia, Wikidata and YAGO. Since these datasets are generated in a template based manner using existing facts from the KBs, they do not represent the real world challenge of incomplete KBs. One of the main goals of our approach is to handle the issue of incomplete KBs."
        },
        {
            "heading": "A.6.2 Temporal KBQA Systems:",
            "text": "TEQUILA (Jia et al., 2018b) is one of the first attempts to address temporal question answering over KBs. It used an existing KBQA engine (Abujabal et al., 2017) to answer individual sub-questions and perform a temporal reasoning over the answers to derive the final answer. SYGMA (Neelam et al., 2022a) is another system that works on a Wikidata and uses \u03bb-expressions to represent the facts and their temporal reasoning operators. EXAQT (Jia et al., 2021) is another temporal KBQA system that uses entity and temporal embeddings to generate final answers. TempoQR (Mavromatis et al., 2021) is another system that tries to improve on top of CRONQA (Saxena et al., 2021) and introduces temporal KB-completion aspect into temporal Questions answering. We did not consider these two systems as they work on curated subset of wikidata which has all the temporal facts to answer the given dataset. TEQUILA uses a prespecified set of temporal signals (10 signal words) to decompose questions into sub-questions at sentence level in a rule-based manner. Instead, we follow the approach similar to SYGMA that use a sophisticated semantic parsing approach involv-\ning AMR (Abstract Meaning Representation) (Banarescu et al., 2013) and \u03bb-calculus (Zettlemoyer and Collins, 2012) to get logical representations of the questions. This enables decomposition of the questions at semantic level and is likely robust to linguistic variations as well."
        },
        {
            "heading": "A.7 KB + Text for QA",
            "text": "There have been past work exploring effectiveness of using KB and text resources for complex QA (Sun et al., 2018; Xiong et al., 2019). However, none of them address the temporal context addressed in this work. Prior work using a combination of KB and text have largely been based on end-to-end neural models. GRAFT-Net (Sun et al., 2018) constructs a sub-graph from KB and text corpora using an early fusion technique. The task of QA is then reduced to binary classification over the nodes of this sub-graph. PullNet (Sun et al., 2019) proposes to build sub-graph through an iterative process(Xiong et al., 2019), utilise a graph-attention based KB reader and knowledgeaware text reader.\nAll these methods are based on end-to-end neural models that require large amount of training data and offer little interpretability, which is essential to evaluate intermediate stages of complex QA systems. Additionally, labeling large amounts of data for KBQA is hard (Trivedi et al., 2017). In this work, we extend modular approach described in (Neelam et al., 2022a), additionally incorporating it with a targeted extraction pipeline. We made this choice as this particular approach integrates multiple, reusable modules that are pre-trained for their specific individual tasks (semantic parsers, entity and relational linkers, rankers and re-rankers and reading comprehension model) thus offering interpretability and flexibility for optimal combination of textual extraction with KBQA. Additionally, this does not require a large amount of domain-specific training data."
        },
        {
            "heading": "A.7.1 Question Decomposition",
            "text": "Our work uses a form of logical query decomposition, based on \u03bb-expression of the NL question, to help effectively combine the KB with the text resources. Some of the past work in the literature on QA have also explored question decomposition. BREAK IT down (Wolfson et al., 2020) is a popular benchmark data that captures complex question as an ordered list of tasks, that when executed in sequence will derive the final answer. It introduced\nquestion decomposition meaning representation (QDMR) to represent decomposed questions in an intermediate form resembling SQL. TEQUILA (Jia et al., 2018b) used temporal signal (words) based question decomposition to turn natural language questions into sub questions. STAG (Yih et al., 2015) defines core inferential chain and constraints which are analogous to the main and aux defined in our work. However, it\u2019s important to note that STAGG doesn\u2019t execute explicit decomposition of the lambda in the manner we do.\nQuestion System Path\nGround Answer\nSystem Answer\nComments\nWho won best actor when Alfred Junge won best art direction? Aux Failure (text extraction) Ronald Colman\nYul Brynner Top Retrieved Fact: In addition to the same producer, director and star, the first two films in the trilogy had the same cinematographer F. A. Freddie Young, composer Mikl Rufzsa, art director Alfred Junge and costume designer Roger Furse. The costumes for this film were executed by Elizabeth Haffenden. In 1955, she would take over from Furse as costume designer for the final film in the trilogy, Quentin Durward. Alfred Junge remained as art director. Issue: The issue here is incorrect Auxiliary fact extraction by the ectraction pipeline. As can be seen, an irrelevant passage has been detected as the fact containing Auxiliary interval. Hence the extracted Auxiliary fact was 1957 as opposed to the correct date 1947. As a result the reformed lambda consisted of erroneous temporal constraint due to to which the KBQA pipeline returned an incorrect answer as expected.\nWhich team did Wayne Rooney play for before joining Manchester United? Aux Failure (text extraction) Everton F.C.\nEngland Top Retrieved Fact: Wayne Mark Rooney born 24 October 1985 is an English professional football manager and former player. He is the manager of EFL Championship club Derby County, for whom he previously served as interim player-manager. He spent much of his playing career as a forward while also being used in various midfield roles. Widely considered to be one of the best players of his generation, Rooney is the record goalscorer for both the England national team and Manchester United.Rooney joined the Everton youth team at the age of nine and made his professional debut for the club in 2002 at the age of 16. He spent two seasons at the Merseyside club, before moving to Manchester United for 325.6 million in the 2004 summer transfer window where he won 16 trophies and became the only English player, alongside teammate Michael Carrick, to win the Premier League, FA Cup, UEFA Champions League, League Cup, UEFA Europa League, and FIFA Club World Cup."
        }
    ],
    "title": "Best of Both Worlds: Towards Improving Temporal Knowledge Base Question Answering via Targeted Fact Extraction",
    "year": 2023
}