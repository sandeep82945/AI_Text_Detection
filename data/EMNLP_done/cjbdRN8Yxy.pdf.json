{
    "abstractText": "Large language models (LLMs) achieved remarkable performance across various tasks. However, they face challenges in managing long documents and extended conversations, due to significantly increased computational requirements, both in memory and inference time, and potential context truncation when the input exceeds the LLM\u2019s fixed context length. This paper proposes a method called Selective Context that enhances the inference efficiency of LLMs by identifying and pruning redundancy in the input context to make the input more compact. We test our approach using common data sources requiring long context processing: arXiv papers, news articles, and long conversations, on tasks of summarisation, question answering, and response generation. Experimental results show that Selective Context significantly reduces memory cost and decreases generation latency while maintaining comparable performance compared to that achieved when full context is used. Specifically, we achieve a 50% reduction in context cost, resulting in a 36% reduction in inference memory usage and a 32% reduction in inference time, while observing only a minor drop of .023 in BERTscore and .038 in faithfulness on four downstream applications, indicating that our method strikes a good balance between efficiency and performance. Code and data are available at https://github.com/ liyucheng09/Selective_Context.",
    "authors": [
        {
            "affiliations": [],
            "name": "Yucheng Li"
        },
        {
            "affiliations": [],
            "name": "Bo Dong"
        },
        {
            "affiliations": [],
            "name": "Frank Guerin"
        },
        {
            "affiliations": [],
            "name": "Chenghua Lin"
        }
    ],
    "id": "SP:dace679822ca3269716d1d92b71f6c00fd3f56ed",
    "references": [
        {
            "authors": [
                "Satanjeev Banerjee",
                "Alon Lavie."
            ],
            "title": "Meteor: An automatic metric for mt evaluation with improved correlation with human judgments",
            "venue": "Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summariza-",
            "year": 2005
        },
        {
            "authors": [
                "Iz Beltagy",
                "Matthew E Peters",
                "Arman Cohan."
            ],
            "title": "Longformer: The long-document transformer",
            "venue": "arXiv preprint arXiv:2004.05150.",
            "year": 2020
        },
        {
            "authors": [
                "Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901",
            "year": 2020
        },
        {
            "authors": [
                "S\u00e9bastien Bubeck",
                "Varun Chandrasekaran",
                "Ronen Eldan",
                "Johannes Gehrke",
                "Eric Horvitz",
                "Ece Kamar",
                "Peter Lee",
                "Yin Tat Lee",
                "Yuanzhi Li",
                "Scott Lundberg"
            ],
            "title": "Sparks of artificial general intelligence: Early experiments with gpt-4",
            "year": 2023
        },
        {
            "authors": [
                "Razvan Bunescu",
                "Oseremen O Uduehi."
            ],
            "title": "Distribution-based measures of surprise for creative language: Experiments with humor and metaphor",
            "venue": "Proceedings of the 3rd Workshop on Figurative Language Processing (FLP), pages 68\u201378.",
            "year": 2022
        },
        {
            "authors": [
                "Alexis Chevalier",
                "Alexander Wettig",
                "Anirudh Ajith",
                "Danqi Chen."
            ],
            "title": "Adapting language models to compress contexts",
            "venue": "arXiv preprint arXiv:2305.14788.",
            "year": 2023
        },
        {
            "authors": [
                "Wei-Lin Chiang",
                "Zhuohan Li",
                "Zi Lin",
                "Ying Sheng",
                "Zhanghao Wu",
                "Hao Zhang",
                "Lianmin Zheng",
                "Siyuan Zhuang",
                "Yonghao Zhuang",
                "Joseph E. Gonzalez",
                "Ion Stoica",
                "Eric P. Xing"
            ],
            "title": "Vicuna: An opensource chatbot impressing gpt-4 with 90%* chatgpt",
            "year": 2023
        },
        {
            "authors": [
                "Rewon Child",
                "Scott Gray",
                "Alec Radford",
                "Ilya Sutskever."
            ],
            "title": "Generating long sequences with sparse transformers",
            "venue": "arXiv preprint arXiv:1904.10509.",
            "year": 2019
        },
        {
            "authors": [
                "Peter Clark",
                "Isaac Cowhey",
                "Oren Etzioni",
                "Tushar Khot",
                "Ashish Sabharwal",
                "Carissa Schoenick",
                "Oyvind Tafjord."
            ],
            "title": "Think you have solved question answering? try arc, the ai2 reasoning challenge",
            "venue": "ArXiv, abs/1803.05457.",
            "year": 2018
        },
        {
            "authors": [
                "Dan Hendrycks",
                "Collin Burns",
                "Steven Basart",
                "Andy Zou",
                "Mantas Mazeika",
                "Dawn Xiaodong Song",
                "Jacob Steinhardt."
            ],
            "title": "Measuring massive multitask language understanding",
            "venue": "ArXiv, abs/2009.03300.",
            "year": 2020
        },
        {
            "authors": [
                "Yucheng Li."
            ],
            "title": "Unlocking context constraints of llms: Enhancing context efficiency of llms with self-information-based content filtering",
            "venue": "ArXiv, abs/2304.12102.",
            "year": 2023
        },
        {
            "authors": [
                "Chin-Yew Lin."
            ],
            "title": "Rouge: A package for automatic evaluation of summaries",
            "venue": "Text summarization branches out, pages 74\u201381.",
            "year": 2004
        },
        {
            "authors": [
                "Jesse Mu",
                "Xiang Lisa Li",
                "Noah Goodman."
            ],
            "title": "Learning to compress prompts with gist tokens",
            "venue": "arXiv preprint arXiv:2304.08467.",
            "year": 2023
        },
        {
            "authors": [
                "Kishore Papineni",
                "Salim Roukos",
                "Todd Ward",
                "WeiJing Zhu."
            ],
            "title": "Bleu: a method for automatic evaluation of machine translation",
            "venue": "Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 311\u2013318.",
            "year": 2002
        },
        {
            "authors": [
                "Alec Radford",
                "Jeffrey Wu",
                "Rewon Child",
                "David Luan",
                "Dario Amodei",
                "Ilya Sutskever"
            ],
            "title": "Language models are unsupervised multitask learners",
            "venue": "OpenAI blog,",
            "year": 2019
        },
        {
            "authors": [
                "Claude E Shannon."
            ],
            "title": "A mathematical theory of communication",
            "venue": "The Bell system technical journal, 27(3):379\u2013423.",
            "year": 1948
        },
        {
            "authors": [
                "Hugo Touvron",
                "Thibaut Lavril",
                "Gautier Izacard",
                "Xavier Martinet",
                "Marie-Anne Lachaux",
                "Timoth\u00e9e Lacroix",
                "Baptiste Rozi\u00e8re",
                "Naman Goyal",
                "Eric Hambro",
                "Faisal Azhar"
            ],
            "title": "Llama: Open and efficient foundation language models",
            "year": 2023
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin."
            ],
            "title": "Attention is all you need",
            "venue": "Advances in neural information processing systems, 30.",
            "year": 2017
        },
        {
            "authors": [
                "Alex Wang",
                "Kyunghyun Cho",
                "Mike Lewis."
            ],
            "title": "Asking and answering questions to evaluate the factual consistency of summaries",
            "venue": "ArXiv, abs/2004.04228.",
            "year": 2020
        },
        {
            "authors": [
                "Ernst-Jan C. Wit",
                "Marie Gillette."
            ],
            "title": "What is linguistic redundancy",
            "venue": "University of Chicago.",
            "year": 1999
        },
        {
            "authors": [
                "Susan Zhang",
                "Stephen Roller",
                "Naman Goyal",
                "Mikel Artetxe",
                "Moya Chen",
                "Shuohui Chen",
                "Christopher Dewan",
                "Mona Diab",
                "Xian Li",
                "Xi Victoria Lin"
            ],
            "title": "Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068",
            "year": 2022
        },
        {
            "authors": [
                "Tianyi Zhang",
                "Varsha Kishore",
                "Felix Wu",
                "Kilian Q Weinberger",
                "Yoav Artzi."
            ],
            "title": "Bertscore: Evaluating text generation with bert",
            "venue": "arXiv preprint arXiv:1904.09675.",
            "year": 2019
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Large language models (LLMs) have demonstrated remarkable power and impressive generalisation abilities across a wide range of natural language processing tasks, as well as real-life applications (Brown et al., 2020; Touvron et al., 2023; Bubeck et al., 2023). However, a major challenge for existing LLMs is processing longer context. Dealing with longer context with LLMs is fundamen-\n\u2217 Corresponding author\ntal in scenarios such as having long conversations, document summarisation, and question answering given long documents. However, it is very computationally expensive, particularly with Transformer based LLMs, due to the quadratic growth of memory and computation associated with the 2-D attention matrix (Vaswani et al., 2017). This makes LLMs less accessible and sometimes leads to context truncation during inference. Moreover, due to the above limitation, existing LLMs were usually pre-trained with fixed-context windows, which further constrains their capability in processing longer context.\nThere are active attempts in reducing the computation and memory cost of the Transformer architecture with sparse attention (Child et al., 2019) or local dense attention (Beltagy et al., 2020). There are also efforts to learn soft prompts with further distillation to save context cost during inference (Mu et al., 2023; Chevalier et al., 2023). In contrast to existing approaches that primarily focus on architectures or distillations, we introduce a fresh perspective to tackle the redundancy in the input context itself, thus proposing a complementary, modelagnostic approach that can be potentially combined with other architecture optimisation methods to further enhance inference efficiency.\nThe proposed method is motivated by the potential redundancy and repetition in human language, which has two main sources. The first is the inherent redundancy of natural language. For example, in the conversation \"A: Did you get the chance to pick up groceries today?\", \"B: Yes, I did get the groceries.\", the underlined part can be seen as a common redundancy in communication. Linguistic studies suggest redundancy is ubiquitous in language (Wit and Gillette, 1999). The other type of input redundancy is from the overlap with training material. As the example in Fig. 1 shows, if some parts of input have already been included in the pre-training stage of LLMs, then it is safe to delete them and the model can still generate the correct answer. In summary, redundancy in the input context, while beneficial for human comprehension, can be superfluous for LLMs and might lead to unnecessary computational expense.\nIn this paper, we propose Selective Context, which prunes redundant content in a given input context, thereby reducing the computational cost and making better use of the fixed context length in LLMs. Selective Context evaluates informativeness of lexical units (i.e., tokens, phrases, or sentences) with self-information (Shannon, 1948) computed by a base causal language model. By selectively retaining content with higher self-information, our method provides a more compact and efficient context representation for LLMs to process without compromising their performance on various applications.\nWe evaluate the effectiveness and different settings of Selective Context on arXiv papers, BBC News, and real conversation on ShareGPT.com with four NLP tasks: summarisation, question answering, original context reconstruction, and conversation. Experimental results demonstrate that our proposed method can significantly enhance context efficiency of LLMs during inference while maintaining comparable performance compared to that achieved when full context is used."
        },
        {
            "heading": "2 Self-Information",
            "text": "Self-information, also known as surprisal or information content, is a fundamental concept in information theory that quantifies the amount of information conveyed by an event given a distribution (Shannon, 1948). In the context of language modelling, the event can be regarded as one step of generation (i.e., a token) and the distribution\ncorresponds to its output distribution. So the selfinformation of a token can be defined as the negative log likelihood:\nI(x) = \u2212 log2 P (xt|x0, x1, ..., xt\u22121) (1)\nwhere I(x) represents the self-information of token x and P (x) denotes its output probability.\nIn information theory, self-information measures the level of surprise or uncertainty associated with an event; rare events convey more information and thus have higher self-information, while common events convey less information and have lower self-information. In the context of language modelling, self-information can be used to assess the informativeness of lexical units, e.g., words, phrases, or sentences. Lexical units with lower self-information are less informative and thus are more likely to be inferred from the context. As a result, we may treat these parts of input as redundant during LLM inference.\nIn NLP, self-information has been used to measure surprise in creative language artefacts (Bunescu and Uduehi, 2022). In addition, related concepts of self-information such as entropy and perplexity are widely used in language model optimisation and evaluation.\nH(S) = 1\nN \u03a3tI(xt) (2)\nPP(S) = 2H(S) (3)\nwhere the entropy H(S) of the sentence S = (x0, ..., xn) is the average self-information of words in the sentence, and perplexity PP(S) of the sentence can be calculated with entropy. The property of self-information that is especially relevant to our method is the additivity.\nI(x0, x1) = \u2212 log2 P (x0, x1) = \u2212 log2 P (x0)P (x1|x0) = \u2212 log2 P (x0)\u2212 log2 P (x1|x0) = I(x0)I(x1) (4)\nThis means we can calculate the self-information of a lexical unit by simply summing the selfinformation of the tokens in it."
        },
        {
            "heading": "3 Method",
            "text": "Selective Context optimises the input context by filtering out redundant or non-essential content to reduce computational cost and make better use of the limited context window. In implementation,\nwe first 1) employ a causal language model such as GPT (Radford et al., 2019; Brown et al., 2020), OPT (Zhang et al., 2022), or LLaMA (Touvron et al., 2023), computing self-information for each token. We then 2) merge tokens, along with their corresponding self-information values, into lexical units, which can be phrases or sentences. This step is optional if tokens are being used as the basic units. Finally, 3) we eliminate content that is deemed least necessary to render the input more compact."
        },
        {
            "heading": "3.1 Computing Self-Information",
            "text": "Given a context C = x0, x1, ..., xn, where xi denotes a token, we use a base language model M to compute the self-information for each token xt as follows:\nI(xi) = \u2212 log2 P (xi|x0, x1, ..., xi\u22121) (5)"
        },
        {
            "heading": "3.2 Merging into Lexical Units",
            "text": "If the content filtering of selective context is directly performed on the token level, it might lead to very disjoint context. Therefore apart from token level filtering, we also conduct the filtering procedure on phrase and sentence level. We call a basic unit in our filtering a lexical unit, which could be a token, a phrase or a sentence in our setting.\nTo enable selective context to work on phrases and sentences, we merge tokens and their selfinformation into lexical units. Each lexical unit u consists of multiple tokens (xt, ..., xt+\u03b1), and we can calculate its self-information by summing the self-information of its individual tokens according\nto the additivity property of self-information:\nI(u) = \u03b1\u2211 i=t I(xi) (6)\nThe NLTK sentence tokenizer is employed to obtain sentence level lexical units. And we use spacy1 to merge tokens into noun phrases. We do not merge verb phrases as it might produce very long phrases."
        },
        {
            "heading": "3.3 Selective Retention of Informative Context",
            "text": "With the self-information of each lexical unit computed, we can now evaluate their informativeness. Instead of using a fixed threshold or retaining a fixed number of top k lexical units, we design a percentile-based filtering approach to adaptively select the most informative content.\nFirst, we rank the lexical units based on their self-information values in descending order. Then, we compute the p-th percentile of self-information values among all lexical units.\nIp = np.percentile([I(u0), .., I(uk)], p) (7) Next, we selectively retain lexical units with selfinformation values greater than or equal to the p-th percentile, constructing a filtered context C \u2032:\nC \u2032 = Ui | I(Ui) \u2265 Ip, 1 \u2264 i \u2264 n (8)\nThe percentile-based filtering is a more flexible approach to retain the most informative content depending on the distribution of self-information values in the given context. In Figure 2, we present\n1https://spacy.io/api/ pipeline-functions#merge_noun_chunks\nan example on phrase level where p is set to 50, which means half of phrases are filtered out. In this case, the context after processing by selective context only retains 57.2% of tokens, which saves 42.8% of context length."
        },
        {
            "heading": "4 Experiments",
            "text": "The goal of Selective Context is to reduce the redundancy in the input context without compromising the generation quality of LLMs. As a result, we are expecting the answers given both selective context and the original context to be as close as possible. We take the generated answer given full context as the reference answer, and compare to the generated answer given the selective context in our experiments."
        },
        {
            "heading": "4.1 Datasets",
            "text": "Selective Context prunes redundancy in the input context to allow very long context processing for LLMs. However, existing benchmarks for LLMs, such as MMLU (Hendrycks et al., 2020) and ARC (Clark et al., 2018), are mostly single round question answering and are thus not suitable to evaluate our proposed method. Therefore, we collect three test sets consisting of long documents and conversations to evaluate Selective Context. Statistics in detail are presented in Table 4. BBC News: A dataset containing news articles collected from the British Broadcasting Corporation (BBC). This dataset covers a wide range of topics, including politics, business, sports, and technology. We use the full content of each news article in our experiments. arXiv Articles: A dataset consisting of latest academic papers, spaning various scientific disciplines, such as computer science, physics, and mathematics. As arXiv articles can be quite long, we only process the first two sections (usually introduction and background) for each paper in our experiments. ShareGPT.com: ShareGPT.com is a platform where ChatGPT users share their surprising and interesting conversation with ChatGPT. This dataset consists of conversations in different languages and in various scenarios (e.g., coding, chitchat, writing assistant, etc.). We use the ShareGPT dataset for the conversation task in our experiments.\nThe three evaluation datasets were created carefully to avoid data contamination. Data samples in the BBC News, arXiv, and ShareGPT.com datasets were all created after March 2023, which is after\nthe release of all LLMs in our experiments. Considering some of baseline models are continually being updated, we employ the latest version released before 30 March 2023 to make sure models have never seen our test set in their pre-training and fine-tuning stage. In addition, as some of LLMs in our experiments have a max_length of 2048 tokens, we do not include articles or conversations exceeding this length."
        },
        {
            "heading": "4.2 Models",
            "text": "We test Selective Context on the following models: GPT-3.5 and GPT-4: GPT-3.5 also known as ChatGPT, which is likely to be further fine-tuned from GPT-3 and InstructGPT. GPT-4 is the latest model from OpenAI, which has demonstrated substantially improved capability on complex reasoning compared to its predecessor. GPT-3.5 and GPT4 are unfortunately not open-source, we can only access these models via web api2. LLaMA-7B, 13B, 30B: LLaMA is a family of open-source language models released by Meta, which is reported to outperform GPT-3 with less parameters. The LLaMA family includes models with size ranging from 7B to 65B. To investigate the effect of scaling law to Selective Context, we experiment with LLaMA with 7B, 13B, and 30B parameters. Vicuna-7B, 13B: Vicuna (Chiang et al., 2023) is a family of open-source language models instructtuned from LLaMA. According to their technical report, Vicuna models perform quite well on a list of multitasking benchmarks."
        },
        {
            "heading": "4.3 Tasks and Metrics",
            "text": "We evaluate Selective Context on four tasks: Original Context Reconstruction: Given a compressed context produced by Selective Context, this task aims to evaluate whether models are able to reconstruct the original context. This task assesses how well the filtered context retains the essential information from the original context. In our experiments, the compressed contexts are used as input, and the original contexts are used as reference answers. Summarisation: Given a context, the task is to generate a summary that captures the main points of the document. This task aims to evaluate whether Selective Context affects the overall understanding of models on the input contexts. In\n2https://platform.openai.com/docs/ api-reference\nour experiments, the input and output are the compressed context and the summaries generated based on the compressed contexts. Summaries based on the original (full) contexts are treated as the reference answers.\nQuestion Answering (QA): Given a document and a set of questions, the task is to generate answers based on the information available in the document. This task aims to evaluate models\u2019 understanding of a specific query. Here we first generate questions and answers based on the original context, where these answers are treated as reference answers, and then ask LLMs to answer these questions with selective context.\nConversation: This task is only for the ShareGPT dataset. Given a conversation history and a user query, the task is to generate a response to the query based on the previous conversation history. This task aims to evaluate selective context\u2019s performance on conversation. Specifically, we ask LLMs to answer the users\u2019 last query of ShareGPT conversation instances with selective context applied on the previous conversation history.\nWe employ four metrics to assess the performance of our models on the tasks: BLEU, METEOR, ROUGE, and BERTScore. BLEU (Papineni et al., 2002) calculates n-gram precision, which is the proportion of n-grams in the generated text that are also present in the reference text. METEOR (Banerjee and Lavie, 2005) takes additional features such as synonymy, stemming and word order into consideration, which leads to more comprehensive evaluation. ROUGE (Lin, 2004) focuses on how much of the important information in the reference text is present in the generated summary. BERTScore (Zhang et al., 2019) leverages contextualised embeddings from pre-trained language models like BERT, computing the cosine similarity between the generated text and reference text embeddings to capture semantic similarity more effectively than traditional n-gram-based metrics.\nAs mentioned before, we use the generated answers given the full contexts as the reference answers. When testing the deterministic decoding strategy (greedy decoding), we take one single run on full context as the reference answer. When testing the non-deterministic decoding strategy (temperature = 0.7), we run multiple times on full context to obtain multiple reference answers to address the randomness in decoding. The metrics are computed based on the set of refer-\nence answers. In our experiment, we set the number of reference answers to 4."
        },
        {
            "heading": "4.4 Experimental Settings",
            "text": "We use the smaller base causal language model for self-information computing in our experiments. For the LLaMA family and vicuna family, we employ LLaMA-7B to compute self-information. For the OpenAI family, we use a smaller GPT-3 variant curie for self-information computing, which is available on OpenAI web API. In self-information computing, we do not process the entire context at once. This is due to our observation on the tendency of LLMs to give later lexical units lower selfinformation. Instead, we compute self-information sentence by sentence in our experiments.\nIn our experiments, we compare the two different dimensions that are adjustable in Selective Context. Compression Ratios: We experiment with different content reduction ratios in Selective Context: 0.2, 0.35, 0.5, 0.65, and 0.8. These ratios determine the proportion of content to be filtered out, allowing us to study the trade-off between efficiency and performance as the amount of retained information varies. Lexical Units: Lexical units are the basic element of content reduction in Selective Context. It can be sentence, phrases, or tokens. As mentioned in \u00a73.2, we remove the redundancy in input context by a specific lexical unit level."
        },
        {
            "heading": "5 Results",
            "text": "Except for \u00a75.5, all results of selective context presented are at the phrase level (the optimal)."
        },
        {
            "heading": "5.1 Overview",
            "text": "In Table 1, we first compare the performance of Selective Context against the Original Context to see how well Selective Context preserves useful information when reducing context cost. The metrics are averaged across all models mentioned in \u00a74.2. The performance drop is shown in parentheses.\nAs demonstrated in the table, using Selective Context only leads to a marginal drop when the reduction ratio is set to 0.2 or 0.35, despite it significantly reducing the context cost. The BLEU score drops by only 0.05 when 20% of the content is reduced. And the number is even smaller when it comes to ROUGE-1, where the drop is just 0.03. This indicate a high level of consistency be-\ntween answers given selective contexts and original contexts when the reduction ratio is 0.2. Selective Context also yields impressive results when 35% of the content is reduced, with BERT scores around 0.9 and ROUGE-1 scores over 0.5. The drops become noticeable as the reduction ratio rises to 0.5, where the average BLEU score drops 0.17 and the average ROUGE-1 drops 0.12. A reduction ratio of 0.65 and 0.8 tends to be less valuable, as shown by the 0.18 drop on ROUGE-1 and 0.32 drop on BERTScore-F1.\nWe then compare Selective Context against the Random compression baseline as shown in Table 2. We observe that using Selective Context allows LLMs to generate very similar answers to the reference answers (answers given full context) although we significantly reduce the context cost. Selective Context maintains BERTScore-F1 above 0.9 when the compression ratio is 0.5 or lower, which shows a high similarity with the reference answers. ROUGE demonstrates the same trend: ROUGE-1 continues to be above 0.64 and ROUGE-L keeps above 0.5 when the ratio is under 0.5. We also notice that Selective Context is significantly more effective than the random baseline: Selective Context with compression ratio of 0.5 shows a better overlapping with the reference answer than Ran-\ndom baseline with only 20% content compression."
        },
        {
            "heading": "5.2 Faithfulness",
            "text": "To evaluate to what extent selective context affects the faithfulness of the LLMs generated content, we perform manual tests on our question answering results based on the idea of Wang et al. (2020). We evaluate 1000 question-answer pairs (200 for each ratio) with the following procedure: 1) We first extract OpenIE tuples from the answers of selective context, and then 2) manually evaluate whether each tuple is entailed by the reference answer. If the model\u2019s answer is \"Sorry, I don\u2019t know\", we treat it as \"Sorry\" cases and do not consider it as unfaithfulness.\nAs shown in the Table 3, we find that gpt-3.5 tends to generate shorter answers or refuses to an-\nswer the questions if it fails to identify necessary evidence in the given selective context. With a compression ratio of 0.65, gpt-3.5 refuses to answer 19 questions (9% of 200), and the answers are 35% shorter than the reference answer (131 tokens in average). However, selective context doesn\u2019t significantly affect the faithfulness across all compression ratios. About 3.8% of all tuples are not entailed by the reference answer when the compression ratio is 0.5, and this number rises slightly to 5.1% as the compression ratio increases to 0.65."
        },
        {
            "heading": "5.3 Tasks",
            "text": "In this part, we break down and analyse the performances of Selective Context in the four different NLP tasks: summarisation, question answering, original context reconstruction, and conversation. The results are as shown in Fig. 3. First, the results on the Original Context Reconstruction task (RC) show the steepest drop with increasing compression ratio, however, Selective Context allows LLMs to preserve most of the key points in the\noriginal context when the reduction ratio is lower than 0.5, as demonstrated by a rather high ROUGE score. Second, we notice that the curves of question answering and summarisation decrease gradually and are continually higher than those of the other two tasks evaluated by BERTScore. We could say Selective Context is especially suitable for tasks of summarisation and answer generation."
        },
        {
            "heading": "5.4 Scaling and Instruct-Tuning",
            "text": "We perform human evaluation to explore the effect of model scales and supervised instruct-tuning on Selective Context. We asked three college students to evaluate 1150 generated summaries from llama and vicuna (about 55 per model and ratio) and record whether they accept the generation as a reasonable summary. As shown in Figure 4, we find no specific trends between the scales and generation quality given Selective Context. The vicuna family demonstrates similar summarisation capability with 7b and 13b parameters. And so does the llama family, larger models do not show stronger robustness towards Selective Context. But instruct-tuned model vicuna demonstrates significantly superior performance\nthan llama models given selective context indicating instruct-tuning might help the model to be more robust against context compression. Given selective context, llama models often fail to follow instructions and go wild very easily."
        },
        {
            "heading": "5.5 Lexical Units",
            "text": "We test the effect of Selective Context based on different lexical units: tokens, phrases, and sentences via BERTScore-F1. As shown in Table 5, employing phrase as the basic lexical units in Selective Context is the optimal approach, consistently outperforming the other two variants, followed by token-level Selective Context. Removing redundancy at sentence-level is a rather unstable implementation compared to the token and phrase-level. This experiment indicates that a reasonable granularity can be crucial in Selective Context."
        },
        {
            "heading": "5.6 Case Study",
            "text": "To have a straightforward impression on how well LLMs generate with selective context, we present two summaries given the full and selective context respectively in Figure 6. The original document and processing to obtain selective context are presented in Appendix B.\nWe first found that preparing selective context is extremely efficient. It takes a one-time cost of 46.1 ms to build selective context for the example paragraph, which includes computing self-information\nand performing lexical unit tokenisation. This ensures that the initial stage of establishing a selective context incurs very little overhead. Secondly, it shows selective context significantly reduces the memory usage of the GPU and accelerates the generation process. With a compression ratio of 0.5, selective context reduces about 36% of the memory cost in inference and makes generation 1.32 times faster (per token). By comparing the content of the two summaries, we see that the summary given selective context missed relevant information about the research background (as denoted by the [1] marker), such as the use of machine learning in autonomous driving technology and instead starts with the different methods directly. This is due to the background parts not being selected and removed as redundancy before feeding to vicuna. We tried to ask vicuna\n\"what is the background of this study?\", given the selective context, and obtained a decent answer:\n\"the research background of this paper is likely to be situated in the domain of autonomous driving technology and the application of artificial intelligence (AI) for improving vehicle safety and decision-making capabilities.\".\nThis demonstrates that LLMs are likely to be able to infer the deleted parts of background information in the selective context. Selective context also affects vicuna\u2019s decision on what informa-\ntion should be included in the summary as the second summary includes details about, for example, FMHSA and UCD block (as denoted by the [2] marker) which are not covered in the summary generated with the full context. We find no factual errors in the summary given selective context."
        },
        {
            "heading": "6 Conclusion",
            "text": "We introduced Selective Context to improve the context efficiency of LLMs in inference by deleting redundant content measured by self-information. Our extensive experiments on arXiv papers, BBC news articles, and conversation transcripts showed that our proposed method can significantly reduce GPU memory cost, accelerate generation with minor performance decrease, and potentially enable LLMs to handle long documents and extended conversations without the risk of context truncation."
        },
        {
            "heading": "7 Limitations",
            "text": "Selective Context demonstrates promising results, but it is still necessary to note a couple of potential limitations. Firstly, our approach is somewhat influenced by the phrase boundary detection procedure. We employ the noun phrase tokenisation algorithm provided by spacy in our experiments. However, we do not consider verb phrases as there is no mature solution for verb phrase tokenisation. We speculate that we can achieve better compression performance with dependency tree-based filtering procedure which might lead to better boundary identification of lexical units. Secondly, in the experiment section, we use percentile to control the pruning process. However, the optimal compression percentile varies based on specific tasks and context. Developing a tool to find the optimal threshold can further enhance the effectiveness of selective context."
        },
        {
            "heading": "A Dataset statistics",
            "text": ""
        },
        {
            "heading": "B Example of selective context on long context",
            "text": "Here we present an example of selective context on a rather long context. The original paragraphs is from https://arxiv.org/abs/ 2303.07352. The original paragraphs is shown in Fig. 7. The resulting context is shown in Fig. 8. The reference summary is given in Fig. 9."
        },
        {
            "heading": "C The Previous Version of Selective Context",
            "text": "If you\u2019re looking for the previous of the paper, please check (Li, 2023)."
        }
    ],
    "title": "Compressing Context to Enhance Inference Efficiency of Large Language Models",
    "year": 2023
}