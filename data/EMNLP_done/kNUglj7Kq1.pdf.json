{
    "abstractText": "The scarcity of data in many of the world\u2019s languages necessitates the transfer of knowledge from other, resource-rich languages. However, the level of scarcity varies significantly across multiple dimensions, including: i) the amount of task-specific data available in the source and target languages; ii) the amount of monolingual and parallel data available for both languages; and iii) the extent to which they are supported by pretrained multilingual and translation models. Prior work has largely treated these dimensions and the various techniques for dealing with them separately; in this paper, we offer a more integrated view by exploring how to deploy the arsenal of cross-lingual transfer tools across a range of scenarios, especially the most challenging, low-resource ones. To this end, we run experiments on the AmericasNLI and NusaX benchmarks over 20 languages, simulating a range of few-shot settings. The best configuration in our experiments employed parameter-efficient language and task adaptation of massively multilingual Transformers, trained simultaneously on source language data and both machine-translated and natural data for multiple target languages. In addition, we show that pre-trained translation models can be easily adapted to unseen languages, thus extending the range of our hybrid technique and translation-based transfer more broadly. Beyond new insights into the mechanisms of cross-lingual transfer, we hope our work will provide practitioners with a toolbox to integrate multiple techniques for different real-world scenarios. Our code is available at https: //github.com/parovicm/unified-xlt.",
    "authors": [
        {
            "affiliations": [],
            "name": "Alan Ansell"
        },
        {
            "affiliations": [],
            "name": "Marinela Parovi\u0107"
        },
        {
            "affiliations": [],
            "name": "Ivan Vuli\u0107"
        },
        {
            "affiliations": [],
            "name": "Anna Korhonen"
        },
        {
            "affiliations": [],
            "name": "Edoardo Maria Ponti"
        }
    ],
    "id": "SP:2315719d458682b9af594b4b3fcfdb6f67c1f767",
    "references": [
        {
            "authors": [
                "\u017deljko Agi\u0107",
                "Ivan Vuli\u0107."
            ],
            "title": "JW300: A widecoverage parallel corpus for low-resource languages",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3204\u2013 3210, Florence, Italy. Association for Computational",
            "year": 2019
        },
        {
            "authors": [
                "Jesujoba O. Alabi",
                "David Ifeoluwa Adelani",
                "Marius Mosbach",
                "Dietrich Klakow"
            ],
            "title": "Adapting pretrained language models to African languages",
            "year": 2022
        },
        {
            "authors": [
                "Alan Ansell",
                "Edoardo Ponti",
                "Anna Korhonen",
                "Ivan Vuli\u0107."
            ],
            "title": "Composable sparse fine-tuning for crosslingual transfer",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1778\u20131796,",
            "year": 2022
        },
        {
            "authors": [
                "Alan Ansell",
                "Edoardo Maria Ponti",
                "Jonas Pfeiffer",
                "Sebastian Ruder",
                "Goran Glava\u0161",
                "Ivan Vuli\u0107",
                "Anna Korhonen."
            ],
            "title": "MAD-G: Multilingual adapter generation for efficient cross-lingual transfer",
            "venue": "Findings of the Association for Computational Linguis-",
            "year": 2021
        },
        {
            "authors": [
                "Mikel Artetxe",
                "Vedanuj Goswami",
                "Shruti Bhosale",
                "Angela Fan",
                "Luke Zettlemoyer."
            ],
            "title": "Revisiting machine translation for cross-lingual classification",
            "venue": "ArXiv, abs/2305.14240.",
            "year": 2023
        },
        {
            "authors": [
                "Mikel Artetxe",
                "Gorka Labaka",
                "Eneko Agirre."
            ],
            "title": "Translation artifacts in cross-lingual transfer learning",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 7674\u20137684, Online. Association for Computa-",
            "year": 2020
        },
        {
            "authors": [
                "Elad Ben Zaken",
                "Yoav Goldberg",
                "Shauli Ravfogel."
            ],
            "title": "BitFit: Simple parameter-efficient fine-tuning for transformer-based masked language-models",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2:",
            "year": 2022
        },
        {
            "authors": [
                "Damian Blasi",
                "Antonios Anastasopoulos",
                "Graham Neubig."
            ],
            "title": "Systematic inequalities in language technology performance across the world\u2019s languages",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics",
            "year": 2022
        },
        {
            "authors": [
                "David Brambila"
            ],
            "title": "Diccionario RaramuriCastellano: Tarahumar",
            "year": 1976
        },
        {
            "authors": [
                "Gina Bustamante",
                "Arturo Oncevay",
                "Roberto Zariquiey."
            ],
            "title": "No data to crawl? monolingual corpus creation from PDF files of truly low-resource languages in Peru",
            "venue": "Proceedings of the Twelfth Language Resources and Evaluation Conference, pages",
            "year": 2020
        },
        {
            "authors": [
                "Ji",
                "Pascale Fung",
                "Graham Neubig",
                "Timothy Baldwin",
                "Sebastian Ruder",
                "Herry Sujaini",
                "Sakriani Sakti",
                "Ayu Purwarianti"
            ],
            "title": "NusaCrowd: Open Source Initiative for Indonesian NLP Resources",
            "year": 2022
        },
        {
            "authors": [
                "Luis Chiruzzo",
                "Pedro Amarilla",
                "Adolfo R\u00edos",
                "Gustavo Gim\u00e9nez Lugo."
            ],
            "title": "Development of a Guarani - Spanish parallel corpus",
            "venue": "Proceedings of the Twelfth Language Resources and Evaluation Conference, pages 2629\u20132633, Marseille, France. Eu-",
            "year": 2020
        },
        {
            "authors": [
                "Alexis Conneau",
                "Kartikay Khandelwal",
                "Naman Goyal",
                "Vishrav Chaudhary",
                "Guillaume Wenzek",
                "Francisco Guzm\u00e1n",
                "Edouard Grave",
                "Myle Ott",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Unsupervised cross-lingual representation learning at scale",
            "venue": "Pro-",
            "year": 2020
        },
        {
            "authors": [
                "Alexis Conneau",
                "Shijie Wu",
                "Haoran Li",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Emerging cross-lingual structure in pretrained language models",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages",
            "year": 2020
        },
        {
            "authors": [
                "Rub\u00e9n Cushimariano Romano",
                "Richer C. Sebasti\u00e1n Q."
            ],
            "title": "\u00d1aantsipeta ash\u00e1ninkaki birakochaki",
            "venue": "diccionario ash\u00e1ninka-castellano. versi\u00f3n preliminar. http://www.lengamer.org/ publicaciones/diccionarios/.",
            "year": 2008
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
            "year": 2019
        },
        {
            "authors": [
                "Coto-Solano",
                "Thang Vu",
                "Katharina Kann."
            ],
            "title": "AmericasNLI: Evaluating zero-shot natural language understanding of pretrained multilingual models in truly low-resource languages",
            "venue": "Proceedings of the",
            "year": 2022
        },
        {
            "authors": [
                "Hao Fei",
                "Meishan Zhang",
                "Donghong Ji."
            ],
            "title": "Cross-lingual semantic role labeling with highquality translated training corpus",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7014\u20137026, On-",
            "year": 2020
        },
        {
            "authors": [
                "Isaac Feldman",
                "Rolando Coto-Solano."
            ],
            "title": "Neural machine translation models with back-translation for the extremely low-resource indigenous language Bribri",
            "venue": "Proceedings of the 28th International Conference on Computational Linguistics, pages 3965\u2013",
            "year": 2020
        },
        {
            "authors": [
                "Ana-Paula Galarreta",
                "Andr\u00e9s Melgar",
                "Arturo Oncevay."
            ],
            "title": "Corpus creation and initial SMT experiments between Spanish and Shipibo-konibo",
            "venue": "Proceedings of the International Conference Recent Advances in Natural Language Processing, RANLP",
            "year": 2017
        },
        {
            "authors": [
                "Iker Garc\u00eda-Ferrero",
                "Rodrigo Agerri",
                "German Rigau."
            ],
            "title": "Model and data transfer for cross-lingual sequence labelling in zero-resource settings",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2022, pages 6403\u20136416, Abu Dhabi, United",
            "year": 2022
        },
        {
            "authors": [
                "Iker Garc\u00eda-Ferrero",
                "Rodrigo Agerri",
                "German Rigau"
            ],
            "title": "T-projection: High quality annotation projection for sequence labeling",
            "year": 2022
        },
        {
            "authors": [
                "Demi Guo",
                "Alexander Rush",
                "Yoon Kim."
            ],
            "title": "Parameter-efficient transfer learning with diff pruning",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Lan-",
            "year": 2021
        },
        {
            "authors": [
                "Ximena Gutierrez-Vasques",
                "Gerardo Sierra",
                "Isaac Hernandez Pompa."
            ],
            "title": "Axolotl: a web accessible parallel corpus for Spanish-Nahuatl",
            "venue": "Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC\u201916),",
            "year": 2016
        },
        {
            "authors": [
                "Pengcheng He",
                "Jianfeng Gao",
                "Weizhu Chen."
            ],
            "title": "DeBERTav3: Improving deBERTa using ELECTRAstyle pre-training with gradient-disentangled embedding sharing",
            "venue": "The Eleventh International Conference on Learning Representations.",
            "year": 2023
        },
        {
            "authors": [
                "Gesmundo",
                "Mona Attariyan",
                "Sylvain Gelly."
            ],
            "title": "Parameter-efficient transfer learning for NLP",
            "venue": "Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pages 2790\u20132799.",
            "year": 2019
        },
        {
            "authors": [
                "Edward J Hu",
                "yelong shen",
                "Phillip Wallis",
                "Zeyuan AllenZhu",
                "Yuanzhi Li",
                "Shean Wang",
                "Lu Wang",
                "Weizhu Chen"
            ],
            "title": "LoRA: Low-rank adaptation of large language models",
            "venue": "In International Conference on Learning Representations",
            "year": 2022
        },
        {
            "authors": [
                "Junjie Hu",
                "Sebastian Ruder",
                "Aditya Siddhant",
                "Graham Neubig",
                "Orhan Firat",
                "Melvin Johnson."
            ],
            "title": "XTREME: A massively multilingual multitask benchmark for evaluating cross-lingual generalisation",
            "venue": "Proceedings of the 37th International",
            "year": 2020
        },
        {
            "authors": [
                "Alankar Jain",
                "Bhargavi Paranjape",
                "Zachary C. Lipton."
            ],
            "title": "Entity projection via machine translation for cross-lingual NER",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International",
            "year": 2019
        },
        {
            "authors": [
                "Pratik Joshi",
                "Sebastin Santy",
                "Amar Budhiraja",
                "Kalika Bali",
                "Monojit Choudhury."
            ],
            "title": "The state and fate of linguistic diversity and inclusion in the NLP world",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages",
            "year": 2020
        },
        {
            "authors": [
                "Iman Jundi",
                "Gabriella Lapesa."
            ],
            "title": "How to translate your samples and choose your shots? analyzing translate-train & few-shot cross-lingual transfer",
            "venue": "Findings of the Association for Computational Linguistics: NAACL 2022, pages 129\u2013150, Seattle,",
            "year": 2022
        },
        {
            "authors": [
                "Katharina Kann",
                "Kyunghyun Cho",
                "Samuel R. Bowman."
            ],
            "title": "Towards realistic practices in lowresource natural language processing: The development set",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing",
            "year": 2019
        },
        {
            "authors": [
                "Wei-Jen Ko",
                "Ahmed El-Kishky",
                "Adithya Renduchintala",
                "Vishrav Chaudhary",
                "Naman Goyal",
                "Francisco Guzm\u00e1n",
                "Pascale Fung",
                "Philipp Koehn",
                "Mona Diab"
            ],
            "title": "Adapting high-resource NMT models to translate low-resource related languages",
            "year": 2021
        },
        {
            "authors": [
                "Fajri Koto",
                "Ikhwan Koto."
            ],
            "title": "Towards computational linguistics in Minangkabau language: Studies on sentiment analysis and machine translation",
            "venue": "Proceedings of the 34th Pacific Asia Conference on Language, Information and Computation, pages 138\u2013",
            "year": 2020
        },
        {
            "authors": [
                "Anne Lauscher",
                "Vinit Ravishankar",
                "Ivan Vuli\u0107",
                "Goran Glava\u0161."
            ],
            "title": "From zero to hero: On the limitations of zero-shot language transfer with multilingual Transformers",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Lan-",
            "year": 2020
        },
        {
            "authors": [
                "Brian Lester",
                "Rami Al-Rfou",
                "Noah Constant."
            ],
            "title": "The power of scale for parameter-efficient prompt tuning",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3045\u20133059, Online and Punta Cana, Domini-",
            "year": 2021
        },
        {
            "authors": [
                "Xiang Lisa Li",
                "Percy Liang."
            ],
            "title": "Prefix-tuning: Optimizing continuous prompts for generation",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language",
            "year": 2021
        },
        {
            "authors": [
                "Manuel Mager",
                "Di\u00f3nico Carrillo",
                "Ivan Meza."
            ],
            "title": "Probabilistic finite-state morphological segmenter for wixarika (huichol) language",
            "venue": "Journal of Intelligent & Fuzzy Systems, 34(5):3081\u20133087.",
            "year": 2018
        },
        {
            "authors": [
                "Elena Mihas."
            ],
            "title": "A\u00f1aani katonkosatzi parenini, El idioma del alto Peren\u00e9",
            "venue": "Milwaukee, WI: Clarks Graphics.",
            "year": 2011
        },
        {
            "authors": [
                "Benjamin Muller",
                "Yanai Elazar",
                "Beno\u00eet Sagot",
                "Djam\u00e9 Seddah."
            ],
            "title": "First align, then predict: Understanding the cross-lingual ability of multilingual BERT",
            "venue": "Proceedings of the 16th Conference of the European Chapter of the Association for Computa-",
            "year": 2021
        },
        {
            "authors": [
                "Gao",
                "Vedanuj Goswami",
                "Francisco Guzm\u00e1n",
                "Philipp Koehn",
                "Alexandre Mourachko",
                "Christophe Ropers",
                "Safiyyah Saleem",
                "Holger Schwenk",
                "Jeff Wang"
            ],
            "title": "No language left behind: Scaling humancentered machine translation",
            "year": 2022
        },
        {
            "authors": [
                "Jaehoon Oh",
                "Jongwoo Ko",
                "Se-Young Yun."
            ],
            "title": "Synergy with translation artifacts for training and inference in multilingual tasks",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 6747\u20136754, Abu",
            "year": 2022
        },
        {
            "authors": [
                "John E Ortega",
                "Richard Alexander Castro-Mamani",
                "Jaime Rafael Montoya Samame."
            ],
            "title": "Overcoming resistance: The normalization of an Amazonian tribal language",
            "venue": "Proceedings of the 3rd Workshop on Technologies for MT of Low Resource Languages,",
            "year": 2020
        },
        {
            "authors": [
                "Marinela Parovi\u0107",
                "Goran Glava\u0161",
                "Ivan Vuli\u0107",
                "Anna Korhonen."
            ],
            "title": "BAD-X: Bilingual adapters improve zero-shot cross-lingual transfer",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational",
            "year": 2022
        },
        {
            "authors": [
                "Jonas Pfeiffer",
                "Sebastian Ruder",
                "Ivan Vuli\u0107",
                "Edoardo Maria Ponti"
            ],
            "title": "Modular deep learning",
            "year": 2023
        },
        {
            "authors": [
                "Jonas Pfeiffer",
                "Ivan Vuli\u0107",
                "Iryna Gurevych",
                "Sebastian Ruder."
            ],
            "title": "MAD-X: An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
            "year": 2020
        },
        {
            "authors": [
                "Jonas Pfeiffer",
                "Ivan Vuli\u0107",
                "Iryna Gurevych",
                "Sebastian Ruder."
            ],
            "title": "UNKs everywhere: Adapting multilingual language models to new scripts",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 10186\u201310203,",
            "year": 2021
        },
        {
            "authors": [
                "Telmo Pires",
                "Eva Schlinger",
                "Dan Garrette."
            ],
            "title": "How multilingual is multilingual BERT? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4996\u20135001, Florence, Italy",
            "venue": "Association for Computational Linguis-",
            "year": 2019
        },
        {
            "authors": [
                "Edoardo Maria Ponti",
                "Julia Kreutzer",
                "Ivan Vuli\u0107",
                "Siva Reddy"
            ],
            "title": "Modelling latent translations for cross-lingual transfer",
            "year": 2021
        },
        {
            "authors": [
                "Ayu Purwarianti",
                "Ida Ayu Putu Ari Crisdayanti."
            ],
            "title": "Improving bi-lstm performance for indonesian sentiment analysis using paragraph vector",
            "venue": "2019 International Conference of Advanced Informatics: Concepts, Theory and Applications (ICAICTA), pages",
            "year": 2019
        },
        {
            "authors": [
                "Sylvestre-Alvise Rebuffi",
                "Hakan Bilen",
                "Andrea Vedaldi."
            ],
            "title": "Learning multiple visual domains with residual adapters",
            "venue": "Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc.",
            "year": 2017
        },
        {
            "authors": [
                "Sakriani Sakti",
                "Satoshi Nakamura."
            ],
            "title": "Towards language preservation: Design and collection of graphemically balanced and parallel speech corpora of Indonesian ethnic languages",
            "venue": "2013 International Conference Oriental COCOSDA held jointly",
            "year": 2013
        },
        {
            "authors": [
                "Fabian David Schmidt",
                "Ivan Vuli\u0107",
                "Goran Glava\u0161."
            ],
            "title": "Don\u2019t stop fine-tuning: On training regimes for few-shot cross-lingual transfer with multilingual language models",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language",
            "year": 2022
        },
        {
            "authors": [
                "Herry Sujaini."
            ],
            "title": "Improving the role of language model in statistical machine translation (IndonesianJavanese)",
            "venue": "International Journal of Electrical and Computer Engineering, 10:2102\u20132109.",
            "year": 2020
        },
        {
            "authors": [
                "Yi-Lin Sung",
                "Varun Nair",
                "Colin A Raffel."
            ],
            "title": "Training neural networks with fixed sparse masks",
            "venue": "Advances in Neural Information Processing Systems, volume 34, pages 24193\u201324205. Curran Associates, Inc.",
            "year": 2021
        },
        {
            "authors": [
                "J\u00f6rg Tiedemann."
            ],
            "title": "Parallel data, tools and interfaces in OPUS",
            "venue": "Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC\u201912), pages 2214\u20132218, Istanbul, Turkey. European Language Resources Association",
            "year": 2012
        },
        {
            "authors": [
                "Bayu Distiawan Trisedya",
                "Dyah Inastra."
            ],
            "title": "Creating Indonesian-Javanese Parallel Corpora Using Wikipedia Articles",
            "venue": "2014 International Conference on Advanced Computer Science and Information System, pages 239\u2013245.",
            "year": 2014
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141 ukasz Kaiser",
                "Illia Polosukhin."
            ],
            "title": "Attention is all you need",
            "venue": "Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc.",
            "year": 2017
        },
        {
            "authors": [
                "Bryan Wilie",
                "Karissa Vincentio",
                "Genta Indra Winata",
                "Samuel Cahyawijaya",
                "Xiaohong Li",
                "Zhi Yuan Lim",
                "Sidik Soleman",
                "Rahmad Mahendra",
                "Pascale Fung",
                "Syafri Bahar",
                "Ayu Purwarianti"
            ],
            "title": "IndoNLU: Benchmark and resources for evaluating Indonesian",
            "year": 2020
        },
        {
            "authors": [
                "INDspeech_NEWS_EthnicSR (Sakti",
                "Nakamura"
            ],
            "title": "KoPI-NLLB (Cahyawijaya et al., 2022); LibriVox-Indonesia (Wirawan, 2022)",
            "venue": "NLLBSeed (NLLB Team et al.,",
            "year": 2013
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "While training data is available for a wide range of NLP tasks in a handful of high-resource languages, the vast majority of the world\u2019s languages with their billions of speakers do not fit into this\n*Equal contribution.\ncategory (Joshi et al., 2020; Blasi et al., 2022). The lack of labelled data makes it difficult or impossible to directly train effective NLP systems for these languages. For this reason, researchers have looked for ways to harness data from one or more high-resource \u201csource\u201d languages to compensate for a shortage of data in low-resource \u201ctarget\u201d languages in a process known as \u201ccross-lingual transfer\u201d (XLT). Different techniques have been developed to deal with the various dimensions of resource scarcity, which encompass not just data availability, but also the degree of support by pretrained models. These research threads have generally been investigated somewhat independently. In this paper, we attempt to unify several of the most prominent threads of XLT research into a single framework. Specifically, we synthesise findings from zero-shot (ZS) XLT with massively multilingual transformers (MMTs), few-shot (FS) XLT, XLT for low-resource languages and XLT through machine translation (MT) to formulate a practical, general-purpose approach to cross-lingual transfer, with a focus on low-resource scenarios.\nMassively multilingual Transformers (MMTs), Transformer-based architectures (Vaswani et al., 2017) pretrained with an unsupervised objective such as masked language modelling (MLM) on text from a large number of languages, are perhaps the most fundamental tool for contemporary XLT. Prominent examples of MMTs include mBERT (Devlin et al., 2019), XLM-R (Conneau et al., 2020a) and mDeBERTa (He et al., 2023). In addition to providing broad language coverage, MMTs have been shown to learn representations which have a degree of cross-lingual alignment, even though they do not receive any explicit cross-lingual signal during training (Conneau et al., 2020b; Muller et al., 2021). This allows an MMT fine-tuned for a specific task in a given \u201csource\u201d language to perform the same task in another \u201ctarget\u201d language with a level of performance generally\nmuch better than random chance (Pires et al., 2019; Wu and Dredze, 2019), despite never having seen a single example of the task in the target language; this is known as \u201czero-shot\u201d cross-lingual transfer.\nWhile MMTs typically cover around 100 languages, this is still only a small fraction of the world\u2019s estimated 7,000 languages. Pfeiffer et al. (2020) and Ansell et al. (2022) have shown that an effective strategy for ZS-XLT for target languages not covered by the MMT is to learn a parameterefficient fine-tuning (PEFT) to specialise the MMT to that \u201cunseen\u201d language. The resulting language module can be composed with a task module, typically yielding much better performance than the MMT could achieve without language adaptation.\nIn contrast to ZS-XLT, often more realistic is the few-shot case (FS-XLT), where a small number of gold-standard target language examples are available during training. Though it may be expensive to annotate target language data, especially for lowresource languages where native speakers are hard to access, prior work has shown that using even a small amount during training can yield significant gains in performance (Zhao et al., 2021). While early approaches to FS-XLT involved fine-tuning first on the source language data, then separately on the few target language shots (Lauscher et al., 2020), recent work has shown that it is more effective to jointly train on both at once (Xu and Murray, 2022; Schmidt et al., 2022).\nAnother tool often employed for cross-lingual transfer is machine translation (MT). MT approaches can generally be categorised as translatetrain or translate-test (Hu et al., 2020). To confine the scope of our work, we consider only the translate-train approach, which has so far been predominant, although Artetxe et al. (2023) have made a strong case for considering translate-test further in future work. We consider two translate-train variants: TTRAIN-SINGLE, where a model is trained for each target language using only its own translated data; and TTRAIN-ALL, where one model covering all target languages is trained on their translated data and the source language data simultaneously.\nIn this work, we consider how best to employ the above techniques in response to cross-lingual transfer scenarios with varying levels of data scarcity. We thus explore several promising directions of integrating the zero-shot, few-shot and translatetrain techniques across a range of resource levels in order to delve deeper into: (i) to what extent these\ntechniques and the different data sources that they exploit are complementary, (ii) what is the most effective way of combining different data sources in order to maximise the performance, and (iii) how much each of the available sources of data contributes to the overall performance. We aim to equip practitioners with a recipe for how to use the available data resources in the most effective way.\nWe experiment on the AmericasNLI natural language inference (NLI) dataset for American languages (Ebrahimi et al., 2022), and the NusaX sentiment analysis (SA) dataset for Indonesian languages (Winata et al., 2023). We find that combining language adaptation, few-shot learning and translation can be highly effective, yielding average performance gains of 14-24 points over the zero-shot baseline without language adaptation."
        },
        {
            "heading": "2 Methodology",
            "text": ""
        },
        {
            "heading": "2.1 Background",
            "text": "Because MMTs divide their capacity among many languages, they may often perform sub-optimally with respect to a single source or target language. Furthermore, we are sometimes interested in a target language not covered by the MMT. A naive solution to these problems is to prepare the MMT with continued pretraining on the target language before proceeding to task fine-tuning. While this can improve performance, Pfeiffer et al. (2020) and Ansell et al. (2022) show that a more effective approach is to apply a form of composable, parameterefficient fine-tuning during continued pretraining: Pfeiffer et al. (2020) employ adapters (Rebuffi et al., 2017; Houlsby et al., 2019), while Ansell et al. (2022) propose sparse fine-tunings (SFTs) learned through an algorithm they call \u201cLottery Ticket Sparse Fine-Tuning\u201d (LT-SFT). The resulting language-specific module (\u201clanguage module\u201d) can be composed with a similar module trained for the task of interest (\u201ctask module\u201d) to perform zero-shot transfer. This approach is not only more efficient than sequential full fine-tuning in terms of model size and training time; keeping the MMT weights frozen while training the language modules helps prevent the model from forgetting important knowledge learned during pretraining.\nWhile SFT composition generally exhibits somewhat better zero-shot cross-lingual transfer performance across a range of tasks than adapter composition (Ansell et al., 2022; Alabi et al., 2022), and avoids the overhead incurred during inference by\nadapters, adapters are more efficient to train and leave the base MMT fully unmodified. In this work, we consider both methods. However, we note that other modular, parameter-efficient methods exist and could be used with our tools in future work (Pfeiffer et al., 2023).\nMulti-Source Training. Multi-source training is an extension to parameter-efficient language adaptation, where a task adapter is trained using data from several source languages simultaneously, often yielding large gains in cross-lingual transfer performance as a result of the task adapter learning more language-agnostic representations (Ansell et al., 2021, 2022). Multi-source training requires that each training batch consists of examples from the same language so that the module for the relevant language can be applied during each step."
        },
        {
            "heading": "2.2 Recipe",
            "text": "We propose a recipe for cross-lingual transfer which is flexible and effective across scenarios of resource scarcity. It can be summarised as: 1. Select a base MMT and train language modules\nfor the source language and all target languages for which monolingual data is available. 2. Using a multilingual NMT model, translate the task data into every target language it supports; if there are target languages the MT model does not support but for which parallel data is available, adapt it using this parallel data. 3. Learn a task module through joint multi-source training on all available data (i.e. source data, translated data and any gold-standard target language (\u201cfew-shot\u201d) data available). We propose two methodological novelties to enhance this recipe.\nFew-shot upsampling. When gold-standard target language data is available during training, it is generally present in a much smaller quantity than the source language data (in our case, the difference is in orders of magnitude, but it can vary). Furthermore, it is typically higher in quality than machinetranslated target language data. For this reason, we suggest upsampling this few-shot data relative to the source and machine-translated data during multi-source training. We show in our experiments that this can improve downstream performance.\nNMT model adaptation. While recent multilingual NMT models such as NLLB (NLLB Team et al., 2022) provide impressive language coverage, there are still many languages they do not\nsupport. We therefore adapt NLLB to unseen target languages by initialising a new language token and embedding for the target language and then performing continued pretraining with parallel data for the relevant language pair.1"
        },
        {
            "heading": "3 Experimental Setup",
            "text": ""
        },
        {
            "heading": "3.1 Evaluation Tasks and Languages",
            "text": "We evaluate our models on two classification tasks: natural language inference (NLI) and sentiment analysis (SA). For NLI, we use the AmericasNLI dataset (Ebrahimi et al., 2022), which covers 10 low-resource languages from the Americas. For SA, we opt for the NusaX dataset (Winata et al., 2023), spanning 10 low-resource Indonesian languages. In the NLI task, the source language is English, while for SA it is Indonesian. We provide the list of all datasets and languages used in Table 1. These tasks are particularly amenable to the translate-train approach since the labels are preserved even after data has been translated into another language. The complete overview of the languages and their codes is given in Appendix A."
        },
        {
            "heading": "3.2 Models and Training Details",
            "text": "MMT. In this work, we use the base version of XLM-R (Conneau et al., 2020a), an MMT with 270M parameters pretrained on 100 languages.2\nNMT model. As our primary MT model for obtaining translated data, we choose the NLLB model with 3.3B parameters (NLLB Team et al., 2022), trained to translate between any pair of 200+ languages, including many low-resource languages. We also experiment with two additional NLLB variants: distilled models with 600M and 1.3B parameters, enabling us to understand the effect of model size on the \u201cquality\u201d of the obtained data. Despite the broad language coverage, half of our target languages are unsupported by the NLLB models (7\n1In fact, it is not necessary for the source language in the parallel corpus to match the intended source language for cross-lingual transfer, since multilingual NMT models can, in theory, support unseen transfer directions provided the source and target languages have been seen as part of other pairs during training; this is the case in NMT adaptation for AmericasNLI, where the parallel data is Spanish-to-X but the transfer direction is English-to-X.\n2While more powerful MMTs are available, such as XLMR-large or mDeBERTa (He et al., 2023), our primary purpose is not the maximisation of raw performance, nor a comparison of different MMTs, so we opt for a smaller model to stretch our computational budget over a broad range of scenarios and languages.\nlanguages from the AmericasNLI and 3 languages from NusaX dataset).\nWe adapt the 3.3B parameter NLLB model to unseen languages through continued pretraining on the parallel corpora listed in Appendix A. We perform full fine-tuning for 5 epochs with a batch size of 8 and an initial learning rate of 2 \u00b7 10\u22125 which is linearly decreased to zero during training.\nLanguage Modules. In general, we use the same algorithms and hyperparameters as the original papers (Pfeiffer et al., 2020; Ansell et al., 2022) when training language modules. However, we use the variant of MAD-X proposed by Pfeiffer et al. (2021), where the last adapter layers are dropped for an increase in cross-lingual transfer performance. We provide a list of resources for the monolingual corpora in Appendix A. Language modules are trained for a minimum of 100 epochs and 100,000 steps with a batch size of 8, a learning rate of 5 \u00b7 10\u22125 and a maximum sequence length of 256. We evaluate the language modules every 1,000 steps with low-resource languages, and every 5,000 steps with high-resource languages. Finally, we choose the module that has obtained the lowest perplexity on the validation set, which is created by taking 5% of the unlabelled data for low-resource languages or 1% for high-resource languages.\nTask Modules. We again follow Pfeiffer et al. (2020) and Ansell et al. (2022) except where stated otherwise. We train task adapters with a reduction factor of 16 (i.e. the ratio between the dimension of the MMT hidden state and the dimension of the adapter hidden state is 16) and task SFTs with 8% density. When jointly training on data from\nmore than one language, the training examples are batched such that each batch consists of examples from a single language, and the batches are ordered randomly. For the configurations which employ language adaptation, the language module for the relevant language is activated at the beginning of the training step and deactivated at the end of the step, following Ansell et al. (2021).\nAmericasNLI task modules are trained for 5 epochs with a batch size of 32 and an initial learning rate of 2 \u00b7 10\u22125. Evaluation is carried out every 625 steps and the checkpoint with the best evaluation accuracy is selected at the end of training. NusaX task modules are trained for 10 epochs (or 3 during the full fine-tuning phase of LT-SFT), with a batch size of 16 and an initial learning rate of 2 \u00b7 10\u22125. They are evaluated after every 250 steps and the final module is the one with the best evaluation F1 score. For both tasks, the learning rate is linearly decreased to zero over the course of training."
        },
        {
            "heading": "3.3 Configurations and Ablations",
            "text": "ZS-XLT. We include zero-shot transfer results with language adaptation, equivalent to MAD-X (Pfeiffer et al., 2020) in the case of adapters. We also have a variant where language adaptation is not employed, thus only the task module is used for training and inference. These variants are denoted by ZS and ZS \u2013 LA, respectively.\nFS-XLT. In our default FS-XLT setup, \u201cFSSINGLE\u201d we add K = 100 target shots to the source language task data, training a separate task module for each target language. We also consider \u201cFS-ALL\u201d, where a single task module is trained\non the source language data plus K = 100 shots from each target language. We investigate the effect of different numbers of shots by also carrying out FS-SINGLE experiments with K \u2208 {20, 500}.3 We employ language adaptation in all these setups, but as an ablation, we also test K = 100 without language adaptation (denoted as FS \u2013 LA).\nIn all FS experiments, the model is jointly trained on source and target data, as per Xu and Murray (2022). We upsample the data in the target language(s) by a factor of 10 to increase its presence during training, since K is still rather small compared to the number of examples available in the source language. During training, we only evaluate on the source language data following Xu and Murray (2022), who point out that the presence of large evaluation sets in truly low-resource languages is unrealistic4, and show that while evaluating on the target language is still beneficial for the joint training procedure, the gap becomes much smaller. They stress that such data would be better used for training, in line with Kann et al. (2019).\nTranslate-Train. In our main translate-train variant, named TTRAIN-ALL, we create a single task module covering all the target languages, which is trained and evaluated on the translated data of all target languages together with the source language data. We also consider TTRAIN-SINGLE, where a separate task module is trained on the data of each target language alone.\nFS-XLT meets Translate-Train. In a final set of experiments, we investigate to what extent the benefits gained from the few-shot and translate-train methods add up when they are combined. To test this, we introduce the FS + TTRAIN-ALL configuration, where we train a single task module on the union of the source language data and translated and few-shot data (with K = 100) for every target language. This module is evaluated on the source language data and the translated data in all target languages."
        },
        {
            "heading": "4 Results and Discussion",
            "text": "Main Results. The results of our primary configurations on NLI and SA are presented in Table 2,\n3For the AmericasNLI target languages NAH and OTO we actually use 223 and 377 shots respectively under the K = 500 setting since this is the maximum available.\n4The requirement for evaluation data in the target language originated from two-step methods, where such data is needed to prevent the model from overfitting to the small number of examples in the target language used during the second stage.\nwith ablations shown in Table 3.5 We find that the various cross-lingual transfer techniques we consider can be combined very effectively to improve performance. For instance, the average SA performance can be improved from the most basic ZS \u2013 LA setting by 14-17 points (depending on the PEFT method) through the use of language adaptation, translate-train and few-shot techniques with K = 100 shots (FS + TTRAIN-ALL). In the case of the NLI task, the gains under the same conditions are 19-24 points. Each of these components individually adds several points of performance, and although the gain from using FS and TTRAIN together is much smaller than the sum of their individual gains, it is still 1-2 points better than using either technique on its own. Although we did not consider FS + TTRAIN-ALL with K = 500 shots, the strength of FS-SINGLE with K = 500 as shown in Figure 1 suggests that this gap would be larger with larger K. The finding that high-quality machine translation of the entire source dataset cannot eliminate the utility of human-crafted examples contains a potentially useful lesson \u2013 we would encourage designers of datasets for cross-lingual transfer to provide at least two splits for target languages, even if the training/validation split contains only 100 examples. The relative value of few-shot and machine-translated data appears to be task-dependent. Whereas for AmericasNLI we see TTRAIN outperforming FS by 4-6 points, neither approach has a clear advantage on the NusaX task.\nMT Model Size. In Table 4, we see the effect of translation model quality on TTRAIN-ALL performance, with gains of 0.5-2 points from upscaling the NLLB model from 600 million to 3.3 billion parameters. This upscaling comes at a relatively small cost in the translate-train setup, since the training data only needs to be translated once for each target language. Translate-test setups, on the other hand, incur the cost of translating each example encountered at inference time, which is potentially much more costly for large-scale deployments.\nMT adaptation. The adaptation of the NMT model to new languages appears to be highly effective, with these languages generally enjoying large gains from the use of TTRAIN despite the small size of the parallel corpora available: the AmericasNLI\n5For Nusa-X, our main results exclude NIJ for which no monolingual data is available and thus no language module was trained; results without language adaptation are available in Appendix B.\nlanguages have parallel corpora containing 5,000- 17,000 sentences, while the NusaX parallel corpora have fewer than 1,000 sentences. It is interesting to note that for AmericasNLI, the language pairs used in MT model adaptation are different from\nthose used during cross-lingual transfer: the source language in the parallel corpora is Spanish, so the English-to-X direction required during translation of the MultiNLI dataset is completely unseen. The success of the TTRAIN configurations on this task is thus a testament to the strength and flexibility of multilingual NMT.\nNumber of Shots. We observe a rather large impact on performance from increasing the amount of few-shot data. While even 20 shots are enough to bring about a 3-5 point average gain on the NusaX task, we do not see a plateau in performance on either task even with the increase from 100 to 500 shots. Upsampling the few shots seems beneficial, yielding a 0.5-4 point gain in performance when K = 100. A finer-grained and wider exploration of this finding is warranted in future work.\nLanguage Resourcefulness. As suggested by prior work (Pfeiffer et al., 2020; Ansell et al., 2021) and by Table 3, language adaptation has a very large impact on all configurations. In the case of the\nTTRAIN-ALL configuration, language adaptation incurs gains of 3-4 points, while for ZS and FS they\n100 101 102\nSize of monolingual corpora in MB\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nR el\nat iv\ne Sc\nor es\nMonolingual Corpora Size vs Performance\nZS - LA FS + TTRAIN-ALL NLLB-seen MMT-seen NLLB-seen MMT-unseen NLLB-unseen MMT-unseen\nFigure 3: The scores of ZS - LA and FS + TTRAINALL methods with SFTs against the monolingual corpus sizes. All target languages (from NLI and SA) are shown and they are grouped based on the coverage by the NLLB and MMT models. Relative scores are displayed (a fraction of the source language performance).\nvary between 6-13 points. Figure 2 further illustrates this effect on all three methods with adapters, showing the gains from language adaptation on NLI languages against the size of their monolingual corpora: languages with larger corpora generally exhibit larger gains. The effect is visibly less pronounced for the TTRAIN-ALL where a relatively large amount of the target data (albeit translated) lessens the significance of language modules and monolingual corpora size. Finally, Figure 3 illustrates that the performance of our strongest configuration (FS + TTRAIN-ALL) with SFTs is the highest for languages that are seen by either the NLLB or MMT model \u2013 these are (incidentally) the languages with the largest corpora size too. Conversely, this pattern is absent with the ZS \u2013 LA configuration.\nLanguage families for our target languages are given in Appendix A. While they could have an impact on the performance, it is difficult to disentangle the effect of the language family from the\namount of data available without having a much larger set of evaluation languages.\nPEFT Method. The relative performances of the various configurations are very similar regardless of PEFT method, and in accordance with previous work (Ansell et al., 2022; Alabi et al., 2022), SFTs consistently outperform adapters by 2-3 points. However, we estimate that training adapters is generally around 3 times faster than training SFTs."
        },
        {
            "heading": "5 Related Work",
            "text": "Parameter-Efficient Fine-Tuning Methods. Parameter-efficient fine-tuning methods have emerged from a necessity to reduce compute and memory requirements of fine-tuning when dealing with large models. These methods can generally be grouped into those that modify the subset of parameters of the pretrained LLM (Ben Zaken et al., 2022; Guo et al., 2021; Sung et al., 2021) and those that introduce a completely fresh set of parameters to be updated (Li and Liang, 2021; Lester et al., 2021; Houlsby et al., 2019; Hu et al., 2022) allowing for different interactions with the pretrained model. They have been adopted for cross-lingual transfer as they are preferable when dozens of different fine-tunings for different languages and tasks need to be learned, stored and combined (Pfeiffer et al., 2020; Ansell et al., 2022; Parovic\u0301 et al., 2022). For a comprehensive overview of parameter-efficient fine-tuning, we refer the reader to Pfeiffer et al. (2023).\nFew-Shot Cross-Lingual Transfer. Lauscher et al. (2020) and later Zhao et al. (2021) demonstrate the effectiveness of few-shot over the zero-shot cross-lingual transfer, showing that continued training of a source-trained model on a small number of labelled examples in the target language significantly increases performance (target-adapting). Schmidt et al. (2022) trade-off efficiency for performance by replacing the sequential fine-tuning procedure of Lauscher et al. (2020) with joint training on source and target language, showing it also improves training stability and robustness. They additionally show that first fine-tuning on multiple target languages provides extra performance gains. We adopt their joint training procedure, combining it further with parameter-efficient fine-tuning methods and language adaptation. Xu and Murray (2022) also exploit a joint source-and-target training procedure, further extending it to all target languages simultaneously instead of having language-\nspecific models, which becomes particularly attractive when dealing with a large number of target languages. They also introduce stochastic gradient surgery to circumvent the issue of conflicting gradients among languages.\nJundi and Lapesa (2022) compare few-shot and translation-based approaches, trying to gain an insight into which approach is better and under which circumstances. We consider these approaches in combination rather than in competition and find that the use of few-shot data can enhance performance even when a machine translation of the full source language dataset is available. However, their work complements ours by proposing a way to identify the examples which may be most profitable for humans to translate into target language \u201cshots.\u201d\nWinata et al. (2022) study few-shot cross-lingual transfer on languages unseen by MMTs using the NusaX dataset. They analyse the effectiveness of several few-shot strategies focusing on selecting languages for transfer and different learning dynamics exhibited by different types of MMTs.\nMachine Translation for Cross-Lingual Transfer. The translate-train and translate-test approaches are common baselines for cross-lingual transfer (Conneau et al., 2020a; Hu et al., 2020). A number of enhancements have been proposed. Artetxe et al. (2020) showed that translate-test performance could be improved by training on backtranslated rather than the original source language data to better model translation artefacts encountered at inference time. Ponti et al. (2021) note that translation-based approaches suffer from an error accumulation over the phases of the pipeline. They re-interpret this pipeline as a single model with an intermediate \u201clatent translation\u201d between the target text and its classification label, permitting the translation model to be fine-tuned according to a feedback signal from the task loss. Oh et al. (2022) show that the translate-train and translatetest approaches can be combined synergistically. Artetxe et al. (2023) show that translate-test is more favourable relative to translate-train than previously thought when better translation and monolingual models are used, and when measures are taken to correct the MT-induced mismatch between the data encountered at train and inference time. While we only consider applying translation-based crosslingual transfer to classification tasks, prior work has considered its application to sequence labelling tasks as well (Jain et al., 2019; Fei et al., 2020;\nGarc\u00eda-Ferrero et al., 2022; Garc\u00eda-Ferrero et al., 2022). For simplicity, we employ only continued pretraining on the standard MT task when adapting NLLB to unseen languages. Ko et al. (2021) enhance NMT model adaptation with additional tasks: denoising autoencoding, which exploits monolingual target language data; back-translation; and adversarial training which encourages the encoder to output language-agnostic features."
        },
        {
            "heading": "6 Conclusions and Future Work",
            "text": "We have investigated how to combine several crosslingual transfer techniques which are applicable across several dimensions of resource scarcity into a single framework. We find that parameterefficient language adaptation, few-shot learning and translate-train are complementary when employed in a multi-source training setup with fewshot upsampling. However, our training setup supports the use of any subset of these techniques depending on the availability of the necessary data and models. We remark on the significance of the finding that gold-standard few-shot target data can improve performance even when the entirety of the training data is translated into the target language by a high-quality NMT model. We also observe that languages not natively supported by an NMT model can benefit from translate-train through a simple adaptation procedure even with a small amount of parallel data.\nLimitations\nOur experiments are based on two parameterefficient fine-tuning methods: adapters and SFTs. This choice facilitates comparisons with the prior work in the area of cross-lingual transfer since these two methods have been studied extensively. However, we note that other modular and parameterefficient fine-tuning methods are available and could be used in combination with our framework (Pfeiffer et al., 2023).\nOur evaluation relies solely on classification tasks, as the data labels in these tasks are preserved upon translation into another language. This is not the case with sequence-labelling tasks, where an additional challenge lies in projecting the labels after obtaining the translation. Restricting our experiments to the classification tasks enables us to have a more controlled environment for studying only the effects of different data sources which is the main focus of this work. Studying other task\nfamilies could be done as part of future work. During the adaptation of the MT model to unsupported languages, we only consider continued training with parallel data. While further performance increases could be achieved with the usage of monolingual data sources and backtranslation following Ko et al. (2021), we opt for simplicity, exploiting the monolingual data only with the language modules. Furthermore, this aligns with our goal which is not to maximize the raw performance but rather to study the effects of different data sources and their mutual interactions.\nDue to a large number of experiments across many methods and ablations, we report all our results based on a single run. However, the large number of target languages we average over and the replication of the core findings across the two PEFT methods adds confidence that they are correct.\nFinally, training language modules is typically computationally expensive. However, the modular design of cross-lingual transfer methods that we consider, enables us to train language modules only once and reuse them across all of our experiments."
        },
        {
            "heading": "Acknowledgments",
            "text": "We would like to thank Mikel Artetxe for discussing this work with us and providing helpful advice. We also thank the anonymous reviewers for their comments.\nAlan wishes to thank David and Claudia Harding for their generous support via the Harding Distinguished Postgraduate Scholarship Programme and Churchill College, Cambridge for travel assistance. Marinela Parovic\u0301 is supported by Trinity College External Research Studentship. Ivan Vulic\u0301 acknowledges the support of a personal Royal Society University Research Fellowship \u2018Inclusive and Sustainable Language Technology for a Truly Multilingual World\u2019 (no 221137; 2022\u2013)."
        },
        {
            "heading": "A Languages",
            "text": "The complete overview of languages, their codes and families, together with the monolingual data sizes and resources is provided in Table 5. The sizes and resources for the parallel corpora used in the MT model adaptation are given in Table 6."
        },
        {
            "heading": "B Ablation Experiments",
            "text": "We present per language results of our ablation experiments in Table 7. The summarised results are given in Table 3."
        },
        {
            "heading": "C Full Results with Different Number of Shots K",
            "text": "We give full results with the different number of gold-standard target shots K, where K \u2208 {0, 20, 100, 500}. The setting K = 0 resembles the ZS approach, while the rest of the values fall within FS. The results are shown in Table 8, with their summary given in Figure 1."
        }
    ],
    "title": "Unifying Cross-Lingual Transfer across Scenarios of Resource Scarcity",
    "year": 2023
}