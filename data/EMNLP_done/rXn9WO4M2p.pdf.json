{
    "abstractText": "Language Models (LMs) pre-trained with selfsupervision on large text corpora have become the default starting point for developing models for various NLP tasks. Once the pre-training corpus has been assembled, all data samples in the corpus are treated with equal importance during LM pre-training. However, due to varying levels of relevance and quality of data, equal importance to all the data samples may not be the optimal choice. While data reweighting has been explored in the context of task-specific supervised learning and LM fine-tuning, model-driven reweighting for pretraining data has not been explored. We fill this important gap and propose PRESENCE, a method for jointly reweighting samples by leveraging self-influence (SI) scores as an indicator of sample importance and pre-training. PRESENCE promotes novelty and stability for model pre-training. Through extensive analysis spanning multiple model sizes, datasets, and tasks, we present PRESENCE as an important first step in the research direction of sample reweighting for pre-training language models.",
    "authors": [
        {
            "affiliations": [],
            "name": "Megh Thakkar"
        },
        {
            "affiliations": [],
            "name": "Tolga Bolukbasi"
        },
        {
            "affiliations": [],
            "name": "Sriram Ganapathy"
        },
        {
            "affiliations": [],
            "name": "Shikhar Vashishth"
        },
        {
            "affiliations": [],
            "name": "Sarath Chandar"
        },
        {
            "affiliations": [],
            "name": "Partha Talukdar"
        }
    ],
    "id": "SP:860718785feb9416a7039048e53e2caf1ff22c18",
    "references": [
        {
            "authors": [
                "Sumyeong Ahn",
                "Seongyoon Kim",
                "Se-Young Yun."
            ],
            "title": "Mitigating dataset bias by using per-sample gradient",
            "venue": "The Eleventh International Conference on Learning Representations.",
            "year": 2023
        },
        {
            "authors": [
                "Daniel Molina",
                "Richard Benjamins"
            ],
            "title": "Explainable artificial intelligence (xai): Concepts, taxonomies, opportunities and challenges toward responsible ai",
            "venue": "Information fusion,",
            "year": 2020
        },
        {
            "authors": [
                "Alexei Baevski",
                "Sergey Edunov",
                "Yinhan Liu",
                "Luke Zettlemoyer",
                "Michael Auli."
            ],
            "title": "Cloze-driven pretraining of self-attention networks",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th Inter-",
            "year": 2019
        },
        {
            "authors": [
                "Irina Bejan",
                "Artem Sokolov",
                "Katja Filippova"
            ],
            "title": "Make every example count: On stability and utility of self-influence for learning from noisy nlp datasets",
            "year": 2023
        },
        {
            "authors": [
                "Pengfei Chen",
                "Ben Ben Liao",
                "Guangyong Chen",
                "Shengyu Zhang."
            ],
            "title": "Understanding and utilizing deep neural networks trained with noisy labels",
            "venue": "International Conference on Machine Learning, pages 1062\u20131070.",
            "year": 2019
        },
        {
            "authors": [
                "Cody Coleman",
                "Christopher Yeh",
                "Stephen Mussmann",
                "Baharan Mirzasoleiman",
                "Peter Bailis",
                "Percy Liang",
                "Jure Leskovec",
                "Matei Zaharia."
            ],
            "title": "Selection via proxy: Efficient data selection for deep learning",
            "venue": "International Conference on Learning Representa-",
            "year": 2020
        },
        {
            "authors": [
                "Alexis Conneau",
                "Kartikay Khandelwal",
                "Naman Goyal",
                "Vishrav Chaudhary",
                "Guillaume Wenzek",
                "Francisco Guzm\u00e1n",
                "Edouard Grave",
                "Myle Ott",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Unsupervised cross-lingual representation learning at scale",
            "venue": "Pro-",
            "year": 2020
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
            "year": 2019
        },
        {
            "authors": [
                "Danny Driess",
                "Fei Xia",
                "Mehdi SM Sajjadi",
                "Corey Lynch",
                "Aakanksha Chowdhery",
                "Brian Ichter",
                "Ayzaan Wahid",
                "Jonathan Tompson",
                "Quan Vuong",
                "Tianhe Yu"
            ],
            "title": "Palm-e: An embodied multimodal language model",
            "venue": "arXiv preprint arXiv:2303.03378",
            "year": 2023
        },
        {
            "authors": [
                "Rae",
                "Laurent Sifre"
            ],
            "title": "An empirical analysis",
            "year": 2022
        },
        {
            "authors": [
                "Kenji Kawaguchi",
                "Haihao Lu."
            ],
            "title": "Ordered sgd: A new stochastic optimization framework for empirical risk minimization",
            "venue": "International Conference on Artificial Intelligence and Statistics, pages 669\u2013679. PMLR.",
            "year": 2020
        },
        {
            "authors": [
                "Pang Wei Koh",
                "Percy Liang."
            ],
            "title": "Understanding black-box predictions via influence functions",
            "venue": "Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pages 1885\u20131894.",
            "year": 2017
        },
        {
            "authors": [
                "Alex Krizhevsky",
                "Ilya Sutskever",
                "Geoffrey E. Hinton."
            ],
            "title": "Imagenet classification with deep convolutional neural networks",
            "venue": "Commun. ACM, 60(6):84\u201390.",
            "year": 2017
        },
        {
            "authors": [
                "Sameer Kumar",
                "Victor Bitorff",
                "Dehao Chen",
                "Chiachen Chou",
                "Blake Hechtman",
                "HyoukJoong Lee",
                "Naveen Kumar",
                "Peter Mattson",
                "Shibo Wang",
                "Tao Wang",
                "Yuanzhong Xu",
                "Zongwei Zhou"
            ],
            "title": "Scale mlperf-0.6 models on google tpu-v3 pods",
            "year": 2019
        },
        {
            "authors": [
                "Aitor Lewkowycz",
                "Anders Andreassen",
                "David Dohan",
                "Ethan Dyer",
                "Henryk Michalewski",
                "Vinay Ramasesh",
                "Ambrose Slone",
                "Cem Anil",
                "Imanol Schlag",
                "Theo Gutman-Solo"
            ],
            "title": "Solving quantitative reasoning problems with language models",
            "year": 2022
        },
        {
            "authors": [
                "Pantelis Linardatos",
                "Vasilis Papastefanopoulos",
                "Sotiris Kotsiantis."
            ],
            "title": "Explainable ai: A review of machine learning interpretability methods",
            "venue": "Entropy, 23(1):18.",
            "year": 2020
        },
        {
            "authors": [
                "Hong Liu",
                "Sang Michael Xie",
                "Zhiyuan Li",
                "Tengyu Ma"
            ],
            "title": "Same pre-training loss, better downstream: Implicit bias matters for language models",
            "year": 2022
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Roberta: A robustly optimized bert pretraining approach",
            "venue": "arXiv preprint arXiv:1907.11692.",
            "year": 2019
        },
        {
            "authors": [
                "Paul-Aymeric Martin McRae",
                "Prasanna Parthasarathi",
                "Mido Assran",
                "Sarath Chandar."
            ],
            "title": "Memory augmented optimizers for deep learning",
            "venue": "International Conference on Learning Representations.",
            "year": 2022
        },
        {
            "authors": [
                "Mansheej Paul",
                "Surya Ganguli",
                "Gintare Karolina Dziugaite."
            ],
            "title": "Deep learning on a data diet: Finding important examples early in training",
            "venue": "Advances in Neural Information Processing Systems.",
            "year": 2021
        },
        {
            "authors": [
                "Geoff Pleiss",
                "Tianyi Zhang",
                "Ethan Elenberg",
                "Kilian Q Weinberger."
            ],
            "title": "Identifying mislabeled data using the area under the margin ranking",
            "venue": "Advances in Neural Information Processing Systems, 33:17044\u2013 17056.",
            "year": 2020
        },
        {
            "authors": [
                "Garima Pruthi",
                "Frederick Liu",
                "Satyen Kale",
                "Mukund Sundararajan."
            ],
            "title": "Estimating training data influence by tracing gradient descent",
            "venue": "Advances in Neural Information Processing Systems, 33:19920\u201319930.",
            "year": 2020
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J Liu."
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "The Journal of Machine Learning Research,",
            "year": 2020
        },
        {
            "authors": [
                "Mengye Ren",
                "Wenyuan Zeng",
                "Bin Yang",
                "Raquel Urtasun."
            ],
            "title": "Learning to reweight examples for robust deep learning",
            "venue": "ICML.",
            "year": 2018
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Marvin Ritter",
                "Maarten Bosma",
                "Alexandre Passos",
                "Jeremy Maitin-Shepard",
                "Noah Fiedel",
                "Mark Omernick",
                "Brennan Saeta",
                "Ryan Sepassi",
                "Alexander Spiridonov",
                "Joshua Newlan",
                "Andrea Gesmundo"
            ],
            "title": "Scaling up models and",
            "year": 2022
        },
        {
            "authors": [
                "Hadi Salman",
                "Andrew Ilyas",
                "Logan Engstrom",
                "Ashish Kapoor",
                "Aleksander Madry"
            ],
            "title": "Do adversarially robust imagenet models transfer better",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Teven Le Scao",
                "Thomas Wang",
                "Daniel Hesslow",
                "Lucile Saulnier",
                "Stas Bekman",
                "M Saiful Bari",
                "Stella Bideman",
                "Hady Elsahar",
                "Niklas Muennighoff",
                "Jason Phang"
            ],
            "title": "What language model to train if you have one million gpu hours",
            "year": 2022
        },
        {
            "authors": [
                "Swabha Swayamdipta",
                "Roy Schwartz",
                "Nicholas Lourie",
                "Yizhong Wang",
                "Hannaneh Hajishirzi",
                "Noah A. Smith",
                "Yejin Choi."
            ],
            "title": "Dataset cartography: Mapping and diagnosing datasets with training dynamics",
            "venue": "Proceedings of the 2020 Conference on Empirical",
            "year": 2020
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141 ukasz Kaiser",
                "Illia Polosukhin."
            ],
            "title": "Attention is all you need",
            "venue": "Advances in Neural Information",
            "year": 2017
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin."
            ],
            "title": "Attention is all you need",
            "venue": "Advances in neural information processing systems, 30.",
            "year": 2017
        },
        {
            "authors": [
                "Guillaume Wenzek",
                "Marie-Anne Lachaux",
                "Alexis Conneau",
                "Vishrav Chaudhary",
                "Francisco Guzm\u00e1n",
                "Armand Joulin",
                "Edouard Grave."
            ],
            "title": "CCNet: Extracting high quality monolingual datasets from web crawl data",
            "venue": "Proceedings of the Twelfth Lan-",
            "year": 2020
        },
        {
            "authors": [
                "Sang Michael Xie",
                "Hieu Pham",
                "Xuanyi Dong",
                "Nan Du",
                "Hanxiao Liu",
                "Yifeng Lu",
                "Percy Liang",
                "Quoc V. Le",
                "Tengyu Ma",
                "Adams Wei Yu"
            ],
            "title": "Doremi: Optimizing data mixtures speeds up language model pretraining",
            "year": 2023
        },
        {
            "authors": [
                "Linting Xue",
                "Noah Constant",
                "Adam Roberts",
                "Mihir Kale",
                "Rami Al-Rfou",
                "Aditya Siddhant",
                "Aditya Barua",
                "Colin Raffel."
            ],
            "title": "mT5: A massively multilingual pre-trained text-to-text transformer",
            "venue": "Proceedings of the 2021 Conference of the North American Chap-",
            "year": 2021
        },
        {
            "authors": [
                "Cheng Yang",
                "Shengnan Wang",
                "Chao Yang",
                "Yuechuan Li",
                "Ru He",
                "Jingqiao Zhang"
            ],
            "title": "Progressively stacking 2.0: A multi-stage layerwise training method for {bert} training speedup",
            "year": 2021
        },
        {
            "authors": [
                "Chih-Kuan Yeh",
                "Joon Kim",
                "Ian En-Hsu Yen",
                "Pradeep K Ravikumar."
            ],
            "title": "Representer point selection for explaining deep neural networks",
            "venue": "Advances in neural information processing systems, 31.",
            "year": 2018
        },
        {
            "authors": [
                "Xiaoyong Yuan",
                "Pan He",
                "Qile Zhu",
                "Xiaolin Li."
            ],
            "title": "Adversarial examples: Attacks and defenses for deep learning",
            "venue": "IEEE transactions on neural networks and learning systems, 30(9):2805\u20132824.",
            "year": 2019
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Language models (LM), typically pre-trained on large volumes of unlabeled text data, have become ubiquitous model choices for various challenging downstream tasks (Lewkowycz et al., 2022; Driess et al., 2023). The fundamental direction pursued for improving language model pre-training involves increasing the amount of training data or scaling model size (Scao et al., 2022). The training data is generally assembled from scraping the web and filtered using manually crafted heuristics that often require domain expertise (Xue et al., 2021). A key similarity in these prior works is the uniform treatment of the data samples available in the assembled corpora, without any regard for the data quality.\n\u2217Work done while at Google Research India\nPrior works for both model-based sample selection (Swayamdipta et al., 2020) and reweighting (Mindermann et al., 2022) use a supervised learning setup. They often rely on curating special validation sets (Jain and Shenoy, 2022), proxy models (Pleiss et al., 2020; Mindermann et al., 2022), or utilizing loss and prediction uncertainty signals based on ground-truth labels (Kawaguchi and Lu, 2020; Coleman et al., 2020). Adaptation of these methods to pre-training is often non-trivial. Performance of a pre-trained model on downstream tasks cannot be predicted by its pre-training validation performance. Moreover, offline filtering using proxy models is quite expensive for the massive scale of pre-training data (Liu et al., 2022).\nIn this paper, we attempt to develop an effective data reweighting framework for language model pre-training. We use self-influence (SI), the degree to which a given training sample affects model training and its own prediction, as an indicator of sample importance for pre-training. SI scores have been previously shown to be effective in identifying noisy and outlier samples (Yeh et al., 2018), but these evaluations have been limited to supervised settings. We first verify the ability of SI scores to predict sample quality of pre-training data, such as noisy text and domain mismatched samples. We then probe their effectiveness for pre-training dataset selection by using them to filter out noisy samples in the pre-training data derived from the web.\nBased on our analysis which shows that selfinfluence scores can be used as an indicator of sample importance, we propose PRESENCE: Pretraining data Re-weighting with Self-influence. PRESENCE is an online and adaptive data reweighting method that uses self-influence scores to weigh samples in a training batch. We note that during pre-training, the training loss decreases exponentially in the initial steps, with a minimal decrease in loss values in the subsequent stages (Yang\net al., 2021). Furthermore, well-trained models can identify noisy samples better when used to calculate SI scores, as compared to models in very early stages of training (Pruthi et al., 2020). Based on these observations, we formulate a two stage reweighting strategy: (i) in the first stage of learning, data samples with higher SI scores are emphasized more to drive learning using influential samples, while (ii) in the second stage, data samples with higher SI scores are de-emphasized. This limits the impact of noisy and unreliable samples while giving more weight to the higher quality samples. To the best of our knowledge, this is the first work that evaluates the use of influence functions for sample selection and reweighting at the scale of pre-training. Our contributions are as follows:\n\u2022 We initiate a study into data reweighting for pretraining and establish the relationship of selfinfluence (SI) scores with sample characteristics such as noise and domain mismatched information in the training data.\n\u2022 We present sequential data filtering using SI scores as an effective data selection strategy for pre-training, and evaluate the performance of models pre-trained on large-scale filtered datasets. We call this method PRESENCESequential.\n\u2022 Building on our findings, we propose PRESENCE, a model-driven sample reweighting method using self-influence scores that jointly weighs samples and enables learning. PRESENCE promotes novelty and stability for model pre-training.\n\u2022 Through extensive experiments and analyses spanning multiple model sizes, datasets, and tasks, we demonstrate that PRESENCE provides consistent gains over pre-training using randomly sampled pre-training corpora or SI score based filtered data. We believe PRESENCE is an important step in the research direction of data sample weighting for pretraining."
        },
        {
            "heading": "2 Background: TracIn and Self-influence",
            "text": "Though PRESENCE can be used with any influence function, we use TracIn (Pruthi et al., 2020) based self-influence score due to its scalability, generalizability, and effectiveness in identifying outliers.\nSelf-influence using TracIn: TracIn computes influence, i.e., how the loss on the test point changes during the training process whenever the training sample of interest was utilized by a first-\norder gradient approximation. For a model f with parameters \u03b8 and loss function l(f\u03b8, \u00b7), the gradient g(\u03b8, \u00b7) for a sample z is g(f\u03b8, z) = \u2207l(f\u03b8, z). The TracIn(f\u03b8, \u00b7, \u00b7) influence of training sample z on test sample z\u2032 is given by,\nTracIn(f\u03b8, z, z\u2032) = g(f\u03b8, z) \u00b7 g(f\u03b8, z\u2032) (1)\nSelf-influence score measures the influence a sample has on itself. This is identical to replacing z\u2032 with z in Equation 1, giving TracInSI(f\u03b8, \u00b7) as,\nTracInSI(f\u03b8, z) = g(f\u03b8, z) \u00b7 g(f\u03b8, z) (2)"
        },
        {
            "heading": "2.1 Relationship between Self-Influence and Sample Quality",
            "text": "We investigate the relationship between selfinfluence (SI) scores and sample quality by probing noisy and domain mismatched samples. We expect these samples to have high self-influence scores as they tend to reduce the loss w.r.t. a well-trained model (Yeh et al., 2018). We use a pre-trained mT5-base (Xue et al., 2021) model and calculate self-influence with a span-corruption loss. We randomly sample 10, 000 samples for three languages in mC4 (Raffel et al., 2020) and calculate the selfinfluence scores of clean or original samples and their corresponding noisy samples, i.e., samples with a permuted word order. Similarly, we calculate average self-influence scores over domain mismatched samples and compare them with average scores over randomly sampled English corpus.\nAs shown in Figure 1, we observe substantially high average self-influence scores for noisy samples across all languages as well as for domain mismatched text in English. The results indicate\nthat SI scores can be used to distinguish between correct and noisy text and they can also be used to detect data from a novel domain."
        },
        {
            "heading": "3 PRESENCE-Sequential: Filtering Pre-training Data using Self-Influence",
            "text": "Extending TracIn based Self-influence for Pretraining: As pre-training is computationally expensive, we leverage the layer agnostic nature of TracIn and introduce an optimized layer-wise self-influence calculation. For layers K = {k1, k2, . . . , kK} of model f\u03b8, let f\u03b8,k denote the parameters of layer k. Self-influence for any layer set K \u2282 K is,\nTracInSIK(f\u03b8, z) = \u2211 k\u2208K TracInSI(f\u03b8,k, z) (3)\nAs shown in Section 2.1, there is a relation between SI scores and the sample quality. We leverage this property to filter large-scale web corpora in an offline manner to create more suitable pretraining data. We present an overview of our offline sequential filtering strategy using self-influence scores, called PRESENCE-Sequential, in Figure 2. Assuming that a model requires N training samples for pre-training, we choose N samples from a set of N \u2032 > N samples by filtering out samples with the highest SI scores using a proxy model trained\non randomly sampled data (SI Scoring Model). To obtain a relevant pre-training set D (|D| = N), from the larger corpora D\u2032 (|D\u2032| = N \u2032), we use the scoring model F\u03b8(\u00b7) to calculate the SI scores using Equation 3 for all samples di \u2208 D\u2032,\nTracInSIK(F\u03b8, D\u2032) = {TracInSIK(F\u03b8, di|di \u2208 D\u2032)} (4)\nWe sort TracInSIK(F\u03b8, D\u2032) in increasing order of SI scores and filter out N \u2032\u2212N samples with the highest score. The remaining N samples comprise the filtered set D used for pre-training,\nD = {di|di \u2208 D\u2032} \u2200i : i \u2208 sorted(TracInSIK(F\u03b8, D\u2032))[1 : N ]\n(5)\nPre-training Setup: We use the mC4 dataset (Xue et al., 2021) and pre-train an mT5-base model for 200, 000 steps on randomly shuffled data, and use this as the \u2019Scoring Model (F\u03b8)\u2019 to create the\nfiltered dataset. We pre-train an mT5-base model from scratch on the filtered mC4 set for 200, 000 steps by choosing samples with the least SI scores that are theoretically more suitable for model learning. The models are trained with a batch size of 1024, with an input token length of 1024 and output token length of 229. Following Raffel et al. (2020), we use a base learning rate of 1.0 with 10000 warmup steps, an inverse square root learning rate decay schedule, and a loss-normalizing factor of 234496. We use the first layer of the encoder and the first layer of the decoder in the set K for TracInSIK.\nDownstream Tasks and Fine-tuning: Following Xue et al. (2021), we utilize datasets across 5 tasks from the XTREME multilingual benchmark (Hu et al., 2020), including Question Answering, Sentence-Pair, and Structured Prediction. We evaluate on (i) zero-shot cross-lingual transfer: where the fine-tuning data is only in English, and (ii) translate-train: where the fine-tuning data is in English and translated into the target languages for all the downstream datasets. We summarize the datasets used for evaluation in Table 1. We finetune all the models on the downstream tasks using a batch size of 128, with a learning rate of 0.001, and a dropout rate of 0.1 for 20, 000 steps."
        },
        {
            "heading": "3.1 Results and Analysis",
            "text": "We compare the performance of the model pre-trained on filtered web corpora (mT5base+PRESENCE-Sequential) with the baseline model trained on randomly sampled data in Table 2. We observe that when we filter out samples with high SI scores, we obtain consistent gains over the baseline models. This indicates that SI\nscores can be used as an indicator of sample quality and can be used for pre-training dataset filtering. To further test our hypotheses, we pre-train a model on data created by removing low SI samples (reverse ranking). We label this model mT5base+PRESENCE-Sequential-reverse. This model performs significantly worse compared to the baseline, further validating that SI scores are indeed an indicator of the sample quality, and are effective in identifying noisy samples in the large-scale pre-training corpora.\nHowever, as mentioned, PRESENCE-Sequential requires different expensive sequential processes: (i) Pre-train a SI scoring model, and (ii) pre-train a second model on the filtered dataset. Since pretraining is computationally expensive, we explore a joint sample reweighting adaptation next."
        },
        {
            "heading": "4 PRESENCE: Sample Reweighting using",
            "text": "Self-influence\nIn this approach, we use the SI scores in an online joint setting by reweighting samples at the minibatch level. We calculate sample SI scores at each training step and use them to weigh sample gradients before aggregating them for the gradient update. To formulate sample reweighting using SI scores, we consider batch B = {zi| i \u2208 [1, n]}, where zi denotes a sample. We calculate SI scores using Equation 3 for each sample to get array S, where |S| = n,\nS = {si|si = TracInSIK(f\u03b8, zi); i \u2208 [1, n]} , (6)\nwhere si denotes the SI score of zi. We normalize S for numerical stability and uniformity,\nS \u2190 normalize(S) = S \u2212 \u00b5(S)\u221a \u03c32(S) + \u03f5 , (7)\nwhere \u00b5(\u00b7) and \u03c32(\u00b7) denote the mean and variance, respectively, and \u03f5 is a small number. To calculate relative weights for the samples\u2019 gradients, we use a softmax function, softmax(\u00b7) over each si with temperature \u03c4 to get weights wi,\nwi = softmax(si, \u03c4) = e\u03c4 \u00b7si\u2211\nsi\u2208S e \u03c4 \u00b7si\n(8)\nHere, wi gives the weight for the gradient of sample zi. Using weights wi, the gradient G for the model update is given by,\nG = \u2211 zi\u2208B wi \u00b7 g(f\u03b8, zi) (9)"
        },
        {
            "heading": "4.1 Two-staged Reweighting for Pre-training",
            "text": "A common concern of pre-training is the redundancy of training data even in massive corpora mined from the web (Raffel et al., 2020). Evidently, training loss decreases exponentially early in the pre-training (Figure 3). Hence, as training proceeds, it quickly becomes the case that almost all of the computation involved in training is spent on concepts the model has already seen many times.\nHigh SI scores indicate high gradient norms, which are critical for driving model training (McRae et al., 2022). However, encountering high SI scores from well-trained models is often a signal of noise or outlier samples (Pruthi et al., 2020). We combine the information about the property of SI scores relative to the ability of the model to calculate them and the nature of the pre-training data to devise a novel two-stage sample reweighting strategy. We utilize the temperature term \u03c4 when calculating the softmax weights (Equation 8) to formulate the two stages. In the first stage, which we call \u2019direct\u2019 weighting, we choose \u03c4 > 0, giving the data samples with higher SI scores more emphasis, driving the model learning and promoting novelty. In the second stage, or \u2019inverse\u2019 weighting, where the model has matured, we use \u03c4 < 0. This de-emphasizes the data samples with higher\nAlgorithm 1 Microbatched Training B\u2190 Batch G = {} \u2190 Gradient array G = 0\u2190 Gradient initialization\nfor microbatch bi in minibatch B do gi = \u2207l(f\u03b8, bi) G \u2190 G \u222a gi\nfor gi in G do G\u2190 G+ gi\nSI scores to limit the impact of noisy and unreliable samples. Two-staged reweighting ensures that the model learns novel information early, and is able to eliminate noise at a later stage with stable learning. For temperatures \u03c41 > 0 and \u03c42 < 0, the softmax temperature at training step i is given by,\n\u03c4 = { \u03c41, i \u2264 I \u03c42, i > I , (10)\nwhere I denotes the step where we switch stages. We refer to model-driven online sample reweighting strategy using SI scores as PRESENCE. We now adapt PRESENCE to the scale of pre-training."
        },
        {
            "heading": "5 Pre-training Adaptation",
            "text": "Pre-training requires a large batch size and is prone to instability (Krizhevsky et al., 2017). We thus adapt PRESENCE for pre-training by applying it at the microbatch level. This provides dual benefits of regularizing the pre-training while being computationally efficient."
        },
        {
            "heading": "5.1 Reweighting Microbatch Gradients using",
            "text": "Self-Influence\nMicrobatched Training Microbatched training enables the use of a larger effective minibatch size. It involves dividing the full minibatch B into smaller batches, called microbatches, and individually calculating gradients for each microbatch. These gradients are then aggregated to get the minibatch gradient G. We present a standard microbatched training algorithm in Algorithm 1, assuming that a minibatch B is divided into n microbatches, i.e. B = {bi|i \u2208 [1, n]}.\nWe first calculate the self-influence for a microbatch by replacing the individual sample z with a microbatch b in Equation 3 to calculate the loss. MicrobatchSIK(f\u03b8, \u00b7) for microbatch b is,\nMicrobatchSIK(f\u03b8, b) = TracinSIK(f\u03b8, b) (11)\nTo formulate microbatch level reweighting using their self-influence scores, we calculate the selfinfluence using Equation 11 for each microbatch to get array S , where |S| = n,\nS = {si|si = MicrobatchSIK(f\u03b8, bi); i \u2208 [1, n]} , (12)\nwhere si denotes the SI score of bi. Using the updated array S in Equation 6 and microbatch training strategy (Algorithm 1), we obtain the gradient for the model update G using Algorithm 2 with SI based reweighting in Algorithm 2."
        },
        {
            "heading": "5.2 Training Setup",
            "text": "We use two different variants of the T5 architecture (Raffel et al., 2020), namely mT5-base and mT5-large for comparisons and pre-train on the mC4 dataset (Xue et al., 2021). We refer to our corresponding reweighted variants as mT5-basePRESENCE and mT5-large-PRESENCE respectively. We pre-train the models with an input length 1024 and output length 229, using batch sizes of 1024 for mT5-base and 512 for mT5-large. We use lossnormalization during training with a loss normalization factor of 234496 for mT5-base and 117248 for mT5-large. For mT5-base-PRESENCE, we divide\nAlgorithm 2 Weighted Microbatched Training B\u2190 Batch G = {} \u2190 Gradient array S = {} \u2190 Self-influence array G = 0\u2190 Gradient initialization \u03c4 \u2190 weighting temperature\nfor microbatch bi in minibatch B do gi = \u2207l(f\u03b8, bi) si = gi \u00b7 gi G \u2190 G \u222a gi S \u2190 S \u222a si\nS \u2190 normalize(S) W = {softmax(si, \u03c4)|si \u2208 S} for gi, wi in G,W do\nG\u2190 G+ gi \u00b7 wi\nthe minibatch into n = 8 microbatches and for mT5-large-PRESENCE, we divide the minibatch into n = 4 microbatches. We select \u03c41 = 1, \u03c42 = \u22121, and I = 100, 000 for the two-staged learning. We use the first layer of the encoder and first layer of the decoder as the layer set K. We use a base learning rate of 1.0 with 10, 000 warm-up steps and an inverse square root decay schedule, pre-training for 1 million steps."
        },
        {
            "heading": "6 Results and Analysis",
            "text": ""
        },
        {
            "heading": "6.1 Effectiveness of PRESENCE",
            "text": "We compare the performance of using PRESENCE with mT5-base (mT5-base+PRESENCE) and mT5large (mT5-large+PRESENCE) with random pretraining in Table 3. We observe that for both variants, using PRESENCE helps improve performance on all the datasets considered. This validates the\neffectiveness of PRESENCE, indicating that generating SI scores at microbatch level offers a smoother and more stable scoring approach as opposed to sample-level SI scoring. The average improvement with the PRESENCE framework is more for the large mT5 variants. A larger model (mT5-large) potentially generates more reliable SI scores when used for reweighting compared to the mT5-base model. We hypothesize these two factors as the key reasons for the significant improvements observed for the PRESENCE approach, particularly for the mT5-large model.\nWe also make some interesting observations for zero-shot and translate-train dataset variants. For both mT5-base and mT5-large, we observe more significant gains for PRESENCE when the training data is available in the target languages (translate-train). This indicates that reweighting might be beneficial for the model to adapt better across languages as compared to unweighted training. For instance, we consider the English subset from XQuAD and XNLI for translate-train settings. We observe that PRESENCE improves performance significantly for mt5-large experiments (Table 4)."
        },
        {
            "heading": "6.2 Comparison with PRESENCE-Sequential",
            "text": "We compare PRESENCE with the multi-step PRESENCE-Sequential explained in Section 3. For a fair comparison, we pre-train mT5-base-PRESENCE for 200, 000 steps on randomly sampled data and present the results in Figure 4. We observe that even though PRESENCE does not look at the complete data at once and operates in a joint online setting, it performs comparably, and in some cases, outperforms PRESENCE-Sequential. This indicates\nthat our online adaptation of microbatch reweighting using SI scores is competitive for model pretraining relative to sequential offline dataset filtering. One possible reason might be that the online reweighting relies upon the most recent model weights for calculating influence of training samples, providing more suitable signals for data reweighting as compared to the offline setting. Further, the joint online version forms an elegant and computationally efficient alternative to the sequential offline approach, providing an opportunity for scaling reweighting to larger models and datasets."
        },
        {
            "heading": "6.3 Impact of Two-staged Learning",
            "text": "We analyze the impact of using our two stage reweighting strategy by comparing its performance with models pre-trained purely with direct, i.e., \u03c4 = 1 (PRESENCE-D) and inverse, i.e., \u03c4 = \u22121 (PRESENCE-I) weighting. We train all the variants for 200, 000 steps and compare their performance in Table 5. As shown, we observe superior performance of PRESENCE compared to the other reweighting strategies. This supports our hypothesis that pre-training probably happens in two parts: the model quickly learns new information in the first stage, after which all new information seems redundant. The second stage is important to stabilize the pre-training. To further test this, we perform reweighting in the reverse order, first performing inverse weighting and then direct weighting (PRESENCE-I-D). This strategy causes a degradation in the performance, as the inverse weighting initially may slow down training,\nTemperature ( )\n45\n60\n75\n90\n-2 -1 0 1 2\nMLQA TyDi QA XNLI Ev al ua tio n M et ric\n\u03c4\nFigure 5: Effect of temperature \u03c4 during reweighting on translate-train versions of QA and sentence pair tasks. Discussions in Section 6.4.\nwhile the direct weighting in later stages leads to increased use of noisy samples. However, there are certain datasets where either purely direct or inverse weighting perform better than PRESENCE. We believe that self-influence scores develop correlations with multilingual data based on their quantity in the corpora, which may cause varied trends in downstream tasks."
        },
        {
            "heading": "6.4 Scaling Microbatch Gradient Weights",
            "text": "Since our method currently uses two discrete values of temperature \u03c4 in Equation 8, we probe its effect on single-stage reweighting during model pretraining. These findings can be used to formulate a more granular and automated reweighting strategy using temperature scaling or continuous temperature scheduling, which we leave as future work. We pre-train models with \u03c4 = {\u22122,\u22121, 1, 2} for 200, 000 steps and evaluate them on the downstream tasks in Figure 5. We observe that increasing the magnitude of \u03c4 both positively and negatively affects the model performance. A possible reason might be that high positive \u03c4 leads to a large variance in the microbatch gradient weights, leading to unstable training, whereas high negative \u03c4 results in much slower convergence compared to the baselines."
        },
        {
            "heading": "7 Related Work",
            "text": "Datasets for Pre-training LMs Language models are generally pre-trained on large-scale corpora scraped from the web (Devlin et al., 2019; Liu et al., 2019; Baevski et al., 2019). The most common source of obtaining large-scale data is Common\nCrawl1, a publicly-available web archive that provides \u201cweb extracted text\u201d. Raffel et al. (2020) use various heuristics such as retaining lines that end in terminal punctuations, retaining lines and pages based on a minimum number of words, etc to clean Common Crawl. Wenzek et al. (2020) use a Kneser-Ney language model (Heafield, 2011) and calculate the perplexity over training data and a high quality target domain to extract high quality documents. Multilingual pre-training has also known to depend on language and domain distributions in the corpora (Conneau et al., 2020; Du et al., 2022; Hoffmann et al., 2022). Multilingual pre-training involves an additional step of boosting low-resource language data in the corpora using temperature sampling (Conneau et al., 2020; Arivazhagan et al., 2019; Xue et al., 2021). DoReMi (Xie et al., 2023) uses a smaller proxy model to calculate domain weights of different sources comprising the mixture of the pre-training data to pretrain larger models. These works either rely on expertly crafted heuristics or require training additional models for dataset selection and filtering.\nInfluence Functions and Training Data Attribution Influence functions help to trace a model\u2019s prediction through the learning algorithm and back to its training data, a practice commonly known as Training Data Attribution (TDA) (Guu et al., 2023). Influence functions have been extensively used in deep learning as a means of model interpretability and explainability (Linardatos et al., 2020; Arrieta et al., 2020; Guidotti et al., 2018), adversarial learning (Yuan et al., 2019; Salman et al., 2020), federated learning (Kairouz et al., 2021; Geiping et al., 2020), and identifying outliers or mislabeled samples (Koh and Liang, 2017; Yeh et al., 2018). TracIn (Pruthi et al., 2020) introduces a scalable and general first-order approximation to calculate gradient based influence, and extends it to the minibatch level. Bejan et al. (2023) formulates an automated curricular strategy using SI scores for data cleaning and filtering for NLP tasks. Influence functions have mostly been applied for supervised learning with ground truth labels for the data and have generally been explored in an offline setting.\nData selection and online adaptation in supervised learning Selection functions for supervised learning often leverage training dynamics such as high loss (Jiang et al., 2019; Kawaguchi and\n1https://commoncrawl.org/about/\nLu, 2020) or high prediction uncertainty (Coleman et al., 2020) to select \"hard\" points. Swayamdipta et al. (2020) use the change in loss over the course of training rather than each step to also eliminate noisy samples. Removing noisy training samples using offline methods is another direction for selecting training data for supervised learning (Chen et al., 2019; Pleiss et al., 2020). Paul et al. (2021) use norm of the gradient or self-influence to identify important samples early in the training to heavily prune datasets. RHO-loss (Mindermann et al., 2022) calculates a heldout loss using a proxy model and uses a combination of model loss and heldout loss to select non-noisy, non-redundant, and taskrelevant samples. Ahn et al. (2023) uses per-sample gradient norm to assign importance probabilities, and trains a biased model to formulate a debiased model training strategy. Ren et al. (2018) and (Jain and Shenoy, 2022) use meta-learning for reweighting training samples within a batch for increasing robustness and selective prediction respectively. These works operate in supervised settings, requiring controlled validation sets or proxy models and adapting them to pre-training is non-trivial."
        },
        {
            "heading": "8 Conclusion and Future Work",
            "text": "We introduce PRESENCE - a method for jointly reweighting samples using self-influence (SI) scores and pre-training. We conduct an in-depth analysis of the relationship between SI scores and sample quality from a pre-training perspective and use them as a filtering objective for pre-training data selection. As sequential filtering is expensive at the scale of pre-training, we formulate PRESENCE as a joint adaptation for sample reweighting. PRESENCE outperforms baselines trained on randomly sampled and SI-filtered data on 5 datasets across 3 tasks. We believe that PRESENCE is an important first step in the research direction of data sample weighting for pre-training.\nAs future work, we plan to explore relationships between samples in the pre-training corpora and influence functions across languages, data sources, and domains. We also plan to formulate automated reweighting strategies using temperature scaling schedules based on the training step, training loss, and sample influence scores.\nLimitations\nAs a pre-training strategy, PRESENCE is computationally expensive for finding the optimal hyper-\nparameters, particularly for the two-staged learning. Calculation of self-influence score is only done using the first layers of the encoder and decoder for computational optimization, however, using more layers might lead to more representative weighting information. Even though we believe the training overhead of PRESENCE is significantly lesser compared to the overhead of existing methods such as sequential offline filtering, our implementation on microbatches requires a training time higher by 30% compared to training the models on randomly sampled data without any reweighting. Since the gradients across microbatches are independent, there can be ways to parallelize the computation. Our two stage training strategy currently switches at a training step which is chosen based on the total training steps, looking at the loss curve, and following how warm-up steps for learning rate schedules are decided for LM pre-training, which is ad-hoc. This can be formalized based on training loss, microbatch self-influence scores, or anchored to the dataset itself, and may lead to more suitable sample reweighting using temperature scaling."
        },
        {
            "heading": "Acknowledgements",
            "text": "SC is supported by the Canada CIFAR AI Chairs program, the Canada Research Chair in Lifelong Machine Learning, and the NSERC Discovery Grant. The authors would like to thank Shubham Mittal for his assistance in compiling and analyzing the language-wise and multilingual results, and Dheeraj Rajagopal, Lucas Dixon, Pradeep Shenoy, and Kelvin Guu for insightful discussions on the work. The authors are also grateful to the reviewers and the area chairs for their helpful reviews and discussions."
        },
        {
            "heading": "A Two-staged Reweighting and Learning Rate Schedulers",
            "text": "We create an analogy between our two stage reweighting and the transformer learning rate scheduler (Vaswani et al., 2017a). The learning rate lr at step step for a model with input and output dimensionality dmodel and warm-up steps warmup is given by,\nlr = d\u22120.5model \u00b7min(step \u22120.5, step.warmup\u22121.5) (13)\nThis corresponds to increasing the learning rate linearly for the first warmup training steps, and decreasing it thereafter proportionally to the inverse\nsquare root of the step number. We contrast these types of learning rate schedulers with our two stage reweighting strategy. Increasing the learning rate for a given number of steps warms up the model more by boosting the gradients, and thereafter a decay is used to enable the model to reach a minima better. Intuitively, we also aim to achieve similar learning dynamics using our two stage learning: in the first stage of learning, data samples with higher SI scores are emphasized more to drive more learning, while in the subsequent second stage, the data samples with higher SI scores are de-emphasized to limit the impact of noisy and unreliable samples while giving more weight to better quality samples and for more stable training. We believe that as future work, we can use temperature scaling schedulers inspired from learning rate schedulers to automate reweighting curricula.\nB Infrastructure\nWe use seqio and T5X (Roberts et al., 2022) to train our models. We use 64 TPU (Kumar et al., 2019) chips for pre-training all the models and use 8 TPU chips for fine-tuning the base variant and 16 TPU chips for fine-tuning the large variant."
        },
        {
            "heading": "C Maturity of Models and SI Scores",
            "text": "Self-influence (SI) scores are calculated using the model gradients for a given objective. Their reliability, thus depends on the maturity of the model, i.e. how well the model is trained, which is being used to calculate them. Since SI scores are generally used for relative analyses, the models need not be trained till convergence. This characteristic relationship between the model\u2019s ability to predict correct labels and the reliability of SI scores becomes an important consideration when adapting SI scores for online adaptations. We have observed\nthat models trained for about 20% of training steps give decently reliable SI scores, however, better strategies such as choosing checkpoints where the loss decreases the most and averaging scores of multiple checkpoints have been proposed in related works (Pruthi et al., 2020). We believe that the direct weighting stage of PRESENCE that drives more learning also acts as an added warmup for the model\u2019s ability to predict noisy samples for the subsequent inverse weighting stage, enabling it to stabilize the training further. We leave the deeper analyses of SI scores on training samples early in the pre-training and in later stages as future work."
        }
    ],
    "title": "Self-Influence Guided Data Reweighting for Language Model Pre-training",
    "year": 2023
}