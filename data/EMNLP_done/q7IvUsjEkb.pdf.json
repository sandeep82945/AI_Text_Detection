{
    "abstractText": "Multi-path voting methods like self-consistency have been used to mitigate reasoning errors in large language models caused by factual errors and illusion generation. However, these methods require excessive computing resources as they generate numerous reasoning paths for each problem. Our experiments show that on the arithmetic reasoning task, SVAMP, half of the problems fail to obtain noticeable accuracy gains when voting with more than three paths. In this paper, we propose a novel multipath voting technique called Dynamic Voting, which effectively reduces the number of reasoning paths during multi-path voting while preserving accuracies by applying early exiting for problems that large language models can confidently solve. Experimental evaluations on arithmetic, commonsense, and symbolic reasoning tasks under few-shot and zeroshot settings demonstrate that Dynamic Voting achieves comparable accuracies employing significantly fewer reasoning paths. Notably, one of our Dynamic Voting strategies outperforms self-consistency using only 24.7% of the number of paths on the LetterConcat task in the few-shot setting. Furthermore, Dynamic Voting showcases strong robustness of thresholds. It also demonstrates excellent generalizability when combined with other voting techniques, different models, and diverse prompts.",
    "authors": [
        {
            "affiliations": [],
            "name": "Mingfeng Xue"
        },
        {
            "affiliations": [],
            "name": "Dayiheng Liu"
        },
        {
            "affiliations": [],
            "name": "Wenqiang Lei"
        },
        {
            "affiliations": [],
            "name": "Xingzhang Ren"
        },
        {
            "affiliations": [],
            "name": "Baosong Yang"
        },
        {
            "affiliations": [],
            "name": "Jun Xie"
        },
        {
            "affiliations": [],
            "name": "Yidan Zhang"
        },
        {
            "affiliations": [],
            "name": "Dezhong Peng"
        },
        {
            "affiliations": [],
            "name": "Jiancheng Lv"
        }
    ],
    "id": "SP:52c8b4e83e4ca6fe087ea22fd8252403232c868c",
    "references": [
        {
            "authors": [
                "Berkant Barla Cambazoglu",
                "Hugo Zaragoza",
                "Olivier Chapelle",
                "Jiang Chen",
                "Ciya Liao",
                "Zhaohui Zheng",
                "Jon Degenhardt."
            ],
            "title": "Early exit optimizations for additive machine learned ranking systems",
            "venue": "WSDM, pages 411\u2013420. ACM.",
            "year": 2010
        },
        {
            "authors": [
                "Wenhu Chen",
                "Xueguang Ma",
                "Xinyi Wang",
                "William W. Cohen."
            ],
            "title": "Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks",
            "venue": "CoRR, abs/2211.12588.",
            "year": 2022
        },
        {
            "authors": [
                "Zhoujun Cheng",
                "Tianbao Xie",
                "Peng Shi",
                "Chengzu Li",
                "Rahul Nadkarni",
                "Yushi Hu",
                "Caiming Xiong",
                "Dragomir Radev",
                "Mari Ostendorf",
                "Luke Zettlemoyer",
                "Noah A. Smith",
                "Tao Yu."
            ],
            "title": "Binding language models in symbolic languages",
            "venue": "CoRR,",
            "year": 2022
        },
        {
            "authors": [
                "Narang",
                "Gaurav Mishra",
                "Adams Yu",
                "Vincent Y. Zhao",
                "Yanping Huang",
                "Andrew M. Dai",
                "Hongkun Yu",
                "Slav Petrov",
                "Ed H. Chi",
                "Jeff Dean",
                "Jacob Devlin",
                "Adam Roberts",
                "Denny Zhou",
                "Quoc V. Le",
                "Jason Wei"
            ],
            "title": "Scaling instruction-finetuned language models",
            "year": 2022
        },
        {
            "authors": [
                "Peter Clark",
                "Isaac Cowhey",
                "Oren Etzioni",
                "Tushar Khot",
                "Ashish Sabharwal",
                "Carissa Schoenick",
                "Oyvind Tafjord."
            ],
            "title": "Think you have solved question answering? try arc, the AI2 reasoning challenge",
            "venue": "CoRR, abs/1803.05457.",
            "year": 2018
        },
        {
            "authors": [
                "Karl Cobbe",
                "Vineet Kosaraju",
                "Mohammad Bavarian",
                "Jacob Hilton",
                "Reiichiro Nakano",
                "Christopher Hesse",
                "John Schulman."
            ],
            "title": "Training verifiers to solve math word problems",
            "venue": "CoRR, abs/2110.14168.",
            "year": 2021
        },
        {
            "authors": [
                "Andrew Drozdov",
                "Nathanael Sch\u00e4rli",
                "Ekin Aky\u00fcrek",
                "Nathan Scales",
                "Xinying Song",
                "Xinyun Chen",
                "Olivier Bousquet",
                "Denny Zhou."
            ],
            "title": "Compositional semantic parsing with large language models",
            "venue": "CoRR, abs/2209.15003.",
            "year": 2022
        },
        {
            "authors": [
                "Luyu Gao",
                "Aman Madaan",
                "Shuyan Zhou",
                "Uri Alon",
                "Pengfei Liu",
                "Yiming Yang",
                "Jamie Callan",
                "Graham Neubig."
            ],
            "title": "PAL: program-aided language models",
            "venue": "CoRR, abs/2211.10435.",
            "year": 2022
        },
        {
            "authors": [
                "Mor Geva",
                "Daniel Khashabi",
                "Elad Segal",
                "Tushar Khot",
                "Dan Roth",
                "Jonathan Berant."
            ],
            "title": "Did aristotle use a laptop? A question answering benchmark with implicit reasoning strategies",
            "venue": "Trans. Assoc. Comput. Linguistics, 9:346\u2013361.",
            "year": 2021
        },
        {
            "authors": [
                "Damoc",
                "Aurelia Guy",
                "Simon Osindero",
                "Karen Simonyan",
                "Erich Elsen",
                "Jack W. Rae",
                "Oriol Vinyals",
                "Laurent Sifre."
            ],
            "title": "Training compute-optimal large language models",
            "venue": "CoRR, abs/2203.15556.",
            "year": 2022
        },
        {
            "authors": [
                "Yigitcan Kaya",
                "Sanghyun Hong",
                "Tudor Dumitras."
            ],
            "title": "Shallow-deep networks: Understanding and mitigating network overthinking",
            "venue": "ICML, volume 97 of Proceedings of Machine Learning Research, pages 3301\u20133310. PMLR.",
            "year": 2019
        },
        {
            "authors": [
                "Yifei Li",
                "Zeqi Lin",
                "Shizhuo Zhang",
                "Qiang Fu",
                "Bei Chen",
                "Jian-Guang Lou",
                "Weizhu Chen."
            ],
            "title": "On the advance of making language models better reasoners",
            "venue": "CoRR, abs/2206.02336.",
            "year": 2022
        },
        {
            "authors": [
                "Wang Ling",
                "Dani Yogatama",
                "Chris Dyer",
                "Phil Blunsom."
            ],
            "title": "Program induction by rationale generation: Learning to solve and explain algebraic word problems",
            "venue": "ACL (1), pages 158\u2013167. Association for Computational Linguistics.",
            "year": 2017
        },
        {
            "authors": [
                "Weijie Liu",
                "Peng Zhou",
                "Zhiruo Wang",
                "Zhe Zhao",
                "Haotang Deng",
                "Qi Ju."
            ],
            "title": "Fastbert: a selfdistilling BERT with adaptive inference time",
            "venue": "ACL, pages 6035\u20136044. Association for Computational Linguistics.",
            "year": 2020
        },
        {
            "authors": [
                "Paul F. Christiano",
                "Jan Leike",
                "Ryan Lowe."
            ],
            "title": "Training language models to follow instructions with human feedback",
            "venue": "CoRR, abs/2203.02155.",
            "year": 2022
        },
        {
            "authors": [
                "Arkil Patel",
                "Satwik Bhattamishra",
                "Navin Goyal."
            ],
            "title": "Are NLP models really able to solve simple math word problems? In NAACL-HLT, pages 2080\u2013 2094",
            "venue": "Association for Computational Linguistics.",
            "year": 2021
        },
        {
            "authors": [
                "Michael Persin."
            ],
            "title": "Document filtering for fast ranking",
            "venue": "SIGIR, pages 339\u2013348. ACM/Springer.",
            "year": 1994
        },
        {
            "authors": [
                "Ayoub",
                "Jeff Stanway",
                "Lorrayne Bennett",
                "Demis Hassabis",
                "Koray Kavukcuoglu",
                "Geoffrey Irving."
            ],
            "title": "Scaling language models: Methods, analysis & insights from training gopher",
            "venue": "CoRR, abs/2112.11446.",
            "year": 2021
        },
        {
            "authors": [
                "Subhro Roy",
                "Dan Roth."
            ],
            "title": "Solving general arithmetic word problems",
            "venue": "EMNLP, pages 1743\u20131752. The Association for Computational Linguistics.",
            "year": 2015
        },
        {
            "authors": [
                "Timo Schick",
                "Jane Dwivedi-Yu",
                "Roberto Dess\u00ec",
                "Roberta Raileanu",
                "Maria Lomeli",
                "Luke Zettlemoyer",
                "Nicola Cancedda",
                "Thomas Scialom."
            ],
            "title": "Toolformer: Language models can teach themselves to use tools",
            "venue": "CoRR, abs/2302.04761.",
            "year": 2023
        },
        {
            "authors": [
                "Roy Schwartz",
                "Gabriel Stanovsky",
                "Swabha Swayamdipta",
                "Jesse Dodge",
                "Noah A. Smith."
            ],
            "title": "The right tool for the job: Matching model and instance complexities",
            "venue": "ACL, pages 6640\u20136651. Association for Computational Linguistics.",
            "year": 2020
        },
        {
            "authors": [
                "Asher Mullokandov",
                "Ashish Sabharwal",
                "Austin Herrick",
                "Avia Efrat",
                "Aykut Erdem",
                "Ayla Karakas"
            ],
            "title": "Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. CoRR, abs/2206.04615",
            "year": 2022
        },
        {
            "authors": [
                "Alon Talmor",
                "Jonathan Herzig",
                "Nicholas Lourie",
                "Jonathan Berant."
            ],
            "title": "Commonsenseqa: A question answering challenge targeting commonsense knowledge",
            "venue": "NAACL-HLT (1), pages 4149\u20134158. Association for Computational Linguistics.",
            "year": 2019
        },
        {
            "authors": [
                "Surat Teerapittayanon",
                "Bradley McDanel",
                "H.T. Kung."
            ],
            "title": "Branchynet: Fast inference via early exiting from deep neural networks",
            "venue": "ICPR, pages 2464\u20132469. IEEE.",
            "year": 2016
        },
        {
            "authors": [
                "Azhar",
                "Aur\u00e9lien Rodriguez",
                "Armand Joulin",
                "Edouard Grave",
                "Guillaume Lample."
            ],
            "title": "Llama: Open and efficient foundation language models",
            "venue": "CoRR, abs/2302.13971.",
            "year": 2023
        },
        {
            "authors": [
                "Paul A. Viola",
                "Michael J. Jones."
            ],
            "title": "Robust realtime face detection",
            "venue": "ICCV, page 747. IEEE Computer Society.",
            "year": 2001
        },
        {
            "authors": [
                "Xuezhi Wang",
                "Jason Wei",
                "Dale Schuurmans",
                "Quoc V. Le",
                "Ed H. Chi",
                "Denny Zhou."
            ],
            "title": "Rationaleaugmented ensembles in language models",
            "venue": "CoRR, abs/2207.00747.",
            "year": 2022
        },
        {
            "authors": [
                "Xuezhi Wang",
                "Jason Wei",
                "Dale Schuurmans",
                "Quoc V. Le",
                "Ed H. Chi",
                "Denny Zhou."
            ],
            "title": "Selfconsistency improves chain of thought reasoning in language models",
            "venue": "CoRR, abs/2203.11171.",
            "year": 2022
        },
        {
            "authors": [
                "Jason Wei",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Maarten Bosma",
                "Ed H. Chi",
                "Quoc Le",
                "Denny Zhou."
            ],
            "title": "Chain of thought prompting elicits reasoning in large language models",
            "venue": "CoRR, abs/2201.11903.",
            "year": 2022
        },
        {
            "authors": [
                "Yixuan Weng",
                "Minjun Zhu",
                "Shizhu He",
                "Kang Liu",
                "Jun Zhao."
            ],
            "title": "Large language models are reasoners with self-verification",
            "venue": "CoRR, abs/2212.09561.",
            "year": 2022
        },
        {
            "authors": [
                "Ji Xin",
                "Raphael Tang",
                "Jaejun Lee",
                "Yaoliang Yu",
                "Jimmy Lin."
            ],
            "title": "Deebert: Dynamic early exiting for accelerating BERT inference",
            "venue": "ACL, pages 2246\u20132251. Association for Computational Linguistics.",
            "year": 2020
        },
        {
            "authors": [
                "Shunyu Yao",
                "Jeffrey Zhao",
                "Dian Yu",
                "Nan Du",
                "Izhak Shafran",
                "Karthik Narasimhan",
                "Yuan Cao."
            ],
            "title": "React: Synergizing reasoning and acting in language models",
            "venue": "CoRR, abs/2210.03629.",
            "year": 2022
        },
        {
            "authors": [
                "Xi Ye",
                "Greg Durrett."
            ],
            "title": "The unreliability of explanations in few-shot in-context learning",
            "venue": "CoRR, abs/2205.03401.",
            "year": 2022
        },
        {
            "authors": [
                "har",
                "Tianlu Wang",
                "Luke Zettlemoyer"
            ],
            "title": "2022a. OPT: open pre-trained transformer language models. CoRR, abs/2205.01068",
            "year": 2022
        },
        {
            "authors": [
                "Zhuosheng Zhang",
                "Aston Zhang",
                "Mu Li",
                "Alex Smola."
            ],
            "title": "Automatic chain of thought prompting in large language models",
            "venue": "CoRR, abs/2210.03493.",
            "year": 2022
        },
        {
            "authors": [
                "Denny Zhou",
                "Nathanael Sch\u00e4rli",
                "Le Hou",
                "Jason Wei",
                "Nathan Scales",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Olivier Bousquet",
                "Quoc Le",
                "Ed H. Chi."
            ],
            "title": "Least-to-most prompting enables complex reasoning in large language models",
            "venue": "CoRR, abs/2205.10625.",
            "year": 2022
        },
        {
            "authors": [
                "Wangchunshu Zhou",
                "Canwen Xu",
                "Tao Ge",
                "Julian J. McAuley",
                "Ke Xu",
                "Furu Wei."
            ],
            "title": "BERT loses patience: Fast and robust inference with early exit",
            "venue": "NeurIPS.",
            "year": 2020
        },
        {
            "authors": [
                "Wang"
            ],
            "title": "E Prompts and Instructions The prompts and instructions used in this paper are shown from Table 8 through Table 13. All of the prompts 1 are rewritten from the prompts",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Prominent large language models (LLMs) like GPT-3 (Brown et al., 2020; Ouyang et al., 2022), Chinchilla (Hoffmann et al., 2022), and PaLM (Chowdhery et al., 2022; Chung et al., 2022) demonstrate exceptional performance in natural language processing tasks. Despite the success, their performance on reasoning-heavy tasks, such as mathematics, common sense, and logical reasoning tasks, remains limited when solely increasing\n\u2217 Corresponding author.\nthe model size (Rae et al., 2021). To address this issue, Wei et al. (2022) propose chain of thoughts (CoT) to guide LLMs to reason step by step, which however suffers from factual errors and hallucinations in reasoning paths (Cobbe et al., 2021; Wang et al., 2022b; Li et al., 2022; Weng et al., 2022; Ye and Durrett, 2022). To mitigate the errors of CoT, researchers propose multi-path voting methods, including self-consistency (Wang et al., 2022b), DIVERSE (Li et al., 2022), and rationale-augmented ensembles (Wang et al., 2022a), which generate multiple reasoning paths and aggregate the paths with voting. However, multiple reasoning paths lead to significant increases in computational resources. For instance, self-consistency generates 40 reasoning paths per question, while DIVERSE employs 100, resulting in tens of times more computational effort.\nThis paper aims to address the following problem: how to achieve comparable accuracy in multipath voting using significantly fewer computational resources? One intuition is that simple problems can be solved with fewer reasoning paths and tilting computational resources to complex problems will effectively improve the overall performance of reasoning. In our empirical investigation on an arithmetic reasoning task, SVAMP (Patel et al., 2021), 57.5% of the problems demonstrate a remarkable level of accuracy, reaching 95.7%, with only three unanimous reasoning paths. And augmenting the reasoning paths on these problems does not significantly improve the accuracy. This shows that generating more paths on half of the problems in SVAMP is a waste of computational resources with negligible benefit. A more detailed analysis of resource wastage can be found in Appendix A.\nInspired by Early Exiting (Viola and Jones, 2001), which prioritizes computational resources to critical features to minimize computational consumption, we propose Dynamic Voting to reduce the number of reasoning paths while maintaining\ncomparable accuracies in reasoning with LLMs. The core idea behind Dynamic Voting is to apply early exiting for problems that the LLMs can confidently solve with a handful of paths, thus avoiding unnecessary generation of numerous paths.\nA key challenge in Dynamic Voting lies in the identification of problems that the LLMs can confidently solve. Self-consistency establishes a robust association between voting consistency and accuracy, which aligns with our finding that a few unanimous paths on SVAMP lead to high accuracy. Thus, we adopt voting consistency as the confidence of the LLMs in solving problems and validate it through a range of experiments.\nDynamic Voting is a simple and effective method that involves multiple rounds of voting with corresponding consistency thresholds. Initially, the LLM generates a few paths and votes on them. If the voting consistency threshold is reached, the voting concludes and the current voting result is outputted. Otherwise, the LLM generates another reasoning path and all the generated paths are used for the subsequent round of voting. This iterative process continues until the threshold is reached or the number of paths reaches a preset maximum. Figure 1 illustrates an example of the process.\nIn practice, we introduce two Dynamic Voting strategies: Confidence-based Dynamic Voting (CDV) and Percentage-based Dynamic Voting (PDV). CDV employs a fixed threshold in each round, offering a straightforward approach. However, it is limited by the need for prior knowledge regarding the probability of correctly solving the problem. For example, when the probability of correct reasoning is 0.5, setting a threshold of 0.9 renders early exiting unlikely. To overcome this limitation, PDV applies early exiting to a fixed proportion of questions that obtain the highest consistency in the current voting round, offering greater flexibility and adaptability in determining when to terminate the voting process. We evaluate these strategies using the Openai GPT-3.5 model1 in fewshot and zero-shot settings. Following Wei et al. (2022) and Wang et al. (2022b), we conduct experiments on Arithmetic Reasoning (GSM8K (Cobbe et al., 2021), SVAMP (Patel et al., 2021)), Commonsense Reasoning (CSQA (Talmor et al., 2019), StrategyQA (Geva et al., 2021)), and Symbolic Reasoning (LetterConcat (Wei et al., 2022)) tasks.\n1We utilize the GPT-3.5-turbo-0301 API (https:// platform.openai.com/docs/models/gpt-3-5) since it is fee friendly and not updated iteratively like GPT-3.5-turbo.\nDynamic Voting achieves comparable accuracies using significantly fewer reasoning paths than selfconsistency2. Across the five evaluated datasets, Dynamic Voting achieves a comparable average accuracy to self-consistency while employing less than 45% of the reasoning paths. Specifically, in the few-shot setting on LetterConcat, CDV outperforms self-consistency using 24.7% of the reasoning paths. Moreover, Dynamic Voting demonstrates substantial accuracy improvements under reduced path constraints. When both are limited to 25% reasoning path usage3, Dynamic Voting achieves an average accuracy gain of 1.6 and 4.7 over self-consistency in the few-shot and zero-shot settings, respectively. Remarkably, PDV improves by 12.7 on LetterConcat in the zero-shot setting. Furthermore, Dynamic Voting attains higher accuracies when employing comparable paths. On GSM8K, CDV and PDV achieve improvements of 1.9 and 1.0, respectively, compared to the highest accuracy of self-consistency in the zero-shot setting with less than 100% reasoning path usage.\nIn addition to its resource efficiency, Dynamic Voting exhibits several excellent features. Dynamic Voting demonstrates stronger robustness of thresholds than self-consistency, providing flexibility in setting appropriate threshold levels. Moreover, Dynamic Voting performs well when combined with other voting techniques, different LLMs, and diverse prompts."
        },
        {
            "heading": "2 Related Works",
            "text": ""
        },
        {
            "heading": "2.1 CoT Reasoning with LLMs",
            "text": "Large language models have achieved remarkable success in natural language processing tasks (Srivastava et al., 2022), even surpassing human-level performance in some cases (Brown et al., 2020; Zhang et al., 2022a; Scao et al., 2022; Ouyang et al., 2022; Hoffmann et al., 2022; Chowdhery et al., 2022; Chung et al., 2022). However, the ability of these models to solve complex reasoning problems does not improve significantly by simply increasing model size (Rae et al., 2021). To address this, Wei et al. (2022) propose chain of thoughts (CoT), which is a sequence of intermediate steps (Ling et al., 2017), to assist the model in reasoning step by step.\n2We consider self-consistency as the basic voting technique due to its computational simplicity.\n3We adopt the same path setting as self-consistency, taking forty paths per question as the maximum (100%).\nSeveral approaches have been proposed to improve the performance of CoT. Some researchers attempt to break down complex problems into subproblems, which are then solved sequentially (Zhou et al., 2022; Drozdov et al., 2022). Others explore the use of additional tools such as code, retrieval, and symbolic languages to provide extra information (Yao et al., 2022; Cheng et al., 2022; Chen et al., 2022; Gao et al., 2022; Schick et al., 2023). A simpler yet effective approach is selfconsistency (Wang et al., 2022b), which generates multiple reasoning paths and yields the final output through majority vote. Another approach involves calibrating the reasoning paths using an additional verifier (Li et al., 2022; Ye and Durrett, 2022). However, calibrating approaches require training additional verifiers and exhibit performance degradation on out-of-distribution problems.\nMulti-path voting is effective, but comes with the drawback of causing a substantial increase in computational resources due to multi-path generation. To address this issue, this paper aims to achieve a comparable accuracy with significantly fewer reasoning paths in reasoning with LLMs."
        },
        {
            "heading": "2.2 Early Exiting",
            "text": "Early Exiting optimizes inference and resource utilization by interrupting computation when adequate results are produced. To accelerate real-time face detection, Viola and Jones (2001) propose to select a few crucial features from a vast number of possible features for further processing. Similarly, Persin (1994) and Cambazoglu et al. (2010) identify potentially highly ranked documents beforehand to improve retrieval efficiency. In addition, some researchers attempt to reduce the number of model layers involved in computation by relying on intermediate layer outputs through entropybased (Teerapittayanon et al., 2016; Xin et al., 2020; Liu et al., 2020), confidence-based (Kaya et al., 2019; Schwartz et al., 2020), and patiencebased (Zhou et al., 2020) indicators. Drawing on these insights, we propose Dynamic Voting that aims to conserve computational resources in multipath voting by performing early exiting to prevent unnecessary path generation. To the best of our knowledge, this is the first work to apply early exiting to voting with LLMs."
        },
        {
            "heading": "3 Dynamic Voting",
            "text": "This section provides a comprehensive exposition of our proposed method, Dynamic Voting. We present an overview of the method in Section 3.1. We then introduce two Dynamic Voting strategies, Confidence-based Dynamic Voting and Percentagebased Dynamic Voting in Section 3.2 and 3.3."
        },
        {
            "heading": "3.1 Overall Process",
            "text": "As stated in Section 1, the consistency of the votes has a strong correlation with the reasoning accuracy. Thus, we estimate the confidence of the LLMs using consistency. How to reasonably quantify voting consistency is one of the crucial factors in the success of Dynamic Voting. In this paper, we simply use the proportion of the majority votes in each voting round as consistency and experimentally demonstrate the effectiveness of this plain scheme.\nFigure 1 illustrates an example of the overall process of Dynamic Voting, which involves multiple rounds of generation and voting progress. We first set the consistency threshold t, the initial number of votes v, and the maximum number of votes V . For a problem p, we use the LLM to generate v reasoning paths and vote on these paths. If the voting consistency reaches t, the voting result is considered the final reasoning output, and the generation and voting process is exited. Otherwise, Dynamic Voting will proceed to the next round where the LLM generates another path and the voting process is carried out on all the v+1 generated paths. This generation and voting process continues until the voting consistency reaches t or until V paths are generated and voted on.\nIt is worth noting that varied thresholds result in different accuracies and computational resource consumption. A loose threshold allows more questions to exit early, which saves more computational resources but reduces accuracy. Conversely, strict thresholds lead to more computational resource consumption and higher accuracy."
        },
        {
            "heading": "3.2 Confidence-based Dynamic Voting",
            "text": "In Confidence-based Dynamic Voting (CDV), a consistency threshold tc is set in each round of voting. If the proportion of the majority votes in the current round reaches the set threshold, the result is considered valid, and early exiting is performed. For example, there are 10 paths in the k-th round of voting and tc = 0.5, the voting ends if the number of paths to an answer reaches 5. Otherwise, the pro-\ncess continues to the next round of path generation and voting.\nIn general, achieving high confidence in later rounds of the Dynamic Voting process is less likely as those questions that the LLM can solve confidently are usually exited in the early stages. An ideal option for designing confidence-based thresholds would be to gradually decrease the threshold value as the number of rounds increases. However, in this paper, we opt for a simple approach of setting a constant threshold value for all rounds to avoid complex fine-grained threshold tuning."
        },
        {
            "heading": "3.3 Percentage-based Dynamic Voting",
            "text": "One challenge in effectively utilizing CDV is the selection of an appropriate threshold based on prior knowledge, as discussed in Section 1. To facilitate a simpler and more efficient application of Dynamic Voting, we propose a strategy that automatically adjusts the consistency threshold when dealing with batch problems, called Percentagebased Dynamic Voting (PDV). In PDV, we set a percentage threshold tp, and then the questions with the top tp% of consistencies in each round will be applied early exiting. The percentage threshold is solely employed to control the proportion of questions exiting early in each round, eliminating the need for any prior knowledge about the LLMs or the questions themselves.\nPDV can handle batch problems quite efficiently as it ensures that a certain percentage of questions always exit in each voting round. The disadvantage of PDV is that it does not work on individual questions well. It is noteworthy that in PDV,\nan increased threshold implies the early exit of a greater number of questions each round, which corresponds to lower accuracy and reduced computational resource consumption."
        },
        {
            "heading": "4 Experiments",
            "text": ""
        },
        {
            "heading": "4.1 Datasets",
            "text": "Following Wei et al. (2022) and Wang et al. (2022b), we conduct experiments on Arithmetic, Commensense, and Symbolic reasoning tasks. It is worth noting that GPT-3.5 with self-consistency is capable of achieving close to 100% accuracy on certain tasks such as MultiArith (Roy and Roth, 2015), ARC (Clark et al., 2018), and CoinFlip (Wei et al., 2022). Consequently, we carefully select several representative datasets that are moderately challenging for GPT-3.5.\nFor the Arithmetic Reasoning task, we utilize GSM8K (Cobbe et al., 2021) and SVAMP (Patel et al., 2021), two challenging sets of math word problems. For Commensense Reasoning, we employ CSQA (Talmor et al., 2019) and StrategyQA (Geva et al., 2021), both of which are considered to be highly challenging question answering benchmarks that require a general understanding of common sense. Finally, for Symbolic Reasoning, we take the LetterConcat task (Wei et al., 2022), in which GPT-3.5 is required to concatenate the last letters of four given words.\nIn our experiments, we use the test split for GSM8K and SVAMP. For CSQA, we use the dev split since the labels of the test set are not available. As for LetterConcat, we construct the test set by\nrandomly generating 1000 examples using the 1000 most commonly used surnames and first names from Name Census4. The settings for the aforementioned four datasets align with those employed by Wang et al. (2022b). Regarding StrategyQA, we randomly sample 1000 questions from the trainfiltered set because the evaluation on the test set necessitates the use of the StrategyQA Leaderboard5. Multiple evaluations on various thresholds are required in this study and frequent submissions are not allowed in the Leaderboard."
        },
        {
            "heading": "4.2 Prompts and Instructions",
            "text": "We conduct experiments on the GPT-3.5 model6 in both few-shot and zero-shot settings. To form the prompts for the few-shot setting, we use the same demonstrations as proposed by Wang et al. (2022b), with minor modifications made to create dialog histories that GPT-3.5 can leverage. In the zero-shot setting, we provide one-sentence instructions that set constraints on the desired word count and the format of the final answer. The complete prompts and instructions used on the five evaluated datasets are provided in Appendix E."
        },
        {
            "heading": "4.3 Hyper Parameters",
            "text": "We compare the performance of Dynamic Voting and Self-consistency under various path usage constraints, including 25%, 50%, 75%, and 100%. Specifically, following Wang et al. (2022b), the maximum path constraint (100%) is defined as voting with forty votes per question.\nFor Self-consistency, constraints result in a reduced number of votes per question. For example, a usage constraint of 25% implies the use of 10 paths per question, while a constraint of 50% corresponds to 20 paths, and so forth. In the case of Dynamic Voting, the initial round commences with 3 votes. The constraints in Dynamic Voting indicate the percentage of used paths on a dataset relative to the maximum path constraint.\nTo determine the appropriate thresholds for Dynamic Voting under different reasoning path usage constraints, we follow the methodology presented by Kaya et al. (2019) and use a validation set comprising 100 randomly selected samples from the training set. In CDV, the threshold search range is\n4https://namecensus.com/. 5https://leaderboard.allenai.org/strategyqa/\nsubmissions/public. 6We utilize the GPT-3.5-turbo-0301 API (https:// platform.openai.com/docs/models/gpt-3-5).\nset between 0.1 and 0.95, with a granularity of 0.05. For PDV, the range is set between 1 and 30, with a granularity of 1. Notably, the threshold searching necessitates only one generation of 40 paths for 100 examples on each dataset, thus the searching computational overhead is trivial. The searched thresholds can be found in Appendix B.\nIn accordance with Wang et al. (2022b), we incorporate temperature sampling to generate diverse reasoning paths. During decoding, we set the temperature to 1.5 without top-k/top-p truncation."
        },
        {
            "heading": "4.4 Experimental Results",
            "text": ""
        },
        {
            "heading": "4.4.1 Fewer Paths for Comparable Accuracy",
            "text": "Table 1 and Figure 2 present the experimental results on various reasoning path usage constraints and the following conclusions can be drawn:\n1) Dynamic Voting achieves comparable accuracies to self-consistency with significantly fewer reasoning paths. In Table 1, in the few-shot and zero-shot settings, Dynamic Voting requires only 25% and 50% of the reasoning path usages, respectively, while maintaining average accuracies on par with self-consistency. In particular, the CDV in the zero-shot setting on StrategyQA and the PDV in the few-shot setting on LetterConcat meet the accuracy of self-consistency using less than 25% of the paths. In Figure 2, Dynamic Voting achieves the highest accuracy of SC using less than 60% of the paths. This highlights the potential of Dynamic Voting to conserve substantial computational resources compared to self-consistency.\n2) Dynamic Voting significantly outperforms self-consistency on all tasks under a limited reasoning path usage. In Table 1, When the reasoning path usage constraint is set to 25%, Dynamic Voting surpasses self-consistency on all five tasks. On average, CDV and PDV exhibit performance improvements of 1.6 and 1.6 in the few-shot setting, and 4.7 and 4.8 in the zero-shot setting, respectively, compared to self-consistency. In Figure 2, Dynamic Voting consistently achieves higher accuracies than SC when the path usage limit is below 80%. These results indicate that Dynamic Voting offers a substantial advantage over self-consistency in scenarios with constrained computational resources.\n3) Dynamic Voting reduces reasoning path usage with negligible performance degradation under extremely strict thresholds. When applying strict thresholds in Dynamic Voting, only a\nsmall number of questions exit early in each voting round. The \u2264 100% results in Table 1 exemplify this situation7. Despite these stringent thresholds, Dynamic Voting achieves a reduction in reasoning path usage with an average accuracy loss of no more than 0.1% in both few-shot and zero-shot settings. In particular, the CDV in the few-shot setting uses only 43.7% of the usage while maintaining high accuracy. These findings emphasize the practical applicability of Dynamic Voting, even if the threshold search phase is omitted and a strict threshold is employed directly."
        },
        {
            "heading": "4.4.2 Higher Accuracy on Comparable Paths",
            "text": "In Dynamic Voting, the preset maximum number of votes V also affects the accuracy. We set varying maximums and analyze the effect of different maximums on Dynamic Voting. The thresholds are set to those corresponding to \u2264 50% in Table 1, with which Dynamic Voting achieves accuracies comparable to self-consistency on most tasks.\nAs shown in Table 2, Dynamic Voting uses more reasoning paths and the accuracies improve as the maximum increases. It is noteworthy that PDV in the zero-shot setting only utilizes additional 3.6%\n7In the experiments, we set tc = 0.95 and tp = 1.\nof paths to achieve a decent improvement of 0.6 on average accuracy when the maximum is increased from 40 to 100. When V is set to 100, the accuracies of Dynamic Voting on several datasets exceed self-consistency with still fewer reasoning paths, such as GSM8K, SVAMP, and CSQA in the zero-shot setting. This demonstrates that the accuracy can be improved by increasing the maximum number of votes in Dynamic Voting and Dynamic Voting typically outperforms self-consistency with comparable reasoning path usage."
        },
        {
            "heading": "4.5 Analyses",
            "text": "To get a better insight into Dynamic Voting, we further analyze the applicability of Dynamic Voting in different application scenarios. The high robustness of thresholds provides high fault tolerance in threshold settings, making the method more user-friendly in practical applications. High generalizability when combined with different voting techniques, LLMs, and prompts demonstrates the reliability and expansibility. The analyses are designed for answering the following questions: Q1: Does Dynamic Voting show strong robustness of thresholds? Q2: Does Dynamic Voting still work with multi-path voting techniques that use Verifiers? Q3: Does Dynamic Voting still work on other large language models? Q4: Does Dynamic Voting still work across different prompts?"
        },
        {
            "heading": "4.5.1 Robustness of Thresholds",
            "text": "As detailed in Section 3.1, the selection of thresholds involves a trade-off between accuracy and resource consumption. However, determining the optimal threshold is a challenging task due to the variable probabilities of correct reasoning by LLMs across different problems. The robustness of threshold settings offers greater flexibility, rendering the approaches more user-friendly and practical.\nTo assess the robustness of threshold settings, we conduct an analysis of the accuracy variations in both self-consistency and Dynamic Voting under different thresholds, and the results are presented in Figure 3. We observe that both self-consistency and Dynamic Voting exhibit a similar trade-off trend: a stricter threshold tends to produce a higher accuracy rate. Notably, the accuracy of Dynamic Voting varies more smoothly with changing thresholds on\nall the five evaluated tasks, suggesting that it possesses superior robustness of threshold compared to self-consistency. The findings are consistent in the zero-shot setting, with further details provided in Appendix D."
        },
        {
            "heading": "4.5.2 Dynamic Voting with Verifier",
            "text": "There are also multi-voting techniques to improve reasoning performance using additional verifiers, such as DIVERSE (Li et al., 2022). We conduct experiments with DIVERSE on the GSM8K dataset to assess the compatibility of Dynamic Voting with additional verifiers. As shown in Table 3, Dynamic Voting demonstrates similar efficiency to DIVERSE, that is, Dynamic Voting outperforms DIVERSE by a wide margin at low reasoning path usage and achieves comparable accuracies as DIVERSE with less than half the reasoning paths. This highlights the strong compatibility of Dynamic Voting when combined with other voting techniques."
        },
        {
            "heading": "4.5.3 Dynamic Voting on Other LLMs",
            "text": "Dynamic Voting is a technique of integration at the output side, with dynamic adjustment of the number of inferences, which can be used logically on all language models. We further experiment with\nOpenai\u2019s code-davinci-0028 API, GPT-49 API, and LLaMA-7B (Touvron et al., 2023) on the GSM8K task in the few-shot setting. The performance of GPT-4 stands out with high accuracy and consistency, achieving an impressive accuracy rate of 95.2% when employing four votes in SC. Moreover, we observe no significant improvement in accuracy when increasing the number of votes from four to ten. Consequently, we consider four votes in the SC method as 100% path usage for our experiments with GPT-4, while 25% path usage indicates a single answer without voting. When conducting experiments with Code-davinci-002 and LLaMA7B, we maintain the 100% path usage as forty votes. The results of the experiments are presented in Table 4. Dynamic Voting outperforms selfconsistency significantly at low reasoning paths usage and achieves comparable accuracies to the best performance of self-consistency using fewer reasoning paths, which demonstrates the generalizability of Dynamic Voting across different models. It is noteworthy that achieving a target accuracy of 95% using GPT-4 incurs a cost exceeding $40 when employing SC, whereas the cost is less than $24 when utilizing Dynamic Voting."
        },
        {
            "heading": "4.5.4 Dynamic Voting Across Varied Prompts",
            "text": "Different prompts can lead to varying reasoning accuracies in large language models (Wang et al., 2022b,a; Zhang et al., 2022b). In this analysis, we aim to investigate the efficiency of Dynamic Voting when employed with different prompts. We compare the accuracies of Dynamic Voting and self-consistency on the GSM8K dataset at various paths usage levels, using another two distinct prompts, one generated by ChatGPT10 (Prompt 2) and one written by the authors (Prompt 3). The whole prompts used can be found in Appendix E.\nAs shown in Table 5, Dynamic Voting con-\n8https://platform.openai.com/docs/models/ gpt-3-5.\n9https://platform.openai.com/docs/models/ gpt-4.\n10https://chat.openai.com.\nsistently achieves higher accuracies than selfconsistency when the reasoning paths usage is below 50%. Moreover, Dynamic Voting always achieves accuracies comparable to self-consistency with only about 50% of the reasoning paths, regardless of the prompt used. These findings highlight the robustness of Dynamic Voting in maintaining high performance across diverse prompts."
        },
        {
            "heading": "5 Conclusion",
            "text": "To address the problem of computational resource wastage caused by multi-path voting using large language models, this paper proposes Dynamic Voting and designs Confidence-based and Percentagebased Dynamic Voting methods, which achieve comparable performance to self-consistency using significantly fewer reasoning paths. We demonstrate the effectiveness and efficiency of Dynamic Voting on Arithmetic Reasoning (GSM8K, SVAMP), Commensense Reasoning (CSQA, StaategyQA), and Symbolic Reasoning (LetterConcat) tasks using the GPT-3.5 model in few-shot and zero-shot settings. Dynamic Voting achieves comparable accuracies to self-consistency on all the five evaluated tasks with less than half of the reasoning paths. Moreover, Dynamic Voting performs more robustly for the selection of thresholds than selfconsistency and demonstrates its generalizability combined with other voting techniques, different LLMs, and varied prompts. Our study provides valuable insights for developing efficient and effective reasoning methods with large language models. Future research can explore more sophisticated Dynamic Voting methods or explore the application of Dynamic Voting in other learning scenarios.\nLimitations\nDynamic Voting is an effective approach that can significantly reduce computational resource consumption when using large language models (LLMs) for multi-path voting to solve reasoning problems. Moreover, it has shown promising performance in both the few-shot and zero-shot settings. Nevertheless, we acknowledge five limitations of this approach.\nFirstly, Dynamic Voting cannot calibrate the wrong answer to a problem that LLMs would not be able to solve. LLMs may stubbornly assume that certain incorrect facts are correct and repeatedly generate these in their reasoning. In such cases, Dynamic Voting can only end the path generation\nprocess early without calibrating the answer to be correct.\nSecondly, Confidence-based Dynamic Voting requires some prior knowledge of the likelihood of the LLMs solving the problem. An inappropriate threshold setting can still invalidate Dynamic Voting, for instance, if the model has a probability of generating a correct answer below 0.1 when faced with a difficult problem and we set the threshold to 0.9, this can degrade Dynamic Voting into selfconsistency.\nThirdly, Percentage-based Dynamic Voting is not adept at handling individual questions, which presents a challenge for deploying applications that handle single instance requests in real time.\nFourthly, due to the inaccessibility of other large language models, Dynamic Voting has been tested only on the GPT series models and LLaMA-7B. Yet, this limitation is currently widespread in works that explore LLMs.\nLastly, Dynamic Voting has only been tested on English reasoning tasks with specific answers and is not suited to natural language generation tasks such as summarization and translation. However, it is essential to note that this limitation is not unique to Dynamic Voting but is inherent in all current multi-path voting methods.\nIn summary, while Dynamic Voting is a valuable approach for reducing computational resource consumption in multi-path voting, it is crucial to consider these limitations carefully to optimize its performance and applicability in real-world scenarios. Subsequent efforts will center on ameliorating these limitations and extending the scope of Dynamic Voting scenarios.\nEthics Statement\nWe hereby affirm that all co-authors of this paper are cognizant of and uphold the ACL Code of Ethics. In this study, we present an effective approach that significantly curtails resource consumption while using large models for multi-path voting to resolve reasoning problems. Our contribution concentrates on the methodology rather than the development of data and language models, which does not raise any ethical issues. Nonetheless, the publicly accessible datasets and pre-trained models employed in this research may harbor negative effects, such as gender and religious bias. As a result, we urge other researchers to exercise caution when employing our methods and these data."
        },
        {
            "heading": "Acknowledgments",
            "text": "This work is supported by the Fundamental Research Funds for the Central Universities under Grant 1082204112364 and the Key Program of the National Science Foundation of China under Grant 61836006."
        },
        {
            "heading": "A Resource Wastage in Multi-path Voting",
            "text": "Multi-path voting methods like self-consistency emerge as potent techniques for enhancing the reasoning accuracy of large language models. However, the drawback of these methods lies in the inefficient utilization of computational resources due to the generation of an excessive number of reasoning paths for each problem.\nWe extensively explore the computational resource wastage in multi-path voting, using selfconsistency on GPT-3.5 across five distinct datasets as an example. When voting with 3 paths, we find that there are 39.4%, 57.5%, 47.2%, 33.8%, and 59.0% of the questions on the GSM8K, SVAMP, CSQA, StrategyQA, and LetterConcat datasets obtain unanimous votes, respectively. Then we increase the number of paths for voting on the questions receiving unanimous votes, and the results are presented in Table 6. It becomes evident that augmenting the number of paths does not yield a significant enhancement in accuracy for these questions. In particular, a degradation in accuracy can be observed on StrategyQA. This observation implies that allocating substantial inference computational resources to nearly half of the problems in the datasets is redundant, as an investment of over ten times the computational resources results in less than 1% improvement in accuracy. To mitigate the issue, Dynamic Voting applies early exiting for problems that demonstrate high consistency in voting, thereby curbing unnecessary computations."
        },
        {
            "heading": "B Searched Thresholds",
            "text": "The searched thresholds are shown in Table 7."
        },
        {
            "heading": "C Results under Different Path Usages",
            "text": "The experimental results under different path usages are shown in Figure 5. It is clear that Dynamic Voting consistently achieves higher accuracies than self-consistency on all the five evaluated datasets when paths are limited to under 50%. Except for\nthe few-shot setting on StrategyQA and LetterConcat, Dynamic Voting outperforms self-consistency using fewer paths."
        },
        {
            "heading": "D Zero-shot Results with Different Thresholds",
            "text": "The experimental results with different thresholds in the zero-shot setting are shown in Figure 4. The results in the zero-shot setting lead to similar conclusions as those from the results in the few-shot setting in Figure 3. That is, Dynamic Voting exhibits greater robustness of thresholds compared to self-consistency."
        },
        {
            "heading": "E Prompts and Instructions",
            "text": "The prompts and instructions used in this paper are shown from Table 8 through Table 13. All of the prompts 1 are rewritten from the prompts in Wang et al. (2022b) into dialogue form. On GSM8K, prompt 2 is generated by ChatGPT11, and prompt 3 is written by the authors of this paper. All experiments in this paper are done based on prompts 1 except for the analysis on prompts in Section 4.5.4.\n11https://chat.openai.com.\nGSM8K / SVAMP Prompt 1 - Fewshot Setting:\nsystem: You are a helpful assistant good at mathematical reasoning.\nuser: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done,\nthere will be 21 trees. How many trees did the grove workers plant today?\nassistant: We start with 15 trees. Later we have 21 trees. The difference must be the number of trees they\nplanted. So, they must have planted 21 - 15 = 6 trees. The answer is 6.\nuser: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\nassistant: There are 3 cars in the parking lot already. 2 more arrive. Now there are 3 + 2 = 5 cars. The\nanswer is 5.\nuser: Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\nassistant: Leah had 32 chocolates and Leah's sister had 42. That means there were originally 32 + 42 = 74\nchocolates. 35 have been eaten. So in total they still have 74 - 35 = 39 chocolates. The answer is 39.\nuser: Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many\nlollipops did Jason give to Denny?\nassistant: Jason had 20 lollipops. Since he only has 12 now, he must have given the rest to Denny. The\nnumber of lollipops he has given to Denny must have been 20 - 12 = 8 lollipops. The answer is 8.\nuser: Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does\nhe have now?\nassistant: He has 5 toys. He got 2 from mom, so after that he has 5 + 2 = 7 toys. Then he got 2 more from\ndad, so in total he has 7 + 2 = 9 toys. The answer is 9.\nuser: There were nine computers in the server room. Five more computers were installed each day, from\nmonday to thursday. How many computers are now in the server room?\nassistant: There are 4 days from monday to thursday. 5 computers were added each day. That means in total"
        },
        {
            "heading": "4 * 5 = 20 computers were added. There were 9 computers in the beginning, so now there are 9 + 20 = 29 computers. The answer is 29.",
            "text": "user: Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many\ngolf balls did he have at the end of wednesday?\nassistant: Michael initially had 58 balls. He lost 23 on Tuesday, so after that he has 58 - 23 = 35 balls. On\nWednesday he lost 2 more so now he has 35 - 2 = 33 balls. The answer is 33.\nuser: Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\nassistant: She bought 5 bagels for $3 each. This means she spent 5 * $3 = $15 on the bagels. She had $23 in\nbeginning, so now she has $23 - $15 = $8. The answer is 8.\nuser: [question]\nGSM8K / SVAMP Instruction - Zeroshot Setting:\nsystem: You are a helpful assistant good at mathematical reasoning.\nuser: [question] Solve this problem in 150 words and end with 'the answer is x.'\nTable 8: The prompt and instruction used on GSM8K and SVAMP. The notation [question] refers to the question that is being evaluated.\nGSM8K / SVAMP Prompt 2:\nsystem: You are a helpful assistant good at mathematical reasoning.\nuser: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done,\nthere will be 21 trees. How many trees did the grove workers plant today?\nassistant: To find out how many trees the grove workers planted, we need to subtract the initial number of\ntrees from the final number of trees: Number of trees planted = final number of trees - initial number of trees\n= 21 - 15 = 6. Therefore, the grove workers planted 6 trees today. The answer is 6.\nuser: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\nassistant: If there are 3 cars in the parking lot and 2 more cars arrive, the total number of cars in the parking\nlot is: 3 + 2 = 5. Therefore, there are 5 cars in the parking lot. The answer is 5.\nuser: Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\nassistant: Leah had 32 chocolates and her sister had 42, so they had a total of: 32 + 42 = 74 chocolates. If\nthey ate 35 chocolates, the number of chocolates they have left is: 74 - 35 = 39 chocolates. Therefore, they\nhave 39 chocolates left in total. The answer is 39.\nuser: Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many\nlollipops did Jason give to Denny?\nassistant: To find out how many lollipops Jason gave to Denny, we need to subtract the final number of\nlollipops Jason has from the initial number of lollipops he had: Number of lollipops given to Denny = initial\nnumber of lollipops - final number of lollipops = 20 - 12 = 8. Therefore, Jason gave 8 lollipops to Denny.\nThe answer is 8.\nuser: Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does\nhe have now?\nassistant: Shawn has five toys initially. For Christmas, he got two toys each from his mom and dad, so he\ngot a total of: 2 (toys from mom) + 2 (toys from dad) = 4 toys. Adding the four toys from Christmas to his\ninitial five toys, we get: 5 + 4 = 9. Therefore, Shawn has 9 toys now. The answer is 9.\nuser: There were nine computers in the server room. Five more computers were installed each day, from\nmonday to thursday. How many computers are now in the server room?\nassistant: If there were 9 computers to start with and 5 more were added each day from Monday to Thursday,\nthat means 4 days have passed since Monday to Thursday. So, the number of additional computers added\nwould be: 5 + 5 + 5 + 5 = 20. Therefore, the total number of computers in the server room now would be: 9\n+ 20 = 29. Hence, there are 29 computers in the server room now. The answer is 29.\nuser: Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many\ngolf balls did he have at the end of wednesday?\nassistant: Michael had 58 golf balls. On Tuesday, he lost 23 golf balls, which leaves him with: 58 - 23 = 35\ngolf balls. On Wednesday, he lost 2 more golf balls, which leaves him with: 35 - 2 = 33 golf balls. Therefore,\nMichael had 33 golf balls at the end of Wednesday. The answer is 33.\nuser: Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\nassistant: Olivia had $23 to start with. She bought 5 bagels at $3 each, which is a total cost of: 5 x $3 = $15.\nTherefore, after buying the bagels, Olivia has: $23 - $15 = $8 left. So, Olivia has $8 left after buying bagels.\nThe answer is 8.\nuser: [question]\nTable 9: The prompt 2 used on GSM8K and SVAMP.\nGSM8K / SVAMP Prompt 3:\nsystem: You are a helpful assistant good at mathematical reasoning.\nuser: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done,\nthere will be 21 trees. How many trees did the grove workers plant today?\nassistant: Step 1: We start with 15 trees. Later we have 21 trees. The difference must be the number of trees\nthey planted. So, they must have planted 21 - 15 = 6 trees. Step 2: The answer is 6.\nuser: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\nassistant: Step 1: There are 3 cars in the parking lot already. 2 more arrive. Now there are 3 + 2 = 5 cars.\nStep 2: The answer is 5.\nuser: Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\nassistant: Step 1: Leah had 32 chocolates and Leah's sister had 42. That means there were originally 32 + 42\n= 74 chocolates. Step 2: 35 have been eaten. So in total they still have 74 - 35 = 39 chocolates. Step 3: The\nanswer is 39.\nuser: Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many\nlollipops did Jason give to Denny?\nassistant: Step 1: Jason had 20 lollipops. Since he only has 12 now, he must have given the rest to Denny.\nThe number of lollipops he has given to Denny must have been 20 - 12 = 8 lollipops. Step 2: The answer is 8.\nuser: Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does\nhe have now?\nassistant: Step 1: He has 5 toys. He got 2 from mom, so after that he has 5 + 2 = 7 toys. Step 2: Then he got"
        },
        {
            "heading": "2 more from dad, so in total he has 7 + 2 = 9 toys. Step 3: The answer is 9.",
            "text": "user: There were nine computers in the server room. Five more computers were installed each day, from\nmonday to thursday. How many computers are now in the server room?\nassistant: Step 1: There are 4 days from monday to thursday. 5 computers were added each day. That means\nin total 4 * 5 = 20 computers were added. Step 2: There were 9 computers in the beginning, so now there are"
        },
        {
            "heading": "9 + 20 = 29 computers. Step 3: The answer is 29.",
            "text": "user: Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many\ngolf balls did he have at the end of wednesday?\nassistant: Step 1: Michael initially had 58 balls. He lost 23 on Tuesday, so after that he has 58 - 23 = 35 balls.\nStep 2: On Wednesday he lost 2 more so now he has 35 - 2 = 33 balls. Step 3: The answer is 33.\nuser: Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\nassistant: Step 1: She bought 5 bagels for $3 each. This means she spent 5 * $3 = $15 on the bagels. Step 2:\nShe had $23 in beginning, so now she has $23 - $15 = $8. Step 3: The answer is 8.\nCSQA Prompt 1 - Fewshot Setting:\nsystem: You are a helpful assistant good at commonsense reasoning.\nuser: What do people use to absorb extra ink from a fountain pen?\nAnswer Choices: (a) shirt pocket (b) calligrapher's hand (c) inkwell (d) desk drawer (e) blotter\nassistant: The answer must be an item that can absorb ink. Of the above choices, only blotters are used to\nabsorb ink. So the answer is (e).\nuser: What home entertainment equipment requires cable?\nAnswer Choices: (a) radio shack (b) substation (c) television (d) cabinet\nassistant: The answer must require cable. Of the above choices, only television requires cable. So the\nanswer is (c).\nuser: The fox walked from the city into the forest, what was it looking for?\nAnswer Choices: (a) pretty flowers (b) hen house (c) natural habitat (d) storybook\nassistant: The answer must be something in the forest. Of the above choices, only natural habitat is in the\nforest. So the answer is (c).\nuser: Sammy wanted to go to where the people were. Where might he go?\nAnswer Choices: (a) populated areas (b) race track (c) desert (d) apartment (e) roadblock\nassistant: The answer must be a place with a lot of people. Of the above choices, only populated areas have\na lot of people. So the answer is (a).\nuser: Where do you put your grapes just before checking out?\nAnswer Choices: (a) mouth (b) grocery cart (c) super market (d) fruit basket (e) fruit market\nassistant: The answer should be the place where grocery items are placed before checking out. Of the above\nchoices, grocery cart makes the most sense for holding grocery items. So the answer is (b).\nuser: Google Maps and other highway and street GPS services have replaced what?\nAnswer Choices: (a) united states (b) mexico (c) countryside (d) atlas\nassistant: The answer must be something that used to do what Google Maps and GPS services do, which is\nto give directions. Of the above choices, only atlases are used to give directions. So the answer is (d).\nuser: Before getting a divorce, what did the wife feel who was doing all the work?\nAnswer Choices: (a) harder (b) anguish (c) bitterness (d) tears (e) sadness\nassistant: The answer should be the feeling of someone getting divorced who was doing all the work. Of the\nabove choices, the closest feeling is bitterness. So the answer is (c).\nuser: [question]\nCSQA Instruction - Zeroshot Setting:\nsystem: You are a helpful assistant good at commonsense reasoning.\nuser: [question] Solve this problem in 80 words and end with 'the answer is x.'\nTable 11: The prompt and instruction used on CSQA.\nStrategyQA Prompt 1 - Fewshot Setting:\nsystem: You are a helpful assistant good at commonsense reasoning.\nuser: Yes or no: Do hamsters provide food for any animals?\nassistant: Hamsters are prey animals. Prey are food for predators. Thus, hamsters provide food for some\nanimals. So the answer is yes.\nuser: Yes or no: Could Brooke Shields succeed at University of Pennsylvania?\nassistant: Brooke Shields went to Princeton University. Princeton University is about as academically\nrigorous as the University of Pennsylvania. Thus, Brooke Shields could also succeed at the University of\nPennsylvania. So the answer is yes.\nuser: Yes or no: Hydrogen\u2019s atomic number squared exceeds number of Spice Girls?\nassistant: Hydrogen has an atomic number of 1. 1 squared is 1. There are 5 Spice Girls. Thus, Hydrogen\u2019s\natomic number squared is less than 5. So the answer is no.\nuser: Yes or no: Is it common to see frost during some college commencements?\nassistant: College commencement ceremonies can happen in December, May, and June. December is in the\nwinter, so there can be frost. Thus, there could be frost at some commencements. So the answer is yes.\nuser: Yes or no: Could a llama birth twice during War in Vietnam (1945-46)?\nassistant: The War in Vietnam was 6 months. The gestation period for a llama is 11 months, which is more\nthan 6 months. Thus, a llama could not give birth twice during the War in Vietnam. So the answer is no.\nuser: Yes or no: Would a pear sink in water?\nassistant: The density of a pear is about 0.6g/cm^3, which is less than water. Objects less dense than water\nfloat. Thus, a pear would float. So the answer is no.\nuser: [question]\nStrategyQA Instruction - Zeroshot Setting:\nsystem: You are a helpful assistant good at commonsense reasoning.\nuser: [question] Solve this problem in 80 words and end with 'the answer is x.'\nTable 12: The prompt and instruction used on StrategyQA.\nLetterConcat Prompt 1 - Fewshot Setting:\nsystem: You are a helpful assistant good at symbolic reasoning.\nuser: Take the last letters of the words in \"Elon Musk\" and concatenate them.\nassistant: The last letter of \"Elon\" is \"n\". The last letter of \"Musk\" is \"k\". Concatenating them is \"nk\". The\nanswer is nk.\nuser: Take the last letters of the words in \"Larry Page\" and concatenate them.\nassistant: The last letter of \"Larry\" is \"y\". The last letter of \"Page\" is \"e\". Concatenating them is \"ye\". The\nanswer is ye.\nuser: Take the last letters of the words in \"Sergey Brin\" and concatenate them.\nassistant: The last letter of \"Sergey\" is \"y\". The last letter of \"Brin\" is \"n\". Concatenating them is \"yn\". The\nanswer is yn.\nuser: Take the last letters of the words in \"Bill Gates\" and concatenate them.\nassistant: The last letter of \"Bill\" is \"l\". The last letter of \"Gates\" is \"s\". Concatenating them is \"ls\". The\nanswer is ls.\nuser: [question]\nLetterConcat Instruction - Zeroshot Setting:\nsystem: You are a helpful assistant good at symbolic reasoning.\nuser: [question] Solve this problem in 80 words and end with 'the answer is x.'\nTable 13: The prompt and instruction used on LetterConcat."
        }
    ],
    "title": "Dynamic Voting for Efficient Reasoning in Large Language Models",
    "year": 2023
}