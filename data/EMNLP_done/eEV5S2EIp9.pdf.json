{
    "abstractText": "Slot labeling (SL) is a core component of taskoriented dialogue (TOD) systems, where slots and corresponding values are usually language-, taskand domain-specific. Therefore, extending the system to any new language-domaintask configuration requires (re)running an expensive and resource-intensive data annotation process. To mitigate the inherent data scarcity issue, current research on multilingual ToD assumes that sufficient English-language annotated data are always available for particular tasks and domains, and thus operates in a standard cross-lingual transfer setup. In this work, we depart from this often unrealistic assumption. We examine challenging scenarios where such transfer-enabling English annotated data cannot be guaranteed, and focus on bootstrapping multilingual data-efficient slot labelers in transfer-free scenarios directly in the target languages without any English-ready data. We propose a two-stage slot labeling approach (termed TWOSL) which transforms standard multilingual sentence encoders into effective slot labelers. In Stage 1, relying on SL-adapted contrastive learning with only a handful of SLannotated examples, we turn sentence encoders into task-specific span encoders. In Stage 2, we recast SL from a token classification into a simpler, less data-intensive span classification task. Our results on two standard multilingual TOD datasets and across diverse languages confirm the effectiveness and robustness of TWOSL. It is especially effective for the most challenging transfer-free few-shot setups, paving the way for quick and data-efficient bootstrapping of multilingual slot labelers for TOD.",
    "authors": [
        {
            "affiliations": [],
            "name": "Evgeniia Razumovskaia"
        },
        {
            "affiliations": [],
            "name": "Ivan Vuli\u0107"
        },
        {
            "affiliations": [],
            "name": "Anna Korhonen"
        }
    ],
    "id": "SP:106ca3a11656371638e130ebffff7fba2b94de4f",
    "references": [
        {
            "authors": [
                "vaoghene Ahia",
                "Bonaventure F.P. Dossou",
                "Kelechi Ogueji",
                "Thierno Ibrahima DIOP",
                "Abdoulaye Diallo",
                "Adewale Akinfaderin",
                "Tendai Marengereke",
                "Salomey Osei"
            ],
            "title": "MasakhaNER: Named entity recognition for African languages",
            "year": 2021
        },
        {
            "authors": [
                "Inigo Casanueva",
                "Ivan Vuli\u0107",
                "Georgios Spithourakis",
                "Pawe\u0142 Budzianowski."
            ],
            "title": "NLU++: A multilabel, slot-rich, generalisable dataset for natural language understanding in task-oriented dialogue",
            "venue": "Findings of the Association for Computational Lin-",
            "year": 2022
        },
        {
            "authors": [
                "Oralie Cattan",
                "Sophie Rosset",
                "Christophe Servan."
            ],
            "title": "On the cross-lingual transferability of multilingual prototypical models across NLU tasks",
            "venue": "Proceedings of the 1st Workshop on Meta Learning and Its Applications to Natural Language Processing,",
            "year": 2021
        },
        {
            "authors": [
                "Beiduo Chen",
                "Wu Guo",
                "Bin Gu",
                "Quan Liu",
                "Yongchao Wang."
            ],
            "title": "Multi-level contrastive learning for cross-lingual alignment",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2022, Virtual and Singapore,",
            "year": 2022
        },
        {
            "authors": [
                "Sumit Chopra",
                "Raia Hadsell",
                "Yann LeCun."
            ],
            "title": "Learning a similarity metric discriminatively, with application to face verification",
            "venue": "2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR\u201905), volume 1, pages",
            "year": 2005
        },
        {
            "authors": [
                "Kristina Toutanova"
            ],
            "title": "2019. BERT: Pre-training",
            "year": 2019
        },
        {
            "authors": [
                "Milan Gritta",
                "Ruoyu Hu",
                "Ignacio Iacobacci."
            ],
            "title": "CrossAligner & co: Zero-shot transfer methods for task-oriented cross-lingual natural language understanding",
            "venue": "Findings of the Association for Computational Linguistics: ACL 2022, pages 4048\u20134061,",
            "year": 2022
        },
        {
            "authors": [
                "Milan Gritta",
                "Ignacio Iacobacci."
            ],
            "title": "XeroAlign: Zero-shot cross-lingual transformer alignment",
            "venue": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 371\u2013381, Online. Association for Computational Linguistics.",
            "year": 2021
        },
        {
            "authors": [
                "Matthew Henderson",
                "Ivan Vuli\u0107."
            ],
            "title": "ConVEx: Data-efficient and few-shot slot labeling",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages",
            "year": 2021
        },
        {
            "authors": [
                "Pratik Joshi",
                "Sebastin Santy",
                "Amar Budhiraja",
                "Kalika Bali",
                "Monojit Choudhury."
            ],
            "title": "The state and fate of linguistic diversity and inclusion in the NLP world",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages",
            "year": 2020
        },
        {
            "authors": [
                "Diederik P. Kingma",
                "Jimmy Ba."
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings.",
            "year": 2015
        },
        {
            "authors": [
                "Jitin Krishnan",
                "Antonios Anastasopoulos",
                "Hemant Purohit",
                "Huzefa Rangwala."
            ],
            "title": "Multilingual codeswitching for zero-shot cross-lingual intent prediction and slot filling",
            "venue": "Proceedings of the 1st Workshop on Multilingual Representation Learning, pages",
            "year": 2021
        },
        {
            "authors": [
                "Jason Krone",
                "Yi Zhang",
                "Mona Diab."
            ],
            "title": "Learning to classify intents and slot labels given a handful of examples",
            "venue": "Proceedings of the 2nd Workshop on Natural Language Processing for Conversational AI, pages 96\u2013108, Online. Association for Computa-",
            "year": 2020
        },
        {
            "authors": [
                "Anne Lauscher",
                "Vinit Ravishankar",
                "Ivan Vuli\u0107",
                "Goran Glava\u0161."
            ],
            "title": "From zero to hero: On the limitations of zero-shot language transfer with multilingual Transformers",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Lan-",
            "year": 2020
        },
        {
            "authors": [
                "Shining Liang",
                "Linjun Shou",
                "Jian Pei",
                "Ming Gong",
                "Wanli Zuo",
                "Xianglin Zuo",
                "Daxin Jiang."
            ],
            "title": "Multi-level contrastive learning for cross-lingual spoken language understanding",
            "venue": "ArXiv preprint, abs/2205.03656.",
            "year": 2022
        },
        {
            "authors": [
                "Fangyu Liu",
                "Ivan Vuli\u0107",
                "Anna Korhonen",
                "Nigel Collier."
            ],
            "title": "Fast, effective, and self-supervised: Transforming masked language models into universal lexical and sentence encoders",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural",
            "year": 2021
        },
        {
            "authors": [
                "Zihan Liu",
                "Jamin Shin",
                "Yan Xu",
                "Genta Indra Winata",
                "Peng Xu",
                "Andrea Madotto",
                "Pascale Fung."
            ],
            "title": "Zero-shot cross-lingual dialogue systems with transferable latent variables",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natu-",
            "year": 2019
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter."
            ],
            "title": "Decoupled weight decay regularization",
            "venue": "7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net.",
            "year": 2019
        },
        {
            "authors": [
                "Olga Majewska",
                "Evgeniia Razumovskaia",
                "Edoardo Maria Ponti",
                "Ivan Vulic",
                "Anna Korhonen."
            ],
            "title": "Cross-lingual dialogue dataset creation via outline-based generation",
            "venue": "ArXiv preprint, abs/2201.13405.",
            "year": 2022
        },
        {
            "authors": [
                "Shikib Mehri",
                "Mihail Eric."
            ],
            "title": "Example-driven intent prediction with observers",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2979\u20132992,",
            "year": 2021
        },
        {
            "authors": [
                "Zaiqiao Meng",
                "Fangyu Liu",
                "Ehsan Shareghi",
                "Yixuan Su",
                "Charlotte Collins",
                "Nigel Collier."
            ],
            "title": "Rewirethen-probe: A contrastive recipe for probing biomedical knowledge of pre-trained language models",
            "venue": "Proceedings of the 60th Annual Meeting of the As-",
            "year": 2022
        },
        {
            "authors": [
                "Mahdi Namazifar",
                "Alexandros Papangelis",
                "Gokhan Tur",
                "Dilek Hakkani-T\u00fcr."
            ],
            "title": "Language model is all you need: Natural language understanding as question answering",
            "venue": "ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal",
            "year": 2021
        },
        {
            "authors": [
                "Jonas Pfeiffer",
                "Sebastian Ruder",
                "Ivan Vuli\u0107",
                "Edoardo Maria Ponti."
            ],
            "title": "Modular deep learning",
            "venue": "CoRR, abs/2302.11529.",
            "year": 2023
        },
        {
            "authors": [
                "Edoardo Maria Ponti",
                "Julia Kreutzer",
                "Ivan Vulic",
                "Siva Reddy."
            ],
            "title": "Modelling latent translations for cross-lingual transfer",
            "venue": "ArXiv preprint, abs/2107.11353.",
            "year": 2021
        },
        {
            "authors": [
                "Libo Qin",
                "Minheng Ni",
                "Yue Zhang",
                "Wanxiang Che."
            ],
            "title": "Cosda-ml: Multi-lingual code-switching data augmentation for zero-shot cross-lingual NLP",
            "venue": "Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI 2020,",
            "year": 2020
        },
        {
            "authors": [
                "Evgeniia Razumovskaia",
                "Goran Glavas",
                "Olga Majewska",
                "Edoardo M Ponti",
                "Anna Korhonen",
                "Ivan Vulic."
            ],
            "title": "Crossing the conversational chasm: A primer on natural language processing for multilingual task-oriented dialogue systems",
            "venue": "Journal of",
            "year": 2022
        },
        {
            "authors": [
                "Evgeniia Razumovskaia",
                "Ivan Vuli\u0107",
                "Anna Korhonen."
            ],
            "title": "Data augmentation and learned layer aggregation for improved multilingual language understanding in dialogue",
            "venue": "Findings of the Association for Computational Linguistics: ACL 2022,",
            "year": 2022
        },
        {
            "authors": [
                "Nils Reimers",
                "Iryna Gurevych."
            ],
            "title": "SentenceBERT: Sentence embeddings using Siamese BERTnetworks",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natu-",
            "year": 2019
        },
        {
            "authors": [
                "Nils Reimers",
                "Iryna Gurevych."
            ],
            "title": "Making monolingual sentence embeddings multilingual using knowledge distillation",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 4512\u20134525,",
            "year": 2020
        },
        {
            "authors": [
                "Nils Rethmeier",
                "Isabelle Augenstein."
            ],
            "title": "A primer on contrastive pretraining in language processing: Methods, lessons learned & perspectives",
            "venue": "ACM Computing Surveys (CSUR).",
            "year": 2021
        },
        {
            "authors": [
                "Peter J Rousseeuw."
            ],
            "title": "Silhouettes: a graphical aid to the interpretation and validation of cluster analysis",
            "venue": "Journal of computational and applied mathematics, 20:53\u201365.",
            "year": 1987
        },
        {
            "authors": [
                "Sebastian Ruder",
                "Ivan Vuli\u0107",
                "Anders S\u00f8gaard."
            ],
            "title": "Square one bias in NLP: Towards a multidimensional exploration of the research manifold",
            "venue": "Findings of the Association for Computational Linguistics: ACL 2022, pages 2340\u20132354, Dublin,",
            "year": 2022
        },
        {
            "authors": [
                "Sebastian Schuster",
                "Sonal Gupta",
                "Rushin Shah",
                "Mike Lewis."
            ],
            "title": "Cross-lingual transfer learning for multilingual task oriented dialog",
            "venue": "Proceedings",
            "year": 2019
        },
        {
            "authors": [
                "Kaitao Song",
                "Xu Tan",
                "Tao Qin",
                "Jianfeng Lu",
                "TieYan Liu."
            ],
            "title": "Mpnet: Masked and permuted pretraining for language understanding",
            "venue": "Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing",
            "year": 2020
        },
        {
            "authors": [
                "Yixuan Su",
                "Fangyu Liu",
                "Zaiqiao Meng",
                "Tian Lan",
                "Lei Shu",
                "Ehsan Shareghi",
                "Nigel Collier."
            ],
            "title": "TaCL: Improving BERT pre-training with token-aware contrastive learning",
            "venue": "Findings of the Association for Computational Linguistics: NAACL 2022, pages",
            "year": 2022
        },
        {
            "authors": [
                "Yusheng Su",
                "Xu Han",
                "Yankai Lin",
                "Zhengyan Zhang",
                "Zhiyuan Liu",
                "Peng Li",
                "Jie Zhou",
                "Maosong Sun."
            ],
            "title": "Css-lm: A contrastive framework for semisupervised fine-tuning of pre-trained language models",
            "venue": "IEEE/ACM Transactions on Audio, Speech, and",
            "year": 2021
        },
        {
            "authors": [
                "Gokhan Tur",
                "Renato De Mori."
            ],
            "title": "Spoken language understanding: Systems for extracting semantic information from speech",
            "venue": "John Wiley & Sons.",
            "year": 2011
        },
        {
            "authors": [
                "Shogo Ujiie",
                "Hayate Iso",
                "Eiji Aramaki."
            ],
            "title": "Biomedical entity linking with contrastive context matching",
            "venue": "ArXiv preprint, abs/2106.07583.",
            "year": 2021
        },
        {
            "authors": [
                "Rob van der Goot",
                "Ibrahim Sharaf",
                "Aizhan Imankulova",
                "Ahmet \u00dcst\u00fcn",
                "Marija Stepanovi\u0107",
                "Alan Ramponi",
                "Siti Oryza Khairunnisa",
                "Mamoru Komachi",
                "Barbara Plank"
            ],
            "title": "From masked language modeling to translation: Non-English auxiliary tasks improve",
            "year": 2021
        },
        {
            "authors": [
                "Laurens van der Maaten",
                "Geoffrey E. Hinton."
            ],
            "title": "Visualizing non-metric similarities in multiple maps",
            "venue": "Machine Learning, 87(1):33\u201355.",
            "year": 2012
        },
        {
            "authors": [
                "Ivan Vuli\u0107",
                "Goran Glava\u0161",
                "Fangyu Liu",
                "Nigel Collier",
                "Edoardo Maria Ponti",
                "Anna Korhonen."
            ],
            "title": "Exposing cross-lingual lexical knowledge from multilingual sentence encoders",
            "venue": "ArXiv preprint, abs/2205.00267.",
            "year": 2022
        },
        {
            "authors": [
                "Ivan Vuli\u0107",
                "Pei-Hao Su",
                "Samuel Coope",
                "Daniela Gerz",
                "Pawe\u0142 Budzianowski",
                "I\u00f1igo Casanueva",
                "Nikola Mrk\u0161i\u0107",
                "Tsung-Hsien Wen."
            ],
            "title": "ConvFiT: Conversational fine-tuning of pretrained language models",
            "venue": "Proceedings of the 2021 Conference on Empiri-",
            "year": 2021
        },
        {
            "authors": [
                "Danqing Wang",
                "Jiaze Chen",
                "Hao Zhou",
                "Xipeng Qiu",
                "Lei Li."
            ],
            "title": "Contrastive aligned joint learning for multilingual summarization",
            "venue": "Findings of the Association for Computational Linguistics: ACLIJCNLP 2021, pages 2739\u20132750, Online. Association",
            "year": 2021
        },
        {
            "authors": [
                "Weijia Xu",
                "Batool Haider",
                "Saab Mansour."
            ],
            "title": "End-to-end slot alignment and recognition for crosslingual NLU",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 5052\u20135063, Online. Association",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction and Motivation",
            "text": "Slot labeling (SL) is a crucial natural language understanding (NLU) component for task-oriented dialogue (TOD) systems (Tur and De Mori, 2011). It aims to identify slot values in a user utterance and fill the slots with the identified values. For instance, given the user utterance \u201cTickets from Chicago\nto Milan for tomorrow\u201d, the airline booking system should match the values \u201cChicago\u2019\u2019, \u201cMilan\u201d, and \u201ctomorrow\u201d with the slots departure_city, arrival_city, and date, respectively.\nBuilding TOD systems which support new domains, tasks, and also languages is challenging, expensive and time-consuming: it requires large annotated datasets for model training and development, where such data are scarce for many domains, tasks, and most importantly - languages (Razumovskaia et al., 2022a). The current approach to mitigate the issue is the standard cross-lingual transfer. The main \u2018transfer\u2019 assumption is that a suitable large English annotated dataset is always available for a particular task and domain: (i) the systems are then trained on the English data and then directly deployed to the target language (i.e., zero-shot transfer), or (ii) further adapted to the target language relying on a small set of target language examples (Xu et al., 2020; Razumovskaia et al., 2022b) which are combined with the large English dataset (i.e., few-shot transfer). However, this assumption might often be unrealistic in the context of TOD due to a large number of potential tasks and domains that should be supported by TOD systems (Casanueva et al., 2022). Furthermore, the standard assumption implicitly grounds any progress of TOD in other languages to the English language, hindering any system construction initiatives focused directly on the target languages (Ruder et al., 2022).\nTherefore, in this work we depart from this often unrealistic assumption, and propose to focus on transfer-free scenarios for SL instead. Here, the system should learn the task in a particular domain directly from limited resources in the target language, assuming that any English data cannot be guaranteed. This setup naturally calls for constructing a versatile multilingual data-efficient method that leverages scarce annotated data as effectively as possible and should thus be especially applicable to low-resource languages (Joshi et al., 2020).\nPutting this challenging setup into focus, we thus propose a novel two-stage slot-labeling approach, dubbed TWOSL. TWOSL recasts the SL task into a span classification task within its two respective stages. In Stage 1, a multilingual general-purpose sentence encoder is fine-tuned via contrastive learning (CL), tailoring the CL objective towards SLbased span classification; the main assumption is that representations of phrases with the same slot type should obtain similar representations in the specialised encoder space. CL allows for a more efficient use of scarce training resources (Fang et al., 2020; Su et al., 2021; Rethmeier and Augenstein, 2021). Foreshadowing, it manages to separate the now-specialised SL-based encoder space into slottype specialised subspaces, as illustrated later in Figure 2. These SL-aware encodings are more interpretable and allow for easier classification into slot types in Stage 2, using simple MLP classifiers.\nWe evaluate TWOSL in transfer-free scenarios on two standard multilingual SL benchmarks: MultiATIS++ (Xu et al., 2020) and xSID (van der Goot et al., 2021), which in combination cover 13 typologically diverse target languages. Our results indicate that TWOSL yields large and consistent improvements 1) across different languages, 2) in different training set size setups, and also 3) with different input multilingual encoders. The gains are especially large in extremely low-resource setups. For instance, on MultiATIS++, with only 200 training examples in the target languages, we observe an improvement in average F1 scores from 49.1 without the use of TWOSL to 66.8 with TWOSL, relying on the same multilingual sentence encoder. Similar gains were observed on xSID, and also with other training set sizes. We also report large gains over fine-tuning XLM-R for SL framed as the standard token classification task (e.g., from 50.6 to 66.8 on MultiATIS++ and from 43.0 to 52.6 on xSID with 200 examples), validating our decision to recast the task in TWOSL as a span classification task.\nIn summary, the results suggest the benefits of TWOSL for transfer-free multilingual slot labeling, especially in the low-resource setups when only several dozen examples are available in the target language: this holds promise to quicken SL development cycles in future work. The results also demonstrate that multilingual sentence encoders can be transformed into effective span encoders using contrastive learning with a handful of examples. The CL procedure in TWOSL exposes their phrase-\nlevel semantic \u2018knowledge\u2019 (Liu et al., 2021; Vulic\u0301 et al., 2022). In general, we hope that this work will inspire and pave the way for further research in the challenging transfer-free few-shot setups for multilingual SL as well as for other NLP tasks. The code for TWOSL will be available online."
        },
        {
            "heading": "2 Related Work",
            "text": "Multilingual Slot Labeling. Recently, the SL task in multilingual contexts has largely benefited from the development of multilingually pretrained language models (PLMs) such as mBERT (Devlin et al., 2019) and XLM-R (Conneau et al., 2020). These models are typically used for zero-shot or few-shot multilingual transfer (Xu et al., 2020; Krone et al., 2020; Cattan et al., 2021). Further, the representational power of the large multilingual PLMs for cross-lingual transfer has been further refined through adversarial training with latent variables (Liu et al., 2019) and multitask training (van der Goot et al., 2021).\nOther effective methods for cross-lingual transfer are translation-based, where either the training data in the source language is translated into the target language or the evaluation data is translated into the source (translate-train and translate-test, respectively; Schuster et al. (2019); Razumovskaia et al. (2022a)). The issues with these methods for SL are twofold. First, the translations might be of lower quality for low-resource languages or any language pair where large parallel datasets are lacking. Second, they involve the crucial label-projection step, which aligns the words in the translated utterances with the words in the source language. Therefore, (i) applying translation-based methods to sequence labeling tasks such as SL is not straightforward (Ponti et al., 2021), (ii) it increases the number of potential accumulated errors (Fei et al., 2020), and (iii) requires powerful word alignment tools (Dou and Neubig, 2021).\nSeveral methods were proposed to mitigate the issues arising from the label-projection step. Xu et al. (2020) propose to jointly train slot tagging and alignment algorithms. Gritta and Iacobacci (2021) and Gritta et al. (2022) fine-tune the models for post-alignment, i.e., explicitly aligning the source and translated data for better cross-lingual dialogue NLU. These approaches still rely on the availability of parallel corpora which are not guaranteed for low-resource languages. Thus, alternative approaches using code-switching (Qin et al.,\n2020; Krishnan et al., 2021) were proposed. All of the above methods assume the availability of an \u2018aid\u2019 for cross-lingual transfer such as a translation model or a bilingual lexicon; more importantly, they assume the existence of readily available taskannotated data in the source language.\nData-Efficient Methods for Slot Labeling. One approach to improve few-shot generalisation in TOD systems is to pretrain the models in a way that is specifically tailored to conversational tasks. For instance, ConVEx (Henderson and Vulic\u0301, 2021) fine-tunes only a subset of decoding layers on conversational data. QANLU (Namazifar et al., 2021) and QASL (Fuisz et al., 2022) use questionanswering for data-efficient slot labeling in monolingual English-only setups by answering questions based on reduced training data.\nIn addition, methods for zero-shot cross-lingual contrastive learning have been developed (Gritta et al., 2022; Gritta and Iacobacci, 2021) which allow for efficient use of available annotated data. Further, Qin et al. (2022) and Liang et al. (2022) use code-switched examples to improve performance on intent classification and slot labeling in a zero-shot setting by training on code-switched examples on slot-value, sentence and word levels. Unlike prior work, TWOSL focuses on adapting multilingual sentence encoders to the SL task in transfer-free low-data setups."
        },
        {
            "heading": "3 Methodology",
            "text": "Preliminaries. We assume the set of Ns slot types S = {SL1, . . . , SLNs} associated to an SL task. Each word token in the input sentence/sequence s = w1, w2, ..., wn should be assigned a slot label yi, where we assume a standard BIO tagging\nscheme for sequence labeling (e.g., the labels are O, B-SL1, I-SL1,. . ., I-SLNs).\n1 We also assume that M SL-annotated sentences are available in the target language as the only supervision signal.\nThe full two-stage TWOSL framework is illustrated in Figure 1, and we describe its two stages in what follows."
        },
        {
            "heading": "3.1 Stage 1: Contrastive Learning for Span",
            "text": "Classification\nStage 1 has been inspired by contrastive learning regimes which were proven especially effective in few-shot setups for cross-domain (Su et al., 2022; Meng et al., 2022; Ujiie et al., 2021) and crosslingual transfer (Wang et al., 2021; Chen et al., 2022), as well as for task specialisation of generalpurpose sentence encoders and PLMs for intent detection (Mehri and Eric, 2021; Vulic\u0301 et al., 2021). To the best of our knowledge, CL has not been coupled with the TOD SL task before.\nInput Data Format for CL. First, we need to reformat the input sentences into the format suitable for CL. Given M annotated sentences, we transform each of them into M triples of the following format: (smask, sp, L). Here, (i) smask is the original sentence s, but with word tokens comprising a particular slot value masked from the sentence; (ii) sp is that slot value span masked from the original sentence; (iii) L is the actual slot type associated with the span sp. Note that L can be one of the Ns slot types from the slot set S or a special None value denoting that sp does not capture a proper slot value. One exam-\n1For simplicity and clarity, we will focus on the standard BIO scheme, while the method is fully operational with other tagging schemes as well.\nple of such a triple is (smask=Ich ben\u00f6tige einen Flug von [MASK] [MASK] nach Chicago, sp=New York, L=departure_city). In another example, (smask=[MASK] mir die Preise von Boston nach Denver, sp=Zeige, L=None). Note that sp can span one or more words as in the examples above, which effectively means masking one or more words from the original sentence. We limit the length of sp to the maximum of maxsp consecutive words.\nPositive and Negative Pairs for CL. The main idea behind CL in Stage 1 is to adapt the input (multilingual) sentence encoder to the span classification task by \u2018teaching\u2019 it to encode sentences carrying the same slot types closer in its CL-refined semantic space. The pair p=(smask, sp) is extracted from the corresponding tuple, and the encoding of the pair is a concatenation of encodings of smask and sp encoded separately by the sentence encoder. CL proceeds in a standard fashion relying on sets of positive and negative CL pairs. A positive pair (actually, \u2018a pair of pairs\u2019) is one where two pairs pi and pj contain the same label L in their corresponding tuple, but only if L \u0338= None.2 A negative pair is one where two pairs pi and pj contain different labels Li and Lj in their tuples, but at least one of the labels is not None.\nFollowing prior CL work (Vulic\u0301 et al., 2021), each positive pair (pi, pj) is associated with 2K negative pairs, where we randomly sample K negatives associated with pi and K negatives for pj . Finally, for the special and most efficient CL setup where the ratio of positive and negative pairs is 1 : 1, we first randomly sample the item from the positive pair (pi, pj), and then randomly sample a single negative for the sampled pi or pj .\nOnline Contrastive Loss. Fine-tuning the input sentence encoder with the positive and negative pairs proceeds via a standard online contrastive loss. More formally:\nLcontr(si, sj , f) = 1[yi = yj ]\u2225f(si)\u2212 f(sj)\u22252+ +1[yi \u0338= yj ]m\u0307ax(0,m\u2212 \u2225f(si)\u2212 f(sj)\u22252)\nwhere si and sj are two examples with labels yi and yj , f is the encoding function and m is a hyperparameter defining the margin between samples of different classes.\nSimilarly to the original contrastive loss (Chopra et al., 2005), it aims at 1) reducing the semantic\n2Put simply, we disallow pulling closer pairs which do not convey any useful slot-related semantic information.\ndistance, formulated as the cosine distance, between representations of examples forming the positive pairs, and 2) increase the distance between representations of examples forming the negative pairs. The online version of the loss, which typically outperforms its standard variant (Reimers and Gurevych, 2019), focuses only on hard positive and hard negative examples: the distance is higher than the margin m for positive examples, and below m for negative examples."
        },
        {
            "heading": "3.2 Stage 2: Span Identification and",
            "text": "Classification\nThe aim of Stage 2 is to identify and label the slot spans, relying on the embeddings produced by the encoders fine-tuned in the preceding Stage 1. In order to identify the slot spans, we must consider every possible subspan of the input sentence, which might slow down inference. Therefore, to boost inference speed, we divide Stage 2 into two steps. In Step 1, we perform a simple binary classification, aiming to detected whether a certain span is a slot value for any slot type from S . Effectively, for the input pair (smask, sp) the binary classifier returns 1 (i.e., \u2018sp is some slot value\u2019) or 0. The 0-examples for training are all subspans of the sentences which are not associated with any slot type from S.\nStep 2 is a multi-class span classification task, where we aim to predict the actual slot type from S for the input pair (smask, sp). The binary filtering Step 1 allows us to remove all input pairs for which the Step 1 prediction is 0, and we thus assign slot types only for the 1-predictions from Step 1. Put simply, Step 1 predicts if span covers any proper slot value, while Step 2 maps the slot value to the actual slot type. We can directly proceed with Step 2 without Step 1, but the training data then also has to contain all the examples with spans where L=None, see Figure 1 again.\nThe classifiers in both steps are implemented as simple multi-layer perceptrons (MLP), and the input representation in both steps is the concatenation of the respective encodings for smask and sp."
        },
        {
            "heading": "4 Experimental Setup",
            "text": "Training Setup and Data. The standard few-shot setup in multilingual contexts (Razumovskaia et al., 2022a; Xu et al., 2020) assumes availability of a large annotated task-specific dataset in English, and a handful of labeled examples in the target language. However, as discussed in \u00a71, this as-\nDataset Domains Slots Languages Examples per Lang\nMultiATIS++ 1 84 de, fr, pt, tr, hi 5,871\nxSID 7 33 ar, da, de, de-st, id, it, ja, kk, nl, sr, tr 800\nTable 1: Multilingual SL datasets in the experiments.\nsumption might not always hold. That is, the English data might not be available for many targetlanguage specific domains, especially since the annotation for the SL task is also considered more complex than for the intent detection task (van der Goot et al., 2021; Xu et al., 2020; FitzGerald et al., 2022). We thus focus on training and evaluation in these challenging transfer-free setups.\nWe run experiments on two standard multilingual SL datasets, simulating the transfer-free setups: MultiATIS++ (Xu et al., 2020) and xSID (van der Goot et al., 2021). Their data statistics are provided in Table 1, with language codes in Appendix A. For low-resource scenarios, we randomly sample M annotated sentences from the full training data. Since xSID was originally intended only for testing zero-shot cross-lingual transfer, we use its limited dev set for (sampling) training instances.\nA current limitation of TWOSL is that it leans on whitespace-based word token boundaries in the sentences: therefore, in this work we focus on a subset of languages with that property, leaving further adaptation to other languages for future work.\nInput Sentence Encoders. We experiment both with multilingual sentence encoders as well as general multilingual PLMs in order to (i) demonstrate the effectiveness of TWOSL irrespective of the underlying encoder, and to (ii) study the effect of pretraining task on the final performance. 1) XLMR (Conneau et al., 2020) is a multilingual PLM, pretrained with a large multilingual dataset in 100 languages via masked language modeling. 2) Multilingual mpnet (Song et al., 2020) is pretrained for paraphrase identification in over 50 languages; the model was specifically pretrained in a contrastive fashion to effectively encode sentences. 3) We also run a subset of experiments with another stateof-the-art multilingual sentence encoder, LaBSE (Feng et al., 2022), to further verify that TWOSL can be disentangled from the actual encoder.3 All\n3Prior work has demonstrated effectiveness of models pretrained for span encoding in few-shot settings (Henderson and Vulic\u0301, 2021; Coope et al., 2020). However, they are not directly comparable with TWOSL as they are based on Englishonly encoders and are not publicly available.\nmodels are used in their \u2018base\u2019 variants, with 12 hidden-layers and encoding the sequences into 768- dimensional vectors. This means that the actual encodings of (smask, sp) pairs, which are fed to MLPs in Stage 2, are 1,536-dimensional; see \u00a73.\nHyperparameters and Optimisation. We rely on sentence-transformers (SBERT) library (Reimers and Gurevych, 2019, 2020) for model checkpoints and contrastive learning in Stage 1. The models are fine-tuned for 10 epochs with batch size of 32 using the default hyperparameters in SBERT: e.g., the margin in the contrastive loss is fixed to m = 0.5. maxsp is fixed to 5 as even the longest slot values very rarely exceed that span length. Unless stated otherwise, K = 1, that is, the ratio of positive-tonegative examples is 1 : 2, see \u00a73.1.\nIn Stage 2, we train binary and multi-class MLPs with the following number of hidden layers and their size, respectively: [2,500, 1,500] and [3,600, 2,400, 800], and ReLU as the non-linear activation. The Step 1 binary classifier is trained for 30 epochs, while the Step 2 MLP is trained for 100 epochs. The goal in Step 1 is to ensure high recall (i.e., to avoid too aggressive filtering), which is why we opt for the earlier stopping. As a baseline, we fine-tune XLM-R for the token classification task, as the standard SL task format (Xu et al., 2020; Razumovskaia et al., 2022b). Detailed training hyperparameters are provided in Appendix B. All results are averages across 5 random seeds.\nEvaluation Metric. For direct comparability with standard token classification approaches we rely on token-level micro-F1 as the evaluation metric. For TWOSL this necessitates the reconstruction of the BIO-labeled sequence Y from the predictions for the (smask, sp, Lpred) tuples. For every sentence s we first identify all the tuples (smask, sp, Lpred) associated with s such that the predicted slot type Lpred \u0338= None. In Y the positions of sp are filled with BLpred , complemented with the corresponding number of ILpred if the length of sp > 1. Following that, the rest of the positions are set to the O label."
        },
        {
            "heading": "5 Results and Discussion",
            "text": "Before experimenting in the planned multilingual context, we evaluate our model in English, based on the standard ATIS dataset. The models are trained with the hyper-parameters described in \u00a74. For English, we use LaBSE (Feng et al., 2022) as a sequence encoder as it has demonstrated state-of-theart results in prior experiments in dialogue-specific\ntasks (Casanueva et al., 2022). The results in Table 3 demonstrate that TWOSL is effective in lowdata English-only setups, with large gains even atop such a strong sentence encoder as LaBSE. This indicates that TWOSL can be used to bootstrap any project when only a handful of in-domain data points are available.\nImpact of Contrastive Learning in TWOSL on Slot Representations. Further, before delving deep into quantitative analyses, we investigate what\neffect CL in Stage 1 of TWOSL actually has on span encodings, and how it groups them over slot types. The aim of CL in Stage 1 is exactly to make the representations associated with particular cluster into coherent groups and to offer a clearer separation of encodings across slot types. As proven previously for the intent detection task (Vulic\u0301 et al., 2021), such well-divided groups in the encoding space might facilitate learning classifiers on top of\nthe fine-tuned encoders. As revealed by a t-SNE plot (van der Maaten and Hinton, 2012) in Figure 2, which shows the mpnet-based encodings before and after Stage 1, exactly that effect is observed.\nNamely, the non-tuned mpnet encoder already provides some separation of encodings into slot type-based clusters (Figure 2a), but the groupings are less clear and noisier. In contrast, in Figure 2b the examples are clustered tightly by slot type, with clear separations between different slot type-based clusters. This phenomenon is further corroborated by the automated Silhouettes cluster coherence metric (Rousseeuw, 1987): its values are \u03c3 = \u22120.02 (before Stage 1) and \u03c3 = 0.67 (after Stage 1). In sum, this qualitative analysis already suggests the effectiveness of CL for the creation of customised span classification-oriented encodings that support the SL task. We note that the same observations hold for all other languages as well as for all other (data-leaner) training setups.\nMain Results. The results on xSID and MultiATIS++ are summarised in Table 2 and Figure 3, respectively. The scores underline three important trends. First, TWOSL is much more powerful than the standard PLM-based (i.e., XLM-R-based) token classification approach in very low-data setups, when only a handful (e.g., 50-200) annotated examples in the target language are available as the only supervision. Second, running TWOSL on top of a general-purpose multilingual encoder such as mpnet yields large and consistent gains, and this is clearly visible across different target languages in both datasets, and across different data setups. Third, while the token classification approach is able to recover some performance gap as more annotated data become available (e.g., check Figure 3 with 800 examples), TWOSL remains the peak-performing approach in general.\nA finer-grained inspection of the scores further reveals that for low-data setups, even when exactly the same model is used as the underlying encoder (i.e., XLM-R), TWOSL offers large benefits over token classification with full XLM-R fine-tuning, see Table 2. The scores also suggest that the gap between TWOSL and the baselines increases with the decrease of annotated data. The largest absolute and relative gains are in the 50-example setup, followed by the 100-example setup, etc.: e.g., on xSID, the average gain is +9.5 F1 points with 200 training examples, while reaching up to +35.3 F1 points with 50 examples. This finding corroborates\n50 100 200 500\nDE FR DE FR DE FR DE FR\nXLM-R 82.4 74.5 83.4 75.5 87.9 78.3 89.8 85.1 TWOSL: mpnet w/o CL 56.3 55.2 62.9 57.7 68.5 63.8 71.2 70.2 TWOSL: mpnet w/ CL 82.1 75.2 76.8 71.8 84.7 78.9 87.6 84.6\nTable 4: Results on German and French in MultiATIS++ for standard few-shot setup where English annotated data is combined with a few target language examples.\nthe power of CL especially for such low-data setups. Finally, the results in Table 2 also hint that TWOSL works well with different encoders: it improves both mpnet and XLM-R as the underlying multilingual sentence encoders."
        },
        {
            "heading": "5.1 Ablations and Further Analyses",
            "text": "TWOSL for Low-Resource Languages. TWOSL is proposed for extremely low-resource scenarios, when only a handful of examples in a target language are available. This is more likely to happen for low-resource languages, which in addition are usually not represented enough in pretraining of PLMs (Conneau et al., 2020). To evaluate TWOSL on low-resource languages, we apply it to Named Entity Recognition (NER) task in several low-resource African languages. We focused on NER as i) similarly to slot labelling, NER is a sequence labelling task; ii) limited annotated data is available in low-resource languages for NER. Specifically, we use MasakhaNER (Adelani et al., 2021) dataset in our experiments focusing on Yoruba (yor) and Luo (luo). The experiments were conducted with 100 training examples, using XLM-R (base) as a sentence encoder. The rest of the setup was kept the same as described in \u00a74.\nTWOSL has brought considerable improvements for both languages: from 14.46 to 45.38 F-1 for yor and from 16.56 to 36.46 F-1 for luo. These results further indicate the effectiveness of the method for low-resource languages as well as for languagedomain combinations with scarce resources.\nTWOSL in Standard Few-Shot Setups. TWOSL has been designed with a primary focus on transferfree, extremely low-data setups. However, another natural question also concerns its applicability and effectiveness in the standard few-shot transfer setups, where we assume that a large annotated dataset for the same task and domain is available in the source language: English. To this end, we run several experiments on MultiATIS++, with German and French as target languages, where we first fine-\ntune the model on the full English training data, before running another fine-tuning step (Lauscher et al., 2020) on the M = 50, 100, 200, 500 examples in the target language.\nOverall, the results in Table 4 demonstrate that TWOSL maintains its competitive performance, although the token classification approach with XLM-R is a stronger method overall in this setup. TWOSL is more competitive for French as the target language. The importance of CL in Step 1 for TWOSL is pronounced also in this more abundant data setup. We leave further exploration and adaptation of TWOSL to transfer setups for future work.\nImpact of Binary Filtering in Stage 2. In order to understand the benefit of Step 1 (i.e., binary filtering) in Stage 2, we compare the performance and inference time with and without that step. We focus on the xSID dataset in the 200-example setup. The scores, summarised in Table 10 in Appendix E, demonstrate largely on-par performance between the two variants. The main benefit of using Step 1 is thus its decrease of inference time, as reported in Figure 4, where inference was carried out on a single NVIDIA Titan XP 12GB GPU. The filtering step, which relies on a more compact and thus quicker classifier, greatly reduces the number of examples that have to undergo the final, more expensive slot type prediction (i.e., without filtering all the subspans of the user utterance must be processed) without harming the final performance.\nDifferent Multilingual Encoders. The results in Table 2 have already validated that TWOSL offers gains regardless of the chosen multilingual encoder (e.g., XLM-R versus mpnet). However, the effectiveness of TWOSL in terms of absolute scores is naturally dependent on the underlying multilingual capabilities of the original multilingual encoder. We thus further analyse how the performance changes in the same setups with different\nAR DA ID IT KK SR TR AVG\n50 examples\nXLM-R-Sent w/o CL 12.7 22.5 23.3 26.9 20.1 22.7 25.7 22.0 XLM-R-Sent w/ CL 40.2 41.2 46.7 49.2 43.1 41.5 44.7 43.8 mpnet w/o CL 23.9 26.8 28.5 26.3 22.6 23.9 24.8 25.2 mpnet w/ CL 36.8 39.2 43.4 42.2 33.5 39.7 42.8 39.6 LaBSE w/o CL 21.4 31.7 33.7 31.6 29.0 25.5 27.7 28.6 LaBSE w/ CL 41.8 47.0 48.2 48.3 41.8 36.6 37.9 43.1\n100 examples\nXLM-R-Sent w/o CL 28.1 30.9 30.6 36.3 29.4 27.0 34.2 30.9 XLM-R-Sent w/ CL 39.0 42.9 51.4 36.4 41.6 35.7 50.4 42.5 mpnet w/o CL 34.3 36.3 37.6 38.7 30.0 33.7 32.8 34.8 mpnet w/ CL 43.5 44.2 52.6 47.2 40.4 46.5 51.1 46.5 LaBSE w/o CL 31.7 40.7 37.9 38.5 34.9 37.1 37.1 36.8 LaBSE w/ CL 47.3 50.2 49.6 53.6 45.5 39.4 42.8 46.9\nTable 5: Results on xSID for a sample of languages with different sentence encoders. XLM-R-Sent denotes using XLM-R as a standard sentence encoder.\nencoders. We compare XLM-R-Sent (i.e., XLM-R used a sentence encoder, mean-pooling all subword embeddings), mpnet, and LaBSE on a representative set of 7 target languages on xSID. In the majority of the experimental runs, LaBSE with TWOSL yields the highest absolute scores. This comes as no surprise as LaBSE was specifically customised to improve sentence encodings for low-resource languages and in low-resource setups (Feng et al., 2022). Interestingly, XLM-R performs the best in the \u2018lowest-data\u2019 50-example setup: we speculate this might be due to a smaller model size, which makes it harder to overfit in extremely low-resource setups. Finally, the scores again verify the benefit of TWOSL when applied to any underlying encoder.\nNumber of Negative Pairs. The ratio of positiveto-negative examples, controlled by the hyperparameter K, has a small impact on the overall performance, as shown in Figure 5. We observe some slight performance gains when moving from 1 neg-\native example to 2 (cf., the 50-example setup for AR and 100-example setup for ID in xSID). In such cases, the increase in the number of negative pairs can act as data augmentation for the extreme low-resource scenarios. This hyper-parameter also impacts the trade-off between training time and the stability of results. With fewer negative examples, training is quicker, but the performance is less stable: e.g., in the 50-example setup for German in MultiATIS++, the standard deviation is \u03c3 = 7.45, \u03c3 = 2.36 and \u03c3 = 3.21 with 1,2 and 4 negatives per positive, respectively. Therefore, as stated in \u00a74, we use the setup with 2 negatives-per-positive in our experiments, indicating the good trade-off between efficiency and stability."
        },
        {
            "heading": "6 Conclusion and Future Work",
            "text": "We proposed TWOSL, a two-stage slot labeling approach which turns multilingual sentence encoders into slot labelers for task-oriened dialogue (TOD), which was proven especially effective for slot labeling in low-resource setups and languages. TWOSL was developed with the focus on transfer-free fewshot multilingual setups, where sufficient Englishlanguage annotated data are not readily available to enable standard cross-lingual transfer approaches. In other words, the method has been created for bootstrapping a slot labeling system in a new language and/or domain when only a small set of annotated examples is available. TWOSL first converts multilingual sentence encoders into task-specific span encoders via contrastive learning. It then casts slot labeling into the span classification task supported by the fine-tuned encoders from the previous stage. The method was evaluated on two standard multilingual TOD datasets, where we validated its strong performance across diverse languages and different training data setups.\nDue to its multi-component nature, a spectrum of extensions focused on its constituent components is possible in future work, which includes other formulations of contrastive learning, tuning the models multilingually, mining (non-random) negative pairs and extending the method to crossdomain transfer learning for ToD (Majewska et al., 2022), especially for rare domains not covered by standard datasets. In this work, we have focused on the sample-efficient nature of TWOSL. We expect it to be complementary to modular and parameterefficient techniques (Pfeiffer et al., 2023). In the long run, we plan to use the method for large-\nscale fine-tuning of sentence encoders to turn them into universal span encoders which can then be used on sequence labelling tasks across languages and domains. TWOSL can be further extended to slot labelling with nested slots as well as to other \u2018non-TOD\u2019 sequence labelling tasks (e.g., NER) for which evaluation data exists for truly low-resource languages: e.g., on African languages (Adelani et al., 2021).\nLimitations\nTWOSL relies on whitespace-based word boundaries. Thus, it is only applicable to languages which use spaces as word boundaries. We plan to extend and adapt the method to other languages, without this property, in our subsequent work. Additionally, the approach has been only tested on the languages which the large multilingual PLMs have seen during their pretraining. We plan to adapt and test the same approach on unseen languages in the future.\nAs mentioned in \u00a76, we opted for representative multilingual sentence encoders and components of contrastive learning that were proven to work well for other tasks in prior work (Reimers and Gurevych, 2020; Vulic\u0301 et al., 2021) (e.g., the choice of the contrastive loss, adopted hyper-parameters), while a wider exploration of different setups and regimes in TWOSL\u2019s Stage 1 and Stage 2 might further improve performance and offer additional low-level insights.\nThe scope of our multilingual evaluation is also constrained by the current availability of multilingual evaluation resources for TOD NLU tasks.\nFinally, in order to unify the experimental protocol across different languages, and for a more comprehensive coverage and cross-language comparability, we relied on multilingual encoders throughout the work. However, we stress that for the transfer-free scenarios, TWOSL is equally applicable to monolingual encoders for respective target languages, when such models exist, and this might yield increased absolute performance."
        },
        {
            "heading": "Acknowledgments",
            "text": "The work was in part supported by a Huawei research donation to the Language Technology Lab at the University of Cambridge. Ivan Vulic\u0301 is also supported by a personal Royal Society University Research Fellowship \u2018Inclusive and Sustainable Language Technology for a Truly Multilingual World\u2019 (no 221137; 2022\u2013)."
        },
        {
            "heading": "A Language codes",
            "text": "Language codes which are used in the paper are provided in Table 6."
        },
        {
            "heading": "B Training Hyperparameters",
            "text": "The values for training hyperparameters for each stage of TWOSL and token classification are provided in Table 7. For information about Adam and AdamW optimizers we refer the reader to Kingma and Ba (2015) and Loshchilov and Hutter (2019), respectively."
        },
        {
            "heading": "C Full Scores on MultiATIS++",
            "text": "The exact numerical scores on MultiATIS++ across different languages and setups, which were used as the source for Figure 3 in the main paper, are provided in Table 8."
        },
        {
            "heading": "D Standard Deviation for xSID results using mpnet as a sentence encoder",
            "text": "The standard deviation of 5 runs for xSID are presented in Table 9. The standard deviation which is considerably lower than the margin between the performance of systems without and with contrastive learning, proving the significance of improvements that CL provides."
        },
        {
            "heading": "E Results on xSID with and without the Binary Filtering Step",
            "text": "A comparison of results with and without applying the binary filtering step (i.e., Step 1 in Stage 2 of TWOSL) is provided in Table 10; see \u00a75.1 for the discussion supported by this set of results."
        },
        {
            "heading": "F Number of negative examples for",
            "text": "MultiATIS++"
        }
    ],
    "title": "Transfer-Free Data-Efficient Multilingual Slot Labeling",
    "year": 2023
}