{
    "abstractText": "The recent success of SimCSE has greatly advanced state-of-the-art sentence representations. However, the original formulation of SimCSE does not fully exploit the potential of hard negative samples in contrastive learning. This study introduces an unsupervised contrastive learning framework that combines SimCSE with hard negative mining, aiming to enhance the quality of sentence embeddings. The proposed focal-InfoNCE function introduces self-paced modulation terms in the contrastive objective, downweighting the loss associated with easy negatives and encouraging the model focusing on hard negatives. Experimentation on various STS benchmarks shows that our method improves sentence embeddings in terms of Spearman\u2019s correlation and representation alignment and uniformity. Our code is available at: https://github.com/puerrrr/ Focal-InfoNCE.",
    "authors": [
        {
            "affiliations": [],
            "name": "Pengyue Hou"
        },
        {
            "affiliations": [],
            "name": "Xingyu Li"
        }
    ],
    "id": "SP:fdea00556592a4ef18042605820d881500b8c3e3",
    "references": [
        {
            "authors": [
                "Eneko Agirre",
                "Carmen Banea",
                "Claire Cardie",
                "Daniel Cer",
                "Mona Diab",
                "Aitor Gonzalez-Agirre",
                "Weiwei Guo",
                "Inigo Lopez-Gazpio",
                "Montse Maritxalar",
                "Rada Mihalcea"
            ],
            "title": "Semeval-2015 task 2: Semantic textual similarity, english, spanish and pilot on",
            "year": 2015
        },
        {
            "authors": [
                "Eneko Agirre",
                "Carmen Banea",
                "Claire Cardie",
                "Daniel Cer",
                "Mona Diab",
                "Aitor Gonzalez-Agirre",
                "Weiwei Guo",
                "Rada Mihalcea",
                "German Rigau",
                "Janyce Wiebe."
            ],
            "title": "Semeval-2014 task 10: Multilingual semantic textual similarity",
            "venue": "Proceedings of the 8th interna-",
            "year": 2014
        },
        {
            "authors": [
                "Eneko Agirre",
                "Carmen Banea",
                "Daniel Cer",
                "Mona Diab",
                "Aitor Gonzalez Agirre",
                "Rada Mihalcea",
                "German Rigau Claramunt",
                "Janyce Wiebe."
            ],
            "title": "Semeval2016 task 1: Semantic textual similarity, monolingual and cross-lingual evaluation",
            "venue": "SemEval-2016.",
            "year": 2016
        },
        {
            "authors": [
                "Eneko Agirre",
                "Daniel Cer",
                "Mona Diab",
                "Aitor Gonzalez-Agirre."
            ],
            "title": "Semeval-2012 task 6: A pilot on semantic textual similarity",
            "venue": "* SEM 2012: The First Joint Conference on Lexical and Computational Semantics\u2013Volume 1: Proceedings of the",
            "year": 2012
        },
        {
            "authors": [
                "Eneko Agirre",
                "Daniel Cer",
                "Mona Diab",
                "Aitor GonzalezAgirre",
                "Weiwei Guo."
            ],
            "title": " sem 2013 shared task: Semantic textual similarity",
            "venue": "Second joint conference on lexical and computational semantics (* SEM), volume 1: proceedings of the Main confer-",
            "year": 2013
        },
        {
            "authors": [
                "Daniel Cer",
                "Mona Diab",
                "Eneko Agirre",
                "Inigo LopezGazpio",
                "Lucia Specia."
            ],
            "title": "Semeval-2017 task 1: Semantic textual similarity-multilingual and cross-lingual focused evaluation",
            "venue": "arXiv preprint arXiv:1708.00055.",
            "year": 2017
        },
        {
            "authors": [
                "Daniel Cer",
                "Yinfei Yang",
                "Sheng-yi Kong",
                "Nan Hua",
                "Nicole Limtiaco",
                "Rhomni St John",
                "Noah Constant",
                "Mario Guajardo-Cespedes",
                "Steve Yuan",
                "Chris Tar"
            ],
            "title": "Universal sentence encoder for english",
            "venue": "In Proceedings of the 2018 conference on empiri-",
            "year": 2018
        },
        {
            "authors": [
                "Ting Chen",
                "Simon Kornblith",
                "Mohammad Norouzi",
                "Geoffrey Hinton."
            ],
            "title": "A simple framework for contrastive learning of visual representations",
            "venue": "International conference on machine learning, pages 1597\u20131607. PMLR.",
            "year": 2020
        },
        {
            "authors": [
                "Yung-Sung Chuang",
                "Rumen Dangovski",
                "Hongyin Luo",
                "Yang Zhang",
                "Shiyu Chang",
                "Marin Solja\u010di\u0107",
                "ShangWen Li",
                "Wen-tau Yih",
                "Yoon Kim",
                "James Glass."
            ],
            "title": "Diffcse: Difference-based contrastive learning for sentence embeddings",
            "venue": "arXiv preprint",
            "year": 2022
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "arXiv preprint arXiv:1810.04805.",
            "year": 2018
        },
        {
            "authors": [
                "Tianyu Gao",
                "Xingcheng Yao",
                "Danqi Chen."
            ],
            "title": "Simcse: Simple contrastive learning of sentence embeddings",
            "venue": "arXiv preprint arXiv:2104.08821.",
            "year": 2021
        },
        {
            "authors": [
                "John Giorgi",
                "Osvald Nitski",
                "Bo Wang",
                "Gary Bader."
            ],
            "title": "Declutr: Deep contrastive learning for unsupervised textual representations",
            "venue": "arXiv preprint arXiv:2006.03659.",
            "year": 2020
        },
        {
            "authors": [
                "Pengyue Hou",
                "Jie Han",
                "Xingyu Li"
            ],
            "title": "Improving adversarial robustness with self-paced hardclass pair reweighting",
            "venue": "In Proceedings of the ThirtySeventh AAAI Conference on Artificial Intelligence",
            "year": 2023
        },
        {
            "authors": [
                "Ting Jiang",
                "Jian Jiao",
                "Shaohan Huang",
                "Zihan Zhang",
                "Deqing Wang",
                "Fuzhen Zhuang",
                "Furu Wei",
                "Haizhen Huang",
                "Denvy Deng",
                "Qi Zhang."
            ],
            "title": "Promptbert: Improving bert sentence embeddings with prompts",
            "venue": "arXiv preprint arXiv:2201.04337.",
            "year": 2022
        },
        {
            "authors": [
                "Prannay Khosla",
                "Piotr Teterwak",
                "Chen Wang",
                "Aaron Sarna",
                "Yonglong Tian",
                "Phillip Isola",
                "Aaron Maschinot",
                "Ce Liu",
                "Dilip Krishnan."
            ],
            "title": "Supervised contrastive learning",
            "venue": "Advances in Neural Information Processing Systems, 33:18661\u201318673.",
            "year": 2020
        },
        {
            "authors": [
                "Tsung-Yi Lin",
                "Priya Goyal",
                "Ross Girshick",
                "Kaiming He",
                "Piotr Doll\u00e1r."
            ],
            "title": "Focal loss for dense object detection",
            "venue": "Proceedings of the IEEE international conference on computer vision, pages 2980\u20132988.",
            "year": 2017
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Roberta: A robustly optimized bert pretraining approach",
            "venue": "arXiv preprint arXiv:1907.11692.",
            "year": 2019
        },
        {
            "authors": [
                "Marco Marelli",
                "Stefano Menini",
                "Marco Baroni",
                "Luisa Bentivogli",
                "Raffaella Bernardi",
                "Roberto Zamparelli"
            ],
            "title": "A sick cure for the evaluation of compositional distributional semantic models",
            "venue": "In Lrec,",
            "year": 2014
        },
        {
            "authors": [
                "Hyun Oh Song",
                "Yu Xiang",
                "Stefanie Jegelka",
                "Silvio Savarese."
            ],
            "title": "Deep metric learning via lifted structured feature embedding",
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4004\u20134012.",
            "year": 2016
        },
        {
            "authors": [
                "Nils Reimers",
                "Iryna Gurevych."
            ],
            "title": "Sentence-bert: Sentence embeddings using siamese bert-networks",
            "venue": "arXiv preprint arXiv:1908.10084.",
            "year": 2019
        },
        {
            "authors": [
                "Joshua Robinson",
                "Ching-Yao Chuang",
                "Suvrit Sra",
                "Stefanie Jegelka."
            ],
            "title": "Contrastive learning with hard negative samples",
            "venue": "arXiv preprint arXiv:2010.04592.",
            "year": 2020
        },
        {
            "authors": [
                "Florian Schroff",
                "Dmitry Kalenichenko",
                "James Philbin."
            ],
            "title": "Facenet: A unified embedding for face recognition and clustering",
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition, pages 815\u2013823.",
            "year": 2015
        },
        {
            "authors": [
                "Yifan Sun",
                "Changmao Cheng",
                "Yuhan Zhang",
                "Chi Zhang",
                "Liang Zheng",
                "Zhongdao Wang",
                "Yichen Wei."
            ],
            "title": "Circle loss: A unified perspective of pair similarity optimization",
            "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recogni-",
            "year": 2020
        },
        {
            "authors": [
                "Feng Wang",
                "Huaping Liu."
            ],
            "title": "Understanding the behaviour of contrastive loss",
            "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2495\u20132504.",
            "year": 2021
        },
        {
            "authors": [
                "Tongzhou Wang",
                "Phillip Isola."
            ],
            "title": "Understanding contrastive representation learning through alignment and uniformity on the hypersphere",
            "venue": "International Conference on Machine Learning, pages 9929\u20139939. PMLR.",
            "year": 2020
        },
        {
            "authors": [
                "Xing Wu",
                "Chaochen Gao",
                "Yipeng Su",
                "Jizhong Han",
                "Zhongyuan Wang",
                "Songlin Hu."
            ],
            "title": "Smoothed contrastive learning for unsupervised sentence embedding",
            "venue": "arXiv preprint arXiv:2109.04321.",
            "year": 2021
        },
        {
            "authors": [
                "Xing Wu",
                "Chaochen Gao",
                "Liangjun Zang",
                "Jizhong Han",
                "Zhongyuan Wang",
                "Songlin Hu."
            ],
            "title": "Esimcse: Enhanced sample building method for contrastive learning of unsupervised sentence embedding",
            "venue": "arXiv preprint arXiv:2109.04380.",
            "year": 2021
        },
        {
            "authors": [
                "Yuanmeng Yan",
                "Rumei Li",
                "Sirui Wang",
                "Fuzheng Zhang",
                "Wei Wu",
                "Weiran Xu."
            ],
            "title": "Consert: A contrastive framework for self-supervised sentence representation transfer",
            "venue": "arXiv preprint arXiv:2105.11741.",
            "year": 2021
        },
        {
            "authors": [
                "Kun Zhou",
                "Beichen Zhang",
                "Wayne Xin Zhao",
                "JiRong Wen."
            ],
            "title": "Debiased contrastive learning of unsupervised sentence representations",
            "venue": "arXiv preprint arXiv:2205.00656.",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Unsupervised learning of sentence embeddings has been extensively explored in natural language processing (NLP) (Cer et al., 2018; Giorgi et al., 2020; Yan et al., 2021), aiming to generate meaningful representations of sentences without the need for labeled data. Among various approaches, SimCSE (Gao et al., 2021) achieves state-of-the-art performance in learning high-quality sentence embeddings through contrastive learning. Due to its simplicity and effectiveness, various efforts have been made to improve the contrastive learning of sentence embeddings from different aspects, including alleviating false negative pairs (Wu et al., 2021a; Zhou et al., 2022) and incorporating more informative data augmentations (Wu et al., 2021b; Chuang et al., 2022).\nLeveraging hard-negative samples in contrastive learning is of significance (Schroff et al., 2015; Oh Song et al., 2016; Robinson et al., 2020). Nevertheless, unsupervised contrastive learning ap-\nproaches often face challenges in hard sample mining. Specifically, the original training paradigm of unsupervised-SimCSE proposes to use contradiction sentences as \"negatives\". But such implementation only guarantees that the contradiction sentences are \"true negatives\" but not necessarily hard. With a large number of easy negative samples, the contribution of hard negatives is thus prone to being overwhelmed,\nTo address this issue, we propose a novel loss function, namely Focal-InfoNCE, in the paradigm of unsupervised SimCES for sentence embedding. Inspired by the focal loss (Lin et al., 2017), the proposed Focal-InfoNCE loss assigns higher weights to the harder negative samples in model training and reduces the influence of easy negatives accordingly. By doing so, focal Info-NCE encourages the model to focus more on challenging pairs, forcing it to learn more discriminative sentence representations. In addition, to adapt the dropout strategy for positive pair construction in SimCSE, we further incorporate a positive modulation term in the contrastive objective, which reweights the positive pairs in model optimization. We conduct extensive experiments on various STS benchmark datasets (Agirre et al., 2012, 2013, 2014, 2015, 2016; Cer et al., 2017; Marelli et al., 2014) to evaluate the effectiveness of Focal Info-NCE. Our results demonstrate that Focal Info-NCE significantly improves the quality of sentence embeddings and outperforms unsupervised-SimCSE by an average of 1.64%, 0.82%, 1.51%, and 0.75% Spearan\u2019s correlation on BERT-base, BERT-large, RoBERTabase, and RoBERTa-large, respectively."
        },
        {
            "heading": "2 Related Work",
            "text": ""
        },
        {
            "heading": "2.1 Unsupervised SimCSE",
            "text": "SimCSE (Gao et al., 2021) provides an unsupervised contrastive learning solution to SOTA performance in sentence embedding. Following previ-\nous work (Chen et al., 2020), it optimizes a pretrained model with the cross-entropy objective using in-batch negatives. Formally, given a minibatch of N sentences, {xi}Ni=1, let hi be the sentence representation of xi with the pre-trained language model such as BERT (Devlin et al., 2018) or RoBERTa (Liu et al., 2019). SimCSE\u2019s training objective, InfoNCE, can be formulated as\nli = \u2212log esim(hi,h + i )/\u03c4\u2211N\nj=1 e sim(hi,hj)/\u03c4\n, (1)\nwhere \u03c4 is a temperature hyperparameter and sim(hi, hj) represents the cosine similarity between sentence pairs (xi, xj). Note, h+i is the representation of an augmented version of xi, which constitutes the positive pair of xi. For notation simplification, we will use sip and s i,j n to represent the similarities between positive pairs and negative pairs in this paper.\nUnsupervised SimCSE uses model\u2019s built-in dropout as the minimal \"data augmentation\" and passes the same sentence to the same encoder twice to obtain two sentence embeddings as positive pairs. Any two sentences within a mini-batch form a negative pair. It should be noted that in contrastive learning, model optimizaiton with hard negative samples helps learn better representations. But SimCSE doesn\u2019t distinguish hard negatives and easy ones. We show in this work that incorporating hard negative sample mining in SimCSE boosts the quality of sentence embedding."
        },
        {
            "heading": "2.2 Sample Re-weighting in Machine Learning",
            "text": "Re-weighting is a simple yet effective strategy for addressing biases in machine learning. It downweights the loss from majority classes and obtains a balanced learning solution for minority groups. Re-weighting is also a common technique for hard example mining in deep metric learning (Schroff et al., 2015) and contrastive learning (Chen et al., 2020; Khosla et al., 2020). Recently, self-paced re-weighting is explored in various tasks, such as object detection(Lin et al., 2017), person reidentification(Sun et al., 2020), and adversarial training (Hou et al., 2023). It re-weights the loss of each sample adaptively according to model\u2019s optimization status and encourages a model to focus on learning hard cases. To the best of our knowledge, this study constitutes the first attempt to incorporate self-paced re-weighting strategy in unsupervised sentence embedding."
        },
        {
            "heading": "3 Focal-InfoNCE for Sentence Embedding",
            "text": "This study follows the unsupervised SimCSE framework for sentence embedding. Instead of taking the InfoNCE loss in Eq. (1), we introduce a self-paced reweighting objective function, FocalInfoNCE, to up-weight hard negative samples in contrastive learning. Specifically, for each sentence xi, Focal-infoNCE is formulated as\nli = \u2212log e(s\ni p) 2/\u03c4\u2211N j \u0338=i e si,jn (s i,j n +m)/\u03c4 + e(s i p) 2/\u03c4 , (2)\nwhere m is a hardness-aware hyperparameter that offers flexibility in adjusting the re-weighting strategy. Within a mini-batch of N sentences, the final loss function, L = \u2211N i=1 li, can be derived as\nL = \u2212log e \u2211N i=1 (sip) 2/\u03c4\n\u03a0Ni=1[ \u2211N j \u0338=i e si,jn (s i,j n +m)/\u03c4 + e(s i p)\n2/\u03c4 ] (3)\nAnalysis of Focal-InfoNCE: Compare with InfoNCE in Eq. (1), Focal-InfoNCE introduces selfpaced modulation terms on sp and sn, proportional to the similarity quantification. Let\u2019s first focus on the modulation term, si,jn + m, on negative pairs. Prior arts have shown that pre-trained language models usually suffer from anisotropy in sentence embedding (Wang and Isola, 2020). Finetuning the pretrained models with contrastive learning on negative samples, especially hard negative samples, improves uniformity of representations, mitigating the anisotropy issue. In SimCSE, si,jn quantifies the similarity between negatives xi and xj . If si,jn is large, xi and xj are hard negatives for current model. Improving the model with such hard negative pairs encourage representation\u2019s uniformity. To this end, we propose to upweight the corresponding term si,jn /\u03c4 by a modulation factor s i,j n +m. The partial derivative of Focal-InforNCE with respect to si,jn is\n\u2202L\n\u2202si,jn = N\u2211 j \u0338=i 2 \u03c4 es i,j n (s i,j n +m)/\u03c4 Zi (si,jn +m), (4)\nwhere Zi = \u2211N j \u0338=i e si,jn (s i,j n +m)/\u03c4 + e(s i p)\n2/\u03c4 . According to Eq. (4), comparing to easy negatives, hard negative samples that associates with higher similarity score si,jn contribute more to the loss function. This implies that a model optimized with the proposed Focal-InfoNCE focuses more on hardnegative samples. Our experiments also show that\nFocal-InfoNCE improves uniformity in sentence embeddings.\nTo uncover the insight of the modulation term sip on positive cases, let\u2019s revisit SimCSE. In SimCSE, the positive pair is formed by dropout with random masking. Thus a low similarity score sp indicates semantic information loss introduced by dropout. Since such a low similarity is not attributed to model\u2019s representation capability, we should mitigate its effect on model optimization. Hence, Focal-inforNCE assigns a small weight to the dissimilary positive pair. The partial derivative with respect to si,jn is\n\u2202L \u2202sip = 2 \u03c4 ( e(s\ni p) 2/\u03c4 Zi \u2212 1)sip, (5)\nwhich suggests that positive pairs with lower similarity scores in SimCSE contributes less to model optimization. We show in the experiments that Focal-InfoNCE improves the allignment of sentense embeddings as well.\nDue to the modulation terms on both positive and negative samples, Focal-InfoNCE reduces the chances of the model getting stuck in sub-optimal solutions dominated by easy pairs. We show in our experiment that the proposed Focal-InfoNCE can easily fit into most contrastive training frameworks for sentence embeddings."
        },
        {
            "heading": "4 Experiments",
            "text": "We evaluate Focal-InfoNCE on 7 semantic similarity tasks: STS 12-16 (Agirre et al., 2012, 2013, 2014, 2015, 2016), STS Benchmark (Cer et al., 2017) and SICK-Relatedness (Marelli et al., 2014). Spearman\u2019s correlation between predicted and ground truth scores is used as the numerical performance metric. Our implementation closely follows unsupervised-SimCSE (Gao et al., 2021). Briefly, starting with pre-trained models BERTbase, BERTlarge (Devlin et al., 2018) and RoBERTabase, and RoBERTalarge (Liu et al., 2019), we take the [cls] embeddings as the sentence representations and fine-tune the models with 106 randomly sampled English Wikipedia sentences. The hyperparameter \u03c4 for the four models are {0.7, 0.7, 0.5, 0.5} respectively and we use m = 0.3 in this experiment. To make a fair comparison, we adopted the same batch size and learning rate as unsupervisedSimCSE, shown in Table 1."
        },
        {
            "heading": "4.1 Comparison to Prior Arts",
            "text": "Table 2 shows the performance of the different models with and without the Focal-InfoNCE loss. In general, we observe improvements in Spearman\u2019s correlation scores when incorporating the proposed Focal-InfoNCE. For example, with SimCSE-BERTbase, the average score increases from 75.68 to 77.32 when using Focal-InfoNCE."
        },
        {
            "heading": "4.2 Alignment and Uniformity",
            "text": "Alignment and uniformity are two key properties to measuring the quality of contrastive representations (Gao et al., 2021). By specifically focusing on challenging negative samples, focal-InfoNCE encourages the model to pay closer attention to negative instances that are difficult to distinguish from positive pairs. In Table. 3, we incorporate the proposed focal-InfoNCE into different contrastive learning for sentence embeddings and show improvements in both alignment and uniformity of the resulting representations."
        },
        {
            "heading": "4.3 Ablation Studies on Hyperparameters",
            "text": "We conducted ablation studies to analyze two key factors in Focal-InfoNCE: temperature \u03c4 and the hardness hyperparameter m.\nTemperature \u03c4 is a hyper-parameter used in the InfoNCE loss function that scales the logits before computing the probabilities. (Wang and Liu, 2021) show that the temperature plays a key role in controlling the strength of penalties on hard negative samples. In this ablation, we set the temperature as 0.03, 0.05, 0.07, and 0.1, explore the effect of different temperature values on the model\u2019s performance and report the results in Table 4.\nThe hardness hyper-parameter m controls the rescaling of the negative samples in the contrastive loss function. Figure. 1 visualizes the rescaling effects of m. Specifically, our Focal-InfoNCE loss regards negative pairs with cosine similarity larger than (1-m) as hard negative examples and vice versa. The loss is then up-weighted or downweighted proportionally. Table. 5 demonstrates that our method is not sensitive to m and the optimal setting can usually be found between 0.2 and 0.3. One\npossible explanation for this could be the simplicity of the data augmentation employed by SimCSE, resulting in a consistent positive similarity score of approximately 0.85 (Wu et al., 2021a)."
        },
        {
            "heading": "5 Discussion",
            "text": ""
        },
        {
            "heading": "5.1 Qualititave Analysis",
            "text": "We conducted a qualitative analysis as follows. We sampled a subset of hard negative pairs from the STS-B training data and analyzed their characteristics. We noticed that these hard negative pairs often involved sentences with domain-specific terms, or sentences with a high degree of syntactic similarity. For example, sentences like \"This is a very unusual request.\" and \"I think it\u2019s fine to ask this question.\" have a notable cosine similarity of 0.64 by the pre-trained BERT base model, though their\nunderlying tones and attitudes are completely different. The proposed Focal InfoNCE effectively guides the model\u2019s attention by putting more penalties toward these cases and encourages the model to learn a better sentence embedding to separate them."
        },
        {
            "heading": "5.2 Statistical Significance Analysis",
            "text": "To validate the statistical significance of performance increase, we conducted three more sets of experiments, each with different random seed, and\nreported the mean and standard deviation (std) in Table 2. Based on the results, we calculated pairedt tests with a standard significance level of 0.05. All p-values over the various STS tasks with different base models are smaller than 0.05, which indicate that our focal-InfoNCE improves the performance significantly in the statistical sense."
        },
        {
            "heading": "5.3 Compatibility with Existing Methods",
            "text": "As reported in Table 3, focal-InfoNCE improves DiffCSE in terms of representation alignment, uniformity, and Spearman\u2019s correlation. In contrast, our attempts to integrate focal-InfoNCE with PromptBERT(Jiang et al., 2022) did not yield any improvements over the baseline models. We hypothesize that this outcome could be attributed to the different treatments of noise in positive pairs. In SimCSE and DiffCSE(Chuang et al., 2022), positive pairs are generated with the same template but different dropout noise. Our focal-InfoNCE downweights the false positive pairs introduced by the random masking mechanism. But in PromptBERT where different positive templates are used for contrastive learning, the template biases have been incorporated in the contrastive loss. Consequently, downweighting false positive pairs with focal-InfoNCE might be unnecessary on PromptBERT."
        },
        {
            "heading": "6 Conclusions",
            "text": "This paper introduced a novel unsupervised contrastive learning objective function, FocalInfoNCE, to enhance the quality of sentence embeddings. By combining SimCSE with self-paced hard negative re-weighting, model optimization was benefited from hard negatives. Extensive experiments shows the effectiveness of the proposed method on various STS benchmarks."
        },
        {
            "heading": "7 Limitations",
            "text": "The effectiveness of the Focal-InfoNCE depends on the quality of pre-trained models. When a pretrained model leads to bad representations, the similarity scores may mislead model finetuing. In addition, the positive re-weighting strategy in this study is quite simple. We believe that more sophisticated mechanisms to address semantic information loss in positive pairs would further improve the performance."
        }
    ],
    "title": "Improving Contrastive Learning of Sentence Embeddings with Focal-InfoNCE",
    "year": 2023
}