{
    "abstractText": "Text-video based multimodal event extraction refers to identifying event information from the given text-video pairs. Existing methods predominantly utilize video appearance features (VAF) and text sequence features (TSF) as input information. Some of them employ contrastive learning to align VAF with the event types extracted from TSF. However, they disregard the motion representations in videos and the optimization of contrastive objective could be misguided by the background noise from RGB frames. We observe that the same event triggers correspond to similar motion trajectories, which are hardly affected by the background noise. motivated by this, we propose a Three Stream Multimodal Event Extraction framework (TSEE) that simultaneously utilizes the features of text sequence and video appearance, as well as the motion representations to enhance the event extraction capacity. Firstly, we extract the optical flow features (OFF) as motion representations from videos to incorporate with VAF and TSF. Then we introduce a Multi-level Event Contrastive Learning module to align the embedding space between OFF and event triggers, as well as between event triggers and types. Finally, a Dual Querying Text module is proposed to enhance the interaction between modalities. Experimental results show that TSEE outperforms the state-of-theart methods, which demonstrates its superiority.",
    "authors": [
        {
            "affiliations": [],
            "name": "Jiaqi Li"
        },
        {
            "affiliations": [],
            "name": "Chuanyi Zhang"
        },
        {
            "affiliations": [],
            "name": "Miaozeng Du"
        },
        {
            "affiliations": [],
            "name": "Dehai Min"
        },
        {
            "affiliations": [],
            "name": "Yongrui Chen"
        },
        {
            "affiliations": [],
            "name": "Guilin Qi"
        }
    ],
    "id": "SP:8be1c8e3cb02bf6f1e83a1951fe33f81ae0f274f",
    "references": [
        {
            "authors": [
                "Joao Carreira",
                "Andrew Zisserman."
            ],
            "title": "Quo vadis, action recognition? a new model and the kinetics dataset",
            "venue": "CVPR.",
            "year": 2017
        },
        {
            "authors": [
                "Kelvin CK Chan",
                "Shangchen Zhou",
                "Xiangyu Xu",
                "Chen Change Loy."
            ],
            "title": "Basicvsr++: Improving video super-resolution with enhanced propagation and alignment",
            "venue": "CVPR.",
            "year": 2022
        },
        {
            "authors": [
                "Brian Chen",
                "Xudong Lin",
                "Christopher Thomas",
                "Manling Li",
                "Shoya Yoshida",
                "Lovish Chum",
                "Heng Ji",
                "ShihFu Chang."
            ],
            "title": "Joint multimedia event extraction from video and article",
            "venue": "EMNLP Findings.",
            "year": 2021
        },
        {
            "authors": [
                "Xiang Chen",
                "Ningyu Zhang",
                "Lei Li",
                "Shumin Deng",
                "Chuanqi Tan",
                "Changliang Xu",
                "Fei Huang",
                "Luo Si",
                "Huajun Chen."
            ],
            "title": "Hybrid transformer with multi-level fusion for multimodal knowledge graph completion",
            "venue": "SIGIR.",
            "year": 2022
        },
        {
            "authors": [
                "Xiang Chen",
                "Ningyu Zhang",
                "Lei Li",
                "Yunzhi Yao",
                "Shumin Deng",
                "Chuanqi Tan",
                "Fei Huang",
                "Luo Si",
                "Huajun Chen."
            ],
            "title": "Good visual guidance makes a better extractor: Hierarchical visual prefix for multimodal entity and relation extraction",
            "venue": "arXiv preprint",
            "year": 2022
        },
        {
            "authors": [
                "Wenting Song",
                "Huajun Chen."
            ],
            "title": "Meaformer: Multi-modal entity alignment transformer for meta modality hybrid",
            "venue": "arXiv preprint arXiv:2212.14454.",
            "year": 2022
        },
        {
            "authors": [
                "Zhuo Chen",
                "Lingbing Guo",
                "Yin Fang",
                "Yichi Zhang",
                "Jiaoyan Chen",
                "Jeff Z Pan",
                "Yangning Li",
                "Huajun Chen",
                "Wen Zhang."
            ],
            "title": "Rethinking uncertainly missing and ambiguous visual modality in multi-modal entity alignment",
            "venue": "arXiv preprint arXiv:2307.16210.",
            "year": 2023
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "arXiv preprint arXiv:1810.04805.",
            "year": 2018
        },
        {
            "authors": [
                "Alexey Dosovitskiy",
                "Philipp Fischer",
                "Eddy Ilg",
                "Philip Hausser",
                "Caner Hazirbas",
                "Vladimir Golkov",
                "Patrick Van Der Smagt",
                "Daniel Cremers",
                "Thomas Brox."
            ],
            "title": "Flownet: Learning optical flow with convolutional networks",
            "venue": "ICCV.",
            "year": 2015
        },
        {
            "authors": [
                "Beliz Gunel",
                "Jingfei Du",
                "Alexis Conneau",
                "Ves Stoyanov."
            ],
            "title": "Supervised contrastive learning for pretrained language model fine-tuning",
            "venue": "arXiv preprint arXiv:2011.01403.",
            "year": 2020
        },
        {
            "authors": [
                "Junjie Huang",
                "Wei Zou",
                "Zheng Zhu",
                "Jiagang Zhu."
            ],
            "title": "An efficient optical flow based motion detection method for non-stationary scenes",
            "venue": "CCDC. IEEE.",
            "year": 2019
        },
        {
            "authors": [
                "Eddy Ilg",
                "Nikolaus Mayer",
                "Tonmoy Saikia",
                "Margret Keuper",
                "Alexey Dosovitskiy",
                "Thomas Brox"
            ],
            "title": "Flownet 2.0: Evolution of optical flow estimation with deep networks",
            "year": 2017
        },
        {
            "authors": [
                "Dan Iter",
                "Kelvin Guu",
                "Larry Lansing",
                "Dan Jurafsky."
            ],
            "title": "Pretraining with contrastive sentence objectives improves discourse performance of language models",
            "venue": "arXiv preprint arXiv:2005.10389.",
            "year": 2020
        },
        {
            "authors": [
                "Huaizu Jiang",
                "Erik Learned-Miller."
            ],
            "title": "Dcvnet: Dilated cost volume networks for fast optical flow",
            "venue": "WACV.",
            "year": 2023
        },
        {
            "authors": [
                "Aakash Kaku",
                "Sahana Upadhya",
                "Narges Razavian."
            ],
            "title": "Intermediate layers matter in momentum contrastive self supervised learning",
            "venue": "NIPS, 34:24063\u2013 24074.",
            "year": 2021
        },
        {
            "authors": [
                "Prannay Khosla",
                "Piotr Teterwak",
                "Chen Wang",
                "Aaron Sarna",
                "Yonglong Tian",
                "Phillip Isola",
                "Aaron Maschinot",
                "Ce Liu",
                "Dilip Krishnan."
            ],
            "title": "Supervised contrastive learning",
            "venue": "NIPS.",
            "year": 2020
        },
        {
            "authors": [
                "Minseon Kim",
                "Jihoon Tack",
                "Sung Ju Hwang."
            ],
            "title": "Adversarial self-supervised contrastive learning",
            "venue": "NIPS.",
            "year": 2020
        },
        {
            "authors": [
                "Seonhoon Kim",
                "Seohyeong Jeong",
                "Eunbyul Kim",
                "Inho Kang",
                "Nojun Kwak."
            ],
            "title": "Self-supervised pretraining and contrastive representation learning for multiple-choice video qa",
            "venue": "AAAI.",
            "year": 2021
        },
        {
            "authors": [
                "Manling Li",
                "Ruochen Xu",
                "Shuohang Wang",
                "Luowei Zhou",
                "Xudong Lin",
                "Chenguang Zhu",
                "Michael Zeng",
                "Heng Ji",
                "Shih-Fu Chang."
            ],
            "title": "Clip-event: Connecting text and images with event structures",
            "venue": "CVPR.",
            "year": 2022
        },
        {
            "authors": [
                "Manling Li",
                "Alireza Zareian",
                "Qi Zeng",
                "Spencer Whitehead",
                "Di Lu",
                "Heng Ji",
                "Shih-Fu Chang."
            ],
            "title": "Cross-media structured common space for multimedia event extraction",
            "venue": "ACL.",
            "year": 2020
        },
        {
            "authors": [
                "Ruiyu Li",
                "Makarand Tapaswi",
                "Renjie Liao",
                "Jiaya Jia",
                "Raquel Urtasun",
                "Sanja Fidler."
            ],
            "title": "Situation recognition with graph neural networks",
            "venue": "ICCV.",
            "year": 2017
        },
        {
            "authors": [
                "Fan Liu",
                "Delong Chen",
                "Xiaoyu Du",
                "Ruizhuo Gao",
                "Feng Xu."
            ],
            "title": "Mep-3m: A large-scale multimodal e-commerce product dataset",
            "venue": "Pattern Recognition.",
            "year": 2023
        },
        {
            "authors": [
                "Fan Liu",
                "Delong Chen",
                "Fei Wang",
                "Zewen Li",
                "Feng Xu."
            ],
            "title": "Deep learning based single sample face recognition: a survey",
            "venue": "Artificial Intelligence Review.",
            "year": 2023
        },
        {
            "authors": [
                "Jian Liu",
                "Yubo Chen",
                "Kang Liu",
                "Wei Bi",
                "Xiaojiang Liu."
            ],
            "title": "Event extraction as machine reading comprehension",
            "venue": "EMNLP.",
            "year": 2020
        },
        {
            "authors": [
                "Jian Liu",
                "Yubo Chen",
                "Kang Liu",
                "Jun Zhao."
            ],
            "title": "Neural cross-lingual event detection with minimal parallel resources",
            "venue": "EMNLP.",
            "year": 2019
        },
        {
            "authors": [
                "Jian Liu",
                "Yufeng Chen",
                "Jinan Xu."
            ],
            "title": "Multimedia event extraction from news with a unified contrastive learning framework",
            "venue": "MM.",
            "year": 2022
        },
        {
            "authors": [
                "Shuaicheng Liu",
                "Kunming Luo",
                "Nianjin Ye",
                "Chuan Wang",
                "Jue Wang",
                "Bing Zeng."
            ],
            "title": "Oiflow: Occlusion-inpainting optical flow estimation by unsupervised learning",
            "venue": "TIP.",
            "year": 2021
        },
        {
            "authors": [
                "Yaojie Lu",
                "Qing Liu",
                "Dai Dai",
                "Xinyan Xiao",
                "Hongyu Lin",
                "Xianpei Han",
                "Le Sun",
                "Hua Wu."
            ],
            "title": "Unified structure generation for universal information extraction",
            "venue": "ACL.",
            "year": 2022
        },
        {
            "authors": [
                "Ao Luo",
                "Fan Yang",
                "Kunming Luo",
                "Xin Li",
                "Haoqiang Fan",
                "Shuaicheng Liu."
            ],
            "title": "Learning optical flow with adaptive graph reasoning",
            "venue": "AAAI.",
            "year": 2022
        },
        {
            "authors": [
                "R\u00e9mi Marsal",
                "Florian Chabot",
                "Ang\u00e9lique Loesch",
                "Hichem Sahbi."
            ],
            "title": "Brightflow: Brightness-changeaware unsupervised learning of optical flow",
            "venue": "WACV.",
            "year": 2023
        },
        {
            "authors": [
                "Thien Huu Nguyen",
                "Kyunghyun Cho",
                "Ralph Grishman."
            ],
            "title": "Joint event extraction via recurrent neural networks",
            "venue": "NAACL.",
            "year": 2016
        },
        {
            "authors": [
                "Thien Huu Nguyen",
                "Ralph Grishman."
            ],
            "title": "Event detection and domain adaptation with convolutional neural networks",
            "venue": "ACL, pages 365\u2013371.",
            "year": 2015
        },
        {
            "authors": [
                "Sarah Pratt",
                "Mark Yatskar",
                "Luca Weihs",
                "Ali Farhadi",
                "Aniruddha Kembhavi."
            ],
            "title": "Grounded situation recognition",
            "venue": "ECCV. Springer.",
            "year": 2020
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J Liu."
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "JMLR.",
            "year": 2020
        },
        {
            "authors": [
                "Arka Sadhu",
                "Tanmay Gupta",
                "Mark Yatskar",
                "Ram Nevatia",
                "Aniruddha Kembhavi."
            ],
            "title": "Visual semantic role labeling for video understanding",
            "venue": "CVPR.",
            "year": 2021
        },
        {
            "authors": [
                "Xiaohui Song",
                "Longtao Huang",
                "Hui Xue",
                "Songlin Hu."
            ],
            "title": "Supervised prototypical contrastive learning for emotion recognition in conversation",
            "venue": "arXiv preprint arXiv:2210.08713.",
            "year": 2022
        },
        {
            "authors": [
                "Deqing Sun",
                "Xiaodong Yang",
                "Ming-Yu Liu",
                "Jan Kautz."
            ],
            "title": "Pwc-net: Cnns for optical flow using pyramid, warping, and cost volume",
            "venue": "CVPR.",
            "year": 2018
        },
        {
            "authors": [
                "Zachary Teed",
                "Jia Deng."
            ],
            "title": "Raft: Recurrent all-pairs field transforms for optical flow",
            "venue": "ECCV. Springer.",
            "year": 2020
        },
        {
            "authors": [
                "Meihan Tong",
                "Shuai Wang",
                "Yixin Cao",
                "Bin Xu",
                "Juanzi Li",
                "Lei Hou",
                "Tat-Seng Chua."
            ],
            "title": "Image enhanced event detection in news articles",
            "venue": "AAAI.",
            "year": 2020
        },
        {
            "authors": [
                "Laurens Van der Maaten",
                "Geoffrey Hinton."
            ],
            "title": "Visualizing data using t-sne",
            "venue": "JMLR.",
            "year": 2008
        },
        {
            "authors": [
                "David Wadden",
                "Ulme Wennberg",
                "Yi Luan",
                "Hannaneh Hajishirzi."
            ],
            "title": "Entity, relation, and event extraction with contextualized span representations",
            "venue": "arXiv preprint arXiv:1909.03546.",
            "year": 2019
        },
        {
            "authors": [
                "C. Strassel S. Medero J. Maeda K. Walker"
            ],
            "title": "Ace 2005 multilingual training corpus",
            "venue": "LDC.",
            "year": 2006
        },
        {
            "authors": [
                "Chenguang Wang",
                "Xiao Liu",
                "Zui Chen",
                "Haoyun Hong",
                "Jie Tang",
                "Dawn Song."
            ],
            "title": "Deepstruct: Pretraining of language models for structure prediction",
            "venue": "ACL.",
            "year": 2022
        },
        {
            "authors": [
                "Shuo Wang",
                "Meizhi Ju",
                "Yunyan Zhang",
                "Yefeng Zheng",
                "Meng Wang",
                "Guilin Qi."
            ],
            "title": "Cross-modal contrastive learning for event extraction",
            "venue": "DASFAA. Springer.",
            "year": 2023
        },
        {
            "authors": [
                "Ziqi Wang",
                "Xiaozhi Wang",
                "Xu Han",
                "Yankai Lin",
                "Lei Hou",
                "Zhiyuan Liu",
                "Peng Li",
                "Juanzi Li",
                "Jie Zhou."
            ],
            "title": "Cleve: Contrastive pre-training for event extraction",
            "venue": "arXiv preprint arXiv:2105.14485.",
            "year": 2021
        },
        {
            "authors": [
                "Patrick Y Wu",
                "Richard Bonneau",
                "Joshua A Tucker",
                "Jonathan Nagler."
            ],
            "title": "Dictionary-assisted supervised contrastive learning",
            "venue": "arXiv preprint arXiv:2210.15172.",
            "year": 2022
        },
        {
            "authors": [
                "Sai Yang",
                "Fan Liu",
                "Delong Chen",
                "Jun Zhou."
            ],
            "title": "Few-shot classification via ensemble learning with multi-order statistics",
            "venue": "arXiv preprint arXiv:2305.00454.",
            "year": 2023
        },
        {
            "authors": [
                "Sai Yang",
                "Fan Liu",
                "Shaoqiu Zheng",
                "Ying Tan."
            ],
            "title": "Jlcsr: Joint learning of compactness and separability representations for few-shot classification",
            "venue": "IEEE Transactions on Cognitive and Developmental Systems.",
            "year": 2023
        },
        {
            "authors": [
                "Shunyu Yao",
                "Jian Yang",
                "Xiangqun Lu",
                "Kai Shuang."
            ],
            "title": "Contrastive learning for event extraction",
            "venue": "ICML, pages 167\u2013172.",
            "year": 2022
        },
        {
            "authors": [
                "Mark Yatskar",
                "Luke Zettlemoyer",
                "Ali Farhadi."
            ],
            "title": "Situation recognition: Visual semantic role labeling for image understanding",
            "venue": "CVPR.",
            "year": 2016
        },
        {
            "authors": [
                "Zhenrui Yue",
                "Huimin Zeng",
                "Bernhard Kratzwald",
                "Stefan Feuerriegel",
                "Dong Wang."
            ],
            "title": "Qa domain adaptation using hidden space augmentation and selfsupervised contrastive adaptation",
            "venue": "arXiv preprint arXiv:2210.10861.",
            "year": 2022
        },
        {
            "authors": [
                "Ningyu Zhang",
                "Luoqiu Li",
                "Xiang Chen",
                "Shumin Deng",
                "Zhen Bi",
                "Chuanqi Tan",
                "Fei Huang",
                "Huajun Chen."
            ],
            "title": "Differentiable prompt makes pre-trained language models better few-shot learners",
            "venue": "arXiv preprint arXiv:2108.13161.",
            "year": 2021
        },
        {
            "authors": [
                "Tongtao Zhang",
                "Spencer Whitehead",
                "Hanwang Zhang",
                "Hongzhi Li",
                "Joseph Ellis",
                "Lifu Huang",
                "Wei Liu",
                "Heng Ji",
                "Shih-Fu Chang."
            ],
            "title": "Improving event extraction via multimodal integration",
            "venue": "MM.",
            "year": 2017
        },
        {
            "authors": [
                "Zihua Zheng",
                "Ni Nie",
                "Zhi Ling",
                "Pengfei Xiong",
                "Jiangyu Liu",
                "Hao Wang",
                "Jiankun Li."
            ],
            "title": "Dip: Deep inverse patchmatch for high-resolution optical flow",
            "venue": "CVPR.",
            "year": 2022
        },
        {
            "authors": [
                "Mohammadreza Zolfaghari",
                "Yi Zhu",
                "Peter Gehler",
                "Thomas Brox."
            ],
            "title": "Crossclr: Cross-modal contrastive learning for multi-modal video representations",
            "venue": "ICCV.",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Event extraction (EE) is a fundamental task which aims to recognize the event structure from texts (Nguyen et al., 2016; Nguyen and Grishman, 2015; Wadden et al., 2019; Lu et al., 2022). Recent years have witnessed the booming of the multimodal event extraction (MEE). MEE (Pratt et al., 2020; Sadhu et al., 2021; Li et al., 2017) extends EE\n\u2217 J. Li and C. Zhang contributed equally to this work and should be considered co-first authors.\n\u2020 Corresponding author.\nby merging complementary information from multiple modalities such as texts, images or videos. Specifically, texts provide abstract semantics while visual data supplies concrete instances(Liu et al., 2023a,b; Yang et al., 2023a,b). Compared with text-image based MEE (TIMEE) (Li et al., 2022; Liu et al., 2022; Li et al., 2020; Zhang et al., 2017; Tong et al., 2020), text-video based MEE (TVMEE) (Chen et al., 2021; Wang et al., 2023) contains more context and scene information. Moreover, TVMEE presents temporal data that could capture the dynamic evolution of events, making it an area of significant interest.\nExisting methods in TVMEE (Chen et al., 2021; Wang et al., 2023) extract text sequence features (TSF) and video appearance features (VAF) from texts and RGB frames by adopting pre-trained language and video models respectively. However,\nthey neglect the motion representations in videos. In TVMEE, motion representations may play an important role, as they furnish details on the motion and behavior of objects in videos. Furthermore, we observe that identical event triggers correspond to analogous motion representations. To explore the relationship between motion representations and event triggers, we introduce the optical flow features (OFF) (Dosovitskiy et al., 2015) as object motion representations. OFF (Ilg et al., 2017; Sun et al., 2018; Jiang and Learned-Miller, 2023; Marsal et al., 2023; Liu et al., 2021) represents the movement of objects in a sequence between consecutive frames and is extensively applied in video fields, such as video understanding (Teed and Deng, 2020; Luo et al., 2022), video superresolution (Zheng et al., 2022; Chan et al., 2022), etc. As shown in Figure 1, we compare three triggers \u2018rally\u2019, \u2018meeting\u2019 and \u2018arrested\u2019. For each sample we visualize the text, frames and OFF extracted from the corresponding frames. It could be observed that OFF is similar if it refers to the same trigger. In contrast, heterogeneous triggers usually point to dissimilar OFF.\nA previous work (Wang et al., 2023) applies contrastive learning to reduce the distance between VAF and event types. Although VAF extracted from continuous frames may provide useful motion information, it also contains misguiding background noise. To be specific, the background noise is various scenes in heterogeneous videos. It does not provide any event semantics and can suppress the alignment between visual cues and event types. However, this issue could be properly alleviated by utilizing OFF because OFF solely exploits the object motion representations and filters out the scene information.\nIn this work, we design a novel framework, Three Stream Multimodal Event Extraction (TSEE), which simultaneously leverages three modality features (text sequence, video appearance and motion representations) to improve the event extraction capability. To begin with, we employ pre-trained I3D (Carreira and Zisserman, 2017) and PWC (Sun et al., 2018) models to extract VAF and OFF from each video respectively. For the input text, we adopt a pre-trained language model (Devlin et al., 2018; Raffel et al., 2020) to obtain TSF. Then we propose a Multi-level Event Contrastive Learning (MECL) module, aiming to align the feature representations between OFF and event\ntriggers, as well as event types and triggers. We align each pair in the embedding space by introducing a multi-level contrastive objective. Lastly, we propose a Dual Querying Text (DQT) module to increase the interaction between modalities. In this module, VAF and OFF retrieve the cross-modality attention of each token in TSF respectively.\nThe contributions of our work could be summarized as follows:\n\u2022 We propose a novel framework called TSEE that leverages the motion representations in videos. To the best of our knowledge, we are the first to introduce optical flow features into TVMEE.\n\u2022 Our proposed modules, MECL and DQT, significantly improve the model performance. MECL aligns the embedding space of OFF, event triggers and types. DQT enhances the interaction among text, video and optical flow modalities.\n\u2022 The experimental results on two benchmark datasets demonstrate the superiority of our framework over the state-of-the-art methods."
        },
        {
            "heading": "2 Related Work",
            "text": ""
        },
        {
            "heading": "2.1 Event Extraction",
            "text": "In the field of event extraction research, the initial work primarily focused on sentence-level studies in the text. Some works have explored the use of convolutional neural networks (Nguyen and Grishman, 2015; Nguyen et al., 2016), recurrent neural networks (Nguyen and Grishman, 2015; Liu et al., 2019, 2020), graph neural networks (Li et al., 2017), and later emerging pre-trained language models (Wadden et al., 2019; Wang et al., 2022; Lu et al., 2022) for handling the extraction of triggers and arguments. In the field of computer vision, event extraction is operationalized as situation recognition (Pratt et al., 2020; Sadhu et al., 2021; Yatskar et al., 2016; Li et al., 2017), with tasks primarily involving the classification and extraction of frames containing entities and roles (arguments) from images with actions (visual events)(Zhang et al., 2021; Chen et al., 2022a,b). In recent years, there has been an emergence of using multimodal information for event extraction(Chen et al., 2022c, 2023). (Zhang et al., 2017) demonstrated the effectiveness of using visually based entity data to extract events. Previous multimodal event extraction models (Li et al., 2020; Liu et al., 2022) mostly dealt with visual data in the form of images, (Chen et al., 2021) pioneered a model that can jointly extract events from text and video data. They used\na pre-trained text-video retrieval model to find the most relevant text-video pairs. Based on (Chen et al., 2021)\u2019s approach, (Wang et al., 2023) introduced supervised contrastive learning to enhance the representation of the two modalities for further event extraction."
        },
        {
            "heading": "2.2 Supervised Contrastive Learning",
            "text": "Contrast learning is a technique that trains models to distinguish between similar and different examples. Self-supervised representation learning methods such as (Kim et al., 2020; Yue et al., 2022; Kim et al., 2021; Kaku et al., 2021; Iter et al., 2020) divide each sample into positive and negative samples, learning feature representations by comparing the similarity between sample pairs. Works such as (Gunel et al., 2020; Wu et al., 2022; Gunel et al., 2020; Song et al., 2022) optimize the supervised contrastive objective for supervised contrastive learning. For event extraction tasks, (Wang et al., 2021) proposes a contrastive pre-training framework that uses semantic structures. (Yao et al., 2022) introduces an efficient event extraction model with a contrastive objective to distinguish triggers and arguments. (Zolfaghari et al., 2021) presents a more effective cross-modal contrastive learning loss function, compared to directly using loss functions designed for visual data."
        },
        {
            "heading": "2.3 Optical Flow",
            "text": "Most of the existing methods for extracting optical flow rely on pixel-by-pixel prediction using neural networks. Optical flow extraction models have various model structures, including encoderdecoder architecture (Dosovitskiy et al., 2015), iterative refinement of multiple FlowNet modules using cumulative superposition (Ilg et al., 2017), feature processing and extraction through a pyramid structure (Sun et al., 2018), and construction of cost volumes with different expansion coefficients (Jiang and Learned-Miller, 2023). (Marsal et al., 2023) trains optical flow estimation networks by training two networks that jointly estimate optical flow and brightness changes. (Liu et al., 2021) addresses the optical flow problem of occluded pixels. (Chan et al., 2022) utilizes temporal information and proposes a novel approach to combine optical flow and deformable alignment in videos. (Huang et al., 2019) employs optical flow to solve motion detection issues related to dynamic background and changing foreground appearance."
        },
        {
            "heading": "3 Approach",
            "text": ""
        },
        {
            "heading": "3.1 Task Definition",
            "text": "Given a text-video pair (T, V ), we denote the sequence of input text tokens as T = {t1, t2, ..., tm}. We sample from the video every 16 frames to get the clip sequence V = {c1, c2, ..., ck}. In TVMEE, each sample is annotated a set of event types E = {e1, e2, ...}. Our goal is to jointly extract event triggers and event arguments. An event trigger is a word or phrase in a sentence that indicates the occurrence of an event. For example, in the sentence \u2018John bought a new car yesterday\u2019, the word \u2018bought\u2019 is the event trigger, indicating the occurrence of a buying event. Event argument extraction is to identify relevant pieces of information or arguments from texts. The pieces commonly involve an event, such as subject, object, verb, and other modifiers. Then these roles are mapped to their semantic roles such as agent, patient, location, time, and so on. Take the above sentence as an example, \u2018John\u2019, \u2018car\u2019 and \u2018yesterday\u2019 are the event arguments referring to the buying event and the roles are \u2018buyer\u2019, \u2018product\u2019 and \u2018time\u2019 respectively."
        },
        {
            "heading": "3.2 Feature Extraction",
            "text": "Our framework utilizes information from both text and video features as shown in Figure 2. In particular, the video incorporates features from two perspectives. The first is the video appearance features, which represents color, texture, shape, and other visual cues. Secondly, motion features provide information about dynamics of objects within the scene. We employ corresponding pre-trained models to extract these features respectively. Text feature extraction. The input text tokens are encoded using pre-trained T5-base (Raffel et al., 2020) with dt hidden dimensions. Thus each input sequence is represented as a matrix FT \u2208 Rnl\u00d7dt , where nl is the length of sequence. Video feature extraction. We input each clip sequence into the I3D network pretrained on Kinetics dataset and the PWC network pretrained on Sintel dataset. Then we obtain a sequence of VAF and OFF. To represent a video-level feature, we sum up all the features within the sequence . VAF and OFF are denoted as FV \u2208 Rdv and FO \u2208 Rdo ."
        },
        {
            "heading": "3.3 Multi-level Event Contrastive Learning",
            "text": "We observe that identical event triggers usually involve similar motion representations, which are not affected by background noise. Additionally, in\nthe event extraction, an event type is correlated to various triggers. Motivated by the above observations, we propose a Multi-level Event Contrastive Learning (MECL) module. This module aligns the feature representations between OFF and triggers. The embedding spaces of event types and triggers are also aligned using this module. We apply supervised contrastive learning (Khosla et al., 2020) in this module and define multi-level labels for different event levels.\nEvent type level. Since an event type corresponds to various triggers, we use event types as the anchors for triggers. Our purpose is to push the triggers referring to the identical event type close. In this level, we define all the event types of the dataset event schema as the label set E = {e1, e2, ..., ep}, where p is the number of event types in the dataset event schema.\nEvent trigger level. Considering the same event triggers correspond to similar motion trajectories in videos, we regard the triggers as the anchors for OFF. The label set in this level is all the triggers W = {w1, w2, ...} in the dataset. For each trigger we could obtain the embedding index from pretrained language model as the label index.\nGiven a batch of N samples, we first select the samples annotated with one event type for computing contrastive loss. It is for the reason that if a sample has more than one event, OFF may contain multiple motion trajectories. Thus OFF could not be directly assigned the certain single label of event. After filtering the samples, we obtain a smaller batch of OFF FOc , the trigger words Wc, as\nwell as the corresponding event types Ec. For the Event type level, positive pairs of each event type consist of all referring trigger words and the event type itself. In contrast, the negative pairs comprise irrelevant trigger words and the event type itself. For Event trigger level, each trigger\u2019s positive pairs are composed of optical flow features that point to the trigger and the trigger. Conversely, the negative pairs are made up of optical flow features that are unrelated to the trigger and the trigger itself.\nConsidering the i-th sample in this smaller batch, we first enter wi and ei into a pre-trained T5-base model to obtain respective feature representations:\nzi = T5(wi),\nxi = T5(ei). (1)\nThen we adopt the supervised contrastive learning to optimize contrastive loss of the Event type level and Event trigger level Ltype and Ltrig:\nLtype = \u2212 B\u2211 i=1 log exp(xi \u00b7 zi/\u03c4)\u2211 zl\u2208Wc\\ziexp(x i \u00b7 zl/\u03c4) ,\nLtrig = \u2212 B\u2211 i=1 log exp(zi \u00b7 F iO/\u03c4)\u2211 FuO\u2208FOc\\F i O exp(zi \u00b7 F uO/\u03c4) ,\n(2)\nwhere B is the number of samples after filtering, and \u03c4 is the temperature parameter of supervised contrastive learning. Finally the multi-level loss Lmulti is defined as :\nLmulti = Ltype + Ltrig. (3)\nFormally, the Multi-level Event Contrastive Learning algorithm is shown as Algorithm 1."
        },
        {
            "heading": "3.4 Dual Querying Text",
            "text": "We design a Dual Querying Text (DQT) module to enhance the interaction among three modalities. The intuition is to query TSF which token responds to VAF or OFF. For example, if the input text has the word Police describing the argument of the event, this token would respond to VAF. It is because VAF may contain visual cues whose semantics are close to this argument. To encode the dual queries of TSF, We utilize two transformer architectures. The attention scores of each token reflect the degree of response to VAF or OFF.\nFor TSF, VAF, OFF denoted as FT \u2208 Rnl\u00d7dt , FV \u2208 Rdv and FO \u2208 Rdo in Section 3.2, VAF and OFF are projected into queries respectively. In both transformer architectures, TSF is projected to obtain keys and values. Then we adopt a softmax function to calculate the dual attention weights:\nAv = softmax( FV Hq1H \u22a4 k1 F\u22a4T\u221a\ndt )FTHv1 ,\nAo = softmax( FOHq2H \u22a4 k2 F\u22a4T\u221a\ndt )FTHv2 , (4)\nWhere Hq, Hk, Hv are three projection matrices for query, key and value respectively. The output attention scores are aggregated as follows:\nFA = Av \u00b7 FT +Ao \u00b7 FT (5)"
        },
        {
            "heading": "4 Experiment",
            "text": "Datasets. We evaluate our approach on two openended TVMEE datasets: TVEE (Wang et al., 2023) and VM2E2 (Chen et al., 2021). TVEE dataset contains 7598 text-video pairs. The international news videos with captions are collected from the On Demand News channel. The event schema is from the ACE2005 (Walker, 2006) benchmark that consists of 8 superior event types and 33 event types. Contact.Speech, Disaster.Disaster and Accident.Accident are added to the event schema because the schema in ACE2005 could not cover all the event types in videos. The TVEE dataset is randomly divided into train, valid, and test sets in a ratio of 8:1:1. VM2E2 is a collection of text and video data that includes 13,239 sentences and 860 videos. Within the dataset, there are 562 pairs of sentences and videos that share the same event\nAlgorithm 1 Multi-level Event Contrastive Learning\nRequire: OFF FO, event types E = {e1, e2, ...}, event triggers W = {w1, w2, ...},event type positive pairs Spy = \u2205, event type negative pairs Sny = \u2205, event trigger positive pairs Spg = \u2205, event trigger negative pairs Sng = \u2205, filtering batch Sf = \u2205, supervised contrastive learning function CON.\n1: for (FO, E)i in batch do 2: if Len(Ei)==1 then 3: zi \u2190 T5(wi) 4: xi \u2190 T5(ei) 5: Sf .append(zi, xi, F iO) 6: else 7: CONTINUE 8: end if 9: end for\n10: for (z, x, FO)j in Sf do 11: if zj refers to xj then 12: Spy.append(zj , xj) 13: else 14: Sny.append(zj , xj) 15: end if 16: if F jO refers to z\nj then 17: Spg.append(zj , F j O) 18: else 19: Sng.append(zj , F j O) 20: end if 21: end for 22: Ltype = CON(Spy)+CON(Sny) 23: Ltrig = CON(Spg)+CON(Sng) 24: return Ltype + Ltrig\ntype, with each pair containing only one event. The dataset defines 16 multimodal event types based on the LDC ontology. Following (Chen et al., 2021), we split VM2E2 into 411 and 151 samples.\nEvaluation Metrics. Following (Wang et al., 2023), we utilize the same evaluation metrics to report text, video and multimodal evaluation results. The evaluation metrics include: Precision (P), Recall (R) and F-score (F1). The performance of text event extraction is evaluated by two subtasks: event trigger extraction and event argument extraction. The correctness of a trigger prediction is determined by whether its type and span align with the labels, while for an argument prediction, it is determined by whether its span and all the roles\nalign with the labels.\nImplementation Details. We use Pytorch and a 2080 Ti GPU to implement our framework and conduct experiments. We apply a pre-trained T5base (Raffel et al., 2020), as the TSF encoder. For the video input, we separately adopt pre-trained I3d (Carreira and Zisserman, 2017) and PWC (Sun et al., 2018) to extract VAF and OFF. For the event extraction decoder, we use CRF decoder following (Wang et al., 2023). The dimension of TSF, VAF and OFF are 768, 1024 and 1024 respectively. We utilize a linear function to project the dimension of VAF and OFF to 768. Following (Wang et al., 2023), we train our model for 15 epochs and the batchsize is set 16. The optimizer is Adam and the learning rate is 10e-5. Following (Yao et al., 2022), we utilize 0.3 for the parameter \u03c4 in MECL."
        },
        {
            "heading": "4.1 Baselines",
            "text": "Following (Wang et al., 2023), we compare our model with other methods in three settings, which are Text Event Extraction, Video Event Extraction, Multimodal Event Extraction. Text Event Extraction. For text event extraction, we only utilize text input. We compare the following models in this setting:\n- DEEPSTRUCT (Wang et al., 2022) : It is the state-of-the-art method in text event extraction.\nIt proposes structure pretraining to let language model understand the structure in the text.\n-CoCoEET (Wang et al., 2023) : It uses the text encoder and a CRF decoder of CoCoEE without CoLearner module.\n-TSEET : It utilizes the T5-base encoder and a CRF encoder to extract events with text modality. It is without MECL module and DQT module. Video Event Extraction. We only use video input as the video event extraction. We compare the models as follows:\n-JSL (Pratt et al., 2020) :We follow (Wang et al., 2023) to use a sota model JSL in video event extraction. Key frames are utilized to detect events.\n-CoCoEEV (Wang et al., 2023) : It utilizes the video encoder of CoCoEE and a video event decoder without CoLearner module.\n-TSEEV : It utilizes a pre-trained I3D model to extract video features and the decoder is set the same as (Wang et al., 2023). It is also without MECL module and DQT module. Multimodal Event Extraction. This is our full task setting. We compare the models as follows:\n-JMMT (Chen et al., 2021) :It utilizes a transformer encoder to jointly encode the text and video inputs. The visual features include video-level features and image-level features.\n-CoCoEE (Wang et al., 2023) :It is the state-ofthe-art model in text-video based event extraction.\nIt contrasts the event types and video features."
        },
        {
            "heading": "4.2 Main Results",
            "text": "The experiment results on TVEE and VM2E2 datasets are presented in Table 1. We could find that when the input data only consists of text, DEEPSTRUCT, CoCoEET and TSEET achieve similar performance. Specifically, on TVEE dataset TSEET performs 0.1% and 0.6% better than CoCoEET on F1 of trigger extraction. However, DEEPSTRUCT reaches a higher F1 than TSEET and CoCoEET on VM2E2 dataset. This could be because the ability to extract event information from text of the three models is comparable.\nWhen there is only video data in input, CoCoEEV and TSEEV show the comparable performance and are both better that JSL on two datasets. It is because JSL applies to static frames while CoCoEEV and TSEEV adopt pre-trained models of videos to capture dynamic information.\nThe results of multimodal data input show that TSEE achieve the best performance on most of the evaluation metrics compared with existing stateof-the-art methods. On TVEE dataset, our model reaches 81.5% F1 of trigger extraction compared with 77.1% and 78.5% achieved by JMMT and CoCoEE respectively. This result demonstrates that the integration of motion representations in videos is helpful for the multimodal event extraction task. On VM2E2 dataset, the F1 score of trigger extraction is improved from 47.5% (CoCoEE) to 51.7%, where the improvement is larger than that of TVEE dataset. This may be the reason that in VM2E2 dataset, each sample is annotated with only one event. The MECL module would not filter any sample in every batch when computing contrastive loss, thus obtaining better feature representations and boosting the performance of the model. We\nnotice that JMMT performs well in the recall metric, such as 54.9% argument extraction recall on TVEE dataset and 56.3% trigger extraction recall on VM2E2 dataset. This may be that JMMT utilizes the additional object detection model to inject proposal features of key frames to the transformer encoder, improving the recall of triggers and arguments in samples.\nWe also observe that the results show the similar trends from single modality to multimodal input, which verifies that injecting multimodal input to TSEE and CoCoEE both boosts the performance in all metrics. Specifically, the incorporation of video to TSEEV boosts the F1 performance from 76.4% to 81.5% on TVEE dataset. For CoCoEEV, the F1 score is improved from 76.3% to 78.5%."
        },
        {
            "heading": "4.3 Ablation Study",
            "text": "To validate the effectiveness of different innovations and parts in TSEE, we conduct ablation studies on the two datasets. We investigated three main units of TSEE: (1) Integration of optical flow feature; (2) Multi-level Event Contrastive Learning module; (3) Dual Querying Text module. The baseline in the first line applies sum function to VAF and TSF. The results are summarized in Table 2. Effectiveness of OFF. In this part we extract OFF from video data and sum up VAF, TSF and OFF. From Table 2, we observe that the integration of OFF improves all evaluation metrics over baseline on the two datasets, verifying that OFF provides beneficial information for event extraction. Effectiveness of MECL. To evaluate the influence of MECL module, we utilize MECL module based on the second line of each part. As shown in Table 2, MECL module brings the most improvement to our framework, such as 2.8% trigger extraction F1\nscore on TVEE dataset and 3.7% trigger extraction F1 score on VM2E2 dataset. This demonstrates that MECL module could refine the feature representations and align the features between heterogeneous modalities, thus boosting the performance. Effectiveness of DQT. We also evaluate the impact of DQT module. From Table 2, we could find that DQT module improves the performance of all evaluation metrics significantly on the two datasets. It is worth noting that the recall metric of trigger extraction is boosted from 50.6% to 53.5% on VM2E2 dataset and so is argument extraction recall metric from 25.3% to 27.4%. The reason is perhaps that in the DQT module, each text token is queried by the VAF and OFF, thus enhancing the ability of searching instances contained in videos."
        },
        {
            "heading": "4.4 Visualization of T-SNE for MECL",
            "text": "To verify the impact of MECL module, we use t-SNE (Van der Maaten and Hinton, 2008) to visualize the manifold of TSEE with and without MECL module. Our MECL module is designed to reduce the distance between OFF and event triggers. We randomly sampled 1500 OFF trained with or without MECL module on TVEE dataset. The visualization results are shown in Figure 3, where OFF belonging to the same trigger is marked in the same color. It could be clearly seen that OFF trained with MECL module in subfigure is obviously separated into various compact clusters. However, when OFF is not trained with MECL module, there is no distinctiveness between OFF belonging to different triggers. This result demonstrates that our MECL module does well in aligning the semantics of different modalities."
        },
        {
            "heading": "4.5 Case Study on DQT",
            "text": "In order to intuitively show the effectiveness of DQT module, we conduct case studies on TVEE dataset. As shown in Figure 4, we visualize the attention heatmaps based on the attention scores output by DQT. As DQT utilizes VAF and OFF to query each token in TSF respectively, each sample corresponds to two lists of attention scores. From Figure 4, we could observe that for each sample, the frame appearance or motion related tokens are paid more attention by VAF or OFF. In the first example, When VAF queries police, protesters and march, it gives more attention scores than other tokens. We could also observe that OFF attends to the clashed most. In the second example, each token is allocated similar attention score by VAF. This may be the reason that the pre-trained I3D model does not have the knowledge of instances such as Trump and Kim. OFF gives a higher attention score to meeting because this token could provide motion information. From the above analysis we can see that DQT module does well in understanding the relationship between multimodal semantics."
        },
        {
            "heading": "5 Conclusion",
            "text": "In this paper, we propose a Three Stream Multimodal Event Extraction framework that explores the utilization of motion representations in textvideo based multimodal event extraction (TVMEE) tasks. Optical flow features are extracted from videos as motion representations to incorporate with other modalities. To improve alignment among feature representations, we propose a Multilevel Event Contrastive Learning module. A Dual Querying Text module is also designed to help en-\nhance the interaction between different modalities. TSEE achieves the state-of-the-art results on two datasets, demonstrating the effectiveness of our framework. In future work, we will explore the utilization of large language model (LLM) in fusing modality features to boost TIMEE performance.\nLimitations\nThe main limitation of our work is the offline training. As the insufficiency of GPU resources, we need to extract the VAF and OFF in advance and could not optimize the video pre-trained model online. The other limitation is the inapplicability of open-domain event extraction. As both two datasets are annotated in a close-domain event set, our framework can not deal with open-domain event extraction."
        },
        {
            "heading": "Acknowledgements",
            "text": "This work is partially supported by National Nature Science Foundation of China under No. U21A20488, 62302149 and 62372155. We thank the Big Data Computing Center of Southeast University for providing the facility support on the numerical calculations in this paper."
        }
    ],
    "title": "Three Stream Based Multi-level Event Contrastive Learning for Text-Video Event Extraction",
    "year": 2023
}