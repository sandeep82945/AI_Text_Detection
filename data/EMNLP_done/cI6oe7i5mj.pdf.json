{
    "abstractText": "Markets and policymakers around the world hang on the consequential monetary policy decisions made by the Federal Open Market Committee (FOMC). Publicly available textual documentation of their meetings provides insight into members\u2019 attitudes about the economy. We use GPT-4 to quantify dissent among members on the topic of inflation. We find that transcripts and minutes reflect the diversity of member views about the macroeconomic outlook in a way that is lost or omitted from the public statements. In fact, diverging opinions that shed light upon the committee\u2019s \u201ctrue\u201d attitudes are almost entirely omitted from the final statements. Hence, we argue that forecasting FOMC sentiment based solely on statements will not sufficiently reflect dissent among the hawks and doves. 1 The Road to FOMC Transparency The Federal Open Market Committee (FOMC) is responsible for controlling inflation in the United States, using instruments which dramatically affect the housing and financial markets, among others. For most of the 20th century, conventional wisdom held that monetary policy is most effective when decision-making was shrouded in secrecy; the tightlipped Alan Greenspan, a past chairman of the Fed, quipped about \u201clearning to mumble with great incoherence.\u201d But times change. Blinder et al. (2008) show how the emergence of greater transparency and strategic communication became an important feature of 21st century central banking. Fed communication is now an integral component of monetary policy, and \u201cFed watchers\u201d dote on every word. The FOMC first started releasing public statements following their meetings in February 1994. This meager documentation grew and now consists of three types for each official meeting: carefully produced and highly stylized Figure 1: The transcripts (left) contains opinions and disagreements but statements (right) are concise. We analyze both datasets using GPT-4 prompting. one page statements are released immediately after each FOMC meeting, followed about three weeks later by lengthier minutes, and finally five years later by full, verbatim transcripts. Subsequently this triplet is referred to as documents. We find minutes closely reflect the content of transcripts, so to avoid redundancy, we focus our analysis on transcripts and statements. Increased FOMC communication has prompted social science research spanning the disciplines of economics, sociology, finance and political science (Section 2.1). Financial market participants are also keenly interested. Billions, if not trillions of dollars are traded on the Fed\u2019s words. The interpretations\u2014 right or wrong\u2014of what the FOMC \u201creally means\u201d move markets and affect the economy. However, relying upon documents as data in the social sciences is a challenge due to the lack of structure and the cost of annotation (Grimmer and Stewart, 2013; Gentzkow et al., 2019; Ash and Hansen, 2023). Hansen and Kazinnik (2023) show that Generative Pre-training Transformer (GPT) models outperform a suite of commonly used NLP methods on text quantification. Motivated by these results, we set out to quantify the language of the FOMC using GPT-4 (OpenAI, 2023) by preparing a combined data set of FOMC documents from 1994Figure 2: Text taken from an FOMC meeting on December 11, 2001. A dovish statement does not reflect the hawkish sentiment of Speaker 1. GPT-4 can quantify dissent lost from transcripts to statements. 2016 (Section 2).1 We conclude that transcripts contain more dissent than statements (Section 3). 2 FOMC Data: Transcript to Statement The FOMC normally meets eight times per year in order to assess current economic conditions, ultimately deciding upon the path for monetary policy. We aggregate and release the official publicly available text documenting these deliberations by the Fed as an aligned corpora of documents from 1994 to 2016.2 These text documents are similar in content, but transcripts and statements are dramatically different in style and detail (Figure 1).3 For our purposes, the statements required no pre-processing. For the transcripts, we use regular expressions to partition and then re-aggregate the text by each unique speaker. See Appendix D for an example of each document type. 2.1 Lessons from Social Science Past work has used at most one form of FOMC meeting documentation, but rarely multiple in conjunction. For example, in the finance literature, https://github.com/DenisPeskoff/FedNLP https://www.federalreserve.gov/ monetarypolicy/fomc_historical.htm Transcripts only became being released in 1994 and due to the 5 year lag the latest transcripts are not yet available. Mazis and Tsekrekos (2017) apply Latent Semantic Analysis to FOMC statements to identify the main \"themes\" used by the committee and how well they explain variation in treasury yields. Gu et al. (2022) use minutes to investigate how the tonality of committee deliberations impacts subsequent stock market valuations. Political scientists use transcripts to estimate committee members\u2019 preferences on inflation and unemployment (Baerg and Lowe, 2020). Economists have assessed the role of communication in achieving monetary policy objectives by looking at similar documents (Romer and Romer, 2004; Handlan, 2020). H\u00fcpper and Kempa (2023) investigate the extent to which shifting inflation focus is reflected in full transcripts. Edison and Carcel (2021) apply Latent Dirichlet Allocation (LDA) to transcripts to detect the evolution of prominent topics. Hansen et al. (2017) use LDA to quantify transcripts and identify how transparency affects the committee\u2019s deliberations.",
    "authors": [
        {
            "affiliations": [],
            "name": "Denis Peskoff"
        },
        {
            "affiliations": [],
            "name": "Adam Visokay"
        },
        {
            "affiliations": [],
            "name": "Brandon M. Stewart"
        }
    ],
    "id": "SP:404a74a1073e985d7656ad8b79c24ca346e1d9f1",
    "references": [
        {
            "authors": [
                "Nicole Baerg",
                "Will Lowe."
            ],
            "title": "A textual taylor rule: estimating central bank preferences combining topic and scaling methods",
            "venue": "Political Science Research and Methods, 8(1):106\u2013122.",
            "year": 2020
        },
        {
            "authors": [
                "Alan S. Blinder",
                "Michael Ehrmann",
                "Marcel Fratzscher",
                "Jakob De Haan",
                "David-Jan Jansen."
            ],
            "title": "Central bank communication and monetary policy: A survey of theory and evidence",
            "venue": "Journal of Economic Literature, 46(4):910\u201345.",
            "year": 2008
        },
        {
            "authors": [
                "Hali Edison",
                "Hector Carcel."
            ],
            "title": "Text data analysis using latent dirichlet allocation: an application to fomc transcripts",
            "venue": "Applied Economics Letters, 28(1):38\u201342.",
            "year": 2021
        },
        {
            "authors": [
                "Matthew Gentzkow",
                "Bryan Kelly",
                "Matt Taddy."
            ],
            "title": "Text as data",
            "venue": "Journal of Economic Literature, 57(3):535\u201374.",
            "year": 2019
        },
        {
            "authors": [
                "Justin Grimmer",
                "Brandon M. Stewart."
            ],
            "title": "Text as data: The promise and pitfalls of automatic content analysis methods for political texts",
            "venue": "Political Analysis, 21(3):267\u2013297.",
            "year": 2013
        },
        {
            "authors": [
                "Chen Gu",
                "Denghui Chen",
                "Raluca Stan",
                "Aizhong Shen."
            ],
            "title": "It is not just what you say, but how you say it: Why tonality matters in central bank communication",
            "venue": "Journal of Empirical Finance, 68:216\u2013231.",
            "year": 2022
        },
        {
            "authors": [
                "Zihan Guan",
                "Zihao Wu",
                "Zhengliang Liu",
                "Dufan Wu",
                "Hui Ren",
                "Quanzheng Li",
                "Xiang Li",
                "Ninghao Liu"
            ],
            "title": "Cohortgpt: An enhanced gpt for participant recruitment in clinical study",
            "year": 2023
        },
        {
            "authors": [
                "Amy Handlan."
            ],
            "title": "Text shocks and monetary surprises: Text analysis of fomc statements with machine learning",
            "venue": "Published Manuscript.",
            "year": 2020
        },
        {
            "authors": [
                "Anne Lundgaard Hansen",
                "Sophia Kazinnik"
            ],
            "title": "Can chatgpt decipher fedspeak? Available at SSRN",
            "year": 2023
        },
        {
            "authors": [
                "Stephen Hansen",
                "Michael McMahon",
                "Andrea Prat."
            ],
            "title": "Transparency and Deliberation Within the FOMC: A Computational Linguistics Approach",
            "venue": "The Quarterly Journal of Economics, 133(2):801\u2013 870.",
            "year": 2017
        },
        {
            "authors": [
                "C. Hutto",
                "Eric Gilbert."
            ],
            "title": "Vader: A parsimonious rule-based model for sentiment analysis of social media text",
            "venue": "Proceedings of the International AAAI Conference on Web and Social Media, 8(1):216\u2013225.",
            "year": 2014
        },
        {
            "authors": [
                "Florian H\u00fcpper",
                "Bernd Kempa."
            ],
            "title": "Inflation targeting and inflation communication of the federal reserve: Words and deeds",
            "venue": "Journal of Macroeconomics, 75:103497.",
            "year": 2023
        },
        {
            "authors": [
                "Will Lowe",
                "Kenneth Benoit",
                "Slava Mikhaylov",
                "Michael Laver."
            ],
            "title": "Scaling policy preferences from coded political texts",
            "venue": "Legislative Studies Quarterly, 36(1):123\u2013155.",
            "year": 2011
        },
        {
            "authors": [
                "P. Mazis",
                "A. Tsekrekos."
            ],
            "title": "Latent semantic analysis of the fomc statements",
            "venue": "Review of Accounting and Finance, 16:179\u2013217.",
            "year": 2017
        },
        {
            "authors": [
                "Ellen E. Meade",
                "David Stasavage."
            ],
            "title": "Publicity of Debate and the Incentive to Dissent: Evidence from the US Federal Reserve",
            "venue": "The Economic Journal, 118(528):695\u2013717.",
            "year": 2008
        },
        {
            "authors": [
                "Kenny Peng",
                "Arunesh Mathur",
                "Arvind Narayanan."
            ],
            "title": "Mitigating dataset harms requires stewardship: Lessons from 1000 papers",
            "venue": "arXiv preprint arXiv:2108.02922.",
            "year": 2021
        },
        {
            "authors": [
                "Christina D Romer",
                "David H Romer."
            ],
            "title": "A new measure of monetary shocks: Derivation and implications",
            "venue": "American economic review, 94(4):1055\u20131084.",
            "year": 2004
        },
        {
            "authors": [
                "Peter Tillmann."
            ],
            "title": "Financial markets and dissent in the ecb\u2019s governing council",
            "venue": "European Economic Review, 139:103848.",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 The Road to FOMC Transparency",
            "text": "The Federal Open Market Committee (FOMC) is responsible for controlling inflation in the United States, using instruments which dramatically affect the housing and financial markets, among others. For most of the 20th century, conventional wisdom held that monetary policy is most effective when decision-making was shrouded in secrecy; the tightlipped Alan Greenspan, a past chairman of the Fed, quipped about \u201clearning to mumble with great incoherence.\u201d But times change.\nBlinder et al. (2008) show how the emergence of greater transparency and strategic communication became an important feature of 21st century central banking. Fed communication is now an integral component of monetary policy, and \u201cFed watchers\u201d dote on every word. The FOMC first started releasing public statements following their meetings in February 1994. This meager documentation grew and now consists of three types for each official meeting: carefully produced and highly stylized\none page statements are released immediately after each FOMC meeting, followed about three weeks later by lengthier minutes, and finally five years later by full, verbatim transcripts. Subsequently this triplet is referred to as documents. We find minutes closely reflect the content of transcripts, so to avoid redundancy, we focus our analysis on transcripts and statements.\nIncreased FOMC communication has prompted social science research spanning the disciplines of economics, sociology, finance and political science (Section 2.1). Financial market participants are also keenly interested. Billions, if not trillions of dollars are traded on the Fed\u2019s words. The interpretations\u2014 right or wrong\u2014of what the FOMC \u201creally means\u201d move markets and affect the economy. However, relying upon documents as data in the social sciences is a challenge due to the lack of structure and the cost of annotation (Grimmer and Stewart, 2013; Gentzkow et al., 2019; Ash and Hansen, 2023).\nHansen and Kazinnik (2023) show that Generative Pre-training Transformer (GPT) models outperform a suite of commonly used NLP methods on text quantification. Motivated by these results, we set out to quantify the language of the FOMC using GPT-4 (OpenAI, 2023) by preparing a combined data set of FOMC documents from 1994-\n2016 (Section 2).1 We conclude that transcripts contain more dissent than statements (Section 3)."
        },
        {
            "heading": "2 FOMC Data: Transcript to Statement",
            "text": "The FOMC normally meets eight times per year in order to assess current economic conditions, ultimately deciding upon the path for monetary policy. We aggregate and release the official publicly available text documenting these deliberations by the Fed as an aligned corpora of documents from 1994 to 2016.2 These text documents are similar in content, but transcripts and statements are dramatically different in style and detail (Figure 1).3 For our purposes, the statements required no pre-processing. For the transcripts, we use regular expressions to partition and then re-aggregate the text by each unique speaker. See Appendix D for an example of each document type."
        },
        {
            "heading": "2.1 Lessons from Social Science",
            "text": "Past work has used at most one form of FOMC meeting documentation, but rarely multiple in conjunction. For example, in the finance literature,\n1https://github.com/DenisPeskoff/FedNLP 2https://www.federalreserve.gov/\nmonetarypolicy/fomc_historical.htm 3Transcripts only became being released in 1994 and due to the 5 year lag the latest transcripts are not yet available.\nMazis and Tsekrekos (2017) apply Latent Semantic Analysis to FOMC statements to identify the main \"themes\" used by the committee and how well they explain variation in treasury yields. Gu et al. (2022) use minutes to investigate how the tonality of committee deliberations impacts subsequent stock market valuations. Political scientists use transcripts to estimate committee members\u2019 preferences on inflation and unemployment (Baerg and Lowe, 2020). Economists have assessed the role of communication in achieving monetary policy objectives by looking at similar documents (Romer and Romer, 2004; Handlan, 2020). H\u00fcpper and Kempa (2023) investigate the extent to which shifting inflation focus is reflected in full transcripts. Edison and Carcel (2021) apply Latent Dirichlet Allocation (LDA) to transcripts to detect the evolution of prominent topics. Hansen et al. (2017) use LDA to quantify transcripts and identify how transparency affects the committee\u2019s deliberations."
        },
        {
            "heading": "2.2 Hawks and Doves",
            "text": "We take the transcripts to best represent FOMC members\u2019 underlying attitudes and think of the statements as stylized representations of what they wish to communicate publicly. To identify disagreement, we need to go beyond the statements and look closely at the language employed by members\nin their remarks throughout meeting transcripts. Dissenting votes are rare because of a strong historical norm: members dissent only if they feel very strongly that the committee\u2019s decision is wrong. Modest disagreements do not merit dissent.4 That said, members of the committee do frequently voice detectable disagreements with one another at meetings. Disagreements frequently concern the state of the economy, the outlook for inflation, and many other things, including where the range for the federal funds rate should be set that day. Such debate and deliberation among members is a routine and productive element of the meetings. These disagreements are more clearly expressed in the transcripts. Daniel Tarullo\u2019s comments in a 2016 transcript illustrate the point:\n\u201cit is institutionally important for us to project an ability to agree, even if only at a fairly high level, and that is why I abstained rather than dissented over each of the past several years [...] I have gone out of my way in the past four years not to highlight publicly my points of difference with the statement.\u201d"
        },
        {
            "heading": "2.3 Manual Analysis to Create a Gold Label",
            "text": "Dissent amongst speakers is normally concentrated on the discussion of the economic and financial situation of the U.S, specifically inflation targeting. For example, in the January 2016 meeting transcript, the committee discusses their 2 percent inflation projection in the context of factors such as oil prices, the job market, and the Chinese economy. Many of the members argue that the inflation projection of 2 percent will not be accurate, while others who do support the 2 percent projection qualify their support with varying degrees of uncertainty. In this meeting, President Mester concludes,\n\u201cMy reasonable confidence that inflation will gradually return to our objective over time recognizes there is and has always been large uncertainty regarding inflation forecasts.\u201d\nWhile Mr. Tarullo argues in opposition,\n\u201c. . . I didn\u2019t have reasonable confidence that inflation would rise to 2 percent.\n4Notably, this is not true of the monetary policy committees of other nations; on some of them, dissent is common\u2014even expected (Tillmann, 2021).\nNothing since then has increased my confidence. To the contrary, a few more doubts have crept in.\u201d\nIf the meeting statement was an accurate representation of what transpired at the meeting, it would follow that the uncertainty of the committee regarding their inflation forecast would be communicated. Instead, the diverging individual opinions are omitted in the final statement, where:\n\u201cThe Committee currently expects that, with gradual adjustments in the stance of monetary policy, economic activity will expand at a moderate pace and labor market indicators will continue to strengthen. Inflation is expected to remain low in the near term, in part because of the further declines in energy prices, but to rise to 2 percent over the medium term as the transitory effects of declines in energy and import prices dissipate and the labor market strengthens\u201d\nWe apply this level of granular analysis to create a gold label for statements, classifying each one according to Hawk and Dove definitions proposed by Hansen and Kazinnik (2023): Dovish (-1.0), Mostly Dovish (-0.5), Neutral (0), Mostly Hawkish (.5), Hawkish (1.0). A trained undergraduate does a first pass and escalates any borderline cases to a FOMC expert for adjudication. We manually review the January 2016 transcript and pair it with a simple computational analysis, which finds that most dissent at that time surrounded topics relating to inflation (Appendix C)."
        },
        {
            "heading": "2.4 GPT-4 \u201cReads\u201d Terse Documents",
            "text": "GPT-4, and Large Language Models more broadly, are a suitable tool for rapid linguistic processing at scale. We produce three different measurements of hawk/dove sentiment using statements and two using transcripts. Hansen and Kazinnik (2023) use GPT-3 to quantify 500 sentences selected uniformly at random from FOMC statements between 2010 and 2020. We extend this by using GPT-4 to quantify all 3728 sentences in statements from 1994 to 2016 (Appendix B). A limitation of this approach is that the holistic sentiment of the meeting is not captured because each sentence is scored independently\u2014without context.\nThe first statement measurement we propose is to simply take an unweighted mean of all individual\nsentence scores for each meeting. Because most sentences have nothing to do with inflation (62% of sentences scored as Neutral), we hypothesize that this method has further limitations. We resolve this by ingesting each entire statement into our GPT-4 prompt. Both of these measurements, along with manual gold labels, can be seen in Figure 3.\nFor the final statement measurement, we construct a logit-scaled score (Lowe et al., 2011),\n\u03b8(L) = log\n( Hawk + 0.5\nDove+ 0.5 ) where Hawk and Dove are the sums of the hawk-\nish and dovish scores, respectively. In this approach \u03b8(L) ignores sentences scored as Neutral, placing more emphasis on the relative rather than absolute differences between hawkish and dovish sentiment. Furthermore, since \u03b8(L) has no predefined end points, this allows us to generate positions at any level of extremity, which more appropriately reflects the outlier meetings.\nWhen measuring hawk/dove sentiment using transcripts, the vast amount of text adds an additional challenge. Rather than evaluating a 100+ page transcript at the sentence level, we instead evaluate transcripts at the speaker level. That is to\nsay, we use GPT-4 to quantify hawk/dove sentiment for each distinct speaker within each transcript, and then aggregate those by meeting date. In contrast to the 3728 individual sentence observations across all statements, we evaluate 5691 speaker observations across all transcripts. We then follow the same steps outlined above to calculate unweighted average and logit-scaled scores.\nThe logit-scaled measurements for transcripts and statements track one another quite closely over most of the years, demonstrating that GPT-4 is effective at identifying similar content across document types of dramatically different length and style. It is worth noting, however, that the logitscaled transcript scores have larger extremes than statements, especially the upper bound: (-3.66, 3.37) and (-3.40, 2.40), respectively. This underscores how the FOMC devotes considerable attention to curating their communication strategy to convey confidence and unity despite dissent among the hawks and doves. The logit-scaled scores begin to diverge in 2012, with transcripts trending more neutral/hawkish while statements remain mostly dovish. Hence, we propose a direct comparison."
        },
        {
            "heading": "3 LLMs Quantify Economic Text",
            "text": ""
        },
        {
            "heading": "3.1 Measuring Dissent",
            "text": "We can use the sentence-level statement and speaker-level transcript scores from GPT-4 to compute a measure of dissent for each meeting using the following algorithm:\n1. From the list of scores for each meeting, count the number of hawkish/mostly hawkish and the number of dovish/mostly dovish scores. 2. If there is at least one hawkish score and at least one dovish score within the same meeting, assign Dissent = 1. Else, Dissent = 0.\nWe find that 47% of statements and 82% of transcripts contain dissent. We also compute the conditional probability of a transcript containing dissent given the associated statement binary, P (T = 1|S = 1) and P (T = 1|S = 0). We find that when a statement contains dissent, P (T = 1|S = 1), the transcript agrees more than 97% of the time. However, for statements scored as having no dissent, P (T = 1|S = 0) we find that more than 69% of associated transcripts are scored as containing dissent. This means that for the 53% of statements that don\u2019t show signs of dissenting opinions, there is likely dissent in the transcript as evidenced by the speaker-level hawk/dove scores."
        },
        {
            "heading": "3.2 Conclusion and Next Steps",
            "text": "Our method of ingesting the entire statement for an aggregate prediction better captures the extremes, which more closely mirror the gold label human annotation and suggests that Large Language Models can avoid the noise in this nuanced context. The F1 score for this comparison is 0.57. While this is rather low as a measure of model \u201cfit\u201d, it is important to note that the results rarely flip sentiment (from hawkish to dovish, or mostly hawkish to mostly dovish), rather, it just seems to mostly disagree on adjacent categories. See Figure 3 for a visual comparison of the sentence-level, entire text, and manual scores. Of note, the inconsistent provision of statements and relatively high volatility in hawk/dove sentiment before 2000 is consistent with Meade and Stasavage (2008) and Hansen et al. (2017) who have also studied the 1993 change in FOMC communication strategy. We demonstrate that GPT-4 can identify the extremes in dissenting hawk and dove perspectives despite the indications of a clear consensus in the statements. This empirical finding supports our manual analysis.\nWhile we focus on transcripts and statements, future work may consider an even more fine grained analysis, incorporating minutes as well. We found the content of the minutes to more closely resemble the transcripts than the statements, but differences do exist and remain underexplored.\nAdditionally, we note that GPT-4 scores made more neutral predictions than the gold standard manual labels. To improve upon this, we created a balanced few-shot example using sentences from FOMC statements not included in our sample \u2014 meetings since 2020. This marginally improved the prediction \u201cfit\u201d (F1 of 57% to 58%), but we expect that this could be improved much further with additional prompt engineering.\nGPT-4 is able to quickly quantify stylized economic text. Our results from quantifying dissent support the hypothesis that dissenting opinions on the topic of inflation omitted from FOMC statements can be found in the associated transcripts. As LLMs continue to improve, we expect that it will be possible to study even more nuanced questions than the ones we answer here.\nLimitations\nSubstantively, strategic signaling in the FOMC is a challenging topic and this is only an initial investigation. Dissent does not have clear ground truth\nlabels and thus we are reliant on human judgment and our team\u2019s substantive expertise on monetary policy. Finally, as with much current research, our work relies on OpenAI\u2019s GPT API, which poses challenges to computational reproducibility, as it relies on the stability of an external system that we cannot control.\nEthics Statement\nThe FOMC is a high-stakes body whose activities are already subject to substantial scrutiny. There is some ethical risk in exploring linguistic signals of hidden information. For example, based on the substantive literature we believe that dissent is intentionally signalled in meetings in order to set up future discussions or lay claims to particular positions. Nevertheless, attribution of intention (as implied by \u2019dissent\u2019) always involves some level of error that could be uncomfortable to meeting participants who feel mischaracterized. We also emphasize that our approach to capturing dissent would not be appropriate to use outside this specific context without careful validation. Finally, by making the FOMC data more easily available to the NLP community, we also assume some ethical responsibility for the potential uses of that data (see e.g. Peng et al., 2021). We spend under $1500 on computation and under $1000 on annotation and believe our results to be reasonably reproducible. We feel that these concerns are ultimately minor given that all participants are public officials who knew their transcripts would ultimately be released."
        },
        {
            "heading": "Acknowledgements",
            "text": "This material is based upon work supported by the National Science Foundation under Grant #2127309 to the Computing Research Association for the CIFellows 2021 Project. We thank the Initiative for Data-Driven Social Science at Princeton University for grant support and OpenAI for technical support. Visokay is supported by the National Institute of Mental Health of the NIH under Award Number #DP2MH122405 and by the Center for Statistics and the Social Sciences at the University of Washington."
        },
        {
            "heading": "A Appendix",
            "text": "Our appendix contains the GPT prompt we used, a section on our computational analysis, and an example of the statement (Figure 6), transcript (Figure 7), and minutes (Figure 8) for the January 29- 30, 2008 meetings."
        },
        {
            "heading": "B GPT-4",
            "text": "We used GPT-4 heavily for our experiments and analysis. Recent work (OpenAI, 2023; Liu et al., 2023; Guan et al., 2023) that successfully uses GPT4 for classification gave us confidence in its quality. Additionally, a human expert on our team examined GPT-4 generated labels, and found that in a sample of 25, our expert agreed with 19 labels with high confidence, and 22 labels with at least moderate confidence.\nB.1 Methodology\nWe used variants of a single prompt template for all of our tasks. It contains the relevant labels (Hawkish, Dovish, etc.) as well as their definitions. It includes space for some INPUT and asks which label best applies to the input. When processing different documents, such as transcripts, we would switch out the word statements in the prompt for the appropriate document word.\nFor statements, we 0-shot prompted GPT-4 to label each statement as one of the five labels. We also ran an experiment in which we classified each sentence of each statements as one of the labels, then averaged the sentence scores to get to statement score. Finally, we reran this sentence classification with a 10-shot prompt. The prompt was similar to below, except with 10 examples of sentences and their classifications at the beginning.\nFor minutes, we 0-shot prompted GPT-4-32K to label each statement as one of the five labels.\nFor transcripts, we 0-shot prompted GPT-4-32K to examine all of each speakers speech, and provide each speaker a single label for each transcript.\nFor all API calls to OpenAI, we only modified the model (either GPT-4 or GPT-4-32K). We did not change any other settings.\np r o m p t _ t e m p l a t e = \" \" \" < s t a t e m e n t > INPUT </ s t a t e m e n t > < l a b e l s >\nDovish : S t r o n g l y e x p r e s s e s a b e l i e f t h a t t h e economy may be growing t o o s l o w l y and may need s t i m u l u s t h r o u g h mon\u2212 e t a r y p o l i c y . Most ly d o v i s h : O v e r a l l message\ne x p r e s s e s a b e l i e f t h a t t h e economy may\nbe growing t o o s l o w l y and may need s t i m u l u s t h r o u g h moneta ry p o l i c y . N e u t r a l : E x p r e s s e s n e i t h e r a hawkish nor d o v i s h view and i s mo s t l y o b j e c t i v e . Most ly hawkish : O v e r a l l message\ne x p r e s s e s a b e l i e f t h a t t h e economy i s\ngrowing t o o q u i c k l y and may need t o be s lowed down t h r o u g h moneta ry p o l i c y . Hawkish : S t r o n g l y e x p r e s s e s a\nb e l i e f t h a t t h e economy i s growing\nt o o q u i c k l y and may need t o be s lowed down t h r o u g h moneta ry p o l i c y . </ l a b e l s > Which l a b e l b e s t a p p l i e s a p p l i e s\nt o t h e s t a t e m e n t ( Dovish , Most ly Dovish , N e u t r a l , Most ly\nHawkish , Hawkish ) ? \" \" \""
        },
        {
            "heading": "C Computational Analysis",
            "text": "We paired our manual review of the January 26-27, 2016 transcript with a computational analysis of dissent in the meeting. We stratified the meeting into nine topics, each corresponding to a portion of the transcript content. As a baseline, we counted the number of speakers in each section to see if this metric could reflect dissent. This technique, however, seemed to reflect the length of the conversation as opposed to the degree to which members disagreed with one another.\nOur next approach was to do a sentiment analysis of each topic to see if the prevalence of negativity could indicate dissent. We supposed that negative sentiment would be high if the speakers opposed the stance of either other individuals or the committee as a whole. Using the VADER lexicon (Hutto and Gilbert, 2014), we calculated the\nsentiment of each sentence within the nine topics. Since VADER is trained on web-based social media content, which is typically more abrupt than the formal language appearing in the FOMC transcript, we conducted the sentiment analysis by sentence to optimize the method\u2019s performance.\nTo analyze dissent more specifically, we computed the fraction of negative sentences in each topic. For this analysis, we set the threshold negativity score to be 0.1. That is, sentences with a negativity score of 0.1 or higher were classified as negative while all others were not. This number determined by manually reviewing what sentences were captured by varying thresholds and evaluating whether or not they conveyed dissent. When the threshold was set too low (0.05), four out of ten randomly selected sentences conveyed dissent. When set too high (0.15), seven out of ten randomly selected sentences conveyed dissent, but many sentences that indicated dissent were omitted. At the threshold of 0.1, still seven out of ten randomly selected sentences conveyed dissent, and more sentences that conveyed dissent were captured."
        },
        {
            "heading": "D Document Examples",
            "text": "See Figures 6, 7, and 8 for examples of the documents."
        }
    ],
    "title": "GPT Deciphering Fedspeak: Quantifying Dissent Among Hawks and Doves",
    "year": 2023
}