{
    "abstractText": "Scientific literature review generation aims to extract and organize important information from an abundant collection of reference papers and produces corresponding reviews while lacking a clear and logical hierarchy. We observe that a high-quality catalogue-guided generation process can effectively alleviate this problem. Therefore, we present an atomic and challenging task named Hierarchical Catalogue Generation for Literature Review as the first step for review generation, which aims to produce a hierarchical catalogue of a review paper given various references. We construct a novel English Hierarchical Catalogues of Literature Reviews Dataset with 7.6k literature review catalogues and 389k reference papers. To accurately assess the model performance, we design two evaluation metrics for informativeness and similarity to ground truth from semantics and structure. Our extensive analyses verify the high quality of our dataset and the effectiveness of our evaluation metrics. We further benchmark diverse experiments on state-of-theart summarization models like BART and large language models like ChatGPT to evaluate their capabilities. We further discuss potential directions for this task to motivate future research.",
    "authors": [
        {
            "affiliations": [],
            "name": "Kun Zhu"
        },
        {
            "affiliations": [],
            "name": "Xiaocheng Feng"
        },
        {
            "affiliations": [],
            "name": "Xiachong Feng"
        },
        {
            "affiliations": [],
            "name": "Yingsheng Wu"
        },
        {
            "affiliations": [],
            "name": "Bing Qin"
        },
        {
            "affiliations": [],
            "name": "Peng Cheng"
        }
    ],
    "id": "SP:a1dfb1af7dbb34f4e65a4bcd6104dc33c37552ef",
    "references": [
        {
            "authors": [
                "Nouf Ibrahim Altmami",
                "Mohamed El Bachir Menai."
            ],
            "title": "Automatic summarization of scientific articles: A survey",
            "venue": "Journal of King Saud University-Computer and Information Sciences, 34(4):1011\u20131028.",
            "year": 2022
        },
        {
            "authors": [
                "Satanjeev Banerjee",
                "Alon Lavie."
            ],
            "title": "Meteor: An automatic metric for mt evaluation with improved correlation with human judgments",
            "venue": "Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summariza-",
            "year": 2005
        },
        {
            "authors": [
                "Iz Beltagy",
                "Kyle Lo",
                "Arman Cohan."
            ],
            "title": "Scibert: A pretrained language model for scientific text",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language",
            "year": 2019
        },
        {
            "authors": [
                "Iz Beltagy",
                "Matthew E Peters",
                "Arman Cohan."
            ],
            "title": "Longformer: The long-document transformer",
            "venue": "arXiv preprint arXiv:2004.05150.",
            "year": 2020
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "arXiv preprint arXiv:1810.04805.",
            "year": 2018
        },
        {
            "authors": [
                "G\u00fcnes Erkan",
                "Dragomir R Radev."
            ],
            "title": "Lexrank: Graph-based lexical centrality as salience in text summarization",
            "venue": "Journal of artificial intelligence research, 22:457\u2013479.",
            "year": 2004
        },
        {
            "authors": [
                "Michael Fire",
                "Carlos Guestrin."
            ],
            "title": "Overoptimization of academic publishing metrics: observing goodhart\u2019s law in action",
            "venue": "GigaScience, 8(6):giz053.",
            "year": 2019
        },
        {
            "authors": [
                "Tsu-Jui Fu",
                "William Yang Wang",
                "Daniel McDuff",
                "Yale Song."
            ],
            "title": "Doc2ppt: Automatic presentation slides generation from scientific documents",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 634\u2013642.",
            "year": 2022
        },
        {
            "authors": [
                "Yubin Ge",
                "Ly Dinh",
                "Xiaofeng Liu",
                "Jinsong Su",
                "Ziyao Lu",
                "Ante Wang",
                "Jana Diesner."
            ],
            "title": "Baco: A background knowledge-and content-based framework for citing sentence generation",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Compu-",
            "year": 2021
        },
        {
            "authors": [
                "Pengcheng He",
                "Yi Mao",
                "Kaushik Chakrabarti",
                "Weizhu Chen."
            ],
            "title": "X-sql: reinforce schema representation with context",
            "venue": "arXiv preprint arXiv:1908.08113.",
            "year": 2019
        },
        {
            "authors": [
                "Cong Duy Vu Hoang",
                "Min-Yen Kan."
            ],
            "title": "Towards automated related work summarization",
            "venue": "Coling 2010: Posters, pages 427\u2013435.",
            "year": 2010
        },
        {
            "authors": [
                "Yue Hu",
                "Xiaojun Wan"
            ],
            "title": "Automatic generation of related work sections in scientific papers",
            "year": 2014
        },
        {
            "authors": [
                "Gautier Izacard",
                "Edouard Grave."
            ],
            "title": "Leveraging passage retrieval with generative models for open domain question answering",
            "venue": "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume,",
            "year": 2021
        },
        {
            "authors": [
                "Rahul Jha",
                "Catherine Finegan-Dollak",
                "Ben King",
                "Reed Coke",
                "Dragomir Radev."
            ],
            "title": "Content models for survey generation: a factoid-based evaluation",
            "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th",
            "year": 2015
        },
        {
            "authors": [
                "Mike Lewis",
                "Yinhan Liu",
                "Naman Goyal",
                "Marjan Ghazvininejad",
                "Abdelrahman Mohamed",
                "Omer Levy",
                "Ves Stoyanov",
                "Luke Zettlemoyer"
            ],
            "title": "Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension",
            "year": 2019
        },
        {
            "authors": [
                "Pengcheng Li",
                "Wei Lu",
                "Qikai Cheng."
            ],
            "title": "Generating a related work section for scientific papers: an optimized approach with adopting problem and method information",
            "venue": "Scientometrics, 127(8):4397\u2013 4417.",
            "year": 2022
        },
        {
            "authors": [
                "Chin-Yew Lin."
            ],
            "title": "ROUGE: A package for automatic evaluation of summaries",
            "venue": "Text Summarization Branches Out, pages 74\u201381, Barcelona, Spain. Association for Computational Linguistics.",
            "year": 2004
        },
        {
            "authors": [
                "Shuaiqi LIU",
                "Jiannong Cao",
                "Ruosong Yang",
                "Zhiyuan Wen."
            ],
            "title": "Generating a structured summary of numerous academic papers: Dataset and method",
            "venue": "Proceedings of the Thirty-First International Joint Conference on Artificial Intelli-",
            "year": 2022
        },
        {
            "authors": [
                "Yao Lu",
                "Yue Dong",
                "Laurent Charlin."
            ],
            "title": "Multixscience: A large-scale dataset for extreme multidocument summarization of scientific articles",
            "venue": "arXiv preprint arXiv:2010.14235.",
            "year": 2020
        },
        {
            "authors": [
                "Kelvin Luu",
                "Xinyi Wu",
                "Rik Koncel-Kedziorski",
                "Kyle Lo",
                "Isabel Cachola",
                "Noah A Smith."
            ],
            "title": "Explaining relationships between scientific documents",
            "venue": "arXiv preprint arXiv:2002.00317.",
            "year": 2020
        },
        {
            "authors": [
                "Rada Mihalcea",
                "Paul Tarau."
            ],
            "title": "Textrank: Bringing order into text",
            "venue": "Proceedings of the 2004 conference on empirical methods in natural language processing, pages 404\u2013411.",
            "year": 2004
        },
        {
            "authors": [
                "Saif Mohammad",
                "Bonnie Dorr",
                "Melissa Egan",
                "Ahmed Hassan",
                "Pradeep Muthukrishnan",
                "Vahed Qazvinian",
                "Dragomir Radev",
                "David Zajic."
            ],
            "title": "Using citations to generate surveys of scientific paradigms",
            "venue": "In",
            "year": 2009
        },
        {
            "authors": [
                "Benjamin Paa\u00dfen."
            ],
            "title": "Revisiting the tree edit distance and its backtracing: A tutorial",
            "venue": "arXiv preprint arXiv:1805.06869.",
            "year": 2018
        },
        {
            "authors": [
                "Benjamin Paa\u00dfen",
                "Bassam Mokbel",
                "Barbara Hammer."
            ],
            "title": "A toolbox for adaptive sequence dissimilarity measures for intelligent tutoring systems",
            "venue": "Proceedings of the 8th International Conference on Educational Data Mining (EDM 2015), page",
            "year": 2015
        },
        {
            "authors": [
                "Lawrence Page",
                "Sergey Brin",
                "Rajeev Motwani",
                "Terry Winograd."
            ],
            "title": "The pagerank citation ranking: Bringing order to the web",
            "venue": "Technical report, Stanford InfoLab.",
            "year": 1999
        },
        {
            "authors": [
                "Kishore Papineni",
                "Salim Roukos",
                "Todd Ward",
                "WeiJing Zhu."
            ],
            "title": "Bleu: a method for automatic evaluation of machine translation",
            "venue": "Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 311\u2013318.",
            "year": 2002
        },
        {
            "authors": [
                "LIU Shuaiqi",
                "Jiannong Cao",
                "Ruosong Yang",
                "Zhiyuan Wen"
            ],
            "title": "Generating a structured summary of numerous academic papers: Dataset and method",
            "year": 2022
        },
        {
            "authors": [
                "Xiaoping Sun",
                "Hai Zhuge."
            ],
            "title": "Automatic generation of survey paper based on template tree",
            "venue": "2019 15th International Conference on Semantics, Knowledge and Grids (SKG), pages 89\u201396. IEEE.",
            "year": 2019
        },
        {
            "authors": [
                "Bowen Tan",
                "Zichao Yang",
                "Maruan Al-Shedivat",
                "Eric Xing",
                "Zhiting Hu."
            ],
            "title": "Progressive generation of long text with pretrained language models",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational",
            "year": 2021
        },
        {
            "authors": [
                "Ross Taylor",
                "Marcin Kardas",
                "Guillem Cucurull",
                "Thomas Scialom",
                "Anthony Hartshorn",
                "Elvis Saravia",
                "Andrew Poulton",
                "Viktor Kerkez",
                "Robert Stojnic."
            ],
            "title": "Galactica: A large language model for science",
            "venue": "arXiv preprint arXiv:2211.09085.",
            "year": 2022
        },
        {
            "authors": [
                "Jesse Vig",
                "Alexander Fabbri",
                "Wojciech Kryscinski",
                "Chien-Sheng Wu",
                "Wenhao Liu."
            ],
            "title": "Exploring neural models for query-focused summarization",
            "venue": "Findings of the Association for Computational Linguistics: NAACL 2022, pages 1455\u20131468, Seattle,",
            "year": 2022
        },
        {
            "authors": [
                "Bailin Wang",
                "Richard Shin",
                "Xiaodong Liu",
                "Oleksandr Polozov",
                "Matthew Richardson."
            ],
            "title": "Rat-sql: Relation-aware schema encoding and linking for textto-sql parsers",
            "venue": "arXiv preprint arXiv:1911.04942.",
            "year": 2019
        },
        {
            "authors": [
                "Lucy Lu Wang",
                "Kyle Lo."
            ],
            "title": "Text mining approaches for dealing with the rapidly expanding literature on covid-19",
            "venue": "Briefings in Bioinformatics, 22(2):781\u2013799.",
            "year": 2021
        },
        {
            "authors": [
                "Pancheng Wang",
                "Shasha Li",
                "Kunyuan Pang",
                "Liangliang He",
                "Dong Li",
                "Jintao Tang",
                "Ting Wang."
            ],
            "title": "Multi-document scientific summarization from a knowledge graph-centric view",
            "venue": "arXiv preprint arXiv:2209.04319.",
            "year": 2022
        },
        {
            "authors": [
                "Jane Webster",
                "Richard T Watson."
            ],
            "title": "Analyzing the past to prepare for the future: Writing a literature review",
            "venue": "MIS quarterly, pages xiii\u2013xxiii.",
            "year": 2002
        },
        {
            "authors": [
                "Thomas Wolf",
                "Lysandre Debut",
                "Victor Sanh",
                "Julien Chaumond",
                "Clement Delangue",
                "Anthony Moi",
                "Pierric Cistac",
                "Tim Rault",
                "R\u00e9mi Louf",
                "Morgan Funtowicz"
            ],
            "title": "Transformers: State-of-the-art natural language processing",
            "year": 2020
        },
        {
            "authors": [
                "Jia-Yan Wu",
                "Alexander Te-Wei Shieh",
                "Shih-Ju Hsu",
                "Yun-Nung Chen."
            ],
            "title": "Towards generating citation sentences for multiple references with intent control",
            "venue": "arXiv preprint arXiv:2112.01332.",
            "year": 2021
        },
        {
            "authors": [
                "Xinyu Xing",
                "Xiaosheng Fan",
                "Xiaojun Wan."
            ],
            "title": "Automatic generation of citation texts in scholarly papers: A pilot study",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6181\u20136190.",
            "year": 2020
        },
        {
            "authors": [
                "Huiyan Xu",
                "Zhongqing Wang",
                "Yifei Zhang",
                "Xiaolan Weng",
                "Zhijian Wang",
                "Guodong Zhou."
            ],
            "title": "Document structure model for survey generation using neural network",
            "venue": "Frontiers of Computer Science, 15:1\u2013",
            "year": 2021
        },
        {
            "authors": [
                "Lili Yao",
                "Nanyun Peng",
                "Ralph Weischedel",
                "Kevin Knight",
                "Rui Yan"
            ],
            "title": "Plan-and-write: Towards better automatic storytelling",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2019
        },
        {
            "authors": [
                "Kaizhong Zhang",
                "Dennis Shasha."
            ],
            "title": "Simple fast algorithms for the editing distance between trees and related problems",
            "venue": "SIAM journal on computing, 18(6):1245\u20131262.",
            "year": 1989
        },
        {
            "authors": [
                "Tianyi Zhang",
                "Varsha Kishore",
                "Felix Wu",
                "Kilian Q. Weinberger",
                "Yoav Artzi"
            ],
            "title": "Bertscore: Evaluating text generation with bert",
            "venue": "In International Conference on Learning Representations",
            "year": 2020
        },
        {
            "authors": [
                "Wei Zhao",
                "Maxime Peyrard",
                "Fei Liu",
                "Yang Gao",
                "Christian M. Meyer",
                "Steffen Eger."
            ],
            "title": "MoverScore: Text generation evaluating with contextualized embeddings and earth mover distance",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in",
            "year": 2019
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Today\u2019s researchers can publish their work not only in traditional venues like conferences and journals but also in e-preprint libraries and mega-journals, which is fast, convenient, and easy to access (Fire and Guestrin, 2019). This enables rapid development and sharing of academic achievements. For example, statistics show that more than 50,000 publications occurred in 2020 in response to COVID19 (Wang and Lo, 2021). Therefore, researchers are overwhelmed by considerable reading with the explosive growth in the number of scientific papers, urging more focus on scientific literature review generation (Altmami and Menai, 2022).\nPioneer studies on scientific literature review generation explore citation sentence generation (Xing et al., 2020; Luu et al., 2020; Ge et al., 2021; Wu et al., 2021) and related work generation (Hoang and Kan, 2010; Hu and Wan, 2014; Li et al., 2022; Wang et al., 2022). However, these methods can only generate short summaries, while a literature review is required to provide a comprehensive and sufficient overview of a particular topic (Webster and Watson, 2002). Benefiting from the development of language modeling (Lewis et al., 2019), recent works directly attempt survey generation (Mohammad et al., 2009; Jha et al., 2015; Shuaiqi et al., 2022) but usually suffer from a disorganized generation without hierarchical guidance. As illustrated in Figure 1(A), we take gpt-3.5-turbo1, a large language model trained on massive amounts of diverse data including scientific papers, as the summarizer to generate scientific literature reviews. Direct generation can lead to disorganized reviews with content repetition and logical confusion.\nSince hierarchical guidance is effective for text generation (Yao et al., 2019), as shown in Figure 1(B), we observe that the catalogue, representing the author\u2019s understanding and organization of existing research, is also beneficial for scientific literature review generation. Therefore, the generation process can be divided into two steps. First is generating a hierarchical catalogue, and next is each part of the review. Figure 1(C) reveals that even state-of-the-art language models can not obtain a reliable catalogue, leaving a valuable and challenging problem for scientific literature review generation.\nTo enable the capability of generating reasonable hierarchical catalogues, we propose a novel and challenging task of Hierarchical Catalogue Generation for scientific Literature Review, named as HiCatGLR, which is a first step towards automatic review generation. We construct the first benchmark for HiCatGLR by gathering html for-\n1A version of ChatGPT. openai.com/blog/chatgpt\nmat2 of survey papers\u2019 catalogues along with abstracts of their reference papers from Semantics Scholar3. After meticulous filtering and manual screening, we obtain the final Hierarchical Catalogue Dataset (HiCaD) with 7.6k referencescatalogue pairs, which is the first to decompose the review generation process and seek to explicitly model a hierarchical catalogue. The resulting HiCaD has an average of 81.1 reference papers for each survey paper, resulting in an average input length of 21,548 (Table 1), along with carefully curated hierarchical catalogues as outputs.\nDue to the structural nature of catalogues, traditional metrics like BLEU and ROUGE can not accurately reflect their generation quality. We specially design two novel evaluations for the catalogue generation, where Catalogue Edit Distance Similarity (CEDS) measures the similarity to ground truth and Catalogue Quality Estimate (CQE) measures the degree of catalogue standardization from the frequency of catalogue template words in results.\nTo evaluate the performance of various methods on our proposed HiCatGLR, we study both\n2https://ar5iv.labs.arxiv.org/ 3https://www.semanticscholar.org/\nend-to-end (one-step) generation and step-by-step generation for the hierarchical catalogues, where the former generally works better and the latter allows for more focus on target-level headings. We benchmark different methods under fine-tuned or zero-shot settings to observe their capabilities, including the recent large language models.\nIn summary, our contributions are threefold: \u2022 We observe the significant effect of hierarchi-\ncal guidance on literature review generation and propose a new task, HiCatGLR (Hierarchical Catalogue Generation for Literature Review), with the corresponding dataset HiCaD.\n\u2022 We design several evaluation metrics for informativeness and structure accuracy of generated catalogues, whose effectiveness is ensured with detailed analyses.\n\u2022 We study both fine-tuned and zero-shot settings to evaluate models\u2019 abilities on the HiCatGLR, including large language models."
        },
        {
            "heading": "2 Datasets: HICAD",
            "text": "We now introduce HiCaD, including the task definitions, data sources, and processing procedures. We also provide an overall statistical analysis."
        },
        {
            "heading": "2.1 Definitions",
            "text": "The input of this task is the combination of the title t of the target survey S and representative information of reference articles {R1, R2, ..., Rn}, where n is the number of reference articles cited by S. Considering the cost of data collection and experiments, we take abstracts as the representation of corresponding references. Besides, we restrict each abstract to 256 words, where the exceeding part will be truncated. The output is the catalogue C = {c1, ..., ck} of S , where k is the number of catalogue items. ci is an item in the catalogue consisting of a level mark li \u2208 {L1, L2, L3}, which represents the level of the catalogue item, and the content {wi1, wi2, ..., wip} with p number of words. As shown in Figure 1, \"Pre-training\" is the firstlevel heading, \"Architecture\" is the second-level heading, and \" Mainstream Architectures\" is the third-level heading. In our experiment, we only keep up to the third-level headings and do not consider further lower-level headings."
        },
        {
            "heading": "2.2 Dataset Construction",
            "text": "Our dataset is collected from two sources: arXiv4 and Semantics Scholar5. We keep papers6 containing the words \u201csurvey\u201d and \u201creview\u201d in the title and remove ones with \u201cbook review\u201d and \u201ccomments\u201d. We finally select 11,435 papers that are considered to be review papers. It is straightforward to use a crawler to get all 11,435 papers in PDF format according to the arxiv-id. However, extracting catalogue from PDF files is difficult where structural information is usually dropped during converting. Therefore, we attempt ar5iv7 to get the papers in HTML format. This website processes articles from arXiv as responsive HTML web pages by converting from LaTeX via LaTeXML8. Some authors do not upload their LaTeX code, we have to skip these and collect 8,397 papers.\nFor the output part, we obtain the original catalogues by cleaning up the HTML files. Then we replace the serial number from the heading with the level mark <Li> using regex. For input, we collate the list of reference papers and only keep the valid papers where titles and abstracts exist. We convert\n4https://arxiv.org/ 5https://www.semanticscholar.org/ 6These survey papers\u2019 metadata are obtained from Kaggle up to the end of April 2023. https://www.kaggle.com/ datasets/Cornell-University/arxiv.\n7https://ar5iv.org/ 8https://github.com/brucemiller/LaTeXML\nall words to lowercase for subsequent generation and evaluation. Finally, after removing data with less than 5 catalogue items and less than 10 valid references, we obtain 7,637 references-catalogue pairs. We count the fields to which each paper belongs (Table 6). We choose the computer science field with the largest number of papers for the experiment and split the 4,507 papers into training (80%), validation (10%), and test (10%) sets."
        },
        {
            "heading": "2.3 Dataset Statistics and Analysis",
            "text": "Taking the popular scientific dataset as an example, we present the characteristics of the different multidocument scientific summarization tasks in Table 1. Dataset Multi-Xscience is proposed by Lu et al. (2020), which focuses on writing the related work section of a paper based on its abstract with 4.4 articles cited in average. Dataset BigSurvey-MDS is the first large-scale multi-document scientific summarization dataset using review papers\u2019 introduction section as target (LIU et al., 2022), where previous work usually takes the section of related work as the target. Both BigSurvey and our HiCatGLR task have more than 70 references, resulting in over 10,000 words of input, while their output is still the scale of a standard text paragraph, similar to Multi-Xscience. A natural difference between our task and others is that our output contains hierarchical structures, which place high demands on logic and conciseness for generation.\nTo measure how abstractive our target catalogues are, we present the proportion of novel n-grams in the target summaries that do not appear in the source (Table 2). The abstractiveness of HiCaD is lower than that of BigSurvey-MDS and MultiXScience, which suggests that writing catalogues focus on extracting keywords from references. This conclusion is in line with the common sense that literature review is closer to reorganization than innovation. Therefore, our task especially challenges summarizing ability rather than generative ability.\nWe also analyze the share of each level of the catalogue in the whole catalogue. Figure 2 shows the value and proportional relationship of the average number of catalogue items as well as average word length at different levels. It can be seen that the second-level headings have the most weight, being 44.32% of the average number and 48.50% of the average word length. Table 3 shows the weight of the headings at each level in the catalogue from the perspective of word coverage. We calculate\nROUGE 9 scores between different levels of headings (L1, L2, L3) and the general catalogue \"Total\". Similar to the above, the secondary headings have the highest Rouge-1 score of 57.9 for the entire catalogue. Moreover, the Rouge-1 score of 18.5 for the first and second-level headings L1-L2 indicates some overlaps between the first and second levels. The low Rouge scores of L1-L3 and L2-L3 reveal that there are indeed different usage and wording distributions between different levels."
        },
        {
            "heading": "3 Metrics",
            "text": "To evaluate the quality of generated catalogues, we propose two evaluation methods, Catalogue Quality Estimate (CQE) and Catalogue Edit Distance Similarity (CEDS). The former takes a textual perspective, while the latter integrates text and structure. We use these evaluation methods to assess the informativeness of the generated catalogues and the gap between the generated catalogues and the\n9The ROUGE scores in this paper are all computed by ROUGE-1.5.5 script with the option \"-c 95 -r 1000 -n 2 -a -m\"\noracle ones, respectively."
        },
        {
            "heading": "3.1 Catalogue Quality Estimate",
            "text": "There are some fixed templates in the catalogue, such as introduction, methods, and conclusion, which usually do not contain information about domain knowledge and references. The larger the percentage of template words in the catalog, the less valid information is available. Therefore, the proportion of template words can indicate the information content of the catalogue to some extent. We collate a list of templates and calculated the percentage of templates in the catalogue items as Catalogue Quality Estimate (CQE). CQE evaluation metric measures the informativeness of the generated catalogue through template words statistics. Table 7 lists all template words. The CQE of oracle catalogues in the test set is 11.1%."
        },
        {
            "heading": "3.2 Catalogue Edit Distance Similarity",
            "text": "Catalogue Edit Distance Similarity (CEDS) measures the semantic similarity and structural similarity between the generated and oracle catalogues. Traditional automatic evaluation methods in summarization tasks (such as ROUGE (Lin, 2004) and BERTScore (Zhang* et al., 2020)) can only measure semantic similarity. However, catalogues are texts with a hierarchical structure, so the level of headings in catalogues also matters.\nWe are inspired by the thought of edit distance which is commonly used in the ordered labeled tree similarity metric. An ordered labeled tree is one in which the order from left to right among siblings is significant (Zhang and Shasha, 1989). The tree\nedit distance (TED) between two ordered labelled trees Ta, Tb is defined as the minimum number of node edit operations that transform Ta into Tb (Paa\u00dfen, 2018). There are three edit operations on the ordered labeled tree: deletion, insertion, and modification, each with a distance of one. Similar to TED, the definition of catalogue edit distance (CED) is the minimum distance of item edit operations that transform a catalogue Ca into another catalogue Cb, where each entry in the catalogue is a node. The difference is that we calculate the distance between two items according to the similarity of nodes when modification:\nDistance(x, y)=min(1, \u03b1\u00d7(1\u2212Similarity(x, y)).\nWe leverage BERTScore to obtain the similarity between two items of the catalogue based on the embeddings from SciBERT (Beltagy et al., 2019):\nSimilarity(x, y) = (Sci-)BERTScore(x, y).\nSciBERT is more suitable for scientific literature than BERT because it was pretrained on a large multi-domain corpus of scientific publications. We take hyperparameter \u03b1 = 1.2 during experiments. Therefore, we can define Catalogue Edit Distance Similarity (CEDS) as:\nCEDS(Ca, Cb) = 100\u00d7 ( 1\u2212 CED(Ca, Cb)\nmax(|Ca|, |Cb|)\n) .\nWe use the library Python Edit Distances10 proposed by Paa\u00dfen et al. (2015) for the implementation of algorithms. A detailed example of node alignment and the conversion process between two catalogues is given in Appendix F."
        },
        {
            "heading": "4 Models",
            "text": "We now introduce our explorations in hierarchical catalogue generation for literature review, including end-to-end and step-by-step approaches."
        },
        {
            "heading": "4.1 End-to-End Approach",
            "text": "One of the main challenges in generating the catalogue is handling long input contexts. The intuitive and straightforward generation method is the endto-end model, and we experiment with two models that specialize in processing long text. We consider an encoder-decoder model for the end-to-end approach. The model takes the title of a survey\n10https://gitlab.ub.uni-bielefeld.de/bpaassen/ python-edit-distances/-/tree/master\nand the information from its reference papers as input before generating the survey catalogue. The title and references are concatenated together and fed to the encoder. However, existing transformerbased models with O(N2) computational complexity show an explosion in computational overhead as the input length increases significantly.\nWe choose Fusion-in-Decoder (FiD) (Izacard and Grave, 2021), a framework specially designed for long context in open domain question answering, to handle the end-to-end catalogue generation. As shown in Figure 3, the framework processes the combination of the survey title and information from each reference paper independently by the encoder. The decoder pays attention to the concatenation of all representations from the encoder. Unlike previous encoder-decoder models, the FiD model processes papers separately in the encoder. Therefore, the input can be extended to a large number of contexts since it only performs self-attention over one reference paper each time. This allows the computation time of the model to grow linearly with the length of the input. Besides, the joint processing of reference papers in the decoder can better facilitate the interaction of multiple papers."
        },
        {
            "heading": "4.2 Step-by-Step Approach",
            "text": "Another main challenge in generating the catalogue is modeling relationships between catalogue items at different levels. Motivated by Tan et al. (2021), we explore an effective approach for incremental generation. Progressive generation divides the complicated problem of generating a complete catalogue into more manageable steps, namely the generation of hierarchical levels of the catalogue. Different from generating everything in one step, the progressive generation allows the model to perform high-level abstract planning and then shift attention to increasingly concrete details. Figure 5 illustrates the generation process."
        },
        {
            "heading": "5 Experiments",
            "text": "We study the performance of multiple models on the HiCaD dataset. Detailed analysis of the generation quality is provided, including correlation validation of the proposed evaluation metrics with human evaluation and ROUGE for abstractiveness."
        },
        {
            "heading": "5.1 Baselines",
            "text": "Due to the large input length, we choose an encoderdecoder transformer model that can handle the long text and its backbone model to implement FiD besides various extractive models in our experiments. (I) LexRank (Erkan and Radev, 2004) is an unsupervised extractive summarization approach based on graph-based centrality scoring of sentences. (II) TextRank (Mihalcea and Tarau, 2004) is a graph-based ranking algorithm improved from Google\u2019s PageRank (Page et al., 1999) for keyword extraction and document summarization, which uses co-occurrence information (semantics) between words within a document to extract keywords. (III) BART (Lewis et al., 2019) is a pre-trained sequence-to-sequence Transformer model to reconstruct the original input text from the corrupted text with a denoising auto-encoder. (IV) Longformer-Encoder-Decoder (LED) (Beltagy et al., 2020) is built on BART but adopts the sparse global attention mechanisms in the encoder part, which alleviates the input size limitations of the BART model (1024 tokens) to 16384 tokens."
        },
        {
            "heading": "5.2 Implementation Details",
            "text": "We use dropout with the probability 0.1 and a learning rate of 4e \u2212 5. The optimizer is Adam with \u03b21 = 0.9 and \u03b22 = 0.999. We also adopt the learning rate warmup and decay. During the decoding process, we use beam search with a beam size of 4 and tri-gram blocking to reduce repetitions. We adopt the implementations of LED from HuggingFace\u2019s Transformers (Wolf et al., 2020) and FiD from SEGENC (Vig et al., 2022). We maximize the input length to 16,384 tokens. All the models are trained on one NVIDIA A100-PCIE-80GB."
        },
        {
            "heading": "5.3 Results",
            "text": "As shown in Table 4, we calculated the ROUGE scores, BERTScore, Catalogue Edit Distance Similarity (CEDS) and Catalogue Quality Estimate (CQE) between the generated catalogues and oracle ones separately. We especially remove all level mark symbols (e.g. <L1>) for evaluation. In order to compare the ability of different methods to generate different levels of headings in detail, we calculated ROUGE scores for each level of headings with the corresponding level of oracle ones.\nWe first analyze the performance of traditional extractive approaches. Since these extractive models can not generate catalogues with an explicit hierarchy as abstractive methods, we only calculate the ROUGE scores of extracted results to the entire oracle catalogues (Column Total in Table 4). We take the ROUGE score (11.9/4.5/11.4) between titles of literature reviews and entire oracle catalogues\nas a threshold because titles are the most concise and precise content related to reviews. LexRank (10.5/1.7/10.0) and TextRank (12.9/1.2/12.1) only achieves similarly results as the threshold. This means extractive methods are not suitable for hierarchical catalogue generation.\nBy comparing all evaluation scores, the end-toend approach achieves higher similarity with the ground truth than step-by-step on the whole catalogue. Generally, the model with a larger number of parameters performs better. However, there are still duplication and hierarchical errors in the current results (See case studies in Appendix F).\nLarge language models have shown excellent performance on many downstream tasks due to massive and diverse pre-training. We also test two representative large language models: Galactica (Taylor et al., 2022) and ChatGPT (gpt-3.5-turbo). Galactica (GAL) is a large language model for science that achieve state-of-the-art results over many scientific tasks. The corpus it used includes over 48 million papers, textbooks, scientific websites and so on. ChatGPT is the best model recognized by the community and also trained on ultra large scale corpus. The corpus used by these two models in the pre-training phase contains scientific literature and thus we consider that the knowledge of the models contain the reference papers. From evaluation results, instructions understanding and answer readability, ChatGPT generates far better results than GAL. It\u2019s worth noting that large language models can not outperform models (LED-large) specially trained for this task. This reveals that simply stacking knowledge and parameters may not be a good solution for catalogue generation which requires more capabilities on logic and induction. See Appendix D for specific details and analysis."
        },
        {
            "heading": "5.4 Consistency Check",
            "text": ""
        },
        {
            "heading": "5.4.1 Human Evaluation",
            "text": "To demonstrate the validity of our proposed evaluation metrics, we conduct consistency tests on CEDS with human evaluation. We generate 50 catalogues for each implementation with a total number of 450 samples, each sample is evaluated by three professional evaluators. We skip Galactica6.7b for subsequent experiments since it can hardly generate reasonable catalogues. Evaluators are required to assess the quality of catalogues based on the similarity (ranging from one to five, where five represents the most similar) to the oracle one.\nFirst, we test the human evaluations and CEDS of corresponding data for normal distribution. After the Shapiro-Wilk test, the p-values for the two sets are 0.520 and 0.250, all greater than 0.05 (Table 8). That means these data groups can be considered to meet the normal distribution, which enables further Pearson correlation analysis. Person correlation analysis shows that p-values between CEDS and Human are 0.027, more diminutive than 0.05 (Table 9). The r-value is 0.634, which represents a strong positive correlation. Therefore, we consider CEDS as a valid evaluation indicator."
        },
        {
            "heading": "5.4.2 Automatic Evaluation",
            "text": "We also conduct a pair-wise consistency test between all the automatic indicators measured (Table 5) by Pearson\u2019s correlation coefficient, where only ROUGE-L in ROUGE was computed.\nFirst, a noticeable trend is that the ROUGE-L of the first-level catalogue (L1RL) is not correlated with any other metrics. We infer that this is due to the ease of generating first-level headings, which perform similarly across methods and reach bottlenecks. The second & third-level catalogue (L2RL, L3RL) exhibit an extremely strong positive correlation with the total one (TotalRL), which suggests that the effectiveness of generating second & third-level headings affects the overall performance of catalogue generation. Second, Catalogue Quality Estimate (CQE) negatively correlates with the ROUGE-L in three levels (L2RL, L3RL, TotalRL). This indicates that domain knowledge and reference information are mainly found in the secondary and tertiary headings, which is in line with human perception. Finally, we study the difference between TotalRL and BERTScore. We find that TotalRL and BERTScore are not relevant, but they are correlated with CEDS respectively. This means that CEDS can combine both ROUGE and BERTScore indicators and better describe the similarity between generated results and oracle ones."
        },
        {
            "heading": "6 Related Work",
            "text": "Survey generation Our work belongs to the multidocument summarization task of generation, which aims to reduce the reading burden of researchers. Survey generation is the most challenging task in multi-document scientific summarization. The early work was mainly an extractive approach based on various content selection ways (Mohammad et al. (2009), Jha et al. (2015)). The results\ngenerated by unsupervised selection models have significant coherence and duplication problems.\nTo make the generated results have a clear hierarchy, Hoang and Kan (2010) additionally inputs an associated topic hierarchy tree that describes a target paper\u2019s topics to drive the creation of an extractive related work section. Sun and Zhuge (2019) proposes a template-based framework for survey paper automatic generation. It allows users to compose a template tree that consists of two types of nodes, dimension node and topic node. Shuaiqi et al. (2022) trains classifiers based on BERT(Devlin et al., 2018) to conduct category-based alignment, where each sentence from academic papers is annotated into five categories: background, objective, method, result, and other. Next, each research topic\u2019s sentences are summarized and concatenated together. However, these template trees are either inflexible or require the users to have some background knowledge to give, which could be more friendly to beginners. Also, this defeats the original purpose of automatic summarization to aid reading.\nText structure Generation There are some efforts involving the automatic generation of text structures. Xu et al. (2021) employs a hierarchical model to learn structure among paragraphs, selecting informative sentences to generate surveys. But this structure is not explicit. Tan et al. (2021) generate long text by producing domain-specific keywords and then refining them into complete passages. The keywords can be considered as a structure\u2019s guide for the subsequent generation. Trairatvorakul et al. reorganize the hierarchical structures of three to six articles to generate a hierarchical structure for a survey. Fu et al. (2022) gen-\nerate slides from a multi-modal document. They generate slide titles, i.e., structures of one target document, via summarizing and placing an object. These efforts don\u2019t really model the relationships between multiple documents, which is far from the hierarchical structure of a review paper. This paper focuses on the generation of a hierarchical catalogue for literature review. Traditional automatic evaluation methods for summarization calculates similarity among unstructured texts from the perspective of overlapping units and contextual word embeddings (Papineni et al. (2002), Banerjee and Lavie (2005), Lin (2004), Zhang* et al. (2020), Zhao et al. (2019)). Tasks involving structured text generation are mostly measured using the accuracy, such as SQL statement generation (He et al. (2019), Wang et al. (2019)), which ignores semantic information. Assessing the similarity of catalogues to standard answers should consider both structural and semantic information."
        },
        {
            "heading": "7 Conclusion",
            "text": "In this work, we observe the significant effect of hierarchical guidance on literature review generation and introduce a new task called hierarchical catalogue generation for literature review (HiCatGLR) and develop a dataset HiCaD out of arXiv and Semantic Scholar Dataset. We empirically evaluate two methods with eight implementations for catalogue generation and found that the end-to-end model based on LED-large achieves the best result. CQE and CEDS are designed to measure the quality of generated results from informativeness, semantics, and hierarchical structures. Dataset and code are available at https://github.com/zhukun1020/HiCaD. Our analy-\nsis illustrates how CEDS resolves some of the limitations of current metrics. In the future, we plan to explore a better understanding of input content and the solution to headings level error, repetition, and semantic conflict between headings.\nLimitations\nThe hierarchical catalogue generation task can help with literature review generation. However, there is still much room for improvement in the current stage, especially in generating the second & thirdlevel headings, which requires a comprehensive understanding of all corresponding references. Besides, models will need to understand each reference completely rather than just their abstracts in the future. Our dataset, HiCaD, does not cover a comprehensive range of domains. In the future, we need to continue to expand the review data in other fields, such as medicine. Currently, we only experiment on a single domain due to limitations of collected data resources, where knowledge transfer across domains matters for catalogues generation.\nEthics Statement\nIn this paper, we present a new dataset, and we discuss here some relevant ethical considerations. (1) Intellectual property. The survey papers and their reference papers used for dataset construction are shared under the CC BY-SA 4.0 license. It is free for research use. (2) Treatment of data annotators. We hire the annotators from proper channels and pay them fairly for the agreed-upon salaries and workloads. All hiring is done under contract and in accordance with local regulations."
        },
        {
            "heading": "Acknowledgements",
            "text": "Xiaocheng Feng is the corresponding author of this work. We thank the anonymous reviewers for their insightful comments. This work was supported by the National Key R&D Program of China via grant No.2020AAA0106502, National Natural Science Foundation of China (NSFC) via grant 62276078, the Key R&D Program of Heilongjiang via grant 2022ZX01A32, the International Cooperation Project of PCL, PCL2022D01 and the Fundamental Research Funds for the Central Universities (Grant No.HIT.OCEF.2023018)."
        },
        {
            "heading": "A Dataset Domain",
            "text": "We count the fields to which the dataset belongs, and the computer science field have the most papers, followed by mathematics. In this paper, we use only papers from the computer science domain for experiments."
        },
        {
            "heading": "B Template Words",
            "text": "The list of template words used when calculating CQE in this paper is listed in Table 7."
        },
        {
            "heading": "C Consistency Check",
            "text": ""
        },
        {
            "heading": "D Large Language Models",
            "text": "In this section, we experiment with the effect of two foundation models on the hierarchical catalogue generation task.\nD.1 Galactica\nGalactica has five sizes ranging from 125M to 120B parameters and the standard size 6.7B is the max size we can use. We experimented one-shot test on our test set. There are two generation samples in Figure 4. It can be seen that there is a lot of repetition in the results.\nD.2 ChatGPT\nThe prompt of what we use to generate a directory using ChatGPT is:\nYour task is to write a table of contents for the review paper by recalling relevant papers, organizing and classifying them according to the given review paper topic. Only the first, second and third level headings need to be written, no detailed explanation is required. Please ensure that your catalogue is well-structured, clear, and concise, and accurately represents the topic\u2019s main research findings and methodologies.\nTitle: A Survey of Large Language Models Table of Contents:"
        },
        {
            "heading": "E Step by Step Method",
            "text": "Figure 5 illustrates one of the generation processes of the step-by-step generation method. Instead of generating the whole catalogue C directly, we propose to generate step-by-step: source \u2192 L1 \u2192 L1,2 \u2192 L1,2,3(target), where Li is the i-th level headings of the catalogue. For example, in the first step, we input survey title and information about its reference documents and generate the firstlevel headings L1 of the catalogue for that survey. Then, L1 is added to the input, and the next step generates the first two levels headings L1,2 of the\ncatalogue. Finally, it generates the whole catalogue with a method similar to the previous step. The generation process corresponds to a decomposition of the conditional probability as:\nP (t|s) = P (L1|s)P (L1,2|s, L1)P (L1,2,3|s, L1,2)"
        },
        {
            "heading": "F Case Study",
            "text": "Table 10 and Figure 6 present an example of the best catalogues generated by end-to-end models and step-by-step models about the title \"a survey of domain adaptation for neural machine translation\".\nTable 10 gives an example of alignment between catalogue items when calculating the Catalogue Edit Distance (CED). If an item does not match any other entity, then the required action for this item is insertion or deletion, so the cost is 1. If a node matches a node, then the required action is modification. The operation cost of modification is calculated according to the similarity. It is worth noting that Node[2] in the generated result does not match Node[1] in the ground truth, even though they have exactly the same content. It is because the former is a secondary heading, and the latter is a primary heading. If the alignment is forced, the cost required will be greater than the current result. Thus this example shows that not only does the metric CEDS measure catalogues from a semantic perspective, but also takes into account the hierarchical structure of the catalogue.\nThe parts marked in red in Figure 6 are the generation problems we observe. The first problem is duplicating the content in a single catalogue item, e.g. \"monolingual monolingual data\". The second one is the hierarchy error between sibling nodes. For example, \"evaluation metrics\" conceptually contain \"automatic evaluation\". Finally, there is a recurrence of the heading \"applications of nmt domain adaptation\". In summary, there are still duplication and hierarchical errors in the current results."
        }
    ],
    "title": "Hierarchical Catalogue Generation for Literature Review: A Benchmark",
    "year": 2023
}