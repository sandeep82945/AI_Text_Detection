{
    "abstractText": "Multi-Modal Entity Alignment (MMEA) is a critical task that aims to identify equivalent entity pairs across multi-modal knowledge graphs (MMKGs). However, this task faces challenges due to the presence of different types of information, including neighboring entities, multimodal attributes, and entity types. Directly incorporating the above information (e.g., concatenation or attention) can lead to an unaligned information space. To address these challenges, we propose a novel MMEA transformer, called MoAlign, that hierarchically introduces neighbor features, multi-modal attributes, and entity types to enhance the alignment task. Taking advantage of the transformer\u2019s ability to better integrate multiple information, we design a hierarchical modifiable self-attention block in a transformer encoder to preserve the unique semantics of different information. Furthermore, we design two entity-type prefix injection methods to integrate entity-type information using type prefixes, which help to restrict the global information of entities not present in the MMKGs. Our extensive experiments on benchmark datasets demonstrate that our approach outperforms strong competitors and achieves excellent entity alignment performance.",
    "authors": [
        {
            "affiliations": [],
            "name": "Qian Li"
        },
        {
            "affiliations": [],
            "name": "Cheng Ji"
        },
        {
            "affiliations": [],
            "name": "Shu Guo"
        },
        {
            "affiliations": [],
            "name": "Zhaoji Liang"
        },
        {
            "affiliations": [],
            "name": "Lihong Wang"
        },
        {
            "affiliations": [],
            "name": "Jianxin Li"
        }
    ],
    "id": "SP:98eb9e565f5bdc26f57c84bccfbe19dfd40b033b",
    "references": [
        {
            "authors": [
                "Stanislaw Antol",
                "Aishwarya Agrawal",
                "Jiasen Lu",
                "Margaret Mitchell",
                "Dhruv Batra",
                "C Lawrence Zitnick",
                "Devi Parikh."
            ],
            "title": "Vqa: Visual question answering",
            "venue": "ICCV, pages 2425\u20132433.",
            "year": 2015
        },
        {
            "authors": [
                "Antoine Bordes",
                "Nicolas Usunier",
                "Alberto Garc\u00edaDur\u00e1n",
                "Jason Weston",
                "Oksana Yakhnenko."
            ],
            "title": "Translating embeddings for modeling multirelational data",
            "venue": "Advances in Neural Information Processing Systems 26: 27th Annual Conference on",
            "year": 2013
        },
        {
            "authors": [
                "Xianshuai Cao",
                "Yuliang Shi",
                "Jihu Wang",
                "Han Yu",
                "Xinjun Wang",
                "Zhongmin Yan."
            ],
            "title": "Cross-modal knowledge graph contrastive learning for machine learning method recommendation",
            "venue": "MM \u201922: The 30th ACM International Conference on Multimedia,",
            "year": 2022
        },
        {
            "authors": [
                "Liyi Chen",
                "Zhi Li",
                "Yijun Wang",
                "Tong Xu",
                "Zhefeng Wang",
                "Enhong Chen."
            ],
            "title": "MMEA: entity alignment for multi-modal knowledge graph",
            "venue": "Knowledge Science, Engineering and Management - 13th International Conference, KSEM 2020, Hangzhou,",
            "year": 2020
        },
        {
            "authors": [
                "Zhuo Chen",
                "Jiaoyan Chen",
                "Wen Zhang",
                "Lingbing Guo",
                "Yin Fang",
                "Yufeng Huang",
                "Yuxia Geng",
                "Jeff Z. Pan",
                "Wenting Song",
                "Huajun Chen."
            ],
            "title": "Meaformer: Multi-modal entity alignment transformer for meta modality hybrid",
            "venue": "CoRR, abs/2212.14454.",
            "year": 2022
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
            "year": 2019
        },
        {
            "authors": [
                "Xiang Fang",
                "Daizong Liu",
                "Pan Zhou",
                "Yuchong Hu."
            ],
            "title": "Multi-modal cross-domain alignment network for video moment retrieval",
            "venue": "IEEE Transactions on Multimedia.",
            "year": 2022
        },
        {
            "authors": [
                "Xiang Fang",
                "Daizong Liu",
                "Pan Zhou",
                "Guoshun Nan."
            ],
            "title": "You can ground earlier than see: An effective and efficient pipeline for temporal sentence grounding in compressed videos",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pat-",
            "year": 2023
        },
        {
            "authors": [
                "Xiang Fang",
                "Daizong Liu",
                "Pan Zhou",
                "Zichuan Xu",
                "Ruixuan Li."
            ],
            "title": "Hierarchical local-global transformer for temporal sentence grounding",
            "venue": "IEEE Transactions on Multimedia.",
            "year": 2023
        },
        {
            "authors": [
                "Hao Guo",
                "Jiuyang Tang",
                "Weixin Zeng",
                "Xiang Zhao",
                "Li Liu."
            ],
            "title": "Multi-modal entity alignment in hyperbolic space",
            "venue": "Neurocomputing, 461:598\u2013607.",
            "year": 2021
        },
        {
            "authors": [
                "Phillip Howard",
                "Arden Ma",
                "Vasudev Lal",
                "Ana Paula Sim\u00f5es",
                "Daniel Korat",
                "Oren Pereg",
                "Moshe Wasserblat",
                "Gadi Singer."
            ],
            "title": "Cross-domain aspect extraction using transformers augmented with knowledge graphs",
            "venue": "Proceedings of the 31st ACM",
            "year": 2022
        },
        {
            "authors": [
                "Zhiwei Hu",
                "V\u00edctor Guti\u00e9rrez-Basulto",
                "Zhiliang Xiang",
                "Ru Li",
                "Jeff Z. Pan."
            ],
            "title": "Transformer-based entity typing in knowledge graphs",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu",
            "year": 2022
        },
        {
            "authors": [
                "Jin Jiang",
                "Mohan Li",
                "Zhaoquan Gu."
            ],
            "title": "A survey on translating embedding based entity alignment in knowledge graphs",
            "venue": "Sixth IEEE International Conference on Data Science in Cyberspace, DSC 2021, Shenzhen, China, October 9-11, 2021, pages",
            "year": 2021
        },
        {
            "authors": [
                "Qian Li",
                "Shu Guo",
                "Yangyifei Luo",
                "Cheng Ji",
                "Lihong Wang",
                "Jiawei Sheng",
                "Jianxin Li."
            ],
            "title": "Attribute-consistent knowledge graph representation learning for multi-modal entity alignment",
            "venue": "Proceedings of the ACM Web Conference 2023, WWW",
            "year": 2023
        },
        {
            "authors": [
                "Yangning Li",
                "Jiaoyan Chen",
                "Yinghui Li",
                "Yuejia Xiang",
                "Xi Chen",
                "Haitao Zheng."
            ],
            "title": "Vision, deduction and alignment: An empirical study on multi-modal knowledge graph alignment",
            "venue": "CoRR, abs/2302.08774.",
            "year": 2023
        },
        {
            "authors": [
                "Zhenxi Lin",
                "Ziheng Zhang",
                "Meng Wang",
                "Yinghui Shi",
                "Xian Wu",
                "Yefeng Zheng."
            ],
            "title": "Multi-modal contrastive representation learning for entity alignment",
            "venue": "Proceedings of the 29th International Conference on Computational Linguistics, COLING 2022,",
            "year": 2022
        },
        {
            "authors": [
                "Fangyu Liu",
                "Muhao Chen",
                "Dan Roth",
                "Nigel Collier."
            ],
            "title": "Visual pivoting for (unsupervised) entity alignment",
            "venue": "Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence,",
            "year": 2021
        },
        {
            "authors": [
                "Pengfei Liu",
                "Weizhe Yuan",
                "Jinlan Fu",
                "Zhengbao Jiang",
                "Hiroaki Hayashi",
                "Graham Neubig."
            ],
            "title": "Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing",
            "venue": "ACM Comput. Surv., 55(9):195:1\u2013195:35.",
            "year": 2023
        },
        {
            "authors": [
                "Weijie Liu",
                "Peng Zhou",
                "Zhe Zhao",
                "Zhiruo Wang",
                "Qi Ju",
                "Haotang Deng",
                "Ping Wang."
            ],
            "title": "K-BERT: enabling language representation with knowledge graph",
            "venue": "The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innova-",
            "year": 2020
        },
        {
            "authors": [
                "Xiao Liu",
                "Shiyu Zhao",
                "Kai Su",
                "Yukuo Cen",
                "Jiezhong Qiu",
                "Mengdi Zhang",
                "Wei Wu",
                "Yuxiao Dong",
                "Jie Tang."
            ],
            "title": "Mask and reason: Pre-training knowledge graph transformers for complex logical queries",
            "venue": "KDD \u201922: The 28th ACM SIGKDD Conference on",
            "year": 2022
        },
        {
            "authors": [
                "Ye Liu",
                "Hui Li",
                "Alberto Garc\u00eda-Dur\u00e1n",
                "Mathias Niepert",
                "Daniel O\u00f1oro-Rubio",
                "David S. Rosenblum."
            ],
            "title": "MMKG: multi-modal knowledge graphs",
            "venue": "The Semantic Web - 16th International Conference, ESWC 2019, Portoro\u017e, Slovenia, June 2-6, 2019, Proceed-",
            "year": 2019
        },
        {
            "authors": [
                "Zhiyuan Liu",
                "Yixin Cao",
                "Liangming Pan",
                "Juanzi Li",
                "Tat-Seng Chua."
            ],
            "title": "Exploring and evaluating attributes, values, and structures for entity alignment",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP",
            "year": 2020
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark",
                "Gretchen Krueger",
                "Ilya Sutskever"
            ],
            "title": "Learning transferable visual models from natural language",
            "year": 2021
        },
        {
            "authors": [
                "Kevin J Shih",
                "Saurabh Singh",
                "Derek Hoiem."
            ],
            "title": "Where to look: Focus regions for visual question answering",
            "venue": "CVPR, pages 4613\u20134621.",
            "year": 2016
        },
        {
            "authors": [
                "Rui Sun",
                "Xuezhi Cao",
                "Yan Zhao",
                "Junchen Wan",
                "Kun Zhou",
                "Fuzheng Zhang",
                "Zhongyuan Wang",
                "Kai Zheng."
            ],
            "title": "Multi-modal knowledge graphs for recommender systems",
            "venue": "CIKM \u201920: The 29th ACM International Conference on Information and Knowl-",
            "year": 2020
        },
        {
            "authors": [
                "Jiaan Wang",
                "Fandong Meng",
                "Duo Zheng",
                "Yunlong Liang",
                "Zhixu Li",
                "Jianfeng Qu",
                "Jie Zhou."
            ],
            "title": "A survey on cross-lingual summarization",
            "venue": "Trans. Assoc. Comput. Linguistics, 10:1304\u20131323.",
            "year": 2022
        },
        {
            "authors": [
                "Jiaan Wang",
                "Fandong Meng",
                "Duo Zheng",
                "Yunlong Liang",
                "Zhixu Li",
                "Jianfeng Qu",
                "Jie Zhou."
            ],
            "title": "Towards unifying multi-lingual and cross-lingual summarization",
            "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Lin-",
            "year": 2023
        },
        {
            "authors": [
                "Xiang Wang",
                "Xiangnan He",
                "Yixin Cao",
                "Meng Liu",
                "Tat-Seng Chua."
            ],
            "title": "KGAT: knowledge graph attention network for recommendation",
            "venue": "Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, KDD 2019,",
            "year": 2019
        },
        {
            "authors": [
                "Zeshi Wang",
                "Mohan Li",
                "Zhaoquan Gu."
            ],
            "title": "A review of entity alignment based on graph convolutional neural network",
            "venue": "Sixth IEEE International Conference on Data Science in Cyberspace, DSC 2021, Shenzhen, China, October 9-11, 2021, pages",
            "year": 2021
        },
        {
            "authors": [
                "Zhichun Wang",
                "Qingsong Lv",
                "Xiaohan Lan",
                "Yu Zhang."
            ],
            "title": "Cross-lingual knowledge graph alignment via graph convolutional networks",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Bel-",
            "year": 2018
        },
        {
            "authors": [
                "Zhichun Wang",
                "Qingsong Lv",
                "Xiaohan Lan",
                "Yu Zhang."
            ],
            "title": "Cross-lingual knowledge graph alignment via graph convolutional networks",
            "venue": "Proceedings of the 2018 conference on empirical methods in natural language processing, pages 349\u2013357.",
            "year": 2018
        },
        {
            "authors": [
                "Sen Yang",
                "Xuanhan Wang",
                "Lianli Gao",
                "Jingkuan Song."
            ],
            "title": "MKE-GCN: multi-modal knowledge embedded graph convolutional network for skeletonbased action recognition in the wild",
            "venue": "IEEE International Conference on Multimedia and Expo, ICME",
            "year": 2022
        },
        {
            "authors": [
                "Yunzhi Yao",
                "Shaohan Huang",
                "Li Dong",
                "Furu Wei",
                "Huajun Chen",
                "Ningyu Zhang."
            ],
            "title": "Kformer: Knowledge injection in transformer feed-forward layers",
            "venue": "Natural Language Processing and Chinese Computing: 11th CCF International Confer-",
            "year": 2022
        },
        {
            "authors": [
                "Xiangru Zhu",
                "Zhixu Li",
                "Xiaodan Wang",
                "Xueyao Jiang",
                "Penglei Sun",
                "Xuwu Wang",
                "Yanghua Xiao",
                "Nicholas Jing Yuan."
            ],
            "title": "Multi-modal knowledge graph construction and application: A survey",
            "venue": "CoRR, abs/2202.05786.",
            "year": 2022
        },
        {
            "authors": [
                "Chen"
            ],
            "title": "2020) proposed a method that uses a multi-modal fusion module to integrate knowledge representations of different types",
            "year": 2021
        },
        {
            "authors": [
                "Chen"
            ],
            "title": "In addition, we compared our method with four MMEA methods focusing on utilizing multi-modal attributes: (7) PoE (Liu et al., 2019) utilizes image features and measures credibility by matching the semantics of entities",
            "year": 2019
        },
        {
            "authors": [
                "EVA (Liu"
            ],
            "title": "2021) combines multi-modal attributes and relations with an attention mechanism to learn the importance of modality",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Multi-modal entity alignment (MMEA) is a challenging task that aims to identify equivalent entity pairs across multiple knowledge graphs that feature different modalities of attributes, such as text and images. To accomplish this task, sophisticated models are required to effectively leverage information from different modalities and accurately align entities. This task is essential for various applications, such as cross-lingual information retrieval, question answering (Antol et al., 2015; Shih et al., 2016), and recommendation systems (Sun et al., 2020; Xu et al., 2021).\n\u2217Corresponding author.\nMMEA (Liu et al., 2019; Li et al., 2023b; Liu et al., 2021; Lin et al., 2022) is challenging due to the heterogeneity of MMKGs (e.g., different neighbors, multi-modal attributes, distinct types), which makes it difficult to learn rich knowledge representations. Previous approaches such as PoE (Liu et al., 2019) concatenated all modality features to create composite entity representations but failed to capture interactions among heterogeneous modalities. More recent works (Chen et al., 2020; Guo et al., 2021) designed multi-modal fusion modules to better integrate attributes and entities, but still did not fully exploit the potential interactions among modalities. These methods also ignored inter-modality dependencies between entity pairs, which could lead to incorrect alignment. Generally speaking, although MMKGs offer rich attributes and neighboring entities that could be useful for multi-mdoal entity alignment, current methods have limitations in (i) ignoring the differentiation and personalization of the aggregation of heterogeneous neighbors and modalities leading to the misalignment of cross-modal semantics, and (ii) lacking the use of entity heterogeneity resulting in the non-discriminative representations of different\nmeaning/types of entities. Therefore, the major challenge of MMEA task is how to perform differentiated and personalized aggregation of heterogeneous information of the neighbors, modalities, and types. Although such information is beneficial to entity alignment, directly fusing will lead to misalignment of the information space, as illustrated in Figure 1. Firstly, notable disparities between different modalities make direct alignment a challenging task. For example, both the visual attribute of entity Ruby in MMKG1 and the neighbor information of the entity Ruby in MMKG2 contain similar semantics of programming, but data heterogeneity may impede effective utilization of this information. Secondly, complex relationships between entities require a thorough understanding and modeling of contextual information and semantic associations. Entities such as the Ruby, the Perl, and the entity Larry Wall possess unique attributes, and their inter-relationships are non-trivial, necessitating accurate modeling based on contextual information and semantic associations. Furthermore, the existence of multiple meanings for entities further exacerbates the challenge of distinguishing between two entities, such as in the case of the Ruby, which has different meanings in the MMKG1 and MMKG3 where it may be categorized as a jewelry entity or a programming language entity, respectively.\nTo overcome the aforementioned challenges, we propose a novel Multi-Modal Entity Alignment Transformer named MoAlign1. Our framework hierarchically introduces neighbor, multimodal attribute, and entity types to enhance the alignment task. We leverage the transformer architecture, which is known for its ability to process heterogeneous data, to handle this complex task. Moreover, to enable targeted learning on different modalities, we design a hierarchical modifiable self-attention block in the Transformer encoder, which builds associations of task-related intra-modal features through the layered introduction. Additionally, we introduce positional encoding to model entity representation from both structure and semantics simultaneously. Furthermore, we integrate entitytype information using an entity-type prefix, which helps to restrict the global information of entities that are not present in the multi-modal knowledge graphs. This prefix enables better filtering out of\n1The source code is available at https://github.com/ xiaoqian19940510/MoAlign.\nunsuitable candidates and further enriches entity representations. To comprehensively evaluate the effectiveness of our proposed approach, we design training objectives for both entity and context evaluation. Our extensive experiments on benchmark datasets demonstrate that our approach outperforms strong competitors and achieves excellent entity alignment performance. Our contributions can be summarized as follows.\n\u2022 We propose a novel MMEA framework named MoAlign, which effectively integrates heterogeneous information through the multi-modal KG Transformer.\n\u2022 We design a hierarchical modifiable selfattention block to build associations of taskrelated intra-modal features through the layered introduction and design an entity-type prefix to further enrich entity representations.\n\u2022 Experimental results indicate that the framework achieves state-of-the-art performance on the public multi-modal entity alignment datasets."
        },
        {
            "heading": "2 Preliminaries",
            "text": "Multi-Modal Knowledge Graph. A multimodal knowledge graph (MKG) is represented by four sets: entities (E), relations (R), multi-modal attributes (A), and triplets (T ). The size of each set is denoted by NE , NR, and NA. The multi-modal attributes are divided into text (AT ) and image (AI) attributes. The relation setR includes entity relations (RE), text attribute relations (RT ), and image attribute relations (RI). The set of triplets T includes entity triplets, text attribute triplets, and image attribute triplets.\nMulti-Modal Entity Alignment Task. Multimodal entity alignment (Chen et al., 2020; Guo et al., 2021; Liu et al., 2021; Chen et al., 2022) aims to determine if two entities from different multimodal knowledge graphs refer to the same realworld entity. This involves calculating the similarity between pairs of entities, known as alignment seeds. The goal is to learn entity representations from two multi-modal knowledge graphs (MKG1 and MKG2) and calculate the similarity between a pair of entity alignment seeds (e, e\u2032) taken from these KGs. The set of entity alignment seeds is denoted as S = {(e, e\u2032) | e \u2208 E , e\u2032 \u2208 E \u2032, e \u2261 e\u2032}."
        },
        {
            "heading": "3 Framework",
            "text": "This section introduces our proposed framework MoAlign. As shown in Figure 2, we introduce positional encoding to simultaneously model entity representation from both modality and structure. To hierarchically introduce neighbor and multi-modal attributes, we design a hierarchical modifiable selfattention block. This block builds associations of task-related intra-modal features through the layered introduction. Furthermore, for integrating entity-type information, we design a prefix-injected self-attention mechanism, which helps to restrict the global information of entities not present in the MMKGs. Additionally, MoAlign also design training objectives for both entity and context evaluation to comprehensively assess the effectiveness."
        },
        {
            "heading": "3.1 Multi-Modal Input Embedding",
            "text": ""
        },
        {
            "heading": "3.1.1 Multi-Modality Initialization",
            "text": "The textual attribute is initialized by BERT (Devlin et al., 2019). For the visual attributes, to enable direct processing of images by a standard transformer, the image is split into a sequence of patches (Dosovitskiy et al., 2021). We then perform a flatten operation to convert the matrix into a one-dimensional vector similar to word embeddings, which is similar to embeddings in BERT (Devlin et al., 2019) and concatenate them to form the image embedding vector, denoted as vv. However, the initialization\nof word embeddings follows a specific distribution, whereas the distribution of image embeddings generated using this method is not consistent with the pretraining model\u2019s initial distribution. Therefore, to standardize the distribution of image embeddings to be consistent with the distribution of the vocabulary used by the pretraining model, we perform the following operation:\nvv \u2190 (vv \u2212mean(vv))/std(vv) \u00b7 \u03bb, (1)\nwhere mean(vv) and std(vv) are the mean and standard deviation of vv, respectively, and \u03bb is the standard deviation of truncated normal function."
        },
        {
            "heading": "3.1.2 Positional Encoding",
            "text": "For the transformer input, we additionally input two types of positional encoding to maintain structure information of multi-modal KG as follows.\nModality Positional Encoding. To enable the model to effectively distinguish between entities, textual attributes, image attributes, and introduced entity types, we incorporate a unique position code for each modality. These position codes are then passed through the encoding layers of the model to enable it to differentiate between different modalities more accurately and learn their respective features more effectively. Specifically, we assign position codes of 1, 2, 3, and 4 to entities, textual attributes, image attributes, and introduced entity\ntypes, respectively. By incorporating this modality positional encoding information into the encoding process, our proposed model is able to incorporate modality-specific features into the alignment task.\nStructure Positional Encoding. To capture the positional information of neighbor nodes, we introduce a structure positional encoding that assigns a unique position code to each neighbor. This allows the model to distinguish between them and effectively incorporate the structure information of the knowledge graph into alignment process. Specifically, we assign a position code of 1 to the current entity and its attributes. For first-order neighbors, we randomly initialize a reference order and use it to assign position codes to each neighbor and its corresponding relation as 2n and 2n + 1, respectively. Additionally, we assign the same structure positional encoding of attributes to their corresponding entities. By doing so, the model can differentiate between different neighbor nodes and effectively capture their positional information.\nTo fully utilize the available information, we extract relation triplets and multi-modal attribute triplets for each entity. The entity is formed by combining multi-modal sequences as {e,(e1,r1),. . . ,(en, rn),(a1,v1),. . . ,(am,vm), eT }, where (ei, ri) represents the i-th neighbor of entity e and its relation. (aj ,vj) represents the j-th attribute of entity e and its value vj , and it contains textual and visual attributes. n and m are the numbers of neighbors and attributes. eT is the type embeddding."
        },
        {
            "heading": "3.2 Hierarchical Modifiable Self-Attention",
            "text": "To better capture the dependencies and relationships of entities and multi-modal attributes, we incorporate cross-modal alignment knowledge into the transformer. Specifically, a hierarchical modifiable self-attention block is proposed to better capture complex interactions between modalities and contextual information. The block focuses on associations of inter-modal (between modalities, such as textual and visual features) by cross-modal attention. By doing so, the model can capture more informative and discriminative features."
        },
        {
            "heading": "3.2.1 Hierarchical Multi-Head Attention",
            "text": "We propose a novel hierarchical block that incorporates distinct attention mechanisms for different inputs, which facilitates selective attention toward various modalities based on their significance in the\nalignment task. The transformer encoder comprises three separate attention mechanisms, namely neighbor attention, textual attention, and visual attention. We utilize different sets of queries, keys, and values to learn diverse types of correlations between the input entity, its attributes, and neighbors.\nNeighbor Multi-Head Attention. More specifically, in the first layer of the hierarchical block of the l-th transformer layer, we employ the input entity as queries and its neighbors as keys and values to learn relations between the entity and its neighbors:\ne(l.1) = MH-Attn(e, ei, ei), (2)\nwhere ei is the neighbor of entity e and e(l.1) is the representation of entity e in the first layer.\nTextual Multi-Head Attention. In the second layer of the hierarchical block, the input entity and its textual attributes are used as queries and keys/values to learn the correlations between the entity and its textual attributes.\ne(l.2) = MH-Attn(e(l.1), [at;vt], [at;vt]), (3)\nwhere (at,vt) represents the textual attribute of entity e and its value, and e(l.2) is the representation of entity e in the second layer.\nVisual Multi-Head Attention. In the third layer of the hierarchical block, the input entity and its visual attributes are used similarly to the second layer, to learn the correlations between the entity and its visual attributes.\ne(l.3) = MH-Attn(e(l.2), [av;vv], [av;vv]), (4)\nwhere (av,vv) represents the visual attribute of entity e and its value, and e(l.3) is the representation of entity e in the third layer. By incorporating neighbor attention, textual attribute attention, and visual attribute attention, our model can capture various correlations between the input entity, its attributes, and neighbors in a more effective manner."
        },
        {
            "heading": "3.2.2 Modifiable Self-Attention",
            "text": "To learn a specific attention matrix for building correlations among entities and attributes, we design a modifiable self-attention mechanism. We manage to automatically and adaptively generate an information fusion mask matrix based on sequence features. The length of the sequence and the types of information it contains may vary depending on the entity, as some entities may lack certain attribute information and images.\nModifiable Self-Attention Mechanism. To adapt to the characteristics of different sequences, it is necessary to assign \"labels\" to various information when generating the sequence, such as using [E] to represent entities and [R] to represent relations. This way, the positions of various information in the sequence can be generated, and the mask matrix can be generated accordingly.\nThese labels need to be processed by the model\u2019s tokenizer after inputting the model and generating the mask matrix to avoid affecting the subsequent generation of embeddings. We can still modify the vocabulary to allow the tokenizer to recognize these words and remove them. The mask matrix can be generated based on the positions of various information, represented by labels, as follows:\nMij= { 1, if(i, j, \u00b7)\u2208T orT (i) = T (j) 0, otherwise , (5)\nwhere T (\u00b7) is the type mapping function of entities and attributes.\nSubsequently, residual connections and layer normalization are utilized to merge the output of hierarchical modifiable self-attention el and apply a position-wise feed-forward network to each element in the output sequence of the self-attention. The output sequence, residual connections, and layer normalization are employed as:\ne(l) = LayerNorm ( e(l\u22121) + FFN(e(l)) ) , (6)\nwhere el is the output of the hierarchical modifiable self-attention in the l-th layer of the transformer. The use of residual connections and layer normalization helps stabilize training and improve performance. It enables our model to better understand and attend to different modalities , leading to more accurate alignment results."
        },
        {
            "heading": "3.3 Entity-Type Prefix Injection",
            "text": "Prefix methods can help improve alignment accuracy by providing targeted prompts to the model to improve generalization, including entity type information (Liu et al., 2023). The entity-type injection introduces a type of information that takes as input a pair of aligned seed entities from different MMKGs and uses entities as prompts. The goal is to inject entity-type information into the multi-head attention and feed-forward neural networks.\nPrefix-Injected Self-Attention Mechanism. Specifically, we create two sets of prefix vectors for entities, pk,pv \u2208 Rnt\u00d7d, for keys and values separately, where nt is the number of entity types. These prefix vectors are then concatenated with the original key K and value V . The multi-head attention in Eq.(2) is performed on the newly formed prefixed keys and values as follows:\nMH-Attn(Q,K,V)=Concat(h1,\u00b7 \u00b7 \u00b7,hN)Wo, hi=Pi-Attn ( QWqi ,[KW k i ;p k],[VWvi ;p v] ) , (7)\nwhere dN is typically set to d/N , and N is number of head. hi denotes i-th output of self-attention in l-th layer. Wo \u2208 Rd\u00d7d,Wqi ,Wki ,Wvi \u2208 Rd\u00d7dN are learnable parameters. In this paper, we use the type embedding eT as the prefix.\nIn order to effectively incorporate alignment information, we utilize the mask token to influence attention weights. The mask token serves as a means to capture effective neighbors and reduce computational complexity. To achieve this, we set attention weights m to a large negative value based on the mask matrix Mij in the modifiable self-attention mechanism. Consequently, attention weights for the mask token are modified as follows:\nPi-Attn(Q,Kt,Vt)=Softmax\n( QKTt+m\u221a\ndk\n) Vt, (8)\nwhere Q,Kt, and Vt represent the query of the entity, the key containing entity type, and the value containing entity type matrices, respectively. dk refers to the dimensionality of key vectors, while m represents the attention weights.\nPrefix-Injected Feed Forward Network. Recent research suggests that the feed-forward layers within the transformer architecture store factual knowledge and can be regarded as unnormalized key-value memories (Yao et al., 2022). Inspired by this, we attempt to inject entity type into each feed-forward layer to enhance the model\u2019s ability to capture the specific information related to the entity type. Similar to prefix injection in attention, we first repeat the entity type eT to create two sets of vectors, \u03a6k,\u03a6v \u2208 Rnt\u00d7d. These vectors are then concatenated with the original parameter matrices of the first and second linear layers.\nFFN(E) = f ( E \u00b7 [Wkf ;\u03a6k] ) \u00b7 [Wvf ;\u03a6v], (9)\nwhere f denotes the non-linearity function, E represents the output of the hierarchical modifiable self-attention. Wkf ,W v f are the parameters."
        },
        {
            "heading": "3.4 Training Objective",
            "text": "In order to effectively evaluate the MMEA from multiple perspectives, we propose a training objective function that incorporates both aligned entity similarity and context similarity. This objective function serves to assess the quality of entity alignment and takes into consideration both the entities themselves and their surrounding context.\nAligned Entity Similarity. The aligned entity similarity constraint loss is designed to measure similarity between aligned entity pairs and ensure that embeddings of these entities are close to each other in the embedding space.\nLEA=sim(e, e\u2032)\u2212sim(e, e\u2032)\u2212sim(e, e\u2032), (10)\nwhere (e, e\u2032) represent the final embeddings of the aligned seed entities (e, e\u2032) from knowledge graphs KG1 and KG2. e and e\u2032 denote the negative samples of the seed entities. sim(\u00b7, \u00b7) refers to the cosine distance between the embeddings.\nContext Similarity. The context similarity loss is employed to ensure that the context representations of aligned entities are close to each other in the embedding space.\nLCon=sim(o,o\u2032)\u2212sim(o,o\u2032)\u2212sim(o,o\u2032), (11)\nwhere (o,o\u2032) denote the final context representations ([cls] embedding) of the aligned seed entities (e, e\u2032). o and o\u2032 represent the negative samples of the seed entities\u2019 context.\nThe total alignment loss L is computed as:\nL = \u03b1LEA + \u03b2LCon, (12)\nwhere \u03b1, \u03b2 are learnable hyper-parameters."
        },
        {
            "heading": "4 Experiment",
            "text": ""
        },
        {
            "heading": "4.1 Dataset",
            "text": "We have conducted experiments on two of the most popular datasets, namely FB15K-DB15K and FB15K-YAGO15K, as described in (Liu et al., 2019). The FB15K-DB15K dataset2 is an entity alignment dataset of FB15K and DB15K MMKGs, while the latter is a dataset of FB15K and YAGO15K MMKGs. Consistent with prior works (Chen et al., 2020; Guo et al., 2021), we split each dataset into training and testing sets in proportions of 2:8, 5:5, and 8:2, respectively. The MRR, Hits@1, and Hits@10 are reported for evaluation on different proportions of alignment seeds.\n2https://github.com/mniepert/mmkb"
        },
        {
            "heading": "4.2 Comparision Methods",
            "text": "We compare our method with three EA baselines (TransE (Bordes et al., 2013), GCN-align (Wang et al., 2018a), and AttrGNN (Liu et al., 2020b)), which aggregate text attributes and relation information, and introduced image attributes initialized by VGG16 for entity representation using the same aggregation method as for text attributes. We further use three transformer models (BERT (Devlin et al., 2019), ViT (Dosovitskiy et al., 2021), and CLIP (Radford et al., 2021)) to incorporate multi-modal information, and three MMEA methods (PoE (Liu et al., 2019), Chen et al. (Chen et al., 2020), HEA (Guo et al., 2021), EVA (Liu et al., 2021), and ACK-MMEA (Li et al., 2023a)) to focusing on utilizing multi-modal attributes. The detail is given in Appendix B."
        },
        {
            "heading": "4.3 Implementation Details",
            "text": "We utilized the optimal hyperparameters reported in the literature for all baseline models. Our model was implemented using PyTorch, an opensource deep learning framework. We initialized text and image attributes using bert-base-uncased3 and VGG164, respectively. To ensure fairness, all baselines were trained on the same data set partition. The best random dropping rate is 0.35, and coefficients \u03b1, \u03b2 were set to 5 and 2, respectively. All hyperparameters were tuned on validation data using 5 trials. All experiments were performed on a server with one GPU (Tesla V100)."
        },
        {
            "heading": "4.4 Main Results",
            "text": "To verify the effectiveness of MoAlign, we report overall average results in Table 1. It shows performance comparisons on both two datasets with different splits on training/testing data of alignment seeds, i.e., 2:8, 5:5, and 8:2. From the table, we can observe that: 1) Our model outperforms all baselines of both EA, multi-modal Transformer-based, and MMEA methods, in terms of three metrics on both datasets. It demonstrates that our model is robust to different proportions of training resources and learns a good performance on few-shot data. 2) Compared to EA baselines (1-3), especially for MRR and Hits@1, our model improves 5% and 9% up on average on FB15K-DB15K and FB15KYAGO15K, tending to achieve more significant improvements. It demonstrates that the effective-\n3https://github.com/huggingface/transformers 4https://github.com/machrisaa/tensorflow-vgg\nness of multi-modal context information benefits incorporating alignment knowledge. 3) Compared to multi-modal transformer-based baselines, our model achieves better results and the transformerbased baselines perform better than EA baselines. It demonstrates that transformer-based structures can learn better MMEA knowledge. 4) Compared to MMEA baselines, our model designs a Hierarchical Block and modifiable self-attention mechanism, the average gains of our model regarding MRR, Hits@1, and Hits@10 are 2%, 1.4%, and 1.7%, respectively. The reason is that our method incorporates multi-modal attributes and robust context entity information. All the observations demonstrate the effectiveness of MoAlign."
        },
        {
            "heading": "4.5 Discussions for Model Variants",
            "text": "To investigate the effectiveness of each module in MoAlign, we conduct variant experiments, showcasing the results in Table 2. The \"\u2193\" means the value of performance degradation compared to the MoAlign. We can observe that: 1) The impact of the Hierarchical Block tends to be more significant. We believe the reason is that the adaptive introduction of multi-modal attributes and neighbors captures more clues. 2) By replacing the modifiable self-attention to the multi-head self-attention, the performance decreased significantly. It demonstrates that the modifiable self-attention captures more effective multi-modal attribute and relation information. 3) When we remove all image at-\ntributes as \u201cw/o image attribute\", our method drops 2.7% and 2.7% on average on FB15K-DB15K and FB15K-YAGO15K. It demonstrates that image attributes can improve model performance and our method utilizes image attributes effectively by capturing more alignment knowledge."
        },
        {
            "heading": "4.6 Impact of Multi-modal Attributes",
            "text": "To further investigate the impact of multi-modal attributes on all compared methods, we report the results by deleting different modalities of attributes, as shown in Figure 3. From the figure, we can observe that: 1) The variants without the text or image attributes significantly decline on all evaluation metrics, which demonstrates that the multi-modal attributes are necessary and effective for MMEA. 2) Compared to other baselines, our model derives better results both in the case of using all multi-modal attributes or abandoning some of them. It demonstrates our model makes full use of existing multimodal attributes, and multi-modal attributes are effective for MMEA. All the observations demonstrate that the effectiveness of the MMKG transformer encoder and the type-prompt encoder.\nIn addition, to investigate the impact of the order in which multi-modal attributes are introduced to the model, we conduct experiments with different orders of introducing neighbor information, tex-\ntual attributes, and visual attributes. As shown in the Table 3, the introduction order has a significant impact on our model. Specifically, the best performance is achieved when textual attributes, visual attributes, and neighbor information are introduced in that order. This suggests that aggregating attribute information to learn a good entity representation first, and then incorporating neighbor information, can effectively utilize both the attributes and neighborhood information of nodes."
        },
        {
            "heading": "4.7 Impact of Interference Data",
            "text": "We investigate the impact of interference data on our proposed method for MMEA. Specifically, we randomly replace 5%, 10%, 15%, 20%, 25% and 30% of the neighbor entities and attribute information to test the robustness of our method, as shown in Figure 4. The experimental results demonstrate that our method exhibits better tolerance to interference compared to the baseline methods. This suggests that our approach, which incorporates hierarchical information from different modalities and introduces type prefix to entity representations, is capable of handling interference data and improving the robustness of the model."
        },
        {
            "heading": "5 Conclusion",
            "text": "This paper proposes a novel MMEA framework. It incorporates cross-modal alignment knowledge using a two-stage transformer encoder to better capture complex inter-modality dependencies and semantic relationships. It includes a MMKG transformer encoder that uses self-attention mechanisms to establish associations between intra-modal features relevant to the task. Our experiments show that our approach outperforms competitors.\nLimitations\nOur work hierarchically introduces neighbor, multimodal attribute, and entity types to enhance the alignment task. Empirical experiments demonstrate that our method effectively integrates heterogeneous information through the multi-modal KG Transformer. However, there are still some limitations of our approach can be summarized as follows:\n\u2022 Due to the limitation of the existing MMEA datasets, we only experiment on entity, text, and image modalities to explore the influence of multi-modal features. We will study more modalities in future work.\n\u2022 Our approach employs the transformer encoder architecture, which entails a substantial time overhead. In forthcoming investigations, we intend to explore the feasibility of leveraging promptbased techniques to mitigate the computational burden and expedite model training.\nEthics Statement\nIn this work, we propose a new MMEA framework that hierarchically introduces neighbor, multimodal attribute, and entity types to benchmark our architecture with baseline architectures on the two MNEA datasets.\nData Bias. Our framework is tailored for multimodal entity alignment in the general domain. Nonetheless, its efficacy may be compromised when confronted with datasets exhibiting dissimilar distributions or in novel domains, potentially leading to biased outcomes. The experimental results presented in the section are predicated on particular benchmark datasets, which are susceptible to such biases. As a result, it is imperative to exercise caution when assessing the model\u2019s generalizability and fairness.\nComputing Cost/Emission. Our study, which involves the utilization of large-scale language models, entails a substantial computational overhead. We acknowledge that this computational burden has a detrimental environmental impact in terms of carbon emissions. Specifically, our research necessitated a cumulative 588 GPU hours of computation utilizing Tesla V100 GPUs. The total carbon footprint resulting from this computational process is estimated to be 65.27 kg of CO2 per run, with a total of two runs being conducted."
        },
        {
            "heading": "Acknowledgment",
            "text": "We thank the anonymous reviewers for their insightful comments and suggestions. Jianxin Li is the corresponding author. The authors of this paper were supported by the NSFC through grant No.62225202, 62106059."
        },
        {
            "heading": "A Related Work",
            "text": "A.1 Multi-Modal Entity Alignment In the real-world, due to the multi-modal nature of KGs, there have been several works (Zhu et al., 2022; Wang et al., 2021; Jiang et al., 2021; Fang et al., 2022) that have started to focus on MMEA technology. One popular approach is to use embeddings to represent entities and their associated modalities (Cao et al., 2022; Yang et al., 2022). These embeddings can then be used to measure the similarity between entities and align them across different KGs. Wang et al. (Wang et al., 2018b) proposed a framework that uses cross-modal embeddings to align entities across different modalities, such as text and images. The model maps entities from different modalities into a shared embedding space, where entities that correspond to the same real-world object are close to each other. However, this approach cannot capture the potential interactions among heterogeneous modalities, limiting its capacity for performing accurate entity alignments. To address this limitation, some researchers have proposed multi-modal knowledge embedding methods that can discriminatively generate knowledge representations of different types of knowledge and then integrate them. Chen et al. (2020) proposed a method that uses a multi-modal fusion module to integrate knowledge representations of different types. Similarly, Guo et al. (2021) proposed a GNN-based model that learns to aggregate information from different modalities and propagates it across the knowledge graph to align entities. It developed a hyperbolic multi-modal entity alignment (HEA) approach that combines both attribute and entity representations in the hyperbolic space and uses aggregated embeddings to predict alignments. EVA (Liu et al., 2021) combines images, structures, relations, and attributes information for the MMEA with a learnable weighted attention to learn the importance of each modal attributes. Despite these advances, existing methods often ignore contextual gaps between entity pairs, which may limit the effectiveness of alignment.\nA.2 Knowledge Graph Transformer The Transformer architecture, originally proposed for natural language processing tasks, has been applied to various knowledge graph tasks as well (Liu et al., 2022; Hu et al., 2022; Howard et al., 2022; Wang et al., 2023, 2022; Fang et al., 2023b,a). For example, the KGAT model (Wang et al., 2019)\nuses a graph attention mechanism to capture the complex relations between entities in a knowledge graph, and a Transformer to learn representations of the entities for downstream tasks such as link prediction and entity recommendation. The K-BERT model (Liu et al., 2020a) extends this approach by pre-training a Transformer on a large corpus of textual data, and then fine-tuning it on a knowledge graph to improve entity and relation extraction. The transformer model has the ability to model longrange dependencies, where entities and their associated modalities can be distant from each other in the knowledge graph. Furthermore, Transformers utilize attention mechanisms to weigh the importance of different inputs and focus on relevant information, which is particularly useful for aligning entities across different modalities."
        },
        {
            "heading": "B Comparision Methods",
            "text": "We compared our method with three EA baselines that aggregate text attributes and relation information, and introduced image attributes initialized by VGG16 for entity representation using the same aggregation method as for text attributes. The three EA methods compared are as follows: (1) TransE (Bordes et al., 2013) assumes that the entity embedding v should be close to the attribute embedding a plus their relation r. (2) GCN-align (Wang et al., 2018a) transfers entities and attributes from each language to a common representation space through GCN. (3) AttrGNN (Liu et al., 2020b) divides the KG into multiple subgraphs, effectively modeling various types of attributes.\nWe also compared our method with three transformer models that incorporate multi-modal information: (4) BERT (Devlin et al., 2019) is a pre-trained model to generate representations. (5) ViT (Dosovitskiy et al., 2021) is a visual transformer model that partitions an image into patches and feeds them as input sequences. (6) CLIP (Radford et al., 2021) is a joint language and vision model architecture that employs contrastive learning.\nIn addition, we compared our method with four MMEA methods focusing on utilizing multi-modal attributes: (7) PoE (Liu et al., 2019) utilizes image features and measures credibility by matching the semantics of entities. (8) Chen et al. (Chen et al., 2020) designs a fusion module to integrate multi-modal attributes. (9) HEA (Guo et al., 2021) characterizes MMKG in hyperbolic space. (10)\nEVA (Liu et al., 2021) combines multi-modal attributes and relations with an attention mechanism to learn the importance of modality. (11) ACKMMEA (Liu et al., 2021) designs an attributeconsistent KG representation framework to compensate contextual gaps."
        }
    ],
    "title": "Multi-Modal Knowledge Graph Transformer Framework for Multi-Modal Entity Alignment",
    "year": 2023
}