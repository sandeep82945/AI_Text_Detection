{
    "abstractText": "Commonsense norms are defeasible by context: reading books is usually great, but not when driving a car. While contexts can be explicitly described in language, in embodied scenarios, contexts are often provided visually. This type of visually grounded reasoning about defeasible commonsense norms is generally easy for humans, but (as we show) poses a challenge for machines, as it necessitates both visual understanding and reasoning about commonsense norms. We construct a new multimodal benchmark for studying visual-grounded commonsense norms: NORMLENS. NORMLENS consists of 10K human judgments accompanied by freeform explanations covering 2K multimodal situations, and serves as a probe to address two questions: (1) to what extent can models align with average human judgment? and (2) how well can models explain their predicted judgments? We find that state-of-theart model judgments and explanations are not well-aligned with human annotation. Additionally, we present a new approach to better align models with humans by distilling social commonsense knowledge from large language models. The data and code are released at https://seungjuhan.me/normlens.",
    "authors": [
        {
            "affiliations": [],
            "name": "Seungju Han"
        },
        {
            "affiliations": [],
            "name": "Junhyeok Kim"
        },
        {
            "affiliations": [],
            "name": "Jack Hessel"
        },
        {
            "affiliations": [],
            "name": "Liwei Jiang"
        },
        {
            "affiliations": [],
            "name": "Jiwan Chung"
        },
        {
            "affiliations": [],
            "name": "Yejin Son"
        },
        {
            "affiliations": [],
            "name": "Yejin Choi"
        },
        {
            "affiliations": [],
            "name": "Youngjae Yu"
        }
    ],
    "id": "SP:d7c498316db27b94f8d9cd5b63d58dfa5fb8106e",
    "references": [
        {
            "authors": [
                "Satanjeev Banerjee",
                "Alon Lavie."
            ],
            "title": "Meteor: An automatic metric for mt evaluation with improved correlation with human judgments",
            "venue": "Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summariza-",
            "year": 2005
        },
        {
            "authors": [
                "Wei-Lin Chiang",
                "Zhuohan Li",
                "Zi Lin",
                "Ying Sheng",
                "Zhanghao Wu",
                "Hao Zhang",
                "Lianmin Zheng",
                "Siyuan Zhuang",
                "Yonghao Zhuang",
                "Joseph E. Gonzalez",
                "Ion Stoica",
                "Eric P. Xing"
            ],
            "title": "Vicuna: An opensource chatbot impressing gpt-4 with 90%* chatgpt",
            "year": 2023
        },
        {
            "authors": [
                "Wenliang Dai",
                "Junnan Li",
                "Dongxu Li",
                "Anthony Meng Huat Tiong",
                "Junqi Zhao",
                "Weisheng Wang",
                "Boyang Li",
                "Pascale Fung",
                "Steven Hoi."
            ],
            "title": "Instructblip: Towards general-purpose vision-language models with instruction tuning",
            "venue": "arXiv preprint",
            "year": 2023
        },
        {
            "authors": [
                "Maxwell Forbes",
                "Jena D. Hwang",
                "Vered Shwartz",
                "Maarten Sap",
                "Yejin Choi."
            ],
            "title": "Social chemistry 101: Learning to reason about social and moral norms",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing",
            "year": 2020
        },
        {
            "authors": [
                "Seungju Han",
                "Jack Hessel",
                "Nouha Dziri",
                "Yejin Choi",
                "Youngjae Yu."
            ],
            "title": "Champagne: Learning real-world conversation from large-scale web videos",
            "venue": "arXiv preprint arXiv:2303.09713.",
            "year": 2023
        },
        {
            "authors": [
                "Dan Hendrycks",
                "Collin Burns",
                "Steven Basart",
                "Andrew Critch",
                "Jerry Li",
                "Dawn Song",
                "Jacob Steinhardt."
            ],
            "title": "Aligning ai with shared human values",
            "venue": "Proceedings of the International Conference on Learning Representations (ICLR).",
            "year": 2021
        },
        {
            "authors": [
                "Jack Hessel",
                "Jena D Hwang",
                "Jae Sung Park",
                "Rowan Zellers",
                "Chandra Bhagavatula",
                "Anna Rohrbach",
                "Kate Saenko",
                "Yejin Choi."
            ],
            "title": "The Abduction of Sherlock Holmes: A Dataset for Visual Abductive Reasoning",
            "venue": "ECCV.",
            "year": 2022
        },
        {
            "authors": [
                "Youngkyu Hong",
                "Seungju Han",
                "Kwanghee Choi",
                "Seokjun Seo",
                "Beomsu Kim",
                "Buru Chang."
            ],
            "title": "Disentangling label distribution for long-tailed visual recognition",
            "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Liwei Jiang",
                "Jena D Hwang",
                "Chandra Bhagavatula",
                "Ronan Le Bras",
                "Maxwell Forbes",
                "Jon Borchardt",
                "Jenny Liang",
                "Oren Etzioni",
                "Maarten Sap",
                "Yejin Choi."
            ],
            "title": "Delphi: Towards machine ethics and norms",
            "venue": "arXiv preprint arXiv:2110.07574.",
            "year": 2021
        },
        {
            "authors": [
                "Zhijing Jin",
                "Sydney Levine",
                "Fernando Gonzalez Adauto",
                "Ojasv Kamal",
                "Maarten Sap",
                "Mrinmaya Sachan",
                "Rada Mihalcea",
                "Josh Tenenbaum",
                "Bernhard Sch\u00f6lkopf"
            ],
            "title": "When to make exceptions: Exploring language models as accounts of human moral judgment",
            "year": 2022
        },
        {
            "authors": [
                "Jeff Johnson",
                "Matthijs Douze",
                "Herv\u00e9 J\u00e9gou."
            ],
            "title": "Billion-scale similarity search with GPUs",
            "venue": "IEEE Transactions on Big Data, 7(3):535\u2013547.",
            "year": 2019
        },
        {
            "authors": [
                "Douwe Kiela",
                "Hamed Firooz",
                "Aravind Mohan",
                "Vedanuj Goswami",
                "Amanpreet Singh",
                "Pratik Ringshia",
                "Davide Testuggine."
            ],
            "title": "The hateful memes challenge: Detecting hate speech in multimodal memes",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Hyunwoo Kim",
                "Jack Hessel",
                "Liwei Jiang",
                "Ximing Lu",
                "Youngjae Yu",
                "Pei Zhou",
                "Ronan Le Bras",
                "Malihe Alikhani",
                "Gunhee Kim",
                "Maarten Sap",
                "Yejin Choi."
            ],
            "title": "Soda: Million-scale dialogue distillation with social commonsense contextualization",
            "venue": "arXiv",
            "year": 2022
        },
        {
            "authors": [
                "Nikita Kitaev",
                "Dan Klein."
            ],
            "title": "Constituency parsing with a self-attentive encoder",
            "venue": "Annual Meeting of the Association for Computational Linguistics.",
            "year": 2018
        },
        {
            "authors": [
                "Sydney Levine",
                "Joshua Rottman",
                "Taylor Davis",
                "Elizabeth O\u2019Neill",
                "Stephen Stich",
                "Edouard Machery"
            ],
            "title": "Religious affiliation and conceptions of the moral domain",
            "venue": "Social Cognition,",
            "year": 2021
        },
        {
            "authors": [
                "Junnan Li",
                "Dongxu Li",
                "Silvio Savarese",
                "Steven Hoi."
            ],
            "title": "Blip-2: Bootstrapping language-image pretraining with frozen image encoders and large language models",
            "venue": "arXiv preprint arXiv:2301.12597.",
            "year": 2023
        },
        {
            "authors": [
                "Chin-Yew Lin."
            ],
            "title": "Rouge: A package for automatic evaluation of summaries",
            "venue": "Text summarization branches out, pages 74\u201381.",
            "year": 2004
        },
        {
            "authors": [
                "Tsung-Yi Lin",
                "Michael Maire",
                "Serge Belongie",
                "James Hays",
                "Pietro Perona",
                "Deva Ramanan",
                "Piotr Doll\u00e1r",
                "C Lawrence Zitnick."
            ],
            "title": "Microsoft coco: Common objects in context",
            "venue": "Computer Vision\u2013 ECCV 2014: 13th European Conference, Zurich,",
            "year": 2014
        },
        {
            "authors": [
                "Haotian Liu",
                "Chunyuan Li",
                "Qingyang Wu",
                "Yong Jae Lee."
            ],
            "title": "Visual instruction tuning",
            "venue": "arXiv preprint arXiv:2304.08485.",
            "year": 2023
        },
        {
            "authors": [
                "Clark."
            ],
            "title": "Self-refine: Iterative refinement with self-feedback",
            "venue": "ArXiv, abs/2303.17651.",
            "year": 2023
        },
        {
            "authors": [
                "Sewon Min",
                "Julian Michael",
                "Hannaneh Hajishirzi",
                "Luke Zettlemoyer."
            ],
            "title": "Ambigqa: Answering ambiguous open-domain questions",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 5783\u2013",
            "year": 2020
        },
        {
            "authors": [
                "OpenAI."
            ],
            "title": "Chatgpt",
            "venue": "https://chat.openai. com/.",
            "year": 2022
        },
        {
            "authors": [
                "Kishore Papineni",
                "Salim Roukos",
                "Todd Ward",
                "WeiJing Zhu."
            ],
            "title": "Bleu: a method for automatic evaluation of machine translation",
            "venue": "Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 311\u2013318.",
            "year": 2002
        },
        {
            "authors": [
                "Jae Sung Park",
                "Chandra Bhagavatula",
                "Roozbeh Mottaghi",
                "Ali Farhadi",
                "Yejin Choi."
            ],
            "title": "Visualcomet: Reasoning about the dynamic context of a still image",
            "venue": "In Proceedings of the European Conference on Computer Vision (ECCV).",
            "year": 2020
        },
        {
            "authors": [
                "Jordi Pont-Tuset",
                "Jasper Uijlings",
                "Soravit Changpinyo",
                "Radu Soricut",
                "Vittorio Ferrari."
            ],
            "title": "Connecting vision and language with localized narratives",
            "venue": "Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Pro-",
            "year": 2020
        },
        {
            "authors": [
                "Valentina Pyatkin",
                "Jena D Hwang",
                "Vivek Srikumar",
                "Ximing Lu",
                "Liwei Jiang",
                "Yejin Choi",
                "Chandra Bhagavatula."
            ],
            "title": "Reinforced clarification question generation with defeasibility rewards for disambiguating social and moral situations",
            "venue": "arXiv preprint",
            "year": 2022
        },
        {
            "authors": [
                "Noah Shinn",
                "Beck Labash",
                "Ashwin Gopinath."
            ],
            "title": "Reflexion: an autonomous agent with dynamic memory and self-reflection",
            "venue": "ArXiv, abs/2303.11366.",
            "year": 2023
        },
        {
            "authors": [
                "Stephen Stich."
            ],
            "title": "The quest for the boundaries of morality",
            "venue": "The Routledge handbook of moral epistemology, pages 15\u201337. Routledge.",
            "year": 2018
        },
        {
            "authors": [
                "Zeerak Talat",
                "Hagen Blix",
                "Josef Valvoda",
                "Maya Indira Ganesh",
                "Ryan Cotterell",
                "Adina Williams."
            ],
            "title": "On the machine learning of ethical judgments from natural language",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Asso-",
            "year": 2022
        },
        {
            "authors": [
                "Elliot Turiel."
            ],
            "title": "The development of social knowledge: Morality and convention",
            "venue": "Cambridge University Press.",
            "year": 1983
        },
        {
            "authors": [
                "Yizhong Wang",
                "Yeganeh Kordi",
                "Swaroop Mishra",
                "Alisa Liu",
                "Noah A Smith",
                "Daniel Khashabi",
                "Hannaneh Hajishirzi."
            ],
            "title": "Self-instruct: Aligning language model with self generated instructions",
            "venue": "arXiv preprint arXiv:2212.10560.",
            "year": 2022
        },
        {
            "authors": [
                "Peter West",
                "Chandra Bhagavatula",
                "Jack Hessel",
                "Jena Hwang",
                "Liwei Jiang",
                "Ronan Le Bras",
                "Ximing Lu",
                "Sean Welleck",
                "Yejin Choi."
            ],
            "title": "Symbolic knowledge distillation: from general language models to commonsense models",
            "venue": "Proceedings of the",
            "year": 2022
        },
        {
            "authors": [
                "Rowan Zellers",
                "Yonatan Bisk",
                "Ali Farhadi",
                "Yejin Choi."
            ],
            "title": "From recognition to cognition: Visual commonsense reasoning",
            "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR).",
            "year": 2019
        },
        {
            "authors": [
                "Andy Zeng",
                "Maria Attarian",
                "Brian Ichter",
                "Krzysztof Choromanski",
                "Adrian Wong",
                "Stefan Welker",
                "Federico Tombari",
                "Aveek Purohit",
                "Michael Ryoo",
                "Vikas Sindhwani",
                "Johnny Lee",
                "Vincent Vanhoucke",
                "Pete Florence"
            ],
            "title": "Socratic models: Composing",
            "year": 2022
        },
        {
            "authors": [
                "Caleb Ziems",
                "Jane Dwivedi-Yu",
                "Yi-Chia Wang",
                "Alon Halevy",
                "Diyi Yang."
            ],
            "title": "Normbank: A knowledge bank of situational social norms",
            "venue": "arXiv preprint arXiv:2305.17008.",
            "year": 2023
        }
    ],
    "sections": [
        {
            "text": "We construct a new multimodal benchmark for studying visual-grounded commonsense norms: NORMLENS. NORMLENS consists of 10K human judgments accompanied by freeform explanations covering 2K multimodal situations, and serves as a probe to address two questions: (1) to what extent can models align with average human judgment? and (2) how well can models explain their predicted judgments? We find that state-of-theart model judgments and explanations are not well-aligned with human annotation. Additionally, we present a new approach to better align models with humans by distilling social commonsense knowledge from large language models. The data and code are released at https://seungjuhan.me/normlens."
        },
        {
            "heading": "1 Introduction",
            "text": "Reasoning about commonsense norms1 highly depends on the context in which actions are performed (Pyatkin et al., 2022; Jin et al., 2022; Ziems et al., 2023). While an action reading a book is generally considered positive, the action is deemed\n1One line of developmental moral psychology tradition argues moral and social conventional norms present salient distinctions (Turiel, 1983). Nevertheless, recent studies point out that these two concepts are inherently interconnected without meaningful distinctions (Stich, 2018). Additionally, other recent studies identify that what counts as moral or socially acceptable is highly provincial (Levine et al., 2021). In this work, we consider a wide range of socio-moral judgments for our inclusive definition of commonsense norms.\nby image? Our NORMLENS dataset is a multimodal benchmark to evaluate how well models align with human reasoning about defeasible commonsense norms, incorporating visual grounding.\nto be wrong in the context of driving a car because the attention should be focused on the road. Understanding the defeasible commonsense norms \u2014 norms that could be further strengthened or attenuated based on the context \u2014 are crucial, and prior works (Hendrycks et al., 2021; Jiang et al., 2021; Forbes et al., 2020) have primarily focused on the defeasible norms based solely on text inputs.\nHowever, real-world scenarios often lack explicit contextual information described in language. Consider the situations depicted in Figure 1: when humans see the first image, the action of reading a book will be considered to be wrong. Conversely, when looking at the second image, the same action will be considered to be okay as reading a book together while sitting on the couch is viewed positively. When humans make judgments, they perceive the visual scene, make adjustments to reflect the visual defeasible cues, and then make intuitive judgments. It is a more natural process to go directly from visual scene to judgment, but this is very understudied.\nIn this work, we study model capacity for visually grounded reasoning about defeasible common-\nsense norms that align with humans. To this end, we introduce NORMLENS, a dataset consisting of 10K human annotations about 2K multimodal situations. Our dataset covers diverse situations about defeasible commonsense norms (\u00a72). Each situation consists of a visual context and an associated action, and five human annotators make moral judgments about the situation and provide explanations for the judgments.\nTo construct a truly multimodal benchmark centered around defeasible commonsense norms, we employ a data collection pipeline that is based on human-AI collaboration (see Figure 3). The starting point is image-description pairs sourced from existing vision-language datasets \u2014 Sherlock (Hessel et al., 2022), COCO captions (Lin et al., 2014), and Localized Narratives (Pont-Tuset et al., 2020) dataset. Then, we utilize language models (LMs) to generate a set of multimodal situations conditioned on input descriptions such that: (1) the generated action is morally appropriate given the context provided by the input image description, and (2) in contrast, the generated action is morally inappropriate under the generated situation (\u00a72.1). Finally, for each multimodal situation, we employ human annotation to collect moral judgments and explanations (\u00a72.2).\nAn important consideration in constructing our benchmark is the subjective nature of moral judgments (Talat et al., 2022), which can lead to disagreements among individuals when facing a single situation. For instance, in the last image of Figure 2, one human annotator deems it is rude to read a book during a concert, while others find it is okay or reading a book is impractical during a\nconcert. To consider this inherent characteristic of moral reasoning task, we organize our benchmark by splitting the dataset into two different parts (NORMLENSHA and NORMLENSMA) based on the degree of agreement among human annotators (\u00a72.3).\nWe design two tests based on NORMLENS to study how well models\u2019 predictions align with humans in this context (\u00a73). Given a multimodal situation, a model is asked to (1) provide a moral judgment about the situation, and (2) offer a plausible explanation for its judgment. Experimental results demonstrate that these tests are challenging even for state-of-the-art large pretrained models (\u00a74). In particular, models struggle to account for defeasible visual contexts, and also often fail to identify cases where humans agree that the action is impossible to perform.\nFinally, we investigate a method for improving model agreement with human judgment without relying on additional human annotations (\u00a75). We begin by utilizing image-description pairs once more, seeding image descriptions into the LM to generate 90K instances of actions with judgments and explanations. Then, we construct multimodal situations by combining the generated actions and images that are paired with provided descriptions. Subsequently, we fine-tune models using these generated examples, and find that fine-tuned models exhibit better alignments with humans, achieving the highest improvement of 31.5% compared to the counterpart in the judgment task for NORMLENSHA.\nIn summary, our main contributions are: 1. NORMLENS, a new dataset/benchmark of\n10K human annotations covering 2K multimodal situations about commonsense norms.\n2. Two new tasks posed over the corpus: making judgments and explaining judgments.\n3. Experimental results demonstrating that while these two tasks remain challenging for models, that multimodal models can be improved with a newly proposed text-only distillation step."
        },
        {
            "heading": "2 Overview of NORMLENS",
            "text": "The NORMLENS dataset is a new multimodal benchmark. The purpose of the corpus is to assess models\u2019 capacity to perform visually-grounded reasoning about defeasible commonsense norms. The dataset covers wide range of multimodal situations in real-world. Each situation in the dataset is annotated by multiple human annotators with moral judgments and explanations about judgments (as in Figure 2).\nTo collect NORMLENS, we employ human-AI collaboration. Given a multimodal situation, we collect human judgments, which serve as labels to measure correlation between model predictions. In early testing, we found that humans had trouble concocting diverse and interesting multimodal situations. Thus, we utilize a LM to help \u201cbrainstorm\" input situations. More specifically, we (1) generate multimodal situations that follow the requirement using AI models (\u00a72.1), especially considering the defeasibility of commonsense norms, and (2) employ human annotators to collect actual human judgments and explanations about the generated multimodal situations (\u00a72.2). Detailed analysis about the dataset is provided in \u00a72.3. Our data pipeline is illustrated in Figure 3."
        },
        {
            "heading": "2.1 Generating Multimodal Situations about Defeasible Commonsense Norms with AI",
            "text": "To sample situations that manifest multimodallydefeasible commonsense norms, we define a requirement: generated situations should consist an action that itself is generally considered to be \u201cokay,\" but wrong for given context (e.g., an action is \u201creading a book\u201d, and context is \u201cdriving a car\u201d). This stage consists of three steps: (1) generating text-form situations (D \u2192 ST0 ), (2) gradually filtering the situations that do not meet the requirement (ST0 \u2192 ST1 \u2192 ST2 ), (3) retrieving the image to convert text-form situations into multimodal situations (ST2 \u2192 SM0 ), and (4) running a diversity filter (SM0 \u2192 SM1 ). Details about prompts and filters are\nin Appendix B. We use ChatGPT (GPT-3.5-turbo) as our LM for the data-generation pipeline.\nGenerating Text-form Situations with LM. To initiate, we randomly sample 15K image descriptions D = {d0, ..., dN\u22121} (not the image) from existing vision-language datasets. We concatenated three datasets for a source to promote diversity: Sherlock (Hessel et al., 2022), Localized Narratives (Pont-Tuset et al., 2020), and COCO Captions (Lin et al., 2014) dataset. These datasets are characterized by different design principles: for image descriptions, Sherlock provides inferences, Localized Narratives offers fine-grained details, and COCO captions presents representative captions for the given images.\nBy feeding D to the LM, we generate text-form situations. Given the image description di, the LM is prompted with di to generate action and context pair (ai, cTi ) under the following instruction: generated action ai should be morally okay with the given image description di, but should be morally wrong with the generated context cTi . For example, when di is \u201ctwo people seating together on sofa\u201d, then possible ai is \u201creading a book\u201d and cTi is \u201cdriving a car\u201d. After generation, we have ST0 = {(a0, cT0 ), ..., (aM\u22121, cTM\u22121)}. Note that we generate three action-context pairs per given image description, so M = 3N .\nSequential Filtration with LM. The LMgenerated actions are error prone: while we instruct the LM to generate the action ai which is not morally acceptable for a generated context ci, the LM frequently generates actions that are okay or not possible to perform in the ci; Madaan et al. (2023); Shinn et al. (2023) also observe LMs sometimes fail to follow complex instructions.\nInspired by the success of iterative refinement with simpler instructions, we apply two automatic sequential filters using the LM. The first filter (implemented with a prompt) attempts to remove impossible actions: for example, if the generated action is follow the traffic rules and the generated context is a group of people running in a park, then this situation should be filtered because there is no traffic rules in the park for runners. Second filter (also implemented with a prompt) aims to remove examples from ST1 if the LM predicts that generated action ai is morally appropriate to perform in the generated context cTi . After filtration, we have ST2 = {(a0, cT0 ), ..., (aL\u22121, cTL\u22121)}, where L\nis number of instances after sequential filtration.\nCreating Multimodal Situations by Image Retrieval. We create multimodal situations SM0 from ST2 . We construct a FAISS index (Johnson et al., 2019) of 1.4M image descriptions {d1, ..., dM} (which is a superset of D in the first step), by using the LM to turn image descriptions into LM-based text embeddings. Then, we use generated text-form context cTi as a query to find the similar image description dl from the index and obtain the corresponding image of the description xl. Finally, we yield 18K multimodal situations SM0 = {(a0, x0), ..., (aL\u22121, xL\u22121)}.\nDiversity Filtration. We observe that certain keywords like funeral and hospital come up frequently in the contexts in SM0 . To enrich the diversity of the contexts, we set up the list of specific keywords and filter out examples if the language description d of the image x includes one of the specific keywords. We keep the occurrence of these keywords from contexts under 30."
        },
        {
            "heading": "2.2 Collecting Annotations from Humans",
            "text": "After the first stage, we randomly sample 2.2K instances from SM1 and ask human workers to provide annotations. Further details concerning human annotations processes, including on the annotation interface, can be found in Appendix C.\nMaking Judgments and Explaining Judgments. Our procedure involves instructing human annotators to make judgments, denoted as yi, pertaining to a given multimodal situation, represented as (ai, xi). They are provided with three options: the action is (1) morally inappropriate, (2) morally appropriate, and (3) not possible to perform phys-\nically. We also request the annotators to descriptively explain their judgments in free-form text ei. To account for the subjectivity inherent in moral judgments, each situation is annotated by five different people.\nValidation. After the previous annotation step, we exclude annotations with implausible explanations about judgments by additional validation step. For example, consider the first situation in Figure 2. If someone labeled the situation as Okay. with the explanation \u201cIt is morally okay to read a book, because reading a book is always great\u201d, then this annotation should be excluded as the explanation does not make sense. Each annotation (yi, ei) for the situation (xi, ai) is provided to one worker, and workers are asked to review the explanations for the judgments. After reviewing, they mark each annotations as either I agree or I do not agree. Only annotations that are marked as I agree are retained."
        },
        {
            "heading": "2.3 Dataset Analysis",
            "text": "The result of our data pipeline is 2.2K multimodal situations (image-action pairs) with pertaining multiple moral judgments and explanations.\nDisagreement Among Annotators. We observe that for approximately half of the situations, there is a divergence in the judgments offered by different annotators (as in the third and the fourth examples in Figure 2). This discrepancy is induced by the inherent variability of moral reasoning, in which commonsense norms can be influenced by cultural differences and diverse perspectives.\nWe take into account this inherent subjectivity by splitting the dataset into two subparts: NORMLENSHA (HA=High Agreement) and NORMLENSMA (MA=Medium Agreement). In NORM-\nLENSHA, there is a unanimous consensus among all annotators regarding the moral judgment for situations, as in the first and the second situations in Figure 2. In NORMLENSMA, two out of three options regarding the moral judgment are chosen by annotators, e.g., one annotator chooses Wrong., and the other four annotators choose Okay., as in the third situation in Figure 2. We note that in 10% (230) of instances, human annotation results exhibit that all judgments could be possible (e.g., the last situation in Figure 2). We have excluded these instances from the evaluation, but they will still be made available as they can serve as a potentially valuable resource for further exploration.\nWeakness of LM for Creating Situations. We find the necessity of our human annotation stage to construct the benchmark about commonsense norms. As shown in Table 1, more than 70% of the situations are judged as okay or impossible. Considering that we only run annotations with the situations that the system determined to be morally wrong, it suggests that machine-generated judgments are frequently misaligned with human judgments. In other words, it is not possible to construct high-quality benchmark about commonsense norms without human annotations."
        },
        {
            "heading": "3 Task Overview",
            "text": "We conduct two tests based on NORMLENS to examine the extent to which the models\u2019 predictions aligns with humans on visually grounded reasoning task regarding defeasible commonsense norms.\nMaking Judgments. The first test requires models to provide a moral judgment about given mul-\ntimodal situation to investigate how well the models align with human judgments. Given an action ai and an image xi, the model returns a judgment y\u0302i. There is a corresponding set of human judgments, denoted as Yi = {y0i , ..., y n\u22121 i }, and\nn (\u2264 5) varies. There are three possible judgments \u2014 Wrong (Wr.), Okay (Ok.), and Action is Impossible (Im.) \u2014 i.e., y\u0302i and yki must be included in {Wr.,Ok., Im.}. To measure the degree of alignment, we use precision as a metric, i.e., model is considered in alignment with human judgments if one of the yki \u2208 Yi is equal to y\u0302i.\nExplaining Judgments. We further require models to provide explanations about their judgments since moral judgments are subjective; thus, the underlying rationale of judgment becomes crucial. Assume that model returns a judgment y\u0302i for a given situation and generates an explanation e\u0302i about y\u0302i. We assess how well the generated explanation e\u0302i is aligned with humans\u2019 explanation about judgments. Inspired by Min et al. 2020, we use an explanation score Ei that is formulated as Ei = max0\u2264j\u2264n\u22121 \u03b4(y\u0302i, y j i ) \u00b7 f(e\u0302i, e j i ), where \u03b4(y\u0302i, y j i ) = 1 if y\u0302i is the same as y j i else it is a zero, and f(e\u0302i, e j i ) is a similarity score between generated explanation and the human\u2019s explanation. For the similarity score f , we take into account BLEU2 (Papineni et al., 2002), Rouge-L (Lin, 2004), and METEOR (Banerjee and Lavie, 2005). As NORMLENSMA may contain varying numbers of explanations per label, we assess models solely on the explaining task using NORMLENSHA."
        },
        {
            "heading": "4 Do Pretrained Models Align Well with Humans?",
            "text": ""
        },
        {
            "heading": "4.1 Models",
            "text": "For sanity check, we incorporate two model-less baselines: Random guesses the judgment randomly, and Majority Vote always selects the most frequent class (i.e., Im. for NORMLENSHA). We provide four in-context examples as additional inputs for all baselines below.\nLM. Our text-only unimodal baselines include an open-source language model (Vicuna-13B; Chiang et al. 2023) and a comprehensive list of the state-of-the-art proprietary LMs such as GPT4 (GPT-4-0314; OpenAI 2023), ChatGPT (GPT3.5-turbo; OpenAI 2022), and GPT-3 (Curie and Davinci; Brown et al. 2020). The baselines evaluate how well machines can align with human judg-\nments only with actions. We do not test the LMs against explanation generation since our human explanations are strongly dependent on the visual inputs and are not directly comparable to the explanations only for action.\nSocratic Model (SM). SM (Zeng et al., 2022) works in a two-staged framework, where the first stage transforms the visual inputs into intermediate text descriptions using a vision-language model (VLM), and the next stage applies reasoning on the descriptions using the LM. To implement SMs, we use the same set of LMs as described above and use BLIP-2 Flan-12B (Li et al., 2023) as the VLM.\nVLM. Different from SMs, here we include baselines that directly output the judgments from the VLMs without an external reasoning stage. We cover the state-of-the-art pretrained VLMs LLaVA (Liu et al., 2023), BLIP-2 (Li et al., 2023), and InstructBLIP (Dai et al., 2023)."
        },
        {
            "heading": "4.2 Results",
            "text": "Metrics. We report the scores averaged classwise: we first compute averages of scores per class and then get the final score by averaging the classlevel scores uniformly. We employ this macro average to counteract the class imbalance (Hong et al., 2021) in NORMLENS.\nMaking Judgments. We share three notable findings from our results on the judgment task (Table 2). (1) In general, pretrained models partially align their predictions with averaged human judgments,\nbut a gap remains between model predictions and human agreement. In particular, models except for SMs with powerful LMs (ChatGPT/GPT-4) perform almost on par with Majority Vote. (2) Visual inputs are important. All the SMs clearly outperform their text-only counterparts (LM) except for GPT-3 Davinci. (3) Reasoning capability is also crucial. All VLMs show a low level of alignment, particularly in NORMLENSHA where they score between 34.0% to 41.9% and are outcompeted by Majority Vote. In contrast, SM paired with powerful LMs exhibit the highest level of alignment among the baselines, with the best model (GPT-4) achieving 74.7% and 85.9% on NORMLENSHA and NORMLENSMA, respectively. Additionally, we note that VLMs utilizing Vicuna-13B show lower scores than the text-only counterpart, suggesting that these VLMs are not effectively utilizing visual perception for reasoning.\nExplaining Judgments. As shown in Table 2b, SM built on GPT-4 achieves the best explanation scores among the baselines in NORMLENSHA, establishing a strong baseline for the task. As in the previous judgment task, we attribute this strong performance of GPT-4 to its formidable reasoning capability (Bubeck et al., 2023). The score gaps between SM using GPT-4 and the other baselines are also significant. We believe these gaps indicate that VLMs require a stronger reasoning capability to perform reasoning on NORMLENS.\nError Analysis on Making Judgments. To investigate the difficulties encountered by models\nwhen making judgments, in Table 3, we provide classwise precision scores on NORMLENSHA (full break-down results are in Appendix E). Overall, except for SM with stronger LMs (ChatGPT/GPT-4), models show low judgment scores on Wrong. and Impossible. classes. On the other hand, SM with GPT-4 shows impressive scores across all three classes, particularly excelling in the Impossible. class compared to baselines, resulting in the highest overall score. Interestingly, SM with ChatGPT achieves the highest score on Wrong. class (71.1%). We suspect that this might be attributed to the data pipeline using ChatGPT, which is employed to collect multimodal situations that are likely to be morally wrong based on judgments of ChatGPT.\nWe raise an interesting question: considering the fact that ChatGPT is employed in our data pipeline, why does SM with ChatGPT only exhibits 71.1% on the Wrong class, rather than nearing 100%? We suspect that this is due to errors in BLIP-2 prediction. The key distinction between ChatGPT in the data pipeline and SM with ChatGPT in the testing situation is the inclusion of precise image descriptions. To explore this further, with SM built on ChatGPT, we further test on the judgment task by using ground-truth image descriptions as inputs instead of relying on BLIP-2 predictions. The model shows a higher score in the Wrong. class (80.2% v.s. 71.1%), but demonstrates lower scores in the other classes (Okay - 59.7% v.s. 67.7%, Impossible - 42.1% v.s. 52.9%). This result infers that visual reasoning capability is crucial for SMs, as the scores are highly affected by visual grounding."
        },
        {
            "heading": "5 Better Aligning Models with Humans",
            "text": "Our findings indicate that most SMs and VLMs face challenges when it comes to visually grounded reasoning about defeasible commonsense norms. Here, we explore an efficient solution that can enhance both SMs and VLMs for better alignment with human values. Drawing inspirations from recent works that distill knowledge from LMs (West et al., 2022; Wang et al., 2022; Kim et al., 2022), we propose using text-only LMs to build annotations for our multimodal problem automatically.\nWe use the LM (ChatGPT) to generate 90K examples of multimodal situations, including moral judgments and explanations. In particular, we begin with randomly sampling 30K image descriptions from image-text datasets (same dataset in \u00a72.1). Then, we prompt the LM with the given image description to generate three different actions that are: (1) morally wrong, (2) morally okay, and (3) unrelated to the context. Finally, these generated actions are then combined with the images associated with the provided image descriptions, resulting in the construction of multimodal situations. These instances are splitted into train-validation sets with an 8:1 ratio and use the valid set for the hyperparameter search.\nThere are significant distinctions between the data pipeline discussed in \u00a72 and the generation process described here. Firstly, the data pipeline involves the collection of human annotations. Secondly, the data pipeline places emphasis on defeasibility, employing specific instructions for LM to generate examples, which are then subjected to multiple filtration steps.\nResults. Automatic training data generation offers an accessible alternative to expensive human annotations. We fine-tune the SMs (only the LM parts) and VLMs to predict judgment and explanations when the generated situation is given. As shown in 4a, the machine-generated data improves alignment scores in most cases. Especially, scores\nin Wrong. and Impossible. classes are improved across the board as depicted in Table 4b.\nStill, there is a decrease in scores for the Okay. class, indicating that the machine-generated data induces more conservative model decisions. More details are described in Appendix F."
        },
        {
            "heading": "6 Related Works",
            "text": "Visually Grounded Reasoning. Various tasks have emerged in the field of visually grounded reasoning, including commonsense reasoning (Zellers et al., 2019; Park et al., 2020) and abductive reasoning (Hessel et al., 2022). With the advent of LMs that have powerful reasoning capabilities (Chiang et al., 2023; OpenAI, 2023), methods that harness the general reasoning capabilities of LMs for visual grounded reasoning settings are proposed (Wu et al., 2023; Chase, 2022). For example, Socratic Models (Zeng et al., 2022) turn visual contexts into language description and take this description as input for LMs to perform reasoning. In contrast, there exist vision-language models that process visual information and directly perform reasoning (Li et al., 2023; Dai et al., 2023; Liu et al., 2023; Han et al., 2023). Despite their general visual grounded reasoning capability and potent applications, their reasoning abilities about commonsense norms are not yet explored.\nCommonsense Norms. Jiang et al. (2021) present Delphi, a commonsense moral reasoning model trained to present a descriptive view of ethical judgments. In ClarifyDelphi, Pyatkin et al.\n(2022) work towards contextualizing moral reasoning, producing a system to ask clarification questions to elicit the context surrounding a judgment. In contrast, our work directly generates contextualizations to strengthen or attenuate the morality of an action without asking specific questions. Jin et al. (2022) propose MoralExceptQA, a task aimed at assessing the acceptability of violating established moral rule in different situations. With NormBank, Ziems et al. (2023) introduce a framework for grounded reasoning about situational norms, adding auxiliary information such as environmental conditions and agent characteristics. Rather than these forms of atomic groundings in certain categories, in NORMLENS we provide freetext contextualizations, and we also add supporting commonsense rationales which justify how each piece of context alters the morality of the action."
        },
        {
            "heading": "7 Conclusion",
            "text": "We introduce NORMLENS, a new dataset of visual-grounded commonsense norms. Based on NORMLENS, we design two tests to measure how well models align with humans on visually grounded reasoning tasks about commonsense norms. These tests demonstrate that even state-ofthe-art large pretrained models cannot easily make predictions that match with humans. We encourage further explorations to investigate the abilities to ground on visual contexts to reason about defeasible commonsense norms."
        },
        {
            "heading": "8 Limitations",
            "text": "NORMLENS is manually annotated by Englishspeaking workers who reside in Canada, UK, and US. Therefore, it may not cover all commonsense norms based on different sociocultural backgrounds or diverse perspectives. Furthermore, our experiments focus on aligning with averaged crowd judgments: averaging can mask valid minority perspectives. While we consider high and medium agreement datasets explicitly as a step to account for this, future work would be well-suited to explicitly model annotator disagreements. We hope to extend the coverage of commonsense norms to more diverse backgrounds and perspectives. Moreover, we plan to scale the dataset to cover a broader range of situations, which will promote models to better align with humans in ethical perspectives."
        },
        {
            "heading": "9 Acknowledgement",
            "text": "We thank our colleagues on the Beaker Team at the Allen Institute for AI for their assistance with the compute infrastructure. This work was supported by Institute of Information & communications Technology Planning & Evaluation (IITP) grant funded by the Korea government (MSIT) (No.2020-0-01361) and NCSOFT Vision/NLP Center."
        },
        {
            "heading": "B Generating Multimodal Situations about Defeasible Commonsense Norms",
            "text": "We employ ChatGPT (GPT-3.5-turbo) to generate textual situations and filtering generated situations, as described in \u00a72.1. Throughout our data pipeline, we use temperature sampling with a temperature value of 0.1, a top-p value of 0.95, and set both the frequency and presence penalty values to 0. The prompt templates that are used for situation generation and filtering are described in Table 5, Table 6, and Table 7. For diversity filtration, we set specific keywords as \u201cfuneral\u201d, \u201clibrary\u201d, \u201chospital\u201d, \u201cconstruction\u201d, \u201ccourtroom\u201d, and \u201chistorical\u201d."
        },
        {
            "heading": "C Collecting Annotations from Human",
            "text": "We utilize Amazon Mechanical Turk (MTurk) for worker recruitment in order to perform task annotations. To recruit qualified human annotators on MTurk, we establish qualification tasks. In order to guarantee fair compensation for the human annotators, we provide an hourly wage of $15 for their valuable contributions. Figure 6 and Figure 7 depict the interfaces used for collecting human annotations."
        },
        {
            "heading": "D Prompt Templates for Large Pretrained Models",
            "text": "We provide prompt templates that are used to perform reasoning with large pretrained models, in Table 9, Table 10, and Table 11."
        },
        {
            "heading": "E Full Break-down of Evaluation Results",
            "text": "We provide full break-down of alignment scores, which provides detailed results about \u00a74.2. As we already provide results for judgment task on NORMLENSHA, we further provide results for judgment task on NORMLENSMA (Table 12, Table 13, and Table 14) and explanation task on NORMLENSHA (Table 15 and Table 16)."
        },
        {
            "heading": "F Enhancing Large Pretrained Models.",
            "text": "Generating Multimodal Situations. For situation generation, we employ the prompt illustrated in Table 8. To encourage diversity, we utilize temperature sampling with a temperature value of 0.7, and we set the top-p value to 0.95 and assign 0 values for both frequency and presence penalty.\nFine-tuning Details. We fine-tune large pretrained models on generated examples to enhance them. To conduct fine-tuning on VLMs, we adhere to the fine-tuning specifications outlined in (Liu et al., 2023) for LLaVA and (Dai et al., 2023) for InstructBLIP. We train both models for one epoch. We use initial learning rate of 2e-5 with using batch size of 32 to train LLAVA, and use initial learning rate of 1e-5 using batch size of 16 to train InstructBLIP.\nWhen fine-tuning SMs, we solely focus on finetuning the language model component of the model. For fine-tuning the SM based on Vicuna-13B, we follow the fine-tuning details presented in (Chiang et al., 2023), while for fine-tuning GPT-3 Curie and Davinci, we utilize the OpenAI fine-tuning API. In particular, when fine-tuning Vicuna-13B, we use learning rate of 2e-5 with one epoch of training, using batch size of 256 (with gradient accumulation steps of 8).\nExplanation (E; \u2191)"
        }
    ],
    "title": "Reading Books is Great, But Not if You Are Driving! Visually Grounded Reasoning about Defeasible Commonsense Norms",
    "year": 2023
}