{
    "abstractText": "Data contamination has become prevalent and challenging with the rise of models pretrained on large automatically-crawled corpora. For closed models, the training data becomes a trade secret, and even for open models, it is not trivial to detect contamination. Strategies such as leaderboards with hidden answers, or using test data which is guaranteed to be unseen, are expensive and become fragile with time. Assuming that all relevant actors value clean test data and will cooperate to mitigate data contamination, what can be done? We propose three strategies that can make a difference: (1) Test data made public should be encrypted with a public key and licensed to disallow derivative distribution; (2) demand training exclusion controls from closed API holders, and protect your test data by refusing to evaluate without them; (3) avoid data which appears with its solution on the internet, and release the web-page context of internet-derived data along with the data. These strategies are practical and can be effective in preventing data contamination.",
    "authors": [
        {
            "affiliations": [],
            "name": "Alon Jacovi"
        },
        {
            "affiliations": [],
            "name": "Avi Caciularu"
        },
        {
            "affiliations": [],
            "name": "Omer Goldman"
        },
        {
            "affiliations": [],
            "name": "Yoav Goldberg"
        }
    ],
    "id": "SP:3a871032dfa06e324257ba74276064c67cd3cc36",
    "references": [
        {
            "authors": [
                "Wang",
                "William Wang",
                "Bohan Wu",
                "Jiajun Wu",
                "Yuhuai Wu",
                "Sang Michael Xie",
                "Michihiro Yasunaga",
                "Jiaxuan You",
                "Matei Zaharia",
                "Michael Zhang",
                "Tianyi Zhang",
                "Xikun Zhang",
                "Yuhui Zhang",
                "Lucia Zheng",
                "Kaitlyn Zhou",
                "Percy Liang"
            ],
            "title": "On the opportunities",
            "year": 2022
        },
        {
            "authors": [
                "S\u00e9bastien Bubeck",
                "Varun Chandrasekaran",
                "Ronen Eldan",
                "Johannes Gehrke",
                "Eric Horvitz",
                "Ece Kamar",
                "Peter Lee",
                "Yin Tat Lee",
                "Yuanzhi Li",
                "Scott Lundberg"
            ],
            "title": "Sparks of artificial general intelligence: Early experiments with gpt-4",
            "year": 2023
        },
        {
            "authors": [
                "Nicholas Carlini",
                "Jamie Hayes",
                "Milad Nasr",
                "Matthew Jagielski",
                "Vikash Sehwag",
                "Florian Tramer",
                "Borja Balle",
                "Daphne Ippolito",
                "Eric Wallace."
            ],
            "title": "Extracting training data from diffusion models",
            "venue": "arXiv preprint arXiv:2301.13188.",
            "year": 2023
        },
        {
            "authors": [
                "Nicholas Carlini",
                "Daphne Ippolito",
                "Matthew Jagielski",
                "Katherine Lee",
                "Florian Tramer",
                "Chiyuan Zhang."
            ],
            "title": "Quantifying memorization across neural language models",
            "venue": "The International Conference on Learning Representations (ICLR).",
            "year": 2023
        },
        {
            "authors": [
                "Nicholas Carlini",
                "Florian Tramer",
                "Eric Wallace",
                "Matthew Jagielski",
                "Ariel Herbert-Voss",
                "Katherine Lee",
                "Adam Roberts",
                "Tom B Brown",
                "Dawn Song",
                "Ulfar Erlingsson"
            ],
            "title": "Extracting training data from large language models",
            "year": 2021
        },
        {
            "authors": [
                "Kent K Chang",
                "Mackenzie Cramer",
                "Sandeep Soni",
                "David Bamman."
            ],
            "title": "Speak, memory: An archaeology of books known to chatgpt/gpt-4",
            "venue": "arXiv preprint arXiv:2305.00118.",
            "year": 2023
        },
        {
            "authors": [
                "Wei-Lin Chiang",
                "Zhuohan Li",
                "Zi Lin",
                "Ying Sheng",
                "Zhanghao Wu",
                "Hao Zhang",
                "Lianmin Zheng",
                "Siyuan Zhuang",
                "Yonghao Zhuang",
                "Joseph E. Gonzalez",
                "Ion Stoica",
                "Eric P. Xing"
            ],
            "title": "Vicuna: An opensource chatbot impressing gpt-4 with 90%* chatgpt",
            "year": 2023
        },
        {
            "authors": [
                "Leshem Choshen",
                "Elad Venezian",
                "Noam Slonim",
                "Yoav Katz"
            ],
            "title": "Fusing finetuned models for better pretraining",
            "year": 2022
        },
        {
            "authors": [
                "Jesse Dodge",
                "Maarten Sap",
                "Ana Marasovi\u0107",
                "William Agnew",
                "Gabriel Ilharco",
                "Dirk Groeneveld",
                "Margaret Mitchell",
                "Matt Gardner."
            ],
            "title": "Documenting large webtext corpora: A case study on the colossal clean crawled corpus",
            "venue": "Proceedings of the",
            "year": 2021
        },
        {
            "authors": [
                "Alexander Fabbri",
                "Irene Li",
                "Tianwei She",
                "Suyi Li",
                "Dragomir Radev."
            ],
            "title": "Multi-news: A large-scale multi-document summarization dataset and abstractive hierarchical model",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational",
            "year": 2019
        },
        {
            "authors": [
                "Bjarke Felbo",
                "Alan Mislove",
                "Anders S\u00f8gaard",
                "Iyad Rahwan",
                "Sune Lehmann."
            ],
            "title": "Using millions of emoji occurrences to learn any-domain representations for detecting sentiment, emotion and sarcasm",
            "venue": "Conference on Empirical Methods in Natural Lan-",
            "year": 2017
        },
        {
            "authors": [
                "Prakhar Ganesh",
                "Yao Chen",
                "Xin Lou",
                "Mohammad Ali Khan",
                "Yin Yang",
                "Hassan Sajjad",
                "Preslav Nakov",
                "Deming Chen",
                "Marianne Winslett."
            ],
            "title": "Compressing large-scale transformer-based models: A case study on BERT",
            "venue": "Transactions of the Association for",
            "year": 2021
        },
        {
            "authors": [
                "Peng Gao",
                "Jiaming Han",
                "Renrui Zhang",
                "Ziyi Lin",
                "Shijie Geng",
                "Aojun Zhou",
                "Wei Zhang",
                "Pan Lu",
                "Conghui He",
                "Xiangyu Yue",
                "Hongsheng Li",
                "Yu Qiao."
            ],
            "title": "Llama-adapter v2: Parameter-efficient visual instruction model",
            "venue": "arXiv preprint arXiv:2304.15010.",
            "year": 2023
        },
        {
            "authors": [
                "Abbas Ghaddar",
                "Phillippe Langlais."
            ],
            "title": "Coreference in Wikipedia: Main concept resolution",
            "venue": "Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning, pages 229\u2013 238, Berlin, Germany. Association for Computational",
            "year": 2016
        },
        {
            "authors": [
                "Cheng-Yu Hsieh",
                "Chun-Liang Li",
                "Chih-Kuan Yeh",
                "Hootan Nakhost",
                "Yasuhisa Fujii",
                "Alexander Ratner",
                "Ranjay Krishna",
                "Chen-Yu Lee",
                "Tomas Pfister"
            ],
            "title": "Distilling step-by-step! outperforming larger language models with less training data and smaller",
            "year": 2023
        },
        {
            "authors": [
                "Rie Johnson",
                "Tong Zhang."
            ],
            "title": "Effective use of word order for text categorization with convolutional neural networks",
            "venue": "CoRR, abs/1412.1058.",
            "year": 2014
        },
        {
            "authors": [
                "Daniel Khashabi",
                "Gabriel Stanovsky",
                "Jonathan Bragg",
                "Nicholas Lourie",
                "Jungo Kasai",
                "Yejin Choi",
                "Noah A. Smith",
                "Daniel Weld."
            ],
            "title": "GENIE: Toward reproducible and standardized human evaluation for text generation",
            "venue": "Proceedings of the 2022 Con-",
            "year": 2022
        },
        {
            "authors": [
                "John Kirchenbauer",
                "Jonas Geiping",
                "Yuxin Wen",
                "Jonathan Katz",
                "Ian Miers",
                "Tom Goldstein"
            ],
            "title": "A watermark for large language models",
            "year": 2023
        },
        {
            "authors": [
                "Lagunas",
                "Alexander Rush",
                "Thomas Wolf."
            ],
            "title": "Datasets: A community library for natural language processing",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 175\u2013184, Online",
            "year": 2021
        },
        {
            "authors": [
                "Yuhui Zhang",
                "Yuta Koreeda"
            ],
            "title": "Holistic evaluation of language models",
            "year": 2022
        },
        {
            "authors": [
                "Nelson F. Liu",
                "Tianyi Zhang",
                "Percy Liang"
            ],
            "title": "Evaluating verifiability in generative search engines",
            "year": 2023
        },
        {
            "authors": [
                "Inbal Magar",
                "Roy Schwartz."
            ],
            "title": "Data contamination: From memorization to exploitation",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 157\u2013165, Dublin, Ireland. Association",
            "year": 2022
        },
        {
            "authors": [
                "Marc Marone",
                "Benjamin Van Durme"
            ],
            "title": "Data portraits: Recording foundation model training data",
            "year": 2023
        },
        {
            "authors": [
                "Michael Moor",
                "Oishi Banerjee",
                "Zahra Shakeri Hossein Abad",
                "Harlan M Krumholz",
                "Jure Leskovec",
                "Eric J Topol",
                "Pranav Rajpurkar."
            ],
            "title": "Foundation models for generalist medical artificial intelligence",
            "venue": "Nature, 616(7956):259\u2013265.",
            "year": 2023
        },
        {
            "authors": [
                "MosaicML."
            ],
            "title": "Mosaicml inference",
            "venue": "https://www. mosaicml.com/inference.",
            "year": 2023
        },
        {
            "authors": [
                "Gene Myers."
            ],
            "title": "A fast bit-vector algorithm for approximate string matching based on dynamic programming",
            "venue": "Journal of the ACM (JACM), 46(3):395\u2013 415.",
            "year": 1999
        },
        {
            "authors": [
                "Simon Ott",
                "Konstantin Hebenstreit",
                "Valentin Li\u00e9vin",
                "Christoffer Egeberg Hother",
                "Milad Moradi",
                "Maximilian Mayrhauser",
                "Robert Praas",
                "Ole Winther",
                "Matthias Samwald"
            ],
            "title": "Thoughtsource: A central hub for large language model reasoning data",
            "year": 2023
        },
        {
            "authors": [
                "Aleksandra Piktus",
                "Christopher Akiki",
                "Paulo Villegas",
                "Hugo Lauren\u00e7on",
                "G\u00e9rard Dupont",
                "Alexandra Sasha Luccioni",
                "Yacine Jernite",
                "Anna Rogers"
            ],
            "title": "The roots search tool: Data transparency for llms",
            "year": 2023
        },
        {
            "authors": [
                "Chengwei Qin",
                "Aston Zhang",
                "Zhuosheng Zhang",
                "Jiaao Chen",
                "Michihiro Yasunaga",
                "Diyi Yang"
            ],
            "title": "Is chatgpt a general-purpose natural language processing task solver? arXiv preprint arXiv:2302.06476",
            "year": 2023
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J. Liu."
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "Journal of Machine Learning Research,",
            "year": 2020
        },
        {
            "authors": [
                "P Venkata Rajeev",
                "V Smrithi Rekha."
            ],
            "title": "Recommending products to customers using opinion mining of online product reviews and features",
            "venue": "2015 International Conference on Circuits, Power and Computing Technologies [ICCPCT-2015], pages 1\u20135. IEEE.",
            "year": 2015
        },
        {
            "authors": [
                "Yasaman Razeghi",
                "Robert L Logan IV",
                "Matt Gardner",
                "Sameer Singh."
            ],
            "title": "Impact of pretraining term frequencies on few-shot numerical reasoning",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2022, pages 840\u2013854, Abu Dhabi,",
            "year": 2022
        },
        {
            "authors": [
                "Marco T\u00falio Ribeiro",
                "Tongshuang Wu",
                "Carlos Guestrin",
                "Sameer Singh."
            ],
            "title": "Beyond accuracy: Behavioral testing of NLP models with checklist",
            "venue": "CoRR, abs/2005.04118.",
            "year": 2020
        },
        {
            "authors": [
                "Vinu Sankar Sadasivan",
                "Aounon Kumar",
                "Sriram Balasubramanian",
                "Wenxiao Wang",
                "Soheil Feizi"
            ],
            "title": "Can ai-generated text be reliably detected",
            "year": 2023
        },
        {
            "authors": [
                "Aarohi Srivastava",
                "Abhinav Rastogi",
                "Abhishek Rao",
                "Abu Awal Md Shoeb",
                "Abubakar Abid",
                "Adam Fisch",
                "Adam R Brown",
                "Adam Santoro",
                "Aditya Gupta",
                "Adri\u00e0 Garriga-Alonso"
            ],
            "title": "Beyond the imitation game: Quantifying and extrapolating",
            "year": 2022
        },
        {
            "authors": [
                "Siqi Sun",
                "Zhe Gan",
                "Yuwei Fang",
                "Yu Cheng",
                "Shuohang Wang",
                "Jingjing Liu."
            ],
            "title": "Contrastive distillation on intermediate representations for language model compression",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language",
            "year": 2020
        },
        {
            "authors": [
                "Rohan Taori",
                "Ishaan Gulrajani",
                "Tianyi Zhang",
                "Yann Dubois",
                "Xuechen Li",
                "Carlos Guestrin",
                "Percy Liang",
                "Tatsunori B. Hashimoto."
            ],
            "title": "Stanford alpaca: An instruction-following llama model",
            "venue": "https:// github.com/tatsu-lab/stanford_alpaca.",
            "year": 2023
        },
        {
            "authors": [
                "Esko Ukkonen."
            ],
            "title": "Algorithms for approximate string matching",
            "venue": "Information and control, 64(13):100\u2013118.",
            "year": 1985
        },
        {
            "authors": [
                "Jason Wei",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Maarten Bosma",
                "Ed H. Chi",
                "Quoc Le",
                "Denny Zhou."
            ],
            "title": "Chain of thought prompting elicits reasoning in large language models",
            "venue": "CoRR, abs/2201.11903.",
            "year": 2022
        },
        {
            "authors": [
                "Xiang Wei",
                "Xingyu Cui",
                "Ning Cheng",
                "Xiaobin Wang",
                "Xin Zhang",
                "Shen Huang",
                "Pengjun Xie",
                "Jinan Xu",
                "Yufeng Chen",
                "Meishan Zhang"
            ],
            "title": "Zeroshot information extraction via chatting with chatgpt",
            "venue": "arXiv preprint arXiv:2302.10205",
            "year": 2023
        },
        {
            "authors": [
                "Mitchell Wortsman",
                "Gabriel Ilharco",
                "Jong Wook Kim",
                "Mike Li",
                "Simon Kornblith",
                "Rebecca Roelofs",
                "Raphael Gontijo Lopes",
                "Hannaneh Hajishirzi",
                "Ali Farhadi",
                "Hongseok Namkoong",
                "Ludwig Schmidt."
            ],
            "title": "Robust fine-tuning of zero-shot models",
            "venue": "In",
            "year": 2022
        },
        {
            "authors": [
                "Ribeiro"
            ],
            "title": "2020), although in the context unit tests for text",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Common NLP models today are large language models trained on data crawled from the internet (Raffel et al., 2020; Bommasani et al., 2022; Touvron et al., 2023). This data is often malformed or obfuscated in ways that make it difficult to audit at scale (Bommasani et al., 2022; Mitchell et al., 2023). In particular, evaluation data that is also available on the web may be used as part of training, and it can be challenging to verify whether it was used in practice or not (OpenAI, 2023; Google, 2023a).1 Worse, for many closed models, training data is considered a trade secret and thus unknown to the research community. Such models are being evaluated on data that cannot be certified to be\n1For example, OpenAI (2023) found that the BIG-Bench benchmark (Srivastava et al., 2022) was compromised to an extent that prevented its usage entirely.\nunseen during training (Brown et al., 2020). Indeed, signs show that such models were exposed to test data (Dodge et al., 2021; Magar and Schwartz, 2022), and we refer to this as data contamination.\nThe above issue of internet-crawled training data is one of two prominent scenarios of data contamination we consider in this work. The second is in the access to closed models via APIs.2 Such models are frequently used in research for various purposes (Wei et al., 2023; Qin et al., 2023; Moor et al., 2023), and thus they are also evaluated (Srivastava et al., 2022; Bubeck et al., 2023). In most cases, the institution behind the API reserves the option to use the data sent to them as training data in further iterations.3 In this case, any valuable evaluation data sent to closed API models for any purpose is potentially compromised for any subsequent evaluations using the same data.\nThe NLP evaluation community is now witness to two urgent crises: Data contamination in training data crawled from the internet, and in training data collected from calls to a closed API. The implications are severe\u2014not only is much of our evaluation methodology potentially compromised, but we also cannot fully identify the scope and magnitude of the contamination, even for open models.\nWe engage with the two crises by outlining three practical strategies that individual researchers can enact to protect the integrity of their evaluations:\n\u2192 Strategy 1: Protect data from automatic crawlers using public key encryption and a license that forbids distribution of adaptations (\u201cno derivatives\u201d).\n2E.g., OpenAI\u2019s GPT series (Brown et al., 2020), MosaicML Inference (MosaicML, 2023), and Google\u2019s Bard and PaLM API (Google, 2023b).\n3OpenAI currently provides exclusion guarantees for certain API calls as of March 1, 2023. This guarantee does not extend to data sent before this date or to data sent through the ChatGPT and DALL-E Labs interfaces. Google\u2019s Bard provides no exclusion guarantee exists as of this writing. Sources: openai.com/policies/api-data-usage-policies; bard. google.com/faq\n\u2192 Strategy 2: Withhold from evaluating APIs that give no training exclusion options.\n\u2192 Strategy 3: Avoid data that appears with its solution on the internet. If the data originates from the internet, release its context with it."
        },
        {
            "heading": "2 Setting",
            "text": "We consider two independent scenarios of data contamination, and three assumptions underlying our mitigation strategies:\nScenario 1 (internet crawled corpora): The model to be evaluated is based on a training set derived automatically from the internet. The training set is closed or large enough that it is difficult to exhaustively and routinely search for all possible instances of data contamination from public test sets (both exact and approximate matches).\nNote that we include cases in which the model potentially trained on some form of the data\u2019s solution, even if it was not trained on the data verbatim. For example, in the task of sentiment classification of Amazon product reviews (Johnson and Zhang, 2014), the answer (the review\u2019s rating) is available on the internet, and possibly compromised in training, even if the dataset was processed and its final form was not available for the model.\nScenario 2 (closed API models): The model to be evaluated is a closed model behind an API, and there is no global or conditional guarantee of exclusion from future training. Any API call which contains test data compromises that test data for all models under the API holder and their affiliates.\nAssumption 1 (presumption of contamination): In all cases, if it is possible that the training data has been contaminated, we assume that it is. In other words, if some test data is accessible to automatic internet crawling systems, we consider it compromised. If it is possible that the API holder is using test data from API calls in training, we consider it compromised. The strategies in this work are designed to protect the evaluation under this strict condition.\nAssumption 2 (sincere actors): We are concerned with a setting where the evaluators and model developers are non-adversarial\u2014all actors appreciate clean test data and will not seek to \u201ccheat\u201d evaluations, as they share an incentive to reliably prove the value of their work.\nThe challenging scenarios in Section 2 primarily stem from a lack of resources and conflicting\nbusiness interests rather than adversarial sabotage. Therefore, we assume that all actors are sincere in their desire to keep evaluation data outside of model training, and leave research on strategies against adversarial actors to others.\nAssumption 3 (contamination inheritance): Models that were trained on data derived from other models (Jiao et al., 2020; Taori et al., 2023; Chiang et al., 2023; Hsieh et al., 2023, inter alia), models that are using the weights of other models (Sun et al., 2020; Ganesh et al., 2021; Choshen et al., 2022), and ensembles of other models (Wortsman et al., 2022a,b) will all be considered as contaminated as their \u201cancestors\u201d. This applies even when the ancestor was used to train only a part of the model (e.g., Gao et al., 2023)."
        },
        {
            "heading": "3 Why is Data Contamination Prevalent?",
            "text": ""
        },
        {
            "heading": "3.1 Closed models give no guarantees or controls about held-out data",
            "text": "Models with private training data\u2014whether the models themselves are closed or not\u2014make it impossible to know if they were trained on particular evaluation data.4\nEven when using test data that is guaranteed to be unknown to a closed API model (e.g., by using data created after the model was last updated or by using data before it is publicly released), on the first moment that this data is used to evaluate the closed model, it ceases to be unknown, and there is currently no standardized training exclusion controls by API holders. Furthermore, for much of evaluation research that evaluates models before the data is publicly released, the API holders will not know whether the data being sent to them belongs to a would-be evaluation set or not."
        },
        {
            "heading": "3.2 Even with open data, detection is difficult",
            "text": "Even for models whose training data is known, it can be challenging to understand exactly what they were trained on or to filter out test data before training (Mitchell et al., 2023).5 The scale of the corpora and the rapid pace of model development and evaluation development hinders thorough checks. Additionally, repurposing data from the\n4There is research on methods for deriving whether particular data existed in a model\u2019s training from the model alone (Carlini et al., 2021, 2023a; Chang et al., 2023). However, such methods are fragile to false negatives.\n5Some efforts exist to provide tools for auditing training data: E.g., Marone and Durme (2023); C4-Search (2023); Piktus et al. (2023).\ninternet is common when constructing evaluation data\u2014some of which revoke exact-match or ngrammatch detection. Fuzzy matching is expensive to run at scale and must be performed routinely between every evaluation set and pretraining dataset.\nAlthough exhaustive checks are rare in practice, even when performed, false negatives and false positives are still possible. As noted by OpenAI (2023), who ran a partial exact match, false negatives will manifest when a slightly modified example is in the training data. This issue persists for fuzzy matching, which works based on assumptions (Ukkonen, 1985) and only covers specific cases (Myers, 1999). Data contamination that is manifested in any way that stealthily evades a particular fuzzy-match method will not be found. This is an inherent limitation of data contamination detection, and such cases occur in practice (Razeghi et al., 2022; Carlini et al., 2023b).\nFinally, while detection is possible if done exhaustively and routinely\u2014this is a reactive measure, not a preventative measure, for evaluators who have no control over the training data but full control of the evaluation data. The data which was compromised is to be discarded and replaced. The strategies we propose in this work, and in particular Strategy 1, are preventative."
        },
        {
            "heading": "3.3 Existing mitigation strategies are imperfect",
            "text": "There are two strategies to mitigate data contamination in current practice:\nLive leaderboards with hidden answers. The answers to test data are kept hidden behind an interface that only reveals the evaluation result. There are weaknesses with this approach:\nPartial mitigation: The test data itself, sans answers, is still available on the internet. This data can be automatically crawled, the hidden answers are at risk of being compromised if the test data is repurposed (Scenario 1) or independently labeled by the model developer behind closed doors without knowledge that it is benchmark data (Scenario 2). And finally, live leaderboards are traditionally only applied to test sets, but development sets need protection from training to be effective, as well.\nRarely enforced: Live leaderboards are costly and time-consuming to maintain. In practice, this constraint proves to be significantly restrictive: They are rarely implemented, rarely used, and get\ndiscontinued with time.6\nResponsibility of evaluation falls to the benchmark host: In the case of automatic evaluation metrics, this poses no issue, but when evaluation is costly or time consuming\u2014such as with human evaluation\u2014the leaderboard host alone often must shoulder the costs of all evaluations (Khashabi et al., 2022).\nCreating (very) new data. Most trivially, it is possible to run the evaluation before any data is publicized. Additionally, models sometimes admit some guarantees about the last possible date in which their parameters were updated (Touvron et al., 2023), or otherwise they can be assumed to be frozen within some reasonable leeway (e.g., a minute or an hour). Using data which was only created after the model was last frozen is a strategy to guarantee that the data is unseen, even if the data is public (Liu et al., 2023). Creating counterfactual versions (or other augmentations) of older data achieves the same purpose.\nOf course, this strategy is extremely inefficient: The guarantee vanishes for newer models so that the evaluation data soon loses relevance. This requires evaluation research to continue to create new data for every evaluation, in an expensive cat-andmouse game."
        },
        {
            "heading": "4 Suggested Mitigation Strategies",
            "text": "4.1 Strategy 1: Encrypt test data with a public key, and use a \u201cNo Derivatives\u201d license"
        },
        {
            "heading": "Conditions: Scenario 1.",
            "text": "This strategy is simple and cheap yet is an impressively potent guard against non-adversarial crawling of plain text test data in training corpora: Simply upload the test data after encrypting its contents, alongside the key used to decrypt it. A simple method is by compressing the data in a passwordprotected archive (e.g., with zip).\nA license with a No Derivatives clause, such as \u201cCC BY-ND 4.0\u201d,7 will protect the data from being redistributed without its encryption, while still being otherwise permissive.8\n6We randomly selected five datasets with leaderboards from the Allen Institute for Artificial Intelligence\u2019s leaderboard website. We found that an average of 4.2 models cited the datasets of these leaderboards, even though they did not submit their results to each corresponding leaderboard, opting to use their development sets instead.\n7creativecommons.org/licenses/by-nd/4.0/deed 8See Section 5 for additional discussion on licenses.\nUnlike the strategy of contamination detection, this strategy is preventative when employed by evaluation practitioners since it stops the evaluation data from being compromised, to begin with.\nImportantly, the goal here is not to protect the data from adversarial actors (as in the case with live leaderboards), but to protect the data from automatic crawling. Therefore, the encryption key can be safely released with the encrypted data, and the encryption method can be simple and fast, so long as it sufficiently distorts the text. However, we warn against using standard obfuscation or compression methods that are not key-protected, since some crawling systems include pipelines of automatic decompression or deobfuscation.\nFinally, online dataset hubs (e.g., Hugging Face Datasets, Lhoest et al., 2021; ThoughtSource, Ott et al., 2023), should withhold from including test data in their online viewer directly, and specify when or whether hidden data was previously available in plain text.9 For showcasing the data, a sample of compromised representative examples can be presented instead.10\nStrategy 1 Corollary: Few-shot prompts. Fewshot prompts are demonstrations used as training data. Although they are not regarded as evaluation data, they are used primarily to evaluate model generalization under strict data constraints. Few-shot evaluation relies on the assumption that this small number of demonstrations will approximate model behavior on any small number of demonstrations.\nFew-shot prompts in the literature are commonly displayed in papers, in online repositories, and in online demos\u2014in plain text. They appear alongside other prompts, other data, and other relevant information that should be considered unintended for their original purpose. These prompts are often reused many times in subsequent work, and thus appear many times on the internet in \u201cunnatural\u201d contexts more-so related to NLP research (e.g., the prompts by Wei et al., 2022). We consider such prompts as compromised. When a model is given\n9In the case of Hugging Face Datasets, it is possible to use \u201cgated repositories\u201d to block test data from crawler access, although they are not currently used for this purpose. Gating mechanisms and web-page metadata that requests not to be crawled can both be used to deal with crawling for website hosts. Note that this approach does not prevent the data from being redistributed to more vulnerable hosts.\n10Alternatively, more sophisticated tricks can be implemented to relax this issue, such as decrypting the data on mouse hover, or requiring the decryption key on every viewing, but we leave such investigations to others.\nan evaluation prompt which is compromised, the evaluation ceases to be representative.\nTherefore, we should consider prompts with data as evaluation data: Avoid uploading them to the internet in plain text (including inside papers). Since such prompts are relatively inexpensive to annotate, we should avoid re-using them at all when we suspect that they were compromised, and annotate new prompts instead.\n4.2 Strategy 2: Refuse to send test data to closed API models until exclusion controls are implemented"
        },
        {
            "heading": "Conditions: Scenario 2.",
            "text": "Scenario 2 is a strict scenario and difficult to guard against without cooperation from the API host. Since we consider the API host as a sincere actor that values reliable evaluation, there are incentives in place for evaluation practitioners to demand this cooperation and for the API hosts to comply.\nForemost, since the very first API usage during evaluation compromises the test data, this strategy calls for not evaluating the closed API model until this situation changes in order to protect the integrity of the data. This, in turn, pressures the API host to provide appropriate training exclusion controls in order to participate in evaluation practice and research. Mechanically, the API host may comply by implementing a system to request exclusion from future training.11\nAs an intermediate strategy, in the absence of an exclusion guarantee, it is possible to prioritize collecting cheaper, \u201csingle-use\u201d data that can be used for the purpose of a less-reliable evaluation estimate: (1) By holding out a portion of training data, if it exists; (2) By creating synthetic variants of the data, or synthetic data altogether.\n4.3 Strategy 3: Avoid data that appears with its solution on the internet"
        },
        {
            "heading": "Conditions: Scenario 1.",
            "text": "In order to reduce data collection costs, data for training and evaluation is commonly repurposed from the internet.12 The data labels are then either derived automatically from context (e.g., review score for product reviews, Rajeev and Rekha,\n11We leave details on the implementation of such a system to the individual institutions. In case of conflicting business incentives, they may restrict exclusion controls to specific users approved for research or find other solutions.\n12E.g., Wikipedia, PubMed, Twitter, Reddit, and so on.\n2015; coreference resolution with Wikipedia entity links, Ghaddar and Langlais, 2016; news article summaries, Fabbri et al., 2019; emojis, Felbo et al., 2017) or manually labeled.\nWhen the data is used for evaluation, it is presented without the context in which the data originally appeared on the internet. Under Scenario 1, however, this context is potentially known to the model: The model can memorize the context in which a given data instance appeared in, and recall this context when the instance is presented to it for labeling. In such cases, we treat the evaluation data as compromised.\nThe case of automatic labeling, where the label is directly derived from the context, is a trivial case of data contamination and therefore should be avoided when constructing evaluation benchmarks. However, the case of manual annotation should also be carefully scrutinized: If the original context of the test instance contains information that can be helpful to the model in solving the instance, the model can use this context to cheat the evaluation, even if the solution was manually annotated. This is a particularly challenging case of data contamination under Scenario 1, as it is difficult to detect when the connection between the context and the instance label is nuanced.\nStrategy 3 calls for two actions: (A) Releasing context information alongside the evaluation data. This context is not intended to be used in the evaluation, but as documentation to enable any evaluation practitioner to execute point B, if they wish to scrutinize the integrity of the data; (B) Detecting and discarding instances where the context, which is not intended to serve as input to the model, is indicative of the solution in some significant way. Such instances can be used for training, but are not suitable for a benchmark. In particular, the practice of collecting automatic labels for evaluation tasks from the internet is fragile under Scenario 1, and should only be executed with great caution."
        },
        {
            "heading": "5 Discussion and Open Questions",
            "text": "Documenting existing evaluations for contamination. Research that collects existing test sets into large-scale benchmarks (e.g., Liang et al., 2022; Srivastava et al., 2022) should be attentive to the issues raised in this paper, and in particular employ Strategy 3. This is crucial for benchmarks that incorporate test sets predating the common practice of internet-crawled training corpora, and are\ntrivially compromised in Scenario 1 in a way that renders Strategies 1 and 2 irrelevant (e.g., review sentiment, Maas et al., 2011). A thorough assessment of the internet presence of current evaluation sets is needed to check what portion of them can be found explicitly in plain text, or implicitly through the origins of the data.\nCentralized strategies. In this work, we primarily focus on decentralized strategies that are effective at the level of individual researchers or individual evaluation sets. Centralized strategies would be helpful under the assumption that many or all relevant actors can work together from the outset. Although this assumption is more strict, the potential benefits merit additional research.\nPartially effective strategies. It may be additionally helpful to develop strategies that are only effective under certain conditions, or only partially effective. In Appendix A we discuss one such strategy of templated test examples. Other examples include maintaining a research database of evaluation data and contamination events. This will make it easier and more convenient for model practitioners to make sure that test data is not included in training, or disclosed to this database if the training of a particular model included a certain test example. Such a database is not a strictly effective mitigation strategy, but it may potentially help if model developers cooperate. Another example is watermarking data so that it can be detected more easily (Sadasivan et al., 2023; Kirchenbauer et al., 2023). While this is an active field for model-generated data, it is unexplored for watermarking test data for contamination detection, where the watermarking may be applied to metadata, or other non-intrusive parts of the data context."
        },
        {
            "heading": "6 Conclusions",
            "text": "This paper serves foremost as a call to action to encrypt all evaluation data uploaded to the internet or otherwise evade automatic crawling with gating mechanisms; to withhold from sending unseen evaluation data to closed API models without exclusion controls; to reassess internet-derived evaluation data; and to enrich the discussion around possible robust strategies under more strict assumptions like adversarial actors or negligent actors. We call on the research community to address the issue of data contamination to the best of our ability. In particular, we should be conscious of who and what can access our test data, and plan accordingly."
        },
        {
            "heading": "Limitations",
            "text": "On negligent actors. Negligence can unavoidably decrease or invalidate the effectiveness of the strategies in all cases. For example, a negligent actor may re-upload a plain-text version of a dataset and make it accessible to crawlers. And in particular, Strategy 2 is vulnerable. Nevertheless, researchers can take additional steps to decrease the likelihood of negligence scenarios, or set guidelines for resolving such cases after the contamination event occurred. We highlight two negligence scenarios here:\n\u201cI accidentally sent test data to a closed API model for the first time.\u201d The most important action to take in this scenario is to publicize the event, and notify the original contact behind the test data. Depending on the quantity of compromised examples, it may be possible to continue using the rest of the data, and regardless, the data remains uncompromised for other models. Contacting the API holder to notify them about the identity of the data may also be helpful. And finally, consider collecting replacements to the data, if this is possible.\n\u201cSomeone else, unaware of Strategy 2, sent test data to a closed API model.\u201d This is the most vulnerable weakness of Strategy 2. The API holder is the only actor capable of detecting the event, if they have knowledge of what evaluation data to exclude from future training. Otherwise, this scenario may be reduced by adding sufficient warnings to the data\u2019s host web-page, or the data file itself. Alternatively, the access to expensive test data can be restricted only to trusted institutions and individuals, although this kind of mechanism may be too restrictive.\nOn derivatives. What constitutes derivative work, or an adaptation, in a legal setting depends on local copyright law, and decryption may not necessarily constitute an adaptation. The purpose of the license in Strategy 1 is not to guarantee the ability to legally enforce encrypted distribution but to encourage it among sincere actors.\nTest data licenses that specifically address data contamination scenarios can help in making the strategies more reliable and enforceable. For example, a dataset license that forbids sending its test data to closed API models that don\u2019t comply with specific exclusion controls can provide legal means to enforce Strategy 2 downstream; and a license\nthat specifically forbids using the data for model training or requires encrypted distribution can still permit the distribution of derivatives while serving a similar function to a \u201cno derivatives\u201d license.\nOn sincere actors. While the \u201csincere actors\u201d assumption is generally feasible, naturally it does not always hold. We can conceive one plausible case where it does not hold: When using crowdsourcing for large manual annotation efforts, it is certainly possible for some crowdworkers to be adversarial\u2014and use closed API models, such as the freely-available ChatGPT, to automate the labeling against guidelines, if they believe they will not be caught. This will compromise the data, particularly in the case of ChatGPT\u2019s web interface, since it does not have training exclusion guarantees as of this writing (Footnote 3). This is an important and challenging scenario to be aware of when using crowdsourcing to annotate test data."
        },
        {
            "heading": "Acknowledgements",
            "text": "We are grateful to Omar Sanseviero for his helpful comments. This project received funding from the European Research Council (ERC) under the European Union\u2019s Horizon 2020 research and innovation programme, grant agreement No. 802774 (iEXTRACT)."
        },
        {
            "heading": "Conditions: Scenario 1 or 2.",
            "text": "This strategy requires significant effort, is not a complete defense, and does not apply in many cases, but is nevertheless one of the few practical strategies to guard against Scenario 2.\nFor some tasks, particularly with textual data, it is possible to counterfactually augment test data by converting it into a programmatical template.13 For example, in tasks that require arithmetic reasoning, the answer can change as conditioned on a number, and this can be derived automatically. In summarization tasks, any information in the input (e.g., entity names, dates, or more complex semantic information) should be reflected in the summary.\nIn test time, for a given evaluation attempt, only one counterfactual variant is sampled with a seed value, and the variant is used across all evaluated systems in that attempt. The seed is forfeit once the evaluation is completed, and the seed (or test data itself) can be publicized.\nThe more elaborate the counterfactual variant is, the stronger its function as an unseen test case, even if the source instance is compromised. Although this strategy is expensive to implement, it is worth noting that test sets are traditionally small.\n13As discussed by Ribeiro et al. (2020), although in the context unit tests for text models, which are deliberately simple."
        }
    ],
    "title": "Stop Uploading Test Data in Plain Text: Practical Strategies for Mitigating Data Contamination by Evaluation Benchmarks",
    "year": 2023
}