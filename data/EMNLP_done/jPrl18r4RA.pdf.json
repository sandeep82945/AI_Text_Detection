{
    "abstractText": "Large language models encode impressively broad world knowledge in their parameters. However, the knowledge in static language models falls out of date, limiting the model\u2019s effective \u201cshelf life.\u201d While online fine-tuning can reduce this degradation, we find that naively fine-tuning on a stream of documents leads to a low level of information uptake. We hypothesize that online fine-tuning does not sufficiently attend to important information. That is, the gradient signal from important tokens representing factual information is drowned out by the gradient from inherently noisy tokens, suggesting that a dynamic, context-aware learning rate may be beneficial. We therefore propose learning which tokens to upweight. We meta-train a small, autoregressive model to reweight the language modeling loss for each token during online fine-tuning, with the objective of maximizing the out-ofdate base question-answering model\u2019s ability to answer questions about a document after a single weighted gradient step. We call this approach Context-aware Meta-learned Loss Scaling (CaMeLS). Across three different distributions of documents, our experiments find that CaMeLS provides substantially improved information uptake on streams of thousands of documents compared with standard fine-tuning and baseline heuristics for reweighting token losses.",
    "authors": [
        {
            "affiliations": [],
            "name": "Nathan Hu"
        },
        {
            "affiliations": [],
            "name": "Eric Mitchell"
        },
        {
            "affiliations": [],
            "name": "Christopher D. Manning"
        },
        {
            "affiliations": [],
            "name": "Chelsea Finn"
        }
    ],
    "id": "SP:792fd01ba080df2a20535ff8727a72f85654f098",
    "references": [
        {
            "authors": [
                "Sid Black",
                "Leo Gao",
                "Phil Wang",
                "Connor Leahy",
                "Stella Biderman"
            ],
            "title": "GPT-Neo: Large Scale Autoregressive Language Modeling with MeshTensorflow",
            "year": 2021
        },
        {
            "authors": [
                "Arslan Chaudhry",
                "Marc\u2019Aurelio Ranzato",
                "Marcus Rohrbach",
                "Mohamed Elhoseiny"
            ],
            "title": "Efficient lifelong learning with a-GEM",
            "venue": "In International Conference on Learning Representations",
            "year": 2019
        },
        {
            "authors": [
                "Aakanksha Chowdhery",
                "Sharan Narang",
                "Jacob Devlin",
                "Maarten Bosma",
                "Gaurav Mishra",
                "Adam Roberts",
                "Paul Barham",
                "Hyung Won Chung",
                "Charles Sutton",
                "Sebastian Gehrmann"
            ],
            "title": "Palm: Scaling language modeling with pathways",
            "year": 2022
        },
        {
            "authors": [
                "Kevin Clark",
                "Kelvin Guu",
                "Ming-Wei Chang",
                "Panupong Pasupat",
                "Geoffrey Hinton",
                "Mohammad Norouzi."
            ],
            "title": "Meta-learning fast weight language models",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages",
            "year": 2022
        },
        {
            "authors": [
                "Bhuwan Dhingra",
                "Jeremy R. Cole",
                "Julian Martin Eisenschlos",
                "Daniel Gillick",
                "Jacob Eisenstein",
                "William W. Cohen."
            ],
            "title": "Time-Aware Language Models as Temporal Knowledge Bases",
            "venue": "Transactions of the Association for Computational Linguis-",
            "year": 2022
        },
        {
            "authors": [
                "Shibhansh Dohare",
                "Richard S. Sutton",
                "A. Rupam Mahmood"
            ],
            "title": "Continual backprop: Stochastic gradient descent with persistent randomness",
            "year": 2022
        },
        {
            "authors": [
                "Vanhoucke",
                "Karol Hausman",
                "Marc Toussaint",
                "Klaus Greff",
                "Andy Zeng",
                "Igor Mordatch",
                "Pete Florence."
            ],
            "title": "Palm-e: An embodied multimodal language model",
            "venue": "arXiv preprint arXiv:2303.03378.",
            "year": 2023
        },
        {
            "authors": [
                "Aaron Gokaslan",
                "Vanya Cohen",
                "Ellie Pavlick",
                "Stefanie Tellex."
            ],
            "title": "Openwebtext corpus",
            "venue": "http: //Skylion007.github.io/OpenWebTextCorpus.",
            "year": 2019
        },
        {
            "authors": [
                "Kelvin Guu",
                "Kenton Lee",
                "Zora Tung",
                "Panupong Pasupat",
                "Ming-Wei Chang."
            ],
            "title": "REALM: retrievalaugmented language model pre-training",
            "venue": "CoRR, abs/2002.08909.",
            "year": 2020
        },
        {
            "authors": [
                "William L. Hamilton",
                "Jure Leskovec",
                "Dan Jurafsky."
            ],
            "title": "Diachronic word embeddings reveal statistical laws of semantic change",
            "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages",
            "year": 2016
        },
        {
            "authors": [
                "Joel Jang",
                "Seonghyeon Ye",
                "Sohee Yang",
                "Joongbo Shin",
                "Janghoon Han",
                "Gyeonghun KIM",
                "Stanley Jungkyu Choi",
                "Minjoon Seo."
            ],
            "title": "Towards continual knowledge learning of language models",
            "venue": "International Conference on Learning Representations.",
            "year": 2022
        },
        {
            "authors": [
                "F. Jelinek",
                "B. Merialdo",
                "S. Roukos",
                "M. Strauss."
            ],
            "title": "A dynamic language model for speech recognition",
            "venue": "Speech and Natural Language: Proceedings of a Workshop Held at Pacific Grove, California, February 19-22, 1991.",
            "year": 1991
        },
        {
            "authors": [
                "Roland Kuhn."
            ],
            "title": "Speech recognition and the frequency of recently used words: A modified Markov model for natural language",
            "venue": "Coling Budapest 1988 Volume 1: International Conference on Computational Linguistics.",
            "year": 1988
        },
        {
            "authors": [
                "Vivek Kulkarni",
                "Rami Al-Rfou",
                "Bryan Perozzi",
                "Steven Skiena."
            ],
            "title": "Statistically significant detection of linguistic change",
            "venue": "Proceedings of the 24th International Conference on World Wide Web, WWW \u201915, page 625\u2013635, Republic and Canton of Geneva,",
            "year": 2015
        },
        {
            "authors": [
                "Wei Li",
                "Wenhao Wu",
                "Moye Chen",
                "Jiachen Liu",
                "Xinyan Xiao",
                "Hua Wu."
            ],
            "title": "Faithfulness in natural language generation: A systematic survey of analysis, evaluation and optimization methods",
            "venue": "ArXiv, abs/2203.05227.",
            "year": 2022
        },
        {
            "authors": [
                "Shayne Longpre",
                "Kartik Perisetla",
                "Anthony Chen",
                "Nikhil Ramesh",
                "Chris DuBois",
                "Sameer Singh."
            ],
            "title": "Entity-based knowledge conflicts in question answering",
            "venue": "arXiv preprint arXiv:2109.05052.",
            "year": 2021
        },
        {
            "authors": [
                "David Lopez-Paz",
                "Marc\u2019Aurelio Ranzato"
            ],
            "title": "Gradient episodic memory for continual learning",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Divyam Madaan",
                "Jaehong Yoon",
                "Yuanchun Li",
                "Yunxin Liu",
                "Sung Ju Hwang."
            ],
            "title": "Representational continuity for unsupervised continual learning",
            "venue": "International Conference on Learning Representations.",
            "year": 2022
        },
        {
            "authors": [
                "Michael McCloskey",
                "Neal J. Cohen."
            ],
            "title": "Catastrophic interference in connectionist networks: The sequential learning problem",
            "venue": "Gordon H. Bower, editor, Psychology of Learning and Motivation, volume 24 of Psychology of Learning and Motivation,",
            "year": 1989
        },
        {
            "authors": [
                "Kevin Meng",
                "David Bau",
                "Alex Andonian",
                "Yonatan Belinkov."
            ],
            "title": "Locating and editing factual associations in GPT",
            "venue": "ArXiv:2202.05262.",
            "year": 2022
        },
        {
            "authors": [
                "Eric Mitchell",
                "Charles Lin",
                "Antoine Bosselut",
                "Chelsea Finn",
                "Christopher D. Manning."
            ],
            "title": "Fast model editing at scale",
            "venue": "CoRR.",
            "year": 2021
        },
        {
            "authors": [
                "Eric Mitchell",
                "Charles Lin",
                "Antoine Bosselut",
                "Christopher D Manning",
                "Chelsea Finn."
            ],
            "title": "Memorybased model editing at scale",
            "venue": "Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning",
            "year": 2022
        },
        {
            "authors": [
                "A. Gupta",
                "X. Chen",
                "A. Saparov",
                "M. Greaves",
                "J. Welling."
            ],
            "title": "Never-ending learning",
            "venue": "Commun. ACM, 61(5):103\u2013115.",
            "year": 2018
        },
        {
            "authors": [
                "Miles Osborne",
                "Ashwin Lall",
                "Benjamin Van Durme."
            ],
            "title": "Exponential reservoir sampling for streaming language models",
            "venue": "Proceedings of the 52nd Annual",
            "year": 2014
        },
        {
            "authors": [
                "Alec Radford",
                "Jeffrey Wu",
                "Rewon Child",
                "David Luan",
                "Dario Amodei",
                "Ilya Sutskever."
            ],
            "title": "Language models are unsupervised multitask learners",
            "venue": "Ms., OpenAI.",
            "year": 2018
        },
        {
            "authors": [
                "Pranav Rajpurkar",
                "Jian Zhang",
                "Konstantin Lopyrev",
                "Percy Liang"
            ],
            "title": "Squad: 100,000+ questions for machine comprehension of text",
            "year": 2016
        },
        {
            "authors": [
                "Dushyant Rao",
                "Francesco Visin",
                "Andrei Rusu",
                "Razvan Pascanu",
                "Yee Whye Teh",
                "Raia Hadsell."
            ],
            "title": "Continual unsupervised representation learning",
            "venue": "Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc.",
            "year": 2019
        },
        {
            "authors": [
                "Marek Rei."
            ],
            "title": "Online representation learning in recurrent neural language models",
            "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 238\u2013243, Lisbon, Portugal. Association for Computational Linguistics.",
            "year": 2015
        },
        {
            "authors": [
                "Mengye Ren",
                "Michael Louis Iuzzolino",
                "Michael Curtis Mozer",
                "Richard Zemel."
            ],
            "title": "Wandering within a world: Online contextualized few-shot learning",
            "venue": "International Conference on Learning Representations.",
            "year": 2021
        },
        {
            "authors": [
                "G. Salton",
                "M.J. McGill."
            ],
            "title": "Introduction to Modern Information Retrieval",
            "venue": "McGraw-Hill, Inc., New York, NY, USA.",
            "year": 1986
        },
        {
            "authors": [
                "Sandhaus",
                "Evan"
            ],
            "title": "The new york times annotated corpus",
            "year": 2008
        },
        {
            "authors": [
                "Victor Sanh",
                "Lysandre Debut",
                "Julien Chaumond",
                "Thomas Wolf."
            ],
            "title": "Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter",
            "venue": "NeurIPS EMC2 Workshop.",
            "year": 2019
        },
        {
            "authors": [
                "Hanul Shin",
                "Jung Kwon Lee",
                "Jaehong Kim",
                "Jiwon Kim."
            ],
            "title": "Continual learning with deep generative replay",
            "venue": "Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc.",
            "year": 2017
        },
        {
            "authors": [
                "Chenglei Si",
                "Zhe Gan",
                "Zhengyuan Yang",
                "Shuohang Wang",
                "Jianfeng Wang",
                "Jordan Lee Boyd-Graber",
                "Lijuan Wang."
            ],
            "title": "Prompting GPT-3 to be reliable",
            "venue": "The Eleventh International Conference on Learning Representations.",
            "year": 2023
        },
        {
            "authors": [
                "Anton Sinitsin",
                "Vsevolod Plokhotnyuk",
                "Dmitry Pyrkin",
                "Sergei Popov",
                "Artem Babenko."
            ],
            "title": "Editable neural networks",
            "venue": "ICLR.",
            "year": 2020
        },
        {
            "authors": [
                "Sebastian Thrun",
                "Tom M. Mitchell."
            ],
            "title": "Lifelong robot learning",
            "venue": "Robotics and Autonomous Systems, 15(1):25\u201346. The Biology and Technology of Intelligent Autonomous Agents.",
            "year": 1995
        },
        {
            "authors": [
                "Ben Wang",
                "Aran Komatsuzaki."
            ],
            "title": "GPT-J6B: A 6 Billion Parameter Autoregressive Language Model",
            "venue": "https://github.com/kingoflolz/ mesh-transformer-jax.",
            "year": 2021
        },
        {
            "authors": [
                "Jiexin Wang",
                "Adam Jatowt",
                "Masatoshi Yoshikawa"
            ],
            "title": "Archivalqa: A large-scale benchmark dataset for open domain question answering over historical news collections",
            "year": 2022
        },
        {
            "authors": [
                "Dani Yogatama",
                "Chong Wang",
                "Bryan R. Routledge",
                "Noah A. Smith",
                "Eric P. Xing."
            ],
            "title": "Dynamic language models for streaming text",
            "venue": "Transactions of the Association for Computational Linguistics, 2:181\u2013 192.",
            "year": 2014
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Large language models learn impressively broad world knowledge through large-scale unsupervised pre-training, which they can leverage for a wide variety of downstream tasks (Brown et al., 2020; Chowdhery et al., 2022; Bubeck et al., 2023). However, large language models are typically static artifacts, and as the world changes, the knowledge encoded in their parameters becomes stale. While retrieval-augmented models are one approach to\n* Equal contribution.\nmitigating the staleness issue, even very large language models often fail to correctly update their memorized predictions when presented with counterfactual retrieved information (Longpre et al., 2021; Li et al., 2022; Si et al., 2023). Moreover, purely parametric language models are uniquely suited for edge computing due to their compact size (relative to a large retrieval index) and simplicity of inference (Gerganov, 2023). Recent work has thus considered variants of online fine-tuning on a stream of documents to efficiently perform direct updates to the knowledge inside of a large language model (Lazaridou et al., 2021; Jang et al., 2022).\nIdeally, we could simply fine-tune a language model on an online stream of documents, and the information contained in those documents would be readily available for the model to use in a variety of downstream tasks such as answering questions about the information in the documents. Unfortunately, we find that in this online adaptation setting, fine-tuning with a well-tuned learning rate leads to a nearly negligible improvement in a question-\nanswering model\u2019s ability to answer questions relating to the stream of documents. We hypothesize that naive fine-tuning is not effective in the online adaptation setting because the negative log likelihood (NLL) loss does not accurately reflect the importance of a token. That is, tokens containing important factual information may receive relatively small NLL loss and therefore a small fine-tuning gradient. For example, consider the NLL of the word Rishi and the word Reports in the phrase The UK Prime Minister is Rishi Sunak. Reports suggest . . . for a slightly out-of-date language model. Because Rishi Sunak was a well-known politician before becoming Prime Minister, a model may place reasonably high probability mass on his name (even if other completions are higher probability). On the other hand, \u2018Reports\u2019 will invariably receive low probability, because the distribution over the first word in a sentence is unavoidably high entropy.\nThis hypothesis suggests that we can improve upon online adaptation by only fine tuning on a subset of tokens which are most likely to lead to useful updates. One natural approach to identify such factual tokens is through salient spans (Guu et al., 2020). Another common technique used to weight words it via TF-IDF scores (Salton and McGill, 1986). We find that fine-tuning while using these heuristics does improve information uptake. However, it is unclear if such heuristic choices are optimal. As an alternative, we explore a method for learning a per-token importance weights corresponding to the utility of fine-tuning on that token. However, such utility is difficult to define, and even with a suitable definition, dense per-token annotations of utility are extremely time-consuming to collect. We thus select a definition of utility that enables using distant supervision of the utility of each token: a high utility token is one whose fine-tuning gradient improves a question-answering model\u2019s\nability to answer questions about the contents of the surrounding document.\nUsing this notion of a token\u2019s utility for online learning, we propose Context-aware Meta-learned Loss Scaling (CaMeLS), an approach to online adaptation that meta-trains an importance weighting model to identify such tokens in a document. Given a dataset of documents and queries, we use a meta-learning loss to train our weighting model: first, in an \u2018inner loop,\u2019 we update a base model (a proxy for the model we will update at test time) using the gradient of NLL of the document, weighted by the outputs of the importance weighting model. Next, in the \u2018outer loop\u2019, the loss is computed by evaluating the updated base model\u2019s performance on the corresponding query. This outer loss is used to updated the parameters of the importance weighting model. During online fine-tuning on a stream of documents, we simply re-weight the online loss using the importance-weighting model\u2019s output.\nAlthough the process used to train CaMeLS uses a proxy model (i.e., a stand-in for the model we will update at test time), one might hope that the importance of tokens would be independent of the model used for inner loop updates; to a significant degree, we intuit that the importance of a token should be an innate trait of underlying text. Indeed, we find that the meta-learned importance weights generalize across models; for each dataset, we metatrain our importance weighting model once using DistilGPT-2 (Sanh et al., 2019) as the base model and successfully use these weighting model without modification to update GPT-J 6B (Wang and Komatsuzaki, 2021). Across three online adaptation benchmarks based on streams of news and Wikipedia articles, CaMeLS substantially improves knowledge acquisition over naive fine-tuning as well as salient span and TF-IDF based baselines."
        },
        {
            "heading": "2 Related Work",
            "text": "Adapting to new data or task distributions is typically studied in the context of continual or lifelong learning (Thrun and Mitchell, 1995; Mitchell et al., 2018). Continual learning in deep networks involves the challenge of simultaneously avoiding catastrophic forgetting (McCloskey and Cohen, 1989), the process under which a neural network\u2019s performance on old tasks or data is dramatically degraded by the process of learning new information, while maintaining plasticity (Dohare et al., 2022), or the ability to adapt to the latest change in the data distribution, even after many changes have already been experienced. While most work in continual learning considers sequences of supervised data (Kirkpatrick et al., 2016; Lopez-Paz and Ranzato, 2017; Shin et al., 2017; Chaudhry et al., 2019), some work also studies continual few-shot (Ren et al., 2021) or unsupervised learning (Rao et al., 2019; Madaan et al., 2022), which is closer to the setting in this paper. However, these works typically focus on streams of visual data.\nDynamic, or streaming, language models were first considered in the context of n-gram language models, combining a cache of recently-used words to update the predictive probabilities of a tri-gram model (Kuhn, 1988; Jelinek et al., 1991; Osborne et al., 2014). Later work describes online EMbased algorithms for efficiently updating n-gram models (Yogatama et al., 2014). Other studies investigate the evolution of decontextualized word embeddings over as a result of temporal shifts in the use of language (Kulkarni et al., 2015; Hamilton et al., 2016) or the use of vector memories to store recent information when training recurrent neural networks online (Rei, 2015). More recently, several studies have explored methods for updating large neural language models, typically through online fine-tuning on a stream of documents (Lazaridou et al., 2021) with architectural constraints (Jang et al., 2022) or explicit conditioning on time (Dhingra et al., 2022) used as strategies to reduce forgetting of old information. Clark et al. (2022) use meta-learning to reduce the compute requirements of online fine-tuning. However, recent work suggests that while increasing the size of language models may largely mitigate the problem of forgetting old information (Driess et al., 2023), improving the efficiency of acquisition of new knowledge is still a challenge, and this problem is therefore the focus of the present\nwork. Other methods for dynamically updating the knowledge in parametric language models develop specialized techniques, called model editors, designed to make targeted edits to individual facts (Sinitsin et al., 2020; Mitchell et al., 2021; Meng et al., 2022) or behaviors (Mitchell et al., 2022). However, model editors assume access to annotations of the tokens or facts that must be updated; in this work, we study the problem of learning which tokens in an unlabeled sequence of documents are important."
        },
        {
            "heading": "3 Meta-Learning Improved Online Adaptation of Large Language Models",
            "text": "Given an out-of-date language model and a stream of recent documents, we aim to update the model such that it effectively answers typical queries about the documents in the stream. By focusing only on retaining knowledge relevant to the \u2018typical\u2019 queries, we avoid the need to completely memorize the documents, making the problem tractable. We study question-answering (QA) models specifically, as the question-answer format makes assessing a model\u2019s knowledge straightforward. In this section, we formalize this problem setting and then describe an approach to this setting, Context-aware Meta-learned Loss Scaling."
        },
        {
            "heading": "3.1 Unsupervised Online Adaptation",
            "text": "We consider a setting in which an out-of-date model f\u03b8base is updated with an online stream\n2 of recent documents Dtest = {xi}, ultimately producing an updated model f\u03b8\u2032 . The updated model f\u03b8\u2032 is then evaluated with a set of queries Qtest = {qi} with labels Ytest = {yi}, where the the ith query is drawn from a distribution of queries relating to ith document: qi, yi \u223c p(qi, yi|xi). For example, qi may be a question about some information in document xi, and yi the answer to that question implied by the document. Crucially, when using Dtest to update f\u03b8base , we do not have access to Qtest. Thus, our methodology for updating f\u03b8base must be broad rather than query specific. In order to make this problem tractable (i.e., not requiring complete memorization of the document stream), we assume that we have an additional corpus of documents Dtrain and corresponding query samples Qtrain and labels Ytrain generated by a similar generative process to Qtest, Ytest. This training set enables\n2Dtest is typically an online stream of documents, but could be an arbitrary ordering over a static collection of documents.\nlearning the types of queries that may be of interest, informing how we should update our model to maximize the performance on test queries while minimizing disturbance to its prior knowledge or behaviors. We next describe an algorithm for leveraging this dataset to more efficiently update our base model on the test stream of documents Dtest."
        },
        {
            "heading": "3.2 CaMeLS: Context-aware Meta-learned Loss Scaling",
            "text": "The goal of CaMeLS is to distill the information in the training documents, queries, and labels into a parameter vector \u03d5. This vector summarizes the optimal way to update a base model on a document stream to maximize retention of information likely to be relevant to test queries. CaMeLS accomplishes this goal by training a weighting model w\u03d5 (a small autoregressive language model 3) that reweights the online NLL loss used in typical online fine-tuning, focusing on the tokens whose NLL gradient is most useful for updating a small proxy base model\u2019s knowledge. In other words, the weighting model is trained to re-weight the NLL loss such that the proxy model is able to correctly answer questions about a document after one gradient step on the modified NLL of the document. The weighting model is trained with an episodic bi-level optimization, which we explain next in detail (also see Figure 3).\nDuring each episode, a training document-querylabel triple (x, q, y) is sampled from Dtrain and a\n3Section 3.4 contains details on the weighting model\u2019s architecture.\nlocality example xloc from Dloc. Dloc is a dataset of unlabeled text representing the distribution over which we want the base model\u2019s behavior to remain generally unchanged. For all experiments, we use the OpenWebText dataset (Gokaslan et al., 2019) as Dloc. Let \u03b8base denote the parameters of the proxy base model at the start of the episode. The update to the weighting model involves three steps: 1) computing the weights for the training document, 2) updating the small proxy base model on the weighted NLL on the training document, and 3) backpropagating the \u2018outer loop\u2019 loss4 of the updated proxy model on a query and label from the training document. These steps are shown in Figure 3. Let L(f\u03b8, x,a) denote the weighted NLL of f\u03b8 on document x using weights a. Steps 1 & 2 are described by the inner loop update rule:\n\u03b8\u2032 = \u03b8base \u2212 \u03b1\u2207\u03b8baseL(f\u03b8base , x, w\u03d5(x)) (1)\nThe inner loop learning rate \u03b1 can be fixed, sampled, or learned. For all of our experiments, we use a fixed inner learning rate of \u03b1 = 5e\u2212 4. After the updated proxy model is computed, we compute an outer loop loss measuring the effectiveness of the weighted adaptation procedure on the document x:\nLouter = \u2212 log p\u03b8\u2032(y|q) + clocLloc(\u03b8base, \u03b8\u2032, xloc) (2)\nIn addition to the negative log likelihood of label given the query and updated base model parameters, the outer loss has a locality term Lloc which prevents the updated base model parameters from excessively changing the base model\u2019s behavior. cloc is set to .1 for all experiments. Lloc is the sum of the KL divergences Liloc between the base model before and after adaptation conditioned on each prefix xiloc of the locality input xloc, with\nLiloc(\u03b8base, \u03b8 \u2032, xloc) = KL ( p\u03b8base(\u00b7|xiloc)\u2225p\u03b8\u2032(\u00b7|xiloc) ) (3)\nFinally, we perform a single update to the weighting model\u2019s parameters by computing the gradient of the outer loop loss with respect to \u03d5. We optimize \u03d5 with the Adam optimizer, using a learning rate of 1e-5. We accumulate outer loop gradients over 24 examples (document-query-label triples) split into 4 batches of 6 triples.\n4Performing a single update to the proxy model is the inner loop and updating the weighting model according to the updated proxy model\u2019s loss on the query is considered the outer loop of a bi-level optimization used to train CaMeLS."
        },
        {
            "heading": "3.3 Mitigating Train-Test Shift",
            "text": "The single-step training procedure described above optimizes for effective knowledge retention for a single document. However, in our online adaptation setting, we may update for hundreds or thousands of documents before we evaluate on our downstream queries. In order to mitigate this traintest shift, we modify CaMeLS with two strategies. First, we do not use the same base model parameters during each episode of training. This is done to prevent the weighting model from overfitting to a single base model state. For most training episodes, the starting base model parameters adapted in the inner loop are the final base model parameters in the previous episode. Every creset = 4 episodes of training, the starting base model parameters are reset to those of the original base model. Second, instead of performing an inner update on a single document, we sample an inner batch of k = 6 document-query-label triples (x1, q1, y1), . . . , (xk, qk, yk) for each episode. A sequence of k inner loop updates is performed:\n\u03b8i = \u03b8i\u22121 \u2212 \u03b1\u2207\u03b8L(f\u03b8i\u22121 , xi, w\u03d5(xi)) (4)\nwhere \u03b80 = \u03b8base and \u03b8\u2032 = \u03b8k. The outer loss is computed as before, but now averaging the querylabel loss over the inner batch. By allowing inner loop updates to accumulate during adaptation, \u03d5 learns an updating strategy that preserves the knowledge of prior updates and maintains the base model\u2019s ability to learn from subsequent updates."
        },
        {
            "heading": "3.4 Compute & Architecture of CaMeLS",
            "text": "Optimizing bi-level objectives like the one used by CaMeLS is rather memory and compute-intensive, requiring memory and compute proportional to the depth of the inner loop (the batch size used for multiple inner loop updates) and proportional to the size of our base/proxy model - each inner loop step creates an updated copy of the base model parameters in the computation graph. However, CaMeLS only requires a lightweight base model; our experiments use DistilGPT-2 as the base model during meta-training, but we find strong transfer to much larger base models during evaluation. The weighting model itself is also small; all experiments use DistilGPT-2 as the weighting model (a MLP with a single hidden state of size 128 is used as the head to produce token weights). Using the base and weighting models described, we are able to train weighting models using 6 inner loop steps on a single NVIDIA A40 GPU.\nMethod Time Per Doc Total GPU Memory\nWe next discuss the compute costs of using a trained CaMeLS weighting model for online adaptation. The additional compute needed for CaMeLS is very small compared to uniform fine-tuning \u2014 is a single forward pass of a weight model for each document we update on. For large models, the weight model overhead is small compared to the time needed to run a forward and backward pass of the base model. Compared to standard uniform fine-tuning, CaMeLS requires slightly more GPU memory to store the weight model and is slightly slower per document. Table 1 shows compute measurements during online adaptation of GPT-2 XL on StreamingQA."
        },
        {
            "heading": "4 Experiments",
            "text": "After outlining datasets and experimental details, we present several experiments aimed at understanding CaMeLS\u2019s behavior in unsupervised online adaptation. Section 4.3 studies the extent to which CaMeLS\u2019s importance weights improve knowledge retention in online adaptation. Section 4.4 qualitatively and quantitatively explores the weights themselves, suggesting several ablations of CaMeLS that we explore in Section 4.5. Section 4.6 evaluates the cross-dataset generalization of CaMeLS weights, and finally we examine the forgetting and plasticity dynamics of CaMeLS within the document stream in Section 4.7."
        },
        {
            "heading": "4.1 Datasets",
            "text": "We apply CaMeLS to three question answering datasets with corresponding source articles. We\npartition the datasets into 5 splits. Three of these splits (train, valid, test) are used for training, hyperparameter tuning, and evaluating the CaMeLS weighting model. In order to fine-tune the initial QA base models from generic language models, we reserve two more disjoint splits (in order to prevent reusing questions during initial QA tuning and online adaptation), labeled QA train and QA valid. Additional details on dataset splits and samples are in Appendix A. At evaluation time, a stream of documents is sampled from the test split. The documents length and text stream lengths are shown in Table 2. In the StreamingQA setting, models must adapt to an entire article as opposed to a selected paragraph, making it our most challenging setting. StreamingQA (Li\u0161ka et al., 2022): The StreamingQA dataset contains a combination of humanwritten and language model generated questions. Questions are generated from English WMT news articles published between 2007 and 2020. SQuAD (Rajpurkar et al., 2016): The Stanford Question Answering Dataset (SQuAD) contains human generated questions from Wikipedia articles. The answer to each question is a span contained in a paragraph from Wikipedia. ArchivalQA (Wang et al., 2022): The ArchivalQA dataset contains automatically generated questions. Questions are generated from articles in the New York Times Annotated Corpus (Sandhaus, Evan, 2008). The answer to each question is a span contained in an article."
        },
        {
            "heading": "4.2 Experimental protocol details",
            "text": "We conducted evaluations on two families of autoregressive language models, the GPT-2 (Radford et al., 2018) and GPT-Neo families (Black et al., 2021), as well as GPT-J (Wang and Komatsuzaki, 2021). We note that all models evaluated use the same text tokenization. For all datasets, we first fine-tune each pretrained model on questionanswer pairs from that dataset. These tuned models represent the static language models we wish to update and will be referred to as base models. For each dataset, a single weighting model is trained. The proxy language model used during weighting model training is DistilGPT-2 fine-tuned on the QA train split of the respective dataset.\nAt evaluation time, the base model is updated on a stream of documents sampled from the test split 5. The final adapted base model is evaluated on\n5We use an Adam Optimizer most experimental runs. Due\nthe questions corresponding to the documents in the sampled stream. We compare CaMeLS with 4 baselines. First is standard fine tuning or Uniform where tokens are equally weighted. In Uniform + QA-tune we additionally fine tune for question answering after adaptation. Next we consider common weighting heuristics. Salient Spans corresponds to assigning a uniform weight to tokens in salient spans and no weight to all other tokens. In TF-IDF + 5% Cutoff, we first compute TF-IDF scores using the both the adaptation documents and additional in distribution documents. To account for stopwords, we remove the 5% of words with lowest TF-IDF scores. The remaining TF-IDF scores are used to reweight the tokens. 6 For each combination of base model and online adaptation strategy, the learning rate used at test time was chosen via hyper parameter sweep on a stream of documents sampled from the validation set.7"
        },
        {
            "heading": "4.3 CaMeLS improves knowledge retention",
            "text": "We first compare the knowledge retained by CaMeLS and baselines for three different data distributions in Figure 4. CaMeLS outperforms other\nto compute constraints, we use an Adafactor optimizer for adaptation of GPT-Neo 2.7B and GPT-J 6B.\n6TF-IDF scores are computed using a word level tokenization. These scores are then mapped to the BPE tokenization of the adapted language models.\n7All learning sweeps are conducted over the following values: [1e-4, 2.5e-5, 6.25e-6, 1.625e-6]. The optimal learning rates are in Appendix D.\nonline adaptation approaches across a range of datasets and weighting models. Despite the difference in scale between the proxy model used during weight training and the evaluated base models, CaMeLS\u2019s learned importance weights generalize well to the largest base model we evaluate, GPT-J 6B, which is over 70 times the size of the proxy model (DistilGPT-2, 82 million parameters) used during training. We find that standard online fine tuning (uniform weighting) with Adam performs very poorly on online adaptation. Even with a tuned learning rate and further training for question answering post adaptation, uniform weighting fails to achieve a significant improvement for several models tested."
        },
        {
            "heading": "4.4 Analysis of learned weights",
            "text": "One benefit of CaMeLS over other methods for meta-learning model updating strategies is that learned updating strategy, token weights, is interpretable. Figure 1 shows the per-token weights on sample text and how they combine with the unweighted gradient norms to produce sparsified pertoken gradient norms. In this section, we provide additional analysis of CaMeLS\u2019s learned weights. We examine the distribution of weighting model outputs on articles in the validation set of StreamingQA in Figure 5. As our qualitative evaluations show, we confirm that the distribution of weights over the entire validation split of StreamingQA is indeed sparse and bimodal. We thus interpret the weighting model as acting as a context-aware binary classifier, determining if a token is informative or uninformative. When binning weights by part of speech, we find that numbers and proper nouns are most frequently assigned a high weight. This result aligns with Lazaridou et al. (2021), who found that\nan outdated language model\u2019s performance most rapidly declines on proper nouns and numbers."
        },
        {
            "heading": "4.5 Ablations",
            "text": "In order to verify that context-aware weights are truly necessary to achieving improved knowledge retention, we now examine several ablations of CaMeLS. In the POS: Resample ablation, the weight of each token is generated by sampling from the distribution of importance weights on all tokens of the same part of speech. In the POS: Mean ablation, each token is weighted by the mean importance weight assigned to tokens of that part of speech. We additionally consider a Bimodal ablation where outputs of the weighting model are rounded to either the largest or smallest value in the distribution of importance weights.\nFigure 6 shows the results on the StreamingQA dataset for two different base models. We observe that ablating the weighting model to only output two values slightly reduces performance, while still achieving significant F1 improvement and outperforming baseline approaches. The strong performance of the binary ablation suggests that a binary decision of whether to train on a given token is an effective approach to online adaptation, though the full version of CaMeLS that allows for variation in the weight magnitude still performs best.\nIn contrast, neither part-of-speech ablation produces effective knowledge retention, either performing worse than the uniform baseline or failing to significantly increase F1 score. This result strongly suggests that although part of speech correlates strongly with learned weights, part of speech alone is not sufficient to determine when a token contains important information. We conclude that context-awareness is indeed helpful for identifying\nimportant tokens in online adaptation."
        },
        {
            "heading": "4.6 Cross Dataset Transfer",
            "text": "Beyond generalizing to new base models, we now study CaMeLS\u2019s ability to generalize to new data distributions. We evaluate CaMeLS\u2019s performance for all nine possible combinations of train and test dataset, using StreamingQA, SQuAD, and ArchivalQA. Figure 7 shows the results. We find that CaMeLS trained on a different dataset still typically outperforms the baseline methods, providing stronger evidence that the weighting scheme learned by CaMeLS is general-purpose. The generalizability of CaMeLS\u2019s weighting model is a key attribute increasing its practical utility."
        },
        {
            "heading": "4.7 Forgetting and plasticity",
            "text": "So far, our evaluations have considered only the QA accuracy at the end of online adaptation. In this section, we investigate the evolution of learning during the online adaptation process. While adapting GPT-2 XL to data from StreamingQA, we evaluate the intermediate models produced by CaMeLS and baseline methods every 200 document updates. Results are plotted for two learning rates. 6.250e-6 is the optimal learning rate for the TF-IDF baseline while 2.500e-5 is the optimal learning rate for all other methods shown. Figure 8 shows the performance when intermediate models are evaluated on the entire set of evaluation queries and additionally evaluated on a set of unrelated queries sampled from the QA validation spit. CaMeLS consistently improves performance on test queries during online adaptation, while the best performing baseline \u2014\nuniform fine-tuning with a learning rate of 2.500e5 and additional QA-tuning \u2014 results in gradual degradation in test performance with improvement only becoming realized after the post-adaptation QA-tuning step. Turning to performance on unrelated queries, we see that all methods result in a gradual degradation in performance on independent queries. At a learning rate of 6.250e-6, all methods lead to comparable degradation in performance on unrelated queries. At a learning rate of 2.5e-6 CaMeLS leads to the lowest drop in unrelated query performance. Taken together, these results suggest that the CaMeLS is able to more effectively update the base model\u2019s knowledge, while still preserving the model\u2019s pre-existing knowledge and its representation of the task.\nFinally, in Figure 9, we aim to answer the questions how long does the model remember the answer to a question after observing it? We show the average improvement in F1 score across test queries against the number of timesteps since the model observed the document containing the answer to the query. Each adaptation method is applied using a uniquely tuned learning rate. After the 200 document sequence containing the relevant document, all methods see a clear average improvement in F1 score, signifying learning is happening. However, we also note that CaMeLS\nproduces both a higher initial improvement as well as a higher asymptotic improvement in F1 score. CaMeLS not only improves the immediate plasticity of the model, integrating knowledge more readily, but also reduces forgetting, preserving the newly-integrated knowledge for longer."
        },
        {
            "heading": "5 Discussion",
            "text": "While large language models are powerful, keeping them up-to-date remains a challenge. In this paper, we consider the unsupervised online language model adaptation setting, in which a language model\u2019s knowledge must be updated using a stream of documents, without annotations of key facts or information. Finding that naive online fine-tuning provides little retention of knowledge from the document stream, we propose Context-aware Metalearned Loss Scaling (CaMeLS), a meta-learning algorithm that learns an importance weighting model to reweight the per-token loss of the online data stream. CaMeLS leverages side information of the form of paired documents and knowledge queries about those documents to identify which tokens in the documents are most likely to be informative for answering downstream queries. Empirically, we find that the importance weighting model learned by CaMeLS consistently improves knowledge retention across three datasets of documents and questions. Crucially, we find that CaMeLS\u2019s importance weighting model generalizes across outdated language models and datasets, meaning that an importance weighting model can be trained once on a small proxy language model (such as DistilGPT-2) and then be immediately used to improve online adaptation of much larger models, like GPT-J 6B. This transferrability of CaMeLS\u2019s weighting model significantly increases its practical utility.\nLimitations & Future Work\nWhile our experiments suggest that learned importance weights consistently improve knowledge retention after unsupervised online adaptation, our study has several limitations. CaMeLS assumes access to side information in the form of training document, query, and label triples. This requirement may be onerous in domains where labeling is expensive. Future work may apply CaMeLS to settings without access to side information queries and labels, i.e., only a purely unlabeled stream of training documents, using the temporal structure of the data as the signal for learning. We study"
        },
        {
            "heading": "600 400 200 0 200 400 600 800",
            "text": "Document Updates since Adaptation\n0.01\n0.00\n0.01\n0.02\n0.03\n0.04\nF1 In\ncr ea\nse\nDocument Adaptation CaMeLS\nSalient Spans Uniform\nUniform + QA-tune TF-IDF + 5%% Cutoff\nFigure 9: While adapting GPT-2 XL on StreamingQA, we examine the average improvement in F1 score of queries against the time since the model observed the corresponding document. The shaded region represents the interval in which the source document was presented. CaMeLS leads to a larger initial improvement and asymptotic improvement in F1 score than other methods. Although this mid-adaptation evaluation does not use QA-tuning, Uniform + QA-tune corresponds to uniform fine tuning using a learning rate optimized to downstream performance given an additional QA-tuning step. Each adaptation method is applied using a uniquely tuned learning rate.\nadaptation on steams of thousands of documents. However, in order to effectively update outdated language models in real-world scenarios, it is reasonable to expect a significantly larger volume of documents. Beyond dataset scale, our experiments study adaptation of base models up to 6B parameters, but recent work suggests the continual learning dynamics of language models changes drastically at extreme scale (100B+ parameters); future work may increase the scale of the present study by considering adaptation on longer streams of documents using larger base evaluation models. Finally, we study only question-answering models and the question-answering task, as it is the most direct form of knowledge retention assessment. Future work may examine knowledge retention in other types of models through alternative downstream tasks that leverage the knowledge in the document stream more indirectly, as well as studying the ability to continually update general-purpose generative models of language or dialogue models."
        },
        {
            "heading": "Acknowledgements",
            "text": "The authors thank Huaxiu Yao for his input at multiple stages of the project. CF and CDM are CIFAR Fellows. EM gratefully acknowledges funding from a Knight-Hennessy Graduate Fellowship. This research was supported in part by Juniper Networks."
        },
        {
            "heading": "A Dataset Details",
            "text": "The sizes of dataset splits are shown in Table 3. Sample documents, questions, and answers are shown in Table 4. Only documents from 2018 and on-wards are used to the train, validation, and test splits of StreamingQA. For SQuAD, the entirety of the validation set of SQuAD is used at our test split. The topics in training set of SQuAD are repartitioned to form the other 4 splits. We divide the validation set of the ArchivalQA dataset to form our 5 splits. These splits are done temporally, using documents from 1987-1990 for QA Training, 1991-1992 for QA Validation, 1993-2001 for Training, 2002-2003 for Validation, and 2004-2007 for Testing."
        },
        {
            "heading": "B Larger Proxy Models",
            "text": "We conduct a preliminary investigation on the effect of using a larger proxy model during CaMeLS meta-training. By default, we use a QA-tuned DistilGPT2 (82M) as the proxy model. We additionally meta-train using a GPT-2 Small (117M) as the proxy model. Due to compute limitations we were not able to meta-train using any larger proxy models. Results on StreamingQA are shown in table 5. We see no significant difference in performance in this setting. Qualitatively, the two weighting models generate similar outputs. We hypothesize that CaMeLS learns a weighting which reflects the innate importance of tokens in the text to answering the meta-training questions, rather than a proxy model specific token importance. We emphasize that this is a hypothesis and believe a more rigorous exploration of proxy model size is an exciting direction for future work."
        },
        {
            "heading": "C Combining CaMeLS with other online Adaptation Methods",
            "text": "There are various other methods for online adaptation which leverage the adaptation documents. Two such methods are in-context learning and retrieval. This section shows preliminary experiments lever-\naging CaMeLS in conjunction with these methods on the ArchivalQA dataset. We show that CaMeLS is complementary to both in-context learning and retrieval; for both methods, the adaptation performance is improved by CaMeLS.\nIn our first set of experiments, we do five-shot in-context learning. We assume we can prompt the model with the oracle document containing the answer to the question (i.e., the best-case scenario for in-context learning). The prompt is formatted as [ex. doc 1] [ex. q 1] [ex. ans 1] . . . [ex. doc 5] [ex. q 5] [ex. ans 5] [oracle test doc] [test question]. We use the base GPT-2 XL and GPT-Neo 1.3B models (QA-tuned models performed much worse with in-context learning). As shown in Table 6, we find that adapting the base models with CaMeLS consistently improves the F1 scores of in-context learning.\nIn a second set of experiments, we consider a simple retrieval setup. Results are shown in Table 7. We fine-tune GPT-2 XL and GPT-Neo 1.3B to answer questions with the source document in the context. We retrieved documents using random, oracle, and BM25 document retrieval. We use CaMeLS to update the parameters of the documentconditioned question-answering models. Across\nmodels and retrievers, Using CaMeLS to adapt document-conditioned question-answering models consistently improves adaptation performance over vanilla retrieval.\nThese results use the CaMeLS weighting model trained using a QA-proxy model on ArchivalQA. We expect the performance of CaMeLS to increase if meta-trained using a proxy model and outer loss more analogous to the evaluation setting. For example, increase performance in the retrieval setting, we could present the source document when computing the outer loss and using a document conditioned QA proxy model. We acknowledge that we do not evaluate any baseline methods and think that extensive comparisons of parametric updating in conjunction with these other methods would be an exciting direction for future work. As is, these results do show that parametric online adaptation can be used to complement document-storage based methods."
        },
        {
            "heading": "D Optimal Online Adaptation Learning Rates",
            "text": "When evaluating loss reweighing methods in our experiments, the learning rate used to adapt our base models is found via a learning rate sweep. For each combination of dataset, base model, and adaptation method, we test a range of learning rates to adapt the base model on a stream of documents from the validation split of the dataset. The best performing learning rates are used for later experiments on the test split of the dataset. We test the following learning rates: [1e-4, 2.5e-5, 6.25e-6, 1.625e-6]. The optimal learning rates found via these sweeps are shown in Figure 10."
        }
    ],
    "title": "Meta-Learning Online Adaptation of Language Models",
    "year": 2023
}