{
    "abstractText": "The Universal Transformer (UT) is a variant of the Transformer that shares parameters across its layers. Empirical evidence shows that UTs have better compositional generalization than Vanilla Transformers (VTs) in formal language tasks. The parameter-sharing also affords it better parameter efficiency than VTs. Despite its many advantages, scaling UT parameters is much more compute and memory intensive than scaling up a VT. This paper proposes the Sparse Universal Transformer (SUT), which leverages Sparse Mixture of Experts (SMoE) and a new stick-breaking-based dynamic halting mechanism to reduce UT\u2019s computation complexity while retaining its parameter efficiency and generalization ability. Experiments show that SUT achieves the same performance as strong baseline models while only using half computation and parameters on WMT\u201914 and strong generalization results on formal language tasks (Logical inference and CFQ). The new halting mechanism also enables around 50% reduction in computation during inference with very little performance decrease on formal language tasks.",
    "authors": [
        {
            "affiliations": [],
            "name": "Shawn Tan"
        },
        {
            "affiliations": [],
            "name": "Yikang Shen"
        },
        {
            "affiliations": [],
            "name": "Zhenfang Chen"
        },
        {
            "affiliations": [],
            "name": "Aaron Courville"
        },
        {
            "affiliations": [],
            "name": "Chuang Gan"
        }
    ],
    "id": "SP:dd8cb8bfa227df21129b4d323e2d41a91b087078",
    "references": [
        {
            "authors": [
                "Leon Bergen",
                "Timothy O\u2019Donnell",
                "Dzmitry Bahdanau"
            ],
            "title": "Systematic generalization with edge transformers",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Samuel R Bowman",
                "Christopher D Manning",
                "Christopher Potts."
            ],
            "title": "Tree-structured composition in neural networks without tree-structured architectures",
            "venue": "arXiv preprint arXiv:1506.04834.",
            "year": 2015
        },
        {
            "authors": [
                "Chen",
                "Hengshuang Zhao",
                "Erik Learned-Miller",
                "Chuang Gan."
            ],
            "title": "Mod-squad: Designing mixture of experts as modular multi-task learners",
            "venue": "arXiv preprint arXiv:2212.08066.",
            "year": 2022
        },
        {
            "authors": [
                "Kenneth Church",
                "Patrick Hanks."
            ],
            "title": "Word association norms, mutual information, and lexicography",
            "venue": "Computational linguistics, 16(1):22\u201329.",
            "year": 1990
        },
        {
            "authors": [
                "R\u00f3bert Csord\u00e1s",
                "Kazuki Irie",
                "J\u00fcrgen Schmidhuber."
            ],
            "title": "The devil is in the detail: Simple tricks improve systematic generalization of transformers",
            "venue": "arXiv preprint arXiv:2108.12284.",
            "year": 2021
        },
        {
            "authors": [
                "Mostafa Dehghani",
                "Stephan Gouws",
                "Oriol Vinyals",
                "Jakob Uszkoreit",
                "\u0141ukasz Kaiser."
            ],
            "title": "Universal transformers",
            "venue": "arXiv preprint arXiv:1807.03819.",
            "year": 2018
        },
        {
            "authors": [
                "Gr\u00e9goire Del\u00e9tang",
                "Anian Ruoss",
                "Jordi Grau-Moya",
                "Tim Genewein",
                "Li Kevin Wenliang",
                "Elliot Catt",
                "Marcus Hutter",
                "Shane Legg",
                "Pedro A Ortega."
            ],
            "title": "Neural networks and the chomsky hierarchy",
            "venue": "arXiv preprint arXiv:2207.02098.",
            "year": 2022
        },
        {
            "authors": [
                "Andrew Drozdov",
                "Nathanael Sch\u00e4rli",
                "Ekin Aky\u00fcrek",
                "Nathan Scales",
                "Xinying Song",
                "Xinyun Chen",
                "Olivier Bousquet",
                "Denny Zhou."
            ],
            "title": "Compositional semantic parsing with large language models",
            "venue": "arXiv preprint arXiv:2209.15003.",
            "year": 2022
        },
        {
            "authors": [
                "Nouha Dziri",
                "Ximing Lu",
                "Melanie Sclar",
                "Xiang Lorraine Li",
                "Liwei Jian",
                "Bill Yuchen Lin",
                "Peter West",
                "Chandra Bhagavatula",
                "Ronan Le Bras",
                "Jena D Hwang"
            ],
            "title": "Faith and fate: Limits of transformers on compositionality",
            "venue": "arXiv preprint arXiv:2305.18654",
            "year": 2023
        },
        {
            "authors": [
                "Maha Elbayad",
                "Jiatao Gu",
                "Edouard Grave",
                "Michael Auli."
            ],
            "title": "Depth-adaptive transformer",
            "venue": "arXiv preprint arXiv:1910.10073.",
            "year": 2019
        },
        {
            "authors": [
                "William Fedus",
                "Barret Zoph",
                "Noam Shazeer"
            ],
            "title": "Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity",
            "year": 2021
        },
        {
            "authors": [
                "Daniel Furrer",
                "Marc van Zee",
                "Nathan Scales",
                "Nathanael Sch\u00e4rli."
            ],
            "title": "Compositional generalization in semantic parsing: Pre-training vs",
            "venue": "specialized architectures. arXiv preprint arXiv:2007.08970.",
            "year": 2020
        },
        {
            "authors": [
                "Alex Graves."
            ],
            "title": "Adaptive computation time for recurrent neural networks",
            "venue": "arXiv preprint arXiv:1603.08983.",
            "year": 2016
        },
        {
            "authors": [
                "Michael Hahn."
            ],
            "title": "Theoretical limitations of selfattention in neural sequence models",
            "venue": "Transactions of the Association for Computational Linguistics, 8:156\u2013 171.",
            "year": 2020
        },
        {
            "authors": [
                "Yiding Hao",
                "Dana Angluin",
                "Robert Frank."
            ],
            "title": "Formal language recognition by hard attention transformers: Perspectives from circuit complexity",
            "venue": "Transactions of the Association for Computational Linguistics, 10:800\u2013810.",
            "year": 2022
        },
        {
            "authors": [
                "Dieuwke Hupkes",
                "Verna Dankers",
                "Mathijs Mul",
                "Elia Bruni"
            ],
            "title": "Compositionality decomposed: How do neural networks generalise",
            "venue": "Journal of Artificial Intelligence Research,",
            "year": 2020
        },
        {
            "authors": [
                "\u0141ukasz Kaiser",
                "Ilya Sutskever."
            ],
            "title": "Neural gpus learn algorithms",
            "venue": "arXiv preprint arXiv:1511.08228.",
            "year": 2015
        },
        {
            "authors": [
                "Jared Kaplan",
                "Sam McCandlish",
                "Tom Henighan",
                "Tom B Brown",
                "Benjamin Chess",
                "Rewon Child",
                "Scott Gray",
                "Alec Radford",
                "Jeffrey Wu",
                "Dario Amodei."
            ],
            "title": "Scaling laws for neural language models",
            "venue": "arXiv preprint arXiv:2001.08361.",
            "year": 2020
        },
        {
            "authors": [
                "Daniel Keysers",
                "Nathanael Sch\u00e4rli",
                "Nathan Scales",
                "Hylke Buisman",
                "Daniel Furrer",
                "Sergii Kashubin",
                "Nikola Momchev",
                "Danila Sinopalnikov",
                "Lukasz Stafiniak",
                "Tibor Tihon"
            ],
            "title": "Measuring compositional generalization: A comprehensive method",
            "year": 2019
        },
        {
            "authors": [
                "Yoon Kim."
            ],
            "title": "Sequence-to-sequence learning with latent neural grammars",
            "venue": "Advances in Neural Information Processing Systems, 34:26302\u201326317.",
            "year": 2021
        },
        {
            "authors": [
                "Zhenzhong Lan",
                "Mingda Chen",
                "Sebastian Goodman",
                "Kevin Gimpel",
                "Piyush Sharma",
                "Radu Soricut."
            ],
            "title": "Albert: A lite bert for self-supervised learning of language representations",
            "venue": "arXiv preprint arXiv:1909.11942.",
            "year": 2019
        },
        {
            "authors": [
                "Yuanpeng Li",
                "Liang Zhao",
                "Jianyu Wang",
                "Joel Hestness."
            ],
            "title": "Compositional generalization for primitive substitutions",
            "venue": "arXiv preprint arXiv:1910.02612.",
            "year": 2019
        },
        {
            "authors": [
                "Bingbin Liu",
                "Jordan T Ash",
                "Surbhi Goel",
                "Akshay Krishnamurthy",
                "Cyril Zhang."
            ],
            "title": "Transformers learn shortcuts to automata",
            "venue": "arXiv preprint arXiv:2210.10749.",
            "year": 2022
        },
        {
            "authors": [
                "Xiaodong Liu",
                "Kevin Duh",
                "Liyuan Liu",
                "Jianfeng Gao."
            ],
            "title": "Very deep transformers for neural machine translation",
            "venue": "arXiv preprint arXiv:2008.07772.",
            "year": 2020
        },
        {
            "authors": [
                "William Merrill",
                "Ashish Sabharwal",
                "Noah A Smith."
            ],
            "title": "Saturated transformers are constant-depth threshold circuits",
            "venue": "Transactions of the Association for Computational Linguistics, 10:843\u2013856.",
            "year": 2022
        },
        {
            "authors": [
                "Ott Myle",
                "Edunov Sergey",
                "Grangier David",
                "Auli Michael"
            ],
            "title": "Scaling neural machine translation",
            "year": 2018
        },
        {
            "authors": [
                "Myle Ott",
                "Sergey Edunov",
                "Alexei Baevski",
                "Angela Fan",
                "Sam Gross",
                "Nathan Ng",
                "David Grangier",
                "Michael Auli."
            ],
            "title": "fairseq: A fast, extensible toolkit for sequence modeling",
            "venue": "arXiv preprint arXiv:1904.01038.",
            "year": 2019
        },
        {
            "authors": [
                "Kishore Papineni",
                "Salim Roukos",
                "Todd Ward",
                "WeiJing Zhu."
            ],
            "title": "Bleu: a method for automatic evaluation of machine translation",
            "venue": "Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 311\u2013318.",
            "year": 2002
        },
        {
            "authors": [
                "Hao Peng",
                "Roy Schwartz",
                "Dianqi Li",
                "Noah A Smith."
            ],
            "title": "A mixture of h-1 heads is better than h heads",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6566\u2013 6577.",
            "year": 2020
        },
        {
            "authors": [
                "Jake Russin",
                "Jason Jo",
                "Randall C O\u2019Reilly",
                "Yoshua Bengio"
            ],
            "title": "Compositional generalization in a deep seq2seq model by separating syntax and semantics. arXiv preprint arXiv:1904.09708",
            "year": 2019
        },
        {
            "authors": [
                "Tal Schuster",
                "Adam Fisch",
                "Jai Gupta",
                "Mostafa Dehghani",
                "Dara Bahri",
                "Vinh Q Tran",
                "Yi Tay",
                "Donald Metzler."
            ],
            "title": "Confident adaptive language modeling",
            "venue": "arXiv preprint arXiv:2207.07061.",
            "year": 2022
        },
        {
            "authors": [
                "Noam Shazeer",
                "Azalia Mirhoseini",
                "Krzysztof Maziarz",
                "Andy Davis",
                "Quoc Le",
                "Geoffrey Hinton",
                "Jeff Dean."
            ],
            "title": "Outrageously large neural networks: The sparsely-gated mixture-of-experts layer",
            "venue": "arXiv preprint arXiv:1701.06538.",
            "year": 2017
        },
        {
            "authors": [
                "Yikang Shen",
                "Shawn Tan",
                "Arian Hosseini",
                "Zhouhan Lin",
                "Alessandro Sordoni",
                "Aaron C Courville."
            ],
            "title": "Ordered memory",
            "venue": "Advances in Neural Information Processing Systems, 32.",
            "year": 2019
        },
        {
            "authors": [
                "Yikang Shen",
                "Zheyu Zhang",
                "Tianyou Cao",
                "Shawn Tan",
                "Zhenfang Chen",
                "Chuang Gan."
            ],
            "title": "Moduleformer: Learning modular large language models from uncurated data",
            "venue": "arXiv preprint arXiv:2306.04640.",
            "year": 2023
        },
        {
            "authors": [
                "Vsevolod Sourkov."
            ],
            "title": "Igloo: Slicing the features space to represent sequences",
            "venue": "arXiv preprint arXiv:1807.03402.",
            "year": 2018
        },
        {
            "authors": [
                "Christian Szegedy",
                "Vincent Vanhoucke",
                "Sergey Ioffe",
                "Jon Shlens",
                "Zbigniew Wojna."
            ],
            "title": "Rethinking the inception architecture for computer vision",
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2818\u20132826.",
            "year": 2016
        },
        {
            "authors": [
                "Sho Takase",
                "Shun Kiyono."
            ],
            "title": "Lessons on parameter sharing across layers in transformers",
            "venue": "arXiv preprint arXiv:2104.06022.",
            "year": 2021
        },
        {
            "authors": [
                "Shawn Tan",
                "Yikang Shen",
                "Timothy J O\u2019Donnell",
                "Alessandro Sordoni",
                "Aaron Courville"
            ],
            "title": "Recursive top-down production for sentence generation with latent trees. arXiv preprint arXiv:2010.04704",
            "year": 2020
        },
        {
            "authors": [
                "Shawn Tan",
                "Khe Chai Sim."
            ],
            "title": "Towards implicit complexity control using variable-depth deep neural networks for automatic speech recognition",
            "venue": "2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 5965\u20135969.",
            "year": 2016
        },
        {
            "authors": [
                "Yi Tay",
                "Mostafa Dehghani",
                "Samira Abnar",
                "Hyung Won Chung",
                "William Fedus",
                "Jinfeng Rao",
                "Sharan Narang",
                "Vinh Q Tran",
                "Dani Yogatama",
                "Donald Metzler"
            ],
            "title": "Scaling laws vs model architectures: How does inductive bias influence scaling",
            "year": 2022
        },
        {
            "authors": [
                "Yi Tay",
                "Mostafa Dehghani",
                "Samira Abnar",
                "Yikang Shen",
                "Dara Bahri",
                "Philip Pham",
                "Jinfeng Rao",
                "Liu Yang",
                "Sebastian Ruder",
                "Donald Metzler."
            ],
            "title": "Long range arena: A benchmark for efficient transformers",
            "venue": "arXiv preprint arXiv:2011.04006.",
            "year": 2020
        },
        {
            "authors": [
                "Ke Tran",
                "Arianna Bisazza",
                "Christof Monz."
            ],
            "title": "The importance of being recurrent for modeling hierarchical structure",
            "venue": "arXiv preprint arXiv:1803.03585.",
            "year": 2018
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin."
            ],
            "title": "Attention is all you need",
            "venue": "Advances in neural information processing systems, 30.",
            "year": 2017
        },
        {
            "authors": [
                "Felix Wu",
                "Angela Fan",
                "Alexei Baevski",
                "Yann Dauphin",
                "Michael Auli."
            ],
            "title": "Pay less attention with lightweight and dynamic convolutions",
            "venue": "International Conference on Learning Representations.",
            "year": 2018
        },
        {
            "authors": [
                "Xiaofeng Zhang",
                "Yikang Shen",
                "Zeyu Huang",
                "Jie Zhou",
                "Wenge Rong",
                "Zhang Xiong."
            ],
            "title": "Mixture of attention heads: Selecting attention heads per token",
            "venue": "arXiv e-prints, pages arXiv\u20132210.",
            "year": 2022
        },
        {
            "authors": [
                "Hao Zheng",
                "Mirella Lapata."
            ],
            "title": "Disentangled sequence to sequence learning for compositional generalization",
            "venue": "arXiv preprint arXiv:2110.04655.",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Recent theoretical work has pointed out that finitedepth Transformers have an issue of expressibility that will result in failure to generalize (Hahn, 2020; Hao et al., 2022; Merrill et al., 2022; Liu et al., 2022). Del\u00e9tang et al. (2022) ran several neural architectures on a suite of different synthetic languages generated from different levels of the Chomsky hierarchy and empirically confirmed these results, showing that VTs have difficulty generalizing to Regular languages. Universal Transformers (UTs; Dehghani et al. 2018) are Transformers that share parameters at every layer of the architecture. Csord\u00e1s et al. (2021) performed several compositional generalization experiments on VTs and UTs\n\u2217 Equal contribution\nalong with absolute and relative position embeddings, and showed that UTs with relative positional embeddings performed better on these tasks.\nHowever, the task of scaling UTs is challenging due to its computation complexity (Kaplan et al., 2020; Tay et al., 2022; Takase and Kiyono, 2021). Consider a VT with P parameters for each layer and L layers. Evaluating such a VT has computation complexity associated with the model size LP . A size-equivalent UT would have a UT block with LP parameters and computation complexity of approximately LP to run the block once. To run such a UT for equivalent L layers would incur a complexity of L2P . This increased computation complexity directly translates to increased training and inference time. According to Takase and Kiyono (2021), UT requires two times the training time and far more GPU memory than VT in WMT English-German translation task.\nSparsely activated neural networks were intro-\nduced to reduce the computation complexity of large models. These networks activate parts of the network conditioned on the input, computing only parts of the model, thereby disentangling the number of parameters from the computation complexity. This method allows for drastically increasing the number of parameters without proportionally increasing the computation complexity. Shazeer et al. (2017) introduced Sparse Mixture of Experts (SMoE), using the top-k operator to allow for sparse computation of experts. This allows for replacing the FeedForword (FFD) layer in the Transformer with an ensemble of Effd FFDs, but only k FFDs (where k < E) would have to be evaluated, conditioned on the input. Zhang et al. (2022) then introduced the Mixture of Attention heads (MoA), which allows Transformers to replace its Multihead Attention (MHA) layer with an ensemble of Eatt attention heads and only activates k heads condition on the input, further sparsifying the model.\nThis paper introduces the Sparse Universal Transformer (SUT), which applies the above sparse computation techniques to UT. Additionally, we replace the per-position halting mechanism in UT with a new stick-breaking formulation that has a probabilistic interpretation, allowing us to introduce an Adaptive Computation Time (ACT; Graves 2016) penalty to minimize layer use. It also provides an easy way to adjust the trade-off between the amount of computation and model performance during inference, further improving the efficiency of the SUT at inference time.\nTo demonstrate effective scaling, we perform experiments on WMT\u201914 English to German translation, showing that an SUT can achieve better performance for the same parameter count, while incurring less computation cost than an equivalent dense UT. Since the UT setting is a specific case of SUT, we show on the Compositional Freebase Questions (CFQ; Keysers et al. 2019) tasks that UTs have better compositional generalization properties, improving upon CFQ results from Csord\u00e1s et al. (2021). Using the Logical Inference task (Bowman et al., 2015), we analyse the behaviour of our UT on length and compositional generalization. Finally, we show that the halting mechanism can be used to achieve further efficiency during inference time, and study the trade-off between efficiency and performance."
        },
        {
            "heading": "2 Background & Related Work",
            "text": "Overcoming VT limitations with UT Dziri et al. (2023) and Liu et al. (2022) find that Vanilla Transformers learn shortcuts for tasks that require multistep compositional operations, and fail to generalize on larger instances of the problem that require more steps. Theoretical results have also shown that Vanilla Transformers have limitations in what they can compute that support these findings (Hahn, 2020; Hao et al., 2022; Merrill et al., 2022). Universal Transformers (Dehghani et al., 2018) are Transformers with tied weights across all layers, and an additional halting mechanism to decide when to stop. In an ideal scenario of infinite layers (now that all layers have the same parameters) the UT, like the Neural GPU (Kaiser and Sutskever, 2015), is Turing-complete, which overcomes many of the abovementioned issues.\nIn practice, even with limited depth, UTs have exhibited properties that afford them better performance in compositional generalization tasks (Csord\u00e1s et al., 2021). UTs allow operations learned in the Transformer during training to be depth-order invariant. If some operations during training are learned to be performed in a certain order, during test time, the UT could generalize to an unseen order of operations.\nChallenges with Scaling the UT Despite these compositional abilities, performance tends to decrease on real-world tasks when using UTs. ALBERT (Lan et al., 2019) improved parameter efficiency by sharing parameters across layers. This was motivated by an observation that Transformers tend to learn to perform similar operations in the layers, and that sharing these parameters would\nreduce this redundancy1. However, the authors observe a dip in performance when sharing parameters, contrary to Dehghani et al. (2018).\nCould the issue be one of model capacity? Experiments with ALBERT show that scaling up ALBERT can outperform the BERT baseline, even on real-world tasks (Lan et al., 2019). Kaplan et al. (2020) also show that a shared-parameter Transformer has better scaling properties in terms of parameter-to-performance, but poorer properties in terms of computation-to-performance, since parameter count causes the computation to increase. Tay et al. (2022) scale up different sequence models, and remark on difficulties with scaling up UTs, limiting the experiments they can perform on UT. Takase and Kiyono (2021) outline several strategies of scaling up shared-parameter Transformers to deal with these issues by using different parametersharing schemes.\nOur experiments show that SMoE techniques can be applied successfully to the UT to scale it up, achieving the UT\u2019s parameter efficiency while not incurring the same computation costs. We also perform experiments that support the compositional generalization claims of prior work, and provide better baselines for those tasks."
        },
        {
            "heading": "3 Method",
            "text": "Like UT, we reuse the same SUT block for every layer of the Transformer. Within each SUT block, we use SMoEs to achieve sparsity for the feedforward network (FFD) and attention heads separately. We use the Mutual Information Maximization loss proposed in Chen et al. (2022) and modified for unsupervised tasks in Shen et al. (2023). Finally, we propose a stick-breaking process formulation of dynamic halting, which affects how the attention mechanism works in the SUT, and the Adaptive Computation Time (ACT) auxiliary loss we use to minimize the number of layers used."
        },
        {
            "heading": "3.1 Sparse Mixture of Experts",
            "text": "A Mixture of Experts module consists of E submodules f1, . . . , fE . There is also a gating network, which we will denote by g(e | h) \u2013 for any input h to the MoE module, the gating network would predict a distribution over the E experts. When k < E, we refer to this as a Sparse Mixture of Experts (SMoE), and g(e | h) > 0 for only k ex-\n1https://ai.googleblog.com/2019/12/ albert-lite-bert-for-self-supervised.html\nperts, while maintaining that \u2211E\ne g(e | h) = 1. The final output of the SMoE is then given by y = \u2211E e=1 g(e | h) \u00b7 fe(h), where g(e | h) = 0, fe(h) will not need to be evaluated, reducing computation cost during training and inference. We replace the Feed-forward layer (FFD) in the Transformer block with a mixture of FFDs. Each Mixture of FFD can be described with a 3-tuple, (E, k,D): E experts, k for the number of experts to be used in the top-k operation, and D for the dimension of the hidden layer for each FFD expert. For the attention layer, we use the Mixture of Multihead Attention (MoMHA) proposed by Zhang et al. (2022) and Shen et al. (2023). Each MoMHA module can be described by a 5-tuple, (E, k,H,D,W ), with E representing the number of experts, K representing the parameter k in a top-k operation, H representing the number of attention heads per expert, and D for the dimensions per head. Like in MoA, MoMHA maintains only a single set of H key-value projections shared among the experts, while there are E query projections of H heads each. W represents the relative position embedding window size, parameterizing 2W + 1 embeddings for W positions before and after the present position. Figure 3 (Left) shows the schematic of a SUT block.\nThis technique has been used to reduce computation costs both during training and inference time for large models.\nMutual Information Maximisation Like other models that rely on conditional activation, auxiliary losses are needed in order to aid learning a module that decides which experts are activated, and to ensure that all experts are used, balancing the load for processing. For this, we use the Mutual Information Maximization introduced in Chen et al. (2022) for the auxiliary loss (to be maximised):\nLMIM = E\u2211\ne=1 g(e) log g(e)\ufe38 \ufe37\ufe37 \ufe38 \u2212H(e)\n\u2212 1 |X | \u2211 h\u2208X E\u2211 e=1\ng(e | h) log g(e | h)\ufe38 \ufe37\ufe37 \ufe38 H(e|h) , (1)\nwhere,\ng(e) = 1 |X | \u2211 h\u2208X g(e|h)\n. Specifically, we use the unsupervised version proposed by Shen et al. (2023) that assumes a uniform distribution over all tokens and layers, resulting in the following auxiliary objective. In the SUT setting, the gating network is used |X | = L \u00b7 T times, where L is the number of layers, and T is the number of timesteps.\nIntuitively, the entropy term increases the entropy of the marginal probability of the gating network predictions, which at its maximum means that the weight for each gating network across the entire minibatch is uniform. The conditional entropy term decreases the conditional entropy, which causes the prediction of the gating network to be sharp, and also penalizes the uniform distribution solution for the gating network."
        },
        {
            "heading": "3.2 Stick-breaking Dynamic Halting",
            "text": "There have been several methods for imbuing models with the ability to make a prediction without having to use all layers of the model (Graves, 2016; Tan and Sim, 2016; Dehghani et al., 2018; Elbayad et al., 2019; Schuster et al., 2022). Motivations for this include: (1) different inputs require different amounts of iteration to make a prediction, (2) reducing computation cost.\nUT implements a similar mechanism, but the UT version of halting is difficult to interpret. Here we choose a principled version of the dynamic halting mechanism based on the stick-breaking process, viewing it as a probability distribution. First, \u03b1\u0302(t)l are the halting probabilities predicted by halt(h(t)l )\nAlgorithm 1 Halting mechanism at a given timestep t\nfor l = 1 to L do if \u2211l\u22121\nl\u2032=1 \u03b1 (t) l\u2032 < \u03b1thresh then \u03b1\u0302 (t) l\u22121 = halt(h (t) l\u22121)\n\u03b1 (t) l\u22121 = \u03b1\u0302 (t) l\u22121 l\u22122\u220f l\u2032=1 (1\u2212 \u03b1\u0302(t)l\u2032 ) a (t) l = Attention(h\n(t) l\u22121\ufe38\ufe37\ufe37\ufe38 Q ,Sl\u22121\ufe38\ufe37\ufe37\ufe38 K ,Sl\u22121\ufe38\ufe37\ufe37\ufe38 V )\nh (t) l = FeedForward(h (t) l\u22121,a (t) l ) s (t) l = ( 1\u2212 \u2211l\u22121 l\u2032=1 \u03b1 (t) l\u2032 ) \u00b7 h(t)l\n+ (\u2211l\u22121\nl\u2032=1 \u03b1 (t) l\u2032 \u00b7 h (t) l\u2032 ) else\nh (t) l = h (t) l\u22121 s (t) l = s (t) l\u22121\nend if end for\n, a function which is implemented by an MLP that takes in the previous layer\u2019s embedding. Then, the probability of any layer halting is computed by\n\u03b1 (t) l = \u03b1\u0302 (t) l l\u22121\u220f l\u2032=1 (1\u2212 \u03b1\u0302(t)l\u2032 ). (2)\nA similar formulation is described in Graves (2016) and Tan and Sim (2016). Algorithm 1 shows how the mechanism is implemented at any given timestep. hl\u22121 is the output of the previous layer for the current timestep.\nConditioned on the fact that we are computing h (t) l , time-step t must not have halted before or at l \u2212 1. So we can use h(t)l , the unhalted state, as\ninput to the computation of the attention query of the block. However, since time-step t can attend to all other timesteps, and it these other steps may have halted, we use the halted states Sl\u22121 for the previous layers.\nHowever, because the halting is a \u2018soft\u2019 decision, we can relax the requirement for evaluating all possible halted states and use the expected halted state as a substitute. Previous halting mechanisms use a \u2018gating\u2019 mechanism of convex sums between previously gated outputs and the current step\u2019s output hl = \u03b1l \u00b7 h\u0302l + (1 \u2212 \u03b1l) \u00b7 hl\u22121 (Dehghani et al., 2018). This can lead to vanishingly small gradients going up the layers as (1\u2212 \u03b1l) multiplies. We can instead compute the expected halted embedding at any l,\ns (t) l =\n( 1\u2212\nl\u22121\u2211 l\u2032=1 \u03b1 (t) l\u2032 ) \u00b7 h(t)l\ufe38 \ufe37\ufe37 \ufe38\nprevious layer if not halted\n+ l\u22121\u2211 l\u2032=1 \u03b1 (t) l\u2032 h (t) l\u2032\ufe38 \ufe37\ufe37 \ufe38\nhalted at < l\n(3)\nIf \u03b1(t)l = 1 for some l, s (t) l = h (t) l , recovering the behavior of the discrete halting decision. We use s(t)l as input to the attention key and value transformations.\nThis probabilistic interpretation also allows us to impose a loss on the expected number of layers used at each step, biasing the model towards fewer iterations, thereby saving computational cost.\nLACT = 1\nT T\u2211 t=1 L\u2211 l=1 \u03b1 (t) l \u00b7 l. (4)\nWe use a threshold \u03b1thresh = 0.999, such that the cumulative sum of the halting probabilities has exceeded this, no computation will be performed for that time step, and the previous layer\u2019s embeddings will be copied. Due to the routing operation required in the implementation fo SMoEs, we can simply route halted states to a \u201cNo Op\u201d expert, leading to real savings in computation cost when halting hits the threshold early. We find that adjusting this threshold after training can maintain performance while saving computation steps."
        },
        {
            "heading": "4 Experiments",
            "text": "First, we show that we can scale the UT with SUT on the WMT\u201914 English-German (Bojar et al., 2014) translation task. We then ran experiments on Compositional Freebase Questions (CFQ; Keysers et al. 2019) to test for compositional generalization\nproperties. To further analyze the behaviour of the model under compositional generalization settings, we test our model on the Logical inference task from (Bowman et al., 2015). All experiments were implemented within the Fairseq framework (Ott et al., 2019) 2."
        },
        {
            "heading": "4.1 English to German Translation",
            "text": "We perform experiments on the WMT\u201914 EnglishGerman translation dataset (Bojar et al., 2014). We use the pre-processing from Liu et al. (2020). We use a joined dictionary and share all word embeddings of the encoder and decoder. For evaluation, we average the last 5 best models according to their negative log-likelihood scores. We report the BLEU scores (Papineni et al., 2002), and also report the MACs (Multiply-Accumulate Operations) to evaluate the runtime computational costs of the\n2Fairseq-based implementation available on: https:// github.com/shawntan/sut\ndifferent models. MACs of previous models were computed in Zhang et al. (2022).\nThe results are reported in Table 1. We compare against strong baselines while accounting for the number of parameters in these models. In addition, we train two UTs by setting E = 1, k = 1, and parameterizing the FFD and Attention layers with parameters to match our \u223c65M, and \u223c110M setting for SUT. The SUTs and UTs both demonstrate good parameter efficiency when compared to previous models. In the \u223c110M parameter class, SUT and UT perform at around 29.4 and 29.6 BLEU respectively, while previous models require \u223c200M parameters. While the SUT does not perform as well as the UT, but the computations required during runtime could be as low as one-fifth of UT. Also, because we keep k constant for SUT, the MACs stays constant as SUT scales up.\nWe ran experiments removing different aspects of the model and its training process, including: MIM auxiliary loss, Mixture of MHA, the ACT loss, and the halting mechanism. The results are in Table 2. The introduction of multiple heads to the MoA was crucial in seeing performance gains on this task, as well as having the MIM loss as a load-balancing auxiliary objective. Interestingly, halting does contribute as much of a performance gain as it does in CFQ.\nAdditionally, we compute the top 5 tokens that occur in conjunction with each expert, regardless of layers, and find that certain associations exist. We pick several experts in Table 3 that show a clear sign of co-occurring with tokens that seem to show a pattern. This suggests that while there may be redundancy between the experts, groups of experts can specialize on certain tasks, resulting in some modularity. Future work can investigate if such\n1The open-source tool PTFLOPS (https://github.com/ sovrasov/flops-counter.pytorch) is used to calculate the MACs.\n2The MACs values of DynamicConv and LightConv are underestimated. Because the PTFLOPS does not support the customized convolution layers.\nmodularity can result in more robust generalization."
        },
        {
            "heading": "4.2 Compositional Freebase Questions",
            "text": "We run experiments on the Compositional Freebase Questions (CFQ; Keysers et al. 2019) dataset to determine the compositional generalization abilities of the SUT. This is a translation task from natural language to a SPARQL query. As an example, the sequence Who wrote M1 and wrote a film would be translated to the target sequence SELECT DISTINCT ?x0 WHERE { ?x0 a people.person . ?x0 film.writer.film ?x1 M1 . ?x1 a film.film }. CFQ tests for compositional generalization using the notion of compound divergence, which measures how different the training set and test set are in terms of combinations of tokens, which they refer to as compounds. To our knowledge, the current best-performing models either finetune a pretrained language model or, use knowledge about the task to design a suitable prompt for a large language model (Drozdov et al., 2022). While the prompting approach is extremely effective at the CFQ task, we view the task as a benchmark for compositional generalization in general and should be viewed in concert with other experiments, especially real-world data (like translation). When using domain knowledge of the task in the prompt, the results may indicate better performance with a specific approach for CFQ (and perhaps other SQL translation tasks) but might be difficult to extrapolate to other settings.\nIn our experiments, we use preprocessing scripts from Zheng and Lapata (2021). The scripts perform preprocessing to the target sequence that simplifies the target sequence the same way performed in Furrer et al. (2020). Accordingly, we train a baseline Transformer on the transformed target. We performed a search on the SUT hyperparameters, using the MCD1 validation set, and the best-performing set of parameters are Attention (E = 1, k = 1, H = 8, D = 64,W = 1) and FFD (E = 1, k = 1, D = 1024), which corresponds to the UT setting. Refer to Appendix A for further details. Since CFQ is a relatively small task, larger scale is not a factor and might suggest that expert specialization may not be as helpful. The results are shown in Table 4. In cases with and without halting, the model already outperforms previous benchmarks, including the UT baseline from Bergen et al. (2021). For a fairer comparison, we use the same hyperparameters as our UT imple-\nmentation, we modify our UT implementation to be more similar to the T5-based UT in Bergen et al. (2021). These changes include: the bucketed relative position bias used by T5, and going from post layer-norm to pre layer-norm. While this results in much improved results compared to the original paper, our implementation of UT still outperforms it.\nThe Dangle (Zheng and Lapata, 2021) model, which beats our model, also requires re-running the encoder for every token decoded. This is an expensive process, but given that both our method and Dangle perform well at this task, is additional evidence that iterative processes are beneficial for compositional generalization."
        },
        {
            "heading": "4.3 Logical Inference",
            "text": "We use the logical inference task from (Bowman et al., 2015) as a test bench for UT. Despite the apparent simplicity of the language, the task inherently requires the model to learn the hierarchical structure of the problem. Each instance of the task comprises of two logical statements, and the goal is to predict if the statements are equivalent, contradictory, disjoint, or entail in either direction. For example, given s1 = a and s2 = a ( or b ),\nthen s1 \u228f s2. The crux of the task is in training the model on sequences that have 0-6 logical operators and evaluating it on sequences that have 7-12 operators. Given our sequence-to-sequence setting, we convert the task into a translation task. The model takes sentence1 #SEP# sentence2 as its source sentence, with the target sentence being the single-token label for that pair.\nWe train a 12 layer model with Attention (E = 12, k = 4, H = 2, D = 32,W = 1) and FFD (E = 12,K = 4, D = 128) and halting. Refer to Appendix A for further details. Training a 12- layer Vanilla Transformer achieves approximately the same results as in Shen et al. (2019), so we report their results. Our results in Table 5 confirm the findings of Tran et al. (2018), showing that with recurrence in SUTs, we are able to generalize to longer sequences of the task. While there are other models that induce a tree structure that performs exceedingly well on the task, we wanted to evaluate our model against other popular architectures. The LSTM is a strong baseline, and we find that UT outperforms it in generalization. We also evaluate UTs on the compositional generalization splits as proposed in (Shen et al., 2019), where the splits A, B, and C are in increasing difficulty. The results show that UTs are able to generalize better for the A and B splits, outperforming the LSTM and VT. Split C is still presents a challenge for the Transformer variants.\nAdditionally, we compute the average halting depth for the test data segmented by operator counts. Because more operators require more nesting of expressions in the sequences, more recursion is required to properly parse the sequence. As expected, in Figure 4, the average halting depth increases as more operators are used. The operator count for these clauses are correlated with length,\nwhich suggests that SUTs may be suited to generalize for length. We include further experiments on length generalization in the Appendix Table 8."
        },
        {
            "heading": "4.4 Post-training Computation Reduction",
            "text": "Does lowering \u03b1thresh after training cause the model to halt earlier, saving computation? How much would that cost us in terms of accuracy?\nWe estimate the skipped SUT block computations given different values of \u03b1thresh \u2208 {0.1, 0.2, . . . , 0.9} by looking at the halting patterns of the decoder given the ground truth sourcetarget pairs. We pass the source-target pair into the model and analyze the halting patterns of the model, giving us a rough estimate of how much computation would be saved as a percentage of computing all layers of the SUT.\nLogical Inference We observe the resulting performance on the hardest split of the test set with 12 operations. Due to the already saturated halting pattern, the halting probability \u03b1l spikes rapidly from close to 0 to higher values, resulting in a near constant \u223c 50% reduction of the computation time regardless of the threshold.\nCFQ Using the MCD1 test split of the dataset, and our best-performing model on MCD1, we perform the \u03b1thresh adjustment. The halting patterns reflect the repeated structure of SQL, using fewer steps for \u2018.\u2018 and \u2018WHERE\u2018, while the main bulk of the region within {...} requires more SUT steps before halting. Surprisingly, when 0.8 \u2264 \u03b1thresh \u2264 0.999, the accuracy remains fairly constant. An estimated 33% of the computation steps were skipped at \u03b1thresh = 0.8. At \u03b1thresh = 0.1, there is a slight increase in the number of computed steps, which\nFigure 5: Above: Plot of 1\u2212\n\u2211l\u22121\nl\u2032=1 \u03b1 (t) l\u2032 , for an example\nLogical Inference input \u2014 x-axis: timesteps, y-axis: layers. This visualizes the halting pattern of the model: dark blue represents halted, while yellow represents active. Below: Efficiency vs. Performance tradeoff curves when \u03b1thresh is adjusted.\nis possible since halting earlier will result in different embeddings, and result in different halting decisions in other timesteps. Overall, the results suggest that we can save about 20% of the SUT computation steps without any drop in accuracy, and about \u223c50% for a 0.2% decrease.\nEnglish-German Translation For this larger dataset, we find that these translation models halt much later, suggesting that the translation task requires more computational steps than the 6-layer SUT we used. However, further increasing the number of layers to 12 layers does not bring about much benefit, as evidenced by the halting in Figure 4, which is an example of the halting mechanism using nearly all layers. For comparison, Admin 60L-12L model, requires a 60-layer encoder to achieve its performance. Even when \u03b1thresh = 1, the skipped computation steps remain at about 33%, compared to 80% in the CFQ task. We find that we can reduce the computation by 9% while still\nretaining a BLEU score of 29.1."
        },
        {
            "heading": "5 Conclusion",
            "text": "We show that it is possible to scale up the UT via SUTs, and SUTs outperforms models of the same capacity in the WMT\u201914 English-to-German translation task. The recursive nature of both UTs and SUTs allows for better inductive biases, which we have demonstrated in synthetic tasks like CFQ and logical inference. VTs have been shown to be poor at these compositional generalization tasks without additional domain knowledge. The stick-breaking dynamic halting mechanism also allows post-training adjustment of computation cost, which is a boon for deployment at scale.\nLimitations While the experiments in this paper show the desirable generalization properties of UTs, there are some aspects of compositional generalization that SUTs do not solve. Importantly, while we demonstrate scaling UTs up via SMoEs, further experiments on larger settings are needed\nto ascertain viability in large scale systems. Other issues may also crop up in the further scaling of SUTs, but we believe there is ample literature to draw on to finding solutions for these problems."
        }
    ],
    "title": "Sparse Universal Transformer",
    "year": 2023
}