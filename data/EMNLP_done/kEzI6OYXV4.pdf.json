{
    "abstractText": "Natural language expresses events with varying granularities, where coarse-grained events (goals) can be broken down into finer-grained event sequences (steps). A critical yet overlooked aspect of understanding event processes is recognizing that not all step events hold equal importance toward the completion of a goal. In this paper, we address this gap by examining the extent to which current models comprehend the essentiality of step events in relation to a goal event. Cognitive studies suggest that such capability enables machines to emulate human commonsense reasoning about preconditions and necessary efforts of everyday tasks. We contribute a high-quality corpus of (goal, step) pairs gathered from the community guideline website WikiHow, with steps manually annotated for their essentiality concerning the goal by experts. The high inter-annotator agreement demonstrates that humans possess a consistent understanding of event essentiality. However, after evaluating multiple statistical and largescale pre-trained language models, we find that existing approaches considerably underperform compared to humans. This observation highlights the need for further exploration into this critical and challenging task1.",
    "authors": [
        {
            "affiliations": [],
            "name": "Haoyu Wang"
        },
        {
            "affiliations": [],
            "name": "Hongming Zhang"
        },
        {
            "affiliations": [],
            "name": "Yueguan Wang"
        },
        {
            "affiliations": [],
            "name": "Yuqian Deng"
        },
        {
            "affiliations": [],
            "name": "Muhao Chen"
        },
        {
            "affiliations": [],
            "name": "Dan Roth"
        }
    ],
    "id": "SP:87e694bcaaca1ac4814022adcbd86905b63bf825",
    "references": [
        {
            "authors": [
                "Yonatan Bisk",
                "Rowan Zellers",
                "Ronan LeBras",
                "Jianfeng Gao",
                "Yejin Choi."
            ],
            "title": "PIQA: reasoning about physical commonsense in natural language",
            "venue": "Proceedings of AAAI 2020, pages 7432\u20137439.",
            "year": 2020
        },
        {
            "authors": [
                "Nathanael Chambers."
            ],
            "title": "Event schema induction with a probabilistic entity-driven model",
            "venue": "Proceedings of EMNLP 2013, pages 1797\u20131807.",
            "year": 2013
        },
        {
            "authors": [
                "Snigdha Chaturvedi",
                "Haoruo Peng",
                "Dan Roth."
            ],
            "title": "Story comprehension for predicting what happens next",
            "venue": "Proceedings of EMNLP 2017, pages 1603\u2013 1614. Association for Computational Linguistics.",
            "year": 2017
        },
        {
            "authors": [
                "Muhao Chen",
                "Hongming Zhang",
                "Qiang Ning",
                "Manling Li",
                "Heng Ji",
                "Kathleen McKeown",
                "Dan Roth."
            ],
            "title": "Event-centric natural language processing",
            "venue": "Proceedings of ACL 2021 Tutorial, pages 6\u201314.",
            "year": 2021
        },
        {
            "authors": [
                "Muhao Chen",
                "Hongming Zhang",
                "Haoyu Wang",
                "Dan Roth."
            ],
            "title": "What are you trying to do? semantic typing of event processes",
            "venue": "Proceedings of CoNLL 2020, pages 531\u2013542.",
            "year": 2020
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of NAACL-HLT 2019, pages 4171\u20134186.",
            "year": 2019
        },
        {
            "authors": [
                "Rotem Dror",
                "Haoyu Wang",
                "Dan Roth."
            ],
            "title": "Zeroshot on-the-fly event schema induction",
            "venue": "Findings of the Association for Computational Linguistics: EACL 2023, pages 705\u2013725, Dubrovnik, Croatia. Association for Computational Linguistics.",
            "year": 2023
        },
        {
            "authors": [
                "Yu Feng",
                "Ben Zhou",
                "Haoyu Wang",
                "Helen Jin",
                "Dan Roth."
            ],
            "title": "Generic temporal reasoning with differential analysis and explanation",
            "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),",
            "year": 2023
        },
        {
            "authors": [
                "Goran Glavas",
                "Jan Snajder",
                "Marie-Francine Moens",
                "Parisa Kordjamshidi."
            ],
            "title": "HiEve: A corpus for extracting event hierarchies from news stories",
            "venue": "Proceedings of LREC 2014, pages 3678\u20133683.",
            "year": 2014
        },
        {
            "authors": [
                "Andrew S. Gordon",
                "Zornitsa Kozareva",
                "Melissa Roemmele."
            ],
            "title": "Semeval-2012 task 7: Choice of plausible alternatives: An evaluation of commonsense causal reasoning",
            "venue": "Proceedings of SemEval@NAACL-HLT 2012, pages 394\u2013398.",
            "year": 2012
        },
        {
            "authors": [
                "Ralph Grishman",
                "David Westbrook",
                "Adam Meyers."
            ],
            "title": "Nyu\u2019s english ace 2005 system description",
            "venue": "ACE, 5.",
            "year": 2005
        },
        {
            "authors": [
                "James A Hanley",
                "Barbara J McNeil."
            ],
            "title": "The meaning and use of the area under a receiver operating characteristic (roc) curve",
            "venue": "Radiology, 143(1):29\u2013",
            "year": 1982
        },
        {
            "authors": [
                "Daniel Khashabi",
                "Sewon Min",
                "Tushar Khot",
                "Ashish Sabharwal",
                "Oyvind Tafjord",
                "Peter Clark",
                "Hannaneh Hajishirzi."
            ],
            "title": "UnifiedQA: Crossing format boundaries with a single QA system",
            "venue": "Findings of EMNLP 2020, pages 1896\u20131907.",
            "year": 2020
        },
        {
            "authors": [
                "Mahnaz Koupaee",
                "William Yang Wang."
            ],
            "title": "Wikihow: A large scale text summarization dataset",
            "venue": "CoRR, abs/1810.09305.",
            "year": 2018
        },
        {
            "authors": [
                "Manling Li",
                "Tengfei Ma",
                "Mo Yu",
                "Lingfei Wu",
                "Tian Gao",
                "Heng Ji",
                "Kathleen McKeown."
            ],
            "title": "Timeline summarization based on event graph compression via time-aware optimal transport",
            "venue": "Proceedings of EMNLP 2021, pages 6443\u20136456.",
            "year": 2021
        },
        {
            "authors": [
                "Ying Lin",
                "Heng Ji",
                "Fei Huang",
                "Lingfei Wu."
            ],
            "title": "A joint neural model for information extraction with global features",
            "venue": "Proceedings of ACL 2020, pages 7999\u20138009.",
            "year": 2020
        },
        {
            "authors": [
                "Qing Lyu",
                "Hongming Zhang",
                "Elior Sulem",
                "Dan Roth."
            ],
            "title": "Zero-shot event extraction via transfer learning: Challenges and insights",
            "venue": "Proceedings of ACL/IJCNLP 2021, pages 322\u2013332. Association for Computational Linguistics.",
            "year": 2021
        },
        {
            "authors": [
                "Andrea Madotto",
                "Samuel Cahyawijaya",
                "Genta Indra Winata",
                "Yan Xu",
                "Zihan Liu",
                "Zhaojiang Lin",
                "Pascale Fung."
            ],
            "title": "Learning knowledge bases with parameters for task-oriented dialogue systems",
            "venue": "Findings of EMNLP 2020.",
            "year": 2020
        },
        {
            "authors": [
                "Sarang Narkhede."
            ],
            "title": "Understanding auc-roc curve",
            "venue": "Towards Data Science, 26(1):220\u2013227.",
            "year": 2018
        },
        {
            "authors": [
                "Qiang Ning",
                "Zhili Feng",
                "Hao Wu",
                "Dan Roth."
            ],
            "title": "Joint reasoning for temporal and causal relations",
            "venue": "Proceedings of ACL 2018, pages 2278\u20132288.",
            "year": 2018
        },
        {
            "authors": [
                "Qiang Ning",
                "Hao Wu",
                "Dan Roth."
            ],
            "title": "A multiaxis annotation scheme for event temporal relations",
            "venue": "Proceedings of ACL 2018, pages 1318\u20131328.",
            "year": 2018
        },
        {
            "authors": [
                "Alec Radford",
                "Jeffrey Wu",
                "Rewon Child",
                "David Luan",
                "Dario Amodei",
                "Ilya Sutskever"
            ],
            "title": "Language models are unsupervised multitask learners",
            "venue": "OpenAI blog,",
            "year": 2019
        },
        {
            "authors": [
                "Evan Sandhaus."
            ],
            "title": "The new york times annotated corpus",
            "venue": "Linguistic Data Consortium, Philadelphia, 6(12):e26752.",
            "year": 2008
        },
        {
            "authors": [
                "Roger C Schank",
                "Robert P Abelson"
            ],
            "title": "Scripts, plans, goals and understanding: An inquiry into human knowledge structures",
            "year": 1977
        },
        {
            "authors": [
                "Niket Tandon",
                "Gerard de Melo",
                "Abir De",
                "Gerhard Weikum."
            ],
            "title": "Knowlywood: Mining activity knowledge from hollywood narratives",
            "venue": "Proceedings of CIKM 2015, pages 223\u2013232.",
            "year": 2015
        },
        {
            "authors": [
                "Haoyu Wang",
                "Muhao Chen",
                "Hongming Zhang",
                "Dan Roth."
            ],
            "title": "Joint constrained learning for eventevent relation extraction",
            "venue": "Proceedings of EMNLP 2020, pages 696\u2013706.",
            "year": 2020
        },
        {
            "authors": [
                "Haoyu Wang",
                "Hongming Zhang",
                "Muhao Chen",
                "Dan Roth."
            ],
            "title": "Learning constraints and descriptive segmentation for subevent detection",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 5216\u20135226, Online",
            "year": 2021
        },
        {
            "authors": [
                "Haoyu Wang",
                "Hongming Zhang",
                "Yuqian Deng",
                "Jacob Gardner",
                "Dan Roth",
                "Muhao Chen."
            ],
            "title": "Extracting or guessing? improving faithfulness of event temporal relation extraction",
            "venue": "Proceedings of the 17th Conference of the European Chapter of the Associa-",
            "year": 2023
        },
        {
            "authors": [
                "Xiaozhi Wang",
                "Ziqi Wang",
                "Xu Han",
                "Wangyi Jiang",
                "Rong Han",
                "Zhiyuan Liu",
                "Juanzi Li",
                "Peng Li",
                "Yankai Lin",
                "Jie Zhou."
            ],
            "title": "MAVEN: A massive general domain event detection dataset",
            "venue": "Proceedings of EMNLP 2020, pages 1652\u20131671.",
            "year": 2020
        },
        {
            "authors": [
                "Adina Williams",
                "Nikita Nangia",
                "Samuel R. Bowman."
            ],
            "title": "A broad-coverage challenge corpus for sentence understanding through inference",
            "venue": "Proceedings of NAACL-HLT 2018, pages 1112\u20131122.",
            "year": 2018
        },
        {
            "authors": [
                "Jeffrey M Zacks",
                "Barbara Tversky."
            ],
            "title": "Event structure in perception and conception",
            "venue": "Psychological bulletin, 127(1):3.",
            "year": 2001
        },
        {
            "authors": [
                "Hongming Zhang",
                "Muhao Chen",
                "Haoyu Wang",
                "Yangqiu Song",
                "Dan Roth."
            ],
            "title": "Analogous process structure induction for sub-event sequence prediction",
            "venue": "Proceedings of EMNLP 2020, pages 1541\u20131550.",
            "year": 2020
        },
        {
            "authors": [
                "Hongming Zhang",
                "Xin Liu",
                "Haojie Pan",
                "Haowen Ke",
                "Jiefu Ou",
                "Tianqing Fang",
                "Yangqiu Song."
            ],
            "title": "ASER: towards large-scale commonsense knowledge acquisition via higher-order selectional preference over eventualities",
            "venue": "CoRR, abs/2104.02137.",
            "year": 2021
        },
        {
            "authors": [
                "Hongming Zhang",
                "Haoyu Wang",
                "Dan Roth."
            ],
            "title": "Zero-shot label-aware event trigger and argument classification",
            "venue": "Proceedings of ACL 2021, Findings, volume ACL/IJCNLP 2021 of Findings of ACL, pages 1331\u20131340. Association for Computational",
            "year": 2021
        },
        {
            "authors": [
                "Jiayao Zhang",
                "Hongming Zhang",
                "Dan Roth",
                "Weijie J. Su."
            ],
            "title": "Causal inference principles for reasoning about commonsense causality",
            "venue": "CoRR, abs/2202.00436.",
            "year": 2022
        },
        {
            "authors": [
                "Li Zhang",
                "Qing Lyu",
                "Chris Callison-Burch."
            ],
            "title": "Intent detection with WikiHow",
            "venue": "Proceedings of AACL 2020, pages 328\u2013333.",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "As a fundamental semantic primitive unit in human language (Jackendoff, 1992), events play a pivotal role in facilitating efficient communication among humans and safe interactions with the world. Recently, the natural language processing (NLP) community has made significant strides in helping machines comprehend events through various directions, such as event extraction (Grishman et al., 2005; Lin et al., 2020), event relation extraction (Ning et al., 2018a; Wang et al., 2020a), event schema induction (Chambers, 2013; Dror\n1The dataset and code are available at http://cogcomp.org/ page/publication_view/1023.\net al., 2023), and event-centric knowledge graph construction (Tandon et al., 2015; Zhang et al., 2021a). However, most of these studies primarily concentrate on modeling horizontal relationships between events, neglecting the internal components of an event (i.e., how an individual perceives an event mention).\nComputational and cognitive studies (Schank and Abelson, 1977; Zacks and Tversky, 2001) indicate that humans can deconstruct a goal event into a discrete representation of finer-grained step events, ultimately facilitating the hierarchical organization of event-related knowledge. As illustrated in Figure 1, when discussing the goal event of \u201cobtaining a Ph.D. degree\u201d, we understand that several steps may occur along the way. For instance, one might receive the offer, pass the qualification exam, complete internships, publish papers, and defend the dissertation. Among these steps, some are deemed essential to the goal, while others are not. For instance, passing the qualification exam is crucial for earning a Ph.D. degree, whereas securing an internship is often not a requirement. This ability to discern the essentiality of steps pertaining to various goals equips humans with the commonsense needed to address problems and carry out daily tasks. Similarly, understanding which steps are essential can profoundly benefit numerous NLP applications. For instance, event\nschema induction (Dror et al., 2023) relies on eventcentric information extraction to derive graphical representations of events from text. In this context, understanding essentiality can enhance the quality of induced schemas by eliminating hallucinations and suggesting the addition of missing crucial events. Moreover, grasping essentiality can potentially benefit intelligent systems for QA tasks (Bisk et al., 2020) and task-oriented dialogue processing (Madotto et al., 2020).\nIn this paper, we aim to assess the depth of understanding that current NLU models possess regarding events in comparison to human cognition. To accomplish this, we introduce a new cognitively inspired problem of detecting essential step events in goal event processes and establish a novel benchmark, Essential Step Detection (ESD), to promote research in this area. Specifically, we gather goals and their corresponding steps from WikiHow2 and manually annotate the essentiality of various steps in relation to the goal. Our experimental findings reveal that although humans consistently perceive event essentiality, current models still have a long way to go to match this level of understanding."
        },
        {
            "heading": "2 Task and Data",
            "text": "The essential step detection task is defined as follows: for each goal G and one of its sub-steps S, the objective is to predict whether the failure of S will result in the failure of G. In our formulation, G and S are presented as natural language sentences. The construction of ESD includes two steps: (1) Data Preparation and (2) Essentiality Annotation. Details of these steps are provided below."
        },
        {
            "heading": "2.1 Data Preparation",
            "text": "WikiHow is a widely-used and well-structured resource for exploring the relationship between goal-oriented processes and their corresponding steps (Koupaee and Wang, 2018; Zhang et al., 2020b). To the best of our knowledge, it is the most appropriate resource for the purpose of our research. Consequently, we begin by collecting 1,000 random goal-oriented processes from WikiHow. To avoid oversimplified and overly complex processes, we only retain those with three to ten steps. Furthermore, given that all WikiHow processes and their associated steps are carefully crafted by humans, the majority of the steps mentioned are essential.\n2WikiHow is a community website featuring extensive collections of step-by-step guidelines.\nTo achieve balance in the dataset, we enlist crowdsourcing workers to contribute optional steps (i.e., those that could occur as part of the process but are not essential)3. We employ three annotators from Amazon Mechanical Turk4, who are native English speakers, to provide optional steps for each goal. To ensure high-quality annotations, we require annotators to hold the \u201cMaster annotator\u201d title. The average cost and time for supplying annotations are 0.1 USD and 32 seconds per instance (approximately 12 USD per hour)."
        },
        {
            "heading": "2.2 Essentiality Annotation",
            "text": "Given that our task necessitates a profound understanding of the events and careful consideration, we ensure annotation quality by employing three well-trained research assistants from our department rather than ordinary annotators to conduct the essentiality annotations. For each goal-step pair, annotators are asked to rate it as 0 (non-essential), 1 (essential), or -1 (the step is not a valid step for the target goal, or the goal/step contains confidential or hostile information)5. Since all annotators are welltrained and fully comprehend our task, we discard any pair that is deemed invalid (i.e., -1) by at least one annotator. This results in 1,515 pairs being retained. We determine the final label based on majority voting. The dataset statistics can be found in Table 1. Altogether, we compile 1,118 essential and 397 non-essential \"goal-step\" pairs. The interannotator agreement, measured by Fleiss\u2019s Kappa6, is 0.611, signifying the high quality of ESD."
        },
        {
            "heading": "3 Experiments",
            "text": "Recently, large-scale pre-trained language models have exhibited impressive language understanding capabilities. To assess the extent to which these models truly understand events, we evaluate them using ESD. Specifically, we benchmark their performance by examining a range of inference meth-\n3The survey template is shown in Appendix Figure 2. 4https://www.mturk.com/ 5The survey template is shown in Appendix Figure 3. 6We utilize tools from https://github.com/Shamya/\nFleissKappa.\nods as detailed below.\n1. Next Sentence Prediction: To ensure that the essentiality detection task aligns with the training objectives of pre-trained masked Language Models (LMs) (Devlin et al., 2019), we verbalize each pair of goal G and step S into two sentences: \u201cTo G\u201d, and \u201cwe must S\u201d. Then we leverage the LM to predict the probability of \u201cwe must S\u201d to be the next sentence of \u201cTo G\u201d.\n2. Perplexity: We also attempt to verbalize a goalstep pair into sentences and employ the perplexity predicted by the Language Model (i.e., GPT2 (Radford et al., 2019)) as an indicator for the predicted perplexity.\n3. Intent Detection: We assess the performance of an Intent Detection model (Zhang et al., 2020b), which is designed to predict the correct intent given an utterance. By setting S as the provided utterance, we employ the model to predict its corresponding goal G.\n4. Textual Entailment: Another alternative is to leverage the logical inference capabilities of a textual entailment model (Williams et al., 2018). In our experiment, we treat G as the premise and S as the hypothesis. If a model understands the essentiality of completing S in order to achieve G, it should be able to infer S from G.\n5. Unified QA: We also experiment with a SOTA QA model. Specifically, We follow the setting in Unified QA (Khashabi et al., 2020) to convert each goal-step pair into a \u201cYes/No\u201d question and then use the predicted probability of \u201cYes\u201d as the indicator for the essentiality.\n6. Prompt with GPT-3 & GPT-4: We test the prompt-based methods as well, which have proven to be powerful for many NLU tasks. Specifically, we manually design templates to convert each goal-step pair into prompts and then ask GPT-3 (Brown et al., 2020) & GPT4 (OpenAI, 2023) to generate True or False labels based on our input.\n7. Corpus Statistics: Last, we present the performance of a corpus statistics model to determine whether such knowledge is explicitly expressed in free text. For each goal-step pair, we first extract the central verbs from the goal and step using dependency parsing tools as their representatives. Subsequently, we employ their normalized co-occurrence frequencies in the New\nYork Times Corpus (NYT) (Sandhaus, 2008) to indicate the relationship between them.\nExamples of all utilized templates and prompts can be found in Appendix Table 4. Given that we have formulated the task as a binary choice problem, which differs from the output of the Perplexity and TE models, we evaluate all models based on the AUROC score (Hanley and McNeil, 1982; Narkhede, 2018) to enable a fair comparison. For Perplexity, we use the perplexity score as the predicted essentiality (lower scores are better). For TE and Intent Detection models, we use the predicted probability of \u201cEntailment\u201d and the likelihood of being entailed as the predicted essentiality. The statistics-based model uses normalized frequency as the essentiality indication signal. For all other baselines, we use the predicted probability of \u201cYes\u201d as the predicted essentiality (higher scores are better). All experiments are conducted using the largest available models and the default hyperparameters."
        },
        {
            "heading": "3.1 Result Analysis",
            "text": "In WikiHow, each goal (e.g., \u201cToast Sunflower Seeds\u201d) is typically associated with a modifier (e.g., \u201cMicrowave Toasting\u201d) to provide a more precise definition. In our experiment, we evaluate whether such a modifier impacts the models\u2019 comprehension of the goal by employing two settings: (1) Full: we concatenate the goal and the modifier as the goal input; (2) Core: we only use the original goal as the goal input.\nWe also report human performance as an upper bound for this task. Specifically, we randomly select 200 instances, ask three ordinary annotators to label them, and report the average performance. As annotators provide binary annotations for the essentiality of a goal-step pair instead of a real value\nlike other models, the AUROC score is equivalent to accuracy. The performances of all models under both settings are presented in Table 2, from which we draw the following observations.\nThe corpus statistics method performs poorly. This could be attributed to two possible reasons: (1) The triggers might not adequately represent the semantics of events, leading to the co-occurrence information between the two triggers being insufficient for predicting the relationship between them; (2) Considering that essentiality knowledge is a form of implicit commonsense knowledge that people seldom discuss, it is difficult to directly identify references to such knowledge within raw corpora.\nIndirect supervision from other NLU tasks proves beneficial. The experiments involving the TE and QA models demonstrate that fine-tuning with these tasks enhances the language models\u2019 ability to better comprehend event essentiality.\nCurrent NLP models, including the massive GPT models, still fall drastically behind human on ESD. This implies that the pre-training and fine-tuning over a limited-size dataset might not be enough to uncover the implicit knowledge we need to understand events, which further proves the value of proposing this new and challenging task.\nAlmost all models7 exhibit better performance when the modifier is provided, which aligns with human performance. This observation suggests that when the description of the goal event is clearer and less ambiguous, the model, similar to humans, can indeed comprehend events more effectively."
        },
        {
            "heading": "3.2 Impact of the Model Size and Discussion",
            "text": "To investigate the impact of pre-trained LMs\u2019 model sizes on their abilities to understand event\n7The only exception is UnifiedQA.\nessentiality, we present the performance of various LM variants in Table 3. The results demonstrate that the model size plays a critical role in the success of these language models. Particularly for GPT-3, reducing the number of parameters to 6.7 billion results in the prompt-based method becoming ineffective. Meanwhile, it raises concerns about the diminishing gain when we further increase the model size. Given that current LMs are already extremely large and expensive to train, it may not be feasible to fully understand events by solely increasing model sizes and corpora. We hope that ESD can promote further research on knowledge acquisition and reasoning, fostering a deeper understanding of events."
        },
        {
            "heading": "4 Related Works",
            "text": "The NLP community has increasingly focused on event understanding (Chen et al., 2021), with research divided into event-centric information extraction (IE) (Grishman et al., 2005; Lin et al., 2020; Wang et al., 2020b; Lyu et al., 2021; Zhang et al., 2021b; Feng et al., 2023) and structural event prediction (Zhang et al., 2021a). Event-centric IE includes recognizing, typing events and inferring their relations (Ning et al., 2018b; Glavas et al., 2014; Wang et al., 2023), while structural event prediction involves context-independent inferences about event structures, causality (Gordon et al., 2012; Sap et al., 2019; Li et al., 2021; Zhang et al., 2022), discourse (Chaturvedi et al., 2017), summaries (Li et al., 2021), and memberships (Zhang et al., 2020a; Chen et al., 2020; Zhang et al., 2020b; Wang et al., 2021).\nThis work pertains to the second research direction, focusing on the internal structure of events rather than relations between them. Unlike previous event membership studies (Zhang et al., 2020a; Chen et al., 2020), this work predicts the essentiality of decomposed subevents, assessing the understanding of internal steps by SOTA LLMs."
        },
        {
            "heading": "5 Conclusion",
            "text": "We introduce ESD, an event understanding task assessing state-of-the-art NLP models\u2019 comprehension of events by identifying essential ones for goal achievement. Experiments show that complex event knowledge is rarely expressed in text, and current large-scale language models struggle with complex event understanding. We will release all data and code to encourage research on complex\nevent knowledge collection and improved reasoning for deeper event understanding."
        },
        {
            "heading": "Acknowledgement",
            "text": "We appreciate the reviewers for their insightful comments and suggestions.\nYueguan Wang was supported by the USC Viterbi-THU Summer Research Program. Muhao Chen is supported by the NSF Grant IIS 2105329, the NSF Grant ITE 2333736, the DARPA MCS program under Contract No. N660011924033 with the United States Office Of Naval Research, a Cisco Research Award, two Amazon Research Awards, and a Keston Research Award.\nThis research is based upon work supported in part by the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), via IARPA Contract No. 2022-22072200003 under the HIATUS Program and IARPA Contract No. 2019-19051600006 under the BETTER Program. This work was also supported by Contract FA8750-19-2-1004 with the US Defense Advanced Research Projects Agency (DARPA). Approved for Public Release, Distribution Unlimited. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of ODNI, IARPA, DARPA, or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copyright annotation therein.\nEthical Statement\nTo the best of our knowledge, this work has no ethical concerns. All the collected data are anonymized, and all annotators are paid higher than the minimum payment requirement.\nLimitations\nA potential limitation of this work is the data scale, which is not enough for training a decent model. However, as we mainly use the dataset as a test set, the current scale is enough for this purpose."
        }
    ],
    "title": "Are All Steps Equally Important? Benchmarking Essentiality Detection in Event Processes",
    "year": 2023
}