{
    "abstractText": "Linguistic communication is prevalent in Human-Computer Interaction (HCI). Speech (spoken language) serves as a convenient yet potentially ambiguous form due to noise and accents, exposing a gap compared to text. In this study, we investigate the prominent HCI task, Referring Video Object Segmentation (RVOS), which aims to segment and track objects using linguistic references. While text input is well-investigated, speech input is underexplored. Our objective is to bridge the gap between speech and text, enabling the adaptation of existing text-input R-VOS models to accommodate noisy speech input effectively. Specifically, we propose a method to align the semantic spaces between speech and text by incorporating two key modules: 1) Noise-Aware Semantic Adjustment (NSA) for clear semantics extraction from noisy speech; and 2) Semantic Jitter Suppression (SJS) enabling R-VOS models to tolerate noisy queries. Comprehensive experiments conducted on the challenging AVOS benchmarks reveal that our proposed method outperforms state-of-the-art approaches.",
    "authors": [
        {
            "affiliations": [],
            "name": "Xiang Li"
        },
        {
            "affiliations": [],
            "name": "Jinglu Wang"
        },
        {
            "affiliations": [],
            "name": "Xiaohao Xu"
        },
        {
            "affiliations": [],
            "name": "Muqiao Yang"
        },
        {
            "affiliations": [],
            "name": "Fan Yang"
        },
        {
            "affiliations": [],
            "name": "Yizhou Zhao"
        },
        {
            "affiliations": [],
            "name": "Rita Singh"
        },
        {
            "affiliations": [],
            "name": "Bhiksha Raj"
        },
        {
            "affiliations": [],
            "name": "Ann Arbor"
        },
        {
            "affiliations": [],
            "name": "Mohamed bin Zayed"
        }
    ],
    "id": "SP:f866a05bc10f7d3fd952c292f6c62e17781f8d05",
    "references": [
        {
            "authors": [
                "Alexei Baevski",
                "Yuhao Zhou",
                "Abdelrahman Mohamed",
                "Michael Auli"
            ],
            "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations. Advances in neural information processing systems, 33:12449\u201312460",
            "year": 2020
        },
        {
            "authors": [
                "Daniel Bolya",
                "Chong Zhou",
                "Fanyi Xiao",
                "Yong Jae Lee."
            ],
            "title": "Yolact: Real-time instance segmentation",
            "venue": "Proceedings of the IEEE/CVF international conference on computer vision, pages 9157\u20139166.",
            "year": 2019
        },
        {
            "authors": [
                "Adam Botach",
                "Evgenii Zheltonozhskii",
                "Chaim Baskin."
            ],
            "title": "End-to-end referring video object segmentation with multimodal transformers",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4985\u20134995.",
            "year": 2022
        },
        {
            "authors": [
                "Jiale Cao",
                "Rao Muhammad Anwer",
                "Hisham Cholakkal",
                "Fahad Shahbaz Khan",
                "Yanwei Pang",
                "Ling Shao."
            ],
            "title": "Sipmask: Spatial information preservation for fast image and video instance segmentation",
            "venue": "Computer Vision\u2013ECCV 2020: 16th European Con-",
            "year": 2020
        },
        {
            "authors": [
                "Ho Kei Cheng",
                "Yu-Wing Tai",
                "Chi-Keung Tang."
            ],
            "title": "Modular interactive video object segmentation: Interaction-to-mask, propagation and difference-aware fusion",
            "venue": "arXiv preprint arXiv:2103.07941.",
            "year": 2021
        },
        {
            "authors": [
                "Ho Kei Cheng",
                "Yu-Wing Tai",
                "Chi-Keung Tang."
            ],
            "title": "Rethinking space-time networks with improved memory coverage for efficient video object segmentation",
            "venue": "arXiv preprint arXiv:2106.05210.",
            "year": 2021
        },
        {
            "authors": [
                "Grzegorz Chrupa\u0142a."
            ],
            "title": "Visually grounded models of spoken language: A survey of datasets, architectures and evaluation techniques",
            "venue": "Journal of Artificial Intelligence Research, 73:673\u2013707.",
            "year": 2022
        },
        {
            "authors": [
                "Zihan Ding",
                "Tianrui Hui",
                "Shaofei Huang",
                "Si Liu",
                "Xuan Luo",
                "Junshi Huang",
                "Xiaoming Wei."
            ],
            "title": "Progressive multimodal interaction network for referring video object segmentation",
            "venue": "The 3rd Large-scale Video Object Segmentation Challenge, page 7.",
            "year": 2021
        },
        {
            "authors": [
                "Ruohan Gao",
                "Kristen Grauman."
            ],
            "title": "Visualvoice: Audio-visual speech separation with cross-modal consistency",
            "venue": "2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 15490\u201315500. IEEE.",
            "year": 2021
        },
        {
            "authors": [
                "Jort F Gemmeke",
                "Daniel PW Ellis",
                "Dylan Freedman",
                "Aren Jansen",
                "Wade Lawrence",
                "R Channing Moore",
                "Manoj Plakal",
                "Marvin Ritter."
            ],
            "title": "Audio set: An ontology and human-labeled dataset for audio events",
            "venue": "2017 IEEE international conference on",
            "year": 2017
        },
        {
            "authors": [
                "David Harwath",
                "James Glass"
            ],
            "title": "Deep multimodal semantic embeddings for speech and images",
            "year": 2015
        },
        {
            "authors": [
                "David Harwath",
                "Wei-Ning Hsu",
                "James Glass."
            ],
            "title": "Learning hierarchical discrete linguistic units from visually-grounded speech",
            "venue": "International Conference on Learning Representations.",
            "year": 2020
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun."
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013 778.",
            "year": 2016
        },
        {
            "authors": [
                "Li Hu",
                "Peng Zhang",
                "Bang Zhang",
                "Pan Pan",
                "Yinghui Xu",
                "Rong Jin."
            ],
            "title": "Learning position and target consistency for memory-based video object segmentation",
            "venue": "arXiv preprint arXiv:2104.04329.",
            "year": 2021
        },
        {
            "authors": [
                "Shijia Huang",
                "Yilun Chen",
                "Jiaya Jia",
                "Liwei Wang."
            ],
            "title": "Multi-view transformer for 3d visual grounding",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15524\u201315533.",
            "year": 2022
        },
        {
            "authors": [
                "Geumbyeol Hwang",
                "Sunwon Hong",
                "Seunghyun Lee",
                "Sungwoo Park",
                "Gyeongsu Chae."
            ],
            "title": "Discohead: Audio-and-video-driven talking head generation by disentangled control of head pose and facial expressions",
            "venue": "ICASSP 2023-2023 IEEE Interna-",
            "year": 2023
        },
        {
            "authors": [
                "Hueihan Jhuang",
                "Juergen Gall",
                "Silvia Zuffi",
                "Cordelia Schmid",
                "Michael J Black."
            ],
            "title": "Towards understanding action recognition",
            "venue": "Proceedings of the IEEE international conference on computer vision, pages 3192\u20133199.",
            "year": 2013
        },
        {
            "authors": [
                "Yuming Jiang",
                "Ziqi Huang",
                "Xingang Pan",
                "Chen Change Loy",
                "Ziwei Liu."
            ],
            "title": "Talk-to-edit: Fine-grained facial editing via dialog",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 13799\u201313808.",
            "year": 2021
        },
        {
            "authors": [
                "Aishwarya Kamath",
                "Mannat Singh",
                "Yann LeCun",
                "Gabriel Synnaeve",
                "Ishan Misra",
                "Nicolas Carion."
            ],
            "title": "Mdetr-modulated detection for end-to-end multi-modal understanding",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer",
            "year": 2021
        },
        {
            "authors": [
                "Takatomo Kano",
                "Sakriani Sakti",
                "Satoshi Nakamura."
            ],
            "title": "Transformer-based direct speech-to-speech translation with transcoder",
            "venue": "2021 IEEE Spoken Language Technology Workshop (SLT), pages 958\u2013 965. IEEE.",
            "year": 2021
        },
        {
            "authors": [
                "Sahar Kazemzadeh",
                "Vicente Ordonez",
                "Mark Matten",
                "Tamara Berg."
            ],
            "title": "Referitgame: Referring to objects in photographs of natural scenes",
            "venue": "Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 787\u2013",
            "year": 2014
        },
        {
            "authors": [
                "Harold W Kuhn."
            ],
            "title": "The hungarian method for the assignment problem",
            "venue": "Naval research logistics quarterly, 2(1-2):83\u201397.",
            "year": 1955
        },
        {
            "authors": [
                "Yi Lei",
                "Shan Yang",
                "Lei Xie."
            ],
            "title": "Fine-grained emotion strength transfer, control and prediction for emotional speech synthesis",
            "venue": "2021 IEEE Spoken Language Technology Workshop (SLT), pages 423\u2013 430. IEEE.",
            "year": 2021
        },
        {
            "authors": [
                "Jinyu Li"
            ],
            "title": "Recent advances in end-to-end automatic speech recognition",
            "venue": "APSIPA Transactions on Signal and Information Processing, 11(1).",
            "year": 2022
        },
        {
            "authors": [
                "Xiang Li",
                "Haoyuan Cao",
                "Shijie Zhao",
                "Junlin Li",
                "Li Zhang",
                "Bhiksha Raj."
            ],
            "title": "Panoramic video salient object detection with ambisonic audio guidance",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 1424\u20131432.",
            "year": 2023
        },
        {
            "authors": [
                "Xiang Li",
                "Jinglu Wang",
                "Xiao Li",
                "Yan Lu."
            ],
            "title": "Hybrid instance-aware temporal fusion for online video instance segmentation",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 1429\u20131437.",
            "year": 2022
        },
        {
            "authors": [
                "Xiang Li",
                "Jinglu Wang",
                "Xiao Li",
                "Yan Lu."
            ],
            "title": "Video instance segmentation by instance flow assembly",
            "venue": "IEEE Transactions on Multimedia.",
            "year": 2022
        },
        {
            "authors": [
                "Xiang Li",
                "Jinglu Wang",
                "Xiaohao Xu",
                "Xiao Li",
                "Bhiksha Raj",
                "Yan Lu."
            ],
            "title": "Robust referring video object segmentation with cyclic structural consensus",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 22236\u201322245.",
            "year": 2023
        },
        {
            "authors": [
                "Xiang Li",
                "Jinglu Wang",
                "Xiaohao Xu",
                "Xiulian Peng",
                "Rita Singh",
                "Yan Lu",
                "Bhiksha Raj."
            ],
            "title": "Rethinking audiovisual segmentation with semantic quantization and decomposition",
            "venue": "arXiv preprint arXiv:2310.00132.",
            "year": 2023
        },
        {
            "authors": [
                "Xiaoya Li",
                "Xiaofei Sun",
                "Yuxian Meng",
                "Junjun Liang",
                "Fei Wu",
                "Jiwei Li."
            ],
            "title": "Dice loss for data-imbalanced nlp tasks",
            "venue": "arXiv preprint arXiv:1911.02855.",
            "year": 2019
        },
        {
            "authors": [
                "Zhen Li",
                "Cheng-Ze Lu",
                "Jianhua Qin",
                "Chun-Le Guo",
                "Ming-Ming Cheng."
            ],
            "title": "Towards an end-to-end framework for flow-guided video inpainting",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 17562\u201317571.",
            "year": 2022
        },
        {
            "authors": [
                "Yongqing Liang",
                "Xin Li",
                "Navid Jafari",
                "Jim Chen."
            ],
            "title": "Video object segmentation with adaptive feature bank and uncertain-region refinement",
            "venue": "Advances in Neural Information Processing Systems, 33.",
            "year": 2020
        },
        {
            "authors": [
                "Tsung-Yi Lin",
                "Piotr Doll\u00e1r",
                "Ross Girshick",
                "Kaiming He",
                "Bharath Hariharan",
                "Serge Belongie."
            ],
            "title": "Feature pyramid networks for object detection",
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2117\u20132125.",
            "year": 2017
        },
        {
            "authors": [
                "Tsung-Yi Lin",
                "Priya Goyal",
                "Ross Girshick",
                "Kaiming He",
                "Piotr Doll\u00e1r."
            ],
            "title": "Focal loss for dense object detection",
            "venue": "Proceedings of the IEEE international conference on computer vision, pages 2980\u20132988.",
            "year": 2017
        },
        {
            "authors": [
                "Yan-Bo Lin",
                "Yi-Lin Sung",
                "Jie Lei",
                "Mohit Bansal",
                "Gedas Bertasius."
            ],
            "title": "Vision transformers are parameter-efficient audio-visual learners",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2299\u20132309.",
            "year": 2023
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter."
            ],
            "title": "Decoupled weight decay regularization",
            "venue": "arXiv preprint arXiv:1711.05101.",
            "year": 2017
        },
        {
            "authors": [
                "Zhuoyan Luo",
                "Yicheng Xiao",
                "Yong Liu",
                "Shuyan Li",
                "Yitong Wang",
                "Yansong Tang",
                "Xiu Li",
                "Yujiu Yang."
            ],
            "title": "Soc: Semantic-assisted object cluster for referring video object segmentation",
            "venue": "arXiv preprint arXiv:2305.17011.",
            "year": 2023
        },
        {
            "authors": [
                "Seoung Wug Oh",
                "Joon-Young Lee",
                "Ning Xu",
                "Seon Joo Kim."
            ],
            "title": "Video object segmentation using space-time memory networks",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 9226\u20139235.",
            "year": 2019
        },
        {
            "authors": [
                "Wenwen Pan",
                "Haonan Shi",
                "Zhou Zhao",
                "Jieming Zhu",
                "Xiuqiang He",
                "Zhigeng Pan",
                "Lianli Gao",
                "Jun Yu",
                "Fei Wu",
                "Qi Tian."
            ],
            "title": "Wnet: Audio-guided video object segmentation via wavelet-based cross-modal denoising networks",
            "venue": "Proceedings of the IEEE/CVF Con-",
            "year": 2022
        },
        {
            "authors": [
                "Jordi Pont-Tuset",
                "Federico Perazzi",
                "Sergi Caelles",
                "Pablo Arbel\u00e1ez",
                "Alex Sorkine-Hornung",
                "Luc Van Gool."
            ],
            "title": "The 2017 davis challenge on video object segmentation",
            "venue": "arXiv preprint arXiv:1704.00675.",
            "year": 2017
        },
        {
            "authors": [
                "Liao Qu",
                "Xianwei Zou",
                "Xiang Li",
                "Yandong Wen",
                "Rita Singh",
                "Bhiksha Raj."
            ],
            "title": "The hidden dance of phonemes and visage: Unveiling the enigmatic link between phonemes and facial features",
            "venue": "arXiv preprint arXiv:2307.13953.",
            "year": 2023
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark"
            ],
            "title": "Learning transferable visual models from natural language supervision",
            "year": 2021
        },
        {
            "authors": [
                "Hamid Rezatofighi",
                "Nathan Tsoi",
                "JunYoung Gwak",
                "Amir Sadeghian",
                "Ian Reid",
                "Silvio Savarese."
            ],
            "title": "Generalized intersection over union: A metric and a loss for bounding box regression",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and",
            "year": 2019
        },
        {
            "authors": [
                "Paul Hongsuck Seo",
                "Arsha Nagrani",
                "Cordelia Schmid."
            ],
            "title": "Avformer: Injecting vision into frozen speech models for zero-shot av-asr",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22922\u201322931.",
            "year": 2023
        },
        {
            "authors": [
                "Seonguk Seo",
                "Joon-Young Lee",
                "Bohyung Han."
            ],
            "title": "Urvos: Unified referring video object segmentation network with a large-scale benchmark",
            "venue": "Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceed-",
            "year": 2020
        },
        {
            "authors": [
                "Hongje Seong",
                "Junhyuk Hyun",
                "Euntai Kim."
            ],
            "title": "Kernelized memory network for video object segmentation",
            "venue": "European Conference on Computer Vision, pages 629\u2013645. Springer.",
            "year": 2020
        },
        {
            "authors": [
                "Hongje Seong",
                "Seoung Wug Oh",
                "Joon-Young Lee",
                "Seongwon Lee",
                "Suhyeon Lee",
                "Euntai Kim"
            ],
            "title": "Hierarchical memory matching network for video object segmentation",
            "year": 2021
        },
        {
            "authors": [
                "Yi Xuan Tan",
                "Navonil Majumder",
                "Soujanya Poria."
            ],
            "title": "Sentence embedder guided utterance encoder (segue) for spoken language understanding",
            "venue": "Proc. Interspeech 2023.",
            "year": 2023
        },
        {
            "authors": [
                "Zhi Tian",
                "Chunhua Shen",
                "Hao Chen."
            ],
            "title": "Conditional convolutions for instance segmentation",
            "venue": "Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part I 16, pages 282\u2013298. Springer.",
            "year": 2020
        },
        {
            "authors": [
                "Haochen Wang",
                "Xiaolong Jiang",
                "Haibing Ren",
                "Yao Hu",
                "Song Bai."
            ],
            "title": "Swiftnet: Real-time video object segmentation",
            "venue": "arXiv preprint arXiv:2102.04604.",
            "year": 2021
        },
        {
            "authors": [
                "Huiyu Wang",
                "Yukun Zhu",
                "Hartwig Adam",
                "Alan Yuille",
                "Liang-Chieh Chen."
            ],
            "title": "Max-deeplab: Endto-end panoptic segmentation with mask transformers",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages",
            "year": 2021
        },
        {
            "authors": [
                "Yuqing Wang",
                "Zhaoliang Xu",
                "Xinlong Wang",
                "Chunhua Shen",
                "Baoshan Cheng",
                "Hao Shen",
                "Huaxia Xia."
            ],
            "title": "End-to-end video instance segmentation with transformers",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Dongming Wu",
                "Tiancai Wang",
                "Yuang Zhang",
                "Xiangyu Zhang",
                "Jianbing Shen."
            ],
            "title": "Onlinerefer: A simple online baseline for referring video object segmentation",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages",
            "year": 2023
        },
        {
            "authors": [
                "Jiannan Wu",
                "Yi Jiang",
                "Peize Sun",
                "Zehuan Yuan",
                "Ping Luo."
            ],
            "title": "Language as queries for referring video object segmentation",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4974\u20134984.",
            "year": 2022
        },
        {
            "authors": [
                "Haozhe Xie",
                "Hongxun Yao",
                "Shangchen Zhou",
                "Shengping Zhang",
                "Wenxiu Sun."
            ],
            "title": "Efficient regional memory network for video object segmentation",
            "venue": "arXiv preprint arXiv:2103.12934.",
            "year": 2021
        },
        {
            "authors": [
                "Chenliang Xu",
                "Shao-Hang Hsieh",
                "Caiming Xiong",
                "Jason J Corso."
            ],
            "title": "Can humans fly? action understanding with multiple classes of actors",
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2264\u20132273.",
            "year": 2015
        },
        {
            "authors": [
                "Kun Yan",
                "Xiao Li",
                "Fangyun Wei",
                "Jinglu Wang",
                "Chenbin Zhang",
                "Ping Wang",
                "Yan Lu."
            ],
            "title": "Two-shot video object segmentation",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2257\u20132267.",
            "year": 2023
        },
        {
            "authors": [
                "Zongxin Yang",
                "Yunchao Wei",
                "Yi Yang."
            ],
            "title": "Collaborative video object segmentation by foregroundbackground integration",
            "venue": "European Conference on Computer Vision, pages 332\u2013348. Springer.",
            "year": 2020
        },
        {
            "authors": [
                "Zongxin Yang",
                "Yunchao Wei",
                "Yi Yang."
            ],
            "title": "Associating objects with transformers for video object segmentation",
            "venue": "Advances in Neural Information Processing Systems, 34.",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Recent advances in vision-language learning have significantly advanced Human-Computer Interactions (HCI). A demanding task within HCI is referring video object segmentation (R-VOS), which involves segmenting and tracking objects in videos based on textual references. The successful development of R-VOS techniques has paved the way for diverse real-world applications such as video editing (Li et al., 2022d) and augmented reality (Huang et al., 2022). Notably, recent R-VOS methods (Luo et al., 2023) have shown unprecedented progress, propelled by the rapid advancement of multimodal foundation models such as CLIP (Radford et al., 2021). These R-VOS models enable various text-referred scenarios, allowing referring segmentation for generalized textual expressions even in complex visual scenes.\nHowever, a more challenging scenario arises in the prevalent speech dialogue system, where the aim is to refer to specific targets using spoken language, i.e., speech. The inherent nature of speech introduces vulnerabilities to disturbances from background noises (sound except for the referring speech). Consequently, crucial information within the spoken content can be distorted or even lost, which poses extra challenges in maintaining effective segmentation when referring to targets verbally. Though previous R-VOS methods achieve remarkable performance with textual queries, their performance with real-world spoken language is rarely discussed. Hence, it is crucial to develop an effective approach to bridge the text and speech to adapt well-trained R-VOS methods (frozen) for speech inputs.\nYet, bridging speech to R-VOS methods introduces new challenges. A straightforward solution involves utilzing automatic speech recognition (ASR) (Li et al., 2022a) to convert speech\nto text, followed by text-conditioned referring segmentation. However, this can result in suboptimal performance for two primary reasons. (1) Noise in language queries of R-VOS. Existing R-VOS models rely on clean text-video pairs, wherein the textual expression unambiguously identifies the target object. Nonetheless, information extracted from speech may be incomplete or distorted due to background noise and ASR errors, leading to inadequate references to the target object. To maintain robust segmentation quality, it is crucial to adapt R-VOS models to handle perturbed referring queries. (2) Noise in speech understanding. In practice, speech and background noise are closely intertwined, making it difficult to accurately comprehend semantic information from speech. Considering the diverse types of noise, an effective noise-tolerant speech understanding approach is vital for achieving robust speech-referring video object segmentation.\nIn this paper, we present STBridge, a novel approach that enables R-VOS models trained on clean text-video pairs to adapt to noisy speech as referring guidance, maintaining robust performance even amidst background noises. As illustrated in Fig. 1, the proposed STBridge links the well-trained R-VOS model with speech input, incorporating two core considerations to improve the model\u2019s robustness: (1) enhancing the welltrained R-VOS model to accept incomplete guidance, and (2) providing the noise-tolerant capability for speech understanding. On the one hand, we introduce a semantic-jitter suppression (SJS) module to help the R-VOS model understand noisy information from referring guidance. The SJS module generates object queries with randomly jittered textual features, allowing the model to learn from incomplete referring guidance under proper supervision. On the other hand, we introduce a noise-aware semantic adjustment (NSA) module, which generates noise-adaptive filters to enhance the speech representation. This differs from traditional speech enhancement, as it focuses solely on encoded semantics during speech understanding, while discarding low-level information, i.e., waveforms.\nWe further introduce a slack semantic alignment to align text and speech queries, enabling the integration of speech input with well-trained R-VOS models. Notably, our method incorporates additional modules without any retraining of R-VOS models, which is essential for numerous real-world\napplications. In summary, our contributions are as follows:\n\u2022 We propose STBridge, a novel approach to bridge speech input to referring segmentation models, enabling segmenting objects with spoken language.\n\u2022 We introduce semantic-jitter suppression and noise-aware semantic adjustment modules to enable the noise-tolerant capability for speech queries.\n\u2022 We conduct extensive experiments on speechreferring segmentation benchmarks and the results of which show our approach performs favorably over prior arts."
        },
        {
            "heading": "2 Related Works",
            "text": "Video segmentation. Video segmentation (Wang et al., 2021c; Li et al., 2022b,c, 2023a; Yan et al., 2023; Li et al., 2023c) is a fundamental task to enable video editing. Semi-supervised video object segmentation (VOS) which leverages a firstframe mask to assign the target object is among the most popular video segmentation tasks due to its high segmentation quality. Some recent works (Yang et al., 2020, 2021) propagate masks by exploring matches among adjacent frames. SpaceTime-Memory networks (STM) (Oh et al., 2019) builds a memory bank for matches. Several works follow the paradigm used in STM and improve the memory construction policy (Xie et al., 2021; Liang et al., 2020; Wang et al., 2021a) or enhance the memory reading strategy (Cheng et al., 2021a; Seong et al., 2020; Hu et al., 2021; Cheng et al., 2021b; Seong et al., 2021; Yang et al., 2021). Recently, Yan et al.(Yan et al., 2023) introduced a two-shot setting for VOS tasks which enables highperformance segmentation with limited annotated frames. Since the VOS task is primarily used for video editing which requires human involvement, to reduce the labor in assigning the target object, referring video object segmentation (R-VOS) is introduced. Specifically, R-VOS aims to segment an object in a video sequence given a linguistic description as the query. ReferFormer (Wu et al., 2022) and MTTR (Botach et al., 2022) are two pioneering works that utilize transformers to decode or fuse multimodal features. Recently, R2-VOS (Li et al., 2023b) introduces a cyclic structural consistency to enhance the robustness of R-VOS. And\nOnlineRefer (Wu et al., 2023) employs the query propagation module to enable the online R-VOS.\nSpoken language understanding. Spoken language, i.e., speech, enables a more natural way for humans to refer to a certain object than using text-based language. Thanks to the emergence of datasets with paired images and speech, e.g., Flicker8K (Harwath and Glass, 2015) and AVOS (Pan et al., 2022), more works (Chrupa\u0142a, 2022; Harwath et al., 2020; Kano et al., 2021; Seo et al., 2023) started to research on the representation of speech and explore the synergy between speech and other modalities, e.g., image and video. For example, LAVISH (Lin et al., 2023) incorporates a small set of latent tokens to align the visual and audio representation, and VisualVoice (Gao and Grauman, 2021) conducts speech separation with the speaker\u2019s facial appearance as a conditional prior. Later, research on speech has also moved towards finer granularity tasks. Some works (Lei et al., 2021) focus on the mono-modal impact of speech to study the subtle semantic information of spoken language to better understand human speech, while others (Jiang et al., 2021) study how to introduce the knowledge of speech understanding to create more natural human-computer interaction applications, e.g. talking head (Hwang\net al., 2023; Li et al., 2023c; Qu et al., 2023)."
        },
        {
            "heading": "3 Method",
            "text": "To ground objects verbally, we start from a frozen text-referring video object segmentation (R-VOS) model (shown as blue modules in Fig. 2), including frozen video-text encoders and a mask decoder. We introduce a speech encoder, a semantic jitter suppression (SJS) module, a noise-aware semantic adjustment (NSA) module, and a semantic alignment constraint to bridge text and speech (shown as pink modules in Fig. 2). During training, STBridge leverages video V , text T , and noisy speech S triplets to align the query spaces between text and speech. Thereafter, we can discard the text branch and directly query the objects with speech during inference."
        },
        {
            "heading": "3.1 Encoders",
            "text": "Frozen video and text encoders. We consider a generic referring segmentation framework that equips a video encoder Ev and a text encoder Et to extract visual and textual features. Let us denote the extracted visual feature as f = Ev(V ) \u2208 RCv\u00d7Lv\u00d7H\u00d7W and extracted text embeddings as gt = Et(T ) \u2208 RC\u00d7Lt , where Cv, C and Lv, Lt are the channel and length of visual and text embeddings respectively. We freeze the video and text\nencoders during both training and inference.\nSpeech encoder. We leverage a transformerbased speech encoder, Wav2Vec2 (Baevski et al., 2020) to extract speech features. We additionally augment two linear layers on top of the last hidden state of Wav2Vec2 to predict noise type. Thereby, each speech embedding corresponds to a noise embedding to describe the noise information. We denote the extracted noisy speech embedding as gns \u2208 RC\u00d7Ls and noise embedding as gn \u2208 RC\u00d7Ls . C and Ls are the channel and length of embeddings."
        },
        {
            "heading": "3.2 Semantic Jitter Suppression",
            "text": "To equip the R-VOS model, which is typically trained on clean data samples, with the noisetolerance capability, we first mimic noisy text embeddings g\u2032t by applying semantic jitters to the original text embeddings gt. After that, we introduce a learnable semantic jitter suppression block \u03c6(\u00b7) to suppress the jitter and generate proper object query q for the following mask decoding.\nSpecifically, we implement the semantic jitter with a linear perturbation function where g\u2032t = m \u25e6 gt + \u03b4. Here, m \u2208 {0, 1}C\u00d7Lt is a binary masking operation at either word-level (along Lt dimension) or channel-level (along C dimension); \u03b4 \u2208 RC\u00d7Lt is a random noise; \u25e6 denotes the Hadamard product. Besides, the jitter suppression block is constructed by cascading a transformer encoder and a global average pooling layer which pools along the word dimension. Formally, the final object query q \u2208 RC\u00d71 can be generated as\nq = \u03c6(m \u25e6 gt + \u03b4). (1)"
        },
        {
            "heading": "3.3 Noise-aware Semantic Adjustment",
            "text": "We introduce noise-aware semantic adjustment (NSA) to adjust inaccurate semantics introduced by noises, which consists of two components: a bi-directional cross-attention for noise-speech interaction and a noise-guided modulation for speech embedding adjustment.\nBi-directional cross-attention (BCA). In BCA, Noise-to-Speech (N-S) and Speech-to-Noise (SN) cross-attention layers are involved to compute noise-aware speech embeddings g\u2032n and speechaware noise embedding g\u2032n. Formally, they take the form:\nhn\u2192s = Softmax ( QTnKs/ \u221a d ) Vs (2)\nhs\u2192n = Softmax ( QTs Kn/ \u221a d ) Vn, (3)\nwhere hn\u2192s and hs\u2192n are outputs of N-S and S-N attention. K, Q, and V are derived by applying linear projections on the original speech or noise embedding. d is the dimension of K and Q. The hn\u2192s and hs\u2192n are fused back to their paths with residual connections (He et al., 2016). We denote the fused embeddings as g\u2032ns and g \u2032 n.\nNoise-guided modulation (NGM). To incorporate noise information into speech embeddings, we propose a noise-guided feature modulation with channel-wise attention (Tian et al., 2020). Different from attention in BCA acting along the time dimension, channel-wise attention directly acting on feature channels is more efficient to exploit semantically meaningful correlations (Wang et al., 2021b; Tian et al., 2020), especially for instance-level correlations (Tian et al., 2020; Cao et al., 2020; Bolya et al., 2019). Given the speech-aware noise embedding g\u2032ns, we first apply a fully connected layer on it to form the dynamic filters \u0398 = {\u03b8i}Lsi=1. Here, each filter \u03b8i \u2208 RC\u00d71 represents the noise information for each timestep and modulates the speech embeddings according to their category and amplitude. Then we utilize channel-wise attention to modulate the noise-aware speech feature g\u2032s, which is given by: gs|n = \u0398 \u25e6 g\u2032ns (4) where gs|n is the modulated speech embeddings and \u25e6 represents Hadmard product. We fuse the gs|n back to g\u2032ns with a residual connection, which derives the final output gs = g\u2032ns + gs|n."
        },
        {
            "heading": "3.4 Frozen Mask Decoder",
            "text": "Referring segmentation methods typically leverage a query-based mask decoder D(q, f) (Wu et al.,\n2022; Botach et al., 2022) that takes an object query q \u2208 RC\u00d71 encoding the object information and a video feature f as inputs to predict the object masks M \u2208 RN\u00d7Lv\u00d7Ho\u00d7Wo , object bounding boxes B \u2208 RN\u00d7Lv\u00d74 and confidence scores S \u2208 RN\u00d7Lv\u00d71 across video frames. N is the object candidate number. Here, we omit the detailed structure (available in Appendix) for simplicity. It is worth mentioning that the object query for the decoder is simply an averaged text embedding for recent popular R-VOS methods, which takes the form: q = pool(gt). The well-trained mask decoder D keeps frozen in our method."
        },
        {
            "heading": "3.5 Training Objectives",
            "text": "We utilize a semantic alignment loss \u03bbalign to align speech and text queries, a noise classification loss Lnoise to facilitate speech understanding, and a segmentation loss Lmatch to segment objects:\nL = \u03bbalignLalign + \u03bbnoiseLnoise + Lmatch (5)\nwhere \u03bbalign and \u03bbnoise are constants.\nSemantic alignment. To bridge the text and speech queries, we conduct semantic alignment between text and speech embeddings. As the object query q requires sentence-level semantics (each sentence describes one object), it is not necessary to enforce a tight sequence-to-sequence alignment between text and speech embeddings (Tan et al., 2023). Instead, we align text and speech embeddings with a loose constraint. Specifically, given a text embedding gt and a speech embedding gs, we first pool them among word and time dimensions correspondingly. After that, an alignment constraint is applied between them\nLalign = \u2225pool(gt)\u2212 pool(gs)\u22252 (6)\nwhere \u2225 \u00b7 \u22252 is the L2-Norm.\nNoise classification. We augment the clean speech with different categories of audio, e.g., dog barking, as noise. We apply a noise classification head on top of noise embedding gn to predict noise categories. Let us denote the predicted probabilities as p \u2208 RNc\u00d71 and the ground truth class as c, where Nc is the noise type number. The noise classification loss Lnoise can be computed as\nLnoise = \u2212logp[c] (7)\nwhere p[c] denotes the probability of class c.\nObject segmentation. Following the object segmentation methods (Wu et al., 2022; Wang et al., 2021c), we assign each mask prediction with a ground-truth label and then apply a set of loss functions between them to optimize the segmentation mask quality. Given a set of predictions y = {yi}Ni=1 and ground-truth y\u0302 = {B\u0302l, S\u0302l, M\u0302l} Lv l=1 where yi = {Bi,l, Si,l,Mi,l}Lvl=1, we search for an assignment \u03c3 \u2208 PN with the highest similarity where PN is a set of permutations of N elements. The similarity can be computed as\nLmatch(yi, y\u0302) = \u03bbboxLbox + \u03bbconfLconf + \u03bbmaskLmask (8)\nwhere \u03bbbox, \u03bbconf and \u03bbmask are constant numbers to balance the losses. Following previous works (Ding et al., 2021; Wang et al., 2021c), we leverage a combination of Dice (Li et al., 2019) and BCE loss as Lmask, focal loss (Lin et al., 2017b) as Lconf , and GIoU (Rezatofighi et al., 2019) and L1 loss as Lbox. The best assignment \u03c3\u0302 is solved by the Hungarian algorithm (Kuhn, 1955)."
        },
        {
            "heading": "3.6 Inference",
            "text": "During inference, we only keep speech and video as inputs. We first pool the speech embedding g\u2032s into a fixed size and then utilize it to replace the text embedding gt. Thereby, noisy speech can replace text to query the visual object. Please note that the text branch stays functional as we froze the R-VOS model during the training of STBridge."
        },
        {
            "heading": "4 Experiment",
            "text": ""
        },
        {
            "heading": "4.1 Datasets and Metrics",
            "text": "Datasets. We conduct experiments on the largescale speech-referring video object segmentation dataset, AudioGuided-VOS (AVOS) (Pan et al., 2022) which augments three R-VOS benchmarks with speech guidance: Ref-YoutubeVOS (Seo et al., 2020), A2D-sentences (Xu et al., 2015) and JHMDB-sentences (Jhuang et al., 2013). Specifically, it involves 18,811 pairs of video sequences and speech audio, which is divided into the training, validation, and test set in a ratio of 0.75, 0.1, and 0.15, respectively. The AVOS test set only contains Ref-YoutubeVOS samples. The A2D-sentences and JHMDB-sentences test sets are evaluated on their original test splits with speech as queries. Based on the AVOS dataset, we synthesize noisy speech by combining randomly picked audio from\nAudioset (Gemmeke et al., 2017). Specifically, a noise ranging from 0 to 40 dB signal-noise ratio (SNR) to the clean speech is sampled during training. For validation and testing, we create noisy speech under 10 dB, 20 dB, and 30 dB SNR for comprehensive evaluation.\nMetrics. We leverage the region similarity J and contour accuracy F (Pont-Tuset et al., 2017) metrics for the evaluation of speech-referring video object segmentation. The overall evaluation metric J&F is the average of J score and F score. Both the J \u2191 and F \u2191 scores are the larger the better."
        },
        {
            "heading": "4.2 Implementation Details",
            "text": "We implement our method in PyTorch. Without losing generality, we leverage ReferFormer (Wu et al., 2022) as our frozen R-VOS model (can be replaced with any query-based model). We train our model for 2 epochs with a learning rate of 1e-4. All experiments are run on 8 NVIDIA V100 GPUs. We adopt batchsize 8 and an AdamW (Loshchilov and Hutter, 2017) optimizer with weight decay 5\u00d7 10\u22124. Images are cropped to have the longest side 640 and the shortest side 360 during training and evaluation. In Eq. 1, we utilize the random noise \u03b4 \u223c Uniform(\u22120.5, 0.5) and a masking ratio of 0.1 for m as default. Please refer to the Appendix for more details."
        },
        {
            "heading": "4.3 Quantitative Results",
            "text": "Segmentation with clean speech. Table 2 compares the proposed STBridge with previous methods using the ResNet-50 (He et al., 2016) backbone. To better analyze the performance of STbridge, we introduce two popular R-VOS baselines (with text query), i.e., ReferFormer (Wu et al., 2022) and MTTR (Botach et al., 2022), and leverage Wav2Vec (same as our speech encoder) (Baevski\net al., 2020) to conduct ASR to adapt them to speech input. We notice that ASR-converted text will degrade the baseline models\u2019 performance even without noise impact. We consider this can result from word errors in the converted text from speech. For example, if the target object \u2018cat\u2019 is wrongly recognized as \u2018cap\u2019 by ASR, the R-VOS model will inevitably segment the wrong object.\nSegmentation with noisy speech. As shown in Table 1, we compare the performance of STBridge to previous text-queried methods with noisy speech as inputs. We modify the signal-noise ratio (SNR) of noisy speech to comprehensively evaluate the noise influence. We notice that ASR-based methods suffer severe performance drops compared to clean speech. In contrast, STBrigde shows a more robust performance with only slight degradation when noise becomes loud."
        },
        {
            "heading": "4.4 Visualization",
            "text": "In Fig. 4, we show the qualitative comparisons between our method, i.e., STBridge, and a cascade of Wav2Vec2 (Baevski et al., 2020) (ASR model) and ReferFormer (Wu et al., 2022) (RVOS model). Note that the ASR model is fairly chosen to have\nthe same speech encoder as STBridge. Notably, our method successfully refers to the correct object while ASR-assisted ReferFormer fails to understand the speech input and predicts wrongly.\nFig. 5 demonstrates the function of the SJS module. By comparing Fig. 5 (b) and Fig. 5 (c), we can notice that SJS module effectively suppresses noises in the jittered embeddings."
        },
        {
            "heading": "4.5 Ablation Experiments",
            "text": "We conduct ablation studies to show the impact of different modules. Unless otherwise specified, all experiments are conducted on the AVOS test set.\nModule effectiveness. We conduct experiments to validate the effectiveness of our proposed modules. We add the proposed modules step-by-step as shown in Table 3. (1) With semantic alignment between textual and speech representation, STBridge achieves 61.0 and 55.7 J&F with clean and noisy speech queries, respectively. (2) After equipping the SJS module, STBridge can better handle noises thus boosting the performance with noisy speech to 59.7 J&F while only marginal improvement is achieved with clean speech queries. (3) The\nequipping of NSA module benefits the performance with both clean and noisy speech queries. We consider the reason is that the two types of attention in NSA better filter out irrelevant features and help the speech embedding focus on the target object. With all modules, STBridge achieves 65.6 and 62.4 J&F for clean and noisy speech correspondingly.\nSemantic jitter type. During training, STBridge generates semantic jitter and then learns to suppress it using the SJS module to enhance the noisetolerant capability of the R-VOS model. We conduct an ablation study to investigate the influence of different semantic jitter types. Specifically, the implemented semantic jitter on the text embedding gt has a form of m \u25e6 gt + \u03b4, where m and \u03b4 are binary mask and random noise respectively. As shown in Table 4, we notice masking among both the word-level and channel-level shows an improvement in performance. Random noise \u03b4 brings an additional 0.5 J&F to the final performance.\nDesign choices. We conduct experiments to ablate the design choices in STBridge and their impacts on the segmentation performance. (1) We first study the effect of frame window size selected from the entire video sequence during training. We notice that the window size only shows a marginal impact on the performance, which can be due to the visual encoder and mask decoder being frozen during training. As shown in Table 5a, we notice a window size of 5 achieves the best performance. (2) After that, we ablate on the loss type for semantic alignment in Table 5b. We leverage L1, L2, and Cosine loss to align the text and speech embeddings. We notice that L2 loss achieves the best perfor-\nmance among them. (3) Semantic jitter suppression is an essential component in STBridge for noise tolerance. We conduct ablation studies to demonstrate the impact of different masking ratios and random noise amplitude. Table 5c demonstrates the performance with different masking ratios of m \u2208 [0, 1]C\u00d7Lt (calculated as 1\u2212 sum(m)C\u00d7Lt ). Small masking ratios cannot provide enough perturbation to the inputs while large ratios may lose the semantics to the target object. We find a masking ratio of 0.1 is a good trade-off as shown in Table 5c. (4) We ablate the amplitude of noise \u03b4 added as a semantic jitter in Table 5d. We notice that an amplitude of 0.5 leads to the best performance.\nNoise category. We conduct an experiment to investigate the impact of different noise categories. We additionally synthesize noisy speech queries\nby mixing clean speech from AVOS test set with different categories of audio recordings from Audioset (Gemmeke et al., 2017). As shown in Fig. 6, we illustrate the results of queries with different noise categories. We notice that sustained and loud noises, e.g., ambulance siren, can lead to a severe performance drop compared to short-lived and faint noises, e.g., horse clip-clop."
        },
        {
            "heading": "5 Conclusion",
            "text": "In conclusion, this paper presents STBridge, a novel approach that enables R-VOS models trained on clean text-video pairs to adapt to noisy speech as referring guidance, maintaining robust performance. The approach incorporates semantic jitter suppression (SJS) and noise-aware semantic adjustment (NSA) modules to enhance noise tolerance in speech queries. Experimental results demonstrate the effectiveness of STBridge, outperforming previous methods on three benchmarks. STBridge expands the applicability of R-VOS models, enabling robust speech-referred video object segmentation in real-world scenarios.\nLimitation. In spite of STBrdge\u2019s high performance on existing benchmarks, we only consider the scenario that text and speech queries are in the same language. Bridging text and speech in different languages can impose more challenges as the semantic spaces may suffer more divergence, which will be our future focus."
        },
        {
            "heading": "A More Implementation Details",
            "text": "Training details. Following the previous referring segmentation methods (Wu et al., 2022; Botach et al., 2022; Kamath et al., 2021), we leverage loss weight coefficients \u03bbdice and \u03bbfocal to balance Dice (Li et al., 2019) and focal (Lin et al., 2017b) losses in Lmask. And \u03bbgiou and \u03bbL1 to balance GIoU (Rezatofighi et al., 2019) and L1 losses in Lbox. During training, we set \u03bbnoise = \u03bbalign = 1, \u03bbconf = \u03bbgiou = \u03bbdice = 2, and \u03bbL1 = \u03bbfocal = 5. We set the layer number in the transformer encoder in \u03c6 as 3. The frozen ReferFormer (Wu et al., 2022) is pre-trained on Ref-COCO/+/g (Kazemzadeh et al., 2014) for 12 epochs and then finetuned on the AVOS training set while with text queries as input for 6 epochs. For the text, speech, and video triplets used in STBridge training, we only ensure the text and speech describe the same object while the words in the text and speech may differ slightly.\nDuring STBridge training, the noisy speech input is generated by mixing the original clean speech with randomly picked audio, which is injected as noise, from AudioSet with an SNR ranging from 0 to 40. The noise categories include \u2019ambulance siren\u2019,\u2019 baby laughter\u2019, \u2019gun shooting\u2019, \u2019cat meowing\u2019, \u2019chainsawing trees\u2019, \u2019coyote howling\u2019, \u2019dog barking\u2019, \u2019driving buses\u2019, \u2019helicopter\u2019, \u2019horse clip-clop\u2019, \u2019lawn mowing\u2019, \u2019lions roaring\u2019, \u2019bird singing\u2019, \u2019guitar\u2019, \u2019glockenspiel\u2019, \u2019piano\u2019, \u2019tabla\u2019, \u2019ukulele\u2019, \u2019violin\u2019, \u2019race car\u2019, \u2019typing keyboard\u2019. The sampling probabilities for each category are the same."
        },
        {
            "heading": "B Detailed Structure of Mask Decoder",
            "text": "We demonstrate the detailed mask decoding process in Figure A. Except for the visual feature ft and object query q as defined in the main paper, we additionally enroll the prototype masks {Pt}Nt=1 and instance embedding {et}Tt=1. Given the object query q \u2208 RC\u00d71 from STBridge, we first repeat it N times to form the input to the transformer decoder TrD where N is the object candidate number (the final output is selected from object candidates based on confidence score). After that, we generate instance embedding {et}Tt=1 for each time step separately using a shared transformer decoder TrD with encoded memory {ft}Tt=1 from visual encoder. The instance embedding here encodes the instance information and is leveraged to guide the mask decoding process. The mask prediction\nMt for each time step t is derived by a dynamic convolution between prototype mask Pt and dynamic weights which are learned from instance embedding et by two fully connected layers. The prototype masks {Pt}Tt=1 is generated by feature pyramid network (FPN) (Lin et al., 2017a) with visual feature {ft}Tt=1.\nC Inference Details of R-VOS models\nTo obtain the final segmentation result, we select the mask (among N candidates) with highest confidence throughout time as:\nM\u0302t = Ms\u0302,t,\ns\u0302 = argmax i\n{Si,1 + \u00b7 \u00b7 \u00b7+ Si,T }Ni=1 (9)\nwhere {M\u0302t}Tt=1 is the masks of referred object. Si,t and Mi,t represent the i-th slot in St and Mt respectively. s\u0302 is the slot with the highest confidence to be the target object. Box predictions can help training procedures but are not used during inference.\nFrozen J&F J F % 65.5 63.7 67.4 ! 58.9 57.3 60.4\nTable A: Ablation study on updating R-VOS parameters during training."
        },
        {
            "heading": "D Training with Trainable R-VOS Model",
            "text": "We conduct additional ablation studies to show the results of training with updating parameters in RVOS model. As shown in Table A, we notice that updating R-VOS parameters during the adaptation to noisy speech inputs will result in severe performance degradation. We consider this because 1) the information in noisy speech input is not enough to accurately refer to the object resulting in noises in the training process, and 2) the S-VOS dataset is smaller than the R-VOS dataset leading to overfitting."
        },
        {
            "heading": "E More Visualization.",
            "text": "As shown in Fig. B, we demonstrate more visualizations of the proposed method. We notice that our method can correctly refer to the target object and help the R-VOS model segment temporally consistent object masks across frames.\n\ud835\udc52!\nObject Query \ud835\udc5e\nTransformer Decoder TrD\nTransformer Decoder TrD Transformer Decoder TrD\n\ud835\udc53!\n\ud835\udc52\" \ud835\udc52#\u22ef\n\u22ef\n\u22ef \ud835\udc43#\ud835\udc43\"\ud835\udc43!\n\ud835\udc40! \ud835\udc40\" \ud835\udc40#\nVideo Feature \ud835\udc53# #$!%\n{\ud835\udc43#}#$!%Prototype Masks\nDot ProductRepeat\nFC FC FC\n\ud835\udc53\" \ud835\udc53#\nFPN\nFigure A: Illustration of mask decoder, which derives mask predictions {Mt}Tt=1 from the visual features {ft}Tt=1 the object query q. Given the query feature q from STBridge, we first repeat it N times to form the input to the transformer decoder TrD where N is the object candidate number (the final output is selected from object candidates based on confidence score). After that, we generate instance embedding {et}Tt=1 for each time step separately using a shared transformer decoder TrD with visual feature {ft}Tt=1 from visual encoder. The mask prediction Mt for each time step t is derived by a dynamic convolution between prototype masks Pt and dynamic weights which are learned from instance embedding et by two fully connected layers. The prototype masks {Pt}Tt=1 is generated by feature pyramid network (FPN) (Lin et al., 2017a) with visual feature {ft}Tt=1.\na person grabbing a crocodile\na white and brown owl standing next to a white cat\na lizard in a small enclosure with two others outside and eat\nthe giraffe walking around\na zebra is standing to the left of view\nSp ee ch\nG T\nO ur s\nSp ee ch\nG T\nO ur s\nSp ee ch\nG T\nO ur s\nSp ee ch\nG T\nO ur s\nSp ee ch\nG T\nO ur s\nFigure B: More visualization of our method on AVOS test set."
        }
    ],
    "title": "Towards Noise-Tolerant Speech-Referring Video Object Segmentation: Bridging Speech and Text",
    "year": 2023
}