{
    "abstractText": "Target-oriented Multimodal Sentiment Classification (TMSC) aims to incorporate visual modality with text modality to identify the sentiment polarity towards a specific target within a sentence. To address this task, we propose a Visual Elements Mining as Prompts (VEMP) method, which describes the semantic information of visual elements with Text Symbols Embedded in the Image (TSEI), Targetaware Adjective-Noun Pairs (TANPs) and image scene caption, and then transform them into prompts for instruction learning of the model Tk-Instruct. In our VEMP, the text symbols embedded in the image may contain the textual descriptions of fine-grained visual elements, and are extracted as input TSEI; we extract adjective-noun pairs from the image and align them with the target to obtain TANPs, in which the adjectives provide emotional embellishments for the relevant target; finally, to effectively fuse these visual elements with text modality for sentiment prediction, we integrate them to construct instruction prompts for instruction-tuning Tk-Instruct which possesses powerful learning capabilities under instructions. Extensive experimental results show that our method achieves state-of-the-art performance on two benchmark datasets. And further analysis demonstrates the effectiveness of each component of our method1.",
    "authors": [
        {
            "affiliations": [],
            "name": "Bin Yang"
        },
        {
            "affiliations": [],
            "name": "Jinlong Li"
        }
    ],
    "id": "SP:3d8ef6f550fca4be304bc413ea0813508352d35d",
    "references": [
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova"
            ],
            "title": "2019. BERT: Pre-training",
            "year": 2019
        },
        {
            "authors": [
                "Feifan Fan",
                "Yansong Feng",
                "Dongyan Zhao."
            ],
            "title": "Multi-grained attention network for aspect-level sentiment classification",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3433\u20133442, Brussels, Bel-",
            "year": 2018
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun."
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013 778.",
            "year": 2016
        },
        {
            "authors": [
                "Zaid Khan",
                "Yun Fu."
            ],
            "title": "Exploiting bert for multimodal target sentiment classification through input space translation",
            "venue": "Proceedings of the 29th ACM International Conference on Multimedia, pages 3034\u20133042. ACM.",
            "year": 2021
        },
        {
            "authors": [
                "Kirby Kuznia",
                "Swaroop Mishra",
                "Mihir Parmar",
                "Chitta Baral."
            ],
            "title": "Less is more: Summary of long instructions is better for program synthesis",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 4532\u2013",
            "year": 2022
        },
        {
            "authors": [
                "Junnan Li",
                "Dongxu Li",
                "Silvio Savarese",
                "Steven Hoi."
            ],
            "title": "Blip-2: Bootstrapping language-image pretraining with frozen image encoders and large language models",
            "venue": "arXiv preprint arXiv:2301.12597v2.",
            "year": 2023
        },
        {
            "authors": [
                "Yan Ling",
                "Jianfei Yu",
                "Rui Xia."
            ],
            "title": "Visionlanguage pre-training for multimodal aspect-based sentiment analysis",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2149\u2013",
            "year": 2022
        },
        {
            "authors": [
                "Di Lu",
                "Leonardo Neves",
                "Vitor Carvalho",
                "Ning Zhang",
                "Heng Ji."
            ],
            "title": "Visual attention model for name tagging in multimodal social media",
            "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),",
            "year": 2018
        },
        {
            "authors": [
                "Man Luo",
                "Sharad Saxena",
                "Swaroop Mishra",
                "Mihir Parmar",
                "Chitta Baral."
            ],
            "title": "Biotabqa: Instruction learning for biomedical table question answering",
            "venue": "CEUR Workshop Proceedings, volume 3180, pages 291\u2013304. CEUR-WS.",
            "year": 2022
        },
        {
            "authors": [
                "Swaroop Mishra",
                "Daniel Khashabi",
                "Chitta Baral",
                "Hannaneh Hajishirzi."
            ],
            "title": "Cross-task generalization via natural language crowdsourcing instructions",
            "venue": "Proceedings of the 60th Annual Meeting of the",
            "year": 2022
        },
        {
            "authors": [
                "Long Ouyang",
                "Jeffrey Wu",
                "Xu Jiang",
                "Diogo Almeida",
                "Carroll Wainwright",
                "Pamela Mishkin",
                "Chong Zhang",
                "Sandhini Agarwal",
                "Katarina Slama",
                "Alex Ray"
            ],
            "title": "Training language models to follow instructions with human",
            "year": 2022
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Ramesh Aditya",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark."
            ],
            "title": "Learning transferable visual models from natural language supervision",
            "venue": "International",
            "year": 2021
        },
        {
            "authors": [
                "Victor Sanh",
                "Albert Webson",
                "Colin Raffel",
                "Stephen Bach",
                "Lintang Sutawika",
                "Zaid Alyafeai",
                "Antoine Chaffin",
                "Arnaud Stiegler",
                "Arun Raja",
                "Manan Dey"
            ],
            "title": "Multitask prompted training enables zero-shot task generalization",
            "year": 2022
        },
        {
            "authors": [
                "Kevin Scaria",
                "Himanshu Gupta",
                "Saurabh Arjun Sawant",
                "Swaroop Mishra",
                "Chitta Baral."
            ],
            "title": "Instructabsa: Instruction learning for aspect based sentiment analysis",
            "venue": "arXiv preprint arXiv:2302.08624.",
            "year": 2023
        },
        {
            "authors": [
                "Siddharth Varia",
                "Shuai Wang",
                "Kishaloy Halder",
                "Robert Vacareanu",
                "Miguel Ballesteros",
                "Yassine Benajiba",
                "Neha Anna John",
                "Rishita Anubhai",
                "Smaranda Muresan",
                "Dan Roth."
            ],
            "title": "Instruction tuning for fewshot aspect-based sentiment analysis",
            "venue": "arXiv preprint",
            "year": 2022
        },
        {
            "authors": [
                "Jiawei Wang",
                "Zhe Liu",
                "Victor Sheng",
                "Yuqing Song",
                "Chenjian Qiu."
            ],
            "title": "Saliencybert: Recurrent attention network for target-oriented multimodal sentiment classification",
            "venue": "Chinese Conference on Pattern Recognition and Computer Vision (PRCV),",
            "year": 2021
        },
        {
            "authors": [
                "Liwen Wang",
                "Rumei Li",
                "Yang Yan",
                "Yuanmeng Yan",
                "Sirui Wang",
                "Wei Wu",
                "Weiran Xu."
            ],
            "title": "Instructionner: A multi-task instruction-based generative framework for few-shot ner",
            "venue": "arXiv preprint arXiv:2203.03903.",
            "year": 2022
        },
        {
            "authors": [
                "Yequan Wang",
                "Minlie Huang",
                "Xiaoyan Zhu",
                "Li Zhao."
            ],
            "title": "Attention-based LSTM for aspectlevel sentiment classification",
            "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 606\u2013615, Austin,",
            "year": 2016
        },
        {
            "authors": [
                "Shen."
            ],
            "title": "Super-NaturalInstructions: Generalization via declarative instructions on 1600+ NLP tasks",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 5085\u20135109, Abu Dhabi, United Arab Emi-",
            "year": 2022
        },
        {
            "authors": [
                "Alex Warstadt",
                "Amanpreet Singh",
                "Samuel R Bowman."
            ],
            "title": "Neural network acceptability judgments",
            "venue": "arXiv preprint arXiv:1805.12471.",
            "year": 2018
        },
        {
            "authors": [
                "Jason Wei",
                "Maarten Bosma",
                "Vincent Zhao",
                "Kelvin Guu",
                "Adams Wei Yu",
                "Brian Lester",
                "Nan Du",
                "Andrew M Dai",
                "Quoc V Le."
            ],
            "title": "Finetuned language models are zero-shot learners",
            "venue": "International Conference on Learning Representations.",
            "year": 2022
        },
        {
            "authors": [
                "Nan Xu",
                "Wenji Mao",
                "Guandan Chen."
            ],
            "title": "Multiinteractive memory network for aspect based multimodal sentiment analysis",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 371\u2013378.",
            "year": 2019
        },
        {
            "authors": [
                "Hao Yang",
                "Yanyan Zhao",
                "Bing Qin."
            ],
            "title": "Facesensitive image-to-emotional-text cross-modal translation for multimodal aspect-based sentiment analysis",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2022
        },
        {
            "authors": [
                "Jianfei Yu",
                "Jing Jiang."
            ],
            "title": "Adapting bert for target-oriented multimodal sentiment classification",
            "venue": "Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI19, pages 5408\u20135414, Macao, China. International",
            "year": 2019
        },
        {
            "authors": [
                "Jianfei Yu",
                "Jing Jiang",
                "Rui Xia."
            ],
            "title": "Entitysensitive attention and fusion network for entity-level multimodal sentiment classification",
            "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing, 28:429\u2013439.",
            "year": 2019
        },
        {
            "authors": [
                "Jianfei Yu",
                "Jieming Wang",
                "Rui Xia",
                "Junjie Li."
            ],
            "title": "Targeted multimodal sentiment classifcation based on coarse-to-fine grained image-target matching",
            "venue": "Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI-",
            "year": 2022
        },
        {
            "authors": [
                "Qi Zhang",
                "Jinlan Fu",
                "Xiaoyu Liu",
                "Xuanjing Huang."
            ],
            "title": "Adaptive co-attention network for named entity recognition in tweets",
            "venue": "Proceedings of the AAAI conference on artificial intelligence, volume 32.",
            "year": 2018
        },
        {
            "authors": [
                "Wenxuan Zhang",
                "Yue Deng",
                "Bing Liu",
                "Sinno Jialin Pan",
                "Lidong Bing."
            ],
            "title": "Sentiment analysis in the era of large language models: A reality check",
            "venue": "arXiv preprint arXiv:2305.15005.",
            "year": 2023
        },
        {
            "authors": [
                "Yichi Zhang",
                "Joyce Chai."
            ],
            "title": "Hierarchical task learning from language instructions with unified transformers and self-monitoring",
            "venue": "Findings of the Association for Computational Linguistics: ACLIJCNLP 2021, pages 4202\u20134213.",
            "year": 2021
        },
        {
            "authors": [
                "Zhe Zhang",
                "Zhu Wang",
                "Xiaona Li",
                "Nannan Liu",
                "Bin Guo",
                "Zhiwen Yu."
            ],
            "title": "Modalnet: an aspectlevel sentiment classification model by exploring multimodal data with fusion discriminant attentional network",
            "venue": "World Wide Web, pages 1\u201318.",
            "year": 2021
        },
        {
            "authors": [
                "Chao Zhao",
                "Wenlin Yao",
                "Dian Yu",
                "Kaiqiang Song",
                "Dong Yu",
                "Jianshu Chen."
            ],
            "title": "Learning-bynarrating: Narrative pre-training for zero-shot dialogue comprehension",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computa-",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Target-oriented Multimodal Sentiment Classification (TMSC) is a fine-grained sentiment analysis task towards multimodal data, which aims to classify the sentiment polarity towards the given opinion target (a word or a phrase) in a sentence-image pair. For example, we should infer that the sentiment polarity towards the target term \"Cleveland Cavaliers\" is negative with the provided sentence and image in Figure 1(a).\n1Our code is publicly available at https://github.com/ long8181/VEMP.\nWith the boom of multimodal multimedia (e.g., tweets) on the Internet, TMSC has been a valuable issue and has aroused a wave of research enthusiasm. Early work (Yu and Jiang, 2019; Wang et al., 2021; Zhang et al., 2021) focused on utilizing attention mechanism to fuse text features and visual features. Recently, Khan and Fu (2021) translated the image into textual image scene caption, which aligned different modalities with heightened interpretability since the image is represented in natural language. We concentrate on this new research formula to project visual modality to text modality for cross-modal fusion.\nHowever, how to adequately mine visual elements to get a comprehensive utilization of visual semantic information remains an unsolved challenge. For instance, image scene caption has two deficiencies: (1) Generally, the image scene caption lacks detailed depiction of the image, which means information loss during the image-to-text translation process. As shown in Figure 1, the image scene captions are just coarse overviews of scenes in images. (2) It mostly involves neutral vo-\ncabulary and lacks explicit affective guidance for sentiment prediction. As depicted in Figure 1, the image scene captions are objective narratives without emotional characterization. To handle these problems, Yang et al. (2022) tried to introduce facial emotion yet their method is constrained to images with facial expressions and thereby lacks generalizability. Therefore, we strive for more universal and efficient solution and propose a Visual Elements Mining as Prompts (VEMP) method.\nFirstly, we extract Text Symbols Embedded in the Image (TSEI) to complement image scene caption with fine-grained details. We notice that in the datasets Twitter-2015 and Twitter-2017 (Zhang et al., 2018; Lu et al., 2018) more than 50% of samples\u2019 corresponding images contain TSEI, which can contribute to TMSC. As delineated in Figure 1, the TSEI \"because it\u2019s a mess\" are pivotal to judge the \"negative\" sentiment polarity towards the target \"Cleveland Cavaliers\" while the TSEI \"most points without a turnover in nba finals (since1978)\" are crucial to infer the \"positive\" sentiment polarity towards to the target \"Kevin Durant\". Therefore, in our VEMP, we attach importance to TSEI and employ Optical Character Recognition (OCR) technique to extract them. In order to denoise the extracted results for improved usability, we have executed elaborate postprocessing refinements, such as spelling correction, grammar filtering, etc.\nSecondly, we design a Target-aware AdjectiveNoun Pairs (TANPs) extraction subtask to capture fine-grained and emotional visual content. Existing research has demonstrated the efficacy of ANPs in TMSC (Zhao et al., 2022), which directly employed ANPs to assist attention-based crossmodal features fusion. From another perspective, as depicted in Figure 1, we observe that the nouns are detailed objects in images and the adjectives imply emotional tendencies, which fill the above two gaps of image scene caption. Thus, we extract ANPs from the image as supplements to image scene caption. Besides, we align ANPs with the target to obtain TANPs since ANPs relevant to the target are helpful while the others introduce noises.\nFinally, we utilize TSEI, TANPs, image scene caption and the sentence in the sentenceimage pair to construct instruction prompts for instruction-tuning the model Tk-Instruct (Wang et al., 2022b), inspired by previous success of in-\nstruction learning across a variety of tasks (Zhang and Chai, 2021; Mishra et al., 2022; Ouyang et al., 2022). Our instruction prompt consists of a task definition, three examples with \"positive\", \"negative\" and \"neutral\" sentiment labels respectively, and a to-be-classified sample.\nOur contributions are as follows: (1) We propose a Visual Elements Mining as Prompts (VEMP) model, which mines TSEI, TANPs and image scene caption as textual descriptions of the image, leading to a more comprehensive understanding of visual modality. To the best of our knowledge, we are the first to take TSEI into consideration for TMSC. Moreover, VEMP is applicable to all types of images and exhibits universality.\n(2) Our VEMP adopt the instruction learning paradigm and achieves state-of-the-art performance on benchmark datasets Twitter-2015 and Twitter-2017, which demonstrates the superiority of our method.\n(3) We conduct experiments on GPT-4 with zero-shot and few-shot settings to explore its potential for TMSC."
        },
        {
            "heading": "2 Related Work",
            "text": ""
        },
        {
            "heading": "2.1 Target-oriented Multimodal Sentiment Classification",
            "text": "Target-oriented Multimodal Sentiment Classification (TMSC), an extension task of Aspect-Based Sentiment Analysis (ABSA) tailored for multimodal data, has attracted extensive attention lately due to the increase of multimodal information on the Web. Xu et al. (2019) and Yu et al. (2019) applied attention mechanism based on LSTM for cross-modal fusion. Yu and Jiang (2019), Wang et al. (2021) and Zhang et al. (2021) further chose BERT-based architecture to model the crossmodal interactions. Khan and Fu (2021) creatively proposed a novel method to translate the image into textual image scene caption, which effectively aligns multimodal features from different feature spaces. However, image scene caption lacks detailed and emotional depiction of the image. To address that, Yang et al. (2022) captured facial emotion from the image, which is limited to images containing facial expressions. Zhao et al. (2022) proposed a knowledge-enhanced framework (KEF) which employed adjective-noun pairs (ANPs) from the image to assist attentionbased models. Ling et al. (2022) proposed a\nVision-Language Pre-training framework (VLPMABSA) and trained it on three types of taskspecific pre-training tasks. Yu et al. (2022) introduced a multi-task learning architecture to capture both coarse-grained and fine-grained image-target matching relations."
        },
        {
            "heading": "2.2 Instruction Learning",
            "text": "Recently, we have witnessed remarkable progress in enhancing the cross-task generalization of models via instructions, such as FLAN (Wei et al., 2022), PromptSource (Sanh et al., 2022), InstructGPT (Ouyang et al., 2022) and Tk-Instruct (Wang et al., 2022b). Furthermore, a myriad of approaches utilizing instructions to handle domainspecific tasks have been proposed (Kuznia et al., 2022; Luo et al., 2022; Varia et al., 2022; Scaria et al., 2023; Wang et al., 2022a)."
        },
        {
            "heading": "3 Method",
            "text": ""
        },
        {
            "heading": "3.1 Task Formalization",
            "text": "Given a set of multimodal samples \u0393, a sample \u03b3 \u2208 \u0393 is a tuple (S, V, T, y), where S = { w1, . . . , wi, . . . , wi+|T |\u22121, . . . , w|S| } is\na |S|-word sentence, V is an image, T ={ wi, . . . , wi+|T |\u22121 } \u2286 S denotes a |T |-word target term, and y \u2208 {positive, negative, neutral} indicates the sentiment polarity towards the target term. Our goal is to learn a function to predict the sentiment polarity y."
        },
        {
            "heading": "3.2 Overview",
            "text": "As shown in Figure 2, our model consists of four modules: TSEI Extraction and Refinement module, Image Scene Caption Generation module, TANPs Extraction and Alignment module, and Instruction Learning with Prompts module. Given a multimodal input (S, V, T ) where S denotes the sentence and V indicates the image, in the first three modules, we extract TSEI, image scene caption and TANPs from V in natural language format respectively, which preserve both coarse-grained and fine-grained descriptions of V and thus harvest a sufficient comprehension of visual semantic information. In the fourth module, we integrate the extracted TSEI, image scene caption and TANPs with S to manually design instruction prompts for instruction-tuning Tk-Instruct, which generates the sentiment prediction in the output. We will demonstrate the model details in the following subsections."
        },
        {
            "heading": "3.3 TSEI Extraction and Refinement",
            "text": "In this module, we extract TSEI from the image V and refine them to reduce noises.\nExtraction with Maximum Area Strategy. We utilize a well-performing OCR model EasyOCR2 to recognize TSEI from the image V and select the primary TSEI from the region with the maximum area. The distribution of TSEI in V can be divided into multiple bounding boxes. TSEI from different bounding boxes are usually semantically incoherent if they are spliced together. Besides, TSEI from small bounding boxes are often too brief to provide valuable information, and instead add noises. Therefore, we concentrate on the primary content and select the recognized TSEI from the bounding box with the maximum area:\nBi = (Xi,0, Xi,1, Yi,0, Yi,1, Ri), i = 1, . . . , I (1)\nAi = |Xi,1 \u2212Xi,0| \u2217 |Yi,1 \u2212 Yi,0| (2)\nO \u2032 = Rj ,where j = arg max\ni=1,...,I Ai (3)\nwhere Bi denotes the ith bounding box; Xi,0, Xi,1, Yi,0 and Yi,1 are the borderline coordinates of the bounding box; Ri is the recognized TSEI; Ai denotes the area of the bounding box; and O \u2032 indicates the selected TSEI. Spelling Correction. Due to the limitation of the image quality and EasyOCR, there will be some errors in O \u2032 such as misrecognition of characters and inappropriate spaces. Thus, we employ Symmetric Delete spelling correction algorithm3 to correct and modify O \u2032 :\nO \u2032\u2032 = SymSpell(O \u2032 ) (4)\nGrammar Filtering. We feed O\u2032\u2032 into the RoBERTa fine-tuned on the CoLA dataset (Warstadt et al., 2018) to judge the grammatical acceptability of O \u2032\u2032 . We filter out those with ungrammatical confidence above the threshold \u03b81 = 0.9. The reserved O \u2032\u2032 is the resulted TSEI.\nHO\u2032\u2032 = RoBERTa(O \u2032\u2032 ) (5)\nJ = Softmax(WH [CLS]\nO\u2032\u2032 + b) (6)\nwhere W \u2208 R768\u00d72 and b \u2208 R2 are parameters learned in fine-tuning; and H [CLS]\nO\u2032\u2032 \u2208 R768 is the\nfinal hidden state of the [CLS] token. 2https://www.jaided.ai/easyocr/ 3https://github.com/wolfgarbe/SymSpell"
        },
        {
            "heading": "3.4 Image Scene Caption Generation",
            "text": "We apply a state-of-the-art vision-language pretrained model BLIP-2 (Li et al., 2023) to generate textual image scene caption of the image V with the following procedures.\nFirst, we encode the image V with a frozen image encoder to get the embeddings:\nHV = Image-Encoder(V ) (7)\nThen, we interact a set of learnable query embeddings as soft prompts with HV to extract language-informative visual representation H\n\u2032 V in\na querying transformer:\nH \u2032 V = FF(Cross-ATT(Self-ATT(Q),HV ))\n(8) where Q denotes the query embeddings; Self-ATT denotes self-attention layer; Cross-ATT denotes cross-attention layer; and FF denotes feed forward layer.\nNext, a fully-connected (FC) layer projects H \u2032 V\ninto the same hidden dimension of a large language model (LLM):\nH \u2032\u2032 V = FC(H \u2032 V ) (9)\nFinally, we pass the generation prompt P along with H\n\u2032\u2032 V to the LLM to generate the image scene\ncaption:\nC = LLM-Decoder(H \u2032\u2032 V , P ) (10)"
        },
        {
            "heading": "3.5 TANPs Extraction and Alignment",
            "text": "We extract ANPs from the image V and conduct alignment with the target T to obtain TANPs.\nANPs Extraction. We follow KEF to use SentiBank toolkit4 to extract ANPs with the Top-5 confidence. And we rewrite these ANPs as \"Noun is/are Adjective\".\nAlignment with Target. We utilize the model CLIP (Radford et al., 2021) to evaluate the relevance between ANPs and the target for alignment. We concatenate ANPs with the target and feed them into the text encoder of CLIP, while input the image into the image encoder of CLIP. Benefiting from pre-training on a large-scale dataset of text-image pairs, CLIP is able to work out the similarity score, which is subsequently passed to a softmax layer to calculate the matching probability. We select ANPs with matching probability exceeding the threshold \u03b82 = 0.5.\nM = Softmax(CLIP(concat(A, T ), V )) (11)\nAT\u2192A = Ai,where Mi > \u03b82 i = 1, . . . , 5 (12) where A denotes ANPs; M \u2208 R5 denotes the matching probability; and AT\u2192A indicates the resulted TANPs.\n4ee.columbia.edu/ln/dvmm/vso/download/sentibank.html"
        },
        {
            "heading": "3.6 Instruction Learning with Prompts",
            "text": "In this module, we aim to fuse the sentence S with the visual elements mined from the image V for modeling. To this end, we leverage the sentence S, the target T , TSEI, TANPs and image scene caption to manually design instruction prompts, and then apply these instruction prompts to instructiontune Tk-Instruct for generating the sentiment prediction.\nAppropriate prompts are essential for stimulating the learning potential of language models (LMs). Empirically, based on the prior success (Scaria et al., 2023), we make an effort to ensure the instruction prompts are concise, explicit, and easily comprehensible. Specifically, each of our instruction prompts comprises a task definition, three examples of \"positive\", \"negative\" and \"neutral\" sentiment labels respectively, and a tobe-classified sample. The format of every example or sample is as \"Sentence: S. Target: T . Scene description of the image: C. Adjective-noun pair extracted from the image: AT\u2192A. Text contained in the image: O \u2032\u2032 \", where C, AT\u2192A and O \u2032\u2032 denote image scene caption, TANPs and TSEI respectively. We employ specific criteria for the selection of our few-shot examples. Firstly, \"positive\", \"negative\", and \"neutral\" sentiment samples should all be included. Given the constraint imposed by Tk-Instruct\u2019s maximum input tokens limit and the lengthy nature of our examples, we opt for 1-shot examples for each sentiment category. Secondly, since our classification task fo-\ncuses on fine-grained targets, we have at least two examples that share the same input but with different targets and sentiment labels. Lastly, we choose samples with acknowledged and uncontroversial sentiment labels. The more detailed instruction prompts example is shown in Appendix A.\nWe use the following loss for instructiontuning:\nL = \u2212 1 |\u0393| |\u0393|\u2211 i=1 logp\u03b8(y i|xi) (13)\nwhere xi denotes the input instruction prompts sequence; \u03b8 is the model parameters; and yi indicates the output sentiment prediction token."
        },
        {
            "heading": "4 Experiment",
            "text": "To validate the advantage of our method, we compare it with state-of-the-art models and GPT-4. And we perform ablation study and case study to verify the effectiveness of each module of our method."
        },
        {
            "heading": "4.1 Experimental Setup",
            "text": "Datasets. We evaluate our method on two benchmark datasets Twitter-2015 and Twitter-2017. Besides, we select samples with TSEI from Twitter2015 and Twitter-2017 to build a new dataset Twitter-TSEI to check the effect of our method on more complex dataset. The statistics of Twitter2015, Twitter-2017 and Twitter-TSEI are shown in Table 1 and Table 2.\nImplementation Details. We employ the model Tk-Instruct-base-def-pos as backbone, and we set the batch size, the learning rate and the training epochs as 16, 5e-5 and 30 respectively. All instruction-tuning experiments are based on PyTorch with an NVIDIA GeForce RTX 3090 GPU.\nEvaluation Metrics We use Accuracy (Acc) and Macro-F1 as evaluation metrics, and report the average of 5 independent training runs as results."
        },
        {
            "heading": "4.2 Compared Baselines",
            "text": "We compare our method with three kinds of baselines as follows:\nVisual-Only. Res-Target, which concatenates the target embeddings and the visual features extracted by ResNet (He et al., 2016) for classification.\nText-Only. (1) AE-LSTM (Wang et al., 2016), an attention-based LSTM which utilizes attention mechanism to concentrate on the aspect in a sentence. (2) MGAN (Fan et al., 2018), a MultiGrained Attention Network which aims to alleviate the information loss of coarse-grained attention mechanism. (3) BERT (Devlin et al., 2019), a bidirectional pre-trained language model which has context understanding abilities. (4) TkInstruct (Wang et al., 2022b), a generative pre-\ntrained language model which is equipped with strong learning capabilities to unseen tasks due to rigorous training following instructions.\nText and Visual. (1) MIMN (Xu et al., 2019), a Multi-Interactive Memory Network which learns both the cross-modality and the single-modality interactions. (2) ESAFN (Yu et al., 2019), an Entity-Sensitive Attention and Fusion Network. (3) TomBERT (Yu and Jiang, 2019), which applys BERT and attention mechanism to model intramodality dynamics and inter-modality dynamics. (4) EF-CapTrBERT (Khan and Fu, 2021), which translates images into auxiliary sentences and then fuse with text-modality through a pre-trained language model. (5) FITE (Yang et al., 2022), which translates facial expressions in images into emotional texts for fusion with text-modality. (6) KEFTomBERT (Zhao et al., 2022), which combines a knowledge-enhanced framework with TomBERT to improve the capability of visual attention and sentiment prediction. (7) VLP-MABSA (Ling et al., 2022), which adopts task-specific visionlanguage pre-training. (8) ITM (Yu et al., 2022), a multi-task learning architecture which captures both coarse-grained and fine-grained image-target matching relations."
        },
        {
            "heading": "4.3 Experimental Results and Analysis",
            "text": "In the subsection, we compare our method with other baselines on Twitter-2015, Twitter-2017 and Twitter-TSEI, and provide empirical analysis.\nBased on the experimental results on Twitter2015 and Twitter-2017 datasets shown in Table 3, our observations are as follows: (1) Compared to EF-CapTrBERT and FITE, which also translate visual modality into text modality for cross-modal fusion, our VEMP outweighs EF-CapTrBERT by 1.84% and 4.00%, and surpasses FITE by 1.19% and 3.72%, on the Macro-F1 score on the Twitter2015 dataset and the Twitter-2017 dataset respectively. Besides, our VEMP also outperforms the state-of-the-art model ITM. These advancements corroborate the remarkable superiority of our method. (2) Tk-Instruct outperforms all other text-only methods and even surpasses multimodal method TomBERT, which demonstrates its excellent learning capabilities under instructions. (3) Our VEMP improves text-only Tk-Instruct on the Macro-F1 score by 3.21% on the Twitter-2015 dataset, and 2.76% on the Twitter-2017 dataset, which demonstrate the effectiveness of our VEMP\nin leveraging visual modality for training models. We also report the experimental results on Twitter-TSEI dataset in Table 4. We can find that VEMP achieves a more significant improvement over ITM than on Twitter-2015 and Twitter-2017. It is reasonable because ITM ignores TSEI while we attach importance to them in our VEMP. Moreover, VEMP still outperforms the text-only TkInstruct since VEMP can effectively exploit visual information."
        },
        {
            "heading": "4.4 Ablation Study",
            "text": "Three Modules for Visual Elements Mining. To investigate the impact of TSEI Extraction and Refinement module, Image Scene Caption Generation module and TANPs Extraction and Alignment module in our model, we remove them separately and observe the resulting consequences.\nAs depicted in Table 5, we can find that: (1) The performance declines significantly after removing the TSEI Extraction and Refinement module. Specifically, the Accuracy and Macro-F1 score drop 1.06% and 1.39% on the Twitter-2015 dataset respectively, and the Accuracy and MacroF1 score drop 1.45% and 1.65% on the Twitter2017 dataset respectively. This indicates that the TSEI Extraction and Refinement module can effectively yield TSEI as beneficial information for TMSC. (2) Without the Image Scene Caption Generation module, the Accuracy and Macro-F1 score decline about 0.5% on the Twitter-2015 dataset and about 1% on the Twitter-2017 dataset, which demonstrates the importance of the Image Scene Caption Generation module in translating the im-\nage into textual scene outline for cross-modal fusion. (3) Eliminating the TANPs Extraction and Alignment module results in performance degradation as well. In particular, the Accuracy declines 1.45% and the Macor-F1 score drops 2.22% on the Twitter-2017 dataset. This illustrates the TANPs Extraction and Alignment module\u2019s contribution to distill TANPs for emotional guidance. (4) Remarkably, we notice that removing the TSEI Extraction and Refinement module leads to the most intense performance decay, which validates the rationality of our motivation to leverage TSEI for TMSC.\nDetailed Units for Visual Elements Mining. We implement further ablation study to check the influence of more detailed units in our VEMP. We remove the Spelling Correction unit, the Grammar Filtering unit, both Spelling Correction and Grammar Filtering units, and the Alignment with Target unit respectively.\nBased on the experimental results in Table 6, we can observe that the performance downgrades on the Twitter-2015 dataset and the Twitter-2017 dataset after removing the Spelling Correction unit, the Grammar Filtering unit or the both of them, which confirms their indispensable impacts on denoising the extracted TSEI. Besides, directly applying the ANPs without alignment with the target also results in performance decline since the ANPs irrelevant to the target add noises.\nInstruction Learning with Prompts Module. To evaluate the effectiveness of Instruction Learning with Prompts module, we replace it with other typical learning paradigms, such as fine-tuning on GPT-3 (Brown et al., 2020), zero-shot learning on GPT-4 and few-shot learning on GPT-4. For fine-tuning GPT-3, We adopt the base model ada\nand set the batch size, the learning rate and the training epochs as 8, 0.05 and 4 respectively. All fine-tuning experiments are implemented on OpenAI servers remotely via OpenAI API. For zeroshot and few-shot learning on GPT-45, we design four kinds of prompts tailored for text-only zero-shot setting, text-only few-shot setting, multimodal zero-shot setting and multimodal few-shot setting respectively, which are adapted from the report (Zhang et al., 2023) that utilized large language models (LLMs) for sentiment analysis. To ensure a fair comparison, we employ the same few-shot examples as instruction learning on TkInstruct. Specific prompt examples are shown in Appendix A.\nTable 7 depicts the experimental results of different learning paradigms. We can observe that: (1) GPT-4 with zero-shot or few-shot setting performs poorly on TMSC. We conjecture the reasons are that TMSC concerns the finegrained target and sentences in tweets are informal, which increase the complexity of the task so that zero-shot and few-shot learning are incompetent. (2) Instruction-tuning Tk-Instruct surpasses other learning paradigms by a wide margin on both text-only and multimodal data, which demonstrates the effectiveness of Instruction Learning with Prompts module. Furthermore, we can conclude that instruction-tuning on language models is competitive and promising in the era of LLMs.\n5The experiments are based on the August 23 version of GPT-4. It is important to note that future updates to the model may potentially impact the results presented in this paper."
        },
        {
            "heading": "4.5 Case Study",
            "text": "To understand the advantage of our method intuitively, we present the predictions of text-only TkInstruct, ITM and VEMP on three test samples, as shown in Table 8. For sample (a), with the aid of the TSEI \"because it\u2019s a mess\" and the TANPs \"man is angry\", our VEMP made the correct prediction whereas other models fell into errors. For sample (b), ITM made a wrong prediction since it just focused on the neutral visual content excluding TSEI. In contrast, VEMP exploited the TSEI \"most points without a turnover in nba finals (since1978)\" which conveyed a positive emotional tendency and thus gave the correct prediction. For sample (c), despite without TSEI in the image, VEMP still predicted the sentiment polarity correctly, benefiting from the TANPs \"family is happy\"."
        },
        {
            "heading": "5 Conclusion",
            "text": "In this paper, we propose a Visual Elements Mining as Prompts (VEMP) method for TargetOriented Multimodal Sentiment Classification (TMSC). In order to describe the image with sufficient details and emotional characterization, we extract Text Symbols Embedded in the Image (TSEI) and Target-aware Adjective-Noun Pairs (TANPs) as supplements to image scene caption. And then we leverage them to construct instruction prompts to instruction-tune Tk-Instruct for sentiment prediction. Experimental results show that our method outperforms state-of-the-art ap-\nproaches and GPT-4. Our approach is universally applicable to all types of images, thus we intend to extend it to other multimodal tasks in the future.\nLimitations\nOur method has a notable limitation that it heavily relies on the precision of visual elements mining results. Despite employing state-of-the-art computer vision models and carefully designing post-processing procedures, there still exists a certain level of errors and noises in the extracted results. Another limitation is that we depend on the text understanding capabilities of Tk-Instruct and instruction-tuning operation to align targets with sentences. In the future, we should explore more effective alignment strategies. In addition, the Tk-Instruct we have chosen may not be considered cutting-edge. Moving forward, we plan to instruction-tune more advanced language models such as GPT-3.5 and GPT-4, provided that they offer fine-tuning accesses.\nEthics Statement\nOur work employs uniform code for all images to extract TSEI, TANPs and image scene caption, ensuring objectivity and avoiding bias. Furthermore, it utilizes two publicly available Twitter datasets while adhering to Twitter\u2019s data policy."
        },
        {
            "heading": "Acknowledgements",
            "text": "We are greatly appreciative of the anonymous reviewers for their valuable feedback and suggestions."
        }
    ],
    "title": "Visual Elements Mining as Prompts for Instruction Learning for Target-Oriented Multimodal Sentiment Classification",
    "year": 2023
}