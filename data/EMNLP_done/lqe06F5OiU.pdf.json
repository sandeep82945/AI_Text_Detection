{
    "abstractText": "Stance detection on social media is challenging for Large Language Models (LLMs), as emerging slang and colloquial language in online conversations often contain deeply implicit stance labels. Chain-of-Thought (COT) prompting has recently been shown to improve performance on stance detection tasks \u2014 alleviating some of these issues. However, COT prompting still struggles with implicit stance identification. This challenge arises because many samples are initially challenging to comprehend before a model becomes familiar with the slang and evolving knowledge related to different topics, all of which need to be acquired through the training data. In this study, we address this problem by introducing COT Embeddings which improve COT performance on stance detection tasks by embedding COT reasonings and integrating them into a traditional RoBERTa-based stance detection pipeline. Our analysis demonstrates that 1) text encoders can leverage COT reasonings with minor errors or hallucinations that would otherwise distort the COT output label. 2) Text encoders can overlook misleading COT reasoning when a sample\u2019s prediction heavily depends on domain-specific patterns. Our model achieves SOTA performance on multiple stance detection datasets collected from social media.",
    "authors": [
        {
            "affiliations": [],
            "name": "Joseph Gatto"
        },
        {
            "affiliations": [],
            "name": "Omar Sharif"
        },
        {
            "affiliations": [],
            "name": "Sarah M. Preum"
        }
    ],
    "id": "SP:fef214b46b1991075224735d95c3fa92b8ad0ef2",
    "references": [
        {
            "authors": [
                "Merouane Debbah",
                "Etienne Goffinet",
                "Daniel Heslow",
                "Julien Launay",
                "Quentin Malartic",
                "Badreddine Noune",
                "Baptiste Pannier",
                "Guilherme Penedo"
            ],
            "title": "Falcon-40B: an open large language model with state-of-the-art performance",
            "year": 2023
        },
        {
            "authors": [
                "Francesco Barbieri",
                "Jose Camacho-Collados",
                "Luis Espinosa Anke",
                "Leonardo Neves."
            ],
            "title": "TweetEval: Unified benchmark and comparative evaluation for tweet classification",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020, pages",
            "year": 2020
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: pre-training of deep bidirectional transformers for language understanding",
            "venue": "CoRR, abs/1810.04805.",
            "year": 2018
        },
        {
            "authors": [
                "Momchil Hardalov",
                "Arnav Arora",
                "Preslav Nakov",
                "Isabelle Augenstein."
            ],
            "title": "A survey on stance detection for mis- and disinformation identification",
            "venue": "Findings of the Association for Computational Linguistics: NAACL 2022, pages 1259\u20131277, Seattle,",
            "year": 2022
        },
        {
            "authors": [
                "Zihao He",
                "Negar Mokhberian",
                "Kristina Lerman"
            ],
            "title": "Infusing knowledge from wikipedia to enhance stance detection",
            "year": 2022
        },
        {
            "authors": [
                "Kornraphop Kawintiranon",
                "Lisa Singh."
            ],
            "title": "Knowledge enhanced masked language model for stance detection",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Lan-",
            "year": 2021
        },
        {
            "authors": [
                "Yingjie Li",
                "Cornelia Caragea."
            ],
            "title": "Target-aware data augmentation for stance detection",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages",
            "year": 2021
        },
        {
            "authors": [
                "Bin Liang",
                "Zixiao Chen",
                "Lin Gui",
                "Yulan He",
                "Min Yang",
                "Ruifeng Xu."
            ],
            "title": "Zero-shot stance detection via contrastive learning",
            "venue": "Proceedings of the ACM Web Conference 2022, WWW \u201922, page 2738\u20132747, New York, NY, USA. Association for Computing",
            "year": 2022
        },
        {
            "authors": [
                "Yuxiao Lin",
                "Yuxian Meng",
                "Xiaofei Sun",
                "Qinghong Han",
                "Kun Kuang",
                "Jiwei Li",
                "Fei Wu."
            ],
            "title": "BertGCN: Transductive text classification by combining GNN and BERT",
            "venue": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages",
            "year": 2021
        },
        {
            "authors": [
                "Rui Liu",
                "Zheng Lin",
                "Yutong Tan",
                "Weiping Wang."
            ],
            "title": "Enhancing zero-shot and few-shot stance detection with commonsense knowledge graph",
            "venue": "In",
            "year": 2021
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov"
            ],
            "title": "Roberta: A robustly optimized bert pretraining approach",
            "year": 2019
        },
        {
            "authors": [
                "Saif Mohammad",
                "Svetlana Kiritchenko",
                "Parinaz Sobhani",
                "Xiaodan Zhu",
                "Colin Cherry."
            ],
            "title": "Semeval2016 task 6: Detecting stance in tweets",
            "venue": "Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016), pages 31\u201341.",
            "year": 2016
        },
        {
            "authors": [
                "Hao Tian",
                "Can Gao",
                "Xinyan Xiao",
                "Hao Liu",
                "Bolei He",
                "Hua Wu",
                "Haifeng Wang",
                "Feng Wu."
            ],
            "title": "SKEP: Sentiment knowledge enhanced pre-training for sentiment analysis",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Lin-",
            "year": 2020
        },
        {
            "authors": [
                "driguez",
                "Robert Stojnic",
                "Sergey Edunov",
                "Thomas Scialom"
            ],
            "title": "Llama 2: Open foundation and finetuned chat models",
            "year": 2023
        },
        {
            "authors": [
                "Jason Wei",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Maarten Bosma",
                "Brian Ichter",
                "Fei Xia",
                "Ed Chi",
                "Quoc Le",
                "Denny Zhou"
            ],
            "title": "Chain-of-thought prompting elicits reasoning in large language models",
            "year": 2023
        },
        {
            "authors": [
                "Bowen Zhang",
                "Daijun Ding",
                "Liwen Jing"
            ],
            "title": "2023a. How would stance detection techniques evolve after the launch of chatgpt",
            "year": 2023
        },
        {
            "authors": [
                "Bowen Zhang",
                "Xianghua Fu",
                "Daijun Ding",
                "Hu Huang",
                "Yangyang Li",
                "Liwen Jing"
            ],
            "title": "Investigating chain-of-thought with chatgpt for stance detection on social media",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Detecting the stance of a text with respect to a certain topic is vital to many NLP tasks (Hardalov et al., 2022). Detecting stances on social media platforms like Twitter poses unique challenges, as emerging knowledge and colloquial language patterns can make it difficult to detect stances without additional context. For example, consider the top tweet shown in Figure 1. This tweet contains no direct mention of Donald Trump, and is thus difficult to classify without further context \u2014 such as how Trump supporters on Twitter widely supported voter fraud propaganda. Such emerging knowledge\nis difficult for LLMs with knowledge cut-offs to understand and may only be discernible by observing similarly labeled samples in the training set.\nOne way to solve this problem is by employing models with extensive world knowledge. For example, recent works have shown that using ChatGPT on Stance Detection can provide significant performance increases (Zhang et al., 2023a,b). Unfortunately, LLMs (e.g., ChatGPT, Llama) still have many issues understanding complex stance relationships from Twitter data. In this study, we highlight two issues with the state-of-the-art Chainof-Thought (COT) prompting approach to stance detection. 1) Implicit Stance Confusion: As shown in Figure 1, LLMs continue to struggle with understanding implicit tweet stance, even when employing advanced prompting strategies like COT reasoning (Wei et al., 2023). 2) Stance Label Hallucination: LLMs are prone to hallucinations, which cause them to output sound reasonings, but for the wrong stance topic (see Figure 1 example). Even\nwhen LLMs analyze the correct topic, they are also prone to using the provided label space incorrectly, producing accurate but ill-structured outputs.\nIn this study, we mitigate these two problems by introducing Chain-of-Thought (COT) Embeddings. Our approach feeds the COT reasoning text to a transformer encoder to be used as an additional feature in a traditional stance detection pipeline. The intuition behind this approach is three-fold: (i) Text encoders are robust to stance label hallucinations if the COT reasoning is correct. This can make incorrect COT predictions useful in a text classification pipeline. (ii) Text encoders can choose to ignore certain signals as needed. Thus, when a sample is too implicit to be understood by LLMs, the model may choose to focus on how similar tweets were classified. (iii) COT reasonings can inject world knowledge into a text encoder. That is, COT texts often contain reasonings and justifications grounded in world knowledge not available in the tweet. We find that, by using this approach, we can achieve state-ofthe-art results on multiple stance detection datasets.\nA summary of our contributions is as follows:\n1. To the best of our knowledge, this is the first investigation into the embedding of COT reasonings. Our approach achieves state-of-theart results on two stance detection datasets: Tweet-Stance (Mohammad et al., 2016; Barbieri et al., 2020) and Presidential-Stance (Kawintiranon and Singh, 2021).\n2. Our error analysis on COT reasoning highlights two key flaws on stance detection tasks: Implicit Stance Confusion and Stance Label Hallucinations. Our approach, Chain-ofThought Embeddings, makes COT outputs more robust to these two issues."
        },
        {
            "heading": "2 Related Work",
            "text": "Stance Detection: This task is a well-explored research problem, where early studies employed various machine learning and deep learning techniques (Hardalov et al., 2022). The emergence of large language models has further pushed the stateof-the-art performance on many stance detection datasets (Li and Caragea, 2021). Many stance detection problems require domain-specific solutions with models which explicitly inject world knowledge into stance detection systems (He et al., 2022;"
        },
        {
            "heading": "Class-wise distribution of topics",
            "text": "Liu et al., 2021). This work is motivated by knowledge infusion but substantially differs from existing works. To the best of our knowledge, while some prior work has used prompting for stance detection (Zhang et al., 2023b), no work has attempted to use LLMs as a knowledge base for improved stance detection. While we also do not explicitly explore LLMs as a knowledge extraction tool, we do find that our method has the capacity to inject world knowledge into a inference pipeline due to the nature of COT text generation.\nLLMs for Stance Detection Recently, few works have used ChatGPT for stance detection directly Zhang et al. (2023a,b). In (Zhang et al., 2023b), the authors achieve superior performance on several stance detection datasets by prompting ChatGPT to do Chain-of-Thought inference. In this study, we use a similar prompting strategy to perform stance detection, but show the benefits of embedding these COT reasoning texts and using them as a feature in a stance detection pipeline."
        },
        {
            "heading": "3 Methods",
            "text": "We employ a 1-shot COT prompt for each tweet in each dataset, aiming to determine the stance of the tweet in relation to a specific topic1. We specifically ask the models to provide a COT reasoning and to include its predicted label in brackets (e.g. [NEUTRAL] for a neutral tweet), so the output may be parsed and converted to a numeric representation. An example tweet and corresponding\n1Please refer to Appendix B for details on the prompts used for each task"
        },
        {
            "heading": "Tweet-Stance",
            "text": ""
        },
        {
            "heading": "Presidential-Stance-Biden",
            "text": ""
        },
        {
            "heading": "Presidential-Stance-Trump",
            "text": "COT excerpt can be found in Figure 1. After producing COT reasoning for a given text, we embed it with a transformer encoder and use it as a part of a stance detection pipeline. We specifically use a RoBERTa model (Liu et al., 2019) trained on Twitter data as our encoder since it has been shown to perform better on Tweet-Stance when compared to RoBERTa-base2. We denote this model as Twitter-RoBERTa (TR) in this paper.\nWe consider three different Twitter-RoBERTa variants in our experiments. TR-Tweet: We finetune with only tweet information. TR-COT: Finetune using only COT reasoning as the input and TRTweet+COT: Fine-tune Twitter-RoBERTa where tweet and COT reasoning are treated as a pair-wise input to the model (i.e. Tweet and COT reasoning texts are concatenated and jointly encoded by the pre-trained language model). All fine-tuning follows the standard text classification pipeline introduced in (Devlin et al., 2018). Please refer to Appendix A for model hyperparameters and training details for each stance detection task."
        },
        {
            "heading": "3.1 Dataset",
            "text": "We assess our method on two well-known Twitterbased stance detection datasets: Tweet-Stance (Mohammad et al., 2016; Barbieri et al., 2020) and Presidential-Stance (Kawintiranon and Singh, 2021). These datasets involve a 3-way classification task to determine whether tweets are in favor, against, or neutral towards a specific topic. The Tweet-Stance dataset comprises five topics: Hillary Clinton (HC), Feminism (FM), Abortion\n2Huggingface Model: cardiffnlp/twitter-roberta-basesep2022\n(LA), Atheism (AT), and Climate Change (CC). The Presidential-Stance dataset contains two subtasks focusing on the 2020 election cycle, with annotation for stance towards presidential candidates Joe Biden (BD) and Donald Trump (TR). The topic-wise and class-wise distribution and statistics for the training, development, and test sets of both datasets are presented in Table 1 and Table 2, respectively. The class-wise distribution indicates that both datasets are skewed towards the against class."
        },
        {
            "heading": "3.2 Evaluation",
            "text": "Tweet-Stance: We report the macro average of the Favor and Against F1 scores as defined in (Barbieri et al., 2020). We report baseline performance of 3 encoder-based stance detection models: BERTSpc (Devlin et al., 2018), BERT-GCN (Lin et al., 2021) and PT-HCL (Liang et al., 2022) as well as two ChatGPT prompting based methods: DQA and StSQA (Zhang et al., 2023b). All baseline scores are extracted from (Zhang et al., 2023b), where we note that evaluation was conducted on only a subset of the label space.\nPresidential-Stance: We report both the perclass F1 score and the macro average F1 score as reported in (Kawintiranon and Singh, 2021). Due to the lack of development set in Presidential-Stance, we report the average results over three experimental trials with different random seeds. We report the results of three baseline models BERT (Devlin et al., 2018), SKEP (Tian et al., 2020), and KEMLM (Kawintiranon and Singh, 2021)."
        },
        {
            "heading": "4 Results",
            "text": ""
        },
        {
            "heading": "4.1 Tweet-Stance",
            "text": "Results on Tweet-Stance are exhibited in Table 3. Results show that TR-Tweet+COT produces the best-performing model on Tweet-Stance, with an F1 score of 76.3. Notably, we can retain most of the performance by only embedding the COT reasoning, as TR-COT has only a 0.6 difference in F1 from TR-Tweet+COT. Our best model provides a 6.1-pt improvement over our ChatGPT COT reasoning model, and simply embedding COT provides a 5.5 boost in F1 vs extracting results from COT directly.\nAfter investigating the subset of samples where TR-Tweet+COT is correct, but disagrees with the prediction from ChatGPT COT, we find that 74%"
        },
        {
            "heading": "Baselines",
            "text": ""
        },
        {
            "heading": "ChatGPT Only",
            "text": ""
        },
        {
            "heading": "COT-Embeddings + Twitter-RoBERTa (TR)",
            "text": "(131/175) of the samples are on tweets incorrectly labeled as neutral by ChatGPT COT. This confirms our intuition that passing COT information to text encoders may help solve the Implicit Stance Confusion problem. Of the remaining 44 samples TRTweet+COT was able to predict correctly, we manually inspected the 20/44 where ChatGPT predicts \u201cAgainst\" when the true label was \"In Favor\". We find that 9/9 samples from the HC, FM, LA, AT topics are examples of stance label hallucination. For example, consider the COT reasoning: \u201c . . . it is clear that [NO] this text is against Jeb Bush and in favor of Hillary\". This text was marked \u201c[NO] = Against Hillary\" by our COT parser but was able to be handled by our encoder model as the reasoning was accurate. The remaining 11 samples in this analysis are from the climate change topic, where most COT errors largely pertain to questions of what it means to be \u201cin favor\" or \u201cagainst\" climate change, which we view as more of a natural misunderstanding than instances of stance label hallucination. Future works may explore better prompts to elicit better predictions on climate change tweets.\nIn Table 5, we evaluate the performance of COT produced by different LLMs. We find that while ChatGPT produces the highest performing COT, we achieve a meaningful performance increase when employing the smaller open-source LLM Llama-2-7b3 (Touvron et al., 2023). Unfortunately,\n3https://huggingface.co/meta-llama/Llama-2-7b-chat-hf\nlower-performing LLMs such as Falcon-7b 4 (Almazrouei et al., 2023) do not provide useful COT, highlighting the importance of LLM performance on this task."
        },
        {
            "heading": "4.2 Presidential-Stance",
            "text": "Table 4 presents the results of the PresidentialStance dataset. Results indicate that our approach outperforms all baseline models. When we analyze the Biden data, TR-Tweet+COT outperforms previous works by 1.4 F1-pts. A very interesting result is the extreme difference in performance between ChatGPT-COT and TR-COT, which provides a 20.7-pt boost in F1 score. This is driven by a large number of Implicit Stance Confusion examples where it\u2019s challenging to understand the label without seeing other training samples. Specifically, our model is correcting Neutral class predictions 56% of the time \u2014 as ChatGPT can assume mentions of democratic figures or ideals are taking a stance on Joe Biden \u2014 which is not always the case, causing under-prediction on Neutral samples. Our error analysis also found stance label hallucinations as ChatGPT was found to go off-topic when the focus of the tweet is on another political figure: \u201cwow bernie sander is the only one who supports democracy #demdebate\" provoked a ChatGPT response of \u201c... this tweet is [IN FAVOR] of Bernie Sanders.\" which is of course not the question being asked.\nSimilarly, on the Trump data, we find that our best-performing model outperforms the closest baseline by 2.4 F1-pts. Interestingly, we note that our best model does not use the tweet information at all, as TR-COT obtains the highest average F1 score (81.5). This outcome suggests that the COT reasoning is often logically sound, but our TR-COT model makes the predictions more robust to errors in the ChatGPT COT output structure.\nIn Table 5, we again evaluate the performance of COT produced by different LLMs on Presidential Stance. We find that on both the Biden and Trump datasets, ChatGPT provides the highest performing COT. On both the Biden and Trump datasets, we also find that Llama-2 performs much better than Falcon, again highlighting the importance of LLM quality in our pipeline. Notably, Llama-2 only provides helpful COT for the Biden dataset, not Trump. This result, however, is expected as ChatGPT, a higher-performing language model than Llama-2-\n4https://huggingface.co/tiiuae/falcon-7b-instruct"
        },
        {
            "heading": "Baselines",
            "text": ""
        },
        {
            "heading": "ChatGPT (gpt-3.5)",
            "text": "7b, only provides a minor improvement over the baseline TR-Tweet."
        },
        {
            "heading": "5 Conclusion",
            "text": "In this study, we have shown that embedding Chainof-Thought reasoning extracted from LLMs (e.g., ChatGPT, Lllama) can boost the performance of stance detection models. Specifically, we highlight how we can outperform vanilla COT by augmenting text encoders with COT embedding. Our analysis highlights how text encoders are robust to LLM hallucinations and aid in the prediction of deeply implicit stance labels. We encourage future works to consider embedding COT reasoning for stance detection and similar tasks using social media data."
        },
        {
            "heading": "6 Limitations",
            "text": "A limitation of this work is that stance detection using COT reasoning is very sensitive to the prompt provided to ChatGPT (Zhang et al., 2023b). In this study, we do not thoroughly investigate which COT prompt produces the best results, but rather try a few standard approaches inspired by related works. Future works aiming to optimize COT prompt structure for stance detection may find ways to reduce the effects of error hallucinations. In general, our work reduces the need for prompt optimization by mitigating issues pertaining to common COT errors.\nAnother limitation of this work is that one of its core takeaways \u2014 that COT Embeddings reduce effects of implicit stance confusion \u2014 may only be applicable to popular social media platforms where colloquial language is constantly changing. Application of COT Embeddings to other domains where all necessary information for inference is present in a single sample (e.g., in certain NLI tasks), COT Embeddings may not be as helpful.\nFinally, we note that the addition of COT embeddings may impact the computational efficiency of the model. Specific measures of computational efficiency are currently outside the scope of this paper. However, we highlight that if one is in a setting where the COT reasoning can be pre-computed, the impact of COT on computational efficiency is limited. While if COT reasonings had to be computed at inference time, there may be noticeable inference speed degradation depending on the efficiency of the LLM used for COT reasoning."
        },
        {
            "heading": "Appendix",
            "text": ""
        },
        {
            "heading": "A Training Details",
            "text": "Tweet-Stance: For training, we aggregate the training set of all stance topics. Since there are a mix of topics in the training set, we include the topic as part of the input (i.e. pretend tweet with topic information) during training. We train each model with a batch size of 16 for a maximum of 10 epochs with early stopping on the development set with patience = 2 keeping other related parameters default 5. The learning rate for each of our models was chosen via a modest grid search of [1e-3, 1e-4, 2e-5, 5e-5]. All other parameters are the default provided by the Huggingface Trainer 6.\nPresidential-Stance: For training, since there is no available development set, we simply fine-tune each model with a batch size of 32 for 5 epochs using the default parameters provided by the Huggingface trainer. We report the average macro F1 score and report the standard deviation over three experimental runs with different random seeds."
        },
        {
            "heading": "B Prompting Details",
            "text": "In our experiments, we use the following COT prompting template.\nRead the following tweet and decide if the stance of the tweet is in favor [IN FAVOR / YES], against [AGAINST / NO], or neutral [NEUTRAL / NONE] with regards to the topic <topic>:\nTweet: <Tweet> Stance: <COT Example>\nTweet: <Tweet> Stance: Lets think about this step by step.\nThe label to be output by the model is in brackets [] and is used to parse the output string to convert the label into a numeric representation. Notice that we explore two label structures, the supporting label is either [IN FAVOR] or [YES], the nonsupporting label is either [AGAINST] or [NO], and the stance-free label is either [NEUTRAL] or [NONE]. We provide examples of both label\n5https://huggingface.co/docs/transformers/ main_classes/callback#transformers.EarlyStoppingCallback\n6huggingface.co/docs/transformers/main_classes/trainer\nstructures in the samples below. The <Tweet> and <Topic> markers are parameters of the prompt and dynamically change given the sample. Consider the following two examples which COT prompt for stances on Atheism and Donald Trump:\nExample 1: Read the following tweet and decide if the stance of the tweet is in favor [YES], against [NO], or neutral [NONE] with regards to the topic Atheism:\nTweet: You cant think by yourself about life and believe in god. It just doesn\u2019t add up #SemST Stance: Lets think step by step. Since this text finds belief in god to be contradicting with the notion of thinking by oneself, it must be the case that [YES] this text is in favor of atheism.\nTweet: <Tweet> Stance: Lets think about this step by step.\nExample 2: Read the following tweet and decide if the stance of the tweet is in favor [IN FAVOR], against [AGAINST], or neutral [NEUTRAL] with regards to the topic Joe Biden:\nTweet: america\u2019s ceos say trump failed on coronavirus \u2013 and they\u2019re backing biden HTTP Stance: Lets think step by step. Since the tweet mentions Trump\u2019s failures and how important CEOs in america are backing Joe Biden, then this tweet is [IN FAVOR] of Joe Biden.\nTweet: <Tweet> Stance: Lets think about this step by step.\n1-shot COT examples were chosen randomly from each training set with the COT reasoning written by the author. We note that our 0-Shot ChatGPT baseline uses the same prompt without the 1-shot COT example.\nC Variability of Presidential-Stance Results\nDue to space constraints we are unavailable to add the standard deviations of our Presidential-Stance experiments to Table 4 in the main paper. Please refer to Table 6 for the standard deviation of each experiment.\nD Variability of Tweet-Stance Results\nWhile Tweet-Stance results are the product of dev set optimization, we also re-run our model five times with five different random seeds to highlight model variability, as this is a fairly low-resource problem. Please refer to Table 7 for the resulting standard deviations of each experiment."
        },
        {
            "heading": "Model F1",
            "text": ""
        }
    ],
    "title": "Chain-of-Thought Embeddings for Stance Detection on Social Media",
    "year": 2023
}