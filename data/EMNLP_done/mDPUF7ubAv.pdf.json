{
    "abstractText": "The success of ChatGPT validates the potential of large language models (LLMs) in artificial general intelligence (AGI). Subsequently, the release of LLMs has sparked the opensource community\u2019s interest in instructiontuning, which is deemed to accelerate ChatGPT\u2019s replication process. However, research on instruction-tuning LLMs in Chinese, the world\u2019s most spoken language, is still in its early stages. Therefore, this paper makes an in-depth empirical study of instruction-tuning LLMs in Chinese, which can serve as a cookbook that provides valuable findings for effectively customizing LLMs that can better respond to Chinese instructions. Specifically, we systematically explore the impact of LLM bases, parameter-efficient methods, instruction data types, which are the three most important elements for instruction-tuning. Besides, we also conduct experiment to study the impact of other factors, e.g., chain-of-thought data and human-value alignment. We hope that this empirical study can make a modest contribution to the open Chinese version of ChatGPT. This paper will release a powerful Chinese LLM that is comparable to ChatGLM. The code and data are available at https: //github.com/PhoebusSi/Alpaca-CoT.",
    "authors": [
        {
            "affiliations": [],
            "name": "Qingyi Si"
        },
        {
            "affiliations": [],
            "name": "Tong Wang"
        },
        {
            "affiliations": [],
            "name": "Zheng Lin"
        },
        {
            "affiliations": [],
            "name": "Xu Zhang"
        },
        {
            "affiliations": [],
            "name": "Yanan Cao"
        },
        {
            "affiliations": [],
            "name": "Weiping Wang"
        }
    ],
    "id": "SP:f9eaee16dcd2e7fedc26bc1592d3dd5c33b61bac",
    "references": [
        {
            "authors": [
                "Wei-Lin Chiang",
                "Zhuohan Li",
                "Zi Lin",
                "Ying Sheng",
                "Zhanghao Wu",
                "Hao Zhang",
                "Lianmin Zheng",
                "Siyuan Zhuang",
                "Yonghao Zhuang",
                "Joseph E. Gonzalez",
                "Ion Stoica",
                "Eric P. Xing"
            ],
            "title": "Vicuna: An opensource chatbot impressing gpt-4 with 90%* chatgpt",
            "year": 2023
        },
        {
            "authors": [
                "Meier-Hellstern",
                "Douglas Eck",
                "Jeff Dean",
                "Slav Petrov",
                "Noah Fiedel"
            ],
            "title": "2022. Palm: Scaling language modeling with pathways",
            "year": 2022
        },
        {
            "authors": [
                "Jason Wei"
            ],
            "title": "Scaling instruction-finetuned language models",
            "year": 2022
        },
        {
            "authors": [
                "Yiming Cui",
                "Ziqing Yang",
                "Xin Yao"
            ],
            "title": "Efficient and effective text encoding for chinese llama and alpaca",
            "year": 2023
        },
        {
            "authors": [
                "Junxian He",
                "Chunting Zhou",
                "Xuezhe Ma",
                "Taylor BergKirkpatrick",
                "Graham Neubig."
            ],
            "title": "Towards a unified view of parameter-efficient transfer learning",
            "venue": "arXiv preprint arXiv:2110.04366.",
            "year": 2021
        },
        {
            "authors": [
                "Jordan Hoffmann",
                "Sebastian Borgeaud",
                "Arthur Mensch",
                "Elena Buchatskaya",
                "Trevor Cai",
                "Eliza Rutherford",
                "Diego de Las Casas",
                "Lisa Anne Hendricks",
                "Johannes Welbl",
                "Aidan Clark"
            ],
            "title": "Training compute-optimal large language models",
            "year": 2022
        },
        {
            "authors": [
                "Or Honovich",
                "Thomas Scialom",
                "Omer Levy",
                "Timo Schick"
            ],
            "title": "Unnatural instructions: Tuning language models with (almost) no human labor",
            "year": 2022
        },
        {
            "authors": [
                "Gesmundo",
                "Mona Attariyan",
                "Sylvain Gelly."
            ],
            "title": "Parameter-efficient transfer learning for nlp",
            "venue": "International Conference on Machine Learning, pages 2790\u20132799. PMLR.",
            "year": 2019
        },
        {
            "authors": [
                "Edward Hu",
                "Yelong Shen",
                "Phil Wallis",
                "Zeyuan AllenZhu",
                "Yuanzhi Li",
                "Lu Wang",
                "Weizhu Chen"
            ],
            "title": "Lora: Low-rank adaptation of large language models",
            "year": 2021
        },
        {
            "authors": [
                "Yunjie Ji",
                "Yan Gong Yong Deng",
                "Qiang Niu Yiping Peng",
                "Baochang Ma",
                "Xiangang Li."
            ],
            "title": "Belle: Be everyone\u2019s large language model engine",
            "venue": "https://github.com/LianjiaTech/BELLE.",
            "year": 2023
        },
        {
            "authors": [
                "Takeshi Kojima",
                "Shixiang Shane Gu",
                "Machel Reid",
                "Yutaka Matsuo",
                "Yusuke Iwasawa"
            ],
            "title": "Large language models are zero-shot reasoners",
            "year": 2023
        },
        {
            "authors": [
                "Taku Kudo",
                "John Richardson"
            ],
            "title": "Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing",
            "year": 2018
        },
        {
            "authors": [
                "Brian Lester",
                "Rami Al-Rfou",
                "Noah Constant."
            ],
            "title": "The power of scale for parameter-efficient prompt tuning",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3045\u20133059, Online and Punta Cana, Domini-",
            "year": 2021
        },
        {
            "authors": [
                "Xiang Lisa Li",
                "Percy Liang"
            ],
            "title": "Prefix-tuning: Optimizing continuous prompts for generation",
            "year": 2021
        },
        {
            "authors": [
                "Xiao Liu",
                "Kaixuan Ji",
                "Yicheng Fu",
                "Weng Lam Tam",
                "Zhengxiao Du",
                "Zhilin Yang",
                "Jie Tang"
            ],
            "title": "Ptuning v2: Prompt tuning can be comparable to finetuning universally across scales and tasks",
            "year": 2022
        },
        {
            "authors": [
                "Zaid Alyafeai",
                "Albert Webson",
                "Edward Raff",
                "Colin Raffel"
            ],
            "title": "Crosslingual generalization through multitask finetuning",
            "year": 2023
        },
        {
            "authors": [
                "Erik Nijkamp",
                "Bo Pang",
                "Hiroaki Hayashi",
                "Lifu Tu",
                "Huan Wang",
                "Yingbo Zhou",
                "Silvio Savarese",
                "Caiming Xiong"
            ],
            "title": "Codegen: An open large language model for code with multi-turn program synthesis",
            "year": 2023
        },
        {
            "authors": [
                "Paul Christiano",
                "Jan Leike",
                "Ryan Lowe"
            ],
            "title": "Training language models to follow instructions with human feedback",
            "year": 2022
        },
        {
            "authors": [
                "Baolin Peng",
                "Chunyuan Li",
                "Pengcheng He",
                "Michel Galley",
                "Jianfeng Gao."
            ],
            "title": "Instruction tuning with gpt-4",
            "venue": "arXiv preprint arXiv:2304.03277.",
            "year": 2023
        },
        {
            "authors": [
                "Jonas Pfeiffer",
                "Ivan Vuli\u0107",
                "Iryna Gurevych",
                "Sebastian Ruder."
            ],
            "title": "Mad-x: An adapter-based framework for multi-task cross-lingual transfer",
            "venue": "arXiv preprint arXiv:2005.00052.",
            "year": 2020
        },
        {
            "authors": [
                "Tong Wang Qingyi Si."
            ],
            "title": "Alpaca-cot: An instruction-tuning platform with unified interface of instruction collection, parameter-efficient methods, and large language models",
            "venue": "https://github.com/ PhoebusSi/alpaca-CoT.",
            "year": 2023
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J. Liu"
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "year": 2020
        },
        {
            "authors": [
                "Rico Sennrich",
                "Barry Haddow",
                "Alexandra Birch"
            ],
            "title": "Neural machine translation of rare words with subword units",
            "year": 2016
        },
        {
            "authors": [
                "Yongliang Shen",
                "Kaitao Song",
                "Xu Tan",
                "Dongsheng Li",
                "Weiming Lu",
                "Yueting Zhuang"
            ],
            "title": "Hugginggpt: Solving ai tasks with chatgpt and its friends in hugging face",
            "year": 2023
        },
        {
            "authors": [
                "Rohan Taori",
                "Ishaan Gulrajani",
                "Tianyi Zhang",
                "Yann Dubois",
                "Xuechen Li",
                "Carlos Guestrin",
                "Percy Liang",
                "Tatsunori B. Hashimoto."
            ],
            "title": "Stanford alpaca: An instruction-following llama model",
            "venue": "https:// github.com/tatsu-lab/stanford_alpaca.",
            "year": 2023
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin."
            ],
            "title": "Attention is all you need",
            "venue": "Advances in neural information processing systems, 30.",
            "year": 2017
        },
        {
            "authors": [
                "Ben Wang",
                "Aran Komatsuzaki."
            ],
            "title": "GPT-J6B: A 6 Billion Parameter Autoregressive Language Model",
            "venue": "https://github.com/kingoflolz/ mesh-transformer-jax.",
            "year": 2021
        },
        {
            "authors": [
                "Xuezhi Wang",
                "Jason Wei",
                "Dale Schuurmans",
                "Quoc Le",
                "Ed Chi",
                "Sharan Narang",
                "Aakanksha Chowdhery",
                "Denny Zhou"
            ],
            "title": "Self-consistency improves chain of thought reasoning in language models",
            "year": 2023
        },
        {
            "authors": [
                "Mishra",
                "Sujan Reddy",
                "Sumanta Patro",
                "Tanay Dixit",
                "Xudong Shen",
                "Chitta Baral",
                "Yejin Choi",
                "Noah A. Smith",
                "Hannaneh Hajishirzi",
                "Daniel Khashabi"
            ],
            "title": "Super-naturalinstructions: Generalization via declarative instructions on",
            "year": 2022
        },
        {
            "authors": [
                "Jason Wei",
                "Maarten Bosma",
                "Vincent Y. Zhao",
                "Kelvin Guu",
                "Adams Wei Yu",
                "Brian Lester",
                "Nan Du",
                "Andrew M. Dai",
                "Quoc V. Le"
            ],
            "title": "Finetuned language models are zero-shot learners",
            "year": 2022
        },
        {
            "authors": [
                "Jason Wei",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Maarten Bosma",
                "Brian Ichter",
                "Fei Xia",
                "Ed Chi",
                "Quoc Le",
                "Denny Zhou"
            ],
            "title": "Chain-of-thought prompting elicits reasoning in large language models",
            "year": 2023
        },
        {
            "authors": [
                "Bo Xu",
                "Yong Xu",
                "Jiaqing Liang",
                "Chenhao Xie",
                "Bin Liang",
                "Wanyun Cui",
                "Yanghua Xiao."
            ],
            "title": "Cndbpedia: A never-ending chinese knowledge extraction system",
            "venue": "Advances in Artificial Intelligence: From Theory to Practice, pages 428\u2013438, Cham.",
            "year": 2017
        },
        {
            "authors": [
                "Fuzhao Xue",
                "Kabir Jain",
                "Mahir Hitesh Shah",
                "Zangwei Zheng",
                "Yang You."
            ],
            "title": "Instruction in the wild: A user-based instruction dataset",
            "venue": "https://github. com/XueFuzhao/InstructionWild.",
            "year": 2023
        },
        {
            "authors": [
                "Jianxin Yang."
            ],
            "title": "Firefly",
            "venue": "https://github.com/ yangjianxin1/Firefly.",
            "year": 2023
        },
        {
            "authors": [
                "Zhengyuan Yang",
                "Zhe Gan",
                "Jianfeng Wang",
                "Xiaowei Hu",
                "Yumao Lu",
                "Zicheng Liu",
                "Lijuan Wang"
            ],
            "title": "An empirical study of gpt-3 for few-shot knowledgebased vqa",
            "year": 2022
        },
        {
            "authors": [
                "Aohan Zeng",
                "Xiao Liu",
                "Zhengxiao Du",
                "Zihan Wang",
                "Hanyu Lai",
                "Ming Ding",
                "Zhuoyi Yang",
                "Yifan Xu",
                "Wendi Zheng",
                "Xiao Xia"
            ],
            "title": "Glm-130b: An open bilingual pre-trained model",
            "venue": "arXiv preprint arXiv:2210.02414",
            "year": 2022
        },
        {
            "authors": [
                "Hui Zeng"
            ],
            "title": "Measuring massive multitask chinese understanding",
            "year": 2023
        },
        {
            "authors": [
                "Ge Zhang",
                "Yemin Shi",
                "Ruibo Liu",
                "Ruibin Yuan",
                "Yizhi Li",
                "Siwei Dong",
                "Yu Shu",
                "Zhaoqun Li",
                "Zekun Wang",
                "Chenghua Lin",
                "Wenhao Huang",
                "Jie Fu"
            ],
            "title": "Chinese open instruction generalist: A preliminary release",
            "year": 2023
        },
        {
            "authors": [
                "Qingru Zhang",
                "Minshuo Chen",
                "Alexander Bukharin",
                "Pengcheng He",
                "Yu Cheng",
                "Weizhu Chen",
                "Tuo Zhao."
            ],
            "title": "Adaptive budget allocation for parameter-efficient fine-tuning",
            "venue": "The Eleventh International Conference on Learning Representations.",
            "year": 2023
        },
        {
            "authors": [
                "Zhuosheng Zhang",
                "Aston Zhang",
                "Mu Li",
                "Hai Zhao",
                "George Karypis",
                "Alex Smola"
            ],
            "title": "Multimodal chain-of-thought reasoning in language models",
            "year": 2023
        },
        {
            "authors": [
                "LLaMA LLaMA (Touvron"
            ],
            "title": "2023) is a decoder-only language model based on the Transformer (Vaswani et al., 2017) architecture, and is trained on more tokens (1T, 1.4T) than what is typically used (Hoffmann et al., 2022)",
            "year": 2022
        },
        {
            "authors": [
                "tuning (Lester"
            ],
            "title": "2021) also involves only training the input prompt embeddings. Differently, it freezes all pre-trained weights. Sequential Adapter For each transformer layer, Series Adapter methods add adapter layers",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "The emergence of ChatGPT gives humanity a real sense of hope for AGI for the first time, and inspires researchers to realize the importance of LLM research. However, the closed source of LLMs (e.g., GPT-3 (Brown et al., 2020) and PaLM (Chowdhery et al., 2022)) coupled with the requirement for massive computing resources to build the exclusive LLM has deterred researchers from reaching the LLM training stage. Subsequently, a series of \"API research\" based on GPT-3 and ChatGPT are con-\n\u2217Equal contribution. \u2020Corresponding author: Zheng Lin.\nstantly emerging, which stimulate the specific capabilities of frozen LLMs (e.g., Chain-of-Thought (Wei et al., 2023; Wang et al., 2023; Kojima et al., 2023)) or guide them to complete specific tasks (Yang et al., 2022; Shen et al., 2023), by calling OpenAI interfaces and carefully designing prompts without model training.\nThe unexpected disclosure of the pre-trained LLaMA (Touvron et al., 2023) model changes this situation, and has sparked a surge of excitement in the LLM research community. This is the first open LLM with competitive performance. Recently, Alpaca (Taori et al., 2023) uses self-instruct (Ouyang et al., 2022) and ChatGPT to generate 52K instructions, which can enable LLaMA to respond to various human instructions like ChatGPT. This open project verifies the important role of instructiontuning (Wei et al., 2022; Chung et al., 2022) open LLMs in replicating the ChatGPT process.\nGiven the open LLM LLaMA and Alpaca\u2019s high-quality instruction data, there is still a challenge for researchers: even the instruction-tuning of the 7B model still requires high computational resources. To address this problem, Alpaca-LoRA extends the parameter-efficient method LoRA to LLaMA, which further reduces the computing cost of instruction-tuning. It further sparks extensive research in the open-source community on instruction-tuning for LLMs. On this basis, more LLMs (e.g, Bloom (Workshop, 2023), GPT-J (Wang and Komatsuzaki, 2021)) are shown to have significant improvements in instruction-following performance with instruction-tuning. On the other hand, more instruction data is constantly being proposed, e.g., Belle (Ji et al., 2023) constructs Chinese instructions in the same way, and ShareGPT collects a large number of real human-ChatGPT conversations.\nHowever, research on instruction-tuning LLMs in Chinese, the world\u2019s most spoken language, is still in its early stages. LLM bases, parameter-\nefficient methods, and instruction data are three essential elements for customizing Chinese ChatGPTlike LLMs. There are no tutorials in the academic community on them yet. Some important questions have not yet been explored and answered: 1) \"Which open LLM is more suitable as a foundation for Chinese instruction-tuning?\", 2) \"How do parameter-efficient methods other than LoRA affect LLMs?\" and 3) \"What is the impact of various types of instruction datasets?\" To answer these questions, we collect a range of LLMs, parameterefficient methods, and instruction datasets. Besides, we consider the AGI (instruction-following) capability and professional knowledge reserve (human exams) of models, and correspondingly select two benchmarks Belle-eval (Ji et al., 2023) and MMCU (Zeng, 2023) for comprehensively evaluation.\nWe also conduct experiments to explore several other factors that may affect the final performance. Specifically, we find that tuning with Chain-ofThought (CoT) data can improve the ability to respond to complex reasoning questions. Different LLMs may be suitable for different language prompts (excluding instruction parts) in instructiontuning. Human-value alignment results into slight performance drop. On the basis of the above findings, this paper carefully instruction-tunes a powerful Chinese LLMs that is comparable to ChatGLM.\nThe contributions can be summarized as follows: (1) We are the first to systematically study on instruction-tuning in Chinese through adequate experiments, which can serve as a cookbook that provides valuable findings for customizing Chinese version of ChatGPT. (2) We release a powerful Chinese LLM that is comparable to ChatGLM."
        },
        {
            "heading": "2 Instruction-tuning Triplets",
            "text": ""
        },
        {
            "heading": "2.1 Preliminaries",
            "text": "Problem Formulation. LLM bases m \u2208 M , parameter-efficient methods p \u2208 P and instruction datasets d \u2208 D are the three crucial elements in instruction-tuning. This section examines the impact of each element in the instruction-tuning triplet (m, p, d) on final performance. We traverse the target element to thoroughly explore its impact, while fixing two elements in the triplet to control the variable. For example, we analyze the impact of different types of instruction datasets by comparing the performance of {(m, p, di)}|D|1 .\nBenchmarks. We select two evaluation benchmarks, Belle-eval and MMCU, to comprehensively evaluate LLM competencies in Chinese. Belle-eval is constructed by self-instruct with ChatGPT, which has 1,000 diverse instructions that involve 10 categories covering common NLP tasks (e.g., QA) and challenging tasks (e.g., code and math). We use ChatGPT to rate the model responses based on the golden answers. This benchmark is considered to be as the assessment of AGI (instruction-following) capability. MMCU is a collection of Chinese multiple choice questions in four professional disciplines of medicine, law, psychology and education (e.g., Gaokao examination). It allows LLMs to take exams in human society in a multiple-choice test manner, making it suitable for evaluating the breadth and depth of knowledge of LLMs across multiple disciplines. More statistics and details are shown in Appendix A.1."
        },
        {
            "heading": "2.2 Open Large Language Models",
            "text": "To answer \"Which open LLM is more suitable as a foundation for Chinese instruction-tuning?\", we\ncollect and evaluate the most widely used open LLMs1 available in the open source community, as shown in Table 1."
        },
        {
            "heading": "2.2.1 Evaluation of Existing LLMs",
            "text": "Performance on Belle-eval. Table 2 shows the scores of open LLMs on Belle-eval. The upper part shows base LLMs while the lower part shows the the supervised fine-tuned (sft) LLMs. We can derive several observations: 1) For base LLMs, Bloom performs the best because its ROOTS (Workshop, 2023) pre-training dataset has a large proportion in Chinese (261B), second only to English. Although moss-base is an LLM specifically proposed for Chinese, its performance is poor as it is obtained through further pre-training based on the CodeGen (Nijkamp et al., 2023) model and has only seen 100B Chinese data. 2) For sft LLMs, ChatGLM outperforms others by large margins, thanks to the fact that it is trained with the most\n1We mainly explore the version around 7b of them (except for 16b of Moss), which strike a balance between performance and computation resource requirements.\nChinese tokens and HFRL. 3) The Open QA, Math, CloseQA and Extract categories are still very challenging for existing open LLMs. 4) Vicuna and moss-sft have clear improvements compared to their bases, LLaMA and moss-base, respectively. The gain brought by Vicuna is more significant (22.2%) because its instruction data, collected by ShareGPT, are the real conversations between humans and ChatGPT, with higher quality. 5) In contrast, the performance of sft models, Bloomz and Bloomz-mt, is reduced compared to the base model Bloom, because they tend to generate a shorter response (refer to Appendix C.1). Unfortunately, ChatGPT often scores lower for short responses. The reason for this phenomenon is that Bloomz and Bloomz-mt are fine-tuned from xP3, which is built from the NLP task collection where many tasks have brief annotations.\nPerformance on MMCU. Table 3 shows the accuracy of LLMs on MMCU, we find that: 1) All base LLMs perform poorly because it is almost difficult to generate content in the specified format before fine-tuning, e.g., outputting option numbers. 2) All sft LLMs outperform their corresponding base LLMs, respectively. In particular, Bloomz performs the best (even beats ChatGLM) because it can generate option number directly as required without generating other irrelevant content (refer to Appendix C.2), which is also due to the data characteristics of its supervised fine-tuning dataset xP3. 3) Among the four disciplines, law is the most challenging for LLMs.\nThe LLMs\u2019 performance on MMCU is much lower than that of Belle-eval because MMCU requires higher professional knowledge. All open\nLLMs still have significant room for improvement compared to ChatGPT."
        },
        {
            "heading": "2.2.2 Instruction-tuning Different LLMs",
            "text": "To determine the appropriateness of different LLMs as a foundation for instruction-tuning in Chinese, we fine-tune all the open LLMs with the same parameter-efficient method LoRA and the same instruction dataset Alpaca-GPT4. The results are shown in Figure 12, where we find that: 1) On Belle-eval, the performance improvement of sft LLMs brought by instruction-tuning is not as significant as that of base LLMs, except for sft Bloomz and Bloomz-mt. This is because the instructions of xP3 used for their supervised fine-tuning are not diverse enough. 2) Vicuna and ChatGLM encounter performance drops after instructiontuning, because Vicuna is trained from real humanChatGPT conversations, with better quality than Alpaca-GPT4. ChatGLM adopts HFRL (Ouyang et al., 2022), which may be no longer suitable for further instruction-tuning. 3) On MMCU, most LLMs achieve performance boosts after instructiontuning, with the exception of Bloomz and Bloomzmt, which have unexpectedly significantly decreased performance. This is because that original Bloomz and Bloomz-mt excel in multiple choice questions, but after further instruction-tuning, they suffer catastrophic forgetting.\n2Full results are shown in Table 10 and 11 in App. B.1.\nAfter instruction-tuning, Bloom has significant improvements and performs well on both benchmarks. Although ChatGLM beats Bloom consistently, it suffers performance drop during instruction-tuning. Therefore, among all open LLMs, Bloom is most suitable as a foundation model in the subsequent experiments for Chinese instruction-tuning exploration."
        },
        {
            "heading": "2.3 Parameter-efficient Methods",
            "text": "For most researchers, parameter-efficient methods are essential for instruction-tuning due to limitations in computing resources. These methods tend to freeze the pre-trained model weights and injects trainable weights (adapters), which greatly reduces the number of trainable parameters. To answer \"How do parameter-efficient methods other than LoRA affect LLMs?\", we collect a range of parameter-efficient methods to instruction-tune\nBloom on Alpaca-GPT4 dataset."
        },
        {
            "heading": "Comparison of Parameter-efficient Methods.",
            "text": "From Table 4, several observations can be derived: 1) SadapterH performs the best among all parameter-efficient methods, which can be used as an alternative to LoRA. 2) P-tuning and prompttuning underperform others by large margins, indicating that only adding trainable layers in the embedding layer are not enough to support LLMs for generation tasks. 3) Although AdaLoRA is an improvement of LoRA, its performance has a clear drop, possibly because the LoRA\u2019s trainable parameters for LLMs are not suitable for further reduction. 4) Comparing the upper and lower parts, it can be seen that increasing the number of trainable parameters for sequential adapters (i.e., SadapterP and SadapterH) does not bring gain, while the opposite phenomenon is observed for parallel adapters (i.e., P-adapter). This may provide inspiration for the design of adapters for LLM. Since LoRA is currently the most popular parameter-efficient method, if not otherwise specified, we adopt LoRA by default in the experiments.\nTraining Loss. Figure 2 shows the training loss of different parameter-efficient methods. We find that: 1) Prompt-tuning and P-tuning converge the slowest and has the highest losses after convergence. This shows that embedding-only adapters are not suitable for instruction-tuning LLMs. 2) The initial loss of AdaLoRA is very high because it requires simultaneous learning of parameter budget allocation, which makes the model unable to fit the training data well. 3) The other methods can\nquickly converge on training data and fit it well."
        },
        {
            "heading": "2.4 Chinese Instructions Datasets",
            "text": "Alpaca (Taori et al., 2023) inspires researchers to further explore instruction data. To systematically explore \"What is the impact of various types of instruction datasets?\", we gather popular open Chinese instructions (as shown in Table 5) to fine-tune Bloom with LoRA.\nPerformance on Belle-eval. As shown in Table 6 upper part, it can be seen that: 1) the instruction data constructed by ChatGPT (e.g., using self-instruction methods or collecting real humanChatGPT conversations) consistently enhances the instruction-following ability with 3.1 \u223c 11-point score increases. 2) Among these datasets, Belle has the best performance due to the largest amount of instruction data. However, the performance of models trained on moss-sft-data, containing more data built in a similar way, is unsatisfactory. This is because moss-sft-data\u2019s instructions sacrifice the diversity to achieve the goals of helpfulness, honey, and harmlessness. 3) The performance brought by the Alpaca-GPT4 instructions is the second best, with only 49K being comparable to the 1.54M Belle. This is because Alpaca-GPT4 uses the GPT4 engine while Belle uses the text-avinci-003 engine, which further illustrates that improving data quality can reduce the demand for data volumes. 4) Instinwild brings the least performance gains among them because the seed instructions it crawls from Tweet (\"in wild\") are not as comprehensive as those (like Alpaca) carefully designed by hu-\nmans. 5) These ChatGPT-based data mainly have a significant improvement effect on open generation tasks such as Brain Storm and Generation, while there is a significant decrease in tasks that require high reading comprehension skills, such as Close QA and Extract, which require completing tasks based on given materials. This inspires researchers to consider the reading-comprehension ability for building more comprehensive instruction datasets.\nThe lower part of Table 6 shows the results of models trained on dataset-based data, which is mainly constructed by collecting NLP or examination datasets. These instruction datasets cause damage to the model\u2019s instruction-following ability, because the form and intent of each NLP or examination dataset are unitary, which can easily be overfitted. Among them, COIG-trans performs the best because it involves over 2000 different\ntasks with a wide variety of task instructions. In contrast, xP33 and COIG-ccmc have the worst negative impact on model performance. Both of them only cover few types of tasks (translation and QA for the former, counterfactual correction conversations for the latter), which hardly cover the popular instructions and tasks for humans.\nPerformance on MMCU. Table 7 compares the performance on MMCU brought by different instruction datasets. 1) Instruction-tuning on each dataset can always result in performance improvement. 2) Among the ChatGPT-based data shown in the upper part, ShareGPT-zh underperforms others by large margins. This may be due to the fact that real users rarely ask multiple choice questions about academic topics. 3) Among the datasetcollection data shown in the lower part, HC3 and COIG-ccmc results in the lowest accuracy because that the unique questions of HC3 is only 13K, and the task format of COIG-ccmc is significantly different with MMCU. 4) COIG-exam4 brings the greatest accuracy improvement, benefiting from the similar task format as MMCU."
        },
        {
            "heading": "3 Other Important Factors",
            "text": "Problem Formulation. In addition to the essential three elements (m, p, d) discussed above, there are many factors worth exploring, e.g., CoT. If not otherwise specified, we use Bloom as the LLM base, LoRA as the parameter-efficient method, and Alpaca-GPT4 as the instruction data. On this basis,\n3We use 1/3 of its Chinese data due to its large quantity. 4Due to the overlap between COIG-exam and MMCU-\nEdu., the accuracy on Edu. discipline will not be reported.\nwe explore its impact by observing the performance changes after considering the target factor.\nChain-of-Thought Data. Chain-of-Thought is a hot topic in LLM research. Existing works find that adding rationales or explanations to the inference prompts (Wei et al., 2023; Wang et al., 2023; Kojima et al., 2023) (based on APIs of GPT-3 and ChatGPT) or training corpus (Wei et al., 2022; Chung et al., 2022; Zhang et al., 2023c) (based on normal language models, e.g, T5(Raffel et al., 2020) and FLAN-T5(Wei et al., 2022)) can enhance the model\u2019s reasoning ability, which is useful for solving complex problems. However, extending CoT into Open LLM has not yet been thoroughly explored. Alpaca-CoT (Qingyi Si, 2023) uses several qualitative examples to demonstrate the effectiveness of CoT in reasoning. A systematic evaluation is still necessary. To this end, this paper conducts experiments to analyze the impact of CoT data for LLMs.\nWe collect 9 CoT datasets and their prompts from FLAN (Wei et al., 2022), and then translates them into Chinese using Google Translate. We compare the performance before and after adding CoT data during instruction-tuning in Table 8. \"Alpaca-GPT4+CoT\" outperforms \"AlpacaGPT4\" in the Code and Math tasks that require strong reasoning ability. Besides, there is also a significant improvement in MMCU education task, which is derived from the questions of Gaokao, involving a range of subjects, e.g., math, physics, history. The accuracy improvement across all subjects illustrates that the CoT reasoning ability is generally required in various subjects. However, CoT training data cannot continue to bring benefits to all tasks, and on the contrary, it will cause slight performance degradation on more tasks. The full results can be found in Appendix B.3.\nInspired by (Kojima et al., 2023), we add a sentence\"\u5148\u601d\u8003\uff0c\u518d\u51b3\u5b9a \" (\"think step by step\" in Chinese) at the end of each instruction, to induce\nthe model to respond to instructions based on the chain-of-thought. As shown in the line of \"AlpacaGPT4+CoT*\", the simple sentence can further improve the performance of reasoning tasks Code and Education, while the Math performance is slightly inferior to \"Alpaca-GPT4+CoT\". This may require us to further explore more robust prompts.\nExpansion of Chinese Vocabulary. Intuitively, the number of Chinese tokens in the tokenizer\u2019s vocabulary affects LLMs\u2019 ability to express Chinese. For example, if a Chinese character is in the vocabulary, it can be represented by a single token, otherwise it may require multiple tokens to represent it. Because Bloom adopts a vocabulary of 250k tokens which cover most Chinese characters, we mainly conduct experiments on LLaMA, which uses SentencePiece (Sennrich et al., 2016; Kudo and Richardson, 2018) (32K vocabulary size) covering few Chinese characters.\nAs shown in Figure 3, we find that the performance of \"llama-voc\" is severely inferior to \"llama\" on Belle-eval, and is almost unable to respond correctly to MMCU\u2019s instruction. This indicates that it is not feasible to perform instruction-tuning without pre-training on vast data. This is because the embedding corresponding to the newly added Chinese token are random and meaningless, which results in the model being unable to understand the meaning of the instructions.\nTo make the newly added Chinese token meaningful, Cui et al. uses 20B and 100B token Chinese corpus to further pre-train LLaMA and obtain \"llama-voc-pre\" and \"llama-voc-pre-l\" models. We use Alpaca-GPT4 to instruction-tune these models, and find that, pre-training on more Chinese cor-\npus with expansion of Chinese vocabulary are consistently helpful for instruction-following ability. Counterintuitively, \"llama-voc-pre-l\" is inferior to \"llama-voc-pre\" on MMCU shows that pre-training on more data may not necessarily lead to higher performance for academic exams.\nThe Languages of Prompts. The popular open instruction-tuned LLMs, e.g., Alpaca and Vicuna, tend to uses prompts in English. One intuitive question is, Is instruction-tuning in Chinese more suitable for using Chinese prompts? Figure 4 shows the results of using Chinese and English prompts based on LLaMA and Bloom. When instructiontuning LLaMA, using Chinese prompts can improve the performance on both benchmarks compared to English prompts, while we observe the opposite phenomenon on Bloom. This demonstrates that using Chinese prompts for models with weaker Chinese abilities (e.g., LLaMA) can effectively help respond in Chinese, while for models with good Chinese abilities (e.g., Bloom), using prompts in English (the language they are better at) can better guide the model to understand the process of fine-tuning with instructions.\nHuman Value Alignment. To avoid LLMs generating toxic content, aligning them with human values is a crucial issue. We add the human-value alignment data built by COIG (see App. A.4 for details) into the instruction-tuning to explore its impact. Figure 5 compares the results of instructiontuning with and without human-value alignment, which shows that the human-value alignment results into a slight performance drop. How to balance the harmlessness and performance of LLMs is a research direction worth exploring in the future."
        },
        {
            "heading": "4 Towards a Better Chinese LLM",
            "text": "Problem Formulation. The goal of this section is to find a optimal triplet (m, p, d) that maximizes the comprehensive capabilities:\nmax m,p,d \u2211 t\u2208T (Et (fd (m,p))) (1)\nwhere Et denotes the evaluation of every generative ability t from both Belle-eval and MMCU T , fd(m,p) denotes the model obtained by instruction-tuning frozen LLM m with parameterefficient method p on instruction dataset d.\nOur Instruction-tuned LLM. On the basis of the findings above, we carefully design the instruction-tuning process and publicly release a Bloom-based high-performance LLM, which is comparable to ChatGLM and far surpassing Moss. In particular, we select a dataset combination with significant gains on Belle-eval or MMCU to improve our model\u2019s comprehensive ability. Besides, we carefully design a suitable prompt to induce our model for better-quality generation. The implementation details can be found in Appendix D.1.\nAs shown in Table 2, our model is superior or comparable to ChatGLM in most categories on Belle-eval, except for the challenging Math and Extract tasks. Besides, our model slightly underperforms ChatGLM on MMCU and outperforms other LLMs that do well in Belle-eval by clear margins. It is worth emphasizing that our model has much fewer trainable parameters (16M) based on LoRA than that of ChatGLM adopting full parameter fine-tuning (6B)."
        },
        {
            "heading": "5 Conclusion",
            "text": "This paper is the first to conduct a thorough empirical study on instruction-tuning open large language models in Chinese, with a detail discussion of a\nrange of large language models, parameter-efficient methods, and Chinese instruction datasets. In addition, we explore several other important factors, including CoT, vocabulary, language of prompts and human-value alignment. Based on the empirical exploration, we publicly release a LLM, that is rival to ChatGLM, with detailed implementation details."
        },
        {
            "heading": "Limitations",
            "text": "Most experimental results are based on parameterefficient methods, which may differ from the results of full parameter fine-tuning. However, we believe that the findings and conclusions in this paper are still applicable for full parameter finetuning. In addition, instruction-tuning based on parameter-efficient methods has broader application and research scenarios."
        },
        {
            "heading": "Ethics Statement",
            "text": "The open LLMs used in this paper may be driven by certain biases in their training data, and pose a risk of toxic generation. There may also exist harmful stereotypes in the open instruction datasets we are discussing. There is still a long way to explore the safety of LLMs."
        },
        {
            "heading": "Acknowledgement",
            "text": "This work was supported by National Natural Science Foundation of China (No. 61976207) and National Social Science Foundation of China (No. 21AZD145)."
        },
        {
            "heading": "A More Details about the Work Involved",
            "text": ""
        },
        {
            "heading": "A.1 Benchmarks",
            "text": "Belle-eval During this evaluation, ChatGPT is used to rate (from 0 to 1) the model response based on the ground truth answer. A score of 0 indicates that the model response is completely unacceptable, while a score of 1 indicates that the response perfectly solves the input instruction. The prompts and instructions for samples in each category are rich and varied. We consider the capability examined in this dataset to be AGI (instrcution-following) capability.\nMMCU MMCU (Zeng, 2023) is collected from online public resources, covering 11845 multiple choice questions in four professional disciplines. There are several subtasks under education and medicine disciplines. The average accuracy of all subtasks is considered the discipline score. Only when a generated answer and the annotated ground truth option number or option content completely match, is the answer considered correct. This evaluation is relatively rigid for expected outputs. We consider the capability examined in this dataset as the reserve of professional knowledge (to deal with human examinations).\nThese two assessments complement each other to some extent. Table 9 shows the data statistics of these two benchmarks."
        },
        {
            "heading": "A.2 Open Large Language Models",
            "text": ""
        },
        {
            "heading": "A.2.1 Base LLMs",
            "text": "LLaMA LLaMA (Touvron et al., 2023) is a decoder-only language model based on the Transformer (Vaswani et al., 2017) architecture, and is trained on more tokens (1T, 1.4T) than what is typically used (Hoffmann et al., 2022). It ranges from\n7B to 65B parameters and outperforms existing LLMs (e.g., GPT-3 (Brown et al., 2020) and PaLM (Chowdhery et al., 2022)) with fewer parameter magnitudes. However, the vocabulary of its tokenizer contains fewer Chinese characters, which affects its expressive power in Chinese.\nBloom Bloom (Workshop, 2023) is a multilingual language model trained on dataset ROOTs, involving 46 natural and 13 programming languages. The proportion of Chinese corpus in pre-training data is second only to English corpus. The maximum version of Bloom has 175B parameters, while the most popular version is 7B.\nMoss-moon-003-base Moss-moon-003-base (base model of MOSS series, moss-base for short) is initialized with CodeGen (Nijkamp et al., 2023) and further self-supervised pre-trained on high-quality Chinese (100B) and English (20B) corpus. All the pre-training data contains about 700B words. It has 16B parameters."
        },
        {
            "heading": "A.2.2 Supervised Fine-tuned LLMs",
            "text": "Vicuna Vicuna (Chiang et al., 2023) is fine-tuned from LLaMA on 70K user-shared ChatGPT conversations gathered by ShareGPT5. Vicuna claims to have achieved 90% performance of ChatGPT on a preliminary evaluation using GPT-4 as a judge, making it the most popular open source LLM. However, further rigorous evaluation is needed, especially in Chinese scenarios.\nBloomz & Bloomz-mt Bloomz and Bloomz-mt are fine-tuned from Bloom on crosslingual task mixture xP3 (Muennighoff et al., 2023) and xP3mt, which contain 13 training tasks in 46 language with prompts in English and in 20 languages, respectively. This supervised fine-tune process aims to further boosts the performance of multilingual tasks.\nMoss-moon-003-sft Moss-moon-003-sft (mosssft for short) is fine-tuned from moss-moon-003base on moss-002-sft-data, which contains 0.57M English and 0.59M Chinese dialogues generated by text-davinci-003, and 0.1M real user instructions (the corresponding response are generated by gpt3.5-turbo) collected during internal test.\nChatGLM ChatGLM-6B (Zeng et al., 2022) is an open bilingual LLM, supporting both Chinese and English. It first completes pre-training on about\n5https://sharegpt.com/\n1T tokens in Chinese and English, and then adds supervised fine-tuning and human feedback reinforcement learning (HFRL) (Ouyang et al., 2022) processes to force model to follow instructions."
        },
        {
            "heading": "A.3 Parameter-efficient Methods",
            "text": "LoRA Low-Rank Adaptation (LoRA) (Hu et al., 2021) injects trainable rank decomposition matrices into each attention layer of the Transformer architecture.\nAdaLoRA AdaLoRA (Zhang et al., 2023b) allocates the parameter budget adaptively to each layer\u2019s LoRA module according to their importance score. Specifically, AdaLoRA parameterizes the incremental updates in the form of singular value decomposition, which allows it to prune the singular values of unimportant updates and reduce their parameter budget.\nPrefix-tuning Inspired by discrete prompts for language models, prefix-tuning (Li and Liang, 2021) adds a sequence of continuous \"virtual tokens\" as a soft prompt (namely prefix) before the original sequences of each transformer layer. During training, prefix weights are trainable while other model parameters are frozen.\nP-tuning Unlike prefix-tuning, p-tuning (Liu et al., 2022) injects trainable continuous tokens into only the embedding layer instead of each layer, resulting in fewer parameters being updated. During training, it freezes partial model parameters.\nPrompt-tuning Similar to p-tuning, prompttuning (Lester et al., 2021) also involves only training the input prompt embeddings. Differently, it freezes all pre-trained weights.\nSequential Adapter For each transformer layer, Series Adapter methods add adapter layers after both attention layers and MLP layers (i.e., SadapterH (Houlsby et al., 2019)), or after MLP layers only (i.e., S-adapterP (Pfeiffer et al., 2020)).\nParallel Adapter Parallel Adapter, namely Padapter (He et al., 2021) adds adapter layers in parallel with attention layers or MLP layers for each transformer layer."
        },
        {
            "heading": "A.4 Chinese Instruction Datasets",
            "text": "AlpacaGPT4 AlpacaGPT4 (Peng et al., 2023) is deemed as an optimized version of Alpaca (Taori et al., 2023) dataset. It uses ChatGPT to translate\nAlpaca\u2019s prompts into Chinese first, and then regenerate these instruction-following data by GPT-4, instead of text-davinci-003.\nBelle Belle (Ji et al., 2023) uses the same method as Alpaca (Taori et al., 2023) to generate instruction data by text-davinci-003, except that Belle only generates Chinese instruction-following data and artificially filters low-quality data. It contains about 1.5M instruction-following data.\nMoss-002-sft-data This is a multi-turn conversation dataset covering helpfulness, honesty, and harmlessness, which is also generated by selfinstruct (Ouyang et al., 2022). We select the 0.59M Chinese conversations among them for the following experiments.\nfirefly Firefly (Yang, 2023) collects 23 Chinese datasets and manually writes several instruction templates for each dataset. It contains a total of 1.65M training samples, covering couplet, poem, essay, and other generation tasks for the performance of traditional literature, and 0.5M Belle data for instruction diversity.\nxP3 xP3 (Muennighoff et al., 2023) is a collection of 16 natural language process datasets across 46 languages with prompts. We select the 3M Chinese instances among them.\ninstinwild Instruction in the Wild (instinwild) (Xue et al., 2023) no longer manually sets initial seed instructions like Alpaca, but crawls and filters 429 instructions from Twitter as the seed instructions, to avoid human involvement and cover more topics. Following self-instruct, it uses the seed instructions to generate more instructions and corresponding responses by text-davinci-003. The Chinese instructions in this dataset are about 52K.\nHC3 HC3 (Guo et al.) is a corpus of HumanChatGPT comparisons that aims to investigate how close ChatGPT is to Human Experts. To this end, it collect about questions from various public question answering datasets (e.g., medicine, law, finance QA) and the corresponding human answers and ChatGPT answers. The Chinese samples in HC3 contain 13K questions, 22K human answers, and 17K Chatgpt answers.\nCOIG COIG (Zhang et al., 2023a) is a Chinese instruction collection, consisting of: Translated instructions contains about 67K instructions which are translated from three datasets: 1.6K task\ndescriptions in Super-NaturalInstructions (Wang et al., 2022) along with a single instance for each of them, 175 instructions of the seed tasks in SelfInstruct, and 66K instructions from Unnatural Instructions (Honovich et al., 2022). CCMC, namely Counterfactual Correction Multi-round Chat, contains about 68K rounds of conversations between students and teachers. This dataset is built by prompting two LLMs to generate conversations based on the entities of knowledge graph dataset CN-DBpedia (Xu et al., 2017) to alleviate the hallucination and factual inconsistency. Exam Instructions contains 63K questions from the main Chinese commonsense tests, e.g., Gaokao, Civil Servant Examination. These questions cover six main subjects: Chinese, English, Politics, Biology, History, and Geology. Exam Instructions contains 34K Chinese samples that present shared human values in the Chinese-speaking world (3K) and regional-culture human values. Table 29 shows some examples in this dataset.\npCLUE pCLUE collects 9 Chinese tasks with a total of 73 different prompts and 1.2M samples. These tasks include 9 Chinese tasks e.g., news classification, natural language reasoning, semantic matching, keyword recognition, reading comprehension, etc.\nTable 25 and Table 26 show representative examples of the above datasets."
        },
        {
            "heading": "B More Experimental Results",
            "text": "B.1 Full results of different LLMs after instruction-tuning.\nTable 10 and 11 show the full Belle-eval and MMCU results after instruction-tuning with LoRA on Alpaca-GPT4 dataset, respectively.\nB.2 Full results of different parameter-efficient methods.\nTable 12 and Table 13 shows the full Belle-eval and MMCU results of instruction-tuning with different parameter-efficient methods, respectively.\nB.3 Full results of instruction-tuning with CoT data.\nTable 14 and 15 show the full Belle-eval and MMCU results of the models instruction-tuned without or with CoT data, respectively. Table 16 shows the detailed results on all subjects in education discipline of MMCU.\nB.4 Full results of LLaMA and its expanded vocabulary versions.\nTable 17 and Table 18 shows the full Belle-eval and MMCU results of LLaMA and its expanded vocabulary versions, respectively.\nB.5 Full results of the comparison of using English and Chinese prompts.\nTable 19 and Table 20 shows the full Belle-eval and MMCU results of using Chinese and English prompts based on LLaMA and Bloom.\nB.6 Full results with human-value alignment\nTable 21 and Table 22 shows the full results of the models instruction-tuned with and without humanvalue alignment data."
        },
        {
            "heading": "C Qualitative Examples",
            "text": ""
        },
        {
            "heading": "C.1 Comparison of responses of Bloom and Bloomz & Bloomz-mt on Belle-eval.",
            "text": "As shown in Table 23, Bloomz & Bloomz-mt tend to generate shorter responses than that of Bloom. Accordingly, ChatGPT rates Bloom higher than Bloomz and Bloomz-mt. We conduct a statistical analysis and find that the average length of Bloom\u2019s response is 481 words, while that of Bloomz and Bloomz-mt are 83 and 58 words.\nC.2 Comparison of different LLMs\u2019 responses on MMCU.\nAs shown in Table 24, all base LLMs fails to generate content in the specified format, i.e., outputting option numbers. Bloomz & Bloomz-mt can directly generate option numbers. Although the generation of ChatGLM mentions the correct answer, but fails to provide the corresponding answer number.\nC.3 Comparison of samples from different instruction datasets.\nIn Table 25 and Table 26, we select a representative sample for each instruction dataset to better understand their respective characteristics.\nC.4 Comparison of the responses from models instruction-tuned on different instruction datasets.\nTo compare the characteristics of models trained on different instruction datasets more intuitively, we present in Table 27 the responses of models instruction-tuned on different datasets for the same question.\nC.5 Comparison of the responses from LLaMA and its expanded vocabulary versions.\nWe present examples of responses from LLaMA and its expanded vocabulary versions in Table 28. The response from \"llama-voc\" is clearly not understanding the meaning of the instruction. Therefore, after expanding the vocabulary, pre-training should be conducted on the vast Chinese corpus before fine-tuning instructions.\nC.6 Examples from human-value alignment dataset.\nThe samples of human-value alignment dataset, built by COIG, are shown in Table 29. These samples are often related to topics such as \"online violence\" and \"gender discrimination\", which are designed to ensure that the model has the correct values when facing relevant topics."
        },
        {
            "heading": "D Experimental details",
            "text": ""
        },
        {
            "heading": "D.1 Experimental Settings.",
            "text": "Our code is modified from library transformers and peft. We will release the codes publicly.\nDuring the training phase, we train the models with 8 A100-80G. A good set of hyperparameters was discovered through experiments: We train models with a linear-warmup learning rate of 5e-4 and batch-size of 512 for 5 epoch (for datasets with less than 100K samples) or 1 epoch (for datasets with over 10K samples). Each sample is truncated to 512 tokens. To improve training efficiency, we load models with 8-bit quantization, except for ChatGLM. We divide 2000 samples into a validation set to observe changes in losses and determine the final checkpoint used.\nDuring the inference phase, we set the max length of the generations to 512, and set temperature=1.0, top_p=0.9, top_k=40, num_beams=10, no_repeat_ngram_size=6, repetition_penalty=1.8. We set float16 precision for inference.\nDuring the evaluation phase, we use ChatGPT with GPT-3.5-turbo-0301 engine to score the models\u2019 generation for Belle-eval. When extracting answers for models\u2019 generation on MMCU, we follow the following steps: 1) If the model does not generate an option number (i.e., A, B, C, D), we\ndetermine the predicted answer by matching the option content that appears in the response. 2) If the model generates the option number, considering that sometimes the model will analyze the content of each option: if all option numbers appear in the response (with different occurrences), we will remove the option numbers that only appear once. Otherwise, we direct use all the option numbers that appears as their final answer."
        },
        {
            "heading": "D.2 Prompts Design.",
            "text": "For the prompts in English, we direct use the same prompt as Alpaca:\n\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n \\n### Instruction:\\n{instruction}\\n \\n### input:\\n{input}\\n\\n### Response:\"\nFor the prompts in Chinese, we design it as follows:\n\"\u4ee5\u4e0b\u662f\u63cf\u8ff0\u4e00\u4e2a\u4efb\u52a1\u7684\u6307\u4ee4\u4ee5\u53ca\u76f8 \u5e94\u7684\u8f93\u5165\uff0c\u8bf7\u6839\u636e\u8981\u6c42\u7ed9\u51fa\u6070\u5f53\u5730\u56de \u590d\u3002\\n\\n ###\u6307\u4ee4\uff1a\\n{instruction}\\n\\n### \u8f93\u5165\uff1a\\n{input}\\n\\n###\u56de\u590d\uff1a\"\nFor the inference on MMCU, we revise the origin prompt as:\n\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{\u8bf7\u9605\u8bfb\u4ee5\n\u4e0b\u9009\u62e9\u9898\u5e76\u7ed9\u51fa\u6b63\u786e\u9009\u9879\uff0c\u4e0d\u8981\u89e3\u91ca\u539f \u56e0\u3002}\\n\\n ### input: \\n{input}\u6b63\u786e\u7b54\u6848\u7684\u5e8f \u53f7\u662f\uff1a\\n\\n### Response:\"\nD.3 Implementation Details of our LLM. Our LLM is trained from Bloom with LoRA. We select a combination of datasets with significant gains on Belle or MMCU, including 10 datasets: Alpaca-GPT4, Belle, ShareGPT-zh, moss-sft-data, installwild, firefly, COIG-trans, pCLUE, and CoT data. To balance the capabilities of our model, we only select 1/3 of moss-sft-data and 1/5 of firefly and pCLUE. The model perform the best with 1.3 epoch instruction-tuning. For the specific prompt, we add a sentence \"\u56de\u7b54\u5c3d\u53ef\u80fd\u8be6\u7ec6\u5177\u4f53\" (\"Answer as detailed and specific as possible\" in Chinese) at the end of the orginal prompts."
        }
    ],
    "title": "An Empirical Study of Instruction-tuning Large Language Models in Chinese",
    "year": 2023
}