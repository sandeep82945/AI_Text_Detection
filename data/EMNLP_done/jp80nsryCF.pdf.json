{
    "abstractText": "Proprietary and closed APIs are becoming increasingly common to process natural language, and are impacting the practical applications of natural language processing, including fewshot classification. Few-shot classification involves training a model to perform a new classification task with a handful of labeled data. This paper presents three contributions. First, we introduce a scenario where the embedding of a pre-trained model is served through a gated API with compute-cost and data-privacy constraints. Second, we propose a transductive inference, a learning paradigm that has been overlooked by the NLP community. Transductive inference, unlike traditional inductive learning, leverages the statistics of unlabeled data. We also introduce a new parameter-free transductive regularizer based on the Fisher-Rao loss, which can be used on top of the gated API embeddings. This method fully utilizes unlabeled data, does not share any label with the third-party API provider and could serve as a baseline for future research. Third, we propose an improved experimental setting and compile a benchmark of eight datasets involving multiclass classification in four different languages, with up to 151 classes. We evaluate our methods using eight backbone models, along with an episodic evaluation over 1,000 episodes, which demonstrate the superiority of transductive inference over the standard inductive setting.",
    "authors": [
        {
            "affiliations": [],
            "name": "Pierre Colombo"
        },
        {
            "affiliations": [],
            "name": "Victor Pellegrain"
        },
        {
            "affiliations": [],
            "name": "Malik Boudiaf"
        },
        {
            "affiliations": [],
            "name": "Victor Storchan"
        },
        {
            "affiliations": [],
            "name": "Myriam Tami"
        },
        {
            "affiliations": [],
            "name": "Ismail Ben Ayed"
        },
        {
            "affiliations": [],
            "name": "Celine Hudelot"
        },
        {
            "affiliations": [],
            "name": "Pablo Piantanida"
        }
    ],
    "id": "SP:6f430f3864d272aa81befe85156ebb09724bcb5d",
    "references": [
        {
            "authors": [
                "Antreas Antoniou",
                "Harrison Edwards",
                "Amos Storkey."
            ],
            "title": "How to train your maml",
            "venue": "arXiv preprint arXiv:1810.09502.",
            "year": 2018
        },
        {
            "authors": [
                "Francesco Barbieri",
                "Jose Camacho-Collados",
                "Luis Espinosa-Anke",
                "Leonardo Neves."
            ],
            "title": "TweetEval:Unified Benchmark and Comparative Evaluation for Tweet Classification",
            "venue": "Proceedings of Findings of EMNLP.",
            "year": 2020
        },
        {
            "authors": [
                "Malik Boudiaf",
                "Hoel Kervadec",
                "Ziko Imtiaz Masud",
                "Pablo Piantanida",
                "Ismail Ben Ayed",
                "Jose Dolz"
            ],
            "title": "Few-shot segmentation without meta-learning: A good transductive inference is all you need",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer",
            "year": 2021
        },
        {
            "authors": [
                "Malik Boudiaf",
                "Imtiaz Ziko",
                "J\u00e9r\u00f4me Rony",
                "Jos\u00e9 Dolz",
                "Pablo Piantanida",
                "Ismail Ben Ayed."
            ],
            "title": "Information maximization for few-shot learning",
            "venue": "Advances in Neural Information Processing Systems, 33:2445\u20132457.",
            "year": 2020
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners. Advances in neural information processing",
            "year": 2020
        },
        {
            "authors": [
                "J-F Cardoso."
            ],
            "title": "Infomax and maximum likelihood for blind source separation",
            "venue": "IEEE Signal processing letters, 4(4):112\u2013114.",
            "year": 1997
        },
        {
            "authors": [
                "I\u00f1igo Casanueva",
                "Tadas Tem\u010dinas",
                "Daniela Gerz",
                "Matthew Henderson",
                "Ivan Vuli\u0107."
            ],
            "title": "Efficient intent detection with dual sentence encoders",
            "venue": "Proceedings of the 2nd Workshop on Natural Language Processing for Conversational AI.",
            "year": 2020
        },
        {
            "authors": [
                "Emile Chapuis",
                "Pierre Colombo",
                "Matteo Manica",
                "Matthieu Labeau",
                "Chlo\u00e9 Clavel."
            ],
            "title": "Hierarchical pre-training for sequence labelling in spoken dialog",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 2636\u20132648,",
            "year": 2020
        },
        {
            "authors": [
                "Mark Chen",
                "Jerry Tworek",
                "Heewoo Jun",
                "Qiming Yuan",
                "Henrique Ponde de Oliveira Pinto",
                "Jared Kaplan",
                "Harri Edwards",
                "Yuri Burda",
                "Nicholas Joseph",
                "Greg Brockman"
            ],
            "title": "Evaluating large language models trained on code",
            "year": 2021
        },
        {
            "authors": [
                "Yanda Chen",
                "Ruiqi Zhong",
                "Sheng Zha",
                "George Karypis",
                "He He."
            ],
            "title": "Meta-learning via language model in-context tuning",
            "venue": "arXiv preprint arXiv:2110.07814.",
            "year": 2021
        },
        {
            "authors": [
                "Meier-Hellstern",
                "Douglas Eck",
                "Jeff Dean",
                "Slav Petrov",
                "Noah Fiedel"
            ],
            "title": "2022. Palm: Scaling language modeling with pathways",
            "year": 2022
        },
        {
            "authors": [
                "Pierre Colombo",
                "Emile Chapuis",
                "Matthieu Labeau",
                "Chlo\u00e9 Clavel."
            ],
            "title": "Code-switched inspired losses for spoken dialog representations",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 8320\u20138337, Online",
            "year": 2021
        },
        {
            "authors": [
                "Pierre Colombo",
                "Emile Chapuis",
                "Matthieu Labeau",
                "Chlo\u00e9 Clavel."
            ],
            "title": "Improving multimodal fusion via mutual dependency maximisation",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 231\u2013245,",
            "year": 2021
        },
        {
            "authors": [
                "Pierre Colombo",
                "Chlo\u00e9 Clavel",
                "Chouchang Yack",
                "Giovanna Varni."
            ],
            "title": "Beam search with bidirectional strategies for neural response generation",
            "venue": "Proceedings of the 4th International Conference on Natural Language and Speech Processing (ICNLSP",
            "year": 2021
        },
        {
            "authors": [
                "Pierre Colombo",
                "Eduardo Dadalto",
                "Guillaume Staerman",
                "Nathan Noiry",
                "Pablo Piantanida."
            ],
            "title": "Beyond mahalanobis distance for textual ood detection",
            "venue": "Advances in Neural Information Processing Systems, 35:17744\u201317759.",
            "year": 2022
        },
        {
            "authors": [
                "Pierre Colombo",
                "Nathan Noiry",
                "Ekhine Irurozki",
                "St\u00e9phan Cl\u00e9men\u00e7on."
            ],
            "title": "What are the best systems? new perspectives on nlp benchmarking",
            "venue": "Advances in Neural Information Processing Systems, 35:26915\u201326932.",
            "year": 2022
        },
        {
            "authors": [
                "Pierre Colombo",
                "Maxime Peyrard",
                "Nathan Noiry",
                "Robert West",
                "Pablo Piantanida."
            ],
            "title": "The glass ceiling of automatic evaluation in natural language generation",
            "venue": "arXiv preprint arXiv:2208.14585.",
            "year": 2022
        },
        {
            "authors": [
                "Pierre Colombo",
                "Pablo Piantanida",
                "Chlo\u00e9 Clavel."
            ],
            "title": "A novel estimator of mutual information for learning to disentangle textual representations",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the",
            "year": 2021
        },
        {
            "authors": [
                "Pierre Colombo",
                "Guillaume Staerman",
                "Chlo\u00e9 Clavel",
                "Pablo Piantanida."
            ],
            "title": "Automatic text evaluation through the lens of Wasserstein barycenters",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages",
            "year": 2021
        },
        {
            "authors": [
                "Pierre Colombo",
                "Guillaume Staerman",
                "Nathan Noiry",
                "Pablo Piantanida."
            ],
            "title": "Learning disentangled textual representations via statistical measures of similarity",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Vol-",
            "year": 2022
        },
        {
            "authors": [
                "Pierre Colombo",
                "Wojciech Witon",
                "Ashutosh Modi",
                "James Kennedy",
                "Mubbasir Kapadia."
            ],
            "title": "Affect-driven dialog generation",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics:",
            "year": 2019
        },
        {
            "authors": [
                "European Commission."
            ],
            "title": "Proposal for a regulation of the european parliament and of the council on european data governance (data governance act)",
            "venue": "COM(2020) 767 final.",
            "year": 2020
        },
        {
            "authors": [
                "Alexis Conneau",
                "Kartikay Khandelwal",
                "Naman Goyal",
                "Vishrav Chaudhary",
                "Guillaume Wenzek",
                "Francisco Guzm\u00e1n",
                "Edouard Grave",
                "Myle Ott",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Unsupervised cross-lingual representation learning at scale",
            "venue": "arXiv",
            "year": 2019
        },
        {
            "authors": [
                "Alexis Conneau",
                "Kartikay Khandelwal",
                "Naman Goyal",
                "Vishrav Chaudhary",
                "Guillaume Wenzek",
                "Francisco Guzm\u00e1n",
                "Edouard Grave",
                "Myle Ott",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Unsupervised cross-lingual representation learning at scale",
            "venue": "Pro-",
            "year": 2020
        },
        {
            "authors": [
                "Thomas M Cover."
            ],
            "title": "Elements of information theory",
            "venue": "John Wiley & Sons.",
            "year": 1999
        },
        {
            "authors": [
                "Ganqu Cui",
                "Shengding Hu",
                "Ning Ding",
                "Longtao Huang",
                "Zhiyuan Liu."
            ],
            "title": "Prototypical verbalizer for prompt-based few-shot tuning",
            "venue": "arXiv preprint arXiv:2203.09770.",
            "year": 2022
        },
        {
            "authors": [
                "Maxime Darrin",
                "Pablo Piantanida",
                "Pierre Colombo."
            ],
            "title": "Rainproof: An umbrella to shield text generators from out-of-distribution data",
            "venue": "arXiv preprint arXiv:2212.09171.",
            "year": 2022
        },
        {
            "authors": [
                "Maxime Darrin",
                "Guillaume Staerman",
                "Eduardo Dadalto C\u00e2mara Gomes",
                "Jackie CK Cheung",
                "Pablo Piantanida",
                "Pierre Colombo."
            ],
            "title": "Unsupervised layer-wise score aggregation for textual ood detection",
            "venue": "arXiv preprint arXiv:2302.09852.",
            "year": 2023
        },
        {
            "authors": [
                "Dorottya Demszky",
                "Dana Movshovitz-Attias",
                "Jeongwoo Ko",
                "Alan Cowen",
                "Gaurav Nemade",
                "Sujith Ravi."
            ],
            "title": "GoEmotions: A Dataset of Fine-Grained Emotions",
            "venue": "58th Annual Meeting of the Association for Computational Linguistics (ACL).",
            "year": 2020
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "arXiv preprint arXiv:1810.04805.",
            "year": 2018
        },
        {
            "authors": [
                "Guneet S Dhillon",
                "Pratik Chaudhari",
                "Avinash Ravichandran",
                "Stefano Soatto."
            ],
            "title": "A baseline for few-shot image classification",
            "venue": "arXiv preprint arXiv:1909.02729.",
            "year": 2019
        },
        {
            "authors": [
                "Ning Ding",
                "Shengding Hu",
                "Weilin Zhao",
                "Yulin Chen",
                "Zhiyuan Liu",
                "Hai-Tao Zheng",
                "Maosong Sun."
            ],
            "title": "Openprompt: An open-source framework for prompt-learning",
            "venue": "arXiv preprint arXiv:2111.01998.",
            "year": 2021
        },
        {
            "authors": [
                "Angela Fan",
                "Shruti Bhosale",
                "Holger Schwenk",
                "Zhiyi Ma",
                "Ahmed El-Kishky",
                "Siddharth Goyal",
                "Mandeep Baines",
                "Onur Celebi",
                "Guillaume Wenzek",
                "Vishrav Chaudhary"
            ],
            "title": "Beyond english-centric multilingual machine translation",
            "venue": "The Journal of Machine",
            "year": 2021
        },
        {
            "authors": [
                "Chelsea Finn",
                "Pieter Abbeel",
                "Sergey Levine."
            ],
            "title": "Model-agnostic meta-learning for fast adaptation of deep networks",
            "venue": "International conference on machine learning, pages 1126\u20131135. PMLR.",
            "year": 2017
        },
        {
            "authors": [
                "Chelsea Finn",
                "Pieter Abbeel",
                "Sergey Levine."
            ],
            "title": "Model-agnostic meta-learning for fast adaptation of deep networks",
            "venue": "Proceedings of the 34th International Conference on Machine Learning.",
            "year": 2017
        },
        {
            "authors": [
                "Tianyu Gao",
                "Adam Fisch",
                "Danqi Chen."
            ],
            "title": "Making pre-trained language models better few-shot learners",
            "venue": "arXiv preprint arXiv:2012.15723.",
            "year": 2020
        },
        {
            "authors": [
                "Tianyu Gao",
                "Xu Han",
                "Zhiyuan Liu",
                "Maosong Sun."
            ],
            "title": "Hybrid attention-based prototypical networks for noisy few-shot relation classification",
            "venue": "Proceedings of the AAAI conference on artificial intelligence, volume 33, pages 6407\u20136414.",
            "year": 2019
        },
        {
            "authors": [
                "Yves Grandvalet",
                "Yoshua Bengio."
            ],
            "title": "Semisupervised learning by entropy minimization",
            "venue": "Advances in neural information processing systems, 17.",
            "year": 2004
        },
        {
            "authors": [
                "Anas Himmi",
                "Ekhine Irurozki",
                "Nathan Noiry",
                "Stephan Clemencon",
                "Pierre Colombo."
            ],
            "title": "Towards more robust nlp system evaluation: Handling missing scores in benchmarks",
            "venue": "arXiv preprint arXiv:2305.10284.",
            "year": 2023
        },
        {
            "authors": [
                "Geoffrey E. Hinton",
                "Oriol Vinyals",
                "Jeffrey Dean."
            ],
            "title": "Distilling the knowledge in a neural network",
            "venue": "ArXiv.",
            "year": 2015
        },
        {
            "authors": [
                "Timothy Hospedales",
                "Antreas Antoniou",
                "Paul Micaelli",
                "Amos Storkey."
            ],
            "title": "Meta-learning in neural networks: A survey",
            "venue": "IEEE transactions on pattern analysis and machine intelligence, 44(9):5149\u20135169.",
            "year": 2021
        },
        {
            "authors": [
                "Neil Houlsby",
                "Andrei Giurgiu",
                "Stanislaw Jastrzebski",
                "Bruna Morrone",
                "Quentin De Laroussilhe",
                "Andrea Gesmundo",
                "Mona Attariyan",
                "Sylvain Gelly."
            ],
            "title": "Parameter-efficient transfer learning for nlp",
            "venue": "International Conference on Machine Learning, pages",
            "year": 2019
        },
        {
            "authors": [
                "Shengding Hu",
                "Ning Ding",
                "Huadong Wang",
                "Zhiyuan Liu",
                "Juanzi Li",
                "Maosong Sun."
            ],
            "title": "Knowledgeable prompt-tuning: Incorporating knowledge into prompt verbalizer for text classification",
            "venue": "arXiv preprint arXiv:2108.02035.",
            "year": 2021
        },
        {
            "authors": [
                "Weihua Hu",
                "Takeru Miyato",
                "Seiya Tokui",
                "Eiichi Matsumoto",
                "Masashi Sugiyama."
            ],
            "title": "Learning discrete representations via information maximizing self-augmented training",
            "venue": "Proceedings of the 34th International Conference on Machine Learning - Vol-",
            "year": 2017
        },
        {
            "authors": [
                "Yuqing Hu",
                "Vincent Gripon",
                "St\u00e9phane Pateux."
            ],
            "title": "Leveraging the feature distribution in transferbased few-shot learning",
            "venue": "Artificial Neural Networks and Machine Learning\u2013ICANN 2021: 30th International Conference on Artificial Neural Net-",
            "year": 2021
        },
        {
            "authors": [
                "Jared Kaplan",
                "Sam McCandlish",
                "Tom Henighan",
                "Tom B Brown",
                "Benjamin Chess",
                "Rewon Child",
                "Scott Gray",
                "Alec Radford",
                "Jeffrey Wu",
                "Dario Amodei."
            ],
            "title": "Scaling laws for neural language models",
            "venue": "arXiv preprint arXiv:2001.08361.",
            "year": 2020
        },
        {
            "authors": [
                "Rabeeh Karimi Mahabadi",
                "Luke Zettlemoyer",
                "James Henderson",
                "Marzieh Saeidi",
                "Lambert Mathias",
                "Veselin Stoyano",
                "Majid Yazdani."
            ],
            "title": "Perfect: Prompt-free and efficient few-shot learning with language models",
            "venue": "Annual Meeting of the Association",
            "year": 2022
        },
        {
            "authors": [
                "Phillip Keung",
                "Yichao Lu",
                "Gy\u00f6rgy Szarvas",
                "Noah A. Smith."
            ],
            "title": "The multilingual Amazon reviews corpus",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP).",
            "year": 2020
        },
        {
            "authors": [
                "Marcin Oleksy",
                "Maciej Piasecki",
                "\u0141ukasz Radli\u0144ski",
                "Konrad Wojtasik",
                "Stanis\u0142aw Wo\u017aniak",
                "Przemys\u0142aw Kazienko"
            ],
            "title": "Chatgpt: Jack of all trades, master of none",
            "year": 2023
        },
        {
            "authors": [
                "Zhenzhong Lan",
                "Mingda Chen",
                "Sebastian Goodman",
                "Kevin Gimpel",
                "Piyush Sharma",
                "Radu Soricut."
            ],
            "title": "Albert: A lite bert for self-supervised learning of language representations",
            "venue": "arXiv preprint arXiv:1909.11942.",
            "year": 2019
        },
        {
            "authors": [
                "Stefan Larson",
                "Anish Mahendran",
                "Joseph J. Peper",
                "Christopher Clarke",
                "Andrew Lee",
                "Parker Hill",
                "Jonathan K. Kummerfeld",
                "Kevin Leach",
                "Michael A. Laurenzano",
                "Lingjia Tang",
                "Jason Mars"
            ],
            "title": "An evaluation dataset for intent classification and out-of",
            "year": 2019
        },
        {
            "authors": [
                "Kwonjoon Lee",
                "Subhransu Maji",
                "Avinash Ravichandran",
                "Stefano Soatto."
            ],
            "title": "Meta-learning with differentiable convex optimization",
            "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10657\u201310665.",
            "year": 2019
        },
        {
            "authors": [
                "Eric Lehman",
                "Evan Hernandez",
                "Diwakar Mahajan",
                "Jonas Wulff",
                "Micah J. Smith",
                "Zachary Ziegler",
                "Daniel Nadler",
                "Peter Szolovits",
                "Alistair Johnson",
                "Emily Alsentzer"
            ],
            "title": "Do we still need clinical language models",
            "year": 2023
        },
        {
            "authors": [
                "Brian Lester",
                "Rami Al-Rfou",
                "Noah Constant."
            ],
            "title": "The power of scale for parameter-efficient prompt tuning",
            "venue": "arXiv preprint arXiv:2104.08691.",
            "year": 2021
        },
        {
            "authors": [
                "Quentin Lhoest",
                "Albert Villanova del Moral",
                "Yacine Jernite",
                "Abhishek Thakur",
                "Patrick von Platen",
                "Suraj Patil",
                "Julien Chaumond",
                "Mariama Drame",
                "Julien Plu",
                "Lewis Tunstall"
            ],
            "title": "Datasets: A community library for natural language processing",
            "year": 2021
        },
        {
            "authors": [
                "Moshe Lichtenstein",
                "Prasanna Sattigeri",
                "Rogerio Feris",
                "Raja Giryes",
                "Leonid Karlinsky."
            ],
            "title": "Tafssl: Task-adaptive feature sub-space learning for fewshot classification",
            "venue": "Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August",
            "year": 2020
        },
        {
            "authors": [
                "Ralph Linsker."
            ],
            "title": "Self-organization in a perceptual network",
            "venue": "Computer, 21(3):105\u2013117.",
            "year": 1988
        },
        {
            "authors": [
                "Haokun Liu",
                "Derek Tam",
                "Mohammed Muqeeth",
                "Jay Mohta",
                "Tenghao Huang",
                "Mohit Bansal",
                "Colin Raffel."
            ],
            "title": "Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning",
            "venue": "arXiv preprint arXiv:2205.05638.",
            "year": 2022
        },
        {
            "authors": [
                "Pengfei Liu",
                "Weizhe Yuan",
                "Jinlan Fu",
                "Zhengbao Jiang",
                "Hiroaki Hayashi",
                "Graham Neubig."
            ],
            "title": "Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing",
            "venue": "ACM Computing Surveys, 55(9):1\u201335.",
            "year": 2023
        },
        {
            "authors": [
                "Yanbin Liu",
                "Juho Lee",
                "Minseop Park",
                "Saehoon Kim",
                "Eunho Yang",
                "Sung Ju Hwang",
                "Yi Yang."
            ],
            "title": "Learning to propagate labels: Transductive propagation network for few-shot learning",
            "venue": "ICLR.",
            "year": 2019
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Roberta: A robustly optimized bert pretraining approach",
            "venue": "arXiv preprint arXiv:1907.11692.",
            "year": 2019
        },
        {
            "authors": [
                "Robert L Logan IV",
                "Ivana Bala\u017eevi\u0107",
                "Eric Wallace",
                "Fabio Petroni",
                "Sameer Singh",
                "Sebastian Riedel."
            ],
            "title": "Cutting down on prompts and parameters: Simple few-shot learning with language models",
            "venue": "arXiv preprint arXiv:2106.13353.",
            "year": 2021
        },
        {
            "authors": [
                "Rabeeh Karimi Mahabadi",
                "Luke Zettlemoyer",
                "James Henderson",
                "Marzieh Saeidi",
                "Lambert Mathias",
                "Veselin Stoyanov",
                "Majid Yazdani."
            ],
            "title": "Perfect: Prompt-free and efficient few-shot learning with language models",
            "venue": "arXiv preprint arXiv:2204.01172.",
            "year": 2022
        },
        {
            "authors": [
                "Sewon Min",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Hannaneh Hajishirzi."
            ],
            "title": "Metaicl: Learning to learn in context",
            "venue": "arXiv preprint arXiv:2110.15943.",
            "year": 2021
        },
        {
            "authors": [
                "Sewon Min",
                "Xinxi Lyu",
                "Ari Holtzman",
                "Mikel Artetxe",
                "Mike Lewis",
                "Hannaneh Hajishirzi",
                "Luke Zettlemoyer"
            ],
            "title": "Rethinking the role of demonstrations: What makes in-context learning work? arXiv preprint arXiv:2202.12837",
            "year": 2022
        },
        {
            "authors": [
                "Nikhil Mishra",
                "Mostafa Rohaninejad",
                "Xi Chen",
                "P. Abbeel."
            ],
            "title": "A simple neural attentive metalearner",
            "venue": "International Conference on Learning Representations.",
            "year": 2017
        },
        {
            "authors": [
                "Niklas Muennighoff",
                "Nouamane Tazi",
                "Lo\u00efc Magne",
                "Nils Reimers."
            ],
            "title": "Mteb: Massive text embedding benchmark",
            "venue": "arXiv preprint arXiv:2210.07316.",
            "year": 2022
        },
        {
            "authors": [
                "Boris N. Oreshkin",
                "Pau Rodriguez",
                "Alexandre Lacoste."
            ],
            "title": "Tadam: Task dependent adaptive metric for improved few-shot learning",
            "venue": "Proceedings of the 32nd International Conference on Neural Information Processing Systems, page 719\u2013729.",
            "year": 2018
        },
        {
            "authors": [
                "Ethan Perez",
                "Douwe Kiela",
                "Kyunghyun Cho."
            ],
            "title": "True few-shot learning with language models",
            "venue": "Advances in neural information processing systems, 34:11054\u201311070.",
            "year": 2021
        },
        {
            "authors": [
                "Jonas Pfeiffer",
                "Andreas R\u00fcckl\u00e9",
                "Clifton Poth",
                "Aishwarya Kamath",
                "Ivan Vuli\u0107",
                "Sebastian Ruder",
                "Kyunghyun Cho",
                "Iryna Gurevych."
            ],
            "title": "Adapterhub: A framework for adapting transformers",
            "venue": "arXiv preprint arXiv:2007.07779.",
            "year": 2020
        },
        {
            "authors": [
                "Georg Pichler",
                "Pierre Jean A Colombo",
                "Malik Boudiaf",
                "G\u00fcnther Koliander",
                "Pablo Piantanida."
            ],
            "title": "A differential entropy estimator for training neural networks",
            "venue": "International Conference on Machine Learning, pages 17691\u201317715. PMLR.",
            "year": 2022
        },
        {
            "authors": [
                "Marine Picot",
                "Francisco Messina",
                "Malik Boudiaf",
                "Fabrice Labeau",
                "Ismail Ben Ayed",
                "Pablo Piantanida."
            ],
            "title": "Adversarial robustness via fisher-rao regularization",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 45(3):2698\u20132710.",
            "year": 2023
        },
        {
            "authors": [
                "Marine Picot",
                "Nathan Noiry",
                "Pablo Piantanida",
                "Pierre Colombo"
            ],
            "title": "Adversarial attack detection under realistic constraints",
            "year": 2022
        },
        {
            "authors": [
                "Marine Picot",
                "Guillaume Staerman",
                "Federica Granese",
                "Nathan Noiry",
                "Francisco Messina",
                "Pablo Piantanida",
                "Pierre Colombo"
            ],
            "title": "2022b. A simple unsupervised data depth-based method to detect adversarial images",
            "year": 2022
        },
        {
            "authors": [
                "Alec Radford",
                "Jeff Wu",
                "Rewon Child",
                "David Luan",
                "Dario Amodei",
                "Ilya Sutskever"
            ],
            "title": "Language models are unsupervised multitask learners",
            "year": 2019
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J Liu."
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "The Journal of Machine Learning Research,",
            "year": 2020
        },
        {
            "authors": [
                "Aniruddh Raghu",
                "Maithra Raghu",
                "Samy Bengio",
                "Oriol Vinyals."
            ],
            "title": "Rapid learning or feature reuse? towards understanding the effectiveness of maml",
            "venue": "arXiv preprint arXiv:1909.09157.",
            "year": 2019
        },
        {
            "authors": [
                "Nils Reimers",
                "Iryna Gurevych."
            ],
            "title": "SentenceBERT: Sentence embeddings using Siamese BERTnetworks",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on",
            "year": 2019
        },
        {
            "authors": [
                "Jonathan S Rosenfeld",
                "Amir Rosenfeld",
                "Yonatan Belinkov",
                "Nir Shavit."
            ],
            "title": "A constructive prediction of the generalization error across scales",
            "venue": "arXiv preprint arXiv:1909.12673.",
            "year": 2019
        },
        {
            "authors": [
                "Andrei Rusu",
                "Dushyant Rao",
                "Jakub Sygnowski",
                "Oriol Vinyals",
                "Razvan Pascanu",
                "Simon Osindero",
                "Raia Hadsell"
            ],
            "title": "Meta-learning with latent embedding optimization",
            "year": 2019
        },
        {
            "authors": [
                "Stephan R Sain"
            ],
            "title": "The nature of statistical learning theory",
            "year": 1996
        },
        {
            "authors": [
                "Timo Schick",
                "Hinrich Sch\u00fctze."
            ],
            "title": "Exploiting cloze questions for few shot text classification and natural language inference",
            "venue": "arXiv preprint arXiv:2001.07676.",
            "year": 2020
        },
        {
            "authors": [
                "Timo Schick",
                "Hinrich Sch\u00fctze."
            ],
            "title": "It\u2019s not just size that matters: Small language models are also few-shot learners",
            "venue": "arXiv preprint arXiv:2009.07118.",
            "year": 2020
        },
        {
            "authors": [
                "Timo Schick",
                "Hinrich Sch\u00fctze."
            ],
            "title": "True fewshot learning with prompts\u2014a real-world perspective",
            "venue": "Transactions of the Association for Computational Linguistics, 10:716\u2013731.",
            "year": 2022
        },
        {
            "authors": [
                "Jake Snell",
                "Kevin Swersky",
                "Richard Zemel."
            ],
            "title": "Prototypical networks for few-shot learning",
            "venue": "Advances in neural information processing systems, 30.",
            "year": 2017
        },
        {
            "authors": [
                "Irene Solaiman."
            ],
            "title": "The gradient of generative ai release: Methods and considerations",
            "venue": "arXiv preprint arXiv:2302.04844.",
            "year": 2023
        },
        {
            "authors": [
                "Kaitao Song",
                "Xu Tan",
                "Tao Qin",
                "Jianfeng Lu",
                "TieYan Liu."
            ],
            "title": "Mpnet: Masked and permuted pretraining for language understanding",
            "venue": "Advances in Neural Information Processing Systems, 33:16857\u2013 16867.",
            "year": 2020
        },
        {
            "authors": [
                "Yisheng Song",
                "Ting Wang",
                "Puyu Cai",
                "Subrota K Mondal",
                "Jyoti Prakash Sahoo."
            ],
            "title": "A comprehensive survey of few-shot learning: Evolution, applications, challenges, and opportunities",
            "venue": "ACM Computing Surveys.",
            "year": 2022
        },
        {
            "authors": [
                "Emma Strubell",
                "Ananya Ganesh",
                "Andrew McCallum"
            ],
            "title": "Energy and policy considerations for deep learning in nlp",
            "year": 2019
        },
        {
            "authors": [
                "Qianru Sun",
                "Yaoyao Liu",
                "Tat-Seng Chua",
                "Bernt Schiele."
            ],
            "title": "Meta-transfer learning for few-shot learning",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 403\u2013412.",
            "year": 2019
        },
        {
            "authors": [
                "Shengli Sun",
                "Qingfeng Sun",
                "Kevin Zhou",
                "Tengchao Lv."
            ],
            "title": "Hierarchical attention prototypical networks for few-shot text classification",
            "venue": "Proceedings of the 2019 conference on empirical methods in natural language processing and the 9th interna-",
            "year": 2019
        },
        {
            "authors": [
                "Flood Sung",
                "Yongxin Yang",
                "Li Zhang",
                "Tao Xiang",
                "Philip Torr",
                "Timothy Hospedales"
            ],
            "title": "Learning to compare: Relation network for few-shot learning",
            "year": 2018
        },
        {
            "authors": [
                "Flood Sung",
                "Yongxin Yang",
                "Li Zhang",
                "Tao Xiang",
                "Philip HS Torr",
                "Timothy M Hospedales."
            ],
            "title": "Learning to compare: Relation network for few-shot learning",
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition, pages",
            "year": 2018
        },
        {
            "authors": [
                "Derek Tam",
                "Rakesh R Menon",
                "Mohit Bansal",
                "Shashank Srivastava",
                "Colin Raffel."
            ],
            "title": "Improving and simplifying pattern exploiting training",
            "venue": "arXiv preprint arXiv:2103.11955.",
            "year": 2021
        },
        {
            "authors": [
                "Wilson L Taylor."
            ],
            "title": "cloze procedure\u201d: A new tool for measuring readability",
            "venue": "Journalism quarterly, 30(4):415\u2013433.",
            "year": 1953
        },
        {
            "authors": [
                "Hugo Touvron",
                "Thibaut Lavril",
                "Gautier Izacard",
                "Xavier Martinet",
                "Marie-Anne Lachaux",
                "Timoth\u00e9e Lacroix",
                "Baptiste Rozi\u00e8re",
                "Naman Goyal",
                "Eric Hambro",
                "Faisal Azhar"
            ],
            "title": "Llama: Open and efficient foundation language models",
            "year": 2023
        },
        {
            "authors": [
                "Vladimir N Vapnik."
            ],
            "title": "An overview of statistical learning theory",
            "venue": "IEEE transactions on neural networks, 10(5):988\u2013999.",
            "year": 1999
        },
        {
            "authors": [
                "Oriol Vinyals",
                "Charles Blundell",
                "Timothy Lillicrap",
                "koray kavukcuoglu",
                "Daan Wierstra"
            ],
            "title": "Matching networks for one shot learning",
            "venue": "In Advances in Neural Information Processing Systems",
            "year": 2016
        },
        {
            "authors": [
                "Wenhui Wang",
                "Furu Wei",
                "Li Dong",
                "Hangbo Bao",
                "Nan Yang",
                "Ming Zhou."
            ],
            "title": "Minilm: Deep selfattention distillation for task-agnostic compression of pre-trained transformers",
            "venue": "Advances in Neural Information Processing Systems, 33:5776\u20135788.",
            "year": 2020
        },
        {
            "authors": [
                "Jason Wei",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Maarten Bosma",
                "Ed Chi",
                "Quoc Le",
                "Denny Zhou."
            ],
            "title": "Chain of thought prompting elicits reasoning in large language models",
            "venue": "arXiv preprint arXiv:2201.11903.",
            "year": 2022
        },
        {
            "authors": [
                "Zhilin Yang",
                "Zihang Dai",
                "Yiming Yang",
                "Jaime Carbonell",
                "Russ R Salakhutdinov",
                "Quoc V Le."
            ],
            "title": "Xlnet: Generalized autoregressive pretraining for language understanding",
            "venue": "Advances in neural information processing systems, 32.",
            "year": 2019
        },
        {
            "authors": [
                "Qinyuan Ye",
                "Bill Yuchen Lin",
                "Xiang Ren."
            ],
            "title": "Crossfit: A few-shot learning challenge for cross-task generalization in nlp",
            "venue": "arXiv preprint arXiv:2104.08835.",
            "year": 2021
        },
        {
            "authors": [
                "Imtiaz Ziko",
                "Jose Dolz",
                "Eric Granger",
                "Ismail Ben Ayed."
            ],
            "title": "Laplacian regularized few-shot learning",
            "venue": "International conference on machine learning, pages 11660\u201311670. PMLR.",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Recent advances in Natural Language Processing (NLP) have been largely driven by the scaling paradigm (Kaplan et al., 2020; Rosenfeld et al., 2019), where larger models with increased parameters have been shown to achieve state-of-the-art results in various NLP tasks (Touvron et al., 2023; Radford et al., 2019). This approach has led to the development of foundation models such as ChatGPT (Lehman et al., 2023; Kocon\u0301 et al., 2023;\n*These authors contributed equally to this work\nBrown et al., 2020), GPT-4 (OpenAI, 2023), GPT3 (Brown et al., 2020), T5 (Raffel et al., 2020), and BERT (Devlin et al., 2018), which have achieved unprecedented performance in text classification (Liu et al., 2019b), language modeling, machine translation (Fan et al., 2021), and coding tasks (Chen et al., 2021a).\nDespite the success of the scaling paradigm, significant challenges still exist especially when the many practical constraints of real-world scenarios have to be met: labeled data can be severely limited (i.e., few-shot scenario (Song et al., 2022; Ye et al., 2021)), data privacy is critical for many industries and has become the subject of increasingly many regulatory pieces (Commission, 2020, 2016), compute costs need to be optimized (Strubell et al., 2019). Furthermore, these challenges are made even more complex as stronger foundation models are now available only through APIs (e.g., OpenAI\u2019s GPT-3, GPT-4 or ChatGPT, Anthropic\u2019s Claude or Google\u2019s PaLM (Chowdhery et al., 2022)) which has led to some of their parameters being concealed, presenting new challenges for model adaptation (Solaiman, 2023). This paper is centered on the fundamental task of fewshot text classification, specifically focusing on cloud-based/API access. Specifically, we formulate three requirements for API-based few-shot learning (FSL) (see Fig. 1): (R1) Black-box scenario. We focus on learning from models that are opaquely deployed in production to the end-user, who only has access to the end-point of the encoder, i.e., the resulting text embedding produced by the final layer of the network. (R2) Low resources / computation time. AI systems are often required to make rapid predictions at high frequencies in various real-world applications. Therefore, any few-shot classifier used in such scenarios should have a low training and inference time, as well as require minimal computational resources.\n(R3) Limited Data Sharing. When utilizing API models, data sharing becomes a major concern. In the current landscape, providers are increasingly offering less transparent procedures for training their networks. As a result, users prefer sharing as little information as possible, such as labeling schema and annotated data, to safeguard their data privacy. Shortcomings of Existing Works. While numerous previous studies have addressed the popular few-shot classification setting, to our knowledge no existing line of work adequately satisfies the three API requirements described above. In particular, prompt-based FSL (Schick and Sch\u00fctze, 2020a) and parameter-efficient fine-tuning FSL (Houlsby et al., 2019) both require access to the model\u2019s gradients, while in-context learning scales poorly with the task\u2019s size (e.g number of shots, number of classes) (Chen et al., 2021b; Min et al., 2021, 2022; Brown et al., 2020) and requires full data sharing. Instead, we focus on methods that can operate within API-based constraints.\nUnder R1, R2, and R3 requirements, the standard inductive learning (Liu et al., 2022) may be quite limiting. To mitigate the labeled data scarcity while retaining API compliance, we revisit transduction (Vapnik, 1999) in the context of textual few-shot classification. Specifically, in the context of FSL, transductive FSL (Liu et al., 2019a) advocates leveraging unlabeled test samples of a task as an additional source of information on the underlying task\u2019s data distribution in order to better define decision boundaries. Such additional source essentially comes for free in many offline applications, including sentiment analysis for customer feedback, legal document classification, or text-based medical diagnosis.\nOur findings corroborate recent findings in computer vision (Liu et al., 2019a; Ziko et al., 2020; Lichtenstein et al., 2020; Boudiaf et al., 2020; Hu et al., 2021b), that substantial gains can be obtained from using transduction over induction, opening new avenue of research for the NLP community. However, the transductive gain comes at the cost of introducing additional hyperparameters, and carefully tuning them. Motivated by Occam\u2019s razor principle, we propose a novel hyperparameter-free transductive regularizer based on Fisher-Rao distances and demonstrate the strongest predictive performances across various benchmarks and models while keeping hyperparameter tuning minimal. We believe that this parameter-free transductive regu-\nlarizer can serve as a baseline for future research.\nContributions\nIn this paper, we make several contributions to the field of textual FSL. Precisely, our contributions are threefold: A new textual few-shot scenario: We present a new scenario for FSL using textual API-based models that accurately capture real-world constraints. Our new scenario opens up new research avenues and opportunities to address the challenges associated with FSL using API-based models, paving the way for improved performance in practical applications. A novel transductive baseline. Our paper proposes a transductive FSL algorithm that utilizes a novel parameter-free Fisher-Rao-based loss. By leveraging only the network\u2019s embedding (R1), our approach enables fast and efficient predictions (R2) without the need to share the labeling schema or the labels of few-shot examples making it compliant with (R3). This innovative method marks a significant step forward in the field of FSL. A truly improved experimental setting. Previous studies on textual few-shot classification (Schick and Sch\u00fctze, 2022, 2020b; Mahabadi et al., 2022; Tam et al., 2021; Gao et al., 2020) have predominantly assessed their algorithms on classification tasks with a restricted number of labels (typically less than five). We take a step forward and create a benchmark that is more representative of realworld scenarios. Our benchmark relies on a total of eight datasets, covering multiclass classification tasks with up to 151 classes, across four different languages. Moreover, we further enhanced the evaluation process by not only considering 10 classifiers trained with 10 different seeds (Logan IV et al., 2021; Mahabadi et al., 2022), but also by relying on episodic evaluation on 1,000 episodes (Hospedales et al., 2021). Our results clearly demonstrate the superiority of transductive methods."
        },
        {
            "heading": "2 Related Work",
            "text": ""
        },
        {
            "heading": "2.1 Few-shot learning in NLP",
            "text": "Numerous studies have tackled the task of FSL in Natural Language Processing (NLP) by utilizing pre-trained language models (Devlin et al., 2018; Liu et al., 2019b; Radford et al., 2019; Yang et al., 2019). These methods can be classified into three major categories: prompt-based, parameterefficient tuning, and in-context learning.\nXQ ZS \u0302YS \u0302YQ XS YS ZQ YS f\u03b8\nBlack-box access to the encoder\ng\u03d5\nLimited Data Sharing: labelling schema and labels are not shared\nLow ressource scenario\nR3\nR1 R2\nUser-only access\nAccessible by both user and api-provider\nApi-provider-only access\nLocal trainingNo training possible\nFigure 1: API-based FSL scenario. The black-box API provides embeddings from the pretrained encoder f\u03b8. The black-box scenario discards existing inductive approaches and in-context learning methods due to the inaccessible of the model\u2019s parameters ((R1)) and privacy concerns ((R3)). This scenario, allows tuning a classification head g\u03d5 (using induction or transduction) at low computational cost (R2) while retaining all support labels locally.\nPrompt-based FSL: Prompt-based FSL involves the use of natural language prompts or templates to guide the model to perform a specific task (Ding et al., 2021; Liu et al., 2023). For example, the seminal work (Schick and Sch\u00fctze, 2020a) proposed a model called PET, which uses a pre-defined set of prompts to perform various NLP tasks as text classification. They also impose a choice of a verbalizer which highly impacts the classification performances (Cui et al., 2022; Hu et al., 2021a). However, recent studies have questioned the benefits of prompt-based learning due to the high variability in performance caused by the choice of prompt (Liu et al., 2022). To address this issue, researchers have proposed prompt tuning which involves a few learnable parameters in addition to the prompt (Lester et al., 2021). Nevertheless, these approaches face limitations when learning from API: (i) encoder access for gradient computation is infeasible (as in R1), (ii) prompting requires to send data and label which raises privacy concerns (as in R3), and (iii) labeling new points is time-consuming (see in R3) and expensive due to the need to send all shots for each input token1. Parameter-efficient fine-tuning. These methods, such as adapters (Houlsby et al., 2019; Pfeiffer et al., 2020), keep most of the model\u2019s parameters fixed during training and only update small feedforward networks that are inserted within the larger model architecture. A recent example is T-FEW (Liu et al., 2022), which adds learned vectors that rescale the network\u2019s internal activations. Additionally, it requires a set of manually created prompts for each dataset making it hard to use in practice. Relying on parameter-efficient fine-tuning methods with an API is not possible due to the need to com-\n1The cost of API queries is determined by the number of input tokens that are transmitted.\npute gradients of the encoder (as per R1) and the requirement to send both the labeling schema and the labels, which violates R3. In Context Learning (ICL). In-context learning models are models that utilize input-to-output training examples as prompts to make predictions, without any parameter updates (Wei et al., 2022). These models, such as text-davinci, rely solely on the provided examples to generate predictions, without any additional training. However, a significant drawback of this approach is that the user must supply the input, label examples, and task description, which becomes prohibitively expensive when the number of classes or shots increases, is slow (Liu et al., 2022) (R2) and raises data privacy concerns (as highlighted in R3). Additionally, the inability to reuse text embeddings for new tasks or with new labels without querying the model\u2019s API limits practicality and scalability, making reusable encoding unfeasible for in-context learning models2. Meta-learning. Meta-learning approaches have for quite long stood as the de-facto paradigm for FSL ((Snell et al., 2017; Rusu et al., 2019; Sung et al., 2018b; Lee et al., 2019; Raghu et al., 2019; Sun et al., 2019a)). In meta-learning, the objective is to provide the model with the intrinsic ability to learn in a data-efficient manner. For instance, MAML ((Finn et al., 2017a; Antoniou et al., 2018)), arguably the most popular meta-learning method, tries to train a model such that it can be fine-tuned end-to-end using only a few supervised samples while retaining high generalization ability. Unlike the three previous lines of work, meta-learning methods operate by modifying the pre-training pro-\n2Furthermore, as the number of considered classes increases, the fixed size of the transformer limits the number of possible shots that can be fed to the model. Previous studies have often neglected this limitation by focusing on a few numbers of labels.\ncedure and therefore assume access to both the training data and the model, which wholly breaks both R1 and R3."
        },
        {
            "heading": "2.2 Inductive vs transductive learning",
            "text": "Learning an inductive classifier on embeddings generated by an API-based model, as proposed by (Snell et al., 2017), is a common baseline for performing FSL. This approach is prevalent in NLP, where a parametric model is trained on data to infer general rules that are applied to label new, unseen data (known as inductive learning (Vapnik, 1999)). However, in FSL scenarios with limited labeled data, this approach can be highly ambiguous and lead to poor generalization.\nTransduction offers an attractive alternative to inductive learning (Sain, 1996). Unlike inductive learning, which infers general rules from training data, transduction involves finding rules that work specifically for the unlabeled test data. By utilizing more data, such as unlabeled test instances, and aiming for a more localized rule rather than a general one, transductive learning has shown promise and practical benefits in computer vision (Boudiaf et al., 2020, 2021; Ziko et al., 2020). Transductive methods yield substantially better performance than their inductive counterparts by leveraging the statistics of the query set (Dhillon et al., 2019). However, this approach has not yet been explored in the context of textual data."
        },
        {
            "heading": "3 API-based Few-shot Learning",
            "text": ""
        },
        {
            "heading": "3.1 Problem Statement",
            "text": "Let \u2126 be the considered vocabulary, we denote \u2126\u2217 its Kleene closure. The Kleene closure corresponds to sequences of arbitrary size written with tokens in\n\u2126, i.e., \u2126\u2217 = \u221e\u22c3 i=0 \u2126i. Given an input space X with X \u2286 \u2126\u2217 and a latent space Z , we consider a pretrained backbone model f\u03b8 : X \u2192 Z = Rd, where \u03b8 \u2208 \u0398 represents the parameters of the encoder and d is the embedding dimension size. In the API-based setting, we assume that we are unable to access the exact structure of f\u03b8 as mentioned in R1. However, we do have access to the last encoder embedding which is available for our use (see R1).\nThe objective of few-shot classification is to learn a classifier from limited labeled data and generalize it to new, unseen tasks or classes. To accomplish this, randomly sampled few-shot tasks are created from a test dataset Dtest := {(xi, yi)}Ntesti=1\nthat has a set of unseen classes Ytest. Each task involves a few labeled examples from K different classes chosen at random among Ytest. These labeled examples constitute the support set S = {xi, yi}i\u2208IS , with a size of |S| = NS \u00d7 K. Additionally, each task has an unlabeled query set Q = {xi}i\u2208IQ composed of |Q| = NQ \u00d7 K unseen examples from each of the K classes. IS and IQ represent the drawn indices during the sampling process for support set and query set, respectively. Pre-trained models use few-shot techniques and the labeled support sets to adapt to the tasks at hand and are evaluated based on their performances on the unlabeled query sets.\nRemark Setting the values of N and K in textual FSL is not standardized, as discussed in Sec. 3.1. Therefore, in all of our experiments, we have relied on setting (N,K) \u2208 {5, 10}2."
        },
        {
            "heading": "3.2 Proposed Transductive Method",
            "text": "NLP few-shot classifiers rely only on inductive inference, while computer vision has shown significant performance improvements using transductive inference for FSL. Transductive inference succeeds in FSL because it jointly classifies all unlabeled query samples of a single task, leading to more efficient and accurate classification compared to inductive methods that classify one sample at a time. Let us begin by introducing some basic notation and definitions before introducing our new transductive loss based on the Fisher-Rao distance.\nIn the API-based few-shot classification setting, our goal is to train a classification head g\u03d5 : Z \u2192 RK that maps the feature representations to the posterior distribution space for making predictions. To simplify the equations for the rest of the paper, we use the following notations for the posterior predictions of each i \u2208 IS \u222a IQ and for the class marginals within Q: pik = g\u03d5(f\u03b8(xi))k = P(Y = k|X = xi; \u03b8, \u03d5) and p\u0302k = 1 |Q| \u2211\ni\u2208IQ pik = P(YQ = k; \u03b8, \u03d5) where X and Y are the r.v.s associated with the raw features and labels, respectively, and where YQ means restriction of the r.v. Y to set Q.\nFor training the classification head in the transductive setting, prior research aims at finding \u03d5 such that \u03d5 = argminCE \u2212 \u03bb \u00d7 RQ3, with CE:= \u2212 1|S| \u2211 i\u2208IS \u2211K k=1 yik log(pik) being the cross-entropy supervision on the support set (in which yik is the kth coordinate of the one-hot en-\n3\u03bb is set to 1 in all the experiments.\ncoded label vector associated to sample i) and RQ being a transductive loss on the query set Q.\nNote that this transductive regularization has been proposed in the literature based on the InfoMax principle (Cardoso, 1997; Linsker, 1988), and the inductive loss can be found by setting \u03bb = 0. In what follows, we review the regularizers introduced in previous work.\nEntropic Minimization (H) An effective regularizer for transductive FSL can be derived from the field of semi-supervised learning, drawing inspiration from the approach introduced in (Grandvalet and Bengio, 2004). This regularizer, proposed in (Dhillon et al., 2019), utilizes the conditional Shannon Entropy (Cover, 1999) of forecast results from query samples during testing to enhance model generalization. Formally:\nRHQ = 1 |Q| \u2211 i\u2208IQ K\u2211 k=1 pik log(pik). (1)\nMutual Information Maximization (I) A promising alternative to entropic minimization for addressing the challenges of transductive FSL is to adopt the Info-max principle. (Boudiaf et al., 2020) extended this idea, introduced in (Hu et al., 2017), and propose as regularizer a surrogate of the mutualinformation RIQ(\u03b1) =:\n\u2212 K\u2211 k=1 p\u0302k log p\u0302k +\u03b1 1 |Q| \u2211 i\u2208IQ K\u2211 k=1 pik log(pik). (2)\nLimitation of existing strategies: Despite its effectiveness, the previous method has a few limitations that should be taken into account. One of these limitations is the need to fine-tune the weight of different entropies using the hyperparameter \u03b1. This parameter-tuning process can be time-consuming and may require extensive experimentation to achieve optimal results. Additionally, recent studies have shown that relying solely on the first Entropic term, which corresponds to the Entropic minimization scenario in Equation 1, can lead to suboptimal performance in FSL."
        },
        {
            "heading": "3.3 A Fisher-Rao Based Regularizer",
            "text": "In the FSL scenario, minimizing parameter tuning is crucial. Motivated by this, in this section, we introduce a new parameter-free transductive regularizer that fits into the InfoMax framework. Additionally, our loss inherits the attractive properties of\nthe Fisher-Rao distance between soft-predictions q := (q1, . . . , qK) and p := (p1, . . . , pK), which is given by (Picot et al., 2023):\ndFR(q,p) := 2 arccos ( K\u2211 k=1 \u221a qk \u00d7 pk ) . (3)\nThe proposed transductive regularizer denoted by RFRQ , for each single few-shot task, can be described as measuring the Fisher-Rao distance between pairs of query samples:\nRFRQ := 1 |Q| \u2211 i\u2208IQ \u2212 log \u2211 j\u2208IQ K\u2211 k=1 \u221a pik \u00d7 pjk (4)\n= 1 |Q| \u2211 i\u2208IQ \u2212 log \u2211 j\u2208IQ cos ( dFR(pi,pj) 2 ) , (5)\nwhere dFR(pi,pj) is the Fisher-Rao distance between pairs of soft-predictions (pi,pj). Furthermore, it is shown that expression (4) yields a surrogate of the Mutual Information as shown by the following proposition. This result to the best of our knowledge is new, as far as we can tell.\nTheorem 1. (Fisher-Rao as a surrogate to maximize Mutual Information) Let (qi)i\u2208IQ be a collection of soft predictions corresponding to the query samples. Then, it holds that \u2200 0 \u2264 \u03b1 \u2264 1:\nRFRQ + log |Q| \u2264 RIQ(1) \u2264 RIQ(\u03b1), (6)\nProof: Further details are relegated to Ap. A. Advantage of RFRQ over R I Q(\u03b1): Similarly to RIQ(\u03b1), R FR Q can be exploited to maximize the Mutual Information. However, RFRQ is parameter-free and thus, it does not require tuning \u03b1."
        },
        {
            "heading": "3.4 Additional Few-shot Inductive Baseline",
            "text": "In addition to the transductive methods of Sec. 3.2, we will explore three additional inductive methods for few-shot classification: prototypical networks, linear probing, and a semi-supervised classifier.\nPrototypical Networks (PT) PT learn a metric space where the distance between two points corresponds to their degree of similarity. During inference, the distance between the query example and each class prototype is computed, and the predicted label is the class with the closest prototype. PT has been widely used in NLP and is considered as a strong baseline (Snell et al., 2017; Sun et al., 2019b; Gao et al., 2019).\nLinear Probing (CE) Fine-tuning a linear head on top of a pretrained model is a popular approach to learn a classifier for classification tasks and was originally proposed in (Devlin et al., 2018).\nSemi-supervised Baselines (SSL). We additionally propose two semi-supervised baselines following two steps. In the first step, a classifier is trained using the support set S and used to label Q. In the second step, the final classifier is trained on both S and Q with the pseudo label obtained from the first step."
        },
        {
            "heading": "4 An Enhanced Experimental Setting",
            "text": ""
        },
        {
            "heading": "4.1 Datasets",
            "text": "Benchmarking the performance of FSL methods on diverse sets of datasets is critical to evaluate their generalization capabilities in a robust manner as well as their potential for real-world applications. Previous work on FSL (Karimi Mahabadi et al., 2022; Perez et al., 2021) mainly focuses on datasets with a reduced number of classes (i.e., K < 5). Motivated by practical considerations we choose to build a new benchmark composed of datasets with a larger number of classes. Specifically, we choose Go Emotion (Demszky et al., 2020), Tweet Eval (Barbieri et al., 2020), Clinc (Larson et al., 2019), Banking (Casanueva et al., 2020) and the Multilingual Amazon Reviews Corpus (Keung et al., 2020). These datasets cover a wide range of text classification scenarios and are of various difficulty4. A summary of the datasets used can be found in Tab. 1."
        },
        {
            "heading": "4.2 Model Choice",
            "text": "The selection of an appropriate backbone model is a critical factor in achieving high performance in few-shot NLP tasks. To ensure the validity and robustness of our findings, we have included a diverse range of transformer-based backbone models in our study, including"
        },
        {
            "heading": "1. Three different sizes of RoBERTa based models",
            "text": "(Liu et al., 2019b). Similar to BERT, RoBERTa is\n4Datasets are available in Dataset (Lhoest et al., 2021)\npretrained using the closed task (Taylor, 1953). We consider two different sizes of the RoBERTa model, namely RoBERTa (B) with 124M parameters and RoBERTa (L) with 355M parameters and DistilRoBERTa, a lighter version of RoBERTa trained through a distillation process (Hinton et al., 2015), for a total of 82M parameters.\n2. Three sentence-transformers encoder (Reimers and Gurevych, 2019). Following (Muennighoff et al., 2022), we consider MPNET-base (Song et al., 2020), MiniLM (Wang et al., 2020), and Albert Small V2 (Lan et al., 2019).\n3. Multilingual models. To address realistic multilingual scenarios, we rely on three sizes of XLMRoBERTa (Conneau et al., 2020, 2019): base (B), large (L) and XL (XL).\n4. text-davinci model: to mimic the typical setting of API-based models, we also conduct experiments on text-davinci, only accessible through OpenAI\u2019s API."
        },
        {
            "heading": "4.3 Evaluation Framework",
            "text": "Prior research in textual FSL typically involves sampling a low number of tasks, typically less than 10, of each dataset. In contrast, we utilize an episodic learning framework that generates a large number of N-shots K-ways tasks. This framework has gained popularity through inductive metalearning approaches, such as those proposed by (Finn et al., 2017b; Snell et al., 2017; Vinyals et al., 2016; Sung et al., 2018a; Mishra et al., 2017; Rusu et al., 2019; Oreshkin et al., 2018), as it mimics the few-shot environment during evaluation and improves model robustness and generalization. In this context, episodic training implies that a different model is initialized for each generated few-shot task, and all tasks are compiled independently in parallel. This approach allows to the computation of more reliable performance statistics by evaluating the generalization capabilities of each method on a more diverse set of tasks. To account for the model\u2019s generalization ability, we average the results for each dataset over 1000 episodes, with the N considered classes varying in every episode. For each experiment, we consider the F1-Score."
        },
        {
            "heading": "5 Experiments",
            "text": "5.1 Case Study of text-davinci In this experiment, we investigate the performance of text-davinci in both its language model and\nembedding-based model forms. We assess its classification capabilities using the aforementioned baseline and explore the language model\u2019s performance when applied in an in-context learning (ICL) setup with prompting.\nTakeaways. From Tab. 2, we observe that SSL performs comparably to CE, which is simpler to use and will be considered as the baseline in the next part of our study. Although ICL slightly outperforms CE, its implementation comes at a significant cost. In ICL, each class requires N shots, forcing the user to send a long input query with additional instructions. This query length becomes prohibitive as the number of classes increases, and on average, it is 58 times longer than using the embedding base API in our benchmark. The lengthy input and ICL approach make it time-consuming for generation (violating R1), require the user to provide labels (violating R2), and prevent the reuse of embeddings for future use (e.g., retrieval, clustering). Additionally, ICL is 60 times more expensive than CE. Thus, we will discard ICL for the subsequent part of this study."
        },
        {
            "heading": "5.2 Overall Results",
            "text": "Global results: To evaluate the effectiveness of various few-shot methods, we conducted a comprehensive analysis of their classification performance across all datasets, all backbones, and all considered N-shots/K-ways scenarios. Results are reported in Tab. 3. An interesting observation is that transductive approaches I and FR outperform their inductive counterparts (CE and PT). Notably, we found that vanilla entropy minimization, which solely relies on H, consistently underperforms in all considered scenarios. Our analysis revealed that FR surpasses traditional fine-tuning based on cross-entropy by a margin of 3.7%.\nMono-lingual experiment: In order to thoroughly analyze the performance of each method, we conducted a per-dataset study, beginning with a\nfocus on the mono-lingual datasets. Fig. 2 reveals that the global trends observed in Tab. 3 remain consistent across datasets of varying difficulty levels. Notably, we observed consistent improvements achieved by transductive regularizers (such as I or FR) over CE. However, the relative improvement is highly dependent on the specific dataset being evaluated. Specifically, FR achieves +6.5% F1-score on Banking, but only a shy +1.5% on Tweet. A strong baseline generally suggests highly discriminative features for the task, and therefore a strong upside in leveraging additional unlabeled features, and vice versa. Therefore, we hypothesize that the potential gains to be obtained through transduction correlate with the baseline\u2019s performance.5"
        },
        {
            "heading": "5.3 Study Under Different Data-Regime",
            "text": "In this experiment, we investigated the performance of different loss functions under varying conditions of \u2019ways\u2019 and \u2019shots\u2019. As shown in Fig. 3, we observed that increasing the number of classes (\u2019ways\u2019) led to a decrease in F1 while increasing the number of examples per class (\u2019shots\u2019) led to an improvement in F1. This can be explained by\n5Additional multilingual results (i.e., on es, de, fr) can be found on Sec. B.3. They exhibit the same behavior.\nthe fact that having more data enables the classifier to better discern the characteristics of each class.\nInterestingly, the relationship between the number of shots and classification F1 may not be the same for all classes or all loss functions. Fig. 3 shows that different loss functions (e.g. FR on banking) benefited greatly from adding a few shots, while others did not show as much improvement. However, this variability is dependent on the specific dataset and language being used, as different classes may have different levels of complexity and variability, and some may be inherently easier or harder to classify than others."
        },
        {
            "heading": "5.4 Ablation Study On Backbones",
            "text": "In this experiment, we examined how different loss functions perform when increasing the number of parameters in various models. The results, presented in Fig. 4, show the average performance across the experiments and are organized by the loss function. We observed an inverse scaling law for both the RoBERTa and XLM-RoBERTa family of models, where increasing the number of parameters led to a decrease in performance for the losses tested. However, within the same family, we observe that the superiority of FR remains consistent. An interesting finding from Fig. 4 is that the transductive regularization technique using FR outperforms other methods on text-davinci. This highlights the effectiveness of FR in improving the performance of the model and suggests that transductive regularization may be a promising approach for optimizing language models."
        },
        {
            "heading": "5.5 Practical Considerations",
            "text": "In this experiment, we adopt a practical standpoint and aim to evaluate the effectiveness of an API model, specifically text-davinci. In Tab. 4, we report the training speed of one episode on a MAC with CPU. Overall, we observed that the transductive loss is slower as it necessitates the computation of the loss on the query set, whereas PT is faster as it does not involve any optimization. Furthermore, we note that FR is comparable in speed to I. To provide a better understanding of these results, we can compare our method with existing approaches (in the light of R2). For instance, PET (Schick and Sch\u00fctze, 2020a) entails a training time of 20 minutes on A100, while ADAPET (Tam et al., 2021) necessitates 10 minutes on the same hardware."
        },
        {
            "heading": "6 Conclusions",
            "text": "This paper presents a novel FSL framework that utilizes API models while meeting critical constraints of real-world applications (i.e., R1, R2, R3). This approach is particularly appealing as it shifts the computational requirements (R2), eliminating the need for heavy computations for the user and reducing the cost of embedding. To provide a better understanding, embedding over 400k sequences cost as low as 7 dollars. In this scenario, our research highlights the potential of transductive losses, which have previously been disregarded by the NLP community. A candidate loss is the FisherRao distance which is parameter-free and could serve as a simple baseline in the future."
        },
        {
            "heading": "7 Limitations",
            "text": "We are optimistic that our research will have a positive impact on society. Nonetheless, it is essential to acknowledge the limitations of API-based fewshot classification models despite their promising results in various tasks. Firstly, the performance of the introduced methods is heavily dependent on the quality of available API models. If the API models do not provide sufficient information or lack diversity, the introduced methods may struggle to accurately classify input texts. Secondly, the blackbox nature of the backbone limits the interpretability of API-based few-shot classification methods, which may hinder their adoption. Ultimately, the aim of this work is to establish a baseline for future research on transductive inference. As a result, not all existing transductive methods are compared in this study."
        },
        {
            "heading": "Acknowledgements",
            "text": "This work was performed using HPC resources from GENCI-IDRIS (Grants 2022- AD01101838, 2023-103256 and 2023-101838)."
        },
        {
            "heading": "A Proof of Proposition 1",
            "text": "In this Appendix, we prove the inequality (Eq. 6) provided in Proposition 1. The right-hand side of (Eq. 6) follows straightforwardly from the definition of RIQ(\u03b1) and the non-negativity of the Shannon entropy. In order to prove the first inequality, we need to introduce the following intermediate result.\nFor any arbitrary random variable (r.v) X and countable r.v Y , and any real number \u03b2, let I\u03b2(X;Y ) := \u2212EX\u22c6Y logEX [ P (Y |X) P (Y |X\u22c6) ]\u03b2 ,\nwhere the r.v X\u22c6 follows the same distribution than X . Notice that it is obvious that I1(X;Y ) = I(X;Y ), where I(X;Y ) is Shannon Mutual Information. Lemma 1. For any arbitrary r.v. X and countable r.v. Y , we have\nI(X;Y ) \u2265 I\u03b2(X;Y ), for 0 \u2264 \u03b2 \u2264 1.\nProof of the lemma: We must show that the different of I(X;Y )\u2212 I\u03b2(X;Y ) is nonnegative. To this end, we write this difference as:\nI(X;Y )\u2212 I\u03b2(X;Y ) (7)\n= \u2212EX\u22c6Y log P 1\u2212\u03b2(Y |X\u22c6)EXP (Y |X)\nEXP \u03b2(Y |X) (8)\n\u2265 \u2212 logEX\u22c6Y P 1\u2212\u03b2(Y |X\u22c6)EXP (Y |X)\nEXP \u03b2(Y |X) (9)\n= \u2212 log \u2211 y\u2208Y EX\u22c6P (y|X\u22c6) P 1\u2212\u03b2(y|X\u22c6)EXP (y|X) EXP \u03b2(y|X) (10)\n= \u2212 log \u2211 y\u2208Y EX\u22c6P \u03b2(y|X\u22c6)EXP (y|X) EXP \u03b2(y|X)\n(11)\n= \u2212 log \u2211 y\u2208Y EXP (y|X) (12)\n= 0, (13)\nwhere the first inequality follows by applying Jensen\u2019s inequality to the function t 7\u2192 \u2212 log(t). Proof of Proposition 1: From Lemma 1, using Jensen\u2019s inequality, we have\nI(X;Y ) = \u2212EX\u22c6Y logEX [ P (Y |X) P (Y |X\u22c6) ] , (14)\n\u2265 \u2212EX\u22c6Y logEX [ P (Y |X) P (Y |X\u22c6) ]\u03b2 (15)\n\u2265 \u2212EX\u22c6 logEXEY |X\u22c6 [ P (Y |X) P (Y |X\u22c6) ]\u03b2 (16) = \u2212EX\u22c6 logEX (17)\u2211 y\u2208Y P \u03b2(Y |X)P 1\u2212\u03b2(Y |X\u22c6), (18)\nwhere inequality (15) follows by applying Lemma 1 and inequality (16) follows by exploiting the convexity of the function t 7\u2192 \u2212 log(t) for any 0 \u2264 \u03b2 \u2264 1. Finally, it is not difficult to check from the definition of the Fisher-Rao distance given by expression (3) that\ncos\n( dFR(P (y|X = x), P (y|X = x\u22c6))\n2 ) = (19)\u2211\ny\u2208Y\n\u221a P (y|X = x)P (y|X = x\u22c6). (20)\nUsing the identity given by (19) in expression (18), and setting \u03b2 = 1/2, we obtain the following lower bound on I(X;Y ):\n\u2212EX\u22c6 logEX cos ( dFR(P (y|X), P (y|X\u22c6))\n2 ) The inequality (6) immediately follows by replacing the distribution of the r.v. X with the empirical distribution on the query and P (y|x) with the soft-prediction corresponding to the feature x, which concludes the proof of the proposition."
        },
        {
            "heading": "B Additional Experimental Results",
            "text": "B.1 Preliminary Classification Results\nPreliminary Experiment. In our experiments, the backbone models are of utmost importance. Our objective in this preliminary experiment is to assess the efficacy of these models when fine-tuning only the model head across a variety of datasets. Through this evaluation, we aim to gain insight into their generalization abilities and any dataset-specific factors that may influence their performance. This information can be utilized to analyze the performance of different models in the few-shot scenario, as described in Sec. 5. We present the results of this experiment in Tab. 5, noting that all classes were considered, which differs from the episodic training approach detailed in Sec. 5.\nB.2 A Dive Into text-davinci results\ntext-davinci appears to be the backbone providing the most informative a priori embeddings in Tab. 5 and could be considered as the prime model for API-based FSL, showcasing the current requirements in this area. It is thus a typical candidate for application uses that must meet the following criteria (R1) - (R3). Therefore, we put a special emphasis on its related results.\nFig. 6 (top) details the text-davinci results of the experiments conducted on the mono-lingual datasets. These plots highlight the consistency of the tendencies that emerged in Tab. 5, Tab. 3 and Fig. 2, namely: the superiority of transductive approaches (FR and I) over inductive ones (CE and PT ), the underperformance of the entropic-minimization-based strategy (H), and the higher amount of information conveyed by text-davinci learned embeddings over other backbones, resulting in higher F1 scores on all datasets.\nThese phenomena still occur in the multi-lingual setting, as illustrated in Fig. 6 (bottom), stressing the superiority of transductive (and especially FR) over other approaches for presumably universal tasks, beyond English-centered ones, and without the need for using language-specific engineering as for prompting-based strategies.\nNote that for both of these settings, the entropic-minimization-based strategy (H) seems to be capped at a 15% F1 score, thus with no improvement over other backbones embeddings, and independently of the dataset difficulty.\nB.3 Multilingual Experiment\nTo provide an exhaustive analysis, we report the same experiment that is made in Sec. 5.2, for multi-lingual models on the Amazon dataset. While both Latin languages (French and Spanish) share close results, with an F1 gain of 2.8% for FR over CE, the results in the German and English language exhibit an F1 increased by almost 4%.\nB.4 Importance of Model Backbones on Monolingual Experiment\nIn this section, we report the results of our experiment aggregated per backbone. The goal is to understand how the different losses behave on the different backbones. The results are presented in Fig. 10. While the trends observed in the previous charts are retrieved for the majority of backbones, some of these models are exceptions. For example, while transductive methods perform generally better than inductive methods, the CE-based method seems to perform slightly better than I for XLM-RoBERTa-xl. Additionally, while FR is the most effective method for the majority of backbones, it is surpassed by I for the alldistilroberta-v1 model. Furthermore, the inverse-scaling-law details are found for the RoBERTa(B/L) and XLM-RoBERTa (B/L) models per dataset. In general, it is interesting to note that although model performance is constrained by dataset difficulty, the performance order of each method is consistent across all 4 datasets for each considered backbone.\nB.4.1 Results Per Language In this experiment, we report the performance of different losses on the Amazon dataset by averaging the results over the number of shots, ways, and model backbones. The results are presented in Tab. 6. Our observations indicate that the transductive regularization improves the results for all languages over the inductive baseline (i.e., CE), with a substantially higher gain for the German language. Additionally, we note that the observed improvements for FR are more consistent. This further demonstrates that the transductive loss can be useful in few-shot NLP. In the future, we would like to explore the application of transductive inference to other NLP tasks such as sequence generation (Pichler et al., 2022; Colombo et al., 2019, 2021d,b) and classification tasks (Chapuis et al., 2020; Colombo et al., 2022d,b; Himmi et al., 2023) as well as NLG evaluation (Colombo et al., 2021e, 2022c, 2021c,a,b) and Safe AI (Colombo et al., 2022a; Picot et al., 2022a,b; Darrin et al., 2022, 2023)."
        }
    ],
    "title": "Transductive Learning for Textual Few-Shot Classification in API-based Embedding Models",
    "year": 2023
}