{
    "abstractText": "ChatGPT\u2019s emergence heralds a transformative phase in NLP, particularly demonstrated through its excellent performance on many English benchmarks. However, the model\u2019s efficacy across diverse linguistic contexts remains largely uncharted territory. This work aims to bridge this knowledge gap, with a primary focus on assessing ChatGPT\u2019s capabilities on Arabic languages and dialectal varieties. Our comprehensive study conducts a largescale automated and human evaluation of ChatGPT, encompassing 44 distinct language understanding and generation tasks on over 60 different datasets. To our knowledge, this marks the first extensive performance analysis of ChatGPT\u2019s deployment in Arabic NLP. Our findings indicate that, despite its remarkable performance in English, ChatGPT is consistently surpassed by smaller models that have undergone finetuning on Arabic. We further undertake a meticulous comparison of ChatGPT and GPT-4\u2019s Modern Standard Arabic (MSA) and Dialectal Arabic (DA), unveiling the relative shortcomings of both models in handling Arabic dialects compared to MSA. Although we further explore and confirm the utility of employing GPT-4 as a potential alternative for human evaluation, our work adds to a growing body of research underscoring the limitations of ChatGPT.",
    "authors": [
        {
            "affiliations": [],
            "name": "Md Tawkat"
        },
        {
            "affiliations": [],
            "name": "Islam Khondaker\u03bb"
        },
        {
            "affiliations": [],
            "name": "Abdul Waheed\u03be"
        },
        {
            "affiliations": [],
            "name": "El Moatez Billah Nagoudi\u03bb"
        },
        {
            "affiliations": [],
            "name": "Muhammad Abdul-Mageed\u03bb"
        }
    ],
    "id": "SP:6d93071ea9bc7a8be62732a29cd5b81e5e880d37",
    "references": [
        {
            "authors": [
                "Ahmed Abdelali",
                "Hamdy Mubarak",
                "Younes Samih",
                "Sabit Hassan",
                "Kareem Darwish."
            ],
            "title": "Arabic Dialect Identification in the Wild",
            "venue": "Proceedings of the Sixth Arabic Natural Language Processing Workshop.",
            "year": 2020
        },
        {
            "authors": [
                "Muhammad Abdul-Mageed",
                "AbdelRahim Elmadany",
                "El Moatez Billah Nagoudi."
            ],
            "title": "ARBERT & MARBERT: Deep bidirectional transformers for Arabic",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the",
            "year": 2021
        },
        {
            "authors": [
                "Muhammad Abdul-Mageed",
                "AbdelRahim Elmadany",
                "El Moatez Billah Nagoudi."
            ],
            "title": "ARBERT & MARBERT: Deep bidirectional transformers for Arabic",
            "venue": "Proceedings of the 59th Annual Meeting of the 9https://alliancecan.ca",
            "year": 2021
        },
        {
            "authors": [
                "Muhammad Abdul-Mageed",
                "Chiyu Zhang",
                "Houda Bouamor",
                "Nizar Habash."
            ],
            "title": "NADI 2020: The first nuanced Arabic dialect identification shared task",
            "venue": "Proceedings of the Fifth Arabic Natural Language Processing Workshop, pages 97\u2013110,",
            "year": 2020
        },
        {
            "authors": [
                "Muhammad Abdul-Mageed",
                "Chiyu Zhang",
                "Azadeh Hashemi",
                "El Moatez Billah Nagoudi."
            ],
            "title": "AraNet: A deep learning toolkit for Arabic social media",
            "venue": "Proceedings of the 4th Workshop on OpenSource Arabic Corpora and Processing Tools, with a",
            "year": 2020
        },
        {
            "authors": [
                "Ibrahim Abu Farha",
                "Walid Magdy."
            ],
            "title": "Benchmarking transformer-based language models for Arabic sentiment and sarcasm detection",
            "venue": "Proceedings of the Sixth Arabic Natural Language Processing Workshop, pages 21\u201331, Kyiv, Ukraine (Virtual). As-",
            "year": 2021
        },
        {
            "authors": [
                "Ali Saleh Alammary."
            ],
            "title": "Bert models for arabic text classification: A systematic review",
            "venue": "Applied Sciences, 12(11).",
            "year": 2022
        },
        {
            "authors": [
                "Bashar Alhafni",
                "Nizar Habash",
                "Houda Bouamor."
            ],
            "title": "User-centric gender rewriting",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 618\u2013631,",
            "year": 2022
        },
        {
            "authors": [
                "Ali Alshehri",
                "El Moatez Billah Nagoudi",
                "Muhammad Abdul-Mageed."
            ],
            "title": "Understanding and detecting dangerous speech in social media",
            "venue": "Proceedings of the 4th Workshop on Open-Source Arabic Corpora and Processing Tools, with a Shared",
            "year": 2020
        },
        {
            "authors": [
                "Mohamed Seghir Hadj Ameur",
                "Farid Meziane",
                "Ahmed Guessoum."
            ],
            "title": "Anetac: Arabic named entity transliteration and classification dataset",
            "venue": "arXiv preprint arXiv:1907.03110.",
            "year": 2019
        },
        {
            "authors": [
                "Mikel Artetxe",
                "Sebastian Ruder",
                "Dani Yogatama."
            ],
            "title": "On the Cross-lingual Transferability of Monolingual Representations",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4623\u20134637.",
            "year": 2020
        },
        {
            "authors": [
                "Ramy Baly",
                "Mitra Mohtarami",
                "James Glass",
                "Llu\u00eds M\u00e0rquez",
                "Alessandro Moschitti",
                "Preslav Nakov."
            ],
            "title": "Integrating stance detection and fact checking in a unified corpus",
            "venue": "Proceedings of the 2018",
            "year": 2018
        },
        {
            "authors": [
                "Yejin Bang",
                "Samuel Cahyawijaya",
                "Nayeon Lee",
                "Wenliang Dai",
                "Dan Su",
                "Bryan Wilie",
                "Holy Lovenia",
                "Ziwei Ji",
                "Tiezheng Yu",
                "Willy Chung",
                "Quyet V. Do",
                "Yan Xu",
                "Pascale Fung"
            ],
            "title": "A multitask, multilingual, multimodal evaluation of chatgpt on reasoning",
            "year": 2023
        },
        {
            "authors": [
                "Houda Bouamor",
                "Nizar Habash",
                "Kemal Oflazer."
            ],
            "title": "A multidialectal parallel corpus of arabic",
            "venue": "LREC, pages 1240\u20131245.",
            "year": 2014
        },
        {
            "authors": [
                "Houda Bouamor",
                "Sabit Hassan",
                "Nizar Habash."
            ],
            "title": "The MADAR shared task on Arabic fine-grained dialect identification",
            "venue": "Proceedings of the Fourth Arabic Natural Language Processing Workshop, pages 199\u2013207.",
            "year": 2019
        },
        {
            "authors": [
                "Lingjiao Chen",
                "Matei Zaharia",
                "James Zou"
            ],
            "title": "2023a. How is chatgpt\u2019s behavior changing over time? arXiv preprint arXiv:2307.09009",
            "year": 2023
        },
        {
            "authors": [
                "Xuanting Chen",
                "Junjie Ye",
                "Can Zu",
                "Nuo Xu",
                "Rui Zheng",
                "Minlong Peng",
                "Jie Zhou",
                "Tao Gui",
                "Qi Zhang",
                "Xuanjing Huang"
            ],
            "title": "2023b. How robust is gpt-3.5 to predecessors? a comprehensive study on language understanding",
            "year": 2023
        },
        {
            "authors": [
                "Wei-Lin Chiang",
                "Zhuohan Li",
                "Zi Lin",
                "Ying Sheng",
                "Zhanghao Wu",
                "Hao Zhang",
                "Lianmin Zheng",
                "Siyuan Zhuang",
                "Yonghao Zhuang",
                "Joseph E. Gonzalez",
                "Ion Stoica",
                "Eric P. Xing"
            ],
            "title": "Vicuna: An opensource chatbot impressing gpt-4 with 90%* chatgpt",
            "year": 2023
        },
        {
            "authors": [
                "Aakanksha Chowdhery",
                "Sharan Narang",
                "Jacob Devlin",
                "Maarten Bosma",
                "Gaurav Mishra",
                "Adam Roberts",
                "Paul Barham",
                "Hyung Won Chung",
                "Charles Sutton",
                "Sebastian Gehrmann"
            ],
            "title": "Palm: Scaling language modeling with pathways",
            "year": 2022
        },
        {
            "authors": [
                "Jason Wei"
            ],
            "title": "2022b. Scaling instruction-finetuned language models",
            "year": 2022
        },
        {
            "authors": [
                "Alexis Conneau",
                "Ruty Rinott",
                "Guillaume Lample",
                "Adina Williams",
                "Samuel R. Bowman",
                "Holger Schwenk",
                "Veselin Stoyanov."
            ],
            "title": "Xnli: Evaluating crosslingual sentence representations",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natu-",
            "year": 2018
        },
        {
            "authors": [
                "Daniel Dahlmeier",
                "Hwee Tou Ng."
            ],
            "title": "Better evaluation for grammatical error correction",
            "venue": "Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages",
            "year": 2012
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
            "year": 2019
        },
        {
            "authors": [
                "Mahmoud El-Haj."
            ],
            "title": "Habibi-a multi dialect multi national arabic song lyrics corpus",
            "venue": "Proceedings of the 12th Language Resources and Evaluation Conference, pages 1318\u20131326.",
            "year": 2020
        },
        {
            "authors": [
                "Mohammed El-Razzaz",
                "Mohamed Waleed Fakhr",
                "Fahima A Maghraby."
            ],
            "title": "Arabic gloss wsd using bert",
            "venue": "Applied Sciences, 11(6):2567.",
            "year": 2021
        },
        {
            "authors": [
                "AbdelRahim Elmadany",
                "El Moatez Billah Nagoudi",
                "Muhammad Abdul-Mageed."
            ],
            "title": "Orca: A challenging benchmark for arabic language understanding",
            "venue": "ArXiv, abs/2212.10758.",
            "year": 2022
        },
        {
            "authors": [
                "AbdelRahim Elmadany",
                "El Moatez Billah Nagoudi",
                "Muhammad Abdul-Mageed"
            ],
            "title": "Orca: A challenging benchmark for arabic language understanding",
            "year": 2023
        },
        {
            "authors": [
                "Kawin Ethayarajh."
            ],
            "title": "How contextual are contextualized word representations? Comparing the geometry of BERT, ELMo, and GPT-2 embeddings",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the",
            "year": 2019
        },
        {
            "authors": [
                "Ali Fadel",
                "Ibraheem Tuffaha",
                "Bara\u2019 Al-Jawarneh",
                "Mahmoud Al-Ayyoub"
            ],
            "title": "Arabic text diacritization using deep neural networks",
            "year": 2019
        },
        {
            "authors": [
                "Ibrahim Abu Farha",
                "Walid Magdy."
            ],
            "title": "From Arabic Sentiment Analysis to Sarcasm Detection: The ArSarcasm Dataset",
            "venue": "Proceedings of the 4th Workshop on Open-Source Arabic Corpora and Processing Tools, with a Shared Task on Offensive Language",
            "year": 2020
        },
        {
            "authors": [
                "Yuan Gao",
                "Ruili Wang",
                "Feng Hou"
            ],
            "title": "How to design translation prompts for chatgpt: An empirical study",
            "year": 2023
        },
        {
            "authors": [
                "Bilal Ghanem",
                "Jihen Karoui",
                "Farah Benamara",
                "V\u00e9ronique Moriceau",
                "Paolo Rosso."
            ],
            "title": "IDAT@FIRE2019: Overview of the Track on Irony Detection in Arabic Tweets",
            "venue": ". In Mehta P., Rosso P., Majumder P., Mitra M. (Eds.) Working Notes of the",
            "year": 2019
        },
        {
            "authors": [
                "Fabrizio Gilardi",
                "Meysam Alizadeh",
                "Ma\u00ebl Kubli"
            ],
            "title": "Chatgpt outperforms crowd-workers for textannotation",
            "year": 2023
        },
        {
            "authors": [
                "Tahmid Hasan",
                "Abhik Bhattacharjee",
                "Md Saiful Islam",
                "Kazi Samin",
                "Yuan-Fang Li",
                "Yong-Bin Kang",
                "M. Sohel Rahman",
                "Rifat Shahriyar"
            ],
            "title": "Xl-sum: Large-scale multilingual abstractive summarization",
            "year": 2021
        },
        {
            "authors": [
                "Amr Hendy",
                "Mohamed Abdelrehim",
                "Amr Sharaf",
                "Vikas Raunak",
                "Mohamed Gabr",
                "Hitokazu Matsushita",
                "Young Jin Kim",
                "Mohamed Afify",
                "Hany Hassan Awadalla"
            ],
            "title": "How good are gpt models at machine translation? a comprehensive evaluation",
            "year": 2023
        },
        {
            "authors": [
                "Haoyang Huang",
                "Tianyi Tang",
                "Dongdong Zhang",
                "Wayne Xin Zhao",
                "Ting Song",
                "Yan Xia",
                "Furu Wei"
            ],
            "title": "Not all languages are created equal in llms: Improving multilingual capability by cross-lingualthought prompting",
            "year": 2023
        },
        {
            "authors": [
                "Wenxiang Jiao",
                "Wenxuan Wang",
                "Jen tse Huang",
                "Xing Wang",
                "Zhaopeng Tu"
            ],
            "title": "Is chatgpt a good translator? yes with gpt-4 as the engine",
            "year": 2023
        },
        {
            "authors": [
                "Jungo Kasai",
                "Yuhei Kasai",
                "Keisuke Sakaguchi",
                "Yutaro Yamada",
                "Dragomir Radev"
            ],
            "title": "Evaluating gpt-4 and chatgpt on japanese medical licensing examinations",
            "year": 2023
        },
        {
            "authors": [
                "Jude Khouja."
            ],
            "title": "Stance prediction and claim verification: An Arabic perspective",
            "venue": "Proceedings of the Third Workshop on Fact Extraction and VERification (FEVER), pages 8\u201317, Online. Association for Computational Linguistics.",
            "year": 2020
        },
        {
            "authors": [
                "Viet Dac Lai",
                "Nghia Trung Ngo",
                "Amir Pouran Ben Veyseh",
                "Hieu Man",
                "Franck Dernoncourt",
                "Trung Bui",
                "Thien Huu Nguyen"
            ],
            "title": "Chatgpt beyond english: Towards a comprehensive evaluation of large language models in multilingual learning",
            "year": 2023
        },
        {
            "authors": [
                "Md Tahmid Rahman Laskar",
                "M Saiful Bari",
                "Mizanur Rahman",
                "Md Amran Hossen Bhuiyan",
                "Shafiq R. Joty",
                "J. Huang."
            ],
            "title": "A systematic study and comprehensive evaluation of chatgpt on benchmark datasets",
            "venue": "ArXiv, abs/2305.18486.",
            "year": 2023
        },
        {
            "authors": [
                "Bohan Li",
                "Hao Zhou",
                "Junxian He",
                "Mingxuan Wang",
                "Yiming Yang",
                "Lei Li."
            ],
            "title": "On the sentence embeddings from pre-trained language models",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
            "year": 2020
        },
        {
            "authors": [
                "Yanran Li",
                "Hui Su",
                "Xiaoyu Shen",
                "Wenjie Li",
                "Ziqiang Cao",
                "Shuzi Niu."
            ],
            "title": "Dailydialog: A manually labelled multi-turn dialogue dataset",
            "venue": "Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers),",
            "year": 2017
        },
        {
            "authors": [
                "moyer",
                "Zornitsa Kozareva",
                "Mona Diab",
                "Veselin Stoyanov",
                "Xian Li"
            ],
            "title": "Few-shot learning with multilingual language models",
            "year": 2022
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
            "venue": "arXiv preprint arXiv:1907.11692.",
            "year": 2019
        },
        {
            "authors": [
                "Yao Lu",
                "Max Bartolo",
                "Alastair Moore",
                "Sebastian Riedel",
                "Pontus Stenetorp."
            ],
            "title": "Fantastically ordered prompts and where to find them: Overcoming fewshot prompt order sensitivity",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Compu-",
            "year": 2022
        },
        {
            "authors": [
                "Saif Mohammad",
                "Felipe Bravo-Marquez",
                "Mohammad Salameh",
                "Svetlana Kiritchenko."
            ],
            "title": "SemEval2018 task 1: Affect in tweets",
            "venue": "Proceedings of The 12th International Workshop on Semantic Evaluation, pages 1\u201317, New Orleans, Louisiana. Association for",
            "year": 2018
        },
        {
            "authors": [
                "Behrang Mohit",
                "Alla Rozovskaya",
                "Nizar Habash",
                "Wajdi Zaghouani",
                "Ossama Obeid."
            ],
            "title": "The first QALB shared task on automatic text correction for Arabic",
            "venue": "Proceedings of the EMNLP 2014 Workshop on Arabic Natural Language Processing",
            "year": 2014
        },
        {
            "authors": [
                "Hamdy Mubarak",
                "Kareem Darwish",
                "Walid Magdy",
                "Tamer Elsayed",
                "Hend Al-Khalifa."
            ],
            "title": "Overview of OSACT4 Arabic offensive language detection shared task",
            "venue": "Proceedings of the 4th Workshop on Open-Source Arabic Corpora and Process-",
            "year": 2020
        },
        {
            "authors": [
                "Hamdy Mubarak",
                "Sabit Hassan",
                "Ahmed Abdelali."
            ],
            "title": "Adult content detection on Arabic Twitter: Analysis and experiments",
            "venue": "Proceedings of the Sixth Arabic Natural Language Processing Workshop, pages 136\u2013144, Kyiv, Ukraine (Virtual). Association",
            "year": 2021
        },
        {
            "authors": [
                "Zaid Alyafeai",
                "Albert Webson",
                "Edward Raff",
                "Colin Raffel"
            ],
            "title": "Crosslingual generalization through multitask finetuning",
            "year": 2022
        },
        {
            "authors": [
                "Albanie",
                "Zaid Alyafeai",
                "Albert Webson",
                "Edward Raff",
                "Colin Raffel."
            ],
            "title": "Crosslingual generalization through multitask finetuning",
            "venue": "ArXiv, abs/2211.01786.",
            "year": 2022
        },
        {
            "authors": [
                "El Moatez Billah Nagoudi",
                "Muhammad Abdul-Mageed",
                "AbdelRahim Elmadany",
                "Alcides Alcoba Inciarte",
                "Md Tawkat Islam Khondaker."
            ],
            "title": "Jasmine: Arabic gpt models for few-shot learning",
            "venue": "arXiv preprint arXiv:2212.10755.",
            "year": 2022
        },
        {
            "authors": [
                "El Moatez Billah Nagoudi",
                "AbdelRahim Elmadany",
                "Muhammad Abdul-Mageed."
            ],
            "title": "AraT5: Textto-text transformers for Arabic language generation",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume",
            "year": 2022
        },
        {
            "authors": [
                "El Moatez Billah Nagoudi",
                "AbdelRahim Elmadany",
                "Muhammad Abdul-Mageed",
                "Tariq Alhindi."
            ],
            "title": "Machine generation and detection of Arabic manipulated and fake news",
            "venue": "Proceedings of the Fifth Arabic Natural Language Processing Workshop, pages",
            "year": 2020
        },
        {
            "authors": [
                "Tarek Naous",
                "Zahraa Bassyouni",
                "Bassel Mousi",
                "Hazem Hajj",
                "Wassim El Hajj",
                "Khaled Shaban."
            ],
            "title": "Open-domain response generation in low-resource settings using self-supervised pre-training of warmstarted transformers",
            "venue": "ACM Transactions on Asian",
            "year": 2023
        },
        {
            "authors": [
                "Yixin Nie",
                "Adina Williams",
                "Emily Dinan",
                "Mohit Bansal",
                "Jason Weston",
                "Douwe Kiela"
            ],
            "title": "Adversarial nli: A new benchmark for natural language understanding",
            "year": 2020
        },
        {
            "authors": [
                "Reham Omar",
                "Omij Mangukiya",
                "Panos Kalnis",
                "Essam Mansour"
            ],
            "title": "Chatgpt versus traditional question answering for knowledge graphs: Current status and future directions towards knowledge graph chatbots",
            "year": 2023
        },
        {
            "authors": [
                "Paul Christiano",
                "Jan Leike",
                "Ryan Lowe"
            ],
            "title": "Training language models to follow instructions with human feedback",
            "year": 2022
        },
        {
            "authors": [
                "Keqin Peng",
                "Liang Ding",
                "Qihuang Zhong",
                "Li Shen",
                "Xuebo Liu",
                "Min Zhang",
                "Yuanxin Ouyang",
                "Dacheng Tao"
            ],
            "title": "Towards making the most of chatgpt for machine translation",
            "year": 2023
        },
        {
            "authors": [
                "Chengwei Qin",
                "Aston Zhang",
                "Zhuosheng Zhang",
                "Jiaao Chen",
                "Michihiro Yasunaga",
                "Diyi Yang"
            ],
            "title": "Is chatgpt a general-purpose natural language processing task solver",
            "year": 2023
        },
        {
            "authors": [
                "Chengwei Qin",
                "Aston Zhang",
                "Zhuosheng Zhang",
                "Jiaao Chen",
                "Michihiro Yasunaga",
                "Diyi Yang"
            ],
            "title": "Is chatgpt a general-purpose natural language processing task solver? CoRR, abs/2302.06476",
            "year": 2023
        },
        {
            "authors": [
                "Michael V. Reiss"
            ],
            "title": "Testing the reliability of chatgpt for text annotation and classification: A cautionary remark",
            "year": 2023
        },
        {
            "authors": [
                "Yves Scherrer."
            ],
            "title": "TaPaCo: A corpus of sentential paraphrases for 73 languages",
            "venue": "Proceedings of the Twelfth Language Resources and Evaluation Conference, pages 6868\u20136873, Marseille, France. European Language Resources Association.",
            "year": 2020
        },
        {
            "authors": [
                "Haitham Seelawi",
                "Ahmad Mustafa",
                "Hesham AlBataineh",
                "Wael Farhan",
                "Hussein T Al-Natsheh."
            ],
            "title": "Nsurl-2019 task 8: Semantic question similarity in arabic",
            "venue": "Proceedings of The First International Workshop on NLP Solutions for Under Re-",
            "year": 2019
        },
        {
            "authors": [
                "Xinyue Shen",
                "Zeyuan Chen",
                "Michael Backes",
                "Yang Zhang"
            ],
            "title": "In chatgpt we trust? measuring and characterizing the reliability of chatgpt",
            "year": 2023
        },
        {
            "authors": [
                "Yiming Tan",
                "Dehai Min",
                "Yu Li",
                "Wenbo Li",
                "Nan Hu",
                "Yongrui Chen",
                "Guilin Qi"
            ],
            "title": "Evaluation of chatgpt as a question answering system for answering complex questions",
            "year": 2023
        },
        {
            "authors": [
                "Gao",
                "Vedanuj Goswami",
                "Francisco Guzm\u00e1n",
                "Philipp Koehn",
                "Alexandre Mourachko",
                "Christophe Ropers",
                "Safiyyah Saleem",
                "Holger Schwenk",
                "Jeff Wang"
            ],
            "title": "No language left behind: Scaling humancentered machine translation",
            "year": 2022
        },
        {
            "authors": [
                "Alex Wang",
                "Amanpreet Singh",
                "Julian Michael",
                "Felix Hill",
                "Omer Levy",
                "Samuel R. Bowman"
            ],
            "title": "Glue: A multi-task benchmark and analysis platform for natural language understanding",
            "year": 2019
        },
        {
            "authors": [
                "Boxin Wang",
                "Chejian Xu",
                "Shuohang Wang",
                "Zhe Gan",
                "Yu Cheng",
                "Jianfeng Gao",
                "Ahmed Hassan Awadallah",
                "Bo Li"
            ],
            "title": "Adversarial glue: A multitask benchmark for robustness evaluation of language models",
            "year": 2022
        },
        {
            "authors": [
                "Jindong Wang",
                "Xixu Hu",
                "Wenxin Hou",
                "Hao Chen",
                "Runkai Zheng",
                "Yidong Wang",
                "Linyi Yang",
                "Haojun Huang",
                "Wei Ye",
                "Xiubo Geng",
                "Binxin Jiao",
                "Yue Zhang",
                "Xing Xie"
            ],
            "title": "On the robustness of chatgpt: An adversarial and out-of-distribution perspective",
            "year": 2023
        },
        {
            "authors": [
                "Yizhong Wang",
                "Yeganeh Kordi",
                "Swaroop Mishra",
                "Alisa Liu",
                "Noah A. Smith",
                "Daniel Khashabi",
                "Hannaneh Hajishirzi."
            ],
            "title": "Self-instruct: Aligning language model with self generated instructions",
            "venue": "ArXiv, abs/2212.10560.",
            "year": 2022
        },
        {
            "authors": [
                "Zengzhi Wang",
                "Qiming Xie",
                "Zixiang Ding",
                "Yi Feng",
                "Rui Xia"
            ],
            "title": "Is chatgpt a good sentiment analyzer? a preliminary study",
            "year": 2023
        },
        {
            "authors": [
                "Jason Wei",
                "Maarten Bosma",
                "Vincent Y Zhao",
                "Kelvin Guu",
                "Adams Wei Yu",
                "Brian Lester",
                "Nan Du",
                "Andrew M Dai",
                "Quoc V Le."
            ],
            "title": "Finetuned language models are zero-shot learners",
            "venue": "arXiv preprint arXiv:2109.01652.",
            "year": 2021
        },
        {
            "authors": [
                "Jules White",
                "Quchen Fu",
                "Sam Hays",
                "Michael Sandborn",
                "Carlos Olea",
                "Henry Gilbert",
                "Ashraf Elnashar",
                "Jesse Spencer-Smith",
                "Douglas C. Schmidt."
            ],
            "title": "A prompt pattern catalog to enhance prompt engineering with chatgpt",
            "venue": "ArXiv, abs/2302.11382.",
            "year": 2023
        },
        {
            "authors": [
                "Jeff Wu",
                "Long Ouyang",
                "Daniel M. Ziegler",
                "Nisan Stiennon",
                "Ryan Lowe",
                "Jan Leike",
                "Paul Christiano"
            ],
            "title": "Recursively summarizing books with human feedback",
            "year": 2021
        },
        {
            "authors": [
                "Jiageng Wu",
                "Xian Wu",
                "Zhaopeng Qiu",
                "Minghui Li",
                "Yefeng Zheng",
                "Jie Yang"
            ],
            "title": "Qualifying chinese medical licensing examination with knowledge enhanced generative pre-training model",
            "year": 2023
        },
        {
            "authors": [
                "Minghao Wu",
                "Abdul Waheed",
                "Chiyu Zhang",
                "Muhammad Abdul-Mageed",
                "Alham Fikri Aji"
            ],
            "title": "2023b. Lamini-lm: A diverse herd of distilled models from large-scale instructions",
            "year": 2023
        },
        {
            "authors": [
                "Linting Xue",
                "Noah Constant",
                "Adam Roberts",
                "Mihir Kale",
                "Rami Al-Rfou",
                "Aditya Siddhant",
                "Aditya Barua",
                "Colin Raffel."
            ],
            "title": "mT5: A massively multilingual pre-trained text-to-text transformer",
            "venue": "Proceedings of the 2021 Conference of the North American Chap-",
            "year": 2021
        },
        {
            "authors": [
                "Omar F Zaidan",
                "Chris Callison-Burch."
            ],
            "title": "Arabic Dialect Identification",
            "venue": "Computational Linguistics, 40(1):171\u2013202.",
            "year": 2014
        },
        {
            "authors": [
                "Susan Zhang",
                "Stephen Roller",
                "Naman Goyal",
                "Mikel Artetxe",
                "Moya Chen",
                "Shuohui Chen",
                "Christopher Dewan",
                "Mona Diab",
                "Xian Li",
                "Xi Victoria Lin"
            ],
            "title": "Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068",
            "year": 2022
        },
        {
            "authors": [
                "Shen Zheng",
                "Jie Huang",
                "Kevin Chen-Chuan Chang"
            ],
            "title": "Why does chatgpt fall short in answering questions faithfully",
            "year": 2023
        },
        {
            "authors": [
                "Qihuang Zhong",
                "Liang Ding",
                "Juhua Liu",
                "Bo Du",
                "Dacheng Tao."
            ],
            "title": "Can chatgpt understand too? a comparative study on chatgpt and fine-tuned bert",
            "venue": "ArXiv, abs/2302.10198.",
            "year": 2023
        },
        {
            "authors": [
                "Qihuang Zhong",
                "Liang Ding",
                "Juhua Liu",
                "Bo Du",
                "Dacheng Tao"
            ],
            "title": "2023b. Can chatgpt understand too? a comparative study on chatgpt and fine-tuned bert",
            "year": 2023
        },
        {
            "authors": [
                "Wenhao Zhu",
                "Hongyi Liu",
                "Qingxiu Dong",
                "Jingjing Xu",
                "Shujian Huang",
                "Lingpeng Kong",
                "Jiajun Chen",
                "Lei Li"
            ],
            "title": "Multilingual machine translation with large language models: Empirical results and analysis",
            "year": 2023
        },
        {
            "authors": [
                "Caleb Ziems",
                "William Held",
                "Omar Shaikh",
                "Jiaao Chen",
                "Zhehao Zhang",
                "Diyi Yang"
            ],
            "title": "Can large language models transform computational social science? CoRR, abs/2305.03514",
            "year": 2023
        },
        {
            "authors": [
                "Michal Ziemski",
                "Marcin Junczys-Dowmunt",
                "Bruno Pouliquen."
            ],
            "title": "The united nations parallel corpus v1",
            "venue": "0. In Lrec.",
            "year": 2016
        },
        {
            "authors": [
                "vital. Gao"
            ],
            "title": "2023) corroborate observations of Peng et al. (2023) by designing prompts that include information such as domain, finding it to improve the MT performance of ChatGPT",
            "year": 2023
        },
        {
            "authors": [
                "Hendy"
            ],
            "title": "2023) shows that ChatGPT is quite good for translating into high-resource target languages but its performance degrades for low-resource languages",
            "venue": "Zhu et al",
            "year": 2023
        },
        {
            "authors": [
                "Tan"
            ],
            "title": "2023) showcase ChatGPT\u2019s limitations on the knowledge-intensive tasks which require math and reasoning skills. They show that ChatGPT is far behind the fully supervised state-of",
            "year": 2023
        },
        {
            "authors": [
                "BERT (Devlin"
            ],
            "title": "2019) and RoBERTa (Liu et al., 2019) baselines. Authors conclude that ChatGPT outperforms BERT and RoBERTa on MNLI, SST2, and RTE while underperforming",
            "year": 2019
        },
        {
            "authors": [
                "Gilardi"
            ],
            "title": "ChatGPT\u2019s evaluation",
            "year": 2023
        },
        {
            "authors": [
                "Ziems"
            ],
            "title": "2023) investigate the zero-shot ability of ChatGPT across a broad spectrum of computational social science benchmarks encompassing various subject areas",
            "year": 2023
        },
        {
            "authors": [
                "Wang"
            ],
            "title": "2023a) attempt to test the robustness and generalization capability of ChatGPT",
            "venue": "The authors evaluate ChatGPT on AdvGLUE Wang et al. (2022a) and ANLI Nie et al",
            "year": 2020
        },
        {
            "authors": [
                "Chen"
            ],
            "title": "2023b) observe on average 35.74% and 43.59% performance drop in NLI and sentiment analysis",
            "venue": "SOTA",
            "year": 2023
        },
        {
            "authors": [
                "Huang"
            ],
            "title": "2023) propose cross-lingual thought prompting to improve the multilingual capabilities of ChatGPT and other LLMs",
            "year": 2023
        },
        {
            "authors": [
                "Wu"
            ],
            "title": "2023a) show that through careful exploitation of domain knowledge, ChatGPT can outperform the average human score on the China National Medical Licensing Examination (CNMLE)",
            "venue": "ChatGPT",
            "year": 2023
        },
        {
            "authors": [
                "Bouamor"
            ],
            "title": "MDPC is a humantranslated collection of 1K sentences in Egyptian, Tunisian, Jordanian, Palestinian, and Syrian Arabic, in addition to English. Code-Switching",
            "year": 2014
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Large language models (LLMs) pretrained on next token prediction brought significant progress to NLP. These models can be finetuned to follow human instructions, allowing users to steer model output (Wei et al., 2021; Wu et al., 2021; Chung et al., 2022b; Ouyang et al., 2022; Muennighoff et al., 2022a). ChatGPT1 is the most prominent among these models and has recently received significant\n1https://openai.com/blog/chatgpt\nattention due to its remarkable abilities. 2\nThe underlying model behind ChatGPT is believed to be pretrained on large datasets from multiple languages, which could enable the model to work well in multilingual settings. There is, however, much hype about ChatGPT\u2019s abilities. In fact, some observations around ChatGPT remain anecdotal, calling for a rigorous evaluation of its abilities in different languages. While multiple investigations of ChatGPT performance on English are accumulating fast (Qin et al., 2023a; Gilardi et al., 2023; Laskar et al., 2023), evaluation on other languages remains largely speculative. This lack of methodic evaluation inhibits our understanding of the capabilities and limitations of LLMs beyond English. This motivates our work for evaluating ChatGPT on the wide collection of languages and language varieties commonly referred to as Arabic. Arabic has rich morphologies and syntactic structures. It also has a vast native speaker population of over 450 million people, making investigations of it necessary for technological inclusion. Combined with this large population, the complexity of Arabic\n2In this paper, we use the term ChatGPT to refer to the gpt-3.5-turbo-0301 model.\nrequires models to have a deep understanding of expansive sociolinguistic contexts. This also makes the evaluation on Arabic all the more important to our understanding of how large multilingual models behave, with implications that go well beyond Arabic.\nConcretely, our objective is to systematically evaluate ChatGPT on a wide range of Arabic natural language understanding (NLU) and natural language generation (NLG) tasks, identifying existing gaps and ultimately possibly guiding the development of more effective Arabic and multilingual applications. Through our comprehensive benchmarking on 44 diverse tasks comprising over 60 datasets, we observe that ChatGPT exhibits inferior performance compared to much smaller finetuned models (Section 6 and Section 7). When we consider the performance of ChatGPT and GPT-4 on country-level dialectal Arabic as compared to Modern Standard Arabic (MSA), the modern-day variety used in formal settings, we reveal even more inferior dialectal performance (Section 8). To the best of our knowledge, this is the first work to study the comparison of ChatGPT and GPT-4 on Arabic dialectal variation.\nThe present study unambiguously demonstrates that, notwithstanding its noteworthy performance on standardized English benchmark datasets (Zhong et al., 2023a), the efficacy of ChatGPT as a universally applicable solution appears to be unsubstantiated when considering languages characterized by extensive linguistic variations, particularly in the context of Arabic. Succinctly put, it is evident that considerable enhancements can still be made pertaining to Arabic and other linguistically analogous languages. Our contributions can be summarized as follows:\n1. We rigorously evaluate ChatGPT on a wide range of Arabic tasks and varieties, offering the first work benchmarking ChatGPT on Arabic NLP at scale.\n2. We contextualize ChatGPT by comparing it against BLOOMZ, another relatively large model (7.1B), and two finetuned dedicated Arabic models.\n3. We perform a systematic evaluation of ChatGPT and GPT-4 on dialectal Arabic (DA) as compared to MSA, revealing the weakness of both models on especially dialects.\n4. We further conduct quality assessments on the models\u2019 generations using both human and GPT-4 as evaluators, finding GPT-4 evaluation to notably align with human evaluation.\n5. Through our empirical analyses, we find that ChatGPT significantly lags behind much smaller Arabic-focused finetuned models on almost all tasks. Our findings should motivate future work focused at improving LLM performance on Arabic languages and varieties."
        },
        {
            "heading": "2 Related Work",
            "text": "We provide a brief overview of the previous works that evaluate ChatGPT on NLP tasks here. We offer a detailed walkthrough in Appendix A.\nPerformance on English. In machine translation (MT), while ChatGPT equates to commercial tools like Google Translate (Jiao et al., 2023), it falls short in domain-specific translation and low-resource languages. However, strategies like pivot prompting (Jiao et al., 2023) and additional information-infused prompts (Peng et al., 2023; Gao et al., 2023) can enhance performance. In question-answering (QA), while ChatGPT shows potential (Zheng et al., 2023; Shen et al., 2023; Omar et al., 2023), it falters on complex opendomain questions and tasks requiring reasoning (Zheng et al., 2023; Tan et al., 2023) and is vulnerable to adversarial examples and perturbations (Shen et al., 2023). For text classification, ChatGPT performs well in zero-shot and few-shot settings (Zhong et al., 2023b; Gilardi et al., 2023; Wang et al., 2023b), sometimes matching or surpassing fully supervised models.\nPerformance on Multilingual Tasks. In multilingual tasks, ChatGPT is found to struggle with generating non-Latin scripts (Bang et al., 2023), but strategies such as prompting task descriptions in a high-resource language such as English can improve results (Lai et al., 2023). In line with this research, we find ChatGPT to work better with English prompts than Arabic prompts. Lai et al. (2023) evaluate ChatGPT in zero-shot setting on seven tasks covering 37 languages including Arabic. The authors show ChatGPT performs generally well for high-resource languages such as English, Russian, and German compared to mediumresource languages such as Arabic. Huang et al. (2023) introduce cross-lingual thought and assess\nthe multilingual capability of ChatGPT on 7 benchmarks. The authors specifically evaluate ChatGPT on Arabic for natural language inference task in few-shot settings. Compared to these concurrent efforts, our work conducts ChatGPT evaluation on Arabic at a much larger scale with 44 diverse tasks, as well as a comprehensive analysis of ChatGPT and GPT-4\u2019s performance on dialectal varieties. Wu et al. (2023a) find that ChatGPT fails to surpass the passing score in Chinese medical licensing exam. Similarly, Kasai et al. (2023) show that ChatGPT fails to pass the Japanese medical licensing exam. In our work, we conclusively show that ChatGPT is inferior to supervised models across a wide range of domains on several varieties of Arabic."
        },
        {
            "heading": "3 Datasets",
            "text": "We evaluate ChatGPT on both Arabic NLU and NLG tasks. For NLU, we use a total of 48 datasets representing 20 different tasks from the ORCA benchmark (Elmadany et al., 2023). The diverse coverage of these datasets lends our evaluation broad representativeness and applicability to many real-world scenarios. For NLG, we compile 23 publicly available datasets from across 13 task clusters. We briefly introduce our evaluation datasets below and provide a detailed description of them in Appendix B. NLU Tasks. In NLU, we include 20 tasks spanning across Arabic-NLI (1),3 claim prediction (1), dialect identification (3), machine-generated text detection (1), paraphrase detection (1), sentiment analysis and emotion detection (2), social meaning detection (9), stance detection (1), and word sense disambiguation (1). Task-specific details with full citation of all datasets are in Appendix B.1. NLG Tasks. We include 24 NLG tasks covering 13 task clusters: code-switched translation (2), diacritization (1), dialect translation (6), grammatical error correction (1), MT (4), news title generation (1), open-domain dialectal generation (3), paraphrasing (1), QA (1), question generation (1), text rewriting (1), transliteration (1), and summarization (1). Again, we provide task-specific details with appropriate references in Appendix B.2."
        },
        {
            "heading": "4 Prompt Design",
            "text": "A prompt is a set of instructions provided to an LLM that programs the model by enhancing its purpose and capabilities (White et al., 2023). A\n3Number of tasks in each cluster are in parentheses.\n0- Sh ot kSh ot\nprompt can influence subsequent interactions with the model as well as its generated outputs by defining a set of specific rules. Therefore, in order to acquire a desired outcome, it is important to clearly define the set of rules and intents to the model. In our initial experiments, we experimented with a diverse set of prompt templates in both English and Arabic. Specifically, we evaluated on dialect identification, machine-generated text detection, and toxicity detection for NLU and machine translation tasks for NLG with both English and Arabic prompts, observing that the English prompt outperforms its Arabic counterpart. Hence, we design a universal prompt template in English as follows: (1) We first set the role of the model (e.g., as an annotator for NLU tasks). (2) We provide name of the task that needs to be performed (e.g., sentiment classification). (3) We define what should be the expected outcome of the model (e.g., the label set for an NLU task). (4) If the task involves k-shot (k > 0) learning, we provide the model with k examples. (5) Finally, we deliver test input to the model to produce the output. We present the templates of our designed prompts in Figure 2. We offer examples of k-shot NLU and NLG tasks in Appendix H."
        },
        {
            "heading": "5 Experiments",
            "text": "We run experiments under 0-shot, 3-shot, 5-shot, and 10-shot settings. For the training data of the few-shot experiments, we randomly sample from each respective training dataset.4 For a k-shot setting, we make sure that if a training sample is selected then it will also be selected for n-shot settings where n > k. For evaluation, we randomly sample a set of 200 examples from the test set of each dataset, to keep the cost man-\n4Comparison between the class distribution of the whole training set and the few-shot samples is in Appendix F.\nageable. We evaluate ChatGPT (gpt-3.5-turbo),5 which is an optimized version of GPT-3.5 series. We set the temperature to 0.0 while generating responses from ChatGPT. We compare this model with BLOOMZ (7.1B parameters),6 which is finetuned on a diverse set of tasks on 46 languages including Arabic (Muennighoff et al., 2022b). We choose BLOOMZ since it is claimed to achieve impressive generalization on unseen tasks. Following Chung et al. (2022a), we use free-form generation for both ChatGPT and BLOOMZ along with simple post-processing, e.g. removing leading or trailing whitespace. Additionally, to further situate performance of ChatGPT, we finetune MARBERTV2 (Abdul-Mageed et al., 2021a) and AraT5 model (Nagoudi et al., 2022b). Again, we choose these models as they are reported to achieve SOTA on NLU and NLG in Abdul-Mageed et al. (2021a) and Nagoudi et al. (2022b), respectively. For both MARBERTV2 and AraT5, we identify the best model on the respective development split (Dev). We train MARBERTV2 for 25 epochs with a patience of 5 and AraT5 for 10 epochs. For both models, we set the learning rate to 5e-5 across all the tasks. We present our overall experimental setup in Figure 1. In addition to reporting the performance of finetuned models on the same 200 set as ChatGPT and BLOOMZ, we report the 3-run average of the models\u2019 performances on the respective full test sets (reported in gray). For NLU tasks, we use macro-F1 scores and for NLG tasks we use the appropriate metric suited to each task (Table 2). We provide examples of NLG model responses in Appendix I."
        },
        {
            "heading": "6 Evaluation on NLU Tasks",
            "text": "Overview. We present our evaluation on NLU tasks in Table 1. We observe that although ChatGPT outperforms instruction-tuned multilingual models like BLOOMZ, the smaller finetuned model MARBERTV2 achieves significantly better results than ChatGPT. We also find that model performance does not necessarily increase as we increase the number of shots. The common wisdom for this behavior is that improvement with few-shot learning is model- and task-dependent, and is often sensitive to the order of the shots Wei et al. (2021); Brown et al. (2020); Lu et al. (2022). We now discuss performance on each understanding task.\n5Snapshot of gpt-3.5-turbo from March 1st 2023. 6https://huggingface.co/bigscience/BLOOMZ-7b1\nEmotion and Sentiment. ChatGPT achieves 17.33% F1 on emotion detection with 5-shot learning, while BLOOMZ achieves 17.13 with 10-shot. Both of these models, however, perform much lower than finetuned MARBERTV2 (F1=68.74). For sentiment, ChatGPT (64.96)7 outperforms BLOOMZ (50.44) by a margin of 14.52 F1 scores. Again, MARBERTV2 (F1=68.92) outperforms both models. Dialect. On binary-level dialect classification, ChatGPT achieves 77.00 F1 with 3-shot learning, outperforming BLOOMZ (49.68 F1) by 27.32 points. Meanwhile, MARBERTV2 (88.00) significantly outperforms both models. On region-level dialect, ChatGPT achieves 30.69 F1, outperforming BLOOMZ (30.19). MARBERTV2 outperforms both models (89.69). On country-level dialect, BLOOMZ exhibits lower performance (14.61) compared to ChatGPT (19.74) and MARBERTV2 (33.88). Similar to the two other dialect tasks, ChatGPT falls behind the much smaller model on country-level dialect ID. Claim and Machine-Generated Text. For claim verification, performance of ChatGPT increases with the increased number of shots (achieving 61.82 F1). BLOOMZ scores less than half (30.87) compared to ChatGPT. ChatGPT falls behind MARBERTV2 by a margin of 3.71. On machinegenerated text detection, ChatGPT and BLOOMZ achieve comparable performance (47.81 and 45.83, respectively). MARBERTV2 (76.23) outperforms both models by a large margin. Toxic Text. On abusive language detection, the best score ChatGPT achieves is 42.03% (with 5- shot learning). This is close to BLOOMZ (42.26, with 5-shot). Again, MARBERTV2 is much better than both (82.96). On hate speech detection, ChatGPT achieves 50.99. It is outperformed by BLOOMZ, which acquires 57.88 with 10-shot learning. Both models are significantly outperformed by MARBERTV2 (78.95). On offensive language, ChatGPT is at 73.76 (with 10-shots), and even its 0-shot outperforms all few-shots of BLOOMZ. Similar to other tasks, MARBERTV2 performs best (97.37). Given concerns about models like ChatGPT generating toxic language, we also manually inspect its errors finding it to be sensitive to false toxicity (i.e., it tends to flag non-toxic text as toxic). Refer to Appendix D for our full toxicity analysis. Irony and Sarcasm. On irony detection, 0-shot ChatGPT achieves 67.27, outperforming BLOOMZ\n7When we report a result without reference to the number of shots, it is typically the best result across all shots.\nbut again lags behind MARBERTV2 (85.83) by a margin of 18.56 points. On sarcasm, ChatGPT (43.99) outperforms BLOOMZ (30.94) but is almost half of MARBERTV2 performance (80.10). Adult and Dangerous Content. ChatGPT achieves 66.12 F1 with 10-shot learning on adult content detection, while BLOOMZ achieves at best 63.70 (with 3-shots). Aligning with the general trend thus far, MARBERTV2 (93.95) outperforms both models by a significant margin. On the dangerous content dataset, ChatGPT only achieves 37.11 F1. Interestingly, ChatGPT is outperformed by BLOOMZ (45.07) in 0-shot learning. However, ChatGPT dominates BLOOMZ in all other few-shot setups. Again, MARBERTV2 outperforms both models (63.97). Demographic Text Classification. On age prediction, ChatGPT achieves 44.11 F1 (with 10-shots), whereas BLOOMZ is at 29.64 (with 5-shots). Here, MARBERTV2 (49.41) outperforms ChatGPT by 5.30. On the gender task, ChatGPT (65.04) outperforms BLOOMZ (40.48). Also, ChatGPT performs better than MARBERTV2 (64.97) by a slight margin. Word Sense Disambiguation. ChatGPT achieves the best score of 53.49 with 3-shots. The other few-shot settings of ChatGPT are outperformed by the corresponding few-shot settings of BLOOMZ. Surprisingly, the finetuned model MARBERTV2 is outperformed by both ChatGPT and BLOOMZ by a significant margin. We suspect this is due to the issue of anisotropy (Ethayarajh, 2019; Li et al., 2020) in BERT models, which we further discuss in Appendix C. Text-Pair Tasks. On paraphrase identification, ChatGPT with 10-shots (85.80) outperforms both BLOOMZ (63.66 with 5-shot) and MARBERTV2 (65.98) by a large margin. Strikingly, ChatGPT with only 3- or 5-shots outperforms the fully-finetuned MARBERTV2 model. This shows the remarkable ability of ChatGPT on semantic tasks such as paraphrase detection. On stance detection, all the few-shot setups of ChatGPT outperform BLOOMZ by a significant margin. However, MARBERTV2 (F1=88.23) outperforms ChatGPT. On natural language inference (XNLI), ChatGPT achieves F1 of 56.43 with 0-shot learning, while the best score of BLOOMZ is at 26.32 (for 10-shots). Both models are outperformed by MARBERTV2 (62.89)."
        },
        {
            "heading": "7 Evaluation on NLG Tasks",
            "text": "Overview. We present the results of our evaluation on generation tasks in Table 2. For NLG, we notice that ChatGPT performs better than BLOOMZ on the majority of tasks. However, following a similar trend to NLU, finetuned AraT5 consistently outperforms ChatGPT. We now present our results for each group of NLG tasks. Text Rewriting and Paraphrase. For text rewriting, BLOOMZ achieves 76.67 BLEU scores (with 0-shots) which is better than ChatGPT results. Surprisingly, BLOOMZ\u2019s performance deteriorates as we\nincrease the number of training examples whereas ChatGPT shows best performance (62.62) with 10- shots. However, both models are significantly dominated by AraT5. For paraphrase generation, BLOOMZ outperforms ChatGPT in all k-shot setups. Noticeably, the performance of ChatGPT monotonically improves with the increased number of shots. AraT5 outperforms both ChatGPT and BLOOMZ with a BLEU of 14.40.\nQuestion Generation and Question Answering. For question generation, ChatGPT is outperformed by 0-shot performance of BLOOMZ. Nevertheless, unlike ChatGPT, BLOOMZ performance does not consistently improve as we increase the number of training shots. Compared to AraT5, both ChatGPT and BLOOMZ exhibit significantly lower scores. For QA, BLOOMZ significantly outperforms ChatGPT in all the few-shot settings. Specifically, BLOOMZ achieves the best score of 76.04 with 0-shot learning, whereas ChatGPT achieves 54.14 at best with 5-shot learning. However, both models are outperformed by AraT5 (81.45). We suspect BLOOMZ performs well on QA since it has been explicitly finetuned on this task using Arabic data.\nSummarization and News Title Generation. For summarization, ChatGPT achieves 20.43 ROUGE, outperforming BLOOMZ by a margin of 6.87 points. Both ChatGPT and BLOOMZ are also outperformed by AraT5. For news title generation, ChatGPT is at 4.72 BLEU points whereas BLOOMZ struggles (1.0 BLEU). Again, AraT5 is better than both (7.72).\nDiacritization and Transliteration. ChatGPT dominates BLOOMZ with significantly lower error rates for diacritization. BLOOMZ even struggles to keep the character error rate (CER) lower than\n1, which means it mistakenly inserts additional characters during prediction. Although ChatGPT (CER=0.05) exhibits impressive performance on this task, AraT5 (CER=0.03) still outperforms it. For transliteration, ChatGPT (0.23) again outperforms BLOOMZ (0.42). However, AraT5 (0.18) achieves even lower error rates than both.\nMachine Translation. ChatGPT outperforms BLOOMZ on all the X (English, Spanish, French, Russian) \u2192 Arabic MT tasks. As expected, both models perform better when English is used as the source language. Although the performance gap is smaller in MT as compared to the general trend in other tasks, AraT5 is still better than ChatGPT.\nCode-Switched Translation. For Jordanian Arabic (Jo) mixed with English \u2192 English and Algerian Arabic (Dz) mixed with French \u2192 French code-switched translation tasks, ChatGPT significantly outperforms BLOOMZ by 10-30 points. ChatGPT performs slightly better for mixed English \u2192 English, while BLOOMZ performs better in mixed French \u2192 French than mixed English \u2192 English translation. Interestingly, both ChatGPT and BLOOMZ show highly superior performance than the finetuned AraT5. We suspect that both models have been pretrained with a lot of data from highresource languages like English and French. This pretraining step helps them to perform better than the Arabic-dedicated smaller model.\nGrammatical Error Correction. We present M2Scorer F0.5 score (Dahlmeier and Ng, 2012) for GEC in Table 2. Due to the longer sequence length (> 4, 096), we exclude 10-shot evaluation for this task. We find ChatGPT to significantly outperform BLOOMZ (48.72 vs. 2.40 F0.5 score). However,\nAraT5 (67.54) is much better than both."
        },
        {
            "heading": "8 Performance on Dialectal Arabic",
            "text": "DA has traditionally been only spoken. It only became easier to collect dialectal data with the proliferation of social media where these varieties started to be used. Regardless, most Arabic dialects remain under-investigated due to rarity of resources. Motivated by this, we dedicate the current section to investigate performance of ChatGPT on DA as compared to MSA. In the context of this investigation, we also compare ChatGPT and GPT-4 on these two broad categories of Arabic.\nFor the current analysis, we first select tasks labeled as involving more DA in ORCA (i.e., all 11 tasks in Figure 3). We then run a strong in-house MSA vs DA classifier (\u223c 88% F1) to separate MSA and DA samples, keeping only samples predicted with 80% confidence. This gives us an average of 119 DA and 78.82 MSA samples across all 11 tasks. We then evaluate the macro-F1 performance of ChatGPT and GPT-4 on these selected samples. Dialectal NLU. To identify the extent to which GPT-4 can detect country-level dialect, we run it on our test data under 0-shot. We find GPT-4 to achieve 26.92 F1, which is 13.06 improvement over ChatGPT performance reported in Table 1 (i.e, 13.86). As shown in Figure 3, both ChatGPT and\nGPT-4 perform better on MSA compared to DA (on 8 and 9 out of the 11 tasks, respectively). It is also clear that GPT-4 outperforms ChatGPT on both MSA and DA for the majority of the tasks (9 tasks for MSA and 7 tasks for DA). On average, GPT-4 improves over ChatGPT by 18.62% for MSA and 10.40% for DA tasks. This finding suggests that (i) both ChatGPT and GPT-4 have likely seen more\nMSA data than DA at some stage of their development and (ii) GPT-4 performs generally superior than ChatGPT in both MSA and DA samples. Dialectal NLG. To evaluate ChatGPT on dialectal generation, we run it on MSA and five Arabic dialects \u2192 English MT tasks from the Multi-dialectal Parallel Corpus (MDPC) proposed by Bouamor et al. (2014). Namely, we use data involving Egyptian, Jordanian, Palestinian, Syrian, and Tunisian.\n(a) K-shot BLEU scores of ChatGPT on MSA and 5 dialects \u2192 English MT. (b) Zero-shot BLEU sores of ChatGPT and GPT-4 on MSA and 5 dialects \u2192 English MT.\nFigure 4: ChatGPT and GPT-4 on dialectal MT.\nAs Figure 4a shows, ChatGPT MSA performance is better than its dialectal performance on all our k-shot settings. This further substantiates what our NLU results above suggest as to ChatGPT\u2019s MSA and DA abilities. Comparing results across dialects, ChatGPT performs best on Egyptian, which may be directly due to availability of Egyptian dialectal data online as compared to rarity of data from other dialects (Nagoudi et al., 2022a). ChatGPT compared to GPT-4. We carry out an additional set of experiments to compare ChatGPT and GPT-4 on the same five dialects and MSA test sets from Bouamor et al. (2014) listed above, but subsampling only 50 data points from each and reporting both models in zero-shot over the subsample. As Figure 4b shows, for MSA and all dialects except Jordanian, GPT-4 still outperforms ChatGPT. We also notice that GPT-4 wins with a large margin on dialects such as Palestinian, Syrian, and Tunisian, all of which are on the low-resource side compared to dialects such as Egyptian. This finding suggests that, compared to ChatGPT, GPT-4 may have seen more Arabic varieties, and perhaps more data from some of the varieties."
        },
        {
            "heading": "9 Human Evaluation",
            "text": "We also perform a set of human evaluations, motivated by the potential of human evaluation to capture subtleties of language that automated metrics may overlook. We carry out our evaluation on eight\nHuman GPT-4\nBLOOMZ ChatGPT GPT-4 BLOOMZ ChatGPT GPT-4\nParaphrase CST (Dz-fr -> fr)\nMT (fr -> ar)\nCST (Jo-en -> en)\nSummarization\nDG. (egy) DG. (lev) DG. (gul)\nFigure 5: Evaluation of the models\u2019 responses by human and GPT-4. A is the best, and D is the worst rating. MT - Machine Translation, CST - Code Switched Translation, DG - Dialogue Generation.\nNLG tasks: code-switched translation (2), dialogue generation (3), machine translation (1), paraphrase (1), and summarization (1). We particularly pick these tasks as they represent diverse generation task categories and ChatGPT performs poorly on some of them in our automatic evaluation setting (Table 2).\nEvaluation Setup. We build our human evaluation framework on top of Wang et al. (2022b), Wu et al. (2023b) and implement a four-level (A, B, C, D) rating system that we adapt to language generation. (Refer to Appendix G.1 for details). We prepare the framework with examples and ask three pairs of native Arabic speakers (total=6) to rate 50 samples each from the output of each task, based on how effectively a sample has fulfilled the task described in the input prompt.\nResults. We aggregate scores from each pair of annotators and use Cohen\u2019s kappa metric to measure the inter-annotator reliability. We find the interannotator agreement more than 0.6 for the majority of the tasks. We report the result in Figure 5 (Refer to Appendix L for more detailed results). From human evaluation, we find that outputs produced by GPT-4 and ChatGPT, for all tasks, are rated as mostly of fine quality. We also find that ChatGPT is rated higher than BLOOMZ for summarization in the human evaluation than it is in the automated setting, perhaps reflecting that, although useful, ROUGE is not an ideal metric for summarization as it captures only token-level overlap rather than deep semantics. In addition, ChatGPT is rated higher than BLOOMZ in human evaluation than in automated evaluation for paraphrasing. This, again, reflects BLEU not being ideal for paraphrase detection for the same reasons as in the case of summarization.\nHuman Analysis on CST. As evident from Table 2, ChatGPT outperforms the finetuned model on the code-switched translation task. This prompted us to further probe ChatGPT \u2019s code-switching abil-\nity using diagnostic test cases. Specifically, we manually evaluate ChatGPT \u2019s capability of English mixed MSA, Egyptian, and Moroccan codeswitched translation generation from plain English text. We ask two annotators who are fluent on the respective varieties, as well as English, to evaluate a diagnostic dataset based on fluency (defined as how fluent the translated text is), faithfulness (defined as how semantically close the translated text is to the source text), and code-switching ability (defined as how accurately the translated text includes code-switching). We present our result in the Table 3.\nWe observe that ChatGPT produces fluent translations that are also semantically close (faithful) to the source text. However, ChatGPT struggles to produce code-switched text (i.e., it generates mostly in Arabic script). Interestingly, this issue is more prevalent for MSA than Egyptian and Moroccan. We hypothesize that this discrepancy cuts across several linguistic categories and involves topics such as translation of endearment expressions; multi-word expressions, idioms, and proverbs; negation; sub-token-level code-switching; and dialectal variants of MSA lexica. We further suspect that the tokens in the source English text are very common in MSA. As a result, the model does not seem able to code-switch the words into English.\n10 Using GPT-4 to Evaluate Responses\nSimilar to Chiang et al. (2023) and Zhou et al. (2023), we assess the quality of the generated responses from ChatGPT, BLOOMZ, and GPT-4 using the GPT-4 model itself. For this purpose, we design a prompt (Figure 8 in Appendix E) that takes the input and the corresponding responses from the models and asks GPT-4 to rate between \u2018A\u2019 and \u2018D\u2019 (where \u2018A\u2019 is the highest) for each response. (Refer to Appendix J and Appendix K for illustrative samples). We do this analysis using a random sample of\n25 data points from each of the datasets evaluated by human annotators cited in Section 9 above.8 As Figure 5 shows, GPT-4 provides more accurate and satisfying responses (compared to ChatGPT and BLOOMZ), followed by ChatGPT, in almost all cases. BLOOMZ is rated \u2018D\u2019 on all of the tasks. Upon inspecting the explanations from GPT-4, we find that this poor rating of BLOOMZ is mostly due to it just copying the input samples rather than following the input instruction properly.\nDoes GPT-4 eval correlate with human eval? Chiang et al. (2023) and Zhou et al. (2023) show a positive correlation between human and GPT-4 evaluation, which we also test for Arabic NLP in this work. Hence, we compute the percentage of evaluation agreement between humans and GPT-4 on the models\u2019 responses. More specifically, for each task, we calculate at how many occurrences human evaluators agree with GPT-4 evaluation. As Figure 6 shows, at least one human evaluator agrees with GPT-4 evaluation 71.5% of the time on average. This suggests that it may be possible to use GPT-4 to automate the time-consuming and costly process of using humans for assessing model performance. This is at least true for the tasks we consider."
        },
        {
            "heading": "11 Conclusion",
            "text": "We presented a comprehensive evaluation of ChatGPT on 44 diverse Arabic NLP tasks. Our eval-\n8This gives us 25 samples x 8 datasets = 200 decisions, for which we use the OpenAI playground. At the time of writing this paper, OpenAI allowed only 25 examples per 3 hours on GPT-4 playground.\nuation covers both NLU and NLG under k-shot settings. Comparing results from ChatGPT to BLOOMZ and finetuned models, we show that ChatGPT, in spite of its large size, can struggle on multiple Arabic tasks compared to much smaller finetuned models. We conduct an extensive quality assessment using both human and GPT-4 evaluation of model responses and find a positive alignment between human and GPT-4 judgment. We further investigate the performance of ChatGPT and GPT-4 on MSA vs DA tasks, revealing inherent limitations of both models, particularly when applied to dialects. Our findings accentuate the inherent multilingual limitations observed in both ChatGPT and GPT-4, thereby indicating a substantial scope for improving these models. We intend to expand our evaluations to encompass additional low-resource languages."
        },
        {
            "heading": "12 Limitations",
            "text": "Experimental Setup. In this work, we evaluate with a randomly selected subset of test samples\nto keep the cost manageable. Although the random selection should ideally represent the whole distribution, the performance may vary slightly when comparing to the whole test set. Additionally, ChatGPT\u2019s version can be updated on a regular interval. Hence, the results and the analyses reported here should be treated accordingly, since the model\u2019s responses can change over time (Chen et al., 2023a). To save time and cost, we perform GPT-4 generation and human, GPT-4 evaluation on a diverse but selective tasks, which we wish to extend to the other tasks in the future. Model Variation. We perform evaluation on three multilingual LLMs, namely, ChatGPT, GPT-4, and BLOOMZ. Additionally, we finetune two Arabic language dedicated SOTA models i.e., MARBERTV2 and AraT5. To keep our work resource-efficient, we do not include other massive LLMs such as PaLM 540B (Chowdhery et al., 2022). Also, since we are already comparing against the finetuned SOTA models (MARBERTV2 and AraT5), we exclude other multilingual models like mT5 (Xue et al., 2021). However, we acknowledge that incorporating a large number of models can facilitate the comparative analysis. Model Evaluation. On open-domain dialectal dialogue generation tasks, we empirically find that all the models perform poorly (close to 0.0 BLEU) with the automatic metric. Since these tasks require the responses to be fluent and coherent and not particularly follow any closed form (e.g., MT tasks), it would not be fair to evaluate the models using such automated metrics. Therefore, we exclude the dialectal dialogue generation tasks from the automatic evaluation (Section 7) and conduct both human (Section 9) and GPT-4 evaluation (Section 10) on them. Findings. Some of our findings suggest that GPT-4 can be employed to automate the process of evaluating model responses, thus replacing humans. Although this can accelerate development of AI systems and deploying them for decision making, it might also have negative implications on workforce. In particular, we recommend that our findings should be couched with caution and not be considered as a reference to replace workforce with AI."
        },
        {
            "heading": "13 Ethics Statement",
            "text": "Data Collection and Release. We collect the NLU evaluation datasets from ORCA. For NLG tasks,\nwe collect from 23 publicly available datasets. To ensure proper credit assignment, we refer users to the original publications (Appendix B). Intended Use. We believe our work will spur further research on studying LLMs on Arabic NLP benchmarks. As our findings show, the existing multilingual LLMs still lag behind compared to the smaller finetuned models on the majority of Arabic NLP tasks. Therefore, our work can arise the interest among the researchers to develop Arabic language dedicated LLMs that can match or outperform the SOTA finetuned models. Potential Misuse and Bias. Since there exists little to no clarity on the training data of ChatGPT and GPT-4, these LLMs can produce potentially harmful and biased contents (Laskar et al., 2023). Therefore, we recommend that these models not be used in applications without careful prior consideration of potential misuse and bias."
        },
        {
            "heading": "Acknowledgments",
            "text": "We acknowledge support from Canada Research Chairs (CRC), the Natural Sciences and Engineering Research Council of Canada (NSERC; RGPIN2018-04267), the Social Sciences and Humanities Research Council of Canada (SSHRC; 435-2018- 0576; 895-2020-1004; 895-2021-1008), Canadian Foundation for Innovation (CFI; 37771), Digital Research Alliance of Canada,9 and UBC Advanced Research Computing-Sockeye.10"
        },
        {
            "heading": "A Literature Review",
            "text": "A.1 Machine Translation Language models trained on large-scale multilingual data have proven effective for a wide range of tasks spread across multiple languages. Jiao et al. (2023) evaluate ChatGPT for machine translation (MT) tasks, reporting that ChatGPT\u2019s performance on MT is on par with commercial translation tools such as google-translate. However, they also find that when translating into a distant language pivot prompting is very effective. In pivot prompting, instead of directly translating the source into the target, first translating the source into a high resource similar to the target language and then translating into the target is followed. Further, Jiao et al. (2023) notice that for domain-specific translation (e.g., in the biomedical filed), ChatGPT\u2019s performance degrades considerably. Peng et al. (2023) find that ChatGPT performs reasonably well in highresource and domain-specific settings but providing additional information can be vital. Gao et al. (2023) corroborate observations of Peng et al. (2023) by designing prompts that include information such as domain, finding it to improve the MT performance of ChatGPT. Extensive evaluation by Hendy et al. (2023) shows that ChatGPT is quite good for translating into high-resource target languages but its performance degrades for low-resource languages. Zhu et al. (2023) evaluate ChatGPT and other LLMs such as XGLM (Lin et al., 2022), OPT (Zhang et al., 2022), and BLOOMZ (Muennighoff et al., 2022a) showing that even though ChatGPT is the best zero-shot and in-context few-shot model considered, it lags behind full supervised no language left behind models (NLLB) (Team et al., 2022).\nA.2 QA ChatGPT demonstrates impressive results on question-answering tasks including user queries where no context is provided. Zheng et al. (2023) evaluate ChatGPT on complex open-domain QA tasks, studying the failures of ChatGPT and proposing methods to improve the faithfulness of its answers. Tan et al. (2023) showcase ChatGPT\u2019s limitations on the knowledge-intensive tasks which require math and reasoning skills. They show that ChatGPT is far behind the fully supervised state-ofart models on these reasoning tasks. Further evaluations of ChatGPT by Shen et al. (2023) on a wide range of QA tasks demonstrate ChatGPT\u2019s perfor-\nmance across various domains. The authors show that ChatGPT underperforms on domain-specific QA and is also very susceptible to adversarial examples and perturbation. Omar et al. (2023) evaluate ChatGPT for QA on knowledge graphs. They notice that even though it falls behind the supervised SOTA methods, it can be a robust QA system for knowledge graphs.\nA.3 Text Classification\nText classification is one task where ChatGPT does exceptionally well in zero-shot and in-context fewshot settings. It is often even on par with fullsupervised models. Zhong et al. (2023b) evaluate ChatGPT on GLUE NLU benchmark (Wang et al., 2019) and compare it against fully supervised BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019) baselines. Authors conclude that ChatGPT outperforms BERT and RoBERTa on MNLI, SST2, and RTE while underperforming these models on other GLUE tasks. Further, ChatGPT\u2019s evaluation by Gilardi et al. (2023) shows that it can outperform well-trained human annotators and crowd-workers for text classification tasks such as relevance, stance, topics, and frame detection. Wang et al. (2023b) test ChatGPT on a wide range of sentiment analysis datasets and find that zero-shot ChatGPT is on par with the fully supervised BERT model but lags behind the SOTA. The non-deterministic nature of ChatGPT prompted Reiss (2023) to assess the reliability and consistency of ChatGPT for text classification tasks in the zero-shot setting. They argue that zero-shot outputs for text classification do not meet the scientific threshold of reliability. The authors also find that minor alterations in prompts can change ChatGPT output. They recommend pooling the outputs from multiple repetitions to improve reliability.\nZiems et al. (2023) investigate the zero-shot ability of ChatGPT across a broad spectrum of computational social science benchmarks encompassing various subject areas including sociology, psychology, literature, history, linguistics, and political science. They find that ChatGPT demonstrates poor performance on tasks characterized by structural complexity (e.g., event arguments) or those entailing subjective expert taxonomies (e.g., hate, empathy). However, ChatGPT attains high performance on tasks that involve either objective ground truth (like fact-checking tasks) or explicit defini-\ntion labels (e.g., anger in emotion detection). Furthermore, it is observed that ChatGPT exhibits a tendency to predict a neutral label that is easily recognized in the colloquial language (e.g., stereotype in the hate speech detection task), instead of utilizing a more precise label derived from the provided taxonomy (e.g., white grievance). Qin et al. (2023b) evaluate ChatGPT on 20 NLP datasets encompassing seven task categories in a zero-shot setting. Although ChatGPT performs less effectively than models fine-tuned specifically for each task, it demonstrates superior reasoning capabilities compared to other instructed finetuned models (e.g., FLAN) in natural language inference, arithmetic reasoning, and QA tasks. Robustness and Generalization. ChatGPT does well on a wide range of downstream NLP tasks often yielding SOTA results if prompted properly. However, several studies reveal that its performance downgrades on domain-specialized tasks and is very susceptible to adversarial examples. Wang et al. (2023a) attempt to test the robustness and generalization capability of ChatGPT. The authors evaluate ChatGPT on AdvGLUE Wang et al. (2022a) and ANLI Nie et al. (2020) for adversarial robustness. They perform an evaluation on Flipkart Review and DDXPlus medical diagnosis for the out-of-distribution (OOD) generalization. The results show that ChatGPT outperforms all considered models (including zero-shot and fully supervised SOTA models) on every task in both settings. Chen et al. (2023b) observe on average 35.74% and 43.59% performance drop in NLI and sentiment analysis tasks, respectively, which further highlights ChatGPT\u2019s vulnerability to challenging and complex scenarios.\nA.4 Multilinguality\nChatGPT is adept at generating high-quality responses to user queries. It also demonstrates impressive capabilities in multiple languages. Lai et al. (2023) evaluate ChatGPT on seven different tasks and 37 different languages belonging to low, medium, and high resource families. The results show that ChatGPT is either at par or even better in some tasks compared to fully supervised SOTA baselines, particularly for the high-resource languages. They also observe that for the low-resource language, providing task descriptions in a highresource language can be helpful. The work on the multilingual evaluation of ChatGPT by Bang et al.\n(2023) establishes that it is better at understanding non-Latin scripts but poor at generating them. Huang et al. (2023) propose cross-lingual thought prompting to improve the multilingual capabilities of ChatGPT and other LLMs. The experiments by Wu et al. (2023a) show that through careful exploitation of domain knowledge, ChatGPT can outperform the average human score on the China National Medical Licensing Examination (CNMLE). Similarly, Kasai et al. (2023) evaluate ChatGPT and other GPT family LLMs on Japanese national medical licensing examinations. They show that GPT-4 passes all six years of the exams, showcasing LLMs\u2019 impressive capability in a language that is typologically distant from English. However, they notice that ChatGPT and GPT3 fail to reach passing criteria and are prone to choosing prohibited options. Although several works (Alammary, 2022; Abu Farha and Magdy, 2021) benchmark transformer models on Arabic understanding tasks like sentiment analysis, to the best of our knowledge, our work is the first to evaluate ChatGPT on Arabic NLU and NLG at scale."
        },
        {
            "heading": "B Dataset",
            "text": "B.1 NLU Tasks\nWe evaluate on four task clusters from ORCA (Elmadany et al., 2022), as follows:11 Sentence Classification. This cluster involves the following tasks and datasets (1) Sentiment Analysis: (Abdul-Mageed et al., 2021b). (2) Social Meaning: Refers to eight social meaning datasets covering prediction of hate and offensive language detection (Mubarak et al., 2020), dangerous speech (Alshehri et al., 2020), sarcasm (Farha and Magdy, 2020), adult content (Mubarak et al., 2021), irony (Ghanem et al., 2019), emotion, age and gender (Mohammad et al., 2018; Abdul-Mageed et al., 2020b). (3) Dialect Identification: Involves three dialect classification levels, binary-level (i.e., MSA vs. DA), region-level (four regions), and country-level (21 countries). The three tasks are built using six datastes: ArSarcasmDia (Farha and Magdy, 2020), AOC dataset (Zaidan and CallisonBurch, 2014), NADI-2020 (Abdul-Mageed et al., 2020a), MADAR (Bouamor et al., 2019), QADI (Abdelali et al., 2020), and Habibi (El-Haj,\n11From ORCA, we exclude topic classification datasets since these typically have long sequences that can often exceed ChatGPT maximum length of 4, 096 tokens.\n2020). (4) Claim Prediction: ANS-claim (Khouja, 2020). (5) Machine Generation, for machinegenerated text detection (Nagoudi et al., 2020). Paraphrase Detection. The goal of this cluster is to identify the similarity between a pair of sentences from a semantic perspective. This cluster contains a semantic text similarity (STS) task and a paraphrase classification task. For our evaluation, we exclude STS, which is a regression task and experiment with paraphrase classification using Arabic Q2Q (Seelawi et al., 2019). Natural Language Inference. This cluster covers the following two tasks: (1) Arabic NLI: Determining whether a text (hypothesis) is false (contradiction), undetermined (neutral), or true (entailment), given a text (premise). This task uses the Arabic part of XNLI corpus (Conneau et al., 2018). (2) Fact-checking: The two datasets Unified-FC (Baly et al., 2018) and ANS (Khouja, 2020) are used to target stance and factuality prediction of claims from news and social media. Word Sense Disambiguation (WSD). The Arabic WSD benchmark (El-Razzaz et al., 2021), an MSA context-gloss pair dataset, is used for this task.\nB.2 NLG Tasks\nFor NLG, we create a benchmark using a collection of 23 publicly available datasets from different genres. We arrange our NLG datasets into 13 different task clusters: Machine Translation and Dialect Translation. The MT cluster is built around the tasks of X \u2192 MSA, where we test the ability of ChatGPT to translate from four foreign languages into MSA. For this, we use the United Nations Parallel Corpus (Ziemski et al., 2016) that covers the six official UN languages: Arabic, English, French, Russian, and Spanish. The dialectal translation cluster consists of Arabic Dialects \u2192 English, where we focus on MT from five Arabic dialects into English using the Multi-dialectal Parallel Corpus (MDPC) proposed by Bouamor et al. (2014). MDPC is a humantranslated collection of 1K sentences in Egyptian, Tunisian, Jordanian, Palestinian, and Syrian Arabic, in addition to English. Code-Switching. The purpose of the codeswitching (CS) task is to translate Arabic dialectal text involving code-switching from a foreign language into that foreign language. We use two human-written (natural) code-switched parallel test\nsets proposed by (Nagoudi et al., 2022b): (1) DZFR \u2192 FR. It consists of code-switched Algerian dialect and French Twitter posts. These posts are manually translated into monolingual French. (2) JO-EN \u2192 EN. This is collected from Jordanian Twitter and consists of code-switched Jordanian dialect and English posts, which are manually translated into monolingual English. Summarization and Title Generation. For this cluster, we use XLSum (Hasan et al., 2021), a diverse, multilingual summarization dataset from BBC news supporting 44 languages (including Arabic). The Arabic part of XLSum is divided into 37.5K for Train and 4.7K for each of the Dev and Test splits. The news articles in XLSum are annotated with summaries and titles, allowing us to use the articles and corresponding titles to evaluate our title generation models. Question Answering and Generation. For both of these tasks, we use TyDiQA (Artetxe et al., 2020), a publicly available, multilingual, human-translated QA datasets. For the QA task, we use (Input: passage, question, and Output: answer) triplets. For the QG task, we switch these (as in Input: passage, answer, and Output: question). Transliteration. The goal of transliteration (TS) is to accurately convert a word or text from one writing system to another, while maintaining the original language\u2019s pronunciation and sound. For that, we use ANETAC dataset (Ameur et al., 2019), an English-Arabic named entity transliteration dataset. It includes 79, 924 pairs of named entities in English and Arabic, which are categorized into three classes: person, location, or organization. Paraphrasing. For this task, we use TaPaCo (Scherrer, 2020) a paraphrase corpus that comprises 73 languages, including Arabic. It was extracted from the Tatoeba database and created by aligning sentences with the same meaning. The Arabic portion of TaPaCo, called AraTaPaCo, contains 3K pairs of sentences. Text Rewriting. The main objective here is to produce a text in the target style while maintaining the content of the original input text. We use the Arabic Parallel Gender Corpus (APGC), which was proposed by Alhafni et al. (2022). This corpus contains pairs of sentences where the input sentence is in one gender (e.g., male) and the target sentence has the same meaning but is in the opposite gender (i.e., female). Grammatical Error Correction. For this task, we use QALB 2014 (Mohit et al., 2014), a manually corrected collection of Arabic texts from online comments written by native Arabic speakers (L1) in Aljazeera articles. The dataset is divided into a training set with 19.4K sentences, a development set with 1.02K sentences, and a test set with 968 sentences. Dialogue Generation. We use the open-domain dialogue generation dataset for Arabic dialects proposed by (Naous et al., 2023). The dataset consists of 1K pairs of utterances and responses, which were translated from the English DailyDialog dataset (Li et al., 2017) by three native translators from the Levantine, Egyptian, and Gulf areas. Diacritization. Arabic Text Diacritization (ATD) is the process involving adding missing diacritics to words or word sequences in Arabic orthography. To accomplish this, we use the Arabic Diacritization dataset proposed by (Fadel et al., 2019)."
        },
        {
            "heading": "C Anisotropy in BERT Models",
            "text": "A prevalent issue with language models is that they suffer from anisotropy (Ethayarajh, 2019; Li et al., 2020) in the embedding space. That is, representations obtained by the models tend to occupy a narrow cone in the hyperspace, making them less informative. This can potentially impact negatively on tasks like WSD because the models need to differentiate the representation of the queried word from the representation of the whole sentence. Forming a good representation of the queried word can help the model to align with the given sense during the finetuning. As the representation of both the word and the sentence are very close due to anisotropy, the model cannot properly align the word with the given sense even after finetuning. Therefore, we suspect that anisotropy might potentially be the reason behind the inferior performance of MARBERTV2 on WSD task (Table 1)."
        },
        {
            "heading": "D ChatGPT Exhibits False-Toxicity",
            "text": "As Table 1 shows, ChatGPT exhibits significantly poor performance compared to the finetuned AraT5 model on the toxic text classification tasks. Given the concern that models such as ChatGPT might not be aligned well for toxic and harmful language, we focus our analysis in part on the toxic language datasets in our evaluation benchmark. To this end, we compute the confusion matrices presented in Figure 7. As we can see, ChatGPT is extremely prone to flagging non-toxic texts as toxic (i.e., a\nhigh rate of false positives). We hypothesize that this may be due to one or more of the following reasons:\n\u2022 Lack of diverse Arabic texts in ChatGPT pretraining data (e.g., not enough data from certain dialects).\n\u2022 ChatGPT is supervised to alleviate the spread of harmful contents, as documented in OpenAI safety standards 12.\nE Prompt Template for GPT-4 Evaluation\nFigure 8 presents the prompt template that we use to rate the model\u2019s reponse with GPT-4."
        },
        {
            "heading": "F Class Distribution for Few-shot Examples",
            "text": "We provide class distribution (in percentage) of the classification tasks (except for Dialect-Country where the number of classes is higher than the number of shots) for the full training dataset and our 10-shot sampling in Table 4 and Table 5, respectively. As evident from Table 4 and Table 5, the class distribution of the few-shot samples is reasonably aligned with the corresponding tasks. Therefore, the sampling process of few-shot prompting indeed respects the class distribution of the full training set for the respective tasks in the ORCA benchmark."
        },
        {
            "heading": "G Human Evaluation Framework",
            "text": "G.1 Task Description Summarization: Given a long text, we ask the model to summarize it. If we are not providing the length/size of the summary in the prompt summary\n12https://openai.com/safety-standards\nof any length shall be accepted as long as it summarizes the input. Paraphrasing: Given the input text provided, we ask the model to generate a paraphrase for the input. Machine Translation: Given an input in a language, we ask the model to translate it into Arabic (or any other language). Text Re-rewriting: Given an input text, we ask the model to rewrite in a specific style. If we are not providing the length/size of the rewriting in the prompt, the output of any length shall be accepted as long as it is accurate without missing/adding new information on its own. Open-ended Dialogue Generation: For each given utterance prompt, the task is to generate a reply. The reply is open-ended and is acceptable as long as it is coherent with the prompt."
        },
        {
            "heading": "H Examples of Prompt",
            "text": "We present some sample prompts of Arabic NLU and NLG tasks that we use for ChatGPT evaluation in Table 6 and Table 7 respectively."
        },
        {
            "heading": "I Examples of Models\u2019 Response",
            "text": "We present the examples of models\u2019 responses to the corresponding prompt in Table 8.\nJ Examples of GPT-4 Evaluation\nWe present the examples of GPT-4 evaluation to the corresponding prompt in Table 9.\nK GPT-4 Explanation on Models\u2019 Evaluation\nIn addition to, rate the models\u2019 responses, we generate the rationale behind the ratings by GPT-4. We manually analyze a subset of responses. We find\nthat the ratings of GPT-4 are based on generation quality, fluency, accurateness, coherency, and overall instruction-following capability. We present such examples of GPT-4\u2019s explanations on the generated ratings to the models\u2019 response in Table 10."
        },
        {
            "heading": "L Human Evaluation",
            "text": "We provide the detail human evaluation on the 8 datasets in Table 11\nRating Criteria\nRating-A \u2022 The output is an acceptable, valid, and satisfying response to the input prompt. Eg: If we ask the model to summarize an input, the produced output should clearly summarize the input text. For paraphrasing and machine translation, the output is fluent and contains the same semantic meaning as the input.\n\u2022 Output is fluent, relevant, and natural response to a conversation. \u2022 The produced output is correct but at the same standard as humans. This is\napplicable for tasks like open-ended dialogue generation. Irrespective of language and tone (layman term vs domain expert, formal or informal), as long as the generated response meets criteria 1 it should be rated as A.\nRating-B \u2022 It has followed the task given in the prompt but has a minor issue that needs to be improved.\n\u2022 Partially acceptable responses shall be labeled as B. \u2022 output has fulfilled the task specified in the prompt with >= 50% accuracy. \u2022 The task-specific criteria for \u2018B\u2019 is as follows:\nSummarization: The model has omitted some key information from the input text in the summary. The model has added some information in the summary that\u2019s not in the input text. Minor overlap between summary and text input. Paraphrasing: The output has minor grammar issues. Minor overlap between input text and generated paraphrase. Open-ended Dialogue Generation: The generated response has minor factual or syntactical issues. The generated response has little irrelevant information. Machine Translation: The output has minor fluency issues. The output has minor syntactical issues. The output may contain few irrelevant information.\nRating-C \u2022 The output is relevant and the model attempts to do the task specified in the prompt but has a major issue in the quality of the output.\n\u2022 The produced output is <50% accurate for the task specified in the prompt. \u2022 The task-specific criteria for \u2018C\u2019 is as follows:\nSummarization: The model has omitted significant information from the input text in the summary. The model has added significant information in the summary that\u2019s not in the input text. Major overlap between summary and text input. Paraphrasing: The output has major grammar issues. The output is the same as input text with a few minor word replacements. Major overlap between input text and generated paraphrase. The output has a major semantic meaning difference compared to the input. Open-ended Dialogue Generation: The generated response has major factual or syntactical issues. Generated response is machine-like, non-fluent, and/or irrelevant. Machine Translation: The produced translation may contain one or two different language tokens except for the target language. The output has major grammatical, syntactical, and fluency issues. The semantic meaning is a little different than the input.\nRating-D \u2022 The output is invalid and totally unacceptable for the task specified in the prompt. \u2022 The produced output is not at all relevant to the task specified in the prompt. Eg:\nThe model didn\u2019t produce any output at all. \u2022 The model has refused to provide an answer. Eg: \u201cAs an AI language model I do\nnot have sufficient ...\u201d \u2022 The model just copies the input as the output \u2022 The produced output is in a different language."
        }
    ],
    "title": "GPTAraEval: A Comprehensive Evaluation of ChatGPT on Arabic NLP",
    "year": 2023
}