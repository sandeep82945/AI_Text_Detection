{
    "abstractText": "With the growing volume of diverse information, the demand for classifying arbitrary topics has become increasingly critical. To address this challenge, we introduce DRAFT, a simple framework designed to train a classifier for fewshot topic classification. DRAFT uses a few examples of a specific topic as queries to construct Customized dataset with a dense retriever model. Multi-query retrieval (MQR) algorithm, which effectively handles multiple queries related to a specific topic, is applied to construct the Customized dataset. Subsequently, we finetune a classifier using the Customized dataset to identify the topic. To demonstrate the efficacy of our proposed approach, we conduct evaluations on both widely used classification benchmark datasets and manually constructed datasets with 291 diverse topics, which simulate diverse contents encountered in real-world applications. DRAFT shows competitive or superior performance compared to baselines that use in-context learning, such as GPT-3 175B and InstructGPT 175B, on few-shot topic classification tasks despite having 177 times fewer parameters, demonstrating its effectiveness.",
    "authors": [
        {
            "affiliations": [],
            "name": "Keonwoo Kim"
        },
        {
            "affiliations": [],
            "name": "Younggun Lee"
        }
    ],
    "id": "SP:30e810e8e8e08959e5ebd6ea2c4e707c4c8c5ee4",
    "references": [
        {
            "authors": [
                "S\u00f6ren Auer",
                "Christian Bizer",
                "Georgi Kobilarov",
                "Jens Lehmann",
                "Richard Cyganiak",
                "Zachary Ives."
            ],
            "title": "Dbpedia: A nucleus for a web of open data",
            "venue": "The semantic web, pages 722\u2013735. Springer.",
            "year": 2007
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners. Advances in neural information processing",
            "year": 2020
        },
        {
            "authors": [
                "Deng Cai",
                "Yan Wang",
                "Lemao Liu",
                "Shuming Shi."
            ],
            "title": "Recent advances in retrieval-augmented text generation",
            "venue": "Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 3417\u20133419.",
            "year": 2022
        },
        {
            "authors": [
                "Danqi Chen",
                "Adam Fisch",
                "Jason Weston",
                "Antoine Bordes."
            ],
            "title": "Reading wikipedia to answer opendomain questions",
            "venue": "arXiv preprint arXiv:1704.00051.",
            "year": 2017
        },
        {
            "authors": [
                "Junfan Chen",
                "Richong Zhang",
                "Yongyi Mao",
                "Jie Xu."
            ],
            "title": "Contrastnet: A contrastive learning framework for few-shot text classification",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, pages 10492\u201310500.",
            "year": 2022
        },
        {
            "authors": [
                "Xiang Chen",
                "Lei Li",
                "Ningyu Zhang",
                "Xiaozhuan Liang",
                "Shumin Deng",
                "Chuanqi Tan",
                "Fei Huang",
                "Luo Si",
                "Huajun Chen."
            ],
            "title": "Decoupling knowledge from memorization: Retrieval-augmented prompt learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Ke-Li Chiu",
                "Annie Collins",
                "Rohan Alexander."
            ],
            "title": "Detecting hate speech with gpt-3",
            "venue": "arXiv preprint arXiv:2103.12407.",
            "year": 2021
        },
        {
            "authors": [
                "Aakanksha Chowdhery",
                "Sharan Narang",
                "Jacob Devlin",
                "Maarten Bosma",
                "Gaurav Mishra",
                "Adam Roberts",
                "Paul Barham",
                "Hyung Won Chung",
                "Charles Sutton",
                "Sebastian Gehrmann"
            ],
            "title": "Palm: Scaling language modeling with pathways",
            "year": 2022
        },
        {
            "authors": [
                "Kevin Clark",
                "Minh-Thang Luong",
                "Quoc V Le",
                "Christopher D Manning."
            ],
            "title": "Electra: Pre-training text encoders as discriminators rather than generators",
            "venue": "arXiv preprint arXiv:2003.10555.",
            "year": 2020
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "arXiv preprint arXiv:1810.04805.",
            "year": 2018
        },
        {
            "authors": [
                "Angela Fan",
                "Claire Gardent",
                "Chlo\u00e9 Braud",
                "Antoine Bordes."
            ],
            "title": "Augmenting transformers with knnbased composite memory for dialog",
            "venue": "Transactions of the Association for Computational Linguistics, 9:82\u2013",
            "year": 2021
        },
        {
            "authors": [
                "Deep Ganguli",
                "Liane Lovitt",
                "Jackson Kernion",
                "Amanda Askell",
                "Yuntao Bai",
                "Saurav Kadavath",
                "Ben Mann",
                "Ethan Perez",
                "Nicholas Schiefer",
                "Kamal Ndousse"
            ],
            "title": "Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons",
            "year": 2022
        },
        {
            "authors": [
                "Tianyu Gao",
                "Xingcheng Yao",
                "Danqi Chen."
            ],
            "title": "Simcse: Simple contrastive learning of sentence embeddings",
            "venue": "arXiv preprint arXiv:2104.08821.",
            "year": 2021
        },
        {
            "authors": [
                "Kelvin Guu",
                "Kenton Lee",
                "Zora Tung",
                "Panupong Pasupat",
                "Mingwei Chang."
            ],
            "title": "Retrieval augmented language model pre-training",
            "venue": "International Conference on Machine Learning, pages 3929\u20133938. PMLR.",
            "year": 2020
        },
        {
            "authors": [
                "Pengcheng He",
                "Xiaodong Liu",
                "Jianfeng Gao",
                "Weizhu Chen."
            ],
            "title": "Deberta: Decoding-enhanced bert with disentangled attention",
            "venue": "arXiv preprint arXiv:2006.03654.",
            "year": 2020
        },
        {
            "authors": [
                "Djoerd Hiemstra."
            ],
            "title": "A probabilistic justification for using tf\u00d7 idf term weighting in information retrieval",
            "venue": "International Journal on Digital Libraries, 3(2):131\u2013 139.",
            "year": 2000
        },
        {
            "authors": [
                "Ari Holtzman",
                "Peter West",
                "Vered Shwartz",
                "Yejin Choi",
                "Luke Zettlemoyer."
            ],
            "title": "Surface form competition: Why the highest probability answer isn\u2019t always right",
            "venue": "arXiv preprint arXiv:2104.08315.",
            "year": 2021
        },
        {
            "authors": [
                "Neil Houlsby",
                "Andrei Giurgiu",
                "Stanislaw Jastrzebski",
                "Bruna Morrone",
                "Quentin De Laroussilhe",
                "Andrea Gesmundo",
                "Mona Attariyan",
                "Sylvain Gelly."
            ],
            "title": "Parameter-efficient transfer learning for nlp",
            "venue": "International Conference on Machine Learning, pages",
            "year": 2019
        },
        {
            "authors": [
                "Edward J Hu",
                "Yelong Shen",
                "Phillip Wallis",
                "Zeyuan Allen-Zhu",
                "Yuanzhi Li",
                "Shean Wang",
                "Lu Wang",
                "Weizhu Chen."
            ],
            "title": "Lora: Low-rank adaptation of large language models",
            "venue": "arXiv preprint arXiv:2106.09685.",
            "year": 2021
        },
        {
            "authors": [
                "Gautier Izacard",
                "Mathilde Caron",
                "Lucas Hosseini",
                "Sebastian Riedel",
                "Piotr Bojanowski",
                "Armand Joulin",
                "Edouard Grave."
            ],
            "title": "Unsupervised dense information retrieval with contrastive learning",
            "venue": "arXiv preprint arXiv:2112.09118.",
            "year": 2022
        },
        {
            "authors": [
                "Gautier Izacard",
                "Edouard Grave."
            ],
            "title": "Leveraging passage retrieval with generative models for open domain question answering",
            "venue": "arXiv preprint arXiv:2007.01282.",
            "year": 2020
        },
        {
            "authors": [
                "Gautier Izacard",
                "Patrick Lewis",
                "Maria Lomeli",
                "Lucas Hosseini",
                "Fabio Petroni",
                "Timo Schick",
                "Jane Dwivedi-Yu",
                "Armand Joulin",
                "Sebastian Riedel",
                "Edouard Grave."
            ],
            "title": "Few-shot learning with retrieval augmented language models",
            "venue": "arXiv preprint",
            "year": 2022
        },
        {
            "authors": [
                "Jared Kaplan",
                "Sam McCandlish",
                "Tom Henighan",
                "Tom B Brown",
                "Benjamin Chess",
                "Rewon Child",
                "Scott Gray",
                "Alec Radford",
                "Jeffrey Wu",
                "Dario Amodei."
            ],
            "title": "Scaling laws for neural language models",
            "venue": "arXiv preprint arXiv:2001.08361.",
            "year": 2020
        },
        {
            "authors": [
                "Jared Kaplan",
                "Sam McCandlish",
                "Tom Henighan",
                "Tom B Brown",
                "Benjamin Chess",
                "Rewon Child",
                "Scott Gray",
                "Alec Radford",
                "Jeffrey Wu",
                "Dario Amodei."
            ],
            "title": "Scaling laws for neural language models",
            "venue": "arXiv preprint arXiv:2001.08361.",
            "year": 2020
        },
        {
            "authors": [
                "Vladimir Karpukhin",
                "Barlas O\u011fuz",
                "Sewon Min",
                "Patrick Lewis",
                "Ledell Wu",
                "Sergey Edunov",
                "Danqi Chen",
                "Wen-tau Yih."
            ],
            "title": "Dense passage retrieval for open-domain question answering",
            "venue": "arXiv preprint arXiv:2004.04906.",
            "year": 2020
        },
        {
            "authors": [
                "Urvashi Khandelwal",
                "Omer Levy",
                "Dan Jurafsky",
                "Luke Zettlemoyer",
                "Mike Lewis."
            ],
            "title": "Generalization through memorization: Nearest neighbor language models",
            "venue": "arXiv preprint arXiv:1911.00172.",
            "year": 2019
        },
        {
            "authors": [
                "Tom Kwiatkowski",
                "Jennimaria Palomaki",
                "Olivia Redfield",
                "Michael Collins",
                "Ankur Parikh",
                "Chris Alberti",
                "Danielle Epstein",
                "Illia Polosukhin",
                "Jacob Devlin",
                "Kenton Lee"
            ],
            "title": "Natural questions: a benchmark for question answering research",
            "year": 2019
        },
        {
            "authors": [
                "Patrick Lewis",
                "Ethan Perez",
                "Aleksandra Piktus",
                "Fabio Petroni",
                "Vladimir Karpukhin",
                "Naman Goyal",
                "Heinrich K\u00fcttler",
                "Mike Lewis",
                "Wen-tau Yih",
                "Tim Rockt\u00e4schel"
            ],
            "title": "Retrieval-augmented generation for knowledge-intensive nlp tasks",
            "year": 2020
        },
        {
            "authors": [
                "Jiachang Liu",
                "Dinghan Shen",
                "Yizhe Zhang",
                "Bill Dolan",
                "Lawrence Carin",
                "Weizhu Chen"
            ],
            "title": "2021a. What makes good in-context examples for gpt-3? arXiv preprint arXiv:2101.06804",
            "year": 2021
        },
        {
            "authors": [
                "Xiao Liu",
                "Kaixuan Ji",
                "Yicheng Fu",
                "Weng Lam Tam",
                "Zhengxiao Du",
                "Zhilin Yang",
                "Jie Tang."
            ],
            "title": "P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks",
            "venue": "arXiv preprint arXiv:2110.07602.",
            "year": 2021
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Roberta: A robustly optimized bert pretraining approach",
            "venue": "arXiv preprint arXiv:1907.11692.",
            "year": 2019
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter."
            ],
            "title": "Decoupled weight decay regularization",
            "venue": "arXiv preprint arXiv:1711.05101.",
            "year": 2017
        },
        {
            "authors": [
                "Kevin Meng",
                "David Bau",
                "Alex Andonian",
                "Yonatan Belinkov."
            ],
            "title": "Locating and editing factual associations in GPT",
            "venue": "Advances in Neural Information Processing Systems, 35.",
            "year": 2022
        },
        {
            "authors": [
                "Sewon Min",
                "Mike Lewis",
                "Hannaneh Hajishirzi",
                "Luke Zettlemoyer."
            ],
            "title": "Noisy channel language model prompting for few-shot text classification",
            "venue": "arXiv preprint arXiv:2108.04106.",
            "year": 2021
        },
        {
            "authors": [
                "Eric Mitchell",
                "Charles Lin",
                "Antoine Bosselut",
                "Chelsea Finn",
                "Christopher D Manning."
            ],
            "title": "Fast model editing at scale",
            "venue": "arXiv preprint arXiv:2110.11309.",
            "year": 2021
        },
        {
            "authors": [
                "Subhabrata Mukherjee",
                "Ahmed Awadallah."
            ],
            "title": "Uncertainty-aware self-training for few-shot text classification",
            "venue": "Advances in Neural Information Processing Systems, 33:21199\u201321212.",
            "year": 2020
        },
        {
            "authors": [
                "Aaron van den Oord",
                "Yazhe Li",
                "Oriol Vinyals."
            ],
            "title": "Representation learning with contrastive predictive coding",
            "venue": "arXiv preprint arXiv:1807.03748.",
            "year": 2018
        },
        {
            "authors": [
                "Long Ouyang",
                "Jeff Wu",
                "Xu Jiang",
                "Diogo Almeida",
                "Carroll L Wainwright",
                "Pamela Mishkin",
                "Chong Zhang",
                "Sandhini Agarwal",
                "Katarina Slama",
                "Alex Ray"
            ],
            "title": "Training language models to follow instructions with human feedback",
            "year": 2022
        },
        {
            "authors": [
                "Ethan Perez",
                "Saffron Huang",
                "Francis Song",
                "Trevor Cai",
                "Roman Ring",
                "John Aslanides",
                "Amelia Glaese",
                "Nat McAleese",
                "Geoffrey Irving."
            ],
            "title": "Red teaming language models with language models",
            "venue": "arXiv preprint arXiv:2202.03286.",
            "year": 2022
        },
        {
            "authors": [
                "Alec Radford",
                "Jeffrey Wu",
                "Rewon Child",
                "David Luan",
                "Dario Amodei",
                "Ilya Sutskever"
            ],
            "title": "Language models are unsupervised multitask learners",
            "venue": "OpenAI blog,",
            "year": 2019
        },
        {
            "authors": [
                "Jack W Rae",
                "Sebastian Borgeaud",
                "Trevor Cai",
                "Katie Millican",
                "Jordan Hoffmann",
                "Francis Song",
                "John Aslanides",
                "Sarah Henderson",
                "Roman Ring",
                "Susannah Young"
            ],
            "title": "Scaling language models: Methods, analysis & insights from training gopher",
            "year": 2021
        },
        {
            "authors": [
                "Stephen Robertson",
                "Hugo Zaragoza"
            ],
            "title": "The probabilistic relevance framework: Bm25 and beyond",
            "venue": "Foundations and Trends\u00ae in Information Retrieval,",
            "year": 2009
        },
        {
            "authors": [
                "Teven Le Scao",
                "Angela Fan",
                "Christopher Akiki",
                "Ellie Pavlick",
                "Suzana Ili\u0107",
                "Daniel Hesslow",
                "Roman Castagn\u00e9",
                "Alexandra Sasha Luccioni",
                "Fran\u00e7ois Yvon",
                "Matthias Gall\u00e9"
            ],
            "title": "Bloom: A 176bparameter open-access multilingual language model",
            "year": 2022
        },
        {
            "authors": [
                "Mohammad Shoeybi",
                "Mostofa Patwary",
                "Raul Puri",
                "Patrick LeGresley",
                "Jared Casper",
                "Bryan Catanzaro."
            ],
            "title": "Megatron-lm: Training multi-billion parameter language models using model parallelism",
            "venue": "arXiv preprint arXiv:1909.08053.",
            "year": 2019
        },
        {
            "authors": [
                "Anshumali Shrivastava",
                "Ping Li."
            ],
            "title": "Asymmetric lsh (alsh) for sublinear time maximum inner product search (mips)",
            "venue": "Advances in neural information processing systems, 27.",
            "year": 2014
        },
        {
            "authors": [
                "Shengli Sun",
                "Qingfeng Sun",
                "Kevin Zhou",
                "Tengchao Lv."
            ],
            "title": "Hierarchical attention prototypical networks for few-shot text classification",
            "venue": "Proceedings of the 2019 conference on empirical methods in natural language processing and the 9th interna-",
            "year": 2019
        },
        {
            "authors": [
                "Romal Thoppilan",
                "Daniel De Freitas",
                "Jamie Hall",
                "Noam Shazeer",
                "Apoorv Kulshreshtha",
                "Heng-Tze Cheng",
                "Alicia Jin",
                "Taylor Bos",
                "Leslie Baker",
                "Yu Du"
            ],
            "title": "Lamda: Language models for dialog applications",
            "venue": "arXiv preprint arXiv:2201.08239",
            "year": 2022
        },
        {
            "authors": [
                "James Thorne",
                "Andreas Vlachos",
                "Christos Christodoulopoulos",
                "Arpit Mittal."
            ],
            "title": "Fever: a large-scale dataset for fact extraction and verification",
            "venue": "arXiv preprint arXiv:1803.05355.",
            "year": 2018
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin."
            ],
            "title": "Attention is all you need",
            "venue": "Advances in neural information processing systems, 30.",
            "year": 2017
        },
        {
            "authors": [
                "Ellen M Voorhees",
                "Dawn M Tice."
            ],
            "title": "Building a question answering test collection",
            "venue": "Proceedings of the 23rd annual international ACM SIGIR conference on Research and development in information retrieval, pages 200\u2013207.",
            "year": 2000
        },
        {
            "authors": [
                "Ruohong Zhang",
                "Yau-shian Wang",
                "Yiming Yang",
                "Donghan Yu",
                "Tom Vu",
                "Likun Lei."
            ],
            "title": "Long-tailed extreme multi-label text classification by the retrieval of generated pseudo label descriptions",
            "venue": "Findings of the Association for Computational Linguistics:",
            "year": 2023
        },
        {
            "authors": [
                "Susan Zhang",
                "Stephen Roller",
                "Naman Goyal",
                "Mikel Artetxe",
                "Moya Chen",
                "Shuohui Chen",
                "Christopher Dewan",
                "Mona Diab",
                "Xian Li",
                "Xi Victoria Lin"
            ],
            "title": "Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068",
            "year": 2022
        },
        {
            "authors": [
                "Xiang Zhang",
                "Junbo Zhao",
                "Yann LeCun."
            ],
            "title": "Character-level convolutional networks for text classification",
            "venue": "Advances in neural information processing systems, 28.",
            "year": 2015
        },
        {
            "authors": [
                "Zihao Zhao",
                "Eric Wallace",
                "Shi Feng",
                "Dan Klein",
                "Sameer Singh."
            ],
            "title": "Calibrate before use: Improving few-shot performance of language models",
            "venue": "International Conference on Machine Learning, pages 12697\u201312706. PMLR.",
            "year": 2021
        },
        {
            "authors": [
                "P Bong Joon"
            ],
            "title": "Ho\u2019s Parasite made history for bagging 3 awards at the 2020 Oscars, which was the most of any film nominated",
            "year": 2020
        },
        {
            "authors": [
                "HN The Huizhou"
            ],
            "title": "Ancient Town is a famous historical and cultural city in southern Anhui Province with over 2000 years of history",
            "year": 2000
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "With the prevalence of the Internet and social media, there is a significant demand for classifying or detecting texts related to specific topics within the vast amount of information pouring in from the Internet. For instance, on social media platforms where an overwhelming volume of content is generated, there may exist a need to monitor and filter content associated with particular issues (e.g., drugs). Additionally, with the remarkable progress in large language models (LLMs) (Brown et al., 2020; Ouyang et al., 2022; Chowdhery et al., 2022; Rae et al., 2021; Scao et al., 2022; Thoppilan et al., 2022) in recent times, there is also an increasing\n*Work done while at Neosapience.\ndemand to detect specific topics within the text generated by LLMs. The use of LLMs often contains ethical concerns with the issues of hallucination, as they possess the capability to generate morally inappropriate or unintended content (Ganguli et al., 2022; Perez et al., 2022). However, as the amount of content shared on social media and generated by LLMs increase exponentially, verifying each piece of content becomes challenging for individuals.\nIn natural language processing (NLP), research related to the challenges mentioned earlier can be considered a topic classification task since the demand for automatically classifying texts on a specific topic exists. Recent pre-trained language models (Devlin et al., 2018; Liu et al., 2019; Clark et al., 2020; He et al., 2020) have gained considerable recognition for their ability to achieve high performance in topic classification tasks.\nTo train a topic classification model, it is common practice to rely on supervised learning using a training dataset with labeled data. Most existing methods on a topic classification have primarily relied on benchmark datasets (Zhang et al., 2015; Auer et al., 2007). To the best of our knowledge, they have predominantly focused on improving model performance on benchmark datasets that involve a limited number of topics rather than addressing long-tailed arbitrary topics in real-world scenarios. Regarding real-world applications, the availability of labeled datasets that cover diverse topics is often limited due to the substantial cost of building such datasets. This constraint poses a challenge for directly deploying topic classification models in practical scenarios, thereby calling for a solution that enables their flexible application.\nFew-shot classification, which performs classification using few text examples, can be applied even without a training dataset for a predefined topic, enabling its extension to tasks classifying a diverse range of arbitrary topics. To the best of our knowledge, existing few-shot classification\nmethods operate exclusively on tasks with two or more defined classes and cannot conduct in oneclass classification tasks (Zhang et al., 2023; Chen et al., 2022a; Mukherjee and Awadallah, 2020; Sun et al., 2019). However, by leveraging LLMs with in-context learning (Brown et al., 2020; Holtzman et al., 2021; Min et al., 2021), we can conduct one-class few-shot topic classification tasks across various topics, showcasing superior performance. This approach garnered significant attention due to its applicability in scenarios with limited labeled data (Zhao et al., 2021; Holtzman et al., 2021; Min et al., 2021). However, successful implementation of in-context learning relies on LLMs with billions of parameters (Kaplan et al., 2020a). Such models suffer from high computational costs, extensive resource requirements, and slow inference speed to apply in real-world applications.\nTo address the few-shot topic classification more efficiently in real-world applications, we propose a simple framework called Dense Retrieval Augmented Few-shot Topic classifier framework (DRAFT), which can classify arbitrary topics given limited labeled data. DRAFT uses a dense retriever model (Karpukhin et al., 2020) to construct Customized dataset, using examples of a target topic provided as queries. Subsequently, a pre-trained language model is finetuned with the Customized dataset for the topic classification task. We evaluate the performance of DRAFT by conducting experiments on general benchmarks and manually constructed datasets with 291 topics. The former datasets are widely regarded as benchmark datasets in recent classification research. In contrast, the latter datasets are manually constructed to replicate real-world scenarios, allowing for a thorough assessment of the diverse topic classification capability. They are used for one-class classification tasks where only a single topic is defined. Our experiment results demonstrate that DRAFT consistently achieves competitive or superior performance compared to LLMs, such as GPT-3 175B and InstructGPT 175B, which have 177 times more parameters on topic classification tasks. These findings provide strong empirical support for the efficacy of DRAFT in tackling few-shot topic classification tasks. The contributions are summarized as follows:\n1. We propose DRAFT as a simple but effective framework that classifies texts related to arbitrary topics using a few labeled data.\n2. To the best of our knowledge, we are the first\nto attempt to classify a topic in texts through a dense retriever model.\n3. We introduce the MQR algorithm, which is the first to accommodate multiple queries simultaneously as inputs for the retriever.\n4. The results of extensive experiments show that DRAFT achieves competitive performance compared to large language models."
        },
        {
            "heading": "2 Related Works",
            "text": "Retrieval-augmented methods Information retrieval aims to find semantically relevant documents based on a query. Traditional methods employ lexical approaches to retrieve support documents using sparse vectors (Hiemstra, 2000; Robertson et al., 2009). With the recent development of deep learning (Vaswani et al., 2017), neural network-based methods have demonstrated high performance. The bi-encoder structure (Karpukhin et al., 2020; Izacard et al., 2022a) offers the advantage of pre-encoding document candidates in an offline setting, which allows for faster computation. Nonetheless, the lack of token-level interaction between query and document tokens in the bi-encoder models can lead to lower performance compared to the cross-encoder models (Devlin et al., 2018). Nevertheless, we use a bi-encoder in DRAFT for efficient retrieval, which facilitates the immediate creation of a classifier for any given topic.\nThere exist various retrieval-augmented methodologies (Cai et al., 2022; Izacard et al., 2022b) in recent NLP research. They encompass solutions for tasks such as fact retrieval (Thorne et al., 2018), open-domain question answering (Chen et al., 2017; Izacard and Grave, 2020; Lewis et al., 2020; Guu et al., 2020), and others. They also include techniques applied during inference to reduce perplexity in language modeling (Khandelwal et al., 2019) and strategies operate akin to memory for specific knowledge or dialogues (Chen et al., 2022b; Fan et al., 2021). All existing retrievalaugmented methods universally handle only a single query as the input to the retriever and subsequently execute downstream tasks. However, unlike existing approaches, DRAFT processes multiple queries simultaneously as input for retrievers.\nIn-context learning with LLMs Recent LLMs (Thoppilan et al., 2022; Zhang et al., 2022; Scao et al., 2022; Rae et al., 2021; Shoeybi et al.,\n2019; Chowdhery et al., 2022) represent a critical development in NLP and have been considered an attempt to develop intelligent language systems with fluency approaching that of humans. According to (Kaplan et al., 2020b), as the scale of the language model increases, its performance is also improved on many tasks that typically require models with hundreds of billions of parameters. LLMs primarily perform tasks through in-context learning (ICL) (Brown et al., 2020; Holtzman et al., 2021; Min et al., 2021), which feeds concatenated prompt and k input-target examples (referred to as \u2018k-shot\u2019) into the model without weight updates. It exhibits superior performance over zero-shot inference across an extensive array of tasks (Zhao et al., 2021; Liu et al., 2021a). For classification tasks, models predict a label from a predefined set of labels with the highest probability. ICL offers the key benefit of allowing a single model to instantly handle multiple tasks with only a limited number of labeled examples, known as few-shot learning. In the context of our research on few-shot topic classification across diverse topics, creating a classifier for each topic without having a training dataset can be regarded as a distinct task. Thus, we use ICL with LLMs, which can perform diverse tasks with only a few labeled examples, as baseline models in the experiments."
        },
        {
            "heading": "3 Method",
            "text": "In this section, we begin by describing the application of DRAFT to a simple binary classification and elaborate on the extension of DRAFT. DRAFT comprises two stages for few-shot topic classification: (1) constructing Customized dataset from multiple queries using a dense retriever model and (2) training a classifier. Figure 1 (a) illustrates the overall process of DRAFT."
        },
        {
            "heading": "3.1 Building Customized dataset",
            "text": "In the first stage, we use a pre-trained dense retriever, a bi-encoder consisting of a query encoder and a passage encoder, to construct Customized dataset. Customized dataset is created by employing a few texts related to a target topic as queries. To efficiently retrieve relevant passages from Data Collection, which serves as the external knowledge base (e.g., Wikipedia), we employ a Maximum Inner Product Search (MIPS) algorithm (Shrivastava and Li, 2014) that finds the vector with the highest inner product value with a given query vector. The retriever is defined by employing a query encoder Equery and a passage encoder Epassage:\np(z | x) \u221d exp (Equery(x)TEpassage(z)).\nEquery embeds query x, and Epassage embeds passages z \u2208 Z, where Z indicates Data Collection.\nWe select the top-k passages with the highest prior probability p(z | x), which is proportional to the inner product of the query and passage vectors.\nAlgorithm 1: Multi-Query Retrieval Result: An array C of target samples Input: An array S with n queries cos(\u03b8)\u2190 1\nnC2\n\u2211n i=1 \u2211n j \u0338=i sim(S[i], S[j])\nfor q = 1, 2, . . . , n do retrieve k passages using MIPS for passage = 1, 2, . . . , k do\n\u03b1\u2190 cos(S[q], passage) cos denotes cosine similarity function if cos(\u03b8) \u2264 \u03b1 then C \u2190 C + [passage]\nend end\nend\nWe define Customized dataset as the construction of a collection of positive and negative samples, including multiple queries, specifically designed to train a topic classifier. To build positive samples within Customized dataset, we propose the MultiQuery Retrieval (MQR) algorithm, depicted in Figure 1 (b). As outlined in Algorithm 1, MQR begins by gathering n sentences or keywords related to the specific topic to form queries. A dense retriever model for each query is employed to retrieve top-k passages, where k denotes the subspace size. From the retrieved n\u00d7 k passages, only passages vectors that exceed a threshold cos(\u03b8) determined by the average pairwise cosine similarity score among the n query vectors are retained. Unlike general dense retriever models that take a single query as input, it can accept multiple queries as input. By combining the n queries and the m passages retained from Data Collection, we collect a total of n+m positive samples. Subsequently, we create an equivalent number of negative samples by randomly selecting passages from Data Collection, thereby forming Customized dataset, which comprises 2(n + m) samples."
        },
        {
            "heading": "3.2 Training a topic classifier in DRAFT",
            "text": "In the following stage, we proceed with fine-tuning a pre-trained language model on the classification task, employing Customized dataset constructed in the preceding stage. The fine-tuning process is similar to (Devlin et al., 2018) for downstream tasks. It passes [CLS] token, a special classification\ntoken representing sentence-level embedding obtained from the hidden vector after passing through the encoder layers through an MLP layer. The classifier is trained using binary cross-entropy loss.\nL = \u2212 1 2(n+m) 2(n+m)\u2211 i=1 \u2211 j\u2208{0,1} 1yi=j ln q(yi | xi),\nwith an indicator function 1, a text xi, a label for the corresponding text yi \u2208 {0, 1}, and a probability on model\u2019s prediction q."
        },
        {
            "heading": "3.3 Expanding the Capabilities of DRAFT",
            "text": "Negative query We can improve DRAFT\u2019s performance by introducing an additional stage after training a classifier. In this stage, we manually incorporate negative queries that belong to a semantically similar category but are different from the target topic. Constructing negative samples follows the process outlined in Section 3.1 for constructing positive samples, which uses MQR. After training the classifier with the updated Customized dataset, it becomes more robust in handling hard negatives that are difficult to classify. This expansion of DRAFT enables it to handle complex cases and enhance its overall performance.\nMulti-class classification Moreover, through the expansion of the class set, it is possible to develop a multi-class classifier. For each class, MQR is employed to construct class-specific positive samples. The training dataset for a multi-class classifier is created by merging the aggregated class-specific constructed positive samples for each class, eliminating the need for a separate process of constructing negative samples within each class due to the presence of independent other defined classes. Subsequently, the classifier is trained with the merged dataset using cross-entropy loss."
        },
        {
            "heading": "4 Experiments",
            "text": "To evaluate the performance of DRAFT, we conduct three experiments with limited labeled data in classification tasks.\n1. We assess the performance of DRAFT in few-shot topic classification by manually constructing datasets comprising 291 unique topics that simulate real-world scenarios.\n2. We explore the expanding capabilities of DRAFT using negative queries on two additional manually constructed datasets.\n3. We evaluate the multi-class classification capabilities of DRAFT using three commonly used benchmark datasets."
        },
        {
            "heading": "4.1 DRAFT Setup",
            "text": "We use a Wikipedia dump from 2018 for Data Collection. All encoders, a bi-encoder, and a classifier used in DRAFT are initialized with SimCSE (Gao et al., 2021) trained through supervised learning. While each encoder shares the same weights and contains approximately 330 million parameters, these weights are not shared during the training of DRAFT. We first train the bi-encoder in DRAFT on Natural Question (Kwiatkowski et al., 2019) using contrastive InfoNCE loss (Oord et al., 2018). Once trained, the weights of the bi-encoder remain fixed, only requiring fine-tuning of a classifier when the type of topic is modified. Since the number of samples extracted from MQR varies for each dataset, the size of Customized dataset used by a classifier in DRAFT for training also fluctuates accordingly.\nWhen fine-tuning a classifier, we use the AdamW (Loshchilov and Hutter, 2017) optimizer with a learning rate of 1e-5 using a linear learning rate scheduler. We divide Customized dataset into an 80% to 20% split to form the training and validation datasets, respectively. The classifier is trained by employing early stopping against the validation loss with the patience of two epochs. We perform all experiments using the PyTorch framework and three 32GB Tesla V100 GPUs."
        },
        {
            "heading": "4.2 Few-shot topic classification task",
            "text": "Dataset To evaluate the effectiveness of DRAFT in few-shot topic classification, specifically in diverse topic scenarios, we construct 291 test datasets. Given the absence of existing benchmark datasets specifically designed for its task, we perform web crawling on the FactsNet website1, which covers a broad spectrum of subjects comprising 291 distinct topics. The content on the website has a three-level hierarchical structure consisting of a major category, a subcategory, and a subtopic. The major categories are comprised of five types: Lifestyle, History, Nature, World, and Science. Additional information about datasets is in Appendix A.\nIn our dataset construction, we leverage contents in subtopics. We consider classifying each subtopic as a distinct topic classification task, resulting in 291 test datasets. We select subtopics with at least\n1The website is as follows: https://facts.net\n50 samples that belong to the positive class. For each dataset, we construct negative samples, composed of easy negatives and hard negatives, to the same size as the positive samples. Easy negatives are randomly sampled from subtopics that belong to a different subcategory than the positive ones; hard negatives are randomly mined from subtopics within the same subcategory as the positive class.\nBaselines We set LLMs as baseline models due to their capability to serve as classifiers for various topics when provided with a few labeled samples for ICL. The format of ICL is described in Appendix B.3. We use two types of LLMs, GPT3 (Brown et al., 2020) and InstructGPT (Ouyang et al., 2022), both with two versions of model sizes: 2.7B and 175B. InstructGPT is an improved version of GPT-3, fine-tuned through reinforcement learning with human feedback to enhance its understanding of prompts.\nDetail settings Given the absence of training datasets, we adopt Quick Facts, a collection of five texts associated with each subtopic in FactsNet, as queries for DRAFT and examples of ICL, serving as sample instances of the positive class. We employ various ICL examples from Quick Facts for LLMs, specifically selecting one, three, or five examples. For DRAFT, we configure a set of five examples as queries, with a subspace size of 10,000 allocated for each query. The examples of Quick Facts can be seen in Appendix B.4.\nTo assess the capability of classifying diverse topics, we derive rankings based on the F1 score for each major category across all methodologies and subsequently compute an average rank based on the five major categories. The robustness of each methodology in topic classification can be assessed through the average rank.\nResults and analysis Table 1 presents the evaluation results of the few-shot classification tasks on diverse topics. We evaluate the F1 score for all 291 subtopics and aggregate the results based on the five major categories. Among the baselines with billions of parameters, except for InstructGPT 175B 1-shot, DRAFT, only with millions of parameters, demonstrates superior performance compared to the others in all categories. When considering the average rankings across five major categories, DRAFT achieves the highest rank of 1.4, followed by InstructGPT 175B 1-shot with an average rank of 1.6, implying DRAFT\u2019s optimality.\nMethod\nMajor category DRAFT Random Noun. Dense.\nAblation study We conduct an ablation study to highlight the difficulty of few-shot topic classification across diverse topics. We establish three simple baselines in this experiment using the same datasets in Table 1. (1) Random method randomly assigns classes to a sample with a uniform distribution of positive and negative classes, where the positive class corresponds to a target topic while the negative class corresponds to all other topics. (2) Noun-based method classifies a sample as a target topic if any nouns from the queries exist in the sample. We use NLTK package2 to extract all nouns. (3) Dense-based method classifies a sample using embeddings obtained from [CLS] token. If the cosine similarity score between a query vector and a sample vector surpasses a threshold determined by MQR for any of the queries, the sample is classified as a target topic. The queries used in (2) and (3) are identical to those used in DRAFT.\nWe observe a clear trend of DRAFT outperforming three baselines across all main categories in Table 2. It implies the difficulty of the task and highlights the limitations of simple approaches, such as Noun-based or Dense-based methods. Nevertheless, the Noun-based method outperforms LLMs\n2NLTK package is from https://www.nltk.org\nin Table 1, excluding InstructGPT 175B 1-shot. Also, the most straightforward approach, Random method, surpasses some LLMs. Thus, we emphasize that LLMs are ineffective for few-shot topic classification tasks across diverse topics."
        },
        {
            "heading": "4.3 Including negative queries on DRAFT",
            "text": "Dataset We manually construct two additional test datasets with Religion and South Korea topics using a similar format to Section 4.2. In contrast to the automatically constructed datasets in Section 4.2 by crawling the website, five annotators manually construct datasets that determine whether each sample belongs to the positive, easy negative, or hard negative class. The easy negative and hard negative in the test dataset share the same negative class label. However, the differentiation is made to demonstrate the construction process of the negative dataset. We define easy negatives as samples unrelated to the positive class in terms of their semantic content. In contrast, hard negatives are samples that fall into semantically similar categories to positive ones but have distinct content. In Religion dataset, we define \u2018Jewish\u2019 and \u2018Islam\u2019 as positive classes. In contrast, the hard negatives consist of content that falls within the religion category but pertains to different specific religions, such as \u2018Buddha\u2019 and \u2018Hinduism\u2019. The easy negatives are composed of content unrelated to religion altogether. The examples of classes within the dataset on Religion dataset, additional details about South Korea dataset, and instructions for annotators can be found in Appendix A.\nBaselines We investigate the impact of different methods on building negative datasets in DRAFT using three distinct methods: M1, M2, and M3. M1 is defined by its use of random sampling, while M2 exclusively employs negative queries as dictated by MQR. M3 is a combination of two methods, with\n50% of the dataset constructed from M1 and the remaining 50% from M2.\nDetail settings We use three positive and two negative queries in Religion dataset, whereas four positive and three negative queries in South Korea dataset. The subspace size associated with each query is set at 10,000. Detailed instances of two types of queries can be seen in Appendix B.4.\nResults and analysis We evaluate the F1 score and accuracy for each methodology on the Religion and South Korea datasets. To further examine the impact of negative queries, we measure the accuracy based on the three distinct classes defined during the dataset construction process, which include the positive, easy negative, and hard negative classes. In Table 3, M1 demonstrates proficient classification of easy negatives, albeit struggles in effectively identifying hard negatives. Conversely, M2 exhibits reasonable performance in classifying hard negatives but at the expense of lower accuracy for easy negatives. Remarkably, M3, which builds a negative dataset constructed through a combination of random sampling and employment of negative queries, consistently shows superior performance. Our experimental findings emphasize the significance of M3 in effectively mitigating bias associated with negative queries while ensuring accurate classification of semantically independent content from positive samples. It verifies the potential of employing negative queries to enhance the performance of DRAFT."
        },
        {
            "heading": "4.4 Multi-class classification task",
            "text": "Dataset We employ three general benchmark datasets to evaluate the performance of DRAFT with a limited number of samples. AGNews (Zhang et al., 2015) is a collection of news articles used for a 4-way topic classification task. DBpedia (Auer et al., 2007) is an ontology dataset for a 14-way\ntopic classification task. TREC (Voorhees and Tice, 2000) is a dataset for a 6-way question classification task, which differs from topic-based content. Additional details can be found in Appendix A.\nBaselines Among various LLMs, we consider GPT-2 XL with 1.3B parameters (Radford et al., 2019) and GPT-3 (Brown et al., 2020) with different model sizes, such as 175B and 2.7B parameters.\nDetail settings We conduct ICL with LLMs to evaluate their performance given k labeled randomly sampled examples from the training dataset for each class, with k being 1, 4, and 8. Similarly, we randomly select eight samples corresponding to each class from the training dataset to serve as queries for DRAFT. Empirically, experiments conducted in Section 4.2 and Section 4.3, which solely rely on one-class samples as few-shot samples, akin to one-class classification, are more challenging compared to multi-class classification, which provides few-shot samples for all classes. Considering the difficulty of the task, we set the subspace size to 50 in this experiment.\nResults and analysis Experiments are run five times, each iteration using a different random seed for sampling queries and examples. Table 43 presents the accuracy results in classification on benchmark datasets. DRAFT outperforms all baselines on the topic classification tasks in both AGNews and DBpedia. Comparing DRAFT with the best performance cases of GPT-3 175B, the results show a difference of 3.7%p accuracy in AGNews and 12.4%p in DBpedia. While DRAFT achieves a lower accuracy than GPT-3 175B in TREC, it still outperforms GPT-2 XL and GPT-3 2.7B.\n3Results on baselines are reported from (Zhao et al., 2021)\nWe find that the performance of DRAFT varies across different benchmark datasets, suggesting that the attribute of Customized dataset plays a crucial role. As DRAFT uses Data Collection to construct Customized dataset, the choice of Data Collection strongly influences its performance. In our experiments, by leveraging Wikipedia as Data Collection, which primarily consists of topic-based content, DRAFT consistently outperforms all baselines with lower variance on AGNews and DBpedia, which also consist of topic-based content. However, on TREC, which involves attributes different from topic-based content, DRAFT exhibits a lower performance compared to GPT-3 175B. These results indicate that DRAFT works outstanding for classifying topics but may not exhibit robust performance in other classification tasks."
        },
        {
            "heading": "5 Discussion",
            "text": "Potential points in DRAFT In DRAFT, the number of queries and the subspace size are considered the most crucial factors among various hyperparameters. Queries provide information related to the target topic, while subspace size determines how many passages related to each query are retrieved from Data Collection. They influence the performance of DRAFT, as they significantly impact the construction of Customized dataset, which directly impacts the process of training a classifier.\nWe investigate the relationship between the number of queries and the subspace size by varying both variables using AGNews. The experiments are repeated five times with different configurations, and the average accuracy results are presented in\nFigure 2. Increasing the number of queries exhibits a positive correlation with accuracy when the subspace size is fixed. The highest accuracy of 88% is achieved with a subspace size of 10 and 50 queries. Although the increase in the number of queries is limited to 50 due to computational resource constraints, the consistent trend implies that more queries can improve performance.\nWe find that DRAFT has the potential for improvement with an increased number of queries. Unlike DRAFT, LLMs can suffer from degraded performance due to the majority label bias (Zhao et al., 2021) when the number of examples from the same class increases in few-shot samples. In Section 4.2, InstructGPT 175B demonstrates a noticeable decline in performance as the value of k increases in k-shot settings. Considering majority label bias and experimental results, DRAFT shows a more robust performance than LLMs.\nData Collection We underscore the importance of Data Collection in Section 4.4. DRAFT shows the highest accuracy on DBpedia due to the similar distribution between the test dataset and Data Collection. However, classifying topics that reflect recent content becomes a challenging issue for DRAFT, which uses a Wikipedia dump from 2018 for Data Collection, since there is a discrepancy between the distribution of the training dataset and that of topics that reflect the recent content. DRAFT can simply solve the problem by building Data Collection with recent knowledge. From the perspective of injecting external knowledge into the model or framework, DRAFT offers a more straightforward approach than LLMs (Meng et al., 2022; Mitchell et al., 2021).\nReal-world applications DRAFT can effectively classify content related to specific topics from the Internet or SNS. It can swiftly generate a classifier for the given topic upon user request with few queries. However, a challenge emerges as the number of tasks for classifying topics escalates significantly with the increasing number of users on the Internet or the growing quantity of topics requested. Storing the weights of all individually trained classifiers for each task is inefficient. Thus, to effectively implement DRAFT in real-world scenarios, one must consider an approach centered on parameter-efficient learning (Houlsby et al., 2019; Liu et al., 2021b; Hu et al., 2021) to allow the efficient management of weights for each task."
        },
        {
            "heading": "6 Conclusion",
            "text": "In this study, we introduce DRAFT, a simple but effective approach that first applies a dense retriever model for few-shot classification across diverse topics. Despite possessing 177 times fewer parameters than LLMs, DRAFT demonstrates superior performance in few-shot topic classification. These results imply the effectiveness of DRAFT in classifying a diverse array of real-world topics. We anticipate that DRAFT holds the potential to be implemented in practical contexts and actively contribute to addressing diverse societal challenges, including those encompassing specific topics."
        },
        {
            "heading": "7 Limitations",
            "text": "In Section 4.3, the negative query is defined as a text with content similar to the positive class but with a different topic. Although DRAFT demonstrates the ability to improve performance upon receiving a negative query, the automatic generation or construction of negative queries is necessary for real-world applications since people cannot manually provide a negative query for every topic.\nAlso, some ambiguous topics need to be calibrated in automatically constructed 291 datasets using the FactsNet website. For example, the queries in \u2018Sports\u2019 topic primarily revolve around baseball and golf. After training DRAFT, it can effectively classify content related to baseball and golf. However, after manually examining the test dataset for \u2018Sports\u2019 topic, it becomes apparent that examples of other sports, such as basketball and tennis, are belonged to the positive class. Although basketball and tennis undoubtedly fall under the sports category, the queries are composed solely of content related to specific examples of sports, such as baseball and golf. The ambiguity of the test dataset poses a challenge for DRAFT, proving to be a difficult task even for humans to classify these contents with the same few-shot samples accurately. Therefore, conducting human manual reviews for each of the 291 subtopics within FactsNet to filter out ambiguous topics could enhance the reliability of experimental results.\nMoreover, a limitation of DRAFT lies in the need for mathematically rigorous proof for the validity of its MQR. As a result, in future research, we plan to undertake endeavors to address the quality issue of FactsNet and establish rigorous mathematical proof for evaluating the effectiveness of MQR."
        },
        {
            "heading": "8 Ethics Statement",
            "text": "We explicitly mention the copyright of the FactsNet dataset, crawled from the website. We firmly state that we use the Factset dataset simply for evaluation due to the absence of a benchmark dataset containing diverse topics. Also, we employ five annotators while building two additional manually constructed datasets in Section 4.3. Our annotators are affiliated with our company and receive compensation through wages for their labeling work. Detailed instruction for the annotation task is in Appendix A. Also, we strongly discourage any misuse of DRAFT for illegal activities."
        },
        {
            "heading": "A Details of datasets",
            "text": "Data information\nMajor category Topic-num Sample-num Avg. token length\nIn this study, we use a total of six datasets. FactsNet is used in Section 4.2. Religion and South Korea are used in Section 4.3. AGNews, DBpedia, and TREC, are used in Section 4.4.\nFactsNet We manually construct 291 test datasets for few-shot topic classification tasks across diverse topics by crawling http://www.facts. net. It has a three-level hierarchical structure consisting of a major category, a subcategory, and a subtopic. The major category comprises six topics (\u2018Lifestyle,\u2019 \u2018History,\u2019 \u2018Nature,\u2019 \u2018World,\u2019 \u2018Science,\u2019 and \u2018General\u2019). However, we exclude \u2018General\u2019 when constructing the dataset since it consists of ambiguous content that does not align well with topic classification. Detailed information for five major categories can be found in Table 5. Table 6 provides types of subcategories under each major category and all subtopics under each subcategory. In constructing the dataset, we set the positive class as 0, the easy negative class as 1, and the hard negative class as 2. The composition of the label is: (0:Positive, 1:Negative), where the positive class belongs to content related to the subtopic, and the negative class contains easy negative and hard negative. Table 7 shows example samples of datasets.\nReligion Unlike FactsNet dataset, we determine whether samples are appropriate for positive and negative classes by examining each sample individually. Five annotators label each sample to determine if it contains content related to the topics defined in Religion. Samples for the hard negative class are constructed with the same method. The instruction for annotators regarding all samples is as follows: \u2018Please choose the correct label for the following sentence. If the following sentence is related to Jewish or Islam, choose 0. If the fol-\nlowing sentence is not related to religion, choose 1. If the following sentence is related to Buddha or Hinduism, choose 2. Answer: 0, 1, 2\u2019. The final label for all classes is determined through a majority vote. Religion dataset consists of 78 samples from https://www.history.com/topics/religion. The composition of the label is: (0:Positive, 1:Negative), where the negative class contains both easy negative and hard negative. Examples of the dataset can be seen in Table 8.\nSouth Korea For South Korea, the test dataset consists of 112 samples from http://www.facts.net, https://pitchfork.com, https://www.britannica.com, and https://en.yna.co.kr. This dataset is constructed using a similar method to Religion dataset. Positive samples are related to \u2018South Korea\u2019. Hard negatives are composed of content related to other Northeast Asian countries, such as \u2018North Korea\u2019, \u2018China\u2019, and \u2018Japan\u2019, which fall under the same country category but differ from South Korea. The instruction for annotators regarding all samples is as follows: \u2018Please choose the correct label for the following sentence. If the following sentence is not related to the country, choose 1. If the following sentence is related to South Korea, choose 0. If the following sentence is related to North Korea, China, or Japan, choose 2. Answer: 0, 1, 2\u2019. The composition of the label is: (0:Positive, 1:Negative). Examples of the dataset can be seen in Table 9.\nAGNews A collection of news articles from ComeToMyHead is used for a 4-way topic classification problem. We use a test dataset consisting of 7,600 samples from https://huggingface.co/ datasets/ag_news. The composition of the label is: (0:World, 1: Sports, 2:Buisness, 3:Sci/Tec).\nDBpedia DBpedia ontology classification dataset is used for a 14-way topic classification problem. We use a test dataset consisting of 70,000 samples from https://huggingface.co/datasets/dbpedia_14. The composition of the label is: (0:Company, 1:EducationalInstitution, 2:Artist, 3:Athlete, 4:OfficeHolder, 5:MeanOfTransportation, 6:Building, 7:NaturalPlace, 8:Village, 9:Animal, 10:Plant, 11:Album, 12:Film, 13:WrittenWork).\nTREC Text Retrieval Conference Question Classification is used for 6-way question classification tasks, which differ from topic-based content classification. The sample consists of a question, and\nthe labels are divided into six types of question answers. A test dataset consisting of 500 samples is used from https://huggingface.co/datasets/trec. The composition of the label is: (0:Abbreviation, 1:Entity, 2:Description and abstract concept, 3:Human being, 4:Location, 5:Numeric value)."
        },
        {
            "heading": "B Details of experiment",
            "text": "B.1 Resources\nWhen training a dense retriever model in DRAFT, a bi-encoder, we employ a Distributed Data Parallel (DDP) setting with three GPUs. However, we only use a single GPU to train the classifier for the topic classification task in DRAFT.\nDuring the experiments with LLMs, we use OpenAI\u2019s API 4 and conduct ICL. The cost for GPT3 and InstructGPT models, with a model size of 175B, is $0.02 per thousand tokens, while the cost for a model size of 2.7B is $0.0004 per thousand tokens. Due to API costs, we run the LLMs experiments directly only in Section 4.2, and in Section 4.4, we take the results from (Zhao et al., 2021). Therefore, the candidates for k in the k-shot setting differ between them.\nB.2 Hyperparameters\nHyperparameter settings for training a bi-encoder are identical to those of (Karpukhin et al., 2020). In all experiments, all classifiers in DRAFT use a fixed batch size of 256 and a maximum token length of 128. As mentioned in Section 4.4, the subspace and number of queries are tuned using grid search for hyperparameter search, using accuracy as the criterion, while the remaining hyperparameters are not tuned. The values used for the subspace size and the number of queries are [1, 5, 10, 50, 100, 500, 1000] and [5, 10, 15, 20, 25, 50], respectively. The samples extracted from the training datasets of benchmark datasets in Section 4.4 are randomly selected using five random seeds: [1234, 5678, 1004, 7777, 9999].\nB.3 Prompts format\nWe refer to (Chiu et al., 2021) for the prompt format on LLMs for ICL in Section 4.2. Table 11 lists examples of prompt formats. The prompt format for LLMs in Section 4.4 can be found in (Zhao et al., 2021).\n4https://openai.com/api/pricing/\nB.4 Query examples In Section 4.2, the queries obtained from DRAFT, sourced from Quick Facts, are additionally utilized as ICL examples for the LLMs. On the other hand, in Section 4.3, five annotators formulate both positive and negative queries relevant to defined classes in Religion and South Korea. Queries used in Section 4.2 and Section 4.3 can be seen in Table 10. For Section 4.4, we select queries randomly from the training dataset, using the random seed referred to in Appendix B.2."
        }
    ],
    "title": "DRAFT: Dense Retrieval Augmented Few-shot Topic Classifier Framework",
    "year": 2023
}