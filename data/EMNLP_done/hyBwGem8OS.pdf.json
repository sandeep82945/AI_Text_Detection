{
    "abstractText": "Multilingual language models (MLLMs) have achieved remarkable success in various crosslingual transfer tasks. However, they suffer poor performance in zero-shot low-resource languages, particularly when dealing with longer contexts. Existing research mainly relies on full-model fine-tuning on large parallel datasets to enhance the cross-lingual alignment of MLLMs, which is computationally expensive. In this paper, we propose InteMATs, a novel approach that integrates multilingual adapters trained on texts of different levels of granularity. To achieve this, we curate a multilingual parallel dataset comprising 42 languages to pre-train sentence-level and document-level adapters under the contrastive learning framework. Extensive experiments demonstrate the effectiveness of InteMATs in improving the cross-lingual transfer performance of MLLMs, especially on lowresource languages. Finally, our comprehensive analyses and ablation studies provide a deep understanding of the high-quality representations derived by InteMATs.",
    "authors": [
        {
            "affiliations": [],
            "name": "Meizhen Liu"
        },
        {
            "affiliations": [],
            "name": "Xu Guo"
        },
        {
            "affiliations": [],
            "name": "Jiakai He"
        },
        {
            "affiliations": [],
            "name": "Jianye Chen"
        },
        {
            "affiliations": [],
            "name": "Siu Cheung Hui"
        },
        {
            "affiliations": [],
            "name": "Fengyu Zhou"
        }
    ],
    "id": "SP:15a547d009d746cad94e2ea1963dde1f850593f0",
    "references": [
        {
            "authors": [
                "Mikel Artetxe",
                "Sebastian Ruder",
                "Dani Yogatama."
            ],
            "title": "On the cross-lingual transferability of monolingual representations",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4623\u20134637.",
            "year": 2020
        },
        {
            "authors": [
                "Mikel Artetxe",
                "Sebastian Ruder",
                "Dani Yogatama."
            ],
            "title": "On the cross-lingual transferability of monolingual representations",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4623\u20134637, Online. Association",
            "year": 2020
        },
        {
            "authors": [
                "Mikel Artetxe",
                "Holger Schwenk."
            ],
            "title": "Massively multilingual sentence embeddings for zeroshot cross-lingual transfer and beyond",
            "venue": "Transactions of the Association for Computational Linguistics, 7:597\u2013610.",
            "year": 2019
        },
        {
            "authors": [
                "Mikel Artetxe",
                "Holger Schwenk."
            ],
            "title": "Massively multilingual sentence embeddings for zeroshot cross-lingual transfer and beyond",
            "venue": "Transactions of the Association for Computational Linguistics, 7:597\u2013610.",
            "year": 2019
        },
        {
            "authors": [
                "Daniel Cer",
                "Mona Diab",
                "Eneko E Agirre",
                "I\u00f1igo LopezGazpio",
                "Lucia Specia."
            ],
            "title": "Semeval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation",
            "venue": "The 11th International Workshop on Semantic Evaluation (SemEval-2017),",
            "year": 2017
        },
        {
            "authors": [
                "Ting Chen",
                "Simon Kornblith",
                "Mohammad Norouzi",
                "Geoffrey Hinton."
            ],
            "title": "A simple framework for contrastive learning of visual representations",
            "venue": "International conference on machine learning, pages 1597\u20131607. PMLR.",
            "year": 2020
        },
        {
            "authors": [
                "Xinlei Chen",
                "Haoqi Fan",
                "Ross Girshick",
                "Kaiming He."
            ],
            "title": "Improved baselines with momentum contrastive learning",
            "venue": "arXiv preprint arXiv:2003.04297.",
            "year": 2020
        },
        {
            "authors": [
                "Zewen Chi",
                "Li Dong",
                "Furu Wei",
                "Nan Yang",
                "Saksham Singhal",
                "Wenhui Wang",
                "Xia Song",
                "Xian-Ling Mao",
                "He-Yan Huang",
                "Ming Zhou."
            ],
            "title": "Infoxlm: An information-theoretic framework for cross-lingual language model pre-training",
            "venue": "Proceedings of the",
            "year": 2021
        },
        {
            "authors": [
                "Zewen Chi",
                "Li Dong",
                "Bo Zheng",
                "Shaohan Huang",
                "XianLing Mao",
                "Heyan Huang",
                "Furu Wei."
            ],
            "title": "Improving pretrained cross-lingual language models via self-labeled word alignment",
            "venue": "arXiv preprint arXiv:2106.06381.",
            "year": 2021
        },
        {
            "authors": [
                "Zewen Chi",
                "Shaohan Huang",
                "Li Dong",
                "Shuming Ma",
                "Bo Zheng",
                "Saksham Singhal",
                "Payal Bajaj",
                "Xia Song",
                "Xian-Ling Mao",
                "He-Yan Huang"
            ],
            "title": "Xlm-e: Cross-lingual language model pre-training via electra",
            "venue": "In Proceedings of the 60th Annual Meeting",
            "year": 2022
        },
        {
            "authors": [
                "Jonathan H Clark",
                "Eunsol Choi",
                "Michael Collins",
                "Dan Garrette",
                "Tom Kwiatkowski",
                "Vitaly Nikolaev",
                "Jennimaria Palomaki."
            ],
            "title": "Tydi qa: A benchmark for information-seeking question answering in typologically diverse languages",
            "venue": "Transactions of the As-",
            "year": 2020
        },
        {
            "authors": [
                "Kevin Clark",
                "Urvashi Khandelwal",
                "Omer Levy",
                "Christopher D. Manning."
            ],
            "title": "What does BERT look at? an analysis of BERT\u2019s attention",
            "venue": "Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP,",
            "year": 2019
        },
        {
            "authors": [
                "Alexis Conneau",
                "Kartikay Khandelwal",
                "Naman Goyal",
                "Vishrav Chaudhary",
                "Guillaume Wenzek",
                "Francisco Guzm\u00e1n",
                "Edouard Grave",
                "Myle Ott",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Unsupervised cross-lingual representation learning at scale",
            "venue": "An-",
            "year": 2019
        },
        {
            "authors": [
                "Alexis Conneau",
                "Guillaume Lample."
            ],
            "title": "Crosslingual language model pretraining",
            "venue": "Advances in neural information processing systems, 32.",
            "year": 2019
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
            "year": 2019
        },
        {
            "authors": [
                "Ning Ding",
                "Yujia Qin",
                "Guang Yang",
                "Fuchao Wei",
                "Zonghan Yang",
                "Yusheng Su",
                "Shengding Hu",
                "Yulin Chen",
                "Chi-Min Chan",
                "Weize Chen"
            ],
            "title": "Delta tuning: A comprehensive study of parameter efficient methods for pre-trained language models",
            "year": 2022
        },
        {
            "authors": [
                "Fangxiaoyu Feng",
                "Yinfei Yang",
                "Daniel Cer",
                "Naveen Arivazhagan",
                "Wei Wang."
            ],
            "title": "Language-agnostic bert sentence embedding",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 878\u2013891.",
            "year": 2022
        },
        {
            "authors": [
                "Tianyu Gao",
                "Xingcheng Yao",
                "Danqi Chen."
            ],
            "title": "Simcse: Simple contrastive learning of sentence embeddings",
            "venue": "2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, pages 6894\u20136910. Association for Computational",
            "year": 2021
        },
        {
            "authors": [
                "Xu Guo",
                "Han Yu."
            ],
            "title": "On the domain adaptation and generalization of pretrained language models: A survey",
            "venue": "arXiv preprint arXiv:2211.03154.",
            "year": 2022
        },
        {
            "authors": [
                "Kaiming He",
                "Xinlei Chen",
                "Saining Xie",
                "Yanghao Li",
                "Piotr Doll\u00e1r",
                "Ross Girshick."
            ],
            "title": "Masked autoencoders are scalable vision learners",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16000\u201316009.",
            "year": 2022
        },
        {
            "authors": [
                "Yifan Hou",
                "Wenxiang Jiao",
                "Meizhen Liu",
                "Carl Allen",
                "Zhaopeng Tu",
                "Mrinmaya Sachan."
            ],
            "title": "Adapters for enhanced modeling of multilingual knowledge and text",
            "venue": "EMNLP 2022.",
            "year": 2022
        },
        {
            "authors": [
                "Neil Houlsby",
                "Andrei Giurgiu",
                "Stanis\u0142aw Kamil Jastrz\u0119bski",
                "Bruna Halila Morrone",
                "Quentin de Laroussilhe",
                "Andrea Gesmundo",
                "Mona Attariyan",
                "Sylvain Gelly"
            ],
            "title": "Parameter efficient transfer learning for nlp",
            "year": 2019
        },
        {
            "authors": [
                "Junjie Hu",
                "Sebastian Ruder",
                "Aditya Siddhant",
                "Graham Neubig",
                "Orhan Firat",
                "Melvin Johnson."
            ],
            "title": "Xtreme: A massively multilingual multi-task benchmark for evaluating cross-lingual generalisation",
            "venue": "International Conference on Machine Learn-",
            "year": 2020
        },
        {
            "authors": [
                "James Kirkpatrick",
                "Razvan Pascanu",
                "Neil Rabinowitz",
                "Joel Veness",
                "Guillaume Desjardins",
                "Andrei A Rusu",
                "Kieran Milan",
                "John Quan",
                "Tiago Ramalho",
                "Agnieszka Grabska-Barwinska"
            ],
            "title": "Overcoming catastrophic forgetting in neural networks",
            "year": 2017
        },
        {
            "authors": [
                "Abdullatif K\u00f6ksal",
                "Arzucan \u00d6zg\u00fcr."
            ],
            "title": "The relx dataset and matching the multilingual blanks for cross-lingual relation classification",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 340\u2013350.",
            "year": 2020
        },
        {
            "authors": [
                "Xiang Lisa Li",
                "Percy Liang."
            ],
            "title": "Prefix-tuning: Optimizing continuous prompts for generation",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language",
            "year": 2021
        },
        {
            "authors": [
                "Linlin Liu",
                "Xin Li",
                "Ruidan He",
                "Lidong Bing",
                "Shafiq Joty",
                "Luo Si."
            ],
            "title": "Enhancing multilingual language model with massive multilingual knowledge triples",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2022
        },
        {
            "authors": [
                "Fuli Luo",
                "Wei Wang",
                "Jiahao Liu",
                "Yijia Liu",
                "Bin Bi",
                "Songfang Huang",
                "Fei Huang",
                "Luo Si."
            ],
            "title": "Veco: Variable and flexible cross-lingual pre-training for language understanding and generation",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for",
            "year": 2021
        },
        {
            "authors": [
                "Joakim Nivre",
                "Marie-Catherine de Marneffe",
                "Filip Ginter",
                "Jan Hajic",
                "Christopher D Manning",
                "Sampo Pyysalo",
                "Sebastian Schuster",
                "Francis Tyers",
                "Daniel Zeman."
            ],
            "title": "Universal dependencies v2: An evergrowing multilingual treebank collection",
            "venue": "In",
            "year": 2020
        },
        {
            "authors": [
                "Aaron van den Oord",
                "Yazhe Li",
                "Oriol Vinyals."
            ],
            "title": "Representation learning with contrastive predictive coding",
            "venue": "arXiv preprint arXiv:1807.03748.",
            "year": 2018
        },
        {
            "authors": [
                "Namuk Park",
                "Wonjae Kim",
                "Byeongho Heo",
                "Taekyung Kim",
                "Sangdoo Yun"
            ],
            "title": "What do selfsupervised vision transformers learn",
            "venue": "In The Eleventh International Conference on Learning Representations",
            "year": 2023
        },
        {
            "authors": [
                "Jonas Pfeiffer",
                "Naman Goyal",
                "Xi Victoria Lin",
                "Xian Li",
                "James Cross",
                "Sebastian Riedel",
                "Mikel Artetxe."
            ],
            "title": "Lifting the curse of multilinguality by pretraining modular transformers",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of",
            "year": 2022
        },
        {
            "authors": [
                "Jonas Pfeiffer",
                "Aishwarya Kamath",
                "Andreas R\u00fcckl\u00e9",
                "Kyunghyun Cho",
                "Iryna Gurevych."
            ],
            "title": "Adapterfusion: Non-destructive task composition for transfer learning",
            "venue": "Proceedings of the 16th Conference of the European Chapter of the Association",
            "year": 2021
        },
        {
            "authors": [
                "Jonas Pfeiffer",
                "Ivan Vuli\u0107",
                "Iryna Gurevych",
                "Sebastian Ruder."
            ],
            "title": "Mad-x: An adapter-based framework for multi-task cross-lingual transfer",
            "venue": "arXiv preprint arXiv:2005.00052.",
            "year": 2020
        },
        {
            "authors": [
                "Sylvestre-Alvise Rebuffi",
                "Hakan Bilen",
                "Andrea Vedaldi."
            ],
            "title": "Learning multiple visual domains with residual adapters",
            "venue": "Advances in neural information processing systems, 30.",
            "year": 2017
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin."
            ],
            "title": "Attention is all you need",
            "venue": "Advances in neural information processing systems, 30.",
            "year": 2017
        },
        {
            "authors": [
                "Yau-Shian Wang",
                "Ashley Wu",
                "Graham Neubig."
            ],
            "title": "English contrastive learning can learn universal cross-lingual sentence embeddings",
            "venue": "arXiv eprints, pages arXiv\u20132211.",
            "year": 2022
        },
        {
            "authors": [
                "Yinfei Yang",
                "Daniel Cer",
                "Amin Ahmad",
                "Mandy Guo",
                "Jax Law",
                "Noah Constant",
                "Gustavo Hernandez Abrego",
                "Steve Yuan",
                "Chris Tar",
                "Yun-Hsuan Sung"
            ],
            "title": "Multilingual universal sentence encoder for semantic retrieval",
            "venue": "In Proceedings of the 58th Annual Meet-",
            "year": 2020
        },
        {
            "authors": [
                "Yuan Zhang",
                "Jason Baldridge",
                "Luheng He."
            ],
            "title": "Paws: Paraphrase adversaries from word scrambling",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
            "year": 2019
        },
        {
            "authors": [
                "Pierre Zweigenbaum",
                "Serge Sharoff",
                "Reinhard Rapp."
            ],
            "title": "Overview of the second bucc shared task: Spotting parallel sentences in comparable corpora",
            "venue": "Proceedings of the 10th Workshop on Building and Using Comparable Corpora, pages 60\u201367.",
            "year": 2017
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Multilingual language models (MLLMs), such as mBERT (Devlin et al., 2019) and XLMR (Conneau et al., 2019), have achieved remarkable success on a wide range of cross-lingual tasks (Zweigenbaum et al., 2017; Artetxe and Schwenk, 2019a; Nivre et al., 2020; Zhang et al., 2019; Clark et al., 2020; Artetxe et al., 2020a). They are pre-trained on large multilingual data using pretext tasks such as masked language modeling (MLM). To apply MLLMs to downstream tasks, we need to fine-tune a separate copy for each specific task.\nHowever, due to limited data during pre-training, MLLMs may struggle to capture cross-lingual alignment for low-resource languages, resulting in subpar performance. To address this issue, subsequent works (Chi et al., 2021b; Liu et al., 2022;\nChi et al., 2022; Luo et al., 2021; Conneau et al., 2019) propose to enhance MLLM representations by continuing the pre-training on more parallel data using techniques such as translation language modeling (e.g., InfoXLM (Chi et al., 2021a) and XLMALIGN (Chi et al., 2021b)), contrastive learning (e.g., mSimCSE (Wang et al., 2022)), and so on (Yang et al., 2020; Feng et al., 2022).\nNevertheless, these approaches often require fine-tuning the entire backbone model, which is computationally expensive and can result in catastrophic forgetting (Kirkpatrick et al., 2017). Moreover, they mainly focus on enhancing sentencelevel representations. As a result, their performance gains on document-level cross-lingual tasks are not as impressive as sentence-level ones (Hu et al., 2020), indicating a need for a finer-grained perspective in enhancing cross-lingual alignment.\nMotivated by recent advancements in parameterefficient adaptation for large language models (LLMs) (Ding et al., 2022; Guo and Yu, 2022), we propose a novel approach named InteMATs, which stands for Integrating Granularity-specific Multilingual Adapters, to enhance cross-lingual transfer performance of MLLMs. Adapter tuning works by training a set of adapter modules conditioned on a frozen LLM (Houlsby et al., 2019). They can equip MLLMs with new knowledge (Hou et al., 2022) and facilitate language-specific adaptation (Pfeiffer et al., 2020, 2022) without modifying pretrained parameters. Different from them, we offer a new perspective for enhancing cross-lingual alignment by exploiting a set of multilingual adapters pre-trained on different levels of text granularity.\nTo obtain these adapters, we curate a multilingual parallel dataset consisting of 42 languages from Wikipedia1 and process them into the sentence-level corpus (DST) and document-level corpus (DDT). Recent findings (Park et al., 2023) reveal that contrastive learning (CL) tends to ex-\n1https://en.wikipedia.org/wiki/Main_Page\ntract global information, whereas masked image modeling (MIM) focuses on capturing local features, which well explained earlier results (Wang et al., 2022). Therefore, we employ CL to train adapters to capture global cross-lingual alignment information to augment the MLLM representations that may initially focus on extracting local information. Specifically, we first train a set of multilingual adapters on DST and DDT respectively. Then, we learn to fuse them by incorporating layer-wise fusion modules (e.g., AdapterFusion (Pfeiffer et al., 2021)) and train them on downstream data to determine the fusion weights.\nExtensive experiments on five kinds of crosslingual transfer tasks, including sentence-level and document-level ones, demonstrate that InteMATs significantly improves the cross-lingual transfer performance of MLLMs (i.e., mBERT and XLMR). In particular, InteMATs excels the stateof-the-art baseline by 4% on BUCC (Zweigenbaum et al., 2017), 7% on Tatoeba (Artetxe and Schwenk, 2019a), and 3.5% on TydiQA (Clark et al., 2020). Notably, InteMATs brings a substantial 30% improvement over its backbone MLLMs in low-resource languages that are never seen during pre-training, demonstrating the high quality of the representations learned by InteMATs. We finally conduct a comprehensive analysis on InteMATs to unravel the distribution of contributions on each component and layer-wise impact within InteMATs. We will make our data and model publicly available for future research."
        },
        {
            "heading": "2 Related Works",
            "text": "Cross-lingual Representation Learning Existing researches mainly adopt the full-model finetuning approach with monolingual or parallel corpus to obtain cross-lingual representations. They employ pretext tasks such as masked language modeling (MLM) (Devlin et al., 2019; Chi et al., 2021b), casual language modeling (CLM) (Conneau and Lample, 2019), and translation language modeling (TLM) (Chi et al., 2021a,b), to train MLLMs. However, from the results reported on the XTREME benchmark (Hu et al., 2020), we observe a decrease in cross-lingual transfer performance for MLLMs as the input text length increases (Hu et al., 2020). Previous researches mainly focus on improving cross-lingual alignment for sentence representations (Wang et al., 2022; Feng et al., 2022; Artetxe and Schwenk, 2019b). There is a lack of\nresearch specifically addressing the enhancement of document-level cross-lingual representations.\nAdapters for MLLMs Recently, Adapter-based approaches (Houlsby et al., 2019; Pfeiffer et al., 2020; Artetxe et al., 2020b; Li and Liang, 2021; Pfeiffer et al., 2022) have gained popularity as a parameter-efficient alternative to traditional finetuning for large language models (LLMs). These adapters are inserted between the transformer layers and are learned from data conditioned on a frozen LLM. They learn language-specific transformations to facilitate quick adaptation to new languages (Artetxe et al., 2020b) or new tasks (Pfeiffer et al., 2020). Recent research show that it is possible to inject new knowledge into MLLMs to enhance their cross-lingual representations (Hou et al., 2022). Different from previous research, this paper fills the gap of granularity perspective in cross-lingual alignment via adapter tuning.\nContrastive Learning for Language Models Contrastive learning (CL) (Gao et al., 2021; He et al., 2022; Chen et al., 2020a,b) has shown much promise in NLP for its capability of capturing discriminative information in an unsupervised manner. Many research works have adopted this pretext task in their MLLM pre-training, such as mSimCSE (Wang et al., 2022), LASER (Artetxe and Schwenk, 2019b), and InfoXLM (Chi et al., 2021a). However, these approaches require fine-tuning the entire backbone, resulting in non-trivial computational overhead. This paper circumvents this challenge by applying CL for pre-training multilingual adapters while leaving MLLMs frozen."
        },
        {
            "heading": "3 Preliminaries",
            "text": "We start by introducing some basic knowledge about adapter tuning and contrastive learning.\nAdapter Tuning: Adapter tuning (Houlsby et al., 2019) is a parameter-efficient transfer learning technique for adapting large pre-trained models to downstream tasks based on adapters (Rebuffi et al., 2017). These adapters are small, new taskspecific modules inserted between the layers of a pre-trained model. Instead of fine-tuning the entire model, adapter tuning only trains the parameters of the adapter modules while keeping the pre-trained model fixed. This approach allows us to specialize a pre-trained model in different tasks while retaining the knowledge acquired during pre-training.\nFollowing Houlsby et al. (2019), we use \u03d5w to denote a pre-trained model with parameters w: \u03d5w(x). For adapter tuning, a new function, \u03c8w,v(x), is composed, where parameters v denote all the adapter modules and w is copied from pretrained weights. The architecture of an adapter module is shown in Figure 1 (a). It consists of two feed-forward layers and a non-linear activation function. For the hidden states hl at layer l, an adapter module works as follows:\nAl(hl) = W 1 l (ReLU(W 2 l hl)) + rl, (1)\nwhere W 2l represents the down-projection matrix, W 1l represents the up-projection matrix, and ReLU is the activation function. rl represents the residual information from the original input, which bypasses the adapter\u2019s transformations.\nInfoNCE: InfoNCE (InfoMax with Noise Contrastive Estimation) (Oord et al., 2018) is a loss function commonly used in self-supervised learning, particularly in contrastive learning methods. The goal is to maximize the mutual information between related samples while minimizing it between unrelated samples, facilitating the discovery of discriminative features in an unsupervised manner.\nGiven a sample x, and a set of N random samples, X = {x1, ...,xN}, which contains one positive sample x+ and N \u2212 1 negative samples x\u2212, we minimize the following negative log-likelihood:\nInfoNCE = \u2212E X\n[ log\nesim(x,x+)/\u03c4\u2211 xi\u2208X e sim(x,xi)\u03c4\n] , (2)\nwhere \u03c4 is a temperature hyperparameter. Following SimCSE (Gao et al., 2021), we employ cosine similarity as a measure to compare the representations of positive pairs and negative pairs: h1\u00b7h2\u2225h1\u2225\u2225h2\u2225 . In this paper, we use pre-trained models such as mBERT (Devlin et al., 2019) and XLMR (Conneau et al., 2019) to encode the input texts and only train all the adapters using the InfoNCE objective."
        },
        {
            "heading": "4 The InteMATs Approach",
            "text": "We now introduce InteMATs, which integrates multilingual adapters to enhance the representations of a fixed MLLM. InteMATs involves two stages. In the first stage, we pre-train two multilingual adapters to be specialized in processing texts of different levels of granularity: sentence-level multilingual adapters (MATs-ST) (\u00a74.1) and documentlevel multilingual adapters (MATs-DT) (\u00a74.2). Inspired by the findings (Park et al., 2023) which\nunraveled the complementary properties of CL and masked image modeling objectives, we use CL to train adapters to augment the MLLMs that have been initially trained with the MLM objective. The goal is to enhance the cross-lingual alignment of MLLMs with pluggable adapters while retaining their pre-learned knowledge. In the second stage, we show how to integrate these adapters for crosslingual transfer tasks. In this paper, we employ AdapterFusion (Pfeiffer et al., 2021) for this purpose (\u00a74.3). InteMATs, however, is not limited to the choice of MLLMs and fusion algorithms. We show the working mechanisms of InteMATs and a concrete example of MATs-ST in Figure 1."
        },
        {
            "heading": "4.1 Sentence-level Multilingual Adapters",
            "text": "Notations. We designate English as the source language and all the other languages as target languages. To train sentence-level multilingual adapters (MATs-ST), we curate an entity-aligned parallel dataset consisting of N sentences in J languages: DST = {Dj}Jj=1, where Dj = {xi}Ni=1. We employ a pre-trained MLLM as the text encoder and use the hidden states from the penultimate layer of the MLLM as text representations. For example, with mBERT, xi is encoded into a sequence of m+ 1 token representations: hi =< e([CLS]), e(t1), ..., e(tm\u22121), e([SEP]) >. Here, [CLS] and [SEP] are classification and separator tokens specially used for learning positional and structural information.\nThe pre-training objective. For every English sample xen, we randomly select another English sample xen+ and N non-English samples x j \u2212 to create contrastive data pairs. We denote the average hidden states of the sequence of tokens as hst, and take it as the sentence representation for a given sentence sample. Similarly, we use het as the representation for the aligned entity.\nWe train MATs-ST on top of a fixed MLLM by minimizing the following contrastive loss on the sentence-level corpus DST:\nLMATs\u2212ST(vs) = InfoNCE DST (henst ,h j st;vs) (3)\n+ InfoNCE DST\n(henet ,h j et;vs), (4)\nwhere the superscript en represents English and j \u2208 [1, J ] represents one of the J languages. vs denotes the parameters of MATs-ST. We encourage cross-lingual alignment at both sentence-level and entity-level representations."
        },
        {
            "heading": "4.2 Document-level Multilingual Adapter",
            "text": "As Hu et al. (2020) demonstrates, the cross-lingual transfer performance of MLLMs tends to degenerate on longer texts, potentially due to their limited capability of understanding longer contexts. Motivated by this observation, we specially train document-level multilingual adapters (MATs-DT) for MLLMs. Similar to the pre-training of MATsST, we curate a document-level parallel dataset comprising J languages: DDT = {Dj}Jj=1. For each document d inDj , we also average the hidden states from the penultimate layer of an MLLM as the document-level representation: hdt. The pre-training objective. The pre-training setup of MATs-DT is similar to that of MATs-ST except using longer input texts. We encourage adapters to capture cross-lingual alignment on the documentlevel corpus, DDT, by minimizing the following contrastive loss:\nLMATs\u2212DT(vd) = InfoNCE DDT (hendt ,h j dt;vd), (5)\nwhere vd denotes the parameters of MATs-DT. Note that we do not explicitly include the loss for entity-level alignment as Eq.4. Our experiments show that incorporating such constraint does not improve cross-lingual transfer performance and, in fact, leads to a decrease in performance. This suggests a conflict between entity-level alignment, which focuses on capturing local information, and document-level alignment, which emphasizes capturing global information."
        },
        {
            "heading": "4.3 Integrating Multilingual Adapters",
            "text": "We now introduce the second stage: knowledge composition. Inspired by AdapterFusion (Pfeiffer et al., 2021), which trains multiple task-specific adapters on top of a shared pre-trained model. We propose to train InteMATs in a similar way to fuse the multilingual adapters on cross-lingual tasks. By incorporating both MATs-ST and MATs-DT at each transformer layer, we seek a more comprehensive view for capturing transferrable features from varying lengths of context.\nAs illustrated in Figure 1, we append the AdapterFusion module right after the MATs-ST and MATs-DT modules, followed by a residual connection to the original transformer output, hl, at layer l. The outputs of MATs-ST (hst,l) and MATs-DT (hdt,l) are used as inputs for both the Value and Key transformations, and the new hidden states after fusion are as follows:\nQl = hlWq, Ql \u2208 R1\u00d7d (6) Kl = [hst,lWk,hdt,lWk],Kl \u2208 R2\u00d7d (7) Vl = [hst,lWv,hdt,lWv], Vl \u2208 R2\u00d7d (8) Ol = softmax(QlK \u22a4 l / \u221a d)Vl, Ol \u2208 R1\u00d7d (9)\nwhere d represents the size of hidden states, [\u00b7, \u00b7] indicates the concatenation of vectors, and Wq,Wk,Wv are d\u00d7dmatrices for computing cross attentions (Vaswani et al., 2017).\nWe train InteMATs on each cross-lingual task to determine the task-specific importance weights for MATs-ST and MATs-DT. In this process, the\nparameters of MATs-ST and MATs-DT are fixed while the parameters of the fusion module, vf , are optimized by minimizing, e.g., cross-entropy loss, on each task-specific dataset DT :\nLInteMATs(vf ) = E (x,y)\u2208DT \u2212logP (\u03c8vf (x) = y) (10)"
        },
        {
            "heading": "5 Experiments",
            "text": "In this section, we present the evaluation details and results of InteMATs across sentence-level and document-level cross-lingual tasks."
        },
        {
            "heading": "5.1 Experimental Setup",
            "text": "Pre-training Corpus. We collect a large, entityaligned, multilingual dataset from Wikipedia2. Each data sample is a summary text with a maximum length of 384 to describe an entity, which spans multiple languages. The dataset covers 42 languages in total, including those extensively studied in the popular XTREME benchmark (Hu et al., 2020). We treat English as the source language and ensure each English sample has at least four parallel samples from other languages. We utilize the first sentence of each summary text and curate the sentence-level parallel dataset DST for training MATs-ST and use the entire raw text DDT for training MATs-DT. More details about the dataset can be found in Appendix A.1.\nMLLM Backbones. We experiment on three representative MLLMs, the base version of mBERT (Devlin et al., 2019), both the base and large versions of XLMR (Conneau et al., 2019), to show the effects on different model types and model scales. Hyperparameters setup under each MLLM can be found in Appendix A.1."
        },
        {
            "heading": "5.2 Baselines",
            "text": "We compare InteMATs against the following 12 baselines. 1) MLLMs without adapters: we directly fine-tune the backbone MLLMs, mBERT, and XLMR (base and large versions), on downstream cross-lingual tasks. We also compare with another four SOTA MLLMs including VECO (Luo et al., 2021), XLM-ALIGN (Chi et al., 2021b), KMLM (Liu et al., 2022), and XLM-E (Chi et al., 2022). 2) On sentence retrieval tasks, we compare with two SOTA methods: mSimCSE (Wang et al., 2022) and InfoXLM (Chi et al., 2021a), which finetune the entire MLLM on their pre-training corpus before applying to downstream tasks. 3) On RELX\n2 https://github.com/martin-majlis/Wikipedia-API\ndataset (K\u00f6ksal and \u00d6zg\u00fcr, 2020), we compare with MTMB (K\u00f6ksal and \u00d6zg\u00fcr, 2020) which introduces distant supervision to enhance MLLMs, and MLKG (Hou et al., 2022) which incorporates multilingual knowledge graphs into MLLMs. 4) On XQuAD dataset (Artetxe et al., 2020a), we compare with two adapter-based methods: MLKG (Hou et al., 2022) and MAD-X (Pfeiffer et al., 2020) which uses one task-specific adapter. Note that we only report the results based on large version of XLMR for MLKG and KMLM.\nEvaluation Setup. We divide the languages in each downstream task into two types: Sup.: stands for supervised language, which is used for finetuning, and ZS: stands for zero-shot language, which is only used for testing."
        },
        {
            "heading": "5.3 Cross-lingual Semantic Textual Similarity",
            "text": "We first evaluate the models on the Cross-lingual Semantic Textual Similarity (STS) task (Cer et al., 2017) to assess the quality of their representations in capturing universal semantics. Following mSimCSE (Wang et al., 2022), we average the embeddings from the first and last layers.\nTable 1 presents the semantic textual similarity among the Arabic (ar), Spanish (es), English (en), and Turkish (tr) languages. Comparing InteMATs (XLMR) with the SOTA model, mSimCSE, we observe that on monolingual groups (ar\u2192ar, es\u2192es), mSimCSE demonstrates the highest textual similarity, while on cross-lingual groups (ar\u2192en, es\u2192en, tr\u2192en), InteMATs produces the highest textual similarity. Note that mSimCSE employs the large version of XLMR as the backbone during pretraining. This implies that InteMATs can enhance the representations of XLMR in capturing crosslingual semantics. Moreover, mSimCSE requires fine-tuning the entire MLLM in the pre-training stage while InteMATs only trains a set of adapters (See Appendix 5.11.). However, when conditioned on mBERT, it does not bring improvement, showing the limit on the choice of MLLMs."
        },
        {
            "heading": "5.4 Cross-lingual Sentence Retrieval",
            "text": "We use BUCC (Zweigenbaum et al., 2017) and Tatoeba (Artetxe and Schwenk, 2019a) datasets to evaluate the cross-lingual sentence retrieval performance of InteMATs. Specifically, given a sample from the source language, e.g., English, the model should correctly retrieve all the similar samples from other xx languages, and vice versa. On Tatoeba, we follow XLM-E (Chi et al., 2022) and mSimCSE (Wang et al., 2022) and report the results on both the 14 common languages (Tatoeba-14) and all the 36 languages (Tatoeba-36).\nTable 2 presents the overall performance comparison. InteMATs, when conditioned on XLMRlarge, outperforms all the baselines by a large margin, establishing a new state-of-the-art on the unsupervised settings of BUCC and Tatoeba. On all selected MLLMs, InteMATs outperforms its counterparts regardless of the model scale, indicating that there is significant potential for enhancing crosslingual alignment in the representations provided by pre-trained MLLMs. InteMATs outperforms the second-best state-of-the-art model, mSimCSE, implying that pre-training adapters through CL is better than the traditional full-model fine-tuning approach in aligning cross-lingual representations. Moreover, InteMATs outperforms other competitive MLLMs, namely XLM-E and InfoXLM, which employ the same backbone but with distinct pretraining tasks."
        },
        {
            "heading": "5.5 Cross-lingual Sentence Classification",
            "text": "We conduct performance evaluation on RELX (K\u00f6ksal and \u00d6zg\u00fcr, 2020), the cross-lingual relation extraction dataset, and PAWS-X (Zhang et al., 2019), the paraphrase detection dataset. RELX contains five languages and PAWS-X contains seven languages. We follow the XTREM benchmark (Hu et al., 2020) and train adapters only on English data and test on non-English data. Following MLKG\n(Hou et al., 2022), we report the performance on the source language, Sup(en), the average performance on all the other n zero-shot languages, ZS(n), and the average performance on all languages in the dataset.\nTable 3 presents the performance comparison results. Generally, InteMATs still takes the highest spot on both datasets, especially on zero-shot languages (ZS). Interestingly, the average performance of InteMATs matches that of VECO on PAWS-X. However, VECO requires fine-tuning the entire XLMR model and is supported with a huge pre-training corpus covering 50 languages (Luo et al., 2021) while InteMATs only trains a few pluggable adapters. Compared with its backbone models, mBERT and XLMR, InteMATs consistently enhances their cross-lingual zero-shot performance by 2% \u223c 3.2%. However, the performance gains on the source language (en) are not consistent, indicating that InteMATs may focus on enhancing the cross-lingual alignment on target languages."
        },
        {
            "heading": "5.6 Cross-lingual Syntactic Analysis",
            "text": "We conduct evaluations on a cross-lingual Part-ofSpeech (POS) dataset (Nivre et al., 2020) from the XTREME benchmark (Hu et al., 2020) to assess a model\u2019s capability of capturing syntactic structures and grammatical properties across 33 languages. All models are trained on English data.\nAs shown in Table 4, InteMATs achieves comparable performance to XLM-ALIGN and outperforms VECO and XLM-E on average. This suggests that instead of fine-tuning the entire MLLMs, transferring the syntactic knowledge acquired from English data to other languages can be easily achieved by only tuning a few adapters. It holds true regardless of the choice of MLLMs. Moreover, InteMATs achieves more performance gains for zeros-hot languages, showing the advantage of adapter tuning for MLLMs."
        },
        {
            "heading": "5.7 Cross-lingual Question-Answering",
            "text": "Question-answering (QA) requires a model to understand a given long context so as to correctly answer the questions by extracting the text span for the true answer from the context. We conduct evaluations on two popular multilingual QA benchmarks: XQuAD (Artetxe et al., 2020a) and TydiQA (Clark et al., 2020).\nTable 5 presents the performance comparison between InteMATs and SOTA models. In general, InteMATs excels all the full-model fine-tuning baselines (VECO, XLM-E, XLM-ALIGN, and KMLM), and adapter-based baselines (MLKG and MAD-X). Conditioned on the same large version of XLMR, InteMATs surpasses the second-best baseline, KMLM, by 0.3% on XQuAD and 3.2% on TydiQA, at a low cost of training. On both supervised and zero-shot benchmarks, InteMATs consistently outperforms fine-tuning the backbone models, mBERT and XLMR, indicating the advantage of incorporating multilingual adapters into pre-trained MLLMs."
        },
        {
            "heading": "5.8 Closing the Cross-Lingual Transfer Gap",
            "text": "We summarize the above cross-lingual performance results and conclude that InteMATs can effectively mitigate the issue of cross-lingual performance degeneration in pre-trained MLLMs, as shown in Table 6. All models are trained using the source English language, achieving a perfor-\nmance of Sen. They are subsequently tested on other languages, yielding an average performance of Stgt. The performance gap, \u2206 = Sen \u2212 Stgt, serves as an evaluation metric to assess the degree of cross-lingual transfer degradation.\nOn all but the POS task, InteMATs based on XLMRlarge, demonstrates the lowest cross-lingual performance degeneration, highlighting its advantage in enhancing cross-lingual knowledge transfer."
        },
        {
            "heading": "5.9 Scaling to Low-resource Languages",
            "text": "We further study how InteMATs performs on languages out of our pre-training corpus. We use eight low-resource languages from the Tatoeba dataset: Cantonese (yue), Vietnamese (vie), Tagalog (tgl), Irish (gle), Georgian (kat), Khmer (khm), Telugu (tel), Serbian (srp).\nTable 7 presents the results. We find that directly applying MLLMs on low-resource languages yields poor performance, with an average accuracy of approximately 25% \u223c 35%. In contrast, when conditioned on XLMRlarge, InteMATs significantly improves the performance to 62%. These findings confirm our hypothesis that pre-trained MLLMs exhibit poor cross-lingual alignment on low-resource languages due to their scarcity of training data during pre-training. InteMATs effectively enhances MLLMs by capturing better cross-lingual alignment information, enabling generalization to unseen low-resource languages. Appendix A.2 provides more details."
        },
        {
            "heading": "5.10 Ablation Study and Analysis",
            "text": "We conduct ablation studies on six tasks to unravel the individual impact of each component in\nInteMATs. Specifically, we compare the backbone MLLMs, MATs-ST, MATs-DT, and InteMATs. The ablation results in Table 8 show that MATs-ST generally outperforms MATs-DT on sentence-level tasks, while MATs-DT performs better than MATsST on document-level tasks. Both MATs-ST and MATs-DT outperform the backbone MLLMs, particularly with substantial gains on BUCC, Tatoeba, and TydiQA. By effectively incorporating these two modules, InteMATs achieves the best performance across the benchmarks."
        },
        {
            "heading": "5.10.1 Analysis of Pre-training Corpus",
            "text": "We further study whether our proposed different granularities of pre-training corpus boosts MLLMs\u2019 performance on cross-lingual understanding tasks. We compare with Initialized (adding a randomly initialized adapter without pre-training) and Mixed (adding an adapter trained on a non-distinguishing granularity of the pre-training corpus), as shown in Table 9.\nWe find that InteMATs consistently outperform Initialized across all tasks, with an average improvement of 2% on mBERT and 4.2% on XLMR backbones. Meanwhile, InteMATs surpass Mixed on all tasks except Tatoeba, achieving average gains of 2.1% on mBERT and 4.4% on XLMR backbones. These findings emphasize that InteMATs by pre-training individually on more diverse range of text granularities can precisely capture and integrate different text granularities of cross-lingual\nalignments, transferring this alignment knowledge to understanding tasks and yielding performance improvements."
        },
        {
            "heading": "5.10.2 Analysis of Pre-training Tasks",
            "text": "We evaluate the pre-training tasks of CL\u2019s advantage in capturing global cross-lingual alignment information. We compare with mBERT-FT, which is fully fine-tuned on the entire raw text DDT, and InteMATs-MLM, which uses the pre-training task of MLM instead of CL to train each adapters.\nTable 10 shows the results. We can observe that MLLMs with adapters can achieve performance enhancements on all tasks but Tatoeba. This suggests employing external adapters for training is more effective for incremental cross-lingual knowledge\u2019s learning, than full model fine-tuning. Meanwhile, InteMATs excel over other MLLMs on four tasks, showing average gains of 1.6% over InteMATsMLM, 2% over Initialized, and 3% over mBERTFT. These results underscore that applying the pre-training task of CL to train adapter can enhance global cross-lingual alignment information, enabling knowledge transferring to understanding tasks and boosting performance."
        },
        {
            "heading": "5.10.3 Analysis of Fusion Activation",
            "text": "InteMATs learns to fuse knowledge from MATs-ST and MATs-DT for different tasks. We retrieve the activation values of its fusion module and visualize the weight distributions for four tasks in Figure 2 (POS, RELX, XQuAD, and TydiQA), with increasing input text lengths.\nWe observe a consistent pattern in InteMATs, where it favors MATs-ST for sentence-level tasks and MATs-DT for document-level tasks. Specifically, on POS and RELX tasks, it relies more on the representations from MATs-ST, while on XQuAD and TydiQA, it relies more on MATs-DT. Moreover, as the network goes deeper, the degree of reliance on MATs-DT increases. This finding confirms with our intuition that granularity-specific adapters are specialized in handling texts of varying lengths. As a result, InteMATs can effectively\nleverage these adapters to enhance cross-lingual alignment regardless of the specific task at hand."
        },
        {
            "heading": "5.10.4 Layer-wise Representation Analysis",
            "text": "We examine InteMATs layer by layer to unravel on which layers it enhances cross-lingual transfer performance. Figure 3 compares InteMATs and XLMR in both base and large versions. We report the sentence retrieval accuracy on the Tatoeba dataset (Artetxe and Schwenk, 2019a) using representations from each transformer layer.\nWe find that InteMATs achieves similar performance to XLMR in the early layers. However, in later layers (layer 2 onwards for the base version and layer 9 onwards for the large version), InteMATs significantly outperforms XLMR. This comparison reveals that the cross-lingual transfer capability of InteMATs is gradually developed in the later layers, which provides improved cross-lingual alignment. This finding complies with previous research that later layers of Transformers tend to extract high-level features (Clark et al., 2019)."
        },
        {
            "heading": "5.11 Model Configuration",
            "text": "We compare the pre-training budget for enhancing the cross-lingual alignments against existing MLLMs in Table 11. The results reveal that InteMATs offers better parameter efficiency during pre-training and performance improvements on\ncross-lingual understanding tasks across various text granularities, requiring fewer computational parameters and smaller training corpus."
        },
        {
            "heading": "6 Conclusion",
            "text": "In this paper, we introduce InteMATs, a novel approach that integrates granularity-specific multilingual adapters, including sentence-level multilingual adapters (MATs-ST) and document-level multilingual adapters (MATs-DT), to enhance MLLMs in cross-lingual transfer tasks. On top of a fixed MLLM (e.g., mBERT and XLMR), we train these adapters on our curated parallel corpus using the contrastive learning objective. Our experiments demonstrate that InteMATs significantly enhanced the cross-lingual transfer performance of MLLMs across sentence-level and document-level tasks. Our comprehensive analyses show that InteMATs can automatically leverage corresponding adapters when dealing with different kinds of tasks.\nLimitations\nWe identify a few limitations of our current work.\n\u2022 First, InteMATs demonstrates limited improvements on structure prediction tasks, i.e., POS dataset. This is not surprising as syntactic structures are not universal across different languages. However, it is possible to share knowledge between languages from the same family, e,g., Romance languages (es, pt, it, fr, ro). We encourage future researchers to pay more attention to the syntactic cross-lingual alignment for MLLMs.\n\u2022 Second, the publicly available benchmarks for cross-lingual transfer evaluation are dominated by sentence-level tasks. As a result, performance comparisons on existing benchmarks could be inadequate to demonstrate a model\u2019s capability of handling longer contexts and transfer that ability to different languages.\nA more comprehensive cross-lingual transfer benchmark is needed.\nReproducibility Statement\nWe elaborate the experiment settings and hyperparameters in the Appendix A.1. We will publish our collected parallel datasets soon, as well as our code.\nEthics Statement\nAll procedures performed in this work were in accordance with ethical standards. We don\u2019t have any ethical concerns in this work."
        },
        {
            "heading": "Acknowledgements",
            "text": "We extend our gratitude to the anonymous reviewers for their valuable feedback and insights. This work was completed at NTU Singapore and was supported by the China Scholarship Council. Funding was also provided by the National Key R and D Program of China (Grant No.2017YFB1302400) and Shandong Province Agricultural Major Application Technology Innovation Project (Grant No.SD2019NJ014)."
        },
        {
            "heading": "A Appendix",
            "text": "A.1 Training Details Training Data Statistics. We present the details of the collected parallel training corpus in Table 12, which covers 42 diverse of languages. Especially, for DDT, we filter out summaries whose tokenized input sequence lengths are greater than 384. For both DST and DDT, we split the corpus into training and validation data in a 9 : 1 ratio.\nTraining Settings. Experiments are conducted on 4 Tesla V 100 GPUs. We set the maximum input sequence length to 384, with a batch size of 8 and an accumulation step of 2. The learning rate is set to 5e \u2212 5, and we use the Adam optimizer with a warm-up step of 1e4 during training and the random seed is set as 123. Since we adopt the random sampling method and contrastive learning, i.e., we randomly sample aligned text in different languages as positive samples, we only take several 12 hours for training MATs-ST, and 2.5 days at more for training MATs-DT. Both training epochs for MATs-ST and MATs-DT are set as 5 epochs.\nA.2 Scaling to Unseen Languages Since we adopt contrastive learning loss for multilingual adapters training, aiming to enhance crosslingual alignments by learning the universal features across languages. To assess the generalization to unseen languages of InteMATs, we collected two groups of parallel sample sets: sentence-level corpus and document-level corpus. Both different level of corpus involve two language set: InDomain set, including languages in pre-training corpus, and Un-Domain set, including languages out-of pre-training corpus 3.\nWe employ the cosine similarity to assess the cross-lingual alignments of individual adapters (MATs-ST and MATs-DT) and InteMATs on both sentence-level and document-level corpus, and mBERT base version is selected as backbone models. The results are shown in Figure 4. Compared to the vanilla mBERT, the mBERT with any multilingual adapters all consistently and significantly enhance the similarities of aligned embeddings across different text granularities in both the In-Domain set and Un-Domain set, while InteMATs demonstrate the highest performance. This indicates that\n3The In-Domain set primarily utilizes the validation set of the collected corpus A.1. We collect additional parallel data to form an Un-Domain set, which includes languages such as dv, gl, ha, ie, ka, ps, ro, sd, sr, tg, tt, wuu, yue.\nour adapters effectively facilitate mBERT in capturing global representation patterns across languages, thereby enabling substantial cross-lingual enhancements.\nA.3 Detailed Experimental Results We provide detailed results for each language on the cross-lingual language tasks. Specifically, we present the results for cross-lingual sentence-level retrieval benchmarks in Tables 13 (based on BUCC dataset), while the results for the cross-lingual relation extraction and classification benchmark are displayed in Tables 14 (on RELX dataset) and 15 (on PAWS-X dataset). The results for cross-lingual structure prediction are shown in Table 16. Meanwhile, we present the results for cross-lingual question answering tasks in Table 17 (on XQuAD dataset) and 18 (on TydiQA dataset)."
        }
    ],
    "title": "InteMATs: Integrating Granularity-Specific Multilingual Adapters for Cross-Lingual Transfer",
    "year": 2023
}