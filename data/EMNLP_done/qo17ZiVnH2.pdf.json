{
    "abstractText": "Large Language Models (LLMs) demonstrate impressive reasoning ability and the maintenance of world knowledge not only in natural language tasks, but also in some vision-language tasks such as open-domain knowledge-based visual question answering (OK-VQA). As images are invisible to LLMs, researchers convert images to text to engage LLMs into the visual question reasoning procedure. This leads to discrepancies between images and their textual representations presented to LLMs, which consequently impedes final reasoning performance. To fill the information gap and better leverage the reasoning capability, we design a framework that enables LLMs to proactively ask relevant questions to unveil more details in the image, along with filters for refining the generated information. We validate our idea on OK-VQA and A-OKVQA. Our method continuously boosts the performance of baselines methods by an average gain of 2.15% on OK-VQA, and achieves consistent improvements across different LLMs1.",
    "authors": [
        {
            "affiliations": [],
            "name": "Ziyue Wang"
        },
        {
            "affiliations": [],
            "name": "Chi Chen"
        },
        {
            "affiliations": [],
            "name": "Peng LiB"
        },
        {
            "affiliations": [],
            "name": "Yang LiuB"
        },
        {
            "affiliations": [],
            "name": "Yang Liu"
        }
    ],
    "id": "SP:75441ef24c31da726436143a5bc9e6bfcea2ae93",
    "references": [
        {
            "authors": [
                "Jean-Baptiste Alayrac",
                "Jeff Donahue",
                "Pauline Luc",
                "Antoine Miech",
                "Iain Barr",
                "Yana Hasson",
                "Karel Lenc",
                "Arthur Mensch",
                "Katherine Millican",
                "Malcolm Reynolds"
            ],
            "title": "Flamingo: a visual language model for few-shot learning",
            "venue": "Advances in Neural",
            "year": 2022
        },
        {
            "authors": [
                "Stanislaw Antol",
                "Aishwarya Agrawal",
                "Jiasen Lu",
                "Margaret Mitchell",
                "Dhruv Batra",
                "C. Lawrence Zitnick",
                "Devi Parikh."
            ],
            "title": "VQA: visual question answering",
            "venue": "2015 IEEE International Conference on Computer Vision, ICCV 2015, Santiago, Chile,",
            "year": 2015
        },
        {
            "authors": [
                "Nilavra Bhattacharya",
                "Qing Li",
                "Danna Gurari."
            ],
            "title": "Why does a visual question have different answers? In 2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019, Seoul, Korea (South), October 27 - November 2, 2019, pages 4270\u20134279",
            "venue": "IEEE.",
            "year": 2019
        },
        {
            "authors": [
                "Xi Chen",
                "Xiao Wang",
                "Soravit Changpinyo",
                "AJ Piergiovanni",
                "Piotr Padlewski",
                "Daniel Salz",
                "Sebastian Goodman",
                "Adam Grycner",
                "Basil Mustafa",
                "Lucas Beyer"
            ],
            "title": "PaLI: A jointly-scaled multilingual language-image model",
            "year": 2022
        },
        {
            "authors": [
                "Zhenfang Chen",
                "Qinhong Zhou",
                "Yikang Shen",
                "Yining Hong",
                "Hao Zhang",
                "Chuang Gan."
            ],
            "title": "See, Think, Confirm: Interactive prompting between vision and language models for knowledge-based visual reasoning",
            "venue": "ArXiv preprint, abs/2301.05226.",
            "year": 2023
        },
        {
            "authors": [
                "Lee",
                "Zongwei Zhou",
                "Xuezhi Wang",
                "Brennan Saeta",
                "Mark D\u00edaz",
                "Orhan Firat",
                "Michele Catasta",
                "Jason Wei",
                "Kathleen S. Meier-Hellstern",
                "Douglas Eck",
                "Jeff Dean",
                "Slav Petrov",
                "Noah Fiedel."
            ],
            "title": "PaLM: Scaling language modeling with pathways",
            "venue": "ArXiv preprint,",
            "year": 2022
        },
        {
            "authors": [
                "Wenliang Dai",
                "Junnan Li",
                "Dongxu Li",
                "Anthony Meng Huat Tiong",
                "Junqi Zhao",
                "Weisheng Wang",
                "Boyang Li",
                "Pascale Fung",
                "Steven Hoi."
            ],
            "title": "InstructBLIP: Towards general-purpose visionlanguage models with instruction tuning",
            "venue": "ArXiv",
            "year": 2023
        },
        {
            "authors": [
                "Vanhoucke",
                "Karol Hausman",
                "Marc Toussaint",
                "Klaus Greff",
                "Andy Zeng",
                "Igor Mordatch",
                "Pete Florence."
            ],
            "title": "PaLM-E: An embodied multimodal language model",
            "venue": "ArXiv preprint, abs/2303.03378.",
            "year": 2023
        },
        {
            "authors": [
                "Yifan Du",
                "Junyi Li",
                "Tianyi Tang",
                "Wayne Xin Zhao",
                "Ji-Rong Wen."
            ],
            "title": "Zero-shot visual question answering with language model feedback",
            "venue": "ArXiv preprint, abs/2305.17006.",
            "year": 2023
        },
        {
            "authors": [
                "Difei Gao",
                "Lei Ji",
                "Luowei Zhou",
                "Kevin Qinghong Lin",
                "Joya Chen",
                "Zihan Fan",
                "Mike Zheng Shou."
            ],
            "title": "AssistGPT: A general multi-modal assistant that can plan, execute, inspect, and learn",
            "venue": "ArXiv preprint, abs/2306.08640.",
            "year": 2023
        },
        {
            "authors": [
                "Feng Gao",
                "Qing Ping",
                "Govind Thattai",
                "Aishwarya N. Reganti",
                "Ying Nian Wu",
                "Prem Natarajan."
            ],
            "title": "Transform-Retrieve-Generate: Natural languagecentric outside-knowledge visual question answering",
            "venue": "IEEE/CVF Conference on Computer Vision and",
            "year": 2022
        },
        {
            "authors": [
                "Fran\u00e7ois Gard\u00e8res",
                "Maryam Ziaeefard",
                "Baptiste Abeloos",
                "Freddy Lecue."
            ],
            "title": "ConceptBert: Concept-aware representation for visual question answering",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 489\u2013498,",
            "year": 2020
        },
        {
            "authors": [
                "Yash Goyal",
                "Tejas Khot",
                "Douglas Summers-Stay",
                "Dhruv Batra",
                "Devi Parikh."
            ],
            "title": "Making the V in VQA matter: Elevating the role of image understanding in visual question answering",
            "venue": "Proceedings of the IEEE conference on computer vision and pattern",
            "year": 2017
        },
        {
            "authors": [
                "Liangke Gui",
                "Borui Wang",
                "Qiuyuan Huang",
                "Alexander Hauptmann",
                "Yonatan Bisk",
                "Jianfeng Gao"
            ],
            "title": "KAT: A knowledge augmented transformer",
            "year": 2022
        },
        {
            "authors": [
                "Jiaxian Guo",
                "Junnan Li",
                "Dongxu Li",
                "Anthony Meng Huat Tiong",
                "Boyang Li",
                "Dacheng Tao",
                "Steven Hoi."
            ],
            "title": "From images to textual prompts: Zero-shot visual question answering with frozen large language models",
            "venue": "Proceedings of the",
            "year": 2023
        },
        {
            "authors": [
                "Yushi Hu",
                "Hang Hua",
                "Zhengyuan Yang",
                "Weijia Shi",
                "Noah A Smith",
                "Jiebo Luo."
            ],
            "title": "PromptCap: Prompt-guided task-aware image captioning",
            "venue": "ArXiv preprint, abs/2211.09699.",
            "year": 2023
        },
        {
            "authors": [
                "Ziniu Hu",
                "Ahmet Iscen",
                "Chen Sun",
                "Kai-Wei Chang",
                "Yizhou Sun",
                "David A Ross",
                "Cordelia Schmid",
                "Alireza Fathi"
            ],
            "title": "2023b. AVIS: Autonomous visual information seeking with large language models",
            "year": 2023
        },
        {
            "authors": [
                "Ziniu Hu",
                "Ahmet Iscen",
                "Chen Sun",
                "Zirui Wang",
                "KaiWei Chang",
                "Yizhou Sun",
                "Cordelia Schmid",
                "David A Ross",
                "Alireza Fathi."
            ],
            "title": "REVEAL: Retrievalaugmented visual-language pre-training with multisource multimodal knowledge memory",
            "venue": "ArXiv",
            "year": 2022
        },
        {
            "authors": [
                "Ehsan Kamalloo",
                "Nouha Dziri",
                "Charles LA Clarke",
                "Davood Rafiei."
            ],
            "title": "Evaluating open-domain question answering in the era of large language models",
            "venue": "ArXiv preprint, abs/2305.06984.",
            "year": 2023
        },
        {
            "authors": [
                "Junlong Li",
                "Zhuosheng Zhang",
                "Hai Zhao."
            ],
            "title": "Self-prompting large language models for opendomain QA",
            "venue": "ArXiv preprint, abs/2212.08635.",
            "year": 2022
        },
        {
            "authors": [
                "Junnan Li",
                "Dongxu Li",
                "Silvio Savarese",
                "Steven Hoi."
            ],
            "title": "BLIP-2: Bootstrapping language-image pretraining with frozen image encoders and large language models",
            "venue": "ArXiv preprint, abs/2301.12597.",
            "year": 2023
        },
        {
            "authors": [
                "Tsung-Yi Lin",
                "Michael Maire",
                "Serge Belongie",
                "James Hays",
                "Pietro Perona",
                "Deva Ramanan",
                "Piotr Doll\u00e1r",
                "C Lawrence Zitnick."
            ],
            "title": "Microsoft COCO: Common objects in context",
            "venue": "Computer Vision\u2013 ECCV 2014: 13th European Conference, Zurich,",
            "year": 2014
        },
        {
            "authors": [
                "Weizhe Lin",
                "Bill Byrne."
            ],
            "title": "Retrieval augmented visual question answering with outside knowledge",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 11238\u201311254, Abu Dhabi, United Arab Emirates. As-",
            "year": 2022
        },
        {
            "authors": [
                "Yuanze Lin",
                "Yujia Xie",
                "Dongdong Chen",
                "Yichong Xu",
                "Chenguang Zhu",
                "Lu Yuan."
            ],
            "title": "REVIVE: Regional visual representation matters in knowledgebased visual question answering",
            "venue": "Advances in Neural Information Processing Systems.",
            "year": 2022
        },
        {
            "authors": [
                "Man Luo",
                "Yankai Zeng",
                "Pratyay Banerjee",
                "Chitta Baral."
            ],
            "title": "Weakly-supervised visual-retrieverreader for knowledge-based question answering",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages",
            "year": 2021
        },
        {
            "authors": [
                "Kenneth Marino",
                "Xinlei Chen",
                "Devi Parikh",
                "Abhinav Gupta",
                "Marcus Rohrbach."
            ],
            "title": "KRISP: integrating implicit and symbolic knowledge for opendomain knowledge-based VQA",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition, CVPR",
            "year": 2021
        },
        {
            "authors": [
                "Kenneth Marino",
                "Mohammad Rastegari",
                "Ali Farhadi",
                "Roozbeh Mottaghi."
            ],
            "title": "OK-VQA: A visual question answering benchmark requiring external knowledge",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach,",
            "year": 2019
        },
        {
            "authors": [
                "Long Ouyang",
                "Jeffrey Wu",
                "Xu Jiang",
                "Diogo Almeida",
                "Carroll Wainwright",
                "Pamela Mishkin",
                "Chong Zhang",
                "Sandhini Agarwal",
                "Katarina Slama",
                "Alex Ray"
            ],
            "title": "Training language models to follow instructions with human",
            "year": 2022
        },
        {
            "authors": [
                "Vivek S. Pai",
                "Peter Druschel",
                "Willy Zwaenepoel."
            ],
            "title": "IO-Lite: A unified I/O buffering and caching system",
            "venue": "Acm Transactions on Computer Systems, 18(1):37\u201366.",
            "year": 2000
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark",
                "Gretchen Krueger",
                "Ilya Sutskever"
            ],
            "title": "Learning transferable visual models from natural language",
            "year": 2021
        },
        {
            "authors": [
                "Sahithya Ravi",
                "Aditya Chinchure",
                "Leonid Sigal",
                "Renjie Liao",
                "Vered Shwartz."
            ],
            "title": "VLC-BERT: Visual question answering with contextualized commonsense knowledge",
            "venue": "Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vi-",
            "year": 2023
        },
        {
            "authors": [
                "Dustin Schwenk",
                "Apoorv Khandelwal",
                "Christopher Clark",
                "Kenneth Marino",
                "Roozbeh Mottaghi."
            ],
            "title": "A-OKVQA: a benchmark for visual question answering using world knowledge",
            "venue": "Computer Vision\u2013 ECCV 2022: 17th European Conference, Tel Aviv,",
            "year": 2022
        },
        {
            "authors": [
                "Ramprasaath R. Selvaraju",
                "Purva Tendulkar",
                "Devi Parikh",
                "Eric Horvitz",
                "Marco T\u00falio Ribeiro",
                "Besmira Nushi",
                "Ece Kamar"
            ],
            "title": "SQuINTing at VQA models: Introspecting VQA models with",
            "year": 2020
        },
        {
            "authors": [
                "Zhenwei Shao",
                "Zhou Yu",
                "Meng Wang",
                "Jun Yu."
            ],
            "title": "Prompting large language models with answer heuristics for knowledge-based visual question answering",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14974\u2013",
            "year": 2023
        },
        {
            "authors": [
                "Alexandre Tamborrino",
                "Nicola Pellican\u00f2",
                "Baptiste Pannier",
                "Pascal Voitot",
                "Louise Naudin."
            ],
            "title": "Pretraining is (almost) all you need: An application to commonsense reasoning",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational",
            "year": 2020
        },
        {
            "authors": [
                "Kohei Uehara",
                "Nan Duan",
                "Tatsuya Harada."
            ],
            "title": "Learning to ask informative sub-questions for visual question answering",
            "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, CVPR Workshops 2022, New Orleans, LA, USA, June",
            "year": 2022
        },
        {
            "authors": [
                "Peng Wang",
                "An Yang",
                "Rui Men",
                "Junyang Lin",
                "Shuai Bai",
                "Zhikang Li",
                "Jianxin Ma",
                "Chang Zhou",
                "Jingren Zhou",
                "Hongxia Yang."
            ],
            "title": "OFA: unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework",
            "venue": "Inter-",
            "year": 2022
        },
        {
            "authors": [
                "Jason Wei",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Maarten Bosma",
                "Fei Xia",
                "Ed H Chi",
                "Quoc V Le",
                "Denny Zhou"
            ],
            "title": "Chain-of-Thought prompting elicits reasoning in large language models",
            "venue": "In Advances in Neural Information Processing Systems",
            "year": 2022
        },
        {
            "authors": [
                "Jialian Wu",
                "Jianfeng Wang",
                "Zhengyuan Yang",
                "Zhe Gan",
                "Zicheng Liu",
                "Junsong Yuan",
                "Lijuan Wang."
            ],
            "title": "GRiT: A generative region-to-text transformer for object understanding",
            "venue": "arXiv preprint arXiv:2212.00280.",
            "year": 2022
        },
        {
            "authors": [
                "Jialin Wu",
                "Jiasen Lu",
                "Ashish Sabharwal",
                "Roozbeh Mottaghi."
            ],
            "title": "Multi-modal answer validation for knowledge-based VQA",
            "venue": "Thirty-Sixth AAAI Conference on Artificial Intelligence, AAAI 2022, Thirty-Fourth Conference on Innovative Applications",
            "year": 2022
        },
        {
            "authors": [
                "Jialin Wu",
                "Raymond Mooney."
            ],
            "title": "Entity-focused dense passage retrieval for outside-knowledge visual question answering",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 8061\u20138072, Abu Dhabi, United",
            "year": 2022
        },
        {
            "authors": [
                "Zhengyuan Yang",
                "Zhe Gan",
                "Jianfeng Wang",
                "Xiaowei Hu",
                "Yumao Lu",
                "Zicheng Liu",
                "Lijuan Wang."
            ],
            "title": "An empirical study of GPT-3 for few-shot knowledgebased VQA",
            "venue": "Thirty-Sixth AAAI Conference on Artificial Intelligence, AAAI 2022, Thirty-Fourth Con-",
            "year": 2022
        },
        {
            "authors": [
                "Pengchuan Zhang",
                "Xiujun Li",
                "Xiaowei Hu",
                "Jianwei Yang",
                "Lei Zhang",
                "Lijuan Wang",
                "Yejin Choi",
                "Jianfeng Gao."
            ],
            "title": "VinVL: Revisiting visual representations in vision-language models",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition, CVPR",
            "year": 2021
        },
        {
            "authors": [
                "Deyao Zhu",
                "Jun Chen",
                "Kilichbek Haydarov",
                "Xiaoqian Shen",
                "Wenxuan Zhang",
                "Mohamed Elhoseiny."
            ],
            "title": "ChatGPT asks, BLIP-2 answers: Automatic questioning towards enriched visual descriptions",
            "venue": "arXiv preprint arXiv:2303.06594.",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Large-scale language models (LLMs) have exhibited impressive capabilities in terms of their world knowledge and reasoning abilities, leading to remarkable achievements in various Natural Language Processing (NLP) tasks such as commonsense reasoning (Tamborrino et al., 2020; Wei et al., 2022) and open-domain question answering (Li et al., 2022; Kamalloo et al., 2023). Building upon the success in the realm of text, recent research has explored the utilization of pre-trained LLMs in Vision-Language (VL) tasks. These studies have shown promising performance, especially for knowledge-intensive tasks such as knowledgebased Visual Question Answering (VQA) (Marino\nBCorresponding authors: Peng Li and Yang Liu. 1Code will be released at https://github.com/\nTHUNLP-MT/FIIG.\net al., 2019; Schwenk et al., 2022), where both image understanding and external knowledge are imperative for answering open-ended questions.\nThe key challenge of leveraging LLMs in VL tasks is to bridge the gap between images and text, i.e., enabling LLMs to understand images. To address this challenge, two approaches have been investigated. The first approach extends the LLM with visual perception modules to form a VisionLanguage Pre-training (VLP) model (Alayrac et al., 2022; Chen et al., 2022; Li et al., 2023). Despite the high performance, this approach requires training on large-scale image-text pairs, which can be com-\nputationally expensive, particularly for LLMs with massive parameters such as GPT-3 (Brown et al., 2020) and PaLM (Chowdhery et al., 2022). The second approach transforms images into textual representations, which are then used as prompts for the LLMs (Yang et al., 2022; Hu et al., 2023a; Chen et al., 2023; Guo et al., 2023). This training-free approach is more cost-effective and enables LLMs to be utilized in a more flexible paradigm, allowing for easy adaptation to different tasks.\nHowever, as discussed in the works of Yang et al. (2022) and Hu et al. (2023a), general image captions may lack the subtle information required to answer visual questions. To resolve this, Yang et al. (2022) compromise captions with image tags, and Hu et al. (2023a) propose incorporating questions into the caption generation process. Despite their successes, it remains impractical to reveal every subtle detail necessary to answer visual questions in a single concise description. As illustrated in Figure 1, the captions fail to spot the \u201carea being stepped over\u201d as the \u201cwater\u201d, resulting in hallucinated answers. Two primary concerns exist regarding existing image-to-text conversion approaches for VQA: (1) The converted textual descriptions might be insufficient to solve the visual questions, or could contain misleading content; (2) Existing methods convert images to text as a preprocessing step of the input. This one-off conversion is a lossy compression of the conveyed information, and does fully provoke the reasoning ability of LLMs.\nIn this paper, we present a framework where LLMs proactively interact with Vision-Language Models (VLMs) to gather specific information of interest, as depicted in Figure 1. This interaction is aimed at automatically seeking and regaining details that may be omitted during the image-totext conversion. To enhance the informativeness of the generated questions and the correctness of their corresponding answers, we design a refinement module to summarize only the useful information for generating the final answers. We validate our approach on OK-VQA and A-OKVQA datasets and conduct experiments across different LLMs. Our contributions are as follows:\n\u2022 We design a model agnostic framework that allows LLMs to proactively interact with VLMs to unveil missing information.\n\u2022 Our method can rectify inaccurate information generated during the image-to-text transformation process and minimize the ambiguity of\nthe converted textual information.\n\u2022 We achieve an average gain of 2.15% on OKVQA over baselines, and attain consistent improvements across different LLMs."
        },
        {
            "heading": "2 Related Work",
            "text": ""
        },
        {
            "heading": "2.1 In-Context Learning for OK-VQA",
            "text": "In-context learning refers to the ability of LLMs to adapt to new tasks given only a few examples without parameter tuning (Brown et al., 2020). To exploit the knowledge possessed by LLMs and extend this ability to OK-VQA, PICa (Yang et al., 2022) converts images into captions and tags, and applies in-context learning with GPT-3. A following work, PromptCap (Hu et al., 2023a), fine-tunes a question-guided captioning model, urging the target messages to appear in the captions. Instead of describing an image by a single caption, Chen et al. (2023) represent an image as the combination of its regions in a Chain-of-Thought (CoT) style (Wei et al., 2022). Prophet (Shao et al., 2023) argues that indistinct captions lead to aimless predicting, and proposes to provide answer candidates with corresponding confident scores as references. These methods select in-context examples according to the similarities between training and test instances.\nHowever, there are unexpected information loss during the conversion from images to text. These methods conduct a compressive one-time conversion to turn images into text, while we prompt the LLM to iteratively ask for detailed information. Our method is orthogonal to these approaches and can continually improve their performances."
        },
        {
            "heading": "2.2 New Question Generation for VQA",
            "text": "In VQA tasks, some questions are ambiguous and might have different answers (Bhattacharya et al., 2019). Uehara et al. (2022) propose to generate new questions to assist the reasoning of the original questions. They train a visual question generator with supervision from human annotated dataset (Selvaraju et al., 2020). It is evaluated on VQA dataset (Antol et al., 2015), and is not extensible to open-domain knowledge-based VQA. Img2prompt (Guo et al., 2023) describes a zeroshot approach on OK-VQA by generating an extra new question for each instances to imitate the few-shot setting without actually using multiple in-context examples. Its question generation procedure is irrelevant to the images. Instead of depending on human annotations, given the question and\nimage information, we directly prompt LLMs to generate new questions targeting the missing information. This is not constrained on pre-annotated datasets and allows us to uncover more potential information and knowledge."
        },
        {
            "heading": "2.3 Incorporating LLMs and VLMs for Vision-Language Tasks",
            "text": "Some concurrent works have also incorporated LLMs and VLMs for VL tasks. ChatCaptioner (Zhu et al., 2023), for instance, aims to capture richer details from images by progressively generating questions with an LLM and then answering these questions using a VLM. The resulting answers are summarized to form image descriptions. However, this approach places a stronger emphasis on the quantity of detected image details rather than their correctness. In some VL tasks, such as VQA, this inevitably introduces noise, leading to inaccurate image descriptions that may result in incorrect model predictions. AVIS (Hu et al., 2023b) also explores the decomposition of visual questions, but its primary focus lies in the planning procedures for tool-use and their corresponding executions."
        },
        {
            "heading": "3 Method",
            "text": "Our overall framework, as illustrated in Figure 2, comprises three key modules: inquiry, refinement and answering. Given the image caption and the\nquestion, the inquiry module prompts an LLM to generate new questions that seek for missing image information required to answer the original question, and uses a VLM to answer them based on the image (\u00a73.1). Then, we adopt a refinement module to summarize information from the generated questions and answers, filtering and extracting the relevant and useful information from them (\u00a73.2). Finally, the answering module prompts the LLM to predict the final answer to the original question with the augmented image information (\u00a73.3).\nBefore delving into the details of our method, we shall declare some important notations. We use I , c, q to denote the image, its caption, and the original question, respectively. The caption c is generated by the image captioning modelMc:\nc =Mc(I), (1)\nwhich is used as the preliminary image information presented to LLMs for question generation. The generated new questions are denoted as q\u2032 = [q\u20321; q \u2032 2; . . . ; q \u2032 K ], with the corresponding answers as a\u2032 = [a\u20321; a \u2032 2; . . . ; a \u2032 K ], where K represents the total number of questions."
        },
        {
            "heading": "3.1 Prompting LLM to Proactively Ask for Information",
            "text": "We leverage the reasoning capabilities of the LLM to identify the essential image information that may\nbe lost during the image-to-text conversion process. Specifically, given the image caption c and the original question q, we first prompt the LLM to generate K new questions q\u2032 that inquire about the additional image information necessary to answer q. Suppose that the k-th question of q\u2032 has L tokens, denoted as q\u2032k = (y 1 k, y 2 k, . . . , y L k ), the decoding process can be formulated as:\nylk = argmax y\u0302lk pLLM\n( y\u0302lk \u2223\u2223\u2223y<lk ;pq, q, c) , (2) where pq is the instruction prompt. The outline of the prompt pq for LLM is as follows:\n/* Instruction for the decomposition task */ Please decompose the TARGET-QUESTION into K sub questions: /* n in-context examples */ TARGET-QUESTION: q1 \\n Catpion: c1 Sub questions: 1. q\u2032 1 1, 2. q \u20321 2, ... ...... TARGET-QUESTION: q \\n Caption: c Sub questions:\nThen, we employ a visual question answering modelMa to answer these new questions based on the original image as:\na\u2032k =Ma(q\u2032k, I), (3) where a\u2032k refers to the answer to the k-th question.\nTo better understand the role of the generated questions and answers, we conducted a preliminary experimental analysis. Specifically, we concatenate each question q\u2032k to its answer a \u2032 k to form a QA pair q\u2032ka \u2032 k = [q \u2032 k; a \u2032 k], and prompt the LLM to answer OK-VQA questions given this representation. We investigate the results of prompting LLM with different contexts: 1) the original question q only; 2) all the QA pairs; 3) one randomly selected QA pair and 4) the best-performing QA pair. For the last case, for each question we calculate the accuracy scores when prompting with each QA pair, and then select the maximum score among them, which represents an upper bound on the performance. These accuracy scores are calculated by the soft scores following Goyal et al. (2017).\nFrom the results presented in Table 1, we can draw two key conclusions. First, the generated questions and answers indeed contain information that helps to answer the original question, comparing the results of Best and Original. Second, the generated QA pairs are noisy, as neither using all QA pairs nor randomly selecting one improves the performance. This highlights the necessity of an information refinement process to filter out irrelevant or misleading information in the generated pairs."
        },
        {
            "heading": "3.2 Refining the Generated Information",
            "text": "Inspired by the preliminary experiments in \u00a73.1, we design a refinement module that can extract useful information for the LLM to answer the original question from the noisy generated QA pairs. Our refinement module includes a summarization stage and a filtering stage.\nFirstly, we summarize q\u2032k and the corresponding answer a\u2032k into a narrative description s \u2032 k. Denoting s\u2032k as a L-token target sequence, we have:\ns\u2032 l k = argmax\ns\u0302\u2032 l k\npLLM\n( s\u0302\u2032 l\nk \u2223\u2223\u2223s\u2032<lk ;ps, q\u2032k, a\u2032k) , (4) where l is the l-th token of the summary s\u2032k and ps is the instruction prompt. The complete templates used in this section are listed in Appendix F.\nThen, we apply a filter to assess the helpfulness of summary s\u2032k to the final prediction. The output is a contribution score of s\u2032k given the image I , the question q, and different combinations of text inputs including question q\u2032k, answer a \u2032 k, summary s\u2032k and question-answer pair [q \u2032 k; a \u2032 k].\nSpecifically, our refinement module consists of two types of encoders, text encoder Enct and image encoder Encv, and a 3-layer multilayer perceptron (MLP). We use the pre-trained weights of CLIP (Radford et al., 2021) to initialize our text and image encoders. Given I , q and s\u2032k, we first generate the visual features hvisual and the textual features hktext as:\nhvisual = Encv(I), (5)\nht = Enct(t), t = {q, q\u2032k, a\u2032k, q\u2032ka\u2032k, s\u2032k}, (6)\nhktext = Avg(ht={q\u2032k,a \u2032 k,q \u2032 ka \u2032 k,s \u2032 k},ht=q), (7)\nwhere hktext is the average of the features of each kind of textual inputs. Then we calculate the fused features of the image I and the summary s\u2032k as:\nzk = MLP([hktext;hvisual]). (8)\nMethod LLM Image Information OK-VQA A-OKVQA Results with accessible LLMs PICa davinci Captions (G) + Tags 48.0* - PICa\u2020\ntext-davinci-002 Captions (G) + Tags 49.67 49.18\n+ Ours Captions (G) + Tags + refined details 53.76 +4.09 51.13 +1.95 PromptCap\u2020\ntext-davinci-002 Captions (Q) 53.50 52.99\n+ Ours Captions (Q) + refined details 54.42 +0.92 53.21 +0.22 Prophet\ntext-davinci-002 Caption (G) + Candidates 57.91 58.2*\n+ Ours Caption (G) + Candidates + refined details 59.34 +1.43 59.79* +1.59 Results with unavailable LLMs\nPromptCap code-davinci-002 Captions (Q) 60.4 56.3\nTable 2: Direct answer accuracy on OK-VQA (test) and A-OKVQA (val). We use \u2020 to denote our reproduced versions, because PromptCap and PICa use different GPT-3 engines in their paper. PICa uses davinci; PromptCap uses code-davinci-002, which is deprecated. * refers to multi-query ensemble because the single query results of Prophet is not reported by Shao et al. (2023). The Captions (G) refers to the caption generated by general-purpose VLP, such as VinVL. The Captions (Q) refers to the PromptCap caption. Candidates refers to answers candidates predicted by VQA model. Refined details refer to our regained information.\nTo optimize the filter, we directly use the groundtruth VQA accuracy yzk of each training instance (I, q, s\u2032k, yzk) in OK-VQA as an intermediate supervision signal. The final loss is formulated as:\nL = \u2212[yzk log(pzk)+(1\u2212yzk) log(1\u2212pzk)], (9)\nwhere pzk = \u03c3(z k) is the contribution score of s\u2032k for answering q given I . Please refer to Appendix A for detailed data construction process.\nDuring inference, we exploit the refinement module to generate the refined information S. We first calculate the contribution score of the original question q as pzq with the trained filter. Then we select all the summaries s\u2032k that have larger contribution scores pzk than pzq to form the refined information set S as:\nS = {pzk |pzk \u2a7e pzq}k=1,2,3,...,K . (10)"
        },
        {
            "heading": "3.3 Answer Reasoning",
            "text": "The answering module prompts a frozen LLM to predict the final answer with both the converted image information such as the caption c and our regained S. For the prompt template, we mostly follow PICa (Yang et al., 2022) as the other methods do, but add a new section for our refined image information as follows (denoted as Template-Few-shot):\n/* Template-Few-shot */ Image information: Sn Caption: Cn\\n Question: qn\\n Answer: an\nwhere an is the ground-truth answer of question qn and n refers to the number of shots used for in-context learning. We denote the template for the test data as Template-Query, which is as follows:\n/* Template-Query */ Image information: S Caption: C\\n Question: q\\n Answer:\nPlease refer to Appendix F for the complete prompt templates with instructions."
        },
        {
            "heading": "4 Experiments",
            "text": "We validate our method on open-domain knowledge-base VQA, and conduct experiments on OK-VQA and A-OKVQA. Implementation details are described in the following sections."
        },
        {
            "heading": "4.1 Dataset",
            "text": "OK-VQA (Marino et al., 2019) is an open-domain knowledge-based VQA dataset with about 9k training and 5k testing questions. The images come from MSCOCO (Lin et al., 2014). The annotated questions are all open-ended, and each of them is associated with 10 groundtruth answers. Due to the deprecation of dataset v1.0, we use v1.1 in this paper. A-OKVQA (Schwenk et al., 2022) augments OK-VQA by the scale, tasks and extra rationales. This dataset has are three splits, training (17k), validation (1.4K) and test (6.7k). A-OKVQA includes two tasks, direct answer (DA) and multiple choices (MC). The DA tasks is the same as OK-VQA, which requires to answer open-ended\nquestions with external knowledge. While the MC task asks to choose an answer from a close set with 4 choices. We focus on the open-domain setting and evaluate our method OK-VQA and the DA task of A-OKVQA. Both datasets employ the soft accuracy (Antol et al., 2015) as the evaluation metric."
        },
        {
            "heading": "4.2 Experimental Settings",
            "text": "We generate 3 new questions for each q, and apply the ensemble filters to refine the generated information. The ensemble filters contains all 4 types of inputs, q\u2032k, a \u2032 k, s \u2032 k and [q \u2032 k; a \u2032 k]. Baseline methods. We apply our method upon existing approaches of the same in-context learning paradigm, including PICa (Yang et al., 2022), PromptCap (Hu et al., 2023a) and Prophet (Shao et al., 2023). The image information of PICa comes from captions and tags (Microsoft Azure tagging API2). PromptCap follows similar settings, but replaces the captioning model by its finetuned question-aware caption model. Apart from the captions, Prophet supplies LLM with extra answer candidates obtained from their fine-tuned VQA models.\nThe best results of PICa and Prophet is reported on the 16-shot and 20-shot settings, respectively. They employ multi-query ensemble to further enhance the performance, where they prompt the LLM for 5 times with different in-context examples. We implement our method upon these baselines following the same settings, where the number of shot is 16 for PICa+Ours and PromptCap+Ours, and 20 for Prophet+Ours.\nLLMs. For answer reasoning, the three baselines employ different LLMs. Prophet employs the LLM engine text-davinci-002, which is an InstructGPT model (Ouyang et al., 2022). PICa uses davinci, a GPT-3 model (Brown et al., 2020). PromptCap uses code-davinci-002, but is now deprecated and the authors suggest to replace it by text-davinci-002 in the published code and model3. Considering the accessibility of LLMs and for fair comparisons, we employ the LLM engine used in Prophet (text-davinci-002) and reproduce the other two using the same engine. The gpt-3.5-turbo-0301 engine is employed for question generation and summarization.\n2Azure Computer Vision API v3.2 for tagging: https://westus.dev.cognitive.microsoft.com/docs/ services/computer-vision-v3-2\n3https://github.com/Yushi-Hu/PromptCap\nVLMs. We use BLIP-2 (Li et al., 2023) to predict answer a\u2032k for q \u2032 k. The choice of VLMs for obtaining caption C varies alone baselines. Specifically, PICa and Prophet uses VinVL (Zhang et al., 2021) to convert images into captions, and PromptCap fine-tunes a question-aware caption model based on OFA (Wang et al., 2022)."
        },
        {
            "heading": "4.3 Results",
            "text": "Table 2 shows the results on OK-VQA and AOKVQA. With the accessible LLMs, we achieve the highest accuracy of 59.34% under single query setting, and enhance the accuracy of baselines by 2.15% on average. PromptCap achieves an accuracy of 60.45% on OK-VQA with code-davinci-002 engine, which is currently inaccessible. While for the reproduction with text-davinci-002, we improve the performance of PromptCap by 0.9%. For A-OKVQA, we consistently observe improvements, and the average increment is1.25%.\nTable 3 lists the results of previous methods on OK-VQA and A-OKVQA, where we report our ensemble results in accordance to Prophet. Within the realm of methods using LLMs, we further improve the best result by 0.2% for both dataset. Specifi-\ncally, we achieve +1.59% on A-OKVQA compared to Prophet. For A-OKVQA, PromptCap achieves the previous highest accuracy, 59.6%. But we cannot directly apply our method upon it because its employed LLM engine is inaccessible now. For multimodal tuning setting, PaLM-E (Driess et al., 2023) and InstructBLIP (Dai et al., 2023) present the state of the art on OK-VQA and A-OKVQA, respectively. While PaLM-E is trained upon a 540B language model, PaLM (Chowdhery et al., 2022), it is over threefold of the largest LLM we use (175B). And InstructBLIP employs instruction tuning."
        },
        {
            "heading": "4.4 Ablation Study",
            "text": "We conduct the following ablation studies: 1) to verify that the gain of our method comes from properly integrating more image information; 2) to compare the effectiveness of different selection schemes in refinement module; 3) to investigate the impact of scaling up the number of generated\nquestions; 4) to analyse the impact of multi-query ensemble; and 5) to examine the consistency of our method across different LLMs.\nAppropriately integrated information helps to boost the performance. We summarise the paradigm to solve VQA task into two categories. A VLM paradigm that directly uses a VLM (e.g., BLIP-2 in Line (i) in Table 4), and an LLM-based paradigm where LLMs collaborate with VLMs to answer visual questions. Following the LLM-based paradigm, we progressively integrate new information and apply refining methods from Line (h) to (a) in Table 4, and observe continual enhancements.\nLine (h) represents results for 4-shot setting following the PICa template. While Line (g) implements our template, Template-Few-shot with n = 4. The BLIP-2 answers are included in \u201cImage information\u201d in Template-Few-shot and Template-Query. The participation of BLIP-2 raises the accuracy by 0.31%. When we feed all generated information (Line (f)), the accuracy only marginally improves by 0.07%. Notably, applying the refinement module to the query instance (Line (e)) results in a significant 2.38% accuracy boost. Line (d) introduces image tags and contributes an additional 0.8% improvement compared to Line (e). Extending to 16-shot setting further boosts the result by 2.06%. While the previous lines only apply information refining to the query instance, we go further in Line (a) by refining the in-context examples. This contributes to an increment of 2.17% compared to Line (b), and is more influential than adding tags, increasing the number of shots and replacing the caption type.\nIn conclusive, the performance benefits from adding more image information. Our method to seek and refine image information makes a substantial contribution in this regard.\nEffectiveness of filtering schemes. We evaluate the performance of different selection schemes in the refinement module, including (1) Ensemble: selecting according to ensemble filters as proposed in \u00a73.2, (2) Single: selecting according to single filter, including filters trained using answers, question, summaries, and question-answer pairs (qa), and (3) All: selecting all generated questions q\u2032 and the original question q without filtering.\nAs shown in Table 5 (a), filters help improve the performance against directly selecting all the questions without filtering. However, single filters make only minor contributions to the final accuracy because each of them addresses only a single aspect of the criteria.\nScaling up the number of generated questions. Intuitively, scaling the amount of generated questions will contribute to extra gain. To verify this, we doubled the number of generated questions to 6. By applying the same selection strategy under 20-shot single query setting, we obtain a final accuracy of 59.68% on OK-VQA, which is slightly higher (+0.34%) than generating only 3 questions.\nImpact of multi-query ensemble. PICa and Prophet exhibit improvements of multi-query ensemble by prompting LLM for 5 times with different in-context examples. We investigate the influence of multi-query ensemble on our method. As shown in Table 5 (b), although the accuracy of our method increases along with the number of ensemble queries, the gap between ours and Prophet\u2019s are narrowed. As the in-context examples are arranged according to the relevance to test instance, the more examples we use, the less coherent to the test instance they will be. Thereby, noises could be introduced with the progressive increase of ensemble queries. Similarly, Chen et al. (2023) also observe a decline in performance when continuously increase the number of ensemble queries.\nConsistency of our method across LLMs. Different LLMs largely affect the results (Shao et al., 2023; Hu et al., 2023a). We investi-\ngate if our method is generalizable across LLMs trained in different ways and with different scales, including LLaMA-7B, LLaMA-13B, and text-davinci-002 (175B). Table 6 proves the effectiveness of our method on both InstructGPT (text-davinci-002 engine) and LLaMA. Results also demonstrate the robustness of our method on the scales of LLM, ranging from 7B to 175B. We notice that our method introduce less improvement on PromptCap compared to Prophet and PICa. We suppose that the captions of PromptCap are derived from the question, and they seem to be more determinate as shown in Figure 3, which easily dominates the reasoning and impair the attendance of other information."
        },
        {
            "heading": "5 Case Study",
            "text": "We conduct comprehensive analysis of the cases, and observe that our method exhibits the following capabilities: 1) to unveil a better level of detail as stated in the \u00a73.1; 2) to rectify inaccurate caption information; and 3) to minimize the ambiguity of provided information, e.g., captions and candidates. Cases are demonstrated in Figure 3. We compared the results of baselines, PtomptCap and Prophet, with and without applying our method. Unveiling Missing Details. Our generated questions bridge the information gap between the question and the image caption by revealing missing details necessary to answer the question. An shown in (2) of Figure 3, the question asks about the \u201ckind of glass\u201d, but relevant features are not included in the PromptCap caption. The absence of detail leads to an improper prediction, \u201cfrosted\u201d. However, the two questions in our method pinpoint the detailed features, \u201ctransparent\u201d and \u201cclear\u201d, and contribute to a correct prediction. These imply the effectiveness of our generated questions. Rectifying Inaccurate Information. Our method can rectify the misleading information provided in the captions. In the first case shown in Figure 3, we correct the wrong message given by PromptCap that it is not a \u201cchicken\u201d sandwich by the question \u201cwhat is in the sandwich\u201d with answer \u201cham\u201d. Minimizing the Ambiguity. By engaging more image information, our method provides evidence to support the correct candidates in Prophet and the proper captions of PromptCap, thereby enhancing the confidence and the reliability of accurately provided information in these baselines. In Figure 3, the candidates of Prophet in (1) properly\nfit the image. However, the LLM does not follow the given confidence and selects the least confident one. In contrast, Figure 3 (2) demonstrates a situation where the most confident candidate is not the correct answer. In this two scenarios, our method supports the correct answer with more detailed information."
        },
        {
            "heading": "6 Conclusion",
            "text": "In this paper, we focus on open-domain knowledgebased VQA and propose a model agnostic framework that successfully unveils missing detail during the image-to-text transformation. Our method acquires the ability to rectify inaccurate information generated by captioning models, and the ability to minimize the ambiguity of the converted textual information for further improvements. Our method can be applied upon existing baselines, and achieves average gains of 2.15% on OK-VQA and 1.25% on A-OKVQA over the baselines. Ablation studies show that our method attain consistent improvements across different LLMs.\nLimitations\nIn this work, we demonstrate the effectiveness of our framework on OK-VQA and A-OKVQA, and show a consistent improvement across different LLMs. However, we do not verify the feasibility of our idea on other vision-language tasks that also require knowledge, such as visual commonsense reasoning. Intuitively, the paradigm to prompt LLMs to uncover missing image details can be applied to a wild range of VL tasks. While the questions in our framework are generated independently, further challenges include to progressively ask informative question upon the previous questions and acquire the accurate answers. We hope that our work will encourage further investigations that explore the capabilities of LLMs in the VL realm."
        },
        {
            "heading": "Acknowledgement",
            "text": "This work is supported by the National Key R&D Program of China (2022ZD0160502) and the National Natural Science Foundation of China (No. 61925601, 62276152). We thank Siyu Wang for her participation in this work, and appreciate all the reviewers for their insightful suggestions."
        },
        {
            "heading": "A Details for Gathering Training Data for Refinement Module",
            "text": "The process for constructing the training data for refinement module as described in \u00a73.2. We denote our generated image information as a set I , containing the four types of generated image information, q\u2032, a\u2032, q\u2032a\u2032 and s\u2032. Q is the set of all the original visual questions, and I refers the corresponding images. yz is resulting training labels used in \u00a73.2, where yzk refers to the label for a single generated information. The soft accuracy score Accsoft(a) is computed following Goyal et al. (2017).\nAlgorithm 1 Pipeline for Supervision Gathering Input: Image, Question and Generated Image Information {I,Q,I} Output: Image Information (I), Original Questions (Q) and Images (I) with labels (yz) Require: Pi,q is the prompt for answer reasoning regarding question q and image i.\n1: procedure GET LABELS(yz) 2: for q \u2208 Q and i \u2208 I and I \u2208 I do 3: a\u2190 LLMreason(Pi,q(I)) 4: accq \u2190 Accsoft(a) 5: if accq > 0 then 6: yz \u2190 1 7: else 8: yz \u2190 0"
        },
        {
            "heading": "B The Declaration of Trainable Modules in Our Pipeline",
            "text": "These models are frozen in our entire pipeline: the captioning modelMc, the VQA modelMa and the LLMs (for question generation, summarization and reasoning).\nThe refinement module in Figure 2 requires training, which includes 4 parts:\n\u2022 An image encoder (for image): to encode the original VQA image;\n\u2022 A question encoder (for text): to encodes the original VQA question;\n\u2022 An information encoder (for text): to encode our generated image information (denoted as \u201cInfo encoder\u201d in Figure 1);\n\u2022 A filter: an MLP-based network that evaluates the helpfulness of the obtained image information towards answering the visual questions."
        },
        {
            "heading": "C Full List of Results",
            "text": "We provide results of previous methods on OKVQA and A-OKVQA in Table 7 and Table 8. The current state-of-the-art on OK-VQA is 66.1 and is contributed by a 562B pre-trained multimodal model, including a 540B language model and a 22B vision encoder. We achieve the highest score in both methods querying external KBs and methods with LLMs. InstructBLIP (Dai et al., 2023) achieves the SOTA of A-OKVQA with multimodal instruction tuning."
        },
        {
            "heading": "D Ensemble with different numbers of shots",
            "text": "The results of ensemble queries are listed in Table 9. We notice that our method improved the result of Prophet by 1.43% in 20-shot single query setting. But the increment decreases to 0.2% when applying ensemble. Also, similar pattern can be found in 16- shot setting. The increments from T=1 to T=3 are relatively significant, which are 1.8% (20-shot) and 2.1% (16-shot). While continuously increasing the number of ensemble queries from T=3 to T=5, the increments decrease to 0.2% and 0.1%. Because the in-context examples are arranged according to the relevance to the test instances, these phenomenon could result from engaging irrelevance examples when enlarging the number of queries.\nWe conduct further examination on the ensemble behavior on OK-VQA dataset and find out that Prophet indeed benefits more from ensemble compared to the other baselines. Table 10 shows the results of the single query setting and the ensem-\nble setting. For Prophet, the gain of the ensemble setting over the single setting is 3.19% (Line 5), which is more than doubled compared to results on Line 1 to Line 4, and is nearly doubled compared to Line 6. This shows an inconsistency regarding the gain of ensemble setting over single setting, and supports our hypothesis that Prophet benefits more from the ensemble setting compared to other methods."
        },
        {
            "heading": "E Experiments with Dense Captions",
            "text": "In our framework, the LLM perceives the image from two parts of the prompt: \u201cCaption\u201d and \u201cImage Information\u201d (as the templates described in \u00a73.3). We incorporate the dense captions (denoted as GRiT (Wu et al., 2022a)) and conduct two types of experiments with the PICa pipeline:\n\u2022 Dense captions directly as \u201cCaption\u201d;\n\u2022 Dense captions as a type of \u201cImage Information\u201d.\nAs shown in Table 11, the first two lines show the influence of dense captions and general captions. Simply replacing the OSCAR captions by GRiT captions reduces the accuracy from 49.67% to 46.16%. Adding GRiT to PICa as the \u201cImage Information\u201d (line 4) also impairs the original PICa performance (line 1) by 0.24%.\nLines 3, 4 and 5 present the performances of different types of \u201cImage Information\u201d. Notably, replacing our information by GRiT\u2019s dense captions decreases the performance by 4.33% (comparing line 4 to line 3). While combining Ours and GRiT captions as the \u201cImage Information\u201d (line 5) reduces the accuracy by 1.21% compared to using only Ours \u201cImage Information\u201d (line 3).\nTo conclude, dense captions introduce noise and degrade the accuracy, regardless of the role as either \u201cCaption\u201d or \u201cImage Information\u201d. In contrast,\nutilizing the image information obtained and refined by our method consistently yields the best results."
        },
        {
            "heading": "F Prompt Templates",
            "text": "In this section, we provide detailed prompt template for question generation, summarization, and in-context learning for visual question reasoning. Since the input length of LLMs are limited, to present LLMs with more potential image information and relevant knowledge, we remove the separators (\u201c===\u201d) used in PICa and its followers.\nF.1 Prompt Templates for Question Generation\nHere is the prompt template and an example for question generation described in \u00a73.1. The number of questions to generate is 3, and the test instance is marked in blue. The template and example are as follows:\nPrompt Template for Question Generation\nPlease decompose the TARGET-QUESTION into 3 questions that can be answered via commonsense knowledge. The sub-questions should not mention another sub-questions. You can use information from the CAPTION.\\n TARGET-QUESTION: qn\\n Caption: Cn\\n Sub questions: 1. q\u2032\nn 1 . 2. q \u2032n 2 , ...\\n\nTARGET-QUESTION: q\\n Sub questions:\\n\nAn Example for Question Generation\nPlease decompose the TARGET-QUESTION into 3 questions that can be answered via commonsense knowledge. The sub-questions should not mention another sub-questions. You can use information from the CAPTION.\\n\nTARGET-QUESTION: What is the hairstyle of the blond called?\\n Caption: Two women tennis players on a tennis court.\\n Sub questions: 1. It this hairstyle long or short? 2. What are the notable features of the hairstyle? 3. What hairstyle are common for women player when they are playing tennis\\n\nTARGET-QUESTION: How old do you have to be in canada to do this?\\n Caption: a couple of people are holding up drinks.\\n Sub questions: 1. Why are people holding up drinks? 2. What is the restriction of age to drink in Canada? 3. What are people drinking?\\n\nTARGET-QUESTION: When was this piece of sporting equipment invented?\\n Caption: A man in a wetsuit carrying a surfboard to the water.\\n Sub questions: 1. What is the man carrying with him? 2. What is the purpose of the sporting equipment? 3. What is the history of the invention of the sporting equipment?\\n\nTARGET-QUESTION: What hair style does the child have?\\n Caption: a little girl with short hair talking on a cell phone.\\n Sub questions:\\n\nF.2 Prompt Templates for Summarization\nIn the refinement module, we summarize the generated questions with corresponding answer into narrative expressions for further process. Here is the prompt template and an example for information summarization described in \u00a73.2, the test instance is marked in blue:\nPrompt Template for Summarization\nPlease summarise the following question and corresponding answer into a description sentence.\\n Q: qn\\n A: an\\n Summary: 1. q\u2032\nn 1 . 2. q \u2032n 2 ,\n...\\n Q: q\\n A: an\\n Summary:\\n\nAn Example for Summarization\nPlease summarise the following question and corresponding answer into a description sentence.\\n\nQ: What is the specific type of drink be?\\n A: martini.\\n Summary: People are drinking martinis.\\n\nQ: What is the legal age to consume alcohol in Canada?\\n A: 18.\\n Summary: People should be at least 18 to consume alcohol in Canada.\\n\nQ: What type of drinks are on the table?\\n A: a soda.\\n Summary: There is a soda on the table.\\n\nQ: How is this beverage made?\\n A: it is a coffee drink\\n Summary:\nF.3 Prompt Templates for Reasoning\nWe employ few-shot in-context learning for answer reasoning. Here is the prompt template described in \u00a73.3, the test instance is marked in blue:\nPrompt Template for Reasoning\nAnswer the questions using the provided image information, captions and extra commonsense knowledge. Answers should be no longer than 3 words:\\n Image information: Sn\\n Caption: Cn\\n Question: qn\\n Answer: an Image information: S\\n Caption: C\\n Question: q\\n Answer:\nWe implement our method with different baselines according to the their default settings for image representation. PICa employs captions with tags as image representation; PromptCap uses thire question-aware captions; and Prophet provides extra answer candidates. There are the examples for PICa+ours, PromptCap+ours and Prophet+ours, our refined information is bolded in the template, and the test instance is marked in blue:\nAn Example for Reasoning with PICa\nAnswer the questions using the provided image information, captions and extra commonsense knowledge. Answers should be no longer than 3 words:\\n\nImage information: the person is skiing; the person is wearing skis on their feet; cross country skiing is a popular activity while skiing.\\n Caption: A man is cross country skiing through a forrest in winter. winter, tree, sky, outdoor recreation, piste, blizzard, ski resort, outdoor, snow, skiing\\n Question: What is this person doing?\\n Answer: cross country ski\nImage information: the person is wearing skis; cross country skis are one of the equipment options for this activity; Snow conditions impact travel safety during this activity.\\n Caption: A man on skis riding through the snow. cross-country skier, footwear, mountain, mountain guide, snowshoe, winter, glacial landform, standing, ski equipment, ice cap\\n Question: What is this person doing?\\n Answer: ski\n...\nImage information:The women steps over the water; Water freezes when it gets cold; The area will change when the temperature reaches 0 degrees.\\n Caption: A woman on skis in the snow near a tree. cross-country skier, footwear, outdoor recreation, blizzard, freezing, snowshoe, winter sport, winter, snow, trekking pole\\n Question: If it gets cold enough what will happen to the area being stepped over?\\n Answer:\nAn Example for Reasoning with PromptCap\nAnswer the questions using the provided image information, captions and extra commonsense knowledge. Answers should be no longer than 3 words:\\n\nImage information: the person is skiing; the person is wearing skis on their feet; cross country skiing is a popular activity while skiing.\\n Caption: A person skiing on a snowy road.\\n Question: What is this person doing?\\n Answer: cross country ski\nImage information: the person is wearing skis; cross country skis are one of the equipment options for this activity; Snow conditions impact travel safety during this activity.\\n Caption: A person skiing down a snowy hill.\\n Question: What is this person doing?\\n Answer: ski\n...\nImage information:The women steps over the water; Water freezes when it gets cold; The area will change when the temperature reaches 0 degrees.\\n Caption: a woman on skis in the snow\\n Question: If it gets cold enough what will happen to the area being stepped over?\\n Answer:\nAn Example for Reasoning with Prophet\nAnswer the questions using the provided image information, captions, candidate answers and extra commonsense knowledge. Each candidate answer is associated with a confidence score within a bracket. The true answer may not be included in the candidate answers. Answers should be no longer than 3 words:\\n\nImage information: the person is skiing; the person is wearing skis on their feet; cross country skiing is a popular activity while skiing.\\n Caption: A man is cross country skiing through a forrest in winter.\\n Question: What is this person doing?\\n Candidatew: ski (0.98), cross country ski (0.63), skiis (0.13), hike (0.11), snow (0.09), cross country (0.02), skiing (0.01), snowboard (0.00), camp (0.00), cold weather (0.00)\\n Answer: cross country ski\nImage information: the person is wearing skis; cross country skis are one of the equipment options for this activity; Snow conditions impact travel safety during this activity.\\n Caption: A man on skis riding through the snow. \\n Question: What is this person doing?\\n Candidatew: ski (0.99), snow (0.66), sky (0.15), water (0.03), skiis (0.02), ski pole (0.01), downhill (0.01), snowboard (0.00), hill (0.00), commuter (0.00)\\n Answer: ski\n...\nImage information:The women steps over the water; Water freezes when it gets cold; The area will change when the temperature reaches 0 degrees.\\n Caption: A woman on skis in the snow near a tree.\\n Question: If it gets cold enough what will happen to the area being stepped over?\\n Candidatew: fall (0.04), crash (0.02), break (0.01), avalanche (0.01), death (0.01), cold (0.00), freeze (0.00), autumn (0.00), oxygen (0.00), drown (0.00)\\n Answer:"
        }
    ],
    "title": "Filling the Image Information Gap for VQA: Prompting Large Language Models to Proactively Ask Questions",
    "year": 2023
}