{
    "abstractText": "Lately, propelled by phenomenal advances around the transformer architecture, the legal NLP field has enjoyed spectacular growth. To measure progress, well-curated and challenging benchmarks are crucial. Previous efforts have produced numerous benchmarks for general NLP models, typically based on news or Wikipedia. However, these may not fit specific domains such as law, with its unique lexicons and intricate sentence structures. Even though there is a rising need to build NLP systems for languages other than English, many benchmarks are available only in English and no multilingual benchmark exists in the legal NLP field. We survey the legal NLP literature and select 11 datasets covering 24 languages, creating LEXTREME. To fairly compare models, we propose two aggregate scores, i.e., dataset aggregate score and language aggregate score. Our results show that even the best baseline only achieves modest results, and also ChatGPT struggles with many tasks. This indicates that LEXTREME remains a challenging task with ample room for improvement. To facilitate easy use for researchers and practitioners, we release LEXTREME on huggingface along with a public leaderboard and the necessary code to evaluate models. We also provide a public Weights and Biases project containing all runs for transparency.",
    "authors": [
        {
            "affiliations": [],
            "name": "Joel Niklaus"
        },
        {
            "affiliations": [],
            "name": "Veton Matoshi"
        },
        {
            "affiliations": [],
            "name": "Pooja Rani"
        },
        {
            "affiliations": [],
            "name": "Andrea Galassi"
        },
        {
            "affiliations": [],
            "name": "Matthias St\u00fcrmer"
        },
        {
            "affiliations": [],
            "name": "Ilias Chalkidis"
        }
    ],
    "id": "SP:7e2c595aacf960effd985221aa9307ac47257d78",
    "references": [
        {
            "authors": [
                "Muhammad Al-Qurishi",
                "Sarah AlQaseemi",
                "Riad Souissi."
            ],
            "title": "Aralegal-bert: A pretrained language model for arabic legal text",
            "venue": "NLLP.",
            "year": 2022
        },
        {
            "authors": [
                "Iosif Angelidis",
                "Ilias Chalkidis",
                "Manolis Koubarakis."
            ],
            "title": "Named entity recognition, linking and generation for greek legislation",
            "venue": "JURIX, volume 313 of Frontiers in Artificial Intelligence and Applications, pages 1\u201310. IOS Press.",
            "year": 2018
        },
        {
            "authors": [
                "V\u00edt Baisa",
                "Jan Michelfeit",
                "Marek Medved",
                "Milo\u0161 Jakub\u00ed\u010dek"
            ],
            "title": "European Union language resources in Sketch Engine",
            "venue": "In Proceedings of the Tenth International Conference on Language Resources and Evaluation",
            "year": 2016
        },
        {
            "authors": [
                "James Barry",
                "Joachim Wagner",
                "Lauren Cassidy",
                "Alan Cowap",
                "Teresa Lynn",
                "Abigail Walsh",
                "M\u2019iche\u2019al J. \u2019O Meachair",
                "Jennifer Foster"
            ],
            "title": "gabert \u2014 an irish language model",
            "venue": "In International Conference on Language Resources and Evaluation",
            "year": 2021
        },
        {
            "authors": [
                "Iz Beltagy",
                "Matthew E. Peters",
                "Arman Cohan."
            ],
            "title": "Longformer: The long-document transformer",
            "venue": "CoRR, abs/2004.05150.",
            "year": 2020
        },
        {
            "authors": [
                "Emily M. Bender",
                "Timnit Gebru",
                "Angelina"
            ],
            "title": "McMillanMajor, and Shmargaret Shmitchell",
            "venue": "FAccT",
            "year": 2021
        },
        {
            "authors": [
                "Emily M. Bender",
                "Alexander Koller."
            ],
            "title": "Climbing towards NLU: on meaning, form, and understanding in the age of data",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, pages",
            "year": 2020
        },
        {
            "authors": [
                "Tobias Brugger",
                "Matthias St\u00fcrmer",
                "Joel Niklaus."
            ],
            "title": "Multilegalsbd: A multilingual legal sentence boundary detection dataset",
            "venue": "Proceedings of the Nineteenth International Conference on Artificial Intelligence and Law, ICAIL \u201923, page 42\u201351, New",
            "year": 2023
        },
        {
            "authors": [
                "Ilias Chalkidis."
            ],
            "title": "Chatgpt may pass the bar exam soon, but has a long way to go for the lexglue benchmark",
            "venue": "CoRR, abs/2304.12202.",
            "year": 2023
        },
        {
            "authors": [
                "Ilias Chalkidis",
                "Manos Fergadiotis",
                "Ion Androutsopoulos."
            ],
            "title": "MultiEURLEX - a multi-lingual and multi-label legal document classification dataset for zero-shot cross-lingual transfer",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Nat-",
            "year": 2021
        },
        {
            "authors": [
                "Ilias Chalkidis",
                "Manos Fergadiotis",
                "Prodromos Malakasiotis",
                "Nikolaos Aletras",
                "Ion Androutsopoulos."
            ],
            "title": "LEGAL-BERT: The muppets straight out of law school",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 2898\u2013",
            "year": 2020
        },
        {
            "authors": [
                "Ilias Chalkidis",
                "Manos Fergadiotis",
                "Dimitrios Tsarapatsanis",
                "Nikolaos Aletras",
                "Ion Androutsopoulos",
                "Prodromos Malakasiotis."
            ],
            "title": "Paragraph-level rationale extraction through regularization: A case study on European court of human rights cases",
            "venue": "In",
            "year": 2021
        },
        {
            "authors": [
                "Ilias Chalkidis",
                "Nicolas Garneau",
                "Catalina Goanta",
                "Daniel Katz",
                "Anders S\u00f8gaard."
            ],
            "title": "LeXFiles and LegalLAMA: Facilitating English multinational legal language model development",
            "venue": "Proceedings of the 61st Annual Meeting of the Association for",
            "year": 2023
        },
        {
            "authors": [
                "Ilias Chalkidis",
                "Abhik Jana",
                "Dirk Hartung",
                "Michael Bommarito",
                "Ion Androutsopoulos",
                "Daniel Katz",
                "Nikolaos Aletras."
            ],
            "title": "LexGLUE: A benchmark dataset for legal language understanding in English",
            "venue": "Proceedings of the 60th Annual Meeting of the",
            "year": 2022
        },
        {
            "authors": [
                "Ilias Chalkidis",
                "Tommaso Pasini",
                "Sheng Zhang",
                "Letizia Tomada",
                "Sebastian Felix Schwemer",
                "Anders S\u00f8gaard."
            ],
            "title": "FairLex: A Multilingual Benchmark for Evaluating Fairness in Legal Text Processing",
            "venue": "arXiv:2203.07228 [cs]. ArXiv: 2203.07228.",
            "year": 2022
        },
        {
            "authors": [
                "Branden Chan",
                "Stefan Schweter",
                "Timo M\u00f6ller."
            ],
            "title": "German\u2019s next language model",
            "venue": "Proceedings of the 28th International Conference on Computational Linguistics, pages 6788\u20136796, Barcelona, Spain (Online). International Committee on Computational Lin-",
            "year": 2020
        },
        {
            "authors": [
                "Meier-Hellstern",
                "Douglas Eck",
                "Jeff Dean",
                "Slav Petrov",
                "Noah Fiedel."
            ],
            "title": "PaLM: Scaling Language Modeling with Pathways",
            "venue": "arXiv:2204.02311 [cs]. ArXiv: 2204.02311.",
            "year": 2022
        },
        {
            "authors": [
                "Ramona Christen",
                "Anastassia Shaitarova",
                "Matthias St\u00fcrmer",
                "Joel Niklaus"
            ],
            "title": "Resolving Legalese: A Multilingual Exploration of Negation Scope Resolution in Legal Documents",
            "year": 2023
        },
        {
            "authors": [
                "Victor Hugo Ciurlino."
            ],
            "title": "Bertbr: a pretrained language model for law texts",
            "venue": "Master\u2019s thesis, Universidade de Bras\u00edlia.",
            "year": 2021
        },
        {
            "authors": [
                "Alexis Conneau",
                "Kartikay Khandelwal",
                "Naman Goyal",
                "Vishrav Chaudhary",
                "Guillaume Wenzek",
                "Francisco Guzm\u00e1n",
                "Edouard Grave",
                "Myle Ott",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Unsupervised cross-lingual representation learning at scale",
            "venue": "Pro-",
            "year": 2020
        },
        {
            "authors": [
                "Ona de Gibert",
                "A Garc\u00eda-Pablos",
                "Montse Cuadros",
                "Maite Melero."
            ],
            "title": "Spanish datasets for sensitive entity detection in the legal domain",
            "venue": "Proceedings of the Thirteenth International Conference on Language Resources and Evaluation",
            "year": 2022
        },
        {
            "authors": [
                "Javier de la Rosa",
                "Eduardo G. Ponferrada",
                "Manu Romero",
                "Paulo Villegas",
                "Pablo Gonz\u00e1lez de Prado Salas",
                "Mar\u00eda Grandury."
            ],
            "title": "BERTIN: efficient pre-training of a spanish language model using perplexity sampling",
            "venue": "Proces. del Leng.",
            "year": 2022
        },
        {
            "authors": [
                "Wietse de Vries",
                "Andreas van Cranenburgh",
                "Arianna Bisazza",
                "Tommaso Caselli",
                "Gertjan van Noord",
                "Malvina Nissim."
            ],
            "title": "Bertje: A dutch bert model",
            "venue": "ArXiv, abs/1912.09582.",
            "year": 2019
        },
        {
            "authors": [
                "Pieter Delobelle",
                "Thomas Winters",
                "Bettina Berendt."
            ],
            "title": "RobBERT: a Dutch RoBERTa-based Language Model",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3255\u20133265, Online. Association for Computational",
            "year": 2020
        },
        {
            "authors": [
                "Jia Deng",
                "Wei Dong",
                "Richard Socher",
                "Li-Jia Li",
                "Kai Li",
                "Li Fei-Fei."
            ],
            "title": "Imagenet: A large-scale hierarchical image database",
            "venue": "2009 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR 2009), 20-25 June 2009, Miami,",
            "year": 2009
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
            "year": 2019
        },
        {
            "authors": [
                "Tagiuri",
                "Paolo Torroni"
            ],
            "title": "A corpus for mul",
            "year": 2021
        },
        {
            "authors": [
                "Pyysalo."
            ],
            "title": "The birth of Romanian BERT",
            "venue": "In",
            "year": 2020
        },
        {
            "authors": [
                "Marta Villegas"
            ],
            "title": "2021b. MarIA: Spanish language",
            "year": 2021
        },
        {
            "authors": [
                "Daniel E. Ho"
            ],
            "title": "Pile of Law: Learning Responsi",
            "year": 2022
        },
        {
            "authors": [
                "Junjie Hu",
                "Sebastian Ruder",
                "Aditya Siddhant",
                "Graham Neubig",
                "Orhan Firat",
                "Melvin Johnson."
            ],
            "title": "XTREME: A massively multilingual multitask benchmark for evaluating cross-lingual generalisation",
            "venue": "ICML, volume 119 of Proceedings of Ma-",
            "year": 2020
        },
        {
            "authors": [
                "Wenyue Hua",
                "Yuchen Zhang",
                "Zhe Chen",
                "Josie Li",
                "Melanie Weber."
            ],
            "title": "LegalRelectra: Mixed-domain Language Modeling for Long-range Legal Text Comprehension",
            "venue": "ArXiv:2212.08204 [cs].",
            "year": 2022
        },
        {
            "authors": [
                "Rasmus Hvingelby",
                "Amalie Brogaard Pauli",
                "Maria Barrett",
                "Christina Rosted",
                "Lasse Malm Lidegaard",
                "Anders S\u00f8gaard."
            ],
            "title": "Dane: A named entity resource for danish",
            "venue": "International Conference on Language Resources and Evaluation.",
            "year": 2020
        },
        {
            "authors": [
                "Wonseok Hwang",
                "Dongjun Lee",
                "Kyoungyeon Cho",
                "Hanuhl Lee",
                "Minjoon Seo."
            ],
            "title": "A multi-task benchmark for korean legal language understanding and judgement prediction",
            "venue": "Thirty-sixth Conference on Neural Information Processing Systems Datasets",
            "year": 2022
        },
        {
            "authors": [
                "Maor Ivgi",
                "Uri Shaham",
                "Jonathan Berant."
            ],
            "title": "Efficient long-text understanding with short-text models",
            "venue": "Transactions of the Association for Computational Linguistics, 11:284\u2013299.",
            "year": 2023
        },
        {
            "authors": [
                "Ganesh Jawahar",
                "Beno\u00eet Sagot",
                "Djam\u00e9 Seddah."
            ],
            "title": "What does BERT learn about the structure of language? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3651\u20133657, Florence, Italy",
            "venue": "Association for",
            "year": 2019
        },
        {
            "authors": [
                "Daniel Martin Katz",
                "Dirk Hartung",
                "Lauritz Gerlach",
                "Abhik Jana",
                "Michael James Bommarito"
            ],
            "title": "Natural Language Processing in the Legal Domain",
            "year": 2023
        },
        {
            "authors": [
                "Bernard Koch",
                "Emily Denton",
                "Alex Hanna",
                "Jacob G. Foster."
            ],
            "title": "Reduced, reused and recycled: The life of a dataset in machine learning research",
            "venue": "NeurIPS Datasets and Benchmarks.",
            "year": 2021
        },
        {
            "authors": [
                "John Koutsikakis",
                "Ilias Chalkidis",
                "Prodromos Malakasiotis",
                "Ion Androutsopoulos."
            ],
            "title": "Greek-bert: The greeks visiting sesame street",
            "venue": "11th Hellenic Conference on Artificial Intelligence, SETN 2020, page 110\u2013117, New York, NY, USA. Association for",
            "year": 2020
        },
        {
            "authors": [
                "Andr\u00e9 Lage-Freitas",
                "H\u00e9ctor Allende-Cid",
                "Orivaldo Santana",
                "L\u00edvia Oliveira-Lage."
            ],
            "title": "Predicting brazilian court decisions",
            "venue": "PeerJ Computer Science, 8:e904. Dataset URL: https://github.com/proflage/predicting-brazilian-",
            "year": 2022
        },
        {
            "authors": [
                "Daniele Licari",
                "Giovanni Comand\u00e9."
            ],
            "title": "ITALIAN-LEGAL-BERT: A pre-trained transformer language model for italian law",
            "venue": "EKAW (Companion), volume 3256 of CEUR Workshop Proceedings. CEUR-WS.org.",
            "year": 2022
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
            "venue": "CoRR, abs/2111.09543(1).",
            "year": 2019
        },
        {
            "authors": [
                "Pedro Henrique Luz de Araujo",
                "Te\u00f3filo E de Campos",
                "Renato RR de Oliveira",
                "Matheus Stauffer",
                "Samuel Couto",
                "Paulo Bermejo."
            ],
            "title": "Lenerbr: a dataset for named entity recognition in brazilian legal text",
            "venue": "International Conference on",
            "year": 2018
        },
        {
            "authors": [
                "Martin Malmsten",
                "Love B\u00f6rjeson",
                "Chris Haffenden."
            ],
            "title": "Playing with words at the national library of sweden - making a swedish bert",
            "venue": "ArXiv, abs/2007.01658.",
            "year": 2020
        },
        {
            "authors": [
                "Louis Martin",
                "Benjamin Muller",
                "Pedro Javier Ortiz Su\u00e1rez",
                "Yoann Dupont",
                "Laurent Romary",
                "\u00c9ric de la Clergerie",
                "Djam\u00e9 Seddah",
                "Beno\u00eet Sagot."
            ],
            "title": "CamemBERT: a tasty French language model",
            "venue": "Proceedings of the 58th Annual Meeting of the As-",
            "year": 2020
        },
        {
            "authors": [
                "Mihai Masala",
                "Radu Cristian Alexandru Iacob",
                "Ana Sabina Uban",
                "Marina Cidota",
                "Horia Velicu",
                "Traian Rebedea",
                "Marius Popescu."
            ],
            "title": "jurBERT: A Romanian BERT model for legal judgement prediction",
            "venue": "Proceedings of the Natural Legal",
            "year": 2021
        },
        {
            "authors": [
                "B.W. Matthews."
            ],
            "title": "Comparison of the predicted and observed secondary structure of t4 phage lysozyme",
            "venue": "Biochimica et Biophysica Acta (BBA) - Protein Structure, 405(2):442\u2013451.",
            "year": 1975
        },
        {
            "authors": [
                "D\u00e1vid M\u00e1rk Nemeskey."
            ],
            "title": "Natural Language Processing Methods for Language Modeling",
            "venue": "Ph.D. thesis, E\u00f6tv\u00f6s Lor\u00e1nd University.",
            "year": 2020
        },
        {
            "authors": [
                "Joel Niklaus",
                "Ilias Chalkidis",
                "Matthias St\u00fcrmer."
            ],
            "title": "Swiss-judgment-prediction: A multilingual legal judgment prediction benchmark",
            "venue": "Proceedings of the Natural Legal Language Processing Workshop 2021, pages 19\u201335, Punta Cana, Dominican Republic.",
            "year": 2021
        },
        {
            "authors": [
                "Joel Niklaus",
                "Daniele Giofre"
            ],
            "title": "Can we pretrain a SotA legal language model on a budget from scratch",
            "venue": "In Proceedings of The Fourth Workshop on Simple and Efficient Natural Language Processing (SustaiNLP),",
            "year": 2023
        },
        {
            "authors": [
                "Joel Niklaus",
                "Robin Mami\u00e9",
                "Matthias St\u00fcrmer",
                "Daniel Brunner",
                "Marcel Gygli."
            ],
            "title": "Automatic Anonymization of Swiss Federal Supreme Court Rulings",
            "venue": "ArXiv:2310.04632 [cs].",
            "year": 2023
        },
        {
            "authors": [
                "Joel Niklaus",
                "Veton Matoshi",
                "Matthias St\u00fcrmer",
                "Ilias Chalkidis",
                "Daniel E. Ho"
            ],
            "title": "2023b. Multilegalpile: A 689gb multilingual legal corpus",
            "year": 2023
        },
        {
            "authors": [
                "Joel Niklaus",
                "Matthias St\u00fcrmer",
                "Ilias Chalkidis."
            ],
            "title": "An empirical study on cross-X transfer for legal judgment prediction",
            "venue": "Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the",
            "year": 2022
        },
        {
            "authors": [
                "Vasile Pais",
                "Maria Mitrofan",
                "Carol Luca Gasan",
                "Vlad Coneschi",
                "Alexandru Ianov."
            ],
            "title": "Named entity recognition in the Romanian legal domain",
            "venue": "Proceedings of the Natural Legal Language Processing Workshop 2021, pages 9\u201318, Punta Cana, Dominican",
            "year": 2021
        },
        {
            "authors": [
                "Christos Papaloukas",
                "Ilias Chalkidis",
                "Konstantinos Athinaios",
                "Despina Pantazi",
                "Manolis Koubarakis."
            ],
            "title": "Multi-granular legal topic classification on Greek legislation",
            "venue": "Proceedings of the Natural Legal Language Processing Workshop 2021, pages",
            "year": 2021
        },
        {
            "authors": [
                "Yifan Peng",
                "Shankai Yan",
                "Zhiyong Lu."
            ],
            "title": "Transfer learning in biomedical natural language processing: An evaluation of BERT and elmo on ten benchmarking datasets",
            "venue": "BioNLP@ACL, pages 58\u201365. Association for Computational Linguistics.",
            "year": 2019
        },
        {
            "authors": [
                "Mat\u00fas Pikuliak",
                "Stefan Grivalsky",
                "Martin Konopka",
                "Miroslav Blst\u00e1k",
                "Martin Tamajka",
                "Viktor Bachrat\u00fd",
                "Mari\u00e1n Simko",
                "Pavol Bal\u00e1zik",
                "Michal Trnka",
                "Filip Uhl\u00e1rik."
            ],
            "title": "Slovakbert: Slovak masked language model",
            "venue": "CoRR, abs/2109.15254.",
            "year": 2021
        },
        {
            "authors": [
                "Inioluwa Deborah Raji",
                "Emily Denton",
                "Emily M. Bender",
                "Alex Hanna",
                "Amandalynne Paullada."
            ],
            "title": "AI and the everything in the whole wide world benchmark",
            "venue": "NeurIPS Datasets and Benchmarks.",
            "year": 2021
        },
        {
            "authors": [
                "Roshan Rao",
                "Nicholas Bhattacharya",
                "Neil Thomas",
                "Yan Duan",
                "Xi Chen",
                "John F. Canny",
                "Pieter Abbeel",
                "Yun S. Song."
            ],
            "title": "Evaluating protein transfer learning with TAPE",
            "venue": "NeurIPS, pages 9686\u20139698.",
            "year": 2019
        },
        {
            "authors": [
                "Vishvaksenan Rasiah",
                "Ronja Stern",
                "Veton Matoshi",
                "Matthias St\u00fcrmer",
                "Ilias Chalkidis",
                "Daniel E. Ho",
                "Joel Niklaus"
            ],
            "title": "Scale: Scaling up the complexity for advanced language model evaluation",
            "year": 2023
        },
        {
            "authors": [
                "Victor Sanh",
                "Lysandre Debut",
                "Julien Chaumond",
                "Thomas Wolf."
            ],
            "title": "Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter",
            "venue": "ArXiv, abs/1910.01108.",
            "year": 2019
        },
        {
            "authors": [
                "Gil Semo",
                "Dor Bernsohn",
                "Ben Hagag",
                "Gila Hayat",
                "Joel Niklaus."
            ],
            "title": "ClassActionPrediction: A challenging benchmark for legal judgment prediction of class action cases in the US",
            "venue": "Proceedings of the Natural Legal Language Processing Workshop 2022,",
            "year": 2022
        },
        {
            "authors": [
                "Zejiang Shen",
                "Kyle Lo",
                "Lauren Yu",
                "Nathan Dahlberg",
                "Margo Schlanger",
                "Doug Downey."
            ],
            "title": "Multilexsum: Real-world summaries of civil rights lawsuits at multiple granularities",
            "venue": "Thirty-sixth Conference on Neural Information Processing Systems",
            "year": 2022
        },
        {
            "authors": [
                "Jakub Sido",
                "Ond\u0159ej Pra\u017e\u00e1k",
                "Pavel P\u0159ib\u00e1\u0148",
                "Jan Pa\u0161ek",
                "Michal Sej\u00e1k",
                "Miloslav Konop\u00edk."
            ],
            "title": "Czert \u2013 Czech BERT-like model for language representation",
            "venue": "Proceedings of the International Conference on Recent Advances in Natural Language Process-",
            "year": 2021
        },
        {
            "authors": [
                "F\u00e1bio Souza",
                "Rodrigo Nogueira",
                "Roberto Lotufo."
            ],
            "title": "Bertimbau: Pretrained bert models for brazilian portuguese",
            "venue": "Intelligent Systems: 9th Brazilian Conference, BRACIS 2020, Rio Grande, Brazil, October 20\u201323, 2020, Proceedings, Part I, page 403\u2013417,",
            "year": 2020
        },
        {
            "authors": [
                "Andrea Tagarelli",
                "Andrea Simeri."
            ],
            "title": "Lamberta: Law article mining based on bert architecture for the italian civil code",
            "venue": "ICRDL.",
            "year": 2022
        },
        {
            "authors": [
                "Hasan Tanvir",
                "Claudia Kittask",
                "Sandra Eiche",
                "Kairit Sirts."
            ],
            "title": "EstBERT: A pretrained languagespecific BERT for Estonian",
            "venue": "Proceedings of the 23rd Nordic Conference on Computational Linguistics (NoDaLiDa), pages 11\u201319, Reykjavik, Ice-",
            "year": 2021
        },
        {
            "authors": [
                "Shavrina Tatiana",
                "Malykh Valentin."
            ],
            "title": "How not to lie with a benchmark: Rearranging NLP leaderboards",
            "venue": "I (Still) Can\u2019t Believe It\u2019s Not Better Workshop at NeurIPS 2021, volume abs/2112.01342.",
            "year": 2021
        },
        {
            "authors": [
                "Rena Torres Cacoullos."
            ],
            "title": "Code-switching strategies: Prosody and syntax",
            "venue": "Frontiers in Psychology,",
            "year": 2020
        },
        {
            "authors": [
                "Hung-yi Lee."
            ],
            "title": "SUPERB-SG: enhanced speech processing universal performance benchmark for semantic and generative capabilities",
            "venue": "ACL (1), pages 8479\u20138492. Association for Computational Linguistics.",
            "year": 2022
        },
        {
            "authors": [
                "Dimitrios Tsarapatsanis",
                "Nikolaos Aletras."
            ],
            "title": "On the ethical limits of natural language processing on legal text",
            "venue": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 3590\u20133599, Online. Association for Computational",
            "year": 2021
        },
        {
            "authors": [
                "Georgios Tziafas",
                "Eugenie de Saint-Phalle",
                "Wietse de Vries",
                "Clara Egger",
                "Tommaso Caselli."
            ],
            "title": "A multilingual approach to identify and classify exceptional measures against covid-19",
            "venue": "Proceedings of the Natural Legal Language Process-",
            "year": 2021
        },
        {
            "authors": [
                "Stefanie Urchs.",
                "Jelena Mitrovi\u0107.",
                "Michael Granitzer."
            ],
            "title": "Design and implementation of german legal decision corpora",
            "venue": "Proceedings of the 13th International Conference on Agents and Artificial Intelligence - Volume 2: ICAART,, pages 515\u2013521.",
            "year": 2021
        },
        {
            "authors": [
                "Serena Villata",
                "Micha\u0142 Araszkiewicz",
                "Kevin D. Ashley",
                "Trevor J.M. Bench-Capon",
                "L. Karl Branting",
                "Jack G. Conrad",
                "Adam Zachary Wyner."
            ],
            "title": "Thirty years of artificial intelligence and law: the third decade",
            "venue": "Artificial Intelligence and Law, 30:561\u2013",
            "year": 2022
        },
        {
            "authors": [
                "Antti Virtanen",
                "Jenna Kanerva",
                "Rami Ilo",
                "Jouni Luoma",
                "Juhani Luotolahti",
                "Tapio Salakoski",
                "Filip Ginter",
                "Sampo Pyysalo."
            ],
            "title": "Multilingual is not enough: Bert for finnish",
            "venue": "ArXiv, abs/1912.07076.",
            "year": 2019
        },
        {
            "authors": [
                "Alex Wang",
                "Yada Pruksachatkun",
                "Nikita Nangia",
                "Amanpreet Singh",
                "Julian Michael",
                "Felix Hill",
                "Omer Levy",
                "Samuel R. Bowman."
            ],
            "title": "Superglue: A stickier benchmark for general-purpose language understanding systems",
            "venue": "NeurIPS, pages 3261\u20133275.",
            "year": 2019
        },
        {
            "authors": [
                "Alex Wang",
                "Amanpreet Singh",
                "Julian Michael",
                "Felix Hill",
                "Omer Levy",
                "Samuel R. Bowman."
            ],
            "title": "GLUE: A multi-task benchmark and analysis platform for natural language understanding",
            "venue": "ICLR (Poster). OpenReview.net.",
            "year": 2019
        },
        {
            "authors": [
                "Wenhui Wang",
                "Furu Wei",
                "Li Dong",
                "Hangbo Bao",
                "Nan Yang",
                "Ming Zhou."
            ],
            "title": "Minilm: Deep selfattention distillation for task-agnostic compression of pre-trained transformers",
            "venue": "Proceedings of the 34th International Conference on Neural Information",
            "year": 2020
        },
        {
            "authors": [
                "Chaojun Xiao",
                "Xueyu Hu",
                "Zhiyuan Liu",
                "Cunchao Tu",
                "Maosong Sun."
            ],
            "title": "Lawformer: A pre-trained language model for chinese legal long documents",
            "venue": "AI Open, 2:79\u201384.",
            "year": 2021
        },
        {
            "authors": [
                "Yiwen Zhang",
                "He Zhou",
                "Shaoweihua Liu",
                "Zhe Zhao",
                "Qipeng Zhao",
                "Cong Yue",
                "Xinrui Zhang",
                "Zhengliang Yang",
                "Kyle Richardson",
                "Zhenzhong Lan."
            ],
            "title": "CLUE: A Chinese language understanding evaluation benchmark",
            "venue": "COLING, pages 4762\u20134772,",
            "year": 2020
        },
        {
            "authors": [
                "Linting Xue",
                "Noah Constant",
                "Adam Roberts",
                "Mihir Kale",
                "Rami Al-Rfou",
                "Aditya Siddhant",
                "Aditya Barua",
                "Colin Raffel."
            ],
            "title": "mT5: A massively multilingual pre-trained text-to-text transformer",
            "venue": "arXiv:2010.11934 [cs]. ArXiv: 2010.11934.",
            "year": 2021
        },
        {
            "authors": [
                "Wen Li",
                "Shinji Watanabe",
                "Abdelrahman Mohamed",
                "Hung-yi Lee."
            ],
            "title": "SUPERB: speech processing universal performance benchmark",
            "venue": "Interspeech, pages 1194\u20131198. ISCA.",
            "year": 2021
        },
        {
            "authors": [
                "Ying Yin",
                "Ivan Habernal."
            ],
            "title": "Privacy-preserving models for legal natural language processing",
            "venue": "Proceedings of the Natural Legal Language Processing Workshop 2022, pages 172\u2013183, Abu Dhabi, United Arab Emirates (Hybrid). Association for Computa-",
            "year": 2022
        },
        {
            "authors": [
                "Buzhou Tang",
                "Qingcai Chen."
            ],
            "title": "CBLUE: A Chinese biomedical language understanding evaluation benchmark",
            "venue": "ACL, pages 7888\u20137915, Dublin, Ireland. Association for Computational Linguistics.",
            "year": 2022
        },
        {
            "authors": [
                "Lucia Zheng",
                "Neel Guha",
                "Brandon R. Anderson",
                "Peter Henderson",
                "Daniel E. Ho."
            ],
            "title": "When does pretraining help? assessing self-supervised learning for law and the casehold dataset of 53,000+ legal holdings",
            "venue": "Proceedings of the Eighteenth International",
            "year": 2021
        },
        {
            "authors": [
                "DistilBERT DistilBERT (Sanh"
            ],
            "title": "2019) is a more compressed version of BERT (Devlin et al., 2019) using teacher-student learning, similar to MiniLM. DistilBERT is distilled from BERT",
            "year": 2019
        },
        {
            "authors": [
                "Lage-Freitas"
            ],
            "title": "Bidirectional Long Short-Term Memory (BiLSTM) pt pt ",
            "venue": "Lage-Freitas et al",
            "year": 2022
        },
        {
            "authors": [
                "Lage-Freitas"
            ],
            "title": "2022) support vector machine",
            "year": 2022
        },
        {
            "authors": [
                "Lage-Freitas"
            ],
            "title": "Bidirectional Long Short-Term Memory (BiLSTM) pt pt ",
            "venue": "Lage-Freitas et al",
            "year": 2022
        },
        {
            "authors": [
                "Lage-Freitas"
            ],
            "title": "2022) support vector machine",
            "year": 2022
        },
        {
            "authors": [
                "Tziafas"
            ],
            "title": "2021) gated recurrent unit",
            "year": 2021
        },
        {
            "authors": [
                "Tziafas"
            ],
            "title": "2021) support vector machine",
            "year": 2021
        },
        {
            "authors": [
                "Tziafas"
            ],
            "title": "zero-shot classification XLM-RoBERTa pretrained on C19 all without fr-be fr-be - ",
            "venue": "Tziafas et al",
            "year": 2021
        },
        {
            "authors": [
                "Tziafas"
            ],
            "title": "zero-shot classification XLM-RoBERTa",
            "venue": "Tziafas et al",
            "year": 2021
        },
        {
            "authors": [
                "Urchs"
            ],
            "title": "2021) tf-idf/decision stump",
            "year": 2021
        },
        {
            "authors": [
                "de de"
            ],
            "title": "tf-idf/support vector machine de de - - - - - ",
            "venue": "Urchs. et al",
            "year": 2021
        },
        {
            "authors": [
                "de de"
            ],
            "title": "Unigram/logistic regression de de - - ",
            "venue": "Urchs. et al",
            "year": 2021
        },
        {
            "authors": [
                "el el"
            ],
            "title": "Support Vector Machines + Bag-of-Words (SVM-BOW) el el - - ",
            "venue": "Papaloukas et al",
            "year": 2021
        },
        {
            "authors": [
                "el el"
            ],
            "title": "Support Vector Machines + Bag-of-Words (SVM-BOW) el el - - ",
            "venue": "Papaloukas et al",
            "year": 2021
        },
        {
            "authors": [
                "el el"
            ],
            "title": "Support Vector Machines + Bag-of-Words (SVM-BOW) el el - - ",
            "venue": "Papaloukas et al",
            "year": 2021
        },
        {
            "authors": [
                "Pais"
            ],
            "title": "CoRoLa word embeddings + MARCELL word embeddings+BiLSTM-CRF + gazetteers",
            "venue": "Pais et al",
            "year": 2021
        },
        {
            "authors": [
                "Chalkidis"
            ],
            "title": "2021a) xlm-roberta-base + Adapters layers all all - - - - - - - 66.4 Chalkidis et al. (2021a) xlm-roberta-base + Adapters layers all en - - - - - - - 67.3 Chalkidis et al. (2021a) xlm-roberta-base + Adapters layers all da - - - - - - ",
            "venue": "Chalkidis et al",
            "year": 2021
        },
        {
            "authors": [
                "Chalkidis"
            ],
            "title": "2021a) xlm-roberta-base + Adapters layers",
            "venue": "Chalkidis et al",
            "year": 2021
        },
        {
            "authors": [
                "Chalkidis"
            ],
            "title": "2021a) xlm-roberta-base + Adapters layers",
            "venue": "Chalkidis et al",
            "year": 2021
        },
        {
            "authors": [
                "Chalkidis"
            ],
            "title": "2021a) xlm-roberta-base + Adapters layers",
            "venue": "Chalkidis et al",
            "year": 2021
        },
        {
            "authors": [
                "Chalkidis"
            ],
            "title": "2021a) xlm-roberta-base + Adapters layers",
            "venue": "Chalkidis et al",
            "year": 2021
        },
        {
            "authors": [
                "Niklaus"
            ],
            "title": "Italian Hierarchical BERT",
            "year": 2021
        },
        {
            "authors": [
                "Niklaus"
            ],
            "title": "Multilingual Hierarchical BERT fr fr ",
            "year": 2021
        },
        {
            "authors": [
                "Niklaus"
            ],
            "title": "Multilingual Hierarchical BERT it it ",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "In the last decade, Natural Language Processing (NLP) has gained relevance in Legal Artificial Intelligence, transitioning from symbolic to subsymbolic techniques (Villata et al., 2022). Such a shift is motivated partially by the nature of legal resources, which appear primarily in a textual format (legislation, legal proceedings, contracts, etc.). Following the advancements in NLP technologies, the legal NLP literature (Zhong et al., 2020; Aletras et al., 2022; Katz et al., 2023) is flourishing\n\u2217 Equal contribution.\nwith many new resources, such as large legal corpora (Henderson et al., 2022), task-specific datasets (Shen et al., 2022; Christen et al., 2023; Brugger et al., 2023; Niklaus et al., 2023a), and pre-trained legal-oriented language models (Chalkidis et al., 2020; Zheng et al., 2021; Xiao et al., 2021; Niklaus and Giofre, 2023; Hua et al., 2022; Chalkidis et al., 2023). Greco and Tagarelli (2023) offer a comprehensive survey on the topic.\nSpecifically, the emergence of pre-trained Language Models (PLMs) has led to significant performance boosts on popular benchmarks like GLUE (Wang et al., 2019b) or SuperGLUE (Wang et al., 2019a), emphasizing the need for more challenging benchmarks to measure progress. Legal benchmark suites have also been developed to systematically evaluate the performance of PLMs, showcasing the superiority of legal-oriented models over generic ones on downstream tasks such as legal document classification or question answering (Chalkidis et al., 2022a; Hwang et al., 2022).\nEven though these PLMs are shown to be effective for numerous downstream tasks, they are general-purpose models that are trained on broaddomain resources, such as Wikipedia or News, and therefore, can be insufficient to address tasks specific to the legal domain (Chalkidis et al., 2020; Hua et al., 2022; Niklaus and Giofre, 2023). Indeed, the legal domain is strongly characterized both by its lexicon and by specific knowledge typically not available outside of specialized domain resources. Laypeople even sometimes call the language used in legal documents \u201clegalese\u201d or \u201clegal jargon\u201d, emphasizing its complexity. Moreover, the length of a legal document usually exceeds the length of a Wikipedia or news article, and in some tasks the relationships between its entities may span across the entire document. Therefore, it is necessary to develop specialized Legal PLMs trained on extensive collections of legal documents and evaluate them on standardized legal benchmarks. While new PLMs capable of handling long documents have been developed in the last years, they are predominantly trained for the general domain and on English data only.\nThe rising need to build NLP systems for languages other than English, the lack of textual resources for such languages, and the widespread use of code-switching in many cultures (Torres Cacoullos, 2020) is pushing researchers to train models on massively multilingual data (Conneau et al., 2020). Nonetheless, to the best of our knowledge, no multilingual legal language model has been proposed so far. Consequently, there is a need for standardized multilingual benchmarks that can be used to evaluate existing models and assess whether more research efforts should be directed toward the development of domain-specific models. This is particularly important for legal NLP where inherently multinational (European Union, Council of Europe) or multilingual (Canada, Switzerland) legal systems are prevalent.\nIn this work, we propose a challenging multilingual benchmark for the legal domain, named LEXTREME. We survey the literature from 2010 to 2022 and select 11 relevant NLU datasets covering 24 languages in 8 subdivisions (Germanic, Romance, Slavic, Baltic, Greek, Celtic, Finnic, and Hungarian) from two language families (IndoEuropean and Uralic). We evaluate five widely used multilingual encoder-based language models as shown in Figure 1 and observe a correlation\nbetween the model size and performance on LEXTREME. Surprisingly, at the low end, DistilBERT (Sanh et al., 2019) strongly outperforms MiniLM (Wang et al., 2020) (36.7 vs 19.0 LEXTREME score) while only having marginally more parameters (135M vs 118M).\nFor easy evaluation of future models, we release the aggregate dataset on the huggingface hub 1\nalong with a public leaderboard and the necessary code to run experiments on GitHub.2 Knowing that our work can not encompass \u201cEverything in the Whole Wide Legal World\u201d (Raji et al., 2021), we design LEXTREME as a living benchmark and provide detailed guidelines on our repository and encourage the community to contribute high-quality multilingual legal datasets.3 Finally, we integrated LEXTREME together with the popular English legal benchmark LexGLUE (Chalkidis et al., 2022a) into HELM (Liang et al., 2022) (an effort to evaluate language models holistically using a large number of datasets from diverse tasks) to ease the adoption of curated legal benchmarks also for the evaluation of large language models such as GPT3 (Brown et al., 2020), PALM (Chowdhery et al., 2022) or LLaMA (Touvron et al., 2023). Contributions of this paper are two-fold:\n1. We review the legal NLP literature to find relevant legal datasets and compile a multilingual legal benchmark of 11 datasets in 24 languages from 8 language groups.\n2. We evaluate several baselines on LEXTREME to provide a reference point for researchers and practitioners to compare to."
        },
        {
            "heading": "2 Related Work",
            "text": ""
        },
        {
            "heading": "2.1 Benchmarks",
            "text": "Benchmarking is an established method to enable easy and systematic comparison of approaches. GLUE (Wang et al., 2019b) is one of the first benchmarks to evaluate general-purpose neural language models. It is a set of supervised sentence understanding predictive tasks in English that were created through aggregation and curation of several existing datasets. However, it became quickly obsolete due to advanced contextual language models, such as BERT (Devlin et al., 2019), which\n1https://huggingface.co/datasets/joelniklaus/ lextreme\n2https://github.com/JoelNiklaus/LEXTREME 3Since the release of this call in February 2023, already\neleven new tasks have been contributed and integrated.\nexcelled on most tasks. Subsequently, its updated version, named SUPERGLUE (Wang et al., 2019a) was proposed, incorporating new predictive tasks that are solvable by humans but are difficult for machines. Both benchmarks proposed an evaluation score computed as an aggregation of the scores obtained by the same model on each task. They are also agnostic regarding the pre-training of the model, and do not provide a specific corpus for it. Inspired by these works, numerous benchmarks have been proposed over the years. We describe some well-known ones in Table 1.\nThe MMLU benchmark is specifically designed to evaluate the knowledge acquired during the pretraining phase of the model by featuring only zeroshot and few-shot learning tasks (Hendrycks et al., 2021). Similarly, SUPERB (Yang et al., 2021) and SUPERB-SG (Tsai et al., 2022) were proposed for speech data, unifying well-known datasets. However, they mainly vary in tasks, e.g., SUPERBSG includes both predictive and generative tasks, which makes it different from the other benchmarks discussed in this section. Additionally, SUPERBSG includes diverse tasks, such as speech translation and cross-lingual automatic speech recognition, which require knowledge of languages other than English. Neither of the two (SUPERB or SUPERB-SG) proposes an aggregated score.\nXTREME (Hu et al., 2020) is a benchmark specifically designed to evaluate the ability of crosslingual generalization of models. It includes six cross-lingual predictive tasks over ten datasets of miscellaneous texts, covering a total of 40 languages. While some original datasets in it were al-\nready designed for cross-lingual tasks, others were extended by translating part of the data using human professionals and automatic methods."
        },
        {
            "heading": "2.2 Benchmarks for the Legal Domain",
            "text": "LEXGLUE (Chalkidis et al., 2022a) is the first benchmark for the legal domain and covers six predictive tasks over five datasets made of textual documents in English from the US, EU, and Council of Europe. While some tasks may not require specific legal knowledge to be solved, others would probably need, or at least benefit from, information regarding the EU or US legislation on the specific topic. Among the main limitations of their benchmark, Chalkidis et al. highlight its monolingual nature and remark that \u201cthere is an increasing need for developing models for other languages\u201d. In parallel, Chalkidis et al. (2022b) released FairLex, a multilingual benchmark for the evaluation of fairness in legal NLP tasks. With a similar aim, Hwang et al. (2022) released the LBOX benchmark, covering two classification tasks, two legal judgment prediction tasks, and one Korean summarization task. Motivated by LEXGLUE and LBOX, we propose a benchmark to encourage multilingual models, diverse tasks, and datasets for the legal domain. Guha et al. (2022) proposed the LEGALBENCH initiative that aims to establish an open and collaborative legal reasoning benchmark for few-shot evaluation of LLMs where legal practitioners and other domain experts can contribute by submitting tasks. At its creation, the authors have already added 44 lightweight tasks. While most tasks require legal reasoning based on the common\nlaw system (mostly prevalent in the UK and former colonies), there is also a clause classification task. For a more comprehensive overview of the many tasks related to automated legal text analysis, we recommend reading the works of Chalkidis et al. (2022a) and Zhong et al. (2020)."
        },
        {
            "heading": "2.3 Legal Language Models",
            "text": "Several works have proposed legal language models (models specifically trained for the legal domain) for several languages other than English. For example, legal language models for English (Chalkidis et al., 2020; Yin and Habernal, 2022; Chalkidis et al., 2023), French (Douka et al., 2021), Romanian (Masala et al., 2021), Italian (Tagarelli and Simeri, 2022; Licari and Comand\u00e9, 2022), Chinese (Xiao et al., 2021), Arabic (Al-Qurishi et al., 2022), Korean (Hwang et al., 2022), and Portuguese (Ciurlino, 2021). Recently, pre-trained multilingual legal language models (Niklaus et al., 2023b; Rasiah et al., 2023) have been released. Unfortunately, these models were not available at the time of submission, so we do not present results as part of this work."
        },
        {
            "heading": "3 LEXTREME Datasets and Tasks",
            "text": ""
        },
        {
            "heading": "3.1 Dataset and Task Selection",
            "text": "To find relevant datasets for the LEXTREME benchmark we explore the literature of NLP and the legal domain, identifying relevant venues such as ACL, EACL, NAACL, EMNLP, LREC, ICAIL, and the NLLP workshop. We search the literature on these venues for the years 2010 to 2022. We search for some common keywords (case insensitive) that are related to the legal domain, e.g., criminal, judicial, judgment, jurisdictions, law, legal, legislation, and dataset, e.g., dataset, and corpus vie their union. These keywords help to select 108 potentially relevant papers. Then, we formulate several criteria to select the datasets. Finally, three authors analyze the candidate papers and perform the selection. We handled the disagreement between authors based on mutual discussion and the majority voting mechanism.\nInclusion criteria: I1: It is about legal text (e.g., patents are not con-\nsidered part of the legal text) I2: It performs legal tasks (e.g., judgment pre-\ndiction) or NLU tasks on legal text in order to have datasets that understand or reason\nabout the legal text, similar to LEXGLUE (Chalkidis et al., 2022a)\nI3: The current tasks are set in a European language, as per the scope of our present work, but we aim to incorporate a broader range of languages in future iterations of LEXTREME\nI4: The dataset is annotated by humans directly or indirectly (e.g., judgement labels are extracted with regexes)\nExclusion criteria: E1: The dataset is not publicly available E2: The dataset does not contain a public license\nor does not allow data redistribution E3: The dataset contains labels generated with ML\nsystems E4: It is not a peer-reviewed paper\nAfter applying the above criteria, we select 11 datasets from 108 papers. We provide the list of all these datasets in our repository."
        },
        {
            "heading": "3.2 LEXTREME Datasets",
            "text": "In the following, we briefly describe the selected datasets. Table 2 provides more information about the number of examples and label classes per split for each task. For a detailed overview of the jurisdictions as well as the number of languages covered by each dataset, see Table 3. Each dataset can have several configurations (tasks), which are the basis of our analyses, i.e., the pre-trained models have always been fine-tuned on a single task. LEXTREME consists of three task types: Single\nLabel Text Classification (SLTC), Multi Label Text Classification (MLTC), and Named Entity Recognition (NER). We use the existing train, validation, and test splits if present. Otherwise, we split the data randomly ourselves (80% train, 10% validation, and 10% test).\nBrazilian Court Decisions (BCD). Legal systems are often huge and complex, and the information is scattered across various sources. Thus, predicting case outcomes from multiple vast volumes of litigation is a difficult task. Lage-Freitas et al. (2022) propose an approach to predict Brazilian legal decisions to support legal practitioners. We use their dataset from the State Supreme Court of Alagoas (Brazil). The input to the models is always the case description. We perform two SLTC tasks: In the BCD-J subset models predict the approval or dismissal of the case or appeal with the three labels no, partial, yes, and in the BCD-U models predict the judges\u2019 unanimity on the decision alongside two labels, namely unanimity, not-unanimity.\nGerman Argument Mining (GAM). Identifying arguments in court decisions is vital and challenging for legal practitioners. Urchs. et al. (2021) assembled a dataset of 200 German court decisions for sentence classification based on argumentative function. We utilize this dataset for a SLTC task. Model input is a sentence, and output is categorized as conclusion, definition, subsumption, or other.\nGreek Legal Code (GLC). Legal documents can cover a wide variety of topics, which makes accurate topic classification all the more important. Papaloukas et al. (2021) compiled a dataset for topic classification of Greek legislation documents. The\ndocuments cover 47 main thematic topics which are called volumes. Each of them is divided into thematic sub categories which are called chapters and subsequently, each chapter breaks down to subjects. Therefore, the dataset is used to perform three different SLTC tasks along volume level (GLC-V), chapter level (GLC-C), and subject level (GLC-S). The input to the models is the entire document, and the output is one of the several topic categories.\nSwiss Judgment Prediction (SJP). Niklaus et al. (2021, 2022), focus on predicting the judgment outcome of 85K cases from the Swiss Federal Supreme Court (FSCS). The input to the models is the appeal description, and the output is whether the appeal is approved or dismissed (SLTC task).\nOnline Terms of Service (OTS). While multilingualism\u2019s benefits (e.g., cultural diversity) in the EU legal world are well-known (Commission, 2005), creating an official version of every legal act in 24 languages raises interpretative challenges. Drawzeski et al. (2021) attempt to automatically detect unfair clauses in terms of service documents. We use their dataset of 100 contracts to perform a SLTC and MLTC task. For the SLTC task (OTSUL), model inputs are sentences, and outputs are classifications into three unfairness levels: clearly fair, potentially unfair and clearly unfair. The MLTC task (OTS-CT) involves identifying sentences based on nine clause topics.\nCOVID19 Emergency Event (C19). The COVID-19 pandemic showed various exceptional measures governments worldwide have taken to contain the virus. Tziafas et al. (2021), presented a dataset, also known as EXCEPTIUS, that contains legal documents with sentence-level annotation from several European countries to automatically identify the measures. We use their dataset to perform the MLTC task of identifying the type of measure described in a sentence. The input to the models are the sentences, and the output is neither or at least one of the measurement types.\nMultiEURLEX (MEU). Multilingual transfer learning has gained significant attention recently due to its increasing applications in NLP tasks. Chalkidis et al. (2021a) explored the cross-lingual transfer for legal NLP and presented a corpus of 65K EU laws annotated with multiple labels from the EUROVOC taxonomy. We perform a MLTC task to identify labels (given in the taxonomy) for\neach document. Since the taxonomy exists on multiple levels, we prepare configurations according to three levels (MEU-1, MEU-2, MEU-3).\nGreek Legal NER (GLN). Identifying various named entities from natural language text plays an important role for Natural Language Understanding (NLU). Angelidis et al. (2018) compiled an annotated dataset for NER in Greek legal documents. The source material are 254 daily issues of the Greek Government Gazette over the period 2000-2017. In all NER tasks of LEXTREME the input to the models is the list of tokens, and the output is an entity label for each token.\nLegalNERo (LNR). Similar to GLN, Pais et al. (2021) manually annotated Romanian legal documents for various named entities. The dataset is derived from 370 documents from the larger MARCELL Romanian legislative subcorpus.4\nLeNER BR (LNB). Luz de Araujo et al. (2018) compiled a dataset for NER for Brazilian legal documents. 66 legal documents from several Brazilian Courts and four legislation documents were collected, resulting in a total of 70 documents annotated for named entities.\nMAPA (MAP). de Gibert et al. (2022) built a multilingual corpus based on EUR-Lex (Baisa et al., 2016) for NER annotated at a coarse-grained (MAP-C) and fine-grained (MAP-F) level."
        },
        {
            "heading": "4 Models Considered",
            "text": "Since our benchmark only contains NLU tasks, we consider encoder-only models for simplicity. Due to resource constraints, we did not fine-tune models larger than 1B parameters."
        },
        {
            "heading": "4.1 Multilingual",
            "text": "We considered the five multilingual models listed in Table 4, trained on at least 100 languages each (more details are in Appendix B). For XLM-R we considered both the base and large version. Furthermore, we used ChatGPT (gpt-3.5-turbo) for zero-shot evaluation of the text classification tasks with less than 50 labels.5 To be fair across tasks we did not consider few-shot evaluation or more\n4https://marcell-project.eu/deliverables.html 5We excluded MultiEurlex because it only contains numeric labels and not textual ones, and because the inputs are very long in 24 languages rendering a valid comparison with reasonable costs impossible.\nsophisticated prompting techniques because of prohibitively long inputs in many tasks."
        },
        {
            "heading": "4.2 Monolingual",
            "text": "In addition to the multilingual models, we also fine-tuned available monolingual models on the language specific subsets. We chose monolingual models only if a certain language was represented in at least three datasets.6 We made a distinction between general purpose models, i.e., models that have been pre-trained on generic data aka NativeBERTs, and legal models, i.e., models that have been trained (primarily) on legal data aka NativeLegalBERTs. A list of the monolingual models can be found in the appendix in Table 8."
        },
        {
            "heading": "4.3 Hierarchical Variants",
            "text": "A significant part of the datasets consists of very long documents, the best examples being all variants of MultiEURLEX; we provide detailed using different tokenizers on all datasets in our online repository. However, the models we evaluated were all pre-trained with a maximum sequence length of only 512 tokens. Directly applying pre-trained language models on lengthy legal documents may necessitate substantial truncation, severely restricting the models. To overcome this limitation, we use hierarchical versions of pre-training models for datasets containing long documents.\nThe hierarchical variants used here are broadly equivalent to those in (Chalkidis et al., 2021b; Niklaus et al., 2022). First, we divide each document into chunks of 128 tokens each. Second, we use the model to be evaluated to encode each of these paragraphs in parallel and to obtain the [CLS] embedding of each chunk which can be used as a context-unaware chunk representation. In order to make them context-aware, i.e. aware of the surrounding chunks, the chunk representations are fed into a 2-layered transformer encoder. Finally, maxpooling over the context-aware paragraph representations is deployed, which results in a document representation that is fed to a classification layer.\nUnfortunately, to the best of our knowledge models capable of handling longer context out of the box, such as Longformer (Beltagy et al., 2020) and SLED (Ivgi et al., 2023) are not available multilingually and predominantly trained on English data only.\n6Which is why we did not include Norwegian pre-trained models, even though Norwegian is covered in C19."
        },
        {
            "heading": "5 Experimental Setup",
            "text": "Multilingual models were fine-tuned on all languages of specific datasets. Monolingual models used only the given model\u2019s language subset.\nSome datasets are highly imbalanced, one of the best examples being BCD-U with a proportion of the minority class of about 2%. Therefore, we applied random oversampling on all SLTC datasets, except for GLC, since all its subsets have too many labels, which would have led to a drastic increase in the data size and thus in the computational costs for fine-tuning. For each run, we used the same hyperparameters, as described in Section A.3.\nAs described in Section 4.3, some tasks contain very long documents, requiring the usage of hierarchical variants to process sequence lenghts of 1024 to 4096 tokens. Based on the distribution of the sequence length per example for each task (cf. Appendix H), we decided on suitable sequence lengths for each task before fine-tuning. A list of suitable sequence lengths are in A.1."
        },
        {
            "heading": "5.1 Evaluation Metrics.",
            "text": "We use the macro-F1 score for all datasets to ensure comparability across the entire benchmark, since it can be computed for both text classification and NER tasks. Mathew\u2019s Correlation Coefficient (MCC) (Matthews, 1975) is a suitable score for evaluating text classification tasks but its applicability to NER tasks is unclear. For brevity, we do not display additional scores, but more detailed (such as precision and recall, and scores per seed) and additional scores (such as MCC) can be found online on our Weights and Biases project.7"
        },
        {
            "heading": "5.2 Aggregate Score",
            "text": "We acknowledge that the datasets included in LEXTREME are diverse and hard to compare due to variations in the number of samples and task complexity (Raji et al., 2021). This is why we always report the scores for each dataset subset, enabling a\n7https://wandb.ai/lextreme/paper_results\nfine-grained analysis. However, we believe that by taking the following three measures, an aggregate score can provide more benefits than drawbacks, encouraging the community to evaluate multilingual legal models on a curated benchmark, thus easing comparisons.\nWe (a) evaluate all datasets with the same score (macro-F1) making aggregation more intuitive and easier to interpret, (b) aggregating the F1 scores again using the harmonic mean, since F1 scores are already rates and obtained using the harmonic mean over precision and recall, following Tatiana and Valentin (2021), and (c) basing our final aggregate score on two intermediate aggregate scores \u2013\u2013 the dataset aggregate and language aggregate score \u2013 thus weighing datasets and languages equally promoting model fairness, following Tatiana and Valentin (2021) and Chalkidis et al. (2022a).\nThe final LEXTREME score is computed using the harmonic mean of the dataset and the language aggregate score. We calculate the dataset aggregate by successively taking the harmonic mean of (i) the languages in the configurations (e.g., de,fr,it in SJP), (ii) configurations within datasets (e.g., OTS-UL, OTS-CT in OTS), and (iii) datasets in LEXTREME (BCD, GAM). The language aggregate score is computed similarly: by taking the harmonic mean of (i) configurations within datasets, (ii) datasets for each language (e.g., MAP, MEU for lv), and (iii) languages in LEXTREME (bg,cs).\nWe do not address the dimension of the jurisdiction, which we consider beyond the scope of this work."
        },
        {
            "heading": "6 Results",
            "text": "In this section, we discuss baseline evaluations. Scores and standard deviations for validation and test datasets across seeds are on our Weights and Biases project or can be found in Table 11, 12, 13, 14. Comparisons with prior results on each dataset can be drawn from the tables provdided in section G in the appendix. Aggregated results by dataset and language are in Tables 5 and 6.\nLarger models are better For both, we see a clear trend that larger models perform better. However, when looking at the individual datasets and languages, the scores are more erratic. Note that XLM-RBase underperforms on MEU (especially on MEU-3; see Table 11 and Table 12) leading to a low dataset aggregate score due to the harmonic mean. Additionally, low performance on MEU-3 has a large impact on its language aggregate score, since it affects all 24 languages.\nDiffering model variance across datasets We observe significant variations across datasets such as GLC, OTS or C19, with differences as large as 52 (in OTS) between the worst-performing MiniLM and the best-performing XLM-R large. MiniLM seems to struggle greatly with these three datasets, while even achieving the best performance on GAM. On other datasets, such as GAM, SJP, and MAP the models are very close together (less than 6 points between best and worst model). Even though XLM-RLarge takes the top spot on aggregate, it only has the best performance in six out of eleven datasets.\nLess variability across languages In contrast to inconsistent results on the datasets, XLM-RLarge outperforms the other multilingual models on most languages (21 out of 24). Additionally, we note that model variability within a language is similar to the variability within a dataset, however, we don\u2019t see extreme cases such as GLC, OTS, or C19.\nMonolingual models are strong Monolingual general-purpose models (NativeBERT in Table 6)\nshow strong performance with only a few exceptions (on Bulgarian, Spanish, Polish, and Slovak). In 13 out of 19 available languages they reach the top performance, leading to the top language aggregate score. The few available models pre-trained on legal data (NativeLegalBERT) slightly outperform multilingual models of the same size.\nChatGPT underperforms We show a comparison of ChatGPT with the best performing multilingual model XLM-RLarge in Table 7. To save costs, we limited the evaluation size to 1000 samples for ChatGPT. We use the validation set instead of the test set to be careful not to leak test data into ChatGPT, possibly affecting future evaluation. Chalkidis (2023) showed that ChatGPT is still outperformed by supervised approaches on LexGLUE. Similarly, we find that much smaller supervised models clearly outperform ChatGPT in all of tested tasks, with very large gaps in GAM and OTS."
        },
        {
            "heading": "7 Conclusions and Future Work",
            "text": "Conclusions We survey the literature and select 11 datasets out of 108 papers with rigorous criteria to compile the first multilingual benchmark for legal NLP. By open-sourcing both the dataset and the code, we invite researchers and practitioners to evaluate any future multilingual models on our benchmark. We provide baselines for five popular multilingual encoder-based language models of different sizes. We hope that this benchmark will foster the creation of novel multilingual legal models and therefore contribute to the progress of natural legal language processing. We imagine this work as a living benchmark and invite the community to extend it with new suitable datasets.\nFuture Work In future work, we plan to extend this benchmark with other NLU tasks and also generation tasks such as summarization, simplification, or translation. Additionally, a deeper analysis of the differences in the behavior of monolingual general-purpose models versus models trained on legal data could provide useful insights for the development of new models. Another relevant aspect that deserves further studies is the impact of the jurisdiction and whether the jurisdiction information is predominantly learned as part of the LLM or is instead learned during fine-tuning. Finally, extending datasets in more languages and evaluating other models such as mT5 (Xue et al., 2021) can be other promising directions."
        },
        {
            "heading": "Acknowledgements",
            "text": "Joel Niklaus is funded by the Swiss National Research Programme \u201cDigital Transformation\u201d (NRP77) grant number 187477. Pooja Rani is funded by the Swiss National Science Foundation with Projects No. 200021_197227. Andrea Galassi is funded by the European Commission\u2019s NextGeneration EU Programme, PNRR - M4C2 - Investimento 1.3, Partenariato Esteso PE00000013 - FAIR - Future Artificial Intelligence Research - Spoke 8 Pervasive AI. Ilias Chalkidis is funded by the Novo Nordisk Foundation (grant NNF 20SA0066568).\nLimitations\nIt is important to not exceed the enthusiasm for language models and the ambitions of benchmarks: many recent works have addressed the limits of these tools and analyzed the consequences of their misuse. For example, Bender and Koller (2020)\nargue that language models do not really learn \u201cmeaning\u201d. Bender et al. (2021) further expand the discussion by addressing the risks related to these technologies and proposing mitigation methods. Koch et al. (2021) evaluate the use of datasets inside scientific communities and highlight that many machine learning sub-communities focus on very few datasets and that often these dataset are \u201cborrowed\u201d from other communities. Raji et al. (2021) offer a detailed exploration of the limits of popular \u201cgeneral\u201d benchmarks, such as GLUE (Wang et al., 2019b) and ImageNET (Deng et al., 2009). Their analysis covers 3 aspects: limited task design, de-contextualized data and performance reporting, inappropriate community use.\nThe first problem concerns the fact that typically tasks are not chosen considering proper theories and selecting what would be needed to prove generality. Instead, they are limited to what is considered interesting by the community, what is available, or other similar criteria. These considerations hold also for our work. Therefore, we cannot claim that our benchmark can be used to assess the \u201cgenerality\u201d of a model or proving that it \u201cunderstands natural legal language\u201d.\nThe second point addresses the fact that any task, data, or metric are limited to their context, therefore \u201cdata benchmarks are closed and inherently subjective, localized constructions\u201d. In particular, the content of the data can be too different from real data and the format of the tasks can be too homogeneous compared to human activities. Moreover, any dataset inherently contains biases. We tackle this limitation by deciding to include only tasks and data that are based on real world scenarios, in an effort to minimize the difference between the performance of a model on our benchmark and its performance on a real world problem.\nThe last aspect regards the negative consequences that benchmarks can have. The competitive testing may encourage misbehavior and the aggregated performance evaluation does create a mirage of cross-domain comparability. The presence of popular benchmarks can influence a scientific community up to the point of steering towards techniques that perform well on that specific benchmark, in disfavor of those that do not. Finally, benchmarks can be misused in marketing to promote commercial products while hiding their flaws. These behaviours obviously cannot be forecasted in advance, but we hope that this analysis of\nthe shortcomings of our work will be sufficient to prevent misuses of our benchmark and will also inspire research directions for complementary future works. For what specifically concerns aggregated evaluations, they provide an intuitive but imprecise understanding of the performance of a model. While we do not deny their potential downsides, we believe that their responsible use is beneficial, especially when compared to the evaluation of a model on only an arbitrarily selected set of datasets. Therefore, we opted to provide an aggregated evaluation and to weigh languages and tasks equally to make it as robust and fair as possible.\nWhile Raji et al. and Koch et al. argument against the misrepresentations and the misuses of benchmarks and datasets, they do not argue against their usefulness. On the contrary, they consider the creation and adoption of novel benchmarks a sign of a healthy scientific community.\nFinally, we want to remark that for many datasets the task of outcome prediction is based not on the document provided by the parties, but on the document provided by the judge along with its decision. For example, Semo et al. (2022) provide a more realistic setup of judgment prediction than other datasets, using actual complaints as inputs. However, due to very limited access to the complaint documents, especially multilingually, creating such datasets is extremely challenging. Thus, most recent works used text from court decisions as proxies. However, predicting the judgment outcome based on text written by the court itself can still be a hard task (as evidenced by results on these datasets). Moreover, it may still require legal reasoning capabilities from models because of the need to pick out the correct information. Additionally, we believe that these tasks can also be interesting to conduct post hoc analyses of decisions.\nEthics Statement\nThe scope of this work is to release a unified multilingual legal NLP benchmark to accelerate the development and evaluation of multilingual legal language models. A transparent multilingual and multinational benchmark for NLP in the legal domain might serve as an orientation for scholars and industry researchers by broadening the discussion and helping practitioners to build assisting technology for legal professionals and laypersons. We believe that this is an important application field, where research should be conducted (Tsarapatsanis\nand Aletras, 2021) to improve legal services and democratize law, while also highlight (inform the audience on) the various multi-aspect shortcomings seeking a responsible and ethical (fair) deployment of legal-oriented technologies.\nNonetheless, irresponsible use (deployment) of such technology is a plausible risk, as in any other application (e.g., online content moderation) and domain (e.g., medical). We believe that similar technologies should only be deployed to assist human experts (e.g., legal scholars in research, or legal professionals in forecasting or assessing legal case complexity) with notices on their limitations.\nAll datasets included in LEXTREME, are publicly available and have been previously published. We referenced the original work and encourage LEXTREME users to do so as well. In fact, we believe this work should only be referenced, in addition to citing the original work, when experimenting with multiple LEXTREME datasets and using the LEXTREME evaluation infrastructure. Otherwise, only the original work should be cited."
        },
        {
            "heading": "A Experiment Details",
            "text": "A.1 Maximum Sequence Lengths\nBrazilian Court Decisions: 1024 (128 x 8) CoVID19: 256 German Argument Mining: 256 Greek Legal Code: 4096 (if speed is important: 2048) (128 x 32 / 16) Greek Legal NER: 512 (max for non-hierarchical) LegalNERo: 512 (max for non-hierarchical) LeNER: 512 (max for non-hierarchical) MAPA: 512 (max for non-hierarchical) MultiEURLEX: 4096 (or for maximum performance 8192) (128 x 32 / 64) Online Terms of Service: 256 Swiss Judgment Prediction: 2048 (or for maximum performance on fr: 4096) (128 x 16 / 32)\nA.2 Total compute\nWe used a total of 689 GPU days.\nA.3 Hyperparameters\nWe used learning rate 1e-5 for all models and datasets without tuning. We ran all experiments with 3 random seeds (1-3). We always used batch size 64. In case the GPU memory was insufficient, we additionally used gradient accumulation. We trained using early stopping on the validation loss with an early-stopping patience of 5 epochs. Because MultiEURLEX is very large and the experiment very long, we just train for 1 epoch and evaluated after every 1000th step when finetuning multilingual models on the entire dataset. For finetuning the monolingual models on languagespecific subsets of MultiEURLEX, we evaluated on the basis of epochs. We used AMP mixed precision training and evaluation to reduce costs. Mixed precision was not used in combination with microsoft/mdeberta-v3-base because it led to errors. For the experiments we used the following NVIDIA GPUs: 24GB RTX3090, 32GB V100 and 80GB A100."
        },
        {
            "heading": "B Model Descriptions",
            "text": "MiniLM. MiniLM (Wang et al., 2020) is the result of a novel task-agnostic compression technique, also called distillation, in which a compact model \u2014 the so-called student \u2014 is trained to reproduce the behaviour of a larger pre-trained model \u2014 the so-called teacher. This is achieved by deep self-attention distillation, i.e. only the self-attention module of the last Transformer layer of the teacher, which stores a lot of contextual information (Jawahar et al., 2019), is distilled. The student is trained by closely imitating the teacher\u2019s final Transformer layer\u2019s self-attention behavior. To aid the learner in developing a better imitation, (Wang et al., 2020) also introduce the self-attention value-relation transfer in addition to the self-attention distributions. The addition of a teacher assistant results in further improvements. For the training of multilingual MiniLM, XLMRBASE was used.\nDistilBERT DistilBERT (Sanh et al., 2019) is a more compressed version of BERT (Devlin et al., 2019) using teacher-student learning, similar to MiniLM. DistilBERT is distilled from BERT, thus both share a similar overall architecture. The pooler and token-type embeddings are eliminated, and the number of layers is decreased by a factor of 2 in DistilBERT. DistilBERT is distilled in very large batches while utilizing gradient accumulation and dynamic masking, but without the next sentence prediction objective. DistilBERT was trained on the same corpus as the original BERT.\nmDEBERTa He et al. (2020) suggest a new model architecture called DeBERTa (Decodingenhanced BERT with disentangled attention), which employs two novel methods to improve the BERT and RoBERTa models. The first is the disentangled attention mechanism, in which each word is represented by two vectors that encode its content and position, respectively, and the attention weights between words are calculated using disentangled matrices on their respective contents and relative positions. To predict the masked tokens during pre-training, an enhanced mask decoder is utilized, which incorporates absolute positions in the decoding layer. Additionally, the generalization of models is enhanced through fine-tuning using a new virtual adversarial training technique. He et al. (2021) introduce mDEBERTa-v3 by further improving the efficiency of pre-training by\nreplacing Masked-Language Modeling (MLM) in DeBERTa with the task of replaced token detection (RTD) where the model is trained to predict whether a token in the corrupted input is either original or replaced by agenerator. Further improvements are achieved via gradient-disentangled embedding sharing (GDES).\nXLM-RoBERTa XLM-R (Conneau et al., 2020) is a multilingual language model which has the same pretraining objectives as RoBERTa (Liu et al., 2019), such as dynamic masking, but not next sentence prediction. It is pre-trained on a large corpus comprising 100 languages. The authors report a significant performance gain over multilingual BERT (mBERT) in a variety of tasks with results competitive with state-of-the-art monolingual models (Conneau et al., 2020).\nC Monolingual Models Overview\n.\nD Dataset Splits\nL an\ngu ag\ne SJ\nP O\nT S-\nU L\nO T\nSC\nT C\n19 M\nE U\n-1 M\nE U\n-2 M\nE U\n-3 M\nA P-\nC M\nA P-\nF\nbg 15\n98 6\n/5 00\n0 /5\n00 0\n15 98\n6 /5\n00 0\n/5 00\n0 15\n98 6\n/5 00\n0 /5\n00 0\n14 11\n/1 66\n/5 60\n14 11\n/1 66\n/5 60\ncs 23\n18 7\n/5 00\n0 /5\n00 0\n23 18\n7 /5\n00 0\n/5 00\n0 23\n18 7\n/5 00\n0 /5\n00 0\n14 64\n/1 76\n/5 63\n14 64\n/1 76\n/5 63\nda 55\n00 0\n/5 00\n0 /5\n00 0\n55 00\n0 /5\n00 0\n/5 00\n0 55\n00 0\n/5 00\n0 /5\n00 0\n14 55\n/1 64\n/5 50\n14 55\n/1 64\n/5 50\nde 35\n45 8\n/4 70\n5 /9\n72 5\n49 1\n/4 2\n/1 03\n44 80\n/4 04\n/1 02\n7 55\n00 0\n/5 00\n0 /5\n00 0\n55 00\n0 /5\n00 0\n/5 00\n0 55\n00 0\n/5 00\n0 /5\n00 0\n14 57\n/1 66\n/5 58\n14 57\n/1 66\n/5 58\nel 55\n00 0\n/5 00\n0 /5\n00 0\n55 00\n0 /5\n00 0\n/5 00\n0 55\n00 0\n/5 00\n0 /5\n00 0\n15 29\n/1 74\n/5 84\n15 29\n/1 74\n/5 84\nen 52\n6 /4\n9 /1\n03 53\n78 /4\n15 /1\n03 8\n64 8\n/8 1\n/8 1\n55 00\n0 /5\n00 0\n/5 00\n0 55\n00 0\n/5 00\n0 /5\n00 0\n55 00\n0 /5\n00 0\n/5 00\n0 89\n3 /9\n8 /4\n08 89\n3 /9\n8 /4 08 es 52 78 5 /5 00 0 /5 00 0 52 78 5 /5 00 0 /5 00 0 52 78 5 /5 00 0 /5 00 0 80 6 /2 48 /1 55 80 6 /2 48 /1 55 et 23 12 6 /5 00 0 /5 00 0 23 12 6 /5 00 0 /5 00 0 23 12 6 /5 00 0 /5 00 0 13 91 /1 63 /5 16 13 91 /1 63 /5 16 fi 42 49 7 /5 00 0 /5 00 0 42 49 7 /5 00 0 /5 00 0 42 49 7 /5 00 0 /5 00 0 13 98 /1 87 /5 31 13 98 /1 87 /5 31 fr 21 17 9 /3 09 5 /6 82 0 14 16 /1 78 /1 78 55 00 0 /5 00 0 /5 00 0 55 00 0 /5 00 0 /5 00 0 55 00 0 /5 00 0 /5 00 0 12 97 /9 7 /4 90 12 97 /9 7 /4 90 ga 13 83 /1 65 /5 15 13 83 /1 65 /5 15 hr 79 44 /2 50 0 /5 00 0 79 44 /2 50 0 /5 00 0 79 44 /2 50 0 /5 00 0 hu 75 /1 0 /1 0 22 66 4 /5 00 0 /5 00 0 22 66 4 /5 00 0 /5 00 0 22 66 4 /5 00 0 /5 00 0 13 90 /1 71 /5 25 13 90 /1 71 /5 25 it 30 72 /4 08 /8 12 51 7 /5 0 /1 02 48 06 /4 32 /1 05 7 74 2 /9 3 /9 3 55 00 0 /5 00 0 /5 00 0 55 00 0 /5 00 0 /5 00 0 55 00 0 /5 00 0 /5 00 0 14 11 /1 62 /5 50 14 11 /1 62 /5 50 lt 23 18 8 /5 00 0 /5 00 0 23 18 8 /5 00 0 /5 00 0 23 18 8 /5 00 0 /5 00 0 14 13 /1 73 /5 48 14 13 /1 73 /5 48 lv 23 20 8 /5 00 0 /5 00 0 23 20 8 /5 00 0 /5 00 0 23 20 8 /5 00 0 /5 00 0 13 83 /1 67 /5 53 13 83 /1 67 /5 53 m t 17 52 1 /5 00 0 /5 00 0 17 52 1 /5 00 0 /5 00 0 17 52 1 /5 00 0 /5 00 0 93 7 /9 3 /4 42 93 7 /9 3 /4 42 nb 22 1 /2 8 /2 8 nl 13 5 /1 8 /1 8 55 00 0 /5 00 0 /5 00 0 55 00 0 /5 00 0 /5 00 0 55 00 0 /5 00 0 /5 00 0 13 91 /1 64 /5 30 13 91 /1 64 /5 30 pl 54 0 /5 0 /1 09 52 78 /4 39 /1 17 5 75 /1 0 /1 0 23 19 7 /5 00 0 /5 00 0 23 19 7 /5 00 0 /5 00 0 23 19 7 /5 00 0 /5 00 0 pt 52 37 0 /5 00 0 /5 00 0 52 37 0 /5 00 0 /5 00 0 52 37 0 /5 00 0 /5 00 0 10 86 /1 05 /3 90 10 86 /1 05 /3 90 ro 15 92 1 /5 00 0 /5 00 0 15 92 1 /5 00 0 /5 00 0 15 92 1 /5 00 0 /5 00 0 14 80 /1 75 /5 57 14 80 /1 75 /5 57 sk 22 97 1 /5 00 0 /5 00 0 22 97 1 /5 00 0 /5 00 0 22 97 1 /5 00 0 /5 00 0 13 95 /1 65 /5 26 13 95 /1 65 /5 26 sl 23 18 4 /5 00 0 /5 00 0 23 18 4 /5 00 0 /5 00 0 23 18 4 /5 00 0 /5 00 0 sv 42 49 0 /5 00 0 /5 00 0 42 49 0 /5 00 0 /5 00 0 42 49 0 /5 00 0 /5 00 0 14 53 /1 75 /5 39 14 53 /1 75 /5 39\nTa bl\ne 9:\nO ve\nrv ie\nw of\nth e\nnu m\nbe ro\nfe xa\nm pl\nes fo\nre ac\nh la\nng ua\nge -s\npe ci\nfic su\nbs et\nof m\nul til\nin gu\nal ta\nsk s.\nT he\nor de\nro ft\nhe va\nlu es\nis tr\nai n\n/v al\nid at\nio n\n/t es\nt.\nL an\ngu ag\ne SJ\nP O\nT S-\nU L\nO T\nSC\nT C\n19 M\nE U\n-1 M\nE U\n-2 M\nE U\n-3 M\nA P-\nC M\nA P-\nF\nbg 21\n/2 1\n/2 1\n12 7\n/1 26\n/1 27\n48 1\n/4 54\n/4 65\n11 /1\n1 /8\n24 /1\n6 /1 3 cs 21 /2 1 /2 1 12 7 /1 26 /1 27 48 6 /4 54 /4 65 11 /1 1 /9 30 /1 7 /1 6 da 21 /2 1 /2 1 12 7 /1 26 /1 27 50 0 /4 54 /4 65 11 /1 0 /1 1 26 /1 4 /1 4 de 2 /2 /2 3 /3 /3 9 /7 /9 21 /2 1 /2 1 12 7 /1 26 /1 27 50 0 /4 54 /4 65 11 /9 /1 0 28 /1 4 /1 4 el 21 /2 1 /2 1 12 7 /1 26 /1 27 50 0 /4 54 /4 65 11 /1 1 /1 1 31 /1 7 /2 0 en 3 /3 /3 9 /8 /9 6 /6 /5 21 /2 1 /2 1 12 7 /1 26 /1 27 50 0 /4 54 /4 65 11 /9 /9 28 /1 7 /1 8 es 21 /2 1 /2 1 12 7 /1 26 /1 27 49 7 /4 54 /4 65 11 /8 /1 1 26 /1 3 /1 8 et 21 /2 1 /2 1 12 7 /1 26 /1 27 48 6 /4 54 /4 65 11 /1 1 /1 1 25 /1 4 /1 7 fi 21 /2 1 /2 1 12 7 /1 26 /1 27 49 3 /4 54 /4 65 11 /1 1 /1 0 24 /1 9 /1 6 fr 2 /2 /2 8 /8 /7 21 /2 1 /2 1 12 7 /1 26 /1 27 50 0 /4 54 /4 65 11 /1 1 /1 1 32 /1 9 /2 6 ga 13 /1 1 /1 1 33 /1 7 /1 8 hr 21 /2 1 /2 1 12 7 /1 26 /1 27 46 9 /4 37 /4 65 hu 4 /1 /1 21 /2 1 /2 1 12 7 /1 26 /1 27 48 6 /4 54 /4 65 11 /1 0 /1 0 20 /1 5 /1 4 it 2 /2 /2 3 /3 /3 9 /8 /9 7 /7 /6 21 /2 1 /2 1 12 7 /1 26 /1 27 50 0 /4 54 /4 65 11 /1 0 /1 1 25 /1 5 /1 6 lt 21 /2 1 /2 1 12 7 /1 26 /1 27 48 6 /4 54 /4 65 11 /1 1 /1 0 28 /1 9 /2 1 lv 21 /2 1 /2 1 12 7 /1 26 /1 27 48 6 /4 54 /4 65 11 /1 1 /1 1 31 /1 5 /2 1 m t 21 /2 1 /2 1 12 7 /1 26 /1 27 48 5 /4 54 /4 65 11 /1 1 /1 1 27 /1 5 /1 5 nb 7 /5 /6 nl 2 /2 /2 21 /2 1 /2 1 12 7 /1 26 /1 27 50 0 /4 54 /4 65 10 /9 /1 0 25 /1 2 /1 4 pl 3 /3 /3 9 /8 /9 7 /5 /3 21 /2 1 /2 1 12 7 /1 26 /1 27 48 6 /4 54 /4 65 pt 21 /2 1 /2 1 12 7 /1 26 /1 27 49 7 /4 54 /4 65 11 /1 0 /1 1 29 /1 4 /1 8 ro 21 /2 1 /2 1 12 7 /1 26 /1 27 48 1 /4 54 /4 65 11 /1 1 /1 1 25 /1 6 /1 8 sk 21 /2 1 /2 1 12 7 /1 26 /1 27 48 5 /4 54 /4 65 11 /1 1 /1 1 25 /1 6 /1 8 sl 21 /2 1 /2 1 12 7 /1 26 /1 27 48 6 /4 54 /4 65 sv 21 /2 1 /2 1 12 7 /1 26 /1 27 49 3 /4 54 /4 65 11 /1 1 /1 0 23 /1 5 /1 5\nTa bl\ne 10\n:O ve\nrv ie\nw of\nth e\nnu m\nbe ro\nfl ab\nel s\nfo re\nac h\nla ng\nua ge\n-s pe\nci fic\nsu bs\net of\nm ul\ntil in\ngu al\nta sk\ns. T\nhe or\nde ro\nft he\nva lu\nes is\ntr ai\nn /v\nal id\nat io\nn /t\nes t.\nE Detailed Multilingual Results\nM ea\nn B\nC D\n-J B\nC D\n-U G\nA M\nG L\nC -V\nG L\nC -C\nG L\nC -S\nSJ P\nO T\nSU\nL O\nT S-\nC T\nC 19\nM E\nU -1\nM E\nU -2\nM E\nU -3\nG L\nN L\nN R\nL N\nB M\nA P-\nC M\nA PF M od el M in iL M 51 .0 52 .8 (\u00b1 6. 7) 55 .1 (\u00b1 6. 6) 72 .1 (\u00b1 0. 9) 82 .0 (\u00b1 1. 0) 39 .4 (\u00b1 1. 0) 5. 1 (\u00b1 1. 6) 68 .9 (\u00b1 0. 7) 71 .0 (\u00b1 5. 0) 15 .3 (\u00b1 3. 4) 5. 8 (\u00b1 1. 5) 64 .8 (\u00b1 0. 3) 23 .5 (\u00b1 0. 6) 6. 5 (\u00b1 0. 3) 66 .6 (\u00b1 1. 4) 62 .9 (\u00b1 6. 4) 98 .2 (\u00b1 0. 1) 79 .0 (\u00b1 0. 3) 49 .3 (\u00b1\n1. 4)\nD is\ntil B\nE R\nT 59\n.2 52\n.1 (\u00b1\n4. 5)\n60 .0\n(\u00b1 9.\n8) 70\n.6 (\u00b1\n1. 7)\n84 .9\n(\u00b1 0.\n5) 68\n.0 (\u00b1\n0. 6)\n33 .9\n(\u00b1 2.\n0) 68\n.7 (\u00b1\n0. 7)\n66 .9\n(\u00b1 3.\n4) 49\n.6 (\u00b1\n9. 1)\n41 .4\n(\u00b1 5.\n6) 68\n.2 (\u00b1\n0. 1)\n37 .3\n(\u00b1 0.\n3) 13\n.8 (\u00b1\n0. 6)\n62 .4\n(\u00b1 4.\n5) 69\n.9 (\u00b1\n8. 9)\n93 .0\n(\u00b1 2.\n6) 76\n.3 (\u00b1\n1. 2)\n48 .6\n(\u00b1 3. 2) m D eB E R Ta v3 60 .1 70 .4 (\u00b1 0. 6) 67 .5 (\u00b1 6. 1) 70 .0 (\u00b1 1. 1) 85 .0 (\u00b1 0. 8) 58 .2 (\u00b1 7. 5) 12 .3 (\u00b1 2. 5) 71 .2 (\u00b1 0. 7) 85 .2 (\u00b1 2. 9) 52 .1 (\u00b1 4. 6) 43 .4 (\u00b1 4. 3) 68 .4 (\u00b1 0. 6) 36 .4 (\u00b1 0. 9) 14 .0 (\u00b1 1. 3) 63 .7 (\u00b1 4. 6) 63 .9 (\u00b1 8. 5) 96 .2 (\u00b1 1. 7) 74 .3 (\u00b1 2. 9) 49 .8 (\u00b1 2. 4) X L M -R B as e 59 .0 67 .5 (\u00b1 2. 2) 63 .4 (\u00b1 12 .3 ) 72 .5 (\u00b1 1. 9) 85 .6 (\u00b1 0. 2) 69 .1 (\u00b1 0. 6) 15 .7 (\u00b1 12 .7 ) 69 .6 (\u00b1 0. 9) 72 .6 (\u00b1 4. 2) 52 .4 (\u00b1 6. 0) 44 .1 (\u00b1 7. 9) 69 .2 (\u00b1 0. 1) 32 .2 (\u00b1 1. 7) 5. 4 (\u00b1 0. 5) 64 .2 (\u00b1 1. 7) 57 .0 (\u00b1 3. 5) 96 .4 (\u00b1 0. 9) 73 .4 (\u00b1 1. 9) 51 .1 (\u00b1 2. 7) X L M -R L ar ge 63 .6 58 .1 (\u00b1 9. 3) 70 .4 (\u00b1 3. 7) 73 .0 (\u00b1 1. 4) 58 .2 (\u00b1 50 .2 ) 73 .0 (\u00b1 0. 9) 38 .9 (\u00b1 33 .7 ) 70 .0 (\u00b1 1. 8) 84 .9 (\u00b1 2. 7) 62 .9 (\u00b1 6. 1) 53 .8 (\u00b1 10 .5 ) 71 .2 (\u00b1 1. 4) 47 .6 (\u00b1 0. 4) 15 .3 (\u00b1 0. 8) 63 .0 (\u00b1 4. 0) 75 .2 (\u00b1 3. 0) 96 .6 (\u00b1 1. 1) 77 .1 (\u00b1 2. 0) 55 .8 (\u00b1 3. 6)\nTa bl\ne 11\n:A ri\nth m\net ic\nm ea\nn of\nm ac\nro -F\n1 an\nd th\ne st\nan da\nrd de\nvi at\nio n\nov er\nal ls\nee ds\nfo rm\nul til\nin gu\nal m\nod el\ns fr\nom th\ne va\nlid at\nio n\nse t.\nT he\nbe st\nsc or\nes ar\ne in\nbo ld\n.\nM ea\nn B\nC D\n-J B\nC D\n-U G\nA M\nG L\nC -V\nG L\nC -C\nG L\nC -S\nSJ P\nO T\nSU\nL O\nT S-\nC T\nC 19\nM E\nU -1\nM E\nU -2\nM E\nU -3\nG L\nN L\nN R\nL N\nB M\nA P-\nC M\nA PF M od el M in iL M 46 .4 49 .4 (\u00b1 7. 4) 56 .7 (\u00b1 7. 9) 73 .3 (\u00b1 0. 9) 81 .7 (\u00b1 0. 5) 39 .4 (\u00b1 1. 4) 5. 2 (\u00b1 1. 6) 67 .6 (\u00b1 1. 2) 74 .6 (\u00b1 1. 1) 14 .1 (\u00b1 3. 1) 6. 0 (\u00b1 1. 9) 62 .0 (\u00b1 0. 4) 21 .7 (\u00b1 0. 5) 5. 6 (\u00b1 0. 3) 43 .6 (\u00b1 2. 4) 46 .5 (\u00b1 1. 5) 86 .1 (\u00b1 0. 3) 62 .7 (\u00b1 1. 7) 39 .9 (\u00b1\n2. 3)\nD is\ntil B\nE R\nT 53\n.5 50\n.3 (\u00b1\n2. 9)\n58 .8\n(\u00b1 8.\n7) 69\n.5 (\u00b1\n0. 9)\n85 .2\n(\u00b1 0.\n8) 70\n.0 (\u00b1\n0. 3)\n33 .2\n(\u00b1 1.\n9) 66\n.7 (\u00b1\n1. 1)\n67 .2\n(\u00b1 4.\n1) 46\n.2 (\u00b1\n8. 9)\n39 .5\n(\u00b1 6.\n3) 63\n.6 (\u00b1\n0. 1)\n33 .6\n(\u00b1 0.\n5) 12\n.0 (\u00b1\n0. 8)\n38 .1\n(\u00b1 2.\n0) 48\n.4 (\u00b1\n5. 2)\n78 .7\n(\u00b1 1.\n1) 61\n.3 (\u00b1\n2. 8)\n40 .6\n(\u00b1 0. 7) m D eB E R Ta v3 55 .2 67 .2 (\u00b1 1. 9) 53 .2 (\u00b1 6. 7) 71 .3 (\u00b1 0. 3) 85 .6 (\u00b1 1. 0) 58 .6 (\u00b1 7. 8) 12 .4 (\u00b1 2. 8) 69 .0 (\u00b1 0. 8) 79 .7 (\u00b1 3. 8) 53 .8 (\u00b1 3. 0) 40 .7 (\u00b1 5. 0) 65 .0 (\u00b1 0. 4) 34 .1 (\u00b1 1. 0) 13 .1 (\u00b1 1. 0) 45 .1 (\u00b1 3. 9) 46 .7 (\u00b1 0. 7) 87 .3 (\u00b1 1. 1) 65 .6 (\u00b1 4. 7) 45 .7 (\u00b1 1. 0) X L M -R B as e 55 .6 65 .4 (\u00b1 3. 6) 61 .6 (\u00b1 11 .2 ) 72 .0 (\u00b1 2. 4) 86 .1 (\u00b1 0. 4) 70 .7 (\u00b1 1. 0) 15 .4 (\u00b1 12 .3 ) 68 .3 (\u00b1 1. 0) 80 .8 (\u00b1 1. 9) 55 .9 (\u00b1 2. 6) 45 .9 (\u00b1 11 .0 ) 65 .6 (\u00b1 0. 1) 29 .8 (\u00b1 1. 5) 4. 7 (\u00b1 0. 4) 46 .4 (\u00b1 1. 9) 45 .6 (\u00b1 0. 6) 87 .4 (\u00b1 1. 0) 58 .0 (\u00b1 2. 4) 41 .8 (\u00b1 2. 4) X L M -R L ar ge 58 .9 55 .1 (\u00b1 7. 6) 62 .3 (\u00b1 3. 6) 73 .1 (\u00b1 1. 5) 58 .3 (\u00b1 50 .3 ) 74 .7 (\u00b1 0. 9) 39 .1 (\u00b1 33 .9 ) 68 .3 (\u00b1 1. 8) 83 .6 (\u00b1 4. 8) 66 .9 (\u00b1 0. 5) 54 .2 (\u00b1 7. 2) 68 .1 (\u00b1 1. 2) 44 .4 (\u00b1 0. 3) 14 .2 (\u00b1 0. 8) 45 .3 (\u00b1 3. 1) 55 .8 (\u00b1 5. 9) 88 .4 (\u00b1 1. 2) 64 .0 (\u00b1 2. 5) 43 .7 (\u00b1 1. 1)\nTa bl\ne 12\n:A ri\nth m\net ic\nm ea\nn of\nm ac\nro -F\n1 an\nd th\ne st\nan da\nrd de\nvi at\nio n\nov er\nal ls\nee ds\nfo rm\nul til\nin gu\nal m\nod el\ns fr\nom th\ne te\nst se\nt. T\nhe be\nst sc\nor es\nar e\nin bo\nld .\nF Detailed Monolingual Results\nM ea\nn B\nC D\n-J B\nC D\n-U G\nA M\nG L\nC -V\nG L\nC -C\nG L\nC -S\nSJ P\nO T\nSU\nL O\nT S-\nC T\nC 19\nM E\nU -1\nM E\nU -2\nM E\nU -3\nG L\nN L\nN R\nL N\nB M\nA P-\nC M\nA PF M od el ia rf m oo se /r ob er ta -b as ebu lg ar ia n 52 .3 - - - - - - - - - - 67 .4 (\u00b1 0. 7) 35 .7 (\u00b1 2. 1) 3. 7 (\u00b1 5. 2) - - - 82 .8 (\u00b1 2. 1) 72 .0\n(\u00b1 2. 8) U W B -A IR /C ze rt -B -b as eca se d 61 .7 - - - - - - - - - - 69 .1 (\u00b1 1. 1) 51 .9 (\u00b1 0. 6) 27 .3 (\u00b1 0. 8) - - - 92 .2 (\u00b1 0. 8) 67 .8 (\u00b1 3. 1) M al te hb /d an is hbe rt -b ot xo 57 .7 - - - - - - - - - - 69 .5 (\u00b1 1. 7) 46 .7 (\u00b1 2. 7) 25 .8 (\u00b1 1. 0) - - - 89 .2 (\u00b1 3. 5) 57 .3 (\u00b1 5. 4) db m dz /b er tba se -g er m an -c as ed 63 .2 - - 72 .8 (\u00b1 0. 9) - - - 70 .9 (\u00b1 1. 1) 76 .3 (\u00b1 5. 4) 46 .2 (\u00b1 1. 9) - 71 .8 (\u00b1 1. 6) 51 .1 (\u00b1 1. 4) 27 .2 (\u00b1 0. 1) - - - 85 .5 (\u00b1 8. 6) 67 .3 (\u00b1 4. 0) de ep se t/g be rt -b as e 62 .3 - - 74 .6 (\u00b1 1. 0) - - - 71 .5 (\u00b1 1. 2) 89 .0 (\u00b1 7. 1) 50 .4 (\u00b1 1. 2) - 71 .6 (\u00b1 1. 3) 49 .9 (\u00b1 0. 2) 25 .9 (\u00b1 0. 5) - - - 74 .4 (\u00b1 6. 6) 53 .2 (\u00b1 9. 2) nl pa ue b/ be rt -b as egr ee kun ca se dv1 66 .1 - - - 87 .9 (\u00b1 0. 4) 74 .5 (\u00b1 0. 3) 61 .7 (\u00b1 0. 3) - - - - 71 .0 (\u00b1 0. 4) 48 .6 (\u00b1 0. 1) 25 .9 (\u00b1 0. 2) 65 .4 (\u00b1 3. 2) - - 98 .2 (\u00b1 1. 3) 61 .3 (\u00b1 1. 4) ro be rt aba se 44 .2 - - - - - - - 92 .3 (\u00b1 0. 3) 65 .6 (\u00b1 6. 9) 35 .0 (\u00b1 2. 5) 48 .5 (\u00b1 1. 0) 0. 0 (\u00b1 0. 0) 0. 0 (\u00b1 0. 0) - - - 75 .4 (\u00b1 2. 2) 37 .2 (\u00b1 6. 7) be rt in -p ro je ct /b er tin -r ob er ta -b as esp an is h 44 .4 - - - - - - - - - - 66 .7 (\u00b1 1. 2) 33 .4 (\u00b1 2. 7) 17 .4 (\u00b1 2. 1) - - - 46 .8 (\u00b1 14 .6 ) 57 .6 (\u00b1 5. 5) Pl an T L -G O B -E S/ ro be rt aba se -b ne 52 .4 - - - - - - - - - - 67 .5 (\u00b1 0. 4) 32 .6 (\u00b1 2. 4) 15 .1 (\u00b1 0. 9) - - - 77 .7 (\u00b1 14 .3 ) 69 .3 (\u00b1 12 .5 ) ta rt uN L P/ E st B E R T 58 .3 - - - - - - - - - - 67 .7 (\u00b1 1. 5) 47 .7 (\u00b1 0. 4) 24 .6 (\u00b1 0. 7) - - - 90 .9 (\u00b1 2. 0) 60 .5 (\u00b1 7. 1) Tu rk uN L P/ be rt -b as efin ni sh -c as ed -v 1 61 .8 - - - - - - - - - - 70 .4 (\u00b1 1. 4) 49 .6 (\u00b1 1. 0) 27 .7 (\u00b1 1. 2) - - - 94 .2 (\u00b1 4. 0) 67 .2 (\u00b1 6. 6) ca m em be rt -b as e 30 .8 - - - - - - 68 .8 (\u00b1 1. 3) - - 16 .7 (\u00b1 1. 7) 69 .6 (\u00b1 0. 2) 41 .9 (\u00b1 3. 6) 18 .6 (\u00b1 2. 9) - - - 0. 0 (\u00b1 0. 0) 0. 0 (\u00b1 0. 0) db m dz /b er tba se -f re nc heu ro pe an aca se d 57 .1 - - - - - - 70 .2 (\u00b1 0. 6) - - 36 .2 (\u00b1 5. 6) 70 .1 (\u00b1 1. 2) 48 .9 (\u00b1 0. 0) 25 .7 (\u00b1 0. 7) - - - 84 .5 (\u00b1 6. 6) 63 .9 (\u00b1 5. 6) D C U -N L P/ be rt -b as eir is hca se dv1 72 .3 - - - - - - - - - - - - - - - - 79 .6 (\u00b1 0. 4) 65 .1 (\u00b1 3. 5) SZ TA K IH LT /h ub er tba se -c c 50 .6 - - - - - - - - - 0. 0 (\u00b1 0. 0) 69 .5 (\u00b1 2. 5) 53 .1 (\u00b1 1. 6) 27 .9 (\u00b1 2. 1) - - - 91 .2 (\u00b1 1. 5) 62 .0 (\u00b1 1. 3) M us ix m at ch /u m be rt oco m m on cr aw lca se dv1 38 .5 - - - - - - 57 .1 (\u00b1 5. 9) 90 .7 (\u00b1 7. 3) 42 .1 (\u00b1 4. 8) 13 .5 (\u00b1 0. 9) 69 .5 (\u00b1 0. 9) 43 .5 (\u00b1 1. 5) 21 .7 (\u00b1 0. 7) - - - 8. 0 (\u00b1 7. 3) 0. 0 (\u00b1 0. 0) db m dz /b er tba se -i ta lia nca se d 53 .6 - - - - - - 56 .5 (\u00b1 3. 6) 57 .2 (\u00b1 1. 1) 51 .5 (\u00b1 2. 8) 31 .5 (\u00b1 2. 1) 70 .9 (\u00b1 1. 3) 48 .5 (\u00b1 0. 5) 25 .4 (\u00b1 0. 4) - - - 87 .5 (\u00b1 3. 8) 53 .2 (\u00b1 3. 8) G ro N L P/ be rt -b as edu tc hca se d 48 .7 - - - - - - - - - 2. 1 (\u00b1 1. 8) 70 .1 (\u00b1 1. 3) 51 .8 (\u00b1 0. 6) 26 .5 (\u00b1 0. 6) - - - 86 .2 (\u00b1 11 .3 ) 55 .5 (\u00b1 6. 5) pd el ob el le /r ob be rt -v 2- du tc hba se 42 .1 - - - - - - - - - 0. 0 (\u00b1 0. 0) 69 .7 (\u00b1 0. 4) 42 .1 (\u00b1 3. 0) 16 .7 (\u00b1 0. 0) - - - 77 .9 (\u00b1 0. 8) 46 .0 (\u00b1 1. 1) dk le cz ek /b er tba se -p ol is hun ca se dv1 50 .6 - - - - - - - 73 .0 (\u00b1 19 .0 ) 54 .9 (\u00b1 1. 1) 29 .2 (\u00b1 3. 6) 69 .9 (\u00b1 1. 8) 50 .0 (\u00b1 2. 8) 26 .4 (\u00b1 0) - - - - - ne ur al m in d/ be rt -b as epo rt ug ue se -c as ed 65 .8 66 .7 (\u00b1 4. 1) 71 .2 (\u00b1 3. 6) - - - - - - - - 68 .7 (\u00b1 3. 9) 48 .4 (\u00b1 0. 9) 26 .2 (\u00b1 1. 1) - - 96 .6 (\u00b1 0. 0) 96 .2 (\u00b1 3. 5) 52 .7 (\u00b1 8. 3) du m itr es cu st ef an /b er tba se -r om an ia nun ca se dv1 59 .9 - - - - - - - - - - 70 .0 (\u00b1 0. 4) 51 .3 (\u00b1 0. 6) 26 .8 (\u00b1 0) - 55 .1 (\u00b1 0. 7) - 86 .8 (\u00b1 2. 1) 69 .5 (\u00b1 1. 4) ge ru la ta /s lo va kb er t 47 .5 - - - - - - - - - - 70 .1 (\u00b1 0. 9) 41 .7 (\u00b1 2. 0) 13 .0 (\u00b1 0. 6) - - - 74 .3 (\u00b1 1. 6) 38 .2 (\u00b1 4. 3) K B /b er tba se -s w ed is hca se d 63 .1 - - - - - - - - - - 70 .8 (\u00b1 1. 8) 51 .8 (\u00b1 0. 3) 27 .8 (\u00b1 0. 8) - - - 95 .8 (\u00b1 1. 6) 69 .5 (\u00b1 4. 3) zl uc ia /c us to m -l eg al be rt 55 .0 - - - - - - - 77 .8 (\u00b1 16 .9 ) 68 .1 (\u00b1 3. 5) 33 .1 (\u00b1 1. 9) 68 .6 (\u00b1 2. 0) 45 .7 (\u00b1 5. 3) 24 .5 (\u00b1 0. 4) - - - 74 .3 (\u00b1 2. 0) 48 .3 (\u00b1 10 .6 ) nl pa ue b/ le ga lbe rt -b as eun ca se d 56 .3 - - - - - - - 93 .4 (\u00b1 1. 1) 67 .8 (\u00b1 3. 6) 29 .4 (\u00b1 3. 0) 71 .2 (\u00b1 0. 5) 50 .0 (\u00b1 1. 9) 25 .9 (\u00b1 0. 5) - - - 78 .0 (\u00b1 0. 5) 35 .0 (\u00b1 7. 6) Pl an T L -G O B -E S/ R oB E R Ta le x 52 .8 - - - - - - - - - - 69 .7 (\u00b1 1. 1) 45 .5 (\u00b1 1. 6) 21 .9 (\u00b1 0) - - - 75 .8 (\u00b1 1. 6) 51 .3 (\u00b1 2. 4) dl ic ar i/I ta lia nL eg al -B E R T 45 .4 - - - - - - 56 .8 (\u00b1 5. 9) 61 .7 (\u00b1 8. 5) 41 .2 (\u00b1 5. 9) 24 .6 (\u00b1 2. 8) 67 .0 (\u00b1 1. 2) 42 .6 (\u00b1 0. 6) 20 .3 (\u00b1 0. 7) - - - 62 .8 (\u00b1 10 .4 ) 31 .9 (\u00b1 17 .5 ) re ad er be nc h/ ju rB E R Tba se 51 .9 - - - - - - - - - - 69 .1 (\u00b1 1. 6) 48 .2 (\u00b1 2. 6) 23 .2 (\u00b1 0. 4) - 52 .7 (\u00b1 7. 8) - 76 .8 (\u00b1 14 .0 ) 41 .2 (\u00b1 6. 7)\nTa bl\ne 13\n:A ri\nth m\net ic\nm ea\nn of\nm ac\nro -F\n1 an\nd th\ne st\nan da\nrd de\nvi at\nio n\nov er\nal ls\nee ds\nfo rm\non ol\nin gu\nal m\nod el\ns fr\nom th\ne va\nlid at\nio n\nse t.\nM ea\nn B\nC D\n-J B\nC D\n-U G\nA M\nG L\nC -V\nG L\nC -C\nG L\nC -S\nSJ P\nO T\nSU\nL O\nT S-\nC T\nC 19\nM E\nU -1\nM E\nU -2\nM E\nU -3\nG L\nN L\nN R\nL N\nB M\nA P-\nC M\nA PF M od el ia rf m oo se /r ob er ta -b as ebu lg ar ia n 46 .7 - - - - - - - - - - 63 .1 (\u00b1 0. 1) 32 .6 (\u00b1 1. 8) 3. 2 (\u00b1 4. 5) - - - 62 .5 (\u00b1 3. 6) 72 .1\n(\u00b1 8. 2) U W B -A IR /C ze rt -B -b as eca se d 51 .6 - - - - - - - - - - 65 .4 (\u00b1 1. 0) 47 .9 (\u00b1 0. 1) 25 .2 (\u00b1 0. 4) - - - 64 .8 (\u00b1 7. 8) 54 .9 (\u00b1 4. 2) M al te hb /d an is hbe rt -b ot xo 46 .7 - - - - - - - - - - 65 .8 (\u00b1 1. 6) 43 .3 (\u00b1 2. 7) 24 .3 (\u00b1 0. 8) - - - 54 .0 (\u00b1 0. 8) 46 .2 (\u00b1 5. 5) db m dz /b er tba se -g er m an -c as ed 56 .1 - - 72 .6 (\u00b1 1. 4) - - - 68 .7 (\u00b1 1. 0) 68 .8 (\u00b1 3. 5) 54 .9 (\u00b1 4. 0) - 68 .1 (\u00b1 0. 9) 47 .6 (\u00b1 1. 1) 25 .0 (\u00b1 0. 1) - - - 57 .2 (\u00b1 4. 2) 41 .7 (\u00b1 2. 1) de ep se t/g be rt -b as e 57 .2 - - 75 .1 (\u00b1 1. 3) - - - 69 .3 (\u00b1 0. 7) 74 .2 (\u00b1 0. 4) 50 .8 (\u00b1 3. 8) - 67 .9 (\u00b1 0. 8) 45 .7 (\u00b1 0. 6) 23 .6 (\u00b1 0. 6) - - - 58 .6 (\u00b1 1. 2) 49 .4 (\u00b1 2. 9) nl pa ue b/ be rt -b as egr ee kun ca se dv1 56 .0 - - - 88 .1 (\u00b1 0. 6) 76 .5 (\u00b1 0. 7) 62 .8 (\u00b1 0. 4) - - - - 67 .6 (\u00b1 0. 1) 45 .7 (\u00b1 0. 2) 23 .5 (\u00b1 0. 5) 47 .0 (\u00b1 4. 3) - - 54 .6 (\u00b1 4. 9) 38 .3 (\u00b1 0. 2) ro be rt aba se 41 .9 - - - - - - - 67 .2 (\u00b1 10 .3 ) 69 .5 (\u00b1 0. 7) 37 .0 (\u00b1 1. 9) 47 .5 (\u00b1 0. 9) 0. 0 (\u00b1 0. 0) 0. 0 (\u00b1 0. 0) - - - 66 .2 (\u00b1 4. 7) 48 .0 (\u00b1 4. 7) be rt in -p ro je ct /b er tin -r ob er ta -b as esp an is h 41 .1 - - - - - - - - - - 63 .4 (\u00b1 0. 7) 30 .1 (\u00b1 2. 3) 15 .4 (\u00b1 1. 8) - - - 42 .9 (\u00b1 13 .3 ) 53 .6 (\u00b1 5. 2) Pl an T L -G O B -E S/ ro be rt aba se -b ne 41 .8 - - - - - - - - - - 64 .3 (\u00b1 0. 5) 28 .8 (\u00b1 2. 2) 13 .1 (\u00b1 1. 0) - - - 52 .3 (\u00b1 10 .2 ) 50 .3 (\u00b1 2. 5) ta rt uN L P/ E st B E R T 42 .9 - - - - - - - - - - 64 .0 (\u00b1 1. 3) 43 .0 (\u00b1 0. 6) 21 .9 (\u00b1 0. 8) - - - 36 .4 (\u00b1 1. 9) 49 .1 (\u00b1 6. 6) Tu rk uN L P/ be rt -b as efin ni sh -c as ed -v 1 47 .2 - - - - - - - - - - 66 .8 (\u00b1 1. 0) 45 .7 (\u00b1 0. 3) 25 .1 (\u00b1 1. 5) - - - 53 .3 (\u00b1 0. 4) 45 .0 (\u00b1 7. 7) ca m em be rt -b as e 29 .3 - - - - - - 69 .7 (\u00b1 1. 4) - - 13 .7 (\u00b1 1. 2) 66 .2 (\u00b1 0. 4) 38 .4 (\u00b1 2. 9) 17 .4 (\u00b1 2. 6) - - - 0. 0 (\u00b1 0. 0) 0. 0 (\u00b1 0. 0) db m dz /b er tba se -f re nc heu ro pe an aca se d 47 .1 - - - - - - 70 .2 (\u00b1 1. 4) - - 36 .0 (\u00b1 1. 9) 65 .7 (\u00b1 1. 1) 45 .2 (\u00b1 0. 4) 23 .7 (\u00b1 0. 7) - - - 52 .0 (\u00b1 3. 4) 36 .7 (\u00b1 3. 3) D C U -N L P/ be rt -b as eir is hca se dv1 44 .1 - - - - - - - - - - - - - - - - 42 .3 (\u00b1 1. 3) 46 .0 (\u00b1 0. 4) SZ TA K IH LT /h ub er tba se -c c 41 .2 - - - - - - - - - 0. 0 (\u00b1 0. 0) 66 .1 (\u00b1 2. 1) 48 .1 (\u00b1 0. 4) 24 .7 (\u00b1 1. 6) - - - 49 .0 (\u00b1 2. 9) 59 .6 (\u00b1 0. 4) M us ix m at ch /u m be rt oco m m on cr aw lca se dv1 36 .1 - - - - - - 57 .4 (\u00b1 1. 1) 76 .9 (\u00b1 3. 2) 40 .9 (\u00b1 2. 8) 16 .3 (\u00b1 0. 5) 65 .9 (\u00b1 1. 1) 39 .9 (\u00b1 1. 5) 20 .0 (\u00b1 0. 7) - - - 7. 7 (\u00b1 7. 0) 0. 0 (\u00b1 0. 0) db m dz /b er tba se -i ta lia nca se d 48 .1 - - - - - - 57 .3 (\u00b1 3. 4) 69 .8 (\u00b1 1. 6) 47 .8 (\u00b1 5. 2) 29 .5 (\u00b1 3. 2) 67 .5 (\u00b1 1. 1) 45 .4 (\u00b1 0. 3) 23 .1 (\u00b1 0. 4) - - - 47 .7 (\u00b1 1. 8) 45 .1 (\u00b1 5. 0) G ro N L P/ be rt -b as edu tc hca se d 40 .0 - - - - - - - - - 4. 4 (\u00b1 0. 5) 66 .1 (\u00b1 1. 2) 47 .2 (\u00b1 0. 4) 24 .0 (\u00b1 0. 2) - - - 51 .2 (\u00b1 2. 3) 47 .2 (\u00b1 2. 0) pd el ob el le /r ob be rt -v 2- du tc hba se 37 .0 - - - - - - - - - 0. 0 (\u00b1 0. 0) 66 .0 (\u00b1 0. 2) 38 .4 (\u00b1 2. 6) 15 .3 (\u00b1 0. 1) - - - 56 .2 (\u00b1 2. 2) 45 .9 (\u00b1 1. 4) dk le cz ek /b er tba se -p ol is hun ca se dv1 45 .9 - - - - - - - 70 .6 (\u00b1 3. 0) 58 .6 (\u00b1 5. 7) 11 .3 (\u00b1 3. 6) 66 .0 (\u00b1 1. 6) 45 .5 (\u00b1 2. 3) 23 .4 (\u00b1 0) - - - - - ne ur al m in d/ be rt -b as epo rt ug ue se -c as ed 57 .6 64 .5 (\u00b1 6. 3) 70 .6 (\u00b1 8. 2) - - - - - - - - 65 .7 (\u00b1 3. 5) 44 .8 (\u00b1 0. 6) 23 .9 (\u00b1 0. 8) - - 87 .2 (\u00b1 0. 2) 62 .8 (\u00b1 0. 2) 41 .7 (\u00b1 1. 7) du m itr es cu st ef an /b er tba se -r om an ia nun ca se dv1 48 .6 - - - - - - - - - - 66 .7 (\u00b1 0. 7) 46 .8 (\u00b1 0. 3) 23 .5 (\u00b1 0) - 43 .1 (\u00b1 2. 4) - 51 .5 (\u00b1 1. 3) 60 .1 (\u00b1 1. 2) ge ru la ta /s lo va kb er t 38 .4 - - - - - - - - - - 66 .6 (\u00b1 0. 4) 37 .5 (\u00b1 2. 2) 11 .9 (\u00b1 0. 4) - - - 42 .0 (\u00b1 0. 9) 33 .8 (\u00b1 0. 5) K B /b er tba se -s w ed is hca se d 50 .0 - - - - - - - - - - 67 .4 (\u00b1 1. 5) 47 .4 (\u00b1 0. 5) 25 .6 (\u00b1 0. 5) - - - 56 .0 (\u00b1 2. 2) 53 .8 (\u00b1 6. 6) zl uc ia /c us to m -l eg al be rt 50 .2 - - - - - - - 67 .5 (\u00b1 6. 7) 69 .1 (\u00b1 4. 3) 32 .7 (\u00b1 2. 7) 65 .0 (\u00b1 2. 1) 42 .2 (\u00b1 5. 3) 22 .6 (\u00b1 0. 4) - - - 57 .3 (\u00b1 2. 8) 45 .4 (\u00b1 2. 9) nl pa ue b/ le ga lbe rt -b as eun ca se d 55 .3 - - - - - - - 88 .9 (\u00b1 3. 5) 71 .2 (\u00b1 2. 5) 29 .4 (\u00b1 4. 8) 67 .6 (\u00b1 0. 7) 46 .5 (\u00b1 1. 2) 24 .3 (\u00b1 0. 5) - - - 67 .7 (\u00b1 2. 1) 46 .8 (\u00b1 6. 4) Pl an T L -G O B -E S/ R oB E R Ta le x 45 .8 - - - - - - - - - - 65 .9 (\u00b1 1. 1) 42 .3 (\u00b1 1. 6) 20 .1 (\u00b1 0) - - - 52 .2 (\u00b1 6. 1) 48 .3 (\u00b1 1. 3) dl ic ar i/I ta lia nL eg al -B E R T 43 .0 - - - - - - 60 .6 (\u00b1 7. 8) 76 .5 (\u00b1 2. 9) 32 .7 (\u00b1 3. 2) 22 .3 (\u00b1 2. 6) 63 .5 (\u00b1 0. 8) 39 .3 (\u00b1 0. 1) 18 .8 (\u00b1 0. 8) - - - 40 .3 (\u00b1 3. 4) 33 .0 (\u00b1 16\n.1 )\nre ad\ner be\nnc h/\nju rB\nE R\nTba\nse 42\n.2 -\n- -\n- -\n- -\n- -\n- 65\n.0 (\u00b1\n0. 7)\n43 .2\n(\u00b1 1.\n9) 20\n.8 (\u00b1\n0. 4)\n- 42\n.1 (\u00b1\n1. 4)\n- 43\n.2 (\u00b1\n1. 7)\n39 .1\n(\u00b1 5.\n5)\nTa bl\ne 14\n:A ri\nth m\net ic\nm ea\nn of\nm ac\nro -F\n1 an\nd th\ne st\nan da\nrd de\nvi at\nio n\nov er\nal ls\nee ds\nfo rm\non ol\nin gu\nal m\nod el\ns fr\nom th\ne te\nst se\nt."
        },
        {
            "heading": "G Original Paper Results",
            "text": "In this section, we present an overview of scores for each configuration of the LEXTREME dataset as provided in the original papers. When certain configurations were not available, no scores were obtained. It should be noted that different papers provide varying scores, making direct comparisons with our results challenging. Additionally, the variability in the training and evaluation procedure used across different papers may impact the resulting scores, which is an important factor to consider. To gain a better understanding of the training and evaluation procedure please refer to the cited references. The LEXTREME scores are calculated by taking the arithmetic mean of each seed (three in total)."
        },
        {
            "heading": "H Histograms",
            "text": "In the following, we provide the histograms for the distribution of the sequence length of the input (sentence or entire document) from each dataset. The length is measured by counting the tokens using the tokenizers of the multilingual models, i.e., DistilBERT, MiniLM, mDeBERTa v3, XLM-R base, XLM-R large. We only display the distribution within the 99th percentile; the rest is grouped together at the end."
        }
    ],
    "title": "LEXTREME: A Multi-Lingual and Multi-Task Benchmark for the Legal Domain",
    "year": 2023
}