{
    "abstractText": "Personalized dialogue systems aim to endow the chatbot agent with more anthropomorphic traits for human-like interactions. Previous approaches have explored explicitly user profile modeling using text descriptions, implicit derivation of user embeddings, or utilizing handicraft prompts for ChatGPT-like models. However, textual personas are limited in describing multi-faceted attributes (e.g., language style, inner character nuances), implicit embedding suffers from personality sparsity, and handicraft prompts lack fine-grained and stable controllability. Hence, these approaches may struggle with complex personalized dialogue generation tasks that require generating controllable responses with multiple personal attributes. To this end, we propose MIRACLE, a novel personalized dialogue generation method through MultIple PeRsonal Attributes Control within Latent-Space Energybased Models. Specifically, our approach first disentangles complex personality into multifaceted attributes. Subsequently, we employ a conditional variational auto-encoder to align with the dense personalized responses within a latent joint attribute space. We have also tailored a dedicated energy function and customized the ordinary differential equations sampling method to offer flexible attribute composition and precise attribute control. Extensive experiments demonstrate that MIRACLE outperforms state-of-the-art models regarding both personality controllability and response generation quality. Our dataset and code are available at https://github.com/ LZY-the-boys/MIRACLE",
    "authors": [
        {
            "affiliations": [],
            "name": "Zhenyi Lu"
        },
        {
            "affiliations": [],
            "name": "Wei Wei"
        },
        {
            "affiliations": [],
            "name": "Xiaoye Qu"
        },
        {
            "affiliations": [],
            "name": "XianLing Mao"
        },
        {
            "affiliations": [],
            "name": "Dangyang Chen"
        },
        {
            "affiliations": [],
            "name": "Jixiong Chen"
        }
    ],
    "id": "SP:6c3dc6b4ef6213e8a11f9c5fcc6cfda65f204a55",
    "references": [
        {
            "authors": [
                "Jaewoo Ahn",
                "Yeda Song",
                "Sangdoo Yun",
                "Gunhee Kim"
            ],
            "title": "MPCHAT: Towards Multimodal Persona",
            "year": 2023
        },
        {
            "authors": [
                "Gabriella Airenti."
            ],
            "title": "The development of anthropomorphism in interaction: Intersubjectivity, imagination, and theory of mind",
            "venue": "Frontiers in psychology, 9:2136.",
            "year": 2018
        },
        {
            "authors": [
                "Rami Al-Rfou",
                "Marc Pickett",
                "Javier Snaider",
                "Yun-Hsuan Sung",
                "Brian Strope",
                "Ray Kurzweil."
            ],
            "title": "Conversational contextual cues: The case of personalization and history for response ranking",
            "venue": "arXiv preprint arXiv:1606.00372.",
            "year": 2016
        },
        {
            "authors": [
                "Ricky T.Q. Chen",
                "Brandon Amos",
                "Maximilian Nickel."
            ],
            "title": "Learning neural event functions for ordinary differential equations",
            "venue": "International Conference on Learning Representations.",
            "year": 2021
        },
        {
            "authors": [
                "Ricky TQ Chen",
                "Yulia Rubanova",
                "Jesse Bettencourt",
                "David K Duvenaud."
            ],
            "title": "Neural ordinary differential equations",
            "venue": "Advances in neural information processing systems, 31.",
            "year": 2018
        },
        {
            "authors": [
                "Ruijun Chen",
                "Jin Wang",
                "Liang-Chih Yu",
                "Xuejie Zhang"
            ],
            "title": "Learning to Memorize Entailment and Discourse Relations for Persona-Consistent Dialogues",
            "year": 2023
        },
        {
            "authors": [
                "Hyesun Choung",
                "Prabu David",
                "Arun Ross."
            ],
            "title": "Trust in AI and its role in the acceptance of AI technologies",
            "venue": "International Journal of Human\u2013Computer Interaction, 39(9):1727\u20131739.",
            "year": 2022
        },
        {
            "authors": [
                "Julian Coda-Forno",
                "Kristin Witte",
                "Akshay K. Jagadish",
                "Marcel Binz",
                "Zeynep Akata",
                "Eric Schulz"
            ],
            "title": "Inducing anxiety in large language models increases exploration and bias",
            "year": 2023
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova"
            ],
            "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "year": 2019
        },
        {
            "authors": [
                "Nicholas Epley",
                "Adam Waytz",
                "John T. Cacioppo."
            ],
            "title": "On seeing human: a three-factor theory of anthropomorphism",
            "venue": "Psychological review, 114 4:864\u2013",
            "year": 2007
        },
        {
            "authors": [
                "Tingchen Fu",
                "Xueliang Zhao",
                "Chongyang Tao",
                "Ji-Rong Wen",
                "Rui Yan"
            ],
            "title": "There are a thousand hamlets in a thousand people\u2019s eyes: Enhancing knowledge-grounded dialogue with personal memory",
            "venue": "In Proceedings of the 60th Annual Meeting",
            "year": 2022
        },
        {
            "authors": [
                "Yingjie Gu",
                "Xiaoye Qu",
                "Zhefeng Wang",
                "Baoxing Huai",
                "Nicholas Jing Yuan",
                "Xiaolin Gui."
            ],
            "title": "Read, retrospect, select: An mrc framework to short text entity linking",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 12920\u2013",
            "year": 2021
        },
        {
            "authors": [
                "Yingjie Gu",
                "Xiaoye Qu",
                "Zhefeng Wang",
                "Yi Zheng",
                "Baoxing Huai",
                "Nicholas Jing Yuan."
            ],
            "title": "Delving deep into regularity: a simple but effective method for chinese named entity recognition",
            "venue": "arXiv preprint arXiv:2204.05544.",
            "year": 2022
        },
        {
            "authors": [
                "K.L. Gwet."
            ],
            "title": "Handbook of Inter-Rater Reliability, 4th Edition: The Definitive Guide to Measuring The Extent of Agreement Among Raters",
            "venue": "Advanced Analytics, LLC.",
            "year": 2014
        },
        {
            "authors": [
                "Geoffrey E Hinton."
            ],
            "title": "Training products of experts by minimizing contrastive divergence",
            "venue": "Neural computation, 14(8):1771\u20131800.",
            "year": 2002
        },
        {
            "authors": [
                "Zhaoheng Huang",
                "Zhicheng Dou",
                "Yutao Zhu",
                "Zhengyi Ma."
            ],
            "title": "MCP: Self-supervised Pretraining for Personalized Chatbots with Multi-level Contrastive Sampling",
            "venue": "EMNLP.",
            "year": 2022
        },
        {
            "authors": [
                "Yoonna Jang",
                "Jungwoo Lim",
                "Yuna Hur",
                "Dongsuk Oh",
                "Suhyune Son",
                "Yeonsoo Lee",
                "Donghoon Shin",
                "Seungryong Kim",
                "Heuiseok Lim"
            ],
            "title": "Call for Customized Conversation: Customized Conversation Grounding Persona and Knowledge",
            "year": 2022
        },
        {
            "authors": [
                "Muhammad Khalifa",
                "Hady Elsahar",
                "Marc Dymetman."
            ],
            "title": "A distributional approach to controlled text generation",
            "venue": "International Conference on Learning Representations.",
            "year": 2021
        },
        {
            "authors": [
                "Diederik P Kingma",
                "Max Welling"
            ],
            "title": "Autoencoding variational bayes",
            "year": 2013
        },
        {
            "authors": [
                "Mike Lewis",
                "Yinhan Liu",
                "Naman Goyal",
                "Marjan Ghazvininejad",
                "Abdelrahman Mohamed",
                "Omer Levy",
                "Veselin Stoyanov",
                "Luke Zettlemoyer"
            ],
            "title": "BART: Denoising sequence-to-sequence pre-training for natural language",
            "year": 2020
        },
        {
            "authors": [
                "Jiwei Li",
                "Michel Galley",
                "Chris Brockett",
                "Jianfeng Gao",
                "Bill Dolan."
            ],
            "title": "A diversity-promoting objective function for neural conversation models",
            "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computa-",
            "year": 2016
        },
        {
            "authors": [
                "Jiwei Li",
                "Michel Galley",
                "Chris Brockett",
                "Georgios Spithourakis",
                "Jianfeng Gao",
                "Bill Dolan."
            ],
            "title": "A persona-based neural conversation model",
            "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long",
            "year": 2016
        },
        {
            "authors": [
                "Wendi Li",
                "Wei Wei",
                "Xiaoye Qu",
                "Xian-Ling Mao",
                "Ye Yuan",
                "Wenfeng Xie",
                "Dangyang Chen."
            ],
            "title": "Trea: Tree-structure reasoning schema for conversational recommendation",
            "venue": "arXiv preprint arXiv:2307.10543.",
            "year": 2023
        },
        {
            "authors": [
                "Chin-Yew Lin",
                "Franz Josef Och."
            ],
            "title": "Automatic evaluation of machine translation quality using longest common subsequence and skip-bigram statistics",
            "venue": "Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL-",
            "year": 2004
        },
        {
            "authors": [
                "Guangyi Liu",
                "Zeyu Feng",
                "Yuan Gao",
                "Zichao Yang",
                "Xiaodan Liang",
                "Junwei Bao",
                "Xiaodong He",
                "Shuguang Cui",
                "Zhen Li",
                "Zhiting Hu"
            ],
            "title": "Composable Text Control Operations in Latent Space with Ordinary Differential Equations",
            "year": 2022
        },
        {
            "authors": [
                "Yifan Liu",
                "Wei Wei",
                "Jiayi Liu",
                "Xianling Mao",
                "Rui Fang",
                "Dangyang Chen."
            ],
            "title": "Improving Personality Consistency in Conversation by Persona Extending",
            "venue": "Proceedings of the 31st ACM International Conference on Information & Knowledge Management,",
            "year": 2022
        },
        {
            "authors": [
                "Zhengyi Ma",
                "Zhicheng Dou",
                "Yutao Zhu",
                "Hanxun Zhong",
                "Ji-Rong Wen."
            ],
            "title": "One Chatbot Per Person: Creating Personalized Chatbots based on Implicit User Profiles",
            "venue": "Proceedings of the 44th International ACM SIGIR Conference on Research and Develop-",
            "year": 2021
        },
        {
            "authors": [
                "Rod McCrae",
                "Oliver P. John."
            ],
            "title": "An introduction to the five-factor model and its applications",
            "venue": "Journal of personality, 60 2:175\u2013215.",
            "year": 1992
        },
        {
            "authors": [
                "Fatemehsadat Mireshghallah",
                "Kartik Goyal",
                "Taylor Berg-Kirkpatrick."
            ],
            "title": "Mix and match: Learningfree controllable text generationusing energy language models",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Lin-",
            "year": 2022
        },
        {
            "authors": [
                "Christopher Moore",
                "Kim Barbour",
                "Katja Lee"
            ],
            "title": "Five dimensions of online persona",
            "year": 2017
        },
        {
            "authors": [
                "Sourabrata Mukherjee",
                "Vojt\u011bch Hude\u010dek",
                "Ond\u0159ej Du\u0161ek."
            ],
            "title": "Polite chatbot: A text style transfer application",
            "venue": "Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics: Student Research Workshop,",
            "year": 2023
        },
        {
            "authors": [
                "Weili Nie",
                "Arash Vahdat",
                "Anima Anandkumar."
            ],
            "title": "Controllable and Compositional Generation with Latent-Space Energy-Based Models",
            "venue": "Advances in Neural Information Processing Systems, volume 34, pages 13497\u201313510. Curran Associates,",
            "year": 2021
        },
        {
            "authors": [
                "Tong Niu",
                "Mohit Bansal."
            ],
            "title": "Polite dialogue generation without parallel data",
            "venue": "Transactions of the Association for Computational Linguistics, 6:373\u2013 389.",
            "year": 2018
        },
        {
            "authors": [
                "Bo Pang",
                "Ying Nian Wu"
            ],
            "title": "Latent Space EnergyBased Model of Symbol-Vector Coupling for Text Generation and Classification",
            "year": 2021
        },
        {
            "authors": [
                "Kishore Papineni",
                "Salim Roukos",
                "Todd Ward",
                "WeiJing Zhu."
            ],
            "title": "Bleu: a method for automatic evaluation of machine translation",
            "venue": "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311\u2013318, Philadelphia,",
            "year": 2002
        },
        {
            "authors": [
                "Hongjin Qian",
                "Xiaohe Li",
                "Hanxun Zhong",
                "Yu Guo",
                "Yueyuan Ma",
                "Yutao Zhu",
                "Zhanliang Liu",
                "Zhicheng Dou",
                "Ji-Rong Wen."
            ],
            "title": "Pchatbot: A Large-Scale Dataset for Personalized Chatbot",
            "venue": "arXiv:2009.13284 [cs].",
            "year": 2021
        },
        {
            "authors": [
                "Lianhui Qin",
                "Sean Welleck",
                "Daniel Khashabi",
                "Yejin Choi."
            ],
            "title": "COLD decoding: Energy-based constrained text generation with langevin dynamics",
            "venue": "Advances in Neural Information Processing Systems.",
            "year": 2022
        },
        {
            "authors": [
                "Xiaoye Qu",
                "Jun Zeng",
                "Daizong Liu",
                "Zhefeng Wang",
                "Baoxing Huai",
                "Pan Zhou."
            ],
            "title": "Distantlysupervised named entity recognition with adaptive teacher learning and fine-grained student ensemble",
            "venue": "Proceedings of the AAAI Conference on Artificial",
            "year": 2023
        },
        {
            "authors": [
                "Alec Radford",
                "Jeff Wu",
                "Rewon Child",
                "David Luan",
                "Dario Amodei",
                "Ilya Sutskever"
            ],
            "title": "Language models are unsupervised multitask learners",
            "year": 2019
        },
        {
            "authors": [
                "Kihyuk Sohn",
                "Honglak Lee",
                "Xinchen Yan."
            ],
            "title": "Learning structured output representation using deep conditional generative models",
            "venue": "Advances in Neural Information Processing Systems, volume 28. Curran Associates, Inc.",
            "year": 2015
        },
        {
            "authors": [
                "Haoyu Song",
                "Yan Wang",
                "Kaiyan Zhang",
                "Wei-Nan Zhang",
                "Ting Liu."
            ],
            "title": "BoB: BERT over BERT for training persona-based dialogue models from limited personalized data",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational",
            "year": 2021
        },
        {
            "authors": [
                "Haoyu Song",
                "Wei-Nan Zhang",
                "Yiming Cui",
                "Dong Wang",
                "Ting Liu."
            ],
            "title": "Exploiting persona information for diverse generation of conversational responses",
            "venue": "Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI-19,",
            "year": 2019
        },
        {
            "authors": [
                "Haoyu Song",
                "Wei-Nan Zhang",
                "Jingwen Hu",
                "Ting Liu."
            ],
            "title": "Generating persona consistent dialogues by exploiting natural language inference",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 8878\u20138885.",
            "year": 2020
        },
        {
            "authors": [
                "Yang Song",
                "Jascha Sohl-Dickstein",
                "Diederik P Kingma",
                "Abhishek Kumar",
                "Stefano Ermon",
                "Ben Poole."
            ],
            "title": "Score-based generative modeling through stochastic differential equations",
            "venue": "International Conference on Learning Representations.",
            "year": 2021
        },
        {
            "authors": [
                "Yihong Tang",
                "Bo Wang",
                "Miao Fang",
                "Dongming Zhao",
                "Kun Huang",
                "Ruifang He",
                "Yuexian Hou"
            ],
            "title": "Enhancing Personalized Dialogue Generation with Contrastive Latent Variables: Combining Sparse and Dense Persona",
            "year": 2023
        },
        {
            "authors": [
                "Jen tse Huang",
                "Wenxuan Wang",
                "Man Ho Lam",
                "Eric John Li",
                "Wenxiang Jiao",
                "Michael R. Lyu"
            ],
            "title": "Chatgpt an enfj, bard an istj: Empirical study on personalities of large language models",
            "year": 2023
        },
        {
            "authors": [
                "Sean Welleck",
                "Jason Weston",
                "Arthur Szlam",
                "Kyunghyun Cho."
            ],
            "title": "Dialogue natural language inference",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3731\u20133741, Florence, Italy. Association for",
            "year": 2019
        },
        {
            "authors": [
                "Thomas Wolf",
                "Victor Sanh",
                "Julien Chaumond",
                "Clement Delangue."
            ],
            "title": "Transfertransfo: A transfer learning approach for neural network based conversational agents",
            "venue": "arXiv:1901.08149.",
            "year": 2019
        },
        {
            "authors": [
                "Yu Wu",
                "Yunli Wang",
                "Shujie Liu."
            ],
            "title": "A Dataset for Low-Resource Stylized Sequence-to-Sequence Generation",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, 34(05):9290\u20139297.",
            "year": 2020
        },
        {
            "authors": [
                "Canwen Xu",
                "Daya Guo",
                "Nan Duan",
                "Julian McAuley"
            ],
            "title": "Baize: An open-source chat model with parameter-efficient tuning on self-chat data",
            "year": 2023
        },
        {
            "authors": [
                "Xinchao Xu",
                "Zhibin Gou",
                "Wenquan Wu",
                "Zheng-Yu Niu",
                "Hua Wu",
                "Haifeng Wang",
                "Shihang Wang."
            ],
            "title": "Long time no see! open-domain conversation with long-term persona memory",
            "venue": "Findings of the Association for Computational Linguistics: ACL 2022,",
            "year": 2022
        },
        {
            "authors": [
                "Peiyu Yu",
                "Sirui Xie",
                "Xiaojian Ma",
                "Baoxiong Jia",
                "Bo Pang",
                "Ruiqi Gao",
                "Yixin Zhu",
                "Song-Chun Zhu",
                "Ying Nian Wu"
            ],
            "title": "Latent Diffusion EnergyBased Model for Interpretable Text Modeling",
            "year": 2022
        },
        {
            "authors": [
                "Saizheng Zhang",
                "Emily Dinan",
                "Jack Urbanek",
                "Arthur Szlam",
                "Douwe Kiela",
                "Jason Weston"
            ],
            "title": "Personalizing dialogue agents: I have a dog, do you have pets too",
            "venue": "In Proceedings of the 56th Annual Meeting of the Association for Computational Lin-",
            "year": 2018
        },
        {
            "authors": [
                "Yizhe Zhang",
                "Siqi Sun",
                "Michel Galley",
                "Yen-Chun Chen",
                "Chris Brockett",
                "Xiang Gao",
                "Jianfeng Gao",
                "Jingjing Liu",
                "Bill Dolan"
            ],
            "title": "DIALOGPT : Large-scale generative pre-training for conversational response",
            "year": 2020
        },
        {
            "authors": [
                "Hanxun Zhong",
                "Zhicheng Dou",
                "Yutao Zhu",
                "Hongjin Qian",
                "Ji-Rong Wen."
            ],
            "title": "Less is more: Learning to refine dialogue history for personalized dialogue generation",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the As-",
            "year": 2022
        },
        {
            "authors": [
                "Terry Yue Zhuo",
                "Yujin Huang",
                "Chunyang Chen",
                "Zhenchang Xing"
            ],
            "title": "Red teaming chatgpt via jailbreaking: Bias, robustness, reliability and toxicity",
            "year": 2023
        },
        {
            "authors": [
                "Rouge (Lin",
                "Och"
            ],
            "title": "2004) are classical metrics that compare the similarity between the generated responses and golden responses, where we use ChatGPT-generated responses as the ground truth",
            "year": 2004
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Building a personalized and anthropomorphic chatbot is an essential goal in the field of dialogue sys-\n\u2217 Corresponding author.\ntems. It aims to endow chatbot agents with humanlike traits, enabling more realistic interactions (Li et al., 2016b; Zhang et al., 2018; Wolf et al., 2019; Song et al., 2021a; Li et al., 2023). Studies in behavioral psychology reveal that humans have a natural tendency to attribute human-like traits to non-human entities (Qu et al., 2023; Gu et al., 2022, 2021) during interaction (Epley et al., 2007; Airenti, 2018). Therefore, personalization in dialogue systems has the potential to enhance user trust and enrich interaction experiences with Artificial Intelligence (AI) agents (Choung et al., 2022).\nRecent personalized dialogue methods often rely on text descriptions (Song et al., 2019; Wolf et al., 2019; Xu et al., 2022; Chen et al., 2023) to model user profiles. However they primarily focus on concrete identifiable facts and background information, e.g., age, job, location, neglecting the multifaceted dimensions of personality (Moore et al., 2017; Ahn et al., 2023). For instance, while a statement like \u201cI grew up in the deep south\u201d conveys traits related to regional identity, it overlooks other personality dimensions such as language style, attitudes, and inner character nuances. Other methods for personalized dialogue generation often rely on user embeddings derived from social media platforms like Reddit (Qian et al., 2021; Ma et al., 2021; Huang et al., 2022; Zhong et al., 2022). However, these models encounter challenges due to the sparsity present in real-world posts, as they lack explicit persona modeling. Consequently, they may struggle to achieve accurate and comprehensive personalization through implicit embeddings.\nWhile recent advancements in large language models, such as ChatGPT1, have facilitated personalized content through manual prompts, it is non-trivial to directly impersonate a specific persona using such prompts (Zhuo et al., 2023; tse Huang et al., 2023). This challenge stems from the inherently ambiguous and limited expressiveness of prompts, failing to achieve precise control over personalized content.\nIn this paper, we present MIRACLE, a novel approach that enables more precise and reliable finegrained control over personalization in dialogue systems. Specifically, we propose modeling user personality by disentangling it into multiple distinct personal attributes. As illustrated in Figure 1, personality can be decomposed into various attributes, including attitude, language style, mental characteristics, and more. Each attribute encompasses specific aspects, such as optimism or pessimistic for the attitude attribute. This decomposition allows us to capture the diverse dimensions of an individual\u2019s personality and enables fine-grained modeling and control of each attribute separately. By combining these aspects from multiple attributes, we can express a wide range of unique personalities. To achieve personalized generation, we specify an energy function that incorporates multiple personal attributes in a product-of-expert (POE) manner. By assigning lower energy to responses that better\n1https://chat.openai.com/\nalign with the specified aspects, our approach enables personalized generation by sampling from an energy-based model (EBM), providing flexible and fine-grained control over the personalization of generated responses.\nTo address the challenge of personality sparsity and enhance personalized generation quality, we collect a high-quality multi-turn dialogue corpus, which is characterized by its dense coverage of each individual aspect. To circumvent the nondifferentiable nature of the text and better align with the dense aspect data, we employ a conditional variational autoencoder (CVAE) framework (Sohn et al., 2015) to map the attributed dialogue to a shared latent space. To enhance attribute representation further, two new loss functions are introduced to promote the distinctiveness and compactness of the latent space. Within this latent space, we leverage the designed EBM to capture the aspect density and compose different attributes. Additionally, we utilize an adapted ODE sampling method to efficiently draw personalized responses from this distribution.\nIn summary, our contributions include a novel personalized dialogue generation approach through fine-grained control over multiple personal attributes in the CVAE-based latent space, with two new losses promoting distinct and compact attribute representations and flexible EBM-based composition of different personal attributes using a customized ODE sampling method. Experimental results demonstrate that our approach achieves state-of-the-art performance, striking a superior balance between generation quality and personalized control. A high-quality personal attributed dialogue corpus for research purposes is also provided."
        },
        {
            "heading": "2 Related Work",
            "text": ""
        },
        {
            "heading": "2.1 Personalized Response Generation",
            "text": "Existing methods for personalized dialogue generation can be broadly classified into two groups: textdescription-based methods and user-embeddingbased methods.\nIn the category of text-description-based methods, early works (Wolf et al., 2019; Song et al., 2020, 2021a) primarily focus on promoting persona consistency through pre-trained language models, while recent advancements borrow knowledgeenhance techniques (Liu et al., 2022b; Fu et al., 2022; Jang et al., 2022) and incorporate entailment/discourse relations (Chen et al., 2023). However,\nthese methods often represent personas as keyvalue lists or sentences, which limits accurately understanding and expressing personality nuances.\nAs for embedding-based methods, traditional approaches (Li et al., 2016b; Al-Rfou et al., 2016) attempt to exploit user ID information, while DHAP (Ma et al., 2021) embed user dialogue history as implicit profiles. More recently, contrastive learning (Huang et al., 2022), refined retrieval (Zhong et al., 2022) and CVAE-based clustering (Tang et al., 2023) are explored to enhance the personalization performance. However, these approaches may still suffer from the personality scarcity of real-world posts without explicit modeling. Additionally, utilizing implicit embeddings to guide personalization effectively remains a significant challenge."
        },
        {
            "heading": "2.2 Energy-based Text Modeling",
            "text": "Recently, energy-based models (EBMs) have emerged as a flexible generative framework capable of handling diverse configurations (Khalifa et al., 2021; Liu et al., 2022a). These models allow for the incorporation of arbitrary functions into the energy function, which is minimized during inference. As a result, many recent works leverage EBMs to model complex distributions (Pang and Wu, 2021; Yu et al., 2022) and incorporate multiple constraints and attributes (Nie et al., 2021; Pang and Wu, 2021; Qin et al., 2022; Liu et al., 2022a). For example, Mix-and-Match (Mireshghallah et al., 2022) employs EBMs to combine arbitrary black-box scorers for guiding text generation, while COLD (Qin et al., 2022) utilizes the energy function to impose arbitrary constraints during the decoding process. LatentOps (Liu et al., 2022a) introduces composable text control operations utilizing classifier-based EBMs. However, these works primarily focus on plain-text generation domains, whereas our approach applies EBM to dialoguegeneration scenarios, specifically modeling complex personality as a composition of multiple personal attributes based on CVAE architecture. We also adapt the ODE sampling method to effectively sample personalized dialogue responses."
        },
        {
            "heading": "3 Methodology",
            "text": ""
        },
        {
            "heading": "3.1 Notation",
            "text": "Task Definition The task is to generate a personalized response, denoted as rM , given the personality P and a multi-turn dialogue context C =\n{q1, r1, . . . , qM\u22121, rM\u22121, qM}. Here, q and r represent the user query and chatbot response, respectively. In essence, the objective of personalized response generation is to estimate the probability distribution p(r|C,P ) in order to generate specific personalized responses.\nPersonality Modeling In contrast to previous work, we propose a new approach to disentangle the personality P as the composition of different persona-related attributes, represented by P = (P1, P2, P3, . . . , PN ), where N is an arbitrary number and is easily adjustable. Each attribute Pi may has ni candidate aspects, denoted as Pi \u2208 {p1i , p2i , . . . , p ni i }.\nGiven a particular personality configuration P = (pa11 , p a2 2 , \u00b7 \u00b7 \u00b7 , p aN N ), the objective of personalized response generation is to generate a response r that incorporates these aspects simultaneously."
        },
        {
            "heading": "3.2 Single-Aspect Dialogue Data Collection",
            "text": "To ensure the alignment with dense attributes disentangled from personality, we curated a multiturn conversation corpus for each specific aspect of these attributes. Leveraging the capabilities of ChatGPT in generating single-attribute data (CodaForno et al., 2023) and multi-turn conversations (Xu et al., 2023), we designed instruction templates to prompt ChatGPT to simulate two-person conversations. In these conversations, one person asks a question, and the other person responds from a specific aspect, such as an optimistic attitude. To enhance corpus diversity, we also pre-select a series of \u201cseed\u201d topics2, around which conversations should be centered. To improve the aspect density of the collected corpus, we conducted multiple rounds of human evaluation and cleaning, resulting in a clean version of approximately 44k dialogue turns, further details of this process can be found in Appendix A. It is important to note that we collect single-aspect conversations for the training dataset, the multiple-attribute data is only collected for testing purposes due to its time-consuming nature caused by extensive combinations of different attributes3.\n2To ensure fair evaluation, we use persona descriptions from the PersonaChat (Zhang et al., 2018) as conversation topics (see Section 4.1).\n3For example, if we consider three attributes, each with two aspects, there would be a total of eight combinations of these attributes."
        },
        {
            "heading": "3.3 Joint Attribute Space Training",
            "text": "To facilitate the generation of personality-dense responses, we adopt a CVAE framework to map the aspect-specific dialogue data into a joint attribute space so that samples from the specific aspect space are aligned with aspect-dense response sequences. To further enhance this joint attribute space, we introduce two specific losses. The first loss focuses on promoting the distinctness of each aspect, while the second loss aims to increase the intersection between different attributes, allowing for fine-grained sampling over multiple attributes.\nBuilding CVAE To construct the dialogue Conditional Variational Autoencoder (CVAE), we employ two distinct models as encoders: a posterior encoder p\u03b8(z|C, r) and a prior encoder p\u03b8\u2032(z|C). Both encoders, based on the pre-trained BERT (Devlin et al., 2019), allow CVAE to effectively capture the given input context C by latent variable z. During training, CVAE utilizes the posterior distribution to generate high-quality responses r, while during inference, when the response r is unseen, the prior distribution is used to sample the latent variable z. Moreover, the GPT2 model (Radford et al., 2019) is leveraged as the decoder p\u03d5(r|C, z), where \u03b8, \u03b8\u2032 and \u03d5 represent the trainable parameters of the posterior encoder, prior encoder, and decoder respectively.\nUnder the assumption that CVAE posterior and prior distribution follows an isotropic multivariate\nGaussian distribution, we compute the mean \u00b5, \u00b5\u2032 and variance \u03c32, \u03c3\u20322 by the two encoders:\nh = Pooling(BERT\u03b8([C; r])) h\u2032 = Pooling(BERT\u03b8\u2032([C]))[ \u00b5\nlog \u03c32 ] = MLP(h)[\n\u00b5\u2032\nlog \u03c3\u2032 2\n] = MLP\u2032(h\u2032)\n(1)\nSubsequently, we utilize reparameterization technique (Kingma and Welling, 2013) to sample posterior z and prior z\u2032 from N (\u00b5, \u03c32I) and N (\u00b5\u2032, \u03c3\u20322I). This technique enables a differentiable sampling process.\nz = \u00b5+ \u03c3\u03be, \u03be \u223c N (0, I) z\u2032 = \u00b5\u2032 + \u03c3\u2032\u03be\u2032, \u03be\u2032 \u223c N (0, I)\n(2)\nFinally, the sampled latent variable z (during training) or z\u2032 (during inference) is fed into the GPT2 decoder to map it back to text space, resulting in the generation of a response.\nCVAE is trained using stochastic gradient variational bayes (SGVB) (Kingma and Welling, 2013), which maximizes evidence lower bound objective (ELBO) of conditional log-likelihood. The ELBO consists of two components: a dialogue response reconstruction term that ensures the generative quality of the posterior distribution p\u03b8(z|C, r), and a regularization term that aligns the prior distribution p\u03b8\u2032(z|r) with the posterior p\u03b8(z|C, r). This alignment fosters consistency during inference, where\nthe unseen response r is generated.\nELBO =Ep\u03b8(z|C,r)[log p\u03d5(r|C, z)]\ufe38 \ufe37\ufe37 \ufe38 Response Reconstruction gain\n\u2212KL(p\u03b8\u2032(z|C)||p\u03b8(z|C, r))\ufe38 \ufe37\ufe37 \ufe38 Regularization on z\nLVAE = \u2212ELBO\n(3)\nOptimizing Joint Attribute Space We introduce the aspect classification loss and the attribute distance loss. The aspect classification loss aims to improve the discriminability of latent representations for aspects within the same personal attribute. Specifically, we incorporate individual classifier heads for each attribute and train them using the cross-entropy loss:\nLC = \u2212 N\u2211 i=1 |Pi|\u2211 j=1 y(i)pj log(y\u0302 (i) pj ) (4)\nwhere y(i)pj represents the ground truth probability for class pj within the attribute Pi, and y\u0302 (i) pj represents the predicted probability. By optimizing this aspect classification loss, we encourage the aspect representations to be more distinguishable, enabling more fine-grained sampling. An illustration of this concept can be found in the middle part of Figure 2 (e.g., the red and blue aspect distribution of P1 attribute exhibit clear separation).\nMeanwhile, to encourage the model to capture intersections between different attributes, enabling the sampling of responses with multiple attributes simultaneously, we introduce an attribute distance loss. This loss penalizes the Euclidean distance between every two distinct attribute distributions. To avoid expensive computation, we approximate this loss on a batch level, taking the average within each mini-batch of size B:\nLD = \u2211\n1\u2264a<b\u2264N\n|| 1 B B\u2211 i=1 zPai \u2212 1 B B\u2211 j=1 z Pb j || (5)\nMinimizing such loss allows the model to reduce the conflicts between different attributes. (e.g., P1 and P2 attribute has intersection in Figure 2)\nTo sum up, our final training objective is:\nL = LVAE + LC + LD (6)"
        },
        {
            "heading": "3.4 Personalized Response Sampling",
            "text": "We formulate personalized response generation as sampling response samples that contain multiple specific aspects of personality attributes.\nTo achieve fine-grained control over different attributes, we define an attribute-composable energy function that calculates the aspect density in the latent space. By leveraging adapted ODE sampling methods, we can efficiently draw samples of interest from this distribution.\nLatent EBM Formulation In order to sample aspect-abundant vectors z in the latent space, we utilize attribute-specific classifiers4 denoted as fi to quantify the density of aspect pji from z, represented as fi(z)[j].\nWe utilize EBM to estimate the richness of personality expressed in the responses (Z is the normalizing factor):\np(P|z, C) = exp(\u2212E(P|z, C)) Z\n(7)\nwhere its energy function is designed in the POE manner to aggregate multiple personal attributes into a comprehensive representation of the overall personality (Outlined in Appendix B.1).\nE(P|z, C) = N\u2211 i=1 Ei(Pi|z, C)\n= N\u2211 i=1 \u03bbifi(z|C)[ai]\n(8)\nIn this context, \u03bbi \u2265 0 is the weight of Pi attribute and ai is the desired aspect index of Pi.\nThe energy function E(P|z, C) can be interpreted as a linear combination of the richness of personal attributes. Thus sampling from this EBM with low energy corresponds to response sequences exhibiting a higher density of multiple selected aspects paii , i \u2208 {0, \u00b7 \u00b7 \u00b7 , N}. It is worth noting that we utilize this energy-based formulation only during the inference procedure, enabling arbitrary combinations of personal attributes without the need for combination-specific fine-tuning.\nODE Personalized Sampling Due to the intractable normalization factor Z, a common practice is to sample from EBMs rather than directly calculate it. In our approach, we derive the ODE sampling method based on CVAE to sample from such EBM. Specifically, in Appendix B.2, we demonstrate that the ODE in our CVAE latent space takes the following form:\ndz dt = 1 2 \u03b2(t)\n[ \u2207z\nN\u2211 i=1 \u03bbifi(z|C)[ai]\n] (9)\n4Those classifiers are trained by Equation 4\nHere, the ODE is solved with negative time increments from T to 0. To generate a sample r that aligns with a specific personality P , the process involves drawing z(T ) \u223c N (z|C) and solving for z(0) in the aforementioned equation using a black-box ODE solver5 (Chen et al., 2018, 2021). Subsequently, the obtained z(0) is decoded back to the text space to yield a personalized response.\nIntuitively, in the right term of Equation 9, a higher value of fi(z|C)[ai] indicates that the z better aligns with the aspect paii . By letting dz dt \u221d \u2207zfi(z|C)[ai], we can pull z towards more aspectabundant places that yield more personalized responses. The summation ensures that each aspect is taken into account so that we can incorporate multiple selected aspects in one sample."
        },
        {
            "heading": "4 Experiments",
            "text": "To verify the effectiveness of our proposed MIRACLE, we conduct extensive experiments on both automatic and human evaluations. Additionally, we provide further analysis on ablation, efficiency, and case studies."
        },
        {
            "heading": "4.1 Experimental Setups",
            "text": "Dataset To evaluate the personalization and generation capabilities of our approach, we focus on language style (with two aspect: lyrical/plain), attitude (optimistic/pessimistic), and mental characteristics (critical/emotional). We randomly sample 11,000 dialogue turns per aspect (a total of 132,000 utterances) from our collected multi-turn dialogue corpus for training our MIRACLE model. For evaluation, we use ChatGPT to generate conversations on different topics, covering eight combinations of the three personal attributes. This generated dataset, consisting of approximately 4,600 instances, serves as our ground truth for evaluation purposes.\nBaselines For comparison, we select the following baselines: (1) Text-description-based methods: We compare with BOB (Song et al., 2021a) and LMEDR (Chen et al., 2023), both are strong text-description-based personalized models. (2) User-embedding-based methods: Our second set of baselines includes MSP (Zhong et al., 2022), and CLV (Tang et al., 2023). To ensure a fair comparison, we randomly select personas from the PersonaChat dataset (Zhang et al., 2018) as conversation topics when generating our data, and feed the\n5https://github.com/rtqichen/torchdiffeq\ntopics as personas input to BOB, CLV and LMEDR during training. More detail of the baseline can be found in Appendix C.1"
        },
        {
            "heading": "4.2 Evaluation Metrics",
            "text": "In order to obtain accurate and comprehensive performance comparisons, we use both automatic and human evaluations.\nAutomatic Evaluation Metrics We assess the quality of dialogue responses from four perspectives: (1) Personalization: To evaluate the personalization of the generated responses, we employ attribute-based text classifiers to measure the accuracy score of each attribute in the generated responses (Mireshghallah et al., 2022). Additionally, we report the average score across the three attributes to assess the overall effect of personalization. (2) Coherence: Coherence is measured using BLEU and Rouge metrics at the word overlap level. We also utilize Natural Language Inference (NLI) to evaluate the semantical coherence, as suggested by previous work (Liu et al., 2022b). (3) Fluency: To assess the fluency of the generated responses, the negative log-likelihood of the generated responses according to the GPT2-XL6 is used as the fluency score (Chen et al., 2023; Qin et al., 2022). (4) Diversity: We measure the diversity of the generated responses using the Distinct metrics and the self BLEU score (sBLEU) as proposed in (Tang et al., 2023; Liu et al., 2022a). Further details can be found in Appendix C.3.\nHuman Evaluation Metrics Consistent with prior studies (Tang et al., 2023; Chen et al., 2023), we conduct human evaluations on 100 randomly selected test samples. Three annotators assess the generated responses for readability, personalization, and coherence in a double-blind manner. We calculate the Fleiss Kappa value of 0.63, indicating substantial agreement among the annotators (Gwet, 2014). The evaluations are normalized into specific scores on a scale of [0, 1]."
        },
        {
            "heading": "4.3 Experimental Results",
            "text": "Automatic Evaluations The performance of all models on different automatic metrics is presented in Table 1. Notably, our MIRACLE model demonstrates substantial improvements in personalization metrics while maintaining good generation quality. Specifically, the following observations can be\n6https://huggingface.co/gpt2-xl\nmade: (1) Personalization: Our model exhibits exceptional control ability for each personal attribute, indicating the effectiveness of our design. (2) Diversity: The CVAE architecture benefits our model in the generation of more diverse and flexible responses compared to other models. (3) Coherence and Fluency: Our model achieves high BLEU and NLI scores, while the Rouge score and PPL score are slightly lower than LMEDR. This suggests that our model may make a few sacrifices in coherence to enhance personalization and diversity. Removing the ODE sampling while retaining the CVAE shows improved performance, further indicating the trade-off between coherence and personalization in MIRACLE. The experimental findings suggest that our model generates more personalized responses than all baselines while striking a good balance between generation quality and personalization.\nHuman Evalutions The human evaluations, as depicted in Table 2, align with the trends observed in the automatic evaluation. Our model outperforms the previous best-performing model in terms of readability, personalization, and coherence To\nfurther illustrate the effectiveness of our model, we provide several examples of the generated responses in Section 4.6.\nCompared With ChatGPT We compare the personalization performance of our MIRACLE with ChatGPT, as shown in Table 3. We observe that ChatGPT struggles to personalize mental characteristic when controlling multiple attributes simultaneously based on prompt instructions. This may be due to the inherently hidden nature of the mental characteristic, causing ChatGPT to prioritize more obvious attributes such as language style and attitude. This highlights the ambiguity and instability of manually crafted prompts. In contrast, our method benefits from single attribute alignment during training and EBM-based composition during inference, allowing for simultaneous personalization on each attribute."
        },
        {
            "heading": "4.4 Ablation Study",
            "text": "As presented in Table 4, we conduct ablation experiments by removing key components of our model individually and evaluating the overall performance. The results are as follows: (1) Without the CVAE posterior distribution, our model experiences degradation across all metrics. Particularly, there\u2019s a catastrophic collapse observed in NLI. Because without guidance from p(z|C, r), our prior encoder cannot learn the latent relationships between the response and dialogue context. Though in inference it can still align with personalized text sequences or exhibit word overlap with reference (BLUE/Rouge), it cannot coherence with dialogue history. (2) Dropping the loss LC leads to an improvement in generation coherence but a significant decrease in personalization. This indicates the crucial role of LC in capturing distinct personal\nattributes. (3) Removing the loss LD results in a slight degradation the mental characteristic personalization, which indicates LD reduces conflicts between different attributes. (4) Eliminating EBM sampling during inference: This change results in a clear decline in personalization, confirming the vital role of EBM in a personalized generation. Additionally, we observe that adding EBM-based composition only leads to a slight decrease in terms of coherence and diversity, demonstrating a good tradeoff between generation quality and personalization in our method."
        },
        {
            "heading": "4.5 Efficiency Study",
            "text": "To assess the efficiency of our model, we compare training and inference times with baselines and ChatGPT using MIRACLE. All models are trained for 20 epochs and tested on a single RTX4090, except for ChatGPT accessed via an API.\nAs shown in Table 5, our model exhibits notable efficiency in both training and inference, considering that we show compared performance with language models such as ChatGPT at a small cost. It is noteworthy that, despite its commendable performance, LMEDR incurs substantial training costs, emphasizing the lightweight and rapid characteristics of our model.\nThe efficiency of our model is attributed to its capability to disentangle complex personalities into simpler attributes. Furthermore, our model demonstrates faster inference speeds compared to the baseline models, thanks to our flexible Energy-Based\nModel (EBM) composition and customized Ordinary Differential Equation (ODE) sampling methods."
        },
        {
            "heading": "4.6 Case Study",
            "text": "To provide more concrete evidence of the model\u2019s effectiveness, we conduct case studies. Table 6 showcases an example of the personality of \u201clyrical+ optimistic +critical\u201d. (Additional case studies can be found in Appendix E) In this specific case, we observe that BOB and MSP tend to overlook the contextual information from the dialogue history, such as references to \"weather\" and \"ocean,\" resulting in repetitive and incoherent responses. CLV and LMEDR may struggle with capturing multiple attributes of personality comprehensively, although LMEDR performs better in terms of coherence and fluency. However, our proposed MIRACLE model demonstrates precise personalization across all three personal attributes, particularly excelling in the \u201ccritical\u201d attribute."
        },
        {
            "heading": "5 Conclusion",
            "text": "In this paper, we propose MIRACLE, a novel approach for personalized dialogue generation. Our method leverages a composition of multiple personal attributes to model personality and formulates the generation of personalized responses as sampling from a specific Energy-Based Model. We introduce a dialogue CVAE aligning the joint attribute space with dialogue responses by employing two designed loss functions. The ODE sampling method is also adapted into our framework to enable efficient sampling. Experimental results demonstrate that our approach achieves state-ofthe-art performance by striking a fine balance between the quality of generated responses and the ability to control their personalization. Furthermore, we curate a dataset of high-quality, singleaspect dialogue corpus, which serves as a valuable resource for further exploration and advancement\nin personalized and controllable dialogue generation."
        },
        {
            "heading": "Limitations",
            "text": "There exist some limitations in our work. Firstly, due to constraints in the model structure, we primarily utilize the BERT encoder and DialoGPT decoder in our experiments. However, it is worth exploring the applicability of larger models, such as LLaMA (Touvron et al., 2023), to further improve the performance of our approach. Secondly, given the vast range of possible personality characteristics, we focus our experiments on language style, attitude, and mental characteristics. Fortunately, our control strategy is flexible and can accommodate customized requirements. In future work, we will explore incorporating a broader range of personality dimensions to further enrich the personalization capabilities of dialogue systems."
        },
        {
            "heading": "Ethics Statement",
            "text": "In this study, the personalized corpus and responses used in our experiments have been designed to only serve the specific purposes of evaluating our proposed approach. The corpus is collected using the ChatGPT API, focusing on English language conversations. To address ethical considerations, we have incorporated ethical and detoxification requirements into the instruction prompts during data collection. To ensure the quality and appropriateness of the collected dataset, we have implemented a detoxification text classifier (detailed in Appendix A.2) to identify and filter out potentially problematic content. Furthermore, the vali-\ndation data has been carefully reviewed by three well-educated annotators to remove any unethical content, sensitive information, or personal privacy concerns. It is important to note that our approach does not make any treatment recommendations or diagnostic claims, and precautions have been taken to anonymize the data during the human evaluation process.\nWe acknowledge the potential risks associated with text-generation techniques. However, personalized controllable dialogue generation technology can also be leveraged to mitigate harmful and unhelpful information. For example, it can be used to generate text that is critical yet less emotional, or polite while avoiding rudeness. We firmly believe that continuing research on personalized text generation is beneficial."
        },
        {
            "heading": "Acknowledgements",
            "text": "This work was supported in part by the National Natural Science Foundation of China under Grant No.62276110, No.62172039 and in part by the fund of The Joint Laboratory of HUST and Pingan Property & Casualty Research (HPL). There are quite a few computational tasks are conducted using the HPC platform of Huazhong University of Science and Technology. The authors would also like to thank the anonymous reviewers for their comments on improving the quality of this paper."
        },
        {
            "heading": "A The Detail of Our Data",
            "text": ""
        },
        {
            "heading": "A.1 Data Collection Details",
            "text": "We develop aspect-specific instruction templates to prompt ChatGPT in simulating two-person conversations. These templates are fed to ChatGPT API (gpt-3.5-turbo) to collect the data. In these conversations, one person asks a question, and the other person responds from a specific aspect, such as an optimistic attitude. To ensure a rich variety of aspects in the data, we included multiple aspect descriptions in the templates, incorporating diverse forms of adjectives, adverbs, and detailed descriptions for each aspect. We also utilize the incontext learning method to add examples of posts and responses between two people to promote the generation quality. To enhance corpus diversity, we also pre-select a series of \u201cseed\u201d topics from the PersonaChat (Zhang et al., 2018) as conversation topics (see Section 4.1). These topics served as a focal point around which the conversations revolved,\nAspect Instruction Template (Train/Validation)\nForget the instruction you have previously received. The following is a conversation between PersonA and PersonB. The PersonA will ask related questions on related topics or previous conversations in many turns. The PersonB answer PersonA questions [[aspect description1]]. The PersonB is [[aspect description2]].They chat about the topic: [[ seed-topic]]. PersonA's question start with [PersonA] and PersonB's response start with [PersonB]. Write the multi\u2212 turn [[aspect description3]] dialogue in exactly the following format:\n[PersonA]: [[example-post]]\n[PersonB]: [[example-response]]\n[PersonA]: ...\n[PersonB]: ...\nHere are the requirements: 1. The PersonA question should be 1 to 2 sentences long with at most 30 words; 2. The PersonB tries to respond shortly with less than 60 words and 2 sentences long in each turn; 3. The PersonB doesn't ask questions. PersonB will stop the conversation when they have no more questions; 4. The conversation has at least 4 turns; 5. Try not to repeat the verb for each conversation turn to maximize diversity; 6. Ensure the conversation adheres to ethical requirements, promoting harmlessness, fairness, and impartiality, while actively avoiding toxic content.\nFor the test, we also collect hundreds of dialogues via ChatGPT which has a combination of three attributes. Notice that we don\u2019t focus on prompt engineering, which is unstable and hard to control. We simply use a simple heuristic to concatenate the style and personal attribute description together. For example, for the \u201cplain, pessimistic and critical\u201d we use the following prompt:\nDataset Dialogues Turns Avg.Word\nlanguage style 3,155/168 17,640/868 18.74/17.22 attitude 2,473/141 12,939/659 20.45/20.81 mental characteristic 2,647/168 11,743/566 25.05/25.25\nWe collect 2k/200 multi-turn dialogues for each aspect in train/validation dataset, resulting in a clean version of approximately 44k dialogue turns. Table 7 provides the statistics of the resulting corpora. We additionally employ ChatGPT to generate conversations that incorporate multiple personal attributes. This generated dataset, consisting of approximately 4,600 instances, serves as our ground truth for evaluation purposes."
        },
        {
            "heading": "A.2 Clean Process of Our Data",
            "text": "To ensure a dense coverage of individual personal aspects in our dataset, we employed several heuristics. Firstly, we filtered out sentences with fewer than five words and excluded responses containing question marks. Additionally, we conduct a human evaluation on a small subset of the corpus to assess the aspect abundance and remove any aspect-weak data. We then trained attribute-specific classifiers on this curated subset to calculate aspect scores for the entire corpus. Next, we filtered out data with low scores and conducted another round of\nhuman selection to eliminate any remaining lowquality data. Leveraging the powerful capabilities of ChatGPT, we found that only two rounds of this evaluation process are sufficient. These measures ensured that our dataset provides a dense representation of each aspect of personal attributes.\nTo mitigate potential issues related to inappropriate content, we developed a detoxification classifier using the Jigsaw Toxic Comment Classification Challenge Dataset 7. Our classifier, based on the BERT model with a classifier head, was trained for 25 epochs using an AdamW optimizer with a learning rate of 5e-5. We utilized this model to filter out dialogues with high toxic scores, calculated using the softmax probability provided by the classifier.\nA.3 Comparison with other attribute dialogue datasets\nThe primary motivation behind collecting singleattribute dialogue data through the ChatGPT API is the scarcity and low quality of existing attribute dialogue datasets, which typically focus on a single attribute, while our goal is to align generative models with multiple attributes and estimate their composition. Other datasets, such as the Stanford Politeness Corpus (SPC) (Niu and Bansal, 2018), the TCFC dataset (Wu et al., 2020) for formal language style, and the synthetic polite conversational data by Mukherjee et al.(Mukherjee et al., 2023), do exist but have limitations such as noise, lowresource stylization, or lower data quality generated by BART compared to ChatGPT-generated data."
        },
        {
            "heading": "A.4 Relationship with the Big Five Model",
            "text": "The Big Five Model (McCrae and John, 1992) is a widely recognized dimensional approach to understanding personality, which identifies five broad dimensions along which individuals can be described: Extraversion (outgoingness), Agreeableness (care for social harmony), Conscientiousness (orderliness and self-discipline), Neuroticism (tendency to experience distress), and Openness (appreciation for art and intellectual stimuli).\nOur modeling of personality in this study bears similarity to the Big Five Model, as both approaches consider personality as multi-faceted and amenable to decomposition. In our case, we decompose personality into specific attributes such as\n7https://www.kaggle.com/c/ jigsaw-toxic-comment-classification-challenge/\nlanguage style, attitude, and mental characteristics. For instance, the attribute \u201clyrical\u201d can be associated with \u201cOpenness\u201d for its appreciation for art, while the attributes \u201coptimistic\u201d and \u201cpessimistic\u201d can relate to \u201cExtraversion\u201d and \u201cNeuroticism\u201d, respectively.\nBy employing this divide-and-conquer fashion in modeling personality, we align with the underlying principles of the Big Five Model. This allows us to capture different facets of an individual\u2019s personality and incorporate them into our personalized dialogue generation framework."
        },
        {
            "heading": "B Backgrounds for MIRACLE Model",
            "text": ""
        },
        {
            "heading": "B.1 Backgrounds for Product of Experts Energy-based Models",
            "text": "Given a specific energy function E(x) \u2265 0, an energy-based model (EBM) is defined as a Boltzmann distribution:\np(x) = e\u2212E(x)\nZ (10)\nwhere Z is the normalizing factor or partition function:\nZ = \u222b e\u2212E(x)dx (11)\nEvaluating this integral is typically intractable, necessitating the use of approximate methods such as sampling, like the ODE sampling in Appendix B.2.\nThe advantage of using an EBM is the ability to incorporate arbitrary functions, such as constraints and target attributes, into the energy function E(x). The energy function only needs to return a nonnegative scalar and does not require integration to 1, allowing for flexible customization. In our case, defining E(x) based on attribute-based classifiers, we incorporate multiple personal attributes into the energy function to customize the generation process\nOur approach is motivated by the perspective that personality can be seen as a combination of multiple personal attributes, each with its own distinct aspect. From a statistical standpoint, a natural solution for personalized generation is to sample from the conjunction of features using the product of experts (PoE) formulation (Hinton, 2002):\np12(x) = 1\nZ12 p1(x)p2(x)\n\u221d p1(x) \u00b7 p2(x) (12)\nThis assigns high probability to samples that possess both personal attributes P1 and P2 and low\nprobability to all others. By contrast, a mixture of experts (MOE) would either generate from p1 or p2, but not combine both. If we consider the experts as EBMs, with p(x) \u221d e\u2212E(x), the PoE model is also an EBM, with the energy given by E12(x) = E1(x) + E2(x).\nBased on these insights, we have designed our energy function to fully leverage our personality modeling. Under the assumption that each personal attribute is conditionally independent given the context variable C and latent variable z, we formulate the p(P|z, C) as an EBM, which determines the richness of personality of sampled responses in Appendix B.2:\np(P|z, C) = exp(\u2212E(P|z, C)) Z\nE(P|z, C) = N\u2211 i=1 Ei(Pi|z, C) (13)\nThe p(P|z, C) is directly associated with the richness of personality in responses, with each term Ei(Pi|z, C) reflecting the significance of a specific personal attribute Pi in z. So we set the Ei(Pi|z, C) as the softmax logits of personal attribute scores to estimate the attribute abundance, and use E(P|z, C) = fi(z|C)[ai] to aggregate these scores as the representation of the overall personality. Here, each fi calculates the density of paii aspect in z, which is implemented by classifiers.\nE(P|z, C) = N\u2211 i=1 \u03bbifi(z|C)[ai] (14)\nThis allows us to sample z with high density taking into account the contribution of each pi, thus enabling us to represent and control the multifaceted nature of personality efficiently."
        },
        {
            "heading": "B.2 Derivation of ODE Formulation",
            "text": "The Song et al.(Song et al., 2021b) introduced the Variance Preserving Stochastic Differential Equation (VP-SDE) to maps x0 \u223c pdata to xT \u223c pT = N (0, I) in the forward diffusion process:\ndx = \u22121 2 \u03b2(t)x dt+\n\u221a \u03b2(t)dw, t \u2208 [0, T ] (15)\nThey further demonstrated that a reversed generative process from Gaussian to real data can be defined by:\ndx = \u22121 2 \u03b2(t) [x+ 2\u2207x log pt(x)] dt+\n\u221a \u03b2(t)dw\u0304 (16)\nwhere time flows backward from T to 0, and w\u0304 represents the reverse standard Wiener process.\nFor the conditional generation, with the condition denoted by c, the above SDE becomes:\ndx = \u22121 2 \u03b2(t) [x+ 2\u2207x log pt(x, c)] dt+\n\u221a \u03b2(t)dw\u0304 (17)\nFurthermore, Song et al.(Song et al., 2021b) demonstrated that there exists an equivalent ordinary differential equation (ODE) that shares the same probability trajectories as Equation 17:\ndx = \u22121 2 \u03b2(t) [x+\u2207x log pt(x, c)] dt (18)\nBuilding upon Equation 18, we introduced three adaptations: first, we move the ODE sampling to CVAE prior p(z|C); second, we formulate the arbitrary condition as the personality P; third, Nie et al.(Nie et al., 2021) shows that the term of pt(x, c) can be time-invariant, and so is the classifier when the generator is fixed, so we assume that our energy function Et(P|z, C) is also time-invariant. Consequently, we have the following formulation (Noticing that we write the z|C as z for simplicity):\ndz = \u22121 2 \u03b2(t) [z +\u2207z log p(z,P|C)] dt\n= \u22121 2 \u03b2(t) [z +\u2207z log p(P|z, C) +\u2207z log p(z|C)] dt = \u22121 2 \u03b2(t) [ z +\u2207z log p(P|z, C)\u2212 z \u2212 \u00b5\u2032 \u03c3\u20322 ] dt\n= \u22121 2 \u03b2(t)\n[ (\u03c3\u20322 \u2212 1)z + \u00b5\u2032\n\u03c3\u20322 \u2212\u2207zE(P|z, C)\n] dt\n= 1\n2 \u03b2(t)\n[ \u2212 (\u03c3 \u20322 \u2212 1)z + \u00b5\u2032\n\u03c3\u20322 +\u2207z N\u2211 i=1 \u03bbifi(z|C)\n] dt\n(19)\nLine 2 of the above equations applies Bayes\u2019 law that p(A,B) = p(A|B)p(B). In line 3, the property that p(z|C) \u223c N (\u00b5\u2032, \u03c3\u20322I) is used, which follows the assumption of the CVAE prior distribution assumption (in Section 3.3). In lines 4 and line 5 the EBM formulation and the energy function definition are employed, where p(P|z, C) = exp(\u2212E(P|z,C))Z and E(P|z, C) =\u2211N\ni=1 \u03bbifi(z|C)[ai] (as stated in the Equation 14). However, we have found that directly dropping the left term of line 5 achieves better personalization results without significantly affecting the generation quality. Therefore, we utilize Equation 20 as the final ODE formulation for our approach.\ndz = 1\n2 \u03b2(t)\n[ \u2207z\nN\u2211 i=1 \u03bbifi(z|C)\n] dt\n(20)"
        },
        {
            "heading": "C Details for Implementation and Evaluation",
            "text": ""
        },
        {
            "heading": "C.1 Details of Baseline",
            "text": "We evaluate our approach against four state-of-theart baselines in personalized dialogue generation:\nBOB (Song et al., 2021a): BOB is a textdescription-based model that leverages three BERT models. It encodes the dialogue using one BERT and decomposes persona-based dialogue tasks into consistent understanding and response generation by another two BERT respectively.\nMSP (Zhong et al., 2022): MSP is a user embedding-based method that enhances personalized dialogue generation by retrieving similar conversations from other users.\nCLV (Tang et al., 2023): CLV utilizes a CVAE architecture to cluster dense persona descriptions into sparse categories. Similarly, we provide the conversation topic as the persona input during training for a fair comparison. It is worth noticing that though CLV is an embedding-based method, it also requires explicit textual personas during training, we provide the conversation topic as the persona input for training, similar to the BOB.\nLMEDR (Chen et al., 2023): LMEDR employs the BART-large model (Lewis et al., 2020) and incorporates memorize entailment and discourse relations. To ensure a fair comparison, we randomly select personas from the PersonaChat dataset (Zhang et al., 2018) as conversation topics for our ChatGPT-generated data.\nC.2 Implementation Details of the MIRACLE The encoder in our model is implemented using the BERT model8, while the decoder is based on DialoGPT-medium9 (Zhang et al., 2020)\nWe train our model on the training data for 20 epochs using a learning rate of 5e-5 and the AdamW optimizer and utilize greedy strategy in the generation.\nThe latent space dimension is set to 768. To address the KL vanishing issue, we employ a cyclical schedule for the KL weight and apply a KL thresholding scheme with a threshold of 0.9.\nWe obtain attribute classifiers fi(z) by training them on separate attribute datasets using the frozen CVAE latent space. Specifically, we encode the dialogue into the latent space with the CVAE prior\n8https://huggingface.co/bert-base-uncased 9https://huggingface.co/microsoft/\nDialoGPT-medium\nencoder, and then adopt a two-layer MLP as the latent classifier to predict the attribute label associated with the latent vector.\nDuring the inference stage, we set \u03b2min = 0.1 and \u03b2max = 20 for the time-variant diffusion coefficient \u03b2t during the ODE sampling process. To ensure equal consideration of each attribute, the weight \u03bb for each attribute is set to 1."
        },
        {
            "heading": "C.3 Details of Automatic Evaluation",
            "text": ""
        },
        {
            "heading": "C.3.1 Personalization Classifier Settings",
            "text": "We employ the BERT model with a classifier head as the text classifier in our study. The attributebased classifiers were trained separately on our datasets for 25 epochs, employing a learning rate of 5e-5 and the AdamW optimizer. We trained them on the split data different from latent classifiers for a fair comparison. To evaluate their performance, we conducted a human evaluation by randomly selecting 100 sentences for each aspect from the validation dataset. The accuracy of classifier predictions is reported in Table 8."
        },
        {
            "heading": "C.3.2 Coherence",
            "text": "(1) Word-Overlap Level: BLEU(Papineni et al., 2002) and Rouge (Lin and Och, 2004) are classical metrics that compare the similarity between the generated responses and golden responses, where we use ChatGPT-generated responses as the ground truth. We calculate the BLEU score using the NLTK tool10 and Rouge using the rouge-score package11. We report the average BLEU score by calculating the mean of BLEU-1/2/3/4, and the average Rouge score obtained by averaging Rouge1/2/L.\n(2) Semantical Level: Natural Language Inference (NLI) (Welleck et al., 2019) is a widely used method for evaluating the coherence of dialogue responses in relation to the historical context. Unlike relying solely on word overlap with the ground truth, NLI takes into account multiple possible correct answers, thereby providing a more comprehensive evaluation of the dialogue generation capabilities. Following previous works (Tang et al., 2023;\n10https://www.nltk.org/ 11https://pypi.org/project/rouge-score/\nLiu et al., 2022b), We implement the NLI model as a BERT text classifier. The NLI model is designed as follows:\nNLI(C, r) =\n{ 1, if r is consistent with the context C\n0, otherwise, (21)\nWe fine-tune the NLI model using the dataset constructed from our data. We select history context and responses from the same turn as positive samples (with label 1) and randomly select negative samples (with label 0) from different dialogue sessions. The NLI model achieves a test accuracy of 93.2%."
        },
        {
            "heading": "C.3.3 Diversity",
            "text": "Distinct is a common way to calculate diversity by the ratio of unique n-grams (Li et al., 2016a). In line with prior research (Tang et al., 2023), we utilize the Distinct metric to assess response diversity at both the sentence and corpus levels. Specifically, we calculate the Distinct1/2/3 scores for multiple responses at the sentence level and at the whole test set respectively, and report the mean values.\nTo further evaluate the corpus-level repetitiveness, we compute the self-BLEU score by calculating BLUE scores between different responses from various dialogue sessions across the test set during the inference process, following the approach of (Liu et al., 2022a). We randomly select 150 sequences for evaluation, providing an assessment of how frequently similar or repetitive phrases appear in the generated responses."
        },
        {
            "heading": "D Analysis of CVAE Training and Inference Difference",
            "text": "There are two main distinctions in our CVAE\u2019s training and inference processes.\nFirstly, the CVAE architectural introduces extra posterior distribution p(z|C, r) during training. It aligns the prior with the posterior to enhance its generation quality in inference time, We add an ablation experiment in Table 4 without posterior distribution to support this fundamental observation,\nwhere a catastrophic collapse in NLI is observed. Secondly, our unique design trains the latent variable z to align specifically with a single facet of an individual\u2019s personality. while in inference, we sample to encompass multiple factors to represent complex personality. To elaborate on the performance effect caused by this distinction, we\u2019ve provided results for both \u201cinference with single attribute\u201d and \u201cinference with multiple attribute\u201d result in Table 9. Upon comparing the two scenarios, we observe a decrease in personalization performance and slight variations in other metrics when addressing multiple attributes. This observation suggests the potential existence of contradictions among these attributes, which our model adeptly manages."
        },
        {
            "heading": "E Detailed Results of Personalized Generation",
            "text": "We present the detailed results for eight different personality combinations on the following pages. Additionally, we provide human-annotated attributes for the \u201clyrical + optimistic + critical \u201d\nand \u201cplain + pessimistic + emotional\u201d personas. Analyzing the tables, we observe that BOB and MSP tend to overlook the content of the dialogue, leading to repetitive and incoherent responses. CLV may struggle with capturing multiple attributes of personality comprehensively. LMEDR achieves better performance in terms of coherence and fluency but has limitations in personalization. Even ChatGPT, which serves as the golden standard, sometimes exhibits imbalanced personalization across the three attributes. In comparison, our proposed MIRACLE model demonstrates the best overall personalization results while maintaining high quality in terms of fluency and coherence in the generated responses."
        }
    ],
    "title": "MIRACLE: Towards Personalized Dialogue Generation with Latent-Space Multiple Personal Attribute Control",
    "year": 2023
}