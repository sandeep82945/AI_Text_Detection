{
    "abstractText": "Attention weight is a clue to interpret how a Transformer-based model makes an inference. In some attention heads, the attention focuses on the neighbors of each token. This allows the output vector of each token to depend on the surrounding tokens and contributes to make the inference context-dependent. We analyze the mechanism behind the concentration of attention on nearby tokens. We show that the phenomenon emerges as follows: (1) learned position embedding has sinusoid-like components, (2) such components are transmitted to the query and the key in the selfattention, (3) the attention head shifts the phases of the sinusoid-like components so that the attention concentrates on nearby tokens at specific relative positions. In other words, a certain type of Transformer-based model acquires the sinusoidal positional encoding to some extent on its own through Masked Language Modeling.",
    "authors": [
        {
            "affiliations": [],
            "name": "Yuji Yamamoto"
        },
        {
            "affiliations": [],
            "name": "Takuya Matsuzaki"
        }
    ],
    "id": "SP:b71d8df0a0b3ecce9de3c610206e45d0cdfa1f4d",
    "references": [
        {
            "authors": [
                "Tyler Chang",
                "Zhuowen Tu",
                "Benjamin Bergen."
            ],
            "title": "The geometry of multilingual language model representations",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 119\u2013136, Abu Dhabi, United",
            "year": 2022
        },
        {
            "authors": [
                "Kevin Clark",
                "Urvashi Khandelwal",
                "Omer Levy",
                "Christopher D. Manning."
            ],
            "title": "What does BERT look at? an analysis of BERT\u2019s attention",
            "venue": "Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for",
            "year": 2019
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association",
            "year": 2019
        },
        {
            "authors": [
                "Philipp Dufter",
                "Martin Schmitt",
                "Hinrich Sch\u00fctze."
            ],
            "title": "Position information in transformers: An overview",
            "venue": "Computational Linguistics, 48(3):733\u2013 763.",
            "year": 2022
        },
        {
            "authors": [
                "Kazuki Irie",
                "Albert Zeyer",
                "Ralf Schl\u00fcter",
                "Hermann Ney."
            ],
            "title": "Language Modeling with Deep Transformers",
            "venue": "Proc. Interspeech 2019, pages 3905\u2013 3909.",
            "year": 2019
        },
        {
            "authors": [
                "Amirhossein Kazemnejad",
                "Inkit Padhi",
                "Karthikeyan Natesan Ramamurthy",
                "Payel Das",
                "Siva Reddy."
            ],
            "title": "The impact of positional encoding on length generalization in transformers",
            "venue": "arXiv preprint arXiv:2305.19466.",
            "year": 2023
        },
        {
            "authors": [
                "Olga Kovaleva",
                "Alexey Romanov",
                "Anna Rogers",
                "Anna Rumshisky."
            ],
            "title": "Revealing the dark secrets of BERT",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on",
            "year": 2019
        },
        {
            "authors": [
                "Mike Lewis",
                "Yinhan Liu",
                "Naman Goyal",
                "Marjan Ghazvininejad",
                "Abdelrahman Mohamed",
                "Omer Levy",
                "Veselin Stoyanov",
                "Luke Zettlemoyer"
            ],
            "title": "BART: Denoising sequence-to-sequence pre\u2013 training for natural language",
            "year": 2020
        },
        {
            "authors": [
                "Yongjie Lin",
                "Yi Chern Tan",
                "Robert Frank."
            ],
            "title": "Open sesame: Getting inside BERT\u2019s linguistic knowledge",
            "venue": "Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 241\u2013253, Flo-",
            "year": 2019
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "RoBERTa: A robustly optimized bert pretraining approach",
            "venue": "arXiv preprint arXiv:1907.11692.",
            "year": 2019
        },
        {
            "authors": [
                "Stephen Merity",
                "Caiming Xiong",
                "James Bradbury",
                "Richard Socher."
            ],
            "title": "Pointer sentinel mixture models",
            "venue": "International Conference on Learning Representations.",
            "year": 2017
        },
        {
            "authors": [
                "Alec Radford",
                "Jeff Wu",
                "Rewon Child",
                "David Luan",
                "Dario Amodei",
                "Ilya Sutskever"
            ],
            "title": "Language models are unsupervised multitask learners",
            "year": 2019
        },
        {
            "authors": [
                "Maithra Raghu",
                "Justin Gilmer",
                "Jason Yosinski",
                "Jascha Sohl-Dickstein."
            ],
            "title": "SVCCA: Singular vector canonical correlation analysis for deep learning dynamics and interpretability",
            "venue": "Advances in Neural Information Processing Systems, volume 30.",
            "year": 2017
        },
        {
            "authors": [
                "Vinit Ravishankar",
                "Anders S\u00f8gaard."
            ],
            "title": "The impact of positional encodings on multilingual compression",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 763\u2013777, Online and Punta Cana, Do-",
            "year": 2021
        },
        {
            "authors": [
                "Peter Shaw",
                "Jakob Uszkoreit",
                "Ashish Vaswani."
            ],
            "title": "Self-attention with relative position representations",
            "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech-",
            "year": 2018
        },
        {
            "authors": [
                "Jianlin Su",
                "Yu Lu",
                "Shengfeng Pan",
                "Ahmed Murtadha",
                "Bo Wen",
                "Yunfeng Liu"
            ],
            "title": "Roformer: Enhanced transformer with rotary position embedding",
            "year": 2022
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141 ukasz Kaiser",
                "Illia Polosukhin."
            ],
            "title": "Attention is all you need",
            "venue": "Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc.",
            "year": 2017
        },
        {
            "authors": [
                "Benyou Wang",
                "Lifeng Shang",
                "Christina Lioma",
                "Xin Jiang",
                "Hao Yang",
                "Qun Liu",
                "Jakob Grue Simonsen."
            ],
            "title": "On position embeddings in BERT",
            "venue": "International Conference on Learning Representations.",
            "year": 2021
        },
        {
            "authors": [
                "Yu-An Wang",
                "Yun-Nung Chen"
            ],
            "title": "What do position embeddings learn? an empirical study of pre-trained language model positional encoding",
            "year": 2020
        }
    ],
    "sections": [
        {
            "text": "Attention weight is a clue to interpret how a Transformer-based model makes an inference. In some attention heads, the attention focuses on the neighbors of each token. This allows the output vector of each token to depend on the surrounding tokens and contributes to make the inference context-dependent. We analyze the mechanism behind the concentration of attention on nearby tokens. We show that the phenomenon emerges as follows: (1) learned position embedding has sinusoid-like components, (2) such components are transmitted to the query and the key in the selfattention, (3) the attention head shifts the phases of the sinusoid-like components so that the attention concentrates on nearby tokens at specific relative positions. In other words, a certain type of Transformer-based model acquires the sinusoidal positional encoding to some extent on its own through Masked Language Modeling."
        },
        {
            "heading": "1 Introduction",
            "text": "The architecture of Transformer (Vaswani et al., 2017) is symmetric with respect to the token position and it captures word order only through the position embedding included in the input. Thanks to this design, Transformer can flexibly learn relationships between tokens while allowing parallelization. To derive a representation of a context, previous language models have used, for instance, recurrent units to process tokens in the given token order, or convolution to aggregate tokens within a given range. In these models, the property of translation invariance of language has been captured through the architecture of the models. In contrast, Transformer receives all tokens at the same time and does not restrict the positions of the tokens on which each token depends; in exchange for it, the translation invariance has to be learned rather than imposed by the architecture.\nPosition embedding is thus the key to make inferences in Transformer context-dependent. While Transformer uses deterministic sinusoidal position embedding, BERT (Devlin et al., 2019) uses a learnable absolute position embedding. The latter learns the positional representation only through word cooccurrences. Clark et al. (2019) and Kovaleva et al. (2019) investigated the attention weights of the self-attention in BERT and found that the attention in some heads is largely determined by relative positions (Figure 1). This implies that even when the absolute position embedding is used, self-attention can make an inference depending on relative position.\nWang et al. (2021) compared various position embedding methods based on the performance in downstream tasks. Their results showed that local translation invariance and asymmetric position bias with respect to the direction improved the performance. Ravishankar and S\u00f8gaard (2021) observed that some columns of absolute position embedding were periodic. Chang et al. (2022) showed that position representation was periodic even in hidden representations. However, it is not clear how the periodicity is used in the model.\nIn this work, we analyze how attention depends on relative position. As a result, we show that the relative positional dependence of attention emerges due to the following factors.\n\u2022 The learnable absolute position embedding has sinusoid-like waves with several limited frequencies (\u00a74.1.1).\n\u2022 Attention heads extract periodic components derived from position embedding from the hidden states. It is made explicit by applying Singular Value Decomposition to the parameters of the pre-trained language model (\u00a74.2).\n\u2022 The self-attention shifts the phase of the periodic components in the query and the key to decide the direction of attention (\u00a74.3, \u00a74.4).\nThus, it becomes partially clear how the selfattention equipped with learnable absolute position embeddings makes inferences based on context. However, it is suggested that, when the attention is strongly concentrated on the adjacent tokens, the word embedding is also a factor that enables inference based on relative position (\u00a75)."
        },
        {
            "heading": "2 Background",
            "text": "In this section, we review the multi-head selfattention mechanism and position embedding."
        },
        {
            "heading": "2.1 Multi-Head Self-Attention",
            "text": "The self-attention mixes the representations of tokens at different positions. The input to the l-th attention layer is a matrix Xl \u2208 RT\u00d7d whose kth row corresponds to the hidden representation of the k-th token as a vector in Rd. The output of the self-attention layer is defined as follows:\nAlh = softmax\n( XlW Q lh(XlW K lh )\nT\u221a d/n\n) (1)\nOlh = AlhXlW V lh (2) MultiHeadl = concat(Ol1, . . . , Oln)WOl (3)\nwhere WQlh , W K lh , W V lh \u2208 Rd\u00d7(d/n) and WOl \u2208 Rd\u00d7d are the parameters, n is the number of attention heads per layer, and the subscripts l and h are the indices of the layer and the head, respectively.\nIn this paper, we refer to the h-th head in the l-th layer as \u201chead(l, h)\u201d for short, and omit the subscripts l, h when the discussions are common to all layers and heads. The matrices XWQ, XWK , and A are called query, key, and attention weight, respectively. If the (i, j) element of A is large, it is interpreted that the i-th token in the sentence attends to the j-th token."
        },
        {
            "heading": "2.2 Position Embedding",
            "text": "Transformer\u2019s position embedding is defined as follows (Sinusoidal Position Embedding; SPE):\nSPE(pos,2i) = sin(pos/10000 2i/d) (4)\nSPE(pos,2i+1) = cos(pos/10000 2i/d). (5)\nVaswani et al. (2017) hypothesized that it allows the model to easily learn to attend by relative positions, since the offset between two tokens can be expressed as a matrix product:[\nsinxi cosxi ] [cos \u03b8 \u2212 sin \u03b8\nsin \u03b8 cos \u03b8 ] = [ sin(xi + \u03b8) cos(xi + \u03b8) ] . (6)\nBERT\u2019s position embedding is a learnable parameter. It is called the Absolute Position Embedding (APE) because each row represents an absolute position in the input sequence.\nRoBERTa (Liu et al., 2019) is identical to BERT in architecture. RoBERTa is pre-trained with 512 tokens for all steps whereas BERT is pre-trained with 128 tokens for 90% of the steps. Hence, RoBERTa\u2019s APE recieves the same number of updates regardless of the positions. Wang and Chen (2020) showed that RoBERTa\u2019s APE is more orderly than BERT\u2019s. We thus use RoBERTa for all experiments to analyze the position dependence of inference."
        },
        {
            "heading": "3 Relative Position Dependence of Attention",
            "text": "In this section, we demonstrate that the attention depends on the relative position. Specifically, we analyze in how many heads the attention is dependent on the relative position, and the variation in the direction and concentration of the attention across the heads. The visualizations of the attention weight Alh in Figure 1 shows that, in (a) and (d), each token strongly attends only to the adjacent token, while in (b) and (e), the attention is put on a few nearby tokens on the left or right side. We show that such a pattern is largely invariant with respect to the inputs, by clustering the patterns of the attention on different inputs.\nTo focus on the relation between the attention and the relative positions, we summarized the attention weight for each input in each head into a vector and applied k-means clustering to them. For that, we defined t-offset trace trt as follows:\ntrt(A) = {\u2211T\u2212t i=1 (A)i,i+t (t \u2265 0)\u2211T+t i=1 (A)i\u2212t,i (t < 0)\n(7)\nand transformed Alh to a vector:\nalh = [tr\u221210(Alh), . . . , tr10(Alh)] \u2208 R21. (8)\nThese vectors represent the trend of attention Alh with respect to relative position.\nWe input 100 sentences of length 512 created from wikitext-2 (Merity et al., 2017) into RoBERTa and computed vectors alh for every head. Figure 2 shows the results of applying the k-means to a total of 100 \u00d7 12 \u00d7 12 vectors alh when the number of clusters was set to 6. In the clusters named leftward and next-to-left\n(resp. rightward and next-to-right), the attention is concentrated on the nearby tokens on the left (resp. right). Particularly, in the clusters named next-to-left and next-to-right, the attention is strongly concentrated on the adjacent tokens. We found that, for each head, the vectors alh corresponding to the 100 inputs were grouped into one or two clusters. This means that there are some heads that always attended to the same direction for all the input."
        },
        {
            "heading": "4 Attention to Nearby Tokens",
            "text": "In this section, we show that the attention depends on relative position due to periodic components in the position embeddings. First, we show that the learnable absolute position embeddings acquire several limited frequency components. Second, we show that some attention heads extract the periodicity derived from position embeddings. Finally, we show that the concentration of attention on the nearby tokens is due to the periodicity."
        },
        {
            "heading": "4.1 Learned Representation of Positions",
            "text": "First, we show that APE includes sinusoid-like components using the Discrete Fourier Transform (DFT). Next, we show that the position embeddings are confined to a relatively low-dimensional subspace (~15 dimensions) using Principal Component Analysis (PCA). Finally, we show that the encoded positional information occupies a similar number of dimensions in the hidden states as in the position embedding, and the dimensionality becomes smaller in the higher layers using Canonical Correlation Analysis (CCA)."
        },
        {
            "heading": "4.1.1 APE Includes Sinusoid-like Waves",
            "text": "We view the RoBERTa\u2019s position embedding Epos \u2208 RT\u00d7d as a collection of d time series of\nlength T . We computed the amplitude spectrum of each column of Epos by applying DFT:\nspeci = abs(DFT(Epos ei)) (9)\nwhere ei \u2208 Rd is the i-th elementary unit vector. The vector Eposei is hence the i-th column of Epos.\nFigure 3a shows the mean and the quartile range of the amplitude spectra of d = 768 time series. The amplitude spectra peak at some frequencies, indicating that the learnable APE has acquired periodicity through pre-training, even though it is not explicitly defined using sinusoidal waves as in Transformer. In contrast, there are no peaks in the amplitude spectrum of a word embedding sequence of a sample text.1 The periodicity is thus an unique property of learned position embeddings.\nWe investigated whether similar properties are present in pre-trained models other than RoBERTa (Figure 3b and 13). The amplitude spectra of the encoder-only models are similar to RoBERTa regardless of language, but ones of GPT-2 (Radford et al., 2019) are higher at lower frequencies.\nThe decoder model probably can focus attention on the neighboring tokens of itself without periodicity-based mechanisms (\u00a74.3). For example, if attention is paid more strongly to the backward token, in BERT, attention is focused on the end of the sentence, but in a decoder with causal attention mask, attention is focused on itself. We leave it to future research to clarify whether this\n1The text taken from abstract and introduction of (Vaswani et al., 2017) was used as input to create the figures in this paper, unless otherwise stated.\nphenomenon really occurs in the causal model, and in this paper we focus on the case where selfattention is symmetric with respect to position."
        },
        {
            "heading": "4.1.2 Dimensionality of Positional Representation",
            "text": "We applied PCA to the position embeddings to see how many dimensions were used to represent the positions. The cumulative principal component contribution rate of the position embedding was 50.51% for the top 4 dimensions, and 92.23% for the top 12 dimensions (see Figure 12 in Appendix A for a plot). This means that the positions are mostly encoded in a low dimensional subspace.\nWe then employed CCA to show how much the input to the self-attention layers included the positional representation. CCA is a method for investigating the relation of two sets of variables by finding the parameters a and b that maximizes the correlation between two synthetic variables Xa and Y b given two inputs X and Y . Raghu et al. (2017) showed that CCA allows layer-to-layer comparison of deep learning models. We used the representation of neurons and layers proposed by them and computed the correlation between the hidden layer and the position embeddings.\nIn this study, the i-th neuron zli of layer l and the l-th layer Z l are represented as follows:\nzli = [ zli(x1), . . . , z l i(xm) ]T (10)\nZ l = [ zl1, . . . , z l n ] (11)\nwhere zli(xj) is the response of the i-th neuron to input xj . We input 200 sentences of length 512 created from wikitext-2 into RoBERTa and collected the responses of each layer for the input of m = 200\u00d7 512 tokens. We then maximized their correlation coefficients \u03c1k:\n\u03c1k = max ak,bk\nCorr(Z l1ak, Z l2bk) (12)\nwhere ak is chosen such that it is orthogonal to a1, . . . ,ak\u22121 and similarly for bk. The CCA coefficients in Figure 4 show that the hidden states of higher layers have lower correlations with the position embeddings. This is in agreement with Lin et al.\u2019s (2019) result that BERT phases out positional information as it passes through the layers.\nThe CCA result indicates that the number of components that are highly correlated with position embedding is only 5~20-dimensions, and the PCA result suggests that it is enough to accommodate the positional representation. This indicates that the hidden states include positional representation in a low-dimensional subspace, similarly to position embedding."
        },
        {
            "heading": "4.2 Positional Representation in Self-Attention",
            "text": "It is not at all clear how the positional representation in the hidden states contribute to the inference in self-attention. We thus analyzed how attention weight is calculated, which is one of the most important process in self-attention."
        },
        {
            "heading": "4.2.1 Rethinking About Query and Key",
            "text": "The attention weight A is determined by the inner product of the rows of the query and the key matrix. We thus expect that the relative position dependence of the attention can be explained by analyzing the relationship between the query and\nthe key. We begin by rethinking about their definition. The two parameter matrices WQ and WK contribute to the attention only through their product WQ(WK)T (Eq. (1)). Hence, they can be seen as a parametrization of a rank-restricted d\u00d7 d matrix, and we may consider another decomposition of the product WQ(WK)T .\nWe found that some sinusoid-like components were obtained from the query and the key by applying Singular Value Decomposition (SVD) to WQ(WK)T . Specifically, the query and the key are redefined as the product of the hidden state and the singular vectors:\nWQ(WK)T = UQS(UK)T (13)\nQ = XUQ, K = XUK (14)\nwhere the matrix S \u2208 Rd\u00d7d is a diagonal matrix diag(s1, . . . , sd/n, 0, . . . , 0) and each element si is a singular value. In the rest of this paper, we refer to Q and K defined above as the query and the key, respectively. As shown in Figure 5, in the head in which the attention depends on relative position, sinusoid-like waves appear in several columns of the redefined query Q and the key K. Moreover, a sine wave is paired with a cosine wave of the same frequency, as the 5-th and the 6-th column for head(1, 1) shown in Figure 5.\nFurthermore, the introduction of SVD provides a new theoretical interpretation of self-attention. Let R be the orthogonal matrix R = (UQ)TUK . Then UK can be written as UK = UQR due to the orthogonality of the singular vectors. Thus, the key K can be written as:\nK = XUK = XUQR = QR (15)\nThat is to say, the rows of the key are the result of an orthogonal transformation of the rows of the\nquery. The relation between the query and the key is thus summarized in the matrix R.\nIn addition, since the singular value matrix S is diagonal, the product of the query and the key can be written as follows:\nXWQ(XWK)T = QSKT = d/n\u2211 i=1 siqik T i (16)\nwhere qi and ki are the i-th columns of Q and K, respectively. Eq. (16) says that the subsets of queries [qi] and keys [ki] corresponding to the top singular values [si] are more important in the calculation of the attention distribution. We hereafter call the matrix QSKT attention score (i.e., attention before applying softmax)."
        },
        {
            "heading": "4.2.2 Spectral Analysis of Query and Key",
            "text": "We computed the amplitude spectrum of each column of the query Qlh in each head, using the same procedure for the position embeddings in \u00a74.1.1:\nspeclhi = abs(DFT(Qlhei)) (17)\nmax-specl = [max h,i (speclhi)f ]f=0,...,T/2. (18)\nBy taking the maximum of speclhi among the heads and the columns of Qlh, we check if there are high peaks in the spectra of the queries in the l-th layer (similarly for the keys and hidden states).\nFigure 6 show that the query and key spectra peak at the same frequency bands as the position embeddings, indicating that attention heads extract periodic representations derived from the position embeddings. Furthermore, some of the peaks disappear in the higher layers (downward in the Figure 6), indicating that the periodic components derived from the position embeddings are not dominant in the higher layers. This is consistent with the result that the correlation between the position embeddings and the hiddem states of each layer gradually decreases (\u00a74.1.2). It also agrees with the result shown by Lin et al. (2019) that position information is discarded between the 3rd and 4th layers."
        },
        {
            "heading": "4.3 Attention Based on Relative Position is due to the Phase Shift",
            "text": "As shown in Figure 5, there are phase shifts in the sinusoid-like components of the query and the key. In this subsection, we focus on the phase shifts, and clarify that the direction and the width of the phase shifts determine where the attention is concentrated. We measure the direction and the width of the phase shift by cross-covariance and crosscorrelation, defined as follows:\nxcovj(t) = {\u2211T\u2212t i=1 qi,jki+t,j (t \u2265 0)\u2211T+t i=1 qi\u2212t,jki,j (t < 0) (19) xcorrj(t) = xcovj(t)\u2212 Et[xcovj(t)]\n\u2225qj\u2225\u2225kj\u2225 (20)\nFor example, Figure 7 shows the cross-correlation between the queries and keys in Figure 5. Both xcorr5(t) and xcorr6(t) attain a maximal value at t = \u22122 and t = \u22123. It means that the phase shift between the queries and the keys are approximately 2.5 tokens.\nIt can be explained theoretically that the direction of phase shift coincides with the direction of attention. The cross-covariance is related to the product of query and key (Eq. (16)):\ntrt(QSKT ) = T\u2212t\u2211 i=1 (QSKT )i,i+t (21)\n= T\u2212t\u2211 i=1 d/n\u2211 j=1 sjqi,jki+t,j (22)\n= d/n\u2211 j=1 sjxcovj(t) (23)\nwhere trt is the t-offset trace defined in Eq. (7) and s1, . . . , sd/n are the singular values of WQ(WK)T . According to Eq. (21-23), the sum of the attention scores at relative position t is equal to the weighted sum of the inner product of qj and kj shifted by t. Therefore if the xcovj(t)s corresponding to the top singular values attain maximal values at the same t, the attention is likely to be concentrated around relative position t.\nFigure 8 and 9 show that it is actually the case. Figure 9 presents the cross-correlations of the query qj and the key kj for js corresponding to the top singular values. We can see how the concentration of attention on nearby tokens is formed by the weighted superposition of the crosscorrelations. In head(1, 1), the maxima of the cross-correlations near t = \u22121 stack up, while the maxima away from t = \u22121 cancel with the minima of other components. Since there are multiple periodic components with different frequencies (Figure 3a), the attention is not concentrated away from each token. In contrast, in head(8, 9), some cross-correlations have narrow peaks only at t = \u22121, and it makes the attention be very concentrated only on the adjacent tokens. However, it is unlikely that the cause of the narrow peaks\nis the sinusoid-like waves, because their period is approximately 8 tokens or more.2"
        },
        {
            "heading": "4.4 Phase Shift Width is the Same even if Frequencies are Different",
            "text": "We saw in the previous subsection that the main factor of the concentration of attention is that the phases of multiple sinusoid-like waves are all shifted by the same number of tokens. In this subsection, we explain it based on the property of the eigenvalues and the eigenvectors of the matrix R that relates the query Q to the key K.\nLet pi \u2208 Cd and \u03bbi \u2208 C be an eigenvector and an eigenvalue of the matrix R, respectively. Since R is an orthogonal matrix, its eigenvalue can be expressed as \u03bbi = ej\u03b8i where j is the imaginary unit. From Eq. (15), the following relation between the query and the key holds:\nKpi = QRpi = Q\u03bbipi = Qpi \u00b7 ej\u03b8i . (24)\nLet two conditions be assumed: (1) for each i = 1, . . . , d, the time series Qpi is sinusoidal with a single frequency fi and (2) the ratio of the argument \u03b8i of the eigenvalue \u03bbi to the frequency fi is constant regardless of i. Then Eq. (24) implies that the phases of Qpi and Kpi differ by a constant number of tokens for any eigenvector pi:\n(Qpi)t = (Kpi)t+\u2206 (25)\nwhere \u2206 = (T/2\u03c0) \u00b7 (\u03b8i/fi). Furthermore, Qt = Kt+\u2206 follows from Eq. (25). We provide the proofs in Appendix D.\nWe verify whether the two conditions hold by analyzing the periodicity of Qpi and the ratio of the frequency to the argument \u03b8i of the eigenvalue \u03bbi. To do so, we define the bivariate functions g for frequency and argument as follows:\ng(f, \u03b8i) = abs(DFT(Qpi))f (26)\nThis function g is shown in Figure 10 as a 2D histogram. Figure 10 shows that the spectrum of Qpi has peaks in different frequency bands according to \u03b8i. It means that the component along each eigenvector pi is fairly band-limited, namely they are sinusoid-like. Furthermore, the spectral peaks\n2Figure 3a shows that the maximum frequency of the position embeddings is around 60. The minimum period is hence 512/60 (> 8) tokens.\nappear around a straight line. Specifically, the ratio \u2206(f, \u03b8) of frequency to phase defined as follows is nearly constant:\n\u2206(f, \u03b8) = T 2\u03c0 \u00b7 \u03b8 f . (27)\nHence, the two conditions are in fact approximately hold. Thus, by multiplying the matrix R, the sinusoid-like components in the key are offset by a certain number of tokens relative to the query."
        },
        {
            "heading": "5 Attention to the Adjacent Tokens",
            "text": "Figure 9 shows that, in head(8, 9), there are not only sinusoid-like waves but also spikes at the relative position t = \u22121. In this section, we dive deeper into the fact that, in some heads, the attention is focused only on the adjacent token. We analyze how the attention changes when we modify either the position embedding or the word embedding component. The following are the results on the modified inputs and insights from them.\nBaseline The input is the sum of word embedding and position embedding as usual. This result is shown in Figure 8 and 9.\nOnly position embedding The word embeddings are replaced with zero vectors. Figure 11a shows that spikes of cross-correlation do not appear for this case. This suggests that position embedding is not the only cause of the spikes.\nOnly word embedding The position embeddings are replaced with zero vectors. Figure 11b shows that most of the cross-correlations are flat for this case. It suggests that word embedding contributes less to make the attention dependent on relative position. However,\nit is interesting that the cross-covariance at t = \u22121 (i.e., left neighbor) is relatively large even without position information.\nShuffling word embeddings The order of word embeddings is shuffled. Figure 11c shows that spikes appear at the relative position t = \u22121 even for this case. It suggests that the contribution of position embedding is significant in determining the direction of attention, since the attention is focused on the preceding word regardless of what it actually is.\nAs mentioned at the end of Section 4.3 and also suggested by Figure 11a, it is unlikely that position embedding is the only cause of the strong concentration of attention on the adjacent tokens. However, if the word embeddings identified the adjacent tokens (i.e., if the narrow peak of cross-correlation appeared due to an interaction of the word embeddings of a frequently occurring bigram), the attention would have been moved to non-neighboring positions by the shuffling of word embeddings, but this was not the case. It is thus suggested that the concept of adjacency in RoBERTa is built upon both word meaning and positional information."
        },
        {
            "heading": "6 Remark on the Learning Process of Position Embeddings",
            "text": "The training data of the masked language modeling is only word sequences without positional information. It suggests that the relative position dependence of attention is acquired by the combination of two factors: (1) the linguistic property that related words tend to appear nearby due to grammatical rules and collocations, and (2) the property that attention is focused on the word that is syntactically or semantically related and hence gives clue to fill the masked token. In appendix E, we demonstrate that the position representation can be acquired with a much smaller amount of training data than pre-training."
        },
        {
            "heading": "7 Related Works",
            "text": "Ravishankar and S\u00f8gaard (2021) observed that some columns of absolute position embedding were periodic and sinusoidal position embedding was more effective in multilingual models than other embedding methods. Chang et al. (2022) showed that position representation was periodic even in hidden representations by using Linear\nDiscriminant Analysis, i.e., by identifying the axes separating the different position representations. We showed that sinusoid-like components could be extracted from the hidden states by applying SVD to the pre-trained parameters of the selfattention even though SVD does not have the objective of separating positional representations.\nVaswani et al. (2017) stated that the sinusoidal position embedding allowed position offsets to be represented by a rotation transformation (Eq. (6)), and this could prompt learning of attention that depends on relative position. We showed that the query and the key in the self-attention included sinusoid-like components and the learned parameters in the self-attention shift the phase of the query and the key relatively. This means that the mechanism hypothesized by Vaswani et al. is in fact acquired through pre-training by masked language modeling. However, the frequencies acquired by the position embedding of RoBERTa are only in the specific frequency bands, whereas sinusoidal position embedding has d/2 frequencies (Eq. (4)-(5)). RoBERTa thus seems to have acquired a more economical positon embedding than sinusoidal position embedding.\nRecently, there are many variants of position\nembedding and Dufter et al. (2022) surveyed them exhaustively. In particular, Rotary Position Embedding (RoPE) (Su et al., 2022), a type of Relative Position Embedding (RPE) (Shaw et al., 2018), relates to the property that the self attetnion acquires the rotation matrix through pre-training. To acquire relative positional dependence, RoPE widens the angle between query and key proportionally to the relative position, while pre-trained self-attention rotates the hidden states containing absolute positional bias by the same angle regardless of position. In other words, APE and selfattention, which are distant components, must acquire frequency and angle of rotation, respectively, to satisfy the relation Eq. (27). If translation invariance is essential to language understanding, the cost of discovering this relation is a possible reason why APEs are inefficient to learn compared to RPEs."
        },
        {
            "heading": "8 Conclusion",
            "text": "We analyzed the concentration of attention based on relative position in the self-attention using the learnable absolute position embedding. As a result, we showed that it is due to relative phase shifts of the periodic components in the query and the key derived from position embeddings. Our results explain in part that absolute position embedding allows inference based on relative position."
        },
        {
            "heading": "9 Limitations",
            "text": "As mentioned in \u00a74.1.1, the positional representation of the GPT-2 differs significantly from ones of the encoder models. Thus, it is impossible to interpret the inferences of the decoder-only model in the same way as in this study.\nIn the tasks of predicting sentence structure (e.g., dependency parsing), the relative order of two tokens is important regardless of the distance between them. However, we analyzed the dependence of the output of each token only on the nearby tokens. Thus, it is unclear whether position embedding provides relative position information that helps determine which of the distant tokens precedes the other.\nWe obtained an interpretable representation by decomposing the attention scores before applying softmax function (Eq. (16)). When analyzing the contribution of the decomposed representations to downstream components (e.g., Value in\nself-attention), the non-linearity of softmax function should be taken into account."
        },
        {
            "heading": "A Cumulative Principal Component Contribution Rate",
            "text": ""
        },
        {
            "heading": "B Amplitude Spectra of Various Models",
            "text": "We computed the amplitude spectra of the position embeddings of bert-base-uncased, cl-tohoku/bert-base-japanese-whole-word -masking, xlm-roberta-base, and gpt2 in the same way as Figure 3a. In Figure 13, the encoder models BERT and RoBERTa both have peaks in their amplitude spectra, and RoBERTa has a higher peaks. On the other hand, the GPT-2 decoder model has only low-frequency components. This suggests that the representation of absolute position embedding is similar between encoder models, regardless of language, but differs significantly between encoder models and decoder models. In fact, Irie et al. (2019) and Kazemnejad et al. (2023) showed that the position embeddings is unnecessary for the decoder models."
        },
        {
            "heading": "C Comparing Different Architectures",
            "text": "This paper analyzed an encoder-only model (RoBERTa). In this section, we apply our methods to other transformer-based architectures: decoderonly and encoder-decoder. The target pre-trained models are GPT-2 (gpt2) for decoder-only and BART (facebook/bart-base) (Lewis et al., 2020) for encoder-decoder, both of which use absolute position embedding and are available on huggingface. To reduce the gap between architectures, when analyzing the decoders, we use the attention scores, which is the matrix before causal masking and softmax function are applied, instead of the attention weights.\nThe attention matrices of GPT-2 show two main patterns: one related to position and the other not. Figure 14a shows that the attention head of GPT2 pays stronger attention to the closer tokens by\nconcentrating attention on the backward tokens, which are masked in the subsequent processing. Figure 15 shows the result of clustering the attention scores of GPT-2 using k-means. The number of clusters was set to 2 and the inputs to k-means were t-offset traces (Eq.(8)) from t = \u221230 to 30. The heads that depend on position are found in the lower layers.\nAlong \u00a74.4, we investigate the relationship between the frequency of the hidden state and the angle of the rotation matrix inherent in the parameters of attention head. Figure 16a differs from the case of RoBERTa (Figure 10) in that the peaks appear horizontally rather than holding a constant ratio between frequency and angle. Furthermore, according to Figure 13, the dominant components in position embedding of GPT-2 are those with frequencies below 10, i.e., with periods longer than 51.2 (= 512/10) tokens. These differences from RoBERTa imply that even when the attention is focused within a few tokens in a particular head, it cannot be attributed to the mechanism described in \u00a74.3.\nIn BART, the tendency in the positional dependence of attention differs between encoder and\ndecoder. In encoder, the patterns of attention weights (Figure 17) and their trend (Figure 18), such as direction and strength, are similar to those of RoBERTa (Figure 1 and 2). On the other hand, comparing Figure 14 and Figure 20, the patterns of decoder is different from those of GPT-2. Such patterns in the BART decoder could not be clustered based on the strength or direction of attention using k-means. Like in GPT-2, Figure 21 shows that the peaks appear horizontally in the decoder.\nFigures for GPT-2\nFigures for BART encoder\nFigures for BART decoder"
        },
        {
            "heading": "D Theorems About the Phase Shift Between the Query and the Key",
            "text": "Theorem 1. The following two assumptions are given:\n1. Qpi is sinusoidal with a single frequency fi.\n2. The ratio of the argument \u03b8i of an eigenvalue \u03bbi to the frequency fi is constant regardless of i."
        },
        {
            "heading": "In this case, Eq. (25) is derived from Eq. (24):",
            "text": "Kpi = Qpi \u00b7 ej\u03b8i (24) =\u21d2 (Qpi)t = (Kpi)t+\u2206 (25)\nProof. Since Qpi is sinusoidal, the t-th element in polar form is as follows:\n(Qpi)t = ri exp[j2\u03c0fit/T ] (28)\nwhere ri is the absolute value of (Qpi)t. Then the t-th element of Kpi can be written as follows from Eq. (24):\n(Kpi)t = ri exp[j2\u03c0fit/T ] \u00b7 exp[j\u03b8i] (29) = ri exp[j(2\u03c0fit/T + \u03b8i)] (30)\n= ri exp[j2\u03c0fi(t+\u2206i)/T ] (31)\nwhere \u2206i = (T/2\u03c0) \u00b7(\u03b8i/fi). Now, since the ratio of the argument \u03b8i to the frequency fi is constant, \u2206i is independent of i, namely, \u2206i can be replaced with a constant \u2206. Thus, (Qpi)t = (Kpi)t+\u2206 holds from Eq. (28) and Eq. (31).\nLet the matrix P = [p1,p2, . . . ,pd], then the following equation holds:\n(QP )t = (KP )t+\u2206. (32)\nTheorem 2. When, in a given basis, the position of each token in the key is offset by \u2206(\u2208 Z) tokens relative to the query, then in any basis, there is an\noffset of \u2206 tokens between the query and the key. That is,\n\u2203P \u2208 Cd\u00d7d s.t. |P | \u0338= 0 and (QP )t = (KP )t+\u2206 (33)\n=\u21d2 \u2200B \u2208 Cd\u00d7n, (QB)t = (KB)t+\u2206 (34)\nwhere Q,K \u2208 RT\u00d7d are the query and the key respectively and n \u2208 N is less than or equal to d. The subscript t indicates the t-th row of the matrix.\nProof. Let B be an arbitrary matrix. Then,\n(QB)ti = (QPP \u22121B)ti (35)\n= d\u2211 k=1 (QP )tk(P \u22121B)ki (36)\n= d\u2211 k=1 (KP )t+\u2206,k(P \u22121B)ki (\u2235 Eq. (33))\n(37)\n= (KPP\u22121B)t+\u2206,i (38)\n= (KB)t+\u2206,i (39)\nIn particular, when B = I , the equation Qt = Kt+\u2206 is obtained."
        },
        {
            "heading": "E How the Relative Position Dependence of Attention Emerges",
            "text": "How does a masked language model acquire the concept of \u201cneighborhood\u201d even though absolute position embedding learns without the information about the order of the tokens? In this section, we demonstrate the process of acquiring the ability to focus attention on nearby tokens by relearning the position embedding under the masked language modeling.\nSuppose that the following sentence is input into a pre-trained model:\n\u201cThis <unk> is <unk> an <unk> <mask> .\u201d\nSince this sentence is collapsed with many UNK tokens, the model cannot fill the MASK correctly. If we re-learn the position embedding, will the model be able to fill it?\nE.1 Experiment\nWe additionally train RoBERTa with the following configurations.\nDataset We used 8645 samples consisting of more than 128 tokens from wikitext-2.\nPutting UNK token Three UNK tokens were inserted after each token in all sentences; the token sequence [t1, t2, ..., t128] was expanded to [t1, unk, unk, unk, t2, unk, unk, unk, . . . , t128, unk, unk, unk].3\nFreezing parameters We freezed all parameters except position embeddings. This encourages the position embeddings to learn to focus attention only on position t = 1, 5, 9, 13, \u00b7 \u00b7 \u00b7 and prevents positionindependent shortcut solutions (e.g., shortening the UNK embedding to ignore it).\nPosition embedding The position embeddings were initialized with random values following a normal distribution N (0, 0.022) before training.\nE.2 Results\nTable 1 shows that, before the additional training, RoBERTa cannot fill MASK with the appropriate tokens (in this case, nouns beginning with a vowel) when UNKs are inserted, but after the additional\n3Since Transformer is symmetric with respect to position, this process is similar to padding. However, the PAD token of RoBERTa published on hugging face is zero-vector, which is inconvenient for analyzing interactions between word embeddings. Thus, we choose UNK tokens.\ntraining, the models can predict them. The attention weights during inference are visualized in Figure 22. Since the attentions are concentrated every fourth token, the model with relearned position embeddings recognizes the relative positions of the non-UNK tokens. This result suggests that the attention is not focused on the tokens that cooccur frequently, but on the tokens that are informative to fill the MASK token. Thus, the relative position dependence of attention is simply caused by the fact that related words appear nearby.\nIn summary, this experiment indicates that the relative position dependence of attention is caused by the combination of two factors: (1) the linguistic property that related words tend to appear nearby due to grammatical rules and collocations, and (2) the property that attention is focused on words that are related in some sense."
        }
    ],
    "title": "Absolute Position Embedding Learns Sinusoid-like Waves for Attention Based on Relative Position",
    "year": 2023
}