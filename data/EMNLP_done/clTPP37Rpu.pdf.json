{
    "abstractText": "Large language models (LLMs) outperform information retrieval techniques for downstream knowledge-intensive tasks when being prompted to generate world knowledge. Yet, community concerns abound regarding the factuality and potential implications of using this uncensored knowledge. In light of this, we introduce CONNER, a COmpreheNsive kNowledge Evaluation fRamework, designed to systematically and automatically evaluate generated knowledge from six important perspectives \u2013 Factuality, Relevance, Coherence, Informativeness, Helpfulness and Validity. We conduct an extensive empirical analysis of the generated knowledge from three different types of LLMs on two widely-studied knowledge-intensive tasks, i.e., open-domain question answering and knowledge-grounded dialogue. Surprisingly, our study reveals that the factuality of generated knowledge, even if lower, does not significantly hinder downstream tasks. Instead, the relevance and coherence of the outputs are more important than small factual mistakes. Further, we show how to use CONNER to improve knowledge-intensive tasks by designing two strategies: Prompt Engineering and Knowledge Selection. Our evaluation code and LLMgenerated knowledge with human annotations will be released1 to facilitate future research.",
    "authors": [
        {
            "affiliations": [],
            "name": "Liang Chen"
        },
        {
            "affiliations": [],
            "name": "Yang Deng"
        },
        {
            "affiliations": [],
            "name": "Yatao Bian"
        },
        {
            "affiliations": [],
            "name": "Zeyu Qin"
        },
        {
            "affiliations": [],
            "name": "Bingzhe Wu"
        },
        {
            "affiliations": [],
            "name": "Tat-Seng Chua"
        },
        {
            "affiliations": [],
            "name": "Kam-Fai Wong"
        },
        {
            "affiliations": [],
            "name": "Hong Kong"
        }
    ],
    "id": "SP:d01af22e08e66770f4f3181d6820c7794a2213a9",
    "references": [
        {
            "authors": [
                "Sid Black",
                "Leo Gao",
                "Phil Wang",
                "Connor Leahy",
                "Stella Biderman"
            ],
            "title": "GPT-Neo: Large scale autoregressive language modeling with Mesh-Tensorflow",
            "year": 2021
        },
        {
            "authors": [
                "Liang Chen",
                "Hongru Wang",
                "Yang Deng",
                "Wai Chung Kwan",
                "Zezhong Wang",
                "Kam-Fai Wong."
            ],
            "title": "Towards robust personalized dialogue generation via order-insensitive representation regularization",
            "venue": "Findings of the Association for Computational Lin-",
            "year": 2023
        },
        {
            "authors": [
                "Yang Deng",
                "Wenqiang Lei",
                "Minlie Huang",
                "Tat-Seng Chua."
            ],
            "title": "Goal awareness for conversational AI: proactivity, non-collaborativity, and beyond",
            "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics: Tutorial Ab-",
            "year": 2023
        },
        {
            "authors": [
                "Emily Dinan",
                "Stephen Roller",
                "Kurt Shuster",
                "Angela Fan",
                "Michael Auli",
                "Jason Weston."
            ],
            "title": "Wizard of wikipedia: Knowledge-powered conversational agents",
            "venue": "arXiv preprint arXiv:1811.01241.",
            "year": 2018
        },
        {
            "authors": [
                "Nouha Dziri",
                "Ehsan Kamalloo",
                "Sivan Milton",
                "Osmar Zaiane",
                "Mo Yu",
                "Edoardo M. Ponti",
                "Siva Reddy"
            ],
            "title": "Faithdial: A faithful benchmark for information-seeking dialogue",
            "year": 2022
        },
        {
            "authors": [
                "John Glover",
                "Federico Fancellu",
                "Vasudevan Jagannathan",
                "Matthew R. Gormley",
                "Thomas Schaaf"
            ],
            "title": "Revisiting text decomposition methods",
            "year": 2022
        },
        {
            "authors": [
                "John Glover",
                "Federico Fancellu",
                "Vasudevan Jagannathan",
                "Matthew R. Gormley",
                "Thomas Schaaf"
            ],
            "title": "Revisiting text decomposition methods for nli-based factuality scoring of summaries",
            "year": 2022
        },
        {
            "authors": [
                "Or Honovich",
                "Leshem Choshen",
                "Roee Aharoni",
                "Ella Neeman",
                "Idan Szpektor",
                "Omri Abend"
            ],
            "title": "q2: Evaluating factual consistency in knowledgegrounded dialogues via question generation and question answering",
            "year": 2021
        },
        {
            "authors": [
                "Hai Hu",
                "Kyle Richardson",
                "Liang Xu",
                "Lu Li",
                "Sandra K\u00fcbler",
                "Lawrence Moss."
            ],
            "title": "OCNLI: Original Chinese Natural Language Inference",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3512\u20133526, Online. Association",
            "year": 2020
        },
        {
            "authors": [
                "Guimin Huang",
                "Min Tan",
                "Sirui Huang",
                "Ruyu Mo",
                "Ya Zhou."
            ],
            "title": "A discourse coherence model for analyzing chinese students\u2019 essay",
            "venue": "2017 International Conference on Progress in Informatics and Computing (PIC), pages 430\u2013434.",
            "year": 2017
        },
        {
            "authors": [
                "Gautier Izacard",
                "Edouard Grave."
            ],
            "title": "Leveraging passage retrieval with generative models for open domain question answering",
            "venue": "EACL 2021, pages 874\u2013880.",
            "year": 2021
        },
        {
            "authors": [
                "Rolf Jagerman",
                "Honglei Zhuang",
                "Zhen Qin",
                "Xuanhui Wang",
                "Michael Bendersky."
            ],
            "title": "Query expansion by prompting large language models",
            "venue": "CoRR, abs/2305.03653.",
            "year": 2023
        },
        {
            "authors": [
                "Ziwei Ji",
                "Nayeon Lee",
                "Rita Frieske",
                "Tiezheng Yu",
                "Dan Su",
                "Yan Xu",
                "Etsuko Ishii",
                "Ye Jin Bang",
                "Andrea Madotto",
                "Pascale Fung."
            ],
            "title": "Survey of hallucination in natural language generation",
            "venue": "ACM Comput. Surv., 55(12).",
            "year": 2023
        },
        {
            "authors": [
                "Prathyusha Jwalapuram",
                "Shafiq R. Joty",
                "Xiang Lin."
            ],
            "title": "Rethinking self-supervision objectives for generalizable coherence modeling",
            "venue": "CoRR, abs/2110.07198.",
            "year": 2021
        },
        {
            "authors": [
                "Saurav Kadavath",
                "Tom Conerly",
                "Amanda Askell",
                "Tom Henighan",
                "Dawn Drain",
                "Ethan Perez",
                "Nicholas Schiefer",
                "Zac Hatfield Dodds",
                "Nova DasSarma",
                "Eli Tran-Johnson"
            ],
            "title": "2022a. Language models (mostly) know what they know",
            "year": 2022
        },
        {
            "authors": [
                "Kaplan."
            ],
            "title": "Language models (mostly) know what they know",
            "venue": "CoRR, abs/2207.05221.",
            "year": 2022
        },
        {
            "authors": [
                "Vladimir Karpukhin",
                "Barlas Oguz",
                "Sewon Min",
                "Patrick Lewis",
                "Ledell Wu",
                "Sergey Edunov",
                "Danqi Chen",
                "Wen-tau Yih."
            ],
            "title": "Dense passage retrieval for opendomain question answering",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural",
            "year": 2020
        },
        {
            "authors": [
                "Mojtaba Komeili",
                "Kurt Shuster",
                "Jason Weston"
            ],
            "title": "Internet-augmented dialogue generation",
            "year": 2021
        },
        {
            "authors": [
                "Wojciech Kryscinski",
                "Bryan McCann",
                "Caiming Xiong",
                "Richard Socher."
            ],
            "title": "Evaluating the factual consistency of abstractive text summarization",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
            "year": 2020
        },
        {
            "authors": [
                "Wojciech Kryscinski",
                "Bryan McCann",
                "Caiming Xiong",
                "Richard Socher."
            ],
            "title": "Evaluating the factual consistency of abstractive text summarization",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP",
            "year": 2020
        },
        {
            "authors": [
                "Uszkoreit",
                "Quoc Le",
                "Slav Petrov."
            ],
            "title": "Natural questions: A benchmark for question answering research",
            "venue": "Transactions of the Association for Computational Linguistics, 7:452\u2013466.",
            "year": 2019
        },
        {
            "authors": [
                "Nayeon Lee",
                "Wei Ping",
                "Peng Xu",
                "Mostofa Patwary",
                "Pascale Fung",
                "Mohammad Shoeybi",
                "Bryan Catanzaro"
            ],
            "title": "Factuality enhanced language models for open-ended text generation",
            "year": 2023
        },
        {
            "authors": [
                "Patrick S.H. Lewis",
                "Ethan Perez",
                "Aleksandra Piktus",
                "Fabio Petroni",
                "Vladimir Karpukhin",
                "Naman Goyal",
                "Heinrich K\u00fcttler",
                "Mike Lewis",
                "Wen-tau Yih",
                "Tim Rockt\u00e4schel",
                "Sebastian Riedel",
                "Douwe Kiela"
            ],
            "title": "Retrieval-augmented generation",
            "year": 2020
        },
        {
            "authors": [
                "Junyi Li",
                "Xiaoxue Cheng",
                "Wayne Xin Zhao",
                "Jian-Yun Nie",
                "Ji-Rong Wen"
            ],
            "title": "Halueval: A largescale hallucination evaluation benchmark for large language models",
            "year": 2023
        },
        {
            "authors": [
                "Yanyang Li",
                "Jianqiao Zhao",
                "Michael R. Lyu",
                "Liwei Wang."
            ],
            "title": "Eliciting knowledge from large pre-trained models for unsupervised knowledgegrounded conversation",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Lan-",
            "year": 2022
        },
        {
            "authors": [
                "Nelson F. Liu",
                "Tianyi Zhang",
                "Percy Liang"
            ],
            "title": "Evaluating verifiability in generative search engines",
            "year": 2023
        },
        {
            "authors": [
                "Yixin Liu",
                "Alexander R. Fabbri",
                "Pengfei Liu",
                "Dragomir Radev",
                "Arman Cohan."
            ],
            "title": "On learning to summarize with large language models as references",
            "venue": "CoRR, abs/2305.14239.",
            "year": 2023
        },
        {
            "authors": [
                "Zihan Liu",
                "Mostofa Patwary",
                "Ryan Prenger",
                "Shrimai Prabhumoye",
                "Wei Ping",
                "Mohammad Shoeybi",
                "Bryan Catanzaro"
            ],
            "title": "Multi-stage prompting for knowledgeable dialogue generation",
            "year": 2022
        },
        {
            "authors": [
                "Potsawee Manakul",
                "Adian Liusie",
                "Mark J.F. Gales."
            ],
            "title": "Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models",
            "venue": "CoRR, abs/2303.08896.",
            "year": 2023
        },
        {
            "authors": [
                "Joshua Maynez",
                "Shashi Narayan",
                "Bernd Bohnet",
                "Ryan T. McDonald."
            ],
            "title": "On faithfulness and factuality in abstractive summarization",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, pages 1906\u20131919.",
            "year": 2020
        },
        {
            "authors": [
                "Sewon Min",
                "Kalpesh Krishna",
                "Xinxi Lyu",
                "Mike Lewis",
                "Wen tau Yih",
                "Pang Wei Koh",
                "Mohit Iyyer",
                "Luke Zettlemoyer",
                "Hannaneh Hajishirzi"
            ],
            "title": "Factscore: Fine-grained atomic evaluation of factual precision in long form text generation",
            "year": 2023
        },
        {
            "authors": [
                "Rodrigo Frassetto Nogueira",
                "Wei Yang",
                "Kyunghyun Cho",
                "Jimmy Lin."
            ],
            "title": "Multi-stage document ranking with BERT",
            "venue": "CoRR, abs/1910.14424.",
            "year": 2019
        },
        {
            "authors": [
                "OpenAI."
            ],
            "title": "GPT-4 technical report",
            "venue": "CoRR, abs/2303.08774.",
            "year": 2023
        },
        {
            "authors": [
                "Paul Christiano",
                "Jan Leike",
                "Ryan Lowe"
            ],
            "title": "Training language models to follow instructions with human feedback",
            "year": 2022
        },
        {
            "authors": [
                "Artidoro Pagnoni",
                "Vidhisha Balachandran",
                "Yulia Tsvetkov."
            ],
            "title": "Understanding factuality in abstractive summarization with FRANK: A benchmark for factuality metrics",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Asso-",
            "year": 2021
        },
        {
            "authors": [
                "Liangming Pan",
                "Xiaobao Wu",
                "Xinyuan Lu",
                "Anh Tuan Luu",
                "William Yang Wang",
                "Min-Yen Kan",
                "Preslav Nakov"
            ],
            "title": "Fact-checking complex claims with program-guided reasoning",
            "year": 2023
        },
        {
            "authors": [
                "Baolin Peng",
                "Michel Galley",
                "Pengcheng He",
                "Hao Cheng",
                "Yujia Xie",
                "Yu Hu",
                "Qiuyuan Huang",
                "Lars Liden",
                "Zhou Yu",
                "Weizhu Chen",
                "Jianfeng Gao"
            ],
            "title": "Check your facts and try again: Improving large language models with external knowledge and automated feed",
            "year": 2023
        },
        {
            "authors": [
                "Fabio Petroni",
                "Aleksandra Piktus",
                "Angela Fan",
                "Patrick Lewis",
                "Majid Yazdani",
                "Nicola De Cao",
                "James Thorne",
                "Yacine Jernite",
                "Vladimir Karpukhin",
                "Jean Maillard"
            ],
            "title": "Kilt: a benchmark for knowledge intensive language tasks",
            "year": 2021
        },
        {
            "authors": [
                "Alec Radford",
                "Jeffrey Wu",
                "Rewon Child",
                "David Luan",
                "Dario Amodei",
                "Ilya Sutskever"
            ],
            "title": "Language models are unsupervised multitask learners",
            "venue": "OpenAI blog,",
            "year": 2019
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J. Liu"
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "year": 2020
        },
        {
            "authors": [
                "Pranav Rajpurkar",
                "Jian Zhang",
                "Konstantin Lopyrev",
                "Percy Liang"
            ],
            "title": "Squad: 100,000+ questions for machine comprehension of text",
            "year": 2016
        },
        {
            "authors": [
                "Vipula Rawte",
                "Amit Sheth",
                "Amitava Das"
            ],
            "title": "A survey of hallucination in large foundation models",
            "year": 2023
        },
        {
            "authors": [
                "Keshav Santhanam",
                "Omar Khattab",
                "Jon Saad-Falcon",
                "Christopher Potts",
                "Matei Zaharia."
            ],
            "title": "Colbertv2: Effective and efficient retrieval via lightweight late interaction",
            "venue": "CoRR, abs/2112.01488.",
            "year": 2021
        },
        {
            "authors": [
                "Tal Schuster",
                "Adam Fisch",
                "Regina Barzilay."
            ],
            "title": "Get your vitamin C! robust fact verification with contrastive evidence",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human",
            "year": 2021
        },
        {
            "authors": [
                "Kurt Shuster",
                "Spencer Poff",
                "Moya Chen",
                "Douwe Kiela",
                "Jason Weston."
            ],
            "title": "Retrieval augmentation reduces hallucination in conversation",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2021, pages 3784\u20133803, Punta Cana, Do-",
            "year": 2021
        },
        {
            "authors": [
                "Robert H Somers."
            ],
            "title": "A new asymmetric measure of association for ordinal variables",
            "venue": "American sociological review, pages 799\u2013811.",
            "year": 1962
        },
        {
            "authors": [
                "James Thorne",
                "Andreas Vlachos",
                "Christos Christodoulopoulos",
                "Arpit Mittal."
            ],
            "title": "FEVER: a large-scale dataset for fact extraction and VERification",
            "venue": "Proceedings of the 2018 Conference of the North American Chapter of",
            "year": 2018
        },
        {
            "authors": [
                "Grave",
                "Guillaume Lample"
            ],
            "title": "Llama: Open and efficient foundation language models",
            "year": 2023
        },
        {
            "authors": [
                "David Wadden",
                "Shanchuan Lin",
                "Kyle Lo",
                "Lucy Lu Wang",
                "Madeleine van Zuylen",
                "Arman Cohan",
                "Hannaneh Hajishirzi."
            ],
            "title": "Fact or fiction: Verifying scientific claims",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language",
            "year": 2020
        },
        {
            "authors": [
                "Alex Wang",
                "Kyunghyun Cho",
                "Mike Lewis."
            ],
            "title": "Asking and answering questions to evaluate the factual consistency of summaries",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5008\u20135020, Online. Asso-",
            "year": 2020
        },
        {
            "authors": [
                "Shuhe Wang",
                "Xiaofei Sun",
                "Xiaoya Li",
                "Rongbin Ouyang",
                "Fei Wu",
                "Tianwei Zhang",
                "Jiwei Li",
                "Guoyin Wang."
            ],
            "title": "GPT-NER: named entity recognition via large language models",
            "venue": "CoRR, abs/2304.10428.",
            "year": 2023
        },
        {
            "authors": [
                "Jason Wei",
                "Maarten Bosma",
                "Vincent Y. Zhao",
                "Kelvin Guu",
                "Adams Wei Yu",
                "Brian Lester",
                "Nan Du",
                "Andrew M. Dai",
                "Quoc V. Le"
            ],
            "title": "Finetuned language models are zero-shot learners",
            "year": 2022
        },
        {
            "authors": [
                "Xiaohui Xie",
                "Qian Dong",
                "Bingning Wang",
                "Feiyang Lv",
                "Ting Yao",
                "Weinan Gan",
                "Zhijing Wu",
                "Xiangsheng Li",
                "Haitao Li",
                "Yiqun Liu",
                "Jin Ma"
            ],
            "title": "T2ranking: A large-scale chinese benchmark for passage ranking",
            "year": 2023
        },
        {
            "authors": [
                "Boyang Xue",
                "Weichao Wang",
                "Hongru Wang",
                "Fei Mi",
                "Rui Wang",
                "Yasheng Wang",
                "Lifeng Shang",
                "Xin Jiang",
                "Qun Liu",
                "Kam-Fai Wong"
            ],
            "title": "Improving factual consistency for knowledge-grounded dialogue systems via knowledge enhancement and alignment",
            "year": 2023
        },
        {
            "authors": [
                "Wenhao Yu",
                "Dan Iter",
                "Shuohang Wang",
                "Yichong Xu",
                "Mingxuan Ju",
                "Soumya Sanyal",
                "Chenguang Zhu",
                "Michael Zeng",
                "Meng Jiang"
            ],
            "title": "Generate rather than retrieve: Large language models are strong context generators",
            "year": 2023
        },
        {
            "authors": [
                "Yue Zhang",
                "Yafu Li",
                "Leyang Cui",
                "Deng Cai",
                "Lemao Liu",
                "Tingchen Fu",
                "Xinting Huang",
                "Enbo Zhao",
                "Yu Zhang",
                "Yulong Chen",
                "Longyue Wang",
                "Anh Tuan Luu",
                "Wei Bi",
                "Freda Shi",
                "Shuming Shi"
            ],
            "title": "Siren\u2019s song in the ai ocean: A survey on hallucination in large",
            "year": 2023
        },
        {
            "authors": [
                "Peiyu Liu",
                "Jian-Yun Nie",
                "Ji-Rong Wen"
            ],
            "title": "A survey of large language models",
            "year": 2023
        },
        {
            "authors": [
                "Yutao Zhu",
                "Huaying Yuan",
                "Shuting Wang",
                "Jiongnan Liu",
                "Wenhan Liu",
                "Chenlong Deng",
                "Zhicheng Dou",
                "Ji-Rong Wen"
            ],
            "title": "Large language models for information retrieval: A survey",
            "year": 2023
        },
        {
            "authors": [
                "C knowledge"
            ],
            "title": "Details of Baselines DPR (Karpukhin et al., 2020) is a supervised dense retrieval model trained on several QA datasets (including NQ) to retrieve the most relevant Wikipedia",
            "year": 2020
        },
        {
            "authors": [
                "LLaMA (Touvron"
            ],
            "title": "2023) is an open-source foundation language model trained on publicly available datasets and shows competitive performance with the best models, including GPT-3 (175B) and PaLM-540B",
            "year": 2023
        },
        {
            "authors": [
                "D evaluation"
            ],
            "title": "Details of Datasets Natural Questions (NQ) (Kwiatkowski et al., 2019) is an open-domain QA dataset, where the questions are mined from real Google search",
            "year": 2019
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "The exceptional success of large language models (LLMs) like ChatGPT and GPT4 (Ouyang et al., 2022; OpenAI, 2023) has fueled a growing interest in substituting traditional models with LLMs to attain superior performance across various NLP tasks (Liu et al., 2023b; Jagerman et al., 2023; Wang et al., 2023). In open-domain question answering (QA) and knowledge-grounded dialogue,\n\u2020Partial work was done in his Tencent AI Lab internship. \u2021 Corresponding author.\n1https://github.com/ChanLiang/CONNER\nLLMs have demonstrated superior performance than information retrieval (IR) models (Karpukhin et al., 2020) when it comes to generating world knowledge (Yu et al., 2023; Liu et al., 2022) for the downstream tasks. However, the knowledge generated may contain inherent issues, such as false statements or off-topic information. Therefore, the lack of extensive evaluation of this knowledge raises concerns about its use in downstream tasks.\nTo this end, four lines of research emerge. Firstly, human evaluations are conducted to assess the generated knowledge from diverse perspectives (Li et al., 2022; Yu et al., 2023; Liu et al., 2023a). However, their time-consuming nature and subjectivity often encounter issues of scalability and reproducibility. Secondly, datasets have been constructed to evaluate open-domain generation with the aid of references (Honovich et al., 2021; Glover et al., 2022a; Lee et al., 2023; Li et al., 2023). These methods, while more objective, are limited by their dependence on humanlabelled references, impacting their real-world applicability and generalizability to dynamically generated content. Thirdly, self-evaluation methods (Kadavath et al., 2022b; Manakul et al., 2023) esti-\nmate a model\u2019s uncertainty in its generated content. Despite simplicity, they lack interpretability and are less effective for long-form answers. Lastly, contemporary studies (Pan et al., 2023; Min et al., 2023) apply fact-checking principles to spot factual inaccuracies. However, these evaluation methods mainly assess a single aspect of the intrinsic quality of generated knowledge, overlooking other facets and their extrinsic impact on downstream tasks, thereby limiting a comprehensive understanding of LLM-generated content.\nIn light of these limitations, we propose CONNER, a COmpreheNsive kNowledge Evaluation fRamework, as illustrated in Figure 1. CONNER is designed to be a reference-free framework that can systematically and automatically evaluate the generated knowledge from six fine-grained perspectives, including diverse intrinsic evaluation of its internal properties, as well as uniform extrinsic evaluation of its impact on specific downstream tasks. The taxonomy of evaluation metrics is presented in Table 1. Based on CONNER, we conduct empirical evaluations on three different types of LLMs, including LLaMA (Wei et al., 2022) (a base LLM), FLAN-T5 (Wei et al., 2022) (an instruction-tuned LLM), ChatGPT (Ouyang et al., 2022) (a commercial LLM trained with human feedbacks). We evaluate them on two widely-studied knowledge-intensive tasks: open-domain QA (Kwiatkowski et al., 2019) and knowledge-grounded dialogue (Dinan et al., 2018).\nOur detailed investigations yield several valuable insights about the LLM-generated knowledge: 1) LLM-generated knowledge surpasses retrieved knowledge in most evaluation perspectives, while it actually suffers from the factuality issue as expected. Notably, the factuality of downstream tasks is found to be less affected by this issue, when compared to the impact of lower relevancy and coherency observed in the retrieved knowledge (\u00a7 4.3). 2) Several critical factors are identified to influence the factuality of the generated knowledge, such as their frequency and length, while few-shot in-context learning and larger size of models do not\nnecessarily guarantee higher quality and reliability (\u00a7 4.4). 3) In addition to assessing and analyzing the generated knowledge from different LLMs, the evaluation outcome of CONNER can be exploited to enhance knowledge generation and further improve the performance of downstream tasks (\u00a7 5).\nOur main contributions are as follows: \u2022 We conduct the first empirical analysis focusing\non both intrinsic quality and extrinsic reliability of the generated knowledge from LLMs.\n\u2022 We propose CONNER, a COmpreheNsive kNowledge Evaluation fRamework that enables the automatic evaluation of LLMs as knowledge generators from diverse perspectives, eliminating the need for human-labelled references.\n\u2022 The extensive evaluation and analysis yield profound insights and valuable practical experience for leveraging LLMs as knowledge generators.\n\u2022 We collect a new set of multi-perspective human judgments of LLM-generated knowledge for two knowledge-intensive generation datasets. We demonstrate that CONNER aligns well with human judgments. The human annotations will be released to facilitate future research."
        },
        {
            "heading": "2 Related Work",
            "text": "Knowledge-intensive tasks rely heavily on access to external knowledge sources, such as opendomain dialogue and QA (Dinan et al., 2018; Kwiatkowski et al., 2019; Petroni et al., 2021). The main-streamed methods (Karpukhin et al., 2020; Lewis et al., 2020; Izacard and Grave, 2021) typically employ IR techniques to first retrieve the relevant knowledge from Wikipedia and then produce the answer or response conditioned on the knowledge. Nowadays, with the powerful capabilities of LLMs (OpenAI, 2023; Kadavath et al., 2022a), a new trending approach is to leverage LLMs to directly generate the relevant knowledge for a given query and then apply the model-generated knowledge to complete the downstream tasks (Liu et al., 2022; Li et al., 2022; Yu et al., 2023). Despite outperforming retrieval-based methods, these knowl-\nedge generation techniques lack rigorous evaluation of their quality and reliability, which may contain misleading or even plausible false information, e.g., hallucination and factual inconsistency.\nThese issues are prevalent across various NLP tasks (Ji et al., 2023). However, most studies target specific downstream tasks, such as text summarization (Maynez et al., 2020; Wang et al., 2020; Kryscinski et al., 2020a; Pagnoni et al., 2021), dialogue generation (Dziri et al., 2022; Chen et al., 2023; Xue et al., 2023; Deng et al., 2023), and fact verification (Thorne et al., 2018; Wadden et al., 2020; Schuster et al., 2021; Pan et al., 2023). These tasks are designed to examine consistency either between the input and output or between the input and a human-labeled reference, e.g., the source document and its summary, the grounded knowledge and the generated response, or a human-written claim and pre-annotated references.\nThe success of LLMs and generative search engines (Zhao et al., 2023; Zhu et al., 2023) have brought hallucinations in LLM outputs (Rawte et al., 2023; Zhang et al., 2023) into focus. Research typically falls into four categories. (Lee et al., 2023; Li et al., 2023) aim to assess the factuality of open-domain generation automatically using specially designed datasets, but their reliance on references may limit real-world applicability. Another stream of work (Li et al., 2022; Yu et al., 2023; Liu et al., 2023a) uses human evaluation to measure output quality, which is difficult to scale. A third approach (Kadavath et al., 2022b; Manakul et al., 2023) detects hallucinations by examining the model\u2019s uncertainty or confidence, which can be inaccurate for long answers. Lastly, recent studies (Peng et al., 2023; Min et al., 2023) apply factchecking principles to spot factual inaccuracies.\nDifferent from previous studies, we propose a comprehensive framework for evaluating knowledge generated by LLMs. Our goal is to automatically test the intrinsic quality and extrinsic impact of generated information in knowledge-intensive tasks, without requiring knowledge labelling or human involvement. Through extensive testing with this framework, we aim to deepen and broaden our understanding of LLM-generated knowledge and provide valuable insights for future research."
        },
        {
            "heading": "3 The Evaluation Framework",
            "text": "We introduce CONNER, a comprehensive and innovative framework, specifically designed for the rig-\norous evaluation of the quality and dependability of knowledge used in knowledge-intensive tasks. CONNER is rooted in in-depth error analysis, paving the way for the construction of an evaluation taxonomy, which integrates six unique perspectives into two coherent categories, as delineated in Table 1. Capitalizing on the advantages of unsupervised metrics, our framework eliminates the need for human-labeled reference knowledge and standardizes scores within an intuitive range of [0, 1], simplifying comparison and interpretation.\nThe subsequent subsections provide a detailed examination of the framework\u2019s design, commencing with the formulation of knowledge-intensive tasks and the identification of associated error patterns. These insights direct the design of our metrics. Through comprehensive intrinsic and extrinsic evaluations, we aim to gain a holistic understanding of the LLMs-generated knowledge."
        },
        {
            "heading": "3.1 Tasks Formulation",
            "text": "Formally, we define the knowledge-intensive task as follows: given a user query q, the goal is to produce an answer with access to knowledge resources as illustrated in Figure. 1. Specifically, the system first obtains the relevant knowledge k that can help answer the query q from knowledge resources K, then the reader generates an answer a using the acquired knowledge k. Specifically, the knowledge resource K can be either a knowledge base for knowledge retrieval or language models for knowledge generation. Detailed formulations of these two settings are presented in Appendix A."
        },
        {
            "heading": "3.2 From Error Patterns to Metrics Design",
            "text": "To identify common errors by LLMs in knowledgeintensive tasks and create a more targeted evaluation framework, we used thematic analysis (Braun and Clarke, 2012). We began by extracting and consolidating patterns from subtle errors in knowledge and answers in responses from LLaMA to 160 samples from NQ (Kwiatkowski et al., 2019) and WoW (Dinan et al., 2018) datasets. To ensure the breadth of the error spectrum was adequately represented, we further substantiated these patterns using additional questions from NQ and WoW. As a result, we discerned four primary error categories in knowledge generation and two in answer generation. In response, we devised four intrinsic metrics for knowledge evaluation and two extrinsic metrics for answer evaluation, as outlined in Table 1."
        },
        {
            "heading": "3.3 Intrinsic Evaluation",
            "text": "Intrinsic evaluation refers to the assessment of the acquired knowledge based on its internal properties and performance, without considering its impact on downstream tasks or applications. In specific, we implement four model-based metrics for evaluating the acquired knowledge in terms of factuality, relevance, informativeness, and coherence.\nFactuality The core of factuality assessment is validating the acquired knowledge by external evidence 2. Given an acquired knowledge k = {s1, . . . , sm} composed of m sentences, we can use a dense retrieval model (Santhanam et al., 2021) or search engine API to recall the li most relevant evidence Ei = {ei,1, . . . , ei,li} for each sentence si from the expert knowledge base or the internet. After collecting all the evidence E = {E1, . . . , Em}, the factuality score is computed as follows:\nSfact(k,E) = min i=1..m f(si, Ei)\n= min i=1..m max j=1..li\nNLI(si, ei,j) (1)\nwhere f(\u00b7) is a function to compute sentence-level factuality, NLI(\u00b7) is a natural language inference model processing a premise-hypothesis pair to output a R3 vector, indicating whether a hypothesis (si) is entailed by, neutral to or refuted by the given premise (ei,j). Following these computations, sentence-level results are aggregated along the entailment dimension using one of three operations: min, mean, or max to match the desired error tolerance level. In this instance, we exemplify the process using min. Finally, we obtain a three-dimensional factuality score Sfact(k,E). From each dimension of this vector, we can derive three fine-grained scores. We denote those scores as factual-consistent, non-verified, and factual-inconsistent, respectively.\nThis strategy seeks to address the shortcomings of traditional factuality metrics (Wang et al., 2020; Honovich et al., 2021; Glover et al., 2022a; Lee et al., 2023) that mainly depend on consistency with human-annotated references. These metrics often fail in emerging knowledge generation scenarios (Table 10), as they struggle with modelgenerated content beyond reference knowledge scope and face difficulties when references are unavailable in real-world applications. Our method of evidence collection and results aggregation effectively tackles these issues.\n2We empirically demonstrate ground-truth knowledge is dispensable for the factuality evaluation in Appendix B.\nRelevance To assess the relevance between a given query q and the acquired knowledge k, we compute the relevance score as follows:\nSrel(k, q) = Matching(k, q) (2)\nThe Matching(\u00b7) function denotes a fine-grained matching model specifically designed for assessing the relevance between the query and knowledge. In our study, we employ the BERT ranking model (Nogueira et al., 2019) for this purpose.\nThis methodology addresses the limitations that arise when traditional relevance metrics are applied within knowledge generation scenarios. Traditional relevance metrics (Karpukhin et al., 2020; Shuster et al., 2021; Komeili et al., 2021), which typically rely on word overlap or similarity with humanwritten references, face two significant challenges. First, these traditional metrics do not correspond well with scenarios where LLMs serve as generative search engines, as evidenced by the unsatisfactory results in Table 10. Second, the reliance on reference knowledge constitutes a substantial challenge, especially when such references are scarce or absent in real-world applications. Contrarily, our BERT ranking model, trained on manually annotated Bing search data, excels at comparing the relevance of different knowledge to a given query.\nCoherence As the acquired knowledge is typically long-form texts composed of multiple sentences, we propose to measure sentence-level cohesion and paragraph-level coherence: the former measures the cohesion of individual sentences, and the latter measures the coherence between sentences. The sentence-level cohesion score Scoh_sent(k) is computed as follows:\nScoh_sent(k) = 1\nm \u2211m i=1 1/PPL(si) (3)\nwhere PPL(\u00b7) is computed by a GPT-based model (Radford et al., 2019; Black et al., 2021), measuring the perplexity for each sentence.\nOn the other hand, the paragraph-level coherence score is determined by the normalized score of a discourse coherence model (Jwalapuram et al., 2021), denoted as Scoh_para(k):\nScoh_para(k) = Scorerpara(s1, ..., sm) (4)\nBy considering both sentence-level cohesion and paragraph-level coherence, we gain insights into the overall coherence of the acquired knowledge.\nInformativeness To assess the informativeness of the procured knowledge\u2014defined as the degree to which the knowledge is novel or unexpected in relation to the model\u2019s existing knowledge about the query\u2014we calculate the informativeness score of the acquired knowledge k given q as follows:\nSinfo(k, q) = 1\u2212 exp\n( 1\nM M\u2211 t=1 lnP\u03b8(kt|k1:t\u22121, q)\n) (5)\nAssuming the unbiased benchmark model \u03b8 encapsulates world knowledge from general pretraining data, we thus select the GPT-2 series models.\nTo grasp the expected behaviour of this metric, consider a simple query: \"What is the capital of the United States?\" The knowledge acquired here is \"Washington\". In this situation, the model\u2019s average probability of generating \"Washington\" is high, as it already knows this fact. Consequently, our informativeness score for this knowledge would be low. Conversely, if the acquired knowledge was \"Chicago\", the model\u2019s probability of generating it would be low. This knowledge is surprising compared to its existing knowledge, resulting in a high informativeness score. On the other hand, for a tough query where the model is clueless, any provided knowledge would score high on informativeness due to the model\u2019s low output probabilities."
        },
        {
            "heading": "3.4 Extrinsic Evaluation",
            "text": "Extrinsic evaluation, in contrast to intrinsic evaluation, focuses on uniformly assessing the performance of the acquired knowledge within the context of different downstream tasks. Specifically, we measure how well the acquired knowledge contributes to the downstream task on two types of metrics (helpfulness and validity). Extrinsic evaluation provides a more comprehensive understanding of the practical value of the acquired knowledge.\nHelpfulness Given a query and answer pair (q, a), we assess to what extent the acquired knowledge k can help answer the query. As we assume no pre-annotated ground-truth knowledge, we use irrelevant knowledge as the baseline. Specifically, we randomly sampled u knowledge {k\u22121 , \u00b7 \u00b7 \u00b7 , k\u2212u } to reduce the variance of baseline estimation. Then the helpfulness score is computed as follows:\nShelp(q, a, k, k \u2212 1 , \u00b7 \u00b7 \u00b7 , k \u2212 u )\n= max(0, 1\u2212 L(q, k, a) 1 u \u2211u i=1 L(q, k \u2212 i , a) ) = max(0, 1\u2212 logP (a|q, k) 1 u \u2211u i=1 logP (a|q, k \u2212 i ) )\n(6)\nwhere L(q, k, a) and L(q, k\u2212i , a) are cross entropy losses of answer generation using k and k\u2212i respectively. Ideally, the generated knowledge k can provide enough information and reduce the L(q, k, a) to zero, and then the helpfulness score equals one. The worst case is the generated knowledge is no better than random knowledge (L(q, k, a) \u2265 1u \u2211u i=1 L(q, k \u2212 i , a)), and the helpfulness score is naturally zero.\nValidity To measure how the reliability of the acquired knowledge affects the factuality of the generated answer a on downstream tasks, we define the validity metric for two types of downstream tasks: span-based answers (e.g., open-domain QA) and open-ended answers (e.g., knowledge-grounded dialogue). As for span-based answers, the generated answers cannot form a complete sentence for factuality measurement. To this end, we concatenate (q, a\u2217) as the premise and (q, a) as the hypothesis for deriving the factual-consistent score of the NLI(\u00b7) model as the validity score:\nSval(q, a \u2217, a) = NLIfact((q, a), (q, a \u2217)) (7)\nwhere a\u2217 denotes the ground-truth answer for downstream tasks and the NLI(\u00b7) model is the same as that of Eq. (1).\nWe demonstrate this measure outperforms traditional metrics like Exact Match and F1 score as shown in Table 10, which rely on literal matches, and often yield low recall. For instance, an entity pair like \u2019PRC\u2019 and \u2019China\u2019 would receive a zero score due to their differing literal presentations.\nAs for open-ended answers, we collect l evidence E = {e1, . . . , el} and adjust Eq. (1) to be:\nSval(a,E) = f(a,E) = max i=1..l NLIfact(a, ei) (8)"
        },
        {
            "heading": "4 Evaluation",
            "text": "In this section, we will first validate our proposed metrics, and then leverage them to comprehensively evaluate three different types of LLMs across two knowledge-intensive tasks, followed by an indepth analysis of the results."
        },
        {
            "heading": "4.1 Metrics Efficacy Validation",
            "text": "To validate the effectiveness of our proposed metrics, we conducted manual evaluations and compared the results with baseline metrics. Specifically, we developed specific annotation guidelines for each metric, detailed in Appendix J, and performed manual annotations accordingly. These\nannotations allowed us to calculate the correlation between each metric and human evaluations. Subsequently, we compared these correlations with baseline metrics (Table 10). Our metrics demonstrated a strong correlation with human evaluations, significantly outperforming the baseline metrics. Details are presented in Chapter 6 and Appendix J."
        },
        {
            "heading": "4.2 Experimental Setups",
            "text": "Baselines Compared with a popular retrievalbased model, DPR (Karpukhin et al., 2020), we evaluate knowledge generation with the three different types of LLMs, including FLAN-T5 (Wei et al., 2022), LLaMA (Touvron et al., 2023), and ChatGPT (Ouyang et al., 2022). By default, we report the results with the largest size of each LLM and adopt greedy decoding in our experiments for reproducibility. Details are presented in Appendix C.\nDatasets We evaluate the generated knowledge on two widely-studied benchmark datasets, including 1) Natural Questions (NQ) (Kwiatkowski et al., 2019), an open-domain QA dataset; and 2) Wizard of Wikipedia (WoW) (Dinan et al., 2018) a knowledge-grounded dialogue dataset. During experiments, we randomly sample 500 examples from the NQ and WoW test sets respectively for evaluation. Details are presented in Appendix D.\nImplementation Details All the adopted models in CONNER are introduced in Appendix E.\nEvaluation Setting Following (Yu et al., 2023), we evaluate the knowledge generation of LLMs under both zero-shot and few-shot settings. After the knowledge acquisition, we perform QA or dialogue generation under the few-shot setting to further investigate the impact of different knowledge acquisition methods on downstream tasks.\n1) Zero-shot Evaluation: We test with varied prompts and report peak performance. A prompt could be \u201cGenerate Wikipedia knowledge for the query. {query}\u201d. Prompts tried are in Appendix F.\n2) Few-shot Evaluation: We construct the prompt with k randomly chosen samples from the training set. The example templates used for knowledge generation are listed in Appendix F and G."
        },
        {
            "heading": "4.3 Overall Evaluation",
            "text": "Table 2 and Table 3 summarize the evaluation results of DPR and three LLM-based knowledge generators on NQ and WoW datasets, respectively. There are several notable observations as follows:\nGenerated knowledge exceeds retrieved knowledge in most evaluation perspectives, except the factuality and informativeness. In both NQ and WoW scenarios, LLMs show remarkable capabilities in generating highly relevant and coherent knowledge. Moreover, the knowledge generated by LLMs also proves to be more beneficial for downstream tasks, regarding both helpfulness and validity. These results highlight the significant ad-\nvantages of utilizing LLMs as knowledge generators in terms of knowledge quality and applicability, rendering them a valuable knowledge resource for various knowledge-intensive applications.\nDespite obtaining lower factuality than retrieved knowledge, generated knowledge contributes more to the factuality of downstream tasks (i.e., higher validity). To investigate the underlying reason, we analyze the correlation between different intrinsic metrics and extrinsic metrics on two tasks. As shown in Tables 5 and 6, the performance of downstream tasks is indeed hindered by the issue of factuality in the generated knowledge from LLMs. However, for retrieval models (e.g., DPR), limitations may arise from the relevance and coherence of the retrieved knowledge, while its high factuality fails to ensure the performance of downstream tasks. We present a case study in Ta-\nble 4, which intuitively shows that the presence of factual errors in non-critical information has minimal impact on downstream tasks, while it is highly impossible to derive the correct answer from the irrelevant retrieved knowledge. While LLaMA and ChatGPT generate knowledge with slightly lower factuality than DPR, it is shown to be adequate for downstream tasks. At this point, the relevance of the acquired knowledge is more critical. Hence, relying solely on the factuality of the knowledge itself is an unreliable means of assessing its impact on the factuality of downstream tasks. Motivated by this finding, we investigate approaches to guiding the generated knowledge selection with the multi-perspective evaluation outcome of CONNER for improving the downstream performance in \u00a7 5. DPR falls short of retrieving relevant and helpful knowledge for knowledge-grounded dialogues. As the DPR model is finetuned on QA datasets to match a question to Wikipedia knowledge, the DPR model struggles to match dialogue utterances with the necessary knowledge. Also, the candidate Wikipedia passages in DPR (100 tokens) are much longer than the knowledge needed in WoW, containing much redundant information. This reveals the shortcomings of supervised dense retrieval models, such as limited transferability and being constrained by knowledge bases.\nFew-shot in-context learning for LLMs generally harms the factuality of generated knowledge. We observe that the length of knowledge generated by few-shot ICL is generally longer than that of zero-shot prompting since the ground-truth knowledge for demonstrations is relatively long. Consequently, LLM is more error-prone (see the analysis of long-form generation in \u00a7 4.4). This indicates that few-shot ICL is not always better than zero-shot ICL in knowledge generation, and the selected demonstrations attach great importance.\nInspired by this, we investigate approaches to guiding the few-shot demonstration selection with the evaluation outcome of CONNER for improving the performance of few-shot ICL in \u00a7 5. FLAN-T5 fails to be a qualified knowledge generator since its generated knowledge is poorly factual and rarely helpful to downstream tasks. Although FLAN-T5 (11B) significantly surpasses many models of the same scale through instruction tuning on numerous tasks, it falls short of being a qualified knowledge generator. As shown in Table 4, such a low factuality leads to frequent occurrences of factual errors in critical information, thereby harming downstream tasks. To this end, we study the scaling of performance w.r.t different perspectives by varying the model size in \u00a7 4.4."
        },
        {
            "heading": "4.4 Further Analysis",
            "text": "We further analyze how different factors affect the quality and reliability of the generated knowledge and discuss our findings below.\nLong-tail Knowledge We investigate the impact of the knowledge frequency on the factuality performance of LLaMA on the WoW dataset. Each data entry in WoW comprises a topic, query, knowledge, and answer. The topic indicates the corresponding Wikipedia page linked to the knowledge. We assess this knowledge\u2019s frequency using Wikipedia pageviews from 2015 to 20213. This enables us to differentiate between common and long-tail knowledge in WoW. Our findings reveal that LLaMA exhibits lower reliability when it is expected to generate rare/long-tail knowledge compared to common knowledge, as depicted in Figure 2(a).\nLong-form Generation We investigate the impact of generation length on the factuality of the generated knowledge. Specifically, we consider knowledge over 40 tokens and take sentences as evaluation units aligned with factuality evaluation.\n3https://wikimedia.org/api/rest_v1\nFigure 2(b) displays the factuality performance based on the number of sentences in the generated knowledge. The results show that LLaMA exhibits higher error rates when generating longform knowledge. Therefore, prompting the LLMs to generate the required knowledge in a concise rather than lengthy manner can benefit factuality.\nImpact of Model Size Figures 3 depicts the performance scaling with the model size, including LLaMA-65B/33B/7B and FLAN-T5-11B/3B/780M. The results are reported on the NQ dataset using zero-shot prompting. We observe that larger models do not necessarily outperform smaller models in terms of intrinsic evaluation (particularly when parameter magnitudes are similar). However, larger models consistently outperform smaller models in terms of extrinsic evaluation(helpfulness and validity). Detailed tables are presented in Appendix I.\n5 Two Use Cases of CONNER\nTo explore how our framework can guide the future design of utilizing LLMs as a knowledge generator, we design two strategies to employ CONNER as a measurement for guiding the Prompt Engineering and Knowledge Selection for knowledge-intensive tasks. We define the overall quality of knowledge k given the query q as follows:\nQknow(q, k) = \u03b3 \u22ba \u00b7 Sintr \u03b3 \u2208 R4 Sintr = [Sfact, Srel, Scoh_para, Sinfo] \u22ba (9)\nwhere Qknow is the linear combination of four instinct metrics Sintr and \u03b3 is the coefficient vector.\nPrompt Engineering We show how to use CONNER to improve knowledge generation by performing prompt engineering for few-shot ICL. We random sample a small set of m samples from the training set, then use Qknow(q, k) as the scoring function to select the top n samples to compose the few-shot prompt. As shown in Table 7, the\nknowledge generated by CONNER-enhanced fewshot prompting outperforms that with random demonstrations on 3 out of 4 perspectives, under the setting of m = 30 and n = 8.\nKnowledge Selection We employ CONNER to improve downstream tasks by selecting high-quality generated knowledge. Specifically, we generate r different knowledge H = {k\u03031, ..., k\u0303r} from LLMs with top-p sampling, then select the generated knowledge for the downstream task, according to k = argmaxk\u0303\u2208HQknow(q, k\u0303). As shown in Table 8, we achieve a relative improvement of 43.15% in helpfulness on ChatGPT with p = 0.9 and r = 5."
        },
        {
            "heading": "6 Human Evaluation",
            "text": "We conducted a human evaluation by randomly selecting 400 samples from the NQ and WoW test sets. Our three annotators provided ratings for the intrinsic and extrinsic metrics for the four models. Additionally, for FLAN-T5 and LLaMA, we annotated the specific locations of factual errors in the generated knowledge, aiming to facilitate future research on fine-grained fallacy detection. Detailed annotation instructions and the statistics of our labelled data can be found in Appendix J.1.\nTo evaluate how well CONNER matches human evaluation of knowledge and compares with several baseline metrics, we measure the Somers\u2019 D correlation (Somers, 1962) between the human rating 0, 1, 2 of the knowledge quality and corresponding metric scores. Table 9 and Table 10 illustrate the results of four models on the NQ dataset. We observe that: (1) CONNER yields consistently good correlations with human evaluation w.r.t different evaluation perspectives (except for informativeness), which indicates that the quality of knowledge can be more effectively evaluated with CONNER. The inconsistency between informativeness and human judgment is attributed to the differences in model\nknowledge and human knowledge. (2) CONNER metrics consistently outperform all other referencereliant metrics, indicating the effectiveness of our framework in the knowledge evaluation scenarios."
        },
        {
            "heading": "7 Conclusion",
            "text": "In this work, we introduce CONNER, a comprehensive evaluation framework designed to automatically assess both the intrinsic quality and extrinsic reliability of the knowledge generated by LLMs. Notably, CONNER is reference-free but demonstrates a better correlation with human judgement compared with previous reference-reliant metrics.\nThrough extensive evaluation and in-depth analysis, we identify several key factors affecting the factuality of generated knowledge. We find although the generated knowledge is less factual than the retrieved knowledge, it remarkably enhances the factuality of downstream tasks over the retrieved ones. Furthermore, we propose two approaches to improve knowledge generation and downstream task performance with the guidance of CONNER. We believe our framework and findings will facilitate the future research of trustworthy AIGC.\nLimitations\nIn this section, we discuss the limitations in this work from three perspectives.\nFirstly, the knowledge we evaluate primarily relies on information sourced from Wikipedia. This choice is driven by two considerations: (1) Large language models (LLMs) are trained on diverse corpora, which may include undisclosed domainspecific or task-specific data. To ensure fairness in our evaluations and enable meaningful comparisons, we focus on the common data sources that all models have learned from, with Wikipedia being a prevalent pre-training corpus for different LLMs. (2) Wikipedia is renowned for its highquality knowledge, providing us with authoritative evidence to validate the generated knowledge. Additionally, leveraging such authoritative evidence enhances the interpretability of our factual judgments. In future work, we aim to expand our evaluations to include a broader range of world knowledge, thus further enhancing the scope and generalizability of our findings.\nSecondly, while our work primarily aims to propose a general framework that can be applied to any language, our evaluation framework presents potential generalization challenges for non-English languages. This is due to its reliance on several common NLP components, a limitation echoed across many NLP methodologies. Encouragingly, the development of model variants in other languages, such as Chinese (Hu et al., 2020; Xie et al., 2023; Huang et al., 2017), indicates the potential for broader applications. Nonetheless, the reality remains that for very low-resource languages without existing NLP models, these components may need to be developed from scratch. This issue represents a challenge that the community needs to address in the future.\nA third limitation is that our assessment of factuality is limited to sentence-level granularity. Through analysis and manual annotation, we have identified that large language models (LLMs) tend to exhibit errors at a more detailed level, particularly concerning numbers, time, and the generation of misleading or fabricated concepts (e.g., key characters, identities, and locations), particularly within parallel structures. To address this limitation, future research will concentrate on developing more fine-grained methods for detecting hallucinations and assessing factual accuracy. To facilitate such research, we have annotated a specific subset of\ndata that targets fine-grained factual errors. Despite these limitations, we believe our work serves as a significant catalyst for the automated evaluation of knowledge generated by large language models, contributing positively to the advancement of more trustworthy AI systems."
        },
        {
            "heading": "Acknowledgements",
            "text": "We extend our sincerest gratitude to Professor Jing Ma, whose insightful discussions and suggestions on factuality evaluation have significantly inspired our design. We are particularly grateful to our three anonymous reviewers, whose thorough and meticulous reviews have considerably improved the quality of our work. Their constructive discussions and insights have undoubtedly enhanced our revisions. This research work is partially supported by CUHK under Project No. 3230377 (Ref. No. KPF23GW20)."
        },
        {
            "heading": "A Details of Problem Formulation",
            "text": "We provide a formulation of the two-step process for knowledge-intensive tasks, as illustrated in Fig.1. Formally, the knowledge-intensive generation problem can be formulated as the following chain rule:\nP (a|q,K) = \u2211\nk P (k|q,K)P (a|q, k) (10)\nwhere P (k|q,K) is the knowledge acquisition process and P (a|q, k) = \u220fN t=1 P (at|a1:t\u22121, q, k) is the autoregressive answer generation process of the reader model based on the acquired knowledge.\nRetrieval-based knowledge acquisition methods use a retrieval model to retrieve the most relevant knowledge from the knowledge resource K = {d1, d2, . . . , dK} composed of K documents:\nP (k = di|q,K) = esim(q,di)\u2211K j=1 e sim(q,dj) (11)\nwhere sim(\u00b7) function is used to measure the similarity, e.g., cosine similarity, between the query and the knowledge document.\nGeneration-based knowledge acquisition methods prompt a large language model to directly generate the required knowledge:\nP (k|q,K) = \u220fM\nt=1 PK(kt|k1:t\u22121, q, prompt) (12)\nwhere prompt denotes the zero-shot or few-shot prompt and the LLM is regarded as the knowledge resource K and PK stands for the distribution induced by the LLM."
        },
        {
            "heading": "B Analysis of Reference Knowledge",
            "text": "We investigated the importance of reference knowledge in evaluating the factuality of generated knowledge. Specifically, we conducted FLANT5 experiments on the WoW dataset using a zeroshot approach. Two sets of experiments were performed: one included reference knowledge in the retrieved evidence pool, while the other did not. Figure 4 illustrates our findings, indicating that the group with reference knowledge exhibits a clear advantage when the number of retrieved evidence is limited. However, as the number of retrieved evidence increases, the performance of both groups converges. These results suggest that reference knowledge is dispensable, particularly when a significant amount of evidence is available. When the number of retrieved evidence surpasses ten, the impact of reference knowledge becomes negligible. We hope this will provide valuable insights for future designs of factuality assessment for generated\nknowledge."
        },
        {
            "heading": "C Details of Baselines",
            "text": "DPR (Karpukhin et al., 2020) is a supervised dense retrieval model trained on several QA datasets (including NQ) to retrieve the most relevant Wikipedia passages given a query. FLAN-T5 (Wei et al., 2022) is an enhanced version of T5 (Raffel et al., 2020) that is instructionfinetuned in 1.8k NLP datasets to acquire the generalization ability to unseen tasks. LLaMA (Touvron et al., 2023) is an open-source foundation language model trained on publicly available datasets and shows competitive performance with the best models, including GPT-3 (175B) and PaLM-540B. ChatGPT is a sibling model to InstructGPT (Ouyang et al., 2022) that is trained to follow instructions in a prompt and provide a detailed response. We adopt text-davinci-003 version for\nevaluation."
        },
        {
            "heading": "D Details of Datasets",
            "text": "Natural Questions (NQ) (Kwiatkowski et al., 2019) is an open-domain QA dataset, where the questions are mined from real Google search queries. The corresponding ground truth knowledge and the answers to the questions are paragraphs and short spans in the Wikipedia pages. Wizard of Wikipedia (WoW) (Dinan et al., 2018) is a knowledge-grounded dialogue dataset designed for information-seeking scenarios, where one speaker introduces knowledge related to a topic to the other speaker by grounding his/her responses in a specific sentence from a Wikipedia page.\nE Implementation Details\nAll the metrics we designed are model-based metrics, utilizing solely off-the-shelf models. We present the models used in Table 14."
        },
        {
            "heading": "F Prompts for Knowledge Generation",
            "text": "F.1 Zero-shot Prompts\nIn our experiments, we observed that zero-shot prompting was highly unstable. Therefore, we conduct experiments using multiple human prompts and select the most effective ones for the WoW and NQ datasets. The human prompts we evaluate are listed in Table 11.\nF.2 Few-shot Prompts\nIn the few-shot setting, our prompt is constructed using k randomly chosen examples from the training set:\nprompt = (example1\\n ... examplek\\n exampletest)\nThe example templates utilized for knowledge generation are provided in Table 12. Please note that exampletest differs from examplei as it does not contain {knowledge} in the placeholder."
        },
        {
            "heading": "G Prompts for Answer Generation",
            "text": "We adopt few-shot prompting on the LLaMA model in answer generation and the example templates used for answer generation are provided in Table 13."
        },
        {
            "heading": "H Detailed Correlations between Intrinsic and Extrinsic Metrics",
            "text": "We listed the detailed correlations between intrinsic and extrinsic metrics for LLaMA, FLAN-T5, and ChatGPT on the NQ dataset in Table 15."
        },
        {
            "heading": "I Table of Model Size Impact",
            "text": "We list the specific numerical values of performance scaling with the model size in Table 16, including LLaMA-65B/33B/7B and FLAN-T511B/3B/780M."
        },
        {
            "heading": "J Details of Human Evaluation",
            "text": "J.1 Human Annotation We conducted a human evaluation with 400 samples from the NQ and WoW test set. Among these, 320 samples were from the zero-shot setting in the NQ dataset, involving all four models, while 80 samples were from the few-shot setting in the WoW dataset, involving one model (ChatGPT). Three expert annotators, who were familiar with the tasks, were employed to rate the acquired knowledge and generated answers based on four intrinsic perspectives and two extrinsic perspectives. Each perspective was scored on a scale of 0, 1, or 2, representing unacceptable, acceptable, and excellent, respectively. The average kappa value of the annotation is 0.612 on 20% cross-annotation data. The detailed instructions for the human annotation can be found in Table 17.\nNote for factuality assessment, the reliable evidence for the generated knowledge k is acquired by the following process: For each sentence in k, we use it as the query to search Google, and regard the top1 Wikipedia webpage as a reliable knowledge source to verify the factuality of this sentence. Another point worth noting is that for the evaluation of validity in WoW, we reused the factuality evaluation process since the responses in WoW are open-ended.\nJ.2 Human Evaluation Results on WoW Based on the provided annotations, we assessed the correlation between ChatGPT\u2019s automatic metrics and human judgement on the WoW dataset. The results are presented in Table 18.\nJ.3 Baseline Metrics We compared it with three reference-reliant metrics in knowledge evaluation. Their definitions and calculation methods are as follows:\nModel Extrinsic Instrinsic\nFact. Rel. Coh-sent. Coh-para. Info.\nFLAN-T5 helpful. 0.15 \u2020 -0.21\u2020 0.20\u2020 -0.21\u2020 0.02\nvalidity 0.23\u2020 -0.16\u2020 0.14\u2020 -0.10\u2020 0.07\nLLAMA helpful. 0.03 0.05 0.06 -0.09 \u2020 -0.01\nvalidity 0.09\u2020 0.07 0.05 -0.06 -0.03\nCHATGPT helpful. 0.16 \u2020 0.03 0.08 0.02 -0.04\u2020\nvalidity 0.22\u2020 0.13\u2020 0.02\u2020 0.09\u2020 0.03\nTable 15: The Somers\u2019 correlation between intrinsic and extrinsic metrics in zero-shot setting on NQ. Correlation scores with p-value < 0.05 are marked with \u2020.\nModel Size Fact. Rel. Coh. Info Help. Val.\nLLaMA 65B 0.942 0.732 0.824 0.757 0.219 0.420 33B 0.656 0.633 0.734 0.608 0.203 0.402 7B 0.773 0.626 0.805 0.662 0.154 0.375\nFLAN-T5 11B 0.584 0.685 0.778 0.673 -0.146 0.325 3B 0.657 0.663 0.816 0.708 -0.155 0.324 780M 0.506 0.729 0.793 0.729 -0.162 0.252\nTable 16: Performance on NQ with varying sizes of FLAN-T5 and LLaMA as knowledge generators. The max(0, .) operation in Eq.6 has been excluded to emphasize the sequential relationship among different sizes of FLAN-T5. Bold and Underlined results represent the best and second-best performances for each model, respectively.\nHallucinated NE Ratio (HE) (Lee et al., 2023) proposed a NE-based metric that is designed with an intuition that a model is hallucinating (making factual errors) if it generates an NE that does not appear in the reference knowledge source. The NE-based metric can be calculated as: HNE = |HALLUNE| / |ALLNE| where ALLNE is the set of all the NEs detected in the LM generation, and HALLUNE is a subset of NEAll that does not appear in the reference Wikipedia knowledge. Note that evaluating NEER requires the existence of refer-\nence knowledge. We adopt \u2212HNE when computing the correlation with human judgement. Entailment Ratio (ER) (Lee et al., 2023) also introduces an NLI-based approach to assess factual knowledge by measuring its entailing relationship with ground-truth/reference knowledge. The entailment ratio is computed as follows: EntailR= |ENTAILgen| / |ALLgen|, where ALLgen is a set of all generated knowledge, and ENTAILgen is the set of generated knowledge that can be entailed by the NLI model. Specifically, we use the entailment probability of each example as its ER score. F1 of knowledge (F1) (Liu et al., 2022) employs a unigram F1 score to evaluate the quality of generated knowledge. This metric measures the overlap between the generated knowledge and the reference knowledge by evaluating word-level matches. By assessing the degree of agreement, the F1 metric provides an estimation of the knowledge quality, specifically from a relevance perspective. NLI-weak-supervised (Kryscinski et al., 2020b) train a classification model on constructed data to perform consistency checking on (document, sentence) pairs. We chose the factCC version as our baseline. NLI-decompose-claim (Glover et al., 2022b) found that in general, sentence-level decomposition is preferable for the hypothesis side of the NLI input. So we also decompose the generated knowledge into sentences and then aggregate the sentence-level scores to produce a document-level score. NLI-multitask fine-tunes the DeBERTa-v3-large model on FEVER and two NLI datasets. Exact Match (EM) (Rajpurkar et al., 2016) use Exact Match to measure the percentage of predictions that match its ground truth answers exactly."
        }
    ],
    "title": "Beyond Factuality: A Comprehensive Evaluation of Large Language Models as Knowledge Generators",
    "year": 2023
}