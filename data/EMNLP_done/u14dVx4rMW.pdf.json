{
    "abstractText": "Recently, Large Language Models (LLMs) have been serving as general-purpose interfaces, posing a significant demand for comprehensive visual knowledge. However, it remains unclear how well current LLMs and their visually augmented counterparts (VaLMs) can master visual commonsense knowledge. To investigate this, we propose IMAGENETVC, a human-annotated dataset specifically designed for zeroand few-shot visual commonsense evaluation across 1,000 ImageNet categories. Utilizing IMAGENETVC, we benchmark the fundamental visual commonsense knowledge of both unimodal LLMs and VaLMs. Furthermore, we analyze the factors affecting the visual commonsense knowledge of large-scale models, providing insights into the development of language models enriched with visual commonsense knowledge. Our code and dataset are available at https://github.com/ hemingkx/ImageNetVC.",
    "authors": [
        {
            "affiliations": [],
            "name": "Heming Xia"
        },
        {
            "affiliations": [],
            "name": "Qingxiu Dong"
        },
        {
            "affiliations": [],
            "name": "Lei Li"
        },
        {
            "affiliations": [],
            "name": "Jingjing Xu"
        },
        {
            "affiliations": [],
            "name": "Tianyu Liu"
        },
        {
            "affiliations": [],
            "name": "Ziwei Qin"
        },
        {
            "affiliations": [],
            "name": "Zhifang Sui"
        }
    ],
    "id": "SP:7682cfe12efe55cd2962fb2bed1f5b42781cdee0",
    "references": [
        {
            "authors": [
                "Yash Agarwal",
                "Devansh Batra",
                "Ganesh Bagler."
            ],
            "title": "Building hierarchically disentangled language models for text generation with named entities",
            "venue": "Proceedings of the 28th International Conference on Computational Linguistics, COLING 2020,",
            "year": 2020
        },
        {
            "authors": [
                "Jean-Baptiste Alayrac",
                "Jeff Donahue",
                "Pauline Luc",
                "Antoine Miech",
                "Iain Barr",
                "Yana Hasson",
                "Karel Lenc",
                "Arthur Mensch",
                "Katherine Millican",
                "Malcolm Reynolds"
            ],
            "title": "Flamingo: a visual language model for few-shot learning",
            "venue": "Advances in Neural",
            "year": 2022
        },
        {
            "authors": [
                "Hessam Bagherinezhad",
                "Hannaneh Hajishirzi",
                "Yejin Choi",
                "Ali Farhadi."
            ],
            "title": "Are elephants bigger than butterflies? reasoning about sizes of objects",
            "venue": "Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, February 12-17, 2016, Phoenix,",
            "year": 2016
        },
        {
            "authors": [
                "Samuel Weinbach"
            ],
            "title": "Gpt-neox-20b: An open-source autoregressive language model. CoRR, abs/2204.06745",
            "year": 2022
        },
        {
            "authors": [
                "Sid Black",
                "Leo Gao",
                "Phil Wang",
                "Connor Leahy",
                "Stella Biderman"
            ],
            "title": "GPT-Neo: Large Scale Autoregressive Language Modeling with MeshTensorflow",
            "year": 2021
        },
        {
            "authors": [
                "Elia Bruni",
                "Gemma Boleda",
                "Marco Baroni",
                "NamKhanh Tran."
            ],
            "title": "Distributional semantics in technicolor",
            "venue": "The 50th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference, July 8-14, 2012, Jeju Island, Ko-",
            "year": 2012
        },
        {
            "authors": [
                "Jia Deng",
                "Wei Dong",
                "Richard Socher",
                "Li-Jia Li",
                "Kai Li",
                "Li Fei-Fei."
            ],
            "title": "Imagenet: A large-scale hierarchical image database",
            "venue": "2009 IEEE conference on computer vision and pattern recognition, pages 248\u2013255. Ieee.",
            "year": 2009
        },
        {
            "authors": [
                "Qingxiu Dong",
                "Lei Li",
                "Damai Dai",
                "Ce Zheng",
                "Zhiyong Wu",
                "Baobao Chang",
                "Xu Sun",
                "Jingjing Xu",
                "Lei Li",
                "Zhifang Sui."
            ],
            "title": "A survey for in-context learning",
            "venue": "CoRR, abs/2301.00234.",
            "year": 2023
        },
        {
            "authors": [
                "Leo Gao",
                "Stella Biderman",
                "Sid Black",
                "Laurence Golding",
                "Travis Hoppe",
                "Charles Foster",
                "Jason Phang",
                "Horace He",
                "Anish Thite",
                "Noa Nabeshima",
                "Shawn Presser",
                "Connor Leahy"
            ],
            "title": "The pile: An 800gb dataset of diverse text for language modeling",
            "year": 2021
        },
        {
            "authors": [
                "Yash Goyal",
                "Tejas Khot",
                "Douglas Summers-Stay",
                "Dhruv Batra",
                "Devi Parikh."
            ],
            "title": "Making the V in VQA matter: Elevating the role of image understanding in visual question answering",
            "venue": "2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR",
            "year": 2017
        },
        {
            "authors": [
                "Chuan Guo",
                "Geoff Pleiss",
                "Yu Sun",
                "Kilian Q. Weinberger."
            ],
            "title": "On calibration of modern neural networks",
            "venue": "Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, volume 70 of",
            "year": 2017
        },
        {
            "authors": [
                "Yaru Hao",
                "Haoyu Song",
                "Li Dong",
                "Shaohan Huang",
                "Zewen Chi",
                "Wenhui Wang",
                "Shuming Ma",
                "Furu Wei."
            ],
            "title": "Language models are general-purpose interfaces",
            "venue": "arXiv preprint arXiv:2206.06336.",
            "year": 2022
        },
        {
            "authors": [
                "Junnan Li",
                "Dongxu Li",
                "Silvio Savarese",
                "Steven C.H. Hoi."
            ],
            "title": "BLIP-2: bootstrapping language-image pre-training with frozen image encoders and large language models",
            "venue": "CoRR, abs/2301.12597.",
            "year": 2023
        },
        {
            "authors": [
                "Junnan Li",
                "Dongxu Li",
                "Caiming Xiong",
                "Steven C.H. Hoi."
            ],
            "title": "BLIP: bootstrapping language-image pretraining for unified vision-language understanding and generation",
            "venue": "International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Balti-",
            "year": 2022
        },
        {
            "authors": [
                "Lei Li",
                "Jingjing Xu",
                "Qingxiu Dong",
                "Ce Zheng",
                "Qi Liu",
                "Lingpeng Kong",
                "Xu Sun"
            ],
            "title": "Can language models understand physical concepts? arXiv preprint arXiv:2305.14057",
            "year": 2023
        },
        {
            "authors": [
                "Xiao Liu",
                "Da Yin",
                "Yansong Feng",
                "Dongyan Zhao."
            ],
            "title": "Things not written in text: Exploring spatial commonsense from visual signals",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),",
            "year": 2022
        },
        {
            "authors": [
                "Tobias Norlund",
                "Lovisa Hagstr\u00f6m",
                "Richard Johansson"
            ],
            "title": "Transferring knowledge from vision to language: How to achieve it and how to measure it",
            "year": 2021
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark",
                "Gretchen Krueger",
                "Ilya Sutskever"
            ],
            "title": "Learning transferable visual models from natural language",
            "year": 2021
        },
        {
            "authors": [
                "Alec Radford",
                "Jeff Wu",
                "Rewon Child",
                "David Luan",
                "Dario Amodei",
                "Ilya Sutskever."
            ],
            "title": "Language models are unsupervised multitask learners",
            "venue": "OpenAI blog.",
            "year": 2019
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J. Liu."
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "J. Mach. Learn. Res., 21:140:1\u2013140:67.",
            "year": 2020
        },
        {
            "authors": [
                "Robin Rombach",
                "Andreas Blattmann",
                "Dominik Lorenz",
                "Patrick Esser",
                "Bj\u00f6rn Ommer."
            ],
            "title": "Highresolution image synthesis with latent diffusion models",
            "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans,",
            "year": 2022
        },
        {
            "authors": [
                "Timo Schick",
                "Hinrich Sch\u00fctze."
            ],
            "title": "Exploiting cloze-questions for few-shot text classification and natural language inference",
            "venue": "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Vol-",
            "year": 2021
        },
        {
            "authors": [
                "Dhruv Shah",
                "Blazej Osinski",
                "Brian Ichter",
                "Sergey Levine."
            ],
            "title": "Lm-nav: Robotic navigation with large pre-trained models of language, vision, and action",
            "venue": "Conference on Robot Learning, CoRL 2022, 14-18 December 2022, Auckland, New Zealand, volume",
            "year": 2022
        },
        {
            "authors": [
                "Alon Talmor",
                "Jonathan Herzig",
                "Nicholas Lourie",
                "Jonathan Berant."
            ],
            "title": "Commonsenseqa: A question answering challenge targeting commonsense knowledge",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
            "year": 2019
        },
        {
            "authors": [
                "Maria Tsimpoukelli",
                "Jacob Menick",
                "Serkan Cabi",
                "S.M. Ali Eslami",
                "Oriol Vinyals",
                "Felix Hill."
            ],
            "title": "Multimodal few-shot learning with frozen language models",
            "venue": "Advances in Neural Information Processing Systems 34: Annual Conference on Neural",
            "year": 2021
        },
        {
            "authors": [
                "Ben Wang",
                "Aran Komatsuzaki."
            ],
            "title": "GPT-J6B: A 6 Billion Parameter Autoregressive Language Model",
            "venue": "https://github.com/kingoflolz/ mesh-transformer-jax.",
            "year": 2021
        },
        {
            "authors": [
                "Weizhi Wang",
                "Li Dong",
                "Hao Cheng",
                "Haoyu Song",
                "Xiaodong Liu",
                "Xifeng Yan",
                "Jianfeng Gao",
                "Furu Wei."
            ],
            "title": "Visually-augmented language modeling",
            "venue": "arXiv preprint arXiv:2205.10178.",
            "year": 2022
        },
        {
            "authors": [
                "Yue Yang",
                "Wenlin Yao",
                "Hongming Zhang",
                "Xiaoyang Wang",
                "Dong Yu",
                "Jianshu Chen."
            ],
            "title": "Z-lavi: Zero-shot language solver fueled by visual imagination",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2022
        },
        {
            "authors": [
                "Yuan Yao",
                "Tianyu Yu",
                "Ao Zhang",
                "Mengdi Li",
                "Ruobing Xie",
                "Cornelius Weber",
                "Zhiyuan Liu",
                "Haitao Zheng",
                "Stefan Wermter",
                "Tat-Seng Chua"
            ],
            "title": "Visually grounded commonsense knowledge acquisition",
            "venue": "arXiv preprint arXiv:2211.12054",
            "year": 2022
        },
        {
            "authors": [
                "Chenyu Zhang",
                "Benjamin Van Durme",
                "Zhuowan Li",
                "Elias Stengel-Eskin."
            ],
            "title": "Visual commonsense in pretrained unimodal and multimodal models",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computa-",
            "year": 2022
        },
        {
            "authors": [
                "har",
                "Tianlu Wang",
                "Luke Zettlemoyer"
            ],
            "title": "2022b. OPT: open pre-trained transformer language models. CoRR, abs/2205.01068",
            "year": 2022
        },
        {
            "authors": [
                "Zihao Zhao",
                "Eric Wallace",
                "Shi Feng",
                "Dan Klein",
                "Sameer Singh."
            ],
            "title": "Calibrate before use: Improving few-shot performance of language models",
            "venue": "Proceedings of the 38th International Conference on",
            "year": 2021
        },
        {
            "authors": [
                "\u2022 MAGMA (Eichenberg"
            ],
            "title": "2022) adds additional adapter layers into frozen LLMs to augment them with visual capabilities. It also finetunes a visual encoder to transform images",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "With the breakthrough progress of Large Language Models (LLMs) in recent years (Brown et al., 2020; Zhang et al., 2022b), LLMs are gradually adopted as general-purpose API interfaces (e.g., ChatGPT1). In addition to language, these intelligent agents, are further required to understand vision knowledge (Hao et al., 2022), especially the visual perception, which is crucial for real-world interactions such as commonsense reasoning (Talmor et al., 2019), recipe generation (Agarwal et al., 2020), and robotic navigation (Shah et al., 2022).\nHowever, current studies lack a systematic evaluation on how well these widely-used LLMs and their variants are capable of visual understanding. Recent research proposes to evaluate the visual capability of models through visual commonsense evaluation (Bagherinezhad et al., 2016;\n\u2217Co-first authors with equal contributions 1https://chat.openai.com\nNorlund et al., 2021). As shown in Figure 1, visual commonsense evaluation aims to evaluate the model\u2019s understanding of commonly shared human knowledge about generic visual concepts, including color (Bruni et al., 2012; Norlund et al., 2021; Zhang et al., 2022a), spatial relations (Liu et al., 2022), relative sizes (Bagherinezhad et al., 2016), etc. Despite their insightful investigations, these studies still have the following limitations from two sides: 1) data side: some research mines visual commonsense attributes based on frequency distributions in plain text corpora, which diverges from human visual perception and exhibits additional textual bias (Zhang et al., 2022a); 2) model side: most existing evaluations only focus on a specific model group, lacking a comprehensive exploration of various model families (Bagherinezhad et al., 2016; Norlund et al., 2021; Liu et al., 2022).\nIn this work, we propose that similar to human beings, models can also answer intricate visual commonsense questions with related images (illustrated in Figure 1). To this end, we introduce IMAGENETVC, a unified zero- and few-shot visual commonsense benchmark incorporating multiple sources of images (e.g., ImageNet (Deng et al., 2009), search images, and synthetic images). From\nthe data side, IMAGENETVC comprises 4,076 highquality QA pairs, encompassing 1,000 ImageNet categories across diverse domains such as color, shape, material, component, etc. Moreover, as a human-annotated dataset, IMAGENETVC utilizes human visual perception to identify shared attributes across relevant images, avoiding textual bias and providing data that is more closely aligned with human knowledge. From the model side, besides unimodal LLMs, IMAGENETVC also enables the evaluation of various Visually-augmented Language Models (VaLMs) to investigate the effect of visual grounding, which compensates for the lack of images in previous benchmarks.\nWith IMAGENETVC, we conduct extensive evaluations on the most widely-used LLMs and VaLMs. We benchmark the visual commonsense capabilities of various LLMs such as OPT, LLaMA, and Falcon and assess the effect of visual grounding in VaLMs with multiple sources of relevant images. We further analyze the co-founding factors that may affect the visual commonsense capability of models, such as model scale, in-context learning, and image sources. We highlight several experimental findings. These findings support the high value of our benchmark in assessing visual commonsense capabilities.\n\u2022 Template-based datasets yield artificially inflated and unstable visual commonsense evaluation, while our manually constructed IMAGENETVC provides evidence that visual commonsense remains challenging for LLMs.\n\u2022 We discover that the acquisition of visual commonsense is an emergent ability for LLMs. For instance, 1.3B could be a potential threshold for unimodal LLMs to emergent with visual commonsense on the component.\n\u2022 In-context learning enhances the understanding of visual commonsense tasks for both LLMs and VaLMs, not only reducing their variance across prompts but also calibrating the model confidence on visual commonsense."
        },
        {
            "heading": "2 Related Work",
            "text": "Large Language Models Text-only Large language models (LLMs) have exhibited outstanding performance across various textual commonsense tasks, benefiting from their training on extensive textual data (Radford et al., 2019; Raffel et al., 2020; Brown et al., 2020). However, the lack of vi-\nsual data (e.g., images) during pretraining restricts their visual commonsense capabilities (Li et al., 2023b). On the other hand, Visually-augmented Language Models (VaLMs) have gained popularity by integrating visual information into LLMs (Tsimpoukelli et al., 2021; Alayrac et al., 2022), which enhance the visual understanding capabilities of language models (Yang et al., 2022; Wang et al., 2022).\nVisual Commonsense Evaluation Visual commonsense knowledge of visual concepts is a fundamental and critical aspect of AI systems seeking to comprehend and reason about the world (Yao et al., 2022; Dong et al., 2022). Previously, several datasets have been proposed to address specific attributes of visual commonsense, including MemoryColors (Norlund et al., 2021), ColorTerms (Bruni et al., 2012), RelativeSize (Bagherinezhad et al., 2016), and Spatial Commonsense (SpatialCS) (Liu et al., 2022). To evaluate general visual commonsense, Zhang et al. (2022a) introduced ViComTe, a template-based dataset consisting of various (subject, object) pairs (such as (sky, blue)). However, its reliance on pure textual input underestimates the visual capabilities of VaLMs. Furthermore, its utilization of template-based formats and automatic extraction techniques leads to substandard data quality and inherent textual biases.\nIn this work, we introduce IMAGENETVC, a human-annotated visual commonsense evaluation dataset that consists of 4K natural language QA pairs across various visual attributes, which supports both LLM and VaLM evaluation with multiple sources of images. We present detailed comparisons of IMAGENETVC with prior work in Table 1."
        },
        {
            "heading": "3 IMAGENETVC",
            "text": "Starting from ImageNet, we construct our IMAGENETVC dataset in a multi-step crowd-sourcing pipeline, including 1) annotator training, 2) commonsense annotation, and 3) cross-check examination. An overall demonstration of our annotation process is illustrated in Figure 2."
        },
        {
            "heading": "3.1 Image Source",
            "text": "We selected ImageNet (Deng et al., 2009) as our image source because it covers a large number of commonly used objects in real-life situations, providing a diverse and representative image source. Additionally, the unified image format in ImageNet with\nDataset HumanAnnotation Multi-attribute Evaluation Support VaLM Region-based Question Natural Language #Category #Test\ndimensions of 256\u00d7256 pixels facilitates annotators\u2019 understanding of images and reduces feature engineering. Specifically, we used the widely-used ImageNet (ILSVRC) 2012 subset,2 consisting of 1.4 million images from 1,000 object categories."
        },
        {
            "heading": "3.2 Prerequisite: Annotator Training",
            "text": "We posted online job listings on Amazon Mechanical Turk3 and received over 500 applications from candidates with Bachelor\u2019s degrees or higher. To ensure dataset quality, we provided training with instructions and guidelines and a quick quiz to assess candidate understanding. Only candidates with scores larger than 95% are hired."
        },
        {
            "heading": "3.3 Phase 1: Commonsense Annotation",
            "text": "Figure 2 shows the commonsense annotation phase, where annotators are provided with category names and 50 randomly sampled images per category. They are instructed to form a question and answer considering shared visual features of the images and their own commonsense knowledge. Visual features may be object-based, such as the color of a entire object, or region-based, such as the color of a specific object part. Annotators first identify a common visual feature of the category, such as The\n2image-net.org/challenges/LSVRC/2012/ 3https://www.mturk.com/\ncolor of a Samoyed\u2019s body is white. They then create a QA pair based on this feature if it aligns with their commonsense understanding of life, such as What is the color of a Samoyed\u2019s body? White.\nTo ensure that the QA pairs reflect visual commonsense rather than visual information tailored to specific images, annotators are instructed to focus on the visual features of each category rather than individual images. They are also provided with annotation examples and guidelines for rejection. The annotation UI and specifications for annotation can be found in Appendix A."
        },
        {
            "heading": "3.4 Phase 2: Cross-Check Examination",
            "text": "The primary objective of the cross-check examination phase is to conduct a rigorous screening and categorization of high-quality QA pairs that meet our requirements. This phase comprises two stages. In Stage 1, a category-level examination is performed, where three examiners are assigned to all annotated QA pairs in the same category. They are required to check all the pairs in the category based on the annotation instructions, rectify any grammatical errors, and eliminate low-quality or noncompliant pairs. Only the QA pairs that all three examiners approve are deemed acceptable. Stage 2 involves a sample-level examination. Although the examination in Stage 1 is efficient, examining\nall QAs in one category simultaneously creates a misalignment with the final testing method (oneby-one QA) and introduces a distribution bias in the examination process. Therefore, in Stage 2, a more thorough sample-level examination is carried out. Three examiners are randomly assigned a QA pair with the corresponding category name from the entire dataset. They vote on whether to accept the QA pair and classify it into the following five subsets: color, shape, material, component, and others. Only the sample that receives the majority vote is approved for acceptance.\nOur 60-day annotated dataset comprises 4,076 items (refer to Table 2) from 1000 ImageNet categories. It consists of 5 individual sub-tasks: color, shape, material, component, and others. More information and examples of IMAGENETVC can be found in Appendix B. All pricing strategy details and a hierarchical supervision process employed are elaborated in Appendix A.3 and A.4."
        },
        {
            "heading": "3.5 Dataset Evaluation",
            "text": "Unlike previous datasets which are template-based, IMAGENETVC comes from diverse real images associated with human-annotated descriptions, which can better represent real-world settings. To assess the strengths of our dataset, we conduct automatic evaluation and human evaluation in this section.\nFirst, we implement GPT-Neo-1.3B (Black et al.,\n2021) with respective subsets of IMAGENETVC and ViComTe, a widely-used dataset, across different prompts.4 Results in Figure 3 indicate that, as a template-based dataset, ViComTe exhibits severe prompt bias, with substantial evaluation variance across different prompts. E.g., the model achieves only 2% accuracy with the prompt \u201cX is of shape Y\u201d but achieves 61% score with \u201cThe shape of the X is Y\u201d. Besides, compared with ViComTe, IMAGENETVC containing region-based questions is more challenging to models. For example, with the suitably selected prompt on the color subset, the model achieves 40% accuracy on ViComTe but only 28% accuracy on IMAGENETVC.\nWe further conducted a human assessment between ViComTe, IMAGENETVC, and QA pairs automatically generated from ChatGPT.5 Specifically, we provided human annotators with sampled data from the two comparison datasets and asked them to vote for the better one considering diversity, difficulty, and factuality. As depicted in Figure 4, in more than 84% of cases, IMAGENETVC outperforms or matches the template-based ViComTe in terms of diversity and difficulty, which is consistent with the results in Figure 3. Moreover, our dataset demonstrates notably higher factual correctness compared to the data automatically generated by ChatGPT in more than 81% of cases.\nTo sum up, our data collection process ensured high-quality annotations with minimal bias and increased diversity, difficulty, and factuality, pro-\n4Prompt details are provided in Appendix C.1. 5Please refer to Appendix C.2 for the detailed process of\nour designed human assessment.\nviding a challenging dataset for advancing research in visual commonsense understanding."
        },
        {
            "heading": "4 Experiments",
            "text": "Our experiments primarily focus on two types of language models: LLMs and VaLMs. Both models have demonstrated promising capabilities in understanding visual information (Li et al., 2023b).\nLarge Language Models We begin with the textonly setting, to explore the visual commonsense capability of LLMs learned from the textual corpus. We focus on the dominant auto-regressive LLM family and benchmark a lot of model variants, including the GPT (Black et al., 2021; Wang and Komatsuzaki, 2021; Black et al., 2022), OPT (Zhang et al., 2022b), LLAMA (Touvron et al., 2023), Falcon (Almazrouei et al., 2023), and Pythia (Biderman et al., 2023).\nVisually-augmented Language Models In our experiments, we mainly evaluate three widely-used open-source VaLMs: Z-LaVI (Yang et al., 2022), BLIP-2 (Li et al., 2023a), and MAGMA (Eichenberg et al., 2022). These VaLMs are mainly built on top of frozen LLMs and incorporate diverse mechanisms to integrate visual information. Further model details are provided in Appendix C.4."
        },
        {
            "heading": "4.1 Evaluation Methods",
            "text": "In this work, we focus on evaluating the zero- and few-shot visual commonsense of LLMs and VaLMs on IMAGENETVC. Following Schick and Sch\u00fctze (2021) and Yang et al. (2022), we treat the zeroshot evaluation as a cloze test, transforming the QA pairs in IMAGENETVC into prompts like \u201c[Question] The answer is [Answer].\u201d6. Formally, each QA pair is converted into a sequence of tokens x = {x0, ..., xi, ..., xn}, in which xi is the answer. In the few-shot setting, examples with the same prompt are concatenated before each QA pair."
        },
        {
            "heading": "4.1.1 LLM Evaluation",
            "text": "Given an LLM M, the sequence of input tokens x = {x0, ..., xn} will first be mapped to text embeddings et = {et(x0), ..., et(xi), ..., et(xn)} by the embedding layer et \u2208 M. Then we utilize the\n6All the prompts utilized for the evaluation of LLMs and VaLMs are shown in Appendix C.1.\nmodel to calculate the score for the answer y \u2208 Y:\nst(y | x) = 1\n\u2212 logPM (xi | x<i)\n= 1\n\u2212 logPM\u2032 (et(xi) | et(x<i))\nwhere M\u2032 denotes the transformer neural network in M, P (\u00b7) is the output probability given by the model. Then we obtain a probability distribution over all answer candidates using softmax:\nqt(y | x) = es(y|x)\u2211\ny\u2032\u2208Y e s(y\u2032|x\u2032) (1)\nWe calibrate the prediction by normalizing the probability distribution following Zhao et al. (2021), to mitigate the bias introduced by prompt formats as well as few-shot examples."
        },
        {
            "heading": "4.1.2 VaLM Evaluation",
            "text": "We incorporate two types of image sources as additional visual inputs for evaluating VaLMs: images retrieved from the web and synthetic images. We adopt Google Image Search to retrieve relevant images and Stable Diffusion (Rombach et al., 2022) for image synthesis. Following Yang et al. (2022), for each QA pair, we utilize CLIP (Radford et al., 2021) to sort images from these two sources based on their similarity with the question and then preserve top-K images as the final image sources. We mainly evaluate two types of VaLMs: prefix-based VaLMs and ensemble-based VaLMs.\nPrefix-based VaLMs Given a QA pair with an image v, prefix-based VaLMs (e.g., BLIP-2 and MAGMA) first utilize a visual encoder to transform the image into a sequence of visual embeddings ev = {e1v, ..., emv }. Then, these embeddings are prefixed into the text embeddings of x and put into the frozen LLM backbone to calculate the score:\ns(y | v,x) = 1\u2212 logPM (xi | v,x<i)\n= 1\n\u2212 logPM\u2032 (et(xi) | ev, et(x<i))\nThe probability distribution with the image v is calculated over all answer candidates, which is the same as Eq (1). If K images are provided, the final distribution will be averaged over all images:\nq(y | v,x) = 1 K K\u2211 i=1 q(y | v(i),x) (2)\nFollowing Dong et al. (2023), for prefix-based VaLMs supporting few-shot evaluations, examples\n{v(j),x(j)}Lj=1 with the same processing will be concatenated in front of each QA pair.\nSince prefix-based VaLMs utilize frozen LLM backbones, they can be regarded as a conditional extension of text-only LLMs. Evaluations between these two model types facilitate a thorough assessment of the effect of visual grounding on visual commonsense ability.\nEnsemble-based VaLMs Given the input tokens x and multiple images v = {v(i)}Ki=1, ensemblebased VaLMs (e.g., Z-LaVI) utilize a frozen CLIP model, which contains a text encoder ft and a visual encoder fv, to project the tokens x and the image v(i) into a shared representation space and compute the relevance score between them:\nsv(y | v(i),x) = cos ( ft(x), fv(v (i)) )\nThen, same as Eq (1) and Eq (2), the probability distribution over all answer candidates and across K images is obtained:\nqv(y | v,x) = 1\nK K\u2211 i=1 softmax ( sv(y | v(i),x) ) where softmax(\u00b7) is a simplified denotation of Eq (1). The final ensemble score is calculated as\na weighted sum over the output distributions of LLMs and CLIP:\nq(y | v,x) = (1\u2212 w) \u00b7 qt(y | x) + w \u00b7 qv(y | v,x)\nwhere w denotes the weight hyperparameter."
        },
        {
            "heading": "4.2 Experimental Details",
            "text": "We adopt Google Image Search to retrieve relevant images and utilize the newly released Stable Diffusion (Rombach et al., 2022) for image synthesis.7 Following Yang et al. (2022), for each QA pair in IMAGENETVC, we obtain 100 images with each of the two methods. These 200 images are sorted using CLIP based on their similarity with the question. We preserve top-10 (K = 10) images for each QA pair as the final image sources. The other experimental details, such as the model implementation, hyperparameters, and computing resources are presented in Appendix C.3."
        },
        {
            "heading": "4.3 Main Results",
            "text": "The main evaluation results of LLMs and VaLMs on IMAGENETVC are shown in Figure 5. Here, we highlight several interesting findings.\n7https://github.com/CompVis/stable-diffusion and we use the sd-v2-1 checkpoint.\nFalcon and LLaMA excel in all four presented LLM model families, especially on the color and component sub-tasks. As shown in Figure 5(a, b), Falcon and LLaMA consistently outperform OPT and GPT across various subsets with both experimental settings, despite the shape subset. Particularly, LLaMA achieves a zero-shot accuracy of 41% on the color subset, surpassing GPT-J with a considerable margin of 13%; Falcon yields the highest few-shot accuracy of 76% on the component subset, which amounts to 14% absolution improvement over OPT. We further present the few-shot results of the largest available LLMs in their own model family in Figure 5(c), where LLaMA-65B shows remarkable superiority over other counterparts.\nIn-context learning (ICL) not only improves the visual commonsense performance of LLMs but also reduces their variance across different prompts. Comparing the results in Figure 5(a) and 5(b), we found that given a few examples (i.e., with ICL), LLMs achieve consistent and remarkable improvement over themselves. For instance, LLaMA-7B with ICL achieves an average score of 62% across five sub-tasks, with a 12% improvement over the zero-shot result. We further show the performance distribution of LLaMA across different prompts in Figure 6, which illustrates that ICL not only improves the model\u2019s performance but also reduces its variance across different prompts. Further analysis is conducted in Section 5.\nVaLMs improve the visual commonsense ability of their LLM backbones, despite small performance gains on the shape subset. As depicted in Figure 5(d, e), BLIP-2 shows remarkable superiority over OPT on the color and material subset, with average accuracy improvements of 17% and 15%, respectively, which indicates that incorpo-\nrating visual information indeed helps to improve LLMs\u2019 visual commonsense capabilities. However, the results also show that the performance gains of VaLMs are small on some sub-tasks: both BLIP-2 and MAGMA only achieve an 0.5% accuracy improvement on the shape sub-task, while Z-LaVI even has performance drops. This demonstrates that VaLMs still have wide room for improvement.\nICL capability of VaLMs should be further valued. As shown in Figure 5(f), MAGMA with ICL achieves consistent improvements across all subsets over itself and the few-shot results of the frozen LLM backbone, indicating that ICL could also improve VaLMs\u2019 visual commonsense performances. However, ICL has been somewhat underinvestigated by previous VaLM research. For example, both Z-LaVI and BLIP-2 only support zeroshot evaluation, with the lack of ICL capability. We hope our research could draw more attention to future work on the ICL capability of VaLMs."
        },
        {
            "heading": "5 Analysis",
            "text": "We further investigate the factors influencing the visual commonsense capabilities of LLMs and VaLMs. For instance, we find that a decent scale (e.g., 1.3B) could be a potential threshold for textonly LLMs to learn visual commonsense. We then analyze several influencing factors of VaLMs, such as image sources and image numbers.\nWhen (at what model scale) do text-only LLMs learn visual commonsense? We show the zeroand few-shot results of three LLM families on the component subset in Figure 7. Take the component sub-task as an example, we find that a decent scale (e.g., 1.3B) could be a starting point for LLMs to emerge with visual commonsense on the com-\nponent8: smaller models at sizes below 1.3B are unable to perform well on the task, with a performance close to random guessing (i.e., ~50% accuracy); while models with a size larger than 1.3B exhibit gradual performance improvements. For example, OPT-30B achieves 59% and 66% average accuracy with zero- and few-shot settings, respectively.\nWhat is the effect of ICL on the calibration of LLMs? Ensuring the reliability of a model\u2019s predictions is a crucial aspect of model evaluation, as mis-calibrated models can lead to incorrect inferences and serious consequences in real-world applications (Guo et al., 2017). To this end, we conducted a calibration analysis to evaluate whether the model confidence on visual commonsense reliably reflects the actual probability of the prediction being correct. Our analysis focuses on the calibration of LLaMA-7B on the component subset of IMAGENETVC. Results in Figure 8 indicate that ICL significantly improves the calibration of the model, increasing the correlation between confidence and accuracy from r = 0.57 to r = 0.98. This is consistent with the aforementioned findings, suggesting that ICL improves the visual commonsense performance of LLMs.\nHow do image sources influence VaLMs? Table 3 shows the ablation results of the image sources used in VaLMs. As illustrated, BLIP-2 with extra image sources (e.g., SEARCH) brings large improvements. This supports IMAGENETVC\u2019s motivation, suggesting that previous visual commonsense evaluations undervalue VaLMs\u2019 potential, as\n8Please note that, as the evaluated LLMs (OPT and Pythia) both rely on the Pile (Gao et al., 2021) as their pre-training corpus, our findings may not be generalized to other LLMs.\nVaLMs\u2019 visual commonsense demands relevant images as input to be suitably stimulated. Among the various image sources, CLIP-ranked images yield the best performance, suggesting that aligning images closely with the question facilitates the generalization of related visual commonsense by models. Thus, we use ranked images as the default image source for our main experiment and analysis.\nWhat is the typical number of images required to capture visual commonsense? We show the model\u2019s average performance on IMAGENETVC with various numbers of images in Figure 9. The results show that the model\u2019s performance increases as the number of top-ranked images gradually increases from 1 to 10, indicating diverse image sources help the model to capture general visual commonsense. However, the improvement is marginal when the image number is larger than 10."
        },
        {
            "heading": "6 Visual Commonsense in Other Models",
            "text": "It is worth noting that, as a general visual commonsense dataset, IMAGENETVC supports various types of models and evaluation settings. Except for the evaluation setting in our main results, we also evaluate several models in the setting of open-ended generation. Specifically, we select two widely-used multimodal models, 12-in-1 (Lu et al., 2020) and BLIP (Li et al., 2022) that are finetuned on the VQAv2 dataset (Goyal et al., 2017) and the famous RLHF model, ChatGPT (gpt-3.5-turbo)"
        },
        {
            "heading": "Models COL. SHA. MAT. COM. OTH. AVG",
            "text": "for evaluation.9 As illustrated in Table 4, the multimodal models finetuned on VQAv2 show strong performance on IMAGENETVC, especially on the color sub-task, with relatively small model scales (e.g., 583M of BLIP). ChatGPT with ICL achieves the best average accuracy score of 75.8% across all compared models. However, it still has a considerable performance gap with humans, which has an average performance of 93.5%."
        },
        {
            "heading": "7 Conclusion",
            "text": "In this paper, we introduced IMAGENETVC, a comprehensive human-annotated dataset for evaluating visual commonsense using both textual and visual inputs. We conducted extensive experiments to evaluate the visual commonsense of both unimodal LLMs and VaLMs using IMAGENETVC. Our results demonstrate the varying degrees of visual commonsense knowledge present in different models, as well as the factors that contribute to the acquisition and enhancement of this knowledge. Additionally, we offer insights into the emergent abilities of LLMs and the strengths of VaLMs in the realm of visual commonsense."
        },
        {
            "heading": "Limitations",
            "text": "While our study provides valuable resources and insights into the visual commonsense knowledge of LLMs and VaLMs, several limitations need to be acknowledged. Firstly, due to the high cost of region-based human annotation, the IMAGENETVC dataset only covers 1,000 ImageNet categories, which may not cover all real-world scenarios. Therefore, it is possible that models may perform differently on other types of images that are outside of the IMAGENETVC categories.\nAdditionally, our study is limited to zero- and few-shot visual commonsense evaluation and only considers models that have been pretrained on\n9The evaluation details are illustrated in Appendix E.\nlarge amounts of text data. Thus, it remains unclear whether fine-tuning on visual commonsense tasks would improve the performance of LLMs and VaLMs. Therefore, this may not fully capture a model\u2019s ability to understand visual commonsense in real-world scenarios where prior knowledge may be available. Besides, although we explored the factors that affect the visual commonsense knowledge of large models, it is still challenging to interpret how the models acquire this knowledge.\nOverall, our study provides a foundation for future work in the field of visual commonsense knowledge and its applications. However, additional research is necessary to address the aforementioned limitations and further advance the field."
        },
        {
            "heading": "Acknowledgements",
            "text": "This paper is supported by the National Key Research and Development Program of China 2020AAA0106700 and NSFC project U19A2065."
        },
        {
            "heading": "Appendix",
            "text": ""
        },
        {
            "heading": "A Annotation Details",
            "text": "In this section, we will provide a comprehensive overview of our annotation process, including the guidelines we follow, the user interface we use, the hierarchical supervision process we employ to ensure data quality, and our payment policy."
        },
        {
            "heading": "A.1 Annotation Guidelines",
            "text": "The annotation of IMAGENETVC involves observing 20-50 images of a given category, finding a vision feature of the category, checking if it conforms to most of the images and our commonsense of life, and then writing a simple question-answer (QA) about this vision feature. The QA should contain one question and one correct answer.\nThe vision features can be object-based (such as the color, shape, material, and spotted/striped patterns of the whole object) or region-based (such as the color, shape, and material of a certain part of the object). They are features that can be seen through the images.\nThe annotation pipeline involves looking at the 20-50 images given and finding a common vision feature of the category. For example, \"The shape of the dorsal fin of the tiger shark is triangle\". The annotators check if this feature conforms to their commonsense of life and if it is written. Then, one QA is created, such as \"What is the shape of the dorsal fin of the tiger shark? Triangle\".\nThe following rules must be followed during the annotation process:\n\u2022 The question should contain the name of the category. Otherwise, the submission will not be passed. \u2022 If the annotator cannot think of a question that can be written or the images cannot be displayed, the annotator can skip this category. \u2022 The first letter of the question needs to be capitalized. \u2022 The end of the question needs to be a question mark. \u2022 Please do not write lots of Yes/No questions. These questions are more likely to be rejected. We encourage the annotators to write more diverse answers.\nIn the annotation examples, we describe how the correct QA is created. Annotators can write their own QA according to this pipeline. The rejected examples include cases where the QA has been\nwritten before, vision features cannot be found in the images, or the QA is not about vision features. Additionally, the QA should conform to our commonsense of life, be strongly related to the category, and be about a specific feature of the category.\nIn summary, the annotation guidelines of IMAGENETVC involve observing images of a category, finding a vision feature, and creating a QA about the feature that conforms to our commonsense of life and follows the rules outlined above. These guidelines ensure the accuracy and consistency of the annotations, making the dataset suitable for use in various applications"
        },
        {
            "heading": "A.2 Annotation UI",
            "text": "Figure 11 shows the annotation user interface used in our human annotation process for model knowledge assessment. The interface consists of three main parts: the task instruction, the annotation pipeline, and the most common cases we reject. The task instruction provides clear guidance for the annotators on how to write effective prompts to assess the model knowledge. The annotation pipeline displays the generated text by the model and allows the annotators to refine their prompts until the generated text matches the expected target answer. The rejected cases section provides examples of prompts that do not meet the criteria and serves as a reference for the annotators to avoid such mistakes. The user interface design is intuitive and user-friendly, which greatly improves the efficiency and accuracy of the human annotation process."
        },
        {
            "heading": "A.3 Hierarchical Supervision",
            "text": "To ensure high-quality annotation, we have implemented a hierarchical supervision process. During the annotation phase, examiners cross-check the annotation results, and annotators who are excessively rejected receive a warning. Those who exceed the warning limit are removed. Additionally, during the examination phase, a random sample check of the examination results is performed by five authors, and examiners with low-quality checks also receive a warning. The hierarchical supervision process guarantees the high-quality execution of the entire annotation process."
        },
        {
            "heading": "A.4 Payment Policy",
            "text": "We compensated the crowd workers with varying rates based on the workload and quality of their\nwork. In Phase 1, workers were tasked with summarizing shared visual characteristics from 50 images and creating QA pairs. If their work was accepted in Phase 2, they would receive $0.50 for each sample. However, if their work was rejected, they would only earn $0.10 per sample. In Phase 2, annotators were paid $0.30 per sample for cross-checking tasks. On average, the annotators received approximately double the local minimum wage per hour."
        },
        {
            "heading": "B Details of IMAGENETVC",
            "text": ""
        },
        {
            "heading": "B.1 Details of the Others subset",
            "text": "Annotated QA pairs that do not belong to the four specified sub-tasks (e.g., color, shape, material, and component) will be categorized into the Others subset. Therefore, The Others subset contains a more diverse range of QA samples, which is more challenging. Figure 10 illustrates the detailed composition of QA types in the Others subset, which covers various topics such as length comparison (21%), relative size (20%), living environment (16%), counting (12%), etc."
        },
        {
            "heading": "B.2 Answer Set",
            "text": "Considering that the results of open-ended generation are uncontrollable, we evaluate all models with constrained decoding in our main experiments. Table 5 shows the list of all possible answers in IMAGENETVC. Besides, we noticed that LLMs tend to predict \u201cyes/no\u201d or numerical answers when evaluated on the others subset. Thus, we split the others subset into three small test sets, containing answer\ntypes of \u201cyes/no\u201d, numbers, and other answers, respectively."
        },
        {
            "heading": "B.3 More Qualitative Examples",
            "text": "We present additional qualitative examples in Table 6, which compare the predictions made by various models. The comparisons between OPT-7B and BLIP-2 demonstrate the effectiveness of incorporating visual information in enhancing the visual commonsense capabilities of LLMs. However, these leading models, including ChatGPT, also encounter difficulties in certain challenging cases, such as determining the color of a flamingo\u2019s beak tip. Besides, these examples highlight ChatGPT\u2019s tendency to prioritize selecting the most commonly associated property of an object as the answer, rather than considering the properties of the specific region in question. We hypothesize that this behavior may be attributed to the higher frequency of these common attributes co-occurring with the object in the pre-training text corpus."
        },
        {
            "heading": "C Experimental Details",
            "text": ""
        },
        {
            "heading": "C.1 Multiple Prompts",
            "text": "Table 7 shows all the intuitive prompt templates utilized to evaluate the LLMs and VaLMs in our main experiments. We do not tune the prompt for each subset in IMAGENETVC.\nIn our data quality experiments, we utilize the original prompts provided by Zhang et al. (2022a) for ViComTe evaluation while adopting the prompts in Table 7 for IMAGENETVC."
        },
        {
            "heading": "C.2 Details of Human Assessment",
            "text": "In the human assessment process of ImageNetVC, annotators were presented with pairs of mutually exclusive random 32 instances from both ImageNetVC and other datasets (e.g., ViComTe) each time. In the whole process of comparison evaluation, we conducted multiple rounds of human assessment based on the dataset containing fewer test samples. Specifically, there are 556 comparison pairs used for evaluation between ImageNetVC and ViComTe, and 510 pairs used for evaluation between ImageNetVC and ChatGPT generated data.\nAnnotators were instructed to evaluate these instances based on the overall quality of the data, choosing the better side when considering three factors respectively: diversity, difficulty, and factuality. The final results were computed and demonstrated as percentages in Figure 4. For instance, the Diversity score depicted in Figure 4(a) signifies that in 86% of cases, the evaluators found ImageNetVC samples to either outperform or match those from\nViComTe concerning diversity."
        },
        {
            "heading": "C.3 Model Implementation",
            "text": "All the LLMs are implemented based on the Huggingface API10. For VaLMs, we utilize the official release of Z-LAVI11, MAGMA12, and BLIP-213 for evaluation. The CLIP model is adapted from the OpenAI\u2019s public source14. We utilize the visionlanguage pretrained checkpoints of VaLMs instead of task-specific finetuned models to evaluate their zero- and few-shot capabilities. The details of VaLMs, including the visual encoder architecture and pretraining data, are included in Appendix C.4. All hyperparameters of VaLMs are the same as that of their origin paper. We conduct our experiments on 6 NVIDIA A40 GPUs with 48GB memory."
        },
        {
            "heading": "C.4 Model Details",
            "text": "We mainly conduct our evaluations with the following VaLMs in our experiments:\n10https://huggingface.co 11https://github.com/YueYANG1996/Z-LaVI 12https://github.com/Aleph-Alpha/magma 13https://github.com/salesforce/LAVIS/tree/\nmain/projects/blip2 14https://github.com/openai/CLIP\n\u2022 Z-LaVI (Yang et al., 2022) introduces a zeroshot framework that ensembles the solutions of LLMs and CLIP (Radford et al., 2021) to handle plain language tasks. The extra visual inputs of CLIP are obtained with web search and synthesis methods. \u2022 MAGMA (Eichenberg et al., 2022) adds additional adapter layers into frozen LLMs to augment them with visual capabilities. It also finetunes a visual encoder to transform images into visual prompts as prefixes. \u2022 BLIP-2 (Li et al., 2023a) introduces a Querying Transformer to extract visual features from the frozen image encoder. It then utilizes these features as visual prompts to augment frozen LLMs with visual information.\nWe show the details of VaLMs evaluated on IM-\nreport the mean accuracy (%) results in 5 different prompts. Numbers that are highlighted in orange represent the percentage of improvement and blue denotes the percentage of performance drop.\nPrompt 1\nAnswer List: [CANDIDATES] [QUESTION] Please select the most possible answer from the above list. Please answer in one word.\nPrompt 2\nAnswer List: [CANDIDATES] [QUESTION] Please only print the answer selected in the above list. Please answer in one word.\nPrompt 3\n[QUESTION] Please select the most possible answer from [CANDIDATES]. Please answer in one word.\nTable 11: The prompts we utilize for ChatGPT evaluation on IMAGENETVC. \u201c[CANDIDATES]\u201d denotes the answer set of the evaluated subset in IMAGENETVC, as shown in Table 5.\nAGENETVC in Table 9, including the extra model parameters (except the frozen LLM backbone), the architecture of the visual encoder, and the number of pretraining images. Since the VaLMs vary in implementation details (e.g., the visual encoder), we cannot make a direct (head-to-head) comparison between the VaLMs and leave it for future investigation."
        },
        {
            "heading": "D Details of Main Experimental Results",
            "text": "We provide detailed experimental numbers of our main results with LLMs and VaLMs in Table 8 and 10, respectively. For LLMs, we show the zero- and few-shot evaluation results of OPT, GPT, Pythia, Falcon, and LLaMA across various model scales. For VaLMs, we compare the performance of Z-LaVI, BLIP-2, and MAGMA with their frozen LLM backbones."
        },
        {
            "heading": "E Evaluation Details of Other Models",
            "text": "This section provides evaluation details of VQA finetuned multimodal models and RLHF models.\nMultimodal Models For multimodal models such as BLIP, we adopt the evaluation settings used in VQAv2 (Goyal et al., 2017). Specifically, for each QA pair and its corresponding image, we evaluate the model using open-ended generation and obtain the output answer. Based on the experimental settings outlined in Section 4.1.2, we provide the top-10 ranked images for each QA pair and determine the final answer by majority prediction.\nRLHF Models We evaluate ChatGPT with constrained prompts and automatically compute the top-1 accuracy. The prompts utilized are presented in Table 11."
        }
    ],
    "title": "IMAGENETVC: Zero- and Few-Shot Visual Commonsense Evaluation on 1000 ImageNet Categories",
    "year": 2023
}