{
    "abstractText": "Recent work in Natural Language Processing and Computer Vision has been using textual information \u2013 e.g., entity names and descriptions \u2013 available in knowledge graphs to ground neural models to high-quality structured data. However, when it comes to non-English languages, the quantity and quality of textual information are comparatively scarce. To address this issue, we introduce the novel task of automatic Knowledge Graph Enhancement (KGE) and perform a thorough investigation on bridging the gap in both the quantity and quality of textual information between English and nonEnglish languages. More specifically, we: i) bring to light the problem of increasing multilingual coverage and precision of entity names and descriptions in Wikidata; ii) demonstrate that state-of-the-art methods, namely, Machine Translation (MT), Web Search (WS), and Large Language Models (LLMs), struggle with this task; iii) present M-NTA, a novel unsupervised approach that combines MT, WS, and LLMs to generate high-quality textual information; and, iv) study the impact of increasing multilingual coverage and precision of non-English textual information in Entity Linking, Knowledge Graph Completion, and Question Answering. As part of our effort towards better multilingual knowledge graphs, we also introduce WikiKGE-10, the first human-curated benchmark to evaluate KGE approaches in 10 languages across 7 language families.",
    "authors": [
        {
            "affiliations": [],
            "name": "Simone Conia"
        },
        {
            "affiliations": [],
            "name": "Min Li"
        },
        {
            "affiliations": [],
            "name": "Daniel Lee"
        },
        {
            "affiliations": [],
            "name": "Umar Farooq Minhas"
        },
        {
            "affiliations": [],
            "name": "Ihab Ilyas"
        },
        {
            "affiliations": [],
            "name": "Yunyao Li"
        }
    ],
    "id": "SP:38855cf95b3299ba0169fec8bf2d38eed5805789",
    "references": [
        {
            "authors": [
                "Oshin Agarwal",
                "Heming Ge",
                "Siamak Shakeri",
                "Rami Al-Rfou."
            ],
            "title": "Knowledge graph based synthetic corpus generation for knowledge-enhanced language model pre-training",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Asso-",
            "year": 2021
        },
        {
            "authors": [
                "Junwei Bao",
                "Nan Duan",
                "Zhao Yan",
                "Ming Zhou",
                "Tiejun Zhao."
            ],
            "title": "Constraint-based question answering with knowledge graph",
            "venue": "Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages",
            "year": 2016
        },
        {
            "authors": [
                "Edoardo Barba",
                "Tommaso Pasini",
                "Roberto Navigli."
            ],
            "title": "ESC: Redesigning WSD with extractive sense comprehension",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human",
            "year": 2021
        },
        {
            "authors": [
                "Michele Bevilacqua",
                "Roberto Navigli."
            ],
            "title": "Breaking through the 80% glass ceiling: Raising the state of the art in word sense disambiguation by incorporating knowledge graph information",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for",
            "year": 2020
        },
        {
            "authors": [
                "Jan A. Botha",
                "Zifei Shan",
                "Daniel Gillick."
            ],
            "title": "Entity Linking in 100 Languages",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 7833\u20137845, Online. Association for Computational Linguistics.",
            "year": 2020
        },
        {
            "authors": [
                "Soumen Chakrabarti",
                "Harkanwar Singh",
                "Shubham Lohiya",
                "Prachi Jain",
                "Mausam ."
            ],
            "title": "Joint completion and alignment of multilingual knowledge graphs",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages",
            "year": 2022
        },
        {
            "authors": [
                "Xuelu Chen",
                "Muhao Chen",
                "Changjun Fan",
                "Ankith Uppunda",
                "Yizhou Sun",
                "Carlo Zaniolo."
            ],
            "title": "Multilingual knowledge graph completion via ensemble knowledge transfer",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020, pages",
            "year": 2020
        },
        {
            "authors": [
                "Yang Chen",
                "Chao Jiang",
                "Alan Ritter",
                "Wei Xu."
            ],
            "title": "Frustratingly easy label projection for cross-lingual transfer",
            "venue": "arXiv preprint arXiv:2211.15613.",
            "year": 2022
        },
        {
            "authors": [
                "Zhe Chen",
                "Yuehan Wang",
                "Bin Zhao",
                "Jing Cheng",
                "Xin Zhao",
                "Zongtao Duan."
            ],
            "title": "Knowledge graph completion: A review",
            "venue": "IEEE Access, 8:192435\u2013 192456.",
            "year": 2020
        },
        {
            "authors": [
                "Simone Conia",
                "Roberto Navigli."
            ],
            "title": "Framing word sense disambiguation as a multi-label problem for model-agnostic knowledge integration",
            "venue": "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Lin-",
            "year": 2021
        },
        {
            "authors": [
                "Alexis Conneau",
                "Kartikay Khandelwal",
                "Naman Goyal",
                "Vishrav Chaudhary",
                "Guillaume Wenzek",
                "Francisco Guzm\u00e1n",
                "Edouard Grave",
                "Myle Ott",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Unsupervised cross-lingual representation learning at scale",
            "venue": "Pro-",
            "year": 2020
        },
        {
            "authors": [
                "Marta R Costa-juss\u00e0",
                "James Cross",
                "Onur \u00c7elebi",
                "Maha Elbayad",
                "Kenneth Heafield",
                "Kevin Heffernan",
                "Elahe Kalbassi",
                "Janice Lam",
                "Daniel Licht",
                "Jean Maillard"
            ],
            "title": "No language left behind: Scaling human-centered machine translation",
            "year": 2022
        },
        {
            "authors": [
                "Nicola De Cao",
                "Ledell Wu",
                "Kashyap Popat",
                "Mikel Artetxe",
                "Naman Goyal",
                "Mikhail Plekhanov",
                "Luke Zettlemoyer",
                "Nicola Cancedda",
                "Sebastian Riedel",
                "Fabio Petroni."
            ],
            "title": "Multilingual autoregressive entity linking",
            "venue": "Transactions of the Association for Com-",
            "year": 2022
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
            "year": 2019
        },
        {
            "authors": [
                "Qingyu Guo",
                "Fuzhen Zhuang",
                "Chuan Qin",
                "Hengshu Zhu",
                "Xing Xie",
                "Hui Xiong",
                "Qing He."
            ],
            "title": "A survey on knowledge graph-based recommender systems",
            "venue": "IEEE Transactions on Knowledge and Data Engineering, 34(8):3549\u20133568.",
            "year": 2022
        },
        {
            "authors": [
                "Kelvin Guu",
                "Kenton Lee",
                "Zora Tung",
                "Panupong Pasupat",
                "Mingwei Chang."
            ],
            "title": "Retrieval augmented language model pre-training",
            "venue": "Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning",
            "year": 2020
        },
        {
            "authors": [
                "S\u00f8gaard."
            ],
            "title": "Challenges and strategies in crosscultural NLP",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 6997\u20137013, Dublin, Ireland. Association for Computational Lin-",
            "year": 2022
        },
        {
            "authors": [
                "Johannes Hoffart",
                "Fabian M. Suchanek",
                "Klaus Berberich",
                "Edwin Lewis-Kelham",
                "Gerard de Melo",
                "Gerhard Weikum."
            ],
            "title": "YAGO2: exploring and querying world knowledge in time, space, context, and many languages",
            "venue": "Proceedings of the 20th In-",
            "year": 2011
        },
        {
            "authors": [
                "Lukas Schmelzeisen",
                "Juan Sequeda",
                "Steffen Staab",
                "Antoine Zimmermann."
            ],
            "title": "Knowledge graphs",
            "venue": "ACM Comput. Surv., 54(4).",
            "year": 2021
        },
        {
            "authors": [
                "Luyang Huang",
                "Lingfei Wu",
                "Lu Wang."
            ],
            "title": "Knowledge graph-augmented abstractive summarization with semantic-driven cloze reward",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online,",
            "year": 2020
        },
        {
            "authors": [
                "Xiao Huang",
                "Jingyuan Zhang",
                "Dingcheng Li",
                "Ping Li."
            ],
            "title": "Knowledge graph embedding based question answering",
            "venue": "Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining, WSDM \u201919, page 105\u2013113, New York, NY,",
            "year": 2019
        },
        {
            "authors": [
                "Zijie Huang",
                "Zheng Li",
                "Haoming Jiang",
                "Tianyu Cao",
                "Hanqing Lu",
                "Bing Yin",
                "Karthik Subbian",
                "Yizhou Sun",
                "Wei Wang."
            ],
            "title": "Multilingual knowledge graph completion with self-supervised adaptive graph alignment",
            "venue": "Proceedings of the 60th Annual Meeting of",
            "year": 2022
        },
        {
            "authors": [
                "Shaoxiong Ji",
                "Shirui Pan",
                "Erik Cambria",
                "Pekka Marttinen",
                "Philip S. Yu."
            ],
            "title": "A survey on knowledge graphs: Representation, acquisition, and applications",
            "venue": "IEEE Transactions on Neural Networks and Learning Systems, 33(2):494\u2013514.",
            "year": 2022
        },
        {
            "authors": [
                "Xin Ji",
                "Wen Zhao."
            ],
            "title": "SKGSUM: Abstractive document summarization with semantic knowledge graphs",
            "venue": "2021 International Joint Conference on Neural Networks (IJCNN), pages 1\u20138.",
            "year": 2021
        },
        {
            "authors": [
                "Ziwei Ji",
                "Nayeon Lee",
                "Rita Frieske",
                "Tiezheng Yu",
                "Dan Su",
                "Yan Xu",
                "Etsuko Ishii",
                "Ye Jin Bang",
                "Andrea Madotto",
                "Pascale Fung."
            ],
            "title": "Survey of hallucination in natural language generation",
            "venue": "ACM Comput. Surv., 55(12).",
            "year": 2023
        },
        {
            "authors": [
                "Mike Lewis",
                "Yinhan Liu",
                "Naman Goyal",
                "Marjan Ghazvininejad",
                "Abdelrahman Mohamed",
                "Omer Levy",
                "Veselin Stoyanov",
                "Luke Zettlemoyer"
            ],
            "title": "BART: Denoising sequence-to-sequence pre-training for natural language",
            "year": 2020
        },
        {
            "authors": [
                "Shuyang Li",
                "Mukund Sridhar",
                "Chandana Satya Prakash",
                "Jin Cao",
                "Wael Hamza",
                "Julian McAuley."
            ],
            "title": "Instilling type knowledge in language models via multi-task QA",
            "venue": "Findings of the Association for Computational Linguistics: NAACL 2022, pages 594\u2013",
            "year": 2022
        },
        {
            "authors": [
                "Zhuliu Li",
                "Yiming Wang",
                "Xiao Yan",
                "Weizhi Meng",
                "Yanen Li",
                "Jaewon Yang."
            ],
            "title": "Taxotrans: Taxonomy-guided entity translation",
            "venue": "Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD \u201922, page",
            "year": 2022
        },
        {
            "authors": [
                "Yankai Lin",
                "Zhiyuan Liu",
                "Maosong Sun",
                "Yang Liu",
                "Xuan Zhu."
            ],
            "title": "Learning entity and relation embeddings for knowledge graph completion",
            "venue": "Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence, January 25-30, 2015, Austin,",
            "year": 2015
        },
        {
            "authors": [
                "Linlin Liu",
                "Xin Li",
                "Ruidan He",
                "Lidong Bing",
                "Shafiq Joty",
                "Luo Si."
            ],
            "title": "Enhancing multilingual language model with massive multilingual knowledge triples",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2022
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "RoBERTa: A robustly optimized BERT pretraining approach",
            "venue": "arXiv preprint arXiv:1907.11692.",
            "year": 2019
        },
        {
            "authors": [
                "Shayne Longpre",
                "Yi Lu",
                "Joachim Daiber."
            ],
            "title": "MKQA: A linguistically diverse benchmark for multilingual open domain question answering",
            "venue": "Transactions of the Association for Computational Linguistics, 9:1389\u20131406.",
            "year": 2021
        },
        {
            "authors": [
                "Kenneth Marino",
                "Ruslan Salakhutdinov",
                "Abhinav Gupta."
            ],
            "title": "The more you know: Using knowledge graphs for image classification",
            "venue": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 20\u201328.",
            "year": 2017
        },
        {
            "authors": [
                "Marco Maru",
                "Simone Conia",
                "Michele Bevilacqua",
                "Roberto Navigli."
            ],
            "title": "Nibbling at the hard core of Word Sense Disambiguation",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages",
            "year": 2022
        },
        {
            "authors": [
                "Zaid Alyafeai",
                "Albert Webson",
                "Edward Raff",
                "Colin Raffel"
            ],
            "title": "Crosslingual generalization through multitask finetuning",
            "year": 2022
        },
        {
            "authors": [
                "Roberto Navigli",
                "Michele Bevilacqua",
                "Simone Conia",
                "Dario Montagnini",
                "Francesco Cecconi."
            ],
            "title": "Ten years of babelnet: A survey",
            "venue": "Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI 2021, Virtual Event /",
            "year": 2021
        },
        {
            "authors": [
                "Roberto Navigli",
                "Simone Conia",
                "Bj\u00f6rn Ross."
            ],
            "title": "Biases in large language models: Origins, inventory and discussion",
            "venue": "J. Data and Information Quality.",
            "year": 2023
        },
        {
            "authors": [
                "Maximilian Nickel",
                "Kevin Murphy",
                "Volker Tresp",
                "Evgeniy Gabrilovich."
            ],
            "title": "A review of relational machine learning for knowledge graphs",
            "venue": "Proceedings of the IEEE, 104(1):11\u201333.",
            "year": 2016
        },
        {
            "authors": [
                "Laurel J. Orr",
                "Megan Leszczynski",
                "Neel Guha",
                "Sen Wu",
                "Simran Arora",
                "Xiao Ling",
                "Christopher R\u00e9."
            ],
            "title": "Bootleg: Chasing the tail with self-supervised named entity disambiguation",
            "venue": "11th Conference on Innovative Data Systems Research, CIDR 2021, Vir-",
            "year": 2021
        },
        {
            "authors": [
                "Baolin Peng",
                "Michel Galley",
                "Pengcheng He",
                "Hao Cheng",
                "Yujia Xie",
                "Yu Hu",
                "Qiuyuan Huang",
                "Lars Liden",
                "Zhou Yu",
                "Weizhu Chen",
                "Jianfeng Gao"
            ],
            "title": "2023a. Check your facts and try again: Improving large language models with external knowledge and automated feed",
            "year": 2023
        },
        {
            "authors": [
                "Ciyuan Peng",
                "Feng Xia",
                "Mehdi Naseriparsa",
                "Francesco Osborne."
            ],
            "title": "Knowledge graphs: Opportunities and challenges",
            "venue": "Artificial Intelligence Review.",
            "year": 2023
        },
        {
            "authors": [
                "Fabio Petroni",
                "Tim Rockt\u00e4schel",
                "Sebastian Riedel",
                "Patrick Lewis",
                "Anton Bakhtin",
                "Yuxiang Wu",
                "Alexander Miller"
            ],
            "title": "Language models as knowledge bases",
            "venue": "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Pro-",
            "year": 2019
        },
        {
            "authors": [
                "Luigi Procopio",
                "Simone Conia",
                "Edoardo Barba",
                "Roberto Navigli."
            ],
            "title": "Entity disambiguation with entity definitions",
            "venue": "Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pages 1297\u20131303,",
            "year": 2023
        },
        {
            "authors": [
                "Alec Radford",
                "Karthik Narasimhan",
                "Tim Salimans",
                "Ilya Sutskever"
            ],
            "title": "Improving language understanding by generative pre-training",
            "year": 2019
        },
        {
            "authors": [
                "Jonathan Raiman",
                "Olivier Raiman."
            ],
            "title": "DeepType: Multilingual entity linking by neural type system evolution",
            "venue": "Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence",
            "year": 2018
        },
        {
            "authors": [
                "Ridho Reinanda",
                "Edgar Meij",
                "Maarte de Rijke."
            ],
            "title": "Knowledge graphs: An information retrieval perspective",
            "venue": "Foundations and Trends\u00ae in Information Retrieval, 14(4):289\u2013444.",
            "year": 2020
        },
        {
            "authors": [
                "manamanchi",
                "Thomas Wang",
                "Beno\u00eet Sagot",
                "Niklas Muennighoff",
                "Albert Villanova del Moral",
                "Thomas Wolf"
            ],
            "title": "BLOOM: A 176B-parameter open-access multilingual language model",
            "year": 2023
        },
        {
            "authors": [
                "Phillip Schneider",
                "Tim Schopf",
                "Juraj Vladika",
                "Mikhail Galkin",
                "Elena Simperl",
                "Florian Matthes."
            ],
            "title": "A decade of knowledge graphs in natural language processing: A survey",
            "venue": "Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Asso-",
            "year": 2022
        },
        {
            "authors": [
                "Baoxu Shi",
                "Tim Weninger."
            ],
            "title": "Open-world knowledge graph completion",
            "venue": "Proceedings of the ThirtySecond AAAI Conference on Artificial Intelligence,",
            "year": 2018
        },
        {
            "authors": [
                "Ralf Steinberger",
                "Bruno Pouliquen",
                "Mijail Kabadjov",
                "Jenya Belyaeva",
                "Erik van der Goot."
            ],
            "title": "JRCNAMES: A freely available, highly multilingual named entity resource",
            "venue": "Proceedings of the International Conference Recent Advances in Natural",
            "year": 2011
        },
        {
            "authors": [
                "Denny Vrande\u010di\u0107",
                "Markus Kr\u00f6tzsch."
            ],
            "title": "Wikidata: A free collaborative knowledgebase",
            "venue": "Commun. ACM, 57(10):78\u201385.",
            "year": 2014
        },
        {
            "authors": [
                "Quan Wang",
                "Zhendong Mao",
                "Bin Wang",
                "Li Guo."
            ],
            "title": "Knowledge graph embedding: A survey of approaches and applications",
            "venue": "IEEE Transactions on Knowledge and Data Engineering, 29(12):2724\u2013 2743.",
            "year": 2017
        },
        {
            "authors": [
                "Wenhan Xiong",
                "Jingfei Du",
                "William Yang Wang",
                "Veselin Stoyanov."
            ],
            "title": "Pretrained encyclopedia: Weakly supervised knowledge-pretrained language model",
            "venue": "8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia,",
            "year": 2020
        },
        {
            "authors": [
                "Yan Xu",
                "Mahdi Namazifar",
                "Devamanyu Hazarika",
                "Aishwarya Padmakumar",
                "Yang Liu",
                "Dilek HakkaniT\u00fcr."
            ],
            "title": "KILM: Knowledge injection into encoder-decoder language models",
            "venue": "arXiv preprint arXiv:2302.09170.",
            "year": 2023
        },
        {
            "authors": [
                "Linting Xue",
                "Noah Constant",
                "Adam Roberts",
                "Mihir Kale",
                "Rami Al-Rfou",
                "Aditya Siddhant",
                "Aditya Barua",
                "Colin Raffel."
            ],
            "title": "mt5: A massively multilingual pre-trained text-to-text transformer",
            "venue": "Proceedings of the 2021 Conference of the North American Chap-",
            "year": 2021
        },
        {
            "authors": [
                "Daochen Zha",
                "Zaid Pervaiz Bhat",
                "Kwei-Herng Lai",
                "Fan Yang",
                "Zhimeng Jiang",
                "Shaochen Zhong",
                "Xia Hu"
            ],
            "title": "Data-centric artificial intelligence: A survey",
            "year": 2023
        },
        {
            "authors": [
                "Yuyu Zhang",
                "Hanjun Dai",
                "Zornitsa Kozareva",
                "Alexander J. Smola",
                "Le Song."
            ],
            "title": "Variational reasoning for question answering with knowledge graph",
            "venue": "Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th inno-",
            "year": 2018
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "The objective of a knowledge graph is to encode our collective understanding of the world in a welldefined, structured, machine-readable representation (Hogan et al., 2021). At a high level, each node of a knowledge graph usually represents a concept (e.g., universe, weather, or president) or an entity (e.g., Albert Einstein, Rome, or The Legend of Zelda), and each edge between two nodes\n\u2217Work done as intern at Apple.\nis a semantic relationship that represents a fact (e.g., \u201cRome is the capital of Italy\u201d or \u201cThe Legend of Zelda is a video game series\u201d). With the wealth of information that knowledge graphs provide, they play a fundamental role in a multitude of real-world scenarios, touching many areas of Artificial Intelligence (Nickel et al., 2016), including Natural Language Processing (Schneider et al., 2022), Computer Vision (Marino et al., 2017), Information Retrieval (Reinanda et al., 2020), and recommender systems (Guo et al., 2022).\nOver the years, knowledge graphs have mainly been adopted as a rich source of human-curated relational information to enhance neural-based models for tasks of varying nature (Huang et al., 2019; Bevilacqua and Navigli, 2020; Orr et al., 2021). However, ever since natural language text has proven to be an effective interface between structured knowledge and language models (Guu et al., 2020; Petroni et al., 2019; Peng et al., 2023a), the value of knowledge graphs has become twofold: besides providing relational information, knowledge graphs have also become a reliable source of high-quality textual information. Indeed, recent approaches have been increasingly reliant on textual information from knowledge graphs to surpass the state of the art (Barba et al., 2021; Chakrabarti et al., 2022; De Cao et al., 2022; Xu et al., 2023).\nUnfortunately, when it comes to non-English languages, the condition of multilingual textual information in knowledge graphs is far from ideal. Indeed, popular resources present a significant gap between English and non-English textual information, hindering the capability of recent approaches to scale to multilingual settings (Peng et al., 2023b) Importantly, this gap exists in high-resource languages even if we consider basic textual properties, such as entity names and entity descriptions. The nature of the problem is dual: disparity in coverage, as the quantity of textual information available in\nnon-English languages is more limited, and precision, as the quality of non-English textual information is usually lower.\nIn this paper, we address the aforementioned coverage and precision issues of textual information in multilingual knowledge graphs via a data-centric approach. Our contributions include the following:\n\u2022 We introduce the task of automatic Knowledge Graph Enhancement (KGE) to tackle the disparity of textual information between English and non-English languages in multilingual knowledge graphs;\n\u2022 We present WikiKGE-10, a novel humancurated benchmark for evaluating KGE systems for entity names in 10 typologically diverse languages: English, German, Spanish, French, Italian, Simplified Chinese, Japanese, Arabic, Russian, and Korean;\n\u2022 We investigate how well Machine Translation (MT), Web Search (WS), and Large Language Models (LLMs) can narrow the gap between English and non-English languages.\n\u2022 We propose M-NTA, a novel unsupervised approach, which combines MT, WS, and LLMs to mitigate the problems that arise when using each system separately;\n\u2022 We demonstrate the beneficial impact of KGE in downstream tasks, including Entity Linking, Knowledge Graph Completion, and Question Answering.\nWe deem that achieving parity of coverage and precision of textual information across languages in knowledge graphs is fundamental to enable better and more inclusive multilingual applications. In the hope that our contributions can set a stepping stone for future research in this field, we release WikiKGE-10 at https://github.com/apple/ml-kge."
        },
        {
            "heading": "2 Related Work",
            "text": "In this section, we provide a brief overview of knowledge graphs, highlighting how textual information from knowledge graphs is now as important as their relational information, showcasing how recent work has successfully integrated textual information into downstream applications, and reviewing how recent efforts have mainly focused on completing relational information in knowledge graphs rather than textual information.\nKnowledge graphs. Even though their exact definition remains contentious, knowledge graphs are usually defined as \u201ca graph of data intended to accumulate and convey knowledge of the real world, whose nodes represent entities of interest and whose edges represent potentially different relations between these entities\u201d (Hogan et al., 2021). Over the years, research endeavors in knowledge graphs have steadily focused their efforts primarily on using their relational information, i.e., the semantic relations between entities. Besides foundational work on knowledge graph embedding techniques, which represent the semantics of an entity by encoding its graph neighborhood (Wang et al., 2017), relational knowledge has been successfully employed in Question Answering to encode properties that generalize over unseen entities (Bao et al., 2016; Zhang et al., 2018; Huang et al., 2019), in Text Summarization to identify the most relevant entities in a text and their relations (Huang et al., 2020; Ji and Zhao, 2021), in Entity Linking to condition the prediction of an instance on knowledge subgraphs (Raiman and Raiman, 2018; Orr et al., 2021), and in Word Sense Disambiguation to produce rich meaning representations that can differentiate closely related senses (Bevilacqua and Navigli, 2020; Conia and Navigli, 2021).\nTextual information in knowledge graphs. While knowledge graphs have been used for the versatility of their relational information, the rapid emergence of modern language models has also represented a turning point in how the research community looks at knowledge graphs. As a matter of fact, the initial wave of Transformer-based language models (Devlin et al., 2019; Radford et al., 2019) were trained purely on text, and, when researchers realized that quantity and quality of training data are two essential factors to enable better generalization capabilities (Liu et al., 2019), it became clear that the textual data available in knowledge graphs could be exploited as a direct interface between human-curated structured information and language models.\nIndeed, prominent knowledge graphs \u2013 Wikidata (Vrandec\u030cic\u0301 and Kr\u00f6tzsch, 2014), DBPedia (Lehmann et al., 2015), YAGO (Hoffart et al., 2011), and BabelNet (Navigli et al., 2021), among others \u2013 feature lexicalizations for each entity in multiple languages, e.g., names, aliases and descriptions of various length. Therefore, textual information in knowledge graph is now as important\nas relational information, with recent developments taking advantage of the former to surpass the previous state of the art in an increasingly wide array of tasks, such as Word Sense Disambiguation (Barba et al., 2021), Entity Linking (Xu et al., 2023; Procopio et al., 2023), Relation Alignment (Chakrabarti et al., 2022), and Language Modeling itself (Xiong et al., 2020; Agarwal et al., 2021; Li et al., 2022a; Liu et al., 2022). Unfortunately, the wide adoption of such techniques in multilingual settings has been strongly limited by the disparity in coverage and quality of entity names and descriptions in multilingual knowledge graphs between English and non-English languages (Peng et al., 2023b).\nKnowledge graph acquisition and completion. Finally, we would like to stress that our endeavor is orthogonal to the efforts that usually fall under the umbrella terms of \u201cknowledge acquisition\u201d (Ji et al., 2022) and \u201cknowledge graph completion\u201d in the literature (Lin et al., 2015; Shi and Weninger, 2018; Chen et al., 2020b). More specifically, the objective of these two tasks is to construct the \u201cstructure\u201d of a knowledge graph, i.e., identifying the set of entities of interest and the (missing) relations between entities. Therefore, the multilingual extensions of these two tasks are concerned about detecting missing nodes or edges in a multilingual knowledge graph (Chen et al., 2020a; Huang et al., 2022; Chakrabarti et al., 2022), whereas we specifically focus on expanding the coverage and precision of textual information in multilingual knowledge graphs. Nonetheless, we argue that increasing coverage and quality of textual information in multilingual knowledge graphs has beneficial cascading effects on tasks like knowledge graph completion, as our experiments show in Section 6."
        },
        {
            "heading": "3 Knowledge Graph Enhancement of",
            "text": "Textual Information\nWhile relational information in knowledge graphs is usually language-agnostic (e.g., \u201cAI\u201d is a field of \u201cComputer Science\u201d independently of the language we consider), textual information is usually language-dependent (e.g., the lexicalizations of \u201cAI\u201d and \u201cComputer Science\u201d vary across languages). With the growing number of languages supported by knowledge graphs, it is increasingly challenging for human editors to maintain their content up-to-date in all languages: therefore, we believe it is important to invest in the development and evaluation of systems that can support humans\nin updating textual information across languages."
        },
        {
            "heading": "3.1 Task definition",
            "text": "Given an entity e in a knowledge graph G, we define Knowledge Graph Enhancement (KGE) as the task of automatically producing textual information about e for each language l \u2208 L, where L is the set of languages of interest. More precisely, KGE encompasses two subtasks:\n\u2022 Increasing coverage of textual information, which consists in providing textual information that is currently unavailable for e in G;\n\u2022 Increasing precision of textual information, which consists in identifying inaccurate or under-specified facts in the textual information already available for e in G.\nTherefore, KGE evaluates the capability of a system to provide new textual information (coverage) as well as its capability to detect errors and inaccuracies in existing textual information (precision). While textual information may refer to any entity property expressed in natural language, in the reminder of this paper, we focus on entity names and entity descriptions in Wikidata, which have become increasingly used in knowledge-infused language models and state-of-the-art systems (see Section 2)."
        },
        {
            "heading": "3.2 Coverage of non-English information",
            "text": "Ideally, we would like every entity e in Wikidata to be \u201ccovered\u201d in all languages, i.e., we would like Wikidata to provide a name and a description of e for each l in the set L of the languages supported by the knowledge graph. In practice, this is not the case in Wikidata, as we can observe in Figure 1, which provides a bird\u2019s-eye view on the availability of entity names and entity descriptions in 9 non-English languages. More precisely, we analyzed the Wikidata entities that have an associated Wikipedia page1 with at least 100 page views in any language over the 12 months between May 2022 and April 2023. Our analysis calls attention to the issue of coverage of entity names and entity descriptions in Wikidata, which is significant even if we only consider head entities \u2013 top-10% of the most popular entities sorted by number of Wikipedia page views \u2013 and restrict the set of languages to German, Spanish, and French, which are\n1The Wikidata-to-Wikipedia mapping is n-to-1 since a Wikidata entity may refer to the entire Wikipedia article or a section of an article.\nusually regarded as \u201chigh-resource\u201d languages. Unsurprisingly, we can observe that the gap in coverage increases when we consider entities belonging to the torso (top-50%) and tail of the popularity distribution, as the coverage of Japanese and Chinese names for tail entities is lower than 15%.\nWe argue that the fact that Wikidata inherits this disparity from Wikipedia, which is edited by a disproportionate number of English-speaking contributors,2 should not detract our attention from this issue. As a matter of fact, a growing number of approaches relies on textual information from Wikidata; therefore, we believe that the stark contrast between today\u2019s great interest for textual information in knowledge graphs and the scarce multilingual coverage revealed by our analysis motivates the development of \u201cdata-centric AI\u201d approaches (Zha et al., 2023) for increasing multilingual coverage, rather than focusing our efforts exclusively on model-centric novelties."
        },
        {
            "heading": "3.3 Precision of non-English information",
            "text": "While non-English coverage of entity names and entity descriptions is critical, another crucial aspect is the level of precision in Wikidata. Indeed, the majority of the approaches that rely on names and descriptions often use such information as-is and overlook the possibility that it may be inaccurate. More specifically, we categorize the causes of\n2The primary language of Wikipedia editors is English (52%), followed by German (18%), Russian and Spanish (both at 10%) [source: UNU-Merit].\ninaccurate information into three main classes:\n\u2022 Human mistakes, when the imprecision was caused by a human editor. For example, entity Q1911 is incorrectly named Oliver Giroud in Spanish instead of Olivier Giroud.\n\u2022 Stale entries, when new information is available but Wikidata has not been updated. For example, the English description of entity Q927916 has been recently updated to include the date of death but the Russian description still indicates the date of birth only.\n\u2022 Under-specific information, when the available information is not incorrect but it is still too generic. For example, the Spanish description for Q345494 is \u201cm\u00fasico japon\u00e9s\u201d (Japanese musician), whereas the German one is \u201cjapanischer Komponist, Pianist, Produzent und Schauspieler (1952\u20132023),\u201d which details his work (composer, pianist, producer, and actor) and includes his birth and death dates.\nAlthough it is not uncommon to encounter instances of these three classes of error in Wikidata, conducting a comprehensive analysis of its entire knowledge graph is unfeasible."
        },
        {
            "heading": "3.4 Evaluating KGE with WikiKGE-10",
            "text": "To address the above-mentioned issues, we present WikiKGE-10, a novel resource for benchmarking data-centric-AI approaches on KGE of entity\nnames in 10 languages: English, German, Spanish, French, Italian, Chinese, Japanese, Korean, Arabic, and Russian. At a high level, WikiKGE10 is designed to feature typologically-different linguistic families, from West Germanic to Romance, Semitic, Slavic, Koreanic, Japonic, and Sino-Tibetan, and, therefore, to enable comparison of entity names across a set L of 10 diverse languages with heterogeneous, possibly nonoverlapping vocabularies and scripts.\nGiven a language l \u2208 L, we uniformly sampled 1000 entities from the top-10% of the entities in Wikidata sorted by the number of page views for their corresponding Wikipedia article in l. We note that the composition of the top-10% entities \u2013 and, therefore, our sample of 1000 entities \u2013 may significantly vary from language to language, as the popularity distribution changes according to what different cultures care about (Hershcovich et al., 2022). After selecting 1000 entities for each language, human graders manually checked their existing names to assess their correctness, while also adding new valid names. The annotation process, which took more than 2,500 human hours, resulted in around 36,000 manually-curated names across 10 languages; we provide more details on the creation of WikiKGE-10 and our guidelines in Appendix A. Importantly, as shown in Table 1, we find that human graders deemed 20% of the entity names in Wikidata to be incorrect and that 40% of the valid entity names could not be found in Wikipedia. In practice, since WikiKGE-10 features manually-curated entity names and indicates which names in Wikidata are incorrect or inaccurate, our benchmark can be used to evaluate the capability of a system to tackle both subtasks in KGE, i.e., increasing coverage and precision of entity names."
        },
        {
            "heading": "4 Methodology",
            "text": "In this section, we consider three broad families of approaches \u2013 MT, WS, and LLMs \u2013 and demonstrate their unsatisfactory performance on narrow-\ning the coverage and precision gap between English and non-English languages. Therefore, we also introduce M-NTA (Multi-source Naturalization, Translation, and Alignment), a simple unsupervised ensembling technique, which overcomes the limitations of MT, WS, and LLMs by combining and ranking their predictions. Here, we direct our attention toward entity names, but we also show that the methodologies discussed in this section can be extended to other types of textual information, such as entity descriptions, in Appendix C."
        },
        {
            "heading": "4.1 Baseline approaches",
            "text": "Machine Translation (MT). When in need of converting information from one language to another, employing MT is a typical choice. Indeed, given a source language ls and a target language lt, a straightforward approach would be to use an MT system to translate the textual information available in ls to lt to increase coverage in lt. However, such an approach is limited in several respects: i) it assumes that all textual information is available in ls, which, in practice, is not the case even when ls is English, i.e., MT cannot be applied if the information to translate is not available in the source language in the first place; ii) it assumes that MT systems are precise, which, again, is not the case: for example, entity names can be complex and ambiguous to translate without additional context (e.g., \u201cApple\u201d could refer to the fruit or the tech company); and, iii) while MT can be employed to increase coverage, it is not clear how to apply MT to identify inaccurate entity names to increase precision of existing textual information.\nWeb Search (WS). A common workflow for looking up textual information in a target language lt is to query Web search engines with queries in a source language ls, such as \u201c[entity-name] in [lt]\u201d, and extract the answer from the search results, possibly limiting the search space to Web pages entirely in lt or originating from countries in which lt is the primary/official language. While\nWS can provide more varied results that are not 1-to-1 translations of the source entity name, we argue that WS suffers from the same fundamental limitations as MT: i) if ls is not complete, then we cannot formulate every search query; ii) WS is prone to biases, especially for ambiguous instances (e.g., googling \u201cplane\u201d shows many results about airplanes, a few results about geometric planes, and none about plane trees); and, iii) using WS to identify and correct imprecise textual information in a knowledge graph is not obvious.\nLarge Language Models (LLMs). Recent LLMs have been shown to be few-shot learners, thanks to what is now known as in-context learning, or the capability of capturing latent relationships between a few input examples to provide an answer for a new task (Brown et al., 2020). With the advent of multilingual LLMs, such as BLOOM (Scao et al., 2023), mT5 (Xue et al., 2021), and their instructionfine-tuned variants (Muennighoff et al., 2022), we can prompt such models for translation, e.g., \u201cHow do you say [entity-name] in [lt]?\u201d, possibly providing a few examples in input to condition the generation of the output. While prompting language models is versatile, relying on LLMs also exposes us to their weaknesses, e.g., hallucinations (Ji et al., 2023) and data biases (Navigli et al., 2023)."
        },
        {
            "heading": "4.2 M-NTA: Multi-source Naturalization, Translation, and Alignment",
            "text": "To address the issues above, we introduce M-NTA, a simple unsupervised technique that combines MT, WS, and LLMs. The intuition behind M-NTA is that obtaining a fact from multiple source systems may offer complementary pieces of information which provide varying views on our world knowledge; we hypothesize that, if distinct views support the same fact, there is a greater chance for the fact to be closer to the ground truth.\nSource systems in M-NTA. The first question, therefore, is how to produce the above-mentioned views on our world knowledge. Given a source language ls and an entity e whose name in ls is ens , M-NTA takes a three-steps approach to generate ent in a target language lt:\n1. Naturalization: as mentioned above, entity names are not suitable for direct translation since they might not provide sufficient context (Li et al., 2022b). To overcome this issue, M-NTA retrieves the textual description eds of\ne in ls from Wikidata and uses it to produce a natural language representation rs(ens , e d s) of e in ls. This allows M-NTA to rely on different representations for polysemous words, e.g., \u201cApple is an American technology company\u201d and \u201cApple is a fruit of the apple tree.\u201d\n2. Translation: next, M-NTA \u201ctranslates\u201d the representation rs(ens , e d s) from ls to lt using\na system f(\u00b7) to obtain a natural language output rt(ent , e d t ) in the target language.\n3. Alignment: finally, M-NTA aligns the output rt(e n t , e d t ) with the input rs(e n s , e d s) to extract\nthe entity name ent .\nMost crucially, M-NTA is transparent to the definition of a source system f(\u00b7). This allows M-NTA to take advantage of any source system f(\u00b7) that is able to produce ent . More specifically, M-NTA can use a set of source systems F = {f1, f2, . . . , fn} in which fi can be an MT, WS or LLM-based system. Not only that, we can leverage the same MT system multiple times by setting the source language ls to different languages, allowing M-NTA to draw knowledge from all the languages of interest to produce better results in lt.\nRanking answers in M-NTA. The second question is how to validate each view by using the other views. In practice, we first consider each view as an answer y = f(\u00b7) provided by a source system f(\u00b7) in the set of source systems F . Then, we assign an agreement score \u03c3(y) to each answer:\n\u03c3(y) = \u2211 \u03d5(y, y\u2032) \u2200y\u2032 = f \u2032(\u00b7), f \u2032 \u2208 F \\ {f}\nwhere \u03d5(y, y\u2032) \u2192 {0, 1} is a function that indicates if y is supported by y\u2032, e.g., in the case of entity names \u03d5(\u00b7, \u00b7) can be implemented as exact string match. In other words, the agreement score \u03c3(y) is higher when an answer y from a source system f is supported by an answer y\u2032 from another source system f \u2032; if y is valid according to multiple source systems, then there is a lower chance for y to be incorrect. On the contrary, if y is not supported by other answers, its agreement score is lower and, therefore, there is a higher chance for y to be incorrect. Finally, we obtain the final set of answers Y by selecting all the answers y whose score \u03c3(y) is greater than or equal to a threshold \u03bb:\nY = {y : \u03c3(y) \u2265 \u03bb}\nwhere \u03bb is a hyperparameter that can be tuned to balance precision and recall of the system, with our experiments indicating that \u03bb = 2 is the most balanced choice for coverage, as discussed in Appendix C.\nDifferently from MT, WS, and LLMs, since each answer in Y is scored and ranked by M-NTA, the application of M-NTA to KGE is straightforward. To increase coverage, we can consider Y as the result, as \u03bb > 1 allows M-NTA to remove unlikely answers; to increase precision, we can consider every value y\u0302 in the KG that is not in Y as an incorrect value."
        },
        {
            "heading": "5 Experiments on KGE",
            "text": "In this section, we evaluate our strong baselines and M-NTA on the task of KGE for entity names and discuss the results obtained on WikiKGE-10."
        },
        {
            "heading": "5.1 Experimental setup",
            "text": "Recently, there has been a surge of interest for multilingual MT systems, i.e., systems that use a unified model for multiple language pairs. Therefore, for the implementation of the MT baseline, we use NLLB-200 (Costa-juss\u00e0 et al., 2022), a state-of-theart multilingual MT system that supports over 200 languages. For WS, we use Google Web Search, as it is often regarded as one of the best WS engines. For LLM prompting, we consider two popular models: i) mT0 (Muennighoff et al., 2022), an openly available instruction-finetuned multilingual LLM based on mT5, and ii) GPT,3 one of the most pop-\n3Experiments with GPT-3 and GPT-3.5 were carried out between March and May 2023. Additional experiments with\nular albeit closed LLMs, which has been proven to show strong multilingual capabilities. Finally, we evaluate M-NTA when scoring and ensembling the outputs from NLLB-200,4 Google Web Search, and GPT-3/3.5/4.\nFor each baseline, the input data is the set of entity names that currently exist in Wikidata in a source language ls, i.e., the entity names in ls are \u201ctranslated\u201d into the target language lt using MT, WS, LLM prompting or M-NTA. We note that, if Wikidata does not include at least one name for an entity e in ls, then none of the systems mentioned above is able to produce a name in lt. M-NTA is able to mitigate this issue by drawing information from multiple source languages at the same time.\nGiven a set of human-curated correct names Y\u0304 from WikiKGE-10 and a set of predicted names Y generated by a system, we compute coverage between Y\u0304 and Y as following:\nPPVC = \u2211 y\u2208Y 1Y\u0304 (y) |Y |\nTPRC = \u2211 y\u0304\u2208Y\u0304 1Y (y\u0304) |Y\u0304 |\nCoverage = 2 PPVC \u00b7TPRC PPVC +TPRC\nwhere PPVC is the positive predictive value, TPRC is the true positive rate, and 1X(x) is the indicator function, which returns 1 if x \u2208 X else 0. We compute precision in a similar way, using the\nGPT-4 were carried out in September 2023. 4For each target language lt, we M-NTA uses the translations from every source language ls \u0338= lt.\nset of human-curated invalid names \u00acY\u0304 and the set of names \u00acY predicted to be incorrect by a system. Note that, to enable a direct and fair comparison, we allow every system to rely on additional contextual information in the form of entity descriptions from Wikidata; we provide more details about the experimental setting in Appendix C."
        },
        {
            "heading": "5.2 Results and discussion",
            "text": "The results on WikiKGE-10 reported in Table 2 highlight two key findings: i) our proposed solution, M-NTA, offers superior performance compared to state-of-the-art techniques in MT, WS, and LLMs on both coverage and precision of entity names; and, ii) the results on WikiKGE-10 indicate that KGE is a very challenging task and that more extensive investigations are needed to design better KGE systems. In the following, we report the main takeaways from our experiments.\nDifferent languages hold different knowledge. Our experimental results show that generating entity names in non-English languages by translating English-only textual information does not provide the best results, as shown in Table 2. This is true not only for the MT system we use in our experiments but also for WS and LLMs, for which we use English-only queries and prompts, respectively. In particular, it is interesting to notice that completely different systems, namely, MT and GPT3.5, produce similar results on average: 41.0% vs. 39.9% in coverage and 58.0% vs. 57.0% in precision. Therefore, we hypothesize that the significant gain in performance by M-NTA \u2013 +12% in coverage and +22% in precision over GPT-3.5 \u2013 is mainly attributable to its effectiveness in combining information across different languages. Indeed, it is interesting to notice that using GPT-4 instead of GPT-3.5 as one of the sources of M-NTA only provides marginal improvements to the overall results in both coverage and precision.\nWS may not be suitable for KGE. The results from our experiments show that WS is the least effective approach to generate entity names. Although we are not disclosed on the inner workings of proprietary search engines, we can qualitatively observe that the results returned from Web searches often include answers for entities that are semantically similar to the one mentioned in the input query. For example, searching Niki Lauda (former F1 driver) in Italian also returns results about Rush (biographical film on Lauda). Relying on semantic\nsimilarity is often a robust strategy for information retrieval, but, in this case, it introduces significant noise, which is undesirable in a knowledge graph.\nPrompting LLMs requires caution. Our experiments also indicate that prompting LLMs is a better option than WS in terms of performance, especially when using GPT. However, we shall keep in mind not to take benchmark results at face value (Maru et al., 2022): analyzing the answers shows one issue that does not surface in our numerical results is that some errors in the predictions provided by LLMs can be significantly worse \u2013 and, therefore, potentially more problematic \u2013 than those made by MT and WS systems. We observe that, especially for uncommon entities and smaller models, LLMs may produce answers that are completely unrelated to the correct answer, including copying part of the prompt or its examples, providing entity names for entirely different entities (e.g., Silvio Berlusconi (Italian politician) for San Cesario sul Panaro (Italian comune)), hallucinating facts (e.g., adding that The Mandalorian (2nd season) is from Star Wars: La venganza de los Sith in Spanish), and also generating nonsense outputs. It follows that, although LLMs are generally better than WS, the risk of using them is higher in case of error, as purely numerical metrics, such as coverage and precision, may hide that some errors are worse than others, i.e., potentially more harmful in downstream applications."
        },
        {
            "heading": "6 Enhancing Textual Information in KGs: Impact on Downstream Tasks",
            "text": "In this section, we demonstrate the beneficial impact of KGE on downstream tasks and its effectiveness in improving the performance of state-ofthe-art techniques in multilingual Entity Linking and Knowledge Graph Completion; we also show that KGE is beneficial for multilingual Question Answering in Appendix E.\nMultilingual Entity Linking (MEL). A direct application of increasing the quantity and quality of textual information in a knowledge graph is MEL, the task of linking a textual mention to an entity in a multilingual knowledge base (Botha et al., 2020). We evaluate the impact of our work on mGENRE (De Cao et al., 2022), a state-of-theart MEL system that fine-tunes mBART (Lewis et al., 2020) to autoregressively generate a Wikidata entity name for a mention in context. As noted\nby De Cao et al. (2022), mGENRE generates entity names by also copying relevant portions of the input mention; however, copying is not possible when the mention of the entity is in a language for which Wikidata does not feature any names. By increasing the coverage and precision of textual information in Wikidata, M-NTA provides mGENRE with a broader coverage of entity names in nonEnglish languages, aiding mGENRE\u2019s capability to rely on copying mechanisms. Indeed, as we can see in Table 3, augmenting mGENRE with M-NTA brings an improvement of 1.2 points in F1 score on average in Wikinews-7, setting a new state-of-theart on this benchmark."
        },
        {
            "heading": "Multilingual Knowledge Graph Completion",
            "text": "(MKGC). Another direct application of KGE is MKGC, the task of predicting missing links between two entities in a multilingual knowledge base (Chen et al., 2020a). Similarly to MEL, we evaluate the downstream impact of our work on a re-implementation of Align-KGC (SoftAsym), a state-of-the-art MKGC system originally proposed by Chakrabarti et al. (2022), which we rebuilt to use our entity names and descriptions to create mBERT-based entity embeddings. As shown in Table 4, using M-NTA to provide more and better entity names and descriptions allows the MKGC system to obtain a consistent improvement across non-English languages on DBP-5L (Chen et al., 2020a), i.e., +1.5 points in terms of Mean Reciprocal Rank (MRR), excluding English. We hypothesize that the larger part of this improvement comes from the fact that the entity descriptions generated by M-NTA are more informative, as suggested by the examples shown in Appendix C.7 (see Table 7). On one hand, this improvement demonstrates the flexibility of M-NTA, as DBP-5L is based on a different knowledge graph, i.e., DBPedia. On the\nother hand, it empirically validates our assumption that increasing coverage and precision of textual information in multilingual knowledge graphs is an effective data-centric way to unlock latent performance in current systems."
        },
        {
            "heading": "7 Conclusion and Future Work",
            "text": "In this paper, we introduced the novel task of automatic Knowledge Graph Enhancement, with the objective of fostering the development and evaluation of data-centric approaches for narrowing the gap in coverage and precision of textual information between English and non-English languages. Thanks to WikiKGE-10, our novel manually-curated benchmark for evaluating KGE of entity names in 10 languages, we brought to light the unsatisfactory capabilities of machine translation, web search, and large language models to bridge this multilingual gap. To this end, we introduced M-NTA, a novel approach to combine the complementary knowledge produced by the above techniques to obtain higher-quality textual information for non-English languages. Not only did M-NTA achieve promising results on WikiKGE-10 but our experiments also demonstrated its beneficial effect across several state-of-the-art systems for downstream applications, namely, multilingual entity linking, multilingual knowledge graph completion, and multilingual question answering.\nWe hope that our novel benchmark and method can represent a milestone for KGE. However, our work demonstrates that, if we aspire to achieve quantity and quality parity across languages, we still need more extensive investigations on how to effectively increase coverage and precision of textual information in multilingual knowledge graphs."
        },
        {
            "heading": "Limitations",
            "text": "Textual information in knowledge graphs. In this paper, we focus on two specific types of textual information, namely, entity names and entity descriptions. Although our discussion on coverage and precision of textual information (or lack thereof) can be extended to other types of textual information, e.g., longer descriptions like Wikipedia abstracts or coreferential information like the anchor text of the hyperlinks in a Wikipedia article, our analysis in Sections 3.2 (\u201cCoverage of nonEnglish information\u201d) and 3.3 (\u201cPrecision of nonEnglish information\u201d) highlights that the gap between English and non-English names and descriptions is very large even for popular entities, ranging from 20% to 60% for entity names and from 30% to 80% for entity descriptions. Furthermore, entity names and entity descriptions are the most widely used types of textual information from knowledge graphs in downstream tasks (see Section 2), and, therefore, we decided to focus our discussion on these two types, which potentially have a more direct impact on downstream applications, as also shown in Section 6. We hypothesize that most of our observations generalize to other types of textual information in knowledge graphs; however, we leave deeper investigations and the creation of benchmarks for other types of textual information in knowledge graphs to future work.\nDifferent knowledge graphs. Our attention is mainly directed at Wikidata, as it is one of the most popular multilingual knowledge graphs used by the research community in Natural Language Processing as well as Information Retrieval and Computer Vision. Therefore, a possible limitation of our work is its generalizability to other knowledge graphs. We hypothesize that our work is generalizable to other knowledge graphs, such as DBPedia, BabelNet, and Open Multilingual WordNet, among others, since entity names (or aliases) and entity descriptions (or definitions) are often available in many of them. Our hunch is partially demonstrated by our empirical experiments on Multilingual Knowledge Graph Completion (see Section 6), as we evaluate the impact of M-NTA on DPB-5L, which is constructed from DBPedia. However, we hope that our work will raise awareness on the issues of multilingual coverage and precision of textual information on as many knowledge graphs as possible, and inspire future work to investigate the\nextent of the problem not only on general knowledge graphs but also on domain-specific ones.\nWikiKGE-10. Although WikiKGE-10 covers a wide range of entities \u2013 a total of 36,434 manuallycurated entity names \u2013 it still focuses only on entities belonging to the head of the popularity distribution of Wikipedia. Our attention is directed to popular entities as we observed a large gap of coverage between English and non-English languages even for entities that are in the top-10%: our benchmark shows that current state-of-the-art techniques, namely, MT, WS, and LLMs, still struggle to provide correct entity names for popular entities. We hypothesize that such techniques will also struggle on less popular entities, i.e., entities belonging to the torso and tail of the popularity distribution. However, we cannot assume that the performance and \u2013 more importantly \u2013 the ranking between MT, WS, and LLMs is the same on torso and tail entities, e.g., WS may be more robust than LLMs in generating names for tail entities. Future work may take advantage of the methodology presented in this paper to create benchmarks for more challenging settings. Last but not least, we stress the fact that the popularity of an entity is variable over time; therefore, entities that are now in the top-10% may not be as popular in the next year, or viceversa, previously unknown entities may become extremely popular in the short-term future.\nM-NTA. In Section 5.2, we demonstrate that MNTA is able to combine information from MT, WS, and LLMs, successfully outperforming the three approaches in increasing coverage and precision of entity names across the 10 languages of WikiKGE10. However, one of its main limitations comes from the fact that M-NTA requires the output from MT, WS, and LLMs, therefore, its inference time and computational cost is equal to the sum of its individual components if run sequentially. Since we want a knowledge graph to contain the best textual information possible, we believe that the increase in performance \u2013 +12% in terms of average F1 score on coverage increase compared to the second best system; +22% on increasing precision \u2013 justifies the additional time and compute required to run M-NTA. However, we look forward to novel methods that will be able to obtain the same or even better results while drastically decreasing the computational requirements."
        },
        {
            "heading": "Acknowledgements",
            "text": "This work would not have been possible without the invaluable feedback by and conversations with Behrang Mohit, Saloni Potdar, Farima Fatahi Bayat, Ronak Pradeep, and Revanth Gangi Reddy."
        },
        {
            "heading": "A Creating WikiKGE-10",
            "text": "In this section, we provide more details on the creation process of WikiKGE-10, our novel humancurated dataset for evaluating automatic approaches on KGE of Wikidata entity names."
        },
        {
            "heading": "A.1 Choice of languages",
            "text": "As mentioned in Section 3.4, one of the main design decision for our benchmark is the selection of 10 languages from a set of diverse typologicallydifferent linguistic families:\n\u2022 West Germanic: English, German;\n\u2022 Romance: Spanish, French, Italian;\n\u2022 Semitic: Arabic;\n\u2022 Sino-Tibetan: Chinese (simplified);\n\u2022 Slavic: Russian;\n\u2022 Koreanic: Korean;\n\u2022 Japonic: Japanese.\nThis design choice makes WikiKGE-10 challenging, as the set of symbols used in each language may or may not vary significantly: for example, a person name may be the same in English and French, but it is highly unlikely that a person name is written in the same way in English and Chinese, which requires at least transliteration. Moreover, the transliteration process between English and Chinese (and also other languages, such as Japanese) is not always deterministic, making it difficult to rely on rule-based approaches to translate a name between these two distant languages. We focused on languages that can be considered high/medium-resource as our quantitative analysis in Section 3.2 shows that coverage of textual information is still far from ideal even for the most popular entities (top-10%) of those high/mediumresource languages. We leave the expansion of our benchmark to lower-resource languages to future work."
        },
        {
            "heading": "A.2 Human annotation process",
            "text": "The objective of the annotation process was to suggest and rate entity names in a target language.\nFirst, given an entity, the human annotators were asked to familiarize themselves with its information: the user interface for the task provided the\nentity names and a short description of the given entity in English retrieved from Wikidata, as well as a built-in panel that directly displayed side-by-side the Wikipedia articles of the corresponding entity both in English and in the target language, if available. This allowed human annotators to familiarize themselves with the entity and catch commonalities and differences between English and non-English information at a glance without leaving the annotation tool.\nAfter learning about the entity, the annotators were tasked with rating entity names that are valid for the given entity with respect to the target language, i.e., if an entity name is valid only in languages that are different from the target language of interest, the annotators were explicitly asked to categorize such names as invalid. More specifically, for each name, an annotator could choose one of the following options:\n\u2022 1 - Incorrect. The name should not be used to refer to the entity in the target language. For example, \u201cpomodori marci\u201d \u2013 the literal translation of \u201ctomatoes that are rotten (fruit)\u201d in Italian \u2013 should never be used to refer to Rotten Tomatoes (the media review site). In addition, the name should always be valid in the target locale; therefore, a name in another language that is not recognized in the target locale should be considered incorrect.\n\u2022 2 - Spelling issues. The name contains minor issues, for example, spelling errors or missing digits. For example, \u201cMichael Jacson\u201d (notice the missing \u201ck\u201d) should not be used to refer to \u201cMichael Jackson\u201d.\n\u2022 3 - Generic, rare or incomplete. The name can be used to refer to this entity but it is very generic, rare or incomplete. For example, \u201cBarack\u201d can be used to refer to \u201cBarack Obama\u201d or \u201cgame\u201d can be used to refer to \u201cvideo game.\u201d Note that nicknames or stage names like \u201cAir Jordan\u201d for Michael Jordan (basketball player) or \u201cMoney\u201d for Floyd Mayweather (boxer) do not fall into this category; they should be categorized as \u201cgood fit\u201d (see below).\n\u2022 4 - Good fit. The name is a good way to refer to this entity (for example, one of its common names, a nickname, or an acronym). For example, \u201cHarvard\u201d can be used to refer to \u201cHarvard University\u201d, \u201cWB\u201d can be used to refer to \u201cWarner Bros.\u201d, \u201cSchumi\u201d is a valid nickname for \u201cMichael Schumacher\u201d.\n\u2022 5 - Perfect fit. The name is the most appropriate name for this entity (usually, its most common name). For example, \u201cHarvard University\u201d (instead of just \u201cHarvard\u201d), \u201cBarack\nObama\u201d (instead of \u201cBarack Hussein Obama II\u201d). In other words, it is the most common or popular entity name to reference the intended entity.\nAnnotators were given the choice to opt out from rating an entity name in case they deemed they did not have enough context (e.g., information from the Wikipedia pages of the entities in English and in the target language) or they did not feel knowledgeable enough about the topic.\nBefore confirming their selection, each annotator had to double-check their choice by searching exact matches of the name under consideration using a Web search engine; a UI component allowed the annotators to directly look up for exact matches in the target language without manually typing a query, making the search easier and speeding up the annotation process. Forcing the annotators to take this extra step allowed them to verify that a named they deemed invalid was indeed invalid, i.e., no or few results from the search engine, or not associated to the entity of interest. An example of an annotation task is shown in Figures 2 and 3.\nWe note that annotators could also suggest new names in the target language for each entity if\nthey knew about other possible valid names. Each suggested name was inserted in the pool of entity names to validate, and, therefore, graded by 3 annotators. On the contrary, annotators could not suggest invalid entity names for an entity, as our objective was to focus on the errors that are already in Wikidata, but could provide feedback in case they noticed that something was wrong in the task.\nA.3 Quality assurance and inter-annotator agreement\nTo guarantee a high-quality output, before participating to the annotation process, each human annotator had to pass an entrance test, which consisted in studying a set of guidelines \u2013 which introduced the annotator to the concepts of entities and knowledge graphs, described the task and the UI elements, and provided a few examples with illustrations \u2013 and in rating 50 entity names correctly. Annotators that could not pass the entrance test could not participate to the actual annotation process (we did not use the 50 entity names in the entrance test in the final dataset).\nFor each target language, we only hired annotators that could certify their proficiency in English\n0.00\n1.00\nAR DE EN ES FR IT JA KO RU ZH All\n0.960.960.960.950.970.960.960.940.950.950.94\nStrong agreement\nTentative\nKrippendorff\u2019s alpha\n0.00\n1.00\nAR DE EN ES FR IT JA KO RU ZH All\n0.790.760.790.79 0.84 0.800.81 0.75 0.83 0.770.78 Strong agreement\nSubstantial\nModerate\nCohen\u2019s Kappa\n0.00\n1.00\nAR DE EN ES FR IT JA KO RU ZH All\n0.960.960.960.950.970.960.960.940.950.950.94\nStrong agreement\nTentative\nKrippendorff\u2019s alpha\nFigure 5: Inter-annotator agreement measured with Krippendorff\u2019s alpha, which takes into account the cardinality of the ratings (1-5), shows strong agreement among annotators.\nand the target language. Annotators were compensated according to the standard hourly wages of their geographic location. On average, each annotator spent about 1 minute for rating an entity name and about 5 minutes on each entity. Since each entity name was rated by 3 annotators, we can estimate that the total human time required by the annotation process is 3 annotators \u00d7 10,000 entities \u00d7 5 minutes / 60 minutes = 2,500 hours.\nAt the end of the annotation process, we measured the inter-annotator agreement in two ways. First, we computed pairwise inter-annotator agreement using Cohen\u2019s kappa. As shown in Figure 4, we can observe an average agreement of 0.79, where a score of 0.60 is usually considered to represent substantial agreement and 0.80 is usually regarded as strong agreement. We also stress that Cohen\u2019s kappa does not take into account the cardinality of the rating values, i.e., for Cohen\u2019s kappa there is no difference between a 1-vs-5 and a 4-vs-5 disagreement. Therefore, we also measured the overall inter-annotator agreement using Krip-\npendorff\u2019s alpha, which shows strong agreement with an average of .96 across all languages, as we can see in Figure 5.5 Overall, the strong interannotator agreement scores validate the results of the annotation process."
        },
        {
            "heading": "B Related Work: Addendum",
            "text": "While WikiKGE-10 is the first benchmark designed to aid development and evaluation of systems for increasing coverage and precision of entity names in multilingual knowledge graphs, there has been previous work that tried to address this issue in other ways. Among them, we acknowledge the existence of JRC-Names (Steinberger et al., 2011). Here, we provide more details on the fundamental differences WikiKGE-10 and JRC-Names, including: i) WikiKGE-10 is completely manually-created; ii) WikiKGE-10 is mapped 1-to-1 to Wikidata; iii) WikiKGE-10 is not limited to persons and organizations; iv) JRC-Names considers names with spelling mistakes as valid names (as they may appear in real-life scenarios), whereas WikiKGE-10 considers them incorrect (as our objective is to obtain a multilingual knowledge graph that is as clean as possible); v) JRC-Names does not distinguish between entities that have the same name, since it is \u201cvery likely that different persons sharing the same first and last name have the same identifier because no disambiguation mechanism is in place.\u201d"
        },
        {
            "heading": "C Methodology: Addendum",
            "text": "In this section, we provide more details on the methods we investigate in our paper, namely, MT, WS, LLMs, and M-NTA."
        },
        {
            "heading": "C.1 Contextualizing entity names",
            "text": "As mentioned in Section 4.1, converting entity names from one language to another \u2013 by using machine translation, looking them up with Web search engines, or querying language models \u2013 is challenging because entity names can be ambiguous. Therefore, we contextualize entity names before converting them from one language to another language, i.e., we add information that a system can use to disambiguate an entity name and produce the correct output in the target language.\nMore specifically, given the fact that we already know the entity identifier associated to the entity\n5The original paper on Krippendorff\u2019s alpha suggests that tentative conclusions can be made with a score greater than 0.67 and strong conclusions can be made with a score greater than 0.80.\nname we would like to translate, we retrieve its corresponding description from Wikidata in the same language as the entity name, and use it to form a pseudo-natural language sentence.6 For example, the entity name Apple is contextualized as \u201cApple is an American technology company\u201d and \u201cApple is a fruit of the apple tree\u201d depending on whether it corresponds to entity Q312 or Q89, respectively. In case of missing entity descriptions for a target language, we construct a simple entity description starting from its instance-of statements in Wikidata, e.g., \u201cAlbert Einstein is a human.\u201d While more complex strategies or more relations may be used to better contextualize entity names, devising more complex strategies \u2013 which may require separate ad hoc solutions for MT, WS, and LLMs \u2013 is beyond the scope of this paper. We leave the investigation of more complex techniques for entity name contextualization to future work.\nC.2 Aligning and de-contextualizing entity names\nWhile the advantage of contextualizing entity names is evident, the main disadvantage is that system will \u201ctranslate\u201d an entity name and also its contextualization information, possibly mixing the two types of textual information. This issue is particularly relevant when translating to a target language with a syntax that is significantly different from the source language or to a target language with non-trivial segmentation rules, e.g., from English to Japanese or Chinese. Therefore, we need to de-contextualize the translated name, i.e., we need to align the translated name to the original name and remove the contextualization information that was translated together with the name.\nTo address this issue, we follow recent studies (Chen et al., 2022) in alignment techniques, which show that MT is surprisingly robust to the insertion of symbols in the input sentence. More specifically, we indicate the start and the end of the entity name in the input sentence with special markers; for example, \u201c[Apple] is an American technology company.\u201d After translating the contextualized entity name into the target language, we detect the start and end markers in the translation and use their position to extract the translated entity name. Our analysis reveals that such an alignment system produces valid alignments most of the time in a\n6Wikidata descriptions can be retrieved from the Wikidata dump. Each entity may have multiple Wikidata descriptions, one for each language if available.\nsubset of manually-inspected instances. While this alignment system can be replaced by more complex alignment techniques, our analysis suggests that alignment errors are not the primary factor in end-to-end evaluation; we measured the number of errors attributable to misalignments and found that only 2% of the translated sentences contains such errors. Therefore, we can conclude that alignment errors are not a major bottleneck to end-to-end performance on WikiKGE-10 \u2013 probably due to the simplicity of the syntactic structure of the sentences that result from the contextualization process \u2013 and leave the investigation of more complex alignment systems to future work."
        },
        {
            "heading": "C.3 MT: implementation details",
            "text": "In our experiments with MT, we decided to limit the number of source languages to 7, namely, German, English, Spanish, French, Italian, Japanese, and Chinese. The main reason behind this choice is that the quality translations from automatic systems has been shown to still lag behind when the source language is a lower-resource language, e.g., Korean. Therefore, in this work, we focus our attention on higher-resource languages for which MT has been proven to achieve satisfactory results on several standard benchmarks, allowing us to iterate faster. We hypothesize that translating from lowerresource languages does not result in performance that is significantly better than what we can see in Table 2, even when the linguistic families of the source and target languages are close. However, we leave an investigation on the effect of carefully choosing source-target language pairs for MT to future work."
        },
        {
            "heading": "C.4 WS: implementation details",
            "text": "In Section 4.1, we discussed how WS can be used to retrieve entity names in a target language: given an entity name in a source language ls, we can perform a search using a query like \u201c[entity-name] in [lt]\u201d to obtain results in a target language lt. Moreover, we can enrich the query by adding contextual information in the form of Wikidata descriptions, as discussed in section C.1, resulting in enriched queries like \u201c[entity-name] ([entity-description]) in [lt]\u201d to mitigate the problem of ambiguous names, e.g., not only there are more than 10 people in Wikipedia that could be referred to as Michael Jordan but also songs and movies.\nMore specifically, given an entity e and one of its names ens and its Wikidata description e d s in a\nsource language ls, we build a search query as described above, limiting the choice of ls to English. Then, we parse the HTML response and collect the most frequently highlighted terms, i.e., those terms that are in bold (between <b></b> tags) or emphasized (between <em></em> tags), in the top-10 websites returned by the search engine. Finally, we keep the top-5 entity names retrieved from the collected terms if they appear at least 2 times among the highlighted results. As discussed in Section 5.2, such an approach \u2013 even though it tries to imitate how humans look up information on the Web \u2013 results in a significant amount of noise due to the collection of a significant number of terms that are only semantically-related to the query and not semantic matches."
        },
        {
            "heading": "C.5 LLMs: implementation details",
            "text": "In our experiments, we investigate two main LLMs: mT0 and GPT. The former is the instructionfinetuned version of mT5, a state-of-the-art multilingual LLM. For mT5, we take into account three variants \u2013 large, xl, and xxl \u2013 which differ in their size to investigate if and to what extent increasing the number of trainable parameters in a language model is beneficial for the task under consideration.\nFor our experiments, we evaluate the effectiveness of mT5 and GPT with one-shot prompts, i.e., we provide a description of the task and one example of input/output to the LLM before requiring them to generate the entity name of interest. More specifically, each prompt is constructed as follows:\n\u2022 Task definition: given an entity name in English and a short description of the entity in English, complete the following with the corresponding entity name in [lt].\n\u2022 Example:\n\u2013 English name: [e\u0302ns ] \u2013 English description: [e\u0302ds] \u2013 [lt] name: [e\u0302nt ]\n\u2022 Task:\n\u2013 English name: [ens ] \u2013 English description: [eds] \u2013 [lt] name:\nwhere lt is the target language, e\u0302 is the entity used for the example, and e is the entity of interest. We choose the example entity e\u0302 at random from the top-10% entities with the only constraint that e\u0302 and\ne have the same entity type, e.g., if we want to generate the name for e and e is a person, then also the example entity e\u0302 shall be a person. Notwithstanding the input/output example provided, we observe that sometimes LLMs, even when they output correct names, do not conform to the same input/output format as the example, e.g., they add preambles (\u201cthe name of X is Y\u201d, \u201cas a language model, I...\u201d) or explanations (\u201cX because...\u201d). This makes it hard to extract the relevant portion of text, resulting in alignment errors."
        },
        {
            "heading": "C.6 M-NTA: implementation details",
            "text": "In this section, we provide more details on three important factors for the implementation of M-NTA, namely, the value of \u03bb, the choice of \u03d5, and the individual contribution of each sub-system (MT, WS, and LLMs) in M-NTA."
        },
        {
            "heading": "C.6.1 The value of \u03bb",
            "text": "In Section 4.2, we introduced M-NTA, our novel approach to combine MT, WS, and LLMs, and described how it scores and ranks the answers Y = {y : \u03c3(y) \u2265 \u03bb} according to a threshold hyperparameter \u03bb, mentioning that \u03bb = 2 is the most robust choice for coverage. Here, we expand our discussion on \u03bb, showing how the choice of its value can significantly vary the precision and recall of the answers provided by M-NTA.\nAt a high level, the intuition behind \u03bb is that it is a hyperparameter that controls the number of \u201csupporting evidences\u201d required by M-NTA to consider an answer as plausible; on the contrary, if an answer is supported by fewer than \u03bb evidences, then M-NTA considers such an answer as noise. Therefore, we can expect that increasing the value of \u03bb will result in more precise predictions at the cost of recall, and decreasing the value of \u03bb will result in more broad coverage but also less precise answers. This is indeed the case in our experiments, as we can see in Figures 6 and 7, in which we can observe that increasing the value of \u03bb decreases the overall recall while increasing the precision of the answers on a sample of the Italian and Korean test sets of WikiKGE-10. Given the results of M-NTA for different values of \u03bb across the 10 languages of WikiKGE-10, we observed that \u03bb = 2 is empirically the best choice on average if we want to balance precision and recall in coverage. However, we also note that the decision about the value of \u03bb can be also affected by the downstream application of interest: if the use case is adding textual\ninformation to a knowledge graph for direct user consumption, then we may want to prefer precision over recall and increase the value of \u03bb accordingly; otherwise, if we want to use textual information for the creation of multilingual embeddings, then we may be more interested in recall for covering as many entities as possible."
        },
        {
            "heading": "C.6.2 The choice of \u03d5",
            "text": "One important factor in the design of M-NTA is the choice of the function \u03d5(y, y\u2032) \u2192 {0, 1}, which establishes whether an answer y from a system f(\u00b7) is supported by the answer y\u2032 from another system f \u2032(\u00b7). While \u03d5 can be any \u201csimilarity\u201d metric, e.g., a measure of vector similarity, the final choice depends on the type of textual information represented by each answer. In this paper, we focus\non entity names, for which even a slight variation between two names can mark the difference between a correct name and an incorrect one, e.g., Olivier and Oliver. Therefore, we choose exact match between lower-cased, punctuation-stripped entity names as the function \u03d5, i.e., a name y is supported by another name y\u2032 if and only if y = y\u2032, except for letter casing (e.g., Canary and canary) and punctuation (Michael B Jordan and Michael B. Jordan). As we will see in section C.7, other forms of \u03d5 may be more appropriate for types of textual information different from entity names."
        },
        {
            "heading": "C.6.3 Ablation study",
            "text": "Throughout the paper, we mentioned multiple times that the main strength of M-NTA is its capability to combine the answers provided by MT, WS, and LLMs. Here, we carry out an ablation study to quantify and better understand the individual impact of each subsystem in MNTA. More specifically, we compare the results of the \u201cfull\u201d M-NTA to M-NTA without Google Web Search (M-NTAno-WS), without GPT-3.5 (MNTAno-LLM), and only with MT from 7 languages (M-NTAno-WS/no-LLM). As we can see in Table 6, even when M-NTA does not rely on answers from WS and LLMs, the results of M-NTAno-WS/no-LLM\nare better than simple translation from the best source language (English). This empirically validates our hypothesis, i.e., different languages hold complementary knowledge and M-NTA is able to combine such knowledge in an effective way.\nC.7 Applying M-NTA to entity descriptions\nWhile the focus of WikiKGE-10 is on entity names, the approaches described in Section 4.1 \u2013 MT, WS, and LLMs \u2013 and M-NTA can also be applied to other types of textual information. As discussed in sections 2 and 3, entity descriptions are another popular type of textual information used in recent approaches. In this section, we describe how MT and M-NTA can be easily adapted to convert entity descriptions from one language to another, while we leave a more in-depth study about the effectiveness of WS and LLMs for entity descriptions to future work.\nAdapting the MT-based approach to generate entity descriptions in a target language is straightforward. In section C.2, we discussed the necessity of using special markers to facilitate the extraction of the translated entity name from the translated sentence, e.g., \u201c[Apple] is an American multinational technology company.\u201d To extract the entity description instead of the entity name, we can simply place the special markers around the entity description, e.g., \u201cApple is an [American multinational technology company].\u201d Thanks to this simple modification, the rest of the pipeline for the MT-based approach can remain the same.\nAdapting M-NTA to generate entity descriptions in a target language requires an additional step,\ni.e., designing an appropriate function \u03d5(y, y\u2032) \u2192 {0, 1} (see section 4.2) to establish when a description y\u2032 = e\u0304dt counts as supporting evidence for a different description y = edt . Indeed, a description may imply another description even if they are not exact matches. For example, the English description for Earth (Q2) is \u201cthird planet from the Sun in the Solar System\u201d, which implies the Spanish description \u201cplanet in the Solar System, third by distance from the Sun\u201d (translated in English from Spanish). To address this issue, we define \u03d5 as follows:\n\u03d5(y, y\u2032) = { 1 if sim(y, y\u2032) > 0.5 0 if sim(y, y\u2032) \u2264 0.5\nwhere sim(\u00b7) is the cosine similarity between the vector representations of y and y\u2032. We compute the vector representations of the descriptions by using XLM-RoBERTa (base) (Conneau et al., 2020)."
        },
        {
            "heading": "D WikiKGE-10: Additional Results",
            "text": "In this section, we provide additional results to complement the main results described in Section 5.\nIndeed, it is interesting to observe how the results would change if we slightly relax the metrics of coverage and precision. In particular, we relax coverage to provide a positive score in case a system is able to provide at least one valid entity name for a given entity. Similarly, we relax precision to provide a positive score in case a system is able to identify at least one invalid entity name for a given entity. Table 8 provides an overview of the results. As one could expect, the scores on coverage increase, as it is easier to provide one valid\nname for an entity instead of the complete list of valid names. However, the performance in precision decrease usually decreases, as we hypothesize that there are entities for which it is more difficult to identify incorrect entity names.\nE Impact on Downstream Tasks: Question Answering\nIn section 6, we have investigated the impact of increasing coverage and precision of textual information in two downstream tasks, namely, multilingual entity linking and multilingual knowledge graph completion. Here, we also investigate the impact of our work on Question Answering (QA), with a specific focus on knowledge-seeking queries. One of the main characteristics of knowledge-seeking queries is that they can be usually answered by navigating a knowledge graph and returning (the name of) an entity, e.g., the answer to the query \u201cWhat is the highest mountain in Washington, US?\u201d is Mount Rainier (Q194057). However, if the knowledge graph does not provide a lexicalization for\nthe entity in the target language, then a knowledgebased QA system will not be able to provide a correct answer. Therefore, increasing the coverage of entity names across languages is essential to extend the support of knowledge-based QA systems to multilingual settings.\nTo quantify the impact of M-NTA on QA, we consider the subset of queries in MKQA (Longpre et al., 2021), a multilingual QA dataset for knowledge-seeking queries, whose type of answer is classified as \u201centity\u201d, i.e., those queries that can be answered by providing the name of a Wikidata entity. Importantly, the original authors of MKQA manually added names (primary names and aliases) for all those Wikidata entities that did not have a lexicalization. Therefore, there is a set of questions in MKQA which are \u201cunanswerable\u201d by a knowledge-based QA system that relies on Wikidata; this set of unanswer-\nable questions impose an upper bound to the results achievable by any knowledge-based QA system. More specifically, we measure the number of answerable/unanswerable queries when relying only on Wikidata7 compared to using an M-NTAaugmented Wikidata (Wikidata + M-NTA) in two settings:\n\u2022 Entity coverage: the number of entities in the answers of MKQA for which Wikidata (or Wikidata + M-NTA) can provide at least one name;\n\u2022 Name coverage: the number of names for the entities in the answers of MKQA that are also present in Wikidata (or Wikidata + M-NTA).\nAs we can see in Table 9, using M-NTA allows us to increase the number of answerable queries both when we look at entity coverage (+3.52% absolute improvement) and name coverage (+2.53% absolute improvement). Notably, M-NTA provides a significant increase in entity coverage for simplified Chinese (+10.29% absolute improvement), which is the language with lowest coverage, but also in English (+0.14% absolute improvement). Although the absolute improvement in English seems small, entity coverage in English is already high in Wikidata (99.09%): another way to look at this improvement is by analyzing the reduction rate in the number of unanswerable queries. As we can see in Figure 8, the reduction rate in the number of unanswerable queries in MKQA can be reduced significantly when using M-NTA to improve the coverage of Wikidata. Even for English, the reduction rate is about 15.4%, which becomes as high as 52.0% and 53.8% in German and Italian, respectively.\n7As of April 2023."
        }
    ],
    "title": "Increasing Coverage and Precision of Textual Information in Multilingual Knowledge Graphs",
    "year": 2023
}