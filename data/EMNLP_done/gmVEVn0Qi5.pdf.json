{
    "abstractText": "While recently developed NLP explainability methods let us open the black box in various ways (Madsen et al., 2022), a missing ingredient in this endeavor is an interactive tool offering a conversational interface. Such a dialogue system can help users explore datasets and models with explanations in a contextualized manner, e.g. via clarification or follow-up questions, and through a natural language interface. We adapt the conversational explanation framework TALKTOMODEL (Slack et al., 2023) to the NLP domain, add new NLP-specific operations such as free-text rationalization, and illustrate its generalizability on three NLP tasks (dialogue act classification, question answering, hate speech detection). To recognize user queries for explanations, we evaluate fine-tuned and few-shot prompting models and implement a novel Adapter-based approach. We then conduct two user studies on (1) the perceived correctness and helpfulness of the dialogues, and (2) the simulatability, i.e. how objectively helpful dialogical explanations are for humans in figuring out the model\u2019s predicted label when it\u2019s not shown. We found rationalization and feature attribution were helpful in explaining the model behavior. Moreover, users could more reliably predict the model outcome based on an explanation dialogue rather than one-off explanations. Disclaimer: This paper contains material that is offensive",
    "authors": [
        {
            "affiliations": [],
            "name": "Nils Feldhus"
        },
        {
            "affiliations": [],
            "name": "Qianli Wang"
        },
        {
            "affiliations": [],
            "name": "Tatiana Anikina"
        },
        {
            "affiliations": [],
            "name": "Sahil Chopra"
        },
        {
            "affiliations": [],
            "name": "Cennet Oguz"
        },
        {
            "affiliations": [],
            "name": "Sebastian M\u00f6ller"
        }
    ],
    "id": "SP:702f698885d96dd99cdb563e95ab2e3b069b6edc",
    "references": [
        {
            "authors": [
                "Siddhant Arora",
                "Danish Pruthi",
                "Norman Sadeh",
                "William W. Cohen",
                "Zachary C. Lipton",
                "Graham Neubig."
            ],
            "title": "Explain, edit, and understand: Rethinking user study design for evaluating model explanations",
            "venue": "Proceedings of the AAAI Conference",
            "year": 2022
        },
        {
            "authors": [
                "Pepa Atanasova",
                "Jakob Grue Simonsen",
                "Christina Lioma",
                "Isabelle Augenstein."
            ],
            "title": "A diagnostic study of explainability techniques for text classification",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing",
            "year": 2020
        },
        {
            "authors": [
                "Giuseppe Attanasio",
                "Debora Nozza",
                "Eliana Pastor",
                "Dirk Hovy."
            ],
            "title": "Benchmarking post-hoc interpretability approaches for transformer-based misogyny detection",
            "venue": "Proceedings of NLP Power! The First Workshop on Efficient Benchmarking in NLP,",
            "year": 2022
        },
        {
            "authors": [
                "Esma Balkir",
                "Isar Nejadgholi",
                "Kathleen Fraser",
                "Svetlana Kiritchenko."
            ],
            "title": "Necessity and sufficiency for explaining text classifiers: A case study in hate speech detection",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the As-",
            "year": 2022
        },
        {
            "authors": [
                "Astrid Bertrand",
                "Tiphaine Viard",
                "Rafik Belloum",
                "James R. Eagan",
                "Winston Maxwell."
            ],
            "title": "On selective, mutable and dialogic XAI: A review of what users say about different types of interactive explanations",
            "venue": "Proceedings of the 2023 CHI Conference",
            "year": 2023
        },
        {
            "authors": [
                "Steven Bird."
            ],
            "title": "NLTK: The Natural Language Toolkit",
            "venue": "Proceedings of the COLING/ACL 2006",
            "year": 2006
        },
        {
            "authors": [
                "Oana-Maria Camburu",
                "Tim Rockt\u00e4schel",
                "Thomas Lukasiewicz",
                "Phil Blunsom."
            ],
            "title": "e-SNLI: Natural language inference with natural language explanations",
            "venue": "Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc.",
            "year": 2018
        },
        {
            "authors": [
                "Zeming Chen",
                "Qiyue Gao",
                "Antoine Bosselut",
                "Ashish Sabharwal",
                "Kyle Richardson."
            ],
            "title": "DISCO: Distilling counterfactuals with large language models",
            "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume",
            "year": 2023
        },
        {
            "authors": [
                "Jason Wei"
            ],
            "title": "Scaling instruction-finetuned language models. arXiv, abs/2210.11416",
            "year": 2022
        },
        {
            "authors": [
                "Christopher Clark",
                "Kenton Lee",
                "Ming-Wei Chang",
                "Tom Kwiatkowski",
                "Michael Collins",
                "Kristina Toutanova."
            ],
            "title": "BoolQ: Exploring the surprising difficulty of natural yes/no questions",
            "venue": "Proceedings of the 2019 Conference of the North American Chap-",
            "year": 2019
        },
        {
            "authors": [
                "Anamaria Crisan",
                "Margaret Drouhard",
                "Jesse Vig",
                "Nazneen Rajani."
            ],
            "title": "Interactive model cards: A human-centered approach to model documentation",
            "venue": "2022 ACM Conference on Fairness, Accountability, and Transparency, FAccT \u201922, page 427\u2013439,",
            "year": 2022
        },
        {
            "authors": [
                "Bhavana Dalvi Mishra",
                "Oyvind Tafjord",
                "Peter Clark."
            ],
            "title": "Towards teachable reasoning systems: Using a dynamic memory of user feedback for continual system improvement",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language",
            "year": 2022
        },
        {
            "authors": [
                "Anubrata Das",
                "Chitrank Gupta",
                "Venelin Kovatchev",
                "Matthew Lease",
                "Junyi Jessy Li."
            ],
            "title": "ProtoTEx: Explaining model decisions with prototype tensors",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume",
            "year": 2022
        },
        {
            "authors": [
                "Jay DeYoung",
                "Sarthak Jain",
                "Nazneen Fatema Rajani",
                "Eric Lehman",
                "Caiming Xiong",
                "Richard Socher",
                "Byron C. Wallace."
            ],
            "title": "ERASER: A benchmark to evaluate rationalized NLP models",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for",
            "year": 2020
        },
        {
            "authors": [
                "Nouha Dziri",
                "Sivan Milton",
                "Mo Yu",
                "Osmar Zaiane",
                "Siva Reddy"
            ],
            "title": "On the origin of hallucinations in conversational models: Is it the datasets or the models",
            "venue": "In Proceedings of the 2022 Conference of the North American Chapter of the Association",
            "year": 2022
        },
        {
            "authors": [
                "Javid Ebrahimi",
                "Anyi Rao",
                "Daniel Lowd",
                "Dejing Dou."
            ],
            "title": "HotFlip: White-box adversarial examples for text classification",
            "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 31\u201336,",
            "year": 2018
        },
        {
            "authors": [
                "Nils Feldhus",
                "Leonhard Hennig",
                "Maximilian Dustin Nasert",
                "Christopher Ebert",
                "Robert Schwarzenberg",
                "Sebastian M\u00f6ller"
            ],
            "title": "Saliency map verbalization: Comparing feature importance representations from model-free and instruction-based",
            "year": 2023
        },
        {
            "authors": [
                "Nils Feldhus",
                "Ajay Madhavan Ravichandran",
                "Sebastian M\u00f6ller."
            ],
            "title": "Mediators: Conversational agents explaining NLP model behavior",
            "venue": "IJCAI 2022 - Workshop on Explainable Artificial Intelligence (XAI), Vienna, Austria. International Joint Con-",
            "year": 2022
        },
        {
            "authors": [
                "Gabrielle Gauthier-Melan\u00e7on",
                "Orlando Marquez Ayala",
                "Lindsay Brin",
                "Chris Tyler",
                "Fr\u00e9d\u00e9ric BranchaudCharron",
                "Joseph Marinier",
                "Karine Grande",
                "Di Le."
            ],
            "title": "Azimuth: Systematic error analysis for text classification",
            "venue": "Proceedings of the 2022 Conference",
            "year": 2022
        },
        {
            "authors": [
                "Timnit Gebru",
                "Jamie Morgenstern",
                "Briana Vecchione",
                "Jennifer Wortman Vaughan",
                "Hanna Wallach",
                "Hal Daum\u00e9 III",
                "Kate Crawford."
            ],
            "title": "Datasheets for datasets",
            "venue": "Commun. ACM, 64(12):86\u201392.",
            "year": 2021
        },
        {
            "authors": [
                "Karan Goel",
                "Nazneen Fatema Rajani",
                "Jesse Vig",
                "Zachary Taschdjian",
                "Mohit Bansal",
                "Christopher R\u00e9."
            ],
            "title": "Robustness gym: Unifying the NLP evaluation landscape",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for",
            "year": 2021
        },
        {
            "authors": [
                "Ana Valeria Gonz\u00e1lez",
                "Anna Rogers",
                "Anders S\u00f8gaard."
            ],
            "title": "On the interaction of belief bias and explanations",
            "venue": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 2930\u20132942, Online. Association for Computational",
            "year": 2021
        },
        {
            "authors": [
                "Mareike Hartmann",
                "Han Du",
                "Nils Feldhus",
                "Ivana KruijffKorbayov\u00e1",
                "Daniel Sonntag."
            ],
            "title": "XAINES: Explaining AI with narratives",
            "venue": "KI - K\u00fcnstliche Intelligenz, 36(3):287\u2013296.",
            "year": 2022
        },
        {
            "authors": [
                "Peter Hase",
                "Mohit Bansal."
            ],
            "title": "Evaluating explainable AI: Which algorithmic explanations help users predict model behavior? In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5540\u20135552, Online",
            "venue": "Association",
            "year": 2020
        },
        {
            "authors": [
                "Fred Hohman",
                "Andrew Head",
                "Rich Caruana",
                "Robert DeLine",
                "Steven M. Drucker."
            ],
            "title": "Gamut: A design probe to understand how data scientists understand machine learning models",
            "venue": "Proceedings of the 2019 CHI Conference on Human Factors in",
            "year": 2019
        },
        {
            "authors": [
                "Neil Houlsby",
                "Andrei Giurgiu",
                "Stanislaw Jastrzebski",
                "Bruna Morrone",
                "Quentin De Laroussilhe",
                "Andrea Gesmundo",
                "Mona Attariyan",
                "Sylvain Gelly."
            ],
            "title": "Parameter-efficient transfer learning for NLP",
            "venue": "Proceedings of the 36th International Conference",
            "year": 2019
        },
        {
            "authors": [
                "Alon Jacovi",
                "Jasmijn Bastings",
                "Sebastian Gehrmann",
                "Yoav Goldberg",
                "Katja Filippova."
            ],
            "title": "Diagnosing ai explanation methods with folk concepts of behavior",
            "venue": "Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency,",
            "year": 2023
        },
        {
            "authors": [
                "Diane Kelly",
                "Paul B. Kantor",
                "Emile L. Morse",
                "Jean Scholtz",
                "Ying Sun."
            ],
            "title": "Questionnaires for eliciting evaluation data from users of interactive question answering systems",
            "venue": "Natural Language Engineering, 15(1):119\u2013141.",
            "year": 2009
        },
        {
            "authors": [
                "Been Kim",
                "Rajiv Khanna",
                "Oluwasanmi O Koyejo."
            ],
            "title": "Examples are not enough, learn to criticize! criticism for interpretability",
            "venue": "Advances in Neural Information Processing Systems, volume 29. Curran Associates, Inc.",
            "year": 2016
        },
        {
            "authors": [
                "Pang Wei Koh",
                "Percy Liang."
            ],
            "title": "Understanding black-box predictions via influence functions",
            "venue": "Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pages 1885\u20131894.",
            "year": 2017
        },
        {
            "authors": [
                "Micha\u0142 Ku\u017aba",
                "Przemys\u0142aw Biecek"
            ],
            "title": "What would you ask the machine learning model? identification of user needs for model explanations based",
            "year": 2020
        },
        {
            "authors": [
                "Himabindu Lakkaraju",
                "Dylan Slack",
                "Yuxin Chen",
                "Chenhao Tan",
                "Sameer Singh."
            ],
            "title": "Rethinking explainability as a dialogue: A practitioner\u2019s perspective",
            "venue": "HCAI @ NeurIPS 2022.",
            "year": 2022
        },
        {
            "authors": [
                "Dong-Ho Lee",
                "Akshen Kadakia",
                "Brihi Joshi",
                "Aaron Chan",
                "Ziyi Liu",
                "Kiran Narahari",
                "Takashi Shibuya",
                "Ryosuke Mitani",
                "Toshiyuki Sekiya",
                "Jay Pujara",
                "Xiang Ren"
            ],
            "title": "XMD: An end-to-end framework for interactive explanation-based debugging of NLP",
            "year": 2023
        },
        {
            "authors": [
                "Yanran Li",
                "Hui Su",
                "Xiaoyu Shen",
                "Wenjie Li",
                "Ziqiang Cao",
                "Shuzi Niu."
            ],
            "title": "DailyDialog: A manually labelled multi-turn dialogue dataset",
            "venue": "Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers),",
            "year": 2017
        },
        {
            "authors": [
                "Pan Lu",
                "Baolin Peng",
                "Hao Cheng",
                "Michel Galley",
                "KaiWei Chang",
                "Ying Nian Wu",
                "Song-Chun Zhu",
                "Jianfeng Gao."
            ],
            "title": "Chameleon: Plug-and-play compositional reasoning with large language models",
            "venue": "arXiv, abs/2304.09842.",
            "year": 2023
        },
        {
            "authors": [
                "Clark."
            ],
            "title": "Self-Refine: Iterative refinement with self-feedback",
            "venue": "Advances in Neural Information Processing Systems.",
            "year": 2023
        },
        {
            "authors": [
                "Andreas Madsen",
                "Siva Reddy",
                "Sarath Chandar."
            ],
            "title": "Post-hoc interpretability for neural NLP: A survey",
            "venue": "ACM Comput. Surv.",
            "year": 2022
        },
        {
            "authors": [
                "Lorenzo Malandri",
                "Fabio Mercorio",
                "Mario Mezzanzanica",
                "Navid Nobani."
            ],
            "title": "ConvXAI: a system for multimodal interaction with any black-box explainer",
            "venue": "Cognitive Computation, 15(2):613\u2013644.",
            "year": 2022
        },
        {
            "authors": [
                "Ana Marasovic",
                "Iz Beltagy",
                "Doug Downey",
                "Matthew Peters."
            ],
            "title": "Few-shot self-rationalization with natural language prompts",
            "venue": "Findings of the Association for Computational Linguistics: NAACL 2022, pages 410\u2013424, Seattle, United States. Association",
            "year": 2022
        },
        {
            "authors": [
                "Binny Mathew",
                "Punyajoy Saha",
                "Seid Muhie Yimam",
                "Chris Biemann",
                "Pawan Goyal",
                "Animesh Mukherjee."
            ],
            "title": "Hatexplain: A benchmark dataset for explainable hate speech detection",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2021
        },
        {
            "authors": [
                "Vivek Miglani",
                "Aobo Yang",
                "Aram H. Markosyan",
                "Diego Garcia-Olano",
                "Narine Kokhlikyan."
            ],
            "title": "Using captum to explain generative language models",
            "venue": "Proceedings of the 3rd Workshop for Natural Language Processing Open Source Software (NLP-OSS),",
            "year": 2023
        },
        {
            "authors": [
                "Tim Miller."
            ],
            "title": "Explanation in artificial intelligence: Insights from the social sciences",
            "venue": "Artificial Intelligence, 267:1\u201338.",
            "year": 2019
        },
        {
            "authors": [
                "Margaret Mitchell",
                "Simone Wu",
                "Andrew Zaldivar",
                "Parker Barnes",
                "Lucy Vasserman",
                "Ben Hutchinson",
                "Elena Spitzer",
                "Inioluwa Deborah Raji",
                "Timnit Gebru."
            ],
            "title": "Model cards for model reporting",
            "venue": "Proceedings of the Conference on Fairness, Account-",
            "year": 2019
        },
        {
            "authors": [
                "Edoardo Mosca",
                "Daryna Dementieva",
                "Tohid Ebrahim Ajdari",
                "Maximilian Kummeth",
                "Kirill Gringauz",
                "Georg Groh."
            ],
            "title": "IFAN: An explainability-focused interaction framework for humans and NLP models",
            "venue": "Proceedings of the 3rd Conference of the",
            "year": 2023
        },
        {
            "authors": [
                "Dong Nguyen."
            ],
            "title": "Comparing automatic and human evaluation of local explanations for text classification",
            "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
            "year": 2018
        },
        {
            "authors": [
                "Van Bach Nguyen",
                "J\u00f6rg Schl\u00f6tterer",
                "Christin Seifert."
            ],
            "title": "Explaining machine learning models in natural conversations: Towards a conversational XAI agent",
            "venue": "The World Conference on eXplainable Artificial Intelligence 2023 (XAI-2023), Lisbon, Portugal.",
            "year": 2023
        },
        {
            "authors": [
                "Pouya Pezeshkpour",
                "Sarthak Jain",
                "Sameer Singh",
                "Byron Wallace."
            ],
            "title": "Combining feature and instance attribution to detect artifacts",
            "venue": "Findings of the Association for Computational Linguistics: ACL 2022, pages 1934\u20131946, Dublin, Ireland. Association",
            "year": 2022
        },
        {
            "authors": [
                "Cho",
                "Iryna Gurevych."
            ],
            "title": "AdapterHub: A framework for adapting transformers",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 46\u201354, Online. Association for Computational",
            "year": 2020
        },
        {
            "authors": [
                "Nazneen Rajani",
                "Weixin Liang",
                "Lingjiao Chen",
                "Margaret Mitchell",
                "James Zou."
            ],
            "title": "SEAL: Interactive tool for systematic error analysis and labeling",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing: System",
            "year": 2022
        },
        {
            "authors": [
                "Arijit Ray",
                "Yi Yao",
                "Rakesh Kumar",
                "Ajay Divakaran",
                "Giedrius Burachas."
            ],
            "title": "Can you explain that? lucid explanations help human-ai collaborative image retrieval",
            "venue": "Proceedings of the AAAI Conference on Human Computation and Crowdsourcing, volume 7,",
            "year": 2019
        },
        {
            "authors": [
                "Nils Reimers",
                "Iryna Gurevych."
            ],
            "title": "SentenceBERT: Sentence embeddings using Siamese BERTnetworks",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natu-",
            "year": 2019
        },
        {
            "authors": [
                "Shuhuai Ren",
                "Yihe Deng",
                "Kun He",
                "Wanxiang Che."
            ],
            "title": "Generating natural language adversarial examples through probability weighted word saliency",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1085\u2013",
            "year": 2019
        },
        {
            "authors": [
                "Samuel R\u00f6nnqvist",
                "Aki-Juhani Kyr\u00f6l\u00e4inen",
                "Amanda Myntti",
                "Filip Ginter",
                "Veronika Laippala."
            ],
            "title": "Explaining classes through stable word attributions",
            "venue": "Findings of the Association for Computational Linguistics: ACL 2022, pages 1063\u20131074, Dublin,",
            "year": 2022
        },
        {
            "authors": [
                "Alexis Ross",
                "Tongshuang Wu",
                "Hao Peng",
                "Matthew Peters",
                "Matt Gardner."
            ],
            "title": "Tailor: Generating and perturbing text with semantic controls",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Pa-",
            "year": 2022
        },
        {
            "authors": [
                "Victor Sanh",
                "Lysandre Debut",
                "Julien Chaumond",
                "Thomas Wolf."
            ],
            "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter",
            "venue": "5th Workshop on Energy Efficient Machine Learning and Cognitive Computing - NeurIPS 2019.",
            "year": 2019
        },
        {
            "authors": [
                "Gabriele Sarti",
                "Nils Feldhus",
                "Ludwig Sickert",
                "Oskar van der Wal."
            ],
            "title": "Inseq: An interpretability toolkit for sequence generation models",
            "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 3: System",
            "year": 2023
        },
        {
            "authors": [
                "Timo Schick",
                "Jane Dwivedi-Yu",
                "Roberto Dess\u00ec",
                "Roberta Raileanu",
                "Maria Lomeli",
                "Luke Zettlemoyer",
                "Nicola Cancedda",
                "Thomas Scialom."
            ],
            "title": "Toolformer: Language models can teach themselves to use tools",
            "venue": "arXiv, abs/2302.04761.",
            "year": 2023
        },
        {
            "authors": [
                "Hendrik Schuff",
                "Heike Adel",
                "Ngoc Thang Vu."
            ],
            "title": "F1 is Not Enough! Models and Evaluation Towards User-Centered Explainable Question Answering",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
            "year": 2020
        },
        {
            "authors": [
                "Hendrik Schuff",
                "Alon Jacovi",
                "Heike Adel",
                "Yoav Goldberg",
                "Ngoc Thang Vu."
            ],
            "title": "Human interpretation of saliency-based explanation over text",
            "venue": "2022 ACM Conference on Fairness, Accountability, and Transparency, FAccT \u201922, page 611\u2013636, New York,",
            "year": 2022
        },
        {
            "authors": [
                "Hua Shen",
                "Chieh-Yang Huang",
                "Tongshuang Wu",
                "Ting-Hao Kenneth Huang."
            ],
            "title": "ConvXAI: Delivering heterogeneous AI explanations via conversations to support human-AI scientific writing",
            "venue": "Computer Supported Cooperative Work and Social Computing,",
            "year": 2023
        },
        {
            "authors": [
                "Clemencia Siro",
                "Mohammad Aliannejadi",
                "Maarten de Rijke."
            ],
            "title": "Understanding user satisfaction with task-oriented dialogue systems",
            "venue": "Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval,",
            "year": 2022
        },
        {
            "authors": [
                "Dylan Slack",
                "Satyapriya Krishna",
                "Himabindu Lakkaraju",
                "Sameer Singh."
            ],
            "title": "Explaining machine learning models with interactive natural language conversations using TalkToModel",
            "venue": "Nature Machine Intelligence.",
            "year": 2023
        },
        {
            "authors": [
                "Julia Strout",
                "Ye Zhang",
                "Raymond Mooney."
            ],
            "title": "Do human rationales improve machine explanations? In Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 56\u201362, Florence, Italy",
            "venue": "As-",
            "year": 2019
        },
        {
            "authors": [
                "Mukund Sundararajan",
                "Ankur Taly",
                "Qiqi Yan."
            ],
            "title": "Axiomatic attribution for deep networks",
            "venue": "Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pages 3319\u20133328. PMLR.",
            "year": 2017
        },
        {
            "authors": [
                "Ian Tenney",
                "James Wexler",
                "Jasmijn Bastings",
                "Tolga Bolukbasi",
                "Andy Coenen",
                "Sebastian Gehrmann",
                "Ellen Jiang",
                "Mahima Pushkarna",
                "Carey Radebaugh",
                "Emily Reif",
                "Ann Yuan"
            ],
            "title": "The language interpretability tool: Extensible, interactive visualizations",
            "year": 2020
        },
        {
            "authors": [
                "Vittorio Torri."
            ],
            "title": "Textual eXplanations for intuitive machine learning",
            "venue": "Master\u2019s thesis, Politecnico di Milano, dec.",
            "year": 2021
        },
        {
            "authors": [
                "Margaret Mitchell",
                "Alexander M. Rush",
                "Thomas Wolf",
                "Douwe Kiela."
            ],
            "title": "Evaluate & evaluation on the hub: Better best practices for data and model measurement",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Lan-",
            "year": 2022
        },
        {
            "authors": [
                "Zijie J. Wang",
                "Duen Horng Chau."
            ],
            "title": "Webshap: Towards explaining any machine learning models anywhere",
            "venue": "Companion Proceedings of the ACM Web Conference 2023, WWW \u201923 Companion, page 262\u2013266, New York, NY, USA. Association for Com-",
            "year": 2023
        },
        {
            "authors": [
                "Daniel S. Weld",
                "Gagan Bansal."
            ],
            "title": "The challenge of crafting intelligible intelligence",
            "venue": "Commun. ACM, 62(6):70\u201379.",
            "year": 2019
        },
        {
            "authors": [
                "Christian Werner."
            ],
            "title": "Explainable ai through rulebased interactive conversation",
            "venue": "Proceedings of the Workshops of the EDBT/ICDT 2020 Joint Conference.",
            "year": 2020
        },
        {
            "authors": [
                "Sarah Wiegreffe",
                "Jack Hessel",
                "Swabha Swayamdipta",
                "Mark Riedl",
                "Yejin Choi."
            ],
            "title": "Reframing human-AI collaboration for generating free-text explanations",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association",
            "year": 2022
        },
        {
            "authors": [
                "Tongshuang Wu",
                "Marco Tulio Ribeiro",
                "Jeffrey Heer",
                "Daniel Weld."
            ],
            "title": "Polyjuice: Generating counterfactuals for explaining, evaluating, and improving models",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics",
            "year": 2021
        },
        {
            "authors": [
                "Yang Xiao",
                "Jinlan Fu",
                "Weizhe Yuan",
                "Vijay Viswanathan",
                "Zhoumianze Liu",
                "Yixin Liu",
                "Graham Neubig",
                "Pengfei Liu."
            ],
            "title": "DataLab: A platform for data analysis and intervention",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational",
            "year": 2022
        },
        {
            "authors": [
                "Marcos Zampieri",
                "Shervin Malmasi",
                "Preslav Nakov",
                "Sara Rosenthal",
                "Noura Farra",
                "Ritesh Kumar."
            ],
            "title": "Predicting the type and target of offensive posts in social media",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the",
            "year": 2019
        },
        {
            "authors": [
                "Guoyang Zeng",
                "Fanchao Qi",
                "Qianrui Zhou",
                "Tingji Zhang",
                "Bairu Hou",
                "Yuan Zang",
                "Zhiyuan Liu",
                "Maosong Sun."
            ],
            "title": "OpenAttack: An open-source textual adversarial attack toolkit",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for",
            "year": 2021
        }
    ],
    "sections": [
        {
            "text": "Disclaimer: This paper contains material that is offensive\nor hateful."
        },
        {
            "heading": "1 Introduction",
            "text": "Framing explanation processes as a dialogue between the human and the model has been motivated in many recent works from the areas of HCI and ML explainability (Miller, 2019; Lakkaraju et al., 2022; Feldhus et al., 2022; Hartmann et al., 2022; Weld and Bansal, 2019; Jacovi et al., 2023). With the growing popularity of large language models (LLMs), the research community has yet to present\na dialogue-based interpretability framework in the NLP domain that is both capable of conveying faithful explanations1 in human-understandable terms and is generalizable to different datasets, use cases and models.\n1While it might be tempting to use ChatGPT, we point out the black-box nature of proprietary software: Most interpretability methods require access to gradients, parameters or training data to make faithful explanations of their behavior. Lastly, it is not possible yet to connect other ML models to it for generating explanations.\nOne-off explanations can only tell a part of the overall narrative about why a model \u201cbehaves\u201d a certain way. Saliency maps from feature attribution methods can explain the model reasoning in terms of what input features are important for making a prediction (Feldhus et al., 2023), while counterfactuals and adversarial examples show how an input needs to be modified to cause a change in the original prediction (Wu et al., 2021). Semantic similarity and label distributions can shed a light on the data which was used to train the model (Shen et al., 2023), while rationales provide a natural language justification for a predicted label (Wiegreffe et al., 2022). These methods do not allow follow-up questions to clarify ambiguous cases, e.g. a most important token being a punctuation (Figure 1) (cf. Schuff et al. 2022), or build a mental model of the explained models.\nIn this work, we build a user-centered, dialoguebased explanation and exploration framework, INTERROLANG, for interpretability and analyses of NLP models. We investigate how the TALKTOMODEL (TTM, Slack et al. 2023) framework can be implemented in the NLP domain: Concretely, we define NLP-specific operations based on the aforementioned explanation types. Our system, INTERROLANG, allows users to interpret and analyze the behavior of language models interactively. We demonstrate the generalizability of INTERROLANG on three case studies \u2013 dialogue act classification, question answering, hate speech detection \u2013 for which we evaluate the intent recognition (parsing of natural language queries) capabilities of both finetuned (FLAN-T5, BERT with Adapter) and few-shot LLM (GPT-Neo). We find that an efficient Adapter\nsetup outperforms few-shot LLMs, but that this task of detecting a user\u2019s intent is far from being solved. In a subsequent human evaluation (\u00a75.3), we first collect subjective quality assessments on each response about the explanation types regarding four dimensions (correctness, helpfulness, satisfaction, fluency). We find a preference for mistakes summaries, performance metrics and free-text rationales. Secondly, we ask the participants about their impressions of the overall explanation dialogues. All of them were deemed helpful, although some (e.g., counterfactuals) have some potential for improvement. Finally, a second user study on simulatability (human forward prediction) provides first evidence for how various NLP explanation types can be meaningfully combined in dialogical settings. Attribution and rationales resulted in very high simulation accuracies and required the least number of turns on average, revealing a need for a longer conversation than single-turn explanations. We open-source our tool2 that can be extended to other models and NLP tasks alongside a dataset collected during the user studies including various operations and manual annotations for the user inputs (parsed texts): Free-text rationales and templatebased responses for the decisions of NLP models include explanations generated from interpretability methods, such as attributions, counterfactuals, and similar examples."
        },
        {
            "heading": "2 Methodology",
            "text": "TALKTOMODEL (Slack et al., 2023) is designed as a system for open-ended natural language dialogues\n2https://github.com/DFKI-NLP/InterroLang\nfor comprehending the behavior of ML models for tabular datasets (including only numeric and categorical features). Our system INTERROLANG retains most of its functionalities: Users can ask questions about many different aspects and slices of the data alongside predictions and explanations. INTERROLANG has three main components (depicted in Figure 2): A dialogue engine parses user inputs into an SQL-like programming language using either Adapters for intent classification or LLM that treats this task as a seq2seq problem, where user inputs are the source and the parses are the targets. An execution engine runs the operations in each parse and generates the natural language response. A text interface (Figure 4) lets users engage in open-ended dialogues and offers pre-defined questions that can be edited. This reduces the users\u2019 workload to deciding on what to ask, essentially."
        },
        {
            "heading": "2.1 Operations",
            "text": "We extend the set of operations in TTM (App. B), e.g. feature attribution and counterfactuals, towards linguistic questions, s.t. they can be used in NLP\nsettings and on Transformers. In Table 1, we categorize all INTERROLANG operations into Attribution, Perturbation, Rationalization, and Data.\nAttribution Feature attribution methods can quantify the importance of input tokens (Madsen et al., 2022) by taking the final predictions and intermediate representations of the explained model into account. Next to simple token-level attributions, we can aggregate them on sentence-level or present global top k attributed tokens across the entire dataset (R\u00f6nnqvist et al., 2022).\nPerturbation Perturbation methods come in many forms and have different purposes: We propose to include counterfactual generation, adversarial attacks and data augmentation as the main representatives for this category. While counterfactuals aim to edit an input text to cause a change in the model\u2019s prediction (Wu et al., 2021), adversarial attacks are about fooling the model to not guess the correct label (Ebrahimi et al., 2018). Data augmentation replaces spans in the input, keeping the outcome the same (Ross et al., 2022).\nRationalization Generating free-text rationales for justifying a model prediction in natural language has been a popular task in NLP (Camburu et al., 2018; Wiegreffe et al., 2022). Such natural language explanations are usually generated by either concatenating the input text with the prediction and then prompting a model to explain the prediction, or by jointly predicting and rationalizing. However, the task has not yet been explored within dialogue-based model interpretability tools.\nSimilarity Inspired by influence functions (Koh and Liang, 2017), this functionality returns a number of instances from the training data that are related to the (local) instance in question. Since influence functions are notoriously expensive to compute, as a proxy, we instead compute the semantic similarity to all other instances in the training data and retrieve the highest ranked instances."
        },
        {
            "heading": "2.2 Intent recognition",
            "text": "We follow TTM and write pairs of utterances and SQL-like parses that can be mapped to operations (Table 1) as well as templates that can be filled.\nWe propose a novel Adapter-based solution (Houlsby et al., 2019; Pfeiffer et al., 2020) for intent recognition and train a model which can classify intents representing the INTERROLANG operations (e.g., adversarial, counterfactual, etc.). We also train a separate Adapter model for the slot tagging, s.t. for each intent we can label the relevant slots. The slot types that can be recognized by the model include id, number, class_names, data_type, metric, include_token and sentence_level. The training details of the Adapter-based approach are listed in Table 9.3\nThe training data for intents are generated from the same prompts that are used for baselines (GPT-Neo and FLAN-T5-base) with the slot values randomly replaced by the actual values from the datasets (e.g., IDs, class names etc.). Some of the prompts are paraphrased to obtain more diverse training data. Adapter models for intents and slots are fine-tuned on top of the same bert-base-uncased model. The performance of\n3Some of the slots are crucial for the intent interpretation and cannot be omitted (e.g., id for the show operation) while other slots are optional and if not specified by the user the default value is chosen. We also implement additional checks for the case when the user input includes deictic expressions (e.g., \u201cthis\u201d in \u201cshow me a counterfactual for this sample\u201d) in which case the ID of the previous instance is selected.\nthis approach is compared to the prompt-based solution in Table 2."
        },
        {
            "heading": "2.3 Dialogue management",
            "text": "We add dialogue management in the form of parsing consecutive operations (Figure 2) and extend it with the ability to handle custom inputs and clarification questions.\nTTM, after translating user utterances into a grammar of production rules, composes its results in a template-filling manner while ensuring semantic coherence between multiple operations. They further argue that such a response generation approach prevents hallucinations commonly found in neural networks and conversational models (Dziri et al., 2022). However, it makes the dialogue less natural. That is why we also add a range of pre-defined responses for fallback that are chosen at random when applicable. Moreover, the GPTbased rationales are also the first example of a fully model-generated response. Our system also recognizes when the user just wants to acknowledge the bot\u2019s response or intends to finish the conversation and it generates the appropriate responses (see App. H for an example).\nWhen designing dialogue systems, the task of keeping track of the dialogue history is essential to better inform the selection of the next action or response. Thus, we store the previous operations and ids and can resolve deictic expressions like \u201cthis sample\u201d or \u201cit\u201d to the ID of the previously mentioned instance. We also check the prediction scores of the intent recognition module to see if there is some problem interpreting the user input, e.g., if several intents get very high scores INTERROLANG asks a clarification question to disambiguate between operations. Also, if we have an intent but some of its non-default slots are missing (not recognized) we can generate a clarification question to resolve it, e.g., \u201cCould you please specify for which instance I should provide a counterfactual?\u201d. This gives us more flexibility and makes the dialogue flow more natural."
        },
        {
            "heading": "3 NLP Models",
            "text": "We selected three use cases in NLP with BERT-type Transformer models trained on standard datasets, all of which we offer users to explore."
        },
        {
            "heading": "3.1 Dialogue Act classification",
            "text": "DailyDialog (Li et al., 2017) is a multi-turn dialogue dataset that covers different topics related to our daily life (e.g., shopping, discussing vacation trips etc.). All conversations are human-written and there are 13,118 dialogues in total with 8 turns per dialogue on average. We limit the training set to the first 1,000 dialogues, the development set to 100 and the test set to 300 dialogues.\nThe dialogue act labels annotated in the dataset are as follows: Inform, Question, Directive and Commissive (see Figure 3a for the distribution of labels). Inform is about providing information in the form of statements or questions. Question is used when the speaker wants to know something and actively asks for information. Directives are about requests, instructions, suggestions and acceptance or rejection of offers. Commissives are labeled when the speaker accepts or rejects requests or suggestions (Li et al., 2017). The Transformer model trained on DailyDialog achieves F1 score 68.7% on the test set after 5 epochs of training with 5e-6 learning rate."
        },
        {
            "heading": "3.2 Question answering",
            "text": "We choose BoolQ (Clark et al., 2019) as the representative dataset which has been analyzed in the explainability context in many works (DeYoung et al., 2020; Atanasova et al., 2020; Pezeshkpour et al., 2022, i.a.). Each of the 16k examples consists of a question, a paragraph from a Wikipedia article, the title of that article, and a \u201cyes\u201d/\u201cno\u201d answer.\nWe let its validation set (3.2k instances)4 be predicted by a fine-tuned DistilBERT (Sanh et al., 2019) model5 with an accuracy of 72.11%. We choose a smaller model, because it is more easily deployable and more error-prone which increases\n4The ground truth labels for the test set are not available. 5https://huggingface.co/andi611/\ndistilbert-base-uncased-qa-boolq\nthe need for explanations."
        },
        {
            "heading": "3.3 Hate speech detection",
            "text": "Hate speech detection is a challenging task to determine user entries on social media if offensive. While better models for hate speech detection are continuously being developed, there is little research on the acceptability aspects of hate speech models. There have been a few studies on this task in the explainability literature, mostly using attributions or binary highlights (Mathew et al., 2021; Balkir et al., 2022; Attanasio et al., 2022).\nOLID (Zampieri et al., 2019) is one of the common benchmark datasets and includes 14,100 tweets to be identified whether they are offensive. Each row in OLID consists of text and label and the label indicates if the twitter text is \u201coffensive\u201d or \u201cnon-offensive\u201d. A fine-tuned mbert-olid-en6 model is used to predict the validation set (2648 instances) and it can achieve an accuracy of 81.42%."
        },
        {
            "heading": "4 Interpretability and Analysis Components",
            "text": "For our implementation and experimental setup, we use the following tools and methods to realize the operations in Table 1:\nAttribution Slack et al. (2023) automatically select \u201cthe most faithful feature importance method for users, unless a user specifically requests a certain technique\u201d. We constrain feature importance to Integrated Gradients (Sundararajan et al., 2017) saliency scores that we obtain from CAPTUM (Miglani et al., 2023), which allows easy replacement with other saliency methods. The attributions are based on token-level as generated by the underlying model, e.g. BERT in our experiments. We also provide caching functionality to pre-compute and\n6https://huggingface.co/sinhala-nlp/ mbert-olid-en\nstore the scores, thus reducing the inference time and mitigating expensive reruns on static inputs.\nPerturbation For counterfactual generation, we use the official Hugging Face implementation of POLYJUICE (Wu et al., 2021)7. Adversarial examples are generated via OPENATTACK (Zeng et al., 2021)8, where we choose PWWS (Ren et al., 2019) as the attacker for our models on a single instance. For data augmentation we use the NLPAUG library9 and replace some tokens in the text based on their embedding similarity computed with the bert-based-cased model. The percentage of words that are augmented for each text is set to 0.3. We display the replaced words in bold, so that the user can easily distinguish between the original instance and the augmented one.\nRationalization As a baseline, we use the parsing model (GPTNeo) in a zero-shot setup to produce free-text explanations based on a concatenation of the input, the classification by the explained BERT-type model (Marasovic et al., 2022) and an instruction asking for an explanation. For an improved version, we produce plausible rationales from ChatGPT10 and then prompt a Dolly-v2-3B11 for few-shot rationales. The rationales are precomputed for all datasets.\nNatural language understanding For computing the semantic similarity, we embed the data point using Sentence Transformers (Reimers and Gurevych, 2019) and compute the cosine similarity to other points (excluding the instance in question) in the respective dataset. In order to retrieve frequent keywords from the whole dataset, we apply the stopwords set defined in NLTK (Bird, 2006) and get a word frequency set. The operation can then return the n most frequent keywords, with n being defined through the user query."
        },
        {
            "heading": "5 Evaluation",
            "text": "We conduct our evaluation based on parsing accuracy and two user studies. After introducing the partitions we used to obtain the parsing (intent recognition) results (\u00a75.2), we describe the setup\n7https://huggingface.co/uw-hai/polyjuice 8https://github.com/thunlp/OpenAttack 9https://github.com/makcedward/nlpaug\n10https://platform.openai.com/docs/ api-reference/chat, March 23 version\n11https://huggingface.co/databricks/ dolly-v2-3b\nof our human evaluation related to user experience and simulatability (\u00a75.3)."
        },
        {
            "heading": "5.1 Datasets",
            "text": "FLAN-T5-base and Adapter-based models are trained on the train set, which contains 505 pairs of user questions and prompts. We automatically extended the set for Adapter by filling in all possible slots with the values from the datasets (Fig. 9). The train set is a combination of manual creation by us and subsequent augmentation using ChatGPT. For evaluation, we created three more partitions (dev, dev-gpt, test) to evaluate the parsing accuracy, as presented in Table 2. The dev set has been manually created by us which consists of 102 pairs of user questions and parsed texts. To construct the dev-gpt set, we leverage ChatGPT to generate semantically similar examples extracted from dev set. The test set is obtained by collecting questions of participants who participated in the user study (\u00a75.3). Unlike TTM, our NLP datasets don\u2019t have a tabular format. Therefore, we had to adjust the parsing approach to be able to handle text inputs relevant to our NLP tasks."
        },
        {
            "heading": "5.2 Automated evaluation: Intent recognition",
            "text": "To answer the question of how well are user questions mapped onto the correct explanations and responses, for all three use cases, we compare the GPT-Neo-2.7B parsing proposed in Slack et al. (2023) with our novel Adapter-based solution (\u00a72.2) and also fine-tune a custom parsing model based on FLAN-T5-base (Chung et al., 2022)."
        },
        {
            "heading": "5.3 Human evaluation",
            "text": "Dialogue evaluation research has raised awareness of measuring flexibility and understanding among many other criteria. There exist automated metrics based on NLP models for assessing the quality of dialogues, but their correlation with human judgments needs to be improved on (Mehri et al., 2022; Siro et al., 2022). While TTM is focused on usability metrics (easiness, confidence, speed, likeliness to use), we target dialogue and explanation quality metrics."
        },
        {
            "heading": "5.3.1 Subjective ratings",
            "text": "A more precise way are user questionnaires (Kelly et al., 2009). We propose to focus on two types of questionnaires: Evaluating a user\u2019s experience (1) with one type of explanation (e.g. attribution), and (2) explanations in the context of the dialogue, with one type of downstream task (e.g., QA). An average of the second dimension will also provide a quality estimate for the overall system.\nConcretely, we let 10 students with computational linguistics and computer science backgrounds12 explore the tool and test out the available operations and then rate the following by giving a positive or negative review (Task A, App. F.1):\n1. Correctness (C), helpfulness (H) and satisfaction (S) on the single-turn-level\n12The participants of our user studies were recruited inhouse: All of them were already working as research assistants in our institute and are compensated monthly based on national regulations. None of them had any prior experience with the explained models.\n2. CHS and Fluency (F) on the dataset-level (when finishing the dialogue)"
        },
        {
            "heading": "5.3.2 Simulatability",
            "text": "We also conduct a simulatability evaluation (Task B, App. F.2), i.e. based on seeing an explanation and the original model input for a previously unseen instance. If a participant can correctly guess what the model predicted for that particular instance (which can also be a wrong classification) (Kim et al., 2016), the explanation they saw would be deemed helpful. We can then express an objective quality estimate of each type of explanation in terms of simulation accuracy, both in isolation and in combination with other explanations.\nEach participant (four authors of this paper + two students from Task A) received nine randomly chosen IDs (three from each dataset). The list of operations (Table 5) is randomized for each ID, serving as the itinerary. After each response, the participant can decide to either perform the simulation (take the guess) or continue with the next in the list. After deciding on a simulated label, they are tasked to assign one helpfulness rating to each operation: 1 = helpful; -1 = not helpful; 0 = unused. Let R be the set of all ratings ri \u0338= 0 and 1t(x) our indicator function. We then calculate our Helpfulness Ratio as follows:\nHelpfulness Ratio = \u2211\nr\u2208R 11(r) |R| .\nLet y\u0302i be the model prediction at index i and y\u0303i the user\u2019s guess on the model prediction, then the simulation accuracy is\nSim(all) = \u2211|R|\ni=1 1y\u0302i (y\u0303) |R| .\nFiltering for all cases where the operation was deemed helpful:\nSim(t = 1) = \u2211|R|\ni=1 1y\u0302i (y\u0303i)\u00b71t(ri) 1t(ri) ."
        },
        {
            "heading": "6 Results and discussion",
            "text": "Parsing accuracy Table 2 shows that our Adapter-based approach (slot tagging and intent recognition) is able to outperform both the GPT-Neo baseline and the fine-tuned FLAN-T5 models, using much fewer parameters and trained on the automatically augmented prompts with replaced slot values.\nHuman preferences Table 3 reveals that most operations were positively received, but there are large differences between the subjective ratings of operations across all three aspects (CHS). We find that data description, performance and mistakes operations consistently perform highly, indicating that they\u2019re essential to model understanding. Among the repertoire of explanation operations, free-text rationale scores highest on average, followed by augmentation and adversarial examples, while counterfactuals are at the bottom of the list. The POLYJUICE GPT was often not able to come up with a perturbation (flipping the label) at all and we see the largest potential of improvement in the choice for a counterfactual generator. The dialogue evaluation in Table 4 also solidifies the overall positive impressions. While BoolQ scored highest on Correctness, DailyDialog was the most favored in Helpfulness and Satisfaction. Fluency showed no differences, mostly because the generated texts are task-agnostic. Satisfaction was lowest across the three use cases. Although the operations were found to be helpful and correct, the satisfaction still leaves some room for improvements, likely due to high affordances (too much information at once) or low comprehensiveness. A more fine-grained evaluation (Siro et al., 2022) might reveal whether this can be attributed to presentation mode, explanation quality or erroneous parses.\nSimulatability Based on Table 5, we can observe that the results align with the conclusions drawn from Table 3. Specifically, free-text rationales provide the most assistance to users, while feature importance was a more useful operation for multiturn simulation, compared to single-turn helpfulness ratings. On the other hand, counterfactual and adversarial examples are found to be least helpful, supporting the findings of Task A. Thus, their results may not consistently satisfy users\u2019 expectations. We detected very few cases where one operation was sufficient. Combinations of explanations are essential: While attribution and ratio-\nnales are needed to let users form their hypotheses about the model\u2019s behavior, counterfactuals and adversarial examples can be sanity checks that support or counter them (Hohman et al., 2019). With Sim(t = 1), we detected that in some cases the explanations induced false trust and led the users to predict a different model output."
        },
        {
            "heading": "6.1 Dataset with our results",
            "text": "We compile a dataset from (1) our templates, (2) the automatically generated explanations, and (3) human feedback on the rationales presented through the interface. The research community can use these to perform further analyses and train more robust and human-aligned models. We collected 1449 dialogue turns from feedback files (Task A) and 188 turns from the simulatability study (Task B). We provide a breakdown in App. G."
        },
        {
            "heading": "7 Related Work",
            "text": "Dialogue systems for interpretability in ML Table 6 shows the range of existing natural language interfaces and conversational agents for explanations. Most notably, CONVXAI (Shen et al., 2023) very recently presented the first dialogue-based interpretability tool in the NLP domain. Their focus, however, is on the single task of LLMs as writing assistants. They also don\u2019t offer dataset exploration methods, their system is constrained to a single dataset (CODA-19) and they have not considered free-text rationalization, which we find is one of the most preferred types of operations. Dalvi Mishra et al. (2022) proposed an interactive system to provide faithful explanations using previous interactions as a feedback. Despite being interactive, it does not provide feasibility of generating rationales on multiple queries subsequently. Bertrand et al. (2023) wrote a survey on prior studies on \u201cdialogic XAI\u201d, while Fig. 6 of Jacovi et al. (2023) highlights that interactive interrogation is needed to construct complete explanation narratives: Feature attribution and counterfactuals complement each other, s.t. the users can build a generalizable mental model.\nVisual interfaces for interpretability in NLP LIT (Tenney et al., 2020), AZIMUTH (GauthierMelan\u00e7on et al., 2022), IFAN (Mosca et al., 2023) and WEBSHAP (Wang and Chau, 2023) offer a broad range of explanations and interactive analyses on both local and global levels. ROBUSTNESS GYM (Goel et al., 2021), SEAL (Rajani et al., 2022), EVALUATE (von Werra et al., 2022), INTERACTIVE\nMODEL CARDS (Crisan et al., 2022) and DATALAB (Xiao et al., 2022) offer model evaluation, dataset analysis and accompanying visualization tools in practice. There are overlaps with INTERROLANG in the methods they integrate, but none of them offer a conversational interface like ours.\nUser studies on NLP interpretability Most influential to our study design are simulatability evaluations (Hase and Bansal, 2020; Nguyen, 2018; Gonz\u00e1lez et al., 2021; Arora et al., 2022; Das et al., 2022; Feldhus et al., 2023). In terms of preference ratings, Strout et al. (2019) evaluated how extractive rationales (discretized attributions) from different models are rated by human annotators. Helpfulness and satisfaction ratings were used in Schuff et al. (2020) and Ray et al. (2019)."
        },
        {
            "heading": "8 Conclusion",
            "text": "We introduce our system, INTERROLANG, which is a user-centered dialogue-based system for exploring the NLP datasets and model behavior. This system enables users to engage in multi-turn dialogues. Based on the findings from our conducted user study, we have determined that one-off explanations alone are usually not sufficient or beneficial. In many cases, users may require multiple explanations to obtain accurate predictions and gain a better understanding of the system\u2019s output.\nFuture work includes making the bot more proactive, so that it can suggest new operations related to the user queries. We also want to investigate the feasibility of using a singular LLM for all tasks (parsing, prediction, explanation generation13, response generation) over the modular setup that we currently employ; Redesigning operations as API endpoints and training LLMs to call them (Lu et al., 2023; Schick et al., 2023), s.t. they can autonomously take care of the entire dialogue management at once. Lastly, refining language models (increasing faithfulness or robustness, aligning with user expectations) through dialogues has gained traction (Lee et al., 2023; Madaan et al., 2023). While we are already collecting valuable data, our framework misses an automated feedback loop to iteratively improve the models.\n13Operations have to be adapted in some cases, e.g., generating matrices for feature attribution (Sarti et al., 2023) and counterfactuals without an external library (Chen et al., 2023).\nLimitations\nINTERROLANG does not exhaust all interpretability methods, because understanding and integrating them requires a lot of resources. We see feature interactions, measurements of biases and component analysis as the most promising future work.\nINTERROLANG does not allow direct model comparison. The models are constrained to their datasets and the use cases are intended to be explored separately.\nUsers can enter custom inputs to get predicted and explained, but they can not modify the dataset on-the-fly, e.g., adding generated adversarial examples or augmentations directly to the current dataset and saving the updated version.\nWe do not offer a solution to mitigate biases or potential harmful effects of language models, but INTERROLANG with its range of explanations is intended to point users into directions where the training data or model behavior is counter-intuitive.\nWe use ChatGPT only for (1) producing highquality rationales to use in demonstrations (\u00a74) and (2) augmenting our intent recognition training data containing utterance-parse pairs (\u00a72.2). We argue that these are legitimate use cases of ChatGPT. For almost every other part of INTERROLANG, ChatGPT is not applicable, though (see Footnote 1). INTERROLANG is a modular system and one of our goals is to have all modules be sourced from readily available tools. ChatGPT can easily be swapped with a sufficiently strong rationalizer and data augmenter, as soon as they become available open source. At the time of implementing INTERROLANG, however, we found that there is a large qualitative gap between ChatGPT and open-source LLMs (Dolly, GPT-Neo) and that\u2019s why we opted to include it in these two parts of our framework.\nEthics Statement\nWe incorporate OLID as one of our datasets, which may contain hateful or offensive words. However, it is important to note that we do not generate any new content that is hateful or offensive. Our usage of the OLID dataset is solely for the purpose of assessing the integration of the hate speech detection task to our system and generating plausible and useful explanations."
        },
        {
            "heading": "Acknowledgments",
            "text": "We are indebted to Gokul Srinivasagan, Maximilian Dustin Nasert, Ammer Ayach, Christopher\nEbert, Urs Alexander Peter, David Meier, Jo\u00e3o Lucas Mendes de Lemos Lins, Tim Patzelt, Elif Kara and Natalia Skachkova for their invaluable work as annotators. We thank Leonhard Hennig, Malte Ostendorff, Jo\u00e3o Lucas Mendes de Lemos Lins and Maximilian Dustin Nasert for their review of earlier drafts and the reviewers of EMNLP 2023 for their helpful and rigorous feedback. This work has been supported by the German Federal Ministry of Education and Research as part of the projects XAINES (01IW20005) and CORA4NLP (01IW20010) and the European Union as part of the AviaTor project (SEP-210730802)."
        },
        {
            "heading": "A Explanatory dialogue systems",
            "text": "Table 6 and Table 7 show the range of existing natural language interfaces and conversational agents for explanations."
        },
        {
            "heading": "B TALKTOMODEL operations",
            "text": "Most TTM operations belonging to their ML, Conversation and Description categories can be trivially adapted. Here, we document the changes:\nDue to Transformers being explained instead of the much smaller scikit-learn models, we applied small changes such as pre-computing predictions (similar to the tricks we used for attributions and rationales).\nMetadata For metadata, we provide an operation following the basic idea of model cards (Mitchell et al., 2019) which supplies information related to model details, intended use of the model, etc., and, analogously, datasheets (Gebru et al., 2021) for training/test data documentation. User questions can target specific aspects of this structured information and the system replies in natural language and/or tabular formats.\nTable 8 shows the rest of the INTERROLANG operations not depicted by Table 1."
        },
        {
            "heading": "C Label distributions of NLP use cases",
            "text": "Figure 3 shows the label distributions of DailyDialog, OLID and BoolQ."
        },
        {
            "heading": "D Adapter training details",
            "text": "Table 9 shows the hyperparameters and training time for the Adapter models for dialogue act classification and slot tagging.\ncently. Task data Num = Numeric/Tabular. CV = Computer vision. Explained model AOG = And-Or graph. DT = Decision Tree. RF = Random Forest. CNN = Convolutional neural network. Tf = Transformer.\nconversational agents for XAI. Explanation types FA = Feature Attribution. CF = Counterfactual Generation.\nMt = Meta information about the model. Sim = Similar examples. RG = Rationale generation. Intent recognition Comm = Commercial product (RASA = RASA NLU; DiF = Google DialogFlow). Embeds = Nearest neighbor based on sentence embedding. Response generation / Dialogue state tracking Rule = Rule- and template-based\nresponse. Evaluation : Automated: ExM = Exact match accuracy. Human: Like = Likert-scale rating.\nFi lte\nrs filter(id) Access single instance by its ID includes(token) Filter instances by token occurrence\nPr ed\nic tio n predict(instance)* Get the prediction of the given instance predict(dataset) Get the prediction distribution across the dataset likelihood(instance) Obtain the given instance\u2019s probability for each class mistakes(dataset) Count number of wrongly predicted instances score(dataset, metric) Determine the relation between predictions and labels\nD at a show(list) Showcase a list of instance countdata(list) Count number of instances within the given list label(dataset) Describe the label distribution across the dataset M et a data(dataset) Information related to training/test data model() Metadata of the model\nA bo\nut function() Inform the functionality of the system self() Self-introduction\nL og ic and(op1, op2) Concatenation of multiple operations or(op1, op2) Selection of multiple filters\nTable 8: TTM operations used in INTERROLANG. *Prediction operation provides support for custom input instances received from users.\nE Interface\nWe extend the TTM interface (Slack et al., 2023) in the following ways: \u2022 Custom inputs: Compared to TTM, which only\nallows user to use instances from three predefined datasets, we provide a selection box that allows individual inputs from the user to be considered. \u2022 Text search: A search engine that allows the user to filter the dataset according to strings. If a query is present, subsequent operations will consider the subset where this filter is applicable. \u2022 Dataset viewer: This shows the first ten instances of the dataset (their IDs and the contents of the text fields) at the start, but in order to make the navigation through the data easier for the user, it will update according to both string filters and\noperations like label filters."
        },
        {
            "heading": "F Annotation instructions",
            "text": "F.1 Task A Figure 5 and Figure 6 show the instructions of the user study on subjective ratings (Task A) as described in \u00a75.3.1. Figure 7 shows a screenshot of the Google Forms in Task A2.\nF.2 Task B Figure 8 shows the instructions of the user study on simulatability described in \u00a75.3.2.\nG INTERROLANG Dataset statistics\nAcross all three datasets we have 659 unique user questions that don\u2019t overlap with the INTERROLANG sample prompts (81.16%) and 153 questions that do overlap. The high number indicates that our prompts approximate the actual user questions rather well. On the other hand, some of the user questions were taken directly from the prompt examples.\nIn particular, OLID has 180 (61.2%) unique user questions with 114 overlaps; DailyDialog has 208 (69.3%) unique user questions and 92 overlaps; BoolQ has 192 (88.1%) unique user questions, 26 overlaps. Across all three datasets this results in 478 unique questions (58.9%) and 334 overlapping ones.\nH Sample Dialogue (BoolQ with Adapter)\nUser: Hi! Which kind of a model do you use?\nUser: And what is the dataset?\nUser: Can you show me how often the model makes incorrect predictions?\nUser: Can you show me some examples of the mistakes?\nUser: Ok, great! What about the id 42? Can you show me this sample?\nUser: Please show me the token attributions for this id.\nUser: Can you do an adversarial attack?\nUser: Ok, thanks! Looks good :)\nUser: What would be the counterfactual for this instance?\nUser: Can you show me the most important features overall (across all data )?\nUser: Ok, I think that\u2019s it for today. Bye!"
        }
    ],
    "title": "INTERROLANG: Exploring NLP Models and Datasets through Dialogue-based Explanations",
    "year": 2023
}