{
    "abstractText": "Developing an educational test can be expensive and time-consuming, as each item must be written by experts and then evaluated by collecting hundreds of student responses. Moreover, many tests require multiple distinct sets of questions administered throughout the school year to closely monitor students\u2019 progress, known as parallel tests. In this study, we focus on tests of silent sentence reading efficiency, used to assess students\u2019 reading ability over time. To generate high-quality parallel tests, we propose to fine-tune large language models (LLMs) to simulate how previous students would have responded to unseen items. With these simulated responses, we can estimate each item\u2019s difficulty and ambiguity. We first use GPT-4 to generate new test items following a list of expert-developed rules and then apply a fine-tuned LLM to filter the items based on criteria from psychological measurements. We also propose an optimal-transport-inspired technique for generating parallel tests and show the generated tests closely correspond to the original test\u2019s difficulty and reliability based on crowdworker responses. Our evaluation of a generated test with 234 students from grades 2 to 8 produces test scores highly correlated (r=0.93) to those of a standard test form written by human experts and evaluated across thousands of K-12 students.",
    "authors": [
        {
            "affiliations": [],
            "name": "Eric Zelikman"
        },
        {
            "affiliations": [],
            "name": "Wanjing Anya Ma"
        },
        {
            "affiliations": [],
            "name": "Jasmine E. Tran"
        },
        {
            "affiliations": [],
            "name": "Diyi Yang"
        },
        {
            "affiliations": [],
            "name": "Jason D. Yeatman"
        },
        {
            "affiliations": [],
            "name": "Nick Haber"
        }
    ],
    "id": "SP:5e74518b70edc5c3e665260f7dbdd59ff699acfd",
    "references": [
        {
            "authors": [
                "Ghodai Abdelrahman",
                "Qing Wang",
                "Bernardo Nunes."
            ],
            "title": "Knowledge tracing: A survey",
            "venue": "ACM Computing Surveys, 55(11):1\u201337.",
            "year": 2023
        },
        {
            "authors": [
                "Manish Agarwal",
                "Rakshit Shah",
                "Prashanth Mannem."
            ],
            "title": "Automatic question generation using discourse cues",
            "venue": "Proceedings of the Sixth Workshop on Innovative Use of NLP for Building Educational Applications, pages 1\u20139.",
            "year": 2011
        },
        {
            "authors": [
                "Ronald D Armstrong",
                "Douglas H Jones",
                "Zhaobo Wang."
            ],
            "title": "Automated parallel test construction using classical test theory",
            "venue": "Journal of Educational Statistics, 19(1):73\u201390.",
            "year": 1994
        },
        {
            "authors": [
                "Ronald D Armstrong",
                "Douglas H Jones",
                "Ing-Long Wu."
            ],
            "title": "An automated test development of parallel tests from a seed test",
            "venue": "Psychometrika, 57:271\u2013288.",
            "year": 1992
        },
        {
            "authors": [
                "Marc Benzahra",
                "Fran\u00e7ois Yvon."
            ],
            "title": "Measuring text readability with machine comprehension: a pilot study",
            "venue": "Workshop on Building Educational Applications Using NLP, pages 412\u2013422.",
            "year": 2019
        },
        {
            "authors": [
                "Christy L Bloomquist."
            ],
            "title": "An examination of the relationship of oral reading fluency, silent reading fluency, reading comprehension, and the Colorado State Reading Assessment",
            "venue": "Utah State University.",
            "year": 2017
        },
        {
            "authors": [
                "Amy Burkhardt",
                "Maya. Yablonski",
                "Jamie Mitchell",
                "Liesbeth Gijbels",
                "Jason D. Yeatman."
            ],
            "title": "Developing items for a silent reading efficiency task (2023 ncme)",
            "venue": "Conference presentation.",
            "year": 2023
        },
        {
            "authors": [
                "Maria Chinkina",
                "Detmar Meurers."
            ],
            "title": "Question generation for language learning: From ensuring texts are read to supporting learning",
            "venue": "Proceedings of the 12th workshop on innovative use of NLP for building educational applications, pages 334\u2013344.",
            "year": 2017
        },
        {
            "authors": [
                "Karl Cobbe",
                "Vineet Kosaraju",
                "Mohammad Bavarian",
                "Mark Chen",
                "Heewoo Jun",
                "Lukasz Kaiser",
                "Matthias Plappert",
                "Jerry Tworek",
                "Jacob Hilton",
                "Reiichiro Nakano"
            ],
            "title": "Training verifiers to solve math word problems",
            "venue": "arXiv preprint arXiv:2110.14168",
            "year": 2021
        },
        {
            "authors": [
                "Tim Dettmers",
                "Mike Lewis",
                "Younes Belkada",
                "Luke Zettlemoyer"
            ],
            "title": "2022a. Llm.int8(): 8-bit matrix multiplication for transformers at scale. NeurIPS",
            "year": 2022
        },
        {
            "authors": [
                "Tim Dettmers",
                "Mike Lewis",
                "Sam Shleifer",
                "Luke Zettlemoyer."
            ],
            "title": "8-bit optimizers via block-wise quantization",
            "venue": "9th International Conference on Learning Representations, ICLR.",
            "year": 2022
        },
        {
            "authors": [
                "Tim Dettmers",
                "Artidoro Pagnoni",
                "Ari Holtzman",
                "Luke Zettlemoyer."
            ],
            "title": "Qlora: Efficient finetuning of quantized llms",
            "venue": "arXiv preprint arXiv:2305.14314.",
            "year": 2023
        },
        {
            "authors": [
                "Benjamin W Domingue",
                "Madison Dell",
                "David Lang",
                "Rebecca Silverman",
                "Jason Yeatman",
                "Heather Hough."
            ],
            "title": "The effect of covid on oral reading fluency during the 2020\u20132021 academic year",
            "venue": "AERA Open, 8:23328584221120254.",
            "year": 2022
        },
        {
            "authors": [
                "Xinya Du",
                "Junru Shao",
                "Claire Cardie."
            ],
            "title": "Learning to ask: Neural question generation for reading comprehension",
            "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1342\u20131352,",
            "year": 2017
        },
        {
            "authors": [
                "Yo Ehara."
            ],
            "title": "Building an english vocabulary knowledge dataset of japanese english-as-a-secondlanguage learners using crowdsourcing",
            "venue": "Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018).",
            "year": 2018
        },
        {
            "authors": [
                "Jean Feydy",
                "Thibault S\u00e9journ\u00e9",
                "Fran\u00e7ois-Xavier Vialard",
                "Shun-ichi Amari",
                "Alain Trouve",
                "Gabriel Peyr\u00e9."
            ],
            "title": "Interpolating between optimal transport and mmd using sinkhorn divergences",
            "venue": "The 22nd International Conference on Artificial Intelligence and",
            "year": 2019
        },
        {
            "authors": [
                "Ronald A. Fisher."
            ],
            "title": "Statistical methods for research workers",
            "venue": "Oliver and Boyd.",
            "year": 1925
        },
        {
            "authors": [
                "Michael Flor",
                "Brian Riordan."
            ],
            "title": "A semantic role-based approach to open-domain automatic question generation",
            "venue": "Proceedings of the Thirteenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 254\u2013263, New Orleans,",
            "year": 2018
        },
        {
            "authors": [
                "Lynn S Fuchs",
                "Douglas Fuchs",
                "Michelle K Hosp",
                "Joseph R Jenkins."
            ],
            "title": "Oral reading fluency as an indicator of reading competence: A theoretical, empirical, and historical analysis",
            "venue": "The Role of Fluency in Reading Competence, Assessment, and",
            "year": 2001
        },
        {
            "authors": [
                "Xinyang Geng",
                "Hao Liu."
            ],
            "title": "Openllama: An open reproduction of llama",
            "venue": "Github.",
            "year": 2023
        },
        {
            "authors": [
                "Wade M Gibson",
                "John A Weiner."
            ],
            "title": "Generating random parallel test forms using ctt in a computerbased environment",
            "venue": "Journal of Educational Measurement, 35(4):297\u2013310.",
            "year": 1998
        },
        {
            "authors": [
                "Tanja Heck",
                "Detmar Meurers."
            ],
            "title": "Parametrizable exercise generation from authentic texts: Effectively targeting the language means on the curriculum",
            "venue": "Proceedings of the 17th Workshop on Innovative Use of NLP for Building Educational Applications (BEA",
            "year": 2022
        },
        {
            "authors": [
                "Edward J Hu",
                "Yelong Shen",
                "Phillip Wallis",
                "Zeyuan Allen-Zhu",
                "Yuanzhi Li",
                "Shean Wang",
                "Lu Wang",
                "Weizhu Chen."
            ],
            "title": "Lora: Low-rank adaptation of large language models",
            "venue": "International Conference on Learning Representations.",
            "year": 2022
        },
        {
            "authors": [
                "Miroslava M Ignjatovi\u0107",
                "Dragan M Boji\u0107",
                "Igor I Tartalja."
            ],
            "title": "A survey on problem formulations and (meta) heuristic-based solutions in automated assembly of parallel test forms",
            "venue": "International Journal of Software Engineering and Knowledge Engineering,",
            "year": 2021
        },
        {
            "authors": [
                "E.S. Johnson",
                "J.L. Pool",
                "D.R. Carter."
            ],
            "title": "Validity evidence for the test of silent reading efficiency and comprehension (tosrec)",
            "venue": "Assessment for Effective Intervention, 37(1):50\u201357.",
            "year": 2011
        },
        {
            "authors": [
                "Stanley Uros Keller."
            ],
            "title": "Automatic generation of word problems for academic education via natural language processing (nlp)",
            "venue": "arXiv preprint arXiv:2109.13123.",
            "year": 2021
        },
        {
            "authors": [
                "Young-Suk Kim",
                "Richard K Wagner",
                "Elizabeth Foster."
            ],
            "title": "Relations among oral reading fluency, silent reading fluency, and reading comprehension: A latent variable study of first-grade readers",
            "venue": "Scientific Studies of Reading, 15(4):338\u2013362.",
            "year": 2011
        },
        {
            "authors": [
                "Young-Suk Kim",
                "Richard K. Wagner",
                "Dina Lopez."
            ],
            "title": "Developmental relations between reading fluency and reading comprehension: A longitudinal study from grade 1 to grade 2",
            "venue": "Journal of Experimental Child Psychology, 113(1):93\u2013111.",
            "year": 2012
        },
        {
            "authors": [
                "Frederic M Lord."
            ],
            "title": "Applications of item response theory to practical testing problems",
            "venue": "Routledge.",
            "year": 1980
        },
        {
            "authors": [
                "Matej Martinc",
                "Senja Pollak",
                "Marko Robnik\u0160ikonja."
            ],
            "title": "Supervised and unsupervised neural approaches to text readability",
            "venue": "Computational Linguistics, 47(1):141\u2013179.",
            "year": 2021
        },
        {
            "authors": [
                "Ruslan Mitkov",
                "Le An Ha."
            ],
            "title": "Computer-aided generation of multiple-choice tests",
            "venue": "Proceedings of the HLT-NAACL 03 Workshop on Building Educational Applications Using Natural Language Processing, pages 17\u201322.",
            "year": 2003
        },
        {
            "authors": [
                "OpenAI."
            ],
            "title": "Gpt-4 technical report",
            "venue": "arXiv.",
            "year": 2023
        },
        {
            "authors": [
                "Chris Piech",
                "Jonathan Bassen",
                "Jonathan Huang",
                "Surya Ganguli",
                "Mehran Sahami",
                "Leonidas J Guibas",
                "Jascha Sohl-Dickstein."
            ],
            "title": "Deep knowledge tracing",
            "venue": "Advances in neural information processing systems, 28.",
            "year": 2015
        },
        {
            "authors": [
                "Manav Rathod",
                "Tony Tu",
                "Katherine Stasaski."
            ],
            "title": "Educational multi-question generation for reading comprehension",
            "venue": "Proceedings of the 17th Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2022), pages 216\u2013223.",
            "year": 2022
        },
        {
            "authors": [
                "Nils Reimers",
                "Iryna Gurevych."
            ],
            "title": "Sentence-bert: Sentence embeddings using siamese bert-networks",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics.",
            "year": 2019
        },
        {
            "authors": [
                "Shibani Santurkar",
                "Esin Durmus",
                "Faisal Ladhak",
                "Cinoo Lee",
                "Percy Liang",
                "Tatsunori Hashimoto"
            ],
            "title": "Whose opinions do language models reflect",
            "venue": "International Conference on Machine Learning",
            "year": 2023
        },
        {
            "authors": [
                "Sarah E Schwarm",
                "Mari Ostendorf."
            ],
            "title": "Reading level assessment using support vector machines and statistical language models",
            "venue": "Proceedings of the 43rd annual meeting of the Association for Computational Linguistics (ACL\u201905), pages 523\u2013530.",
            "year": 2005
        },
        {
            "authors": [
                "Burr Settles",
                "Geoffrey T. LaFlair",
                "Masato Hagiwara."
            ],
            "title": "Machine learning\u2013driven language assessment",
            "venue": "Transactions of the Association for computational Linguistics, 8:247\u2013263.",
            "year": 2020
        },
        {
            "authors": [
                "Luo Si",
                "Jamie Callan."
            ],
            "title": "A statistical model for scientific readability",
            "venue": "Proceedings of the tenth international conference on Information and knowledge management, pages 574\u2013576.",
            "year": 2001
        },
        {
            "authors": [
                "Philipp Sonnleitner."
            ],
            "title": "Using the lltm to evaluate an item-generating system for reading comprehension",
            "venue": "Psychology Science Quarterly, 50(3):345\u2013362.",
            "year": 2008
        },
        {
            "authors": [
                "Megha Srivastava",
                "Noah Goodman."
            ],
            "title": "Question generation for adaptive education",
            "venue": "Association for Computational Linguistics.",
            "year": 2021
        },
        {
            "authors": [
                "Katherine Stasaski",
                "Manav Rathod",
                "Tony Tu",
                "Yunfang Xiao",
                "Marti A Hearst."
            ],
            "title": "Automatically generating cause-and-effect questions from passages",
            "venue": "Proceedings of the 16th Workshop on Innovative Use of NLP for Building Educational Applications, pages",
            "year": 2021
        },
        {
            "authors": [
                "A Jackson Stenner."
            ],
            "title": "Measuring reading comprehension with the lexile framework",
            "venue": "Explanatory Models, Unit Standards, and Personalized Learning in Educational Measurement: Selected Papers by A. Jackson Stenner, pages 63\u201388. Springer.",
            "year": 2022
        },
        {
            "authors": [
                "Koun-Tem Sun",
                "Yu-Jen Chen",
                "Shu-Yen Tsai",
                "ChienFen Cheng."
            ],
            "title": "Creating irt-based parallel test forms using the genetic algorithm method",
            "venue": "Applied measurement in education, 21(2):141\u2013161.",
            "year": 2008
        },
        {
            "authors": [
                "Hugo Touvron",
                "Thibaut Lavril",
                "Gautier Izacard",
                "Xavier Martinet",
                "Marie-Anne Lachaux",
                "Timoth\u00e9e Lacroix",
                "Baptiste Rozi\u00e8re",
                "Naman Goyal",
                "Eric Hambro",
                "Faisal Azhar"
            ],
            "title": "Llama: Open and efficient foundation language models",
            "year": 2023
        },
        {
            "authors": [
                "Julia White",
                "Amy Burkhardt",
                "Jason Yeatman",
                "Noah Goodman."
            ],
            "title": "Automated generation of sentence reading fluency test items",
            "venue": "Proceedings of the Annual Meeting of the Cognitive Science Society, volume 44.",
            "year": 2022
        },
        {
            "authors": [
                "Susan Zhang",
                "Stephen Roller",
                "Naman Goyal",
                "Mikel Artetxe",
                "Moya Chen",
                "Shuohui Chen",
                "Christopher Dewan",
                "Mona Diab",
                "Xian Li",
                "Xi Victoria Lin"
            ],
            "title": "Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068",
            "year": 2022
        },
        {
            "authors": [
                "Banghua Zhu",
                "Hiteshi Sharma",
                "Felipe Vieira Frujeri",
                "Shi Dong",
                "Chenguang Zhu",
                "Michael I Jordan",
                "Jiantao Jiao."
            ],
            "title": "Fine-tuning language models with advantage-induced policy alignment",
            "venue": "arXiv preprint arXiv:2306.02231.",
            "year": 2023
        },
        {
            "authors": [
                "Bowei Zou",
                "Pengfei Li",
                "Liangming Pan",
                "Aiti Aw."
            ],
            "title": "Automatic true/false question generation for educational purpose",
            "venue": "Proceedings of the 17th Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2022), pages 61\u201370.",
            "year": 2022
        },
        {
            "authors": [
                "Feydy"
            ],
            "title": "2019)\u2019s differentiable optimal transport library, geomloss, but unbalanced problems require a reach hyperparameter",
            "year": 2019
        },
        {
            "authors": [
                "F Item"
            ],
            "title": "Information Analysis By fitting a 2-parameter logistic item response theory model to the student responses to both the Lab and AI-filtered test forms, we obtain the Fisher information",
            "venue": "(Fisher,",
            "year": 1925
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Developing an educational test can be resourceintensive and time-consuming, as each item must be written by experts and then evaluated by collecting hundreds of student responses. This process of evaluating items in terms of properties like difficulty and their ability to discriminate between student abilities, known in psychometrics as item calibration, is fundamental to test development.\nFurthermore, schools often require the creation of multiple, distinct test forms (a collection of\n*These authors contributed equally to this work\nunique items) administered throughout the academic year, allowing for close monitoring of student progress while minimizing practice effects. These test forms, designed to be content-equivalent and reliably produce identical individual scores as the original test form, are known as parallel tests. Each step of the test development process, from expert item writing to extensive item calibration through large-scale data collection and ultimately parallel test creation and validation, poses significant demands in terms of resources and time. These challenges emphasize the necessity for an automated and efficient test development framework.\nIn response to the challenge of item development, many works have proposed leveraging language models to generate items for educational assessments and instruction (Agarwal et al., 2011; Stasaski et al., 2021; Srivastava and Goodman, 2021; Heck and Meurers, 2022; Rathod et al., 2022;\nZou et al., 2022; White et al., 2022). However, estimating the relevance, quality, and difficulty of these generated items and test forms is an open challenge that must be carefully addressed for both psychometric and NLP communities as well as stakeholders in our education system. We propose an itemresponse simulator, training LLMs with student response data and simulating the responses of past participants on new items in order to calibrate them.\nAs a case study, we apply this method to develop and calibrate new items for a silent Sentence Reading Efficiency (SRE) task (Burkhardt et al., 2023), which we elaborate on in Section 2. We then propose an optimal-transport-inspired technique for generating parallel test forms with the new items by referring to a well-calibrated human-written test form. In doing so, we make the following contributions:\n1. We address a novel task of automated item calibration for sentence reading efficiency, requiring a model capable of predicting both responses and response times. 2. We propose fine-tuning LLMs to estimate the properties of unseen items by simulating how past participants would have responded. 3. We demonstrate the effectiveness of marginalizing response-conditioned LLM predictions over a distribution of past participants to estimate their responses to new questions. 4. By automatically creating parallel test forms, we deploy our system to K-12 education and demonstrate its high quality through largescale (n = 234) student evaluation.\nOverall, our work advances the measurement of reading efficiency by introducing scalable methods to generate test items. We address the novel task of parallel test generation without collected responses\nand approach parallel test construction with simulated responses as relaxed optimal transport.\nChildren can be sad. True (Response time: slow)\nYou sleep on a log. False (Response time: slow)\n[...]\nSweaters can be made of coal. False (Response time: very slow)\nYou can feed a hamster. False (Response time: very slow)\nYou can fill a balloon with air.\nFigure 3: Example prompt used for training and simulation. Given a prompt similar to this, the item-response simulator predicts the relevant item parameters \u2013 in our case, the student response and their log-response time."
        },
        {
            "heading": "2 Silent Sentence Reading Efficiency",
            "text": "How can we measure students\u2019 reading abilities? Oral Reading Fluency (ORF) is a widely used measure in research and practice, measuring words read correctly per minute (Fuchs et al., 2001; Domingue et al., 2022). Recently, researchers have examined the relationship between oral and silent reading fluency (Kim et al., 2011) and shifted focus to silent reading, as it is the most common form of reading for K-12 students. However, given the lack of observable verbal responses, it is a more challenging measurement task. Our task, silent Sentence Reading Efficiency (SRE), is an online, selfadministrated measure assessing the speed with which a student can read simple English sentences (Burkhardt et al., 2023). Modeled after other standardized silent reading fluency measures such as Test of Silent Reading Efficiency and Comprehension (TOSREC) (Johnson et al., 2011; Kim et al., 2012), SRE requires students to read sentences and respond whether each is True or False, which we\nrefer to as the item truth value (see Fig. 3). A student responds to as many sentences as they can within three minutes, and the final score is the number of correctly answered sentences minus incorrectly answered ones. Unlike TOSREC, SRE targets reading efficiency with less emphasis on reading comprehension. This complicates item development, requiring syntactically and semantically simple sentences with only vocabulary that all schoolage students should be able to read and understand.\nWe focus on SRE for three reasons. First, from an educational perspective, there is a high demand from schools to closely monitor students\u2019 reading development. Thus, it is important to develop diverse parallel test forms with identical difficulty and reliability. Manually authoring thousands of items and collecting the data to match test forms is extremely time-consuming and resource-intensive. Second, from a psychological measurement standpoint, SRE is a task where both accuracy and response time are crucial in determining ability. This characteristic does not align well with classic psychometric models, such as Item Response Theory (Lord, 1980), which focus on accuracy. Third, of particular relevance to the NLP community, measuring sentence-level reading fluency rather than comprehension poses a novel challenge, as traditional readability metrics and linguistic features fail to predict fluency accurately."
        },
        {
            "heading": "3 Training Dataset",
            "text": "We collect student data from 1st grade to high school by working closely with more than 30 diverse school partners in the United States for two school years (See Fig. 5 for breakdown by grade level). All data is collected under IRB guidelines. As part of the SRE validation study (Burkhardt et al., 2023), each student completed two 3-minute\nblocks with different test forms: one with TOSREC grade-specific sentences (maximum number of items varies by grade, but less than 70), and the other the Lab test form, consisting of 130 sentences curated by two human experts through an iterative design process. After filtering students with response times indicative of random guessing (median response times under 500 ms; n = 108), the final fine-tuning dataset includes 1962 participants with 183,782 responses. Fig. 4 indicates that the total score (i.e., the reading efficiency) of each student is more correlated with their median response time than how accurately they respond to each question, which underscores the importance of incorporating response time modeling to capture item difficulty during the item calibration process."
        },
        {
            "heading": "4 Methods",
            "text": ""
        },
        {
            "heading": "4.1 Item Generation",
            "text": "We use GPT-4 (OpenAI, 2023) to generate diverse sentences through zero-shot prompting and avoided giving specific examples. We first generate universally true sentences (temperature = 1, six completions, with at most 5000 tokens generated) and then transform each true sentence into a corresponding\nfalse sentence by asking the model to change 1 or 2 verbs, nouns, or adjectives. After several rounds of iteration, we developed the following prompt \u2014 each rule after the third was added iteratively in response to observed failure cases:\nGenerate 150 sentences. Rules: 1) The sentences have different length between 3 to 15 words. 2) 1st grade students should be able to read and understand the sentences. 3) Use only Kindergarten and high\nfrequency vocabulary words to generate the sentences.\n4) Make sure that each sentence is a sentence stating a universal fact that is immediately , obviously true. 5) If humans are used as the subject in a sentence , make sure it states a life experience that is true for everyone. 6) The sentence should not require any inferential comprehension skills to read nor understand. 7) The sentences should not be subjective nor describe opinions. 8) The sentences should not be centric to any country nor culture. 9) The sentences should be very diverse.\nExcluding unexpected generation output (e.g., blank strings) and exact duplicates, this prompt gave us 761 true sentences. However, we found it generated few long sentences even though \u201c3 to 15 words\u201d was specified, so we prompted the model to generate an additional 100 sentences with an additional rule specifying \u201cat least 10 words in each sentence.\u201d We then used a prompt to transform each true sentence into a corresponding false sentence: Transform each of the following sentences into false sentences by changing 1 or 2 verbs, nouns, or adjectives, and the sentences should be universally false. Ultimately, this produced a corpus with 861 true and 861 false sentences."
        },
        {
            "heading": "4.2 Item Evaluation",
            "text": "For training, we fine-tuned an LLM to predict student responses conditioned on previous responses, which we refer to as an item-response simulator. We train on a manually authored, anonymized corpus of 1,962 participants collected by the lab, ranging from 1st grade to adulthood, containing response times and responses (i.e., true or false), described in Section 3. Specifically, each training example consists of a randomly-selected subset of a sampled participant\u2019s responses, which are arranged in random order. The model was then trained to predict the response and response time\nof the final item, conditioned on the previous items, as visualized in Figure 3. In our final model configuration, we use Low-Rank Adaptation (LoRA) (Hu et al., 2022) on a 13-billion parameter LLaMA model (Touvron et al., 2023) with 8-bit weights (Dettmers et al., 2022a,b) and a substantial 0.5 dropout on the adapter layers and final layer to mitigate overfitting. We include more detail on hyperparameters considered in Appendix E and discuss all differences between the hyperparameters used for the crowdworker and school student evaluations.\nTo apply this model for evaluating new items, we sampled previous participants and insert the generated item as the final item in a randomly sampled subset of their responses. Then, aggregating over many sampled participants per item, we calculate a mean and standard deviation response time for each sentence, as well as the expected proportion of simulated responses that were true or false for each sentence. Figure 6 visualizes the probabilities and response times simulated by the item-response simulator for the generated items, aggregated over a hundred sets of sampled student responses. Crucially, modeling the probabilities allows the simulator to identify ambiguous items \u2013 items for which a nontrivial percent of responses are expected to be incorrect. In Figure 6, false items with higher probabilities and true items with lower probabilities are more ambiguous. We include a qualitative analysis in Appendix A.2."
        },
        {
            "heading": "4.3 Parallel Test Form Construction",
            "text": "We first simulate student responses to both the lab test form and the generated items and try to identify a set of generated items that corresponded well to the lab test form. However, it is important\nto be able to generate multiple distinct test sets \u2013 schools require multiple distinct parallel test forms to be administered throughout the year. A naive strategy would greedily pair the most similar items and then remove them when selecting new items for an additional test. But, this would ensure that successive tests would be gradually less similar to the original test and cannot ensure diverse items in each test. Instead, we duplicate the lab test form once per desired parallel test and find the best pairing between the lab and generated items. This is an unbalanced, constrained optimal transport problem, which we solve as follows: we first assign each duplicated lab item a probability of corresponding to a generated item, with no probability of true items being paired to false items and a term subtracted from the logits proportional to the distance between the lab and generated items.\nWe then minimize 1) the expected distances between the lab items and their paired generated items in the selected parameters (i.e., simulated response time), 2) the semantic similarity (over a threshold) within each copy of the dataset, and 3) the probability that the same generated item would be selected multiple times. This is a non-convex optimization problem, but by initializing the logits to reasonable initial values (e.g., logits proportional to distance), we found that the resulting simulated optimized tests converged and corresponded closely to the parameters that our model simulated for the labgenerated items. Figure 11 visualizes these simulated scores across two simultaneously generated, distinct test sets, as well as the ambiguous set of items for reference when optimizing for both accuracy and response time. Precisely, we sum over:\n\u2113distance = d\u2211\na=1 n\u2211 i=1 m\u2211 j=1 PaijDaij , (1)\n\u2113reuse = d\u2211\na=1 d\u2211 b=1 m\u2211 i=1 m\u2211 j=1,j \u0338=i Pa\u00b7i \u00b7 Pb\u00b7j , (2)\n\u2113cosine = d\u2211\na=1 n\u2211 i=1 n\u2211 j=1,i \u0338=j PaiCijPaj , (3)\nfor P \u2208 Rd\u00d7n\u00d7m, D \u2208 Rn\u00d7m, C \u2208 Rn\u00d7n where n is the number of generated items and m is the number of lab test form items, and d is the number of test copies to optimize. P is the probability that a lab test form item will be mapped to a given generated item, D is the pairwise distance between the lab and generated item parameters, and C is the semantic similarity between generated items.\nNote that we only apply the optimal transport algorithm in crowdworker experiments, as the aim of the school student evaluation is primarily to demonstrate the usefulness of the item-response simulator in a collaborative setting, with more detail on the human-in-the-loop filtering included in Section 5.3. However, in future school deployments, we will use this algorithm, allowing us to jointly handle multiple optimization criteria. Because of the human-inthe-loop filtering setup, it was necessary to instead perform deduplication separately for the school student evaluation, discussed in Appendix C. We discuss implementation details further in Appendix E, and the full pipeline in Figure 2."
        },
        {
            "heading": "4.4 Additional Filtering",
            "text": "For the crowd-worker experiments, we filter the dataset for safety using GPT-4, discussed in Appendix D. For the school experiment, we manually review the questions for safety and ambiguity out of an abundance of caution and responsibility to provide high-quality items, discussed in Section 5.3."
        },
        {
            "heading": "5 Evaluations",
            "text": ""
        },
        {
            "heading": "5.1 Model Evaluation",
            "text": "Experimental design. For our computational model evaluation, we primarily validated our model by evaluating its ability to generalize to a random subset of 10% of the items in the dataset described in Section 3, not used for training. As also done for simulation, for each training example, we concatenated a subset of one student\u2019s responses to items they responded to, arranged in random order, visualized in Figure 3. We exclude the student response for the last item and fine-tune the model to predict it. As a detail, we also found that binning the input text corresponding to the student response times as \u201cvery fast\u201d, \u201cfast\u201d, \u201cmedium\u201d, \u201cslow\u201d, or \u201cvery slow\u201d based on their quantile of overall data reduced overfitting. We believe this may reduce the model\u2019s ability to correlate specific sets of millisecond-level response times with student responses. Note that we calculated the bins based on the overall response time distribution because the variance across questions in the training dataset was much smaller than the variance across students. We include further item-level analysis for both the generated items and the item-response simulator\u2019s evaluations of these items in Appendix A.\nResults. On our evaluation dataset, we find our simulator\u2019s item-aggregated predicted response\ntimes are well-correlated with the item-aggregated true response times, with a correlation coefficient (r) of 0.50, and correspondingly, an r2 of 0.25, and for predicted vs. actual aggregate probabilities, we found r = 0.76 (r2 = 0.58) with a 25.4% RMSE."
        },
        {
            "heading": "5.2 Crowdworker Evaluation",
            "text": "Experimental design. Since school-aged students must be provided with carefully vetted tests with human supervision, we aim to utilize crowdworker evaluation to address questions that cannot be reasonably obtained through student evaluation alone:\n1. Can the parallel AI test form (without any human filtering) reliably produce identical total scores compared with the well-calibrated, human-generated test form? 2. How well can the item-response simulator identify ambiguous generated items, and do these items actually challenge the reliability of the SRE measure?\nTo answer these questions, we develop a new version of the SRE task with three 3-minute blocks randomly ordered: human-generated sentences (Lab form), GPT-generated items filtered by the itemresponse simulator (parallel AI form), and GPTgenerated items with ambiguous items identified by the item-response simulator (ambiguous AI form). To create the two AI-generated splits, we first divide the generated items according to the median accuracy of items in the training data and then follow our construction method in Section 4.3 on the unambiguous items and sample randomly for the ambiguous items. We recruited 50 participants via Prolific and paid \u2248$15.00 per hour. 6 participants\u2019 results were found either incomplete or random guessing and were removed from the data analysis.\nResults. The total score of each participant is counted by the total correct responses subtracted from the total incorrect responses. Figure 7 (top) shows that the total scores produced by the parallel AI test form achieve a high correlation (r = 0.92) with the scores produced by the Lab form. In addition, the overall difficulty of the parallel AI form is highly identical to the difficulty of the Lab form. Figure 7 (bottom) suggests that the test form with ambiguous AI items identified by the item-response simulator correlates much less (r = 0.75) than the parallel test form above. These comparisons suggest the item-response simulator, without human intervention, is able to flag unseen, ambiguous items that actually challenge test reliability."
        },
        {
            "heading": "5.3 School Student Evaluation",
            "text": "Experimental design. The school student evaluation aims to answer three questions:\n1. Can filtered GPT-generated items reliably produce identical total scores compared with human-generated items? 2. Can the item-response simulator outperform traditional readability metrics in predicting the response time and difficulty of unseen items? 3. And qualitatively, what are some items that the item-response simulator could predict well and what are some that it couldn\u2019t?\nWe use the item-response simulator to select the optimal 130 GPT-generated true sentences and 130 false sentences, 260 items in total. Then, to ensure the appropriateness of the test items for schoolaged children to read and test, the authors, who are familiar with K-12 reading assessments, review and remove items that were still ambiguous (20 items, e.g., \u201cA hill is flat and square.\u201d), could have an inappropriate interpretation (4 items), dangerous (2 items: \u201cBabies drink gasoline.\u201d or \u201cSoap is for eating.\u201d), required too much world knowledge (4 items, e.g., \u201cA brick is made from clay\u201d) or are subjective (1 item, \u201cA doll can be fun to play with.\u201d). Note that, in this school experiment, for robustness and out of concern for automation bias, we do not\nautomatically filter the potentially offensive sentences as we do in the crowdworker experiments.\nWe then implement and deploy a new version of the SRE task with 2 blocks: human-generated sentences (Lab test form) and GPT-generated sentences filtered by the item-response simulator according to accuracy and response time (details in App. E.3) and then human experts (AI test form). 234 students in grades 2-8 from a California public school participated in this study, and three students were removed from the analysis due to a median response time under 500ms (i.e., random guessing).\nIn this version, students first see the humangenerated block with a maximum of 130 items. Then, in the second block, students see 130 randomly-ordered GPT-generated sentences (from an item pool of 200 filtered items). The overall predicted item parameters based on the item-response simulator are shown in Table 1.\nResults. There is a high correlation (r = 0.93) in terms of total scores between the filtered AI test form and the Lab form (Fig. 1). We also found that the two test forms match closely in difficulty, although we did not explicitly aim to match the identity line when creating the AI test form for schools.\nIn terms of predicting the unseen item parameters (accuracy and median response time), the item-response simulator outperforms the Flesch Kincaid method (Fig. 8) and accomplishes a task that the traditional psychometric models (e.g., Item Response Theory) cannot due to the unseen items (Table 2) - however, we note that prior work has attempted to model item difficulties using linear\nlogistic test model (Sonnleitner, 2008; Stenner, 2022) and estimate IRT parameters from text data in other contexts (Ehara, 2018; Settles et al., 2020).\nFig. 9 showcases example items that are accurately and inaccurately predicted by the itemresponse simulator. Overall, the simulator can effectively screen difficult or ambiguous items."
        },
        {
            "heading": "6 Related Work",
            "text": ""
        },
        {
            "heading": "6.1 Item Generation",
            "text": "Earlier methods for the automatic item/question generation include rule-based or template-based approaches (e.g., Mitkov and Ha (2003); Flor and Riordan (2018)) and attempts to adaptively generate questions (Du et al., 2017). Additional work has focused on contexts in which generating high-quality questions (e.g., math word problems) requires stepby-step reasoning (Keller, 2021). In particular, question generation for passage-based reading comprehension (Agarwal et al., 2011; Stasaski et al., 2021; Heck and Meurers, 2022; Rathod et al., 2022; Zou et al., 2022) and for teaching second languages (Chinkina and Meurers, 2017; Srivastava and Goodman, 2021) have been especially well-explored.\nMore recently, White et al. (2022) demonstrate the effectiveness of using GPT-3 to create test items comparable to gold standard TOSREC test items for assessing reading fluency at first and eighth-grade levels. However, their approach still requires extensive human intervention and human evaluation for real-world use. Specifically, they demonstrate the feasibility of GPT-3 for item generation but do not develop an approach to calibrate items in terms of difficulty or filter items that would function poorly in the real world.\nDifficult for students but easy for the simulator.\nFalse: Music makes people sleep. True: People wear sunglasses in sunlight. True: Bananas are yellow when ripe.\nEasy for students but difficult for the simulator.\nFalse: Octopuses have two arms. True: We use our stomach to digest food. False: We wear belts to loosen pants.\nEasy for both students and the simulator.\nTrue: A turtle has a shell. False: Books have pages with salsa. True: Children play in the playground.\nDifficult for both students and the simulator.\nFalse: Stars are invisible at night. False: Chairs have no legs. True: Mushrooms grow in damp areas.\nFigure 9: Example item difficulties as perceived by students and by the item-response simulator.\nRelatedly, Srivastava and Goodman (2021) finetune a model to generate questions adaptively for teaching reverse translation by predicting whether a student will correctly complete a translation. While their study is a seminal prior work, it is not directly applicable to parallel test generation. First, we require a fixed item bank that can be closely reviewed before deployment because our items will be given to children and because we cannot have extra latency. Moreover, the best models for many zero-shot item generation tasks cannot be finetuned by the public, but prior work suggests that small models can effectively validate much more powerful models (Cobbe et al., 2021). Finally, we focus on providing reliable assessment, while their approach is only applicable to instruction."
        },
        {
            "heading": "6.2 Item Evaluation",
            "text": "Although item generation has broadly received more attention from the NLP community than item evaluation, there are several influential works in this area, especially leveraging language models. Language models have been used as a proxy for reading, with prior work highlighting the limited correlation between human and language model readability: studies like Schwarm and Ostendorf (2005) and Si and Callan (2001) demonstrate that the perplexity of a language model is a powerful indicator of the likelihood that a sentence is generated from a corpus of a given difficulty. These works are also related to knowledge tracing, i.e.,\nmodeling student knowledge (Piech et al., 2015; Abdelrahman et al., 2023). More recent studies, such as Benzahra and Yvon (2019) and Martinc et al. (2021), contrast this finding, noting that language model perplexity and other statistical metrics were imperfect estimators of readability, but metrics derived from them performed well across datasets. Importantly, predicting reading fluency, while naturally related to reading comprehension, is a distinct and under-explored area."
        },
        {
            "heading": "6.3 Parallel Test Form Construction",
            "text": "The body of work on constructing parallel tests spans decades (Armstrong et al., 1992, 1994; Gibson and Weiner, 1998; Sun et al., 2008; Ignjatovic\u0301 et al., 2021). However, as most of these works point out, identifying the optimal pairing is an NP-hard problem. To circumvent this, the works often rely on item selection heuristics. For example, Armstrong et al. (1992, 1994) calculate the degree to which each item is correlated with the total test score and then attempt to generate tests with similar expected distributions of final scores. Similarly, Sun et al. (2008) uses a genetic algorithm to maximize the similarity in the expected information gain of each item across a set of tests. In contrast, we pose this as a relaxed optimal transport problem \u2013 we frame our optimization as a differentiable, probabilistic relaxation of this underlying optimal transport problem and solve it directly. This allows us to incorporate other optimization criteria that are important but rarely seen in parallel test literature, such as item diversity and truth parity constraints."
        },
        {
            "heading": "7 Conclusion",
            "text": "Our study presents a new framework to generate test items and select them according to their effectiveness to assess students by leveraging large language models and previous student responses. We use silent sentence reading efficiency assessment, a task with implications for the millions of students who take such tests annually, to illustrate the utility of this framework by generating new test items, predicting the difficulty and ambiguity of unseen items, and creating multiple parallel test forms that have comparable quality and appropriateness to human-generated test forms as well as remain appropriately difficult for students. We ultimately validate our approach with a wide range of evaluations, including more traditional machine learning metrics, crowdworker evaluations, as well as a real-world deployment in K-12 classrooms.\nLimitations\nWhile our case study demonstrates the effectiveness of using large language models and previous student responses, it carries several limitations:\nGeneralization to other types of tests and questions: The framework presented in this case study focuses primarily on assessing students\u2019 reading efficiency. However, we have not yet demonstrated that our approach can easily generalize to other kinds of tests and assessments. No aspect of this approach is inherently dependent on the SRE task, but more research is needed to investigate the applicability of our methods to a wider range of educational tests. Although multiple-choice questions are not typically used in real-world silent sentence reading fluency evaluation (Bloomquist, 2017), we believe with a comparable amount of data, we could replicate this success for other test formats such as multiple-choice questions.\nFiltering and evaluation in school student evaluation: Due to ethical considerations, certain aspects of our study, such as automated filtering of ambiguous or inappropriate items, could not be deployed in the school student evaluation. Consequently, the items in the school student evaluation had to be carefully reviewed by experts before administration. This highlights the need for more robust and reliable methods for filtering and evaluating generated items in real-world educational settings. Although we now have validation supporting the item-response simulator, deploying aspects like automatic filtering will require further study, not only to assess accuracy and sensitivity but also to mitigate automation bias and risks.\nFine-tuning and data limitations: The itemresponse simulator was fine-tuned on data collected from diverse schools in the United States, but the model is trained on a uniform distribution of these students. However, the students whose responses are used for training and simulation may differ demographically from the schools to which the tests are deployed. Thus, the model\u2019s performance in predicting item difficulty and ambiguity may not generalize to all populations or school contexts. Moreover, we have not quantified the degree to which additional generated examples from GPT-4 continue to be novel \u2013 language models fine-tuned using reinforcement learning from human feedback (RLHF) are believed to suffer from mode collapse (Zhu et al., 2023), so ensuring that generated items continue to be meaningfully different is essential.\nReliance on closed and restricted LLMs: Our study uses GPT-4 for generating and filtering test items. However, access to GPT-4 may be expensive if generating many thousands of items. In addition, we fine-tuned LLaMA (Touvron et al., 2023), but LLaMA\u2019s license does not support commercial use. As a result, the exact approach in this paper cannot be applied in commercial contexts. Fortunately, LLaMA replications are underway, and artifacts have already been released (Geng and Liu, 2023).\nEthics Statement There are naturally many ethical considerations in the context of this work. First, all handling of student data must be extremely careful and considerate of the privacy of the students. In this work, we have taken care to ensure that the data used is not personally identifiable and that the data is used appropriately. We have also acted in accordance with the guidelines of our IRB. At all points, before presenting data to humans, especially to children, multiple rounds of review and analysis were performed to ensure that the data was appropriate. The value of our work is to largely reduce the burden of the test creation and invite humans to act as the final safeguards and experts to be responsible for reviewing as few items as possible.\nAs for the ethical implications of the work itself, there are several to consider. First, language models exhibit and potentially exacerbate biases that are already present in society. Humans, of course, also exhibit these biases, but by automating this pipeline, we may reduce the possibility of human intervention to correct for these biases. Second, the ability to use language models in this context may result in an emphasis on tests being developed that are compatible with language models \u2013 however, aspects like visual and phonetic information are valuable for reading fluency evaluation and many other tasks, and we should be mindful to avoid tailoring tests closely to language models\u2019 strengths.\nFinally, while the ability to efficiently generate test items could lower the barrier to universal assessment and lead to more equitable assessment policies, it\u2019s important to proceed with caution and keep humans in the loop at each stage \u2013 particularly when it pertains to educational assessment in young children. Thus, as we begin implementing AI-based assessment systems at scale, we advocate for proceeding with caution, keeping educational experts in the loop at each stage, and keeping an eye toward equity as we strive for efficiency."
        },
        {
            "heading": "Acknowledgements",
            "text": "We would like to thank the schools that partnered with us on this research. We thank the Stanford NLP community for their helpful feedback on the abstract of this paper. We appreciate Benjamin W. Domingue and the reviewers for their helpful feedback on the manuscript and Sang T. Truong for highlighting related work. This work was funded by grants from the Stanford-Sequoia research collaborative, Stanford Neuroscience:Translate program, Microsoft, and the Eunice Kennedy Shriver National Institute of Child Health and Human Development R01HD095861 to J.D.Y, and NSF IIS2247357 to D.Y."
        },
        {
            "heading": "A Model Analysis",
            "text": "A.1 Analyzing GPT-4-generated Sentences\nGPT-4 and, more broadly, language models finetuned based on human preferences have wellknown challenges with mode collapse \u2013 although higher temperatures theoretically encourage diversity, this training step causes models to be more likely to express a single \u201cmodal\u201d opinion (Santurkar et al., 2023). We observed a similar pattern, with a much higher degree of similarity across different instances of model generations as opposed to across items within a single generation. This was part of the motivation for encouraging the model to generate many diverse sentences in a single prompt instead of performing many independent calls. The following were some of the highly similar sentences that remained even after the itemresponse-simulator-based filtering, as determined by the sentence embedding model we used to filter the sentences further:"
        },
        {
            "heading": "A stove is for cooking food.",
            "text": "A stove is for cooking.\nFruit grows on trees and plants. Fruits grow on trees and plants.\nCows give us milk to drink. Cows give us milk.\nA toothbrush cleans our teeth. Toothbrushes clean our teeth.\nApples grow on apple trees. Apples grow on trees."
        },
        {
            "heading": "A bicycle has two wheels.",
            "text": "A bike has two wheels.\nVegetables are good for our health. Vegetables are healthy food.\nKites can fly in the sky. Kites fly in the wind."
        },
        {
            "heading": "A boat floats on water.",
            "text": "A boat goes on water.\nIce cream is cold and sweet. Ice cream is cold.\nToothpaste cleans our teeth. Toothpaste helps to clean teeth.\nA.2 Analyzing item-response-simulator Predictions\nWe observe several interesting patterns in our item-response simulator, especially in combination with GPT-4. For example, as mentioned in Figure 6, many of the truest \u201cfalse\u201d sentence sentences generated by GPT-4 were, in reality, true and many of the least false or most uncertain \u201ctrue\u201d sentences were in fact somewhat ambiguous. Note this trend was particularly pronounced for the sup-\nposedly \u201cfalse\u201d sentences, where the item-response simulator determined that the following sentences were the least false (excluding repeated sentences) \u2013 most are at least arguably true:\n[Keys lock doors. Ducks honk and fly. Boats sink in water. We bake food in ovens. A farmer eats food. Rain falls from the ground. Shadows form in darkness. We cut food with spoons. Umbrellas are used to keep us wet during rainstorms. Sailboats have a sail to avoid the wind and stay still on the water. Lightning comes after thunder. Water is frozen. Ants are large insects that can carry heavy loads and work together. We eat when we are full. People use tools to break things. A shirt uncovers our body. Buses take us places slowly. Families swim together. Autumn leaves stay on trees. Blankets help keep you cold. A bathtub releases water. A nap makes us tired.]\nThe mistake of making an explicitly, unambiguously incorrect truth judgment was not an observed failure case for generated \u201ctrue\u201d sentences: for these sentences, the model was unlikely to generate something outright false and more likely to generate something ambiguously true or only subjectively true. For example, the following true examples were the ones where the model was most uncertain - many of them contain statements that are not universally true, confusing, or difficult to assign a truth value. The following \u201ctrue sentences\u201d were among the ones where the model was least certain of the student response:\n1. \u201cFoxes are orange and fast..\u201d Foxes come in a variety of colors and fast is subjective. 2. \u201cA hill is small and round..\u201d Relative to mountains, sure, but this is not universally true. 3. \u201cClocks have twelve numbers.\u201d What about digital clocks and 24-hour clocks? 4. \u201cThe moon changes shape..\u201d The moon appears to change shape in the sky, but it does not actually change shape. 5. \u201cDolphins are not fish..\u201d While true (disregarding folk taxonomy), this is clearly a worldknowledge-heavy question. 6. \u201cRice grows in water..\u201d Again, this requires an unreasonable amount of world knowledge."
        },
        {
            "heading": "B Preliminary Crowdworker Study",
            "text": "We performed an initial crowdworker study where we performed our optimization to match each copy\nof the lab dataset to a generated item in terms of both accuracy and response time. We observed that the difficulty did not perfectly match that of the lab dataset, as represented by the identity line, although it corresponded much more closely than the unfiltered dataset. When comparing the ground truth data to the predictions, the cause of this was clear: the few lab items that the model identified as ambiguous were substantially less ambiguous than predicted. However, for the generated items, they were actually ambiguous. This transformed false negatives into true negatives in the dataset, harming performance. As a result, for our final prolific study and for the school evaluation, we filtered by the median accuracy but did not optimize it when matching generated and lab items.\nFigure 10 shows both filtered and unfiltered test forms correlate well with the lab test form. However, there are important caveats: first, prolific participants represent a substantial distribution shift for the model which was trained primarily on K-12 students \u2013 they performed far better on average;\nsecond, we found that the difficulty of the filtered corpus matched the lab corpus far more."
        },
        {
            "heading": "C Naive Deduplication",
            "text": "After training these models and validating that their predictions appeared reasonable, we filtered the items to include only ones where the predicted response time was closest to the trendline relating sentence length (in words) to response time. We further filtered them by predicted accuracy, selecting only the ones that the model was most confident students would answer correctly (in particular, we chose the top 200 true and false sentences). Finally, we used sentence embeddings generated for each sentence by the \u201cparaphrasempnet-base-v2\u201d model to select the most similar pairs of sentences (in terms of absolute cosine distance, to also capture sentences that were similar in meaning but opposite in truth value) (Reimers and Gurevych, 2019). If the sentences had the same truth value, we would remove one at random. If they were different, we would remove the one corresponding to the majority class. This ensured we were left with a roughly equal number of both true and false sentences, with 260 in total.\nHowever, this carried a limitation: by deduplicating before selecting items, we may eliminate potentially good items. By deduplicating after selecting items, we must anticipate the proportion of items that must be deduplicated in advance. Motivated partly by this, we then explored techniques to simultaneously optimize semantic similarity alongside test set quality in a less heuristic way."
        },
        {
            "heading": "D Additional Filtering",
            "text": "To remove potentially inappropriate items for classroom settings, we further prompt GPT-4 to evaluate the appropriateness of their own generation (with temperature = 0.1, and at most 1000 tokens generated). Note that this was only used for the crowdworker experiments \u2013 for the school student evaluation, we performed this filtering manually out of an abundance of caution. We use the following prompt: Return the following true or false\nsentences if it is potentially offensive or dangerous for children to read.\nE Implementation Details\nE.1 Item Evaluation We explored various training methods, including Low-Rank Adaptation (LoRA) (Hu et al., 2022) on a model using 8-bit weights (Dettmers et al., 2022a,b), optimizing only a subset of the weights represented as an additive term (a matrix constructed as the product of two vectors that is added to the original linear layer weights). We also explored training a linear head on top of a pre-trained model - initially, we used the mean of the final hidden state embeddings, but later observed that this underperformed, as the considered models were autoregressive and the prediction is only possible from the final example, and not the participant-specific few-shot examples. Instead, we used the final example\u2019s final hidden state, which performed better. Each training example corresponds to a subset of a sampled participant\u2019s responses \u2013 however, when we simulate responses to an item for evaluation, we sample many participants, predict their responses to the new item, then aggregate the predictions.\nFor initial exploration, we primarily explored a variety of Open Pre-trained Transformer (OPT) language models with between 125 million and 6.7 billion parameters (Zhang et al., 2022). For the final implementation, we primarily considered LLaMA models, varying from 7 billion parameters to 65 billion parameters (Touvron et al., 2023). The largest model we fine-tuned with LoRA had 13 billion parameters and the largest model we used with a linear classifier had 65 billion parameters, leveraging four 40GB A40 GPUs. In practice, we found that these largest models were too slow for our purposes, ultimately using a 13-billion parameter LLaMA model.\nWe ultimately selected a constant learning rate of 2e\u22125 with a batch size of 32 sets of items, including a random sample of up to 30 student itemresponse pairs (fewer only if the student responded to fewer items). We use Low-Rank Adaptation (LoRA) (Hu et al., 2022) on a model using 4-bit weights (Dettmers et al., 2022a,b, 2023), optimizing only a subset of the weights. We trained our model for 600 steps, each of which contained 32 items, with gradient accumulation over a batch size of 4. Training the model required approximately 6 hours, while simulating all of the GPT-generated items with 100 previous participants required approximately 24 hours.\nE.2 Parallel Test Form Construction\nWe perform our optimization over each item-pair\u2019s logits with Adam with a learning rate of 0.1 and evaluate item similarity using the absolute cosine similarity between their sentence embeddings from SentenceTransformers\u2019s paraphrase-mpnet-base-v2 (Reimers and Gurevych, 2019), disregarding items with absolute similarity below 0.5. Note that we initially attempted to use Feydy et al. (2019)\u2019s differentiable optimal transport library, geomloss, but unbalanced problems require a reach hyperparameter to be specified, and we found that no reach works across all points. Figure 11 showcases the predicted response time and accuracy distributions in each test forms.\nE.3 School Student Evaluation Filtering Based on feedback from experts, for our school student evaluation, we initially filtered using two threshold criteria: first, accuracy must be greater than 85%, which corresponds to the median item accuracy in our dataset; second, the response time should be within a standard deviation of the mean response time per word (disregarding the intercept \u2013 i.e., the extrapolated amount of time we expect a participant to take for an item with no length).\nF Item Information Analysis\nBy fitting a 2-parameter logistic item response theory model to the student responses to both the Lab and AI-filtered test forms, we obtain the Fisher information (Fisher, 1925) for each item. Figure 12 shows that GPT-generated false sentences offer higher item information than human-generated false sentences, while there is no significant difference for true sentences. Although IRT is not the perfect psychometric model for the SRE task, this finding is encouraging to show the potential of the GPT-generated items."
        }
    ],
    "title": "Generating and Evaluating Tests for K-12 Students with Language Model Simulations: A Case Study on Sentence Reading Efficiency",
    "year": 2023
}