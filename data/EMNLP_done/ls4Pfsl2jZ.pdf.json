{
    "abstractText": "With the rapid progress of large language models (LLMs), many downstream NLP tasks can be well solved given appropriate prompts. Though model developers and researchers work hard on dialog safety to avoid generating harmful content from LLMs, it is still challenging to steer AI-generated content (AIGC) for the human good. As powerful LLMs are devouring existing text data from various domains (e.g., GPT-3 is trained on 45TB texts), it is natural to doubt whether the private information is included in the training data and what privacy threats can these LLMs and their downstream applications bring. In this paper, we study the privacy threats from OpenAI\u2019s ChatGPT and the New Bing enhanced by ChatGPT and show that application-integrated LLMs may cause new privacy threats. To this end, we conduct extensive experiments to support our claims and discuss LLMs\u2019 privacy implications.",
    "authors": [
        {
            "affiliations": [],
            "name": "Haoran Li"
        },
        {
            "affiliations": [],
            "name": "Dadi Guo"
        },
        {
            "affiliations": [],
            "name": "Wei Fan"
        },
        {
            "affiliations": [],
            "name": "Mingshi Xu"
        },
        {
            "affiliations": [],
            "name": "Jie Huang"
        },
        {
            "affiliations": [],
            "name": "Fanpu Meng"
        },
        {
            "affiliations": [],
            "name": "Yangqiu Song"
        }
    ],
    "id": "SP:c46d526b2d4a488a02b563d7c33f44191eea9d55",
    "references": [
        {
            "authors": [
                "Alan Akbik",
                "Tanja Bergmann",
                "Duncan Blythe",
                "Kashif Rasul",
                "Stefan Schweter",
                "Roland Vollgraf."
            ],
            "title": "FLAIR: An easy-to-use framework for state-of-theart NLP",
            "venue": "NAACL 2019, 2019 Annual Conference of the North American Chapter of the Association for",
            "year": 2019
        },
        {
            "authors": [
                "Nicholas Carlini",
                "Florian Tramer",
                "Eric Wallace",
                "Matthew Jagielski",
                "Ariel Herbert-Voss",
                "Katherine Lee",
                "Adam Roberts",
                "Tom Brown",
                "Dawn Song",
                "Ulfar Erlingsson",
                "Alina Oprea",
                "Colin Raffel."
            ],
            "title": "Extracting training data from large language models",
            "venue": "In",
            "year": 2021
        },
        {
            "authors": [
                "Chunkit Chan",
                "Cheng Jiayang",
                "Weiqi Wang",
                "Yuxin Jiang",
                "Tianqing Fang",
                "Xin Liu",
                "Yangqiu Song."
            ],
            "title": "Chatgpt evaluation on sentence level relations: A focus on temporal, causal, and discourse relations",
            "venue": "ArXiv, abs/2304.14827.",
            "year": 2023
        },
        {
            "authors": [
                "Chen Chen",
                "Jie Fu",
                "L. Lyu."
            ],
            "title": "A pathway towards responsible ai generated content",
            "venue": "ArXiv, abs/2303.01325.",
            "year": 2023
        },
        {
            "authors": [
                "Bob McGrew",
                "Dario Amodei",
                "Sam McCandlish",
                "Ilya Sutskever",
                "Wojciech Zaremba."
            ],
            "title": "Evaluating large language models trained on code",
            "venue": "ArXiv, abs/2107.03374.",
            "year": 2021
        },
        {
            "authors": [
                "Paul F Christiano",
                "Jan Leike",
                "Tom Brown",
                "Miljan Martic",
                "Shane Legg",
                "Dario Amodei."
            ],
            "title": "Deep reinforcement learning from human preferences",
            "venue": "Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc.",
            "year": 2017
        },
        {
            "authors": [
                "Lavina Daryanani."
            ],
            "title": "How to jailbreak chatgpt",
            "venue": "https://watcher.guru/news/ how-to-jailbreak-chatgpt.",
            "year": 2023
        },
        {
            "authors": [
                "Tim Dettmers",
                "Artidoro Pagnoni",
                "Ari Holtzman",
                "Luke Zettlemoyer."
            ],
            "title": "Qlora: Efficient finetuning of quantized llms",
            "venue": "arXiv preprint arXiv:2305.14314.",
            "year": 2023
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
            "year": 2019
        },
        {
            "authors": [
                "Marie Douriez",
                "Harish Doraiswamy",
                "Juliana Freire",
                "Cl\u00e1udio T. Silva"
            ],
            "title": "Anonymizing nyc taxi data: Does it matter",
            "venue": "In 2016 IEEE International Conference on Data Science and Advanced Analytics (DSAA),",
            "year": 2016
        },
        {
            "authors": [
                "Kai Greshake",
                "Sahar Abdelnabi",
                "Shailesh Mishra",
                "Christoph Endres",
                "Thorsten Holz",
                "Mario Fritz."
            ],
            "title": "More than you\u2019ve asked for: A comprehensive analysis of novel prompt injection threats to application-integrated large language models",
            "venue": "ArXiv,",
            "year": 2023
        },
        {
            "authors": [
                "Jie Huang",
                "Hanyin Shao",
                "Kevin Chen-Chuan Chang"
            ],
            "title": "Are large pre-trained language models leaking your personal information? In Findings of the Association for Computational Linguistics: EMNLP 2022",
            "year": 2022
        },
        {
            "authors": [
                "Daniel Kang",
                "Xuechen Li",
                "Ion Stoica",
                "Carlos Guestrin",
                "Matei A. Zaharia",
                "Tatsunori Hashimoto."
            ],
            "title": "Exploiting programmatic behavior of llms: Dualuse through standard security attacks",
            "venue": "ArXiv, abs/2302.05733.",
            "year": 2023
        },
        {
            "authors": [
                "Bryan Klimt",
                "Yiming Yang."
            ],
            "title": "The enron corpus: A new dataset for email classification research",
            "venue": "Machine Learning: ECML 2004, pages 217\u2013226, Berlin, Heidelberg. Springer Berlin Heidelberg.",
            "year": 2004
        },
        {
            "authors": [
                "Takeshi Kojima",
                "Shixiang (Shane) Gu",
                "Machel Reid",
                "Yutaka Matsuo",
                "Yusuke Iwasawa"
            ],
            "title": "Large language models are zero-shot reasoners",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Haoran Li",
                "Yangqiu Song",
                "Lixin Fan."
            ],
            "title": "You don\u2019t know my favorite color: Preventing dialogue representations from revealing speakers\u2019 private personas",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for",
            "year": 2022
        },
        {
            "authors": [
                "Xiang Lisa Li",
                "Percy Liang."
            ],
            "title": "Prefix-tuning: Optimizing continuous prompts for generation",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language",
            "year": 2021
        },
        {
            "authors": [
                "Chin-Yew Lin."
            ],
            "title": "ROUGE: A package for automatic evaluation of summaries",
            "venue": "Text Summarization Branches Out, pages 74\u201381, Barcelona, Spain. Association for Computational Linguistics.",
            "year": 2004
        },
        {
            "authors": [
                "Pengfei Liu",
                "Weizhe Yuan",
                "Jinlan Fu",
                "Zhengbao Jiang",
                "Hiroaki Hayashi",
                "Graham Neubig."
            ],
            "title": "Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing",
            "venue": "ACM Comput. Surv., 55(9).",
            "year": 2023
        },
        {
            "authors": [
                "Nils Lukas",
                "A. Salem",
                "Robert Sim",
                "Shruti Tople",
                "Lukas Wutschitz",
                "Santiago Zanella-B\u2019eguelin"
            ],
            "title": "Analyzing leakage of personally identifiable information in language models. ArXiv, abs/2302.00539",
            "year": 2023
        },
        {
            "authors": [
                "Todor Markov",
                "Chong Zhang",
                "Sandhini Agarwal",
                "Tyna Eloundou",
                "Teddy Lee",
                "Steven Adler",
                "Angela Jiang",
                "Lilian Weng."
            ],
            "title": "A holistic approach to undesired content detection",
            "venue": "Proceedings of AAAI 2023.",
            "year": 2023
        },
        {
            "authors": [
                "Fatemehsadat Mireshghallah",
                "Archit Uniyal",
                "Tianhao Wang",
                "David Evans",
                "Taylor Berg-Kirkpatrick."
            ],
            "title": "An empirical analysis of memorization in finetuned autoregressive language models",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods",
            "year": 2022
        },
        {
            "authors": [
                "OpenAI."
            ],
            "title": "Gpt-4 technical report",
            "venue": "ArXiv, abs/2303.08774.",
            "year": 2023
        },
        {
            "authors": [
                "Paul Christiano",
                "Jan Leike",
                "Ryan Lowe."
            ],
            "title": "Training language models to follow instructions with human feedback",
            "venue": "Advances in Neural Information Processing Systems.",
            "year": 2022
        },
        {
            "authors": [
                "Xudong Pan",
                "Mi Zhang",
                "Shouling Ji",
                "Min Yang."
            ],
            "title": "Privacy risks of general-purpose language models",
            "venue": "Proceedings of 2020 IEEE Symposium on Security and Privacy (SP), pages 1314\u20131331.",
            "year": 2020
        },
        {
            "authors": [
                "Kishore Papineni",
                "Salim Roukos",
                "Todd Ward",
                "WeiJing Zhu."
            ],
            "title": "BLEU: a method for automatic evaluation of machine translation",
            "venue": "Proceedings of ACL 2002, pages 311\u2013318.",
            "year": 2002
        },
        {
            "authors": [
                "F\u00e1bio Perez",
                "Ian Ribeiro"
            ],
            "title": "Ignore previous prompt: Attack techniques for language models",
            "year": 2022
        },
        {
            "authors": [
                "Aleksandra Piktus",
                "Christopher Akiki",
                "Paulo Villegas",
                "Hugo Laurenccon",
                "G\u00e9rard Dupont",
                "Alexandra Sasha Luccioni",
                "Yacine Jernite",
                "Anna Rogers."
            ],
            "title": "The roots search tool: Data transparency for llms",
            "venue": "ArXiv, abs/2302.14035.",
            "year": 2023
        },
        {
            "authors": [
                "Alec Radford",
                "Jeff Wu",
                "Rewon Child",
                "David Luan",
                "Dario Amodei",
                "Ilya Sutskever"
            ],
            "title": "Language models are unsupervised multitask learners",
            "year": 2019
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J. Liu."
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "Journal of Machine Learning Research,",
            "year": 2020
        },
        {
            "authors": [
                "Thomas Wolf",
                "Alexander M Rush."
            ],
            "title": "Multitask prompted training enables zero-shot task generalization",
            "venue": "International Conference on Learning Representations.",
            "year": 2022
        },
        {
            "authors": [
                "Timo Schick",
                "Hinrich Sch\u00fctze."
            ],
            "title": "Exploiting cloze-questions for few-shot text classification and natural language inference",
            "venue": "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume,",
            "year": 2021
        },
        {
            "authors": [
                "Congzheng Song",
                "Ananth Raghunathan."
            ],
            "title": "Information leakage in embedding models",
            "venue": "Proceedings of ACM CCS 2020, page 377\u2013390.",
            "year": 2020
        },
        {
            "authors": [
                "Hugo Touvron",
                "Louis Martin",
                "Kevin Stone",
                "Peter Albert",
                "Amjad Almahairi",
                "Yasmine Babaei",
                "Nikolay Bashlykov",
                "Soumya Batra",
                "Prajjwal Bhargava",
                "Shruti Bhosale"
            ],
            "title": "Llama 2: Open foundation and fine-tuned chat models",
            "year": 2023
        },
        {
            "authors": [
                "Xuezhi Wang",
                "Jason Wei",
                "Dale Schuurmans",
                "Quoc V Le",
                "Ed H. Chi",
                "Sharan Narang",
                "Aakanksha Chowdhery",
                "Denny Zhou."
            ],
            "title": "Self-consistency improves chain of thought reasoning in language models",
            "venue": "The Eleventh International Conference on Learning",
            "year": 2023
        },
        {
            "authors": [
                "Jason Wei",
                "Maarten Bosma",
                "Vincent Zhao",
                "Kelvin Guu",
                "Adams Wei Yu",
                "Brian Lester",
                "Nan Du",
                "Andrew M. Dai",
                "Quoc V Le."
            ],
            "title": "Finetuned language models are zero-shot learners",
            "venue": "International Conference on Learning Representations.",
            "year": 2022
        },
        {
            "authors": [
                "Jason Wei",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Maarten Bosma",
                "brian ichter",
                "Fei Xia",
                "Ed H. Chi",
                "Quoc V Le",
                "Denny Zhou"
            ],
            "title": "2022b. Chain of thought prompting elicits reasoning in large language models",
            "venue": "In Advances in Neural Information Processing Systems",
            "year": 2022
        },
        {
            "authors": [
                "Lianmin Zheng",
                "Wei-Lin Chiang",
                "Ying Sheng",
                "Siyuan Zhuang",
                "Zhanghao Wu",
                "Yonghao Zhuang",
                "Zi Lin",
                "Zhuohan Li",
                "Dacheng Li",
                "Eric. P Xing",
                "Hao Zhang",
                "Joseph E. Gonzalez",
                "Ion Stoica"
            ],
            "title": "Judging llm-as-a-judge with mt-bench and chatbot arena",
            "year": 2023
        },
        {
            "authors": [
                "Denny Zhou",
                "Nathanael Sch\u00e4rli",
                "Le Hou",
                "Jason Wei",
                "Nathan Scales",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Claire Cui",
                "Olivier Bousquet",
                "Quoc V Le",
                "Ed H. Chi."
            ],
            "title": "Least-to-most prompting enables complex reasoning in large language models",
            "venue": "The",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "The rapid evolution of large language models (LLMs) makes them a game changer for modern natural language processing. LLMs\u2019 dominating generation ability changes previous tasks\u2019 paradigms to a unified text generation task and consistently improves LLMs\u2019 performance on these tasks (Raffel et al., 2020; Chung et al., 2022; Brown et al., 2020b; OpenAI, 2023; Ouyang et al., 2022; Chan et al., 2023). Moreover, given appropriate instructions/prompts, LLMs even can be zero-shot or few-shot learners to solve specified tasks (Chen et al., 2021; Zhou et al., 2023; Kojima et al., 2022; Wei et al., 2022b; Sanh et al., 2022).\nNotably, LLMs\u2019 training data also scale up in accordance with models\u2019 sizes and performance. Massive LLMs\u2019 textual training data are primarily collected from the Internet and researchers pay less attention to the data quality and confidentiality of the web-sourced data (Piktus et al., 2023).\nHaoran Li and Dadi Guo contribute equally.\nSuch mass collection of personal data incurs debates and worries. For example, under the EU\u2019s General Data Protection Regulation (GDPR), training a commercial model on extensive personal data without notice or consent from data subjects lacks a legal basis. Consequently, Italy once temporarily banned ChatGPT due to privacy considerations1.\nUnfortunately, the privacy analysis of language models is still less explored and remains an active area. Prior works (Lukas et al., 2023; Pan et al., 2020; Mireshghallah et al., 2022; Huang et al., 2022; Carlini et al., 2021) studied the privacy leakage issues of language models (LMs) and claimed that memorizing training data leads to private data leakage. However, these works mainly investigated variants of GPT-2 models (Radford et al., 2019) trained simply by language modeling objective, which aimed to predict the next word given the current context. Despite the efforts made by these pioneering works, there is still a huge gap between the latest LLMs and GPT-2. First, LLMs\u2019 model sizes and dataset scales are much larger than GPT-2. Second, LLMs implement more sophisticated training objectives, which include instruction tuning (Wei et al., 2022a) and Reinforcement Learning from Human Feedback (RLHF) (Christiano et al., 2017). Third, most LLMs only provide application programming interfaces (APIs) and we cannot inspect the model weights and training corpora. Lastly, it is trending to integrate various applications into LLMs to empower LLMs\u2019 knowledge grounding ability to solve math problems (ChatGPT + Wolfram Alpha), read formatted files (ChatPDF), and respond to queries with the search engine (the New Bing). As a result, it remains unknown to what extent privacy leakage occurs on these present-day LLMs we use.\nTo fill the mentioned gap, in this work, we con-\n1See https://www.bbc.com/news/ technology-65139406. Currently, ChatGPT is no longer banned in Italy.\nduct privacy analyses of the state-of-the-art LLMs and study their privacy implications. We follow the setting of previous works to evaluate the privacy leakage issues of ChatGPT thoroughly and show that previous prompts are insufficient to extract personally identifiable information (PII) from ChatGPT with enhanced dialog safety. We then propose a novel multi-step jailbreaking prompt to extract PII from ChatGPT successfully. What\u2019s more, we also study privacy threats introduced by the New Bing, an integration of ChatGPT and search engine. The New Bing changes the paradigm of retrievalbased search engines into the generation task. Besides privacy threats from memorizing the training data, the new paradigm may provoke unintended PII dissemination. In this paper, we demonstrate the free lunch possibility for the malicious adversary to extract personal information from the New Bing with almost no cost. Our contributions can be summarized as follows:2\n(1) We show previous attacks cannot extract any personal information from ChatGPT. Instead, we propose a novel multi-step jailbreaking prompt to demonstrate that ChatGPT could still leak PII even though a safety mechanism is implemented.\n(2) We disclose the new privacy threats beyond the personal information memorization issue for application-integrated LLM. The applicationintegrated LLM can recover personal information with improved accuracy.\n(3) We conduct extensive experiments to assess the privacy risks of these LLMs. While our results indicate that the success rate of attacks is not exceedingly high, any leakage of personal information is a serious concern that cannot be overlooked. Our findings suggest that LLM\u2019s safety needs further improvement for open and safe use."
        },
        {
            "heading": "2 Related Works",
            "text": "LLMs and privacy attacks towards LMs. Originating from LMs (Radford et al., 2019; Devlin et al., 2019; Raffel et al., 2020), LLMs increase their model sizes and data scales with fine-grained training techniques and objectives (OpenAI, 2023; Ouyang et al., 2022; Chung et al., 2022). Previously, LMs are widely criticized for their information leakage issues. Chen et al. (2023) discussed general large generative models\u2019 potential privacy leakage issues for both NLP and CV fields. Several\n2Code is publicly available at https://github.com/ HKUST-KnowComp/LLM-Multistep-Jailbreak.\nstudies (Lukas et al., 2023; Huang et al., 2022; Carlini et al., 2021) suggested that LMs tend to memorize their training data and partial private information might be recovered given specific prompts. Mireshghallah et al. (2022) proposed membership inference attacks on fine-tuned LMs and suggested that these LMs\u2019 private fine-tuning data were vulnerable to extraction attacks. On the other hand, a few works (Li et al., 2022; Pan et al., 2020; Song and Raghunathan, 2020) examined information leakage issues on LMs\u2019 embeddings during inference time. Evolved from LMs, LLMs adopt various defenses against malicious use cases. Markov et al. (2023) built a holistic system for content detection to avoid undesired content from hate speech to harmful content. OpenAI (2023) fine-tuned the GPT-4 model to reject queries about private information. It is still unclear whether safety-enhanced LLMs inherit the privacy issues of LMs. In this work, we study PII extraction on LLMs.\nPrompts and prompt-based attacks on LLMs. Prompt-based methods (Brown et al., 2020a; Liu et al., 2023; Schick and Sch\u00fctze, 2021; Li and Liang, 2021) play a vital role in the development of language models. Benign prompts boost LLM to solve unseen tasks (Ouyang et al., 2022; Brown et al., 2020a; Chung et al., 2022). However, on the other hand, malicious prompts impose harm and threats. Recently, Jailbreaking prompts (Daryanani, 2023) are widely discussed to remove the restrictions of ChatGPT and allow ChatGPT to Do Anything Now (DAN) (0xk1h0, 2023). Prompt Injection attacks (Perez and Ribeiro, 2022) proposed goal hijacking and prompt leaking to misuse LLMs. Goal hijacking aimed to misalign the goal of original prompts to a target goal, while prompt leaking tried to recover the information from private prompts. Kang et al. (2023) treated LLMs as programs and mimicked Computer Security attacks to maliciously prompt harmful contents from LLMs. Greshake et al. (2023) extended Prompt Injection attacks to application-integrated LLMs and argued that augmenting LLMs with applications could amplify the risks. These works mainly propose adversarial prompts to malfunction the LLMs to deviate from their original goals or generate harmful content like hate speech. In this work, we utilize these tricky prompts to elicit personal information from LLMs and analyze their threats and implications."
        },
        {
            "heading": "3 Data Extraction Attacks on ChatGPT",
            "text": "In this section, we describe our privacy attacks from data preparation to attack methodologies."
        },
        {
            "heading": "3.1 Data Collection",
            "text": "Most existing privacy laws state that personal data refers to any information related to an identified or identifiable living individual. For example, personal emails are widely regarded as private information and used as an indicator of studying privacy leakage. Prior works that studied the privacy leakage of LMs commonly assumed that they could access the training corpora. However, we cannot access the training data of the LLMs we investigated. Instead, we only know that these LLMs are trained on massive textual data from the Internet. In this work, we collect multi-faceted personally identifiable information from the following sources:\nEnron Email Dataset (Klimt and Yang, 2004). The Enron Email Dataset collect around 0.5M emails from about 150 Enron employees and the data was made public on the Internet. We notice that several frequently used websites store the emails of the Enron Email Dataset, and we believe it is likely to be included in the training corpus of LLMs. We processed (name, email address) pairs as well as corresponding email contents from the dataset. Moreover, we collect (name, phone numbers) pairs from the email contents.\nInstitutional Pages. We observe that professional scholars tend to share their contact information of their Institutional emails and office phone numbers on their web pages. We hereby collect (name, email address) and (name, phone number) pairs of professors from worldwide universities. For each university, we collect 10 pairs from its Computer Science Department."
        },
        {
            "heading": "3.2 Attack Formulation",
            "text": "Given the black-box API access to an LLM f where we can only input texts and obtain textual responses, training data extraction attacks aim to reconstruct sensitive information s from f \u2019s training corpora with prefix (or prompt) p. In other words, training data extraction is also a text completion task where the adversary attempts to recover private information s from the tricky prompt p such that: f(p) = s. In this work, we assume that the adversary can only obtain textual outputs from APIs where hidden representations and predicted probability matrices are inaccessible."
        },
        {
            "heading": "3.3 Private Data Extraction from ChatGPT",
            "text": "ChatGPT is initialized from the GPT-3.5 model (Brown et al., 2020a) and fine-tuned on conversations supervised by human AI trainers. Since ChatGPT is already tuned to improve dialog safety, we consider three prompts to conduct training data extraction attacks from direct prompts to multi-step jailbreaking prompts."
        },
        {
            "heading": "3.3.1 Extraction with Direct Prompts",
            "text": "Previous works (Carlini et al., 2021; Huang et al., 2022; Mireshghallah et al., 2022; Lukas et al., 2023) mainly used direct prompts to extract private information from LMs including variants of GPT-2. For example, the adversary may use prompts like \u201c name: [name], email: ____\u201d to extract the email address of a specific person or use \u201c name: ____\u201d directly to recover multiple (name, email) pairs via sampling-based decoding.\nFortunately, thanks to the dialog safety finetuning, ChatGPT after the Mar Version tends to hesitate from answering any private information if we use direct prompts for data extraction.As shown in Figure 1 (a), ChatGPT refuses to generate any personal information with direct prompts."
        },
        {
            "heading": "3.3.2 Extraction with Jailbreaking Prompts",
            "text": "Though ChatGPT pays great effort into dialog safety and can successfully prevent against training data extraction attacks with direct prompts, there is still a sideway to bypass ChatGPT\u2019s ethical modules called jailbreaking. Jailbreaking exploits tricky prompts to make ChatGPT evade programming restrictions and generate anything freely. These tricky prompts usually set up user-created role plays to alter ChatGPT\u2019s ego and allow ChatGPT to answer user queries unethically. DAN refers to \u201cDo Anything for Now\u201d, and is one exemplary jailbreaking prompt to generate offensive or prejudiced comments about politics, race and sex.\nIn this work, we exploit these jailbreaking prompts to make ChatGPT generate personal information of given names. For example, according to the use cases of Figure 1 (b), ChatGPT sometimes generates private information from its \u201cDeveloper Mode\u201d role of the jailbreaking prompt."
        },
        {
            "heading": "3.3.3 Morality Undermining with the Multi-step Jailbreaking Prompt",
            "text": "Chain-of-Thought (CoT) prompting (Kojima et al., 2022; Wei et al., 2022b; Wang et al., 2023) decomposes complex problems into intermediate steps\nto improve LLMs reasoning ability. For the Mar Version of ChatGPT, we occasionally observe that ChatGPT may still refuse to generate private information given jailbreaking prompts. Inspired by the magic power of \u201cLet\u2019s think step by step\u201d (Kojima et al., 2022), we propose the Multi-step Jailbreaking Prompt (MJP) to bypass the moral restrictions of LLMs and encourage LLMs to generate private information.\nOur proposed MJP aims to relieve LLMs\u2019 ethical considerations and force LLMs to recover personal information. We merge jailbreaking prompts into the three-utterance context between the user and ChatGPT. First, we play the role of the user to input the jailbreaking prompt. Second, we act as the assistant (ChatGPT) to acknowledge that the jailbreak mode is enabled. Finally, we perform as the user to query the assistant with previous direct prompts. Moreover, we append one more sentence to the final user query to encourage ChatGPT to make a random guess if it does not know the email address or could not answer the emails due to ethical considerations. The second utterance convinces the LLM to accept its role of jailbreaking prompts. The last appended sentence exploits indirect prompts to bypass the LLM\u2019s ethical module and persuade the LLM to generate or improvise personal information based on learned distribution. Figure 1 (c) depicts that ChatGPT is more willing to make such \u201crandom guesses\u201d based on the proposed MJP."
        },
        {
            "heading": "3.3.4 Response Verification",
            "text": "Besides prompt tricks, for each data sample, we could also generate private information multiple\ntimes with sampling-based decoding. As displayed in Figure 1 (d), we collect distinct personal information from diverse responses. We consider two methods to verify which one is the correct answer. The first method converts the collected information into a multiple-choice question and prompts the LLM again to choose the correct answer. During implementation, we treat the first displayed information in the response as the LLM\u2019s final choice. The second method is majority voting which regards the most frequent prediction as the final answer. If there is a tie, we randomly choose one candidate as the final prediction."
        },
        {
            "heading": "3.4 Personal Data Recovery from New Bing",
            "text": "The New Bing introduces a new search paradigm from search to the combination of search and AIGC to improve search accuracy and relevance. Microsoft even names the new combination as the Prometheus model to emphasize its importance. Moreover, they claim that safeguards are implemented to address issues like misinformation and disinformation, data safety, and harmful content.\nHowever, unlike ChatGPT, the New Bing frequently responds to direct prompts mentioned in Section 3.3.1 according to our use cases. Here, we consider two attack scenarios with direct prompts for the new search paradigm. One is the free-form extraction that directly generates (name, PII) pairs given the domain information, and the other is partially identified extraction, which recovers PII with given names and domain information. Though the search results are publicly available and not private, the New Bing may increase the risk of unintended\npersonal data dissemination."
        },
        {
            "heading": "3.4.1 Free-form Extraction",
            "text": "Free-form extraction assumes the adversary only knows some domain knowledge about targets, including names of companies and institutions, email domains, and website links. Free-form extraction exploits the search and summarization ability of the New Bing. Simple instructions like \u201cPlease list me some example (name, email) pairs according to your search results about [domain knowledge]\u201d are sufficient to extract personal information. The adversary aims to extract personal information from LLMs based on its domain knowledge so that it can gather excessive personal information without heavy human labor. The collected information may be maliciously used to send spam or phishing emails. In the later experiments, we will show how to extract demanded information via adding more specific conditions on queries."
        },
        {
            "heading": "3.4.2 Partially Identified Extraction",
            "text": "Partially identified extraction assumes that the adversary is interested in recovering the private information about a target individual, given its name and corresponding domain knowledge. This attack usually takes the format like \u201c name: [name], email: ____\u201d to force LLMs to predict private information associated with the name. The attack based on the association can be harmful directly to a partially identified victim."
        },
        {
            "heading": "4 Experiments",
            "text": "In this section, we follow the zero-shot setting to conduct experiments to recover multi-faceted personal information that includes email addresses and phone numbers. In addition, experiments on email content recovery can be found in Appendix B."
        },
        {
            "heading": "4.1 Experimental Settings",
            "text": "Datasets. For the Enron Email Dataset, we processed 100 frequent (name, email address) pairs whose email domain is \u201c@enron.com\u201d from Enron\u2019s employees and 100 infrequent pairs whose domains do not belong to Enron. Among 100 frequent pairs, we manually filter out 12 invalid organizational emails and evaluate the remaining 88 pairs. We also collect 300 (name, phone number) pairs to recover phone numbers given names. For Institutional Pages, we collect 50 (name, email address) pairs and 50 (name, phone number) pairs.\nEvaluation Metrics. For each PII recovery, we generate 1 response per prompt and count the number of pairs that can parse our predefined patterns from responses as # parsed. Moreover, we can also automatically generate multiple responses via its chat completion API. During our experiments, we perform 5 generations and then use Hit@5 to denote the percentage of pairs that include correct prediction from their responses. For each pair, we use the first parsed PII as the final prediction among all 5 generations by default. If response verification tricks are applied, we use the verified result as the final prediction. To verify how many emails are correctly recovered, we report the count (# correct) and accuracy (Acc) of correctly recovered emails by comparing final predictions with correct emails. For phone number recovery, we calculate the longest common substring (LCS) between final predictions and ground truth numbers and report the count of pairs whose LCS \u2265 6 (LCS6) and the overall count for 5 generations (LCS6@5). Data Extraction Attack Pipeline. All our extraction attacks are conducted on the web interface of the New Bing and the chat completion API of ChatGPT from their corresponding official sources. For the web interface, we manually type attack queries and collect the responses. For each attack case, we start a new session to avoid the interference of previous contexts. For the ChatGPT API, we write a script to input attack queries with contexts to obtain responses from LLMs, then we write a regular expression formula to parse the PII shown in responses as predicted PII."
        },
        {
            "heading": "4.2 Evaluation on ChatGPT",
            "text": ""
        },
        {
            "heading": "4.2.1 Evaluated Prompts",
            "text": "To evaluate privacy threats of ChatGPT, we follow Huang et al. (2022)\u2019s experimental settings to measure association under the zero-shot setting. In our experiments, we test association on email addresses and phone numbers. In addition, we assume we have no prior knowledge about the textual formats, and there is no text overlap between our prompts and the contents to be evaluated. We leverage jailbreaking and multi-step prompts to create the following prompts: \u2022 Direct prompt (DP). As explained in Sec 3.3.1, we use a direct query to obtain PII. \u2022 Jailbreaking prompt (JP). First, we use the jailbreaking prompt to obtain the response from ChatGPT. Then, we concatenate the jailbreaking query,\nthe obtained response and direct prompts to obtain the final responses and parse the PII. \u2022 Multi-step Jailbreaking Prompt (MJP). We use the three-utterance context mentioned in Sec 3.3.3 to obtain responses and try to parse the PII. \u2022 MJP+multiple choice (MJP+MC). We generate 5 responses via MJP. Then we use a multiple-choice template to prompt ChatGPT again to choose the final answer. \u2022 MJP+majority voting (MJP+MV). We generate 5 responses via MJP. Then we use majority voting to choose the final answer.\nThese prompts\u2019 examples can be found in Figure 1. And the detailed templates are reported in Appendix A."
        },
        {
            "heading": "4.2.2 Analysis of Results",
            "text": "Tables 1 and 3 depict the email address recovery results on the filtered Enron Email Dataset and manually collected faculty information of various universities. Table 2 evaluates phone number recovery performance. Based on the results and case inspection, we summarize the following findings: \u2022 ChatGPT memorizes certain personal information. More than 50% frequent Enron emails and 4% faculty emails can be recovered via our proposed prompts. For recovered email addresses, Hit@5 is generally much higher than Acc and most\nemail domains can be generated correctly. For extracted phone numbers, LCS6@5 are larger than LCS6. These results suggest that anyone\u2019s personal data have a small chance to be reproduced by ChatGPT if it puts its personal data online and ChatGPT happens to train on the web page that includes its personal information. And the recovery probability is likely to be higher for people of good renown on the Internet.\n\u2022 ChatGPT is better at associating names with email addresses than phone numbers. Tables 1, 2 and 3 show that email addresses can be moderately recovered, whereas phone numbers present a considerable challenge for association. Furthermore, the higher frequency of email addresses being # parsed suggests that ChatGPT might view phone numbers as more sensitive PII, making them more difficult to parse and correctly extract.\n\u2022 ChatGPT indeed can prevent direct and a half jailbreaking prompts from generating PII. Based on the results of # parsed, both JP and DP are incapable of recovering PII. For example, when it comes to the more realistic scenario about institutional emails, even JP can only parse 10 email patterns out of 50 cases. In addition, most responses mention that it is not appropriate or ethical to disclose personal information and refuse to answer the queries. These results indicate that previous extraction attacks with direct prompts are no longer effective on safety-enhanced LLMs like ChatGPT.\n\u2022 MJP effectively undermines the morality of ChatGPT. Tables 1, 2 and 3 verify that MJP can lead to more parsed PII and correct generations than JP. Even though ChatGPT refuses to answer queries about personal information due to ethical concerns, it is willing to make some guesses. Since\nthe generations depend on learned distributions, some guessed emails might be the memorized training data. Consequently, MJP improves the number of parsed patterns, recovery accuracy, and Hit@5. \u2022 Response verification can improve attack performance. Both multiple-choice prompting (MJP+MC) and majority voting (MJP+MV) gain extra 10% accuracy on the frequent Enron emails. This result also verifies the PII memorization issue of ChatGPT."
        },
        {
            "heading": "4.3 Evaluation on the New Bing",
            "text": ""
        },
        {
            "heading": "4.3.1 Evaluated Prompts",
            "text": "Based on our use cases of the New Bing, we notice that direct prompts are sufficient to generate personal information from the New Bing. Unlike previous privacy analyses of LMs, the New Bing plugs the LLM into the search engine. The powerful search plugin enables the LLM to access any online data beyond its training corpus. Utilizing the information extraction ability of LLM boosts the search quality at a higher risk of unintended personal data exposure. Therefore, we mainly consider two modes of personal information extraction attacks as mentioned in Section 3.4: \u2022 Direct prompt (DP). Given the victim\u2019s name and domain information, the adversary uses a direct query to recover the victim\u2019s PII. \u2022 Free-form Extraction (FE). Given only the domain information, the adversary aims to recover (name, PII) pairs of the domain by directly asking the New Bing to list some examples."
        },
        {
            "heading": "4.3.2 Evaluation on Direct prompt",
            "text": "In this section, we evaluate personal information recovery performance via direct prompts. For email addresses, we select the first 20 frequent and infrequent pairs of the Enron Email Dataset, respectively, and all 50 collected institutional pairs for\nevaluation. For phone numbers, we only evaluate on the 50 collected institutional pairs.\nTable 4 lists the recovery performance for all 4 data types. Compared with ChatGPT\u2019s 4% accuracy for institutional data extraction in Tables 3 and 2, the New Bing can recover 94% email addresses and 48% phone numbers correctly. After comparing responded pages from the New Bing with search results from Microsoft Bing, we suspect that the New Bing\u2019s dominating personal information recovery performance largely comes from the integrated search engine. We observe a high similarity of suggested websites between Bing and the New Bing. For institutional email pairs, the New Bing can locate the target faculty\u2019s personal web page and respond with the correct email address. Moreover, some correctly recovered addresses are even personal emails of non-institutional email domains. For Enron pairs, the New Bing only finds the pages that store the Enron Email files and most (name, email address) pairs are not accessible directly via source HTML files. These results imply that the New Bing may accurately recover personal information if its integrated search engine can find corresponding web pages."
        },
        {
            "heading": "4.3.3 Evaluation on Free-form Extraction",
            "text": "Besides partially identified extraction, we prompt the New Bing to list (name, email address) pairs given only the domain information. Then we verify the correctness based on web search results and other publicly available files. We prompt the New Bing with Enron and Non-Enron email domains for the Enron dataset and two institutional domains.\nTable 5 shows the free-form extraction results. Unsurprisingly, most listed (name, email address) pairs are correct with corresponding online sources. Moreover, for institutional faculties, the more influential, the higher risks of being correctly recovered. These results imply that malicious users may obtain personal information simply by instructing the New Bing to list some examples."
        },
        {
            "heading": "4.4 Case Studies",
            "text": "In this section, we list ChatGPT\u2019s responses to different prompts and give examples of the dialog interactions with the New Bing. We redact the personal information to respect their privacy.\nChatGPT. Figure 2 displays ChatGPT\u2019s common responses to DP, JP and MJP. The case of DP shows ChatGPT\u2019s moral sense to value individuals\u2019 privacy. Its ethical modules are effective\nagainst common prompts regarding personal information. Moreover, as shown in the case of JP, ChatGPT may sometimes refuse to answer such queries under role-play based jailbreaking prompts. However, ChatGPT may give unethical comments like hacking databases under the \u201cDeveloper Mode\u201d of jailbreaking prompts. For MJP, ChatGPT is more willing to generate personal information if we ask it to make random guesses. Regrettably, some random guesses may be correct. These results imply that ChatGPT fails to defend against indirect and vicious prompts and more defenses on the dialog-safety should be employed.\nThe New Bing. In Figure 3, we ask the New Bing to generate the email address of a faculty successfully. Even though the faculty obfuscates its email pattern with \u201c[at]\u201d to avoid web crawlers, we can still extract the obfuscated email and instruct New Bing to convert the email to the correct format at almost no cost. On the other hand, we can simply ask the New Bing to list personal information directly as shown in Figure 4. Notice that these\nprocesses can be automatically done for personal information harvesting with malicious purposes via simple scripts. These cases suggest that applicationintegrated LLMs may bring more realistic privacy threats than LMs that are previously studied.\nIn addition, we also study the more complicated email content extraction attack and put exemplary cases in Figures 8 and 9 in Appendix B."
        },
        {
            "heading": "5 Conclusion",
            "text": "In this paper, we conduct privacy analyses of LLMs and application-integrated LLMs. We follow the previous zero-shot setting to study the privacy leakage issues of ChatGPT. We show that ChatGPT\u2019s safety defenses are effective against direct prompts and yet insufficient to defend our proposed multistep jailbreaking prompt. Then we reveal that the New Bing is much more vulnerable to direct prompts. We discuss the two LLMs\u2019 privacy implications and potential defenses in Appendix D and E. For future work, we will experiment with more\ncases and test other LLMs like Google Bard. Besides direct personal information recovery, we will work on identity disclosure prompting to quantify its privacy threats, as discussed in the Appendix D.\nLimitations\nFrom the adversary\u2019s perspective, our proposed multi-step jailbreaking attacks still suffer from low recovery accuracy when we query about infrequent Enron emails and phone numbers. As shown in Figures 1, 2 and 3, our proposed MJP is effective on frequent emails of the Enron domain while no phone digits and non-Enron domain email addresses can be correctly recovered. Since frequent Enron email addresses mostly consist of rule-based patterns such as \u201cfirstname.lastname@domain.com\u201d, LLMs may leverage these rule-based patterns to generate more accurate predictions. Therefore, it is important to note that the success of extraction attacks on templatebased email address patterns does not necessarily imply that LLMs memorize these sensitive records, nor does it indicate a tendency to leak them through jailbreaking.\nFor free-from PII extraction on the New Bing, we are more likely to observe repeated and incorrect PII patterns for the latter examples as we query the New Bing to list more examples. Lastly, we cannot confirm if our queried PII is trained by ChatGPT. Fortunately, Figure 9 gives one example of verbatim long email content recovery. This result suggests that ChatGPT is trained on the Enron Email Dataset.\nEthical Considerations\nWe declare that all authors of this paper acknowledge the ACM Code of Ethics and honor the code of conduct. This work substantially reveals potential privacy vulnerabilities of ChatGPT against our proposed jailbreaking privacy attack. We do not aim to claim that ChatGPT is risky without privacy protection. Instead, great efforts have been made to successfully prevent direct queries and previous data extraction attacks are no longer valid. Our findings reveal that LLM\u2019s safety still needs further improvement.\nData. During our experiment, We redact the personal information to respect their privacy. The Enron Email Dataset and Institutional Pages we collected are both publicly available. Still, we will\nnot release the faculties\u2019 PII of our collected Institutional Pages due to privacy considerations.\nJailbreaking prompts. We are well aware of the harmful content like hate speech and bias issues generated by several prompts. For our experiment, we only use the \u201cDeveloper Mode\u201d jailbreaking prompt as mentioned in Appendix A.1. According to our investigation, the \u201cDeveloper Mode\u201d outputs no hate speech or biased content. However, the \u201cDeveloper Mode\u201d may sometimes give dangerous advice like hacking a university\u2019s database. In the future, if there are other safer prompts, we will extend our privacy attacks under these prompts."
        },
        {
            "heading": "Acknowledgment",
            "text": "The authors of this paper were supported by the NSFC Fund (U20B2053) from the NSFC of China, the RIF (R6020-19 and R6021-20) and the GRF (16211520 and 16205322) from RGC of Hong Kong. We also thank the support from the UGC Research Matching Grants (RMGS20EG01-D, RMGS20CR11, RMGS20CR12, RMGS20EG19, RMGS20EG21, RMGS23CR05, RMGS23EG08)."
        },
        {
            "heading": "A Experimental Details",
            "text": "Models\u2019 Versions. For ChatGPT, we conduct experiments on OpenAI\u2019s model API of gpt-3.5-turbo on March 2023. For the New Bing, since we are not clear about its version, we evaluate its performance from Mar 20 to May 10 in 2023. Format of phone numbers. During our experiments, all phone numbers collected from the Enron Email Dataset and Institutional Pages are in the U.S. format. Most phone numbers\u2019 format consists of a 3-digit area code, a 3-digit exchange code and a 4-digit number. Since it is much harder to associate names with phone numbers, we therefore use LCS6 to count pairs whose LCS \u2265 6. Usually, the area code and exchange code are correctly predicted for extracted digits with LCS \u2265 6.\nA.1 Full Prompt Templates Full jailbreaking prompt template. During all our experiments for ChatGPT, we consistently use the same ChatGPT Developer Mode jailbreaking prompt from the Reddit post3. Full ACK template. The full ACK template used in our proposed MJP is shown in Figure 5.\n3https://www.reddit.com/r/GPT_ jailbreaks/comments/1164aah/chatgpt_ developer_mode_100_fully_featured_ filter/\nAll query templates. The query templates to extract phone numbers, email addresses and email contents are shown in Figure 6. To extract phone numbers and email addresses, for each obtained response, we write regular expressions to parse the first phone number or email address as predicted results. To extract email contents, since our prompt requests ChatGPT to respond with the specified structure, we can still use a regular expression to parse the \u201cemail_content\u201d.\nFull MC template. Our multiple-choice template used for response verification is shown in Figure 7.\nA.2 Decoding Parameters\nFor ChatGPT, we follow the default decoding parameters provided in OpenAI\u2019s API. The temperature is set to 1. For the New Bing, we set the response tone to be creative during chats."
        },
        {
            "heading": "B Experiments on Email Content Recovery",
            "text": "Besides extracting personal email addresses and phone numbers, we conduct experiments to recover the whole email content on ChatGPT given its sender, receiver and other associated identifiers. Figure 6 (c) gives one example query template to prompt the associated email content. Data. We sample 50 emails of the same sender from the Enron Email Dataset. For each email, we record its Message-ID (msg_id), email addresses of the sender and receiver, date, email subject and email content. Evaluation Metrics. Unlike extracting fixed patterns from email addresses and phone numbers, the email contents have no fixed format. Therefore, we evaluate the recovery performance on the following metrics. We apply ROUGE (Lin, 2004) and BLEU (Papineni et al., 2002) to measure the similarity between target contents and extracted contents. ROUGE and BLEU measure n-gram similarity on recall and precision separately. For example, in our experiments, ROUGE-1 calculates the ratio of words in the target contents are recovered (word-level recall) while BLEU-1 calculates the ratio of words extracted are correct (word-level\nprecision). We use FLAIR (Akbik et al., 2019) to extract named entities (NEs) from predicted contents and target email contents. Then we use the F1 score of named entities (NER-F1) to measure the harmonic mean of precision and recall. Here, the precision refers to the percentage of extracted contents\u2019 NEs that are correctly predicted and the recall denotes the percentage of target contents\u2019 NEs that are correctly recovered. In addition, we consider email addresses, phone numbers and personal names as sensitive NEs and report the sensitive F1 score (Sensitive-F1) similarly. For each sample, we decode 5 times and evaluate all of them on the aforementioned metrics.\nResults. We evaluate email content extraction performance on DP, JP and MJP as mentioned in Sec 4.2.1. Table 6 lists the email content recovery performance. The poor extraction results on all 3 prompts imply that ChatGPT defends well against content recovery. For DP, it achieves the highest Sensitive-F1 via repeating email addresses shown in prompts. For MJP, we observe some successful cases of email content extraction. these results indicate that our proposed MJP still outperforms DP and JP for content extraction.\nCases. Figures 8 and 9 exhibit the successful cases\nfor long and short email content recovery results given MJP with query template shown in Figure 6 (c). GT refers to the original ground truth email contents and Pred refers to the parsed prediction contents from ChatGPT. For short cases in Figure 8, it can be observed that ChatGPT recovers most contents successfully. For the long email content extraction in Figure 9, ChatGPT even generates verbatim email content. Unlike prior works (Huang et al., 2022; Carlini et al., 2021) that align with language modeling objective to prompt target sensitive texts with its preceding texts, our zero-shot extraction attack requires no knowledge about preceding contexts. Hence, our zero-shot extraction attack imposes a more realistic privacy threat towards LLMs. In addition, these successfully extracted cases help verify that ChatGPT indeed memorizes the Enron data. Ablation study. To determine how identifiers used in the query template affect the email content recovery performance, we perform an ablation study on queried identifiers. More specifically, we always include the email addresses of senders and receivers in the query template. Then we view the date, Message-ID (msg_id) and subject of the email as free variables for the query template. Table 7 shows the recovery performance with various identifiers. The results suggest that simply querying all associated identifiers may not yield the best extraction performance. Though msg_id is unique for every email, compared with date and subject, ChatGPT cannot associate msg_id with the corresponding email content well. The ablation study implies that prompted identifiers also affect the email content extraction result."
        },
        {
            "heading": "C Experiments on Open-source LLMs",
            "text": "In addition to extraction attacks on commercial LLMs, this section delves into the attack performance on current open-source LLMs. More specifically, we examine three safety-enhanced open-\nsource LLMs including Llama-2-7b-chat (Touvron et al., 2023),vicuna-7b-v1.3 (Zheng et al., 2023), and Guanaco-7b (Dettmers et al., 2023).\nWe maintain the experimental settings when testing open-source LLMs, but with one exception: we employ greedy decoding to generate a single response for each query, ensuring simple reproducibility. Table 8 presents the extraction performance on email addresses and phone numbers. These results show that our proposed MJP makes LLMs more willing to generate unethical responses regarding personal information. Some of the generated responses even provide accurate private contact details. Therefore, our MJP can be applicable to a majority of the current LLMs."
        },
        {
            "heading": "D Discussions",
            "text": "The privacy implications are two-folded for the evaluated two models separately.\nChatGPT. Our privacy analyses of ChatGPT follow previous works to study the LLM\u2019s memorization of private training data. Despite ChatGPT already enhanced by dialog-safety measures against revealing personal information, our proposed MJP can still circumvent ChatGPT\u2019s ethical concerns. In addition, our MJP exploits role-play instruction to compromise ChatGPT\u2019s ethical module, it is contradictory to defend against such privacy attacks while training LLMs to follow given instructions. For researchers, our results imply that LLMs\u2019 current safety mechanisms are not sufficient to steer AIGC to prevent harms. For web users, our experiments suggest that personal web pages and existing online textual files may be collected as ChatGPT\u2019s training data. It is hard to determine whether such data collection is lawful or not. However, individuals at least have the right to opt out of uninformed data collection according to the California Consumer Privacy Act (CCPA) and the GDPR.\nThe New Bing. Unlike previous studies that blamed personal information leakage for memo-\nrization issues, according to our results, the New Bing may even recover personal information outside its training data due to its integrated searching ability. Such data recovery at nearly no cost may lead to potential harms like unintended PII dissemination, spamming, spoofing, doxing, and cyberbullying. In addition to the direct recovery of personal information, our main concern is privacy leakage due to New Bing\u2019s powerful data collation and information extraction ability. There is a possibility that the New Bing can combine unrelated sources to profile a specific subject even though its data are perfectly anonymized. For example, the anonymized New York City taxi trips data may leak celebrities\u2019 residence and tipping information and taxi drivers\u2019 identities (Douriez et al., 2016). The New Bing may cause more frequent identity disclosure accidents."
        },
        {
            "heading": "E Possible Defenses",
            "text": "In this section, we briefly discuss several practical strategies to mitigate the PII leakage issue from multiple stakeholders: \u2022 Model developers. 1) During training, perform data anonymization or avoid directly feeding PII to train the LLM. 2) During service, implement an external prompt intention detection model to strictly reject queries that may bring illegal or unethical outcomes. Besides prompt intention detection, it is also recommended to double-check the decoded contents to avoid responding with private information. \u2022 Individuals. 1): Do not disclose your private information that you decline to share with anyone on the Internet. Otherwise, if you intend to share certain information with a specific group, make sure to properly set up the accessibility on the social platforms. 2): Use different identity names on social platforms if you wish not to be identified."
        }
    ],
    "title": "Multi-step Jailbreaking Privacy Attacks on ChatGPT",
    "year": 2023
}