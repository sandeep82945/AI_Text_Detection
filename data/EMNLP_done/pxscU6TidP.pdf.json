{
    "abstractText": "Recent large language models (LLMs) are promising for making decisions in grounded environments. However, LLMs frequently fail in complex decision-making tasks due to the misalignment between the pre-trained knowledge in LLMs and the actual rules in the environment. Existing methods require either costly gradient computation or lengthy in-context demonstrations. In this paper, we propose AutoPlan, an approach to guide LLMbased agents to accomplish interactive decisionmaking tasks. AutoPlan augments the LLM prompt with a task-solving plan and optimizes it through iterative experience collection and reflection. Our experiments show that AutoPlan, though using no in-context demonstrations, achieves success rates on par with the baselines using human-written demonstrations on ALFWorld and even outperforms them by 8% on HotpotQA. The code is available at https://github.com/owaski/AutoPlan.",
    "authors": [
        {
            "affiliations": [],
            "name": "Siqi Ouyang"
        },
        {
            "affiliations": [],
            "name": "Lei Li"
        }
    ],
    "id": "SP:897f304860d2b92739ea1b6c003fcc945faae586",
    "references": [
        {
            "authors": [
                "Thomas Carta",
                "Cl\u00e9ment Romac",
                "Thomas Wolf",
                "Sylvain Lamprier",
                "Olivier Sigaud",
                "Pierre-Yves Oudeyer"
            ],
            "title": "Grounding large language models in interactive environments with online reinforcement learning",
            "year": 2023
        },
        {
            "authors": [
                "Meier-Hellstern",
                "Douglas Eck",
                "Jeff Dean",
                "Slav Petrov",
                "Noah Fiedel"
            ],
            "title": "2022. Palm: Scaling language modeling with pathways",
            "year": 2022
        },
        {
            "authors": [
                "Geunwoo Kim",
                "Pierre Baldi",
                "Stephen McAleer"
            ],
            "title": "Language models can solve computer",
            "year": 2023
        },
        {
            "authors": [
                "Jacky Liang",
                "Wenlong Huang",
                "Fei Xia",
                "Peng Xu",
                "Karol Hausman",
                "brian ichter",
                "Pete Florence",
                "Andy Zeng"
            ],
            "title": "Code as policies: Language model programs for embodied control",
            "venue": "In Workshop on Language and Robotics at CoRL 2022",
            "year": 2022
        },
        {
            "authors": [
                "Yao Lu",
                "Max Bartolo",
                "Alastair Moore",
                "Sebastian Riedel",
                "Pontus Stenetorp."
            ],
            "title": "Fantastically ordered prompts and where to find them: Overcoming fewshot prompt order sensitivity",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Compu-",
            "year": 2022
        },
        {
            "authors": [
                "Kaixin Ma",
                "Hao Cheng",
                "Yu Zhang",
                "Xiaodong Liu",
                "Eric Nyberg",
                "Jianfeng Gao"
            ],
            "title": "Chain-of-skills: A configurable model for open-domain question answering",
            "year": 2023
        },
        {
            "authors": [
                "Paul Christiano",
                "Jan Leike",
                "Ryan Lowe."
            ],
            "title": "Training language models to follow instructions with human feedback",
            "venue": "Advances in Neural Information Processing Systems.",
            "year": 2022
        },
        {
            "authors": [
                "Noah Shinn",
                "Federico Cassano",
                "Beck Labash",
                "Ashwin Gopinath",
                "Karthik Narasimhan",
                "Shunyu Yao"
            ],
            "title": "Reflexion: Language agents with verbal reinforcement learning",
            "year": 2023
        },
        {
            "authors": [
                "Mohit Shridhar",
                "Xingdi Yuan",
                "Marc-Alexandre Cote",
                "Yonatan Bisk",
                "Adam Trischler",
                "Matthew Hausknecht."
            ],
            "title": "ALFW}orld: Aligning text and embodied environments for interactive learning",
            "venue": "International Conference on Learning Representa-",
            "year": 2021
        },
        {
            "authors": [
                "Taylor Sorensen",
                "Joshua Robinson",
                "Christopher Rytting",
                "Alexander Shaw",
                "Kyle Rogers",
                "Alexia Delorey",
                "Mahmoud Khalil",
                "Nancy Fulda",
                "David Wingate."
            ],
            "title": "An information-theoretic approach to prompt engineering without ground truth labels",
            "venue": "Proceed-",
            "year": 2022
        },
        {
            "authors": [
                "Haotian Sun",
                "Yuchen Zhuang",
                "Lingkai Kong",
                "Bo Dai",
                "Chao Zhang"
            ],
            "title": "Adaplanner: Adaptive planning from feedback with language models",
            "year": 2023
        },
        {
            "authors": [
                "Zihao Wang",
                "Shaofei Cai",
                "Anji Liu",
                "Xiaojian Ma",
                "Yitao Liang"
            ],
            "title": "Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents",
            "year": 2023
        },
        {
            "authors": [
                "Jiannan Xiang",
                "Tianhua Tao",
                "Yi Gu",
                "Tianmin Shu",
                "Zirui Wang",
                "Zichao Yang",
                "Zhiting Hu"
            ],
            "title": "Language models meet world models: Embodied experiences enhance language models",
            "year": 2023
        },
        {
            "authors": [
                "Zhilin Yang",
                "Peng Qi",
                "Saizheng Zhang",
                "Yoshua Bengio",
                "William Cohen",
                "Ruslan Salakhutdinov",
                "Christopher D. Manning."
            ],
            "title": "HotpotQA: A dataset for diverse, explainable multi-hop question answering",
            "venue": "Proceedings of the 2018 Conference on Empiri-",
            "year": 2018
        },
        {
            "authors": [
                "Shunyu Yao",
                "Jeffrey Zhao",
                "Dian Yu",
                "Nan Du",
                "Izhak Shafran",
                "Karthik R Narasimhan",
                "Yuan Cao."
            ],
            "title": "React: Synergizing reasoning and acting in language models",
            "venue": "The Eleventh International Conference on Learning Representations.",
            "year": 2023
        },
        {
            "authors": [
                "Wang",
                "Luke Zettlemoyer"
            ],
            "title": "Opt: Open pretrained transformer language models",
            "year": 2022
        },
        {
            "authors": [
                "Robert Hines"
            ],
            "title": "dancer, musician, and actor.[2] Hines began his career at the age of five, studying tap dance at the Henry LeTang Dance Studio in Manhattan.[3] ... Maurice made his Broadway debut in The Girl in Pink Tights in 1954.[4",
            "year": 1954
        }
    ],
    "sections": [
        {
            "text": "1 Introduction\nThe ability to make decisions lies at the core of human intelligence, enabling us to navigate through a multitude of choices and select the best possible actions based on available information. Recent large language models, trained with trillions of tokens, have gained impressive reasoning ability and now have the potential to act as autonomous agents for decision-making tasks in grounded environments (Zhang et al., 2022; Chowdhery et al., 2022; OpenAI, 2023; Touvron et al., 2023).\nDecision-making tasks in grounded environments can be as simple as calculating mathematical problems with an external calculator or as complex as doing housework. Current LLM can easily use an external calculator by decomposing the formula into atomic function calls (Bubeck et al., 2023). However, LLMs frequently fail in more complex tasks in an environment with many objects and prerequisite dependencies. Considering the Heat task\nin ALFWorld (Shridhar et al., 2021)), LLM agents struggle to find the correct action sequence within the maximum number of actions (Figure 1). The primary reason behind such failures is the misalignment between LLM\u2019s pre-trained knowledge (e.g., generating fluent sentences) and the concrete rule of the grounded environment (e.g., household item functionality in ALFworld). In the ALFWorld environment, the agent can only heat an object with a microwave instead of a toaster. However, the LLM does not learn such knowledge during pretraining, eventually failing the task.\nExisting methods aligning LLMs to desired environments either employ reinforcement learning (RL) and imitation learning (IL) methods (Ouyang et al., 2022; Carta et al., 2023), or provide a few demonstrations to conduct in-context learning (ICL) (Yao et al., 2023). On the one hand, RL and IL methods require computationally costly gradient computation for existing LLMs. On the other hand, the performance of ICL methods highly depends on the selection of demonstrations.\nIn this work, we propose AutoPlan, a purely\nprompt-based method, to guide an LLM to solve such decision-making tasks without costly gradient computation or in-context demonstrations. In high-level speaking, AutoPlan solves the task by iteratively interacting with the given environment conditioning on a task plan described in natural language. Figure 2 illustrates how AutoPlan finds an optimal plan to guide the LLM to heat an object correctly and put it at the target location. AutoPlan starts with an empty plan and uses the LLM agent to collect interaction experiences conditioning on an initial incorrect plan. Then AutoPlan instructs the LLM agent to reflect on the collected experiences and revise the task plan based on the reflection. It further deploys the new task plan to collect more experiences with the LLM agent.\nThe primary technical challenge of this approach is to ensure stable and progressive plan optimization since the plan expressed in natural language can be highly slapdash and versatile. We propose two techniques in AutoPlan: (1) experience batching and (2) SIR reflection. We batch multiple experiences before updating the plan to help reduce variance. We introduce an explicit SIR reflection (Summarization, flaw Identification, plan Revision) to elicit helpful information from the interaction ex-\nperience. We evaluate AutoPlan and other methods on two distinct benchmarks.\nOur contributions are: \u2022 We propose AutoPlan, a novel prompting method\nto align LLMs with the need for grounded decision-making tasks without computing gradients or using human-written demonstrations.\n\u2022 Our experiments show that AutoPlan achieves on-par success rates with baselines involving human-written demonstrations on ALFworld (Shridhar et al., 2021) and even 8% higher accuracy on HotpotQA (Yang et al., 2018).\n\u2022 We verify that larger batch size leads to more stable learning, and the explicit SIR reflection ensures the plan update is practical and progressive.\n2 Related Works\nFinetuned LLM Agent Reinforcement Learning has been widely used to train LLMs to master interactive tasks. ChatGPT (OpenAI, 2023) applies Reinforcement with Human Feedback (RLHF) to finetune a pre-trained LLM, enabling it to communicate interactively with humans. GLAM (Carta et al., 2023) uses LLM as a policy and finetunes it with online RL to improve its abilities to solve text-\nbased decision-making tasks. Experiments demonstrate that LLM policy significantly boosts sample efficiency. Other than RL, Xiang et al. also finetunes the LLM in a supervised manner with experiences collected through Monte Carlo Tree Search (MCTS). However, RL and supervised learning require calculating the gradients and updating the model\u2019s parameters, which is especially costly for LLMs.\nLLM with In-Context Learning As the size of the model and corpus scales, LLM demonstrates In-Context Learning (ICL) abilities, i.e., LLM directly learns from a few demonstrations of a task given in the context. Brown et al. shows that a pre-trained LLM performs strongly on traditional NLP tasks, including question answering and cloze tasks, with ICL. More recent works focus on the design of demonstrations (Sorensen et al., 2022; Lu et al., 2022). Sorensen et al. proposes to retrieve demonstrations with higher mutual information between model input and output. GlobalE&LocalE (Lu et al., 2022) uses entropy statistics to find the most performant permutation of demonstrations. Nonetheless, the ICL LLM agent is still sensitive to the provided demonstrations and requires additional human effort.\nPrompt-based LLM Agent Techniques have recently been developed to adapt LLMs to solve decision-making tasks through prompts. Table 1 illustrates the main difference between works along this line. ReAct (Yao et al., 2023) explicitly reasons over past interactions and determines the following action based on previous thoughts, actions, and observations. Reflexion (Shinn et al., 2023) built on top of ReAct and refines the interaction by iteratively reflecting on its past failed trials of a task\ninstance. However, Reflexion conducts test-time reflection and the reflection for one environment does not transfer to others. RCI (Kim et al., 2023), DEPS (Wang et al., 2023) and AdaPlanner (Sun et al., 2023) start with an initial plan of the task and refine the plan and the decision-making process for each specific task instance. Our AutoPlan instead optimizes a task-level plan and directly applies it to all task instances without further test-time refinement, which could be more efficient during inference.\n3 AutoPlan\nIn this section, we describe AutoPlan in detail. We first describe the general procedure of using LLM to solve an interactive decision-making task. Then we present AutoPlan that solves the task by a textbased plan, obtained by an iterative three-stage process: AutoPlan 1) collects interaction experiences using the task plan at the current step, 2) reflects on the collected experiences, and 3) updates the plan.\n3.1 Problem Formulation\nWe aim to design an LLM-based agent to accomplish an interactive task described in natural language. The agent is provided with a natural language description of the task, possible actions, and environmental observations. The task description P includes a generic abstract description and a concrete task instance with an objective. Let M be the LLM agent, A be the set of possible actions, and O be the set of possible observations from the environment. One could augment the input with a custom prompt X . At each step t, the agent M generates a text action at \u2208 A and receives a text observation ot \u2208 O from the environment. o0 denotes the initial observation, which could be empty. We define\na reward function R(o0:t) = 1 if the objective is achieved based on the observations. The problem of AutoPlan is to design an optimal prompt X to maximize the expected rewards over all possible task instances,\nX \u2217 = argmax X EP [R(o0:T )] , (1)\nwhere T is the maximum number of interaction steps allowed.\nIdeally, the optimal X \u2217 should be adequate for all task instances of the same problem. Since the space of a custom prompt is vast, we frame such a prompt as a plan, which describes a sequence of actions in natural languages. Figure 2 shows a heating task in ALFWorld (Shridhar et al., 2021) and how the LLM agent solves this. Task description includes the available actions and an instance-wise objective (e.g., put a hot bowl in the sidetable). We aim to find an optimal plan as the custom prompt. After the task starts, the agent\u2019s current and visible locations constitute the first observation o0. Then, the agent acts and observes the environment iteratively until it reaches the maximum number of interaction steps T .\nFollowing prior work ReAct (Yao et al., 2023), we extend the original action space A to include L, the space of thoughts expressed in language. As shown in Figure 2, a \"thought\" action (in the form of \"Think[...]\") does not elicit any environmental feedback and solely manifests the reasoning process of the LLM.\n3.2 AutoPlan\nAutoPlan treats a custom prompt X in the form of a task-solving plan that includes a sequence of abstract actions to execute in different scenarios. Such a plan described in natural language resembles the policy network in deep reinforcement learning, but it is more explainable due to its textual form. It is also more token-efficient than in-context demonstrations. Furthermore, stateof-the-art instruction-tuned LLMs demonstrate a strong ability to follow a given plan.\nAs shown in Figure 2, we design a three-stage process to optimize plan X iteratively: 1) experience collection with the current plan, 2) reflection on the collected experiences, and 3) plan update based on reflection.\n3.2.1 Experience Collection AutoPlan starts with an empty plan X0. At each iteration i, a batch of B task instances is randomly selected, denoted as P1, P2, \u00b7 \u00b7 \u00b7 , PB . For each task instance Pj , the LLM agent generates a sequence of thoughts and actions in response to observations from the environment.\nLet Hjt\u22121 = Pj\u2295Xi\u2295(o0, a\u03030, a0, o1, \u00b7 \u00b7 \u00b7 , ot\u22121) be the past interactions before step t. Since we augment the action space with thoughts that do not affect on the environment, at each step t, AutoPlan first obtains the thought,\na\u0303t \u223c M(Hjt\u22121 \u2295 Thought-prompt) (2)\nwhere Thought-prompt is provided in Table 2 to make LLM agent act faithfully to the plan Xi. Then\nwe sample the next action given the thought a\u0303t,\na\u2032t \u223c M(H j t\u22121 \u2295 a\u0303t \u2295 \"Action:\") (3)\nat = F(a\u2032t) (4) Hjt = H j t\u22121 \u2295 a\u0303t \u2295 at \u2295 ot. (5)\nwhere ot is the observation after action at and F is a formalizer used to reformat the action to be acceptable by the environment. Details of the formalizer can be found in Appendix A.1.\nAs shown in Figure 2, a\u0303t, at and ot correspond to \"Think[...]\", \"Action[...]\" and \"Observation[...]\" in the experience of a task instance, where the LLM agent successfully found the bowl on the sidetable but failed to heat it with the toaster.\n3.2.2 SIR Reflection Given the experience HjT and the corresponding reward R(o0:T ) (denoted as Rj), we instruct the LLM agent to reflect on the interaction history through a SIR reflection procedure: 1) Summarize the interaction history, 2) Identify the flawed steps of the plan, 3) Revise the flawed steps,\nsj = M(Hj \u2295Rj \u2295 Summary-prompt) (6) fj = M(Hj \u2295Rj \u2295 Flaw-prompt) (7) rj = M(Hj \u2295Rj \u2295 Flaw-prompt\n\u2295 Rev-prompt) (8)\nwhere Summary/Flaw/Rev-prompts are shown in Table 2. The summarization, flaw, and revision provide necessary and clear information for the plan updater to modify the current plan.\nAs shown in Figure 2, the reflection summarizes the key actions, successfully identifies the flaw part of the plan, where Xi treats the toaster as the appropriate heating appliance, and suggests a revision to use the microwave instead.\n3.2.3 Plan Update With the task descriptions P1, P2, \u00b7 \u00b7 \u00b7 , PB , the current task plan Xi, and the summarizations s1, \u00b7 \u00b7 \u00b7 , sB , identified flaws f1, \u00b7 \u00b7 \u00b7 , fB and revisions r1, \u00b7 \u00b7 \u00b7 , rB , we utilize the LLM to revise Xi and obtain an improved plan Xi+1,\nXi+1 = M(Xi \u2295 (P1, s1, f1, r1)\u2295 \u00b7 \u00b7 \u00b7 \u2295(PB, sB, fB, rB)\u2295 Upd-prompt) (9)\nwhere Upd-prompt (as shown in Table 2) asks the LLM to generate an updated plan given the task instances and reflections.\nIn the example of Figure 2, the plan updater aggregates the task instances with their reflections and rewrites the new plan to use the microwave to heat the target objects instead.\nAfter obtaining a revised plan Xi+1, we continue the iterative process until we reach maximum optimization iterations I . During inference, we follow the same procedure as experience collection except that now we use the final optimized plan XI .\nTo summarize, AutoPlan uses LLM to solve a text-based interactive decision-making task through a task plan described in natural language. The plan is optimized iteratively through a threestage process. The final plan is then used during inference time.\n4 Experiment\nWe aim to answer the following questions: 1) Does AutoPlan improve upon baselines? 2) Is AutoPlan efficient during inference? 3) Does batching stabilize the optimization? 4) Does trio reflection ensure steady progression?\n4.1 Data\nALFWorld is a text-based game enabling agents to navigate and interact with a simulated household to accomplish six types of tasks. Each task instance comes with a high-level objective (e.g., put a hot tomato on the desk), and the agent can achieve the objective through low-level actions described in text (e.g., heat tomato 1 with microwave 2, go to desk 1). Since the environment feedback of invalid actions provided in the original ALFWorld is too primitive, we manually augment the feedback (Table 6) to include possible causes of the invalidity. Further details of ALFWorld can be found in the Appendix B.1.\nWe randomly sample 24 task instances for each type of task from the training games to optimize the task-specific plan and, following prior works (Shridhar et al., 2021; Yao et al., 2023), use 134 unseen validation games1 to evaluate our method. ALFWorld evaluates the success/failure of a task instance by checking if the agent is in the goal state (e.g. if the hot mug is already on the desk).\nHotpotQA is a multi-hop question answering benchmark requiring reasoning over two or more\n1Unseen games have the same task types but different objects, receptacles and household layout.\nWikipedia pages. As in (Yao et al., 2023), the LLM agent is required to answer questions by interacting with a Wikipedia API. The API supports three types of actions: (1) search[entity]: returns the first five sentences from the Wikipedia page of the entity if it exists or suggests top-5 similar entities2. (2) lookup[string]: returns the following sentence containing string. (3) finish[answer]: finishes the task with an answer.\nWe randomly sample 50 hard (question, answer, supporting facts) triples from the official training set to optimize the plan and sample 200 questions from the official development set as the test set3. The final answer is evaluated by three external human annotators rather than exact-match (EM) since the answer provided by the agent and the gold answer can differ drastically in form but share the same meaning. We include the complete annotation instruction in the Appendix B.2 and take the majority vote of 3 annotators as the final evaluation result. The agreement rate (all three annotators agree with each other) is above 90% for all considered models.\n2We notice that this API retrieves the latest information instead of the Wikipedia dump (2017-10-01) used to build HotpotQA dataset, so we modify it to return the historical page of entities before 2017-10-01.\n3200 is a trade-off between our budget and evaluation uncertainty.\n4.2 Method Configurations\nWe use GPT-4-0314 (OpenAI, 2023) as the LLM across all experiments. The maximum number of actions is 10 for HotpotQA and 35 for ALFWorld. The default batch size of task instances is 4 for both HotpotQA and ALFWorld. We use nucleus sampling with p = 0.9 during optimization and greedy decoding during evaluation. The full prompt templates of both environments can be found in the Appendix A.2.\n4.3 Baselines\nWe compare with the following baselines. \u2022 ReAct (K Shot): The custom prompt X consists\nof K demonstrations manually written by human. We reuse the demonstrations provided in (Yao et al., 2023). We have K = 6 for HotpotQA and K = 2 for ALFWorld.\n\u2022 Reflexion (K Shot): Built on top of ReAct, Reflexion conducts iterative test-time reflection for each environment, using the interaction history to revise the actions in the following iterations. We set the number of iterations to be five and use the same custom prompt as in ReAct.\n\u2022 AdaPlanner (Sun et al., 2023) (K Shot): AdaPlanner also proposes to optimize the plan with LLM but using a code-style custom prompt, which is more rigorous but also more restric-\ntive than AutoPlan. Note that AdaPlanner still requires human-written demonstrations to initialize the plan.\n\u2022 Supervised Baseline: For HotpotQA, we select the best available supervised method Chain-ofSkills (Ma et al., 2023) from the leaderboard of fullwiki setting. For ALFWorld, we choose BUTLER (Shridhar et al., 2021), an imitation learning agent trained with 105 human demonstrations for each task type.\n4.4 Main Results\nSuccess Rates The success rate and accuracy of AutoPlan and baselines in ALFWorld and HotpotQA are shown in Table 3 respectively. In ALFWorld, AutoPlan achieves on-par success rates with ReAct (2 Shot), AdaPlanner (1 Shot), and Reflexion (2 Shot) on all six types of tasks and outperforms zero-shot baselines by at most 44% on Heat task. Notably, AutoPlan accomplishes the first four tasks nearly perfectly with success rates approaching 100% and success rates above 90% and 80% for the latter two. In HotpotQA, AutoPlan answers questions even 8% more accurately than ReAct (6 Shot) with human-written demonstrations of how to use the search tool, thanks to the optimized plan.\nError Analysis Of 137 ALFWorld test instances, AutoPlan fails seven due to the inability to locate the target object. One failure stems from a lexical misunderstanding where the LLM confuses a \u201ccup\u201d with a \u201cmug\u201d. Another results from an atypical object location, with the apple to be heated found in a garbage can. The remaining five failures occur due to the LLM\u2019s erroneous prior assumptions about potential object locations, even though the plan points the model towards the most probable ones. Once the agent locates the task instance\u2019s target object(s), it performs all subsequent actions correctly. We observe similar failure patterns in cases of ReAct (2 Shot). With neither the optimized plan nor incontext demonstrations, ReAct (0 Shot) struggles to find the correct action sequence to clean/cool/heat the object even if it finds the target object(s).\nIn HotpotQA, AutoPlan achieves better logical consistency than ReAct (0/6 Shot) thanks to the step-by-step plan. ReAct (6 Shot) performs well when only a few actions are needed but can diverge to unreasonable thought and action processes when the number of actions is considerable. One primary reason is that the demonstrations used in ReAct (6 Shot) involve no more than five actions, which\nagain shows that the ICL method is sensitive to the quality of demonstrations.\nTraining and Inference Cost We measure the training and inference cost of AutoPlan and baselines per instance in Table 4. The cost is calculated based on the official documentation4. AutoPlan requires only marginal additional cost compared to ReAct (0 Shot) while achieving the best result on ALFWorld and HotpotQA.\n4.5 Ablation Study\nThe plan optimization process of AutoPlan can be precarious due to sampling-based decoding. To tackle this, AutoPlan batches multiple task instances together in one iteration to stabilize the optimization and applies an explicit 3-step reflection to elicit helpful information from the interaction\n4https://openai.com/pricing\nhistory. Here, we demonstrate the effectiveness of batching and reflection on task Heat of ALFWorld as this is the task that AutoPlan achieves the largest improvement against the baseline ReAct (0 Shot) with no plan. We first run AutoPlan five times with both batch sizes 2, 4, and 8, and then run five times with and without the last two steps of reflection (flaw identification and revision)5. Then, we measure the mean and standard deviation of test success rates of plans produced in the first three iterations.\nLarger batch size significantly stabilizes the optimization process. As shown in Figure 3a, a larger batch size improves the average success rate and reduces the standard deviation during optimization. We also conducted a t-test comparing batch size 2 and 8 results, and the p-value is no more than 0.110 for all iterations (see Table 5). Carefully examining the interaction histories, we find that with a larger batch size, the agent is more likely to hit the right action during the experience collection stage. As illustrated in Figure 4, the agent with batch size 2 only tried the toaster to heat the object, but with batch size 4, the agent also tried the microwave, the only correct heating appliance for this task.\n5We keep the summary step of reflection since the plan update is meaningless without the interaction summary.\nReflection ensures the optimization goes in the right direction. As shown in Figure 3b, AutoPlan with the complete reflection obtains steady improvements after each iteration, while the success rate of AutoPlan with only the interaction summary bounces back and forth between 0% and 30%, even below the success rate of ReAct (0 Shot). Again we can visualize such a difference in Figure 5. The agent went to the microwave and tried to heat the object but failed because of the wrong action sequence (the correct action sequence can be found in Table 8). AutoPlan with complete reflection explicitly identifies such flawed behavior from the observation and proposes a revision, which is later integrated into the new plan. However, AutoPlan without flaw identification and revision does not realize the valid reason for failure and leads to undesired plan updates.\n5 Conclusion\nWe propose AutoPlan, a prompt-based method, to enable LLM to solve interactive decision-making tasks without gradient computation or in-context demonstrations. AutoPlan conditions LLM on an additional task plan described in natural language, which is obtained through an iterative three-stage process. Experiments show that AutoPlan achieves better results than baselines and is also efficient during inference. The ablation study further confirms the effectiveness of batching and explicit reflection\nin stabilizing the plan optimization process.\nLimitation\nThe improvements of AutoPlan mainly come from two sources: 1) the correct action sequence sampled during exploration; 2) the environment feedback when incorrect actions are sampled by the LLM agent. As shown in Table 6, the feedback directly tells the agent which aspect of the action is invalid. Without such fine-grained feedback, the agent needs to collect more experience, i.e., larger batch size, to make sure the correct action sequence is sampled with high probability.\nAnother limitation is that in order to make AutoPlan works without any demonstration, we rely on GPT-4-0314 to generate action sequences, reflect on the interactions, and update the plan. We tried to use GPT-3.5-turbo-0301, but find out 1) it fails to follow the plan faithfully even explicitly prompted to do so; 2) it generates too many hallucinated contents about the environment, which could (possibly) be handled by better prompt design, but that requires excessive human effort, contradicting the goal of AutoPlan to reduce human effort as much as possible. It is worth trying other state-ofthe-art LLMs such as Claude6 to see which one also works.\nEthics Statement\nWhile AutoPlan is capable of functioning solely with task descriptions and observations, it is imperative to exercise caution while using it in highstakes circumstances, given the inherent unpredictability of LLMs. Furthermore, we earnestly recommend that users carefully assess if the objectives could inadvertently cause harm to others before putting AutoPlan into action.\n6https://www.anthropic.com/index/introducing-claude\nReferences Tom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems, volume 33, pages 1877\u20131901. Curran Associates, Inc.\nS\u00e9bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, and Yi Zhang. 2023. Sparks of artificial general intelligence: Early experiments with gpt-4.\nThomas Carta, Cl\u00e9ment Romac, Thomas Wolf, Sylvain Lamprier, Olivier Sigaud, and Pierre-Yves Oudeyer. 2023. Grounding large language models in interactive environments with online reinforcement learning.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy\nMeier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2022. Palm: Scaling language modeling with pathways.\nWenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, Pierre Sermanet, Tomas Jackson, Noah Brown, Linda Luu, Sergey Levine, Karol Hausman, and brian ichter. 2023. Inner monologue: Embodied reasoning through planning with language models. In Proceedings of The 6th Conference on Robot Learning, volume 205 of Proceedings of Machine Learning Research, pages 1769\u20131782. PMLR.\nGeunwoo Kim, Pierre Baldi, and Stephen McAleer. 2023. Language models can solve computer tasks.\nJacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, brian ichter, Pete Florence, and Andy Zeng. 2022. Code as policies: Language model programs for embodied control. In Workshop on Language and Robotics at CoRL 2022.\nYao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. 2022. Fantastically ordered prompts and where to find them: Overcoming fewshot prompt order sensitivity. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 8086\u20138098, Dublin, Ireland. Association for Computational Linguistics.\nKaixin Ma, Hao Cheng, Yu Zhang, Xiaodong Liu, Eric Nyberg, and Jianfeng Gao. 2023. Chain-of-skills: A configurable model for open-domain question answering.\nOpenAI. 2023. Gpt-4 technical report.\nOpenAI. 2023. Introducing ChatGPT.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Gray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback. In Advances in Neural Information Processing Systems.\nNoah Shinn, Federico Cassano, Beck Labash, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. 2023. Reflexion: Language agents with verbal reinforcement learning.\nMohit Shridhar, Xingdi Yuan, Marc-Alexandre Cote, Yonatan Bisk, Adam Trischler, and Matthew Hausknecht. 2021. {ALFW}orld: Aligning text and embodied environments for interactive learning. In International Conference on Learning Representations.\nTaylor Sorensen, Joshua Robinson, Christopher Rytting, Alexander Shaw, Kyle Rogers, Alexia Delorey, Mahmoud Khalil, Nancy Fulda, and David Wingate. 2022. An information-theoretic approach to prompt engineering without ground truth labels. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 819\u2013862, Dublin, Ireland. Association for Computational Linguistics.\nHaotian Sun, Yuchen Zhuang, Lingkai Kong, Bo Dai, and Chao Zhang. 2023. Adaplanner: Adaptive planning from feedback with language models.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023. Llama: Open and efficient foundation language models.\nZihao Wang, Shaofei Cai, Anji Liu, Xiaojian Ma, and Yitao Liang. 2023. Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents.\nJiannan Xiang, Tianhua Tao, Yi Gu, Tianmin Shu, Zirui Wang, Zichao Yang, and Zhiting Hu. 2023. Language models meet world models: Embodied experiences enhance language models.\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. 2018. HotpotQA: A dataset for diverse, explainable multi-hop question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2369\u20132380, Brussels, Belgium. Association for Computational Linguistics.\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R Narasimhan, and Yuan Cao. 2023. React: Synergizing reasoning and acting in language models. In The Eleventh International Conference on Learning Representations.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. 2022. Opt: Open pretrained transformer language models.\nA Detailed Implementation of AutoPlan\nA.1 Formalizer The formalizer is again a LLM call with specially designed prompt as shown in Figure 6.\nA.2 Full Prompt of AutoPlan Full prompts of ALFWorld and HotpotQA are shown in Figure 7 (experience collection and reflection) and Figure 8 (plan update).\nA.3 Feedback The examples of augmented feedback of ALFWorld are shown in Table 6. We do not add additional feedback for HotpotQA upon the original one designed in ReAct (Yao et al., 2023).\nB Additional Details in Experiments\nB.1 Environments The task types and templates of task objectives of ALFWorld are listed in Table 7. The allowed actions can be found in Figure 7. The correct action sequences for each task can be found in Table 8.\nB.2 Human Evaluation We invite three external human annotators to conduct human evaluation on HotpotQA. Instructions for human annotators are shown in Figure 9. We take the majority votes from human annotators as accuracy and also compute the agreement among three annotators.\nB.3 Significant Test We conduct t-test between success rates of plans generated by batch size 2, 4 and 8 at each iteration. The p-values are shown in Table 5.\nAnnotation Instructions Objective The primary objective is to evaluate the quality of predicted answers generated by an automated method against the ground-truth answers for a set of 200 data points from the HotpotQA dataset. Each data point consists of a question, its corresponding ground-truth answer, supporting facts, and a predicted answer."
        }
    ],
    "title": "AutoPlan: Automatic Planning of Interactive Decision-Making Tasks With Large Language Models",
    "year": 2023
}