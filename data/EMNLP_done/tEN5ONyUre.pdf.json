{
    "abstractText": "Yes-no questions expect a yes or no for an answer, but people often skip polar keywords. Instead, they answer with long explanations that must be interpreted. In this paper, we focus on this challenging problem and release new benchmarks in eight languages. We present a distant supervision approach to collect training data. We also demonstrate that direct answers (i.e., with polar keywords) are useful to train models to interpret indirect answers (i.e., without polar keywords). Experimental results demonstrate that monolingual fine-tuning is beneficial if training data can be obtained via distant supervision for the language of interest (5 languages). Additionally, we show that cross-lingual fine-tuning is always beneficial (8 languages).",
    "authors": [
        {
            "affiliations": [],
            "name": "Zijie Wang"
        },
        {
            "affiliations": [],
            "name": "Md Mosharaf Hossain"
        },
        {
            "affiliations": [],
            "name": "Shivam Mathur"
        },
        {
            "affiliations": [],
            "name": "Terry Cruz Melo"
        },
        {
            "affiliations": [],
            "name": "Kadir Bulut Ozler"
        },
        {
            "affiliations": [],
            "name": "Keun Hee Park"
        },
        {
            "affiliations": [],
            "name": "Jacob Quintero"
        },
        {
            "affiliations": [],
            "name": "MohammadHossein Rezaei"
        },
        {
            "affiliations": [],
            "name": "Shreya Nupur Shakya"
        },
        {
            "affiliations": [],
            "name": "Md Nayem Uddin"
        },
        {
            "affiliations": [],
            "name": "Eduardo Blanco"
        }
    ],
    "id": "SP:7ee51d4d2ff8a5d9393f89473c66258f6e64b035",
    "references": [
        {
            "authors": [
                "Mikel Artetxe",
                "Vedanuj Goswami",
                "Shruti Bhosale",
                "Angela Fan",
                "Luke Zettlemoyer."
            ],
            "title": "Revisiting machine translation for cross-lingual classification",
            "venue": "arXiv preprint arXiv:2305.14240.",
            "year": 2023
        },
        {
            "authors": [
                "Mikel Artetxe",
                "Sebastian Ruder",
                "Dani Yogatama."
            ],
            "title": "On the cross-lingual transferability of monolingual representations",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4623\u20134637, Online. Association",
            "year": 2020
        },
        {
            "authors": [
                "Ron Artstein",
                "Massimo Poesio."
            ],
            "title": "Survey article: Inter-coder agreement for computational linguistics",
            "venue": "Computational Linguistics, 34(4):555\u2013596.",
            "year": 2008
        },
        {
            "authors": [
                "Emily M. Bender",
                "Batya Friedman."
            ],
            "title": "Data statements for natural language processing: Toward mitigating system bias and enabling better science",
            "venue": "Transactions of the Association for Computational Linguistics, 6:587\u2013604.",
            "year": 2018
        },
        {
            "authors": [
                "Abhik Bhattacharjee",
                "Tahmid Hasan",
                "Wasi Ahmad",
                "Kazi Samin Mubasshir",
                "Md Saiful Islam",
                "Anindya Iqbal",
                "M. Sohel Rahman",
                "Rifat Shahriyar"
            ],
            "title": "BanglaBERT: Language model pretraining and benchmarks for low-resource language",
            "year": 2022
        },
        {
            "authors": [
                "Penelope Brown",
                "Stephen C Levinson."
            ],
            "title": "Universals in language usage: Politeness phenomena",
            "venue": "Questions and politeness: Strategies in social interaction, pages 56\u2013311. Cambridge University Press.",
            "year": 1978
        },
        {
            "authors": [
                "Alexandra Canavan",
                "George Zipperlen"
            ],
            "title": "CALLFRIEND Spanish-Non-Caribbean Dialect",
            "year": 1996
        },
        {
            "authors": [
                "Nicholas Carlini",
                "Florian Tramer",
                "Eric Wallace",
                "Matthew Jagielski",
                "Ariel Herbert-Voss",
                "Katherine Lee",
                "Adam Roberts",
                "Tom B Brown",
                "Dawn Song",
                "Ulfar Erlingsson"
            ],
            "title": "Extracting training data from large language models",
            "year": 2021
        },
        {
            "authors": [
                "Casimiro Pio Carrino",
                "Marta R. Costa-juss\u00e0",
                "Jos\u00e9 A.R. Fonollosa."
            ],
            "title": "Automatic Spanish translation of SQuAD dataset for multi-lingual question answering",
            "venue": "Proceedings of the Twelfth Language Resources and Evaluation Conference, pages 5515\u2013",
            "year": 2020
        },
        {
            "authors": [
                "Zewen Chi",
                "Li Dong",
                "Bo Zheng",
                "Shaohan Huang",
                "XianLing Mao",
                "Heyan Huang",
                "Furu Wei."
            ],
            "title": "Improving pretrained cross-lingual language models via self-labeled word alignment",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Compu-",
            "year": 2021
        },
        {
            "authors": [
                "Zewen Chi",
                "Shaohan Huang",
                "Li Dong",
                "Shuming Ma",
                "Bo Zheng",
                "Saksham Singhal",
                "Payal Bajaj",
                "Xia Song",
                "Xian-Ling Mao",
                "Heyan Huang",
                "Furu Wei."
            ],
            "title": "XLM-E: Cross-lingual language model pre-training via ELECTRA",
            "venue": "Proceedings of the 60th Annual",
            "year": 2022
        },
        {
            "authors": [
                "Eunsol Choi",
                "He He",
                "Mohit Iyyer",
                "Mark Yatskar",
                "Wentau Yih",
                "Yejin Choi",
                "Percy Liang",
                "Luke Zettlemoyer"
            ],
            "title": "QuAC: Question answering in context",
            "year": 2018
        },
        {
            "authors": [
                "Christopher Clark",
                "Kenton Lee",
                "Ming-Wei Chang",
                "Tom Kwiatkowski",
                "Michael Collins",
                "Kristina Toutanova."
            ],
            "title": "BoolQ: Exploring the surprising difficulty of natural yes/no questions",
            "venue": "Proceedings of the 2019 Conference of the North American Chap-",
            "year": 2019
        },
        {
            "authors": [
                "Alexis Conneau",
                "Kartikay Khandelwal",
                "Naman Goyal",
                "Vishrav Chaudhary",
                "Guillaume Wenzek",
                "Francisco Guzm\u00e1n",
                "Edouard Grave",
                "Myle Ott",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Unsupervised cross-lingual representation learning at scale",
            "venue": "Pro-",
            "year": 2020
        },
        {
            "authors": [
                "Cathrine Damgaard",
                "Paulina Toborek",
                "Trine Eriksen",
                "Barbara Plank."
            ],
            "title": "I\u2019ll be there for you\u201d: The one with understanding indirect answers",
            "venue": "Proceedings of the 2nd Workshop on Computational Approaches to Discourse, pages 1\u201311, Punta Cana, Do-",
            "year": 2021
        },
        {
            "authors": [
                "Maxime De Bruyn",
                "Ehsan Lotfi",
                "Jeska Buhmann",
                "Walter Daelemans."
            ],
            "title": "MFAQ: a multilingual FAQ dataset",
            "venue": "Proceedings of the 3rd Workshop on Machine Reading for Question Answering, pages 1\u201313, Punta Cana, Dominican Republic. Association for",
            "year": 2021
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
            "year": 2019
        },
        {
            "authors": [
                "Nancy Green",
                "Sandra Carberry."
            ],
            "title": "Interpreting and generating indirect answers",
            "venue": "Computational Linguistics, 25(3):389\u2013435.",
            "year": 1999
        },
        {
            "authors": [
                "Julia Bell Hirschberg."
            ],
            "title": "A theory of scalar implicature (natural languages, pragmatics, inference)",
            "venue": "Ph.D. thesis, University of Pennsylvania.",
            "year": 1985
        },
        {
            "authors": [
                "Beth Ann Hockey",
                "Deborah Rossen-Knill",
                "Beverly Spejewski",
                "Matthew Stone",
                "Stephen Isard."
            ],
            "title": "Can you predict responses to yes/no questions? yes, no, and stuff",
            "venue": "Fifth european conference on speech communication and technology. Citeseer.",
            "year": 1997
        },
        {
            "authors": [
                "Shafiq Joty",
                "Preslav Nakov",
                "Llu\u00eds M\u00e0rquez",
                "Israa Jaradat."
            ],
            "title": "Cross-language learning with adversarial neural networks",
            "venue": "Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017), pages 226\u2013237, Vancouver,",
            "year": 2017
        },
        {
            "authors": [
                "Katharina Kann",
                "Kyunghyun Cho",
                "Samuel R. Bowman."
            ],
            "title": "Towards realistic practices in lowresource natural language processing: The development set",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing",
            "year": 2019
        },
        {
            "authors": [
                "Moshe Koppel",
                "Noam Ordan."
            ],
            "title": "Translationese and its dialects",
            "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 1318\u20131326, Portland, Oregon, USA. Association for",
            "year": 2011
        },
        {
            "authors": [
                "Uszkoreit",
                "Quoc Le",
                "Slav Petrov."
            ],
            "title": "Natural questions: A benchmark for question answering research",
            "venue": "Transactions of the Association for Computational Linguistics, 7:452\u2013466.",
            "year": 2019
        },
        {
            "authors": [
                "Yash Kumar Lal",
                "Niket Tandon",
                "Tanvi Aggarwal",
                "Horace Liu",
                "Nathanael Chambers",
                "Raymond Mooney",
                "Niranjan Balasubramanian."
            ],
            "title": "Using commonsense knowledge to answer why-questions",
            "venue": "Proceedings of the 2022 Conference on Empirical",
            "year": 2022
        },
        {
            "authors": [
                "Patrick Lewis",
                "Barlas Oguz",
                "Ruty Rinott",
                "Sebastian Riedel",
                "Holger Schwenk."
            ],
            "title": "MLQA: Evaluating cross-lingual extractive question answering",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7315\u2013",
            "year": 2020
        },
        {
            "authors": [
                "Weizhao Li",
                "Junsheng Kong",
                "Ben Liao",
                "Yi Cai."
            ],
            "title": "Mitigating contradictions in dialogue based on contrastive learning",
            "venue": "Findings of the Association for Computational Linguistics: ACL 2022, pages 2781\u20132788, Dublin, Ireland. Association for Compu-",
            "year": 2022
        },
        {
            "authors": [
                "Stephanie Lin",
                "Jacob Hilton",
                "Owain Evans."
            ],
            "title": "TruthfulQA: Measuring how models mimic human falsehoods",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3214\u20133252, Dublin,",
            "year": 2022
        },
        {
            "authors": [
                "Patrick Littell",
                "David R. Mortensen",
                "Ke Lin",
                "Katherine Kairis",
                "Carlisle Turner",
                "Lori Levin."
            ],
            "title": "URIEL and lang2vec: Representing languages as typological, geographical, and phylogenetic vectors",
            "venue": "Proceedings of the 15th Conference of the European Chap-",
            "year": 2017
        },
        {
            "authors": [
                "Xiao Liu",
                "Yifan Zhou",
                "Shuhei Ikemoto",
                "Heni Ben Amor."
            ],
            "title": "\u03b1-mdf: An attention-based multimodal differentiable filter for robot state estimation",
            "venue": "7th Annual Conference on Robot Learning.",
            "year": 2023
        },
        {
            "authors": [
                "Annie Louis",
                "Dan Roth",
                "Filip Radlinski."
            ],
            "title": "I\u2019d rather just go to bed\u201d: Understanding indirect answers",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 7411\u20137425, Online. Association for",
            "year": 2020
        },
        {
            "authors": [
                "Quinn McNemar."
            ],
            "title": "Note on the sampling error of the difference between correlated proportions or percentages",
            "venue": "Psychometrika, 12(2):153\u2013157.",
            "year": 1947
        },
        {
            "authors": [
                "Swaroop Mishra",
                "Daniel Khashabi",
                "Chitta Baral",
                "Yejin Choi",
                "Hannaneh Hajishirzi."
            ],
            "title": "Reframing instructional prompts to GPTk\u2019s language",
            "venue": "Findings of the Association for Computational Linguistics: ACL 2022, pages 589\u2013612, Dublin, Ireland. Associa-",
            "year": 2022
        },
        {
            "authors": [
                "Volodymyr Mnih",
                "Nicolas Heess",
                "Alex Graves"
            ],
            "title": "Recurrent models of visual attention",
            "venue": "Advances in neural information processing systems,",
            "year": 2014
        },
        {
            "authors": [
                "Ariana Negar Mohammadi"
            ],
            "title": "Corpus of Conversational Persian Transcripts",
            "year": 2019
        },
        {
            "authors": [
                "Alvaro Morales",
                "Varot Premtoon",
                "Cordelia Avery",
                "Sue Felshin",
                "Boris Katz."
            ],
            "title": "Learning to answer questions from Wikipedia infoboxes",
            "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1930\u20131935, Austin,",
            "year": 2016
        },
        {
            "authors": [
                "Yixin Nie",
                "Mary Williamson",
                "Mohit Bansal",
                "Douwe Kiela",
                "Jason Weston."
            ],
            "title": "I like fish, especially dolphins: Addressing contradictions in dialogue modeling",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Lin-",
            "year": 2021
        },
        {
            "authors": [
                "Pranav Rajpurkar",
                "Jian Zhang",
                "Konstantin Lopyrev",
                "Percy Liang."
            ],
            "title": "SQuAD: 100,000+ questions for machine comprehension of text",
            "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383\u20132392, Austin,",
            "year": 2016
        },
        {
            "authors": [
                "Siva Reddy",
                "Danqi Chen",
                "Christopher D. Manning."
            ],
            "title": "CoQA: A conversational question answering challenge",
            "venue": "Transactions of the Association for Computational Linguistics, 7:249\u2013266.",
            "year": 2019
        },
        {
            "authors": [
                "Arij Riabi",
                "Thomas Scialom",
                "Rachel Keraron",
                "Beno\u00eet Sagot",
                "Djam\u00e9 Seddah",
                "Jacopo Staiano."
            ],
            "title": "Synthetic data augmentation for zero-shot crosslingual question answering",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural",
            "year": 2021
        },
        {
            "authors": [
                "Sebastian Ruder",
                "Avi Sil."
            ],
            "title": "Multi-domain multilingual question answering",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: Tutorial Abstracts, pages 17\u2013 21, Punta Cana, Dominican Republic & Online. As-",
            "year": 2021
        },
        {
            "authors": [
                "Krishna Sanagavarapu",
                "Jathin Singaraju",
                "Anusha Kakileti",
                "Anirudh Kaza",
                "Aaron Mathews",
                "Helen Li",
                "Nathan Brito",
                "Eduardo Blanco."
            ],
            "title": "Disentangling indirect answers to yes-no questions in real conversations",
            "venue": "Proceedings of the 2022 Conference of",
            "year": 2022
        },
        {
            "authors": [
                "Freda Shi",
                "Mirac Suzgun",
                "Markus Freitag",
                "Xuezhi Wang",
                "Suraj Srivats",
                "Soroush Vosoughi",
                "Hyung Won Chung",
                "Yi Tay",
                "Sebastian Ruder",
                "Denny Zhou"
            ],
            "title": "Language models are multilingual chain-of-thought reasoners",
            "venue": "arXiv preprint arXiv:2210.03057",
            "year": 2022
        },
        {
            "authors": [
                "Eyal Shnarch",
                "Carlos Alzate",
                "Lena Dankin",
                "Martin Gleize",
                "Yufang Hou",
                "Leshem Choshen",
                "Ranit Aharonov",
                "Noam Slonim."
            ],
            "title": "Will it blend? blending weak and strong labeled data in a neural network for argumentation mining",
            "venue": "Proceedings",
            "year": 2018
        },
        {
            "authors": [
                "Andreas Stolcke",
                "Klaus Ries",
                "Noah Coccaro",
                "Elizabeth Shriberg",
                "Rebecca Bates",
                "Daniel Jurafsky",
                "Paul Taylor",
                "Rachel Martin",
                "Carol Van Ess-Dykema",
                "Marie Meteer"
            ],
            "title": "Dialogue act modeling for automatic tagging and recognition",
            "year": 2000
        },
        {
            "authors": [
                "Elior Sulem",
                "Jamaal Hay",
                "Dan Roth."
            ],
            "title": "Yes, no or IDK: The challenge of unanswerable yes/no questions",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech-",
            "year": 2022
        },
        {
            "authors": [
                "Niket Tandon",
                "Bhavana Dalvi",
                "Keisuke Sakaguchi",
                "Peter Clark",
                "Antoine Bosselut"
            ],
            "title": "WIQA: A dataset for \u201cwhat if...\u201d reasoning over procedural text",
            "venue": "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing",
            "year": 2019
        },
        {
            "authors": [
                "Sandra A Thompson."
            ],
            "title": "Questions and responses in english conversation by anna-brita stenstr\u00f6m",
            "venue": "Language, 62(1):213\u2013214.",
            "year": 1986
        },
        {
            "authors": [
                "Hugo Touvron",
                "Thibaut Lavril",
                "Gautier Izacard",
                "Xavier Martinet",
                "Marie-Anne Lachaux",
                "Timoth\u00e9e Lacroix",
                "Baptiste Rozi\u00e8re",
                "Naman Goyal",
                "Eric Hambro",
                "Faisal Azhar"
            ],
            "title": "Llama: Open and efficient foundation language models",
            "year": 2023
        },
        {
            "authors": [
                "Xiaoyang Wang",
                "Chen Li",
                "Jianqiao Zhao",
                "Dong Yu."
            ],
            "title": "Naturalconv: A chinese dialogue dataset towards multi-turn topic-driven conversation",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, 35(16):14006\u201314014.",
            "year": 2021
        },
        {
            "authors": [
                "Yida Wang",
                "Pei Ke",
                "Yinhe Zheng",
                "Kaili Huang",
                "Yong Jiang",
                "Xiaoyan Zhu",
                "Minlie Huang."
            ],
            "title": "A large-scale chinese short-text conversation dataset",
            "venue": "Natural Language Processing and Chinese Computing: 9th CCF International Conference, NLPCC",
            "year": 2020
        },
        {
            "authors": [
                "Albert Webson",
                "Ellie Pavlick"
            ],
            "title": "Do promptbased models really understand the meaning of their prompts? In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language",
            "year": 2022
        },
        {
            "authors": [
                "Teven Le Scao",
                "Sylvain Gugger",
                "Mariama Drame",
                "Quentin Lhoest",
                "Alexander Rush."
            ],
            "title": "Transformers: State-of-the-art natural language processing",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System",
            "year": 2020
        },
        {
            "authors": [
                "Linjuan Wu",
                "Shaojuan Wu",
                "Xiaowang Zhang",
                "Deyi Xiong",
                "Shizhan Chen",
                "Zhiqiang Zhuang",
                "Zhiyong Feng."
            ],
            "title": "Learning disentangled semantic representations for zero-shot cross-lingual transfer in multilingual machine reading comprehension",
            "venue": "In",
            "year": 2022
        },
        {
            "authors": [
                "Bo Zheng",
                "Li Dong",
                "Shaohan Huang",
                "Wenhui Wang",
                "Zewen Chi",
                "Saksham Singhal",
                "Wanxiang Che",
                "Ting Liu",
                "Xia Song",
                "Furu Wei."
            ],
            "title": "Consistency regularization for cross-lingual fine-tuning",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Multilingual Question-Answering has recently received substantial attention (Ruder and Sil, 2021; Shi et al., 2022). State-of-the-art models such as XLM-E (Chi et al., 2022), however, achieve only 68% to 76% F1-score on multilingual QuestionAnswering benchmarks such as MLQA (Lewis et al., 2020) and XQuAD (Artetxe et al., 2020). Large Language Models (LLMs) such as InstructGPT (Ouyang et al., 2022) obtain promising results with several English Question-Answering benchmarks (Rajpurkar et al., 2016; Choi et al., 2018; Lin et al., 2022). Closed-source, proprietary LLMs for which the training data is unknown raise issues regarding replicability and data leakage (Carlini et al., 2021). Open-source LLMs such as LLaMa are pretrained on Latin or Cyrillic scripts (Touvron et al., 2023) thus have limitations with other scripts.\n\u2217All authors except the first and last authors are listed in alphabetical order.\n\u2020The work does not relate to the position at Amazon. \u2021Work done at Arizona State University.\nYes-no questions are questions that expect a yes or no for an answer. Humans, however, often answer these kinds of questions without using a yes or no keyword. Rather, they provide indirect answers that must be interpreted to reveal the underlying meaning (see examples in Table 1). Indirect answers are used to ask follow-up questions or provide explanations for negative answers (Thompson, 1986), prevent wrong interpretations (Hirschberg, 1985), or show politeness (Brown and Levinson, 1978). This is true at least in English and the eight additional languages we work with. Note that question answering is usually defined as finding an answer to a question given a collection of documents. On the other hand, interpreting indirect answers to yes-no questions is defined as mapping a known answer to its correct interpretation.\nMany NLP problems were initially investigated in English (Kann et al., 2019). Even though yes-no questions and indirect answers have been studied\nfor decades (Hockey et al., 1997; Green and Carberry, 1999), previous efforts to date have predominantly focused on English (Section 2). In this paper, we tackle this challenging problem in eight additional languages: Hindi (hi), Korean (ko), Chinese (zh), Bangla (bn), Turkish (tr), Spanish (es), Nepali (ne), and Persian (fa).\nThis paper focuses on multilingual interpretation of indirect answers to yes-no questions. Doing so opens the door to several applications. For example, dialogue systems could avoid inconsistencies and contradictions (Nie et al., 2021; Li et al., 2022). Consider the examples in Table 1. Follow-up turns such as How long have you felt like that?, What else is required for your support?, and I can\u2019t wait to see you at the concert (one per example) would be puzzling and probably frustrating to hear.\nThe main contributions are as follows:1\n1. A distant supervision approach to collect yesno questions and direct answers along with their interpretations. We use this approach to collect training data in five languages. 2. Evaluation benchmarks in eight languages in which no resources exist for interpreting indirect answers to yes-no questions. 3. Experimental results showing that training with the direct answers obtained via distant supervision is beneficial to interpret indirect answers in the same language. 4. Experimental results expanding on (3) and showing that multilingual training is beneficial, even when no additional training data for the language of interest is available."
        },
        {
            "heading": "2 Related Work",
            "text": "Question Answering Researchers have targeted, among others, factual questions (Morales et al., 2016), why questions (Lal et al., 2022), questions in context (Choi et al., 2018), questions over procedural texts (Tandon et al., 2019), and natural questions submitted to a search engine (Kwiatkowski et al., 2019). The problem is defined as finding answers to a given question (often within a set of documents). Unlike this line of work, interpreting indirect answers to yes-no questions is about determining the underlying meaning of answers\u2014not finding them.\n1Code, benchmarks, and multilingual training data obtained via distant supervision available at https://github.com/wang-zijie/ yn-question-multilingual\nYes-No Questions have also been studied for decades (Hockey et al., 1997; Green and Carberry, 1999). Recent work includes large corpora such as BoolQ (Clark et al., 2019) (16,000 yes-no questions submitted to a search engine) and extensions including unanswerable questions (Sulem et al., 2022). Yes-no questions have also been studied within the dialogue domain (Choi et al., 2018; Reddy et al., 2019). These dialogues, however, are synthetic and constrained to a handful of topics. Circa (Louis et al., 2020), Friends-QIA (Damgaard et al., 2021), and SWDA-IA (Sanagavarapu et al., 2022), also explore yes-no questions in dialogues (crowdsourced, modified TV scripts, and phone conversations). We are inspired by their interpretations and use their corpora in our experiments (Section 5). All computational works on yes-no questions to date are in English. We are the first to target eight additional languages. Crucially, we do by exploring distant supervision and cross-lingual learning; our approach does not require tedious manual annotations.\nMultilingual Pretraining and Learning Several efforts have investigated multilingual language model pretraining. Both mBERT (Devlin et al., 2019) and XLM-RoBERTa (Conneau et al., 2020) are masked language models pretrained on multilingual corpora. XLM-Align (Chi et al., 2021) improves cross-lingual domain adaptation using self-labeled word alignment.\nPrevious works have also focused on better finetuning approaches for cross-lingual transfer (Joty et al., 2017; Zheng et al., 2021). Others use machine translations to create synthetic multilingual training data or translate multilingual test benchmarks into English (Artetxe et al., 2023). In addition, several efforts have been made on multilingual Question Answering tasks. Wu et al. (2022) present a siamese semantic disentanglement model for multilingual machine reading comprehension. Riabi et al. (2021) propose a multilingual QA dataset leveraging question generation models to generate synthetic instances. Other work develops a Spanish SQuAD dataset based on a TranslateAlign-Retrieve method (Carrino et al., 2020).\nIn this paper, we avoid translations. Instead, we use distant supervision. Our approach (a) is conceptually simple, (b) only requires unannotated corpora and rules for yes-no questions and direct answers in the languages of interest, and, importantly, (c) obtains statistically significantly better results than cross-lingual learning from English."
        },
        {
            "heading": "3 Obtaining Training Data",
            "text": "For training purposes, we work with six languages. Specifically, we work with existing English corpora (Section 3.1) and corpora obtained in five additional languages via distant supervision (Section 3.2). As we shall see, multilingual transfer learning is successful even when no examples in the language of interest are available (Section 5.3)."
        },
        {
            "heading": "3.1 Existing English Corpora",
            "text": "There are three English corpora that include yes-no questions, indirect answers, and their interpretations: Circa (Louis et al., 2020), SWDA-IA (Sanagavarapu et al., 2022) and Friends-QIA (Damgaard et al., 2021). Table 2 presents statistics. Context refers to the text around the question and indirect answer (i.e., dialogue turns before and after).\nCirca was created by asking crowdworkers to write 34k yes-no questions that fit 9 scenarios (e.g., friends talking about food) and indirect answers. It does not include context. We note that the frequency of Middle interpretation is much lower than the other two interpretations. SWDA-IA includes 2.5k yes-no questions and indirect answers from SWDA (Stolcke et al., 2000), a telephone conversation dataset. It includes context (three turns before and after). Unlike Circa, questions and answers come from transcriptions of (almost) unconstrained conversations. Friends-QIA includes 5.9k yes-no questions and indirect answers derived from Friends, a TV show. Unlike Circa and SWDA-IA, questions and answers in Friends-QIA were manually modified to facilitate the task. For example, they remove some yes and no keywords in answers (e.g., yes, yeah, yep) but not others (e.g., of course, absolutely). Further, they add relevant information from the context to questions and delete interjections (e.g., Hey!) among others.\nThe three datasets do not consider the same interpretations (Circa: 8 options, SWDA-IA: 5 options,\nFriends-QIA: 6 options). We cluster them into Yes, No and Middle following our definitions (Section 4, Appendix A) for comparison purposes. Because Circa and SWDA-IA are the only corpora with \u201cnaturally occurring\u201d questions and answers, we choose to not work with Friends-QIA."
        },
        {
            "heading": "3.2 Distant Supervision for New Languages",
            "text": "We follow a distant supervision approach to collect multilingual training data for interpreting indirect answers to yes-no questions. The only requirements are (a) (relatively) large unannotated corpora in the languages of interest and (b) rules to identify yes-no questions, direct answers, and their interpretations in each language. We found that native speakers can write robust rules after few iterations. Source Corpora We made an effort to identify relevant corpora for the eight languages we work with but could only do so in five languages. The other three languages (Bangla, Nepali, and Persian) are spoken by millions, and we could certainly find digital texts in these languages. But (a) creating a large collection of dialogues and (b) splitting noisy transcripts into turns and sentences are outside the scope of this paper. Further, doing so would raise copyright and ethical considerations.\nFor Chinese, we select (a) NaturalConv (Wang et al., 2021), a synthetic dialogue dataset written by crowdworkers covering several topics, and (b) LCCC-base (Wang et al., 2020), posts and replies extracted from Weibo, a Chinese social media platform. For Spanish, we choose CallFriend (Canavan and Zipperlen, 1996), a corpus consisting of unscripted telephone conversations. For Hindi, we collect questions and answers from Twitter using their API. For Korean, we select a question-answering corpus from AI Hub,2 which includes civil complaints and replies from public organizations. For Turkish, we identify FAQ (i.e., Frequently Asked Questions) and CQA (i.e., Community Question Answering) datasets from MFAQ (De Bruyn et al., 2021). All of these corpora are used in accordance with their licenses.\nThese corpora are diverse not only in terms of language. Indeed, they include different genres (written forum discussions, dialogue transcripts, question-answer pairs, etc.) and domains (social media, informal conversations, etc). Identifying Yes-No Questions We define rules based on lexical matching to identify yes-no ques-\n2www.aihub.or.kr\ntions from the aforementioned corpora in each language. The Hindi rules are as follows. A tweet contains a yes-no question if it:\n\u2022 contains a question mark and any of these bigrams: kyA aAp (do you), kyA hm (do we), kyA yh (will this), kyA kBF (does this, does this ever), yh ho sktA h{ (can this happen); \u2022 does not contain these words: khA\\ (where), kyo (why), k{s (how), kOn (who), EkskA (whose), kOnsA (which), yA (or), kb (when); \u2022 has between 3 and 100 tokens; and \u2022 does not (a) contain links, @mentions, #hash-\ntags, or numbers or (b) come from unverified users, retweets, or replies to tweets.\nIdentifying Direct Answers The next set of rules identifies which yes-no questions are followed by a direct answer. We have rules that identify direct answers and their interpretations based on yes and no keywords. We complement these rules with rules to discard some answers that cannot be reliably interpreted regardless of keywords. The rules for Hindi are as follows. A reply tweet to a yes-no question is a direct answer if it:\n\u2022 contains yes keywords: hA\\ (Yes), hA (yes), hA (yes), jF (yes), )!r (sure), shF (correct), EnE[ct !p (definitely), yes, yeah, sure, of course, 100%; or no keywords: nhF (No), nhF\\ (no), mt (don\u2019t), n (not), no, never, n\u2019t (We include a few English keywords since people code-switch between these languages); \u2022 does not contain links, #tags, or more than one @mention (i.e., only replies to the user who asked the yes-no question); \u2022 does not contain question marks; and \u2022 has between 6 to 30 tokens.\nAppendix B details the rules for other languages.\nAnalysis The result of the rules for distant supervision consists of yes-no questions, direct answers, and (noisy) keyword-based interpretations. We prioritize precision over recall, as we need as little noise as possible for training purposes. We estimate quality with a sample of 200 instances per language (Table 3). The rules to identify yes-no questions are almost perfect across four out of five languages (Precision: 0.98\u20131.00). We identify thousands of yes-no questions in all the languages, and many of those (35.2%\u201365.3%) are followed by a direct answer that we can interpret with our rules. The ratio of yes and no interpretations varies across languages, and the precision of the rules to interpret direct answers is high (0.93\u20130.97) in all languages except Hindi (0.65). We believe that (a) the ratios of yes and no depend on the domain of the source corpora and (b) the low precision in Hindi is due to the fact that we work with Twitter as opposed to more formal texts. Note that all the questionanswer pairs identified via distant supervision are interpreted with yes or no. Regardless of the quality of the data, whether it is useful in the training process is an empirical question (Section 5)."
        },
        {
            "heading": "4 Benchmarks in New Languages",
            "text": "We are the first to work on interpreting answers to yes-no questions in languages other than English (Hindi, Korean, Chinese, Bangla, Turkish, Spanish, Nepali, and Persian). We set to work with questions and answers written in the languages of interest to avoid translationese (Koppel and Ordan, 2011), so we create new benchmarks with 300 question-answer pairs per dataset for each language (Chinese and Turkish: 600 samples; other languages: 300 samples). For the five languages\n[hi] Q:kyA hm Es' DoKA KAn k Ele hF p{dA h e h{\\ ? (Are we born only to be cheated?) A: d BA `yv[A hm us y g m \\ p{dA h e h{\\\u0964 (Unfortunately we are born in that era.) Interpretation: Yes\n[ko] Q: \u1100\u1161\u11b7\u1100\u1175\u1100\u1175\u110b\u116e\u11ab\u110b\u1175\u110b\u1175\u11bb\u1102\u1173\u11ab\u1100\u1165\u11ba\u1100\u1161\u11c0\u110b\u1173\u11ab\u1103\u1166\u1103\u116e\u1110\u1169\u11bc\u110b\u1163\u11a8\u1100\u116b\u11ab\u110e\u1161\u11ad\u1102\u1161\u110b\u116d? (I think I have a cold. Can I have a medicine for headache relief?) A: \u1103\u116e\u1110\u1169\u11bc\u110b\u1163\u11a8\u1107\u1169\u1103\u1161\u1102\u1173\u11ab\u1100\u1173\u1102\u1163\u11bc\u1100\u1161\u11b7\u1100\u1175\u110b\u1163\u11a8\u110b\u1173\u1105\u1169\u1103\u1173\u1105\u1175\u1100\u1166\u11bb\u1109\u1173\u11b8\u1102\u1175\u1103\u1161. (I\u2019ll just give you cold medicine rather than a headache reliever.) Interpretation: No\n[ne] Q: Dm Enrp \"tA mA n h C ? (Do you believe in secularism?) A: s\\EvDAnko &yv-TA ho\u0964 (ys{l mA n pC \u0964 (It is a provision of the constitution. So it should be accepted.) Interpretation: Middle\n[fa] Q: \u061f\u0647\u062f\u0634 \u0632\u06cc\u0645\u062a \u0627\u0648\u0647 \u06cc\u0646\u06cc\u0628\u06cc\u0645 \u0644\u0627\u0627\u0628 \u0632\u0627 \u0648\u062a (You are looking from a high elevation, is the weather clean?) A: \u06af\u0646\u0634\u0642 \u0645\u0646\u06cc\u0628\u06cc\u0645 \u0648\u0645\u062f\u0646\u0648\u0627\u0645\u062f \u0645\u0631\u0627\u062f \u0646\u0644\u0627\u0627 (I am now clearly looking at the Damavand (Mountain).) Interpretation: Yes\n[tr] Q: as\u0327ks\u0131z mutlu olabilir misiniz? (Can you be happy without love?) A: her s\u0327ekilde mutlu olmas\u0131n\u0131 bilirim. (I know how to be happy anyway) Interpretation: Yes\n[es] Q: \u00bfTe ha seguido molestando \u00e9l? (Has he continued to bother you?) A: A cada rato. (All the time.) Interpretation: Yes\n[zh] Q: \u8f66\u91cc\u6709\u77ff\u6cc9\u6c34\u74f6\u5417\uff1f (Any water bottle in the car?) A: \u8fd8\u662f\u7528\u7f50\u5934\u74f6\u5427\u3002 (Let\u2019s just use a can.) Interpretation: No\n[bn] Q: Aamar maQay EkTu Hat idey edxeta \u00c6r Aaeq ikna? (Can you please check if I have a fever?) A: Exn km. (It\u2019s less now.) Interpretation: Yes\nTable 4: Examples from the benchmarks we create in eight languages. Answers whose interpretations lean towards yes or no are annotated as such; middle is used for 50/50 splits and unresponsive answers.\nfor which there are large unannotated corpora and we have built rules for (Section 3), we select yes-no question-answer pairs without a direct answer (i.e., those identified by our rules as yes-no questions but not followed by a direct answer). We collect 300 question-answer pairs for the other languages from Bangla2B+ (Bhattacharjee et al., 2022, Bangla), Kantipur Daily (Nepali),3 and LDC2019T11 (Mohammadi, 2019, Persian). Annotation Guidelines We manually annotate the interpretations of indirect answers to yes-no questions in the eight languages using three labels:\n\u2022 Yes: the answer leans towards yes, including probably yes, yes under some conditions, and strong affirmative answers (e.g., Absolutely!).\n\u2022 No: the answer leans towards no. \u2022 Middle: Yes or No interpretations do not apply. Our interpretations are a coarser version than those used in previous work. Appendix A presents a mapping, and Appendix C details the guidelines.\nTable 4 presents examples. In the Hindi (hi) example, the yes interpretation relies on the negative sentiment of the answer. In the Turkish (tr) example, the yes interpretation requires commonsense knowledge about fasting, which includes not drinking water. Similarly, the Persian (fa) example requires commonsense (seeing a mountain implies clear weather). In the Korean (ko) example, the answer provides an alternative, thereby rejecting the\n3https://ekantipur.com/\nrequest. In the Spanish (es) example, the answer affirms the inquiry without using yes or similar keywords. In the Nepali (ne) example, the answer is interpreted as middle since it mentions laws rather than discussing an opinion. In the Chinese (zh) example, the answer suggests using a can, implying that they do not have any water bottles in the car. In the Bangla (bn) example, the answer confirms that the questioner has a fever (despite it is lower).\nInter-Annotator Agreements Native speakers in each language performed the annotations. We were able to recruit at least two annotators for five languages (zh, hi, es, tr, and bn) and one annotator for the rest (ne, ko, fa). Inter-annotator agreements (linearly weighted Cohen\u2019s \u03ba) are as follows: Turkish: 0.87, Hindi: 0.82, Spanish: 0.76, Bangla: 0.73, and Chinese: 0.67. These coefficients are considered substantial (0.6\u20130.8) or (nearly) perfect (>0.8) (Artstein and Poesio, 2008). We refer the reader to Appendix D for a detailed Data Statement.\nDataset Statistics and Label Frequency Table 5 presents the statistics of our training datasets and benchmarks in the new languages. Since we obtain the training datasets via distant supervision (Table 3), they only include yes-no questions with direct answers and thus their interpretations are limited to be yes or no. For the benchmarks, we select instances from each language randomly. This allows us to work with the real distribution of interpretations instead of artificially making it uniform.\nAs shown in Table 5 (bottom), yes is always the most frequent label (34.3%\u201354.2%), but the label distributions vary across languages."
        },
        {
            "heading": "5 Experiments",
            "text": "We develop multilingual models to reveal the underlying interpretations of indirect answers to yesno questions. We follow three settings: (a) crosslingual learning to the new language (after training with English, Section 5.1); (b) monolingual finetuning via distant supervision with the language of interest (Section 5.2); and (c) multilingual finetuning via distant supervision with many languages (Section 5.3). Following previous work (Section 2), we use the question and answer as inputs.\nWe conduct our experiments with two multilingual transformers: XLM-RoBERTa (Conneau et al., 2020) and XLM-Align (Chi et al., 2021). Both are obtained from HuggingFace (Wolf et al., 2020) and were pretrained on hundreds of languages. We split our benchmarks into validation (20%) and test (80%), and report results with the test split. Cross-lingual learning was conducted with a model\ntrained and validated exclusively in English (with Circa and SWDA-IA). Monolingual and multilingual fine-tuning use the validation split in each language for hyperparameter tuning, The test set is always the same. We will discuss results with XLM-RoBERTa, as it outperforms XLM-Align by a small margin. Appendix F and G detail the results with XLM-Align and the hyperparameters."
        },
        {
            "heading": "5.1 Cross-Lingual Learning from English",
            "text": "We start the experiments with the simplest setting: training with existing English corpora (Circa and SWDA-IA) and evaluating with our benchmarks. We consider this as a cross-lingual learning baseline since the model was neither fine-tuned nor validated with the new languages.\nTable 6 presents the results. Training with Circa is always better than (or equal to) SWDA-IA, but combining both yields the best results with most languages. The only exception is Hindi, where training with Circa obtains better results. This is intuitive considering that Circa is a much larger dataset than SWDA-IA (Table 2). The lower re-\nsults with SWDA-IA for Hindi are likely due to the mismatch of domains (SWDA-IA: phone conversation transcripts, Hindi: Twitter)."
        },
        {
            "heading": "5.2 Fine-Tuning with the New Language",
            "text": "We continue the experiments by training models with English corpora (gold annotations, Circa and SWDA-IA) and the data obtained via distant supervision for the language of interest (noisy data, Section 3.2 and Table 5). We adopt the fine-tuning methodology by Shnarch et al. (2018) to blend training data from English corpora and the additional instances obtained via distant supervision. Briefly, we start the training process (first epoch) with the concatenation of the English data and data for the new language. Then, we reduce the data for the new language by a ratio \u03b1 after each epoch. We choose the English corpora based on the best combination from Table 6 for each language. We found that reducing the data for the new language (as opposed to the English data) yields better results. We believe this is because the former: (a) is noisier (distant supervision vs. human annotations) and (b) does not include any middle interpretations.\nTable 7 presents the results. Recall that additional data is only available in five languages. The results show that blended training with English and the new language is always beneficial. The improvements (%\u2206F en) range from 2% to 19%. They are the largest with the languages for which we have the most additional data (Table 3): Turkish (9%) and Chinese (19%). Note that despite the additional Hindi data is noisy (0.67 Precision, Table 3), we observe improvements (2%)."
        },
        {
            "heading": "5.3 Fine-Tuning with Several Languages",
            "text": "We close the experiments exploring whether it is beneficial to train with languages other than En-\nglish and the language of interest. In other words, we answer the following questions: Does multilingual fine-tuning yield better results?\nWe use the same blending strategy as in monolingual fine-tuning (Section 5.2) but adopt a greedy approach to find the best combination of additional languages to train with. We start with the model trained with English and the data for the language of interest if available (Table 7). Otherwise, we start with the model trained with English (Table 6). Then, we add data from an additional language (one at a time) and select the best based on the results with the validation split. We continue this process until including additional languages does not yield higher results with the validation split.\nTable 8 presents the results. The technique is successful. Compared to monolingual fine-tuning (Table 7), multilingual fine-tuning is always beneficial (%\u2206F = 2\u201328). More importantly, the improvements with respect to cross-lingual learning (Table 6) are even higher across all languages (%\u2206F en = 2\u201353). For five out of eight languages, the improvements are at least 11% and statistically significant (McNemar\u2019s test, p < 0.05). Note that multilingual fine-tuning obtains significantly better results with two of the languages for which we could not find source corpora to use distant supervision: Nepali (17%) and Bangla (22%). We hypothesize that the low gains with Persian might be due to the fact that it is the only one using a right-to-left script. Which languages are worth training with? Some of the language combinations that are worth blending with are surprising (Table 8). For example, Chinese (zh) is beneficial for Hindi (hi), Turkish (tr) and Spanish (es). This may be due to the fact that Chinese is one of the languages for which we collect the most data via distant supervision. Similarly, Turkish (tr) is useful for Korean (ko).\nIn order to analyze these surprising results, we sort all language pairs by their similarity (Figure 1). We define language similarity as the cosine similarity between their lang2vec vectors (Littell et al., 2017), more specifically, we use syntactic and language family features from the URIEL typological database. Generally speaking, the more\nsimilar a language the more likely it is to be useful for multilingual fine-tuning. There are only a few exceptions (e.g., Korean with Hindi and Turkish)."
        },
        {
            "heading": "5.4 Examples of Errors",
            "text": "Table 9 presents questions and answers for which our best model fails to identify the correct interpretation. In the Chinese (zh) example, the no interpretation could be obtained by contrasting urban area and airport. However, the model lacks such commonsense knowledge. Similarly, in the Bangla (bn) example, a wife being afraid of dogs most likely indicates that there are no dogs in the household. In the Korean (ko) example, the answer provides both yes and no for different days of the weekend, but the model appears to consider only Sunday. In the Spanish (es) example, the question has a negation thus the answer ought to be interpreted as yes. Answers in both the Turkish (tr) and Hindi (hi) examples include a polar distractor that confuses the model (tr: no longer yet the author has a favorite game; hi: stop yet the answer ought to be interpreted as nobody can stop paying taxes. The answer in Nepali (ne) indicates it could (or could not) happen while the model only takes it as affirmation. The answer in Persian (fa) is a rhetorical question with negation, thus the correct interpretation is yes."
        },
        {
            "heading": "5.5 A Note on Large language Models",
            "text": "Large Language Models (LLMs) obtain impressive results in many tasks (Mishra et al., 2022). Researchers have also shown that LLMs can solve problems even with malicious prompts (Webson and Pavlick, 2022), casting a shadow on what they\nmay understand. LLMs do not outperform our best model (Table 8). First, we note that open-source LLMs are \u201cpretrained only on Latin or Cyrillic scripts\u201d (Touvron et al., 2023), thus they cannot process most of the languages we work with. Second, closed-source LLMs such as ChatGPT are likely to have been pretrained with the data in our benchmarks, so data leakage is an issue. We randomly selected 30 instances for each language from our benchmarks and fed them to ChatGPT via the online interface using the prompt in Appendix H. The best model for each language (Table 8) outperforms ChatGPT by 16.4% on average with these instances (F1: 54.25 vs. 63.13). Regardless of possible data leakage, we acknowledge that ChatGPT obtains impressive results in a zero-shot setting."
        },
        {
            "heading": "6 Conclusions",
            "text": "We have tackled for the first time the problem of interpreting indirect answers to yes-no questions in languages other than English. These kinds of answers do not include yes or no keywords. Their interpretations, however, are often either Yes or No. Indeed, Middle accounts for between 12.7% and\n33.7% depending on the language (Table 3).\nWe have created new evaluation benchmarks in eight languages: Hindi, Korean, Chinese, Bangla, Turkish, Spanish, Nepali, and Persian. Additionally, we present a distant supervision approach to identify yes-no questions and direct answers. Training with this new, noisy, language-specific data is always beneficial\u2014and significantly better in five languages. Cross-lingual learning from many languages to a new language is the best strategy. This is true even for the three languages for which distant supervision was not explored because of difficulties finding large unannotated corpora (Bangla, Nepali, and Persian). Our approach successfully learns to interpret indirect answers from direct answers obtained via distant supervision.\nOur future work includes exploring more robust multilingual fine-tuning (Zheng et al., 2021). We also plan to explore applications of the fundamental research presented here. In particular, we are interested in improving dialogue consistency and avoiding inconsistencies by ensuring that generated turns are compatible with the correct interpretation of indirect answers to yes-no questions.\nLimitations\nWe double-annotate the benchmarks in five out of the eight languages. We are not able to do so in the other three languages (Bangla, Nepali, and Persian) since we are unable to recruit a second native speaker. While great care was taken, including single annotators double and triple checking their work, we acknowledge that not reporting interannotator agreements in these three languages is a weakness.\nWe adopt three labels (Yes, No and Middle) to represent the interpretations of answers to yes-no questions. Some previous works use finer-grained label sets as we discussed in Section 3.1. Considering that (a) there is no universal agreement about the possible ways to interpret answers to yes-no questions and (b) interpretations of answers other than the three we adopted are rare (less than 10% in Circa and Friends-QIA, and less than 20% in SWDA-IA), we argue that three labels are sound.\nWe run the experiments with two models: XLMRoBERTa and XLM-Align. We acknowledge that there are other transformers such as InfoXLM and XLM-E. However, they either obtain similar (or worse) results on existing benchmarks, or are not open-source at the time of writing. Regarding opensource Large Language Models (LLMs) such as LLaMa and Alpaca, we do not run experiments on them since (a) fine-tuning with even the smallest version (parameters: 7B) requires significant computing resources; (b) they are pretrained only on Latin or Cyrillic scripts thus may have limitations targeting other languages.\nOur in-context learning experiments are conducted with ChatGPT based on GPT-3.5 (textdavinci-002), which was the state-of-the-art model at the time we conducted the experiments. GPT4 is already out, and better language models will continue to come out. We acknowledge that testing with GPT-4 may obtain better results, but we argue doing so is not necessary: (a) querying on GPT-4 based ChatGPT interface is limited to a few samples per hour, and (b) OpenAI has never announced the list of supported languages so we would be shooting in the dark. A more serious issue is the fact that GPT-X may have been trained with the data we work with, so any results are to be taken with a grain of salt.\nWe tune several hyperparameters (including the blending factor \u03b1, listed in Table 12 and Table 13) with the train and development splits, and report\nresults with the test set. The results are taken from the output of one run. We acknowledge that the average of multiple runs (e.g., 10) would be more reliable, but they also require much more computational resources (literally, 10 more times).\nEthical Considerations\nData sources and collection. Our benchmarks use source texts in accordance with their licenses. We collect corpora in Spanish and Persian from Linguistic Data Consortium4 under a non-commercial license. Samples in Turkish from MFAQ are used under the Apache-2.0 license. Samples in Chinese from NaturalConv and LCCC are used under a noncommercial license and the MIT license respectively. Samples in Hindi from Twitter are used under the Twitter developer policy. 5 Samples in Korean from AI Hub are used under a non-commercial license. Samples in Bangla from Bangla2B+ are used under the CC BY-NC-SA 4.0 license. Samples in Nepali from Kantipur Daily are used under a non-commercial license."
        },
        {
            "heading": "Acknowledgments",
            "text": "We would like to thank Siyu Liu and Xiao Liu for their help in annotating the Chinese benchmark, and the Chameleon platform (Keahey et al., 2020) for providing computational resources. We also thank the reviewers for their insightful comments.\nThis material is based upon work supported by the National Science Foundation under Grant No. 1845757. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the NSF."
        },
        {
            "heading": "A Mapping Heterogeneous",
            "text": "Interpretations to Yes, No, and Middle\nThe three English corpora (Circa, SWDA-IA, Friends-QIA) use compatible but different interpretations for yes-no questions (Section 3.1). Here we list the mapping to our three labels (Yes, No, and Middle). This mapping is straightforward given their definitions.\nCirca (relaxed labels): \u2022 Yes \u2192 Yes \u2022 No \u2192 No \u2022 Yes, subject to some conditions \u2192 Yes \u2022 In the middle, neither yes nor no \u2192 Middle \u2022 Other: discard \u2022 N/A: discard SWDA-IA: \u2022 Yes \u2192 Yes \u2022 Probably Yes \u2192 Yes \u2022 Middle \u2192 Middle \u2022 Probably No \u2192 No \u2022 No \u2192 No Friends-QIA: \u2022 Yes \u2192 Yes \u2022 No \u2192 No \u2022 Yes, subject to some conditions \u2192 Yes \u2022 Neither yes nor no \u2192 Middle \u2022 Other: discard \u2022 N/A: discard"
        },
        {
            "heading": "B Rules to Identify Yes-No Question and Direct Answers in Multiple languages",
            "text": "Hindi In addition to the rules listed in Section 3.2, we remind the reader that Hindi data was collected before Twitter changed its policies on verified accounts and API usage.\nTurkish For the Turkish corpus (MRQA), we define the following rules to identify yes-no questions:\n\u2022 The conversation turn contains any of the following keywords: m\u0131y\u0131m, miyim, muyum, m\u00fcy\u00fcm, m\u0131s\u0131n, misin, musun, m\u00fcs\u00fcn, m\u0131, mi, mu, m\u00fc, m\u0131y\u0131z, miyiz, muyuz, m\u00fcy\u00fcz, m\u0131s\u0131n\u0131z, misiniz, musunuz, m\u00fcs\u00fcn\u00fcz. All of these words come from the same root (mi, m\u0131, mu, m\u00fc), which is used to make a sentence a yesno question. There is no direct translation into English. \u2022 The turn does not contain wh-questions keywords: ne (what), nerede (where), ne zaman (when), nas\u0131l (how), nasil (informal how), neden (why), kim (who), hangi (which), and kimin (whose).\n\u2022 The turn is less than 50 tokens. The rules to identify direct answers in Turkish rely on yes (evet (yes), evt (yes, informal), eet (yes, informal), tabii (of course), tabi (of course, informal), tabiiki (of course, informal), tabiki (of course, informal), aynen (absolutely), and h\u0131h\u0131 (yes, informal)) and no (hay\u0131r (no), hayir (informal no), hyr (informal no), and yoo (informal no)) keywords.\nRegardless of keywords, we discard questionanswer pairs if the is_accepted field is set to false, which indicates the answer was not selected.\nSpanish For the Spanish corpus (CallFriend), we define the following rules to identify yes-no questions. A conversation turn includes a yes-no question if:\n\u2022 it contains a verb; \u2022 ends with a question mark (\u2018?\u2019); and \u2022 does not contain the following words or\nphrases: por que (why), cuando (when), donde (where), como (how), cuanto (how much/many), quien (who), cual (which, singular), or cuales (which, plural).\nThe rules to identify direct answers in Spanish are defined as follows: The answer turn (i.e., the turn after the question turn) contains yes keywords (si (yes), claro (sure), correct (correcto), vale (ok), por supuesto (sure), quizas (maybe), de acuerdo (understood), asi es (that\u2019s right)) or no keywords (no (no), nah, nope, no se (I don\u2019t know), no lo se (I don\u2019t know), no estoy seguro (I am not sure), ni idea (no idea)).\nIn Spanish, we consider both the accented (and proper) spelling and the unaccented ones).\nChinese For the Chinese corpora (NaturalConv and LCCC-base), we define the following rule to identify yes-no questions: A conversation turn ends with a modal particle \u5417 or \u561b, followed by a question mark (\u2018\uff1f\u2019).6\nWe also define rules to identify direct answers in the turn following yes-no questions. Our rules are defined as follows. The answer turn (i.e., the turn after the question turn) starts with:\n\u2022 yes keywords (\u5bf9 (right),\u597d (okay),\u55ef (uhhuh), \u6069 (uh-huh), \u5f53\u7136 (of course), \u5fc5\u987b (must)) or no keywords: (\u4e0d (no), and\u6ca1 (absence)) ; or \u2022 the first verb in the question, either in the affirmative or negative form. For example, in the yes-no question \u6211\u53ef\u4ee5\u5750\u8fd9\u91cc\u5417\uff1f (Can I sit here?), the first verb is \u53ef\u4ee5 (can). This rule matches turns that start with the verb\u53ef \u4ee5 (can) or its negated form\u4e0d\u53ef\u4ee5 (cannot).\nKorean For the Korean corpus, we define the following rules to identify yes-no questions. Recall that the corpus contains questions and answers, so the rules are designed to differentiate between yes-no questions and other questions rather than identifying yes-no questions in any corpus. A turn contains a yes-no question if it does not:\n\u2022 contain the following keywords indicating whquestions: \u110b\u1165\u1104\u1165\u11c2\u1100\u1166 (how),\u1106\u116f\u1100\u1161 (what),\u1106\u116f\u1100\u1161 (what is),\u110b\u1165\u1104\u1165\u11ab (which is),\u1106\u116e\u1109\u1173\u11ab (what),\u1106\u116f\u110b\u1166 \u110b\u116d (what),\u1106\u116f\u110b\u1168\u110b\u116d (what),\u110b\u1165\u11af\u1106\u1161\u11ab (how much), \u1103\u1173\u11af\u110b\u1165\u1100\u1161\u110b\u116d (how much available),\u1103\u1173\u11af\u110b\u1165\u1100\u1161\u1102\u1161\u110b\u116d (how much do you need),\u1106\u1167\u11be\u1107\u1165\u11ab (how many), \u1106\u1167\u11be\u1111\u1167\u11bc (how big),\u1106\u1167\u11be\u1111\u1167\u11bc (how big),\u1106\u1167\u11be\u1107\u1165\u11ab (how many times),\u110b\u1165\u11ab\u110c\u1166 (when),\u110b\u1165\u1103\u1175 (where),\u110b\u1165 \u1103\u1175\u11ba (where to),\u1106\u116e\u110b\u1165\u11ba\u110b\u1173\u11af (which one),\u110b\u1165\u1104\u1165\u11ab\u1100\u1165 (which one),\u1100\u1165\u11af\u1105\u1167\u110b\u116d (how much time does it take),\u1106\u1167\u11be\u1107\u116e\u11ab (how many minutes),\u1106\u1167\u11be\u1107\u116e\u11ab (how many minutes), \u110b\u1165\u1104\u1165\u11c2\u1100\u1166 (how), \u1106\u1167\u11be \u1109\u1175 (what time),\u110e\u1161\u1105\u1163\u11bc\u1107\u1165\u11ab\u1112\u1169\u1112\u116a\u11a8\u110b\u1175\u11ab (what VIN number),\u1107\u1175 \u110b\u116d\u11bc\u110b\u1173\u11ab\u110b\u116d (how much cost),\u110b\u1165\u1102\u1173 (where),\u110b\u1165\u11af\u1106\u1161 (how much),\u110b\u1165\u1104\u1165\u11ab\u1107\u116e\u1107\u116e\u11ab (which part),\u110b\u116b\u110b\u1161\u11ab (why not), \u110b\u1165\u1102\u1173 \u110d\u1169\u11a8 (which direction), \u110b\u1165\u1104\u1165\u11ab \u1106\u116e\u11af\u1111\u116e\u11b7 (which item), \u1109\u1175\u1101\u1161\u110c\u1175 (until what time), \u1106\u1167\u11be\u1109\u1175\u1101\u1161\u110c\u1175 (by what time),\u1106\u1167\u11be\u110b\u1175\u11af (what date), \u1106\u1167\u11be\u1109\u1175 (what time),\u1106\u1167\u11be\u110b\u1175\u11af (what date),\u1106\u1165\u110b\u1161\u11af\u110b\u1161 \u110b\u1163 (what should I know), \u1106\u116f \u1111\u1175\u11af\u110b\u116d (what do I need), \u1106\u1167\u11be\u1103\u1162 (how many cars), \u1106\u1167\u11be \u1103\u1162 (how many cars),\u110b\u1161\u11c1\u110c\u1161\u1105\u1175\u1102\u1173\u11ab\u110b\u116d (what are the digits); and\n6 This is Chinese, not the English question mark (\u2018?\u2019)\n\u2022 end in the following modal particle words or phrases that indicate statements instead of questions, as in \u1107\u1165\u1109\u1173\u1100\u1173\u1102\u1161\u11ab\u1111\u1169\u11a8\u110b\u116e\u11ab\u110c\u1165\u11ab\u110b\u1166\u1103\u1162\u1112\u1162 \u1100\u1161\u110c\u1175\u1100\u1169\u110c\u1169\u11b7\u1107\u116e\u11af\u1111\u1167\u11ab\u1109\u1175\u11ab\u1100\u1169\u1105\u1173\u11af\u110c\u1169\u11b7\u1112\u1161\u11af\u1105\u1167\u1100\u116e\u110b\u116d. (I was calling if I can file a civic complaint about reckless driving). The full list of particles is as follows: \u1112\u1161\u11af\u1105\u1167\u1100\u116e\u110b\u116d, \u1112\u1161\u1105\u1167\u1100\u116e\u110b\u116d, \u110b\u1161\u1102\u1175\u1106\u1167\u11ab, \u1112\u1162\u11bb\u1102\u1173\u11ab\u1103\u1166\u110b\u116d, \u1100\u1161\u11c0, \u110b\u1173\u11ab\u1103\u1166\u110b\u116d, \u1100\u1165\u1103\u1173\u11ab\u110b\u116d, \u1105\u1167\u11bb\u1102\u1173\u11ab\u1103\u1166\u110b\u116d, \u1100\u1161\u11c0\u110b\u1161\u1109\u1165\u110b\u116d,\u1109\u1175\u11c1\u110b\u1165\u1109\u1165\u110b\u116d,\u110b\u1173\u11ab\u110b\u116d,\u1100\u1161\u110c\u1175\u1100\u116e\u110b\u116d,\u1100\u1161\u11c0\u110b\u1162 \u1109\u1165, \u1112\u1161\u1102\u1173\u11ab\u1103\u1166\u110b\u116d, \u110f\u1161\u1102\u1173\u11ab\u1103\u1166\u110b\u116d, \u110c\u1169\u11b7 \u1112\u1161\u11af\u1101\u1166\u110b\u116d, \u1100\u1161\u1100\u1161 \u110c\u1175\u1100\u1169,\u1102\u116a\u1103\u116e\u1100\u1169\u1102\u1162\u1105\u1167\u11bb\u1102\u1173\u11ab\u1103\u1166.\nThe rules to identify direct answers in Korean rely on keywords:\n\u2022 yes keywords: \u1102\u1166 (yes), \u110b\u1168 (yeah), \u1100\u1173\u1105\u1165\u11c2 (right), and\u1106\u1161\u11bd\u110b\u1161 (correct) ; \u2022 no keywords: \u110b\u1161 \u110b\u1165\u11b9 (ah no), \u110b\u1168 \u110b\u1165\u11b9 (yeah, no),\u110b\u1161\u11ab\u1110\u1161\u1101\u1161\u11b8 (unfortunately),\u110b\u1161\u1102\u1175(no),\u110b\u1161\u1102\u116d (ney), and\u110b\u1161\u1102\u1175\u11b8 (no) ."
        },
        {
            "heading": "C Annotation Guidelines for the Benchmarks",
            "text": "We conduct manual annotations to obtain ground truth interpretations (i.e., gold labels) for indirect answers. We work with three labels: Yes, No, and Middle (Unknown), In order to minimize inconsistencies, we define labels as follows:\n\u2022 Yes: The answer leans towards (or implies) yes or yes under certain conditions or constraints. The latter could be interpreted as probably yes (e.g., Q: Ever done this before? A: Once.). \u2022 No: The answer leans towards (or implies) no, no under certain conditions or constraints (probably no), or provides arguments for no. The last two could be interpreted as probably no (e.g., Q: Can I at least have a drink? A: It\u2019s ten thirty in the morning.) \u2022 Middle (Unknown): The answer is unresponsive (e.g., changes the topic) or uninformative (e.g., \u201cI don\u2019t know\u201d). It should imply or lean towards neither yes nor no. (e.g., Q: Can you connect me to someone else? A: Well what\u2019s the situation?)"
        },
        {
            "heading": "D Data Statement",
            "text": "As recommended by Bender and Friedman (2018), we provide a data statement to better understand the new data presented in this paper.\nCuration Rationale\nWe develop new datasets to help interpret indirect answers to yes-no questions in eight languages.\nFirst, we adopt a rule-based approach to collect yesno questions, direct answers, and interpretations of the answers in five languages. The interpretations were obtained by automatically mapping the positive keyword to Yes, and the negative keyword to No. This data is noisy (see quality estimation in Table 3; Precision = 0.93\u20130.97 except in Hindi (0.67)) and intended to be used for distant supervision.\nSecond, we collect yes-no questions with indirect answers and manually annotate their interpretations in the eight languages we work with. For the five languages we used distant supervision with, we collect questions followed by answers not identified by our rules (i.e., without polar keywords). For the other three languages, we collect questions and indirect answers from scratch.\nSpanish and Chinese corpora come with context (i.e., the turn before the question and after the answer) since they were obtained from conversations. The corpora include only the questions and answers in the other languages.\nWe list all the original data sources for our data in Section 3.\nLanguage Variety We work with eight languages to develop our datasets. The language list is as follows: Bangla as spoken in Bangladesh (bn-BD), Chinese as spoken in Mainland China and written with simplified characters (zh-CN), Korean as spoken in South Korea (ko-KR), Turkish as spoken in Turkey (tr-TR), Spanish as spoken in United States, Canada, Puerto Rico, or The Dominican Republic (es-US, es-CA, es-PR, es-DO), Persian as spoken in Iran (fa-IR), and Nepali as spoken in Nepal (ne-NP). We are not able to provide language variety information for Hindi since the data comes from Twitter.\nSpeaker Demographic Our corpora are collected from various sources. We are not able to provide their speaker demographic since such information is absent in the original sources. However, all the corpora are spoken (or written) by native speakers.\nAnnotator Demographic We recruit 12 annotators consisting of two women and ten men. Their ages range from 18 to 30 years old. Among them, three individuals are native speakers of Chinese, two of Spanish, two of Bangla, one of Persian (and proficient in Turkish), one of Nepali (and proficient in Hindi), one of Hindi, one\nof Turkish, and one of Korean. All of them are highly proficient in English. Ethnic backgrounds are as follows: four individuals are from East Asia, four are from South Asia, one is from the Middle East, one is from Europe, one is from North America, and one is from South America. Socioeconomic backgrounds are as follows: all annotators reported that they are middle class.\nEducational backgrounds are as follows. We have two undergraduate, nine graduate students, and one with a doctoral degree. Nine of them work in NLP-related research areas, two work in other research areas within Computer Science, and one has Mathematics background.\nSpeech Situation\nChinese corpora are from two sources: (a) written by crowdsourcing workers who are given a specific topic, and (b) written by users on social media platforms. The Spanish corpus consists of transcripts of telephone conversations between humans. The Nepali corpus is written by journalists from a daily news website. The Korean corpus is written by civilians and covers complaints and replies from public organizations. The Hindi corpus is written by Twitter users. The Turkish corpus is written by humans and consists of frequently asked questions and community questions. The Bangla corpus comes from a crawled dataset from online sources. The Persian corpus is transcriptions of spoken language by humans having informal conversations, including telephone calls and face-to-face interactions."
        },
        {
            "heading": "E Detailed Results for Cross-Lingual Transferring to a New Language",
            "text": "We provide supplemental results in Table 10 that further include Precision (P), Recall (R) and F1score (F) for our cross-lingual learning experiments. This table complements Table 6."
        },
        {
            "heading": "F Experimental Results with XLM-Align",
            "text": "We present the experimental results obtained with XLM-Align in Table 11. The results are listed in three blocks and each block is comparable to Table 6, Table 7 and Table 8 respectively. We observe a similar trend in the results with both models, while XLM-RoBERTa outperforms XLM-Align in most cases."
        },
        {
            "heading": "G Training Details",
            "text": "Referring to Section 5, we conduct our experiments on an off-the-shelf XLM-RoBERTa-base (parameters: 279M) and XLM-Align-base model (parameters: 279M) from HuggingFace (Wolf et al., 2020). Both models are multilingual transformers with attention mechanism which has been utilized in various domains (Liu et al., 2023; Mnih et al., 2014). We run the experiments on a single NVIDIA Tesla V100 (32GB) GPU. Depending on the size of training datasets, the training time may vary, but approx-\nimately it takes 10 minutes to train 1 epoch.\nWe tune with several hyperparameters to obtain the experimental results with XLM-RoBERTa-base. We list them in Table 12. We also tune the blending factor \u03b1 (i.e., the ratio of data from distant supervision added to each training epoch). We list the \u03b1 for each benchmark in Table 13. For experiments with XLM-Align-base, we follow the same hyperparameters and blending factors."
        },
        {
            "heading": "H Details of In-context Learning with ChatGPT",
            "text": "We test 30 random samples for each language with ChatGPT (text-davinci-002) and our best model, as explained in Section 5.5. Figure 2 presents the prompt. In order to give ChatGPT the most credit possible, we manually map the generated answers from ChatGPT into Yes, No, and Middle."
        }
    ],
    "title": "Interpreting Indirect Answers to Yes-No Questions in Multiple Languages",
    "year": 2023
}