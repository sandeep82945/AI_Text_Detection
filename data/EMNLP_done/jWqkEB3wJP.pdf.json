{
    "abstractText": "Pre-trained language models (PLMs) have demonstrated exceptional performance across a wide range of natural language processing tasks. The utilization of PLM-based sentence embeddings enables the generation of contextual representations that capture rich semantic information. However, despite their success with unseen samples, current PLM-based representations suffer from poor robustness in adversarial settings. In this paper, we propose RobustEmbed, a self-supervised sentence embedding framework that enhances both generalization and robustness in various text representation tasks and against a diverse set of adversarial attacks. By generating high-risk adversarial perturbations to promote higher invariance in the embedding space and leveraging the perturbation within a novel contrastive objective approach, RobustEmbed effectively learns high-quality sentence embeddings. Our extensive experiments validate the superiority of RobustEmbed over the state-of-the-art self-supervised representations in adversarial settings, while also showcasing relative improvements in seven semantic textual similarity (STS) tasks and six transfer tasks. Specifically, our framework achieves a significant reduction in attack success rate from 75.51% to 39.62% for the BERTAttack attack technique, along with enhancements of 1.20% and 0.40% in STS tasks and transfer tasks, respectively.",
    "authors": [
        {
            "affiliations": [],
            "name": "Javad Rafiei Asl"
        },
        {
            "affiliations": [],
            "name": "Eduardo Blanco"
        },
        {
            "affiliations": [],
            "name": "Daniel Takabi"
        }
    ],
    "id": "SP:1f88776967d24c7dfb0636a36a760d29e815269a",
    "references": [
        {
            "authors": [
                "Eneko Agirre",
                "Carmen Banea",
                "Claire Cardie",
                "Daniel Cer",
                "Mona Diab",
                "Aitor Gonzalez-Agirre",
                "Weiwei Guo",
                "Inigo Lopez-Gazpio",
                "Montse Maritxalar",
                "Rada Mihalcea"
            ],
            "title": "Semeval-2015 task 2: Semantic textual similarity, english, spanish and pilot on",
            "year": 2015
        },
        {
            "authors": [
                "Eneko Agirre",
                "Carmen Banea",
                "Claire Cardie",
                "Daniel Cer",
                "Mona Diab",
                "Aitor Gonzalez-Agirre",
                "Weiwei Guo",
                "Rada Mihalcea",
                "German Rigau",
                "Janyce Wiebe."
            ],
            "title": "SemEval-2014 task 10: Multilingual semantic textual similarity",
            "venue": "Proceedings of the 8th Interna-",
            "year": 2014
        },
        {
            "authors": [
                "Eneko Agirre",
                "Carmen Banea",
                "Daniel Cer",
                "Mona Diab",
                "Aitor Gonzalez-Agirre",
                "Rada Mihalcea",
                "German Rigau",
                "Janyce Wiebe."
            ],
            "title": "SemEval-2016 task 1: Semantic textual similarity, monolingual and cross-lingual evaluation",
            "venue": "Proceedings of the",
            "year": 2016
        },
        {
            "authors": [
                "Eneko Agirre",
                "Daniel Cer",
                "Mona Diab",
                "Aitor Gonzalez-Agirre."
            ],
            "title": "SemEval-2012 task 6: A pilot on semantic textual similarity",
            "venue": "*SEM 2012: The First Joint Conference on Lexical and Computational Semantics \u2013 Volume 1: Proceedings of the",
            "year": 2012
        },
        {
            "authors": [
                "Eneko Agirre",
                "Daniel Cer",
                "Mona Diab",
                "Aitor GonzalezAgirre",
                "Weiwei Guo."
            ],
            "title": "SEM 2013 shared task: Semantic textual similarity",
            "venue": "Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Confer-",
            "year": 2013
        },
        {
            "authors": [
                "Necva B\u00f6l\u00fcc\u00fc",
                "Burcu Can",
                "Harun Artuner."
            ],
            "title": "A siamese neural network for learning semanticallyinformed sentence embeddings",
            "venue": "Expert Systems with Applications, 214:119103.",
            "year": 2023
        },
        {
            "authors": [
                "Samuel R. Bowman",
                "Gabor Angeli",
                "Christopher Potts",
                "Christopher D. Manning."
            ],
            "title": "A large annotated corpus for learning natural language inference",
            "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages",
            "year": 2015
        },
        {
            "authors": [
                "Daniel Cer",
                "Mona Diab",
                "Eneko Agirre",
                "I\u00f1igo LopezGazpio",
                "Lucia Specia."
            ],
            "title": "SemEval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation",
            "venue": "Proceedings of the 11th International Workshop on Semantic",
            "year": 2017
        },
        {
            "authors": [
                "Daniel Cer",
                "Yinfei Yang",
                "Sheng-yi Kong",
                "Nan Hua",
                "Nicole Limtiaco",
                "Rhomni St. John",
                "Noah Constant",
                "Mario Guajardo-Cespedes",
                "Steve Yuan",
                "Chris Tar",
                "Brian Strope",
                "Ray Kurzweil."
            ],
            "title": "Universal sentence encoder for English",
            "venue": "Proceedings of",
            "year": 2018
        },
        {
            "authors": [
                "Anirban Chakraborty",
                "Manaar Alam",
                "Vishal Dey",
                "Anupam Chattopadhyay",
                "Debdeep Mukhopadhyay."
            ],
            "title": "A survey on adversarial attacks and defences",
            "venue": "CAAI Transactions on Intelligence Technology, 6(1):25\u201345.",
            "year": 2021
        },
        {
            "authors": [
                "Ting Chen",
                "Simon Kornblith",
                "Mohammad Norouzi",
                "Geoffrey Hinton."
            ],
            "title": "A simple framework for contrastive learning of visual representations",
            "venue": "International conference on machine learning, pages 1597\u20131607. PMLR.",
            "year": 2020
        },
        {
            "authors": [
                "Alexis Conneau",
                "Douwe Kiela."
            ],
            "title": "SentEval: An evaluation toolkit for universal sentence representations",
            "venue": "Proceedings of the Eleventh International",
            "year": 2018
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
            "year": 2019
        },
        {
            "authors": [
                "Ning Ding",
                "Yujia Qin",
                "Guang Yang",
                "Fuchao Wei",
                "Zonghan Yang",
                "Yusheng Su",
                "Shengding Hu",
                "Yulin Chen",
                "Chi-Min Chan",
                "Weize Chen"
            ],
            "title": "Parameter-efficient fine-tuning of large-scale pretrained language models",
            "year": 2023
        },
        {
            "authors": [
                "William B. Dolan",
                "Chris Brockett."
            ],
            "title": "Automatically constructing a corpus of sentential paraphrases",
            "venue": "Proceedings of the Third International Workshop on Paraphrasing (IWP2005).",
            "year": 2005
        },
        {
            "authors": [
                "Xinshuai Dong",
                "Anh Tuan Luu",
                "Rongrong Ji",
                "Hong Liu."
            ],
            "title": "Towards robustness against natural language word substitutions",
            "venue": "International Conference on Learning Representations.",
            "year": 2021
        },
        {
            "authors": [
                "Tianyu Gao",
                "Xingcheng Yao",
                "Danqi Chen."
            ],
            "title": "SimCSE: Simple contrastive learning of sentence embeddings",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6894\u20136910, Online and Punta Cana, Do-",
            "year": 2021
        },
        {
            "authors": [
                "Siddhant Garg",
                "Goutham Ramakrishnan."
            ],
            "title": "BAE: BERT-based adversarial examples for text classification",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6174\u20136181, Online. Association for",
            "year": 2020
        },
        {
            "authors": [
                "Ian J. Goodfellow",
                "Jonathon Shlens",
                "Christian Szegedy."
            ],
            "title": "Explaining and harnessing adversarial examples",
            "venue": "3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceed-",
            "year": 2015
        },
        {
            "authors": [
                "Raia Hadsell",
                "Sumit Chopra",
                "Yann LeCun."
            ],
            "title": "Dimensionality reduction by learning an invariant mapping",
            "venue": "2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR\u201906), volume 2, pages 1735\u20131742. IEEE.",
            "year": 2006
        },
        {
            "authors": [
                "Jens Hauser",
                "Zhao Meng",
                "Damian Pascual",
                "Roger Wattenhofer."
            ],
            "title": "Bert is robust! a case against word substitution-based adversarial attacks",
            "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing",
            "year": 2023
        },
        {
            "authors": [
                "Pengcheng He",
                "Xiaodong Liu",
                "Jianfeng Gao",
                "Weizhu Chen."
            ],
            "title": "Deberta: Decoding-enhanced bert with disentangled attention",
            "venue": "International Conference on Learning Representations.",
            "year": 2021
        },
        {
            "authors": [
                "Minqing Hu",
                "Bing Liu."
            ],
            "title": "Mining and summarizing customer reviews",
            "venue": "Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 168\u2013177.",
            "year": 2004
        },
        {
            "authors": [
                "Haoming Jiang",
                "Pengcheng He",
                "Weizhu Chen",
                "Xiaodong Liu",
                "Jianfeng Gao",
                "Tuo Zhao."
            ],
            "title": "SMART: Robust and efficient fine-tuning for pretrained natural language models through principled regularized optimization",
            "venue": "Proceedings of the 58th",
            "year": 2020
        },
        {
            "authors": [
                "Di Jin",
                "Zhijing Jin",
                "Joey Tianyi Zhou",
                "Peter Szolovits."
            ],
            "title": "Is bert really robust? a strong baseline for natural language attack on text classification and entailment",
            "venue": "Proceedings of the AAAI conference on artificial intelligence, volume 34, pages",
            "year": 2020
        },
        {
            "authors": [
                "Quoc Le",
                "Tomas Mikolov."
            ],
            "title": "Distributed representations of sentences and documents",
            "venue": "International conference on machine learning, pages 1188\u2013 1196. PMLR.",
            "year": 2014
        },
        {
            "authors": [
                "Bohan Li",
                "Hao Zhou",
                "Junxian He",
                "Mingxuan Wang",
                "Yiming Yang",
                "Lei Li."
            ],
            "title": "On the sentence embeddings from pre-trained language models",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
            "year": 2020
        },
        {
            "authors": [
                "Jinfeng Li",
                "Shouling Ji",
                "Tianyu Du",
                "Bo Li",
                "Ting Wang."
            ],
            "title": "TextBugger: Generating adversarial text against real-world applications",
            "venue": "Proceedings 2019 Network and Distributed System Security Symposium. Internet Society.",
            "year": 2019
        },
        {
            "authors": [
                "Linyang Li",
                "Ruotian Ma",
                "Qipeng Guo",
                "Xiangyang Xue",
                "Xipeng Qiu."
            ],
            "title": "BERT-ATTACK: Adversarial attack against BERT using BERT",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages",
            "year": 2020
        },
        {
            "authors": [
                "Zhuorong Li",
                "Daiwei Yu",
                "Minghui Wu",
                "Canghong Jin",
                "Hongchuan Yu."
            ],
            "title": "Adversarial supervised contrastive learning",
            "venue": "Machine Learning, 112(6):2105\u20132130.",
            "year": 2023
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov"
            ],
            "title": "Roberta: A robustly optimized bert pretraining approach",
            "year": 2019
        },
        {
            "authors": [
                "Wattenhofer"
            ],
            "title": "Self-supervised contrastive learn",
            "year": 2022
        },
        {
            "authors": [
                "Bastian Bischoff"
            ],
            "title": "On detecting adversar",
            "year": 2017
        },
        {
            "authors": [
                "Krueger",
                "David Schnurr",
                "Felipe Petroski Such",
                "Kenny Hsu",
                "Madeleine Thompson",
                "Tabarak Khan",
                "Toki Sherbakov",
                "Joanne Jang",
                "Peter Welinder",
                "Lilian Weng"
            ],
            "title": "Text and code embeddings by contrastive pre-training",
            "year": 2022
        },
        {
            "authors": [
                "Jianmo Ni",
                "Gustavo Hernandez Abrego",
                "Noah Constant",
                "Ji Ma",
                "Keith Hall",
                "Daniel Cer",
                "Yinfei Yang."
            ],
            "title": "Sentence-t5: Scalable sentence encoders from pretrained text-to-text models",
            "venue": "Findings of the Association for Computational Linguistics: ACL 2022,",
            "year": 2022
        },
        {
            "authors": [
                "Yixin Nie",
                "Adina Williams",
                "Emily Dinan",
                "Mohit Bansal",
                "Jason Weston",
                "Douwe Kiela."
            ],
            "title": "Adversarial NLI: A new benchmark for natural language understanding",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,",
            "year": 2020
        },
        {
            "authors": [
                "Lin Pan",
                "Chung-Wei Hang",
                "Avirup Sil",
                "Saloni Potdar."
            ],
            "title": "Improved text classification via contrastive adversarial training",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 11130\u201311138.",
            "year": 2022
        },
        {
            "authors": [
                "Bo Pang",
                "Lillian Lee."
            ],
            "title": "A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts",
            "venue": "Annual Meeting of the Association for Computational Linguistics.",
            "year": 2004
        },
        {
            "authors": [
                "Bo Pang",
                "Lillian Lee."
            ],
            "title": "Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales",
            "venue": "Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL\u201905), pages 115\u2013124, Ann",
            "year": 2005
        },
        {
            "authors": [
                "Bo Pang",
                "Lillian Lee."
            ],
            "title": "Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales",
            "venue": "Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL\u201905), pages 115\u2013124, Ann",
            "year": 2005
        },
        {
            "authors": [
                "Jeffrey Pennington",
                "Richard Socher",
                "Christopher Manning."
            ],
            "title": "GloVe: Global vectors for word representation",
            "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1532\u20131543, Doha, Qatar.",
            "year": 2014
        },
        {
            "authors": [
                "Yao Qiu",
                "Jinchao Zhang",
                "Jie Zhou."
            ],
            "title": "Improving gradient-based adversarial training for text classification by contrastive learning and auto-encoder",
            "venue": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 1698\u20131707.",
            "year": 2021
        },
        {
            "authors": [
                "Nils Reimers",
                "Iryna Gurevych."
            ],
            "title": "Sentence-bert: Sentence embeddings using siamese bert-networks",
            "venue": "Conference on Empirical Methods in Natural Language Processing.",
            "year": 2019
        },
        {
            "authors": [
                "Shuhuai Ren",
                "Yihe Deng",
                "Kun He",
                "Wanxiang Che."
            ],
            "title": "Generating natural language adversarial examples through probability weighted word saliency",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1085\u2013",
            "year": 2019
        },
        {
            "authors": [
                "Daniela N. Rima",
                "DongNyeong Heo",
                "Heeyoul Choi."
            ],
            "title": "Adversarial training with contrastive learning in nlp",
            "venue": "Computer Speech & Language. Submitted.",
            "year": 2022
        },
        {
            "authors": [
                "Ali Shafahi",
                "Mahyar Najibi",
                "Zheng Xu",
                "John Dickerson",
                "Larry S Davis",
                "Tom Goldstein."
            ],
            "title": "Universal adversarial training",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 5636\u20135643.",
            "year": 2020
        },
        {
            "authors": [
                "Richard Socher",
                "Alex Perelygin",
                "Jean Wu",
                "Jason Chuang",
                "Christopher D. Manning",
                "Andrew Ng",
                "Christopher Potts."
            ],
            "title": "Recursive deep models for semantic compositionality over a sentiment treebank",
            "venue": "Proceedings of the 2013 Conference on Empiri-",
            "year": 2013
        },
        {
            "authors": [
                "Jianlin Su",
                "Jiarun Cao",
                "Weijie Liu",
                "Yangyiwen Ou."
            ],
            "title": "Whitening sentence representations for better semantics and faster retrieval",
            "venue": "CoRR, abs/2103.15316.",
            "year": 2021
        },
        {
            "authors": [
                "Chi Sun",
                "Xipeng Qiu",
                "Yige Xu",
                "Xuanjing Huang."
            ],
            "title": "How to fine-tune bert for text classification? In China national conference on Chinese computational linguistics, pages 194\u2013206",
            "venue": "Springer.",
            "year": 2019
        },
        {
            "authors": [
                "Dilin Wang",
                "Chengyue Gong",
                "Qiang Liu."
            ],
            "title": "Improving neural language modeling via adversarial training",
            "venue": "International Conference on Machine Learning, pages 6555\u20136565. PMLR.",
            "year": 2019
        },
        {
            "authors": [
                "Dong Wang",
                "Ning Ding",
                "Piji Li",
                "Hai-Tao Zheng."
            ],
            "title": "Cline: Contrastive learning with semantic negative examples for natural language understanding",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the",
            "year": 2021
        },
        {
            "authors": [
                "Qian Wang",
                "Weiqi Zhang",
                "Tianyi Lei",
                "Yu Cao",
                "Dezhong Peng",
                "Xu Wang."
            ],
            "title": "Clsep: Contrastive learning of sentence embedding with prompt",
            "venue": "KnowledgeBased Systems, 266:110381.",
            "year": 2023
        },
        {
            "authors": [
                "Yan Zhang",
                "Ruidan He",
                "Zuozhu Liu",
                "Kwan Hui Lim",
                "Lidong Bing."
            ],
            "title": "An unsupervised sentence embedding method by mutual information maximization",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2020
        },
        {
            "authors": [
                "Chen Zhu",
                "Yu Cheng",
                "Zhe Gan",
                "Siqi Sun",
                "Tom Goldstein",
                "Jingjing Liu."
            ],
            "title": "Freelb: Enhanced adversarial training for natural language understanding",
            "venue": "International Conference on Learning Representations.",
            "year": 2020
        },
        {
            "authors": [
                "Yukun Zhu",
                "Ryan Kiros",
                "Rich Zemel",
                "Ruslan Salakhutdinov",
                "Raquel Urtasun",
                "Antonio Torralba",
                "Sanja Fidler."
            ],
            "title": "Aligning books and movies: Towards story-like visual explanations by watching movies and reading books",
            "venue": "Proceedings of the IEEE in-",
            "year": 2015
        },
        {
            "authors": [
                "Pang",
                "Lee"
            ],
            "title": "2005a) is a sentencelevel dataset consisting of 8,530 training and 1,066 testing highly polar samples, where negative and positive classes are assigned based on calibration among different critics",
            "venue": "SNLI (Bowman et al.,",
            "year": 2005
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Recent research has demonstrated the state-of-theart performance of Pre-trained Language Models (PLMs) in learning contextual word embeddings (Devlin et al., 2019), leading to improved generalization in various Natural Language Processing (NLP) tasks (Yang et al., 2019; He et al., 2021; Ding et al., 2023). The focus of PLMs has extended to acquiring universal sentence embeddings, such as Universal Sentence Encoder (USE) (Cer et al., 2018) and Sentence-BERT (Reimers and Gurevych,\n2019), which effectively capture the semantic representation of the input text. This representation learning facilitates feature generation for classification tasks and enhances large-scale semantic search (Neelakantan et al., 2022).\nThe assessment of PLM-based sentence representation relies on two crucial characteristics: generalization and robustness. While considerable research efforts have been dedicated to developing universal sentence embeddings using PLMs (Reimers and Gurevych, 2019; Zhang et al., 2020; Ni et al., 2022; Neelakantan et al., 2022; Wang et al., 2023; B\u00f6l\u00fcc\u00fc et al., 2023), it is worth noting that despite their promising performance across various downstream classification tasks (Sun et al., 2019; Gao et al., 2021), demonstrating proficiency in generalization, these representations exhibit limitations in terms of robustness in adversarial settings and are vulnerable to diverse adversarial attacks (Nie et al., 2020; Wang et al., 2021). Existing research (Garg and Ramakrishnan, 2020; Wu et al., 2023; Hauser et al., 2023) highlights the poor robustness of these representations, such as BERTbased representations, which can be deceived by replacing a few words in the input sentence.\nIn this paper, we propose RobustEmbed, a robust sentence embedding framework that takes both of these essential characteristics into account. The core concept involves introducing a small adversarial perturbation to the input text and employing the contrastive objective (Chen et al., 2020) to learn high-quality sentence embeddings. RobustEmbed perturbs the embedding space rather than the raw text, which exhibits a positive correlation with generalization and promotes higher invariance. Our framework utilizes the original embedding along with the perturbed embedding as \u201cpositive pairs,\u201d while other sentence embeddings in the same minibatch serve as \u201cnegatives.\u201d The contrastive objective identifies the positive pairs among the negatives. By incorporating norm-bounded adversarial\nperturbation and contrastive objectives, our method enhances the robustness of similar sentences and disperses sentences with different semantics. This straightforward and efficient approach yields superior sentence embeddings in terms of both generalization and robustness benchmarks.\nWe conduct extensive experiments on a wide range of text representation and NLP tasks to verify the effectiveness of RobustEmbed including semantic textual similarity (STS) tasks (Conneau and Kiela, 2018), transfer tasks (Conneau and Kiela, 2018), and TextAttack (Morris et al., 2020). Two first series of experiments evaluate the quality of sentence embeddings on semantic similarity and natural language understanding tasks, while the last series assess the robustness of the framework against state-of-the-art adversarial attacks. RobustEmbed demonstrates significant improvements in robustness, reducing the attack success rate from 75.51% to 39.62% for the BERTAttack attack technique and achieving similar improvements against other adversarial attacks. Additionally, the framework achieves performance improvements of 1.20% and 0.40% on STS tasks and NLP transfer tasks, respectively, when employing the BERTbase encoder.\nContributions. Our main contributions in this paper are summarized as follows:\n\u2022 We introduce RobustEmbed, a novel selfsupervised framework for sentence embeddings that generates robust representations capable of withstanding various adversarial attacks. Existing sentence embeddings are susceptible to such attacks, highlighting a vulnerability in their security. RobustEmbed fills this gap by employing high-risk perturbations within a novel contrastive learning approach.\n\u2022 We conduct extensive experiments to demonstrate the efficacy of RobustEmbed across various text representation tasks and against state-of-the-art adversarial attacks. Empirical results confirm the high efficiency of our framework in terms of both generalization and robustness benchmarks.\n\u2022 To facilitate further research in this important area, our source code is available in the RobustEmbed Repository"
        },
        {
            "heading": "2 Related Work",
            "text": "The early work in text representations focused on applying the distributional hypothesis to predict words based on their context (Mikolov et al., 2013b,a). There are extensive studies on learning universal sentence embeddings using supervised and unsupervised approaches, such as Doc2vec (Le and Mikolov, 2014), SkipThought (Zhu et al., 2015), Universal Sentence Encoder (Cer et al., 2018), and Sentence-BERT (Reimers and Gurevych, 2019). More recently, self-supervised approaches have emerged, employing contrastive objectives to learn effective and robust text representations: SimCSE (Gao et al., 2021) introduced a minimal augmentation strategy to predict the input sentence by applying two different dropout masks. The ConSERT model (Yan et al., 2021) utilized four distinct data augmentation techniques to generate diverse views for the purpose of executing a contrastive objective: adversarial attacks, token shuffling, cut-off, and dropout. Qiu et al. (2021) introduced two adversarial training methods, CARL and RAR, to strengthen the ML model\u2019s defense against gradient-based adversarial attacks. CARL aims to acquire a resilient representation at the sentence level, whereas RAR focuses on enhancing the robustness of individual word representations. Rima et al. (2022) proposed adversarial training with contrastive learning for training natural language processing models. It involves applying linear perturbations to input embeddings and leveraging contrastive learning to minimize the distance between original and perturbed representations. Pan et al. (2022) presents a straightforward approach for regularizing transformer-based encoders during the fine-tuning step. The model achieves noise-invariant representations by generating adversarial examples perturbing word embeddings and leveraging contrastive learning.\nIn comparison to several existing contrastive adversarial learning approaches in the text representation area (Yan et al., 2021; Meng et al., 2022; Qiu et al., 2021; Li et al., 2023; Rima et al., 2022; Pan et al., 2022), our framework stands out by generating more efficient high-risk iterative perturbations in the embedding space. Furthermore, our framework leverages a more powerful contrastive objective approach, leading to high-quality text representations that demonstrate enhanced generalization and robustness properties. Empirical results substantiate the superiority of our approach across\nvarious generalization and robustness benchmarks."
        },
        {
            "heading": "3 Background",
            "text": "In this section, we present an overview of the recent progress in adversarial perturbation generation and self-supervised contrastive learning."
        },
        {
            "heading": "3.1 Adversarial Perturbation Generation",
            "text": "Adversarial perturbation involves adding maliciously crafted perturbations to benign data, which can deceive Machine Learning (ML) models, including deep learning methods (Goodfellow et al., 2015). These perturbations are designed to be imperceptible to humans but can cause the model to make incorrect predictions (Metzen et al., 2017). Adversarial training, which involves incorporating adversarial perturbations during the model training process, has been shown to enhance the model\u2019s robustness against adversarial attacks (Madry et al., 2018; Shafahi et al., 2020; Xu et al., 2020; Wang et al., 2019b). While various perturbation generation techniques have contributed to machine vision (Chakraborty et al., 2021), the progress of these techniques in the NLP domain has been at a slower pace due to the discrete nature of text (Jin et al., 2020). In recent years, instead of directly applying adversarial perturbations to raw text, a few studies have focused on perturbing the embedding space (Wang et al., 2019a; Dong et al., 2021). However, these methods still face challenges in terms of generalization, as they may not be applicable to any ML model and NLP tasks. Utilized within our framework, a more generalized approach for generating high-risk adversarial perturbations involves applying a small noise \u03b4 within a norm ball to the embedding space, aiming to maximize the adversarial loss:\narg max ||\u03b4||\u2264\u03f5 L(f\u03b8(X + \u03b4), y), (1)\nwhere f\u03b8(.) denotes an ML model parameterized with X as the sub-word embeddings, and y is the truth label. Various gradient-based algorithms have been proposed to address this optimization problem. We employ a practical combination of the Fast Gradient Sign Method (FGSM) (Goodfellow et al., 2015) and the Projected Gradient Descent (PGD) technique (Madry et al., 2018) to generate adversarial perturbations that represent worst-case examples."
        },
        {
            "heading": "3.2 Contrastive Learning Based Representation",
            "text": "The objective of contrastive learning is to acquire effective low-dimensional representations by bringing semantically similar samples closer and pushing dissimilar ones further apart (Hadsell et al., 2006). Self-supervised contrastive learning has demonstrated promising results in data representation across domains such as machine vision (Chen et al., 2020), natural language processing (Gao et al., 2021; Neelakantan et al., 2022), and speech recognition (Lodagala et al., 2023). Our framework adopts the contrastive learning concept proposed by Chen et al. (2020) to generate high-quality representations. Let {(xi, x+i )}Ni=1 denote a set of N positive pairs, where xi and x+i are semantically correlated and (zi, z+i ) represents the corresponding embedding vectors for the positive pair (xi, x+i ). We define zi\u2019s positive set as {xposi } = z + i , while the negative set {x neg i } as the set of other positive pairs. Then, the contrastive training objective can be defined as follows:\nLcon,\u03b8(xi, {xposi }, {x neg i }) = (2)\n\u2212 log(\n\u2211 {xposi }\nexp(sim(zi, {xposi })/\u03c4)\u2211 {xposi , x neg i } exp(sim(zi, {xposi , x neg i })/\u03c4) ),\nwhere \u03c4 denotes a temperature hyperparameter and sim(u, v) = u\n\u22a4v \u2225u\u2225.\u2225v\u2225 is the cosine similarity\nbetween two representation vectors. The standard objective function only contains a single sample in the positive set. The total loss is computed over all positive pairs within a mini-batch."
        },
        {
            "heading": "4 The Proposed Adversarial Self-supervised Contrastive Learning",
            "text": "We introduce RobustEmbed, a simple yet effective approach for generating universal text representations through adversarial training of a selfsupervised contrastive learning model. Given a PLM f\u03b8(.) as the encoder and a large unsupervised dataset D, RobustEmbed aims to pre-train f\u03b8(.) on D to enhance the efficiency of sentence embeddings across diverse NLP tasks (improved generalization) and increase resilience against various adversarial attacks (enhanced robustness). Algorithm 1 demonstrates our framework\u2019s approach to generating a norm-bounded perturbation using an iterative process, confusing the f\u03b8(.) model by treating the perturbed embeddings as different instances.\nOur framework then employs a contrastive learning approach to maximize the similarity between the embedding of an input instance and the adversarial embedding of its positive pair. Moreover, Figure 1 provides an overview of our RobustEmbed framework, which aims to achieve adversarial robustness in representations. The framework involves an iterative collaboration between the perturbation generator and the f\u03b8(.) model to generate high-risk perturbations for adversarial contrastive learning during the final training step. The subsequent sections delve into the main components of our framework and provide a detailed analysis of the training objective."
        },
        {
            "heading": "4.1 Perturbation Generation",
            "text": "As the primary step, RobustEmbed aims to generate small perturbations that fool the ML model, leading to incorrect predictions, while remaining nearly imperceptible to humans. The framework uses an approach based on combination of the PGD and FGSM algorithms to generate a perturbation that maximizes the self-supervised contrastive loss, facilitating discrimination between various instances. RobustEmbed employs multiple iterations of this combination, specifically T-step FGSM and K-step PGD, to meticulously reinforce invariance within the embedding space, ultimately resulting in enhanced generalization and robustness.\nIn particular, considering the PLM-based encoder f\u03b8(.) and an input sentence x, RobustEmbed passes the sentence to the f\u03b8(.) model twice: by\nAlgorithm 1: RobustEmbed Algorithm Input: Epoch number E, PLM Encoder f\u03b8 , dataset of\nraw sentences D = {xi}Ni=1, perturbation \u03b4, dropout masks m1 and m2, perturbation bound \u03f5, step sizes \u03b1 and \u03b2, learning rate \u03b7, perturbation modulator \u03bb, regularization parameter \u03b3, perturbation generation iterators K and T , contrastive learning objective Lcon,\u03b8 (eq. 2)\nOutput: Robust Sentence Representation for epoch = 1, ..., E do\nfor minibatch B \u2282 D do \u03b41 \u223c N (0, \u03c32I) X = f\u03b8.embedding(B, m1) X+ = f\u03b8.embedding(B, m2) for t = 1, ...,max(K, T ) do\ng(\u03b4t) = \u2207\u03b4Lcon,\u03b8(X + \u03b4t, {X+}) if t \u2264 K then\n\u03b4t+1pgd = \u03a0\u2225\u03b4\u2225P\u2264\u03f5(\u03b4 t + \u03b1g(\u03b4t)/\u2225g(\u03b4t)\u2225P )\nend if t \u2264 T then\n\u03b4t+1fgsm = \u03a0\u2225\u03b4\u2225P\u2264\u03f5(\u03b4 t + \u03b2sign(g(\u03b4t)))\nend end \u03b4f = \u03bb\u03b4 K pgd + (1\u2212 \u03bb)\u03b4Tfgsm LRobustEmbed, \u03b8 := Lcon,\u03b8(X, {X+, X + \u03b4f}) Ltotal := LRobustEmbed, \u03b8 + \u03b3Lcon,\u03b8(X + \u03b4f , {X+}) \u03b8 = \u03b8 \u2212 \u03b7\u2207\u03b8Ltotal\nend end\napplying the standard dropout twice, two different embeddings of (X,X+) are obtained as \u201cpositive pairs\u201d (Gao et al., 2021). The framework takes the following steps to update the perturbation separately for the PGD and FGSM in iteration k + 1\nand t+ 1 respectively:\n\u03b4k+1pgd = \u03a0\u2225\u03b4\u2225P\u2264\u03f5(\u03b4 k + \u03b1g(\u03b4k)/\u2225g(\u03b4k)\u2225P ), (3)\n\u03b4t+1fgsm = \u03a0\u2225\u03b4\u2225P\u2264\u03f5(\u03b4 t + \u03b2sign(g(\u03b4t))), (4)\nwhere g(\u03b4n) = \u2207\u03b4Lcon,\u03b8(X+\u03b4n, {X+}) with n = t or k is the gradient of the contrastive learning loss with respect to \u03b4. The perturbation is generated by the \u2113\u221e norm-ball around the input embedding with radius \u03f5, and \u03a0 projects the perturbation onto the \u03f5-ball. Further, \u03b1 and \u03b2 are the step sizes of the attacks and sign(.) returns the sign of the vector. Essentially, T-step FGSM and Kstep PGD are mathematically equivalent when P is either 2 or \u221e. Their primary distinctions lie in the number of iterations (i.e., T and K) and the step size of the attack (i.e., \u03b1 and \u03b2 ) used to modify the input perturbation, ultimately generating a unique high-level perturbation. The final perturbation can be obtained through the combination of T-step FGSM and K-step PGD:\n\u03b4final = \u03bb\u03b4 K pgd + (1\u2212 \u03bb)\u03b4Tfgsm, (5)\nwhere 0 \u2264 \u03bb \u2264 1 modulates the relative significance of each separate perturbation in the generation of the final perturbation."
        },
        {
            "heading": "4.2 Robust Contrastive Learning",
            "text": "To achieve robust representation through selfsupervised contrastive learning, adversarial learning objective, which follows a min-max formulation to minimize the maximum risk for any perturbation \u03b4 (Madry et al., 2018), could be defined as follows:\nargmin\u03b8 E(x)\u223cD[max\u2225\u03b4\u2225\u2264\u03f5 Lcon,\u03b8(X + \u03b4, {X+})], (6)\nwhere X + \u03b4 is the adversarial embedding generated by the iterative gradient-based perturbation generation (eq. 5). Our framework utilizes adversarial examples generated in the embedding space, rather than using the original raw text, resulting in an ultimate pre-trained model that is robust against m-way instance-wise adversarial attacks. The framework employs the contrastive learning objective to maximize the similarity between clean examples and their adversarial perturbation by incorporating the adversarial example as the additional element in the positive set:\nLRobustEmbed, \u03b8 := Lcon,\u03b8(x, {xpos, xadv}), (7)\nLtotal := LRobustEmbed, \u03b8 + \u03b3Lcon,\u03b8(xadv, {xpos}), (8)\nwhere xadv represents the adversarial perturbation of the input sample x in the embedding space, and \u03b3 denotes a regularization parameter. The first part of the total contrastive loss (eq. 8) aims to optimize the similarity between the input sample x, its positive pair, and its adversarial perturbation, while the second part serves to regularize the loss by encouraging the convergence of the adversarial perturbation and the positive pair of x."
        },
        {
            "heading": "5 Evaluation and Experimental Results",
            "text": "This section presents a comprehensive set of experiments aimed at validating the effectiveness of our proposed framework in terms of generalization and robustness metrics. In the first two series of experiments, we investigate the performance of our framework on seven semantic textual similarity (STS) tasks and six transfer tasks within the SentEval framework1 to assess the generalization capability of our framework in generating efficient sentence embeddings. In the final series of experiments, we measure the resilience of the embeddings against five state-of-the-art adversarial attacks to assess the robustness capability of our framework in generating robust text representation. Appendices A and B provide training details and ablation studies that illustrate the effects of hyperparameter tuning."
        },
        {
            "heading": "5.1 Semantic Textual Similarity (STS) Tasks",
            "text": "We evaluate our framework on a set of seven semantic textual similarity (STS) tasks, which include STS 2012\u20132016 (Agirre et al., 2012, 2013, 2014, 2015, 2016), STS Benchmark (Cer et al., 2017), and SICK-Relatedness (Marelli et al., 2014). In our experiments, we solely utilize fixed sentence embeddings without any training datasets or regressors. To benchmark our framework\u2019s performance, we compare it against various unsupervised sentence embedding approaches, including: 1) baseline methods such as GloVe (Pennington et al., 2014) and average BERT or RoBERTa embeddings; 2) post-processing methods like BERTflow (Li et al., 2020a) and BERT-whitening (Su et al., 2021); and 3) state-of-the-art methods such as SimCSE (Gao et al., 2021), ConSERT (Yan et al., 2021), USCAL (Miao et al., 2021), and ATCL (Rima et al., 2022). We validate the findings of the SimCSE, ConSERT, and USCAL frameworks\n1https://github.com/facebookresearch/SentEval\nby reproducing their results using our own configuration for BERT and RoBERTa encoders. The results presented in Table 1 demonstrate the superior performance of our RobustEmbed framework compared to various sentence embedding methods across most of the semantic textual similarity tasks. Our framework achieves the highest averaged Spearman\u2019s correlation among state-of-the-art approaches. Specifically, when using the BERT encoder, our framework outperforms the second-best embedding method, USCAL, by a margin of 1.20%. Additionally, RobustEmbed achieves the highest score in the majority of individual STS tasks (6 out of 7) compared to other embedding methods and performs comparably to the SimCSE method on the STS16 task. For the RoBERTa encoder, both the base version and the large version, RobustEmbed outperforms the state-of-the-art embeddings in five out of seven STS tasks and achieves the highest averaged Spearman\u2019s correlation."
        },
        {
            "heading": "5.2 Transfer Tasks",
            "text": "This experiment leverages transfer tasks to evaluate the performance of our framework, RobustEmbed, on diverse text classification tasks, including sentiment analysis and paraphrase identification. Our assessment encompasses six transfer tasks: CR (Hu and Liu, 2004), SUBJ (Pang and Lee, 2004), MPQA (Wiebe et al., 2005), SST2 (Socher et al., 2013), and MRPC (Dolan and Brockett, 2005),\nwith detailed information provided in Appendix E. We adhere to the standard methodology described in Conneau and Kiela (2018) and train a logistic regression classifier on top of the fixed sentence embeddings for our experimental procedure. We replicated the SimCSE, ConSERT, and USCAL frameworks using our configuration for both BERT and RoBERTa encoders. The results presented in Table 2 indicate that our framework demonstrates superior performance in terms of average accuracy when compared to other sentence embedding methods. Specifically, when utilizing the BERT encoder, our framework outperforms the second-best embedding method by a margin of 0.40%. Moreover, RobustEmbed achieves the highest score in four out of six text classification tasks. The similar interpretation of the BERT encoder are also maintained for the RoBERTa encoder, including both the base version and the large version."
        },
        {
            "heading": "5.3 Adversarial Attacks",
            "text": "In this section, we evaluate the robustness of our sentence embedding framework against various adversarial attacks, comparing it with two stateof-the-art sentence embedding models: SimSCE (Gao et al., 2021) and USCAL (Miao et al., 2021). Our evaluation involves fine-tuning a BERT-based PLM using different embedding approaches on seven text classification and natural language inference tasks, namely MRPC (Dolan and Brockett,\n2005), YELP (Zhang et al., 2015), IMDb (Maas et al., 2011), Movie Reviews (MR) (Pang and Lee, 2005a), SST2 (Socher et al., 2013), Standford NLI (SNLI) (Bowman et al., 2015), and Multi-NLI (MNLI) (Williams et al., 2018). Detailed information regarding these tasks can be found in Appendix E. To assess the robustness of the fine-tuned models, we perform adversarial attacks using the TextAttack framework (Morris et al., 2020) to investigate the impact of five efficient adversarial attack techniques: TextBugger (Li et al., 2019), PWWS (Ren et al., 2019), TextFooler (Jin et al., 2020), BAE (Garg and Ramakrishnan, 2020), and\nBERTAttack (Li et al., 2020b). To acquire a more comprehensive insight into the functionality of these attacks, we provide more details in Appendix F. It should be noted that adaptive attacks cannot generate adversarial attacks using the main algorithm of our framework, as it operates exclusively in the embedding space while the input instances of sentence embeddings are raw text. To ensure statistical validity, each experiment was conducted five times, each time using 1000 adversarial attack samples; the reported results shown in this section are the average results of five iterations.\nTable 3 presents the attack success rates of five\nadversarial attack techniques on three sentence embeddings, including our framework. Our embedding framework consistently outperforms the other two embedding methods, demonstrating significantly lower attack success rates across all text classification and natural language inference tasks. Consequently, RobustEmbed achieves the lowest average attack success rate against all adversarial attack techniques. These findings validate the robustness of our embedding framework and highlight the vulnerabilities of the two state-of-the-art sentence embeddings to various adversarial attacks.\nFigure 2 depicts the average number of queries required and the resulting accuracy reduction for a set of 1000 attacks on two fine-tuned sentence embeddings. Green data points represent attacks on the RobustEmbed framework, while red points represent attacks on the USCAL approach (Miao et al., 2021). Connected pairs of points are associated with specific attack techniques. Ideally, a robust sentence embedding should be situated in the top-left region of the diagram, indicating that the attack technique necessitates a larger number of queries to deceive the target model while causing minimal performance degradation. The figure illustrates that, for each attack, RobustEmbed exhibits greater stability compared to the USCAL method. In other words, a larger number of queries is required for RobustEmbed, resulting in a lower accuracy reduction (i.e., better performance) compared to USCAL. This observation holds true for all applied adversarial attacks, indicating the robustness of our framework."
        },
        {
            "heading": "5.4 Robust Embeddings",
            "text": "We introduce a new task called Adversarial Semantic Textual Similarity (AdvSTS) to evaluate the resilience of sentence embeddings within our representation framework. AdvSTS uses an efficient adversarial approach, such as TextFooler, to manipulate a pair of input sentences in a way that encourages the target model to produce a regression score that deviates as much as possible from the true score (the ground truth label). Consequently, we create an adversarial STS dataset by converting all benign instances from the original dataset into adversarial examples. Similar to the STS task, AdvSTS employs Pearson\u2019s correlation metric to assess the correlation between the predicted similarity scores generated by the target model and the human-annotated similarity scores for the adversar-\nial dataset. Table 4 illustrates the attack success rates of five different adversarial attack techniques (namely TextFooler, TextBugger, PWWS, BAE, and BERTAttack) applied to three sentence embeddings, including our framework. These evaluations are carried out for two specific AdvSTS tasks, namely AdvSTS-B and AdvSICK-R. Notably, our embedding framework consistently outperforms the other two embedding methods, showing significantly lower attack success rates across both AdvSTS tasks and all employed adversarial attack techniques.\nIn conclusion, the extensive experiments conducted and the results presented in Tables 1, 2, 3, and 4, as well as Figure 2, provide strong evidence of the exceptional performance of RobustEmbed in various text representation and classification tasks, as well as its resilience against various adversarial attacks and tasks. These findings support the notion that our framework possesses remarkable generalization and robustness capabilities, underscoring its potential as an efficient and versatile approach for generating high-quality sentence embeddings."
        },
        {
            "heading": "5.5 Distribution of Sentence Embeddings",
            "text": "We followed the methodology proposed by Wang and Isola (2020) to employ two critical evaluation metrics, termed alignment and uniformity, to assess the quality of our representations. In the context of positive pairs represented by the distribution ppos, alignment calculates the anticipated distance\nbetween the embeddings of paired instances:\n\u2113align \u225c E (x,x+)\u223cppos\n\u2225f(x)\u2212 f(x+)\u22252. (9)\nUniformity quantifies how uniformly the embeddings are distributed within the representation space:\n\u2113uniform \u225c log E x,y i.i.d.\u223c pdata\ne\u22122\u2225f(x)\u2212f(y)\u2225 2 , (10)\nwhere pdata represents the data distribution. The underlying principle of these metrics is that positive instances should remain closely grouped, while embeddings for random instances should be spread across the hypersphere. Figure 3 illustrates the uniformity and alignment of various sentence embedding models, where lower values correspond to improved performance. In comparison to alternative representations, RobustEmbed achieves a similar level of uniformity (-2.293 vs. -2.305) but demonstrates superior alignment (0.058 vs. 0.073). This highlights the greater efficiency of our framework in optimizing the representation space in two distinct directions."
        },
        {
            "heading": "6 Conclusion and Future Work",
            "text": "In this paper, we proposed RobustEmbed, a selfsupervised sentence embedding framework that significantly enhances robustness against various adversarial attacks while achieving state-of-the-art\nperformance in a wide range of text representation and NLP tasks. Current sentence embeddings are vulnerable to adversarial attacks. RobustEmbed fills this gap by leveraging high-risk adversarial perturbations within a novel contrastive objective approach. We demonstrated the effectiveness of our framework through extensive experiments on semantic textual similarity and transfer learning tasks. Furthermore, Empirical findings substantiate the robustness of RobustEmbed against diverse adversarial attacks. As future work, we aim to explore the use of hard negative examples in the supervised setting to further enhance the efficiency of text representations.\nLimitations\nDespite the ingenuity of our methodology and its impressive performance, our framework does have some potential limitations:\n\u2022 Our framework is primarily designed and optimized for descriptive models, such as BERT, which excel in understanding and representing language, as well as related tasks like text classification. However, it may not be directly applicable to generative models like GPT, which prioritize generating coherent and contextually relevant text. Therefore, there may be limitations in applying our methodology to enhance the generalization and robustness characteristics of generative pre-trained models.\n\u2022 Our framework requires significant GPU resources for pre-training large-scale pre-trained models like RoBERTalarge. Due to limitations in GPU availability, we had to utilize smaller batch sizes during pre-training. While larger batch sizes (e.g., 256 or 512) generally lead to improved performance metrics, our experiments had to compromise and use smaller batch sizes to generate sentence embeddings efficiently given the GPU resource constraints."
        },
        {
            "heading": "A Training Details",
            "text": "In our experimental setup, we initialize our sentence encoder, denoted as f\u03b8, using the checkpoints obtained from BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019). For sentence embedding, RobustEmbed utilizes the representation of the [CLS] token as the starting point and incorporates a pooler layer on top of the [CLS] representations to facilitate contrastive learning objectives. The training process of RobustEmbed involves 2 epochs, with model evaluation conducted every 250 training steps. The best checkpoint, determined by the highest average STS (Semantic Textual Similarity) score, is selected for final evaluation. To train the model, we utilize a dataset consisting of 106 randomly sampled sentences from English Wikipedia, as provided by the SimCSE framework (Gao et al., 2021). The average training time for RobustEmbed is 2-4 hours. As our framework is initialized with pre-trained checkpoints, it exhibits robustness that is not sensitive to batch sizes, thus enabling us to employ batch sizes of either 64 or 128. In terms of transfer tasks, we determine the best hyperparameters based on the averaged score obtained from the development sets of six transfer tasks."
        },
        {
            "heading": "B Ablation Studies",
            "text": "In this section, we analyze the influence of four key hyperparameters in our approach on the overall performance. We utilize BERTbase as the encoder and evaluate the hyperparameters using the development set of STS tasks.\nB.1 Step Sizes in Perturbation Generation\nAs depicted in Algorithm 1, the RobustEmbed framework incorporates two step sizes, denoted as \u03b1 and \u03b2, to perform iterative updates during the PGD and FGSM perturbation generation processes, respectively. Figure 4 illustrates the collaborative effect of varying ranges for these two step sizes in generating high-risk perturbations, which is significant for achieving efficient contrastive learning objective. The results indicate greater improvement when \u03b2 is adjusted in a lower range while \u03b1 is placed in an upper range. Specifically, better performance is observed when \u03b1 and \u03b2 are assigned ranges of [1e-4, 1e-6] and [1e-2, 1e-4], respectively. Therefore, we utilize \u03b1 = 1e-5 and \u03b2 = 1e-3 for our experiments as it achieves the best results among the different arrangements.\nB.2 Step Numbers in Perturbation Generation\nRobustEmbed applies T-step FGSM and K-step PGD iterations to obtain high-risk adversarial perturbations for the contrastive learning objective. To simplify the analysis of perturbation generation iterations, we set K = T. Figure 5 demonstrates the impact of different step numbers (N = K or T) on effectiveness. We observe a gradual improvement as N increases from 1 to 9; however, beyond N=9, the improvement becomes negligible. Moreover, a higher N leads to longer running-time and unfair resource allocation. Hence, we select N=5 for our experiments.\nB.3 Norm Constraint\nTo ensure the imperceptibility of the generated adversarial examples, the magnitude of the perturbation vector, denoted as \u03b4, is controlled in RobustEmbed. Three commonly used norm functions, namely L1, L2, and L\u221e, are employed to restrict the magnitude of \u03b4 to small values. Table 5 presents the averaged Spearman\u2019s correlation of these norm functions across different Semantic Textual Similarity tasks. The L\u221e norm demonstrates superior correlation compared to the other two norms, thus it is selected as the norm function for our experimental evaluation.\nB.4 Modulation Factor\nRobustEmbed incorporates a modulation factor, denoted as 0 \u2264 \u03bb \u2264 1, to adjust the relative significance of each separate perturbation (PGD and FGSM) in the formation of the final perturbation. The performance efficiency of various values for this modulation factor on semantic textual similarity tasks is presented in Table 6. The results indicate that \u03bb = 0.5 achieves the highest averaged correlation among the tested magnitudes, indicat-\ning its effectiveness in generating more powerful perturbations. Therefore, we adopt this setting in the configuration of our framework."
        },
        {
            "heading": "C Adversarial Training Comparison",
            "text": "To compare our framework with other standard adversarial training methods, we fine-tuned our pre-trained model using a similar adversarial training approach as the one employed during the pretraining phase. Subsequently, we compared the fine-tuned model with three standard adversarial training methods, namely PGD, FreeLB (Zhu et al., 2020), and SMART (Jiang et al., 2020), after the fine-tuning step, and presented the experimental results in table 7. As shown, our framework outperforms the three other adversarial training methods, achieving the highest average accuracy for STS and transfer tasks and the lowest average attack success rate under TextFooler, TextBugger, and BERTAttack attacks."
        },
        {
            "heading": "D Contrastive Learning Loss",
            "text": "The first part of the total contrastive loss (Equation 8) optimizes the similarity between the input instance x and its positive pair (xpos), along with the similarity between x and its adversarial perturbation (xadv). Although it indirectly brings xpos and xadv closer, our observations show that regularizing the main objective function (Equation 7) through direct contrastive learning between xpos and xadv (the second part of Equation 8) helps us achieve improved clean accuracy and robustness. Table 8 illustrates the effect of different values of the regularization parameter (\u03b3) on the final performance of our framework. As can be seen, when \u03b3 = 1/128, the framework achieves the highest average accuracy for STS and transfer tasks and the lowest average attack success rate under the TextFooler attack. We employ \u03b3 = 1/128 for all other experiments."
        },
        {
            "heading": "E Text Classification Tasks",
            "text": "This section presents additional information on the text classification tasks used to assess the generalization and robustness capabilities of our framework in comparison to various sentence embedding methods. The MR (Movie Reviews) dataset (Pang and Lee, 2005b) consists of sentence-level samples with sentiment polarity, comprising 8,530 training and 1,066 testing highly polar instances. The CR dataset (Hu and Liu, 2004) is a customer review dataset collected in three steps: extracting products with customer comments, identifying opinion sentences, and labeling each sentence as positive or negative. The SUBJ dataset (Pang and Lee, 2004) contains 5,000 subjective and 5,000 objective sentences from movie reviews, labeled based on subjectivity status and polarity. The MPQA dataset (Wiebe et al., 2005) includes annotated documents from diverse news sources, categorizing opinion states such as beliefs, emotions, sentiments, and speculations. The SST2 dataset (Socher et al., 2013) is a sentence-level dataset with 8,544 training and 2,210 testing highly polar samples, extracted from movie reviews and classified as negative or positive. The MRPC dataset (Dolan and Brockett, 2005) contains 5,801 sentence pairs from news articles, labeled by human annotators to indicate semantic equivalence relationships. The YELP Polarity Review (YELP) dataset (Zhang et al., 2015) consists of document-level samples, with 560,000 training and 38,000 testing highly polar instances classified as negative (1- and 2-star) or positive (4- and 5-star) reviews. The Internet Movie Database\n(IMDb) Review dataset (Maas et al., 2011) contains 25,000 training and 25,000 testing highly polar samples, with negative and positive classes corresponding to review scores of \u22644 and \u22657 out of 10, respectively. Rotten Tomatoes Movie Reviews (MR) (Pang and Lee, 2005a) is a sentencelevel dataset consisting of 8,530 training and 1,066 testing highly polar samples, where negative and positive classes are assigned based on calibration among different critics. SNLI (Bowman et al., 2015) (MNLI (Williams et al., 2018)) is a threeclass dataset comprising 550,152 (392,702) training and 10,000 (19,643) testing human-written sentence pairs in English. Each set of three pairs in SNLI (MNLI) is created using a different image caption from the Flicker30K dataset (Young et al., 2014) (ten sources of text), with the premise sentence serving as the first sentence in each set. The hypothesis sentence of the first, second, and third pair is generated to be in entailment (category 1), contradiction (category 2), and neutral (category 3) with the respective premise sentence. While SNLI uses premise sentences from a relatively homogeneous image caption dataset, MNLI covers a broader range of text styles. The MNLI testing sample pairs are divided into two categories: \u201cMatched\u201d and \u201cMismatched,\u201d where MNLI-Matched pairs share similar context and resemblance to the training pairs compared to MNLI-Mismatched pairs."
        },
        {
            "heading": "F Adversarial Attack Methods",
            "text": "This section presents additional details on the diverse adversarial attack techniques employed to assess the robustness of our sentence embedding framework. The TextBugger method (Li et al., 2019) identifies important words using the Jacobian matrix of the target model and selects an optimal perturbation from five types of generated perturbations. The PWWS method (Ren et al., 2019) utilizes a synonym-swap technique based on a combination of word saliency scores and maximum word-swap effectiveness. TextFooler (Jin et al., 2020) identifies important words, gathers\nsynonyms, and replaces each important word with the most semantically similar and grammatically correct synonym. The BAE method (Garg and Ramakrishnan, 2020) employs four adversarial attack strategies involving word replacement or/and word insertion operations, where a portion of the text is masked and BERT MLM is used to generate substitutions. The BERTAttack method (Li et al., 2020b) consists of two steps: (a) searching for vulnerable words/sub-words and (b) using BERT MLM to generate semantic-preserving substitutes for the vulnerable tokens."
        }
    ],
    "title": "RobustEmbed: Robust Sentence Embeddings Using Self-Supervised Contrastive Pre-Training",
    "year": 2023
}