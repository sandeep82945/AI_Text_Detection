{
    "abstractText": "Can non-programmers annotate natural language utterances with complex programs that represent their meaning? We introduce APEL, a framework in which non-programmers select among candidate programs generated by a seed semantic parser (e.g., Codex). Since they cannot understand the candidate programs, we ask them to select indirectly by examining the programs\u2019 input-ouput examples. For each utterance, APEL actively searches for a simple input on which the candidate programs tend to produce different outputs. It then asks the nonprogrammers only to choose the appropriate output, thus allowing us to infer which program is correct and could be used to fine-tune the parser. As a case study, we recruited human non-programmers to use APEL to re-annotate SPIDER, a text-to-SQL dataset. Our approach achieved the same annotation accuracy as the original expert annotators (75%) and exposed many subtle errors in the original annotations.",
    "authors": [
        {
            "affiliations": [],
            "name": "Ruiqi Zhong"
        },
        {
            "affiliations": [],
            "name": "Charlie Snell"
        },
        {
            "affiliations": [],
            "name": "Dan Klein"
        },
        {
            "affiliations": [],
            "name": "Jason Eisner"
        }
    ],
    "id": "SP:5944c46fada7c1758df8db6505a006a1a46376ed",
    "references": [
        {
            "authors": [
                "Dario Amodei",
                "Chris Olah",
                "Jacob Steinhardt",
                "Paul Christiano",
                "John Schulman",
                "Dan Man\u00e9."
            ],
            "title": "Concrete problems in AI safety",
            "venue": "arXiv preprint arXiv:1606.06565.",
            "year": 2016
        },
        {
            "authors": [
                "Striplin",
                "Yu Su",
                "Zachary Tellman",
                "Sam Thomson",
                "Andrei Vorobev",
                "Izabela Witoszko",
                "Jason Wolfe",
                "Abby Wray",
                "Yuchen Zhang",
                "Alexander Zotov."
            ],
            "title": "Task-oriented dialogue as dataflow synthesis",
            "venue": "Transactions of the Association for Computational Linguis-",
            "year": 2020
        },
        {
            "authors": [
                "Amanda Askell",
                "Yuntao Bai",
                "Anna Chen",
                "Dawn Drain",
                "Deep Ganguli",
                "Tom Henighan",
                "Andy Jones",
                "Nicholas Joseph",
                "Ben Mann",
                "Nova DasSarma"
            ],
            "title": "A general language assistant as a laboratory",
            "year": 2021
        },
        {
            "authors": [
                "Yoram Bachrach",
                "Thore Graepel",
                "Thomas P. Minka",
                "Jo W. Guiver."
            ],
            "title": "How to grade a test without knowing the answers\u2014a Bayesian graphical model for adaptive crowdsourcing and aptitude testing",
            "venue": "ICML.",
            "year": 2012
        },
        {
            "authors": [
                "Yuntao Bai",
                "Saurav Kadavath",
                "Sandipan Kundu",
                "Amanda Askell",
                "Jackson Kernion",
                "Andy Jones",
                "Anna Chen",
                "Anna Goldie",
                "Azalia Mirhoseini",
                "Cameron McKinnon"
            ],
            "title": "Constitutional AI: Harmlessness from AI feedback",
            "year": 2022
        },
        {
            "authors": [
                "Christopher Baik",
                "Zhongjun Jin",
                "Michael J. Cafarella",
                "H.V. Jagadish."
            ],
            "title": "Constructing expressive relational queries with dual-specification synthesis",
            "venue": "CIDR.",
            "year": 2020
        },
        {
            "authors": [
                "Rohan Bavishi",
                "Caroline Lemieux",
                "Roy Fox",
                "Koushik Sen",
                "Ion Stoica."
            ],
            "title": "AutoPandas: Neuralbacked generators for program synthesis",
            "venue": "Proceedings of the ACM on Programming Languages, 3(OOPSLA):1\u201327.",
            "year": 2019
        },
        {
            "authors": [
                "Samuel R Bowman",
                "Jeeyoon Hyun",
                "Ethan Perez",
                "Edwin Chen",
                "Craig Pettit",
                "Scott Heiner",
                "Kamile Lukosuite",
                "Amanda Askell",
                "Andy Jones",
                "Anna Chen"
            ],
            "title": "Measuring progress on scalable oversight for large language models. arXiv preprint arXiv:2211.03540",
            "year": 2022
        },
        {
            "authors": [
                "Anthony Chen",
                "Pallavi Gudipati",
                "Shayne Longpre",
                "Xiao Ling",
                "Sameer Singh."
            ],
            "title": "Evaluating entity disambiguation and the role of popularity in retrievalbased NLP",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics",
            "year": 2021
        },
        {
            "authors": [
                "Bei Chen",
                "Fengji Zhang",
                "Anh Nguyen",
                "Daoguang Zan",
                "Zeqi Lin",
                "Jian-Guang Lou",
                "Weizhu Chen."
            ],
            "title": "Codet: Code generation with generated tests",
            "venue": "The Eleventh International Conference on Learning Representations.",
            "year": 2023
        },
        {
            "authors": [
                "Mark Chen",
                "Jerry Tworek",
                "Heewoo Jun",
                "Qiming Yuan",
                "Henrique Ponde de Oliveira Pinto",
                "Jared Kaplan",
                "Harri Edwards",
                "Yuri Burda",
                "Nicholas Joseph",
                "Greg Brockman"
            ],
            "title": "Evaluating large language models trained on code",
            "year": 2021
        },
        {
            "authors": [
                "Xinyun Chen",
                "Linyuan Gong",
                "Alvin Cheung",
                "Dawn Song."
            ],
            "title": "PlotCoder: Hierarchical decoding for synthesizing visualization code in programmatic context",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and",
            "year": 2021
        },
        {
            "authors": [
                "Xinyun Chen",
                "Chang Liu",
                "Dawn Song."
            ],
            "title": "Execution-guided neural program synthesis",
            "venue": "International Conference on Learning Representations.",
            "year": 2018
        },
        {
            "authors": [
                "Yuxin Chen",
                "Shervin Javdani",
                "Amin Karbasi",
                "J. Andrew Bagnell",
                "Siddhartha Srinivasa",
                "Andreas Krause."
            ],
            "title": "Submodular surrogates for value of information",
            "venue": "Proceedings of AAAI.",
            "year": 2015
        },
        {
            "authors": [
                "Shumo Chu",
                "Chenglong Wang",
                "Konstantin Weitz",
                "Alvin Cheung."
            ],
            "title": "Cosette: An automated prover for SQL",
            "venue": "CIDR.",
            "year": 2017
        },
        {
            "authors": [
                "A.P. Dempster",
                "N.M. Laird",
                "D.B. Rubin."
            ],
            "title": "Maximum likelihood from incomplete data via the EM algorithm",
            "venue": "Journal of the Royal Statistical Society, 39(1):1\u201321.",
            "year": 1977
        },
        {
            "authors": [
                "Liat Ein-Dor",
                "Alon Halfon",
                "Ariel Gera",
                "Eyal Shnarch",
                "Lena Dankin",
                "Leshem Choshen",
                "Marina Danilevsky",
                "Ranit Aharonov",
                "Yoav Katz",
                "Noam Slonim."
            ],
            "title": "Active Learning for BERT: An Empirical Study",
            "venue": "Proceedings of the 2020 Conference on Empirical",
            "year": 2020
        },
        {
            "authors": [
                "Ahmed Elgohary",
                "Saghar Hosseini",
                "Ahmed Hassan Awadallah."
            ],
            "title": "Speak to your parser: Interactive text-to-SQL with natural language feedback",
            "venue": "Annual Meeting of the Association for Computational Linguistics.",
            "year": 2020
        },
        {
            "authors": [
                "Deep Ganguli",
                "Danny Hernandez",
                "Liane Lovitt",
                "Amanda Askell",
                "Yuntao Bai",
                "Anna Chen",
                "Tom Conerly",
                "Nova Dassarma",
                "Dawn Drain",
                "Nelson Elhage"
            ],
            "title": "Predictability and surprise in large generative models",
            "year": 2022
        },
        {
            "authors": [
                "Sumit Gulwani."
            ],
            "title": "Automating string processing in spreadsheets using input-output examples",
            "venue": "ACM Sigplan Notices, 46(1):317\u2013330.",
            "year": 2011
        },
        {
            "authors": [
                "Patrick Haluptzok",
                "Matthew Bowers",
                "Adam Tauman Kalai."
            ],
            "title": "Language models can teach themselves to program better",
            "venue": "The Eleventh International Conference on Learning Representations.",
            "year": 2023
        },
        {
            "authors": [
                "Ari Holtzman",
                "Peter West",
                "Vered Shwartz",
                "Yejin Choi",
                "Luke Zettlemoyer."
            ],
            "title": "Surface form competition: Why the highest probability answer isn\u2019t always right",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Process-",
            "year": 2021
        },
        {
            "authors": [
                "Jared Kaplan",
                "Sam McCandlish",
                "Tom Henighan",
                "Tom B. Brown",
                "Benjamin Chess",
                "Rewon Child",
                "Scott Gray",
                "Alec Radford",
                "Jeffrey Wu",
                "Dario Amodei."
            ],
            "title": "Scaling laws for neural language models",
            "venue": "arXiv preprint arXiv:2001.08361.",
            "year": 2020
        },
        {
            "authors": [
                "Siddharth Karamcheti",
                "Ranjay Krishna",
                "Li Fei-Fei",
                "Christopher Manning."
            ],
            "title": "Mind your outliers! investigating the negative impact of outliers on active learning for visual question answering",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for",
            "year": 2021
        },
        {
            "authors": [
                "Nate Kushman",
                "Regina Barzilay."
            ],
            "title": "Using semantic unification to generate regular expressions from natural language",
            "venue": "Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human",
            "year": 2013
        },
        {
            "authors": [
                "Shuvendu K Lahiri",
                "Aaditya Naik",
                "Georgios Sakkas",
                "Piali Choudhury",
                "Curtis von Veh",
                "Madanlal Musuvathi",
                "Jeevana Priya Inala",
                "Chenglong Wang",
                "Jianfeng Gao."
            ],
            "title": "Interactive code generation via test-driven user-intent formalization",
            "venue": "arXiv preprint",
            "year": 2022
        },
        {
            "authors": [
                "Stefan Larson",
                "Anthony Zheng",
                "Anish Mahendran",
                "Rishi Tekriwal",
                "Adrian Cheung",
                "Eric Guldan",
                "Kevin Leach",
                "Jonathan K. Kummerfeld."
            ],
            "title": "Iterative feature mining for constraint-based data collection to increase data diversity and model robustness",
            "venue": "In",
            "year": 2020
        },
        {
            "authors": [
                "Nada Lavrac",
                "Saso Dzeroski."
            ],
            "title": "Inductive logic programming",
            "venue": "WLP, pages 146\u2013160. Springer.",
            "year": 1994
        },
        {
            "authors": [
                "N. Littlestone",
                "M.K. Warmuth."
            ],
            "title": "The weighted majority algorithm",
            "venue": "Information and Computation, 108(2):212\u2013261.",
            "year": 1994
        },
        {
            "authors": [
                "Zhengjie Miao",
                "Sudeepa Roy",
                "Jun Yang."
            ],
            "title": "Explaining wrong queries using small examples",
            "venue": "Proceedings of the 2019 International Conference on Management of Data, pages 503\u2013520.",
            "year": 2019
        },
        {
            "authors": [
                "Barton P. Miller",
                "Louis Fredriksen",
                "Bryan So."
            ],
            "title": "An empirical study of the reliability of UNIX utilities",
            "venue": "Communications of the ACM, 33(12):32\u201344.",
            "year": 1990
        },
        {
            "authors": [
                "Lingbo Mo",
                "Ashley Lewis",
                "Huan Sun",
                "Michael White."
            ],
            "title": "Towards transparent interactive semantic parsing via step-by-step correction",
            "venue": "Findings of the Association for Computational Linguistics: ACL 2022, pages 322\u2013342, Dublin, Ireland. Association",
            "year": 2022
        },
        {
            "authors": [
                "OpenAI."
            ],
            "title": "GPT-4 technical report",
            "venue": "arXiv.",
            "year": 2023
        },
        {
            "authors": [
                "Long Ouyang",
                "Jeff Wu",
                "Xu Jiang",
                "Diogo Almeida",
                "Carroll L. Wainwright",
                "Pamela Mishkin",
                "Chong Zhang",
                "Sandhini Agarwal",
                "Katarina Slama",
                "Alex Ray"
            ],
            "title": "Training language models to follow instructions with human feedback",
            "year": 2022
        },
        {
            "authors": [
                "Panupong Pasupat",
                "Percy Liang."
            ],
            "title": "Inferring logical forms from denotations",
            "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 23\u201332, Berlin, Germany. Association for Computa-",
            "year": 2016
        },
        {
            "authors": [
                "Gabriel Poesia",
                "Alex Polozov",
                "Vu Le",
                "Ashish Tiwari",
                "Gustavo Soares",
                "Christopher Meek",
                "Sumit Gulwani."
            ],
            "title": "Synchromesh: Reliable code generation from pre-trained language models",
            "venue": "International Conference on Learning Representations.",
            "year": 2022
        },
        {
            "authors": [
                "Nitarshan Rajkumar",
                "Raymond Li",
                "Dzmitry Bahdanau."
            ],
            "title": "Evaluating the text-to-SQL capabilities of large language models",
            "venue": "arXiv preprint arXiv:2204.00498.",
            "year": 2022
        },
        {
            "authors": [
                "Timo Schick",
                "Jane Dwivedi-Yu",
                "Roberto Dess\u00ec",
                "Roberta Raileanu",
                "Maria Lomeli",
                "Luke Zettlemoyer",
                "Nicola Cancedda",
                "Thomas Scialom."
            ],
            "title": "Toolformer: Language models can teach themselves to use tools",
            "venue": "arXiv preprint arXiv:2302.04761.",
            "year": 2023
        },
        {
            "authors": [
                "Torsten Scholak",
                "Raymond Li",
                "Dzmitry Bahdanau",
                "Harm de Vries",
                "Chris Pal."
            ],
            "title": "DuoRAT: Towards simpler text-to-SQL models",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics:",
            "year": 2021
        },
        {
            "authors": [
                "Torsten Scholak",
                "Nathan Schucher",
                "Dzmitry Bahdanau."
            ],
            "title": "PICARD: Parsing incrementally for constrained auto-regressive decoding from language models",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2021
        },
        {
            "authors": [
                "Eric Schulz",
                "Maarten Speekenbrink",
                "Andreas Krause."
            ],
            "title": "A tutorial on Gaussian process regression: Modelling, exploring, and exploiting functions",
            "venue": "Journal of Mathematical Psychology, 85:1\u201316.",
            "year": 2018
        },
        {
            "authors": [
                "Max Sch\u00e4fer",
                "Sarah Nadi",
                "Aryaz Eghbali",
                "Frank Tip"
            ],
            "title": "Adaptive test generation using a large language model",
            "year": 2023
        },
        {
            "authors": [
                "der",
                "Stephon Striplin",
                "Yu Su",
                "Zachary Tellman",
                "Sam Thomson",
                "Andrei Vorobev",
                "Izabela Witoszko",
                "Jason Wolfe",
                "Abby Wray",
                "Yuchen Zhang",
                "Alexander Zotov"
            ],
            "title": "Task-oriented dialogue as dataflow synthesis",
            "year": 2020
        },
        {
            "authors": [
                "Kensen Shi",
                "David Bieber",
                "Rishabh Singh."
            ],
            "title": "TF-Coder: Program synthesis for tensor manipulations",
            "venue": "arXiv preprint arXiv:2003.09040.",
            "year": 2020
        },
        {
            "authors": [
                "Richard Shin",
                "Christopher Lin",
                "Sam Thomson",
                "Charles Chen",
                "Subhro Roy",
                "Emmanouil Antonios Platanios",
                "Adam Pauls",
                "Dan Klein",
                "Jason Eisner",
                "Benjamin Van Durme."
            ],
            "title": "Constrained language models yield few-shot semantic parsers",
            "venue": "Proceedings of",
            "year": 2021
        },
        {
            "authors": [
                "Immanuel Trummer."
            ],
            "title": "CodexDB: Synthesizing code for query processing from natural language instructions using GPT-3 Codex",
            "venue": "Proceedings of the VLDB Endowment, 15(11):2921\u20132928.",
            "year": 2022
        },
        {
            "authors": [
                "Bailin Wang",
                "Richard Shin",
                "Xiaodong Liu",
                "Oleksandr Polozov",
                "Matthew Richardson."
            ],
            "title": "RAT-SQL: Relation-aware schema encoding and linking for textto-SQL parsers",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Lin-",
            "year": 2020
        },
        {
            "authors": [
                "Chenglong Wang",
                "Yu Feng",
                "Rastislav Bodik",
                "Isil Dillig",
                "Alvin Cheung",
                "Amy J. Ko."
            ],
            "title": "Falx: Synthesis-powered visualization authoring",
            "venue": "Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems, pages 1\u201315.",
            "year": 2021
        },
        {
            "authors": [
                "Sida I. Wang",
                "Samuel Ginn",
                "Percy Liang",
                "Christopher D. Manning"
            ],
            "title": "Naturalizing a programming",
            "year": 2017
        },
        {
            "authors": [
                "Xuezhi Wang",
                "Jason Wei",
                "Dale Schuurmans",
                "Quoc V Le",
                "Ed H. Chi",
                "Sharan Narang",
                "Aakanksha Chowdhery",
                "Denny Zhou."
            ],
            "title": "Self-consistency improves chain of thought reasoning in language models",
            "venue": "The Eleventh International Conference on Learning",
            "year": 2023
        },
        {
            "authors": [
                "Yushi Wang",
                "Jonathan Berant",
                "Percy Liang."
            ],
            "title": "Building a semantic parser overnight",
            "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing",
            "year": 2015
        },
        {
            "authors": [
                "Jason Wei",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Maarten Bosma",
                "Fei Xia",
                "Ed Chi",
                "Quoc V Le",
                "Denny Zhou"
            ],
            "title": "Chain-of-thought prompting elicits reasoning in large language models",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Jacob Whitehill",
                "Ting-fan Wu",
                "Jacob Bergsma",
                "Javier Movellan",
                "Paul Ruvolo."
            ],
            "title": "Whose vote should count more: Optimal integration of labels from labelers of unknown expertise",
            "venue": "Advances in Neural Information Processing Systems, volume 22.",
            "year": 2009
        },
        {
            "authors": [
                "Jeff Wu",
                "Long Ouyang",
                "Daniel M. Ziegler",
                "Nisan Stiennon",
                "Ryan Lowe",
                "Jan Leike",
                "Paul Christiano."
            ],
            "title": "Recursively summarizing books with human feedback",
            "venue": "arXiv preprint arXiv:2109.10862.",
            "year": 2021
        },
        {
            "authors": [
                "Yan Yan",
                "Romer Rosales",
                "Glenn Fung",
                "Jennifer G. Dy."
            ],
            "title": "Active learning from crowds",
            "venue": "Proceedings of the 28th International Conference on International Conference on Machine Learning, ICML\u201911, page 1161\u20131168, Madison, WI, USA. Omnipress.",
            "year": 2011
        },
        {
            "authors": [
                "Ziyu Yao",
                "Yiqi Tang",
                "Wen-tau Yih",
                "Huan Sun",
                "Yu Su."
            ],
            "title": "An imitation game for learning semantic parsers from user interaction",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages",
            "year": 2020
        },
        {
            "authors": [
                "Xi Ye",
                "Qiaochu Chen",
                "Isil Dillig",
                "Greg Durrett."
            ],
            "title": "Benchmarking multimodal regex synthesis with complex structures",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6081\u20136094, Online. Association",
            "year": 2020
        },
        {
            "authors": [
                "Tao Yu",
                "Rui Zhang",
                "Kai Yang",
                "Michihiro Yasunaga",
                "Dongxu Wang",
                "Zifan Li",
                "James Ma",
                "Irene Li",
                "Qingning Yao",
                "Shanelle Roman",
                "Zilin Zhang",
                "Dragomir Radev"
            ],
            "title": "Spider: A large-scale human-labeled dataset for complex and cross-domain semantic pars",
            "year": 2018
        },
        {
            "authors": [
                "Luke Zettlemoyer",
                "Michael Collins."
            ],
            "title": "Online learning of relaxed CCG grammars for parsing to logical form",
            "venue": "Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language",
            "year": 2007
        },
        {
            "authors": [
                "Ruiqi Zhong",
                "Charlie Snell",
                "Dan Klein",
                "Jacob Steinhardt."
            ],
            "title": "Describing differences between text distributions with natural language",
            "venue": "International Conference on Machine Learning, pages 27099\u201327116. PMLR.",
            "year": 2022
        },
        {
            "authors": [
                "Ruiqi Zhong",
                "Tao Yu",
                "Dan Klein."
            ],
            "title": "Semantic evaluation for text-to-SQL with distilled test suites",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 396\u2013411, Online. Association for Computa-",
            "year": 2020
        },
        {
            "authors": [
                "Denny Zhou",
                "Nathanael Sch\u00e4rli",
                "Le Hou",
                "Jason Wei",
                "Nathan Scales",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Claire Cui",
                "Olivier Bousquet",
                "Quoc V Le",
                "Ed H. Chi."
            ],
            "title": "Least-to-most prompting enables complex reasoning in large language models",
            "venue": "The",
            "year": 2023
        },
        {
            "authors": [
                "Bachrach"
            ],
            "title": "Whitehill et al. (2009) model each individual annotator\u2019s capability and each question\u2019s difficulty, learning these parameters through agreement information",
            "year": 2011
        },
        {
            "authors": [
                "Zhong"
            ],
            "title": "2020) to generate large informative databases that conform to a given schema c. We draw upon the existing sample database with this schema (provided by the SPIDER dataset",
            "year": 2020
        },
        {
            "authors": [
                "Ye"
            ],
            "title": "2020), which aims to synthesize a regular expression from a natural language description and a few strings that should be accepted and rejected by the regex",
            "year": 2020
        },
        {
            "authors": [
                "Ye"
            ],
            "title": "Intuitively, this baseline tries to check the highest-ranking candidate program, by determining whether a string that it accepts should in fact be accepted. The response",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Semantic parsing often aims to map a natural language utterance to a program, which can be executed on an input (Kushman and Barzilay, 2013). For example, for the utterance \u201cHow old is the youngest person,\u201d we can map to the SQL program SELECT MIN(AGE) FROM PEOPLE, execute it on an input database, and return the output answer. Language models like Codex often achieve substantial few-shot performance (Chen et al., 2021a). However, user-facing applications often involve novel phrases or domain-specific values unseen during pre-training or generic instruction-tuning, so the model still needs more labels for training. Since hiring programming experts to label training data is costly, can we ask non-programmers to label them?\nWe introduce APEL, a framework that indirectly labels programs with non-programmer responses on carefully constructed input-output examples. APEL must be seeded with a moderately good semantic parser: we use Codex for this in our experi-\nNot Informative \ud83d\ude14\nHow old is the youngest person from section P?\nSELECT MIN(Age) from People WHERE Section = \u2018P\u2019\nSELECT MIN(Age) from People s1\ns2\n\u2776 Generate a prior over a candidate list of programs using Codex with few-shot learning\nu =\nindicates what the annotators can see : they only need to select which output (19 or 23) is correct by reading the question u and the database i. We downweight program candidates that are inconsistent with the annotators\u2019 response. Figure 3 illustrates our criteria for i.\nments, prompting it with a few demonstrations that we solicited from expert programmers. We now want to label additional training data. To construct the correct program for a new utterance, APEL uses the seed parser to generate a list of candidate programs with prior probabilities (Figure 1 top), and then asks non-programmer annotators to indicate which candidate is correct.\nHowever, we cannot ask the annotators to select the correct program directly, since even expert programmers might find this challenging (Figure 2). Instead, APEL obtains information indirectly by asking questions about how the program should be-\nhave. We synthesize a program input, execute the candidate programs on it to obtain a list of outputs, and ask the annotators which output is correct given the utterance and the input. We could eliminate the programs that returned other outputs, but annotators may make mistakes, so we instead downweight them via Bayes\u2019 Theorem (Figure 1 bottom). To pin down the correct program, we iteratively reduce the entropy of the distribution over candidates by repeating the process with more synthesized inputs.\nTo reduce annotators\u2019 workload, we propose an algorithm to search for a program input that maximizes the expected information gain of the annotator\u2019s answer (\u00a73), subject to the constraint that the input is simple enough for the annotator to reason about the correct output easily (Figure 3). Since our method resembles the programming by example (PBE) framework (Lavrac and Dzeroski, 1994) and learns an underlying program by actively choosing which examples should be labeled, we call our framework APEL\u2014Active Programming by Examples using a prior derived from a natural Language utterance. Non-Programmer Annotator Study. As a case study, we use APEL to label SQL programs. We built an annotation GUI based on APEL and recruited 11 non-programmers to label a random subset of 240 utterances from the SPIDER (Yu et al.,\nNot Informative \ud83d\ude14\n2018) development set (\u00a76). According to a new carefully constructed gold standard, we achieve the same accuracy as the original SPIDER annotation performed by database experts (75%) and also substantially outperforms the top-1 accuracy of Codex (59%). APEL also exposes subtle errors made by previous experts, which we analyze in \u00a76.6.\nAPEL is a preliminary step towards enabling non-programmers to label utterances with arbitrarily complex programs. Applying it to new semantic parsing tasks still faces some bottlenecks: it requires a seed semantic parser with reasonable top-k accuracy and an efficient procedure to find simple and informative program inputs (\u00a77). However, these bottlenecks might be alleviated by future advances in semantic parsing and program synthesis. With these advances, we hope APEL can facilitate non-programmers to label programs in a broader range of applications in the future.1"
        },
        {
            "heading": "2 Framework",
            "text": "APEL aims to enable humans to indirectly annotate natural language utterances with programs. Here we use text-to-SQL as a case study."
        },
        {
            "heading": "2.1 Outline of APEL",
            "text": "Let u be a given utterance and c a known database schema, which specifies the table names, column names, and the constraints on value types, uniqueness, and foreign keys. We want to synthesize a SQL program s that captures the meaning of u and works properly for any database with schema c.\nFigure 1 illustrates our pipeline. We first feed c and u to a seed semantic parser (e.g. Codex with\n1The corrected annotations and our code for searching for informative and simple databases are on our github: https: //github.com/ruiqi-zhong/EMNLP23-APEL .\na prompt) to generate a prior distribution p over a list of SQL programs s. We then 1) synthesize an input database i, 2) execute the list of programs on i to obtain a list of program outputs o, and 3) display u, i, and the outputs to an annotator a. The annotator generates a response r indicating which output they think is correct. We will then raise the posterior probabilities of the candidates s such that s(i) = r, i.e. s returns r when executing on i. Since we ask the annotator to select the correct output of a program given an input, we call our questions \u201co-selection questions.\u201d\nTo formalize, the posterior distribution over the program s, once we observe the response r, is\np(s | u, a, i, r) \u221d p(s | u, a, i) p(r | s, u, a, i) = p(s | u) p(r | a, s(i)) (1)\nwhere p(s | u) was the prior from the seed parser. Here p(r | a, s(i)) models annotator behavior: the probability that annotator a would have responded with r if s were a correct implementation of u and therefore s(i) were the correct output. To the extent that annotators are accurate, this Bayesian update increases the probability of the correct program.\nWe can ask more o-selection questions to obtain further information about the correct candidate and improve the posterior. If desired, different rounds may use different annotators. For each u, we define\npt(s) def = p(s | u, a1, i1, r1, . . . , at, it, rt) \u221d pt\u22121(s) p(rt | at, s(it)) (2)\nto be the posterior after t > 0 questions, with p0 def = p(s | u) being the prior. Letting T denote our total number of questions about u, our final posterior is pT . Evaluation. We output as our annotation the most probable SQL candidate s\u0302T according to pT , and compare it to a gold standard. To show that our framework is useful, s\u0302T needs to be correct more often than s\u03020, the most probable SQL candidate according to the seed semantic parser p(s | u) . Relation to Weakly Supervised Learning. Appendix A explains how the \u201csoft annotations\u201d provided by the full distribution pT could be used to retrain the semantic parser p(s | u). These improved models would in turn improve the estimate of pT (i.e., an EM algorithm)."
        },
        {
            "heading": "2.2 Criteria for Synthesized Inputs",
            "text": "To generate an o-selection question on round t, APEL needs to synthesize an input database it that\nis both informative and simple. Informative. Our belief as we enter round t is pt\u22121. Once we observe the annotator\u2019s response rt, we will be able to update it to pt. This will achieve an information gain of H(pt\u22121) \u2212 H(pt), where the Shannon entropy H of a distribution over programs s characterizes its remaining uncertainty about which program is correct.\nHowever, pt will depend not only on our choice of question it but also on the annotator\u2019s response rt (equation (2)). We do not know rt yet, but our current belief is that it will be distributed as\npt\u22121(rt) = \u2211 s pt\u22121(s) p(rt | at, s(it)) (3)\nSo our expected Information Gain from asking it is\nIGpt\u22121(it) def = H(pt\u22121)\u2212 Ert\u223cpt\u22121 [H(pt)] (4)\nwhere the expectation is taken under distribution (3). This is high if the candidates s that are most plausible under pt\u22121 tend to return different outputs s(it) and hence different annotator responses rt, making rt informative about s. By contrast, iA in Figure 3 left would yield an uninformative response (IG = 0). Simple. We would like to avoid presenting the annotator with complex inputs such as the large database iB in Figure 3 right. The correct response might be informative, but determining it would require too much human effort. We crudely model the effort required for it as the number of records |it|.\nThe next section proposes a heuristic to synthesize a simple informative database it given schema c, a sample database with schema c, and a distribution pt\u22121 over SQL programs."
        },
        {
            "heading": "3 Optimizing the Input Database",
            "text": "We attempt to maximize the expected information gain IG over all databases that conform to the given schema c and have at most R total records, where R = 30 in our case study. Formally, we search for\ni\u2217t = argmax it:|it|\u2264R IGpt\u22121(it). (5)\nWhen multiple databases have the same IG, we break ties by favoring the smallest database. Since t and p are fixed during the optimization process, we will write IG(i) instead of IGpt\u22121(it) for short.\nOur method can be summarized as \u201cfuzz-thendrop.\u201d Fuzzing (Miller et al., 1990) is an established practice in software testing, where an algorithm generates a large number of random program\ninputs to search for an input that satisfies a property or reveals a bug. In our case, we want to search for an input database that maximizes the information gain. Therefore, we first perform fuzzing by randomly generating a large number of large databases as in Zhong et al. (2020)\u2014see Appendix C for further details\u2014and keep the database i0 that maximizes the expected information gain IG(i0). We then drop records from i0 to satisfy the simplicity criterion for L iterations.\nFor each iteration \u2113 of dropping records, we randomly drop 5% of the records from i\u2113 to obtain i\u2113+1. If this results in a less informative database, i.e., IG(i\u2113+1) < IG(i\u2113), we are willing to retry up to 20 times in hopes of randomly finding an i\u2113+1 that is at least as informative as i\u2113; if we fail in all 20 tries, we keep the best of the 20 despite the decrease in IG. Of the databases smaller than R that we encountered during the L iterations, let i\u0302 be the one with the highest IG:\ni\u0302 def = argmax\ni\u2208{i\u2113:1\u2264\u2113\u2264L},|i|\u2264R IG(i) (6)\nSince our procedure is randomized, we repeat it 3 times, and let i\u2217 be the i\u0302 with the largest IG(\u0302i), breaking ties as before in favor of smaller i\u0302. Finally, we simplify i\u2217 by dropping tables and columns that were not mentioned by any SQL candidates in p.\nOur procedure of dropping records from a large informative database is heavily inspired by Miao et al. (2019), which, given a database i such that s1(i) \u0338= s2(i), provably finds the smallest subset of records in i such that programs s1 and s2 return different outputs. However, their algorithm works only for a restricted family of SQL programs and cannot be adapted to optimize information gain. Our procedure does not provide any provable optimality guarantee, but is more flexible and practical.\nIn practice, simply applying the above procedure can generate unnatural databases and lead to vacuous SQL execution, confusing the annotators. In Appendix C, we illustrate several typical confusions (Figure 6) and explain how we fix them."
        },
        {
            "heading": "4 Experimental Setup",
            "text": "We describe the dataset used to benchmark APEL (\u00a74.1), how we obtained the prior over SQL candidates (\u00a74.2), and our evaluation metrics (\u00a74.4)."
        },
        {
            "heading": "4.1 Dataset",
            "text": "We benchmarked APEL on the development set of SPIDER (Yu et al., 2018), an English text-to-\nSQL dataset with 1034 utterance-SQL pairs distributed under the CC BY-SA 4.0 License. The development set is divided into 20 domains, each with a distinct database schema c, a collection of utterance-SQL (u, s) pairs, and a sample database. We split the 1034 (u, s) pairs equally into a validation split and an evaluation split. We used the validation split to tune our annotator interface (\u00a76), develop our fuzz-then-drop algorithm (\u00a73), and prompt Codex (\u00a74.2) to create a seed parser. We used the evaluation split to evaluate APEL with simulated annotators (\u00a75), and from these drew a random subset of 240 utterances balanced across domains to conduct our human evaluation (\u00a76).\nTo make our evaluation more reliable, we 1) corrected the sample database to conform to the schema c and updated the test suite correspondingly (Appendix D), and 2) established a new gold standard by correcting errors in 61 out of these 240 SQL annotations (\u00a76.2). The corresponding author of SPIDER endorses our corrections (T. Yu, p.c.)."
        },
        {
            "heading": "4.2 Obtaining the SQL Program Prior p0",
            "text": "We generated a prior over SQL program candidates using the Codex (Chen et al., 2021b) language model with few-shot prompting. Given an utterance u with a schema c, we created the prompt (Appendix E.1) by concatenating a linearization of c with eight distinct (uk, sk) pairs from the validation split associated with the same schema c, and finally the utterance u itself. Some (uk, sk) pairs were chosen randomly while others were chosen because uk has the highest TF-IDF similarity to u. We randomly sampled 200 prompts for u by choosing different (uk, sk) pairs, and for each prompt, we asked Codex to generate 20 completions (SQL programs) and filtered out non-executable candidates. To prevent surface form competition (Holtzman et al., 2021), we then merged approximately semantically equivalent candidates by finding sets of candidates that return exactly the same outputs on 1K random databases, using the implementation from Zhong et al. (2020). We define p0 to be the empirical distribution in our sample of the 16 semantic equivalence classes that were most frequent in the sample. Thus, each s in \u00a72 represents an approximate equivalence class rather than a single program.\nTreating the original SPIDER annotation as the ground truth, the top-1 accuracy of p0 on the entire development set is 72% and the top-16 accuracy is 94%. These numbers are not comparable\nto prior works, which usually evaluate on unseen database domains in a zero-shot manner (harder than our setting) but do not predict string literals and DISTINCT keywords, which we need for execution. Appendix E.2 includes more details on top-k accuracy (sometimes also referred to as pass@k).\nNote that the seed parser could be further improved, e.g. with better models (OpenAI, 2023) or prompts (Zhou et al., 2023). However, these improvements are complementary to APEL, whose goal is to improve over the seed parser with nonprogrammer responses. We design our evaluation metrics based on this goal in the next section."
        },
        {
            "heading": "4.3 Separation of Annotators",
            "text": "When optimizing the next input i to show to a given annotator a (\u00a73), we measured its IG by conditioning on the previous responses from a only. That is, we treated each annotator as if they were the first annotator. (Indeed, all annotators for u saw the same first question.) This approach reduced the amount of information about u that our T total questions could extract, but it also reduced the variance of our experiment by running multiple independent (but shorter) annotation sessions on u. To compute the final pT for u, we still aggregated the responses from all annotators using the Bayesian formula (2)."
        },
        {
            "heading": "4.4 Evaluation Metrics",
            "text": "We report the following statistics to evaluate APEL: \u2022 Codex accuracy: how often is the argmax of p0 correct? This baseline reflects the top-1 accuracy of the seed semantic parser.\n\u2022 APEL accuracy: how often is s\u0302T (the argmax of pT ) correct? This reflects the improvement due to our APEL annotators. \u2022 candidate ceiling: how often does the support of p0 contain a correct program? This reflects how much APEL is bottlenecked by the seed semantic parser, whose top 16 candidates form our candidate list.\nIf APEL is effective, we should see that the APEL accuracy is higher than the Codex accuracy. Since SPIDER categorizes its utterances by difficulty (easy/medium/hard/extra), we report the accuracy for each difficulty level. Next we evaluate APEL with both simulated and human annotators."
        },
        {
            "heading": "5 Automated Simulation Evaluation",
            "text": "To automatically test the effectiveness of APEL without additional human annotations, we bench-\nmarked APEL on the entire evaluation split by simulating an oracle annotator who always responds correctly by choosing the output of the correct SQL program and assuming that the SQL annotation provided by SPIDER is always correct. Additionally, to evaluate our database generation algorithm in \u00a73, we compared to OrigDB, an ablated variant of APEL that directly uses the sample database from the original SPIDER dataset. Table 1 reports the candidate ceiling, the Codex accuracy, and the accuracy for both OrigDB and APEL.\nWith 3 rounds of interactions, APEL accuracy (91%) substantially improves on the Codex accuracy (72%), validating the effectiveness of our framework; we achieved this by interacting with our oracle annotator for 1.8 rounds on average. Compared to OrigDB (86%), which uses the sample databases from the original SPIDER dataset to interact for 1 round, APEL\u2019s database generation method leads to higher accuracy since it allows multiple rounds and optimizes for information gain. Furthermore, averaged across all utterances, the databases generated by APEL contained altogether 10 records across all rounds\u2014whereas the OrigDB databases contained on average 33,295 records and would be impractical to present to human annotators. These results highlight the need for the databases to be both simple and informative.2\nAppendix H provides further information on the database sizes, the number of interaction rounds, and the robustness of our database generation method under different hyper-parameter choices."
        },
        {
            "heading": "6 Human Evaluation",
            "text": "We evaluate the effectiveness of APEL with real human annotators. We built an interface designed\n2APEL achieves 81% accuracy with 1 round of interaction.\nto be user-friendly (\u00a76.1) and used it ourselves to establish gold annotations for 240 utterances (\u00a76.2). We then recruited 11 non-programmer subjects to annotate the same utterances (\u00a76.3), aggregated their responses by learning a model of annotator behavior (\u00a76.4), and benchmarked their performance with the newly established gold standard (\u00a76.5). We analyze errors by SPIDER expert annotators in \u00a76.6, qualitative feedback from our subjects in \u00a76.7, and errors made by our subjects in Appendix F."
        },
        {
            "heading": "6.1 Annotation Interface",
            "text": "Figure 4 shows an example o-selection question in our interface, which displays the utterance u on the top, the database i on the left, and up to M = 6 distinct outputs o on the right in random order, followed by a \u201cnone of the above\u201d option; generally, an output may be a string, a number, or a table. The annotator can also use an open-ended response field to optionally report that the question is ambiguous or confusing; we did not use these open-ended responses during data collection, but future work could potentially condition on them in equation (2) (along with the multiple-choice responses).\nTo reduce the annotators\u2019 cognitive load, when the mouse pointer is over a cell, our interface highlights that cell along with all other cells of i and of the outputs o that have the same value. Appendix J describes more features of our interface.\nWe asked each annotator up to 3 consecutive questions for each utterance u, stopping the interaction after 1 or 2 questions if some candidate s already has pt(s) > 0.9, or if our heuristic (\u00a73) fails to find an appropriate database i for the next question."
        },
        {
            "heading": "6.2 Gold Standard Annotation",
            "text": "To establish a clean gold standard, two of the authors annotated all 240 utterances using our own APEL system. Whenever our two responses to a APEL question were different, we reached a consensus through discussion.\nWe closely examined the o-selection questions where our consensus response did not match the output of the SQL program provided by SPIDER, and corrected SPIDER\u2019s program if we felt that our response was strictly better. To avoid biasing against the original annotations, we stuck to the original ones whenever there were ambiguities, and we double-checked each corrected annotation by additionally writing down reasons why we felt it was better. As mentioned in \u00a74.1, we ultimately corrected 61 out of the 240 SQL annotations. \u00a76.6 analyzes these corrections in greater detail."
        },
        {
            "heading": "6.3 Non-Programmer Annotation",
            "text": "Annotation Unit. We split the 240 utterances into 8 units. Each unit contained 30 utterances across 4\u20135 database domains and proved to take 1\u20132 hours to annotate with our interface. For the annotator behavior model p(r | a, s(i)) in equation (1), we assumed that every annotator would respond correctly with 0.7 probability and would otherwise select a response uniformly at random. Recruiting Non-Programmers. As annotators, we recruited 11 university students who 1) were not pursuing/had not received a Computer Science degree and 2) had no prior experience with SQL. Each annotator could annotate any number of units (from 1 to 8) as they wished, but had to annotate them fully. For each unit we rewarded them with $15 as a base payment and a $5 ($10) bonus if their response agreed with our corrected gold standard > 85% (95%) of the time. We received 20 units of annotation in total, and hence each utterance was examined by 2.5 annotators on average. We asked each of them 1.84 questions about the utterance and the databases that we presented contain only 8.05 records on average. We visualize the distribution of the number of questions for each utterance and the database sizes for each question in Figure 5. Participation Procedure. We asked each annotator to: 1) sign a consent form to participate in the study, 2) watch a 12-minute video tutorial that contains our annotation instructions and explains the basics of foreign and primary keys, and 3) complete the annotation task. The tutorial can be\nseen at https://youtu.be/-MlIcCQ21xs and an example unit of the annotation task can be tried at http://35.225.126.31:4200/v0104_4pm_8."
        },
        {
            "heading": "6.4 Learning an Annotator Behavior Model",
            "text": "After collecting the annotations, we used them to improve \u00a76.3\u2019s naive model of annotator behavior p(r | a, s(i)) by learning parameters \u03b1a for each annotator a and \u03b2d for each SPIDER domain d. For a given a and d, the chance that the annotator answers at random is no longer fixed at 0.3, but is modeled as \u03c3(\u03b1a + \u03b2d + b), where \u03c3 is the logistic function and b is a bias term. Larger \u03b1a and \u03b2d predict higher error rates.\nWe maximize the incomplete data log-likelihood\nLu = log p(r1, . . . rT | u, i1, . . . iT )\n= log \u2211 s p0(s) T\u220f t=1 p(rt | at, s(it)) (7)\nsummed over all utterances u. Since we explicitly model the annotator behavior, Lu is sensitive to the domain d of u and the annotator at who answered the question about it. Just as in other adaptive crowdsourcing research, we do not assume access to the gold value of s. Our behavior model will predict a lower annotation error rate for those who tend to agree with other annotators and with Codex.\nOverall, our annotators chose the correct option 72% of the time. To evaluate our unsupervised annotator model, we compared its predicted probability of a correct response on each utterance to the 0/1 indicator of whether the annotator did select the correct option. See Appendix I for a visualization. Our model achieves an AUC ROC score of 0.68 and MSE error of 0.185. For comparison, a simple supervised model that always predicted the true overall probability of 72% would achieve an MSE error of 0.20.\nAs Appendix A explains, one way to maximize equation (7) is to use an EM algorithm that alternates between imputing the unknown s for each utterance (using pT ) and re-estimating the annotator error model based on these imputed \u201cgold\u201d answers. Appendix A also discusses how the annotator error model could be enriched."
        },
        {
            "heading": "6.5 Results",
            "text": "We present the results in Table 2. APEL (75%) is effective, since it significantly outperforms Codex (59%) with p-value < 10\u22123 under a one-sided paired t-test. APEL leads to the highest improvement for utterances for medium or hard difficulty levels, implying that APEL is most effective when the utterances are hard enough so that there is still room for improvement, but not too hard to confuse the non-programmer annotators. Additionally, APEL with non-programmers is on par with previous database experts without the help of a seed parser. We next analyze the errors made by previous database experts."
        },
        {
            "heading": "6.6 Subtle Errors by Database Experts",
            "text": "APEL helped us identify annotation mistakes in the original SPIDER dataset (\u00a76.2). We categorize and present them below to demonstrate where APEL is most helpful. The corresponding author of SPIDER agrees with our analyses (T. Yu, p.c.). Interpreting Database Schema Properly. Suppose each row contains information about an orchestra, including the year the orchestra was founded and its associated recording company. For an utterance \u201cWhich recording company was founded the earliest?\u201d, the correct annotation under this schema should be \u201cNot enough information to tell\u201d. Neither SPIDER nor APEL currently supports this annotation option, though our human annotator reported this issue in their open-ended feedback. SPIDER annotates this utterance incorrectly as SELECT company from TABLE WHERE YEAR =\n(SELECT MIN(YEAR) from TABLE), which looks plausibly correct but actually finds the recording company of the earliest-founded orchestra. Handling All Allowed Values. The annotated SQL should behave appropriately on all plausible cell values. For example, when we are asked about the maximum value in a column that allows NULL cells, we prefer a SQL that skips any NULL cells and returns the maximum of the actual values. As another example, if the utterance is \u201cHow many countries have a republic government form?\u201d, the clause WHERE GOVERNMENT = \"Republic\" will ignore any countries with the government form \u201cFederal Republic\u201d; the correct annotation should be WHERE GOVERNMENT LIKE \"%Republic%\". INNER JOIN vs. LEFT JOIN. Suppose the utterance is \u201cList singer names and number of concerts for each singer.\u201d and the database contains a table of singers and a table with records (s, c) if singer s performed in concert c. The SPIDER annotation is incorrect because it uses INNER JOIN, which fails to return singers with count 0. Ties for Extremals. For the utterance \u201cWho is the youngest person?\u201d, the SPIDER annotation is SELECT NAME FROM PEOPLE ORDER BY AGE LIMIT 1. As APEL discovers, in case of ties, humans prefer a SQL that will return all of the people who have the smallest age, rather than just the first one.\nRemark. Since most of the text-to-SQL models had low performance 3 years ago, Yu et al. (2018) favored short and plausible SQL annotations to make learning easier. These annotation conventions were shared between training and test sets to form a coherent structured prediction task (internal validity). Now that structured prediction is working well enough that the predictions could be used in real-world settings, we should turn to assuring that the SQL annotations actually have the desired effects (external validity). APEL can help in establishing the new gold standard (\u00a76.2)."
        },
        {
            "heading": "6.7 Qualitative Feedback",
            "text": "We informally interviewed some of our subjects to obtain feedback about APEL. Here is some example feedback that indicates future room for improvement:\n\u2022 Boredom: Some subjects complained that the annotation process was boring and they would not want to do it a second time. Future work can design better UIs and interactions to make the annotation more engaging.\n\u2022 Difficulty: While most questions are straightforward, some require onerous computations (e.g., requires adding 170115 and 50456). Even though we set up a time limit for each question, these questions consumed a lot of mental energy. Future work could include an option for the users to skim through all the questions and solve the easier ones first. \u2022 Vagueness: Some utterances were inherently vague. Consider the utterance \u201cCount the number of friends Kyle has.\u201d \u2013 what to do when there are two students named Kyle? Without external affirmation that these questions were indeed vague, some subjects wasted too much time guessing how to interpret them. Future work could elicit confidence ratings, rather than choosing a discrete option."
        },
        {
            "heading": "7 Related Work and Future Directions",
            "text": "Our framework can potentially generalize from text-to-SQL to other semantic parsing tasks. The SQL program s can generalize to other types of executable semantic parses, the input database i can generalize to any well-typed input, and the database query result o = s(i) can generalize to the intended effect of u on i. Applying APEL to a new task would require 1) a seed semantic parser with high top-k accuracy, and 2) an algorithm to find a simple informative program input i. These problems are related to semantic parsing and programming by example, respectively. We now discuss how those two lines of research benefit from APEL and also how they could make APEL more powerful in the future. Semantic Parsing. Semantic parsers have improved significantly over the past decades (Zettlemoyer and Collins, 2007; Scholak et al., 2021a). Recent pretrained models can perform the task without task-specific architectures (Scholak et al., 2021b) or even in a zero/few-shot manner (Shin et al., 2021; Rajkumar et al., 2022).\nHowever, collecting semantic parsing datasets is still challenging since it requires experts. Wang et al. (2015) addresses this by synthetically generating logical forms, using templates to explain them in natural language, and asking crowdworkers to paraphrase them. Still, the paraphrases are usually restricted in linguistic diversity (Larson et al., 2020). Another line of research (Yao et al., 2020; Elgohary et al., 2020; Mo et al., 2022) tries to learn from user interaction based on the surface form\ninformation, e.g., whether a program refers to specific fields. However, their assumption that a nonexpert annotator can recognize the true program s via paraphrases is unrealistic when programs have complex semantics. In contrast, APEL allows nonprogrammers to indirectly label text-to-SQL data via input-output examples.\nApplying APEL to other programming languages requires a seed semantic parser with high top-k accuracy. Such a requirement is more likely to be fulfilled in the future, at least for programming languages that are well-represented in language model training data, as language model capability is expected to improve (Ganguli et al., 2022). Additionally, the seed parser can be improved with better prompting techniques (Trummer, 2022; Wei et al., 2022; Wang et al., 2023; Zhou et al., 2023), which are complementary to our contribution and would make APEL stronger.\nProgramming by Example. PBE has been applied to synthesize regular expressions (Gulwani, 2011), tensor manipulation (Shi et al., 2020), data analysis (Bavishi et al., 2019), and visualization (Wang et al., 2021) programs, etc. Some other recent works such as Ye et al. (2020); Baik et al. (2020) also try to combine semantic parsing with PBE. However, both of them require the users to provide the input-output examples, which can be time-consuming to write.\nTo reduce the users\u2019 workload, we provide the input part of the input-output examples, and focus on only those inputs whose outputs will actually help identify the desired concept. This is a case of active learning, which is broadly used in other applications such as learning real-valued functions, sequence classification, and visual question answering (Schulz et al., 2018; Ein-Dor et al., 2020; Karamcheti et al., 2021). In APEL, for each utterance, we maintain a prior over the function (program) space and learn the desired function by querying it on a sequence of carefully chosen inputs. Similar to our work, Pasupat and Liang (2016) asked non-programmers o-selection questions by synthesizing table inputs, but they did not optimize for question simplicity and focused on a simpler single-table setting. Concurrent to our work, Lahiri et al. (2022) applied a similar framework to label Python functions; unlike them, we validated APEL with human annotators.\nTo reduce the effort required from humans, future work can also use large language models to\nevaluate the program outputs (Chen et al., 2023). Prompting large language models to evaluate their own outputs is a widespread idea. Schick et al. (2023) rerank ad hoc programs generated by a language model based on whether their outputs increase the probability of observed text. Bai et al. (2022) automatically critiques model-generated outputs and refines them for future training.\nTo apply APEL to other programming languages, we need efficient methods to synthesize simple and informative program inputs. Such methods already exist for less expressive languages such as regular expressions or restricted SQL (Miao et al., 2019). Appendix M describes an additional case study where we used APEL (with simulated annotators) to label utterances with regular expressions. To extend APEL to more complicated programs, we can potentially use language models to generate test inputs (Sch\u00e4fer et al., 2023) and train them to search for more informative inputs with selfsupervision (Haluptzok et al., 2023).\nScalable Oversight. As AI systems become more capable of generating candidate responses, an emerging line of research supervises AI systems by providing preferences over AI-generated candidates rather than providing human demonstrations (Askell et al., 2021; Ouyang et al., 2022). Therefore, to supervise AI to perform more complex tasks, it becomes increasingly important to determine human preferences over model outputs that are expensive to verify (Amodei et al., 2016; Bowman et al., 2022), such as full-book summaries or natural language descriptions of distributional properties (Wu et al., 2021; Zhong et al., 2022). Our work presents a strategy to re-weight complex outputs from an AI system (namely, programs) by asking simple informative questions of annotators who do not have to understand the outputs directly."
        },
        {
            "heading": "8 Conclusion",
            "text": "We proposed APEL, enabling non-programmers to indirectly label SQL programs via input-output examples. With advances in semantic parsing and PBE, future work could potentially extend APEL to other applications. We hope non-programmers can label more complex programs in the future, hence decreasing the costs of supervising increasingly capable AI systems that can take complicated actions by generating and executing code."
        },
        {
            "heading": "Acknowledgements",
            "text": "The first author is funded by NSF-Simons Theorinet Grant (NSF Award #2031985). Our human interaction study was approved by UC Berkeley\u2019s Institutional Review Board, and our survey and interface did not collect any personal identifiable information. We thank members of the Berkeley NLP group and Jacob Steinhardt\u2019s group, and the anonymous reviewers for their feedback on our paper draft.\nLimitations\nAPEL is only a preliminary step towards allowing non-programmers to label utterances with programs. We are not close to enabling expert-level labeling for arbitrary programming languages. We only experimented with English utterances, SQL programs, and university students; generalizing this to more natural languages, programming languages, and annotator populations requires more future work.\nOur current implementation is limited to textto-SQL and regular expressions. As mentioned at the end of the last section, applying APEL to other semantic parsing tasks requires effective algorithms to find simple but informative program inputs and a strong seed semantic parser. While we believe that these assumptions are likely to hold in the future, they might not hold currently. We also assumed that we have access to a pool of unlabeled utterances to begin with, while in practice we might not have access to utterances from real users. More future directions are discussed in Appendices B and I.\nAs APEL only considers the function (i.e., inputoutput mapping) computed by a program, it is only able to annotate an utterance with a semantic equivalence class of correct programs. Other factors such as efficiency or readability might be needed to choose among the programs in that class.\nEven though we have shown that APEL has higher accuracy than the seed parser, we have not empirically validated that the seed parser is improved by fine-tuning on the programs selected by APEL, nor did we study whether such improvements are in fact cheaper to attain with nonprogrammers than with expert annotators.\nFinally, no semantic parser should be used to synthesize SQL queries or other semantic forms for high-stakes scenarios without a careful analysis of errors and the downstream harms that they might cause."
        },
        {
            "heading": "A Training on APEL Annotations",
            "text": "As APEL is an annotation method, its ultimate goal is to convert unlabeled utterances u into \u201csupervised\u201d training examples (u, s) for a semantic parser. In this appendix, we discuss how these training examples would be constructed and used.\n(In the main paper, we used \u201cexample\u201d to refer to examples of the form (i, o), used for programming by example (\u00a72). In this appendix, however, we use \u201cexample\u201d to refer to examples of the form (u, s), used as training examples for a semantic parser.)\nTraining on MAP Annotations For each utterance u, APEL solicits indirect annotations and produces a posterior probability distribution pT over programs. The maximum a posteriori (MAP) program is defined by s\u0302 def= argmaxs pT (s).\nMost simply, we could train a semantic parser on the inferred annotations (u, s\u0302). Specifically, one option is to use these annotations to fine-tune the seed parser p0. However, this may not be practical for a very large pretrained model such as Codex, in which case the annotations could be used to train a dedicated semantic parser from scratch.\nDownstream Evaluation In this paper, we evaluated APEL directly by asking whether its inferred annotations (u, s\u0302) are accurate. We measured this by semantic equivalence Js\u0302K = Js\u2217K (see \u00a74.2) on a small test set of pairs (u, s\u2217) that were annotated by experts (using APEL). One could instead evaluate the actual impact of the inferred annotations on parsing performance, by training a semantic parser (e.g., fine-tuning the seed parser) on a large training set of APEL-labeled utterances, and evaluating that parser on the test set.\nBeyond MAP Rather than evaluating only the MAP annotation s\u0302, we could have evaluated APEL\u2019s entire distribution pT using the log-loss, \u2212 log pT (s\u2217 | u), averaged over the test set of (u, s\u2217) pairs. This is a more sensitive evaluation, though harder to interpret than exact-match accuracy.\nBut do we care whether the distribution pT is accurate beyond just its MAP annotation? Yes: one could train a parser p by maximizing its expected log-likelihood\nEs\u223cpT [log p(s | u)] (8)\nsummed over the APEL-annotated utterances u. In effect, this treats pT as giving a set of weighted \u201csoft annotations\u201d for u, not just the MAP annotation. It is equivalent to minimizing the KullbackLeibler divergence KL(pT || p) (summed over u).\nIterative Retraining Once an improved parser has been trained on the inferred annotations\u2014 either the MAP annotations or the soft annotations\u2014 it can be used as an improved prior p0 in the update rule (1). This can inform APEL in the selection of future questions.\nFurthermore, although APEL\u2019s past questions can no longer be changed, we can now reinterpret the annotators\u2019 responses to those questions, by using equation (1) to recompute an improved posterior pT from the improved prior p0. The improved soft annotations from these posteriors can be used to retrain the parser again. This procedure can be iterated to convergence to improve the parser.\nThis iterated procedure is a principled training method. If soft annotations are used for retraining via equation (8) at each iterations, then it is an instance of the Expectation-Maximization (EM) algorithm (Dempster et al., 1977). (If MAP labels are used, it is an instance of the hard or \u201cViterbi\u201d approximation to EM.) EM is guaranteed to converge to a new semantic parsing model p0 that locally maximizes the conditional log-likelihood (7) of all of the observed data\u2014in our case, the annotators\u2019 responses rt for all o-selection questions on all utterances u. Thus, it is simply a log-likelihood training method where the observed data are not direct annotations s, but indirect annotations rt. It marginalizes over the latent programs s.\nRicher Model of Annotator Behavior The annotator model p(r | a, s(i)) can be jointly retrained with the semantic parser at each iteration of EM, to obtain a higher log-likelihood (7). Improving the annotator model in this way should result in more accurate distributions pT at the next iteration of EM, and thus a better semantic parser.\nSpecifically, when we retrain the parser via equation (8), we can also retrain the annotator model to maximize the expected log-likelihood\nEs\u223cpT [ T\u2211 t=1 log p(rt | at, s(it))] (9)\nsummed over the APEL-annotated utterances u. This EM procedure will converge to a maximum of the incomplete-data log-likelihood as in \u00a76.4.\nWe could fit a richer model of annotator behavior than the relatively simple one in \u00a76.4. For example, we could estimate the error rates of individual annotators on different types of questions and program inputs. Intuitively, equation (9) means that we will tend to judge annotators as correct on examples where they agree with the consensus of pT and the other annotators.\nAlthough equation (1) assumed that p(r | s, u, a, i) = p(r | a, s(i)), a rich model could drop this assumption.3 For example, the model might allow that the annotator is more likely to make an error when u is difficult or i is complex. As an annotator\u2019s errors on a given utterance may be influenced by how they answered previous questions, a rich model could even take the form p(rt | s, u, a, i1, r1, . . . , it\u22121, rt\u22121, it), where i1, r1, . . . , it\u22121, rt\u22121 are the previous rounds of interaction with annotator a.\nEvaluating with Task Loss In our discussion of evaluation thus far, we have not given the semantic parser any partial credit. That is, given an utterance u, we have considered any answer other than the correct program s to be equally wrong.\nHowever, more generally, it may be tolerable to find a program whose outputs are correct\u2014or at least close to correct\u2014for most inputs. Let loss(o\u0302 | u, i, o\u2217) denote the task-specific loss of predicting output o\u0302 on input i when the correct output is o\u2217. The loss of predicting program s\u0302 when the correct program is s\u2217 is then\nEi[loss(s\u0302(i) | u, i, s\u2217(i))] (10)\nwhere the expectation Ei is taken over some realistic distribution of inputs for utterance u. This loss function can be used in supervised evaluation.\nDecoding with Task Loss Following standard Bayesian decision-theoretic methods, the semantic parser itself can aim to achieve low loss. No change to the training procedure is needed. Suppose we have trained a semantic parsing model p(s | u) by EM as described above. We now need to extract decisions from this model. If the semantic parser is required to translate u into a single program s\u0302 that will be used for all inputs, then the best program to choose is the program that minimizes the Bayes\n3This means using p(rt | s, u, at, it) in place of p(rt | at, s(it)) in equations (2), (3), (7) and (9).\nrisk\nRp(s\u0302 | u) def = Es\u223cp[Ei[loss(s\u0302(i) | u, i, s(i))]]\n(11)\nThis s\u0302 is not necessarily the MAP program. Once s\u0302 is predicted, the output on any given input i is o\u0302 = s\u0302(i).\nIn some applications, however, it may not be necessary to choose a single program to use for all inputs. Then the best output o\u0302 to return for input i is the output that minimizes the Bayes risk\nRp(o\u0302 | u, i) def = Es\u223cp(\u00b7|u)[loss(o\u0302 | u, i, s(i))] (12)\nIn other words, o\u0302 is now predicted from i by a consensus of multiple programs."
        },
        {
            "heading": "B Extensions to APEL Annotation",
            "text": "The following natural extensions could be explored in future work.\nSelecting Questions with Task Loss A model of task loss as in Appendix A can be used to improve the selection of the next question it. Instead of maximizing the expected reduction in entropy (equation (4)), we can maximize the expected reduction in the Bayes risk (11).\nWhen p is any distribution over programs s for utterance u, define the minimum Bayes risk by\nMBR(p) def = min\ns\u0302 Rp(s\u0302 | u) (13)\nAt the start of round t, we can already achieve a Bayes risk of MBR(pt\u22121). Once we have the annotator\u2019s response rt to question it, we will be able to update pt\u22121 to pt and achieve a Bayes risk of MBR(pt), where pt depends on it and rt via the update (2). Thus, the value of information from asking question it is\nMBR(pt\u22121)\u2212 Ert\u223cpt\u22121 [MBR(pt)] (14)\nThis is essentially the same as the information gain (4), but with task loss replacing log-loss. It is again guaranteed to be \u2265 0.\nRicher Model of Annotator Effort Our objective (5) tries to present the annotator at with a \u201csimple\u201d input database it, as measured by the number of records |it|. However, |it| is only a crude measure of the time or effort that the annotator must expend to answer the multiple-choice question derived from it. For example, annotators typically\nfind it more difficult to perform arithmetic or to reason across multiple tables within it.\nTo predict the annotator\u2019s effort on the input database it, we could attempt to simulate the annotator\u2019s mental execution of the true program s on it.4 As we do not know the true s, we would need to take the expectation of this effort over s \u223c pt\u22121. Mental operations such as adding 3-digit numbers or visually scanning the tables for a specific string could be then given appropriately high costs in the effort model.\nIn addition, perhaps a question that generates more multiple-choice options requires more effort. To capture this, the effort model could assign a cost not only to computing s(it) but also to finding that answer in the list of options. More generally, the model could attempt to capture mental strategies that do not fully compute s(it) but only do enough work to eliminate most of the options.\nFinally, a question generally requires less human cognitive effort if it is related to a preceding question. Querying input database i with utterance u is easier for an annotator who has just queried the same i with a different u\u2032, or queried a different i\u2032 with the same u. How would we train the parameters of the effort model? We observe how quickly the annotator answered each it with rt. Taking this time as a proxy for effort, we can train the effort model to predict it from the true program s (which the annotator is presumed to know) and it. If we do not know s, we can impute it from the observed responses\u2014that is, evaluate the effort model\u2019s log-likelihood in expectation under s \u223c pT , analogously to equations (8) and (9). In short, the effort model could be trained by EM, jointly with the other models.\nChoosing Who Should Annotate What The selection objective (5) manages the tradeoff between annotator effort and information gain by imposing a hard constraint on the annotator effort per o-selection question. We could improve this crude selection objective\u2014especially given good models of annotator behavior and annotator effort\u2014by selecting a question i that achieves high \u201cbang for the buck.\u201d This score is given by the ratio of value of information (equation (4) or (14)) to the expected effort of acquiring that information.\nBoth the numerator and denominator of this ratio depend on the specific annotator a who is an-\n4The same simulation could be also used to help model errors in the annotator\u2019s response rt (Appendix A).\nswering i, if the models have annotator-specific parameters. They also depend on the utterance u. Thus, the score is actually a property of the triple (u, i, a).\nAt each step of APEL annotation, we would select a high-scoring triple\u2014one in which we expect annotator a to usefully evaluate utterance u\u2019s desired behavior on input database i with low effort. The annotator\u2019s response r then influences the scores of other triples involving u. We would stop annotation when no sufficiently high-scoring triple could be found.\nIn the same way, Bachrach et al. (2012) and Whitehill et al. (2009) model each individual annotator\u2019s capability and each question\u2019s difficulty, learning these parameters through agreement information. Yan et al. (2011) explore these ideas for active learning, similarly routing useful questions to competent annotators. Our experiment empirically finds that each annotator\u2019s ability to answer questions and the difficulty of each domain varies wildly (Figure 10); therefore, future work is likely to benefit from actively selecting who to annotate which utterances.\nNon-Myopic Selection Policy The procedure described just above will switch freely among utterances and inputs, if it always selects the highestscoring triple. However, recall that if a followup question on the same utterance or input is eventually to be asked at all, asking it immediately will incur less effort from the annotator. Thus, a good heuristic is to prioritize followup questions over non-followup questions, provided that they score highly enough that it is likely that they will eventually be asked.\nMore generally: Greedily choosing the highestscoring question at each step is common in interactive protocols for information acquisition, such as adaptive quadrature or Bayesian optimization (Schulz et al., 2018). However, this greedy procedure is in general suboptimal. One could do better at selecting the next question by planning ahead to subsequent questions (Chen et al., 2015), though at a higher computational cost.\nAgain, we leave all these refinements to future work."
        },
        {
            "heading": "C Other Synthesis Constraints",
            "text": "This appendix gives further details about our method of database construction (\u00a73). Overall, we\nfollow the recipe of Zhong et al. (2020) to generate large informative databases that conform to a given schema c. We draw upon the existing sample database with this schema (provided by the SPIDER dataset in our experiments) to obtain plausible cell values. Following Zhong et al., we first synthesize cell values for all the \u201cparent\u201d columns (i.e., the columns that are being referenced by a child column in a foreign key relation), and then populate child columns with random elements from the parent columns.\nNaturalness of i As shown in Figure 6a, unrestricted random cell values can confuse annotators who are unfamiliar with databases. Therefore, rather than synthesizing completely random values, we now always copy individual cell values from the sample databases (\u00a74.1), optionally with minor perturbations such as \u00b11 for integer values.\nA database record might also be confusing even if its individual cell values are not. For example, the annotator can be confused by counterfactual information where the U.S. is in Asia as shown in Figure (b). Therefore, we prefer to initialize i0 with the existing database. The annotator can also be confused by uncommon patterns where two people have the same name but different IDs; therefore, if the existing sample database has unique values in a column, we prefer to enforce that i also has unique values in that column.\nNon-vacuous Execution Extremely small i frequently leads to undefined denotations. For example, the maximum of zero elements is a special NULL value, meaning \u201cundefined\u201d; this confuses annotators without a computer science background (Figure 6d). Therefore, we always add a small probability mass ( 116 ) of RETURN NULL to the distribution p and normalize the probabilities of the other candidates proportionally, in order to incentivize our algorithm to produce i such that other SQL candidates will return non-NULL values.\nEven if the returned value is well-defined, small i can lead to confusion if some operators are not needed to answer the question. For example, in Figure 6e, asking the maximum over one element might appear confusing, as we do not need the max operator to obtain a correct denotation. Therefore, we always add into p\u2032 a small probability mass of \u201cneighbor queries\u201d (Zhong et al., 2020) obtained by dropping aggregation operators and WHERE clauses from SQL candidates in p\u2032. This incentivizes our al-\ngorithm to produce i such that the SQL candidates will meaningfully use their operators.\nManaging Tradeoffs between two Criteria All the above tweaks make a tradeoff between the informative and the simplicity criteria in some way: we impose restrictions on i or modify p\u2032 to decrease the annotator effort while sacrificing information gain we can potentially achieve. How do we decide when to apply certain tweaks?\nIn our paper, we always add small probabilities of neighbor queries and RETURN NULL to p\u2032 and use cell values from the existing database. We then consider 3 types of tweaks that we apply if possible: 1) i0 satisfies the uniqueness constraint, 2) i0 is initialized with an existing database, and 3) |i| \u2264 15 rather than 30. We apply these tweaks if they do not prevent us from succeeding. We define in total 23 = 8 different \u201cconfigurations\u201d 0 \u2264 c < 8, each of which specifies what subset of tweaks to apply to the algorithm described in \u00a73. For example, c = 6 = B110 means we apply tweaks 1) and 2). We enumerate from c = 7 to 0 until the algorithm from \u00a73 returns a program input with IG(i) \u0338= 0. In other words, we start by applying all the tweaks and drop the tweaks gradually until we obtain a i with positive expected information gain."
        },
        {
            "heading": "D Fixing SPIDER Databases",
            "text": "We found several issues with the SPIDER databases and fixed them as follows:\n\u2022 Some SPIDER databases do not conform to the foreign key constraint, i.e. some of the child columns contain values not in the parent columns they are referring to. We enforce the foreign key constraint by dropping the illegal records.\n\u2022 In some domains, we identify missing foreign key constraints and add them.\n\u2022 The voter_1 domain does not contain an appropriate foreign key design. Since fixing it would require an effort of re-annotating all 15 associated SQLs, we chose to exclude this domain from our evaluation.\n\u2022 Some Date typed columns are string-valued and use English words to represent values, e.g. nov1,2021. As a result, dec1,2021, which is chronologically later, will be considered smaller alphabetically. We fix this\nby canonicalizing date representations into a yyyy-mm-dd format.\nWe accordingly update the suite of test cases from Zhong et al. (2020) that we use to check whether two SQL forms are equivalent (see \u00a74.2), so that they conform to the new database schema."
        },
        {
            "heading": "E Generating SQL Candidates",
            "text": "E.1 Prompting Codex As sketched in \u00a74.2, we obtain SQL program candidates through few-shot prompting, where the database schema is followed by 4 or 8 (with 50% probability) pairs of natural language utterances with their corresponding SQL queries from the SPIDER development set from the subset of utteranceSQL pairs associated with the same database schema. To select each in-context example, with probability 50% we choose the examples most similar to the utterance u that we want to annotate based on TF-IDF similarity, and with probability 50% we choose a random example that has not yet"
        },
        {
            "heading": "1 0.87 0.80 0.56 0.45 0.72",
            "text": ""
        },
        {
            "heading": "2 0.94 0.89 0.74 0.63 0.84",
            "text": ""
        },
        {
            "heading": "4 0.96 0.93 0.87 0.70 0.89",
            "text": ""
        },
        {
            "heading": "8 0.97 0.95 0.95 0.78 0.92",
            "text": ""
        },
        {
            "heading": "16 0.98 0.96 0.98 0.81 0.94",
            "text": ""
        },
        {
            "heading": "32 0.98 0.96 0.98 0.85 0.95",
            "text": "been selected. Finally we append the natural language utterance u to be annotated, and ask Codex to continue generating text after this prompt, which generates a candidate SQL program corresponding to u. An example prompt can be seen in Figure 7. As mentioned in \u00a74.2, we sampled 200 different prompts, which varied in their selected examples, and for each prompt we sampled 20 candidates from Codex with temperature=1.0 and top_p=0.95.\nE.2 Top-k Accuracy\nAs mentioned in \u00a74.2, we defined p0 as a distribution over the top-k approximate equivalence classes of the 4000 sampled programs, where k = 16. Table 3 reports the candidate ceilings (\u00a74.4) for various other values of k, and Figure 8 graphs these as top-k accuracy curves."
        },
        {
            "heading": "F Errors in Non-Programmer Responses",
            "text": "Ambiguous Utterances. Consider the utterance \u201cWhat are the names of properties that are either houses or apartments with more than 1 room?\u201d Should it be parsed as \u201c(house) or (apartment and room > 1)\u201d, or \u201c(house or apartment) and room > 1\u201d? Another example: \u201cCount the number of friends Kyle has.\u201d What to do when there are two students named Kyle?\nHeavy Computation. It is hard for humans to do arithmetic mentally, e.g., find the average of eight 9-digit values. To avoid demanding such computations, APEL should improve the annotator effort model |i| beyond counting the number of records (Appendix B).\nDatabase Constraints with Common Sense. Database schemas sometimes omit common-sense constraints. For example, according to common sense, \u201cBIRTHDAY + AGE\u201d should always yield the current year, so sorting by BIRTHDAY ascendingly\nis equivalent to sorting by AGE descendingly. However, APEL is able to find a database where these two strategies return different outputs.5 Such a database is unnatural and confuses humans. A possible solution would be to induce such constraints from the sample database and/or the column names in the schema."
        },
        {
            "heading": "G Computation",
            "text": "We did not compute the runtime in a controlled environment, so the statistics in this section are our best estimate.\nFinding a single informative small database can take up to several minutes. The simulation evaluation on the evaluation split in \u00a75 (524 utterances) takes around 240 CPU hours in total.\nFor the human evaluation in \u00a76 (240 utterances), we must pre-compute the databases for each utterance, in order to support real-time interaction. Since we may ask the annotator up to 3 questions about the utterance, the choice of database\n5Even though APEL derives its databases from the original SPIDER sample database, that sample database contains records that do not conform to this unstated constraint.\nit is conditioned on 0\u20132 previous questions and responses (i1, r1, . . . , it\u22121, rt\u22121). We pre-compute the database it for each of these possible histories that we may encounter.6 This takes around 100 CPU hours in total (for the 240 utterances)."
        },
        {
            "heading": "H Simulated Interaction Statistics",
            "text": "The breakdown statistics of candidate and interaction ceiling (see \u00a74.4) can be seen in Table 4, and the distribution database sizes and number of rounds of interaction can be seen in Figure 9.\nRobustness Towards Hyper-Parameter Choices. We vary the hyper-parameters in \u00a73 to test its robustness. Changing 5% to 20%, or decreasing the number of random re-runs, all leads to the same\n6Except that we set a timeout of 40 minutes per utterance. Of the 240 utterances, 7 utterances timed out before computing all of the databases in the response tree. (These primarily came from one domain where SPIDER\u2019s sample database was very large.) If during the interaction with an annotator, we needed a database it that we had not precomputed, we aborted the interaction early (we considered this as a failure to find an appropriate database, in the terms of \u00a72.1). However, this rarely happened, since at each node of the response tree, we considered the most probable responses first.\nperformance of 91%, and the average number of interaction rounds fluctuates by at most 0.05. Additionally, even after increasing the maximal allowed database size R from 15 to 30, we still obtain an average size of 10, since we prefer smaller databases under the same information gain."
        },
        {
            "heading": "I Human Annotation",
            "text": "We provide breakdown statistics on the annotation accuracy of different sources based on difficulty in Table 5.\nMore analysis on the annotator behavior model can be found in Figure 10.\nJ Interface\nSee Figure 11 for a detailed screenshot of our interface. We implemented the front-end of our interface with Angular and the back-end was built with flask and Redis. Users are presented with a sequence of 40 distinct questions, and each question may have multiple rounds. For each round, the user is given a 4 minute time-limit before the interface automatically transitions to the next question. Before being asked questions on a new database, the user is presented with a page displaying all the tables in the database alongside descriptions we wrote for each table (see Figure Figure 12 for an example screenshot). When answering questions, the user is given a link back to this page for reference.\nThe user can either select one of the multiple choice questions presented or select \u201cNo Answer is Correct\u201d, and depending on their selection the user is presented with a differing set of followup questions. Regardless of their selection, we always ask the user two optional followups: \u201cif you think the question is ambiguous, tell us why.\u201d and \u201cif the question looks confusing, tell us why.\u201d In addition to these optional questions, we sometimes ask required followup questions. Specifically, if the user is on their final round and selects an answer which does not agree with the SPIDER annotation, we ask them why they did not select the correct answer according to spider. Or if the user selects \u201cNo Answer is Correct\u201d, we ask \u201cWhat is the answer you have in mind and why?\u201d We use the users\u2019 answers to these followups to collect information on the users\u2019 reasoning in answering questions and to determine issues with the SPIDER dataset.\nWe implemented a number of features in our interface to minimize the annotator effort. One of the largest challenges in this task is answering ques-\ntions across several foreign keys. We implement two distinct mechanisms to make this easier for users. First, we highlight all table values or foreign keys matching the value the mouse is currently hovering over. Second, we give the user the option to merge all foreign keys into a single table by pressing a \u201cmerge\u201d button. We allow the users to choose when to merge because there is a trade-off; while merged mode can make reasoning about foreign keys easier, it also can significantly increase the width of the tables visible to the user.\nSometimes there are tables presented to the user that are not necessary for answering the question, so we give users the option to collapse tables to simplify their display.\nK Video Transcript\nPage 1 In this task, you will be asked to answer questions from several tables.\nPage 2 Here is the overall idea. You will be given a question on the top of the page, several tables on the left of the page, and you need to choose one of the options on the right, that corresponds to the correct answer. In this question, you are\nasked to \u201cShow name, country, age for all singers ordered by age from the oldest to the youngest.\u201d Therefore, we expect the correct option to list the information about Joe Sharp first, since he is older. We look at the options and B is correct. Notice that A is wrong because it does not list the information for all singers, and C is wrong because it lists the singers from the youngest to the oldest.\nAfter you submit the answer, our system will ask you whether there is anything that appears ambiguous or confusing. We don\u2019t need it for this question now.\nPage 3 Let\u2019s go through some more examples.\nPage 4 In this question you are asked \u201cHow many singers do we have?\u201d This is a tricky question. First notice that the tables have changed from before, so you need to re-read the table. Secondly, there are actually two singers, but they have the same name. You should consider them to be two different persons with the same name but different SSN, and hence choose B.\nThere is a time limit shown at the top of the page, and after 4 minutes the system will move on to the\nnext question.\nPage 5 Takeaways:\n\u2022 Names are different from IDs. Two different people can have the same name.\n\u2022 There is a time limit of 4 minutes for each question.\nPage 6 In this question you are asked to find the song names of the singers above the average age. The average age is the mean of these 4 numbers, which is 34.5. The singers John and Rose have age above 34.5, so we can find their songs, which are sun and gentle man, which is D. Use a calculator if you need to!\nAlso, notice that there are other tables, but they are not relevant to the question. Feel free to ignore them. You can also choose to collapse them if that makes it easier, and you can click the button again to view it.\nPage 7 Takeaways:\n\u2022 Use a calculator if you need to.\n\u2022 Not every table is needed.\nPage 8 Here\u2019s the same question and the same table. Let\u2019s say somehow Sun and Gentleman is not one of the options, and then you should report that no answer is correct. Then we will ask you why you think no answer is correct. For example, you can write \u201cgentleman and sun is correct. the average age is 34.5, John and Rose are above this age and have song gentleman and Sun\u201d.\nThe system asks us why we didn\u2019t choose A, we can answer \u201csun is by Rose, who is older than 34.5\u201d. Please tell us enough information so that we can know why your choice is correct - for example if you just say \u201csun is also a correct answer\u201d, it only describes the difference between the two options rather than explaining why it is correct. Giving us more information can help you win more bonus.\nPage 9 Takeaways:\n\u2022 Choose no option is correct and tell us why when you think no options are correct\n\u2022 Tell us why you didn\u2019t choose an answer when we ask you to do so.\nPage 10 The question is \u201cWhat are the full names of all players, sorted by birth date?\u201d First, notice that there are a lot of answers in this case, and you need to scroll down to read through all of them.\nSecondly, there are a lot of ambiguities: for example, the question didn\u2019t mention whether we should sort from youngest to oldest, or the reverse; secondly, the question does not mention whether the first and last name should be in the same column. For these reasons, A, B are both correct. C, D are wrong because the question does not ask for birthday information; F is wrong because it only lists one player and G is wrong for including birthday information. Then we can write in the response: \u201cABE are all correct; not sure if we should sort them from the oldest to youngest or reverse; also not sure whether to put the first and last name into the same column.\u201d But still, make your best guess, let\u2019s say, A.\nThen we click submit, and the system asks us why we didn\u2019t choose C. We explain that \u201cthe question does not ask us for the birthday and it contains redundant information\u201d.\nPage 11 Takeaways:\n\u2022 There can be a lot of options. Make sure to read through every of them\n\u2022 When the question is ambiguous and multiple answers are plausible, tell us why it is ambiguous and what are the plausible answers. But still, first make your best guess and submit.\nPage 12 The question is \u201cGive the names of countries that are in Europe and have a population equal to 80000.\u201d In this fictitious table, Brazil is in Europe and has a population of 80,000. Therefore, the correct answer is A, even though we know that Brazil is in fact in South America. However, it still cannot stop us from answering the question based on the table. Finally, there are many more countries in the world, beyond these three countries in the table, but we should pretend that there are only three countries in the world here.\nPage 13 Takeaways:\n\u2022 Try accepting the information from this table as much as possible and focus on the part useful for answering the question.\n\u2022 If something is not present in the tables, pretend that it does not exist.\nPage 14 Here are some more difficult tables.This is a database that contains information about battles and death. The overall description of the databases can be seen at the top of the page, which says: This\ndatabase contains information about battles, death events, and ships. And then each table has its own description as well. For example, in the ship table, each row contains information about a ship, the 4th row means the ship D was lost in battle with ID 4, and you can look up information about battle 4 in the battle table. To make it convenient for you, whenever you move your cursor to a value, all the same values will be highlighted. Here we notice that according to the 5th row, Ship E was also lost in battle 4.\nTo view multiple tables at the same time, you can choose to zoom out, like this. Then you can zoom back in, like this. You can typically find this option in the Help panel of your browser. Again, if you think some tables are irrelevant, just collapse them like this.\nYou don\u2019t have to study the tables in detail, since they will probably change for the next question.\nPage 15 Takeaways:\n\u2022 You don\u2019t have to study the table content in great detail, since they will be changing.\n\u2022 Zoom-in/out if you need to. You can find them in the helper panel of your browser.\nPage 16 This question is \u201cShow names, results and bulgarian commanders of the battles with no ships lost in the \u2019English Channel\u201d.\nThe question asks for certain battles namely, those that did not lose ships in the English Channel [pause]. Let\u2019s start by finding the battles that did lose ships in the English channel [pause]. Only Battle 5 did; it lost ship C there. So the other battles, Battles 0 and 7, lost no ships there. In fact, Battle 0 lost no ships at all, which is why it doesn\u2019t show up in the second table. We find the names of Battle 0 and 7, along with their other information. Therefore, the answer is E. One very common mistake people make is that they ignored the word \u201cno\u201d, and they chose the battles that lost the ship. Be careful and pay close attention to every word!\nNotice that there was originally the death table. We removed it from the display to make it easier for you.\nThe phrase \u2019Bulgarian commander\u2019 might send you looking for a table that tells you each commander\u2019s nationality. But actually, Bulgarian_commander is a column in the battles table. Presumably this table lists battles that Bulgaria fought. Each battle had two sides, and this column\nis naming the commander for the Bulgarian side. You don\u2019t have to fully understand how the tables are set up, but you should figure out enough to answer the question.\nJust to repeat, to make it easier for you to process this information, whenever your cursor moves to an ID or a piece of text, its counterpart in other tables will light up; whenever you click on a text, the counterpart in the answer will also be highlighted.\nYou can also choose to merge the tables. After you merge the table, there will still be two tables. Each of the rows in the battle table will still contain information about a battle, and each of the rows in the ship table will still contain information about a ship. However, the battle information where the ship is lost is merged into the ship table. Notice that battle 0 will not appear in the ship table, because no ship is lost in the battle, so be careful when you try to interpret the merged table. Click unmerge to recover to the original view.\nFinally, if you forgot what each table means, you can always view them here.\nPage 17 Takeaways:\n\u2022 Pay close attention to how the question is being asked. They might lead to different options. Many mistakes people make are because they did not read the questions carefully.\n\u2022 Sometimes we choose not to show you certain tables and columns if we know for sure they are not needed.\n\u2022 Use the highlight functionality if that helps you to reason across tables.\n\u2022 Use the merge functionality if you need to. Each table will contain information about the same object/entity, but the information about its related objects will be pooled in.\nPage 18 The question is \u201cList the name and date of the battle that has lost the ship named \u2018Lettice\u2019 and the ship named \u2019HMS Atalanta\u2019.\u201d Since there is no ship named \u201cHMS atlanta\u201d, there is no battle that lost both of these ships. So you should choose A, \u201cno result found\u201d.\nPage 19 Takeaways: Choose no_result_found if no answer satisfies the question.\nPage 20 To summarize, here are a couple of things you need to remember to answer the questions correctly:\n\u2022 Pay close attention to how the question is asked; most mistakes are made because of not reading the question carefully.\n\u2022 Accept the information in the table even if they are changing and might be different from the knowledge you have for the real world\n\u2022 IDs are different from names\n\u2022 Some questions might have a lot of options to choose from and you need to read through all of them.\nPage 21 To make it easier for you to answer the questions:\n\u2022 Use the highlight and merge operations when you need to\n\u2022 Use a calculator if you need to\n\u2022 Zoom out to fit the tables into the screen and prevent scrolling.\n\u2022 Not all table or column is needed to answer the questions\nPage 22 For freeform response:\n\u2022 Reporting ambiguities or tell us why the question is confusing only if you need to\n\u2022 Explaining why you did not choose another option when we ask you. Giving us more information can you help you win more bonus."
        },
        {
            "heading": "L Beyond Text-to-SQL",
            "text": "Our framework can be generalized to other semantic parsing applications more broadly, where\n\u2022 the SQL program s can be generalized to other types of executable semantic parses, such as tensor manipulation commands, visualization programs (Chen et al., 2021c), or dataflow graphs (Semantic Machines et al., 2020);\n\u2022 the database schema c can be generalized to include any context that affects the mapping of u to s, e.g., the conversational history preceding u, and the input type required by program s;\n\u2022 the input database i can be generalized to any well-typed input if s is a function, or i can be the program state (e.g., a mapping from variable names to their values) if s is a step in a procedural program;\n\u2022 the database query result o = s(i) can be generalized to the intended effect of u given i, which includes not only an answer or a table, but also an output images, side effects such as file updates (e.g., updates to a database or a document), or robotic actions.\nApplying APEL to a new type of semantic parsing application, rather than utterance-to-SQL, would require the following components:\nA seed semantic parser that is likely to generate a short list of candidates that contain the correct program. This requirement is not hard to satisfy in many applications, given that large language models achieve often achieve high top-k accuracy on generating simple Python snippets (Chen et al., 2021a), JSON data (Poesia et al., 2022), Lispress (Shin et al., 2021) and SQL programs (Scholak et al., 2021b; Rajkumar et al., 2022) with only a few training examples and are likely to continue improving (Kaplan et al., 2020). For example, we achieved 95% top-32 accuracy on SPIDER without any task-specific engineering beyond few-shot prompting (e.g., specialized architectures (Wang et al., 2020), decoding constraints (Scholak et al., 2021b), etc).\nAn algorithm to find an simple informative program input i that satisfies c. Our method in \u00a73 generates random databases by using an existing sample database as a reference and greedily drops rows to optimize the objective in equation (5). Future methods could potentially speed up the optimization process with a learned neural network (Chen et al., 2018) or a constraint solver (Chu et al., 2017).\nA graphical interface that enables the annotators to easily inspect the input i and choose the correct output o, where i, o can be generalized from database tables, strings, or numbers to calendar events (Andreas et al., 2020), voxel structures (Wang et al., 2017), etc. Careful interface design (\u00a76.1) can significantly reduce the effort required from the annotators.\nIn summary, APEL is a general framework for clarifying the semantics of natural language utterances. It elicits information from humans about how the semantic forms should behave when executed on particular inputs.\nIn this paper we demonstrated the value of APEL on a text-to-SQL task. Some future work is out-\nlined in Appendices A\u2013B. It would also be desirable to refine and speed up our heuristics for the challenging problem of finding simple inputs that distinguish among SQL queries. Finally, we look forward to future work that extends APEL to other semantic parsing applications."
        },
        {
            "heading": "M Simulation Experiments on Regex",
            "text": "We include an additional experiment on regular expressions to help the readers understand how to apply APEL to semantic parsing tasks other than SQL.\nDataset. We used the dataset from Ye et al. (2020), which aims to synthesize a regular expression from a natural language description and a few strings that should be accepted and rejected by the regex. For example, an utterance u could be\n\u201cThis is a list of three comma separated strings, where each string must be formed by the substrings \"cz\" or \"rzq\" followed by one to four digits.\u201d\nand the corresponding program s is\n\u201cconcat ( concat ( or ( const(<cf>), const(<rzq>)),repeatrange(<num>, 1 ,4)),concat(<,> ,concat ( concat ( or ( const(<cf>), const(<rzq>)), repeatrange(<num>, 1, 4)), concat ( <,>, concat ( or (const(<cf>),const(<rzq>)), repeatrange(<num>, 1, 4))))))\u201d.\nThe program s maps from any input string i to a boolean output of 1 or 0. Thus, an o-selection question simply asks whether the string i should be accepted or rejected.\nWe used the development split to prompt GPT-4 (OpenAI, 2023) to create the seed semantic parser. We tested APEL on the test-inclusion split, where the utterances come from the same annotators who annotated the development split.\nSeed Semantic Parser. We prompted GPT-4 with few-shot demonstrations to create a seed semantic parser. We sampled 50 demonstrations from the development split to simulate a small \u201ctraining set\u201d of expert labels. For each utterance u we sampled a candidate program from the seed semantic parser 50 times, where each time we randomly\nselected 20 demonstrations from the 50 dev split demonstrations as in-context demonstrations. We then automatically filtered out candidate programs that were syntatically invalid and merged semantically equivalent ones (semantic equivalence of regexps can be tested exactly). Finally, we define p0 to be the empirical distribution of the top-10 equivalence classes. The top-1 accuracy (seed parser) is 64% and the top-10 accuracy (ceiling) is 77%.\nSimple and Informative Program Input. We wish to choose among several candidate programs (or more precisely, equivalence classes). For each pair of candidate programs s1 and s2, we sample 200 program input strings i such that s1(i) \u0338= s2(i), using the efficient implementation from Ye et al. (2020). We then collect all such input strings across all pairs of candidate programs, find the ones with the highest information gain, and break ties by choosing the shortest one as a proxy for simplicity.\nTo evaluate whether the above procedure is effective in generating simple and informative program input, we consider a baseline that only generates an input string that would be accepted by the highestranking candidate regex, again using the implementation from Ye et al. (2020). Intuitively, this baseline tries to check the highest-ranking candidate program, by determining whether a string that it accepts should in fact be accepted. The response (if assumed to be correct) may eliminate some of the programs.\nOur information gain method, by contrast, actively tries to distinguish among the candidates. It hopes to gain up to 1 bit of information about the correct candidate (this is the maximum information that can be derived from the annotator\u2019s 1-bit response). An input string has IG \u2248 1 bit if the candidates that would accept that string currently have about half of the probability mass. In that case, either response (1 or 0) will eliminate about half of the candidates (cf. Littlestone and Warmuth, 1994).\nThat said, any input string i sampled by our procedure will distinguish between two of the candidate programs, and hence will have positive information gain. The annotator\u2019s response r (if assumed to be correct) will eliminate one of those two candidate programs, and perhaps others. Thus, it will take at most 9 rounds of oracle annotation for us to rule out 9 incorrect equivalence classes from our original 10 classes, thus achieving the candidate ceiling (\u00a74.4), regardless of which sampled\ninput string we select at each round.\nResults. After interacting with a simulated annotator for at most three rounds with an oracle annotator as in \u00a75, the APEL accuracy reaches the ceiling of 77% and outperforms the seed parser. In contrast, the baseline only achieves 68% accuracy after three rounds, thus illustrating the importance of optimizing information gain. Finally, APEL uses input strings with an average length of 5.3, while the baseline\u2019s average is 11.6; this suggests that optimizing for simplicity is successful."
        }
    ],
    "title": "Non-Programmers Can Label Programs Indirectly via Active Examples: A Case Study with Text-to-SQL",
    "year": 2023
}