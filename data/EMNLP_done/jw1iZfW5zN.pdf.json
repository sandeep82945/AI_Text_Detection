{
    "abstractText": "Transformer-based encoder-decoder models that generate outputs in a left-to-right fashion have become standard for sequence-tosequence tasks. In this paper, we propose a framework for decoding that produces sequences from the \u201coutside-in\u201d: at each step, the model chooses to generate a token on the left, on the right, or join the left and right sequences. We argue that this is more principled than prior bidirectional decoders. Our proposal supports a variety of model architectures and includes several training methods, such as a dynamic programming algorithm that marginalizes out the latent ordering variable. Our model sets state-of-the-art (SOTA) on the 2022 and 2023 shared tasks, beating the next best systems by over 4.7 and 2.7 points in average accuracy respectively. The model performs particularly well on long sequences, can implicitly learn the split point of words composed of stem and affix, and performs better relative to the baseline on datasets that have fewer unique lemmas (but more examples per lemma).1",
    "authors": [
        {
            "affiliations": [],
            "name": "Marc E. Canby"
        }
    ],
    "id": "SP:6ebd12fdfaae34284e25ea50113cd57f38f87970",
    "references": [
        {
            "authors": [
                "Roee Aharoni",
                "Yoav Goldberg."
            ],
            "title": "Morphological inflection generation with hard monotonic attention",
            "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2004\u20132015, Vancouver,",
            "year": 2017
        },
        {
            "authors": [
                "Kamal Al-Sabahi",
                "Zhang Zuping",
                "Yang Kang."
            ],
            "title": "Bidirectional attentional encoder-decoder model and bidirectional beam search for abstractive summarization",
            "venue": "arXiv preprint arXiv:1809.06662.",
            "year": 2018
        },
        {
            "authors": [
                "Antonios Anastasopoulos",
                "Graham Neubig."
            ],
            "title": "Pushing the limits of low-resource morphological inflection",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natu-",
            "year": 2019
        },
        {
            "authors": [
                "Dzmitry Bahdanau",
                "Kyunghyun Cho",
                "Yoshua Bengio."
            ],
            "title": "Neural machine translation by jointly learning to align and translate",
            "venue": "arXiv preprint arXiv:1409.0473.",
            "year": 2014
        },
        {
            "authors": [
                "Samy Bengio",
                "Oriol Vinyals",
                "Navdeep Jaitly",
                "Noam Shazeer."
            ],
            "title": "Scheduled sampling for sequence prediction with recurrent neural networks",
            "venue": "Advances in Neural Information Processing Systems, volume 28. Curran Associates, Inc.",
            "year": 2015
        },
        {
            "authors": [
                "Toms Bergmanis",
                "Katharina Kann",
                "Hinrich Sch\u00fctze",
                "Sharon Goldwater."
            ],
            "title": "Training data augmentation for low-resource morphological inflection",
            "venue": "Proceedings of the CoNLL SIGMORPHON 2017 Shared Task: Universal Morphological Reinflection, pages",
            "year": 2017
        },
        {
            "authors": [
                "Lukas Biewald."
            ],
            "title": "Experiment tracking with weights and biases",
            "venue": "Software available from wandb.com.",
            "year": 2020
        },
        {
            "authors": [
                "Marc Canby",
                "Aidana Karipbayeva",
                "Bryan Lunt",
                "Sahand Mozaffari",
                "Charlotte Yoder",
                "Julia Hockenmaier."
            ],
            "title": "University of Illinois submission to the SIGMORPHON 2020 shared task 0: Typologically diverse morphological inflection",
            "venue": "Proceedings of the",
            "year": 2020
        },
        {
            "authors": [
                "Ryan Cotterell",
                "Christo Kirov",
                "John Sylak-Glassman",
                "David Yarowsky",
                "Jason Eisner",
                "Mans Hulden."
            ],
            "title": "The SIGMORPHON 2016 shared Task\u2014 Morphological reinflection",
            "venue": "Proceedings of the 14th SIGMORPHON Workshop on Computational",
            "year": 2016
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova"
            ],
            "title": "2019. BERT: Pre-training",
            "year": 2019
        },
        {
            "authors": [
                "Omer Goldman",
                "Khuyagbaatar Batsuren",
                "Khalifa Salam",
                "Aryaman Arora",
                "Garrett Nicolai",
                "Reyt Tsarfaty",
                "Ekaterina Vylomova."
            ],
            "title": "SIGMORPHON\u2013 UniMorph 2023 shared task 0: Typologically diverse morphological inflection",
            "venue": "Proceedings of the",
            "year": 2023
        },
        {
            "authors": [
                "David Guriel",
                "Omer Goldman",
                "Reut Tsarfaty."
            ],
            "title": "Morphological reinflection with multiple arguments: An extended annotation schema and a Georgian case study",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Vol-",
            "year": 2022
        },
        {
            "authors": [
                "Kelvin Guu",
                "Panupong Pasupat",
                "Evan Liu",
                "Percy Liang."
            ],
            "title": "From language to programs: Bridging reinforcement learning and maximum marginal likelihood",
            "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Vol-",
            "year": 2017
        },
        {
            "authors": [
                "Kenji Imamura",
                "Eiichiro Sumita."
            ],
            "title": "Transformerbased double-token bidirectional autoregressive decoding in neural machine translation",
            "venue": "Proceedings of the 7th Workshop on Asian Translation, pages 50\u2013 57, Suzhou, China. Association for Computational",
            "year": 2020
        },
        {
            "authors": [
                "Katharina Kann",
                "Hinrich Sch\u00fctze."
            ],
            "title": "The LMU system for the CoNLL-SIGMORPHON 2017 shared task on universal morphological reinflection",
            "venue": "Proceedings of the CoNLL SIGMORPHON 2017 Shared Task: Universal Morphological Reinflection, pages",
            "year": 2017
        },
        {
            "authors": [
                "Volodymyr Kindratenko",
                "Dawei Mu",
                "Yan Zhan",
                "John Maloney",
                "Sayed Hadi Hashemi",
                "Benjamin Rabe",
                "Ke Xu",
                "Roy Campbell",
                "Jian Peng",
                "William Gropp."
            ],
            "title": "Hal: Computer system for scalable deep learning",
            "venue": "Practice and Experience in Advanced Re-",
            "year": 2020
        },
        {
            "authors": [
                "Jordan Kodner",
                "Sarah Payne",
                "Salam Khalifa",
                "Zoey Liu"
            ],
            "title": "Morphological inflection: A reality check",
            "year": 2023
        },
        {
            "authors": [
                "Carolin Lawrence",
                "Bhushan Kotnis",
                "Mathias Niepert."
            ],
            "title": "Attending to future tokens for bidirectional sequence generation",
            "venue": "arXiv preprint arXiv:1908.05915.",
            "year": 2019
        },
        {
            "authors": [
                "Lemao Liu",
                "Andrew Finch",
                "Masao Utiyama",
                "Eiichiro Sumita."
            ],
            "title": "Agreement on targetbidirectional lstms for sequence-to-sequence learning",
            "venue": "Thirtieth AAAI Conference on Artificial Intelligence.",
            "year": 2016
        },
        {
            "authors": [
                "Sewon Min",
                "Danqi Chen",
                "Hannaneh Hajishirzi",
                "Luke Zettlemoyer."
            ],
            "title": "A discrete hard EM approach for weakly supervised question answering",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the",
            "year": 2019
        },
        {
            "authors": [
                "Myle Ott",
                "Sergey Edunov",
                "Alexei Baevski",
                "Angela Fan",
                "Sam Gross",
                "Nathan Ng",
                "David Grangier",
                "Michael Auli."
            ],
            "title": "fairseq: A fast, extensible toolkit for sequence modeling",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Associa-",
            "year": 2019
        },
        {
            "authors": [
                "Yong Shan",
                "Yang Feng",
                "Jinchao Zhang",
                "Fandong Meng",
                "Wen Zhang."
            ],
            "title": "Improving bidirectional decoding with dynamic target semantics in neural machine translation",
            "venue": "arXiv preprint arXiv:1911.01597.",
            "year": 2019
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141 ukasz Kaiser",
                "Illia Polosukhin."
            ],
            "title": "Attention is all you need",
            "venue": "Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc.",
            "year": 2017
        },
        {
            "authors": [
                "Mans Hulden."
            ],
            "title": "SIGMORPHON 2020 shared task 0: Typologically diverse morphological inflection",
            "venue": "Proceedings of the 17th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology, pages 1\u201339, Online.",
            "year": 2020
        },
        {
            "authors": [
                "Shijie Wu",
                "Ryan Cotterell."
            ],
            "title": "Exact hard monotonic attention for character-level transduction",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1530\u2013 1537, Florence, Italy. Association for Computational",
            "year": 2019
        },
        {
            "authors": [
                "Shijie Wu",
                "Ryan Cotterell",
                "Mans Hulden."
            ],
            "title": "Applying the transformer to character-level transduction",
            "venue": "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 1901\u20131907, Online.",
            "year": 2021
        },
        {
            "authors": [
                "Jitao Xu",
                "Fran\u00e7ois Yvon."
            ],
            "title": "One source, two targets: Challenges and rewards of dual decoding",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 8533\u20138546, Online and Punta Cana, Dominican Re-",
            "year": 2021
        },
        {
            "authors": [
                "Changbing Yang",
                "Ruixin (Ray) Yang",
                "Garrett Nicolai",
                "Miikka Silfverberg"
            ],
            "title": "Generalizing morphological inflection systems to unseen lemmas",
            "venue": "In Proceedings of the 19th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology,",
            "year": 2022
        },
        {
            "authors": [
                "Xiangwen Zhang",
                "Jinsong Su",
                "Yue Qin",
                "Yang Liu",
                "Rongrong Ji",
                "Hongji Wang."
            ],
            "title": "Asynchronous bidirectional decoding for neural machine translation",
            "venue": "Proceedings of the AAAI conference on artificial intelligence, volume 32.",
            "year": 2018
        },
        {
            "authors": [
                "Zhirui Zhang",
                "Shuangzhi Wu",
                "Shujie Liu",
                "Mu Li",
                "Ming Zhou",
                "Tong Xu"
            ],
            "title": "Regularizing neural machine translation by target-bidirectional agreement",
            "year": 2019
        },
        {
            "authors": [
                "Long Zhou",
                "Jiajun Zhang",
                "Chengqing Zong."
            ],
            "title": "Synchronous bidirectional neural machine translation",
            "venue": "Transactions of the Association for Computational Linguistics, 7:91\u2013105.",
            "year": 2019
        },
        {
            "authors": [
                "Long Zhou",
                "Jiajun Zhang",
                "Chengqing Zong",
                "Heng Yu."
            ],
            "title": "Sequence generation: From both sides to the middle",
            "venue": "arXiv preprint arXiv:1906.09601.",
            "year": 2019
        },
        {
            "authors": [
                "Ran Zmigrod",
                "Tim Vieira",
                "Ryan Cotterell."
            ],
            "title": "Exact paired-permutation testing for structured test statistics",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech-",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Transformer-based encoder-decoder architectures (Bahdanau et al., 2014; Vaswani et al., 2017) that decode sequences from left to right have become dominant for sequence-to-sequence tasks. While this approach is quite straightforward and intuitive, some research has shown that models suffer from this arbitrary constraint. For example, models that decode left-to-right are often more likely to miss tokens near the end of the sequence, while rightto-left models are more prone to making mistakes near the beginning (Zhang et al., 2019; Zhou et al., 2019a). This is a result of the \u201csnowballing\u201d effect, whereby the model\u2019s use of its own incorrect predictions can lead future predictions to be incorrect (Bengio et al., 2015; Liu et al., 2016).\n1Our code is available at https://github.com/ marccanby/bidi_decoding/tree/main.\nWe explore this issue for the task of morphological inflection, where the goal is to learn a mapping from a word\u2019s lexeme (e.g. the lemma walk) to a particular form (e.g. walked) specified by a set of morphosyntactic tags (e.g. V;V.PTCP;PST). This has been the focus of recent shared tasks (Cotterell et al., 2016, 2017, 2018; McCarthy et al., 2019; Vylomova et al., 2020; Pimentel et al., 2021; Kodner et al., 2022; Goldman et al., 2023). Most approaches use neural encoder-decoder architectures, e.g recurrent neural networks (RNNs) (Aharoni and Goldberg, 2017; Wu and Cotterell, 2019) or transformers (Wu et al., 2021).2 To our knowledge, Canby et al. (2020) is the only model that uses bidirectional decoding for inflection; it decodes the sequence in both directions simultaneously and returns the one with higher probability.\nIn this paper, we propose a novel framework for bidirectional decoding that supports a variety of model architectures. Unlike previous work (\u00a72), at each step the model chooses to generate a token on the left, generate a token on the right, or join the left and right sequences.\nThis proposal is appealing for several reasons. As a general framework, this approach supports a wide variety of model architectures that may be task-specific. Further, it generalizes L2R and R2L decoders, as the model can choose to generate sequences in a purely unidirectional fashion. Finally, the model is able to decide which generation order is best for each sequence, and can even produce parts of a sequence from each direction. This is particularly appropriate for a task like inflection, where many words are naturally split into stem and affix. For example, when producing the form walked, the model may chose to generate the stem\n2Orthogonal to the concerns in this paper, various data augmentation schemes such as heuristic alignment or rulebased methods (Kann and Sch\u00fctze, 2017; Anastasopoulos and Neubig, 2019) or the use of multilingual data (Bergmanis et al., 2017; McCarthy et al., 2019) have been proposed to improve these standard architectures.\nwalk from the left and the suffix ed from the right. We explore several methods for training models under this framework, and find that they are highly effective on the 2023 SIGMORPHON shared task on inflection (Goldman et al., 2023). Our method improves by over 4 points in average accuracy over a typical L2R model, and one of our loss functions is particularly adept at learning split points for words with a clear affix. We also set SOTA on both the 2022 and 2023 shared tasks (Kodner et al., 2022), which have very different data distributions."
        },
        {
            "heading": "2 Prior Bidirectional Decoders",
            "text": "Various bidirectional decoding approaches have been proposed for tasks such as machine translation and abstractive summarization, including ones that use some form of regularization to encourage the outputs from both directions to agree (Liu et al., 2016; Zhang et al., 2019; Shan et al., 2019), or algorithms where the model first decodes the entire sequence in the R2L direction and then conditions on that sequence when decoding in the L2R direction (Zhang et al., 2018; Al-Sabahi et al., 2018). Still more methods utilize synchronous decoding, where the model decodes both directions at the same time and either meet in the center (Zhou et al., 2019b; Imamura and Sumita, 2020) or proceed until each direction\u2019s hypothesis is complete (Zhou et al., 2019a; Xu and Yvon, 2021). Lawrence et al. (2019) allows the model to look into the future by filling placeholder tokens at each timestep."
        },
        {
            "heading": "3 A Bidirectional Decoding Framework",
            "text": "The following sections present a general framework for training and decoding models with bidirectional decoding that is irrespective of model architecture, subject to the constraints discussed in \u00a73.3."
        },
        {
            "heading": "3.1 Probability Factorization",
            "text": "For unidirectional models, the probability of an L2R sequence \u2212\u2192y = y1 \u00b7 \u00b7 \u00b7 yn or an R2L sequence\u2190\u2212y = yn \u00b7 \u00b7 \u00b7 y1 given an input x is defined as\nP (\u2212\u2192y |x) = |y|\u220f i=1 P (\u2212\u2192y i|\u2212\u2192y <i,x) (1) P (\u2190\u2212y |x) = |y|\u220f j=1 P (\u2190\u2212y j |\u2190\u2212y <j ,x) (2)\nwhere \u2212\u2192y i = yi or\u2190\u2212y j = yn\u2212j+1 is the ith or jth token in a particular direction. Generation begins\nwith a start-of-sentence token; at each step a token is chosen based on those preceding, and the process halts once an end-of-sentence token is predicted.\nIn contrast, our bidirectional scheme starts with an empty prefix $ and suffix #. At each timestep, the model chooses to generate the next token of either the prefix or the suffix, and then whether or not to join the prefix and suffix. If a join is predicted, then generation is complete.\nWe define an ordering o = o(1) \u00b7 \u00b7 \u00b7 o(n) as a sequence of left and right decisions: that is, o(t) \u2208 {L,R}. We use y(t) to refer to the token generated at time t under a particular ordering, and\u2212\u2192y (\u2264t) and \u2190\u2212y (\u2264t) to refer to the prefix and suffix generated up to (and including) time t.3 An example derivation of the word walked is shown below:\nDropping the dependence on x for notational convenience, we define the joint probability of output sequence y and ordering o as\nP (y,o) = |y|\u220f t=1 P (o(t)|\u2212\u2192y (<t),\u2190\u2212y (<t)) \u00b7\nP (y(t) | o(t),\u2212\u2192y (<t),\u2190\u2212y (<t)) \u00b7Q(t) (3)\nwhere Q(t) is the probability of joining (or not joining) the prefix and suffix:\nQ(t) = { P (join | \u2212\u2192y (\u2264t),\u2190\u2212y (\u2264t)) if t = |y| 1\u2212 P (join | \u2212\u2192y (\u2264t),\u2190\u2212y (\u2264t)) otherwise"
        },
        {
            "heading": "3.2 Likelihood and MAP Inference",
            "text": "To compute the likelihood of a particular sequence y, we need to marginalize over all orderings: P (y|x) = \u2211 o P (y,o|x). Since we cannot enumerate all 2|y| orderings, we have developed an exact O(|y|2) dynamic programming algorithm, reminiscent of the forward algorithm for HMMs.\nTo simplify notation, let PL(\u2212\u2192y i | \u2212\u2192y <i,\u2190\u2212y <j) (or PR(\u2190\u2212y j | \u2212\u2192y <i,\u2190\u2212y <j)) be the probability of\n3We use superscripts to refer to timesteps, and subscripts for sequence positions. Note that if, at a particular timestep t, we have prefix \u2212\u2192y \u2264i and suffix\u2190\u2212y \u2264j , then i+ j = t.\ngenerating the ith token from the left (or the jth token from the right), conditioned on \u2212\u2192y <i and\u2190\u2212y <j , the prefix and suffix generated thus far: PL( \u2212\u2192y i|\u2212\u2192y<i,\u2190\u2212y <j)=P (L |\u2212\u2192y<i,\u2190\u2212y <j)\u00b7P (\u2212\u2192y i|L,\u2212\u2192y<i,\u2190\u2212y<j) PR( \u2190\u2212y j|\u2212\u2192y<i,\u2190\u2212y <j)=P (R |\u2212\u2192y<i,\u2190\u2212y <j)\u00b7P (\u2190\u2212y j|R,\u2212\u2192y<i,\u2190\u2212y<j) Let Qij be the join probability for \u2212\u2192y \u2264i and\u2190\u2212y \u2264j :\nQij = { P (join | \u2212\u2192y \u2264i,\u2190\u2212y \u2264j) if i+ j = |y| 1\u2212 P (join | \u2212\u2192y \u2264i,\u2190\u2212y \u2264j) otherwise\n(4)\nFinally, denote the joint probability of a prefix\u2212\u2192y \u2264i and suffix\u2190\u2212y \u2264j by f [i, j].\nWe set the probability of an empty prefix and suffix (the base case) to 1:\nf [0, 0] = 1\nThe probability of a non-empty prefix \u2212\u2192y \u2264i and empty suffix \u03f5 can be computed by multiplying f [i\u22121, 0] (the probability of prefix\u2212\u2192y <i and empty suffix \u03f5) by PL(\u2212\u2192y i | \u2212\u2192y <i, \u03f5) (the probability of generating \u2212\u2192y i) and the join probability Qi0:\nf [i, 0] = f [i\u2212 1, 0] \u00b7 PL(\u2212\u2192y i|\u2212\u2192y <i, \u03f5) \u00b7Qi0 Analogously, we define\nf [0, j] = f [0, j \u2212 1] \u00b7 PR(\u2190\u2212y j |\u03f5,\u2190\u2212y <j) \u00b7Q0j Finally, f [i, j] represents the case where both prefix \u2212\u2192y \u2264i and suffix \u2190\u2212y \u2264j are non-empty. This prefix-suffix pair can be produced either by appending \u2212\u2192y i to the prefix \u2212\u2192y <i and leaving the suffix unchanged, or by appending\u2190\u2212y j to the suffix\u2190\u2212y <j and leaving the prefix unchanged. The sum of the probabilities of these cases gives the recurrence:\nf [i, j] = f [i\u2212 1, j] \u00b7 PL(\u2212\u2192y i|\u2212\u2192y <i,\u2190\u2212y \u2264j) \u00b7Qij+ f [i, j \u2212 1] \u00b7 PR(\u2190\u2212y j |\u2212\u2192y \u2264i,\u2190\u2212y <j) \u00b7Qij\nAfter filling out the dynamic programming table f , the marginal probability P (y) can be computed by summing all entries f [i, j] where i+ j = |y|:\nP (y) = \u2211 i,j I(i+ j = |y|) \u00b7 f [i, j]\nIf all local probabilities can be calculated in constant time, the runtime of this algorithm is O(|y|2).\nAs an aside, the MAP probability, or the probability of the best ordering for a given sequence, can be calculated by replacing each sum with a max:\nf [i, j]=max ( f [i\u2212 1, j]\u00b7PL(\u2212\u2192y i|\u2212\u2192y <i,\u2190\u2212y \u2264j)\u00b7Qij , f [i, j \u2212 1] \u00b7 PR(\u2190\u2212y j |\u2212\u2192y \u2264i,\u2190\u2212y <j) \u00b7Qij\n) max\no P (y,o) = max i,j\n( I(i+ j = |y|) \u00b7 f [i, j] ) The best ordering itself can be found with a backtracking procedure similar to Viterbi for HMM\u2019s."
        },
        {
            "heading": "3.3 Why does dynamic programming work?",
            "text": "Dynamic programming (DP) only works for this problem if the local probabilities (i.e. the token, join, and order probabilities) used to compute f [i, j] depend only on the prefix and suffix corresponding to that cell, but not on a particular ordering that produced the prefix and suffix. This is similar to the how the Viterbi algorithm relies on the fact that HMM emission probabilities depend only on the hidden state and not on the path taken.\nTo satisfy this requirement, the model\u2019s architecture should be chosen carefully. Any model that simply takes a prefix and suffix as input and returns the corresponding local probabilities is sufficient. However, one must be careful if designing a model where the hidden representation is shared or reused across timesteps. This is particularly problematic if hidden states computed from both the prefix and suffix are reused. In this case, the internal representations will differ depending on the order in which the prefix and suffix were generated, which would cause a DP cell to rely on all possible paths to that cell \u2212 thus breaking the polynomial nature of DP."
        },
        {
            "heading": "3.4 Training",
            "text": "We propose two different loss functions to train a bidirectional model. Based on our probability factorization, we must learn the token, join, and order probabilities at each timestep.\nOur first loss function LxH(\u03b8) trains each of these probabilities separately using cross-entropy loss. However, since ordering is a latent variable, it cannot be trained with explicit supervision. Hence, we fix the order probability to be 0.5 at each timestep, making all orderings equi-probable.\nWe then define S to contain the indices of all valid prefix-suffix pairs in a given sequence y:\nS = {(i, j) | 1 \u2264 i, j,\u2264 |y|; i+ j \u2264 |y|}\nHence, S has O(|y|2) elements. Finally, we define a simple loss LxH(\u03b8) that averages the cross-entropy loss for the token probabilities (based on the next token in \u2212\u2192y or\u2190\u2212y ) and join probabilities (based on whether the given prefix and suffix complete y):\nLxH(\u03b8) = 1\n3\n(\u2212\u2192 L (\u03b8) + \u2190\u2212 L (\u03b8) + L(join)(\u03b8) ) \u2212\u2192 L (\u03b8) = \u2212 1|S| \u2211 (i,j)\u2208S logP (\u2212\u2192y i | \u2212\u2192y <i,\u2190\u2212y <j ,x; \u03b8) \u2190\u2212 L (\u03b8) = \u2212 1|S| \u2211 (i,j)\u2208S logP (\u2190\u2212y j | \u2212\u2192y <i,\u2190\u2212y <j ,x; \u03b8)\nL(join)(\u03b8) = \u2212 1|S| \u2211\n(i,j)\u2208S\nlogQij\nwhere Qij is defined as in Equation 4. Due to the size of S, this loss takes O(|y|2) time to train.4 Given that a typical unidirectional model takes O(|y|) time to train, we also propose an O(|y|) approach that involves sampling from S; this is presented in Appendix F.\nAn alternative is to train with Maximum Marginal Likelihood (MML) (Guu et al., 2017; Min et al., 2019), which learns the order probabilities via marginalization. This is more principled because it directly optimizes P (y | x), the quantity of interest. The loss is given by LMML(\u03b8) = \u2212 logP (y|x; \u03b8), which is calculated with the dynamic programming algorithm described in \u00a73.2.5 Learning the order probabilities enables the model to assign higher probability mass to orderings it prefers and ignore paths it finds unhelpful.\nThis loss also requires O(|y|2) time to train."
        },
        {
            "heading": "3.5 Decoding",
            "text": "The goal of decoding is to find y such that y = argmaxy P (y|x). Unfortunately, it is not computationally feasible to use the likelihood algorithm in \u00a73.2 to find the best sequence y, even with a heuristic like beam search. Instead, we use beam search to heuristically identify the sequence y and ordering o that maximize the joint probability P (y,o|x):\ny,o = argmaxy,oP (y,o|x)\n4This assumes that local probabilities take O(1) time to compute, which is not the case for most neural architectures; however, this section is about the runtime of the training algorithms without regard to model architecture.\n5Appendix C describes how to train this loss in practice.\nThe formula for P (y,o|x) is given by Equation 3. Each hypothesis is a prefix-suffix pair. We start with a single hypothesis: an empty prefix and suffix, represented by start- and end-of-sentence tokens. At a given timestep, each hypothesis is expanded by considering the distribution over possible actions: adding a token on the left, adding a token on the right, or joining. The k best continuations are kept based on their (joint) probabilities. Generation stops once all hypotheses are complete (i.e. the prefix and suffix are joined)."
        },
        {
            "heading": "4 Model Architecture",
            "text": "Our architecture (Figure 1) is based on the character-level transformer (Wu et al., 2021), which has proven useful for morphological inflection. First, the input sequence x is encoded with a typical Transformer encoder; for the inflection task, this consists of the lemma (tokenized by character) concatenated with a separator token and set of tags.\nGiven a prefix \u2212\u2192y \u2264i and suffix\u2190\u2212y \u2264j (as well as the encoder output), the decoder must produce each direction\u2019s token probabilities, the join probability, and the order probability. We construct the input to the decoder by concatenating the prefix and suffix tokens with some special classification tokens:\n\u27e8cJ , cO,\u2212\u2192y 1, ...,\u2212\u2192y i, cL2R, cR2L,\u2190\u2212y j , ...,\u2190\u2212y 1\u27e9\nThe tokens cJ , cO, cL2R, and cR2L are special classification tokens that serve a purpose similar to the CLS embedding in BERT (Devlin et al., 2019). We feed this input to a Transformer decoder as follows:\nsJ , sO, ..., sL2R, sR2L, ... = Decoder(\u27e8\u00b7 \u00b7 \u00b7 \u27e9)\nThese vectors are fed through their own linear layers and softmax, giving the desired probabilities:\nP (join | \u2212\u2192y \u2264i,\u2190\u2212y \u2264j) = Softmax(sOV )\nP (order | \u2212\u2192y \u2264i,\u2190\u2212y \u2264j) = Softmax(sJU)\nP (\u2212\u2192y i | \u2212\u2192y \u2264i,\u2190\u2212y \u2264j) = Softmax(sJ \u2212\u2192 W ) P (\u2190\u2212y j | \u2212\u2192y \u2264i,\u2190\u2212y \u2264j) = Softmax(sJ \u2190\u2212 W )\nSince this architecture does have cross-attention between the prefix and suffix, the decoder hidden states for each prefix-suffix pair must be recomputed at each timestep to allow for DP (see \u00a73.3)."
        },
        {
            "heading": "5 Experimental Setup",
            "text": "Datasets. We experiment with inflection datasets for all 27 languages (spanning 9 families) from the SIGMORPHON 2023 shared task (Goldman et al., 2023). Each language has 10,000 training and 1,000 validation and test examples, and no lemma occurs in more than one of these partitions. We also show results on the 20 \u201clarge\u201d languages from the SIGMORPHON 2022 shared task (Kodner et al., 2022), which has a very different sampling of examples in the train and test sets. A list of all languages can be found in Appendix A.\nTokenization. Both the lemma and output form are split by character; the tags are split by semicolon. For the 2023 shared task, where the tags are \u201clayered\u201d (Guriel et al., 2022), we also treat each open and closed parenthesis as a token. Appendix B describes the treatment of unknown characters.\nModel hyperparameters. Our models are implemented in fairseq (Ott et al., 2019). We experiment with small, medium, and large model sizes (ranging from \u223c240k to \u223c7.3M parameters). For each language, we select a model size based on the L2R and R2L unidirectional accuracies; this procedure is detailed in Appendix A.\nThe only additional parameters in our bidirectional model come from the embeddings for the 4 classification tokens (described in \u00a74); hence, our unidirectional and bidirectional models have roughly the same number of parameters.\nTraining. We use a batch size of 800, an Adam optimizer (\u03b21 = 0.9, \u03b22 = 0.98), dropout of 0.3, and an inverse square root scheduler with initial learning rate 1e\u221207. Training is halted if validation accuracy does not improve for 7,500 steps. All validation accuracies are reported in Appepndix A.\nInference. Decoding maximizes joint probability P (y,o|x) using the beam search algorithm of \u00a73.5 with width 5. In some experiments, we rerank the 5 best candidates according to their marginal probability P (y|x), which can be calculated with dynamic programming (\u00a73.2).\nModels. We experiment with the following models (see Appendices D and F for more variants):\n\u2022 L2R & R2L: Standard unidirectional transformer baselines, trained with the loss given in Equations 1 and 2.\n\u2022 BL2: A naive \u201cbidirectional\u201d baseline that returns either the best L2R or R2L hypothesis based on which has a higher probability.\n\u2022 xH & MML: Our bidirectional transformer (\u00a74) trained under the cross-entropy or MML loss of \u00a73.4, and decoded under P (y,o|x).\n\u2022 xH-Rerank & MML-Rerank: These variants rerank the 5 candidates returned by beam search of the xH and MML models according to their marginal probability P (y|x).\n\u2022 BL2-xH & BL2-MML: These methods select the best L2R or R2L candidate, based on which has higher marginal probability under the xH or MML model."
        },
        {
            "heading": "6 Empirical Results",
            "text": ""
        },
        {
            "heading": "6.1 Comparison of Methods",
            "text": "Accuracies averaged over languages are shown in Table 1; results by language are in Appendix D.\nBaselines. BL2, which selects the higher probability among the L2R and R2L hypotheses, improves by more than 2.3 points in average accuracy over the best unidirectional model. This simple scheme serves as an improved baseline against which to compare our fully bidirectional models.\nxH & MML. Our bidirectional xH model is clearly more effective than all baselines, having a statistically significant degradation in accuracy on only 3 languages. The MML method is far less effective, beating L2R and R2L but not BL2. MML may suffer from a discrepancy between training and inference, since inference optimizes joint probability while training optimizes likelihood.\nxH- & MML-Rerank. Reranking according to marginal probability generally improves both bidirectional models. xH-Rerank is the best method overall, beating BL2 by over 1.75 points in average accuracy. MML-Rerank is better than either unidirectional model but still underperforms BL2.\nBL2-xH & BL2-MML. Selecting the best L2R\nor R2L hypothesis based on marginal probability under xH or MML is very effective. Both of these methods improve over BL2, which chooses between the same options based on unidirectional probability. BL2-xH stands out by not having a statistically significant degradation on any language.\nComparison with Prior SOTA. Goldman et al. (2023) presents the results of seven other systems submitted to the task; of these, five are from other universities and two are baselines provided by the organizers. The best of these systems is the neural baseline (a unidirectional transformer), which achieves an average accuracy of 81.6 points. Our best system, xH-Rerank, has an accuracy of 84.38 points, achieving an improvement of 2.7 points."
        },
        {
            "heading": "6.2 Improvement by Language",
            "text": "Table 1 shows that the best methods are xH-Rerank (by average accuracy) and BL2-xH (improves upon BL2 on the most languages). Figure 3 illustrates this by showing the difference in accuracy between each of these methods and the best baseline BL2.\nThe plots show that accuracy difference with BL2 has a higher range for xH-Rerank (\u22122.6% to 8.7%) than for BL2-xH (\u22120.5% to 5.8%). This is because xH-Rerank has the ability to generate new hypotheses, whereas BL2-xH simply discriminates between the same two hypotheses as BL2."
        },
        {
            "heading": "7 Analysis of Results",
            "text": ""
        },
        {
            "heading": "7.1 Length of Output Forms",
            "text": "Figure 2 shows the accuracies by output form length for BL2 and our best method xH-Rerank. xH-Rerank outperforms the baseline at every length (except 10), but especially excels for longer outputs (\u2265 16 characters). This may be due to the bidirectional model\u2019s decreased risk of \u201csnowballing\u201d: it can delay the prediction of an uncertain token by\ngenerating on the opposite side first, a property not shared with unidirectional models."
        },
        {
            "heading": "7.2 How does generation order compare with the morphology of a word?",
            "text": "In this section we consider only forms that can be classified morphologically as prefix-only (e.g. will |walk) or suffix-only (e.g. walk|ed), because these words have an obvious split point. Ideally, the bidirectional model will exhibit the desired split point by decoding the left and right sides of the form from their respective directions.\nWe first classify all inflected forms in the test set as suffix-only, prefix-only, or neither. We do this by aligning each lemma-form pair using Levenshtein distance and considering the longest common substring that has length of at least 3 to be the stem.6 If the inflected form only has an affix attached to the stem, then it is classified as prefix-only or suffixonly; otherwise, it is considered neither.7\n6For Japanese, we allow the stem to be of length 1 due to the prevalence of Kanji characters in the dataset.\n7This heuristic approach is likely to work well on examples without infixes or phonetic alternations that obscure the stem. A potential drawback is that it is based on individual lemmaform pairs; a probabilistic method that collects evidence from other examples may be beneficial. However, we feel that the interpretability of this approach as well as qualitative analysis supporting its efficacy makes it sufficient for our study.\nFigure 4 shows the percentage of words that are prefix-only, suffix-only, or neither for each language. Most languages favor suffix-only inflections, although Swahili strongly prefers prefixes and several other languages have a high proportion of words without a clear affix.\nFinally, Figure 5 shows the percentage of words with a clear affix on which each bidirectional model has the correct analysis. A correct analysis occurs when the model joins the left and right sequences at the correct split point and returns the correct word.\nIt is immediately obvious that the MML models tend to exhibit the correct analysis, while the xH models generally have the wrong analysis. This make sense because MML learns the latent order-\ning variable, unlike cross-entropy. Despite MML\u2019s success at learning this morphology, it tends to have lower accuracy than xH; we explore this by breaking down accuracy by word type in Figure 6.\nLearning the ordering seems to be harmful when there is no obvious affix: compared with BL2, MML barely drops in accuracy on prefix- and suffix-only forms but degrades greatly when there is no clear split. The xH model, which does not learn ordering, improves in all categories.\nWe conclude that MML models better reflect the stem-affix split than cross-entropy models but have lower accuracy. Improving the performance of MML models while maintaining their linguistic awareness is a promising direction for future work."
        },
        {
            "heading": "7.3 Ablation Study: Does bidirectional decoding help?",
            "text": "In this section, we analyze to what extent the bidirectional models\u2019 improvement is due to their ability to produce tokens from both sides and meet at any position. To this end, we force our trained xH and MML models to decode in a fully L2R or R2L manner by setting the log probabilities of tokens in the opposite direction to \u2212\u221e at inference time. The results are shown in Table 4.\nThe bidirectional models perform poorly when not permitted to decode from both sides. This is particularly detrimental for the MML model, which is expected as the marginalized training loss enables the model to assign low probabilities to some orderings. Clearly, our MML model does not favor unidirectional orderings.\nThe xH model, on the other hand, does not suffer\nas much from unidirectional decoding. Since it was trained to treat all orderings equally, we would expect it to do reasonably well on any given ordering. Nonetheless, it still drops by about 7 points for L2R decoding and about 13 points for R2L decoding. This shows that the full bidirectional generation procedure is crucial to the success of this model."
        },
        {
            "heading": "7.4 Results on 2022 Shared Task",
            "text": "We also train our bidirectional cross-entropy model on the 2022 SIGMORPHON inflection task (Kodner et al., 2022), which, unlike the 2023 data, does have lemmas that occur in both the train and test sets. The results are shown in Table 2. All of our methods (including the baselines) outperform the best submitted system (Yang et al., 2022) on the 2022 data; our best method BL2-xH improves by over 4.7 points in average accuracy.\nHowever, only BL2-xH outperforms the baseline BL2 (barely), which is in stark contrast to the 2023 task, where all cross-entropy-based methods beat the baseline considerably. To make the comparison between the years more fair, we evaluate the 2022 models only on lemmas in the test set that did not occur in training. Again, only BL2-xH outperforms the baseline, this time by a wider margin; xH and xH-Rerank still underperform.\nWe posit that this discrepancy is likely due to the considerably different properties of the 2022 and 2023 datasets, which are shown in Table 3. The 2023 languages have far fewer unique lemmas and have many more forms per lemma. Hence, it seems that our bidirectional model improves much more compared with the baseline when there are fewer but more \u201ccomplete\u201d paradigms.\nThis investigation shows that the performance of inflection models depends substantially on the data sampling, which is not always controlled for. Kodner et al. (2023) makes progress on this matter, but\ndoes not explicitly examine paradigm \u201ccompleteness\u201d, which should be a focus in future studies."
        },
        {
            "heading": "8 Conclusion",
            "text": "We have proposed a novel framework for bidirectional decoding that allows a model to choose the generation order for each sequence, a major difference from previous work. Further, our method enables an efficient dynamic programming algorithm for training, which arises due to an independence assumption that can be built into our transformer-based architecture. We also present a simple beam-search algorithm for decoding, the outputs of which can optionally be reranked using the likelihood calculation. Our model beats SOTA on both the 2022 and 2023 shared tasks without resorting to data augmentation. Further investigations show that our model is especially effective on longer output words and can implicitly learn the morpheme boundaries of output sequences.\nThere are several avenues for future research. One open question is the extent to which data augmentation can improve accuracy. We also leave open the opportunity to explore our bidirectional framework on other sequence tasks, such as machine translation, grapheme-to-phoneme conversion, and named-entity transliteration. Various other architectures could also be investigated, such as the bidirectional attention mechanism of Zhou et al. (2019b) or non-transformer based approaches. Finally, given the effectiveness of MML reranking, it could be worthwhile to explore efficient approaches to decode using marginal probability."
        },
        {
            "heading": "9 Limitations",
            "text": "We acknowledge several limitations to our work. For one, we only demonstrate experiments on the\ninflection task, which is fairly straightforward in some ways: there is typically only one output for a given input (unlike translation, for example), and a large part of the output is copied from the input. It would be informative to test the efficacy of our bidirectional framework on more diverse generation tasks, such as translation or question-answering.\nFrom a practical standpoint, the most serious limitation is that, in order to use dynamic programming, the model architecture cannot be trained with a causal mask: all hidden states must be recomputed at each timestep. Further, our xH and MML schemes are quadratic in sequence length. These two properties cause the training time of our bidirectional method to be O(|y|4) in runtime rather than O(|y|2) (like the standard transformer).8 Alleviating these constraints would enable a wider variety of experiments on tasks with longer sequences."
        },
        {
            "heading": "Acknowledgements",
            "text": "This work utilizes resources supported by the National Science Foundation\u2019s Major Research Instrumentation program, grant #1725729, as well as the University of Illinois at Urbana-Champaign. In particular, we made significant use of the HAL computer system (Kindratenko et al., 2020). We would also like to acknowledge Weights & Biases (Biewald, 2020), which we utilized to manage our experiments."
        },
        {
            "heading": "A Datasets, Hyperparameter Tuning, & Validation Accuracies",
            "text": "The languages in the SIGMORPHON 2022 and 2023 datasets are listed in Tables 7 and 8. We experiment with small, medium, and large model sizes for each language, whose configurations and approximate number of parameters can be found in Table 5:\nFor each language, we train L2R and R2L models (with random initialization) for each hyperparameter size (a total of 6 models per language), and select a size based on the average of the L2R and R2L validation accuracies. The model sizes chosen for each language, along with each language\u2019s validation accuracies, are reported in Tables 13 and 14.\nNote that the number of parameters vary slightly among languages due to different vocabulary sizes (i.e. number of unique characters in the training set), and the bidirectional models also have a small number of extra parameters due to the additional classification tokens described in \u00a74."
        },
        {
            "heading": "B Handling Unknown Characters",
            "text": "If an unknown character is encountered in a lemma at test time, then a special UNK character is used; however, this character is not explicitly trained. If an UNK character is predicted by the model, then we replace it with the first (leftmost) unknown character in the lemma; if no such character exists then it is ignored.\nWe adopt a special scheme for Japanese, which has a very high number of unknown characters. All characters that occur fewer than 100 times in the training set are considered \u201cunknown\u201d. If a lemma has n unknown tokens, then these are replaced with UNK1, ..., UNKn; the corresponding tokens in the inflected form are replaced as well. In this way, the model can learn to copy rare or unknown characters to their appropriate locations in the output. At test time, each predicted unknown token is replaced with its corresponding character in the lemma."
        },
        {
            "heading": "C Tempering the Order Distribution at Train Time",
            "text": "Initial empirical results showed that training with MML loss caused the model to quickly reach a \u201cdegenerate\u201d state, where every sequence was decoded in the same direction. To encourage the model to explore different orderings at an early stage, we temper the order probabilities over a warmup period. The temperature is degraded from initial temperature \u03c40 to 1 over a period of W steps as follows:\n\u03c4n = \u03c40 \u2212 1 W a (W \u2212 n)a + 1\nThe parameter a controls how fast the shift occurs, and n corresponds to the training step. This temperature is applied to the softmax of order probabilities for the first W steps of training.\nIn our experiments, we set W = 4, 000, \u03c40 = 50 and a = 2."
        },
        {
            "heading": "D All Results",
            "text": "The accuracies for all languages in our study are shown in Table 9 (2023 data) and Table 10 (2022 data). These tables also display L2RRerank (which reranks the 5 candidates from the L2R model\u2019s beam search under the cross-entropy or MML model), R2L-Rerank, and (L2R+R2L)Rerank (which reranks the 10 candidates returned from the L2R and R2L\u2019s beam search under the cross-entropy or MML model)."
        },
        {
            "heading": "E Oracle Scores",
            "text": "Table 11 shows the oracle score for each method; this gives an upper bound for choosing among a set of hypotheses. We see that both xH-Rerank and BL2-Rerank approach their respective bounds: the average accuracy for xH-Rerank is within 1 point of its oracle score, and the average accuracy for BL2-xH is within 2 points of its oracle score."
        },
        {
            "heading": "F Cross-entropy with Random Path (xH-Rand)",
            "text": "The cross-entropy loss presented in \u00a73.4 requires enumerating all O(|y|2) prefix-suffix pairs. Here, we propose an O(|y|) variant in which the join loss is averaged over a random set of prefix-suffix pairs for each word. Specifically, the set S is defined such that there is only one (i, j) pair for each 1 \u2264 k \u2264 |y| where i + j = k. Otherwise, this loss LxH-Rand(\u03b8) is the same as the cross-entropy loss of \u00a73.4. Since this loss has an O(|y|) runtime, it has the same complexity as a standard unidirectional loss (assuming all local probabilities take constant time to compute).\nTable 12 compares the accuracies of this model with the other bidirectional variants discussed in \u00a76. Reranking xH-Rand is slightly better than not reranking, and this performs well: its average accuracy is almost 1 percentage point higher than BL2 and it improves on 15/27 languages. xH-Rand is better than MML but not as good as xH. Nonetheless, its faster runtime and competitive performance makes this a useful method."
        },
        {
            "heading": "G Additional Results",
            "text": "G.1 Accuracy by Length Figure 2 in \u00a77.1 compares the accuracy of our bidirectional method xH-Rerank with that of the baseline BL2 by the length of the output form. Figure 7 shows a similar comparison for BL2-xH (our other best method) with BL2; consistent with the analysis of \u00a76.2, there is less of a difference between these methods, but BL2-xH does equal or outperform BL2 at all lengths.\nFigure 8 shows the distribution of output form length across all languages.\nG.2 Accuracy by Part-of-Speech Figures 10 and 9 compare the accuracies of xHRerank and BL2-xH (our best bidirectional methods) with the accuracy of BL2 by part-of-speech. We see that xH-Rerank maintains or improves accuracy over BL2 in all categories except V.MSDR (masdars), and BL2-xH maintains or improves accuracy in all categories except V.MSDR and V.PTCP (participles). These categories make up a small fraction of the data; this can be seen in Figure 11, which shows the distribution of part-of-speech categories across all languages.\nG.3 What orderings does each method prefer?\nIn this section, we investigate the ordering preferences for each method: does a model prefer to decode words entirely in the L2R or R2L direction, or partially in each direction? These results can be seen for each language in Figure 12.\nBoth the xH and MML methods have a strong tendency to decode words partially in each di-\nrection; however, MML models clearly have a higher proportion of words decoded from both directions than their xH counterparts. Out of the words decoded entirely in one direction, the xH model shows a slight preference for R2L generations, though most languages have words decoded from both directions. On the other hand, for the MML model, no language shows a preference for R2L generations over L2R generations; in fact, R2L generations are extremely rare for the MML models.\nG.4 Empirical Inference Times\nGiven that our bidirectional model must recompute previous hidden states at each timestep during inference (see \u00a74), we wish to compare the empirical slowdown in decoding for our bidirectional models compared with unidirectional models. The average number of seconds taken to decode 50 examples is shown in Table 6.\nRecomputing hidden states at each step slows\ndown inference by a factor of about 3. However, in practice, we barely notice the difference on this task, as the test sets have only 1,000 examples each. Given the strong outperformance of the bidirectional methods over the unidirectional baselines (and even over the naive bidirectional baseline BL2), one must therefore make a tradeoff between time and performance.\nL anguage\nL inguistic Inform ation\nL anguage C\node L\n2R TestA ccuracies R 2L TestA ccuracies M\nodelSize C\nhosen Fam ily G enus Sm all M edium L arge Sm all M edium L arge\nG ulfA rabic A fro-A siatic\nSem itic\nafb 75.20\n74.70 75.10\n78.10 76.10\n75.50 sm all A m haric A fro-A siatic Sem itic am h 86.20 84.40 83.50 82.80 89.30 83.20 m edium E gyptian A rabic A fro-A siatic Sem itic arz 87.20 89.60 87.80 88.10 87.30 86.70 sm all B elarusian Indo-E uropean Slavic bel 71.70 73.00 70.30 68.80 74.80 70.10 large D anish Indo-E uropean G erm anic dan 87.70 88.40 87.10 86.20 87.80 86.50 m edium G erm an Indo-E uropean G erm anic deu 80.80 75.70 73.10 76.50 77.10 78.00 large E nglish Indo-E uropean G erm anic eng 94.60 94.50 94.80 93.80 94.00 93.20 m edium Finnish U ralic Finnic fin 81.60 78.30 76.90 72.60 74.00 72.80 m edium French Indo-E uropean R om ance fra 63.70 65.10 57.20 67.70 59.50 50.10 sm all A ncientG reek Indo-E uropean H ellenic grc 49.40 53.20 50.80 47.50 39.10 34.80 m edium H ebrew A fro-A siatic Sem itic heb 87.41 93.25 91.14 88.22 90.33 87.41 large H ebrew (U nvocalized) A fro-A siatic Sem itic hebu /heb_unvoc 79.40 78.50 76.50 71.40 74.10 73.40 m edium H ungarian U ralic U gric hun 77.70 77.40 73.30 66.10 71.10 70.40 sm all A rm enian Indo-E uropean A rm enian hye 82.00 86.00 84.00 86.40 76.90 65.70 sm all Italian Indo-E uropean R om ance ita 89.30 82.90 91.40 93.90 78.60 79.80 sm all Japanese Japonic \u2212 jap 93.10 93.80 91.90 89.60 91.00 92.60 m edium G eorgian K artvelian K arto-Z an kat 79.70 76.30 76.60 79.50 82.00 69.00 sm all K haling Sino-Tibetan K iranti klr 98.30 99.40 95.80 95.60 98.30 98.30 m edium M acedonian Indo-E uropean Slavic m kd 90.90 89.70 89.70 89.40 92.00 88.80 m edium N avajo N a-D en\u00e9 Southern A thabaskan nav 53.70 53.40 45.60 48.90 53.30 52.60 sm all R ussian Indo-E uropean Slavic rus 82.10 88.20 84.00 84.90 86.00 80.10 sm all Sanskrit Indo-E uropean Indic san 61.50 52.40 41.50 60.70 44.10 44.00 sm all Sam i U ralic Finnic sm e 64.10 63.40 56.50 61.10 65.30 56.40 m edium Spanish Indo-E uropean R om ance spa 90.40 90.30 88.90 93.50 91.20 87.50 m edium A lbanian Indo-E uropean A lbanian sqi 82.50 85.00 80.00 88.30 84.40 75.00 m edium Sw ahili N iger-C ongo B antu sw a 89.00 92.70 86.90 92.90 92.90 92.40 m edium Turkish Turkic O ghuz tur 88.80 88.80 89.10 87.40 83.20 80.00 sm all\nTable 7: 2023 D atasetInform ation.Inform ation on each language in the 2023 dataset,including language fam ily and genus,baseline accuracies on test set,and m odelsize chosen (based on validation accuracies).\nL anguage\nL inguistic Inform ation\nL anguage C\node L\n2R TestA ccuracies R 2L TestA ccuracies M\nodelSize C\nhosen Fam ily G enus Sm all M edium L arge Sm all M edium L arge\nO ld E nglish\nIndo-E uropean\nG erm anic ang\n56.78 59.93\n62.06 60.49\n63.03 64.55 large A rabic A fro-A siatic Sem itic ara 77.74 76.94 77.49 77.84 77.59 77.24 sm all A ssam ese Indo-E uropean Indic asm 81.46 80.75 81.11 74.42 84.87 83.57 m edium E venki Tungusic N orthern Tungusic evn 56.11 56.05 54.68 57.37 54.79 52.09 sm all G othic Indo-E uropean G erm anic got 72.52 73.47 73.62 70.11 68.51 74.77 large H ebrew A fro-A siatic Sem itic heb 47.90 49.50 49.35 52.70 50.55 53.10 large H ungarian U ralic U gric hun 68.20 78.40 75.80 72.65 76.40 75.80 m edium A rm enian Indo-E uropean A rm enian hye 89.90 91.90 91.15 90.15 92.90 92.70 m edium G eorgian K artvelian K arto-Z an kat 84.15 86.10 84.50 83.85 88.15 88.95 large K azakh Turkic K ipchak kaz 64.14 64.34 62.99 62.54 69.11 68.15 m edium\nK halkha M ongolian\nM ongolic\n\u2212 khk\n46.26 48.59\n48.69 46.97\n48.59 48.43\nm edium\nK orean\nK oreanic\n\u2212 kor\n57.13 57.18\n57.54 57.23\n58.76 57.64 large K arelian U ralic Finnic krl 65.18 71.49 67.94 70.79 70.89 72.39 m edium L udic U ralic Finnic lud 63.16 74.14 68.27 72.32 78.34 56.98 m edium O ld N orse Indo-E uropean G erm anic non 82.92 84.43 84.73 82.92 82.32 85.89 large Polish Indo-E uropean Slavic pol 90.10 90.40 88.95 89.30 89.15 89.90 m edium Pom ak Indo-E uropean Slavic pom a 66.98 65.68 67.83 66.88 64.23 67.88 large Slovak Indo-E uropean Slavic slk 91.90 92.55 93.25 93.25 94.10 93.45 large Turkish Turkic O ghuz tur 94.15 93.85 93.70 93.65 95.40 92.90 m edium Veps U ralic Finnic vep 60.91 61.26 63.37 60.56 60.41 62.57 large\nTable 8: 2022 D atasetInform ation.Inform ation on each language in the 2022 dataset,including language fam ily and genus,baseline accuracies on test set,and m odelsize chosen (based on validation accuracies).O nly large datasets (7,000 train exam ples)are used.\nM odel Size B\naselines Standalone\nxH -R erank M M L -R erank\nB L D iscrim inator L 2R -R erank R 2L -R erank (L 2R +R 2L )-R erank L 2R R 2L B L 2 xH M M L xH M M L xH M M L xH M M L xH M M L xH M M L xH M M L\nafb sm all 75.20 78.10 80.70\n84.10 \u2217 78.70 \u2217\n84.60 \u2217 84.90 \u2217\n81.50 79.20\n82.20 \u2217 80.30 80.20\n78.00 \u2217\n80.00 76.90\n\u2217 82.90\n\u2217 79.20\nam h\nm edium\n84.40 89.30 88.90 88.90\n83.40 \u2217\n88.60 87.90\n85.90 \u2217 83.40 \u2217\n90.60 \u2217 87.30 \u2217\n85.80 \u2217 82.90 \u2217\n89.30 84.70\n\u2217 89.10\n83.60 \u2217\narz sm all 87.20 88.10 89.20\n89.10 87.50\n88.70 88.90\n87.30 \u2217 87.40 \u2217\n88.70 89.10\n89.20 88.50\n88.30 89.20\n89.10 88.90\nbel large\n70.30 70.10 73.50 72.90\n72.80 72.90\n73.20 74.00\n72.90 74.70\n74.40 71.80\n73.60 71.60\n73.80 72.10 74.10 dan m edium 88.40 87.80 88.80 86.50 \u2217 83.60 \u2217 87.50 86.30 \u2217 85.00 \u2217 83.60 \u2217 89.50 89.60 89.30 85.70 \u2217 86.80 \u2217 85.70 \u2217 88.30 85.40 \u2217 deu large 73.10 78.00 79.70 80.20 81.10 79.70 80.80 80.80 81.00 79.70 81.00 \u2217 74.80 \u2217 75.70 \u2217 80.30 81.50 \u2217 80.20 81.90 \u2217 eng m edium 94.50 94.00 95.60 95.70 95.80 95.70 96.20 96.00 95.80 95.90 95.80 95.50 95.60 94.90 94.80 95.50 95.80 fin m edium 78.30 74.00 79.20 83.60 \u2217 77.70 82.70 \u2217 83.90 \u2217 77.70 77.90 78.80 78.50 81.20 \u2217 81.00 78.10 78.70 81.20 \u2217 79.80 fra sm all 63.70 67.70 69.30 71.70 71.60 72.90 \u2217 73.50 \u2217 74.90 \u2217 71.50 74.70 \u2217 74.40 \u2217 72.00 \u2217 70.70 72.70 \u2217 74.00 \u2217 74.80 \u2217 75.00 \u2217 grc m edium 53.20 39.10 48.90 56.00 \u2217 53.50 \u2217 56.00 \u2217 55.70 \u2217 53.60 \u2217 53.30 \u2217 53.70 \u2217 55.10 \u2217 54.60 \u2217 55.80 \u2217 41.50 \u2217 43.10 \u2217 56.20 \u2217 59.30 \u2217 heb large 91.14 87.41 92.95 92.45 84.09 \u2217 92.45 92.25 86.10 \u2217 84.09 \u2217 93.55 91.04 \u2217 91.74 90.53 \u2217 88.62 \u2217 84.99 \u2217 91.64 86.20 \u2217 hebu m edium 78.50 74.10 77.30 83.70 \u2217 75.00 \u2217 83.60 \u2217 83.60 \u2217 81.50 \u2217 75.00 \u2217 79.30 \u2217 78.20 79.30 \u2217 77.50 79.40 \u2217 74.20 \u2217 82.40 \u2217 76.20 hun sm all 77.70 66.10 76.30 84.30 \u2217 79.30 \u2217 85.00 \u2217 85.10 \u2217 82.30 \u2217 80.10 \u2217 79.80 \u2217 81.20 \u2217 81.70 \u2217 79.10 \u2217 75.20 76.10 81.40 \u2217 81.10 \u2217 hye sm all 82.00 86.40 88.40 94.20 \u2217 86.50 94.30 \u2217 94.20 \u2217 88.40 86.20 91.40 \u2217 91.10 \u2217 85.10 \u2217 81.50 \u2217 88.80 87.60 92.70 \u2217 88.00 ita sm all 89.30 93.90 95.80 94.40 92.70 \u2217 93.70 \u2217 92.10 \u2217 95.30 92.70 \u2217 97.20 \u2217 96.40 90.10 \u2217 90.60 \u2217 93.90 \u2217 95.40 94.30 \u2217 94.70 jap m edium 93.80 91.00 92.80 94.90 \u2217 92.30 94.90 \u2217 94.20 93.50 92.10 94.20 \u2217 93.40 94.80 \u2217 93.00 94.40 \u2217 92.20 94.80 \u2217 92.40 kat sm all 79.70 79.50 84.10 81.30 \u2217 81.40 \u2217 82.90 82.60 83.50 81.10 \u2217 84.70 84.60 82.70 \u2217 84.30 81.40 \u2217 81.10 \u2217 83.80 83.70 klr m edium 99.40 98.30 99.40 99.40 99.40 99.40 99.40 99.40 99.40 99.40 99.40 99.40 99.40 99.10 99.00 99.40 99.40 m kd m edium 89.70 92.00 91.90 92.10 91.40 92.40 92.40 93.20 91.50 92.40 91.90 93.80 \u2217 92.00 93.20 92.70 92.80 91.90 nav sm all 53.70 48.90 54.00 55.10 57.10 \u2217 55.60 55.00 57.00 \u2217 57.00 \u2217 55.10 55.60 \u2217 54.90 56.40 \u2217 54.30 55.60 56.00 57.10 \u2217 rus sm all 82.10 84.90 87.40 84.20 \u2217 82.10 \u2217 85.50 \u2217 85.40 \u2217 84.70 \u2217 83.30 \u2217 87.30 87.30 83.30 \u2217 81.20 \u2217 86.30 86.00 86.30 84.30 \u2217 san sm all 61.50 60.70 63.30 67.70 \u2217 59.60 \u2217 65.90 66.40 \u2217 66.10 \u2217 60.60 69.10 \u2217 68.80 \u2217 67.10 \u2217 65.50 61.90 60.50 \u2217 66.60 \u2217 65.20 sm e m edium 63.40 65.30 69.90 67.40 75.60 \u2217 67.30 69.00 70.00 75.20 \u2217 71.80 \u2217 70.90 66.60 \u2217 70.00 67.90 71.10 69.90 75.00 \u2217 spa m edium 90.30 91.20 90.90 93.20 \u2217 93.40 \u2217 93.30 \u2217 93.30 \u2217 93.30 \u2217 93.50 \u2217 91.50 91.00 91.50 91.20 91.80 91.70 92.30 \u2217 92.00 \u2217 sqi m edium 85.00 84.40 87.60 91.00 \u2217 82.50 \u2217 91.90 \u2217 89.90 \u2217 82.80 \u2217 82.40 \u2217 88.60 \u2217 85.90 \u2217 89.30 85.80 \u2217 87.80 85.30 \u2217 90.30 \u2217 86.60 sw a m edium 92.70 92.90 93.10 96.60 \u2217 90.50 \u2217 96.60 \u2217 95.60 \u2217 91.40 \u2217 90.50 \u2217 93.10 93.00 93.10 92.80 93.30 92.90 93.50 93.00 tur sm all 88.80 87.40 90.90 94.00 \u2217 89.90 94.20 \u2217 93.40 \u2217 93.00 \u2217 89.90 91.00 90.30 92.40 \u2217 91.60 88.10 \u2217 86.10 \u2217 93.30 \u2217 91.30 Average 80.26 79.65 82.59 84.25 81.43 84.38 84.26 82.90 81.50 84.00 83.54 82.64 81.85 81.81 81.29 84.11 83.00 N um ber \u2265 19/27 9/27 18/27 18/27 17/27 9/27 24/27 18/27 16/27 16/27 11/27 8/27 19/27 14/27 N um ber (p \u2264 0 .0 5) > 12/27 5/27 12/27 12/27 8/27 5/27 12/27 7/27 9/27 3/27 3/27 2/27 12/27 7/27 N um ber (p \u2264 0 .0 5) = 12/27 11/27 13/27 12/27 12/27 12/27 15/27 17/27 11/27 15/27 18/27 15/27 14/27 16/27 N um ber (p \u2264 0 .0 5) < 3/27 11/27 2/27 3/27 7/27 10/27 0/27 3/27 7/27 9/27 6/27 10/27 1/27 4/27 Table 9: A llA ccuracies(2023 data).A num beris starred (*)ifitshow s a statistically significantdifference w ith the bestbaseline B L 2;a num beris colored in green ifitim proves overB L2 (regardless ofsignificance)using a paired perm utation test(Zm igrod etal.,2022);and a num berisbold ifitis the bestforthe language.\nM odel Size B\naselines B idirectional B aselinesR erank\nL 2R\nR 2L\nB L 2 xH\nxH -R erank B L 2-xH\nL 2R -R erank-xH\nR 2L -R erank-xH\n(L 2R +R 2L )-R erank-xH\nang large\n62.06 64.55\n65.26 60.89\n61.05 65.77\n62.77 64.20 63.53 ara sm all 77.74 77.84 79.45 76.19 77.09 79.20 78.70 77.59 78.45 asm m edium 80.75 84.87 85.73 83.42 83.22 87.44 83.87 83.67 85.18 evn sm all 56.11 57.37 60.70 56.86 56.86 61.10 56.51 58.00 58.35 got large 73.62 74.77 75.33 72.17 72.62 75.83 74.32 74.02 74.07 heb large 49.35 53.10 53.95 49.95 49.95 52.00 49.35 50.80 50.35 hun m edium 78.40 76.40 78.15 77.00 77.00 77.80 77.60 76.60 77.05 hye m edium 91.90 92.90 93.60 90.30 90.85 93.90 92.10 92.60 92.05 kat large 84.50 88.95 88.95 92.00 91.50 89.75 87.70 90.30 91.70 kaz m edium 64.34 69.11 70.36 65.70 66.70 70.96 63.19 69.46 69.01 khk m edium 48.59 48.59 48.94 48.94 48.99 48.99 48.79 48.94 48.99 kor large 57.54 57.64 59.11 57.64 57.69 59.93 57.59 58.81 58.81 krl m edium 71.49 70.89 73.40 64.38 65.93 72.34 69.59 68.99 68.49 lud m edium 74.14 78.34 82.19 64.37 63.82 80.62 70.34 77.13 71.26 non large 84.73 85.89 87.95 84.88 85.03 88.05 84.83 85.84 85.64 pol m edium 90.40 89.15 90.95 89.40 89.15 90.80 89.70 89.40 89.70\npom a\nlarge 67.83\n67.88 69.78\n67.88 67.93\n70.14 69.73\n68.83 69.03\nslk large\n93.25 93.45\n94.20 94.05\n94.00 94.90\n93.55 93.10 94.15 tur m edium 93.85 95.40 95.75 95.60 95.60 95.60 95.20 95.80 95.75 vep large 63.37 62.57 65.43 63.67 63.32 65.48 64.33 63.87 64.53 Average 73.20 74.48 75.96 72.76 72.91 76.03 73.49 74.40 74.30 N um ber \u2265 2/20 2/20 13/20 0/20 3/20 3/20\nTable 10: A llA ccuracies(2022 data).A num beris colored in green ifitim proves overB L 2,and a num berisbold ifitis the bestforthe language."
        }
    ],
    "title": "A Framework for Bidirectional Decoding: Case Study in Morphological Inflection",
    "year": 2023
}