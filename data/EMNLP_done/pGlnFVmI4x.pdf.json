{
    "abstractText": "This paper presents FlowSUM, a normalizing flows-based variational encoder-decoder framework for Transformer-based summarization. Our approach tackles two primary challenges in variational summarization: insufficient semantic information in latent representations and posterior collapse during training. To address these challenges, we employ normalizing flows to enable flexible latent posterior modeling, and we propose a controlled alternate aggressive training (CAAT) strategy with an improved gate mechanism. Experimental results show that FlowSUM significantly enhances the quality of generated summaries and unleashes the potential for knowledge distillation with minimal impact on inference time. Furthermore, we investigate the issue of posterior collapse in normalizing flows and analyze how the summary quality is affected by the training strategy, gate initialization, and the type and number of normalizing flows used, offering valuable insights for future research.",
    "authors": [
        {
            "affiliations": [],
            "name": "Yu Yang"
        },
        {
            "affiliations": [],
            "name": "Xiaotong Shen"
        }
    ],
    "id": "SP:2e6643e23dca96128029dea5032a6fc98957b415",
    "references": [
        {
            "authors": [
                "Vladimir I Bogachev."
            ],
            "title": "Measure Theory",
            "venue": "Springer.",
            "year": 2007
        },
        {
            "authors": [
                "Samuel R. Bowman",
                "Luke Vilnis",
                "Oriol Vinyals",
                "Andrew Dai",
                "Rafal Jozefowicz",
                "Samy Bengio."
            ],
            "title": "Generating sentences from a continuous space",
            "venue": "Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning, pages 10\u201321,",
            "year": 2016
        },
        {
            "authors": [
                "Arthur Brazinskas",
                "Mirella Lapata",
                "Ivan Titov."
            ],
            "title": "Unsupervised opinion summarization as copycatreview generation",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, pages",
            "year": 2020
        },
        {
            "authors": [
                "Xi Chen",
                "Diederik P. Kingma",
                "Tim Salimans",
                "Yan Duan",
                "Prafulla Dhariwal",
                "John Schulman",
                "Ilya Sutskever",
                "Pieter Abbeel."
            ],
            "title": "Variational lossy autoencoder",
            "venue": "5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April",
            "year": 2017
        },
        {
            "authors": [
                "Eric Chu",
                "Peter J. Liu."
            ],
            "title": "Meansum: A neural model for unsupervised multi-document abstractive summarization",
            "venue": "Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA,",
            "year": 2019
        },
        {
            "authors": [
                "Arman Cohan",
                "Franck Dernoncourt",
                "Doo Soon Kim",
                "Trung Bui",
                "Seokhwan Kim",
                "Walter Chang",
                "Nazli Goharian."
            ],
            "title": "A discourse-aware attention model for abstractive summarization of long documents",
            "venue": "Proceedings of the 2018 Conference of the North",
            "year": 2018
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
            "year": 2019
        },
        {
            "authors": [
                "Papamakarios."
            ],
            "title": "Neural spline flows",
            "venue": "Ad-",
            "year": 2019
        },
        {
            "authors": [
                "Dragomir Radev"
            ],
            "title": "Multi-news: A large-scale",
            "year": 2019
        },
        {
            "authors": [
                "Zhenglu Yang"
            ],
            "title": "Document summarization with",
            "year": 2020
        },
        {
            "authors": [
                "Zihao Fu",
                "Wai Lam",
                "Anthony Man-Cho So",
                "Bei Shi."
            ],
            "title": "A Theoretical Analysis of the Repetition Problem in Text Generation",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 12848\u201312856.",
            "year": 2021
        },
        {
            "authors": [
                "Mathieu Germain",
                "Karol Gregor",
                "Iain Murray",
                "Hugo Larochelle."
            ],
            "title": "MADE: masked autoencoder for distribution estimation",
            "venue": "Proceedings of the 32nd International Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015,",
            "year": 2015
        },
        {
            "authors": [
                "Bogdan Gliwa",
                "Iwona Mochol",
                "Maciej Biesek",
                "Aleksander Wawer."
            ],
            "title": "SAMSum corpus: A humanannotated dialogue dataset for abstractive summarization",
            "venue": "Proceedings of the 2nd Workshop on New Frontiers in Summarization, pages 70\u201379, Hong",
            "year": 2019
        },
        {
            "authors": [
                "Albert Gu",
                "\u00c7aglar G\u00fcl\u00e7ehre",
                "Thomas Paine",
                "Matt Hoffman",
                "Razvan Pascanu."
            ],
            "title": "Improving the gating mechanism of recurrent neural networks",
            "venue": "Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Vir-",
            "year": 2020
        },
        {
            "authors": [
                "Junxian He",
                "Daniel Spokoyny",
                "Graham Neubig",
                "Taylor Berg-Kirkpatrick."
            ],
            "title": "Lagging inference networks and posterior collapse in variational autoencoders",
            "venue": "7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA,",
            "year": 2019
        },
        {
            "authors": [
                "Karl Moritz Hermann",
                "Tom\u00e1s Kocisk\u00fd",
                "Edward Grefenstette",
                "Lasse Espeholt",
                "Will Kay",
                "Mustafa Suleyman",
                "Phil Blunsom."
            ],
            "title": "Teaching machines to read and comprehend",
            "venue": "Advances in Neural Information Processing Systems 28: Annual Conference on Neu-",
            "year": 2015
        },
        {
            "authors": [
                "Geoffrey Hinton",
                "Oriol Vinyals",
                "Jeff Dean."
            ],
            "title": "Distilling the knowledge in a neural network",
            "venue": "arXiv preprint arXiv:1503.02531.",
            "year": 2015
        },
        {
            "authors": [
                "Ari Holtzman",
                "Jan Buys",
                "Li Du",
                "Maxwell Forbes",
                "Yejin Choi."
            ],
            "title": "The curious case of neural text degeneration",
            "venue": "8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net.",
            "year": 2020
        },
        {
            "authors": [
                "Yoon Kim",
                "Sam Wiseman",
                "Andrew C. Miller",
                "David A. Sontag",
                "Alexander M. Rush."
            ],
            "title": "Semiamortized variational autoencoders",
            "venue": "Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsm\u00e4ssan, Stock-",
            "year": 2018
        },
        {
            "authors": [
                "Diederik P. Kingma",
                "Jimmy Ba."
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings.",
            "year": 2015
        },
        {
            "authors": [
                "Diederik P. Kingma",
                "Max Welling."
            ],
            "title": "Autoencoding variational bayes",
            "venue": "2nd International Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings.",
            "year": 2014
        },
        {
            "authors": [
                "Durk P Kingma",
                "Tim Salimans",
                "Rafal Jozefowicz",
                "Xi Chen",
                "Ilya Sutskever",
                "Max Welling."
            ],
            "title": "Improved variational inference with inverse autoregressive flow",
            "venue": "Advances in Neural Information Processing Systems, volume 29. Curran Associates,",
            "year": 2016
        },
        {
            "authors": [
                "Mike Lewis",
                "Yinhan Liu",
                "Naman Goyal",
                "Marjan Ghazvininejad",
                "Abdelrahman Mohamed",
                "Omer Levy",
                "Veselin Stoyanov",
                "Luke Zettlemoyer"
            ],
            "title": "BART: Denoising sequence-to-sequence pre-training for natural language",
            "year": 2020
        },
        {
            "authors": [
                "Chin-Yew Lin."
            ],
            "title": "ROUGE: A package for automatic evaluation of summaries",
            "venue": "Text Summarization Branches Out, pages 74\u201381, Barcelona, Spain. Association for Computational Linguistics.",
            "year": 2004
        },
        {
            "authors": [
                "Yang Liu",
                "Mirella Lapata."
            ],
            "title": "Text summarization with pretrained encoders",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing",
            "year": 2019
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Roberta: A robustly optimized bert pretraining approach",
            "venue": "arXiv preprint arXiv:1907.11692.",
            "year": 2019
        },
        {
            "authors": [
                "Tien-Ching Luo",
                "Jen-Tzung Chien."
            ],
            "title": "Variational dialogue generation with normalizing flows",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2021, Toronto, ON, Canada, June 6-11, 2021, pages 7778\u20137782. IEEE.",
            "year": 2021
        },
        {
            "authors": [
                "Shashi Narayan",
                "Shay B. Cohen",
                "Mirella Lapata."
            ],
            "title": "Don\u2019t give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Lan-",
            "year": 2018
        },
        {
            "authors": [
                "Thong Nguyen",
                "Anh Tuan Luu",
                "Truc Lu",
                "Tho Quan."
            ],
            "title": "Enriching and controlling global semantics for text summarization",
            "venue": "Proceedings of the 2021",
            "year": 2021
        },
        {
            "authors": [
                "George Papamakarios",
                "Iain Murray",
                "Theo Pavlakou."
            ],
            "title": "Masked autoregressive flow for density estimation",
            "venue": "Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017,",
            "year": 2017
        },
        {
            "authors": [
                "Romain Paulus",
                "Caiming Xiong",
                "Richard Socher."
            ],
            "title": "A Deep Reinforced Model for Abstractive Summarization",
            "venue": "6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference",
            "year": 2018
        },
        {
            "authors": [
                "Victor Prokhorov",
                "Ehsan Shareghi",
                "Yingzhen Li",
                "Mohammad Taher Pilehvar",
                "Nigel Collier."
            ],
            "title": "On the importance of the Kullback-Leibler divergence term in variational autoencoders for text generation",
            "venue": "Proceedings of the 3rd Workshop on Neural Gen-",
            "year": 2019
        },
        {
            "authors": [
                "Weizhen Qi",
                "Yu Yan",
                "Yeyun Gong",
                "Dayiheng Liu",
                "Nan Duan",
                "Jiusheng Chen",
                "Ruofei Zhang",
                "Ming Zhou."
            ],
            "title": "ProphetNet: Predicting future n-gram for sequence-to-SequencePre-training",
            "venue": "Findings of the Association for Computational Linguistics:",
            "year": 2020
        },
        {
            "authors": [
                "Alec Radford",
                "Jeff Wu",
                "Rewon Child",
                "David Luan",
                "Dario Amodei",
                "Ilya Sutskever."
            ],
            "title": "Language models are unsupervised multitask learners",
            "venue": "OpenAI Blog, 1(8):9.",
            "year": 2019
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J Liu."
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "The Journal of Machine Learning Research,",
            "year": 2020
        },
        {
            "authors": [
                "Marc\u2019Aurelio Ranzato",
                "Sumit Chopra",
                "Michael Auli",
                "Wojciech Zaremba"
            ],
            "title": "Sequence level training with recurrent neural networks",
            "venue": "In 4th International Conference on Learning Representations,",
            "year": 2016
        },
        {
            "authors": [
                "Nils Reimers",
                "Iryna Gurevych."
            ],
            "title": "SentenceBERT: Sentence embeddings using Siamese BERTnetworks",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natu-",
            "year": 2019
        },
        {
            "authors": [
                "Danilo Jimenez Rezende",
                "Shakir Mohamed."
            ],
            "title": "Variational inference with normalizing flows",
            "venue": "Proceedings of the 32nd International Conference on",
            "year": 2015
        },
        {
            "authors": [
                "Danilo Jimenez Rezende",
                "Shakir Mohamed",
                "Daan Wierstra."
            ],
            "title": "Stochastic backpropagation and approximate inference in deep generative models",
            "venue": "Proceedings of the 31st International Conference on Machine Learning, volume 32 of Proceedings of Ma-",
            "year": 2014
        },
        {
            "authors": [
                "Robin Rombach",
                "Andreas Blattmann",
                "Dominik Lorenz",
                "Patrick Esser",
                "Bj\u00f6rn Ommer."
            ],
            "title": "Highresolution image synthesis with latent diffusion models",
            "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans,",
            "year": 2022
        },
        {
            "authors": [
                "Sascha Rothe",
                "Shashi Narayan",
                "Aliaksei Severyn."
            ],
            "title": "Leveraging pre-trained checkpoints for sequence generation tasks",
            "venue": "Transactions of the Association for Computational Linguistics, 8:264\u2013280.",
            "year": 2020
        },
        {
            "authors": [
                "Raphael Schumann."
            ],
            "title": "Unsupervised abstractive sentence summarization using length controlled variational autoencoder",
            "venue": "CoRR, abs/1809.05233.",
            "year": 2018
        },
        {
            "authors": [
                "Abigail See",
                "Peter J. Liu",
                "Christopher D. Manning."
            ],
            "title": "Get to the point: Summarization with pointergenerator networks",
            "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1073\u2013",
            "year": 2017
        },
        {
            "authors": [
                "Stanislau Semeniuta",
                "Aliaksei Severyn",
                "Erhardt Barth."
            ],
            "title": "A hybrid convolutional variational autoencoder for text generation",
            "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 627\u2013637, Copenhagen,",
            "year": 2017
        },
        {
            "authors": [
                "Iulian Vlad Serban",
                "Alessandro Sordoni",
                "Ryan Lowe",
                "Laurent Charlin",
                "Joelle Pineau",
                "Aaron C. Courville",
                "Yoshua Bengio."
            ],
            "title": "A hierarchical latent variable encoder-decoder model for generating dialogues",
            "venue": "Proceedings of the Thirty-First AAAI Conference",
            "year": 2017
        },
        {
            "authors": [
                "Hendra Setiawan",
                "Matthias Sperber",
                "Udhyakumar Nallasamy",
                "Matthias Paulik."
            ],
            "title": "Variational neural machine translation with normalizing flows",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7771\u2013",
            "year": 2020
        },
        {
            "authors": [
                "Dinghan Shen",
                "Yizhe Zhang",
                "Ricardo Henao",
                "Qinliang Su",
                "Lawrence Carin."
            ],
            "title": "Deconvolutional latent-variable model for text sequence matching",
            "venue": "In",
            "year": 2018
        },
        {
            "authors": [
                "Xiaoyu Shen",
                "Hui Su",
                "Shuzi Niu",
                "Vera Demberg."
            ],
            "title": "Improving variational encoder-decoders in dialogue generation",
            "venue": "Proceedings of the ThirtySecond AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Arti-",
            "year": 2018
        },
        {
            "authors": [
                "Sam Shleifer",
                "Alexander M Rush."
            ],
            "title": "Pretrained summarization distillation",
            "venue": "arXiv preprint arXiv:2010.13002.",
            "year": 2020
        },
        {
            "authors": [
                "Kaitao Song",
                "Xu Tan",
                "Tao Qin",
                "Jianfeng Lu",
                "TieYan Liu."
            ],
            "title": "MASS: masked sequence to sequence pre-training for language generation",
            "venue": "Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach,",
            "year": 2019
        },
        {
            "authors": [
                "Jinsong Su",
                "Shan Wu",
                "Deyi Xiong",
                "Yaojie Lu",
                "Xianpei Han",
                "Biao Zhang."
            ],
            "title": "Variational recurrent neural machine translation",
            "venue": "Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications",
            "year": 2018
        },
        {
            "authors": [
                "Esteban G Tabak",
                "Cristina V Turner."
            ],
            "title": "A family of nonparametric density estimation algorithms",
            "venue": "Communications on Pure and Applied Mathematics, 66(2):145\u2013164.",
            "year": 2013
        },
        {
            "authors": [
                "Ilya O. Tolstikhin",
                "Olivier Bousquet",
                "Sylvain Gelly",
                "Bernhard Sch\u00f6lkopf."
            ],
            "title": "Wasserstein autoencoders",
            "venue": "6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track",
            "year": 2018
        },
        {
            "authors": [
                "Dustin Tran",
                "Keyon Vafa",
                "Kumar Krishna Agrawal",
                "Laurent Dinh",
                "Ben Poole."
            ],
            "title": "Discrete flows: Invertible generative models of discrete data",
            "venue": "Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Process-",
            "year": 2019
        },
        {
            "authors": [
                "Rianne van den Berg",
                "Leonard Hasenclever",
                "Jakub M. Tomczak",
                "Max Welling."
            ],
            "title": "Sylvester normalizing flows for variational inference",
            "venue": "Proceedings of the Thirty-Fourth Conference on Uncertainty in",
            "year": 2018
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N. Gomez",
                "Lukasz Kaiser",
                "Illia Polosukhin."
            ],
            "title": "Attention is all you need",
            "venue": "Advances in Neural Information Processing Systems 30: Annual Conference on Neural",
            "year": 2017
        },
        {
            "authors": [
                "Li Wang",
                "Junlin Yao",
                "Yunzhe Tao",
                "Li Zhong",
                "Wei Liu",
                "Qiang Du."
            ],
            "title": "A reinforced topicaware convolutional sequence-to-sequence model for abstractive text summarization",
            "venue": "arXiv preprint arXiv:1805.03616.",
            "year": 2018
        },
        {
            "authors": [
                "Wenlin Wang",
                "Zhe Gan",
                "Hongteng Xu",
                "Ruiyi Zhang",
                "Guoyin Wang",
                "Dinghan Shen",
                "Changyou Chen",
                "Lawrence Carin."
            ],
            "title": "Topic-guided variational auto-encoder for text generation",
            "venue": "Proceedings of the 2019 Conference of the North American Chap-",
            "year": 2019
        },
        {
            "authors": [
                "Zhengjue Wang",
                "Zhibin Duan",
                "Hao Zhang",
                "Chaojie Wang",
                "Long Tian",
                "Bo Chen",
                "Mingyuan Zhou."
            ],
            "title": "Friendly topic assistant for transformer based abstractive summarization",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural",
            "year": 2020
        },
        {
            "authors": [
                "Zichao Yang",
                "Zhiting Hu",
                "Ruslan Salakhutdinov",
                "Taylor Berg-Kirkpatrick."
            ],
            "title": "Improved variational autoencoders for text modeling using dilated convolutions",
            "venue": "Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Syd-",
            "year": 2017
        },
        {
            "authors": [
                "Biao Zhang",
                "Deyi Xiong",
                "Jinsong Su",
                "Hong Duan",
                "Min Zhang."
            ],
            "title": "Variational neural machine translation",
            "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 521\u2013530, Austin, Texas. Association for",
            "year": 2016
        },
        {
            "authors": [
                "Jingqing Zhang",
                "Yao Zhao",
                "Mohammad Saleh",
                "Peter J. Liu."
            ],
            "title": "PEGASUS: pre-training with extracted gap-sentences for abstractive summarization",
            "venue": "Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020,",
            "year": 2020
        },
        {
            "authors": [
                "Tianyi Zhang",
                "Varsha Kishore",
                "Felix Wu",
                "Kilian Q. Weinberger",
                "Yoav Artzi."
            ],
            "title": "Bertscore: Evaluating text generation with BERT",
            "venue": "8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30,",
            "year": 2020
        },
        {
            "authors": [
                "Shengjia Zhao",
                "Jiaming Song",
                "Stefano Ermon."
            ],
            "title": "Infovae: Information maximizing variational autoencoders",
            "venue": "arXiv preprint arXiv:1706.02262.",
            "year": 2017
        },
        {
            "authors": [
                "Chujie Zheng",
                "Kunpeng Zhang",
                "Harry Jiannan Wang",
                "Ling Fan",
                "Zhe Wang."
            ],
            "title": "Topic-guided abstractive text summarization: a joint learning approach",
            "venue": "arXiv preprint arXiv:2010.10323.",
            "year": 2020
        },
        {
            "authors": [
                "Chunting Zhou",
                "Graham Neubig."
            ],
            "title": "Morphological inflection generation with multi-space variational encoder-decoders",
            "venue": "Proceedings of the CoNLL SIGMORPHON 2017 Shared Task: Universal Morphological Reinflection, pages 58\u201365, Vancouver. As-",
            "year": 2017
        },
        {
            "authors": [
                "Yicheng Zou",
                "Lujun Zhao",
                "Yangyang Kang",
                "Jun Lin",
                "Minlong Peng",
                "Zhuoren Jiang",
                "Changlong Sun",
                "Qi Zhang",
                "Xuanjing Huang",
                "Xiaozhong Liu"
            ],
            "title": "Topic-oriented spoken dialogue summarization for customer service with saliency-aware topic",
            "year": 2021
        },
        {
            "authors": [
                "See"
            ],
            "title": "2017) and follow the text",
            "venue": "Multi-News (Fabbri et al.,",
            "year": 2020
        },
        {
            "authors": [
                "PEGASUS+Flow-NTM (Nguyen"
            ],
            "title": "2021) is a topic-aware model built on PEGASUS. It utilizes a Flow-based NTM and a contextualized gating mechanism to integrate topic information into the encoder and the decoder",
            "year": 2021
        },
        {
            "authors": [
                "Berg"
            ],
            "title": "2018) generalize the planar flows",
            "year": 2018
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Abstractive summarization (See et al., 2017; Paulus et al., 2018; Wang et al., 2018) aims to generate summaries by rephrasing or introducing novel words to capture the most salient information in the source text. Many abstractive summarization models (Liu and Lapata, 2019; Zhang et al., 2020a; Rothe et al., 2020; Raffel et al., 2020) are based on the Transformers architecture (Vaswani et al., 2017) and have consistently produced state-of-theart summarization quality. However, issues such as exposure bias (Ranzato et al., 2016; Qi et al., 2020), lack of text generation diversity (Holtzman et al., 2020), and insufficient capturing of semantic information (Reimers and Gurevych, 2019; Wang et al., 2020) remain.\nVariational models have gained increasing research interest (Zhang et al., 2016; Su et al., 2018; Wang et al., 2019; Fu et al., 2020) as they address\nthese issues by introducing uncertainty in predictions through learning a probability distribution over latent variables. A variational model enables diverse text generation (Du et al., 2022), smoother output spaces, and semantically meaningful latent codes (Wang et al., 2019) that guide the generation of coherent and informative summaries.\nNonetheless, existing variational models have not fully achieved the aforementioned desirable properties due to two main challenges. Firstly, the semantic information in the source text may possess a complex structure. However, since introducing latent variables complicates parameter estimation, many current models (Fu et al., 2020; Zheng et al., 2020) represent latent codes using a Gaussian distribution, which is insufficient for capturing the intricacies of the latent space and could potentially reduce model performance. To enrich latent distributions, researchers suggest replacing the highly restricted isotropic Gaussian with normalizing flows (Rezende and Mohamed, 2015). Normalizing flows can generate complex distributions while preserving density in an analytical form, and they have been integrated into variational autoencoder (VAE) (Kingma and Welling, 2014; Rezende et al., 2014) and variational encoder-decoder (VED) (Serban et al., 2017; Zhou and Neubig, 2017) frameworks to better approximate the latent posterior. This approach has found application in various domains, including text generation (Wang et al., 2019), neural machine translation (Setiawan et al., 2020), and dialogue generation (Luo and Chien, 2021). Despite this progress, the operating characteristics of normalizing flows on summarization tasks have yet to be investigated.\nSecondly, as reported by previous studies (Bowman et al., 2016; Kingma et al., 2016; Chen et al., 2017), variational models tend to experience posterior collapse during training, which occurs when the KL term vanishes to zero, indicating that the model fails to learn meaningful latent codes. This\nproblem becomes more severe when modeling discrete data with a strong auto-regressive decoder (He et al., 2019), which is the case for Transformerbased summarization models. To resolve this issue, several solutions have been proposed, such as employing a less auto-regressive decoder network (Yang et al., 2017; Semeniuta et al., 2017; Shen et al., 2018a), modifying the training objective (Zhao et al., 2017; Tolstikhin et al., 2018; Prokhorov et al., 2019), and proposing new training strategies (Kim et al., 2018; He et al., 2019). However, most existing work focuses on the VAE framework with Gaussian latent distribution, yet limited work considers the VED framework with normalizing flows. In particular, two questions remain unclear: (1) when the latent distribution is modeled by normalizing flows, does the posterior collapse problem still exist? (2) when posterior collapse exists, what are the appropriate strategies to achieve good summarization quality within the VED framework?\nThis paper introduces FlowSUM1, a normalizing flows-based VED framework for Transformerbased summarization, along with a controlled alternate aggressive training (CAAT) strategy and a refined gate mechanism to resolve the two challenging issues. Our contributions include:\n1. We employ normalizing flows to enrich the latent posterior distribution and integrate the latent code into Transformer-based models in a plug-and-play manner, demonstrating its effectiveness through extensive experiments.\n2. We propose a controlled alternate aggressive training strategy and a refined gate mechanism to mitigate the posterior collapse problem and improve training efficacy.\n3. Our findings suggest that FlowSUM facilitates knowledge distillation while having a negligible effect on inference time, implying normalizing flows\u2019 potential for transferring knowledge from advanced large language models.\n4. We investigate the posterior collapse problem for different normalizing flows and examine how the quality of a summary is impacted by the training strategy, gate initialization, and the type and depth of normalizing flows.\n1Code is available at https://github.com/ yuyangstat/flowsum.\nThis article consists of five sections. Section 2 provides an overview of normalizing flows, VED, and a summary of related studies. Section 3 describes the proposed model architecture and the training strategies employed. Section 4 presents the experimental setup and results, and Section 5 concludes the paper with some discussions."
        },
        {
            "heading": "2 Backgrounds",
            "text": ""
        },
        {
            "heading": "2.1 Normalizing Flows",
            "text": "Normalizing flows (NF) (Rezende and Mohamed, 2015) is a type of generative model that has gained popularity in recent years. The fundamental idea involves mapping a simple probability density (e.g., Gaussian) to a more complex one through a series of invertible transformations. One of the key advantages of NF is that it allows for exact likelihood evaluations, which is crucial for many applications such as density estimation (Papamakarios et al., 2017), data generation (Tran et al., 2019), and variational inference (Kingma et al., 2016). A flow-based model consists of two components: a base distribution pu(u) and a transformation f(\u00b7) : RD \u2192 RD, where f must be invertible and both f and f\u22121 must be differentiable. Let x = f(u) where u \u223c pu(u), then the density of x can be obtained via a change of variables (Bogachev, 2007):\npx(x) = pu(u) |det Jf (u)|\u22121\n= pu(f \u22121(x)) \u2223\u2223det Jf\u22121(x)\u2223\u2223 . (1) In this paper, we examine several NFs, including planar flows (Rezende and Mohamed, 2015), radial flows (Rezende and Mohamed, 2015), Sylvester flows (van den Berg et al., 2018), real-valued non-volume preserving (RealNVP) transformation (Dinh et al., 2017), inverse autoregressive flow (IAF) (Kingma et al., 2016), rational-quadratic neural spline flows (RQNSF) (Durkan et al., 2019), and rational-linear neural spline flows (RLNSF) (Dolatabadi et al., 2020). We delegate the detailed discussion of transformation and invertibility to Appendix J. Throughout the paper, for each type, we composeK layers of transformation fK\u25e6\u00b7 \u00b7 \u00b7\u25e6f1(\u00b7), which remains invertible and differentiable."
        },
        {
            "heading": "2.2 Variational Encoder-Decoders",
            "text": "Variational encoder-decoders (VEDs) (Zhang et al., 2016; Serban et al., 2017; Zhou and Neubig, 2017; Shen et al., 2018b), which can be seen as an extension of variational autoencoders (VAEs) (Kingma\nand Welling, 2014; Rezende et al., 2014), have been widely used to understand the conditional data generation process. Given an input x, the framework posits the existence of a latent variable z \u223c p(z | x;\u03d5), and the generation of y relies on p(y|x, z; \u03b8). With this premise, the conditional data generation can be formulated as in Eq. 2.\np(y | x;\u03d5, \u03b8) = \u222b p(z | x;\u03d5)p(y | x, z; \u03b8)dz\n(2) Since the marginal p(y | x;\u03d5, \u03b8) is intractable, we employ variational inference to estimate the parameters. This involves maximizing the evidence lower bound (ELBO), a surrogate of the log-likelihood, as defined in Eq. 3. The underlying idea is to propose a parameterized distribution q(z | x, y;\u03c8), known as the variational posterior, to approximate the true posterior distribution p(z | x, y;\u03d5, \u03b8). The greater the flexibility in q(z | x, y;\u03c8), the better the approximation, and the more effective the surrogate ELBO becomes. See more details in Appendix B.\nELBOVED = E q(z|x,y;\u03c8) [log p(y | x, z; \u03b8)]\u2212 KL(q(z | x, y;\u03c8)\u2225p(z | x;\u03d5)) (3) For summarization, we parameterize p(y | x, z; \u03b8) as an encoder-decoder model that generates summaries conditioned on the input text and latent code."
        },
        {
            "heading": "2.3 Related Work",
            "text": ""
        },
        {
            "heading": "2.3.1 Transformer-based Summarization Models",
            "text": "Transformer-based models equipped with pretraining and fine-tuning techniques have enjoyed significant success in many NLP tasks, including text summarization. Liu and Lapata (2019) proposed BertSUM for extractive and abstractive tasks, utilizing the pre-trained BERT encoder (Devlin et al., 2019). To better align the pre-trained encoder for document understanding with the decoder trained from scratch for text generation, Rothe et al. (2020) demonstrated the effectiveness of leveraging pre-trained BERT (Devlin et al., 2019), GPT-2 (Radford et al., 2019), and RoBERTa (Liu et al., 2019) checkpoints to build sequence-to-sequence (S2S) models for tasks including summarization. Another approach is to address both document understanding and generation in a unified framework by first pre-training some general-purpose S2S models and then fine-tuning on downstream tasks,\nfor instance, BART (Lewis et al., 2020), MASS (Song et al., 2019), UniLM (Dong et al., 2019), ProphetNet (Qi et al., 2020), and T5 (Raffel et al., 2020). In addition, Zhang et al. (2020a) proposed PEGASUS with a pre-training objective tailored for abstractive summarization, achieving significant improvements across multiple datasets."
        },
        {
            "heading": "2.3.2 Variational Summarization",
            "text": "Variational summarization models come in two different flavors: unsupervised and supervised. In the unsupervised domain, researchers commonly utilize variational autoencoders in conjunction with specific control mechanisms for summary generation, as exemplified by prior work such as Schumann (2018); Chu and Liu (2019); Brazinskas et al. (2020). In the supervised realm, there are generally two primary approaches. The first approach models the conditional probability of the target sentences p(y | x) as in Eq. 2, whereas the second approach models the joint probability of the source and target sentences p(x, y) with\u222b p(z)p(x | z)p(y | z, x)dz. Our model belongs to the first category, akin to prior studies like Setiawan et al. (2020); Fu et al. (2020). In contrast, other works, including Zheng et al. (2020); Nguyen et al. (2021); Zou et al. (2021), adopt the second type by jointly modeling topics and sequence-to-sequence generation. Most of them assume a simple Gaussian latent prior, except for Nguyen et al. (2021), which employs normalizing flows to model neural topic models and enrich global semantics. However, they did not specify the choice of normalizing flows and how they addressed posterior collapse. To the best of our knowledge, there remains limited research on the application of normalizing flows in variational summarization models and their operating characteristics."
        },
        {
            "heading": "3 Normalizing Flows Enhanced Summarization Model",
            "text": ""
        },
        {
            "heading": "3.1 FlowSUM Model Architecture",
            "text": "As illustrated in Fig. 1, FlowSUM consists of three components: an NF latent module, a Transformerbased encoder-decoder, and a refined gate mechanism. The NF latent module focuses on modeling the variational posterior q(z | x, y;\u03c8), whereas the encoder-decoder, combined with the refined gate, models the conditional generation p(y|x, z; \u03b8) with latent code. As a simplification, we assume the conditional prior p(z | x;\u03d5) is a standard Gaussian as\nin Setiawan et al. (2020). Throughout this section, let e be the embedding size, m,n be the length of the input source and target summary respectively, \u2113 be the latent dimension of the NF latent module, d be the dimension of the decoder\u2019s hidden states, {xi}mi=1 be the input source text, {yj}nj=1 be the target summary text, and x \u2208 Re be the average embedding of the untruncated input source text2. NF Latent Module. To model the variational posterior q(z | x, y;\u03c8), we follow Zhou and Neubig (2017) and assume all the information in y is contained in x3. Therefore, we have q(z | x, y;\u03c8) = q(z | x;\u03c8), which allows us to parameterize q(z | x;\u03c8) with neural networks (NNs) and normalizing flows using the amortization and reparameterization tricks (Kingma and Welling, 2014). The NF latent module comprises of an inference network q0(\u00b7) and a normalizing flows model. The inference network takes x as input and produces two output vectors, \u00b50 \u2208 R\u2113 and log(\u03c30) \u2208 R\u2113. Using the reparameterization trick, a random sample z0 \u2208 R\u2113 is drawn from N(\u00b50, diag(\u03c320)). Afterward, the normalizing flows model applies a sequence of K invertible transformations to z0 to obtain the latent\n2Let V be the vocabulary size, {Ev}Vv=1 be the input embeddings, and {bv}Vv=1 be the Bag-of-Words (BoW) of the input source text, then x = ( \u2211V v=1 bvEv)/( \u2211V v=1 bv) \u2208 R\ne. In addition, when we don\u2019t truncate the input text, bT1 = m holds. However, if we truncate the input due to encoder constraints, then bT1 > m, and the BoW vector will contain information that would otherwise have been lost.\n3See detailed discussion in Appendix C.\ncode z = zK = fK \u25e6 \u00b7 \u00b7 \u00b7 \u25e6 f1(z0) \u2208 R\u2113.4 Note that when K = 0, the model reverts to the traditional VED framework, and we refer to this degenerated version as VEDSUM. Gated Transformer-based Encoder-Decoder. Our model adopts the Transformer-based encoderdecoder. The encoder processes the input text and learns a sequence of hidden representations, and the decoder generates a summary based on the encoder\u2019s hidden states and the previously generated tokens. We incorporate the latent information into the decoder with a gate mechanism, which mixes the latent vector zK with the decoder\u2019s last layer of hidden states {hj}nj=1. As pointed out in Gu et al. (2020), the saturation property of traditional gating mechanisms hinders gradient-based optimization. Therefore, following their proposal, we use a refined gate mechanism designed to allow for better gradient flow. Let \u03c3(\u00b7) be the sigmoid function. We generate the gated fused hidden states {h\u2032j}nj=1 as in Eq. 4.\nz\u2032K =W zzK \u2208 Rd, where W z \u2208 Rd\u00d7\u2113 fj = \u03b4 ( W f [ hj ; z \u2032 K ]) \u2208 Rd, where W f \u2208 Rd\u00d72d\nrj = \u03b4 ( W r [ hj ; z \u2032 K ]) \u2208 Rd, where W r \u2208 Rd\u00d72d\ngj = (1\u2212 rj) \u00b7 f2j + rj ( 1\u2212 (1\u2212 fj)2 ) \u2208 Rd\nh\u2032j = (1\u2212 gj) \u00b7 hj + gj \u00b7 z\u2032K \u2208 Rd (4) Afterward, the fused hidden states are passed to a language model (LM) Head layer, where they are transformed into vectors modeling the probabilities of each word in the vocabulary."
        },
        {
            "heading": "3.2 Training Objective",
            "text": "Traditional VEDs usually assume q(z | x;\u03c8) to be a Gaussian, allowing analytical computation of the KL term in ELBO. However, in our normalizing flows-based VED, the variational posterior q(z | x) = qK(zK | x) can be complex and hence the KL term in Eq. 3 lacks an analytical form. Therefore, we rewrite the ELBO via a change of variables to enable analytical evaluation5:\nELBONF-VED =Eq0(z0) [log p (y | x, zK) + log p (zK | x)]\n\u2212Eq0(z0) [ log q0 (z0)\u2212 \u2211K k=1 log |det Jfk (zk\u22121)| ] ,\n(5) 4The log-determinant of the Jacobian at each layer is\nrecorded along the forward call for loss computation. 5See derivation in Appendix B Eq. 9.\nwhere q0 is z0\u2019s probability density function, a Gaussian distribution modeled by NNs, and det Jfk(\u00b7) is the determinant of fk\u2019s Jacobian.\nLet LCE denote the cross-entropy loss and LVI denote the loss introduced by the variational latent module. Applying the idea of Monte Carlo to Eq. 5, we obtain the training objective as below. Note that LVI is a Monte Carlo estimate of the KL divergence between the variational posterior qK and the conditional prior distribution p(zK | x).\nL = LCE + LVI = \u2212 \u2211n j=1 log p (yj | {xi} m i=1 , zK , y<j)\n+ log q0 (z0)\u2212 \u2211K\nk=1 log |det Jfk (zk\u22121)| \u2212 log p (zK | x)\n(6)"
        },
        {
            "heading": "3.3 Mitigating Posterior Collapse",
            "text": "To remedy posterior collapse, we consider two strategies, aiming to preserve the expressiveness of the latent variable and improve the overall summary quality. The first approach, called \u03b2C-VAE (Prokhorov et al., 2019), replaces the KL term with \u03b2|KL\u2212C|, where \u03b2 is a scaling factor, and C \u2265 0 is a threshold that regulates the magnitude of the KL term. When C > 0, the KL term is expected to be discouraged from getting close to 0.\nWe propose the second approach, Controlled Alternate Aggressive Training (CAAT), inspired by the lagging inference strategy (He et al., 2019). This strategy uses the observation that the inference network cannot accurately approximate the true posterior in the initial stages of training. As outlined in Alg. 1 in Appendix A, CAAT comprises two stages. In the first stage, we alternately update the variational parameters and the entire parameters6 for a specified number of steps. In the second stage, we train all parameters jointly, as in basic VAE training, for the remainder of the training."
        },
        {
            "heading": "3.4 NF-enhanced Knowledge Distillation",
            "text": "Normalizing flows can learn complex and multimodal distributions (Papamakarios et al., 2017), which makes them a promising approach for knowledge distillation tasks that involve integrating information from multiple sources (Hinton et al., 2015). To investigate the impact of normalizing flows on knowledge distillation, we adopt two knowledge distillation methods by Shleifer and Rush\n6In our preliminary experiments, we find that if we alternate between variational and encoder-decoder parameters, the training becomes unstable and generates NaN values. Therefore, we alternate between variational and all parameters.\n(2020): Shrink and Fine-Tune (SFT) and Pseudolabels (PL). SFT shrinks the teacher model and re-finetunes the shrunk model. In contrast, the PL method initializes the student model with the compressed version produced by SFT and then finetunes using the pseudo-labeled data generated by the teacher model. In this study, we fine-tune the model on the augmented data with both original and pseudo-labeled data, enabling it to more effectively switch between generated summaries and ground truth, thereby mitigating exposure bias."
        },
        {
            "heading": "4 Experiments",
            "text": ""
        },
        {
            "heading": "4.1 Datasets",
            "text": "We evaluate the effectiveness of FlowSUM on six public benchmark datasets7, including CNN/Daily Mail (CNN/DM) (Hermann et al., 2015), XSum (Narayan et al., 2018), Multi-News (Fabbri et al., 2019), arXiv, PubMed (Cohan et al., 2018), and SAMSum (Gliwa et al., 2019). These datasets exhibit various summary styles and lengths, and their corresponding statistics are shown in Table 1. Refer to Appendix E for more details."
        },
        {
            "heading": "4.2 Implementation Details",
            "text": "We configure the inference net q0(z0|x) to be a feedforward neural network and set the latent dimension \u2113 to 300 and the number of NF layers K \u2208 {2, 4, 6, 8}. For models that use \u03b2C-VAE, we set \u03b2 = 1 and C = 0.1, and for those using CAAT, we conduct one epoch of aggressive training with nalt = 15 and two epochs of non-aggressive training. See more details in Appendix G."
        },
        {
            "heading": "4.3 Baselines",
            "text": "We use BART (Lewis et al., 2020) and BERT2BERT (Rothe et al., 2020) as two backbone models. We refer to the PL knowledge distilled\n7We access them through Hugging Face Datasets, which provides reproducible code for processing texts and generating train/validation/test splits.\nFlowSUM as FlowSUM-PLKD. Our comparison involves the following baselines: PG+Cov (See et al., 2017), BERT2BERT (Rothe et al., 2020), BERTSUM (Liu and Lapata, 2019), BART (Lewis et al., 2020), PEGASUS (Zhang et al., 2020a), VHTM (Fu et al., 2020), TAS (Zheng et al., 2020), and PEGASUS+Flow-NTM (Nguyen et al., 2021). See Appendix F for more detailed descriptions."
        },
        {
            "heading": "4.4 Results",
            "text": ""
        },
        {
            "heading": "4.4.1 Automatic Evaluation",
            "text": "We evaluate the generated summary quality using ROUGE scores (Lin, 2004) and BERTScore (Zhang et al., 2020b)8. Specifically, we utilize the overlap of unigrams and bigrams (ROUGE-1 and ROUGE-2) to evaluate the informativeness, and the longest common subsequence (ROUGE-L) for fluency. Moreover, we report BERTScore, which gauges semantic similarity based on contextual embeddings. Furthermore, we present rep-w (Fu et al., 2021)9 and the average length of summaries to gain a better understanding of the quality.\nWe compare the proposed model against baseline models in ROUGE scores in Tables 2 and 3. On CNN/DM, FlowSUM (BERT2BERT) greatly outperforms BERT2BERT, whereas VEDSUM adds noise to the model and leads to a decrease in performance. With the BART backbone, FlowSUM achieves an absolute improvement over the BART model with +0.48, +0.08, and +0.75 in R-1, 2, and L scores, respectively. However, on XSum, the variational models do not perform well when the gold summaries involve only one sentence. VEDSUM leads to a significant decrease in performance, whereas with FlowSUM, the decrease in ROUGE scores is less severe, leading to +0.12, -0.15, and -0.25 in R-1, 2, and L scores, respectively.\nTable 4 uses BART as the backbone and compares BART, VEDSUM, and FlowSUM across all datasets. Overall, variational models produce summaries of superior quality for datasets with long summaries, such as CNN/DM, Multi-News, arXiv, and PubMed, and FlowSUM further enhances the performance beyond VEDSUM. However, when it comes to datasets featuring short summaries such as XSum and SAMSum, the variational component markedly diminishes the model performance.\n8We obtain both metrics using Hugging Face Evaluate and report the F1 scores.\n9rep-w is calculated as the proportion of the current token that appears in the previous w tokens. Refer to Appendix D for the detailed definition.\nWe hypothesize that brief summaries may be more susceptible to disturbances and are more prone to being affected by noise. Nevertheless, incorporating NF modules alleviates these reductions and accomplishes comparable outcomes. Furthermore, we observe that both variational models tend to generate lengthier summaries, while FlowSUM exhibits fewer issues with repetition compared to VEDSUM."
        },
        {
            "heading": "4.4.2 On NF-enhanced Knowledge Distillation",
            "text": "We use PEGASUS as the teacher model to generate pseudo-labels on the CNN/DM training set. In this study, we explore the effects of knowledge distillation on BART and DistilBART, a shrunken version of BART. We examine two variations of DistilBART: dBART-6-6, which replicates 6 layers10 of the BART encoder and decoder, and dBART-12-3, which duplicates all layers of the BART encoder and 3 layers11 of the decoder.\nTable 5 presents the impact of the PL approach on the original BART model. Training the BART\n10The 0, 2, 4, 7, 9, and 11th layer. 11The 0, 6, and 11th layer.\nModel ROUGE \u2191 1/2/L BERTScore \u2191 rep-w \u2193 Length\nCNN/DM\nBART 44.16/21.28/40.90 89.40 8.31 84.11 VEDSUM 44.34/21.09/41.37 89.20 8.43 88.63 FlowSUM 44.64/21.36/41.65 89.46 8.43 92.24\nMulti-News\nBART 42.56/15.34/36.67 86.69 9.76 133.42 VEDSUM 43.91/16.68/38.10 87.04 9.95 128.79 FlowSUM 44.42/17.01/38.36 87.09 9.91 128.87\narXiv\nBART 42.55/15.92/37.89 85.35 17.23 130.68 VEDSUM 43.05/16.34/38.26 85.44 16.63 130.92 FlowSUM 43.11/16.26/38.31 85.45 16.55 132.88\nPubMed\nBART 41.57/16.72/36.94 84.65 13.26 136.10 VEDSUM 44.21/19.20/39.32 85.07 12.76 138.70 FlowSUM 44.55/19.50/39.59 85.16 12.59 138.09\nXSum\nBART 45.14/22.27/37.25 92.16 4.63 25.54 VEDSUM 43.62/20.27/35.06 91.75 5.96 31.22 FlowSUM 45.26/22.12/37.00 92.13 4.95 28.71\nSAMSum\nmodel on augmented data worsens the performance compared to training on the original data. In contrast, VEDSUM-PLKD achieves improvements in all three ROUGE scores, and FlowSUM-PLKD with RQNSF achieves the highest R-2 score, albeit with some sacrifice in R-1 and R-L12. However, planar flows appear to be unsuitable for knowledge distillation via PL. To better understand FlowSUMPLKD, we visualize the latent distribution (see Appendix I) and demonstrate how the NF\u2019s ability to capture multi-modality could account for its impressive performance.\nTable 6 investigates the two DistilBART variants with RQNSF. With FlowSUM, both variants achieve improvements, suggesting that NF is beneficial for the SFT approach. Previous experiments from Shleifer and Rush (2020) showed that PL performed worse than SFT on CNN/DM. However, our experiments reveal that the NF latent module unleashes the potential of PL. When trained on augmented data, FlowSUM-PLKD (dBART-6-6)\n12This can be explained by the teacher model\u2019s worse performance in these two metrics.\nachieves R-1/2/L improvements of 0.92/0.47/1.01 over dBART-6-6, and FlowSUM-PLKD (dBART12-3) achieves improvements of 0.66/0.49/0.63 over dBART-12-3, much more than the SFT approach. Furthermore, FlowSUM does not introduce additional computational burden at inference, and the time cost is primarily related to the length of the generated summaries."
        },
        {
            "heading": "4.4.3 Analysis on NF Types and Depth",
            "text": "We investigate the effect of NF types and the number of NF layers on the Multi-News dataset13. Table 7 explores the effect of NF types. Simple flows like Planar and Radial yield inferior performance compared to the VAE counterpart, whereas more complex flows tend to achieve greater improvements. Overall, IAF and RQNSF emerge as the best-performing NF types.\nTable 8 delves further into IAF and RQNSF, investigating the effect of NF depth. The findings indicate that adding more layers does not always lead to improved performance. We hypothesize that when the encoder-decoder model is well-trained, the increased complexity of the NF module may introduce more noise, outweighing the benefits of better latent modeling and subsequently worsening the summary quality.\n13We choose Multi-News due to its smaller size, enabling us to conduct experiments with reduced computational cost."
        },
        {
            "heading": "4.4.4 Analysis on Training Strategies",
            "text": "We implement standard VAE training, \u03b2C-VAE, and CAAT on VEDSUM and FlowSUM models, and we evaluate their effectiveness with different types of normalizing flows. Table 9 shows that VEDSUM and FlowSUM models with residual flows, including planar, radial, and Sylvester flows, suffer from posterior collapse, whereas those with more complex flows do not. Moreover, applying \u03b2C-VAE to VEDSUM and FlowSUM models with residual flows does not effectively mitigate posterior collapse but even exacerbates the issue. Furthermore, for models with planar, RealNVP, and IAF flows, training with \u03b2C-VAE worsens ROUGE scores, while for radial and Sylvester flows, it improves performance. Notably, the two neural spline flows are not impacted by \u03b2C-VAE training.\nConcerning CAAT, we note that applying it to treat severe posterior collapses such as VEDSUM and FlowSUM with residual flows can cause instability in training while producing NaN values. Hence, it is only effective for models with KL divergence that is not close to zero. Nonetheless, when applicable, CAAT enhances the quality of summaries, particularly when utilized with the topperforming NFs, namely IAF and RQNSF.\nIn addition, we explore the impact of gate score initialization. The standard method initializes gating weights with small deviations from zero, resulting in an initial gate score close to 0.5. In contrast, the near-zero initialization method initializes gating weights such that the resulting gate score is approx-\nimately 0.05. Our experiments using FlowSUM (BERT2BERT) with RQNSF as the base model reveal that CAAT + Standard Gate Score Initialization yields the best results and the most stable training process, as illustrated in Table 10 and Figures 2 to 3 in Appendix H. This suggests that by setting a large initial gate score and forcing the model to learn from the NF latent module, we can better capture latent code information."
        },
        {
            "heading": "5 Conclusions and Discussions",
            "text": "This paper introduces FlowSUM, a normalizing flows-based Variational Encoder-Decoder (VED) framework for text summarization. It outperforms a leading non-latent model across multiple datasets. This enhanced performance is attributed to the flexible posterior distributions provided by normalizing flows. We also analyze the operating characteristics and the posterior collapse problem of normalizing flows and propose an effective training strategy for complex flows. Moreover, we demonstrate that in-\ncorporating normalizing flows is highly effective for knowledge distillation with minimal impact on inference time.\nFlowSUM illustrates the advantages of incorporating flexible latent modeling. Considering the remarkable achievements of Latent Diffusion Models (LDMs) in generating images (Rombach et al., 2022), adopting LDMs for capturing latent representation may produce comparable or even superior outcomes in text summarization. In this scenario, the gating mechanism may not be an appropriate choice. A direct correlation between the latent vector and the target text may be more suitable for executing the diffusion process. Enhancing the architecture to leverage diffusion models could be a potential avenue for future research.\nLimitations\nFlowSUM has demonstrated excellent results on datasets with long summaries. However, its performance on short-summary datasets like XSum and SAMSum has been unsatisfactory. The underlying cause could be attributed to suboptimal hyperparameter tuning or the incompatibility of FlowSUM with short summaries. Additional investigations are needed to identify the root cause.\nFurthermore, we did not fine-tune the hyperparameters of the normalizing flows model, such as the latent dimension, the number of bins in spline coupling layers, and the neural network in IAF, RealNVP, RLNSF, and RQNSF. Moreover, we opted for a small batch size due to memory limitations. Adjusting these hyperparameters could potentially enhance the model\u2019s performance.\nDue to limited computational resources, we utilized BART and BERT2BERT as the backbone models instead of newer architectures. Further research may focus on verifying the effectiveness of FlowSUM on more advanced structures.\nEthics Statement\nOur research entailed developing a new text summarization framework. Although no private data were utilized, we acknowledge the potential societal impacts of our work. Therefore, we adhered to pertinent ethical guidelines and implemented rigorous procedures to guarantee the accuracy of our results."
        },
        {
            "heading": "Acknowledgements",
            "text": "This work was supported in part by NSF grant DMS-1952539 and NIH grants R01AG069895, R01AG065636, R01AG074858, U01AG073079."
        },
        {
            "heading": "A Controlled Alternate Aggressive Training (CAAT)",
            "text": "Algorithm 1 Controlled Alternate Aggressive Training (CAAT) Input: number of aggressive training steps nagg; maximum number of training steps nmax; number of alternating steps nalt.\n1: \u03b8,\u03c8 \u2190 Initialize encoder-decoder parameters and variational parameters respectively 2: for i = 1, 2, \u00b7 \u00b7 \u00b7 , nagg do 3: X\u2190 Random data minibatch 4: if i mod nalt = 0 then 5: Compute g\u03b8,\u03c8 \u2190 \u2207\u03c8,\u03b8L(X;\u03b8,\u03c8) 6: Update \u03b8,\u03c8 using gradients g\u03b8,\u03c8 7: else 8: Compute g\u03c8 \u2190 \u2207\u03c8L(X;\u03b8,\u03c8) 9: Update \u03c8 using graidents g\u03c8\n10: for i = nagg, nagg + 1, \u00b7 \u00b7 \u00b7 , nmax do 11: X\u2190 Random data minibatch 12: Compute g\u03b8,\u03c8 \u2190 \u2207\u03c8,\u03b8L(X;\u03b8,\u03c8) 13: Update \u03b8,\u03c8 using gradients g\u03b8,\u03c8 14: if early stopping criterion is met then 15: break\nAnother advantage of the controlled alternate aggressive training (CAAT) strategy is that it provides us with more control. It is commonly assumed that\nallowing the model more freedom to learn, even if the NF latent module is not helpful, will not harm performance. However, our experiments suggest that this assumption does not hold, particularly for short-summary datasets where the model will not learn on its own to avoid hurting the original performance. The CAAT strategy allows us to effectively freeze the encoder-decoder parameters by setting nagg and nalt to large values, ensuring that when the nf module is unhelpful, it will not significantly harm performance."
        },
        {
            "heading": "B Deeper Dive into the Evidence Lower Bound (ELBO)",
            "text": "Within the VED framework, the conditional data generation process can be expressed as follows:\np(y | x;\u03d5, \u03b8) = \u222b p(z | x;\u03d5)p(y | x, z; \u03b8)dz.\nThe subsequent challenge revolves around parameter estimation. Typically, the conditional latent prior is assumed as p(z | x;\u03d5) = N(0, I) for simplification (hence eliminating the \u03d5 parameter). Despite this, the likelihood p(y | x; \u03b8) remains computationally intractable to evaluate. Variational inference tackles this issue by introducing a variational distribution q(z | x, y;\u03c8) from a specific parametric family, aiming to approximate the actual posterior p(z | x, y). Here, \u03b8 denotes the model parameters, and \u03c8 refers to the variational parameters. Instead of attempting to estimate \u03b8 solely through maximizing the challenging log-likelihood, the approach involves joint estimation of both \u03b8 and \u03c8 by optimizing the ELBO.\nExamining Eq. 7 and 8, it\u2019s evident that the ELBO represents a lower bound of the loglikelihood. Moreover, a smaller value of KL(q(z | x, y)\u2225p(z | x, y)) indicates a closer alignment between the variational posterior and the true posterior, thereby bringing the ELBO closer to the log-likelihood. This insight propels the adoption of normalizing flows to model a flexible family of variational posterior.\nKL(q(z | x, y)\u2225p(z | x, y)) =Eq(z|x,y)[log q(z | x, y)]\u2212 Eq(z|x,y) [ log p(z, x, y)\np(x, y) ] =Eq(z|x,y)[log q(z | x, y)]\n\u2212 Eq(z|x,y) [ log p(z, x, y)\np(x, z) \u00b7 p(x, z) p(x) \u00b7 p(x) p(x, y) ] =Eq(z|x,y)[log q(z | x, y)]\u2212 Eq(z|x,y)[log p(y | x, z)] \u2212 Eq(z|x,y)[log p(z | x)] + Eq(z|x,y)[log p(y | x)] =KL(q(z | x, y)\u2225p(z | x))\u2212 Eq(z|x,y)[log p(y | x, z)] + Eq(z|x,y)[log p(y | x)] \u2a7e0\n(7)\nELBOVED =Eq(z|x,y)[log p(y | x, z)]\u2212KL(q(z | x, y)\u2225p(z | x)) = log p(y | x)\u2212KL(q(z | x, y)\u2225p(z | x, y)) \u2264 log p(y | x)\n(8)\nELBONF-VED =Eq(z|x)[log p(y | x, z)] + Eq(z|x) log p(z | x) \u2212 Eq(z|x)[log q(z | x)] =Eq0(z0) [log p (y | x, zK) + log p (zK | x)] \u2212 Eq0(z0) [log qK (zK)] =Eq0(z0) [log p (y | x, zK) + log p (zK | x)]\n\u2212 Eq0(z0) [ log q0 (z0)\u2212 K\u2211 k=1 log |det Jfk (zk\u22121)| ] ,\n(9)\nwhere q0 and qK are the probability density function for z0 and zK respectively.\nC Discussion on q(z | x, y) = q(z | x) we choose to assume q(z | x, y) = q(z | x) for the following reasons. Firstly, this assumption is grounded in the nature of summarization, where y can be viewed as a condensed form of x and hence it is sensible to assume all the information in y is contained in x. Secondly, as evidenced by Zhang et al. (2016), it is plausible to condition the posterior on both x and y. However, their approach suffers from difficulties during prediction. In prediction, the target text y is not accessible, making it hard to sample from q(z | x, y). Zhang et al. (2016) suggests taking the prior\u2019s mean as the latent code, but in our paper, the prior is a Gaussian whereas the posterior is a complex distribution modeled by normalizing flows, and taking such a strategy would diminish the benefit of using normalizing flows. Thirdly, it has been shown empirically by Eikema and Aziz (2019) that by restricting the conditioning of the posterior to x alone, their model\nachieves higher accuracy. Therefore, we consider q(z | x, y) = q(z | x) as our modeling strategy."
        },
        {
            "heading": "D Repetition Measures",
            "text": "Let s represent the sentences in a result set D, |s| be the number of tokens in s, st be the tth token, and si:j be the sub-sequence of s from the ith token to the jth token. The rep-w (Fu et al., 2021) is then defined by Equation 10.\nrep-w = 1 |D| \u2211 s\u2208D 1 |s| |s|\u2211 t=2 1 [ st \u2208 smax(t\u2212w,1):t\u22121 ] (10)"
        },
        {
            "heading": "E Datasets",
            "text": "CNN/Daily Mail (Hermann et al., 2015) consists of 312,085 online news articles, with one article paired with a multi-sentence summary. We use the non-anonymized version as in See et al. (2017) and follow the text processing14 in Lewis et al. (2020). XSum (Narayan et al., 2018) contains 227k BBC articles, each summarized in a single sentence. Multi-News (Fabbri et al., 2019) is a multidocument dataset comprising 56k pairs of news articles and multi-sentence summaries. arXiv, PubMed (Cohan et al., 2018) are two scientific paper document datasets from arXiv.org (113k) and PubMed (215k). Each pair consists of a scientific article\u2019s body document and its abstract. SAMSum (Gliwa et al., 2019) includes 16k conversations annotated with summaries by linguists. Unlike structured texts, the information in dialogues is scattered across different speakers\u2019 utterances, increasing the summarization difficulty."
        },
        {
            "heading": "F Baseline Models",
            "text": "PG+Cov (See et al., 2017) is a pointer-generator (PG) network supplemented with a coverage mechanism that addresses the Out-Of-Vocabulary problem and minimizes word repetition. BERT2BERT (Rothe et al., 2020) initializes both the encoder and the decoder with the pre-trained BERT checkpoints and adds cross-attention layers. BERTSUM (Liu and Lapata, 2019) builds on top of BERT and applies a fine-tuning scheduler to better align the encoder and the decoder. BART (Lewis et al., 2020) is a pretrained denoising autoencoder with the standard sequence-to-\n14We update the data loading script following https:// github.com/facebookresearch/fairseq/issues/1401.\nsequence Transformer architecture. In this paper, we use BART as the encoder-decoder backbone. PEGASUS (Zhang et al., 2020a) is a large Transformer-based S2S model, pre-trained on massive text data using a self-supervised objective called gap sentence generation, designed for abstractive summarization. VHTM (Fu et al., 2020) is a variational hierarchical model built on the PG network. It models the topic proportion vector with isotropic Gaussian and fuses in topic information at diverse granularity levels. TAS (Zheng et al., 2020) is a topic-guided Transformer-based S2S model that injects the topicword matrix into the LMHead layer and jointly trains the NTM and encoder-decoder model. PEGASUS+Flow-NTM (Nguyen et al., 2021) is a topic-aware model built on PEGASUS. It utilizes a Flow-based NTM and a contextualized gating mechanism to integrate topic information into the encoder and the decoder.\nG Implementation Details\nG.1 NF Latent Module\nWe configure the inference net q(z0|x) to be a feedforward neural network with three hidden layers of dimension \u2208 {300, 600}, Tanh activations, and a 0.1 dropout rate. We set the latent dimension \u2113 to 300 and the number of NF layers \u2208 {2, 4, 6, 8}. For spline coupling layers (RLNSF and RQNSF), we set the number of bins to 4, the bound to 3.0, the split dimension to \u2113/2, and the neural network to have two hidden layers with the dimension \u2113. For RealNVP, the split dimension is \u2113/2, and the neural network has one hidden layer with a dimension of 10\u2113. For IAF, the neural network features one hidden layer of the dimension 3\u2113 + 1. Moreover, we set \u03b2 = 1 and C = 0.1 for models that use \u03b2CVAE, and for models that use CAAT, we conduct one epoch of aggressive training with nalt = 15, followed by two epochs of non-aggressive training.\nG.2 Optimization\nWe train the models using the Adam optimizer (Kingma and Ba, 2015) with \u03b21 = 0.9, \u03b22 = 0.999, and \u03f5 = 10\u22128. The initial learning rate is 5\u00d7 10\u22125. We employ a linear learning rate scheduler that increases the learning rate from 0 to the initial learning rate during the warmup stage and decreases it from the initial learning rate to 0 after the warmup stage. We also apply the gradient clipping technique with a maximum gradient norm of 1.0. Fur-\nthermore, we terminate the training early when the perplexity fails to improve for eight or sixteen consecutive evaluation calls.\nG.3 Model Hyper Parameters\nTable 11 provides the hyper-parameters for the models discussed in Table 4 - 7, for the sake of reproducibility. To ensure fair comparisons, unless otherwise specified, the VEDSUM models typically employ the same set of hyper-parameters as their FlowSUM counterparts, except with standard training and no NF layers applied. Additionally, the models in Table 8 have the same hyper-parameters as those in Table 7, except for the number of NF layers used. Lastly, in Table 9, all FlowSUM models use 4 NF layers and the same set of hyperparameters as those in Table 7 but vary in their training strategies."
        },
        {
            "heading": "H Experiments on Training Strategies and Gate Initialization",
            "text": "The training curves for the methods in Table 10 are illustrated in Figure 2. The plot demonstrates that the gate score decreases gradually and remains high during aggressive training when CAAT is combined with standard initialization. This combination compels the model to utilize the latent code information effectively. Moreover, as presented in Figure 2c, even though CAAT combined with standard initialization starts with a high perplexity, it achieves a lower perplexity level than other approaches by the end. By examining the training procedure in detail, Figure 3 further indicates that CAAT contributes to greater training stability than standard training.\nI Visualization of Latent Distribution\nTo gain a better understanding of how normalizing flows contribute to knowledge distillation, we selected several examples from the CNN/Daily Mail and XSum datasets and visualized the resulting latent distribution generated by the FlowSUM-PLKD model, as shown in Figure 4 and 5. For both cases, the transformed latent code zK exhibited a highly flexible distribution. Notably, in the CNN/Daily Mail example, the first dimension of the second example demonstrated a clear bi-modal distribution, indicating the model\u2019s ability to capture information from multiple sources. Similarly, in the XSum dataset examples, we observed distinct multi-modal patterns."
        },
        {
            "heading": "J Normalizing Flows",
            "text": "Planar flow Proposed by Rezende and Mohamed (2015), the planar flow can be expressed as in Eq. 11. It applies contractions or expansions in the direction perpendicular to the hyperplane w\u22a4z + b = 0. Its Jacobian determinant can be computed in time O(D) as in Eq. 12, using the matrix determinant lemma. In addition, we need to note that this flow is not invertible for all values of u and w. When the derivative of the activation function h\u2032(\u00b7) is positive and bounded from above, w\u22a4u > \u2212 1supx h\u2032(x) is sufficient to ensure invertibility15.\n15In our code, we perform a transformation on u : u \u2190 u+ [ log ( 1 + exp ( w\u22a4u )) \u2212 1\u2212w\u22a4u ] \u00b7 w w\u22a4w\nand restrict the activation h(\u00b7) to be one of leakyrelu, relu, and tanh to meet this condition.\nf(z) = z+ uh ( w\u22a4z+ b ) , (11)\ndet J = 1 + h\u2032 ( w\u22a4z+ b ) w\u22a4u (12)\nwhere {u,w \u2208 RD, b \u2208 R} are free parameters and h(\u00b7) is a smooth element-wise non-linear activation function with derivative h\u2032(\u00b7).\nRadial flow The radial flow (Tabak and Turner, 2013; Rezende and Mohamed, 2015) takes the form of Eq. 13. It applies radial contractions and expansions around a reference point. Similar to the planar flow, we can apply the matrix determinant lemma to calculate the Jacobian determinant inO(D) time, as in Eq. 14. To guarantee invertibility, we usually require \u03b2 > \u2212\u03b116.\nf(z) = z+ \u03b2h(\u03b1, r) (z\u2212 z0) , (13)\ndet J = ( 1 + \u03b1\u03b2\nh2\n) (1 + \u03b2h)D\u22121 (14)\nwhere z0 \u2208 RD is the reference point, \u03b2 \u2208 R, \u03b1 \u2208 R+ are free parameters, r = \u2225z \u2212 z0\u2225 is the norm of z \u2212 z0, and h(\u03b1, r) = 1\u03b1+r .\nSylvester flow The Sylvester flows (van den Berg et al., 2018) generalize the planar flows to have M hidden units, as in Eq. 15. To achieve better computational efficiency, van den Berg et al. (2018) proposes the parameterization as in Eq. 16, with which the Jacobian determinnant reduces to Eq. 17 and can be computed in O(M). Similar to the planar flows, when h\u2032(\u00b7) is positive and bounded from above, R\u0303iiRii > \u2212 1supx h\u2032(x) for all i \u2208 {1, . . . , D} is sufficient to ensure invertibility.\nf(z) = z+Uh ( W\u22a4z+ b ) , (15)\nwhere {U \u2208 RD\u00d7M ,W \u2208 RD\u00d7M ,b \u2208 RM} are the free parameters and h(\u00b7) is an element-wise activation function.\nf(z) = z+QRh ( R\u0303QT z+ b ) , (16)\ndet J = det ( IM + diag ( h\u2032 ( R\u0303QT z+ b )) R\u0303R ) (17) where R and R\u0303 are upper triangular M \u00d7M matrices, and Q = (q1 . . . qM ) consists of an orthonormal set of vectors.\n16In our code, we perform a transformation on \u03b2 : \u03b2 \u2190 \u2212\u03b1+ log ( 1 + e\u03b2 ) to guarantee invertibility.\nAutoregressive Flows The masked autoregressive flow (MAF) (Papamakarios et al., 2017) was motivated by MADE (Germain et al., 2015), which is an autoregressive model for density estimation. MAF generalizes the conditional distribution to be Gaussian and generates data in a recursive way as in Eq. 18. Given a data point x, the inverse transformation can be performed in parallel as in Eq. 19. The Jacobian of the inverse transformation is lower-triangular by design due to the autoregressive structure, hence its absolute determinant can be expressed as in Eq. 20. The set of functions {f\u00b5i , f\u03b1i} are autoregressive neural networks following the approaches in MADE.\nxi = ui exp\u03b1i + \u00b5i, (18)\nwhere \u00b5i = f\u00b5i (x1:i\u22121) , \u03b1i = f\u03b1i (x1:i\u22121) and ui \u223c N (0, 1).\nui = (xi \u2212 \u00b5i) exp (\u2212\u03b1i) (19)\n\u2223\u2223det J\u22121\u2223\u2223 = exp(\u2212\u2211 i \u03b1i ) (20)\nLikewise, the inverse autoregressive flow (IAF) (Kingma et al., 2016) uses MADE with Gaussian conditionals and generates data as in Eq. 21. Its Jacobian determinant has a simple form as in Eq. 22. The main difference between IAF and MAF lies in the history variables. MAF uses previous data variables x1:i\u22121 to compute \u00b5i and \u03b1i, whereas IAF uses previous random variables u1:i\u22121 for the computation. In terms of sampling and density evaluation, IAF can sample in parallel and need to evaluate sequentially, whereas MAF has to sample sequentially and can evaluate in parallel. Since we care more about the sampling efficiency in variational inference, we choose IAF in the paper.\nxi = ui exp\u03b1i + \u00b5i, (21)\nwhere \u00b5i = f\u00b5i (u1:i\u22121) and \u03b1i = f\u03b1i (u1:i\u22121).\n|det J | = exp (\u2211 i \u03b1i ) (22)\nAffine Coupling The affine coupling layer, proposed in NICE (Dinh et al., 2015) and later generalized in RealNVP (Dinh et al., 2017) takes the following form.\n{ y1:d = x1:d\nyd+1:D = xd+1:D \u2299 exp (s (x1:d)) + t (x1:d) (23) where s : Rd 7\u2192 RD\u2212d and t : Rd 7\u2192 RD\u2212d are scale and translation transformation function respectively, and \u2299 is the element-wise product.\nIts Jacobian determinant can be efficiently computed as det J = exp [\u2211 j s (x1:d)j ] . Since the computation does not involve the Jacobian of s or t, we can make these two functions arbitrarily complex and use neural networks to model them. The coupling layers are usually composed of permutation layers to ensure every component gets modified, and since the Jacobian determinant of permutation is 1, the Jacobian determinant remains tractable.\nSpline Coupling Neural spline flows (Durkan et al., 2019; Dolatabadi et al., 2020) use monotonic rational-quadratic splines or monotonic rationallinear splines as the coupling transformation to achieve more flexibility and yet remain differentiable and invertible. The monotonic rationalquadratic spline uses K + 1 monotonically increasing knots {( x(k), y(k) )}K k=0\nto set up K bins, each of which is defined as a rational-quadratic function17 that is monotonically increasing. It maps [\u2212B,B] to [\u2212B,B] and defines the transformation outside the range to be identity transformation. Let sk = ( yk+1 \u2212 yk ) / ( xk+1 \u2212 xk\n) and \u03be(x) = ( x\u2212 xk ) / ( xk+1 \u2212 xk ) , the rationalquadratic function in the kth bin takes the form of Eq. 24 and the Jacobian determinant of the rationalquadratic neural spline flows (RQNSF) can be written as in Eq. 25.\n\u03b1(k)(\u03be) \u03b2(k)(\u03be) = y(k) + (y(k+1)\u2212y(k))[s(k)\u03be2+\u03b4(k)\u03be(1\u2212\u03be)] s(k)+[\u03b4(k+1)+\u03b4(k)\u22122s(k)]\u03be(1\u2212\u03be)\n(24) det J = \u220f k d dx\n[ \u03b1(k)(\u03be)\n\u03b2(k)(\u03be)\n]\n= \u220f k (s(k)) 2 [\u03b4(k+1)\u03be2+2s(k)\u03be(1\u2212\u03be)+\u03b4(k)(1\u2212\u03be)2]\n[s(k)+[\u03b4(k+1)+\u03b4(k)\u22122s(k)]\u03be(1\u2212\u03be)] 2\n(25) The rational-linear neural spline flows (RLNSF) work similarly, except with monotonically increasing linear rational functions in each bin. Neural splines combine the best of autoregressive flows\n17A rational-quadratic function is defined as the quotient of two quadratic polynomial functions.\nand coupling layers (such as NICE and RealNVP) in that it has both an analytic single-pass inverse and sufficient flexibility, as demonstrated in Durkan et al. (2019)."
        },
        {
            "heading": "K Example Analysis",
            "text": "In this section, we analyze several instances from CNN/Daily Mail and XSum, showcasing diverse outcomes generated by different summarization models.18\n18It is worth mentioning that a few of the grammatical errors in the summaries can be attributed to the source text itself."
        }
    ],
    "title": "Boosting Summarization with Normalizing Flows and Aggressive Training",
    "year": 2023
}