{
    "abstractText": "With the rapid advancement in deep generative models, recent neural Text-To-Speech (TTS) models have succeeded in synthesizing humanlike speech. There have been some efforts to generate speech with various prosody beyond monotonous prosody patterns. However, previous works have several limitations. First, typical TTS models depend on the scaled sampling temperature for boosting the diversity of prosody. Speech samples generated at high sampling temperatures often lack perceptual prosodic diversity, thereby hampering the naturalness of the speech. Second, the diversity among samples is neglected since the sampling procedure often focuses on a single speech sample rather than multiple ones. In this paper, we propose DPP-TTS: a text-to-speech model based on Determinantal Point Processes (DPPs) with a new objective function and prosody diversifying module. Our TTS model is capable of generating speech samples that simultaneously consider perceptual diversity in each sample and among multiple samples. We demonstrate that DPP-TTS generates speech samples with more diversified prosody than baselines in the side-by-side comparison test considering the naturalness of speech at the same time.",
    "authors": [
        {
            "affiliations": [],
            "name": "Seongho Joo"
        },
        {
            "affiliations": [],
            "name": "Hyukhun Koh"
        },
        {
            "affiliations": [],
            "name": "Kyomin Jung"
        }
    ],
    "id": "SP:5d7c87e5332a91c23e8aac4516eadf2762365491",
    "references": [
        {
            "authors": [
                "Jimmy Ba",
                "Jamie Ryan Kiros",
                "Geoffrey E. Hinton."
            ],
            "title": "Layer normalization",
            "venue": "ArXiv, abs/1607.06450.",
            "year": 2016
        },
        {
            "authors": [
                "Mathieu Blondel",
                "A. Mensch",
                "Jean-Philippe Vert."
            ],
            "title": "Differentiable divergences between time series",
            "venue": "AISTATS.",
            "year": 2021
        },
        {
            "authors": [
                "Jianfei Chen",
                "Cheng Lu",
                "Biqi Chenli",
                "J. Zhu",
                "Tian Tian."
            ],
            "title": "Vflow: More expressive generative flows with variational data augmentation",
            "venue": "ArXiv, abs/2002.09741.",
            "year": 2020
        },
        {
            "authors": [
                "Sangwoo Cho",
                "Chen Li",
                "Dong Yu",
                "H. Foroosh",
                "Fei Liu."
            ],
            "title": "Multi-document summarization with determinantal point processes and contextualized representations",
            "venue": "ArXiv, abs/1910.11411.",
            "year": 2019
        },
        {
            "authors": [
                "Marco Cuturi",
                "Mathieu Blondel."
            ],
            "title": "Soft-dtw: a differentiable loss function for time-series",
            "venue": "ICML.",
            "year": 2017
        },
        {
            "authors": [
                "Marco Cuturi",
                "Jean-Philippe Vert",
                "\u00d8ystein Birkenes",
                "T. Matsui."
            ],
            "title": "A kernel for time series based on global alignments",
            "venue": "2007 IEEE International Conference on Acoustics, Speech and Signal Processing ICASSP \u201907, 2:II\u2013413\u2013II\u2013416.",
            "year": 2007
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "ArXiv, abs/1810.04805.",
            "year": 2019
        },
        {
            "authors": [
                "Jeff Donahue",
                "S. Dieleman",
                "Mikolaj Binkowski",
                "Erich Elsen",
                "K. Simonyan."
            ],
            "title": "End-to-end adversarial text-to-speech",
            "venue": "ArXiv, abs/2006.03575.",
            "year": 2021
        },
        {
            "authors": [
                "Conor Durkan",
                "Artur Bekasov",
                "Iain Murray",
                "George Papamakarios."
            ],
            "title": "Neural spline flows",
            "venue": "NeurIPS.",
            "year": 2019
        },
        {
            "authors": [
                "Mike Gartrell",
                "Insu Han",
                "Elvis Dohmatob",
                "Jennifer Gillenwater",
                "Victor-Emmanuel Brunel."
            ],
            "title": "Scalable learning and map inference for nonsymmetric determinantal point processes",
            "venue": "ArXiv, abs/2006.09862.",
            "year": 2021
        },
        {
            "authors": [
                "Jennifer Gillenwater",
                "A. Kulesza",
                "Sergei Vassilvitskii",
                "Zelda E. Mariet."
            ],
            "title": "Maximizing induced cardinality under a determinantal point process",
            "venue": "NeurIPS.",
            "year": 2018
        },
        {
            "authors": [
                "Jonathan Ho",
                "Xi Chen",
                "A. Srinivas",
                "Yan Duan",
                "P. Abbeel."
            ],
            "title": "Flow++: Improving flow-based generative models with variational dequantization and architecture design",
            "venue": "ArXiv, abs/1902.00275.",
            "year": 2019
        },
        {
            "authors": [
                "Wei-Ning Hsu",
                "Y. Zhang",
                "Ron J. Weiss",
                "H. Zen",
                "Yonghui Wu",
                "Yuxuan Wang",
                "Yuan Cao",
                "Ye Jia",
                "Z. Chen",
                "Jonathan Shen",
                "P. Nguyen",
                "Ruoming Pang."
            ],
            "title": "Hierarchical generative modeling for controllable speech synthesis",
            "venue": "ArXiv, abs/1810.07217.",
            "year": 2019
        },
        {
            "authors": [
                "Tom Kenter",
                "Manish Sharma",
                "Robert A.J. Clark."
            ],
            "title": "Improving the prosody of rnn-based english text-to-speech synthesis by incorporating a bert model",
            "venue": "INTERSPEECH.",
            "year": 2020
        },
        {
            "authors": [
                "Jaehyeon Kim",
                "Sungwon Kim",
                "Jungil Kong",
                "Sungroh Yoon."
            ],
            "title": "Glow-tts: A generative flow for text-to-speech via monotonic alignment search",
            "venue": "ArXiv, abs/2005.11129.",
            "year": 2020
        },
        {
            "authors": [
                "Jaehyeon Kim",
                "Jungil Kong",
                "Juhee Son."
            ],
            "title": "Conditional variational autoencoder with adversarial learning for end-to-end text-to-speech",
            "venue": "ArXiv, abs/2106.06103.",
            "year": 2021
        },
        {
            "authors": [
                "Jungil Kong",
                "Jaehyeon Kim",
                "Jaekyoung Bae."
            ],
            "title": "Hifi-gan: Generative adversarial networks for efficient and high fidelity speech synthesis",
            "venue": "Advances in Neural Information Processing Systems, volume 33, pages 17022\u201317033. Curran Associates,",
            "year": 2020
        },
        {
            "authors": [
                "A. Kulesza",
                "B. Taskar."
            ],
            "title": "Determinantal point processes for machine learning",
            "venue": "Found. Trends Mach. Learn., 5:123\u2013286.",
            "year": 2012
        },
        {
            "authors": [
                "Alex Kulesza",
                "Ben Taskar."
            ],
            "title": "Structured determinantal point processes",
            "venue": "NIPS.",
            "year": 2010
        },
        {
            "authors": [
                "Adrian La\u0144cucki."
            ],
            "title": "Fastpitch: Parallel text-tospeech with pitch prediction",
            "venue": "ICASSP.",
            "year": 2021
        },
        {
            "authors": [
                "Yoonhyung Lee",
                "Joongbo Shin",
                "Kyomin Jung."
            ],
            "title": "Bidirectional variational inference for nonautoregressive text-to-speech",
            "venue": "ICLR.",
            "year": 2021
        },
        {
            "authors": [
                "N. Li",
                "Shujie Liu",
                "Yanqing Liu",
                "Sheng Zhao",
                "Ming Liu."
            ],
            "title": "Neural speech synthesis with transformer network",
            "venue": "AAAI.",
            "year": 2019
        },
        {
            "authors": [
                "Jinglin Liu",
                "Chengxi Li",
                "Yi Ren",
                "Feiyang Chen",
                "Zhou Zhao."
            ],
            "title": "Diffsinger: Singing voice synthesis via shallow diffusion mechanism",
            "venue": "AAAI Conference on Artificial Intelligence.",
            "year": 2021
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter."
            ],
            "title": "Decoupled weight decay regularization",
            "venue": "International Conference on Learning Representations.",
            "year": 2019
        },
        {
            "authors": [
                "Diganta Misra."
            ],
            "title": "Mish: A self regularized non-monotonic neural activation function",
            "venue": "ArXiv, abs/1908.08681.",
            "year": 2019
        },
        {
            "authors": [
                "Thi-Thu-Trang Nguyen",
                "Nguyen Hoang Ky",
                "Albert Rilliard",
                "Christophe d\u2019Alessandro"
            ],
            "title": "Prosodic boundary prediction model for vietnamese text-tospeech",
            "venue": "In Interspeech",
            "year": 2020
        },
        {
            "authors": [
                "Wei Ping",
                "Kainan Peng",
                "Andrew Gibiansky",
                "Sercan \u00d6. Arik",
                "Ajay Kannan",
                "Sharan Narang",
                "Jonathan Raiman",
                "John Miller."
            ],
            "title": "Deep voice 3: Scaling text-to-speech with convolutional sequence learning",
            "venue": "arXiv: Sound.",
            "year": 2018
        },
        {
            "authors": [
                "Yi Ren",
                "Chenxu Hu",
                "Xu Tan",
                "Tao Qin",
                "Sheng Zhao",
                "Zhou Zhao",
                "Tie-Yan Liu."
            ],
            "title": "Fastspeech 2: Fast and high-quality end-to-end text to speech",
            "venue": "ArXiv, abs/2006.04558.",
            "year": 2021
        },
        {
            "authors": [
                "Yi Ren",
                "Jinglin Liu",
                "Zhou Zhao."
            ],
            "title": "Portaspeech: Portable and high-quality generative textto-speech",
            "venue": "Advances in Neural Information Processing Systems.",
            "year": 2021
        },
        {
            "authors": [
                "Yi Ren",
                "Yangjun Ruan",
                "Xu Tan",
                "Tao Qin",
                "Sheng Zhao",
                "Zhou Zhao",
                "Tie-Yan Liu."
            ],
            "title": "Fastspeech: Fast, robust and controllable text to speech",
            "venue": "NeurIPS.",
            "year": 2019
        },
        {
            "authors": [
                "Angelien A. Sanderman",
                "Ren\u00e9 Collier."
            ],
            "title": "Prosodic phrasing and comprehension",
            "venue": "Language and Speech, 40:391 \u2013 409.",
            "year": 1997
        },
        {
            "authors": [
                "Peter Shaw",
                "Jakob Uszkoreit",
                "Ashish Vaswani."
            ],
            "title": "Self-attention with relative position representations",
            "venue": "NAACL.",
            "year": 2018
        },
        {
            "authors": [
                "Jonathan Shen",
                "Ruoming Pang",
                "Ron J. Weiss",
                "M. Schuster",
                "Navdeep Jaitly",
                "Zongheng Yang",
                "Z. Chen",
                "Yu Zhang",
                "Yuxuan Wang",
                "R. Skerry-Ryan",
                "R. Saurous",
                "Yannis Agiomyrgiannakis",
                "Yonghui Wu"
            ],
            "title": "Natural tts synthesis by conditioning",
            "year": 2018
        },
        {
            "authors": [
                "R. Skerry-Ryan",
                "Eric Battenberg",
                "Y. Xiao",
                "Yuxuan Wang",
                "Daisy Stanton",
                "Joel Shor",
                "Ron J. Weiss",
                "R. Clark",
                "R. Saurous."
            ],
            "title": "Towards end-toend prosody transfer for expressive speech synthesis with tacotron",
            "venue": "ArXiv, abs/1803.09047.",
            "year": 2018
        },
        {
            "authors": [
                "G. Sun",
                "Y. Zhang",
                "Ron J. Weiss",
                "Yuanbin Cao",
                "H. Zen",
                "Yonghui Wu."
            ],
            "title": "Fully-hierarchical finegrained prosody modeling for interpretable speech synthesis",
            "venue": "ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Process-",
            "year": 2020
        },
        {
            "authors": [
                "Antti Suni",
                "Sofoklis Kakouros",
                "Martti Vainio",
                "Juraj \u0160imko."
            ],
            "title": "Prosodic prominence and boundaries in sequence-to-sequence speech synthesis",
            "venue": "ArXiv, abs/2006.15967.",
            "year": 2020
        },
        {
            "authors": [
                "Aarne Talman",
                "Antti Suni",
                "Hande Celikkanat",
                "Sofoklis Kakouros",
                "J\u00f6rg Tiedemann",
                "Martti Vainio."
            ],
            "title": "Predicting prosodic prominence from text with pretrained contextualized word representations",
            "venue": "Proceedings of the 22nd Nordic Conference on Compu-",
            "year": 2019
        },
        {
            "authors": [
                "Rafael Valle",
                "Kevin J. Shih",
                "R. Prenger",
                "Bryan Catanzaro."
            ],
            "title": "Flowtron: an autoregressive flowbased generative network for text-to-speech synthesis",
            "venue": "ArXiv, abs/2005.05957.",
            "year": 2021
        },
        {
            "authors": [
                "Iv\u00e1n Vall\u00e9s-P\u00e9rez",
                "Julian Roth",
                "Grzegorz Beringer",
                "R. Barra-Chicote",
                "J. Droppo."
            ],
            "title": "Improving multi-speaker tts prosody variance with a residual encoder and normalizing flows",
            "venue": "ArXiv, abs/2106.05762.",
            "year": 2021
        },
        {
            "authors": [
                "Vincent Wan",
                "Chun an Chan",
                "Tom Kenter",
                "Jakub V\u00edt",
                "Robert A.J. Clark."
            ],
            "title": "Chive: Varying prosody in speech synthesis with a linguistically driven dynamic hierarchical conditional variational network",
            "venue": "ArXiv, abs/1905.07195.",
            "year": 2019
        },
        {
            "authors": [
                "Yuxuan Wang",
                "Daisy Stanton",
                "Yu Zhang",
                "R. SkerryRyan",
                "Eric Battenberg",
                "Joel Shor",
                "Y. Xiao",
                "Fei Ren",
                "Ye Jia",
                "R. Saurous."
            ],
            "title": "Style tokens: Unsupervised style modeling, control and transfer in end-to-end speech synthesis",
            "venue": "ICML.",
            "year": 2018
        },
        {
            "authors": [
                "Yunyang Xiong",
                "Zhanpeng Zeng",
                "Rudrasis Chakraborty",
                "Mingxing Tan",
                "Glenn M. Fung",
                "Yin Li",
                "Vikas Singh."
            ],
            "title": "Nystr\u00f6mformer: A nystr\u00f6m-based algorithm for approximating self-attention",
            "venue": "AAAI.",
            "year": 2021
        },
        {
            "authors": [
                "Guanghui Xu",
                "Wei Song",
                "Zhengchen Zhang",
                "Chao Zhang",
                "Xiaodong He",
                "Bowen Zhou."
            ],
            "title": "Improving prosody modelling with cross-utterance bert embeddings for end-to-end speech synthesis",
            "venue": "ICASSP 2021 - 2021 IEEE International Confer-",
            "year": 2020
        },
        {
            "authors": [
                "Zhenhui Ye",
                "Rongjie Huang",
                "Yi Ren",
                "Ziyue Jiang",
                "Jinglin Liu",
                "Jinzheng He",
                "Xiang Yin",
                "Zhou Zhao"
            ],
            "title": "Clapspeech: Learning prosody from text context with contrastive language-audio pre-training",
            "year": 2023
        },
        {
            "authors": [
                "Zhenhui Ye",
                "Zhou Zhao",
                "Yi Ren",
                "Fei Wu."
            ],
            "title": "Syntaspeech: Syntax-aware generative adversarial text-to-speech",
            "venue": "ArXiv, abs/2204.11792.",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "In the past few years, Text-To-Speech (TTS) models have made a lot of progress in synthesizing human-like speech (Li et al., 2019; Ping et al., 2018; Ren et al., 2019; Shen et al., 2018; Kim et al., 2021). Furthermore, in the latest studies, several TTS models made high-quality speech samples even in the end-to-end setting without a two-stage synthesis process (Donahue et al., 2021; Kim et al., 2021). Based on these technical developments, TTS models are now able to generate high-fidelity speech.\nMeanwhile, human speech contains diverse prosody patterns regarding intonation, stress, and\n\u2020 Corresponding author\nrhythm beyond the fidelity of speech. To reflect such acoustic features on generated speech, there have been many attempts to synthesize speech with rich and diverse prosodic patterns. One of the widely used approaches for prosody modeling is to exploit generative models like VAEs and flow models (Hsu et al., 2019; Lee et al., 2021; Ren et al., 2021b; Valle et al., 2021; Vall\u00e9s-P\u00e9rez et al., 2021). These generative TTS models control the extent of variation in speech by sampling prior distribution with adequate temperatures. In other works, auxiliary features such as syntax information and text semantics from BERT embeddings are used to enhance the prosody of speech (Ye et al., 2022; Xu et al., 2020).\nHowever, previous approaches are subject to several limitations. First, sampling with high temperatures for generating diverse prosody patterns often severely degrades the naturalness of speech. In the experiment section, we show that previous TTS models often fail to generate diverse and smooth speech samples to the listeners in diverse sampling temperature settings. Second, previous works for prosody modeling treat each sample independently, thereby not guaranteeing the diversity among separately generated samples.\nIn this paper, for generating diverse speech samples, we resolve the two aforementioned limitations by adopting Determinantal point processes (DPPs), which have typically shown great results in modeling diversity among multiple samples in various machine learning tasks such as text summarization (Cho et al., 2019) and recommendation systems (Gartrell et al., 2021). We devise a novel TTS model equipped with the new objective function and prosody diversifying module based on DPPs. Through our adaptive MIC objective function and DPP kernel, we can effectively generate speech samples with diverse and natural prosody. However, with the standard DPP, it is challenging to model a fine-grained level of prosody for non-monotonous speech. For more sophisticated prosody modeling, we also propose conditional DPPs which utilize the conditional information for the sampling. Specifically, we first segment the whole input text into more fine-grained text segments, and then sample prosodic features for the targeted segment, reflecting its neighbor segments to generate more expressive samples.\nIn the process of adopting conditional DPPs into our TTS models, two technical difficulties must be handled. First, the range of targets for the sampling of conditional DPP is ambiguous. Second, prosodic features usually vary in length. We resolve the first issue by extracting key segments from the input text using Prosodic Boundary Detector (PBD). Next, we resolve the second issue by adopting the similarity metric, soft dynamic time warping discrepancy (Soft-DTW) (Cuturi and Blondel, 2017), among the segments of variable lengths.\nExperimental results demonstrate that our training methodology becomes stable with the new adaptive MIC objective function, overcoming the instability of sampling-based learning. In addition, our DPP-TTS is highly effective at generating more expressive speech and guaranteeing the quality of speech. We find that our model consistently gains a higher diversity score across all temperature settings than baselines.\nIn summary, our contributions of the paper are as follows:\n\u2022 We show that the fine-grained level of the prosody modeling method based on PBD contributes to more diverse prosody in each generated speech sample.\n\u2022 We generate more diverse prosody patterns\nfrom a single utterance by adopting conditional DPPs into our TTS model with the novel objective function.\n\u2022 We evaluate and demonstrate that our model outperforms the baselines in terms of prosody diversity while showing more stable naturalness."
        },
        {
            "heading": "2 Background",
            "text": ""
        },
        {
            "heading": "2.1 Determinantal point processes",
            "text": "Given ground set Y which consists of prosodic features in our case, a determinantal point processes (DPP) defines a probability distribution for all subsets of Y via a positive semi-definite matrix L as followings:\nPL(Y ) = det(LY )\ndet(L+ I) , (1)\nwhere det(\u00b7) is the determinant of a matrix, LY denotes the submatrix of L whose entries are indexed by the subset Y and det(L + I) in the denominator acts as a normalization constant. To model diversity between items, the DPP kernel L is usually constructed as a symmetric similarity matrix S, where Sij represents the similarity between two items xi and xj . Kulesza and Taskar (2010) propose decomposing the kernel L as a Gram matrix incorporating a quality vector to weigh each item according to its quality by defining the kernel matrix as Li,j = qi \u00b7 Sij \u00b7 qj where qi denotes the quality of the item. The quality can be chosen as the likelihood of the prosodic feature in our context. Given two prosodic feature Y1 = {i, j}, the probability PL({i, j}) to sample two prosodic features is proportional to det(LY1) = q 2 i \u00b7 q2j \u00b7 (1 \u2212 S2ij). Therefore, two prosodic features are unlikely sampled together if they are highly similar to each other. In contrast, they are more likely sampled together if they have high quality values."
        },
        {
            "heading": "2.2 Conditional determinantal point processes",
            "text": "If DPPs are used for diversifying prosodic features corresponding to the target utterance, it would result in diversity among generated samples. However, there still can be monotonous patterns in each generated utterance. To resolve this issue, it is necessary to model the prosodic features of the target accounting into the neighboring segments of the target. In previous studies, DPPs are extended to\nconditional DPPs, for a subset B \u2286 Y not intersecting with A we have\nP(Y = A \u222aB|A \u2286 Y ) = P(Y = A \u222aB) P(A \u2286 Y ) (2)\n= det(LA\u222aB)\ndet(L+ IA\u0304) , (3)\nwhere IA\u0304 is the matrix with ones in the diagonal entries indexed by elements ofY\u2212A and zeros elsewhere. In conditional DPPs, items in the ground set Y can be sampled according to kernel considering given contexts A. In this work, prosodic features corresponding to neighbor segments of the target are used as conditions for conditional DPPs."
        },
        {
            "heading": "2.3 Prosody phrasing",
            "text": "As humans usually speak out with some pauses to convey a message and the speaker\u2019s intention, the utterance is required to be divided into more fine-grained segments, referred as prosodic units. Ye et al. (2023) show that the prosody pattern reflecting relevant text context contributes to the enhanced TTS. Suni et al. (2020); Nguyen et al. (2020) suggest that prosodic boundary plays an important role in the naturalness and intelligibility of speech. To incorporate an inherent prosodic structure of input utterance within its context, we build Prosodic Boundary Detector (PBD) trained on a large corpus along with prominence labels."
        },
        {
            "heading": "3 DPP-TTS",
            "text": "Our model DPP-TTS is composed of base TTS based on FastSpeech2 (Ren et al., 2019), Prosody Diversifying module (PDM), and Prosody boundary Detector (PBD). Once the base TTS is trained, PDM is inserted in front of the prosody predictor and trained with the method which will be described in detail in Section 3.2. We describe the main modules of DPP-TTS in the following subsection 3.1. For the detailed architecture of DPP-TTS, refer to Appendix A."
        },
        {
            "heading": "3.1 Main modules of DPP-TTS",
            "text": "Prosody predictor Our prosody predictor estimates the distribution of the duration and pitch in prosody. For more human-like rhythm and pitch in prosody, the stochastic prosody predictor is built upon normalizing flows. Specifically, the stochastic duration predictor estimates the distribution of phoneme duration and the stochastic pitch predictor estimates the distribution of phoneme-level pitch from the hidden sequence. At the training stage, the prosody predictor learns the mapping from the distribution of prosodic features to normal distribution. At inference, it predicts the phoneme-level duration or pitch by reversing the learned flows. In addition, it also serves as the density estimator for prosodic features during the training of PDM which will be described in detail in the next subsection 3.2. The prosody predictor is trained to max-\nimize a variational lower bound of the likelihood of the phoneme duration or pitch. More details regarding the prosody predictor are in Appendix A.\nPDM Since the prosodic predictor is only trained with the lower bound of likelihood objective, it is not enough to generate diverse and nonmonotonous prosody patterns. For more expressive speech modeling, PDM is added in front of the prosody predictor as shown in Figure 2. Its role is to map samples from a standard normal distribution to another distribution for diverse prosodic features of speech. This module is trained with an objective based on conditional DPPs which is described in Section 2.2. At inference of DPP-TTS, multiple prosodic candidates are generated by PDM. Subsequently, the prosodic feature of speech is selected via MAP inference when a single sample is generated, otherwise, multiple prosodic features are generated via the DPP sampling.\nProsodic Boundary Detector We utilize the prosodic boundary based on the prominence of words, by reflecting the assumption that people unconsciously pronounce the sentence focusing on what intentions they want to convey. To decide the boundary, we create Prosodic Boundary Detector (PBD) whose backbone is a pretrained SentenceTransformer. A Prominence dataset (Talman et al., 2019), consisting of Librispeech scripts and prominence classes, is used for training our PBD. The input of PBD is a text sequence, and PBD predicts each word\u2019s prominence level. Based on the prominence level, the prosodic units are extracted from the input utterance."
        },
        {
            "heading": "3.2 Training process of PDM",
            "text": "In this section, we explain the methodology described in Algorithm 1 for training the prosody diversifying module (PDM). The training process mainly consists of three steps: segmentation of an input text, generation of prosodic feature candidates, and building DPP kernel. We also explain the conditional maximum induced cardinality (MIC) objective for training PDM. The overall training procedure is depicted in Figure 2."
        },
        {
            "heading": "3.2.1 Segmentation of the input text",
            "text": "In this stage, targets in input text for diversification of prosody are chosen. Given a sentence, PBD predicts the positions of prominent words. After that, those positions are used to divide the context and target sequences. Specifically, each target starts\nat the prominent word and ends right before another prominent word. In addition, adjacent left and right contexts with the same number of words for the target are chosen.\nAlgorithm 1 Training of PDM Require: TextEncoder f(\u00b7), PDM parameterized\n\u03b8, a prosody predictor g(\u00b7), number of candidates nc, noise scale \u03f5 1: while not converged do 2: htext \u2190 f(text) 3: Split htext into [htarget, hcontext] and store indices {it, ic} for the target and context 4: Sample latent code zcontext \u2208 RT with noise scale \u03f5 5: Get prosodic features of contexts: dcontext \u2190 [g(htext, zcontext)]ic 6: Sample latent codes ztarget with noise scale \u03f5 7: Get latent codes after PDM: ztarget \u2190 PDM(ztarget) \u2208 Rnc\u00d7T 8: Get nc prosodic features of targets:\n(d1target, ...d nc target) = [g(htext, ztarget)]it\n9: Concatenate contexts and targets: [dcontext, d i target]\n10: Get quality of targets along with contexts: qitarget = Likelihood([dcontext, d i target]) 11: Build the kernel of conditional DPPs: L\u2190 Build kernel(qitarget, [dCon, ditarget]) 12: Calculate the loss function: Ldiversity \u2190 \u2212tr(I \u2212 [(L+ IA\u0304)\u22121]A\u0304) 13: Update \u03b8 with the gradient\u2207Ldiversity 14: end while"
        },
        {
            "heading": "3.2.2 Generation of prosodic feature candidates",
            "text": "In this stage, multiple prosodic candidates are generated for DPP sampling as shown in Figure 2. First, a hidden sequence is generated from an input text through the text encoder. Second, the pretrained prosody predictor generates nc prosodic features conditioned on the hidden sequence by utilizing samples from the normal distribution. Meanwhile, other new samples from a normal distribution are fed into PDM, and then the prosodic predictor conditioned on the hidden sequence generates new prosodic features of the target from the output features of PDM. Finally, the latter-generated target prosodic features substitute former-generated target features, and then nc prosodic candidates are generated with the target and context entangled."
        },
        {
            "heading": "3.2.3 Construction of DPP kernel",
            "text": "Generated candidates are split into left and right context dL, dR and n targets d1, d2, ...dn, then the candidate set for DPP is constructed as shown in the Figure 2. Next, the kernel of conditional DPP is built by incorporating both the diversity and quality(likelihood) of each candidate feature. Here, quality features guarantee the smooth transition of prosody. The kernel of conditional DPPs is defined as L = diag(q) \u00b7 S \u00b7 diag(q), where S is the similarity matrix and q is the quality vector."
        },
        {
            "heading": "3.3 Objective function",
            "text": "Similarity metric In the process of constructing the DPP kernel, we need to define the similarity metric between features. However, target sequences and context sequences often vary in length. As the Euclidean distance is not applicable to calculate the similarity between two sequences, we utilize Soft DTW:\nSi,j = exp(\u2212dtwD\u03b3 (di,dj)) (4)\n, where dtwD\u03b3 denotes soft-DTW discrepancy with a metric D and smoothing parameter \u03b3. When the metric D is chosen as the L1 distance D(x,y) =\u2211\ni |xi \u2212 yi| or half gaussian D(x,y) = ||x \u2212 y||22 + log(2 \u2212 exp(\u2212||x\u2212 y||22)), the similarity matrix becomes positive semi-definite (Blondel et al., 2021; Cuturi et al., 2007). In this work, L1 distance is used as the metric of Soft-DTW so that S to be positive semi-definite.\nQuality metric To reflect the naturalness of prosodic features, quality scores are calculated based on the estimated density of predicted features. Given the features x, posterior q(zi|x;\u03d5) and joint likelihood p(x, zi; \u03b8) where \u03d5, \u03b8 are parameters of the prosody predictor based on the variational method, the density values of predicted features are calculated with importance sampling using the prosody predictor: p(x; \u03b8, \u03d5) \u2248 \u2211N i=1 p(x,zi;\u03b8) q(zi|x;\u03d5) . In the experiment, we empirically find that it is more helpful not to give a penalty to the quality score if the likelihood is greater than the specific threshold. With log-likelihood \u03c0(x) = log p(x), the quality score of single sample is defined as\nq(x) =\n{ w if \u03c0(x) >= k\nw \u00b7 exp(\u03c0(x)\u2212 k) otherwise (5)\n, where w is a quality weight and the threshold value k was set as the average density of the training dataset in the experiment. We need to measure the diversity with respect to kernel L to train the PDM. One straightforward choice is the maximum likelihood (MLE) objective, logPL(Y ) = log det(LY )\u2212 log det(L+I). However, there are some cases where almost identical prosodic features are predicted. We recognize that such cases cause the objective value to become near zero, only to make the training process unstable. Instead, maximum induced cardinality (MIC) (Gillenwater et al., 2018) objective which is defined as EY \u223cPL[|Y |] can be an alternative. It does not suffer from training instability. In this work, context segments dL, dR are used as the condition in the MIC objective of conditional DPPs. The objective function with respect to the candidate set [dL, dR, d1, d2, ..., dN ] and its derivative are as follows: Proposition 1 (MIC objective of CDPPs) With respect to the candidate set [dL, dR, d1, d2, ..., dN ], the MIC objective of conditional DPPs and its derivative are as follows:\nLMIC = tr(I \u2212 [(L(\u03b8) + IA\u0304)\u22121]A\u0304), (6)\n\u2202LMIC \u2202\u03b8 = ((L+ IA\u0304) \u22121IA\u0304(L+ IA\u0304) \u22121)T \u2202L \u2202\u03b8 ,\n(7) where A denotes the set of contexts (dL, dR), A\u0304 denotes the complement of the set A and \u03b8 is the parameter of PDM.\nRemark 1 The MLE objective becomes unstable since the determinant volume is close to zero if two similar items are included. In contrast, the MIC objective guarantees stability as the gradient of our objective guarantees the full-rank structure.\nDetailed proof is presented in Appendix B. At inference, for predicting a single prosodic feature, MAP inference is performed across sets with just a single item as follows: x\u2217 = argmaxx\u2208A\u0304 log det(L{x}\u222aA). Otherwise, the kDPP sampling method is used for sampling multiple prosodic features when multiple speech samples are generated. The detailed procedure of training PDM and the inference of DPP-TTS are in Appendix C."
        },
        {
            "heading": "4 Experiment setup",
            "text": ""
        },
        {
            "heading": "4.1 Dataset and preprocessing",
            "text": "We conduct experiments on the LJSpeech dataset which consists of audio clips with approximately\n24 hours lengths. We split audio samples into 12500/100/500 samples for the training, validation, and test set. Audio samples with 22kHz sampling rate are transformed into 80 bands melspectrograms through the Short-time Fourier transform (STFT) with 1024 window size and 256 hop length. International Phonetic Alphabet (IPA) sequences are used as input for phoneme encoder. Text sequences are converted to IPA phoneme sequences using Phonemizer1 software. Following (Kim et al., 2020), the converted sequences are interspersed with blank tokens, which represent the transition from one phoneme to another phoneme."
        },
        {
            "heading": "4.2 Implementation Details",
            "text": "Both the base TTS and PDM are trained using the AdamW optimizer (Loshchilov and Hutter, 2019) with \u03b21 = 0.8, \u03b22 = 0.99 and \u03bb = 0.01. The initial learning rate is 2\u00d7 10\u22124 for the basic TTS and 1 \u00d7 10\u22125 for the PDM with exponential learning decay. Base TTS is trained with 128 batch size for 270k steps on 4 NVIDIA RTX A5000 GPUs. PDM is trained with 8 batch size for 2k steps on single GPU. The quality weight of the model is set as w = 10. We prepare two DPP-TTS versions for the evaluation: (1): DPP-TTS-d: a model that has the duration diversifying module and DPP-TTS-p: a model that has the pitch diversifying module."
        },
        {
            "heading": "4.3 Baselines",
            "text": "We compare our model with the following stateof-the-art models: 1) VITS2 (Kim et al., 2021), an end-to-end TTS model based on conditional VAE and normalizing flows. It mainly has two parameters for the sampling: the standard deviation of input noise \u03c3 to duration predictor and a scale factor \u03c4 to the standard deviation of prior distribution which controls other variations of speech (e.g., pitch and energy); 2) Flowtron3 (Valle et al., 2021), an autoregressive flow-based TTS model; 3) DiffSpeech4 (Liu et al., 2021), a diffusionbased probabilistic text-to-speech model; 4) SyntaSpeech5 (Ye et al., 2022), a TTS model using a syntactic graph of input sentence for prosody modeling. HiFi-GAN (Kong et al., 2020) is used as the vocoder for synthesizing waveforms from\n1https://github.com/bootphon/phonemizer 2https://github.com/jaywalnut310/vits 3https://github.com/NVIDIA/flowtron 4https://github.com/MoonInTheRiver/DiffSinger 5https://github.com/yerfor/SyntaSpeech\nthe mel-spectrograms. In addition, we also compare our model with baseline DPP-TTS w/o PDM which does not utilize PDM for prosody modeling. Audio samples used for the evaluation are in the supplementary material and the Demo page6."
        },
        {
            "heading": "4.4 Evaluation Method",
            "text": "Side-by-Side evaluation To evaluate the perceptual diversity of our model, we conduct a side-byside evaluation of the prosody of speech samples. Via Amazon Mechanical Turk (AMT), we assign ten testers living in the United States to a pair of audio samples (i.e., DPP-TTS and a baseline), and ask them to listen to audio samples and choose among three options: A: sample A has more varied prosody than sample B, Same: sample A and B are equally varied in prosody, B: the opposite of first option. For DPP-TTS-d, testers are asked to focus on the rhythmic variation of the speech sample other than different aspects of prosody. Likewise for DPP-TTS-p, the testers are asked to focus on the pitch variation of speech samples. Importantly, the testers are asked to ignore about pace, and volume of speech samples to eliminate possible bias on speech samples as possible.\nMOS In addition, we also conduct the MeanOpinion-Score (MOS) to evaluate the naturalness of prosody for generated samples. Ten testers are assigned to each audio sample. Given reference speech samples to each score, testers are asked to give a score between 1 to 5 on a 9-scale based on the sample\u2019s naturalness, In addition, they are asked to focus on the prosody aspect of audio samples.\nQuantitative evaluation In addition to human evaluation, We conduct quantitative evaluations for our DPP-TTS, DPP-TTS w/o PDM, and VITS with high temperature 7. We have used the following metrics for evaluating our model and the baseline. \u03c3p: phoneme-level standard deviation of duration or pitch in a speech. This metric reflects the prosodic diversity inside each speech sample.\nDeterminant: a determinant of the similarity matrix is used to evaluate the diversity among prosodic features of 10 generated samples. Cosine similarity is used as a metric between two features. Since dissimilar items increase the volume of the matrix, higher determinant values indicate that generated samples are more diverse.\n6https://dpp-tts.github.io/ 7Same counterparts as the side-by-side evaluation are used.\nInference time: the inference time for synthesizing a waveform is calculated in the TTS model. The inference speed is evaluated on Intel(R) Core(TM) i7-7800X CPU and a single NVIDIA RTX 3080 GPU. Computation time is averaged over 100 forward passes."
        },
        {
            "heading": "5 Results",
            "text": ""
        },
        {
            "heading": "5.1 Perceptual Diversity Results",
            "text": "We first report the side-by-side evaluation results between our model and baselines along with the MOS test. Results are shown in Table 1. We observe three findings from the evaluation: 1) Scaling temperature of Flowtron and VITS model does not contribute much to the perceptual diversity of rhythm and pitch while the MOS degrades by a large margin; 2) Our DPP-TTS model outperforms the DPP-TTS w/o PDM in the side-byside evaluation which proves the usefulness PDM with MIC training objective; 3) Our model DPPTTS outperforms the four baselines in the sideby-side evaluation for rhythm and pitch comparison. Since LJSpeech dataset consists of short text transcripts, we conduct the side-by-side evaluation for paragraph samples to evaluate prosody modeling in longer input texts. As the evaluation of the LJSpeech dataset, listeners are asked to choose among the three options. In the side-by-side evaluation of prosody in a paragraph, both the DPPTTS-d/p outperform the baseline as Table 2 shows. More testers voted for our model in the evaluation for paragraph than the LJSpeech dataset. We speculate that this is due to diverse prosody patterns\nbeing more prominent in longer texts."
        },
        {
            "heading": "5.2 Quantitative evaluation",
            "text": "Table 3 shows the result of \u03c3p, determinant, and inference time8. In both duration and pitch \u03c3p, DPP-TTS outperforms baselines by having a higher standard deviation in the phoneme-level features. It demonstrates that DPP-TTS generates a speech with more dynamic pitch and rhythm than the baseline. The determinants of duration and pitch sets of\n8If the model does not have a separate duration predictor like VITS, it is challenging to evaluate the duration of phonemes, therefore only the pitch evaluation is included for other baselines.\nTable 4: Side-by-side comparison between the model with PBD and the fixed-length baseline without PBD.\nModel A/B A Same B Model A MOS Model B MOS\nDPP-TTS-d/Fixed-length 58.2% 24.5% 17.3% 4.02\u00b1 0.08 3.78\u00b1 0.06 DPP-TTS-p/Fixed-length 61.8% 23.4% 14.8% 3.98\u00b1 0.08 3.78\u00b1 0.06\nFigure 3: Loss trajectory for the MIC and MLE objective. While the training dynamic of MLE objective is unstable due to the structure, the MIC objective shows faster convergence.\nDPP-TTS also outperform the baseline. It shows that DPP-TTS generates more samples with diverse prosody than baselines. Finally, DPP-TTS results in 0.044 seconds of inference speed. Although its inference speed is slower than the baseline, our model is applicable in practice since the inference speed of our model is 22.7x faster than real-time."
        },
        {
            "heading": "5.3 Model analysis",
            "text": "MIC vs MLE objective for training PDM We conduct ablation studies to verify the effectiveness of MIC training objective for conditional DPPs. Fig 3 shows the training loss trajectory. The norm of the gradient for MLE objective blows up when prosodic features in the candidate set are nearly identical leading to unstable training. In contrast, since our MIC objective guarantees the full rank structure of the gradient matrix, the training does not suffer from instability, leading to faster convergence.\nAdjusting the extent of variation To study the impact of quality weight w in quality metric q(x) for DPP sampling, we generate multiple speech samples with different values of quality weight. For a large magnitude of quality weight, our model likely generates smoother prosody patterns. As the magnitude of quality weight w decreases, the\nmodel starts to generate more dynamic and diverse prosody patterns. For examples of the pitch contour from generated samples, refer to Appendix G.\nCase study To analyze why and how the naturalness drops after applying PDM, we investigate some samples that get low MOS by testers. The degradation cases mainly fall into two cases. First, in some utterances, the prosodic transition between adjacent words arises too rapidly. A quality metric that imposes a penalty for the rapid transition may address this problem. Second, speech realization at a particular level of pitch is somewhat awkward. This problem is likely to appear since the TTS model has not seen enough prosody patterns during the training stage. We believe that speech augmentation related to pitch or duration will address this problem.\nEffectiveness of PBD We conduct an experiment to verify the effectiveness of PBD over the fixedlength baseline. While PBD dynamically adjusts the length of the target considering the prosodic boundary, the fixed-length baseline just extracts the target of fixed length. For example, with length n = 3, the first three words are selected as the context, the next three words are selected as the target, and the next three words are selected as the context, and so on. We can see from Table 4, both the perceptual diversity and MOS of the baseline decrease compared to the model using PBD. It indicates the advantage of PBD which dynamically adjusts the target length considering the prosodic boundary."
        },
        {
            "heading": "6 Related Works",
            "text": "There have been many efforts in TTS research to enhance the expressivity of generated speech. Learning latent prosody embedding at the sentence level is proposed to generate more expressive speech (Skerry-Ryan et al., 2018; Wang et al., 2018). Wan et al. (2019) presents a hierarchical conditional VAE model to generate speech with more expressive prosody and Sun et al. (2020) propose a hierarchical VAE-based model for fine-grained prosody modeling. In addition, a speech syn-\nthesis model incorporating linguistic information BERT (Devlin et al., 2019) is proposed to get enriched text representation by (Kenter et al., 2020). However, the controllability of speech attributes like pitch and rhythm is not fully resolved and the expressivity or diversity of generated speech is still far off the human.\nMeanwhile, since prosody is directly related to duration, pitch, and energy, text-to-speech models that explicitly control these features are proposed (Lan\u0301cucki, 2021; Ren et al., 2021a). A flowbased stochastic duration predictor is also proposed to generate speech with more diverse rhythms and it shows superior performance compared to a deterministic duration predictor (Kim et al., 2021). In this work, we incorporate a stochastic duration, and pitch predictor upon Fastspeech2 (Ren et al., 2021a) to model more expressive prosody."
        },
        {
            "heading": "7 Conclusion",
            "text": "We propose a Prosody Diversifying Module (PDM) that explicitly diversifies prosodic features like duration or pitch to avoid a monotonous speaking style, using conditional MIC objective. Previous research has indicated that well-positioned prosodic boundaries are helpful for understanding the meaning of speech (Sanderman and Collier, 1997). There are some possible avenues to improve our model for more realistic prosody. A more sophisticated segmentation rule that reflects human prosodic boundaries can improve the naturalness of our model. Incorporating linguistic representation well-aligned with human intonation contributes to accurate prosodic boundaries. Such methods will boost the performance of our model.\nLimitations\nSince our methodology depends on the segmentation of input utterances, a sophisticated boundary for input utterances is necessary. We believe that more advanced research for prosody boundary detectors will contribute to smoother and more expressive prosody for TTS. The expression of speech at a certain pitch level can be somewhat unnatural. This issue probably arises because the TTS model hasn\u2019t been sufficiently exposed to prosody patterns during its training phase. We believe that improvements related to pitch or duration in speech augmentation could solve this issue. We plan to resolve this problem by using an advanced augmentation method and large pretrained models. For the\nquality metric for building DPP kernel, we have used the density value of each sample. We use an importance sample scheme for the value estimation, however, it needs multiple samples for exact evaluation. We believe a more efficient sampling scheme contributes to generating a more exact evaluation of the naturalness of prosodic features."
        },
        {
            "heading": "Acknowledgements",
            "text": "We thank anonymous reviewers for their constructive and insightful comments. K. Jung is with ASRI, Seoul National University, Korea. This work was supported by Samsung Electronics. This work was partly supported by Institute of Information communications Technology Planning Evaluation (IITP) grant funded by the Korea government(MSIT) [NO.2021-0-01343, Artificial Intelligence Graduate School Program (Seoul National University)]."
        },
        {
            "heading": "A Details of DPP-TTS",
            "text": "Our model DPP-TTS is composed of a Seq2Seq module for generating the mel-spectrogram, a prosody predictor for predicting duration and pitch sequences, and a prosody diversifying module (PDM). At the first stage, the base TTS model which consists of the Seq2Seq module and the prosody predictor is trained as shown in Figure 4a. Once the base TTS is trained, PDM is inserted in front of the prosody predictor and trained with the method which will be described in detail in Section 3.2. We describe the main modules of DPPTTS and their roles in the following subsection.\nA.1 Main modules of DPP-TTS\nSeq2Seq module The role of the Seq2seq module is generating mel-spectrograms from phoneme sequences. The module is adapted from FastSpeech2 (Ren et al., 2021a) with some modifications. The model consists of four main parts: a phoneme prenet, a phoneme encoder, a variance adaptor, and a mel-spectrogram decoder. In the phoneme encoder, phoneme sequences are processed through a stack of feed-forward transformer blocks with a relative positional representation (Shaw et al., 2018). In variance adaptor at training, pitch embeddings and energy embeddings 9 are added to encoded hidden representations and then hidden representations are expanded according to ground-truth duration labels 10. At inference, these prosodic features are provided from predictions of the prosody predictor. Finally, expanded representations are processed through a stack of feedforward transformer blocks and mel-spectrograms with 80 channels are generated after the linear projection. The Seq2Seq module is trained to minimize L1 distance between the predicted and target mel-spectrogram.\nProsody predictor In FastSpeech2, the variance adaptor consists of deterministic predictors for predicting prosodic features. However, a deterministic prosodic predictor is not expressive enough to learn the speaking style of a person. For diverse rhythm and pitch, a stochastic duration predictor and pitch predictor are built upon normalizing flows. Specifically, the stochastic duration\n9For the brevity, the deterministic energy predictor is omitted in the figure.\n10Ground-truth labels are obtained via monotonic alignment search (Kim et al., 2020) between the phonemes and melspectrogram.\npredictor estimates the distribution of phoneme duration and the stochastic pitch predictor estimates the distribution of phoneme-level pitch from the hidden sequence. At the training stage, the prosody predictor learns the mapping from the distribution of prosodic features to normal distribution. At inference, it predicts the phoneme-level duration or pitch by reversing the learned flows. In addition, it also serves as the density estimator for prosodic features during the training of PDM which will be described in detail in Section 4. The prosody predictor is trained to maximize a variational lower bound of the likelihood of the phoneme duration or pitch. More details regarding the prosody predictor are in Appendix A.\nPDM Although the stochastic duration and pitch predictor are trained to generate a speech with diverse rhythm and pitch, the prosody predictor may favors major modes and it can lead to the monotonous prosodic pattern in the speech. For more expressive speech modeling, PDM is added in front of the prosody predictor as shown in Figure 4b. Its role is to map latent codes from a standard normal distribution to another distribution for diverse prosodic features of speech. This module is trained with an objective based on conditional DPPs which is described in Section 2.2. At inference of DPP-TTS, multiple prosodic candidates are generated by PDM. Subsequently, the prosodic feature of speech is selected via MAP inference, or multiple prosodic features are sampled when multiple speech samples are generated.\nProsodic Boundary Detector There are two ways of diversifying prosodic features: autoregressive and non-autoregressive methods. Since autoregressive methods require much time to generate the output sequence, we employ the non-autoregressive method in our model. For the non-autoregressive approach, some prosodic features of speech need to be set when trying to diversify overall prosodic features. In this perspective, a sentence needs to be divided into a context sequence and target sequence, utilizing the prosodic boundary based on the prominence of words by reflecting the assumption that people unconsciously pronounce the sentence focusing on what intentions they want to convey. To decide the boundary, we make Prosodic Boundary Detector (PBD) whose backbone is a pretrained Sentence-Transformer. A Prominence dataset (Talman et al., 2019) consisting of Librispeech scripts\nand prominence classes is used for training PBD. The input of PBD is a text sequence, and PBD predicts each word\u2019s prominence level."
        },
        {
            "heading": "B Details of prosody predictor and PDM",
            "text": "B.1 Prosody predictor\nTraining It is hard to use the maximum likelihood objective directly to train duration predictor because the duration of each phoneme is 1) a discrete integer and a scalar, which hinders expressive transformation because invertibility should remain in normalizing flow. To train duration predictor, duration values are extended to continuous values using variational dequantization (Ho et al., 2019) and are augmented with extra dimensions using variational data augmentation (Chen et al., 2020). Specifically, duration sequence d becomes continuous as d \u2212 u where u\u2019s value is restricted to [0, 1) and augmented as [d \u2212 u, v] with a extra random variable v. Two random variables u and v are sampled through approximate posterior q\u03d5(u, v|d, htext). The ELBO can be calculated as follows:\nlog p\u03b8(d|htext) \u2265 Eq\u03d5(u,v|d,htext)[log p\u03b8(d\u2212 u, v|htext) q\u03d5(u, v|d, htext) ]\nLike the duration predictor, the ELBO for pitch predictor can be calculated as follows:\nlog p\u03b8(p|htext) \u2265 Eq\u03d5(v|p,htext)[log p\u03b8(p, v|htext) q\u03d5(v|p, htext) ],\nwhere p denotes the pitch sequences. Both the duration and pitch predictor are trained on negative lower bound of likelihood.\nArchitecture The pitch predictor and duration predictor share identical architecture except for the additional random variable u. We will introduce the architecture of the duration predictor whose diagram is shown in Figure 5a. Duration predictor consists of condition encoder for hidden sequence, posterior encoder for the duration sequence, and the main flow blocks g(\u00b7). Specifically, posterior encoder maps latent codes from normal distribution to random variable [u, v] and flow g(\u00b7) maps [d\u2212 u, v] to normal distribution. Figure 5b shows the coupling of normalizing flows. First, input x is split into [x0, x1] and then x0 is processed by a 1x1 convolution block. The output of the convolution block is processed by dilated convolution and Nystromer block and their outputs are concatenated. The concatenated output is processed by LayerNorm, Mish activation, and Convolution block. Finally, spline flows (Durkan et al., 2019) are parameterized by the output and then the output of spline flows y and x0 are concatenated.\nB.2 PDM\nThe architecture of PDM is shown in Figure 5c. First, the hidden sequence is processed by convolution block followed by the dilated convolution block conditioned on hidden sequences, Layernorm, and 1x1 convolution block. After that, noise from a normal distribution is processed by 1x1 convolution followed by dilated convolution block, LayerNorm, and Mish activation.\nAs the prosody pattern of human speech is correlated with the local context and global context of the text, we design the architecture of the model to encode both the local and global features. Specifically, dilated convolution blocks are stacked to encode local features and a transformer block is used to encode global features in the coupling layers of normalizing flows. However, using the vanilla transformer to encode the global features in the coupling layer requires too large computational complexity. Therefore, Nystr\u00f6mformer (Xiong et al., 2021) which is a Nystr\u00f6m-Based algorithm for approximating self-attention is used to encode global context for more efficient memory usage in the coupling layer of normalizing flow. Encoded local features and global features are concatenated and processed through LayerNorm (Ba et al., 2016), Mish activation (Misra, 2019) and a 1x1 convolution module.\nFollowing the duration predictor model in Kim et al. (2021), variational dequantization (Ho et al.,\n2019) is used for the duration predictor since the phoneme duration is a discrete integer. In addition, variational augmentation (Chen et al., 2020) is used to expand channel dimensions for expressive flows in both the duration and pitch predictor. The stochastic and pitch predictor are trained by maximizing their variational lower bounds (ELBO). Details of the duration and pitch predictor are described in Appendix B."
        },
        {
            "heading": "C Proof of Proposition 1",
            "text": "Objective From equation [45] in (Kulesza and Taskar, 2012), the marginal kernel of conditional DPPs given the appearance of set A has a following form:\nKA = I \u2212 [(L+ IA\u0304)\u22121]A\u0304 (8)\nIn addition, from equation [34] in Kulesza and Taskar (2012) the expected cardinality of Y given the marginal kernel K is:\nE[|Y |] = N\u2211\nn=1\n\u03bbn \u03bbn + 1 = tr(K) (9)\nFrom equation 8, 9, the expected cardinality of conditional DPP given the appearance of set A is:\nE[|Y |] = tr(KA) = tr(I\u2212[(L+IA\u0304)\u22121]A\u0304) (10)\nDerivative For the proof, we will start with the following lemma:\nLemma 1 Given a matrix E and non-singular matrix A, following equation holds:\n\u2202\n\u2202A tr(ETA\u22121E) = \u2212(A\u22121EETA\u22121)T (11)\nProof. First, consider \u2202\u2202Aij tr(E TA\u22121E). Since\ntrace and derivative operator are interchangeable,\n\u2202\n\u2202Aij tr(ETA\u22121E) = tr(\n\u2202\n\u2202Aij (ETA\u22121E))\n= \u2212tr(ETA\u22121 \u2202A \u2202Aij A\u22121E)\n(12) By setting \u2202A\u2202Aij = E\nij where Eij denotes the matrix whose (i, j) component is 1 and 0 elsewhere and C = \u2212ETA\u22121EijA\u22121E,\n\u2212 tr(ETA\u22121EijA\u22121E) = \u2211 i\u2032 Ci\u2032 i\u2032\n= \u2212 \u2211 i\u2032 \u2211 k1 \u2211 k2 (ETA\u22121)ii\u2032k1E ij k1k2 (A\u22121E)k2i\u2032\n= \u2212 \u2211 i\u2032 (ETA\u22121)i\u2032 i(A \u22121E)ji\u2032 =\n= \u2212 \u2211 i\u2032 (A\u2212TE)ii\u2032 (E TA\u2212T)i\u2032j = \u2212(A\u2212TEETA\u2212T)ij = \u2212(A\u22121EETA\u22121)Tij\n(13) =\u21d2 \u2202\u2202Aij tr(E TA\u22121E) = \u2212(A\u22121EETA\u22121)Tij .\nNow, with respect to set a A whose cardinality is p and matrix L \u2208 R(p+q)\u00d7(p+q)\n\u2202\n\u2202\u03b8 tr(I \u2212 [(L+ IA\u0304)\u22121]A\u0304) = \u2212 \u2202 \u2202\u03b8 tr([(L+ IA\u0304) \u22121]A\u0304) = \u2212 \u2202 \u2202\u03b8 tr(ET(L+ IA\u0304) \u22121E),\n(14)\nwhere E denotes [ 0 Iq ] \u2208 R(p+q)\u00d7q. Then by Lemma 1.\n\u2212 \u2202 \u2202\u03b8 tr(ET(L+ IA\u0304) \u22121E) = ((L+ IA\u0304) \u22121EET(L+ IA\u0304) \u22121)TL \u2032 (\u03b8) = ((L+ IA\u0304) \u22121Iq(L+ IA\u0304) \u22121)TL \u2032 (\u03b8) (15)\nThe proof of Proposition 1 is now finished.\nD Inference of DPP-TTS\nAlgorithm 2 Inference of DPP-TTS Require: TextEncoder f(\u00b7), Decoder h(\u00b7), PDM,\na prosody predictor g(\u00b7), noise scale \u03f5 1: htext \u2190 f(text) 2: Split htext into [htarget, hcontext] 3: Sample latent code zcontext \u2208 RrmT with\nnoise scale \u03f5 4: Get prosodic features of contexts: dcontext \u2190\ng\u22121(hcontext, zcontext) 5: Get quality of contexts:\nqcontext = Density estimation(dcontext) 6: Sample latent codes ztarget with noise scale \u03f5 7: Get latent codes after PDM: ztarget \u2190\nPDM(ztarget) \u2208 Rnc\u00d7T 8: Get nc prosodic features of targets:\n(d1target, d 2 target, ...d nc target)\n9: Get quality of targets: qtarget = Density estimation(dtarget)\n10: Concatenate contexts and targets: [qcontext, qtarget], [dcontext, dtarget] 11: Build the kernel of conditional DPPs: L\u2190 Build kernel([qc, qt], [dc, dt]) 12: Perform the MAP inference: d\u2217 \u2190 argmaxd logdet(Ld\u222adcontext) 13: Synthesize wavs with prosodic features: y \u2190 h(htext, d \u2217)"
        },
        {
            "heading": "E Sample paragraph for the side-by-side comparison test",
            "text": "Known individually and collectively as ShaiHulud, the sandworms are these supermassive beings that plow through the deserts of Arrakis, consuming everything that dares venture unprepared into their territory. The worms are what make harvesting spice so difficult because they tend to eat whatever tools off-worlders use to mine it. They are also sacred to the Fremen, who seem to know ways to navigate around them, and, somehow, they\u2019re linked to the creation of spice. Think of them as big honking metaphors for the sublime powers of nature that loom beyond human understanding, like a desert full of Moby Dicks"
        },
        {
            "heading": "F Adjusting the extent of variation",
            "text": "Table 5 demonstrates that adjusting quality weight can trade-off between diversity and naturalness without having a severe impact on naturalness. The pitch and log-duration plots are shown in Figure 6."
        }
    ],
    "title": "DPP-TTS: Diversifying prosodic features of speech via determinantal point processes",
    "year": 2023
}