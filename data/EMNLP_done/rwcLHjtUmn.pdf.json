{
    "abstractText": "Webpages have been a rich, scalable resource for vision-language and language only tasks. Yet only pieces of webpages are kept in existing datasets: image-caption pairs, long text articles, or raw HTML, never all in one place. Webpage tasks have resultingly received little attention and structured image-text data left underused. To study multimodal webpage understanding, we introduce the Wikipedia Webpage suite (WikiWeb2M) containing 2M pages with all of the associated image, text, and structure data1. We verify its utility on three generative tasks: page description generation, section summarization, and contextual image captioning. We design a novel attention mechanism Prefix Global, which selects the most relevant image and text content as global tokens to attend to the rest of the webpage for context. By using page structure to separate such tokens, it performs better than full attention with lower computational complexity. Extensive experiments show that the new data in WikiWeb2M improves task performance compared to prior work.",
    "authors": [
        {
            "affiliations": [],
            "name": "Andrea Burns"
        },
        {
            "affiliations": [],
            "name": "Krishna Srinivasan"
        },
        {
            "affiliations": [],
            "name": "Joshua Ainslie"
        },
        {
            "affiliations": [],
            "name": "Geoff Brown"
        },
        {
            "affiliations": [],
            "name": "Bryan A. Plummer"
        },
        {
            "affiliations": [],
            "name": "Kate Saenko"
        },
        {
            "affiliations": [],
            "name": "Jianmo Ni"
        },
        {
            "affiliations": [],
            "name": "Mandy Guo"
        }
    ],
    "id": "SP:9caf7c1d0fe86cd87cf54433bc7dcc1a27d7a306",
    "references": [
        {
            "authors": [
                "Armen Aghajanyan",
                "Bernie Huang",
                "Candace Ross",
                "Vladimir Karpukhin",
                "Hu Xu",
                "Naman Goyal",
                "Dmytro Okhonko",
                "Mandar Joshi",
                "Gargi Ghosh",
                "Mike Lewis",
                "Luke Zettlemoyer."
            ],
            "title": "Cm3: A causal masked multimodal model of the internet",
            "venue": "arXiv:2201.07520.",
            "year": 2022
        },
        {
            "authors": [
                "Joshua Ainslie",
                "Santiago Ontanon",
                "Chris Alberti",
                "Vaclav Cvicek",
                "Zachary Fisher",
                "Philip Pham",
                "Anirudh Ravula",
                "Sumit Sanghai",
                "Qifan Wang",
                "Li Yang."
            ],
            "title": "ETC: Encoding long and structured inputs in transformers",
            "venue": "Empirical Methods in Natural",
            "year": 2020
        },
        {
            "authors": [
                "Iz Beltagy",
                "Matthew E. Peters",
                "Arman Cohan."
            ],
            "title": "Longformer: The long-document transformer",
            "venue": "arXiv:2004.05150.",
            "year": 2020
        },
        {
            "authors": [
                "Ali Furkan Biten",
                "Llu\u00eds G\u00f3mez",
                "Mar\u00e7al Rusi\u00f1ol",
                "Dimosthenis Karatzas."
            ],
            "title": "Good news, everyone! context driven entity-aware captioning for news images",
            "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR).",
            "year": 2019
        },
        {
            "authors": [
                "Andrea Burns",
                "Deniz Arsan",
                "Sanjna Agrawal",
                "Ranjitha Kumar",
                "Kate Saenko",
                "Bryan A. Plummer."
            ],
            "title": "A dataset for interactive vision language navigation with unknown command feasibility",
            "venue": "European Conference on Computer Vision (ECCV).",
            "year": 2022
        },
        {
            "authors": [
                "Biplab Deka",
                "Zifeng Huang",
                "Chad Franzen",
                "Joshua Hibschman",
                "Daniel Afergan",
                "Yang Li",
                "Jeffrey Nichols",
                "Ranjitha Kumar."
            ],
            "title": "Rico: A mobile app dataset for building data-driven design applications",
            "venue": "User Interface Software and Technology (UIST).",
            "year": 2017
        },
        {
            "authors": [
                "Jia Deng",
                "Wei Dong",
                "Richard Socher",
                "Li-Jia Li",
                "Kai Li",
                "Li Fei-Fei."
            ],
            "title": "ImageNet: A Large-Scale Hierarchical Image Database",
            "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR).",
            "year": 2009
        },
        {
            "authors": [
                "Mandy Guo",
                "Joshua Ainslie",
                "David Uthus",
                "Santiago Ontanon",
                "Jianmo Ni",
                "Yun-Hsuan Sung",
                "Yinfei Yang."
            ],
            "title": "LongT5: Efficient text-to-text transformer for long sequences",
            "venue": "Findings of the Association for Computational Linguistics: NAACL.",
            "year": 2022
        },
        {
            "authors": [
                "Izzeddin Gur",
                "Ofir Nachum",
                "Yingjie Miao",
                "Mustafa Safdari",
                "Austin Huang",
                "Aakanksha Chowdhery",
                "Sharan Narang",
                "Noah Fiedel",
                "Aleksandra Faust."
            ],
            "title": "Understanding html with large language models",
            "venue": "arXiv:2210.03945.",
            "year": 2022
        },
        {
            "authors": [
                "Izzeddin Gur",
                "Ulrich Rueckert",
                "Aleksandra Faust",
                "Dilek Hakkani-Tur."
            ],
            "title": "Learning to navigate the web",
            "venue": "International Conference on Learning Representations (ICLR).",
            "year": 2019
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun."
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR).",
            "year": 2016
        },
        {
            "authors": [
                "Jack Hessel",
                "Ari Holtzman",
                "Maxwell Forbes",
                "Ronan Le Bras",
                "Yejin Choi."
            ],
            "title": "CLIPScore: a referencefree evaluation metric for image captioning",
            "venue": "Empirical Methods in Natural Language Processing (EMNLP).",
            "year": 2021
        },
        {
            "authors": [
                "Geoffrey E. Hinton",
                "Oriol Vinyals",
                "Jeffrey Dean."
            ],
            "title": "Distilling the knowledge in a neural network",
            "venue": "arXiv:1503.02531.",
            "year": 2015
        },
        {
            "authors": [
                "Chao Jia",
                "Yinfei Yang",
                "Ye Xia",
                "Yi-Ting Chen",
                "Zarana Parekh",
                "Hieu Pham",
                "Quoc V. Le",
                "Yunhsuan Sung",
                "Zhen Li",
                "Tom Duerig."
            ],
            "title": "Scaling up visual and vision-language representation learning with noisy text supervision",
            "venue": "International Conference",
            "year": 2021
        },
        {
            "authors": [
                "Sheng Jia",
                "Jamie Kiros",
                "Jimmy Ba."
            ],
            "title": "Domq-net: Grounded rl on structured language",
            "venue": "International Conference on Learning Representations (ICLR).",
            "year": 2019
        },
        {
            "authors": [
                "Chris Kedzie",
                "Kathleen McKeown",
                "Hal Daum\u00e9 III."
            ],
            "title": "Content selection in deep learning models of summarization",
            "venue": "Empirical Methods in Natural Language Processing (EMNLP).",
            "year": 2018
        },
        {
            "authors": [
                "Yang Li",
                "Jiacong He",
                "Xin Zhou",
                "Yuan Zhang",
                "Jason Baldridge."
            ],
            "title": "Mapping natural language instructions to mobile UI action sequences",
            "venue": "Association for Computational Linguistics (ACL).",
            "year": 2020
        },
        {
            "authors": [
                "Chin-Yew Lin."
            ],
            "title": "ROUGE: A package for automatic evaluation of summaries",
            "venue": "Association for Computational Linguistics (ACL).",
            "year": 2004
        },
        {
            "authors": [
                "Evan Zheran Liu",
                "Kelvin Guu",
                "Panupong Pasupat",
                "Tianlin Shi",
                "Percy Liang."
            ],
            "title": "Reinforcement learning on web interfaces using workflow-guided exploration",
            "venue": "International Conference on Learning Representations (ICLR).",
            "year": 2018
        },
        {
            "authors": [
                "Fuxiao Liu",
                "Yinghan Wang",
                "Tianlu Wang",
                "Vicente Ordonez."
            ],
            "title": "Visualnews: Benchmark and challenges in entity-aware image captioning",
            "venue": "Empirical Methods in Natural Language Processing (EMNLP).",
            "year": 2021
        },
        {
            "authors": [
                "Medhini Narasimhan",
                "Arsha Nagrani",
                "Chen Sun",
                "Michael Rubinstein",
                "Trevor Darrell",
                "Anna Rohrbach",
                "Cordelia Schmid."
            ],
            "title": "Tl; dw? summarizing instructional videos with task relevance and crossmodal saliency",
            "venue": "European Conference on Com-",
            "year": 2022
        },
        {
            "authors": [
                "Khanh Nguyen",
                "Ali Furkan Biten",
                "Andres Mafla",
                "Lluis Gomez",
                "Dimosthenis Karatzas."
            ],
            "title": "Show, interpret and tell: Entity-aware contextualised image captioning in wikipedia",
            "venue": "arXiv:2209.10474.",
            "year": 2022
        },
        {
            "authors": [
                "Daniel Nkemelu",
                "Peggy Chi",
                "Daniel Castro Chin",
                "Krishna Srinivasan",
                "Irfan Essa."
            ],
            "title": "Automatic multi-path web story creation from a structural article",
            "venue": "arXiv:2310.02383.",
            "year": 2023
        },
        {
            "authors": [
                "Kishore Papineni",
                "Salim Roukos",
                "Todd Ward",
                "WeiJing Zhu."
            ],
            "title": "BLEU: A method for automatic evaluation of machine translation",
            "venue": "Association for Computational Linguistics (ACL).",
            "year": 2002
        },
        {
            "authors": [
                "Luc Pauwels."
            ],
            "title": "A Multimodal Framework for Analyzing Websites as Cultural Expressions",
            "venue": "Journal of Computer-Mediated Communication (JCMC).",
            "year": 2012
        },
        {
            "authors": [
                "Amy Pu",
                "Hyung Won Chung",
                "Ankur P Parikh",
                "Sebastian Gehrmann",
                "Thibault Sellam."
            ],
            "title": "Learning Compact Metrics for MT",
            "venue": "Empirical Methods of Natural Language Processing (EMNLP).",
            "year": 2021
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark",
                "Gretchen Krueger",
                "Ilya Sutskever"
            ],
            "title": "Learning transferable visual models from natural language",
            "year": 2021
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J. Liu."
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "Journal of Machine Learning Research",
            "year": 2020
        },
        {
            "authors": [
                "Piyush Sharma",
                "Nan Ding",
                "Sebastian Goodman",
                "Radu Soricut."
            ],
            "title": "Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning",
            "venue": "Association for Computational Linguistics (ACL).",
            "year": 2018
        },
        {
            "authors": [
                "Tianlin Shi",
                "Andrej Karpathy",
                "Linxi Fan",
                "Jonathan Hernandez",
                "Percy Liang."
            ],
            "title": "World of bits: An open-domain platform for web-based agents",
            "venue": "34th International Conference on Machine Learning (ICML).",
            "year": 2015
        },
        {
            "authors": [
                "Krishna Srinivasan",
                "Karthik Raman",
                "Jiecao Chen",
                "Michael Bendersky",
                "Marc Najork."
            ],
            "title": "Wit: Wikipedia-based image text dataset for multimodal multilingual machine learning",
            "venue": "International ACM Conference on Special Interest Group on Information",
            "year": 2021
        },
        {
            "authors": [
                "Reuben Tan",
                "Bryan A. Plummer",
                "Kate Saenko",
                "J.P. Lewis",
                "Avneesh Sud",
                "Thomas Leung."
            ],
            "title": "Newsstories: Illustrating articles with visual summaries",
            "venue": "European Conference on Computer Vision (ECCV).",
            "year": 2022
        },
        {
            "authors": [
                "Ramakrishna Vedantam",
                "C. Lawrence Zitnick",
                "Devi Parikh."
            ],
            "title": "Cider: Consensus-based image description evaluation",
            "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR).",
            "year": 2015
        },
        {
            "authors": [
                "Alexandra Vtyurina",
                "Adam Fourney",
                "Meredith Ringel Morris",
                "Leah Findlater",
                "Ryen W. White."
            ],
            "title": "Bridging screen readers and voice assistants for enhanced eyes-free web search",
            "venue": "International ACM Conference on Computers and Accessibility (AS-",
            "year": 2019
        },
        {
            "authors": [
                "Linzi Xing",
                "Wen Xiao",
                "Giuseppe Carenini."
            ],
            "title": "Demoting the lead bias in news summarization via alternating adversarial learning",
            "venue": "Association for Computational Linguistics (ACL).",
            "year": 2021
        },
        {
            "authors": [
                "Jheng-Hong Yang",
                "Carlos Lassance",
                "Rafael Sampaio de Rezende",
                "Krishna Srinivasan",
                "Miriam Redi",
                "St\u00e9phane Clinchant",
                "Jimmy Lin."
            ],
            "title": "Atomic: An image/text retrieval test collection to support multimedia content creation",
            "venue": "International ACM Con-",
            "year": 2023
        },
        {
            "authors": [
                "Manzil Zaheer",
                "Guru Guruganesh",
                "Kumar Avinava Dubey",
                "Joshua Ainslie",
                "Chris Alberti",
                "Santiago Ontanon",
                "Philip Pham",
                "Anirudh Ravula",
                "Qifan Wang",
                "Li Yang"
            ],
            "title": "Big bird: Transformers for longer sequences",
            "year": 2020
        },
        {
            "authors": [
                "Chenguang Zhu",
                "Ziyi Yang",
                "Robert Gmyr",
                "Michael Zeng",
                "Xuedong Huang."
            ],
            "title": "Leveraging lead bias for zero-shot abstractive news summarization",
            "venue": "International ACM Conference on Special Interest Group on Information Retrieval (SIGIR).",
            "year": 2021
        },
        {
            "authors": [
                "Wanrong Zhu",
                "Jack Hessel",
                "Anas Awadalla",
                "Samir Yitzhak Gadre",
                "Jesse Dodge",
                "Alex Fang",
                "Youngjae Yu",
                "Ludwig Schmidt",
                "William Yang Wang",
                "Yejin Choi"
            ],
            "title": "Multimodal c4: An open, billion-scale corpus of images interleaved with text",
            "year": 2023
        },
        {
            "authors": [
                "Nguyen"
            ],
            "title": "2022) and use the reference description as the ground truth caption to be generated",
            "venue": "Lastly for image captioning,",
            "year": 2022
        },
        {
            "authors": [
                "Kita"
            ],
            "title": "The choir sang the German premiere of Joseph Jongen\u2019s Mass for choir, brass ensemble and organ",
            "venue": "Op. 130,",
            "year": 1988
        },
        {
            "authors": [
                "Golovnin Bay"
            ],
            "title": "Page description qualitative examples. We include three random samples from the page description test set and compare the target page description and predicted model output text. Group article and 1900s-1950s Growth and Expansion section. The target section summary",
            "year": 1950
        },
        {
            "authors": [
                "Alice Bemis"
            ],
            "title": "Taylor was a philanthropist and was inducted into the Colorado Women\u2019s Hall of Fame in 2010. For her significant contributions to Colorado College, Colorado Springs Fine Arts Center and the Colorado Springs Day Nursery",
            "year": 2010
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Webpages are a source of multimodal, structured content which have been used for both pretraining and finetuning purposes. Large scale noisy text or multimodal datasets scraped from the web have been used to pretrain large language or contrastive models (Raffel et al., 2020; Jia et al., 2021; Radford et al., 2021; Aghajanyan et al., 2022). Downstream tasks built from webpages have included instruction following, image captioning, news captioning, image-sentence retrieval, and image-article retrieval (Shi et al., 2015; Li et al., 2020; Gur et al., 2022; Sharma et al., 2018; Biten et al., 2019; Liu et al., 2021; Srinivasan et al., 2021; Tan et al., 2022).\n\u2217Work done during an internship at Google. 1Data can be downloaded at https://github.com/ google-research-datasets/wit/blob/main/wikiweb2m. md.\nYet limited prior work has studied tasks to evaluate multimodal webpage understanding itself.\nMany classification and generation problems can be studied with webpages: taxonomic webpage classification, webpage retrieval, web image captioning, and page summarization. However, to date there is no open source, multimodal dataset that retains all webpage content. E.g., the Wikipedia Image Text (WIT) dataset (Srinivasan et al., 2021) does not retain HTML structure and misses out on text sections. Thus, we propose the new Wikipedia Webpage (WikiWeb2M) dataset of over 2M pages, which unifies webpage content to include all text, images, and their location (e.g., section index) in a single sample. Table 1 compares the statistics of WikiWeb2M to the existing English WIT dataset.\nFigure 1 shows an example of our WikiWeb2M benchmark suite; we design a set of tasks that require webpage understanding at varying degrees of\ngranularity. Specifically, we use page description generation, section summarization, and contextual image captioning to evaluate a model\u2019s ability to understand a webpage at a global, regional, and local level, respectively. For page description generation, the goal is to generate an overarching global description of the webpage. The task of section summarization generates a sentence that captures the key content of one section. Finally, contextual image captioning generates a caption for one image within the webpage.\nWikiWeb2M\u2019s tasks will allow for general study of multimodal content understanding with manyto-many text and image relationships and can also specifically improve interaction with web content. For example, a webpage description may provide a user who is blind more agency by allowing them to preview content before listening to the entire body of image and text with a screen reader (Vtyurina et al., 2019). In addition to contextual captioning and section summarization aiding assistive technology, these tasks can be used for modern content generation, as there is growing interest in providing multimodal web snippets (Nkemelu et al., 2023). The study of webpages in a multimodal context has even been motivated from a sociological and anthropological perspective (Pauwels, 2012).\nWhile we curate a new dataset with Wikipedia, we note it is just one of many domains that could be used to study multimodal webpage understanding. Instructional websites, news articles, recipes, blogs, and more have bodies of text and images interleaved by layout or HTML structure.\nWe utilize the T5 (Raffel et al., 2020) framework to address the WikiWeb2M tasks. One challenge in modeling webpage tasks is the length of the input data (i.e., a long sequence results from flattening webpage text and images). While the full attention originally used in T5 is performant, it results in a quadratic computational complexity with respect to the input sequence length. Thus, we define a new mixture of local-global attention, Prefix Global,\nwhich uses our structured webpage data to select the most salient text and images as global tokens in the prefix of our input sequence. Prefix Global is ultimately more efficient, meaning longer input sequences can be used to reach better task performance. Our results can be beneficial to the many structured image-text domains outside of webpages such as mobile apps, figures, posters, infographics, and documents.\nWe include ablations across multiple axes: the pretrained checkpoint we initialize from, the input sequence length, the feature inputs, and the attention mechanism. We importantly find that images improve performance for all tasks, while prior work on contextual image captioning claimed otherwise (Nguyen et al., 2022). We are also able to improve task performance now that we have access to the entire page\u2019s content. Still, there is plenty of room to improve upon our benchmark suite.\nWe summarize our contributions below:\n\u2022 A new open source multimodal webpage dataset, WikiWeb2M, containing 2M pages curated from English Wikipedia articles. Each sample contains all text, images, and structure present per page.\n\u2022 A suite of multimodal generation webpage tasks that reflect webpage understanding at three granularities: page description, section summarization, contextual image captioning.\n\u2022 A new attention mechanism, Prefix Global, which is a mixture of local-global attention that separates a prefix of global tokens. By defining more salient content from structured pages, it can outperform full attention while requiring fewer attention computations.\n\u2022 Ablations on attention, sequence length, input features, and model size. Images can help all tasks, notably by over 15% on contextual captioning, and page context boosts average performance by over 4% and 3% for section summarization and captioning, respectively."
        },
        {
            "heading": "2 The WikiWeb2M Dataset",
            "text": "We create the Wikipedia Webpage (WikiWeb2M) dataset to have an all-in-one multimodal webpage dataset where all text, image, and structure content is retained. WikiWeb2M is built by starting with the Wikipedia Image Text (WIT; Srinivasan et al., 2021) English pages2. We re-scrape webpage samples and keep all text, image, and structure available, providing more contextual data which can be used to model existing tasks like contextual image captioning, as well as enable new webpage understanding tasks like page description generation. We start with WIT URLs to create a high quality multimodal webpage dataset that has already gone through extensive content and topic filtering.\nEach webpage sample includes the page URL, page title, section titles, section text, images and their captions, and indices for each section, their parent section, their children sections, and more. This differs from WIT, which defined individual samples as image-caption pairs with metadata (e.g., originating section title). Appendix A.3 includes a comparison of fields available in WikiWeb2M versus WIT. In Table 1, we report the number of sections and images compared to the English subset of WIT. We add nearly 1M total images to the dataset by keeping the images on a webpage regardless of whether they have image captions available.\nWe provide section counts by type: structural, heading, text only, image only, and both text and image. Structural and heading sections do not contain immediate text. The former has subsections. For heading sections, a section title was available, while the content linked to a different article, was empty, or only had tables. We only retain sections if they are content sections (e.g., not the \u201cSee Also\u201d section). A significant 6.8M text sections are in WikiWeb2M, none of which were available in WIT. For image quality control, we keep JPEG and PNG image types3. We make a random 90/5/5 split and\n2WIT held a CC BY-SA 3.0 license, and additional data we recover in WikiWeb2M is publicly available on Wikipedia.\n3We release image URLs, where they can be fetched.\nshow the number of pages, sections, and images per split in Table 2. Note that Table 2 reflects statistics of WikiWeb2M, which is later refined to build our downstream tasks datasets. It can be repurposed for other webpage understanding tasks or reprocessed with different data filters."
        },
        {
            "heading": "2.1 The WikiWeb2M Tasks",
            "text": "We apply WikiWeb2M to three tasks which reflect different granularities of webpage understanding: the page, section, or element level. Table 3 contains task sample counts which are achieved by further task-specific filtering; these data processing steps are included in Appendix A.1, with a discussion of potential dataset noise in Appendix A.2. We describe the tasks below.\nPage Description Generation. In the task of page description generation, the goal is to generate a description of a page given the rest of the webpage\u2019s image, text, and structure. We use the Wikipediaprovided page description (not collecting annotations) and generate summaries from multimodal inputs, which differs from existing text-only article summarization work; this matters when we want to create a multimodal snippet from a webpage.\nSection Summarization. The goal of section summarization is to generate a single sentence that highlights a particular section\u2019s content. The summary is generated given all images and (non-summary) text present in the target and context sections; see Figure 3 for a task example. Following the leading sentence bias, we use the first sentence of a section as a pseudo summary (which is removed from the model inputs). We also found that a majority of human annotators deemed the first sentence as a reasonable summary; these findings are later discussed in Appendix F.\nContextual Image Captioning. Nguyen et al. (2022) proposed contextual image captioning with WIT as the task of captioning an image given the image and its webpage context. Target images are those available in WIT to ensure they have quality\ncaptions that can be reconstructed. A Wikipedia image can have three caption types (not all are always available): the alt-text, reference, and attribution descriptions. Alt-text serves as a text description for accessibility purposes, the reference description comes directly below the image in the rendered webpage, and the attribution description contains captions unique to the image across all webpages it appears in. Prior work only input the image, attribution description and associated section text because that was all that was available."
        },
        {
            "heading": "3 Prefix Global Attention",
            "text": "When structured image-text data is available, we need not treat all images and text equally. With webpages, it may be more sensible to isolate certain parts as more important. E.g., in contextual image captioning, the model should focus on the target image and section it came from, while using the rest of the page as additional context. We can now isolate these inputs with the WikiWeb2M dataset because we have structural metadata signaling where each image and text element are located, as opposed to a bag of images and a single long body of text. I.e., the new structure available in our dataset can both serve as new inputs to the model and enable new attention mechanisms.\nWe thus propose Prefix Global, a local-global attention, to capitalize on this intuition. A mixture of local and global attention weights provides the means to designate certain inputs as \u201cglobal\u201d tokens which can specially attend to the rest of the\ninput sequence, while others only have local attention to a radius of r tokens to the left and right. Not only is it desirable to prioritize more salient image and text content from the input data, but it can also reduce the computational complexity of the attention mechanism. While full attention is performant by allowing all input tokens to attend to each other, it results in a quadratic computational complexity (O(l2) for a sequence of length l).\nFigure 2 illustrates our Prefix Global and prior work\u2019s Transient Global attention schemes, where in each the ith row represents what the ith token can attend to. Guo et al. (2022) introduced LongT5 as an adaptation of the T5 model with Transient Global (TGlobal) attention to balance the efficiency of local attention, which allows for much longer input sequences to be held in memory, with the higher performance of full attention. TGlobal resulted in similar or better performance than full attention with much longer sequence lengths, while having a complexity of O(l(r + k)) for a variety of text summarization tasks (where k = l16 ).\nIn addition to the local attention of TGlobal (see the blue attention diagonal in Figure 2 (left)), \u201ctransient\u201d global tokens are defined on the fly per layer. TGlobal defines k globals as the average of every 16 input tokens, which are additionally attended to by all other inputs. As a result, TGlobal has more global tokens as the sequence length increases. In contrast, shown in Figure 2 (right), Prefix Global uses a constant number of global tokens. Specifically, it takes a prefix of the input sequence. This is\ninspired by the leading sentence bias (Kedzie et al., 2018; Xing et al., 2021; Zhu et al., 2021), which shows that earlier content in a body of text is often of greater importance. We define different prefixes for each task in Section 4. While we use section structure to define our prefixes, Prefix Global can use structure from other sources: HTML/the Document Object Model, rendered webpage regions, PDF document layouts, or simply knowing a priori what task inputs are most salient.\nPrefix Global has a computational complexity of O((l\u2212 k) \u00b7 r+ k \u00b7 l) for k global tokens, similar to local-global attention schemes ETC (Ainslie et al., 2020), Longformer (Beltagy et al., 2020), and BigBird (Zaheer et al., 2020). However, Prefix Global does not require any special pretraining and instead finetunes directly from full attention checkpoints (T5 in our case). This is distinct from LongT5, which also required pretraining with TGlobal attention to be effective. Thus, as we show in Section 5 with Prefix Global\u2019s higher performance, it is both a more flexible and performant attention. We also are the first to demonstrate using a local-global attention with multimodal inputs, and further show Prefix Global\u2019s ability to be performant in multimodal finetuning from a text-only checkpoint."
        },
        {
            "heading": "4 Experiments",
            "text": "We now detail the model variants used for experiments, parameter settings for reproducing our set up, the metrics used for evaluation, and key ablations we perform.\nModel Architectures. We benchmark with the T5 (Raffel et al., 2020) encoder-decoder framework. T5 takes a sequence of image and text inputs\nand we embed images in our input sequence using a frozen ViT model (Dosovitskiy et al., 2021). We note that finetuning ViT may further improve performance. We compare three models defined by different encoder attention schemes: the original T5 which uses full attention, LongT5 with TGlobal attention by Guo et al. (2022) (checkpoints are publicly available), and our Prefix Global attention. We finetune all models from a T5 checkpoint pretrained with full attention on the text-only C4 dataset, and a ViT pretrained on either ImageNet (Deng et al., 2009) or JFT (Hinton et al., 2015). Parameter Settings. We finetune each model for 218 steps as done by Raffel et al. (2020) with a 128 batch size. Each model is trained on 16 TPUs, with the base model taking between 24-32 hours to run4 (varies by task) with an input sequence length of 1024. We do not perform hyperparameter tuning: all models use the Adafactor optimizer with a constant learning rate of 1e-3, an Adafactor offset of 1M to account for pretraining steps, and loss normalizing factor of 218. For Prefix Global experiments, the default prefix size k is 512. For both Transient Global and Prefix Global, the local attention neighborhood r is set to 127, as done in LongT5 (Guo et al., 2022). Metrics. For quantitative results, we report BLEU4 (Papineni et al., 2002), ROUGE-L (Lin, 2004), and CIDEr (Vedantam et al., 2015) metrics from a single run. BLEURT (Pu et al., 2021), CLIPScore and RefCLIPScore (Hessel et al., 2021) are additionally reported in Appendix C for all results in the main text. We include qualitative results in Appendix B; we perform two qualitative studies\n4Example packing can further improve model efficiency.\nto (1) inspect the quality of generated text for all finetuning tasks and (2) discuss when and why images may help the more text-based tasks of page description generation and section summarization.\nAblations. We compare each attention at different input lengths. Our sequence length ablations also include experiments where Prefix Global and TGlobal have the same number of global tokens to strictly compare how they define global tokens. Then we ablate webpage inputs (section text, titles, structure, images, image captions) and use the best feature combinations for any remaining experiments. We run experiments with different model sizes (B16 or L16 T5 + ViT5) for Prefix Global at a 1k input sequence length. Lastly, we verify that WikiWeb2M\u2019s new annotations improve performance over prior work. Specifically, we ablate if the target, description, or context sections are input and if sections only from WIT vs. WikiWeb2M are input (since many text and multimodal context sections were not originally kept in the WIT dataset)."
        },
        {
            "heading": "4.1 Defining Prefix Global Attention Inputs",
            "text": "Each sample\u2019s images are always included as part of the input\u2019s prefix tokens. We ablated the number of images that contribute to each task\u2019s prefix and include ablations in Appendix D.3. We use six images for page description and one image input for section summarization and image captioning.\nWe describe each task\u2019s prefix below. Note that we remove the text that serves as the target summary or caption from our inputs to the model for each task; this ensures there is no model \u201ccheating.\u201d E.g., for section summarization, since we utilize the first sentence of a section as its pseudo target summary, we remove it from the inputs to the model.\nPage Description. We input the images, page URL, page title, and all sections (index, title, text, captions) in their structured page order. In addition to the images, URL, and page title participating in the prefix, we also include all section titles and section first sentences (up to 512 tokens). This outperformed keeping the section titles and text concatenated in order; see Appendix D.1.\nSection Summarization. The target section to be summarized is prepended to each sample\u2019s input sequence. This means the target section\u2019s index, title, non-summary text, images, and captions contribute to the global tokens of Prefix Global. Then the page\n5The base/large T5 model used 220M/770M parameters.\nURL, title, and remaining sections follow in order. Figure 3 illustrates how an input sequence is defined with Prefix Global for section summarization.\nContextual Image Captioning. Similar to section summarization, the target image and its originating section\u2019s content contribute to the prefix tokens (the index, title, text, and non-target captions), followed by the URL, page title, and context sections."
        },
        {
            "heading": "5 Results",
            "text": "We now detail experimental results, first evaluating performance and efficiency of each attention type at different sequence lengths. Then, we report input feature, model size, and annotation ablations."
        },
        {
            "heading": "5.1 Attention and Sequence Length",
            "text": "Performance Comparison. We begin by evaluating performance for each task (page description, section summarization, and contextual image captioning) when training T5 encoders with different attention types and input sequence lengths in Figure 4. Prefix Global always performs better than TGlobal. We include two Prefix Global settings: a fixed Prefix512 which sets 512 input tokens to the prefix (default used for all other experiments), as well as a PrefixTGlobal which assigns the same number of global tokens as TGlobal. PrefixTGlobal uses l 16 globals, where l is the input sequence length (TGlobal aggregates every 16 input tokens as a global token). This allows us to compare the way both attention mechanisms define global tokens.\nDespite TGlobal defining additional side inputs as global tokens, it consistently underperforms Prefix Global even with the same number of globals. This confirms that defining a special prefix from the input sequence is better than taking aggregates over the full sequence. In Appendix D.1, we also show that just using the prefix of the in-order page inputs for page description (as opposed to pulling out the section titles and first sentences) performs better than TGlobal. These results collectively show Prefix Global to be preferable to TGlobal. One key takeaway is that separating out more relevant inputs (via structure or other known biases like leading sentence bias) is a good idea.\nFull attention and Prefix Global generally have higher performance at longer sequence lengths. It is impressive that Prefix Global scales or maintains performance with larger sequences even when its number of globals is fixed to 512 (i.e., the number of globals is not scaled with respect to input"
        },
        {
            "heading": "1024 325,632 916,480 1,048,576",
            "text": ""
        },
        {
            "heading": "2048 782,336 2,225,152 4,194,304",
            "text": "length). On the other hand, while TGlobal scales the number of globals to sequence length, its performance does not consistently scale. E.g., performance plateaus or even drops at 4k input sequence length for page description and section summarization, respectively. This may be because TGlobal defines globals as aggregates over the full input sequence, which could introduce more noise or less semantically rich text at longer sequence lengths.\nOne anomalous result occurs for image captioning: Prefix Global with 256 globals (PrefixTGlobal at 4k input length) outperforms the 512 variant; as we did not exhaustively ablate the number of global tokens, further performance gains could be reached by optimizing the number of globals per task.\nPrefix Global outperforms full attention at all sequence lengths on image captioning, which may be due to the global tokens including the target image and most relevant section content. This should ease the process of learning the most relevant tokens by allowing full attention between the first k target section tokens with the rest of the input sequence, while contextual information from other sections has local attention. For section summarization and\n6Full attention will OOM at 4k input length.\npage description, Prefix Global outperforms full attention at the 4k sequence length, while full attention cannot fit in memory. Given that the entire page\u2019s content can be useful for generating a page level description, it is sensible that full attention may perform better for smaller sequence lengths as it allows for attention between all input tokens.\nEfficiency Comparison. Prefix Global can outperform full attention, while only requiring O((l \u2212 k) \u00b7 r + k \u00b7 l) attention complexity for k global tokens. When implementing the Prefix Global attention, we manually created tensors representing block sparsity to avoid computing the full cross attention. We provide the approximate number of FLOPs for each attention mechanism in Table 4 when ignoring the number of attention heads and embedding dimension. At the 2k input sequence length Prefix Global requires about half the FLOPs of full attention, and experimentally takes about half the time to complete the same experiment with all other settings fixed. The number of FLOPs of Prefix Global at 4k is just over those of full attention at the 2k input length, and is able to fit into memory and maximize performance for each task.\nLastly, the full attention and Prefix Global FLOP difference grows with sequence length. This can sometimes be seen experimentally: performance gaps are larger between full and Prefix Global for page description at 2k vs. 1k (0.20 vs. 0.09)."
        },
        {
            "heading": "5.2 Feature Ablations",
            "text": "We investigate the role of each input feature with Prefix Global attention and fix sequence length to 1k. Starting with just the text available from web-\n7Structure features are kept if they were helpful in textonly experiments. I.e., they are included for page description and image captioning, but not for section summarization.\npage sections, we incrementally add section titles, indices and special tokens defining section structure (the struct column of Table 5), the captions of images within each section, and the images. Each input boosts performance8 except section structure which has mixed results; for multimodal experiments we include these extra tokens if they helped in the text-only experiments. This may be due to these extra tokens consuming global tokens in the prefix that otherwise could have been more useful.\nImages and their captions both improve performance, but result in the highest performance for each task when used in tandem. This illustrates that even when text captions are available, having their visual counterpart is beneficial. In Table 5,\n8BLEU-4 is less consistent than ROUGE-L and CIDEr.\nwhen we include captions for the image captioning task, it refers to context captions from other images in the page that never serve as target images. Interestingly, this boosts performance. We suspect contextual captions help the model to learn the style of captions we aim to generate."
        },
        {
            "heading": "5.3 Pretrained Checkpoint and Model Size",
            "text": "In Table 6, we perform additional experiments with ViT pretrained on JFT and large T5/ViT models. Unsurprisingly, larger models result in better performance. For page description and section summarization, scaling the model size results in larger performance gains than the impact of any individual feature we ablated. On the other hand, model size has smaller gains for image captioning compared to the impact of our feature ablations; the worst to best performance gap changed by an average of 17.66% for feature ablations and only by 2.43% for model size, where we average the performance delta of BLEU-4, ROUGE-L, and CIDEr.\nPreference to ViT representations pretrained on JFT or ImageNet varies by task: section summarization tends to prefer JFT, while page description and image captioning consistently perform best with large ImageNet trained representations."
        },
        {
            "heading": "5.4 Comparison to WIT Annotations",
            "text": "The proposed WikiWeb2M is a superset of WIT. For the same set of webpages, we unify all sections into a webpage sample and reintroduce millions of sections and images that were not kept in WIT. Table 7 contains runs when using the original WIT data, the WIT data reprocessed to join the page sections it originally contained, and our WikiWeb2M.\nFor section summarization, the page description is more important than the other context sections. The page description may be more generally rele-\nvant to all sections, while each section to be summarized contains a distinct topic compared to the context sections from other parts of the webpage. Lastly, we find WikiWeb2M\u2019s additional context sections improve captioning performance the most compared to those already available in WIT (comparing the last two rows of Table 7). This confirms the importance of the new annotations in WikiWeb2M compared to those available in prior work."
        },
        {
            "heading": "6 Related Work",
            "text": "Webpage tasks have been studied with text only HTML for web element classification, HTML description generation, and web navigation. Gur et al. (2022) proposed finetuning Large Language Models for these tasks. Reinforcement Learning methods have also trained agents to perform language commands in handcrafted web environments (Gur et al., 2019; Liu et al., 2018; Jia et al., 2019).\nWikipedia has previously been used to develop downstream tasks. For example, WIT (Srinivasan et al., 2021) released image-caption pairs from Wikipedia, in addition to some contextual section text. While WIT does not contain all of the page content, Nguyen et al. (2022) studied contextual image captioning with the available annotations. This is a webpage task and not strictly an imagetext problem, as additional section text is included to aid in Wikipedia image captioning, where captions often contain finer-grained, knowledge based information. AToMiC also studied ways to improve multimodal retrieval with Wikipedia by defining more realistic evaluation sets (Yang et al., 2023).\nAghajanyan et al. (2022) proposed CM3, a Transformer with a causally masked pretraining objective. While CM3 relied on pretraining data\nfrom the web containing the images and HTML of a webpage, this dataset was not open sourced. Their results illustrated that rich HTML data could be used to learn representations for tasks such as image generation, image in-filling, and entity disambiguation and linking. This demonstrates that webpage data can generalize to non-webpage tasks, but leaves webpage specific problems unexplored.\nTo our knowledge there is no open source multimodal webpage data that captures all modalities. C4 was recently extended to a multimodal version, MMC4 (Zhu et al., 2023). However, MMC4 does not retain structure, and instead uses CLIP scores to noisily match images to chunks of text that it could be aligned with. MMC4 has not yet been used for pretraining or downstream applications. In mobile apps, the closest domain to webpages, there are two open source datasets that contain all modalities (text, image, and structure): Rico (Deka et al., 2017) and MoTIF (Burns et al., 2022)."
        },
        {
            "heading": "7 Conclusion",
            "text": "In this paper we study three generative tasks for multimodal webpage understanding: page description generation, section summarization, and contextual image captioning. To do so, we present the WikiWeb2M dataset, which retains all of the text, images, and structure from more than 2M pages. We propose a new attention, Prefix Global, which outperforms full attention by allowing the most salient text and images to specially attend to all inputs. Extensive ablations on attention mechanism, sequence length, model size and checkpoint, input features and section type reveal the most impactful factors on our benchmark suite and verify using WikiWeb2M to study webpage understanding."
        },
        {
            "heading": "Limitations",
            "text": "The WikiWeb2M dataset reprocessed the webpages available in WIT. We begin with only the English subset of WIT, while it originally contained 108 languages. Our dataset is limited to English and does not cover the vast multilingual data on Wikipedia. We can extend our dataset to cover all languages in WIT, but acknowledge it is monolingual to date.\nFor page description generation and section summarization, we use pseudo summaries that are readily available from Wikipedia pages. While this is desirable from a scalability perspective and is practiced in other works, it can limit the evaluation quality of these tasks. However, we did perform a small scale pilot to collect human annotations for the section summarization task in which we asked the annotators if the first sentence sufficed; 94% of the time the majority vote out of five was yes. Pseudo summaries have also been used for other tasks like summarizing instructional videos (Narasimhan et al., 2022).\nFor the model settings we explore, we did not try all exhaustive combinations of features, attention mechanism, model configuration, and input length. We also only use T5 variants, but note T5 is stateof-the-art for generation style problems. Lastly, we design our set of fine-tuning tasks for generative tasks. Our work currently does not include tasks like webpage taxonomy classification or webpage retrieval, but additional tasks like topic classification could be performed with WikiWeb2M."
        },
        {
            "heading": "Ethics Statement",
            "text": "While the Internet provides a vast and rich domain to collect data from, it also has potential risks. Wikipedia is a highly curated and monitored knowledge base of articles, but it can be edited by the public, which can create potential quality risks. Additionally, Wikipedia is a largely fact-based domain, where incorrectly summarizing an article could result in misinformation. We hope our dataset can be used as a new resource to improve the accuracy and factual correctness of text generation machine learning models. As we use Wikipedia data, there is no user data nor P.I.I. in the proposed WikiWeb2M dataset. Additionally, we ran analysis to remove a small subset of pages with potentially sensitive topics (e.g., natural disasters, funeral, blood)."
        },
        {
            "heading": "Acknowledgements",
            "text": "This work is supported, in part, by the Google Ph.D. Fellowship program."
        },
        {
            "heading": "A Additional Dataset Details",
            "text": ""
        },
        {
            "heading": "A.1 Dataset Processing",
            "text": "We now provide additional details on the filters and data processing steps used to convert WikiWeb2M into our three downstream task datasets.\nFor page description, we retain a page from WikiWeb2M if it is not list-heavy and contains at least two sections with image or text content that do not contain a list or table. A small subset of Wikipedia pages are essentially lists9; we consider pages that\n9For example, https://en.wikipedia.org/wiki/ List_of_mammals_of_the_United_States\nexplicitly have \u201clist_of\u201d in their URL to be list heavy-pages and remove them. We also remove pages with fewer than two rich sections to ensure there is enough content for a page description task to be appropriate.\nFor a page section to serve as a target section for the task of section summarization, we require it to have at least five sentences, contain neither a table nor list, and not be the root section. We filter out the root because the root (first) section is often the page description, which we choose to keep as a distinct task.\nLastly for image captioning, we follow Nguyen et al. (2022) and use the reference description as the ground truth caption to be generated. However, unlike Nguyen et al. (2022), we do not input the attribution description for the target image to be captioned because it often heavily overlaps with the reference description (the reference description is used as the target text to generate). We further discuss this design choice in Appendix E. Again, we only consider images that were originally in WIT as target images to be captioned to ensure quality captions. We also only keep target images to be images which have a reference description of at least three words.\nWe additionally note that in WikiWeb2M we release all three types of captions for each image (the alternative-text, reference description, and attribution description), although not all three are always available for each image. The alternative text caption for images is used for accessibility purposes, and future work can focus on generating these descriptions, as opposed to the reference description."
        },
        {
            "heading": "A.2 Dataset Noise",
            "text": "Our dataset is built from the web, being processed from raw HTML. Noise may exist in our dataset in the formatting of text, e.g., mathematical formulas may have additional formatting text around them. In building our task datasets, the only noise we may introduce to the best of our knowledge is the processing of first sentences. The first sentence is separated from each section for the section summarization pseudo summary. It is also used as part of the global inputs for page description generation.\nSpecifically, the code used to parse the first sentence may prematurely split a sentence from a period that does not signal the end of the sentence. We did manually inspect samples early on and found this to be rare (e.g., 96/100 random samples were\nsplit correctly). Additionally, the sentences which were split prematurely could still be valid standalone sentences. Our released dataset does include all raw text, so others can reprocess it as they see fit. Figure 5 includes a code snippet for the sentence preprocessor we use and Table 8 illustrates the 4/100 prematurely split sentences from our random sample."
        },
        {
            "heading": "A.3 Dataset Analysis",
            "text": "We provide a side by side comparison of the fields we open source with the WikiWeb2M dataset compared to those pre-existing in WIT in Table 9. Note that in addition to the new fields we introduce, WikiWeb2M has different data processing which allows for a great number of sections and images to be retained, as seen in Tables 10-12. In the main text, Table 1 provided the aggregate counts over all splits for each section type and the number of images in our dataset versus prior work WIT. Tables 10, 11, and 12 provide the same statistics but now broken down for train, validation, and test sets, respectively.\nIn Figure 6, two WikiWeb2M dataset samples are visually illustrated. Specifically, the webpages on the topics of Succulent Plant and the Aguas Livres Aqueduct are shown on the left of the figure to visualize the original webpage for illustration purposes, and on the right we show a subset of the fields available in our WikiWeb2M samples for these same pages.\nFor additional data analysis we provide sequence length details. We include the median, average, 90th percentile, and maximum sequence length values for all input fields that make up a sample\u2019s input sequence in the train split. We define sequence length as the number of tokens after preprocessing and SentencePiece tokenization. See Table 13 for the sequence length of the page URL, title, description, section title and text, and image captions available (the alt-text, attribution description, and reference description).\nLastly, we provide aggregate high level statistics over the train split of WikiWeb2M in Table 14. This includes statistics on the number of sections per page (the number of total sections as well as the number of content sections, with the latter referring to sections which contain image or text content) and the number of images per page or section."
        },
        {
            "heading": "B Qualitative Examples",
            "text": "To qualitatively evaluate our model\u2019s ability on our suite of webpage understanding tasks, we include two qualitative analyses. First, we report several random output samples for page description generation, section summarization, and contextual image captioning in Tables 15-17. In Appendix B.1, we discuss our findings from these randomly sampled outputs. Next, in Appendix B.2, we include sample outputs whose metrics improved the most from the text-only model to the multimodal model, to explore why images can be helpful for webpage understanding tasks. In this second setting we investigate samples for page description and section summarization, since these tasks do not obviously require images in the same manner as contextual image captioning."
        },
        {
            "heading": "B.1 General Qualitative Examples",
            "text": "We start with our first analysis of randomly sampled outputs for all three fine-tuning tasks. Samples are selected from the test set."
        },
        {
            "heading": "B.1.1 Page Description Generation",
            "text": "Beginning with page description in Table 15, the target and predicted output text are provided for three random pages on the topics of the Horahora Power Station, Hedevig Lund, and Cape Nome.\nFor the first article on the Horahora Power Station, the predicted output text is quite coherent and well formed, despite containing some inaccurate details that conflict with the content of the webpage. Our model correctly references the date it was opened (1913) and the date the power station was flooded (1947). It also correctly references Lake Karapiro, which was formed and ultimately led to the submerging of the power station. On the other hand, the name of the \u201cWaikato\u201d River was swapped with \u201cWaihi.\u201d The model also referred to Horahora as a coal-fired station when it is actually a hydroelectric power station.\nNext, for the shorter article on Hedevig Lund, we find the model prediction to be very close to the target page description, although the painter\u2019s last names are slightly incorrect. Upon inspecting the page text, it appears the model included additional last names from the painter\u2019s parents\u2019 names (Ole Wilhelm Erichsen and Abel Marie n\u00e9e Isaachsen). In future work, methods that use pointer networks or direct copying from input text can be used to ameliorate these named entity failures.\nThe Cape Nome article is another example with a slightly longer page description (four sentences). This sample strongly illustrates the model\u2019s ability to convey a factually accurate and topically relevant page description even when the output text does not entirely contain the same content as the target description. Both the target and predicted descriptions begin with an overall summary of the topic, followed by geographical information. Our model\u2019s generated text also provides some historical context that is accurately summarized from the article, which the ground truth description does not. It seems our model attempts to summarize each of the sections on the page to form a coherent page description, which may differ from the target page description on Wikipedia (i.e., the Wikipedia page description need not cover topics from the entire webpage and can vary in style page to page)."
        },
        {
            "heading": "B.1.2 Section Summarization",
            "text": "For the task of section summarization, we include links to the webpage and the target section to be summarized from the article, and the target and predicted text in Table 16. Starting with the historical section on imageboards, we find that the target section summary is slightly more section specific than the predicted summary. I.e., the model generated summary \u201cFutallaby is a free and open-source imageboard script\u201d could be in sections other than\nthe historical section. That being said, the historical section does discuss that Futallaby is freely available, making the model predictions sensible, relevant, and factually correct.\nIn the second section summarization example on the topic of Jamaica\u2019s pirate economy, the target summary discusses Spanish resistance to English occupancy to provide context for the growing pirate economy. However, neither the target nor predicted section summary directly address the pirate economy. The model prediction is mostly accurate with correct references to English occupancy in 1655, but implicitly refers to Port Royal as a fort at the foot of the Blue Mountains, which geographically, is slightly questionable.\nThe third section summarization sample concerns the science fantasy novel \u201cThuvia, Maid of Mars,\u201d written by Edgar Rice Burroughs. Our trained model correctly references Burroughs finishing a novel by June 1914, but it was Thuvia, Maid of Mars he finished, not the book \u201cTarzan.\u201d The model seems to have confused multiple facts relating to this book: Thuvia, Maid of Mars was the fourth, not third, novel in the Barsoom series and Tarzan was not a part of this novel series (although Burroughs did also write Tarzan)."
        },
        {
            "heading": "B.1.3 Contextual Image Captioning",
            "text": "Lastly, in Table 17, we provide qualitative examples for contextual image captioning. We include links to the webpage and image (as well as illustrate the image within the table), plus the target and predicted image captions. In the first image from the Longpr\u00e9-le-Sec commune in France, while the target caption describes the main road in the image, a church is also present at the end of the road. This is confirmed by another image on the webpage which shows the same church. Thus, while our model did not predict the same exact target caption, it is still visually and factually accurate.\nThe second image is a photo of a painting of a\nwoman. This image has a more generic target caption, and it appears that our model tends to prefer generating detailed captions. As a result, it contains factually inaccurate information, stating the painting itself is of Rabindranath, the son of Maharshi Devendranath Tagore, who developed Santiniketan. Additionally, the generated caption states the mural is at the Ashram Complex in Santiniketan. While the painting is at Santiniketan, it is not confirmed to be at the Ashram Complex given the content of the article; while the article states \u201cIt [the Ashram Complex] has beautiful frescoes by Nandalal Bose,\u201d it remains ambiguous.\nThen, for the third randomly selected image from\nthe article on WWOR-TV, the model\u2019s generated caption is quite accurate to the image and also overlaps heavily with the content of the ground truth caption. The only subtle inaccuracy in the predicted text is that it states the TV logo was in use from the early 1970s to early 1980s, when it was actually used until the year 1987, which should be considered the late 1980s."
        },
        {
            "heading": "B.2 Unimodal to Multimodal Qualitative Examples",
            "text": "We now select a random subset of test set outputs for page description and section summarization for the best performing text-only model and the best performing multimodal (image and text) model. These models are base size T5/ViT models, as that was the model size used to perform feature ablations. Specifically, we select samples which have a higher ROUGE-L score with the multimodal model. For a random subset of 1000 samples, we reverse sort by the change in ROUGE-L between the unimodal and multimodal models, looking to inspect the samples most positively impacted by the inclusion of images. We hope to understand the settings under which images can aid these tasks.\nAs noted previously, we include this analysis for page description and section summarization, since these tasks may not require images, while image captioning inherently uses the input image. These examples are included in Tables 18 and 19. We did find that some of the most improved samples were an artifact of the text-only model repeating text tokens many times (a common failure of text generation models) and do not include those in our examples."
        },
        {
            "heading": "B.2.1 Page Description",
            "text": "Starting with page description generation, we include three examples in Table 18. For each page we link to the Wikipedia article and include the target page description, the page description generated by the text-only model (noted as text under the type column), and the page description generated by the multimodal model (noted as multi under the type column) for comparison.\nAcross these examples, we find one trend: images in the webpage can improve the generated description\u2019s specificity. In the page description task, we allow up to six images present on the page to be included. Starting with the first example from the webpage on Joan Carling, we see that the page description output from the multimodal model touches upon more topics and specifies details beyond a high-level sentence summary (the latter being more similar to the output from the textonly model). The multimodal model\u2019s generated text includes references to the Champions of the Earth Lifetime Achievement Award that was given to Joan Carling as well as greater detail about her relationship with the Philippines and being of the Igorot people. These events and concepts are both captured in the images in the page, which seem to help specify more detail from the page in the generated description.\nSimilarly, for the second example from the page on Alice Bemis Taylor, the text-only model generated a brief and high level summary about her. On the other hand, the images on the webpage portray numerous important locations (either her childhood home or community spaces she actively participated in or founded such as the Colorado Springs Arts Center and Day Nursery). As a result of in-\ncluding these images, the generated description now included more specific information regarding these places. By including these images, their corresponding captions and related textual concepts in the input sequence are more greatly attended to. This may be a byproduct of images always serving as global tokens with our Prefix Global attention mechanism.\nLastly, we include the descriptions generated for the webpage on the topic of the Franco-German Parliamentary Assembly. Again this example demonstrates that images can help maintain the correct level of specificity for the page description. In this case, the images help refocus the description to be more high-level than the description generated from the text model, which includes too much\ndetail (unlike the prior examples which tended towards too little). The two images on this page are centered around the respective meeting locations of the parliamentary in France and Germany, which helps focus the topic of the page description."
        },
        {
            "heading": "B.2.2 Section Summarization",
            "text": "In Table 19 we now include section summarization examples for several pages and include links to the webpage and specific section to be summarized. Slightly different from the trend found for page description generation, we see that images can improve the topical relevance of the generated section summary.\nFor example, with the webpage on Johann Joachim Quantz, the target section summary for the section on the Court of Frederick discusses how Quantz joined the court as a flute teacher to Frederick II. The image in this section illustrates Frederick the Great playing flute, which directs the model to focus on this instrument. The text-only model fails to mention anything about instruments in the summary, only discussing interpersonal relationships. On the other hand, the multimodal model mentions both the flute and specifically Frederick playing the flute, which is depicted in the included image and also explains the reason why Quantz joined the court.\nNext, the section on the 1800s history with respect to the St. Peter Catholic Church (in Montgomery, Alabama) continues to illustrate how images can improve topic relevance. While the text model references the year the church was founded, the multimodal model references the location, which the target summary also describes (although they reference the location of the church at different degrees of granularity). If a specific section does not have images, an image is included from the images in the chronological order they appear on the webpage. This means that for this section, the included image is the first image on the page - a map of Alabama showing the location of the St. Peter Catholic Church.\nA third example can be found with the P&T\nGroup article and 1900s-1950s Growth and Expansion section. The target section summary discusses the opening of the Shanghai office. The summary generated from the text-only model discusses the P&T Group expanding its business, which while correct, does not discuss the connection between opening new offices and expanding business. On the other hand, with the help of the HSBC Shanghai office image input to the multimodal, the multimodal model generated a description which explicitly mentioned the office opening. However, due to both Shanghai and Hong Kong locations being mentioned in the section, the model confuses the two in the generated section summary."
        },
        {
            "heading": "C Additional Metrics",
            "text": "For results reported in the main paper, we additionally report BLEURT, CLIPScore, and RefCLIPScore (the latter two are only relevant for contextual image captioning). See Tables 20, 21, 22. All results trends stay the same except that BLEURT is insensitive to most of the feature ablation results for section summarization and page description\ngeneration."
        },
        {
            "heading": "D Additional Model Ablations",
            "text": "We now provide additional experiments that could not fit into the main text, which include data processing ablations, additional feature ablations, and further comparisons to prior work."
        },
        {
            "heading": "D.1 Page Description",
            "text": "We first performed ablations on the data processing of WikiWeb2M for the page description generation task dataset. Specifically, we tried varying the number of content sections required for a particular page to be retained. See Table 23 for comparison of when we required two vs. three vs. four sections to contain image or text content without a table or list. We found the added samples improve performance consistently (i.e., the most performant setting is when the number of content sections required per page is set to two).\n10Structure features are kept if they were helpful in textonly experiments. I.e., they are included for page description and image captioning, but not for section summarization.\nWe also allow for text only pages to be kept in the dataset, as there are a small subset (roughly 2% of pages) that do not have any images after our processing. This could be due to the Wikipedia pages changing since the original WIT dataset was released, or because we only allow JPEG and PNG images while WIT contained some other image types like SVG. We include additional ablations in Table 24 showing the effect of including or not including these unimodal pages; their effect is minimal given how few there are in the dataset.\nIn Table 25, we show ablations for our prefix design with the page description generation task. Including the section titles and first sentences of each section in the prefix as global tokens improved performance for a majority of metrics, and we kept this set up for the rest of our experiments. We note that even when not using a specially designed prefix (i.e., flattening the section inputs and allowing the first 512 tokens to serve in the prefix, not separating out section titles or first sentences), the Prefix Global attention mechanism still outperforms Transient Global. This follows the principal from leading sentence bias that earlier information in the input text is more important. Thus, if you have a priori knowledge that a particular part of the input is more important than others, separating it into the prefix of our attention mechanism can be effective."
        },
        {
            "heading": "D.2 Contextual Image Captioning",
            "text": "As image captioning inherently requires images, we performed additional feature ablations on the text features while always including the image (see\nrows 5-9 in Table 26). We verify in row 5 that when inputting only the image and no contextual text, it is incredibly difficult to generate strong captions for these images which contain a lot of fine-grained information. However, in support of the importance of having both images and text inputs, we find that for every text-only to multimodal comparison (where all features are the same except images are included in the latter), the multimodal setting always results in substantial performance gains. For example, quantitatively comparing row 1 and row 6 in Table 26, where either only the section text is input versus the section text and image to be captioned are input, the performance differences are: BLEU-4 9.83 vs. 11.27, ROUGE-L 33.00 vs. 36.90, and CIDEr 133.70 vs. 153.44.\nAgain, this differs from the findings of Nguyen et al. (2022); their experimental design likely minimized the impact of images because they also feed in the attribution description as a textual input, which often is quite similar to the target caption. As a result, the model can \u201ccheat\u201d and utilize the attribution description while not relying on the visual input. We have more discussion regarding this in Appendix E."
        },
        {
            "heading": "D.3 All Tasks",
            "text": "For each task in our WikiWeb2M suite, we also ablated the number of images input to the model. These additional ablations are shown in Table 27. Results in the main text use the 90th percentile number of images per page, six, for page description generation, and only one for section summarization\nand image captioning. Here we also try the average value for number of images per page which is three. We include these ablations for contextual image captioning as well, as we were curious whether having contextual (non-target) images input to the model would help at all. Ultimately it sometimes hurt performance, likely adding noise and making it more challenging for the model to discern which input image was the target image to be captioned. We only used the single target image as input for the rest of our experiments."
        },
        {
            "heading": "E Contextual Image Captioning Task Design",
            "text": "We now provide additional discussion on the task of contextual image captioning and how our input design differs from prior work. Nguyen et al. (2022) recently introduced the task of contextual image captioning. We found our metrics were lower than those they reported for the task and investigated the causes. We ran additional experiments for contextual image captioning with the exact same sample inputs as Nguyen et al. (2022). Specifically, we tried only using the page description, target image, target image\u2019s section text, and the attribution description as a sample\u2019s inputs.\nBy including the attribution description (which often heavily overlaps with the target caption to be generated), our performance is much higher, nearly matching prior work even when using different data splits (the prior work\u2019s dataset splits are not released). We report these reproduced results for our splits in Table 28. As discussed earlier, for our contextual image captioning task, we chose not to input the attribution description of an image given how much overlap it has with the target caption (the reference description). In terms of other experimental differences, we also use ViT (Dosovitskiy et al., 2021) image representations while prior work used ResNet-152 (He et al., 2016), although both were pretrained on ImageNet (Deng et al., 2009)."
        },
        {
            "heading": "F Section Summarization Pseudo Summaries",
            "text": "We were motivated to study the task of section summarization as a subproblem of Webpage Story Generation, which is the task of converting a webpage to an Instagram Story-like format. It consists of one multimodal Story page or slide per section, containing a section summary and paired image (from the same webpage). Our section sum-\nmarization task is a subpart of this problem and we proposed an improvement over the News, textonly CNN/DailyMail PEGASUS model used to generate summaries in the prior Wiki2Story work by Nkemelu et al. (2023). Specifically, our formulation of multimodal section summarization is desirable so that we can also take images as contextual input, as the goal is to generate multimodal content for a user to consume on the topic of a particular webpage (in this case, a Wikipedia article).\nOriginally, we attempted to collect human written section summaries ourselves. But when running an initial data collection pilot we found that when explaining the intended application of webpage stories, a majority of the annotators deemed the first sentence to almost always (94% of the time) be a good enough pseudo summary. In the other cases when a majority of annotators voted otherwise, we found the annotation quality was too poor to use the collected written summaries. It proved very difficult to collect free form summaries of Wikipedia content. For both of these reasons we continued modeling section summarization with our pseudo summaries (the first sentence of the section).\nTo perform this data collection pilot, we used an internal crowd sourcing platform to hire seven crowd workers. They were located in India and paid hourly wages competitive for their locale. They have standard rights as contractors. The last four pages of our paper include a PDF of our instructions to annotators. We also tried to collect labels for well suited images for each section but ultimately did not use these annotations.\nINSTRUCTIONS FOR ANNOTATORS\nWe are collecting annotations for a task called \u201cWiki2Story.\u201d Wiki2Story has so far been defined as a data conversion process from Wikipedia webpages to Wikipedia \u201cstories.\u201d The goal is to turn a Wikipedia webpage into an Instagram-like story which contains one story page per Wikipedia section. Each story page includes a section summary and paired image; see the below examples. We provide two examples of what an Introduction and History story page may look like for these sections of the Apple Wikipedia article.\nWe want to obtain annotations of these section summaries and have you select the most appropriate image to pair with it. In particular, the section summary should be a highlight of the section content. The highlight should: be self contained, condense the section\u2019s factual information into a sentence of ideally fewer than 30 words, and retain enough detail for the reader to learn something from the story page. The highlight is supposed to be an educational glimpse of the full section\u2019s content, remaining fully true to the original text.\nWe provide examples below of both strong and weak summaries for our use case. Note that we expect the summary to contain only factually correct information from what is provided in the original section text. We provide weak summary examples to demonstrate ways the summary style can be incorrect.\nExample 1: The Proverb section of the Apple Wikipedia article. SECTION TEXT The proverb, \"An apple a day keeps the doctor away\", addressing the supposed health benefits of the fruit, has been traced to 19th-century Wales, where the original phrase was \"Eat an apple on going to bed, and you'll keep the doctor from earning his bread\". In the 19th century and early 20th, the phrase evolved to \"an apple a day, no doctor to pay\" and \"an apple a day sends the doctor away\"; the phrasing now commonly used was first recorded in 1922. Despite the proverb, a 2015 study found no evidence that eating an apple daily prevents visits to a physician.\nSTRONG SUMMARIES\n\u2705 The proverb \u201cAn apple a day keeps the doctor away\u201d has been traced back to 19th-century Wales, but has yet to be proven scientifically. \u2705 \u201cAn apple a day keeps the doctor away\u201d originated in Wales in the 19th century with the phrasing \u201cEat an apple on going to bed, and you\u2019ll keep the doctor from earning his bread.\u201d \u2705 The proverb \u201cAn apple a day keeps the doctor away\u201d has had multiple phrasings over the years, first being traced to 19th-century Wales.\nWEAK SUMMARIES These are poor summaries because they use words like \u201cthis\u201d or \u201cit\u201d, have a dialogue-like style, or overly abstract the factual information from the Wikipedia section. \u2716 This section talks about the phrase \u201cAn apple a day keeps the doctor away\u201d and all of\nthe ways it has been said. \u2716 It talks about how apples don\u2019t actually prevent doctor visits. \u2716 The apple proverb has existed for centuries.\nExample 2: The Breeds section of the Dog Wikipedia article. SECTION TEXT Dogs are the most variable mammal on earth with around 450 globally recognized dog breeds. In the Victorian era, directed human selection developed the modern dog breeds, which resulted in a vast range of phenotypes. Most breeds were derived from small numbers of founders within the last 200 years, and since then dogs have undergone rapid phenotypic change and were formed into today's modern breeds due to artificial selection imposed by humans. The skull, body, and limb proportions vary significantly between breeds, with dogs displaying more phenotypic diversity than can be found within the entire order of carnivores. These breeds possess distinct traits related to morphology, which include body size, skull shape, tail phenotype, fur type and color. Their behavioral traits include guarding, herding, and hunting, retrieving, and scent detection. Their personality traits include hypersocial behavior, boldness, and aggression, which demonstrates the functional and behavioral diversity of dogs. As a result, present day dogs are the most abundant carnivore species and are dispersed around the world. The most striking example of this dispersal is that of the numerous modern breeds of European lineage during the Victorian era.\nSTRONG SUMMARIES \u2705 Around 450 dog breeds have been globally recognized, making dogs the most variable\nmammal with significant differences in behavioral traits and physical characteristics. \u2705 The breeding of dogs during the Victorian Era resulted in around 450 globally recognized\ndog breeds from a small number of founders. \u2705 Dogs are the most variable mammal on Earth, having significant variance in skull, body,\nand limb proportions, also differing personality traits like sociability and boldness. \u2705 Present day dogs are the most abundant carnivore with around 450 recognized dog\nbreeds. \u2705 With around 450 globally recognized breeds and high variance in both physical and\nbehavioral characteristics, dogs display more phenotypic diversity than all other carnivores combined.\nWEAK SUMMARIES These are poor summaries because they are either too brief, reduce and abstract the factual information too much, or use words like \u201cthis\u201d and \u201cit.\u201d \u2716 There are many types of dogs on Earth. \u2716 This section discusses the difference behavioral, physical, and personality traits dog\nbreeds can have. \u2716 It describes how dog breeding was done to result in 450 breeds and mentions that dogs\nare the most variable mammal.\nWhen selecting images, we want you to choose the image you see best fit to go with the section content and summary text. It should be topically relevant and the image you feel is most visually appealing.\nUI PLUGIN EXAMPLE (UPDATED) ____________________________________________________________________________ Read the below section from a Wikipedia page. The first sentence is highlighted in yellow. Does the first sentence provide a strong and concise (fewer than 30 words) summary of the contents of the entire section?\nIf you need additional context on this section\u2019s text and or topic to answer this question, you can click here to see the description of the Wikipedia page. Otherwise, continue with the annotation task.\nSection Title: Etymology Wikipedia Page Title: Apple ____________________________________________________\nThe word apple, formerly spelled \u00e6ppel in Old English, is derived from the Proto-Germanic root *ap(a)laz, which could also mean fruit in general. This is ultimately derived from Proto-Indo-European *ab(e)l-, but the precise original meaning and the relationship between both words[clarification needed] is uncertain.\nAs late as the 17th century, the word also functioned as a generic term for all fruit other than berries but including nuts\u2014such as the 14th century Middle English word appel of paradis, meaning a banana. This use is analogous to the French language use of pomme.\nYes, it well summarizes the whole section\nNo, it does not well summarize the whole section\nIF \u201cHERE\u201d WAS CLICKED ON FOR ADDITIONAL CONTEXT ABOVE, SHOW THE FOLLOWING PAGE DESCRIPTION TEXT:\nAn apple is an edible fruit produced by an apple tree (Malus domestica). Apple trees are cultivated worldwide and are the most widely grown species in the genus Malus. The tree originated in Central Asia, where its wild ancestor, Malus sieversii, is still found today. Apples\nhave been grown for thousands of years in Asia and Europe and were brought to North America by European colonists. Apples have religious and mythological significance in many cultures, including Norse, Greek, and European Christian tradition.\nIF THE ANSWER TO THE ABOVE QUESTION WAS NO, SHOW THE FOLLOWING SUMMARIZATION TASK: ____________________________________________________________________________ Please write a single sentence summarizing the Wikipedia section. Keep the summary factually accurate to the original text and summarize the content concisely; try to use 15-30 words at most. The goal is to provide an educational, interesting snippet for a story page of this section.\nSection Title: Etymology Wikipedia Page Title: Apple ____________________________________________________________________________"
        }
    ],
    "title": "A Suite of Generative Tasks for Multi-Level Multimodal Webpage Understanding",
    "year": 2023
}