{
    "abstractText": "Temporal knowledge graph (TKG) forecasting benchmarks challenge models to predict future facts using knowledge of past facts. In this paper, we develop an approach to use in-context learning (ICL) with large language models (LLMs) for TKG forecasting. Our extensive evaluation compares diverse baselines, including both simple heuristics and state-of-the-art (SOTA) supervised models, against pre-trained LLMs across several popular benchmarks and experimental settings. We observe that naive LLMs perform on par with SOTA models, which employ carefully designed architectures and supervised training for the forecasting task, falling within the (-3.6%, +1.5%) Hits@1 margin relative to the median performance. To better understand the strengths of LLMs for forecasting, we explore different approaches for selecting historical facts, constructing prompts, controlling information propagation, and parsing outputs into a probability distribution. A surprising finding from our experiments is that LLM performance endures (\u00b10.4% Hit@1) even when semantic information is removed by mapping entities/relations to arbitrary numbers, suggesting that prior semantic knowledge is unnecessary; rather, LLMs can leverage the symbolic patterns in the context to achieve such a strong performance. Our analysis also reveals that ICL enables LLMs to learn irregular patterns from the historical context, going beyond frequency and recency biases1.",
    "authors": [
        {
            "affiliations": [],
            "name": "Dong-Ho Lee"
        },
        {
            "affiliations": [],
            "name": "Kian Ahrabian"
        },
        {
            "affiliations": [],
            "name": "Woojeong Jin"
        },
        {
            "affiliations": [],
            "name": "Fred Morstatter"
        },
        {
            "affiliations": [],
            "name": "Jay Pujara"
        }
    ],
    "id": "SP:172d36eb489d2adf3329ec0c5da4adad289570d1",
    "references": [
        {
            "authors": [
                "Jon Scott Armstrong."
            ],
            "title": "Principles of forecasting: a handbook for researchers and practitioners, volume 30",
            "venue": "Springer.",
            "year": 2001
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners. Advances in neural information processing",
            "year": 2020
        },
        {
            "authors": [
                "Stephanie C.Y. Chan",
                "Adam Santoro",
                "Andrew Kyle Lampinen",
                "Jane X Wang",
                "Aaditya K Singh",
                "Pierre Harvey Richemond",
                "James McClelland",
                "Felix Hill."
            ],
            "title": "Data distributional properties drive emergent in-context learning in transformers",
            "venue": "Ad-",
            "year": 2022
        },
        {
            "authors": [
                "Aakanksha Chowdhery",
                "Sharan Narang",
                "Jacob Devlin",
                "Maarten Bosma",
                "Gaurav Mishra",
                "Adam Roberts",
                "Paul Barham",
                "Hyung Won Chung",
                "Charles Sutton",
                "Sebastian Gehrmann"
            ],
            "title": "Palm: Scaling language modeling with pathways",
            "year": 2022
        },
        {
            "authors": [
                "Alberto Garc\u00eda-Dur\u00e1n",
                "Sebastijan Duman\u010di\u0107",
                "Mathias Niepert."
            ],
            "title": "Learning sequence encoders for temporal knowledge graph completion",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4816\u20134821,",
            "year": 2018
        },
        {
            "authors": [
                "Shivam Garg",
                "Dimitris Tsipras",
                "Percy S Liang",
                "Gregory Valiant."
            ],
            "title": "What can transformers learn in-context? a case study of simple function classes",
            "venue": "Advances in Neural Information Processing Systems, 35:30583\u201330598.",
            "year": 2022
        },
        {
            "authors": [
                "Julia Gastinger",
                "Timo Sztyler",
                "Lokesh Sharma",
                "Anett Schuelke."
            ],
            "title": "On the evaluation of methods for temporal knowledge graph forecasting",
            "venue": "NeurIPS 2022 Temporal Graph Learning Workshop.",
            "year": 2022
        },
        {
            "authors": [
                "Rishab Goel",
                "Seyed Mehran Kazemi",
                "Marcus Brubaker",
                "Pascal Poupart."
            ],
            "title": "Diachronic embedding for temporal knowledge graph completion",
            "venue": "Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 3988\u20133995.",
            "year": 2020
        },
        {
            "authors": [
                "Michael Hahn",
                "Navin Goyal."
            ],
            "title": "A theory of emergent in-context learning as implicit structure induction",
            "venue": "arXiv preprint arXiv:2303.07971.",
            "year": 2023
        },
        {
            "authors": [
                "Zhen Han",
                "Peng Chen",
                "Yunpu Ma",
                "Volker Tresp."
            ],
            "title": "Explainable subgraph reasoning for forecasting on temporal knowledge graphs",
            "venue": "International Conference on Learning Representations.",
            "year": 2021
        },
        {
            "authors": [
                "Zhen Han",
                "Zifeng Ding",
                "Yunpu Ma",
                "Yujia Gu",
                "Volker Tresp."
            ],
            "title": "Learning neural ordinary equations for forecasting future links on temporal knowledge graphs",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Process-",
            "year": 2021
        },
        {
            "authors": [
                "Dan Hendrycks",
                "Nicholas Carlini",
                "John Schulman",
                "Jacob Steinhardt."
            ],
            "title": "Unsolved problems in ml safety",
            "venue": "arXiv preprint arXiv:2109.13916.",
            "year": 2021
        },
        {
            "authors": [
                "Rob J Hyndman",
                "Yeasmin Khandakar."
            ],
            "title": "Automatic time series forecasting: the forecast package for r",
            "venue": "Journal of statistical software, 27:1\u201322.",
            "year": 2008
        },
        {
            "authors": [
                "Woojeong Jin",
                "Rahul Khanna",
                "Suji Kim",
                "Dong-Ho Lee",
                "Fred Morstatter",
                "Aram Galstyan",
                "Xiang Ren."
            ],
            "title": "ForecastQA: A question answering challenge for event forecasting with temporal text data",
            "venue": "Proceedings of the 59th Annual Meeting of the Asso-",
            "year": 2021
        },
        {
            "authors": [
                "Woojeong Jin",
                "Meng Qu",
                "Xisen Jin",
                "Xiang Ren."
            ],
            "title": "Recurrent event network: Autoregressive structure inferenceover temporal knowledge graphs",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
            "year": 2020
        },
        {
            "authors": [
                "Timoth\u00e9e Lacroix",
                "Guillaume Obozinski",
                "Nicolas Usunier."
            ],
            "title": "Tensor decompositions for temporal knowledge base completion",
            "venue": "International Conference on Learning Representations.",
            "year": 2020
        },
        {
            "authors": [
                "Julien Leblay",
                "Melisachew Wudage Chekol."
            ],
            "title": "Deriving validity time in knowledge graph",
            "venue": "Companion proceedings of the the web conference 2018, pages 1771\u20131776.",
            "year": 2018
        },
        {
            "authors": [
                "Yaguang Li",
                "Rose Yu",
                "Cyrus Shahabi",
                "Yan Liu."
            ],
            "title": "Diffusion convolutional recurrent neural network: Data-driven traffic forecasting",
            "venue": "International Conference on Learning Representations.",
            "year": 2018
        },
        {
            "authors": [
                "Zixuan Li",
                "Xiaolong Jin",
                "Wei Li",
                "Saiping Guan",
                "Jiafeng Guo",
                "Huawei Shen",
                "Yuanzhuo Wang",
                "Xueqi Cheng."
            ],
            "title": "Temporal knowledge graph reasoning based on evolutional representation learning",
            "venue": "Proceedings of the 44th International ACM SIGIR",
            "year": 2021
        },
        {
            "authors": [
                "Stephanie Lin",
                "Jacob Hilton",
                "Owain Evans."
            ],
            "title": "Teaching models to express their uncertainty in words",
            "venue": "Transactions on Machine Learning Research.",
            "year": 2022
        },
        {
            "authors": [
                "Yushan Liu",
                "Yunpu Ma",
                "Marcel Hildebrandt",
                "Mitchell Joblin",
                "Volker Tresp."
            ],
            "title": "Tlogic: Temporal logical rules for explainable link forecasting on temporal knowledge graphs",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 36,",
            "year": 2022
        },
        {
            "authors": [
                "Farzaneh Mahdisoltani",
                "Joanna Biega",
                "Fabian Suchanek."
            ],
            "title": "Yago3: A knowledge base from multilingual wikipedias",
            "venue": "7th biennial conference on innovative data systems research. CIDR Conference.",
            "year": 2014
        },
        {
            "authors": [
                "Sameer Singh"
            ],
            "title": "Impact of pretraining term",
            "year": 2022
        },
        {
            "authors": [
                "Jerry Wei",
                "Jason Wei",
                "Yi Tay",
                "Dustin Tran",
                "Albert Webson",
                "Yifeng Lu",
                "Xinyun Chen",
                "Hanxiao Liu",
                "Da Huang",
                "Denny Zhou"
            ],
            "title": "Larger language models do in-context learning differently. arXiv preprint arXiv:2303.03846",
            "year": 2023
        },
        {
            "authors": [
                "Teven Le Scao",
                "Sylvain Gugger",
                "Mariama Drame",
                "Quentin Lhoest",
                "Alexander Rush."
            ],
            "title": "Transformers: State-of-the-art natural language processing",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System",
            "year": 2020
        },
        {
            "authors": [
                "Sang Michael Xie",
                "Aditi Raghunathan",
                "Percy Liang",
                "Tengyu Ma."
            ],
            "title": "An explanation of in-context learning as implicit bayesian inference",
            "venue": "International Conference on Learning Representations.",
            "year": 2022
        },
        {
            "authors": [
                "Zihao Zhao",
                "Eric Wallace",
                "Shi Feng",
                "Dan Klein",
                "Sameer Singh."
            ],
            "title": "Calibrate before use: Improving few-shot performance of language models",
            "venue": "International Conference on Machine Learning, pages 12697\u201312706. PMLR.",
            "year": 2021
        },
        {
            "authors": [
                "Cunchao Zhu",
                "Muhao Chen",
                "Changjun Fan",
                "Guangquan Cheng",
                "Yan Zhang."
            ],
            "title": "Learning from history: Modeling temporal knowledge graphs with sequential copy-generation networks",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, vol-",
            "year": 2021
        },
        {
            "authors": [
                "Andy Zou",
                "Tristan Xiao",
                "Ryan Jia",
                "Joe Kwon",
                "Mantas Mazeika",
                "Richard Li",
                "Dawn Song",
                "Jacob Steinhardt",
                "Owain Evans",
                "Dan Hendrycks."
            ],
            "title": "Forecasting future world events with neural networks",
            "venue": "Thirty-sixth Conference on Neural Information Pro-",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Knowledge Graphs (KGs) are prevalent resources for representing real-world facts in a structured way. While traditionally, KGs have been utilized for representing static snapshots of \u201ccurrent\u201d knowledge, recently, temporal KGs (TKGs) have gained popularity to preserve the complex temporal dynamics\n\u2217Authors contributed equally. 1https://github.com/usc-isi-i2/isi-tkg-icl\nof knowledge (Leblay and Chekol, 2018; Garc\u00edaDur\u00e1n et al., 2018; Goel et al., 2020; Lacroix et al., 2020). Recent endeavors in the area of TKGs have been focused on predicting future missing links (i.e., forecasting), given a query quadruple q = (si, pj , ?, tT ) and a set of associated historical facts Eq = {(s, p, o, t) |t < tT } \u2286 E (Gastinger et al., 2022). An illustrative example of this task is the question \u201cWhich team will win the Super Bowl in 2023?\u201d that can be expressed as q = (Super bowl, Champion, ?, 2023) and Eq = {(Super bowl, Champion, Los Angeles, 2022), (Super bowl, Champion, Tampa Bay, 2021), ...} (See Table 1). The ultimate objective is to identify the most suitable entity among all the football teams E (e.g., St Louis, Baltimore) to fill the missing field.\nPrior research on TKG forecasting has been primarily focused on developing supervised ap-\nproaches such as employing graph neural networks to model interrelationships among entities and relations (Jin et al., 2020; Li et al., 2021; Han et al., 2021b,a), using reinforcement learning techniques (Sun et al., 2021), and utilizing logical rules (Zhu et al., 2021; Liu et al., 2022). However, these techniques have prominent limitations, including the need for large amounts of training data that include thorough historical information for the entities. Additionally, model selection is a computationally expensive challenge as the stateof-the-art approach differs for each dataset.\nIn this paper, we develop a TKG forecasting approach by casting the task as an in-context learning (ICL) problem using large language models (LLMs). ICL refers to the capability of LLMs to learn and perform an unseen task efficiently when provided with a few examples of input-label pairs in the prompt (Brown et al., 2020). Prior works on ICL usually leverage few-shot demonstrations, where a uniform number of examples are provided for each label to solve a classification task (Min et al., 2022; Wei et al., 2023). In contrast, our work investigates what the model learns from irregular patterns of historical facts in the context. We design a three-stage pipeline to control (1) the background knowledge selected for context, (2) the prompting strategy for forecasting, and (3) decoding the output into a prediction. The first stage uses the prediction query to retrieve a set of relevant past facts from the TKG that can be used as context (Section 3.1). The second stage transforms these contextual facts into a lexical prompt representing the prediction task (Section 3.3). The third stage decodes the output of the LLM into a probability distribution over the entities and generates a response to the prediction query (Section 3.4). Our experimental evaluation performs competitively across a diverse collection of TKG benchmarks without requiring the time-consuming supervised training, or custom-designed architectures.\nWe present extensive experimental results on common TKG benchmark datasets such as WIKI (Leblay and Chekol, 2018), YAGO (Mahdisoltani et al., 2014), and ICEWS (Garc\u00eda-Dur\u00e1n et al., 2018; Jin et al., 2020). Our findings are as follows: (1) LLMs demonstrate the ability to make predictions about future facts using ICL without requiring any additional training. Moreover, these models show comparable performance to supervised approaches, falling within the (-3.6%,\n+1.5%) Hits@1 margin, relative to the median approach for each dataset; (2) LLMs perform almost identically when we replace entities\u2019 and relations\u2019 lexical names with numerically mapped indices, suggesting that the prior semantic knowledge is not a critical factor for achieving such a high performance; and (3) LLMs outperform the best heuristic rule-based baseline on each dataset (i.e., the most frequent or the most recent, given the historical context) by (+10%, +28%) Hits@1 relative margin, indicating that they do not simply select the output using frequency or recency biases in ICL (Zhao et al., 2021)."
        },
        {
            "heading": "2 Problem Formulation",
            "text": "In-Context Learning. ICL is an emergent capability of LLMs that aims to induce a state in the model to perform a task by utilizing contextual input-label examples, without requiring changes to its internal parameters (Brown et al., 2020). Formally, in ICL for classification, a prompt is constructed by linearizing a few input-output pair examples (xi,yi) from the training data. Subsequently, when a new test input text xtest is provided, ICL generates the output ytest \u223c PLLM(ytest | x1,y1, . . . ,xk,yk,xtest) where \u223c refers to decoding strategy.\nTemporal Knowledge Graph Forecasting. Formally, a TKG, G = (V,R, E , T ), is comprised of a set of entities V , relations R, facts E , and timestamps T . Moreover, since time is sequential, G can be split into a sequence of time-stamped snapshots, G = {G1,G2, . . . ,Gt, . . .}, where each snapshot, Gt = (V,R, Et), contains the facts at a specific point in time t. Each fact f \u2208 Et is a quadruple (s, p, o, t) where s, o \u2208 V , p \u2208 R, and t \u2208 T . The TKG forecasting task involves predicting a temporally conditioned missing entity in the future given a query quadruple, (?, p, o, t) or (s, p, ?, t), and previous graph snapshots G1:t\u22121 = {G1,G2, . . . ,Gt\u22121}. Here, the prediction typically involves ranking each entity\u2019s assigned score."
        },
        {
            "heading": "3 In-context Learning for Temporal Knowledge Graph Forecasting",
            "text": "In this work, we focus on 1) modeling appropriate history Eq for a given query quadruple q, 2) converting {Eq, q} into a prompt \u03b8q, and 3) employing ICL to get prediction yq \u223c PLLM(yq | \u03b8q) in a zero-shot manner. Here, the history Eq is modeled on the facts from the previous graph snapshots\nG1:t\u22121 = {G1,G2, . . . ,Gt\u22121}, and we employ token probabilities for yq to get ranked scores of candidate entities in a zero-shot manner. In the rest of this section, we study history modeling strategies (Sec 3.1), response generation approaches (Sec 3.2), prompt construction templates (Sec 3.3), and common prediction settings (Sec 3.4)."
        },
        {
            "heading": "3.1 History Modeling",
            "text": "To model the history Eq, we filter facts that the known entity or relation in the query q has been involved in. Specifically, given the query quadruple q = (s, p, ?, t) under the object entity prediction setting, we experiment with two different aspects of historical facts:\nEntity vs. Pair. Entity includes past facts that contain s, e.g., all historical facts related to Superbowl. In contrast, Pair includes past facts that contain both s and p, e.g., a list of (Superbowl, Champion, Year) as shown in Table 1.\nUnidirectional vs. Bidirectional. Unidirectional includes past facts F wherein s (Entity) or (s, p) (Pair) is in the same position as it is in q (e.g., Unidirectional & Pair \u2013 s and p served as subject and predicate in f \u2208 F). Bidirectional includes past facts F wherein s (Entity) or (s, p) (Pair) appear in any valid position (e.g., Bidirectional & Entity \u2013 s served as subject or object in f \u2208 F). As an example of the Bidirectional setting, given q = (Superbowl, Champion, ?, 2023), we include f = (Kupp, Played, Superbowl, 2022) because s (i.e., Superbowl) is present as the object in f . Moreover, in the Bidirectional setting, to preserve the semantics of the facts in the Eq, we transform the facts where s appears as an object by 1) swapping the object and subject and 2) replacing the relation with its uniquely defined inverse relation (e.g., (fs, fp, fo, ft) \u2192 (fo, f\u22121p , fs, ft))."
        },
        {
            "heading": "3.2 Response Generation",
            "text": "Given a prompt \u03b8q, we pass it to an LLM to obtain the next token probabilities. Then, we use the obtained probabilities to get a ranked list of entities. However, obtaining scores for entities based on these probabilities is challenging as they may be composed of several tokens. To address this challenge, we utilize a mapped numerical label as an indirect logit to estimate their probabilities (Lin et al., 2022)."
        },
        {
            "heading": "3.3 Prompt Construction",
            "text": "Given the history Eq and query q, we construct a prompt using a pre-defined template \u03b8. Specifically, given the query quadruple q = (s, p, ?, t) under the object entity prediction setting, we present two versions of the template \u03b8 with varying levels of information. Our assumption is that each entity or relation has an indexed I(\u00b7) (e.g., 0) and a lexical L(\u00b7) (e.g., Superbowl) form (See Table 2).\nIndex. Index displays every fact, (fs, fp, fo, ft) \u2208 E using the \u201cft:[I(fs), I(fp), nfo . I(fo)]\u201d template where fs, fo \u2208 V , fp \u2208 R, ft \u2208 T , nfo denotes an incrementally assigned numerical label (i.e., indirect logit), and I is a mapping from entities to unique indices. For example, in Table 1, we can use the following mappings are for the entities and relations, respectively: {Superbowl \u2192 0, St Louis \u2192 1, Baltimore \u2192 2} and {Champion \u2192 0}. The query q is then represented as \u201ct:[I(s), I(p),\u201d, concatenated to the end of the prompt. For subject entity prediction, we follow the same procedure from the other side.\nLexical. Lexical follows the same process as Index but uses lexical form L(\u00b7) of entity and relation. Each fact in (fs, fp, fo, ft) \u2208 E is represented as \u201cft:[L(fs),L(fp), nfo . L(fo)]\u201d and the query q is represented as \u201ct:[L(s),L(p),\u201d, concatenated to the end of the prompt."
        },
        {
            "heading": "3.4 Prediction Setting",
            "text": "All the historical facts in the dataset are split into three subsets, Dtrain, Dvalid, and Dtest, based on the chronological order with train < valid < test. Given this split, during the evaluation phase, the TKG forecasting task requires models to predict over Dtest under the following two settings:\nSingle Step. In this setting, for each test query, the model is provided with ground truth facts from past timestamps in the test period. Hence, after making predictions for a test query in a specific\ntimestamp, the ground truth fact for that query is added to the history before moving to the test queries in the next timestamp.\nMulti Step. In this setting, the model is not provided with ground truth facts from past timestamps in the test period and has to rely on its noisy predictions. Hence, after making predictions for a test query in a specific timestamp, instead of the ground truth fact for that query, we add the predicted response to the history before moving to the test queries in the next timestamp. This setting is considered more difficult as the model is forced to rely on its own noisy predictions, which can lead to greater uncertainty with each successive timestamp."
        },
        {
            "heading": "4 Experimental Setup",
            "text": ""
        },
        {
            "heading": "4.1 Datasets",
            "text": "For our experiments, we use the WIKI (Leblay and Chekol, 2018), YAGO (Mahdisoltani et al., 2014), ICEWS14 (Garc\u00eda-Dur\u00e1n et al., 2018), and ICEWS18 (Jin et al., 2020) benchmark datasets with the unified splits introduced in previous studies (Gastinger et al., 2022). Additionally, we extract a new temporal forecasting dataset from the Armed Conflict Location & Event Data Project (ACLED) project2 which provides factual data of crises in a particular region. We specifically focus on incidents of combat and violence against civilians in Cabo Delgado from January 1900 to March 2022, using data from October 2021 to March 2022 as our test set. This dataset aims to investigate whether LLMs leverage prior semantic knowledge to make predictions and how effective they are when deployed in real-world applications. Table 3 presents the statistics of these datasets.\n2https://data.humdata.org/organization/acled"
        },
        {
            "heading": "4.2 Evaluation",
            "text": "We evaluate the models on well-known metrics for link prediction: Hits@k, with k = 1, 3, 10. Following (Gastinger et al., 2022), we report our results in two evaluation settings: 1) Raw retrieves the sorted scores of candidate entities for a given query quadruple and calculates the rank of the correct entity; and 2) Time-aware filter also retrieves the sorted scores but removes the entities that are valid predictions before calculating the rank, preventing them from being considered errors. To illustrate, if the test query is (NBA, Clinch Playoff, ?, 2023) and the true answer is Los Angeles Lakers, there may exist other valid predictions such as (NBA, Clinch Playoff, Milwaukee Bucks, 2023) or (NBA, Clinch Playoff, Boston Celtics, 2023). In such cases, the time-aware filter removes these valid predictions, allowing for accurate determination of the rank of the \u201cLos Angeles Lakers.\u201d In this paper, we present performance with the time-aware filter."
        },
        {
            "heading": "4.3 Models.",
            "text": "As shown in Table 4, we perform experiments on four language model families. Among those, three are open-sourced: GPT2 (Radford et al., 2019), GPT-J (Wang, 2021), and GPT-NeoX (Black et al., 2022). All models employ the GPT-2 byte level BPE tokenizer (Radford et al., 2019) with nearly identical vocabulary size. In addition, we use the gpt-3.5-turbo model to analyze the performance of the instruction-tuned models. However, we do not directly compare this model to other models in terms of size since the actual model size is unknown. As for the TKG baselines, (i.e., RE-Net (Jin et al., 2020), RE-GCN (Li et al., 2021), TANGO (Han et al., 2021b), xERTE (Han et al., 2021a), TimeTraveler (Sun et al., 2021), CyGNet (Zhu et al., 2021), and TLogic (Liu et al., 2022)), we report the numbers presented in prior research (Gastinger et al., 2022). Appendix A.4 provides more details on baseline models."
        },
        {
            "heading": "4.4 ICL Implementation Details.",
            "text": "We implement our frameworks using PyTorch (Paszke et al., 2019) and Huggingface (Wolf et al., 2020). We first collate the facts f \u2208 Dtest based on the identical test query to eliminate any repeated inference. To illustrate, suppose there exist two facts in the test set denoted as (s, p, a, t) and (s, p, b, t) in the object prediction scenario. We consolidate these facts into (s, p, [a, b], t) and forecast only one for (s, p, ?, t). Subsequently, we proceed to generate an output for each test query with history by utilizing the model, obtaining the probability for the first generated token in a greedy approach, and sorting the probability. The outputs are deterministic for every iteration. We retain the numerical tokens corresponding to the numerical label n that was targeted, selected from the top 100 probability tokens for each test query. To facilitate multi-step prediction, we incorporate the top-k predictions of each test query as supplementary reference history. In this paper, we present results with k = 1. It is important to acknowledge that the prediction may contain minimal or no numerical tokens as a result of inadequate in-context learning. This can lead to problems when evaluating rank-based metrics. To mitigate this, we have established a protocol where\nthe rank of the actual value within the predictions is assigned a value of 100, which is considered incorrect according to our evaluation metric. For instruction-tuned model, we use the manual curated system instructions in Appendix A.3."
        },
        {
            "heading": "5 Experimental Results",
            "text": ""
        },
        {
            "heading": "5.1 In-context learning for TKG Forecasting",
            "text": "In this section, we present a multifaceted performance analysis of ICL under Index & Unidirection prompt strategy for both Entity and Pair history.\nQ1: How well does ICL fare against the supervised learning approaches? We present a comparative analysis of the top-performing ICL model against established supervised learning methodologies for TKG reasoning, which are mostly based on graph representation learning. As evident from the results in Table 5, GPT-NeoX with a history length of 100 shows comparable performance to supervised learning approaches, without any fine-tuning on any TKG training dataset.\nQ2: How do frequency and recency biases affect ICL\u2019s predictions? To determine the extent to which LLMs engage in pattern analysis, beyond simply relying on frequency and recency biases,\nwe run a comparative analysis between GPT-NeoX and heuristic-rules (i.e., frequency & recency) on the ICEWS14 dataset, with history length set to 100. frequency identifies the target that appears most frequently in the provided history while recency selects the target associated with the most recent fact in the provided history. The reason for our focus on ICEWS is that each quadruple represents a single distinct event in time. In contrast, the process of constructing YAGO and WIKI involves converting durations to two timestamps to display events across the timeline. This step has resulted in recency heuristics outperforming all of the existing models, showcasing the shortcoming of existing TKG benchmarks (See Appendix A.5). The experimental results presented in Table 6 demonstrate that ICL exhibits superior performance to rule-based baselines. This finding suggests that ICL does not solely rely on specific biases to make predictions, but rather it actually learns more sophisticated patterns from historical data.\nQ3: How does ICL use the sequential and temporal information of events? To assess the ability of LLMs to comprehend the temporal information of historical events, we compare the performance of prompts with and without timestamps. Specifically, we utilize the original prompt format, \u201cft:[I(fs), I(fr), nfo . I(fo)]\u201d, and the time-removed prompt format, \u201c[I(fs), I(fr), nfo . I(fo)]\u201d, make the comparison (See Appendix A.2). Additionally, we shuffle the historical facts in the time-removed prompt format to see how the model is affected by the corruption of sequential information. Figure 1 shows that the absence of time reference can lead to a deterioration in performance, while the random arrangement of historical events may further exacerbate this decline in performance. This observation implies that the model has the capability to forecast the subsequent event by comprehending the sequential order of events.\nQ4: How does instruction-tuning affect ICL\u2019s performance? To investigate the impact of instruction-tuning on ICL, we employ the gpt3.5-turbo model with manually curated system instruction detailed in Appendix 4.4. Since the size of this model is not publicly disclosed, it is challenging to make direct comparisons with other models featured in this paper. Moreover, since this model does not provide output probabilities, we are only able to report the Hit@1 metric.\nTable 7 showcases that the performance of the lexical prompts exceeds that of the index prompts by 0.024, suggesting that instruction-tuned models can make better use of semantic priors. This behavior is different from the other foundation LLMs, where the performance gap between the two prompt types was insignificant (See Figure 4 (a)).\nQ5: How does history length affect ICL\u2019s performance? To evaluate the impact of the history length provided in the prompt, we conduct a set of experiments using varying history lengths. For this purpose, we use the best performing prompt format for each benchmark, i.e., Entity for WIKI, YAGO, ICEWS18, and Pair for ICEWS14. Our results, as shown in Figure 2, indicate a consistent improvement in performance as the history length increases. This suggests that the models learn better as additional historical facts are presented. This observation is connected to few-shot learning in other domains, where performance improves as the number of examples per label increases. However, in our case, the historical patterns presented in the prompt do not explicitly depict the input-label mapping but rather aid in inferring the next step.\nQ6: What is the relation between ICL\u2019s performance and model size? Here, we analyze the connection between model size and performance. Our results, as presented in Figure 3, conform to the expected trend of better performance with larger models. This finding aligns with prior works showing the scaling law of in-context learning performance. Our findings are still noteworthy since they\nshow how scaling model size can facilitate more powerful pattern inference for forecasting tasks."
        },
        {
            "heading": "5.2 Prompt Construction for TKG Forecasting",
            "text": "To determine the most effective prompt variation, we run a set of experiments on all prompt variations, using GPT-J (Wang, 2021) and under the singlestep setting. Comprehensive results for prompt variations can be found in Appendix A.5.\nIndex vs. Lexical Our first analysis compares the performance of index and lexical prompts. This investigation aims to determine whether the model relies solely on input-label mappings or if it also incorporates semantic priors from pre-training to make predictions. Our results (Figure 4 (a)) show that the performance is almost similar (\u00b14e\u2212 3 on average) across the datasets. This finding is aligned with previous studies indicating that foundation models depend more on input-label mappings and are minimally impacted by semantic priors (Wei et al., 2023).\nUnidirectional vs. Bidirectional We next analyze how the relation direction in the history modeling impacts the performance. This analysis aims to ascertain whether including historical facts, where the query entity or pair appears in any position, can improve performance by offering a diverse array of historical facts. Our results (Figure 4 (b)) show that there is a slight decrease in performance when Bidirectional history is employed, with a significant drop in performance observed particularly in the ICEWS benchmarks. These observations may be attributed to the considerably more significant number of entities placed in both subject and object positions in ICEWS benchmarks than YAGO and WIKI benchmarks (See Appendix A.5). This finding highlights the necessity of having robust constraints on the historical data for ICL to comprehend the existing pattern better.\nEntity vs. Pair Finally, we examine the impact of the history retrieval query on performance. Our hypothesis posits that when the query is limited to a single entity, we can incorporate more diverse historical facts. Conversely, when the query is a pair, we can acquire a more focused set of historical facts related to the query. Our results (Figure 4 (c)) indicate that the performance of the model is dependent on the type of data being processed. Specifically, the WIKI and ICEWS18 benchmarks perform better when the query is focused on the entity, as a broader range of historical facts is available. In contrast, the ICEWS14 benchmark performs better when the query is focused on pairs, as the historical facts present a more focused pattern."
        },
        {
            "heading": "6 Related Works",
            "text": "Event Forecasting. Forecasting is a complex task that plays a crucial role in decision-making and safety across various domains (Hendrycks et al., 2021). To tackle this challenging task, researchers have explored various approaches, including statistical and judgmental forecasting (Webby and O\u2019Connor, 1996; Armstrong, 2001; Zou et al., 2022). Statistical forecasting involves leveraging probabilistic models (Hyndman and Khandakar, 2008) or neural networks (Li et al., 2018; Sen et al., 2019) to predict trends over time-series data. While this method works well when there are many past observations and minimal distribution shifts, it is limited to numerical data and may not capture the underlying causal factors and dependencies that affect the outcome. On the other hand, judgmental forecasting involves utilizing diverse sources of information, such as news articles and external knowledge bases, to reason and predict future events. Recent works have leveraged language models to enhance reasoning capabilities when analyzing unstructured text data to answer forecasting inquiries (Zou et al., 2022; Jin et al., 2021).\nTemporal Knowledge Graph. Temporal knowledge graph (TKG) reasoning models are commonly employed in two distinct settings, namely interpolation and extrapolation, based on the facts available from t0 to tn. (1) Interpolation aims to predict missing facts within this time range from t0 to tn, and recent works have utilized embeddingbased algorithms to learn low-dimensional representations for entities and relations to score candidate facts (Leblay and Chekol, 2018; Garc\u00eda-Dur\u00e1n et al., 2018; Goel et al., 2020; Lacroix et al., 2020); (2) Extrapolation aims to predict future facts beyond tn. Recent studies have treated TKGs as a sequence of snapshots, each containing facts corresponding to a timestamp ti, and proposed solutions by modeling multi-relational interactions among entities and relations over these snapshots using graph neural networks (Jin et al., 2020; Li et al., 2021; Han et al., 2021b,a), reinforcement learning (Sun et al., 2021) or logical rules (Zhu et al., 2021; Liu et al., 2022). In our work, we focus on the extrapolation setting.\nIn-context Learning. In-context learning (ICL) has enabled LLMs to accomplish diverse tasks in a few-shot manner without needing parameter adjustments (Brown et al., 2020; Chowdhery et al., 2022). In order to effectively engage in ICL, models can leverage semantic prior knowledge to accurately predict labels following the structure of in-context exemplars (Min et al., 2022; Razeghi et al., 2022; Xie et al., 2022; Chan et al., 2022; Hahn and Goyal, 2023), and learn the input-label mappings from the in-context examples presented (Wei et al., 2023). To understand the mechanism of ICL, recent studies have explored the ICL capabilities of LLMs with regards to the impact of semantic prior knowl-\nedge by examining their correlation with training examples (Min et al., 2022; Razeghi et al., 2022; Xie et al., 2022), data distribution (Chan et al., 2022), and language compositionality (Hahn and Goyal, 2023) in the pre-training corpus. Other recent works show that LLMs can actually learn input-label mappings from in-context examples by showing the transformer models trained on specific linear function class is actually predicting accurately on new unseen linear functions (Garg et al., 2022). More recently, there is a finding that largeenough models can still do ICL using input-label mappings when semantic prior knowledge is not available (Wei et al., 2023)."
        },
        {
            "heading": "7 Conclusion",
            "text": "In this paper, we examined the forecasting capabilities of in-context learning in large language models. To this end, we experimented with temporal knowledge graph forecasting benchmarks. We presented a framework that converts relevant historical facts into prompts and generates ranked link predictions through token probabilities. Our experimental results demonstrated that without any finetuning and only through ICL, LLMs exhibit comparable performance to current supervised TKG methods that incorporate explicit modules to capture structural and temporal information. We also discovered that using numerical indices instead of entity/relation names does not significantly affect the performance, suggesting that prior semantic knowledge is not critical for overall performance. Additionally, our analysis indicated that ICL helps the model learn irregular patterns from historical facts, beyond simply making predictions based on the most common or the most recent facts in the given context. Together, our results and analyses\ndemonstrated that ICL can be a valuable tool for predicting future links using historical patterns, and also prompted further inquiry into the potential of ICL for additional capabilities."
        },
        {
            "heading": "8 Limitations",
            "text": "There are certain limitations to our experiments. First, computing resource constraints restrict our experiments to small-scale open-source models. Second, our methodologies have constraints regarding models where the tokenizer vocabulary comprises solely of single-digit numbers as tokens, such as LLAMA (Touvron et al., 2023). The performance of such models exhibits a similar trend in terms of scaling law concerning model size and history length, but these models demonstrate inferior performance compared to other models of the same model size. Third, our methodologies have certain limitations with respect to link prediction settings. While real-world forecasting can be performed in the transductive setting, where the answer can be an unseen history, our approach is constrained to the inductive setting, where the answer must be one of the histories observed. There are further directions that can be pursued. The first is to explore transductive extrapolation link prediction using LLMs. The second is to analyze the effects of fine-tuning on the results. Lastly, there is the opportunity to investigate the new capabilities of ICL."
        },
        {
            "heading": "9 Acknowledgement",
            "text": "This work was funded in part by the Defense Advanced Research Projects Agency (DARPA) and Army Research Office (ARO) under Contract No. W911NF-21-C-0002 and Contract No. HR00112290106, and with support from the Keston Exploratory Research Award and Amazon.\nThe views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of DARPA, ARO or the U.S. Government."
        },
        {
            "heading": "A Appendix",
            "text": "A.1 Prompt Example\nGiven the test query at timestamp 571, prompt examples for Index and Lexical are shown in Figure 5. Here, we assume the entity dictionary contains \u201cIslamist Militia (Mozambique)\u201d as index 0, \u201cMeluco\u201d as 10, \u201cNamatil\u201d as 36, \u201cMuatide\u201d as 53, \u201cLimala\u201d as 54, and \u201cNacate\u201d as 55, while relation dictionary contains \u201cBattles\u201d as index 1 and \u201cViolence against civilians\u201d as 4. Also, the history setting is unidirectional entity setting where the history length is set to 5.\nA.2 Prompt Example for Analysis\nTo assess the ability of LLMs to comprehend the sequential information of historical events, we compare the performance of prompts with and without timestamps (See Section 5.1 Q3). Figure 6 shows the prompt examples for time-removed and shuffled version of prompts.\nA.3 System Instruction for Instruction-tuned models.\nFor the instruction-model, we use the manual curated system instructions to provide task descriptions and constraint the output format as follow:\nYou must be able to correctly predict the next {object_label} from a given text consisting of multiple quadruplets in the form of \"{time}:[{subject}, {relation}, {object_label}. {object}]\" and the query in the form of \"{time}:[{subject}, {relation},\" in the end.\nYou must generate only the single number for {object_label} without any explanation.\nA.4 Baseline Models\nRE-Net (Jin et al., 2020) leverages an autoregressive architecture that employs a two-step process for learning temporal dependency from a sequence of graphs and local structural dependency from the vicinity. The model represents the likelihood of a fact occurring as a probability distribution that is conditioned on the sequential history of past snapshots.\nRE-GCN (Li et al., 2021) also employs autoregressive architecture while it utilizes multi-layer relation-aware GCN on each graph snapshot to capture the structural dependencies among concurrent facts. Furthermore, the static properties of entities such as entity types, are also incorporated via a static graph constraint component to obtain better entity representations.\nTANGO (Han et al., 2021b) employs autoregressive architecture as well but the use of continuous-time embedding in encoding temporal and structural information is a distinguishing feature of the proposed method, as opposed to RENet (Jin et al., 2020) (Li et al., 2021) and RE-GCN which operate on a discrete level with regards to time.\nxERTE (Han et al., 2021a) employs an attention mechanism that can effectively capture the relevance of important aspects by selectively focusing on them. It employs a sequential reasoning approach over local subgraphs. This process begins with the query and iteratively selects relevant edges of entities within the subgraph, subsequently propagating attention along these edges. After multiple rounds of expansion, the final subgraph represents the interpretable reasoning path towards the predicted outcomes.\nTimeTraveler (Sun et al., 2021) employs reinforcement learning for forecasting. The approach involves the use of an agent that navigates through historical knowledge graph snapshots, commencing from the query subject node. Thereafter, it sequentially moves to a new node by leveraging temporal facts that are linked to the current node, with the ultimate objective of halting at the answer node. To accommodate the issue of unseen-timestamp, the approach incorporates a relative time encoding function that captures time-related information when making decisions.\nCyGNet (Zhu et al., 2021) leverages the statistical relevance of historical facts, acknowledging the recurrence of events in the temporal knowledge graph datasets. It incorporates two inference modes, namely Copy and Generation. The Copy mode determines the likelihood of the query being a repetition of relevant past facts. On the other hand, the Generation mode estimates the probability of each potential candidate being the correct prediction, using a linear classifier. The final forecast is obtained by aggregating the outputs of both modes.\nTLogic (Liu et al., 2022) mines cyclic temporal logical rules by extracting temporal random walks from a graph. This process involves the extraction of temporal walks from the graph, followed by a lift to a more abstract, semantic level, resulting in the derivation of temporal rules that can generalize to new data. Subsequently, the application of these rules generates answer candidates, with the body groundings in the graph serving as explicit and easily comprehensible explanations for the results obtained.\nA.5 Full Experimental Results"
        }
    ],
    "title": "Temporal Knowledge Graph Forecasting Without Knowledge Using In-Context Learning",
    "year": 2023
}